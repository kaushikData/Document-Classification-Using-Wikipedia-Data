{"id": "18934758", "url": "https://en.wikipedia.org/wiki?curid=18934758", "title": "ANDVT", "text": "ANDVT\n\nThe Advanced Narrowband Digital Voice Terminal (ANDVT) is a secure voice terminal for low bandwidth secure voice communications throughout the U.S. Department of Defense. Devices in the ANDVT family include the AN/USC-43 Tactical Terminal (TACTERM), the KY-99A Miniaturized Terminal (MINTERM), and the KY-100 Airborne Terminal (AIRTERM). ANDVT uses LPC-10 voice compression.\n\nThe functions of the MINTERM are similar to those of the TACTERM; its updated design includes an improved modular architecture, and it has been reduced in size. The MINTERM is lightweight, low-power, single channel, half-duplex, narrowband/wideband/wireline terminal providing secure voice and data communications with full key distribution and remote rekey capabilities. The MINTERM is certified to secure traffic up to TOP SECRET.\n\nThe MINTERM improvements include the following:\n\nThe AIRTERM is a lightweight, self-contained secure voice and data terminal that provides secure half-duplex voice, digital data, analog data, and remote-keying capabilities for transmission over radio circuits or wireline media. It is a wideband/narrowband terminal that interoperates with the TACTERM, MINTERM, VINSON, and Single Channel Ground and Airborne Radio System (SINCGARS). AIRTERM accepts classified analog voice information and uses LPC-10 at 2.4 kbit/s in narrowband voice modes and continuously variable slope delta (CVSD) modulation at 12 kbit/s and 16 kbit/s in wideband voice modes. The AIRTERM provides the same connectors, with similar functional pinouts, as the VINSON for the wideband operational modes. \n\n\n"}
{"id": "228312", "url": "https://en.wikipedia.org/wiki?curid=228312", "title": "Additive inverse", "text": "Additive inverse\n\nIn mathematics, the additive inverse of a number is the number that, when added to , yields zero.\nThis number is also known as the opposite (number), sign change, and negation. For a real number, it reverses its sign: the opposite to a positive number is negative, and the opposite to a negative number is positive. Zero is the additive inverse of itself.\n\nThe additive inverse of is denoted by unary minus: − (see the discussion below). For example, the additive inverse of 7 is −7, because 7 + (−7) = 0, and the additive inverse of −0.3 is 0.3, because −0.3 + 0.3 = 0 .\n\nThe additive inverse is defined as its inverse element under the binary operation of addition (see the discussion below), which allows a broad generalization to mathematical objects other than numbers. As for any inverse operation, double additive inverse has no net effect: .\nFor a number and, generally, in any ring, the additive inverse can be calculated using multiplication by −1; that is,  . Examples of rings of numbers are integers, rational numbers, real numbers, and complex numbers.\n\nAdditive inverse is closely related to subtraction, which can be viewed as an addition of the opposite:\nConversely, additive inverse can be thought of as subtraction from zero:\nHence, unary minus sign notation can be seen as a shorthand for subtraction with \"0\" symbol omitted, although in a correct typography there should be no space after unary \"−\".\n\nIn addition to the identities listed above, negation has the following algebraic properties:\n\nThe notation + is usually reserved for commutative binary operations; i.e., such that , for all , . If such an operation admits an identity element (such that for all ), then this element is unique (  ). For a given  , if there exists such that  , then is called an additive inverse of .\n\nIf + is associative ( for all , , ), then an additive inverse is unique. To see this, let and each be additive inverses of ; then\n\nFor example, since addition of real numbers is associative, each real number has a unique additive inverse.\n\nAll the following examples are in fact abelian groups:\n\n\nNatural numbers, cardinal numbers, and ordinal numbers, do not have additive inverses within their respective sets. Thus, for example, we can say that natural numbers \"do\" have additive inverses, but because these additive inverses are not themselves natural numbers, the set of natural numbers is not \"closed\" under taking additive inverses.\n\n"}
{"id": "44990517", "url": "https://en.wikipedia.org/wiki?curid=44990517", "title": "Amari distance", "text": "Amari distance\n\nThe Amari distance is a measure between two nonsingular matrices, useful for checking for convergence in independent component analysis algorithms and for comparing solutions.\n"}
{"id": "44283951", "url": "https://en.wikipedia.org/wiki?curid=44283951", "title": "Amit Sahai", "text": "Amit Sahai\n\nAmit Sahai (; born 1974) is an American computer scientist. He is a professor of computer science at the UCLA and the director of the Center for Encrypted Functionalities at UCLA.\n\nAmit Sahai was born in 1974 in Thousand Oaks, California, to parents who had\nimmigrated from India. He received a B.A. in mathematics with a computer\nscience minor from the University of California, Berkeley, summa cum laude, in\n1996.\nAt Berkeley, Sahai was named Computing Research Association Outstanding\nUndergraduate of the Year, North America, and was a member of the three-person\nteam that won first place in the 1996 ACM International Collegiate Programming Contest.\n\nSahai received his Ph.D. in Computer Science from MIT in 2000, and joined the\ncomputer science faculty at Princeton University. In\n2004 he moved to UCLA, where he currently holds the position of Professor of\nComputer Science.\n\nAmit Sahai's research interests are in security and cryptography, and theoretical\ncomputer science more broadly. He has published more than 100 original\ntechnical research papers.\n\nNotable contributions by Sahai include:\n\nSahai has given a number of invited talks including the 2004 Distinguished Cryptographer Lecture\nSeries at NTT Labs, Japan. He was named an Alfred P. Sloan Foundation Research\nFellow in 2002, received an Okawa Research Grant Award in 2007, a Xerox\nFoundation Faculty Award in 2010, and a Google Faculty Research Award in 2010.\nHis research has been covered by several news agencies including the BBC World\nService.\n"}
{"id": "46695602", "url": "https://en.wikipedia.org/wiki?curid=46695602", "title": "Asano contraction", "text": "Asano contraction\n\nIn complex analysis, a discipline in mathematics, and in statistical physics, the Asano contraction or Asano–Ruelle contraction is a transformation on a separately affine multivariate polynomial. It was first presented in 1970 by Taro Asano to prove the Lee–Yang theorem in the Heisenberg spin model case. This also yielded a simple proof of the Lee–Yang theorem in the Ising model. David Ruelle proved a general theorem relating the location of the roots of a contracted polynomial to that of the original. Asano contractions have also been used to study polynomials in graph theory.\n\nLet formula_1 be a polynomial which, when viewed as a function of only one of these variables is an affine function. Such functions are called separately affine. For example, formula_2 is the general form of a separately affine function in two variables. Any separately affine function can be written in terms of any two of its variables as formula_3. The Asano contraction formula_4 sends formula_5 to formula_6.\n\nAsano contractions are often used in the context of theorems about the location of roots. Asano originally used them because they preserve the property of having no roots when all the variables have magnitude greater than 1. Ruelle provided a more general relationship which allowed the contractions to be used in more applications. He showed that if there are closed sets formula_7 not containing 0 such that formula_5 cannot vanish unless formula_9 for some index formula_10, then formula_11 can only vanish if formula_9 for some index formula_13 or formula_14 where formula_15 Ruelle and others have used this theorem to relate the zeroes of the partition function to zeroes of the partition function of its subsystems.\n\nAsano contractions can be used in statistical physics to gain information about a system from its subsystems. For example, suppose we have a system with a finite set formula_16 of particles with magnetic spin either 1 or -1. For each site, we have a complex variable formula_17 Then we can define a separately affine polynomial formula_18 where formula_19, formula_20 and formula_21 is the energy of the state where only the sites in formula_22 have positive spin. If all the variables are the same, this is the partition function. Now if formula_23, then formula_24 is obtained from formula_25 by contracting the variable attached to identical sites. This is because the Asano contraction essentially eliminates all terms where the spins at a site are distinct in the formula_26 and formula_27.\n\nRuelle has also used Asano contractions to find information about the location of roots of a generalization of matching polynomials which he calls graph-counting polynomials. He assigns a variable to each edge. For each vertex, he computes a symmetric polynomial in the variables corresponding to the edges incident on that vertex. The symmetric polynomial contains the terms of degree equal to the allowed degree for that node. He then multiplies these symmetric polynomials together and uses Asano contractions to only keep terms where the edge is present at both its endpoints. By using the Grace–Walsh–Szegő theorem and intersecting all the sets that can be obtained, Ruelle gives sets containing the roots of several types of these symmetric polynomials. Since the graph-counting polynomial was obtained from these by Asano contractions, most of the remaining work is computing products of these sets.\n"}
{"id": "10993126", "url": "https://en.wikipedia.org/wiki?curid=10993126", "title": "Baldwin–Lomax model", "text": "Baldwin–Lomax model\n\nThe Baldwin–Lomax model is a 0-equation turbulence model used in computational fluid dynamics analysis of turbulent boundary layer flows.\n\n"}
{"id": "31324220", "url": "https://en.wikipedia.org/wiki?curid=31324220", "title": "Barrier cone", "text": "Barrier cone\n\nIn mathematics, specifically functional analysis, the barrier cone is a cone associated to any non-empty subset of a Banach space. It is closely related to the notions of support functions and polar sets.\n\nLet \"X\" be a Banach space and let \"K\" be a non-empty subset of \"X\". The barrier cone of \"K\" is the subset \"b\"(\"K\") of \"X\", the continuous dual space of \"X\", defined by\n\nThe function\n\ndefined for each continuous linear functional \"ℓ\" on \"X\", is known as the support function of the set \"K\"; thus, the barrier cone of \"K\" is precisely the set of continuous linear functionals \"ℓ\" for which \"σ\"(\"ℓ\") is finite.\n\nThe set of continuous linear functionals \"ℓ\" for which \"σ\"(\"ℓ\") ≤ 1 is known as the polar set of \"K\". The set of continuous linear functionals \"ℓ\" for which \"σ\"(\"ℓ\") ≤ 0 is known as the (negative) polar cone of \"K\". Clearly, both the polar set and the negative polar cone are subsets of the barrier cone.\n"}
{"id": "4542", "url": "https://en.wikipedia.org/wiki?curid=4542", "title": "Bra–ket notation", "text": "Bra–ket notation\n\nIn quantum mechanics, bra–ket notation is a standard notation for describing quantum states. It can also be used to denote abstract vectors and linear functionals in mathematics. The notation uses angle brackets (the ⟨ and ⟩ symbols) and a vertical bar (the | symbol), to denote the scalar product of vectors or the action of a linear functional on a vector in a complex vector space. The scalar product or action is written as\n\nThe right part is called the ket ; it is a vector, typically represented as a column vector and written \n\nThe left part is called the bra, ; it is the Hermitian conjugate of the ket with the same label, typically represented as a row vector and is written\n\nA combination of bras, kets, and operators is interpreted using matrix multiplication. A bra and a ket with the same label are Hermitian conjugates of each other.\n\nBra-ket notation was introduced in 1939 by Paul Dirac and is also known as the Dirac notation. \n\nThe bra-ket notation has a precursor in Hermann Grassmann's use of the notation formula_4for his inner products nearly 100 years earlier.\n\nBra–ket notation is a notation for linear algebra, particularly focused on vectors, inner products, linear operators, Hermitian conjugation, and the dual space, for both finite-dimensional and infinite-dimensional complex vector spaces. It is specifically designed to ease the types of calculations that frequently come up in quantum mechanics.\n\nIts use in quantum mechanics is quite widespread. Many phenomena that are explained using quantum mechanics are usually explained using bra–ket notation.\n\nIn simple cases, a ket can be described as a column vector, a bra with the same label is its conjugate transpose (which is a row vector), and writing bras, kets, and linear operators next to each other implies matrix multiplication. However, kets may also exist in uncountably-infinite-dimensional vector spaces, such that they cannot be literally written as a column vector. Also, writing a column vector as a list of numbers requires picking a basis, whereas one can write \" without committing to any particular basis. This is helpful because quantum mechanics calculations involve frequently switching between different bases (e.g. position basis, momentum basis, energy eigenbasis, etc.), so it is better to have the basis vectors (if any) written out explicitly. In some situations involving two important basis vectors they will be referred to simply as \" and \"\".\n\nThe standard mathematical notation for the inner product, preferred as well by some physicists, expresses exactly the same thing as the bra–ket notation,\n\nBras and kets can also be configured in other ways, such as the outer product\n\nwhich can also be represented as a matrix multiplication (i.e., a column vector times a row vector equals a matrix).\n\nIf the ket is an element of a vector space, the bra is technically an element of its dual space—see Riesz representation theorem.\n\nIn mathematics, the term \"vector\" is used to refer generally to any element of any vector space. In physics, however, the term \"vector\" is much more specific: \"Vector\" refers almost exclusively to quantities like displacement or velocity, which have three components that relate directly to the three dimensions of the real world. Such vectors are typically denoted with over arrows () or boldface ().\n\nIn quantum mechanics, a quantum state is typically represented as an element of an abstract complex vector space—for example the infinite-dimensional vector space of all possible wavefunctions (functions mapping each point of 3D space to a complex number). Since the term \"vector\" is already used for something else (see previous paragraph), it is very common to refer to these elements of abstract complex vector spaces as \"kets\", and to write them using ket notation.\n\nKet notation, invented by Dirac, uses vertical bars and angular brackets: . When this notation is used, these quantities are called \"kets\", and is read as \"ket-A\". These kets can be manipulated using the usual rules of linear algebra, for example:\n\nNote how any symbols, letters, numbers, or even words—whatever serves as a convenient label—can be used as the label inside a ket. For example, the last line above involves infinitely many different kets, one for each real number . In other words, the symbol \" has a specific and universal mathematical meaning, while just the \" by itself does not. For example, might or might not be equal to . Nevertheless, for convenience, there is usually some logical scheme behind the labels inside kets, such as the common practice of labeling energy eigenkets in quantum mechanics through a listing of their quantum numbers.\n\nAn inner product is a generalization of the dot product. The inner product of two vectors is a scalar. In neutral notation (notation dedicated to the inner product \"only\"), this might be written , where and are elements of the abstract vector space, i.e. both are \"kets\".\n\nBra–ket notation uses a specific notation for inner products:\n\nBra–ket notation splits this inner product (also called a \"bracket\") into two pieces, the \"bra\" and the \"ket\":\n\nwhere is called a bra, read as \"bra-A\", and is a ket as above.\n\nThe purpose of \"splitting\" the inner product into a bra and a ket is that \"both\" the bra and the ket are meaningful \"on their own\", and can be used in other contexts besides within an inner product. There are two main ways to think about the meanings of separate bras and kets. Accordingly, the interpretation of the expression has a second interpretation, namely that of the action of a linear functional per below.\n\nFor a finite-dimensional vector space, using a fixed orthonormal basis, the inner product can be written as a matrix multiplication of a row vector with a column vector:\nBased on this, the bras and kets can be defined as:\nand then it is understood that a bra next to a ket implies matrix multiplication.\n\nThe conjugate transpose (also called \"Hermitian conjugate\") of a bra is the corresponding ket and vice versa:\nbecause if one starts with the bra\nthen performs a complex conjugation, and then a matrix transpose, one ends up with the ket\n\nA more abstract definition, which is equivalent but more easily generalized to infinite-dimensional spaces, is to say that bras are linear functionals on the space of kets, i.e. linear transformations that input a ket and output a complex number. The bra linear functionals are defined to be consistent with the inner product. Thus, if is the linear functional corresponding to under the Riesz representation theorem, then\n\ni.e. it produces \"the same\" complex number as the inner product does. The terminology for the right hand side is though \"not\" inner product, which always involves two \"kets\". Confusing this is harmless, since the same number is produced in the end.\n\nIn mathematics terminology, the vector space of bras is the dual space to the vector space of kets, and corresponding bras and kets are related by the Riesz representation theorem.\n\nBra–ket notation can be used even if the vector space is not a Hilbert space.\n\nIn quantum mechanics, it is common practice to write down kets which have infinite norm, i.e. non-normalizable wavefunctions. Examples include states whose wavefunctions are Dirac delta functions or infinite plane waves. These do not, technically, belong to the Hilbert space itself. However, the definition of \"Hilbert space\" can be broadened to accommodate these states (see the Gelfand–Naimark–Segal construction or rigged Hilbert spaces). The bra–ket notation continues to work in an analogous way in this broader context.\n\nBanach spaces are a different generalization of Hilbert spaces. In a Banach space , the vectors may be notated by kets and the continuous linear functionals by bras. Over any vector space without topology, we may also notate the vectors by kets and the linear functionals by bras. In these more general contexts, the bracket does not have the meaning of an inner product, because the Riesz representation theorem does not apply.\n\nThe mathematical structure of quantum mechanics is based in large part on linear algebra:\n\nSince virtually every calculation in quantum mechanics involves vectors and linear operators, it can involve, and often does involve, bra–ket notation. A few examples follow:\n\nThe Hilbert space of a spin-0 point particle is spanned by a \"position basis\" , where the label extends over the set of all points in position space. This label is the eigenvalue of the position operator acting on such a basis state, formula_16. Since there are an uncountably infinite number of vector components in the basis, this is an uncountably infinite-dimensional Hilbert space. The dimensions of the Hilbert space (usually infinite) and position space (usually 1, 2 or 3) are not to be conflated.\n\nStarting from any ket in this Hilbert space, one may \"define\" a complex scalar function of , known as a wavefunction,\n\nOn the left-hand side, is a function mapping any point in space to a complex number; on the right-hand side, is a ket consisting of a superposition of kets with relative coefficients specified by that function.\n\nIt is then customary to define linear operators acting on wavefunctions in terms of linear operators acting on kets, by\n\nFor instance, the momentum operator has the following form,\n\nOne occasionally encounters an expression such as\nthough this is something of an abuse of notation. The differential operator must be understood to be an abstract operator, acting on kets, that has the effect of differentiating wavefunctions once the expression is projected into the position basis,\neven though, in the momentum basis, the operator amounts to a mere multiplication operator (by ).\n\nIn quantum mechanics the expression is typically interpreted as the probability amplitude for the state to collapse into the state . Mathematically, this means the coefficient for the projection of onto . It is also described as the projection of state onto state .\n\nA stationary spin- particle has a two-dimensional Hilbert space. One orthonormal basis is:\nwhere is the state with a definite value of the spin operator equal to + and is the state with a definite value of the spin operator equal to −.\n\nSince these are a basis, \"any\" quantum state of the particle can be expressed as a linear combination (i.e., quantum superposition) of these two states:\nwhere and are complex numbers.\n\nA \"different\" basis for the same Hilbert space is:\ndefined in terms of rather than .\n\nAgain, \"any\" state of the particle can be expressed as a linear combination of these two:\n\nIn vector form, you might write\ndepending on which basis you are using. In other words, the \"coordinates\" of a vector depend on the basis used.\n\nThere is a mathematical relationship between , , and ; see change of basis.\n\nThere are a few conventions and abuses of notation that are generally accepted by the physics community, but which might confuse the non-initiated.\n\nIt is common to use the same symbol for \"labels\" and \"constants\" in the same equation. For example, , where the symbol is used simultaneously as the \"name of the operator\" , its \"eigenvector\" and the associated \"eigenvalue\" .\n\nSomething similar occurs in component notation of vectors. While (uppercase) is traditionally associated with wavefunctions, (lowercase) may be used to denote a \"label\", a \"wave function\" or \"complex constant\" in the same context, usually differentiated only by a subscript.\n\nThe main abuses are including operations inside the vector labels. This is done for a fast notation of scaling vectors. E.g. if the vector is scaled by , it might be denoted by , which makes no sense since is a label, not a function or a number, so you can't perform operations on it.\n\nThis is especially common when denoting vectors as tensor products, where part of the labels are moved outside the designed slot, e.g. . Here part of the labeling that should state that all three vectors are different was moved outside the kets, as subscripts 1 and 2. And a further abuse occurs, since is meant to refer to the norm of the first vector—which is a \"label\" denoting a \"value\".\n\nA linear operator is a map that inputs a ket and outputs a ket. (In order to be called \"linear\", it is required to have certain properties.) In other words, if is a linear operator and is a ket, then is another ket.\n\nIn an -dimensional Hilbert space, can be written as an column vector, and then is an matrix with complex entries. The ket can be computed by normal matrix multiplication.\n\nLinear operators are ubiquitous in the theory of quantum mechanics. For example, observable physical quantities are represented by self-adjoint operators, such as energy or momentum, whereas transformative processes are represented by unitary linear operators such as rotation or the progression of time.\n\nOperators can also be viewed as acting on bras \"from the right hand side\". Specifically, if is a linear operator and is a bra, then is another bra defined by the rule\n\nIn an -dimensional Hilbert space, can be written as a row vector, and (as in the previous section) is an matrix. Then the bra can be computed by normal matrix multiplication.\n\nIf the same state vector appears on both bra and ket side,\nthen this expression gives the expectation value, or mean or average value, of the observable represented by operator for the physical system in the state .\n\nA convenient way to define linear operators on a Hilbert space is given by the outer product: if is a bra and is a ket, the outer product\ndenotes the rank-one operator with the rule \n\nFor a finite-dimensional vector space, the outer product can be understood as simple matrix multiplication:\nThe outer product is an matrix, as expected for a linear operator.\n\nOne of the uses of the outer product is to construct projection operators. Given a ket of norm 1, the orthogonal projection onto the subspace spanned by is\n\nJust as kets and bras can be transformed into each other (making into ), the element from the dual space corresponding to is , where denotes the Hermitian conjugate (or adjoint) of the operator . In other words,\n\nIf is expressed as an matrix, then is its conjugate transpose.\n\nSelf-adjoint operators, where , play an important role in quantum mechanics; for example, an observable is always described by a self-adjoint operator. If is a self-adjoint operator, then is always a real number (not complex). This implies that expectation values of observables are real.\n\nBra–ket notation was designed to facilitate the formal manipulation of linear-algebraic expressions. Some of the properties that allow this manipulation are listed herein. In what follows, and denote arbitrary complex numbers, denotes the complex conjugate of , and denote arbitrary linear operators, and these properties are to hold for any choice of bras and kets.\n\n\n\nGiven any expression involving complex numbers, bras, kets, inner products, outer products, and/or linear operators (but not addition), written in bra–ket notation, the parenthetical groupings do not matter (i.e., the associative property holds). For example:\nand so forth. The expressions on the right (with no parentheses whatsoever) are allowed to be written unambiguously \"because\" of the equalities on the left. Note that the associative property does \"not\" hold for expressions that include nonlinear operators, such as the antilinear time reversal operator in physics.\n\nBra–ket notation makes it particularly easy to compute the Hermitian conjugate (also called \"dagger\", and denoted ) of expressions. The formal rules are:\n\nThese rules are sufficient to formally write the Hermitian conjugate of any such expression; some examples are as follows:\n\n\nTwo Hilbert spaces and may form a third space by a tensor product. In quantum mechanics, this is used for describing composite systems. If a system is composed of two subsystems described in and respectively, then the Hilbert space of the entire system is the tensor product of the two spaces. (The exception to this is if the subsystems are actually identical particles. In that case, the situation is a little more complicated.)\n\nIf is a ket in and is a ket in , the direct product of the two kets is a ket in . This is written in various notations:\n\nSee quantum entanglement and the EPR paradox for applications of this product.\n\nConsider a complete orthonormal system (\"basis\"),\nfor a Hilbert space , with respect to the norm from an inner product . \n\nFrom basic functional analysis, it is known that any ket can also be written as\nwith the inner product on the Hilbert space.\n\nFrom the commutativity of kets with (complex) scalars, it follows that\nmust be the \"identity operator\", which sends each vector to itself. \n\nThis, then, can be inserted in any expression without affecting its value; for example\nwhere, in the last identity, the Einstein summation convention has been used.\n\nIn quantum mechanics, it often occurs that little or no information about the inner product of two arbitrary (state) kets is present, while it is still possible to say something about the expansion coefficients and of those vectors with respect to a specific (orthonormalized) basis. In this case, it is particularly useful to insert the unit operator into the bracket one time or more.\n\nFor more information, see Resolution of the identity, \n\nSince , plane waves follow, .\n\nTypically, when all matrix elements of an operator such as \nare available,\nthis resolution serves to reconstitute the full operator,\n\nThe object physicists are considering when using bra–ket notation is a Hilbert space (a complete inner product space).\n\nLet be a Hilbert space and a vector in . What physicists would denote by is the vector itself. That is,\n\nLet be the dual space of . This is the space of linear functionals on . The isomorphism is defined by , where for every we define\nwhere , , and are just different notations for expressing an inner product between two elements in a Hilbert space (or for the first three, in any inner product space). Notational confusion arises when identifying and with and respectively. This is because of literal symbolic substitutions. Let and let . This gives\n\nOne ignores the parentheses and removes the double bars. Some properties of this notation are convenient since we are dealing with linear operators and composition acts like a ring multiplication.\n\nMoreover, mathematicians usually write the dual entity not at the first place, as the physicists do, but at the second one, and they usually use not an asterisk but an overline (which the physicists reserve for averages and the Dirac spinor adjoint) to denote complex conjugate numbers; i.e., for scalar products mathematicians usually write\nwhereas physicists would write for the same quantity\n\n\n\n"}
{"id": "3948656", "url": "https://en.wikipedia.org/wiki?curid=3948656", "title": "Center manifold", "text": "Center manifold\n\nIn mathematics, the center manifold of an equilibrium point of a dynamical system consists of orbits whose behavior around the equilibrium point is not controlled by either the attraction of the stable manifold or the repulsion of the unstable manifold. The first step when studying equilibrium points of dynamical systems is to linearize the system. The eigenvectors corresponding to eigenvalues with negative real part form the stable eigenspace, which gives rise to the stable manifold. Similarly, eigenvalues with positive real part yield the unstable manifold.\n\nThis concludes the story if the equilibrium point is hyperbolic (i.e., all eigenvalues of the linearization have nonzero real part). However, if there are eigenvalues whose real part is zero, then these give rise to the center manifold. If the eigenvalues are precisely zero, rather than just real part being zero, then these more specifically give rise to a slow manifold. The behavior on the center (slow) manifold is generally not determined by the linearization and thus is more difficult to study.\n\nCenter manifolds play an important role in: bifurcation theory because interesting behavior takes place on the center manifold; and multiscale mathematics because the long time dynamics often are attracted to a relatively simple center manifold.\n\nLet formula_1 be a dynamical system with equilibrium point formula_2. The linearization of the system near the equilibrium point is\n\nThe Jacobian matrix formula_4 defines three main subspaces:\nDepending upon the application, other subspaces of interest include center-stable, center-unstable, sub-center, slow, and fast subspaces.\nThese subspaces are all invariant subspaces of the linearized equation.\n\nCorresponding to the linearized system, the nonlinear system has invariant manifolds, each consisting of sets of orbits of the nonlinear system.\n\nThe center manifold existence theorem states that if the right-hand side function formula_11 is formula_12 (formula_13 times continuously differentiable), then at every equilibrium point there exists a neighborhood of some finite size in which there is at least one of \n\nIn example applications, a nonlinear coordinate transform to a normal form can clearly separate these three manifolds. A web service currently undertakes the necessary computer algebra for a range of finite-dimensional systems.\n\nIn the case when the unstable manifold does not exist, center manifolds are often relevant to modelling.\nThe center manifold emergence theorem then says that the neighborhood may be chosen so that all solutions of the system staying in the neighborhood tend exponentially quickly to some solution formula_17 on the center manifold.\nThat is, \nformula_18\nfor some rate formula_19.\nThis theorem asserts that for a wide variety of initial conditions the solutions of the full system decay exponentially quickly to a solution on the relatively low dimensional center manifold.\n\nA third theorem, the approximation theorem, asserts that if an approximate expression for such invariant manifolds, say formula_20, satisfies the differential equation for the system to residuals formula_21 as formula_22, then the invariant manifold is approximated by formula_20 to an error of the same order, namely formula_21.\n\nHowever, some applications, such as to shear dispersion, require an infinite-dimensional center manifold. The most general and powerful theory was developed by Aulbach and Wanner. They addressed non-autonomous dynamical systems formula_25 in infinite dimensions, with potentially infinite dimensional stable, unstable and center manifolds. Further, they usefully generalised the definition of the manifolds so that the center manifold is associated with eigenvalues such that formula_26, the stable manifold with eigenvalues formula_27, and unstable manifold with eigenvalues formula_28. They proved existence of these manifolds, and the emergence of a center manifold, via nonlinear coordinate transforms. Potzsche and Rasmussen established a corresponding approximation theorem for such infinite dimensional, non-autonomous systems.\n\nAs the stability of the equilibrium correlates with the \"stability\" of its manifolds, the existence of a center manifold brings up the question about the dynamics on the center manifold. This is analyzed by the center manifold reduction, which, in combination with some system parameter μ, leads to the concepts of bifurcations.\n\nCorrespondingly, two web services currently undertake the necessary computer algebra to construct just the center manifold for a wide range of finite-dimensional systems (provided they are in multinomial form). \n\nThe Wikipedia entry on slow manifolds gives more examples.\n\nConsider the system\nThe unstable manifold at the origin is the \"y\" axis, and the stable manifold is the trivial set {(0, 0)}. Any orbit not on the stable manifold satisfies an equation on the form formula_30 for some real constant \"A\". It follows that for any real \"A\", we can create a center manifold by piecing together the curve formula_30 for \"x\" > 0 with the negative \"x\" axis (including the origin). Moreover, all center manifolds have this potential non-uniqueness, although often the non-uniqueness only occurs in unphysical complex values of the variables.\n\nAnother example shows how a center manifold\nmodels the Hopf bifurcation that occurs\nfor parameter formula_32 in the\ndelay differential equation\nformula_33. \nStrictly, the delay makes this DE infinite-dimensional.\n\nFortunately, we may approximate such delays by the following trick that keeps the dimensionality finite.\nDefine formula_34\nand approximate the time delayed variable, \nformula_35, by using the intermediaries\nformula_36 and\nformula_37.\n\nFor parameter near\ncritical, formula_38, the delay differential equation is then approximated by the system\nCopying and pasting the appropriate entries, \nthe web service finds that in terms of a complex amplitude formula_40 \nand its complex conjugate formula_41, the center manifold\nand the evolution on the center manifold is\nThis evolution shows the origin is linearly unstable for formula_44, but the cubic nonlinearity then stabilises nearby limit cycles as in classic Hopf bifurcation.\n"}
{"id": "10252066", "url": "https://en.wikipedia.org/wiki?curid=10252066", "title": "Choquet integral", "text": "Choquet integral\n\nA Choquet integral is a subadditive or superadditive integral created by the French mathematician Gustave Choquet in 1953. It was initially used in statistical mechanics and potential theory, but found its way into decision theory in the 1980s, where it is used as a way of measuring the expected utility of an uncertain event. It is applied specifically to membership functions and capacities. In imprecise probability theory, the Choquet integral is also used to calculate the lower expectation induced by a 2-monotone lower probability, or the upper expectation induced by a 2-alternating upper probability.\n\nUsing the Choquet integral to denote the expected utility of belief functions measured with capacities is a way to reconcile the Ellsberg paradox and the Allais paradox.\n\nThe following notation is used:\n\n\nAssume that formula_6 is measurable with respect to formula_7, that is\n\nThen the Choquet integral of formula_6 with respect to formula_7 is defined by:\n\nwhere the integrals on the right-hand side are the usual Riemann integral (the integrands are integrable because they are monotone in formula_12).\n\nIn general the Choquet integral does not satisfy additivity. More specifically, if formula_7 is not a probability measure, it may hold that\n\nfor some functions formula_6 and formula_16.\n\nThe Choquet integral does satisfy the following properties.\n\nIf formula_17 then\n\nFor all formula_19 it holds that\n\nIf formula_21 are comonotone functions, that is, if for all formula_22 it holds that\nthen\n\nIf formula_7 is 2-alternating, then\n\nIf formula_7 is 2-monotone, then\n\nLet formula_29 denote a cumulative distribution function such that formula_30 is formula_31 integrable. Then this following formula is often referred to as Choquet Integral:\nwhere formula_33.\n\nThe Choquet integral was applied in image processing, video processing and computer vision. In behavioral decision theory, Amos Tversky and Daniel Kahneman use the Choquet integral and related methods in their formulation of Cumulative Prospect Theory.\n\n"}
{"id": "26313585", "url": "https://en.wikipedia.org/wiki?curid=26313585", "title": "Claude Lemaréchal", "text": "Claude Lemaréchal\n\nClaude Lemaréchal is a French applied mathematician, and former senior researcher (\"directeur de recherche\") at INRIA near Grenoble, France.\n\nIn mathematical optimization, Claude Lemaréchal is known for his work in numerical methods for nonlinear optimization, especially for problems with nondifferentiable kinks. Lemaréchal and Phil. Wolfe pioneered bundle methods of descent for convex minimization.\n\nIn 1994, Claude Lemaréchal and Roger J-B Wets were each awarded the George B. Dantzig Prize. Recognizing \"original research that has had a major impact on the field of mathematical programming\", the Dantzig Prize is awarded by the Society for Industrial and Applied Mathematics (SIAM) and the Mathematical Programming Society (MPS).\n\nSoon after joining INRIA (then named \"IRIA\"), Lemaréchal had the assignment of helping a glass-manufacturer with a problem of scheduling its production, a problem whose first formulation required minimizing a non-convex function. For this non-convex minimization problem, Lemaréchal applied the theory of Lagrangian duality that was described in Lasdon's \"Optimization Theory for Large Systems\". Because the primal problem was non-convex, there was no guarantee that a solution to the dual problem would provide useful information about the primal. Nonetheless, the dual problem did furnish useful information. Lemaréchal's success with Lagrangian dual methods on nonlinear programming problems with nonconvexities interested Ivar Ekeland and Jean–Pierre Aubin, who applied the Shapley–Folkman lemma to explain the Lemaréchal's success. The Aubin–Ekeland analysis of duality gaps considered the \"convex\"closure of a nonconvex minimization problem — that is, the problem defined by the closed convex hull of the epigraph of the original problem. Following Ekeland and Aubin, similar applications of the Shapley–Folkman lemma are described in optimization monographs and textbooks. These developments were catalyzed by Lemaréchal's demonstration that Lagrangian-dual methods were useful on some optimization problems that lacked convexity.\n\nLemaréchal's research also led to his work on (conjugate) subgradient methods and on bundle methods of descent for convex minimization problems.\n\n\n"}
{"id": "17545909", "url": "https://en.wikipedia.org/wiki?curid=17545909", "title": "Coates graph", "text": "Coates graph\n\nIn mathematics, the Coates graph or Coates flow graph, named after C.L. Coates, is a graph associated with the Coates' method for the solution of a system of linear equations.\n\nThe Coates graph \"G\"(A) associated with an \"n\" × \"n\" matrix A is an \"n\"-node, weighted, labeled, directed graph. The nodes, labeled 1 through \"n\", are each associated with the corresponding row/column of A. If entry \"a\" ≠ 0 then there is a directed edge from node \"i\" to node \"j\" with weight \"a\". In other words, the Coates graph for matrix A is the one whose adjacency matrix is the transpose of A.\n\n"}
{"id": "21105530", "url": "https://en.wikipedia.org/wiki?curid=21105530", "title": "Combinatorial map", "text": "Combinatorial map\n\nA combinatorial map is a combinatorial object modelling topological structures with subdivided objects. Historically, the concept was introduced informally by J. Edmonds for polyhedral surfaces which are planar graphs. It was given its first definite formal expression under the name \"Constellations\" by A. Jacques but the concept was already extensively used under the name \"rotation\" by Gerhard Ringel and J.W.T. Youngs in their famous solution of the Heawood map-coloring problem. The term \"constellation\" was not retained and instead \"combinatorial map\" was favored. The concept was later extended to represent higher-dimensional orientable subdivided objects. Combinatorial maps are used as efficient data structures in image representation and processing, in geometrical modeling. This model is related to simplicial complexes and to combinatorial topology. Note that combinatorial maps were extended to generalized maps that allow also to represent non-orientable objects like the Möbius strip and the Klein bottle. A combinatorial map is a boundary representation model; it represents object by its boundaries.\n\nSeveral applications require a data structure to represent the subdivision of an object. For example, a 2D object can be decomposed into vertices (0-cells), edges (1-cells), and faces (2-cells). More generally, an n-dimensional object is composed with cells of dimension 0 to n. Moreover, it is also often necessary to represent neighboring relations between these cells.\n\nThus, we want to describe all the cells of the subdivision, plus all the incidence and adjacency relations between these cells. When all the represented cells are simplexes, a simplicial complex may be used, but when we want to represent any type of cells, we need to use cellular topological models like combinatorial maps or generalized maps.\n\nThe first works about combinatorial maps develop combinatorial representations of graphs on surfaces which includes planar graphs: \nA 2-dimensional combinatorial map (or 2-map) is a triplet \"M\" = (\"D\", \"σ\", \"α\") such that:\n\nIntuitively, a 2-map corresponds to a planar graph where each edge is subdivided into two darts (sometimes also called half-edges). The permutation \"σ\" gives, for each dart, the next dart by turning around the vertex in the positive orientation; the other permutation \"α\" gives, for each dart, the other dart of the same edge.\n\n\"α\" allows one to retrieve edges (alpha for arête in French), and \"σ\" allows one to retrieve vertices (sigma for sommet in French). We define \"φ\" = \"σ\" o \"α\" which gives, for each dart, the next dart of the same face (phi for face also in French).\n\nSo, there are two ways to represent a combinatorial map depending if the permutation is \"σ\" or \"φ\" (see example below). These two representations are dual to each other: vertices and faces are exchanged.\n\nThe definition of combinatorial map in any dimension is as follows.\n\nAn \"n\"-dimensional combinatorial map (or \"n\"-map) is a (\"n\" + 1)-tuple \"M\" = (\"D\", \"β\", ..., \"β\") such that:\n\nAn \"n\"-dimensional combinatorial map represents the subdivision of a closed orientable \"n\"-dimensional space. A dart is an abstract element which is only required to define one-to-one mappings. The last line of this definition fixes constraints which guarantee the topological validity of the represented object: a combinatorial map represents a quasi-manifold subdivision. The initial definition of 2-dimensional combinatorial maps can be retrieved by fixing \"n\" = 2 and renaming \"σ\" by \"β\" and \"α\" by \"β\".\n\n\n\n"}
{"id": "5845706", "url": "https://en.wikipedia.org/wiki?curid=5845706", "title": "Comparison triangle", "text": "Comparison triangle\n\nDefine formula_1 as the 2-dimensional metric space of constant curvature formula_2. So, for example, formula_3 is the Euclidean plane, formula_4 is the surface of the unit sphere, and formula_5 is the hyperbolic plane.\n\nLet formula_6 be a metric space. Let formula_7 be a triangle in formula_6, with vertices formula_9, formula_10 and formula_11. A comparison triangle formula_12 in formula_1 for formula_7 is a triangle in formula_1 with vertices formula_16, formula_17 and formula_18 such that formula_19, formula_20 and formula_21.\n\nSuch a triangle is unique up to isometry. \n\nThe interior angle of formula_12 at formula_16 is called the comparison angle between formula_10 and formula_11 at formula_9. This is well-defined provided formula_10 and formula_11 are both distinct from formula_9.\n\n"}
{"id": "3504161", "url": "https://en.wikipedia.org/wiki?curid=3504161", "title": "Congruum", "text": "Congruum\n\nIn number theory, a congruum (plural \"congrua\") is the difference between successive square numbers in an arithmetic progression of three squares.\nThat is, if \"x\", \"y\", and \"z\" (for integers \"x\", \"y\", and \"z\") are three square numbers that are equally spaced apart from each other, then the spacing between them, , is called a congruum.\n\nThe congruum problem is the problem of finding squares in arithmetic progression and their associated congrua. It can be formalized as a Diophantine equation: find integers \"x\", \"y\", and \"z\" such that\nWhen this equation is satisfied, both sides of the equation equal the congruum.\n\nFibonacci solved the congruum problem by finding a parameterized formula for generating all congrua, together with their associated arithmetic progressions. According to this formula, each congruum is four times the area of a Pythagorean triangle. Congrua are also closely connected with congruent numbers: every congruum is a congruent number, and every congruent number is a congruum multiplied by the square of a rational number.\n\nFor instance,\nthe number 96 is a congruum, since it is the difference between adjacent squares in the sequence 4, 100, and 196 (the squares of 2, 10, and 14 respectively).\n\nThe first few congrua are:\n\nThe congruum problem was originally posed in 1225, as part of a mathematical tournament held by Frederick II, Holy Roman Emperor, and answered correctly at that time by Fibonacci, who recorded his work on this problem in his \"Book of Squares\".\n\nFibonacci was already aware that it is impossible for a congruum to itself be a square, but did not give a satisfactory proof of this fact. Geometrically, this means that it is not possible for the pair of legs of a Pythagorean triangle to be the leg and hypotenuse of another Pythagorean triangle. A proof was eventually given by Pierre de Fermat, and the result is now known as Fermat's right triangle theorem. Fermat also conjectured, and Leonhard Euler proved, that there is no sequence of four squares in arithmetic progression.\n\nThe congruum problem may be solved by choosing two distinct positive integers \"m\" and \"n\" (with \"m\" > \"n\"); then the number 4\"mn\"(\"m\" −\"n\") is a congruum. The middle square of the associated arithmetic progression of squares is (\"m\" + \"n\"), and the other two squares may be found by adding or subtracting the congruum. Additionally, multiplying a congruum by a square number produces another congruum, whose progression of squares is multiplied by the same factor. All solutions arise in one of these two ways. For instance, the congruum 96 can be constructed by these formulas with \"m\" = 3 and \"n\" = 1, while the congruum 216 is obtained by multiplying the smaller congruum 24 by the square number 9.\n\nAn equivalent formulation of this solution, given by Bernard Frénicle de Bessy, is that for the three squares in arithmetic progression \"x\", \"y\", and \"z\", the middle number \"y\" is the hypotenuse of a Pythagorean triangle and the other two numbers \"x\" and \"z\" are the difference and sum respectively of the triangle's two legs. The congruum itself is four times the area of the same Pythagorean triangle. The example of an arithmetic progression with the congruum 96 can be obtained in this way from a right triangle with side and hypotenuse lengths 6, 8, and 10.\n\nA congruent number is defined as the area of a right triangle with rational sides.\nBecause every congruum can be obtained (using the parameterized solution) as the area of a Pythagorean triangle, it follows that every congruum is congruent. Conversely, every congruent number is a congruum multiplied by the square of a rational number. However, testing whether a number is a congruum is much easier than testing whether a number is congruent. For the congruum problem, the parameterized solution reduces this testing problem to checking a finite set of parameter values. In contrast, for the congruent number problem, a finite testing procedure is known only conjecturally, via Tunnell's theorem, under the assumption that the Birch and Swinnerton-Dyer conjecture is true.\n\n"}
{"id": "11766887", "url": "https://en.wikipedia.org/wiki?curid=11766887", "title": "Curvature of a measure", "text": "Curvature of a measure\n\nIn mathematics, the curvature of a measure defined on the Euclidean plane R is a quantification of how much the measure's \"distribution of mass\" is \"curved\". It is related to notions of curvature in geometry. In the form presented below, the concept was introduced in 1995 by the mathematician Mark S. Melnikov; accordingly, it may be referred to as the Melnikov curvature or Menger-Melnikov curvature. Melnikov and Verdera (1995) established a powerful connection between the curvature of measures and the Cauchy kernel.\n\nLet \"μ\" be a Borel measure on the Euclidean plane R. Given three (distinct) points \"x\", \"y\" and \"z\" in R, let \"R\"(\"x\", \"y\", \"z\") be the radius of the Euclidean circle that joins all three of them, or +∞ if they are collinear. The Menger curvature \"c\"(\"x\", \"y\", \"z\") is defined to be\n\nwith the natural convention that \"c\"(\"x\", \"y\", \"z\") = 0 if \"x\", \"y\" and \"z\" are collinear. It is also conventional to extend this definition by setting \"c\"(\"x\", \"y\", \"z\") = 0 if any of the points \"x\", \"y\" and \"z\" coincide. The Menger-Melnikov curvature \"c\"(\"μ\") of \"μ\" is defined to be\n\nMore generally, for \"α\" ≥ 0, define \"c\"(\"μ\") by\n\nOne may also refer to the curvature of \"μ\" at a given point \"x\":\n\nin which case\n\n\nIn this section, R is thought of as the complex plane C. Melnikov and Verdera (1995) showed the precise relation of the boundedness of the Cauchy kernel to the curvature of measures. They proved that if there is some constant \"C\" such that\n\nfor all \"x\" in C and all \"r\" > 0, then there is another constant \"C\", depending only on \"C\", such that\n\nfor all \"ε\" > 0. Here \"c\" denotes a truncated version of the Menger-Melnikov curvature in which the integral is taken only over those points \"x\", \"y\" and \"z\" such that\n\nSimilarly, formula_11 denotes a truncated Cauchy integral operator: for a measure \"μ\" on C and a point \"z\" in C, define\n\nwhere the integral is taken over those points \"ξ\" in C with\n\n"}
{"id": "7673367", "url": "https://en.wikipedia.org/wiki?curid=7673367", "title": "David P. Robbins", "text": "David P. Robbins\n\nDavid Peter Robbins (12 August 1942 in Brooklyn – 4 September 2003 in Princeton) was an American mathematician. He is most famous for introducing alternating sign matrices. He is also known for his work on generalizations of Heron's formula on the area of polygons, due to which Robbins pentagons (cyclic pentagons with integer side lengths and areas) were named after him.\n\nRobbins grew up in Manhattan, where he attended the Fieldston School. He studied at Harvard, where his undergraduate advisor was Andrew Gleason. He went to MIT to do his graduate work and, after a hiatus during which he taught at Fieldston, finished his Ph.D. in 1970. He then taught at MIT, Phillips Exeter Academy, Hamilton College and Washington and Lee University. In 1980 he moved to Princeton, New Jersey and worked at the Institute for Defense Analyses Center for Communications Research there until his death from pancreatic cancer.\n\nA symposium was held in Robbins' honor in June 2003, the papers from which were published as a special issue of the journal \"Advances in Applied Mathematics\". The Mathematical Association of America established a prize named in his honor in 2005, given every three years to one or more researchers in algebra, combinatorics, or discrete mathematics. The first winner of the prize, in 2008, was Neil Sloane for the On-Line Encyclopedia of Integer Sequences.\n\nThe American Mathematical Society has another prize, the David P. Robbins Prize (AMS) with the same name the first winners of which were Samuel P. Ferguson and Thomas C. Hales for their work on the Kepler conjecture.\n\n"}
{"id": "5451418", "url": "https://en.wikipedia.org/wiki?curid=5451418", "title": "Digital credential", "text": "Digital credential\n\nDigital credentials are the digital equivalent of paper-based credentials. Just as a paper-based credential could be a passport, a driver's license, a membership certificate or some kind of ticket to obtain some service, such as a cinema ticket or a public transport ticket, a digital credential is a proof of qualification, competence, or clearance that is attached to a person. Also, digital credentials prove something about their owner. Both types of credentials may contain personal information such as the person's name, birthplace, birthdate, and/or biometric information such as a picture or a finger print.\n\nBecause of the still evolving, and sometimes conflicting, terminologies used in the fields of computer science, computer security, and cryptography, the term \"digital credential\" is used quite confusingly in these fields. Sometimes passwords or other means of authentication are referred to as credentials. In operating system design, credentials are the properties of a process (such as its effective UID) that is used for determining its access rights. On other occasions, certificates and associated key material such as those stored in PKCS#12 and PKCS#15 are referred to as credentials.\n\nDigital badges are a form of digital credential that indicate an accomplishment, skill, quality or interest. Digital badges can be earned in a variety of learning environments.\n\nReal world credentials are a diverse social phenomenon, and as such are difficult to define. As with digital signatures it is misleading to assume a direct correspondence between the real-world and the digital concept. This holds even if defining criteria for credentials in the digital world could be agreed on.\n\nThe success of digital signatures as a replacement for paper based signatures has lagged behind expectations. On the other hand, many unexpected uses of digital signatures were discovered by recent cryptographic research. A related insight that can be learned from digital signatures is that the cryptographic mechanism need not be confused with overall process that turns a digital signature into something that has more or less the same properties as a paper based signature. Electronic signatures such as paper signatures sent by fax may have legal meaning, while secure cryptographic signatures may serve completely different purposes. We need to distinguish the algorithm from the process.\n\nMoney is usually not seen as a qualification that is attached to a specific person as token money is taken to have a value on its own. Digital assets like digital cash are easily copied. Consequently, digital cash protocols have to make an extra effort to avoid the double spending of coins. Credentials are a proof of qualification that is attached to a person. E-Coins are given to individuals, who cannot pass them on to others, but can only spend them with merchants. As long as they spend a coin only once, they are anonymous, but should they spend a coin twice, they become identifiable and appropriate actions can be taken by the bank. This commonality, the binding to an individual, is why digital cash and digital credentials share many commonalities. In fact most implementations of anonymous digital credential also realize digital cash.\n\nThe main idea behind anonymous digital credentials is that users are given cryptographic tokens which allow them to prove statements about themselves and their relationships with public and private organizations anonymously. This is seen as a more privacy-friendly alternative to keeping and using large centralized and linkable user records. Anonymous digital credentials are thus related to privacy and anonymity.\n\nPaper world analogues of personalized, or non-anonymous credentials are: passports, driving licenses, credit cards, health insurance cards, club membership cards etc. These contain the name of the owner and have some authenticating information such as a signature, PIN or photograph, to stop them being used by anyone other than the rightful owner. Paper world analogues of anonymous credentials are: money, bus and train tickets, and game-arcade tokens. These don't have any personally identifying information and consequently can be transferred between users without the issuers or relying parties being aware of this. Credentials are issued by organizations that ascertain the authenticity of the information which can be provided to verifying entities on demand.\n\nIn order to investigate certain privacy specific properties of credentials, we take a more detailed look at two kind of 'credentials', physical money and credit cards. Without doubt both of them provide adequate information for doing payment transactions. However the amount and quality of the information disclosed varies. Money is protected from forgery by its physical properties. Beyond that, only very little information is revealed:\nCoins feature an ingrained value and the year of coining; in addition bank notes contain a unique serial number in order to provide the traceability required by law enforcement.\n\nOn the other hand, the use of a credit card, whose main purpose is similar to money, allows for the creation of highly detailed records about the card owner. Credit cards are therefore not privacy protecting. The main privacy advantage of money is that its users can remain anonymous. There are however other security and usability properties that make real world cash popular.\n\nCredentials used in a national identification system are also especially privacy relevant. Such an ID, be it a passport, a driver's license, or some other type of card usually contains essential personal information. In certain situations it may be advantageous to reveal only parts of the information contained on the ID, e.g., some lower limit for the person's age or the fact that the person is capable of driving a car.\n\nThe original anonymous credential system proposed by David Chaum is sometimes also referred to as a pseudonym system. This stems from the fact that the credentials of such a system are obtained from and shown to organizations using different pseudonyms which cannot be linked.\n\nThe introduction of pseudonyms is a useful extension to anonymity.\n\"Pseudonyms\" allow users to choose a different name with each organization. While pseudonyms allow organizations to associate users with accounts, organizations cannot determine the real identities of their customers. Nevertheless, by using an anonymous credential, certain statements about the relationship of a user with one organization, under a pseudonym, can be proven to another organization that knows the user only under a different pseudonym.\n\nAnonymous credential systems are related to the concept of untraceable or anonymous payments. In this important work, Chaum presents a new cryptographic primitive, blind signature protocols. In such a scheme the signer neither learns the message he signs, nor the signature the recipient obtains for his message. Blind signatures are an important building block of many privacy-sensitive applications, such as anonymous payments, voting, and credentials. The original idea for an anonymous credential system was derived from blind signatures, but relied on a trusted party for credential transfer—the translation from one pseudonym to another. The blind signature scheme introduced by Chaum was based on RSA signatures and based on the discrete logarithm problem can be used for constructing anonymous credential systems.\n\nStefan Brands generalized digital credentials with secret-key certificate based credentials, improving on Chaum's basic blind-signature based system in both the discrete logarithm and strong RSA assumption settings. Brands credentials provide efficient algorithms and privacy in an unconditional commercial security setting, along with several other features such as a proof of non-membership blacklist.\n\nAnother credential form that adds a new feature to anonymous credentials: multi-show unlinkability. These are the group signature related credentials of Camenisch et al. The introduction of Group signatures opened up the possibility of multi-show unlinkable showing protocols. While blind signatures are highly relevant for electronic cash and one-show credentials, a new cryptographic primitive, called group signature, opened new possibilities for the construction of privacy enhancing protocols. As is observed in their article, group signatures bear a resemblance to Chaum's concept of credential systems.\n\nUsing a group signature scheme, the members of a group can sign a message with their respective secret keys. The resulting signature can be verified by everyone who knows the common public key, but the signature does not reveal any information about the signer except that she is a member of the group. Usually there exists another entity called the group manager, who can reveal the exact identity of the signer, and handles the adding of users to and the removal of users from the group—usually by issuing or revoking group membership certificates. The anonymity, unlinkability, and anonymity revocation provided by group signatures lends itself for a variety of privacy sensitive applications like voting, bidding, anonymous payment, and anonymous credentials\n\nAn efficient constructions for group signatures was given by Ateniese, Camenisch, Joye, and Tsudik.\nThe most efficient multi-show unlinkable anonymous credential systems—the latter is essentially a low profile version of idemix—are based on similar ideas. This is particularly true for credential systems that provide efficient means for implementing anonymous multi-show credentials with credential revocation.\n\nBoth schemes are based on techniques for doing proofs of knowledge.\nProofs of knowledge relying on the discrete logarithm problem for groups of known order and on the special RSA problem for groups of hidden order form the basis for most of today's group signature and anonymous credential systems. Moreover, direct anonymous attestation a protocol for authenticating trusted platform modules is based on the same techniques.\n\nDirect anonymous attestation can be seen as the first commercial application of multi show anonymous digital credentials, even though in this case credentials are not attached to persons, but to chips and consequently computer platforms.\n\nFrom an applications' point of view, the main advantage of Camenisch et al.'s multi-show unlinkable credentials over the more efficient Brands credentials is the multi-show unlinkable property. However, this property is mainly of practical interest in an off-line setting. Brands credentials provide a mechanism that gives analogous functionality without sacrificing performance: an efficient batch issuing protocol which can simultaneously issue many unlinkable credentials. This mechanism can be combined with a privacy preserving certificate refresh process (which gives a fresh unlinkable credential with the same attributes as a previous spent credential).\n\nOnline credentials for learning are digital credentials that are offered in place of traditional paper credentials for a skill or educational achievement. Directly linked to the accelerated development of internet communication technologies, the development of digital badges, electronic passports and massive open online courses (MOOCs) have a very direct bearing on our understanding of learning, recognition and levels as they pose a direct challenge to the status quo. It is useful to distinguish between three forms of online credentials: Test-based credentials, online badges, and online certificates.\n"}
{"id": "13197584", "url": "https://en.wikipedia.org/wiki?curid=13197584", "title": "Discrete frequency domain", "text": "Discrete frequency domain\n\nA discrete frequency domain is a frequency domain that is discrete rather than continuous. \n\nFor example, the discrete Fourier transform maps a function having a discrete time domain into one having a discrete frequency domain. The discrete-time Fourier transform, on the other hand, maps functions with discrete time (discrete-time signals) to functions that have a continuous frequency domain.\n"}
{"id": "53880786", "url": "https://en.wikipedia.org/wiki?curid=53880786", "title": "Donald Aronson", "text": "Donald Aronson\n\nDonald Aronson is an American mathematician currently Professor Emeritus at University of Minnesota and an Elected Fellow of the American Mathematical Society.\n"}
{"id": "35205666", "url": "https://en.wikipedia.org/wiki?curid=35205666", "title": "Drinfeld reciprocity", "text": "Drinfeld reciprocity\n\nIn mathematics, Drinfeld reciprocity, introduced by , is a correspondence between eigenforms of the moduli space of Drinfeld modules and factors of the corresponding Jacobian variety, such that all twisted L-functions are the same.\n\n"}
{"id": "31479005", "url": "https://en.wikipedia.org/wiki?curid=31479005", "title": "Dynamic timing verification", "text": "Dynamic timing verification\n\nDynamic timing verification refers to verifying that an ASIC design is fast enough to run without errors at the targeted clock rate. This is accomplished by simulating the design files used to synthesize the integrated circuit (IC) design. This is in contrast to static timing analysis, which has a similar goal as dynamic timing verification except it does not require simulating the real functionality of the IC.\n\nHobbyists often perform a type of dynamic timing verification when they over-clock the CPUs in their computers in order to find the fastest clock rate at which they can run the CPU without errors. This is a type of dynamic timing verification that is performed after the silicon is manufactured. In the field of ASIC design, this timing verification is preferably performed before manufacturing the IC in order to make sure that IC works under the required conditions before mass production of the IC.\n"}
{"id": "2585991", "url": "https://en.wikipedia.org/wiki?curid=2585991", "title": "Ehrenfest theorem", "text": "Ehrenfest theorem\n\nThe Ehrenfest theorem, named after Paul Ehrenfest, an Austrian theoretical physicist at Leiden University, relates the time derivative of the expectation values of the position and momentum operators \"x\" and \"p\" to the expectation value of the force formula_1 on a massive particle moving in a scalar potential formula_2,\n\nAlthough, at first glance, it might appear that the Ehrenfest theorem is saying that the quantum mechanical expectation values obey Newton’s classical equations of motion, this is not actually the case. If the pair formula_3 were to satisfy Newton's second law, the right-hand side of the second equation would have to be\nwhich is typically not the same as\nIf for example, the potential formula_2 is cubic, (i.e. proportional to formula_7), then formula_8 is quadratic (proportional to formula_9). This means, in the case of Newton's second law, the right side would be in the form of formula_10, while in the Ehrenfest theorem it is in the form of formula_11. The difference between these two quantities is the square of the uncertainty in formula_12 and is therefore nonzero.\n\nAn exception occurs in case when the classical equations of motion are linear, that is, when formula_13 is quadratic and formula_8 is linear. In that special case, formula_15 and formula_16 do agree. Thus, for the case of a quantum harmonic oscillator, the expected position and expected momentum do exactly follow the classical trajectories.\n\nFor general systems, if the wave function is highly concentrated around a point formula_17, then formula_15 and formula_16 will be \"almost\" the same, since both will be approximately equal to formula_20. In that case, the expected position and expected momentum will \"approximately\" follow the classical trajectories, at least for as long as the wave function remains localized in position.\n\nThe Ehrenfest theorem is a special case of a more general relation between the expectation of any quantum mechanical operator and the expectation of the commutator of that operator with the Hamiltonian of the system \n\nwhere is some quantum mechanical operator and is its expectation value. This more general theorem was not actually derived by Ehrenfest (it is due to Werner Heisenberg).\n\nIt is most apparent in the Heisenberg picture of quantum mechanics, where it is just the expectation value of the Heisenberg equation of motion. It provides mathematical support to the correspondence principle.\n\nThe reason is that Ehrenfest's theorem is closely related to Liouville's theorem of Hamiltonian mechanics, which involves the Poisson bracket instead of a commutator. Dirac's rule of thumb suggests that statements in quantum mechanics which contain a commutator correspond to statements in classical mechanics where the commutator is supplanted by a Poisson bracket multiplied by . This makes the operator expectation values obey corresponding classical equations of motion, provided the Hamiltonian is at most quadratic in the coordinates and momenta. Otherwise, the evolution equations still may hold approximately, provided fluctuations are small.\n\nSuppose some system is presently in a quantum state . If we want to know the instantaneous time derivative of the expectation value of , that is, by definition\n\nwhere we are integrating over all space. If we apply the Schrödinger equation, we find that\n\nBy taking the complex conjugate we find\n\nNote , because the Hamiltonian is Hermitian. Placing this into the above equation we have\n\nOften (but not always) the operator is time independent, so that its derivative is zero and we can ignore the last term.\n\nIn the Heisenberg picture, the derivation is trivial. The Heisenberg picture moves the time dependence of the system to operators instead of state vector. Starting with the Heisenberg equation of motion\n\nwe can derive Ehrenfest's theorem simply by projecting the Heisenberg equation onto formula_26 from the right and formula_27 from the left, or taking the expectation value, so\n\nWe can pull the out of the first term since the state vectors are no longer time dependent in the Heisenberg Picture. Therefore,\n\nThe expectation values of the theorem, however, are the very same in the Schrödinger picture as well. For the very general example of a massive particle moving in a potential, the Hamiltonian is simply\n\nwhere is the position of the particle.\n\nSuppose we wanted to know the instantaneous change in momentum . Using Ehrenfest's theorem, we have\n\nsince the operator commutes with itself and has no time dependence. By expanding the right-hand-side, replacing by , we get\n\nAfter applying the product rule on the second term, we have\n\nAs explained in the introduction, this result does \"not\" say that the pair formula_34 satisfies Newton's second law, because the right-hand side of the formula is formula_35 rather than formula_36. Nevertheless, as explained in the introduction, for states that are highly localized in space, the expected position and momentum will \"approximately\" follow classical trajectories, which may be understood as an instance of the correspondence principle.\n\nSimilarly, we can obtain the instantaneous change in the position expectation value.\n\nThis result is actually in exact accord with the classical equation.\n\nIt was established above that the Ehrenfest theorems are consequences of the Schrödinger equation. However, the converse is also true: the Schrödinger equation can be inferred from the Ehrenfest theorems. We begin from\n\nApplications of the product rule leads to\n\nHere, apply Stone's theorem, using to denote the quantum generator of time translation. The next step is to show that this is the same as the Hamiltonian operator used in quantum mechanics. Stone's theorem implies\n\nwhere was introduced as a normalization constant to the balance dimensionality. Since these identities must be valid for any initial state, the averaging can be dropped and the system of commutator equations for are derived:\n\nAssuming that observables of the coordinate and momentum obey the canonical commutation relation . Setting formula_42, the commutator equations can be converted into the differential equations\n\nwhose solution is the familiar quantum Hamiltonian\n\nWhence, the Schrödinger equation was derived from the Ehrenfest theorems by assuming the canonical commutation relation between the coordinate and momentum. If one assumes that the coordinate and momentum commute, the same computational method leads to the Koopman–von Neumann classical mechanics, which is the Hilbert space formulation of classical mechanics. Therefore, this derivation as well as the derivation of the Koopman–von Neumann mechanics shows that the essential difference between quantum and classical mechanics reduces to the value of the commutator .\n"}
{"id": "33439201", "url": "https://en.wikipedia.org/wiki?curid=33439201", "title": "Electoral Calculus", "text": "Electoral Calculus\n\nElectoral Calculus is a political forecasting web site which attempts to predict future United Kingdom general election results. It considers national factors but excludes local issues.\n\nThe site was developed by Martin Baxter, who is a financial analyst specialising in mathematical modelling.\n\nThe site includes maps, predictions and analysis articles. It has a separate section for elections in Scotland.\n\nThe site is based around the employment of scientific techniques on data about Britain's electoral geography, which can be used to calculate the uniform national swing. It takes account of national polls and trends but excludes local issues.\n\nThe calculations were initially based on what is termed the \"Transition Model\", which is derived from the additive uniform national swing model. This uses national swings in a proportional manner to predict local effects. The \"Strong Transition Model\" was introduced in October 2007, and considers the effects of strong and weak supporters. The models are explained in detail on the web site.\n\nIt was listed by \"The Guardian\" in 2004 as one of the \"100 most useful websites\", being \"the best\" for predictions. In 2012 it was described by PhD student Chris Prosser at the University of Oxford as \"probably the leading vote/seat predictor on the internet\". Its detailed predictions for individual seats have been noted by Paul Evans on the localdemocracy.org.uk blog. Academic Nick Anstead noted in his observations from a 2010 \"Personal Democracy Forum\" event, that Mick Fealty of Slugger O'Toole considered Electoral Calculus to be \"massively improved\" in comparison with the swingometer.\n\nWith reference to the 2010 United Kingdom general election, it was cited by journalists Andrew Rawnsley and Michael White in \"The Guardian\". John Rentoul in \"The Independent\" referred to the site after the election.\n\n"}
{"id": "972601", "url": "https://en.wikipedia.org/wiki?curid=972601", "title": "Elementary equivalence", "text": "Elementary equivalence\n\nIn model theory, a branch of mathematical logic, two structures \"M\" and \"N\" of the same signature σ are called elementarily equivalent if they satisfy the same first-order σ-sentences.\n\nIf \"N\" is a substructure of \"M\", one often needs a stronger condition. In this case \"N\" is called an elementary substructure of \"M\" if every first-order σ-formula φ(\"a\", …, \"a\") with parameters \"a\", …, \"a\" from \"N\" is true in \"N\" if and only if it is true in \"M\".\nIf \"N\" is an elementary substructure of \"M\", \"M\" is called an elementary extension of \"N\". An embedding \"h\": \"N\" → \"M\" is called an elementary embedding of \"N\" into \"M\" if \"h\"(\"N\") is an elementary substructure of \"M\".\n\nA substructure \"N\" of \"M\" is elementary if and only if it passes the Tarski–Vaught test: every first-order formula φ(\"x\", \"b\", …, \"b\") with parameters in \"N\" that has a solution in \"M\" also has a solution in \"N\" when evaluated in \"M\". One can prove that two structures are elementary equivalent with the Ehrenfeucht–Fraïssé games.\n\nTwo structures \"M\" and \"N\" of the same signature σ are elementarily equivalent if every first-order sentence (formula without free variables) over σ is true in \"M\" if and only if it is true in \"N\", i.e. if \"M\" and \"N\" have the same complete first-order theory.\nIf \"M\" and \"N\" are elementarily equivalent, one writes \"M\" ≡ \"N\".\n\nA first-order theory is complete if and only if any two of its models are elementarily equivalent.\n\nFor example, consider the language with one binary relation symbol '<'. The model R of real numbers with its usual order and the model Q of rational numbers with its usual order are elementarily equivalent, since they both interpret '<' as an unbounded dense linear ordering. This is sufficient to ensure elementary equivalence, because the theory of unbounded dense linear orderings is complete, as can be shown by Vaught's test.\n\nMore generally, any first-order theory with an infinite model has non-isomorphic, elementary equivalent models, which can be obtained via the Löwenheim–Skolem theorem. Thus, for example, there are non-standard models of Peano arithmetic, which contain other objects than just the numbers 0, 1, 2, etc., and yet are elementarily equivalent to the standard model.\n\n\"N\" is an elementary substructure of \"M\" if \"N\" and \"M\" are structures of the same signature σ such that for all first-order σ-formulas φ(\"x\", …, \"x\") with free variables \"x\", …, \"x\", and all elements \"a\", …, \"a\" of \"N\", φ(\"a\", …, \"a\") holds in \"N\" if and only if it holds in \"M\":\nIt follows that \"N\" is a substructure of \"M\".\n\nIf \"N\" is a substructure of \"M\", then both \"N\" and \"M\" can be interpreted as structures in the signature σ consisting of σ together with a new constant symbol for every element of \"N\". \"N\" is an elementary substructure of \"M\" if and only if \"N\" is a substructure of \"M\" and \"N\" and \"M\" are elementarily equivalent as σ-structures.\n\nIf \"N\" is an elementary substructure of \"M\", one writes \"N\" formula_3 \"M\" and says that \"M\" is an elementary extension of \"N\": \"M\" formula_4 \"N\".\n\nThe downward Löwenheim–Skolem theorem gives a countable elementary substructure for any infinite first-order structure in at most countable signature; the upward Löwenheim–Skolem theorem gives elementary extensions of any infinite first-order structure of arbitrarily large cardinality.\n\nThe Tarski–Vaught test (or Tarski–Vaught criterion) is a necessary and sufficient condition for a substructure \"N\" of a structure \"M\" to be an elementary substructure. It can be useful for constructing an elementary substructure of a large structure.\n\nLet \"M\" be a structure of signature σ and \"N\" a substructure of \"M\". \"N\" is an elementary substructure of \"M\" if and only if for every first-order formula φ(\"x\", \"y\", …, \"y\") over σ and all elements \"b\", …, \"b\" from \"N\", if \"M\" formula_1 \"x\" φ(\"x\", \"b\", …, \"b\"), then there is an element \"a\" in \"N\" such that \"M\" formula_1φ(\"a\", \"b\", …, \"b\").\n\nAn elementary embedding of a structure \"N\" into a structure \"M\" of the same signature σ is a map \"h\": \"N\" → \"M\" such that for every first-order σ-formula φ(\"x\", …, \"x\") and all elements \"a\", …, \"a\" of \"N\", \nEvery elementary embedding is a strong homomorphism, and its image is an elementary substructure.\n\nElementary embeddings are the most important maps in model theory. In set theory, elementary embeddings whose domain is \"V\" (the universe of set theory) play an important role in the theory of large cardinals (see also critical point).\n\n"}
{"id": "2961091", "url": "https://en.wikipedia.org/wiki?curid=2961091", "title": "Fluent calculus", "text": "Fluent calculus\n\nThe fluent calculus is a formalism for expressing dynamical domains in first-order logic. It is a variant of the situation calculus; the main difference is that situations are considered representations of states. A binary function symbol formula_1 is used to concatenate the terms that represent facts that hold in a situation. For example, that the box is on the table in the situation formula_2 is represented by the formula formula_3. The frame problem is solved by asserting that the situation after the execution of an action is identical to the one before but for the conditions changed by the action. For example, the action of moving the box from the table to the floor is formalized as:\n\nThis formula states that the state after the move is added the term formula_5 and removed the term formula_6. Axioms specifying that formula_1 is commutative and non-idempotent are necessary for such axioms to work.\n\n\n"}
{"id": "27169542", "url": "https://en.wikipedia.org/wiki?curid=27169542", "title": "Ganita-yukti-bhasa", "text": "Ganita-yukti-bhasa\n\nGanita-yukti-bhasa (also written as Ganita Yuktibhasa) is either the title or a part of the title of three different books:\n\n\nThis edition of Yuktibhasa has been divided into two volumes, even though the original Malayalam text has no such division. Volume I deals with mathematics and Volume II treats astronomy. Each volume is divided into three parts: First part is an English translation of the relevant Malayalam part of Yuktibhasa, second part contains detailed explanatory notes on the translation, and in the third part the text in the Malayalam original is reproduced. The English translation is by K.V. Sarma and the explanatory notes are provided by Ramasubramanian, K., Srinivas, M.D. and Sriram, M.S..\n\nVolume I dealing with mathematics is divided into seven chapters. The topics discussed are the eight mathematical operations, a certain set of ten problems, arithmetic of fractions, rule of three, Kuttakara (linear indeterminate equations), infinite series and approximations for the ratio of the circumference and diameter of a circle and infinite series and approximations for sines.\n\nVolume II dealing with astronomy is divided into eight chapters. The topics covered are computation of mean and true longitudes of planets, Earth and celestial spheres, fifteen problems relating to ascension, declination, longitude, etc., determination of time, place, direction, etc., from gnomonic shadow, eclipses, Vyatipata (when the sun and moon have the same declination), visibility correction for planets and phases of the moon.\n\n"}
{"id": "2462837", "url": "https://en.wikipedia.org/wiki?curid=2462837", "title": "Generalized Appell polynomials", "text": "Generalized Appell polynomials\n\nIn mathematics, a polynomial sequence formula_1 has a generalized Appell representation if the generating function for the polynomials takes on a certain form:\n\nwhere the generating function or kernel formula_3 is composed of the series \n\nand \n\nand\n\nGiven the above, it is not hard to show that formula_10 is a polynomial of degree formula_11.\n\nBoas–Buck polynomials are a slightly more general class of polynomials.\n\n\nThe generalized Appell polynomials have the explicit representation\n\nThe constant is\n\nwhere this sum extends over all compositions of formula_11 into formula_19 parts; that is, the sum extends over all formula_20 such that\n\nFor the Appell polynomials, this becomes the formula \n\nEquivalently, a necessary and sufficient condition that the kernel formula_3 can be written as formula_24 with formula_25 is that\n\nwhere formula_27 and formula_28 have the power series\n\nand \n\nSubstituting \n\nimmediately gives the recursion relation\n\nFor the special case of the Brenke polynomials, one has formula_12 and thus all of the formula_34, simplifying the recursion relation significantly.\n\n\n"}
{"id": "1194729", "url": "https://en.wikipedia.org/wiki?curid=1194729", "title": "Heine–Cantor theorem", "text": "Heine–Cantor theorem\n\nIn mathematics, the Heine–Cantor theorem, named after Eduard Heine and Georg Cantor, states that if \"f\" : \"M\" → \"N\" is a continuous function between two metric spaces, and \"M\" is compact, then \"f\" is uniformly continuous. An important special case is that every continuous function from a bounded closed interval to the real numbers is uniformly continuous.\n\nSuppose that formula_1 and formula_2 are two metric spaces with metrics formula_3 and formula_4, respectively. Suppose further that formula_5 is continuous, and that formula_1 is compact. We want to show that formula_7 is uniformly continuous, that is, for every formula_8 there exists formula_9 such that for all points formula_10 in the domain formula_1, formula_12 implies that formula_13.\n\nFix some positive formula_8. Then by continuity, for any point formula_15 in our domain formula_1, there exists a positive real number formula_17 such that formula_18 when formula_19 is within formula_20 of formula_15.\n\nLet formula_22 be the open formula_23-neighborhood of formula_15, i.e. the set\n\nSince each point formula_15 is contained in its own formula_22, we find that the collection formula_28 is an open cover of formula_1. Since formula_1 is compact, this cover has a finite subcover. That subcover must be of the form \n\nfor some finite set of points formula_32. Each of these open sets has an associated radius formula_33. Let us now define formula_34, i.e. the minimum radius of these open sets. Since we have a finite number of positive radii, this number formula_35 is well-defined and positive. We may now show that this formula_36 works for the definition of uniform continuity.\n\nSuppose that formula_12 for any two formula_10 in formula_1. Since the sets formula_40 form an open (sub)cover of our space formula_1, we know that formula_15 must lie within one of them, say formula_43. Then we have that formula_44. The Triangle Inequality then implies that\n\nimplying that formula_15 and formula_19 are both at most formula_48 away from formula_49. By definition of formula_48, this implies that formula_51 and formula_52 are both less than formula_53. Applying the Triangle Inequality then yields the desired\n\nFor an alternative proof in the case of formula_55 a closed interval, see the article on non-standard calculus.\n\n"}
{"id": "49253469", "url": "https://en.wikipedia.org/wiki?curid=49253469", "title": "Holmium–magnesium–zinc quasicrystal", "text": "Holmium–magnesium–zinc quasicrystal\n\nA holmium–magnesium–zinc (Ho–Mg–Zn) quasicrystal is a quasicrystal made of an alloy of those three metals that has the shape of a regular dodecahedron, a Platonic solid with 12 five-sided faces. Unlike the similar pyritohedron shape of some cubic-system crystals such as pyrite, this quasicrystal has faces that are true regular pentagons.\n"}
{"id": "525101", "url": "https://en.wikipedia.org/wiki?curid=525101", "title": "Hypersurface", "text": "Hypersurface\n\nIn geometry, a hypersurface is a generalization of the concepts of hyperplane, plane curve, and surface. A hypersurface is a manifold or an algebraic variety of dimension , which is embedded in an ambient space of dimension , generally a Euclidean space, an affine space or a projective space.\nHypersurfaces share, with surfaces in a three-dimensional space, the property of being defined by a single implicit equation, at least locally (near every point), and sometimes globally.\n\nA hypersurface in a (Euclidean, affine, or projective) space of dimension two is a plane curve. In a space of dimension three, it is a surface. \n\nFor example, the equation\ndefines an algebraic hypersurface of dimension in the Euclidean space of dimension . This hypersurface is also a smooth manifold, and is called a hypersphere or an -sphere.\n\nA hypersurface that is a smooth manifold is called a \"smooth hypersurface\".\n\nIn , a smooth hypersurface is orientable. Every connected compact smooth hypersurface is a level set, and separates R into two connected components; this is related to the Jordan–Brouwer separation theorem.\n\nAn algebraic hypersurface is an algebraic variety that may be defined by a single implicit equation of the form\nwhere is a multivariate polynomial. Generally the polynomial is supposed to be irreducible. When it is not the case, the hypersurface is not an algebraic variety, but only an algebraic set. It may depend on the authors or the context whether a reducible polynomial defines a hypersurface. For avoiding ambiguity, the terms \"irreducible hypersurface\" is often used. \n\nAs for algebraic varieties, the coefficients of the defining polynomial may belong to any fixed field , and the points of the hypersurface are the zeros of in the affine space formula_3 where is an algebraically closed extension of .\n\nA hypersurface may have singularities, which are the common zeros, if any, of the defining polynomial and its partial derivatives. In particular, a real algebraic hypersurface is not necessarily a manifold.\n\nHypersurfaces have some specific properties that are not shared with other algebraic varieties. \n\nOne of the main such properties is Hilbert's Nullstellensatz, which asserts that a hypersurface contains an algebraic set if and only if the defining polynomial of the hypersurface has a power that belongs to the ideal generated by the defining polynomials of the algebraic set.\n\nA corollary of this theorem is that, if two irreducible polynomials (or more generally two square-free polynomials) define the same hypersurface, then one is the product of the other by a nonzero constant.\n\nHypersurfaces are exactly the subvarieties of dimension of an affine space of dimension of . This is the geometric interpretation of the fact that, in a polynomial ring over a field, the height of an ideal is 1 if and only if the ideal is a principal ideal. In the case of possibly reducible hypersurfaces, this result may be restated as follows: hypersurfaces are exactly the algebraic sets whose all irreducible components have dimension .\n\nA \"real hypersurface\" is a hypersurface that is defined by a polynomial with real coefficients. In this case the algebraically closed field over which the points are defined is generally the field formula_4 of complex numbers. The \"real points\" of a real hypersurface are the points that belong to formula_5 The set of the real points of a real hypersurface is the \"real part\" of the hypersurface. Often, it is left to the context whether the term \"hypersurface\" refers to all points or only to the real part.\n\nIf the coefficients of the defining polynomial belong to a field that is not algebraically closed (typically the field of rational numbers, a finite field or a number field), one says that the hypersurface is \"defined over\" , and the points that belong to formula_6 are \"rational\" over (in the case of the field of rational numbers, \"over \" is generally omitted).\n\nFor example, the imaginary -sphere defined by the equation\nis a real hypersurface without any real point, which is defined over the rational numbers. It has no rational point, but has many points that are rational over the Gaussian rationals.\n\nA of dimension in a projective space of dimension over a field is defined by a homogeneous polynomial formula_8 in indeterminates. As usual, means that all monomials of have the same degree, or, equivalently that formula_9 for every constant , where is the degree of the polynomial. The of the hypersurface are the points of the projective space whose projective coordinates are zeros of .\n\nIf one chooses the hyperplane of equation formula_10 as hyperplane at infinity, the complement of this hyperplane is an affine space, and the points of the projective hypersuface that belong to this affine space form an affine hypersurface of equation formula_11 Conversely, given an affine hypersurface of equation formula_2 it defines a projective hypersurface, called its , whose equation is obtained by homogenizing . That is, the equation of the projective completion is formula_13 with \nwhere is the degree of .\n\nThese two processes projective completion and restriction to an affine subspace are inverse one to the other. Therefore, an affine hypersurface and its projective completion have essentially the same properties, and are often considered as two points-of-view for the same hypersurface. \n\nHowever, it may occur that an affine hypersurface is nonsingular, while its projective completion has singular points. In this case, one says that the affine surface is . For example, the circular cylinder of equation \nin the affine space of dimension three has a unique singular point, which is at infinity, in the direction .\n\n\n"}
{"id": "2157781", "url": "https://en.wikipedia.org/wiki?curid=2157781", "title": "Jean-Étienne Montucla", "text": "Jean-Étienne Montucla\n\nJean-Étienne Montucla (5 September 1725 – 18 December 1799) was a French mathematician and historian.\n\nMontucla was born at Lyon.\n\nIn 1754 he published an anonymous treatise on quadrature, \"Histoire des recherches sur la quadrature du cercle\". Montucla's deep interest in history of mathematics became apparent with his publication of \"Histoire des Mathématiques\", the first part appearing in 1758. According to George Sarton, the \"Histoire\" is\n\nHe was appointed intendant-secretary of Grenoble in 1758, secretary to the expedition for colonizing Cayenne in 1764, and chief architect and censor-royal for mathematical books in 1765.\n\nThe French Revolution deprived him of his income and left him in great destitution. The offer in 1795 of a mathematical chair in one of the schools of Paris was declined on account of his infirm health, and he was still in straightened circumstances in 1798, when he published a second edition of the first part of his \"Histoire\".\n\nIn 1778 he re-edited Jacques Ozanam's \"Recreations mathématiques\", afterwards published in English by Charles Hutton (4 vols, London, 1803). After his death, his \"Histoire\" was completed by Jérôme Lalande, and published at Paris in 1799–1802 (4 vols).\n\nIvor Grattan-Guinness described the \"Histoire\" as a milestone:\n"}
{"id": "35195419", "url": "https://en.wikipedia.org/wiki?curid=35195419", "title": "Kronecker's congruence", "text": "Kronecker's congruence\n\nIn mathematics, Kronecker's congruence, introduced by Kronecker, states that\nwhere \"p\" is a prime and Φ(\"x\",\"y\") is the modular polynomial of order \"p\", given by\nfor \"j\" the elliptic modular function and τ running through classes of imaginary quadratic integers of discriminant \"n\".\n"}
{"id": "1273629", "url": "https://en.wikipedia.org/wiki?curid=1273629", "title": "Marginal stability", "text": "Marginal stability\n\nIn the theory of dynamical systems and control theory, a linear time-invariant system is marginally stable if it is neither asymptotically stable nor unstable. Roughly speaking, a system is stable if it always returns to and stays near a particular state (called the steady state), and is unstable if it goes farther and farther away from any state, without being bounded. A marginal system, sometimes referred to as having neutral stability, is between these two types: when displaced, it does not return to near a common steady state, nor does it go away from where it started without limit.\n\nMarginal stability, like instability, is a feature that control theory seeks to avoid; we wish that, when perturbed by some external force, a system will return to a desired state. This necessitates the use of appropriately designed control algorithms.\n\nIn econometrics, the presence of a unit root in observed time series, rendering them marginally stable, can lead to invalid regression results regarding effects of the independent variables upon a dependent variable, unless appropriate techniques are used to convert the system to a stable system.\n\nA homogeneous continuous linear time-invariant system is marginally stable if and only if the real part of every pole (eigenvalue) in the system's transfer-function is non-positive, one or more poles have zero real part, and all poles with zero real part are simple roots (i.e. the poles on the imaginary axis are all distinct from one another). In contrast, if all the poles have strictly negative real parts, the system is instead asymptotically stable. If one or more poles have positive real parts, the system is unstable.\n\nIf the system is in state space representation, marginal stability can be analyzed by deriving the Jordan normal form: if and only if the Jordan blocks corresponding to poles with zero real part are scalar is the system marginally stable.\n\nA homogeneous discrete time linear time-invariant system is marginally stable if and only if the greatest magnitude of any of the poles (eigenvalues) of the transfer function is 1, and the poles with magnitude equal to 1 are all distinct. That is, the transfer function's spectral radius is 1. If the spectral radius is less than 1, the system is instead asymptotically stable.\n\nA simple example involves a single first-order linear difference equation: Suppose a state variable \"x\" evolves according to\n\nwith parameter \"a\" > 0. If the system is perturbed to the value formula_2 its subsequent sequence of values is formula_3 If \"a\" < 1, these numbers get closer and closer to 0 regardless of the starting value formula_2 while if \"a\" > 1 the numbers get larger and larger without bound. But if \"a\" = 1, the numbers do neither of these: instead, all future values of \"x\" equal the value formula_5 Thus the case \"a\" = 1 exhibits marginal stability.\n\nA marginally stable system is one that, if given an impulse of finite magnitude as input, will not \"blow up\" and give an unbounded output, but neither will the output return to zero. A bounded offset or oscillations in the output will persist indefinitely, and so there will in general be no final steady-state output. If a continuous system is given an input at a frequency equal to the frequency of a pole with zero real part, the system's output will increase indefinitely (this is known as pure resonance). This explains why for a system to be BIBO stable, the real parts of the poles have to be strictly negative (and not just non-positive).\n\nA continuous system having imaginary poles, i.e. having zero real part in the pole(s), will produce sustained oscillations in the output. For example, an undamped second-order system such as the suspension system in an automobile (a mass–spring–damper system), from which the damper has been removed and spring is ideal, i.e. no friction is there, will in theory oscillate forever once disturbed. Another example is a frictionless pendulum. A system with a pole at the origin is also marginally stable but in this case there will be no oscillation in the response as the imaginary part is also zero (\"jw\" = 0 means \"w\" = 0 rad/sec). An example of such a system is a mass on a surface with friction. When a sidewards impulse is applied, the mass will move and never returns to zero. The mass will come to rest due to friction however, and the sidewards movement will remain bounded.\n\nSince the locations of the marginal poles must be \"exactly\" on the imaginary axis or unit circle (for continuous time and discrete time systems respectively) for a system to be marginally stable, this situation is unlikely to occur in practice unless marginal stability is an inherent theoretical feature of the system.\n\nMarginal stability is also an important concept in the context of stochastic dynamics. For example, some processes may follow a random walk, given in discrete time as \n\nwhere formula_7 is an i.i.d. error term. This equation has a unit root (a value of 1 for the eigenvalue of its characteristic equation), and hence exhibits marginal stability, so special time series techniques must be used in empirically modeling a system containing such an equation.\n\n"}
{"id": "28280731", "url": "https://en.wikipedia.org/wiki?curid=28280731", "title": "Medial graph", "text": "Medial graph\n\nIn the mathematical discipline of graph theory, the medial graph of plane graph \"G\" is another graph \"M(G)\" that represents the adjacencies between edges in the faces of \"G\". Medial graphs were introduced in 1922 by Ernst Steinitz to study combinatorial properties of convex polyhedra, although the inverse construction was already used by Peter Tait in 1877 in his foundational study of knots and links.\n\nGiven a connected plane graph \"G\", its medial graph \"M(G)\" has\nThe medial graph of a disconnected graph is the disjoint union of the medial graphs of each connected component. The definition of medial graph also extends without modification to graph embeddings on surfaces of higher genus.\n\n\nFor a plane graph \"G\", twice the evaluation of the Tutte polynomial at the point (3,3) equals the sum over weighted Eulerian orientations in the medial graph of \"G\", where the weight of an orientation is 2 to the number of saddle vertices of the orientation (that is, the number of vertices with incident edges cyclically ordered \"in, out, in out\"). Since the Tutte polynomial is invariant under embeddings, this result shows that every medial graph has the same sum of these weighted Eulerian orientations.\n\nThe medial graph definition can be extended to include an orientation. First, the faces of the medial graph are colored black if they contain a vertex of the original graph and white otherwise. This coloring causes each edge of the medial graph to be bordered by one black face and one white face. Then each edge is oriented so that the black face is on its left.\n\nA plane graph and its dual do not have the same directed medial graph; their directed medial graphs are the transpose of each other.\n\nUsing the directed medial graph, one can effectively generalize the result on evaluations of the Tutte polynomial at (3,3). For a plane graph \"G\", \"n\" times the evaluation of the Tutte polynomial at the point (\"n\"+1,\"n\"+1) equals the weighted sum over all edge colorings using \"n\" colors in the directed medial graph of \"G\" so that each (possibly empty) set of monochromatic edges forms a directed Eulerian graph, where the weight of a directed Eulerian orientation is 2 to the number of monochromatic vertices.\n\n"}
{"id": "11704554", "url": "https://en.wikipedia.org/wiki?curid=11704554", "title": "Michael Spivey", "text": "Michael Spivey\n\nMichael Spivey (commonly known as Mike Spivey) is a British computer scientist at the University of Oxford.\n\nSpivey was born in 1960 and educated at Archbishop Holgate's Grammar School in York, England. He studied mathematics at Christ's College, Cambridge and then undertook a DPhil in computer science on the Z notation at Wolfson College, Oxford and the Programming Research Group, part of the Oxford University Computing Laboratory.\n\nMike Spivey is a University Lecturer in Computation at the Oxford University Department of Computer Science and Misys and Anderson Fellow of Computer Science at Oriel College, Oxford. His main areas of research interest are compilers and programming languages, especially logic programming. He wrote an Oberon-2 compiler.\n\n\n"}
{"id": "1018197", "url": "https://en.wikipedia.org/wiki?curid=1018197", "title": "Monadic Boolean algebra", "text": "Monadic Boolean algebra\n\nIn abstract algebra, a monadic Boolean algebra is an algebraic structure \"A\" with signature \n\nwhere 〈\"A\", ·, +, ', 0, 1〉 is a Boolean algebra.\n\nThe monadic/unary operator ∃ denotes the existential quantifier, which satisfies the identities (using the received prefix notation for ∃):\n∃\"x\" is the \"existential closure\" of \"x\". Dual to ∃ is the unary operator ∀, the universal quantifier, defined as ∀\"x\" := (∃\"x' \")'. \n\nA monadic Boolean algebra has a dual definition and notation that take ∀ as primitive and ∃ as defined, so that ∃\"x\" := (∀\"x\" ' )' . (Compare this with the definition of the dual Boolean algebra.) Hence, with this notation, an algebra \"A\" has signature 〈·, +, ', 0, 1, ∀〉, with 〈\"A\", ·, +, ', 0, 1〉 a Boolean algebra, as before. Moreover, ∀ satisfies the following dualized version of the above identities:\n∀\"x\" is the \"universal closure\" of \"x\".\n\nMonadic Boolean algebras have an important connection to topology. If ∀ is interpreted as the interior operator of topology, (1)–(3) above plus the axiom ∀(∀\"x\") = ∀\"x\" make up the axioms for an interior algebra. But ∀(∀\"x\") = ∀\"x\" can be proved from (1)–(4). Moreover, an alternative axiomatization of monadic Boolean algebras consists of the (reinterpreted) axioms for an interior algebra, plus ∀(∀\"x\")' = (∀\"x\")' (Halmos 1962: 22). Hence monadic Boolean algebras are the semisimple interior/closure algebras such that:\nA more concise axiomatization of monadic Boolean algebra is (1) and (2) above, plus ∀(\"x\"∨∀\"y\") = ∀\"x\"∨∀\"y\" (Halmos 1962: 21). This axiomatization obscures the connection to topology.\n\nMonadic Boolean algebras form a variety. They are to monadic predicate logic what Boolean algebras are to propositional logic, and what polyadic algebras are to first-order logic. Paul Halmos discovered monadic Boolean algebras while working on polyadic algebras; Halmos (1962) reprints the relevant papers. Halmos and Givant (1998) includes an undergraduate treatment of monadic Boolean algebra.\n\nMonadic Boolean algebras also have an important connection to modal logic. The modal logic S5, viewed as a theory in \"S4\", is a model of monadic Boolean algebras in the same way that S4 is a model of interior algebra. Likewise, monadic Boolean algebras supply the algebraic semantics for \"S5\". Hence S5-algebra is a synonym for monadic Boolean algebra.\n\n\n"}
{"id": "33935937", "url": "https://en.wikipedia.org/wiki?curid=33935937", "title": "Niels Nielsen (mathematician)", "text": "Niels Nielsen (mathematician)\n\nNiels Nielsen (2 December 1865, in Ørslev – 16 September 1931, in Copenhagen) was a Danish mathematician who specialized in mathematical analysis.\n\n\n\n"}
{"id": "19487617", "url": "https://en.wikipedia.org/wiki?curid=19487617", "title": "Obstacle problem", "text": "Obstacle problem\n\nThe obstacle problem is a classic motivating example in the mathematical study of variational inequalities and free boundary problems. The problem is to find the equilibrium position of an elastic membrane whose boundary is held fixed, and which is constrained to lie above a given obstacle. It is deeply related to the study of minimal surfaces and the capacity of a set in potential theory as well. Applications include the study of fluid filtration in porous media, constrained heating, elasto-plasticity, optimal control, and financial mathematics.\n\nThe mathematical formulation of the problem is to seek minimizers of the Dirichlet energy functional, \nin some domain \"formula_2\" where the functions \"formula_3\" represent the vertical displacement of the membrane. In addition to satisfying Dirichlet boundary conditions corresponding to the fixed boundary of the membrane, the functions \"formula_3\" are in addition constrained to be greater than some given \"obstacle\" function \"formula_5\"formula_6. The solution breaks down into a region where the solution is equal to the obstacle function, known as the \"contact set,\" and a region where the solution is above the obstacle. The interface between the two regions is the \"free boundary.\"\n\nIn general, the solution is continuous and possesses Lipschitz continuous first derivatives, but that the solution is generally discontinuous in the second derivatives across the free boundary. The free boundary is characterized as a Hölder continuous surface except at certain singular points, which reside on a smooth manifold.\n\nThe obstacle problem arises when one considers the shape taken by a soap film in a domain whose boundary position is fixed (see Plateau's problem), with the added constraint that the membrane is constrained to lie above some obstacle \"formula_5\"formula_6 in the interior of the domain as well. In this case, the energy functional to be minimized is the surface area integral, or\n\nThis problem can be \"linearized\" in the case of small perturbations by expanding the energy functional in terms of its Taylor series and taking the first term only, in which case the energy to be minimized is the standard Dirichlet energy\n\nThe obstacle problem also arises in control theory, specifically the question of finding the optimal stopping time for a stochastic process with payoff function \"formula_5\"formula_6.\n\nIn the simple case wherein the process is Brownian motion, and the process is forced to stop upon exiting the domain, the solution formula_13 of the obstacle problem can be characterized as the expected value of the payoff, starting the process at formula_14, if the optimal stopping strategy is followed. The stopping criterion is simply that one should stop upon reaching the \"contact set\".\n\nSuppose the following data is given:\nThen consider the set\nwhich is a closed convex subset of the Sobolev space of square integrable functions with square integrable weak first derivatives, containing precisely those functions with the desired boundary conditions which are also above the obstacle. The solution to the obstacle problem is the function which minimizes the energy integral\nover all functions formula_13 belonging to formula_31; the existence of such a minimizer is assured by considerations of Hilbert space theory.\n\nThe obstacle problem can be reformulated as a standard problem in the theory of variational inequalities on Hilbert spaces. Seeking the energy minimizer in the set \"formula_31\" of suitable functions is equivalent to seeking\n\nwhere ⟨ . , . ⟩ : ℝ\"\" × ℝ\"\" → ℝ is the ordinary scalar product in the finite-dimensional real vector space ℝ\"\". This is a special case of the more general form for variational inequalities on Hilbert spaces, whose solutions are functions \"formula_3\" in some closed convex subset \"formula_31\" of the overall space, such that\n\nfor coercive, real-valued, bounded bilinear forms formula_38 and bounded linear functionals formula_39.\n\nA variational argument shows that, away from the contact set, the solution to the obstacle problem is harmonic. A similar argument which restricts itself to variations that are positive shows that the solution is superharmonic on the contact set. Together, the two arguments imply that the solution is a superharmonic function.\n\nIn fact, an application of the maximum principle then shows that the solution to the obstacle problem is the least superharmonic function in the set of admissible functions.\n\nThe solution to the obstacle problem has formula_40 regularity, or bounded second derivatives, when the obstacle itself has these properties. More precisely, the solution's modulus of continuity and the modulus of continuity for its derivative are related to those of the obstacle. \n\nSubject to a degeneracy condition, level sets of the difference between the solution and the obstacle, formula_48 for formula_49 are formula_50 surfaces. The free boundary, which is the boundary of the set where the solution meets the obstacle, is also formula_50 except on a set of \"singular points,\" which are themselves either isolated or locally contained on a formula_52 manifold.\n\nThe theory of the obstacle problem is extended to other divergence form uniformly elliptic operators, and their associated energy functionals. It can be generalized to degenerate elliptic operators as well.\n\nThe double obstacle problem, where the function is constrained to lie above one obstacle function and below another, is also of interest.\n\nThe Signorini problem is a variant of the obstacle problem, where the energy functional is minimized subject to a constraint which only lives on a surface of one lesser dimension, which includes the \"boundary obstacle problem\", where the constraint operates on the boundary of the domain.\n\nThe parabolic, time-dependent cases of the obstacle problem and its variants are also objects of study.\n\n\n\n"}
{"id": "34426869", "url": "https://en.wikipedia.org/wiki?curid=34426869", "title": "Optimal estimation", "text": "Optimal estimation\n\nIn applied statistics, optimal estimation is a regularized matrix inverse method based on Bayes' theorem.\nIt is used very commonly in the geosciences, particularly for atmospheric sounding.\nA matrix inverse problem looks like this:\nThe essential concept is to transform the matrix, A, into a conditional probability and the variables, formula_2 and formula_3 into probability distributions by assuming Gaussian statistics and using empirically-determined covariance matrices.\n\nTypically, one expects the statistics of most measurements to be Gaussian. So for example for formula_4, we can write:\n\nwhere \"m\" and \"n\" are the numbers of elements in formula_2 and formula_3 respectively formula_8 is the matrix to be solved (the linear or linearised forward model) and formula_9 is the covariance matrix of the vector formula_3. This can be similarly done for formula_2:\n\nHere formula_13 is taken to be the so-called \"a-priori\" distribution: formula_14 denotes the a-priori values for formula_15 while formula_16 is its covariance matrix.\n\nThe nice thing about the Gaussian distributions is that only two parameters are needed to describe them and so the whole problem can be converted once again to matrices. Assuming that formula_17 takes the following form:\n\nformula_19 may be neglected since, for a given value of formula_2, it is simply a constant scaling term. Now it is possible to solve for both the expectation value of formula_2, formula_22, and for its covariance matrix by equating formula_17 and formula_24. This produces the following equations:\n\nBecause we are using Gaussians, the expected value is equivalent to the maximum likely value, and so this is also a form of maximum likelihood estimation.\n\nTypically with optimal estimation, in addition to the vector of retrieved quantities, one extra matrix is returned along with the covariance matrix. This is sometimes called the resolution matrix or the averaging kernel and is calculated as follows:\n\nThis tells us, for a given element of the retrieved vector, how much of the other elements of the vector are mixed in. In the case of a retrieval of profile information, it typical indicates the altitude resolution for a given altitude. For instance if the resolution vectors for all the altitudes contain non-zero elements (to a numerical tolerance) in their four nearest neighbours, then the altitude resolution is only one fourth that of the actual grid size.\n"}
{"id": "2915678", "url": "https://en.wikipedia.org/wiki?curid=2915678", "title": "Orbifold notation", "text": "Orbifold notation\n\nIn geometry, orbifold notation (or orbifold signature) is a system, invented by William Thurston and popularized by the mathematician John Conway, for representing types of symmetry groups in two-dimensional spaces of constant curvature.\nThe advantage of the notation is that it describes these groups in a way which indicates many of the groups' properties: in particular, it describes the orbifold obtained by taking the quotient of Euclidean space by the group under consideration.\nGroups representable in this notation include the point groups on the sphere (formula_1), the frieze groups and wallpaper groups of the Euclidean plane (formula_2), and their analogues on the hyperbolic plane (formula_3).\n\nThe following types of Euclidean transformation can occur in a group described by orbifold notation:\n\n\nAll translations which occur are assumed to form a discrete subgroup of the group symmetries being described.\n\nEach group is denoted in orbifold notation by a finite string made up from the following symbols:\n\n\nA string written in boldface represents a group of symmetries of Euclidean 3-space. A string not written in boldface represents a group of symmetries of the Euclidean plane, which is assumed to contain two independent translations.\n\nEach symbol corresponds to a distinct transformation:\n\n\nAn orbifold symbol is called \"good\" if it is not one of the following: \"p\", \"pq\", *\"p\", *\"pq\", for p,q>=2, and p≠q.\n\nAn object is chiral if its symmetry group contains no reflections; otherwise it is called achiral. The corresponding orbifold is orientable in the chiral case and non-orientable otherwise.\n\nThe Euler characteristic of an orbifold can be read from its Conway symbol, as follows. Each feature has a value:\n\n\nSubtracting the sum of these values from 2 gives the Euler characteristic.\n\nIf the sum of the feature values is 2, the order is infinite, i.e., the notation represents a wallpaper group or a frieze group. Indeed, Conway's \"Magic Theorem\" indicates that the 17 wallpaper groups are exactly those with the sum of the feature values equal to 2. Otherwise, the order is 2 divided by the Euler characteristic.\n\nThe following groups are isomorphic:\nThis is because 1-fold rotation is the \"empty\" rotation.\n\nThe symmetry of a 2D object without translational symmetry can be described by the 3D symmetry type by adding a third dimension to the object which does not add or spoil symmetry. For example, for a 2D image we can consider a piece of carton with that image displayed on one side; the shape of the carton should be such that it does not spoil the symmetry, or it can be imagined to be infinite. Thus we have \"n\"• and *\"n\"•. The bullet (•) is added on one- and two-dimensional groups to imply the existence of a fixed point. (In three dimensions these groups exist in an n-fold digonal orbifold and are represented as \"nn\" and *\"nn\".)\n\nSimilarly, a 1D image can be drawn horizontally on a piece of carton, with a provision to avoid additional symmetry with respect to the line of the image, e.g. by drawing a horizontal bar under the image. Thus the discrete symmetry groups in one dimension are *•, *1•, ∞• and *∞•.\n\nAnother way of constructing a 3D object from a 1D or 2D object for describing the symmetry is taking the Cartesian product of the object and an asymmetric 2D or 1D object, respectively.\n\nA first few hyperbolic groups, ordered by their Euler characteristic are:\n\n\n\n"}
{"id": "44052223", "url": "https://en.wikipedia.org/wiki?curid=44052223", "title": "Orthomorphism", "text": "Orthomorphism\n\nIn abstract algebra, an orthomorphism is a certain kind of mapping from a group into itself. Let \"G\" be a group, and let \"θ\" be a permutation of \"G\". Then \"θ\" is an orthomorphism of \"G\" if the mapping \"f\" defined by \"f\"(\"x\") = \"x\" \"θ\"(\"x\") is also a permutation of \"G\". A permutation \"φ\" of \"G\" is a complete mapping if the mapping \"g\" defined by \"g\"(\"x\") = \"xφ\"(\"x\") is also a permutation of \"G\". Orthomorphisms and complete mappings are closely related. \n"}
{"id": "352343", "url": "https://en.wikipedia.org/wiki?curid=352343", "title": "Outerplanar graph", "text": "Outerplanar graph\n\nIn graph theory, an outerplanar graph is a graph that has a planar drawing for which all vertices belong to the outer face of the drawing.\n\nOuterplanar graphs may be characterized (analogously to Wagner's theorem for planar graphs) by the two forbidden minors and , or by their Colin de Verdière graph invariants.\nThey have Hamiltonian cycles if and only if they are biconnected, in which case the outer face forms the unique Hamiltonian cycle. Every outerplanar graph is 3-colorable, and has degeneracy and treewidth at most 2.\n\nThe outerplanar graphs are a subset of the planar graphs, the subgraphs of series-parallel graphs, and the circle graphs. The maximal outerplanar graphs, those to which no more edges can be added while preserving outerplanarity, are also chordal graphs and visibility graphs.\n\nOuterplanar graphs were first studied and named by , in connection with the problem of determining the planarity of graphs formed by using a perfect matching to connect two copies of a base graph (for instance, many of the generalized Petersen graphs are formed in this way from two copies of a cycle graph). As they showed, when the base graph is biconnected, a graph constructed in this way is planar if and only if its base graph is outerplanar and the matching forms a dihedral permutation of its outer cycle. Chartrand and Harary also proved an analogue of Kuratowski's theorem for outerplanar graphs, that a graph is outerplanar if and only if it does not contain a subdivision of one of the two graphs \"K\" or \"K\".\n\nAn outerplanar graph is an undirected graph that can be drawn in the plane without crossings in such a way that all of the vertices belong to the unbounded face of the drawing. That is, no vertex is totally surrounded by edges. Alternatively, a graph \"G\" is outerplanar if the graph formed from \"G\" by adding a new vertex, with edges connecting it to all the other vertices, is a planar graph.\n\nA maximal outerplanar graph is an outerplanar graph that cannot have any additional edges added to it while preserving outerplanarity. Every maximal outerplanar graph with \"n\" vertices has exactly 2\"n\" − 3 edges, and every bounded face of a maximal outerplanar graph is a triangle.\n\nOuterplanar graphs have a forbidden graph characterization analogous to Kuratowski's theorem and Wagner's theorem for planar graphs: a graph is outerplanar if and only if it does not contain a subdivision of the complete graph \"K\" or the complete bipartite graph \"K\". Alternatively, a graph is outerplanar if and only if it does not contain \"K\" or \"K\" as a minor, a graph obtained from it by deleting and contracting edges.\n\nA triangle-free graph is outerplanar if and only if it does not contain a subdivision of \"K\".\n\nA graph is outerplanar if and only if its Colin de Verdière graph invariant is at most two. The graphs characterized in a similar way by having Colin de Verdière invariant at most one, three, or four are respectively the linear forests, planar graphs, and\nlinklessly embeddable graphs.\n\nAn outerplanar graph is biconnected if and only if the outer face of the graph forms a simple cycle without repeated vertices. An outerplanar graph is Hamiltonian if and only if it is biconnected; in this case, the outer face forms the unique Hamiltonian cycle. More generally, the size of the longest cycle in an outerplanar graph is the same as the number of vertices in its largest biconnected component. For this reason finding Hamiltonian cycles and longest cycles in outerplanar graphs may be solved in linear time, in contrast to the NP-completeness of these problems for arbitrary graphs.\n\nEvery maximal outerplanar graph satisfies a stronger condition than Hamiltonicity: it is node pancyclic, meaning that for every vertex \"v\" and every \"k\" in the range from three to the number of vertices in the graph, there is a length-\"k\" cycle containing \"v\". A cycle of this length may be found by repeatedly removing a triangle that is connected to the rest of the graph by a single edge, such that the removed vertex is not \"v\", until the outer face of the remaining graph has length \"k\".\n\nA planar graph is outerplanar if and only if each of its biconnected components is outerplanar.\n\nAll loopless outerplanar graphs can be colored using only three colors; this fact features prominently in the simplified proof of Chvátal's art gallery theorem by . A 3-coloring may be found in linear time by a greedy coloring algorithm that removes any vertex of degree at most two, colors the remaining graph recursively, and then adds back the removed vertex with a color different from the colors of its two neighbors.\n\nAccording to Vizing's theorem, the chromatic index of any graph (the minimum number of colors needed to color its edges so that no two adjacent edges have the same color) is either the maximum degree of any vertex of the graph or one plus the maximum degree. However, in an outerplanar graph, the chromatic index is equal to the maximum degree except when the graph forms a cycle of odd length. An edge coloring with an optimal number of colors can be found in linear time based on a breadth-first traversal of the weak dual tree.\n\nOuterplanar graphs have degeneracy at most two: every subgraph of an outerplanar graph contains a vertex with degree at most two.\n\nOuterplanar graphs have treewidth at most two, which implies that many graph optimization problems that are NP-complete for arbitrary graphs may be solved in polynomial time by dynamic programming when the input is outerplanar. More generally, \"k\"-outerplanar graphs have treewidth O(\"k\").\n\nEvery outerplanar graph can be represented as an intersection graph of axis-aligned rectangles in the plane, so outerplanar graphs have boxicity at most two.\n\nEvery outerplanar graph is a planar graph.\nEvery outerplanar graph is also a subgraph of a series-parallel graph. However, not all planar series-parallel graphs are outerplanar. \nThe complete bipartite graph \"K\" is planar and series-parallel but not outerplanar. On the other hand, the complete graph \"K\" is planar but neither series-parallel nor outerplanar. \nEvery forest and every cactus graph are outerplanar.\n\nThe weak planar dual graph of an embedded outerplanar graph (the graph that has a vertex for every bounded face of the embedding, and an edge for every pair of adjacent bounded faces) is a forest, and the weak planar dual of a Halin graph is an outerplanar graph. A planar graph is outerplanar if and only if its weak dual is a forest, and it is Halin if and only if its weak dual is biconnected and outerplanar.\n\nThere is a notion of degree of outerplanarity. A 1-outerplanar embedding of a graph is the same as an outerplanar embedding. For \"k\" > 1 a planar embedding is said to be \"k\"-outerplanar if removing the vertices on the outer face results in a (\"k\" − 1)-outerplanar embedding. \nA graph is \"k\"-outerplanar if it has a \"k\"-outerplanar embedding.\n\nAn outer-1-planar graph, analogously to 1-planar graphs can be drawn in a disk, with the vertices on the boundary of the disk, and with at most one crossing per edge.\n\nEvery maximal outerplanar graph is a chordal graph. Every maximal outerplanar graph is the visibility graph of a simple polygon. Maximal outerplanar graphs are also formed as the graphs of polygon triangulations. They are examples of 2-trees, of series-parallel graphs, and of chordal graphs.\n\nEvery outerplanar graph is a circle graph, the intersection graph of a set of chords of a circle.\n\n"}
{"id": "44810721", "url": "https://en.wikipedia.org/wiki?curid=44810721", "title": "Problems, Problems, Problems", "text": "Problems, Problems, Problems\n\nProblems, Problems, Problems is a series of educational mathematics textbooks ranging from grade 7-12. The math questions are from previous math contests held by the Centre for Education in Mathematics and Computing at the University of Waterloo. The series is a resource to complement the Canadian high school curriculum and for self-development, although it is also used to study for the CEMC Math Contests. Each combined grade level has several book volumes. Each book contains around 300 questions, and is categorized into various sections. Each question has a reference number in the form \"year-contest-question number\".\n\nThe \"Problems, Problems, Problems\" series collection is a National Library of Canada Cataloging in Publication (CIP).\n"}
{"id": "25452444", "url": "https://en.wikipedia.org/wiki?curid=25452444", "title": "Risk intelligence", "text": "Risk intelligence\n\nRisk intelligence is a concept that generally means \"beyond risk management\", though it has been used in different ways by different writers. The term is being used more frequently by business strategists when discussing integrative business processes related to governance, risk, and compliance.\n\nThe first non-definitive usage of the phrase Risk Intelligence appears in the 1980s and aligns to the definition of \"intelligence\" as being information from an enemy (for example, regarding credit risk.) The topic of balancing risk and innovation using information and the cognitive processes involved also appears at this time. Recent usage is more aligned to \"intelligence\" as understanding and problem solving.\n\nThe US business writer David Apgar defines it as the capacity to learn about risk from experience. \n\nManagement consultant and risk advisor Rick Funston defines risk intelligence as a dynamic approach to protect and create value amid uncertainty.\nIt is an enterprise wide process integrating people, processes (systems), and tools\nto increase information available to decision makers for improved decision making.\n\nThe UK philosopher and psychologist Dylan Evans defines it as \"a special kind of intelligence for thinking about risk and uncertainty\", at the core of which is the ability to estimate probabilities accurately. Evans includes a risk intelligence test (RQ) in his book and on his website (below) analogous to \nIQ or EQ.\n\nAmerican financial executive, author, and Columbia University professor Leo Tilman defined risk intelligence as “The organizational ability to think holistically about risk and uncertainty, speak a common risk language, and effectively use forward-looking risk concepts and tools in making better decisions, alleviating threats, capitalizing on opportunities, and creating lasting value.”\n\nAs an emerging concept, risk intelligence shares characteristics with other topics such as business intelligence and competitive intelligence. As such, there are some in those camps who believe that risk intelligence is the set of processes for the transformation of risk data into meaningful and useful information for risk analysis, treatment and planning purposes.\n\n"}
{"id": "17772141", "url": "https://en.wikipedia.org/wiki?curid=17772141", "title": "Tadashi Nakayama (mathematician)", "text": "Tadashi Nakayama (mathematician)\n\n\n"}
{"id": "30456918", "url": "https://en.wikipedia.org/wiki?curid=30456918", "title": "Topology and Its Applications", "text": "Topology and Its Applications\n\nTopology and Its Applications is a peer-reviewed mathematics journal publishing research on topology. It was established in 1971 as \"General Topology and Its Applications\" (), and renamed to its current title in 1980. The journal currently publishes 18 issues each year in one volume. It is indexed by Scopus, Mathematical Reviews, and Zentralblatt MATH. Its 2004–2008 MCQ was 0.38 and its 2016 impact factor was 0.377.\n"}
{"id": "5966396", "url": "https://en.wikipedia.org/wiki?curid=5966396", "title": "Transpose graph", "text": "Transpose graph\n\nIn the mathematical and algorithmic study of graph theory, the converse, transpose or reverse of a directed graph \"G\" is another directed graph on the same set of vertices with all of the edges reversed compared to the orientation of the corresponding edges in \"G\". That is, if \"G\" contains an edge \"(u,v)\" then the converse/transpose/reverse of \"G\" contains an edge \"(v,u)\" and vice versa.\n\nThe name \"converse\" arises because the reversal of arrows corresponds to taking the converse of an implication in logic. The name \"transpose\" is because the adjacency matrix of the transpose directed graph is the transpose of the adjacency matrix of the original directed graph. \nThere is no general agreement on preferred terminology.\n\nThe converse is denoted symbolically as \"G\"', \"G\", \"G\", or other notations, depending on which terminology is used and which book or article is the source for the notation.\n\nAlthough there is little difference mathematically between a graph and its transpose, the difference may be larger in computer science, depending on how a given graph is represented. For instance, for the web graph, it is easy to determine the outgoing links of a vertex, but hard to determine the incoming links, while in the reversal of this graph the opposite is true. In graph algorithms, therefore, it may sometimes be useful to construct the reversal of a graph, in order to put the graph into a form which is more suitable for the operations being performed on it. An example of this is Kosaraju's algorithm for strongly connected components, which applies depth first search twice, once to the given graph and a second time to its reversal.\n\nA skew-symmetric graph is a graph that is isomorphic to its own transpose graph, via a special kind of isomorphism that pairs up all of the vertices.\n\nThe converse relation of a binary relation is the relation that reverses the ordering of each pair of related objects. If the relation is interpreted as a directed graph, this is the same thing as the transpose of the graph. In particular, the order dual of a partial order can be interpreted in this way as the transposition of a transitively-closed directed acyclic graph.\n"}
{"id": "42693", "url": "https://en.wikipedia.org/wiki?curid=42693", "title": "Upper and lower bounds", "text": "Upper and lower bounds\n\nIn mathematics, especially in order theory, an upper bound of a subset \"S\" of some partially ordered set (\"K\", ≤) is an element of \"K\" which is greater than or equal to every element of \"S\". The term lower bound is defined dually as an element of \"K\" which is less than or equal to every element of \"S\". A set with an upper bound is said to be bounded from above by that bound, a set with a lower bound is said to be bounded from below by that bound. The terms bounded above (bounded below) are also used in the mathematical literature for sets that have upper (respectively lower) bounds. \n\nFor example, 5 is a lower bound for the set { 5, 8, 42, 34, 13934 }; so is 4 if 4 belongs to the set K; but 6 is not as the set is ordered and 6 does not belong to set K.\n\nAnother example: for the set { 42 }, the number 42 is both an upper bound and a lower bound; all other real numbers are either an upper bound or a lower bound for that set.\n\nEvery subset of the natural numbers has a lower bound, since the natural numbers have a least element (0, or 1 depending on the exact definition of natural numbers). An infinite subset of the natural numbers cannot be bounded from above. An infinite subset of the integers may be bounded from below or bounded from above, but not both. An infinite subset of the rational numbers may or may not be bounded from below and may or may not be bounded from above.\n\nEvery finite subset of a non-empty totally ordered set has both upper and lower bounds.\n\nThe definitions can be generalized to functions and even sets of functions.\n\nGiven a function with domain and a partially ordered set as codomain, an element of is an upper bound of if for each in . The upper bound is called \"sharp\" if equality holds for at least one value of .\n\nFunction defined on domain and having the same codomain is an upper bound of if for each in .\n\nFunction is further said to be an upper bound of a set of functions if it is an upper bound of each function in that set.\n\nThe notion of lower bound for (sets of) functions is defined analogously, with ≤ replacing ≥.\n\nAn upper bound is said to be a \"tight upper bound\", a \"least upper bound\", or a \"supremum\" if no smaller value is an upper bound.\nSimilarly a lower bound is said to be a \"tight lower bound\", a \"greatest lower bound\", or an \"infimum\" if no greater value is a lower bound.\n"}
