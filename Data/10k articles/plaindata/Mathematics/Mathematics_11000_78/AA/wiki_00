{"id": "43742110", "url": "https://en.wikipedia.org/wiki?curid=43742110", "title": "Baumgartner's axiom", "text": "Baumgartner's axiom\n\nIn mathematical set theory, Baumgartner's axiom (BA) can be one of three different axioms introduced by James Earl Baumgartner.\n\nAn axiom introduced by states that any two ℵ-dense subsets of the real line are order-isomorphic. Todorcevic showed that this Baumgartner's Axiom is a consequence of the Proper Forcing Axiom.\n\nAnother axiom introduced by states that Martin's axiom for partially ordered sets MA(\"κ\") is true for all partially ordered sets \"P\" that are countable closed, well met and ℵ-linked and all cardinals κ less than 2.\n\nBaumgartner's axiom A is an axiom for partially ordered sets introduced in . A partial order (\"P\", ≤) is said to satisfy axiom A if there is a family ≤ of partial orderings on \"P\" for \"n\" = 0, 1, 2, ... such that \n\n"}
{"id": "3921716", "url": "https://en.wikipedia.org/wiki?curid=3921716", "title": "Categorical set theory", "text": "Categorical set theory\n\nCategorical set theory is any one of several versions of set theory developed from or treated in the context of mathematical category theory.\n\n\n\n"}
{"id": "337713", "url": "https://en.wikipedia.org/wiki?curid=337713", "title": "Composite data type", "text": "Composite data type\n\nIn computer science, a composite data type or compound data type is any data type which can be constructed in a program using the programming language's primitive data types and other composite types. It is sometimes called a structure or aggregate data type, although the latter term may also refer to arrays, lists, etc. The act of constructing a composite type is known as composition.\n\nA codice_1 is C's and C++'s notion of a composite type, a datatype that composes a fixed set of labeled fields or members. It is so called because of the codice_1 keyword used in declaring them, which is short for \"structure\" or, more precisely, \"user-defined data structure\".\n\nIn C++, the only difference between a codice_1 and a class is the default access level, which is \"private\" for classes and \"public\" for codice_1s.\n\nNote that while classes and the codice_5 keyword were completely new in C++, the C programming language already had a crude type of codice_1s. For all intents and purposes, C++ codice_1s form a superset of C codice_1s: virtually all valid C codice_1s are valid C++ codice_1s with the same semantics.\n\nA codice_1 declaration consists of a list of fields, each of which can have any type. The total storage required for a codice_1 object is the sum of the storage requirements of all the fields, plus any internal padding.\n\nFor example:\ndefines a type, referred to as codice_13. To create a new variable of this type, we can write codice_14\nwhich has an integer component, accessed by codice_15, and a floating-point component, accessed by codice_16, as well as the codice_17 and codice_18 components. The structure codice_19 contains all four values, and all four fields may be changed independently.\n\nSince writing codice_13 repeatedly in code becomes cumbersome, it is not unusual to see a codice_21 statement in C code to provide a more convenient synonym for the codice_1.\n\nFor example:\nIn C++ code, the codice_21 is not needed because types defined using codice_1 are already part of the regular namespace, so the type can be referred to as either codice_13 or simply codice_26.\n\nAs another example, a three-dimensional Vector composite type that uses the floating point data type could be created with:\nA variable named codice_27 with a codice_28 composite type would be declared as codice_29 Members of the codice_27 would be accessed using a dot notation. For example, codice_31 would set the codice_32 component of codice_27 equal to 5.\n\nLikewise, a color structure could be created using:\nIn 3D graphics, you usually must keep track of both the position and color of each vertex. One way to do this would be to create a codice_34 composite type, using the previously created codice_28 and codice_36 composite types:\nCreate a variable of type codice_37 using the same format as before: codice_38\n\nAssign values to the components of codice_39 like so:\n\nThe primary use of codice_1 is for the construction of complex datatypes, but sometimes it is used to create primitive structural subtyping. For example, since Standard C requires that if two structs have the same initial fields, those fields will be represented in the same way, the code\n\nwill work correctly.\n\nType signatures (or Function types) are constructed from primitive and composite types, and can serve as types themselves when constructing composite types:\n\n"}
{"id": "34955091", "url": "https://en.wikipedia.org/wiki?curid=34955091", "title": "Conley–Zehnder theorem", "text": "Conley–Zehnder theorem\n\nIn mathematics, the Conley–Zehnder theorem, named after Charles C. Conley and Eduard Zehnder, provides a lower bound for the number of fixed points of Hamiltonian diffeomorphisms of standard symplectic tori in terms of the topology of the underlying tori. The lower bound is one plus the cup-length of the torus (thus 2n+1, where 2n is the dimension of the considered torus), and it can be strengthen to the rank of the homology of the torus (which is 2) provided all the fixed points are non-degenerate, this latter condition being generic in the C-topology.\n\nThe theorem was conjectured by Vladimir Arnold, and it was known as the Arnold conjecture on fixed points of symplectomorphisms. Its validity was later extended to more general closed symplectic manifolds by Andreas Floer and several others.\n"}
{"id": "25228957", "url": "https://en.wikipedia.org/wiki?curid=25228957", "title": "Connes embedding problem", "text": "Connes embedding problem\n\nIn von Neumann algebras, the Connes embedding problem or conjecture, due to Alain Connes, asks whether every type II factor on a separable Hilbert space can be embedded into the ultrapower of the hyperfinite type II factor by a free ultrafilter. The problem admits a number of equivalent formulations.\n\nLet formula_1 be a free ultrafilter on the natural numbers and let \"R\" be the hyperfinite type II factor with trace formula_2. One can construct the ultrapower formula_3 as follows: let formula_4 be the von Neumann algebra of norm-bounded sequences and let formula_5. The quotient formula_6 turns out to be a II factor with trace formula_7, where formula_8 is any representative sequence of formula_9.\n\nConnes' Embedding Conjecture asks whether every type II factor on a separable Hilbert space can be embedded into some formula_3.\n\nThe isomorphism class of formula_3 is independent of the ultrafilter if and only if the continuum hypothesis is true (Ge-Hadwin and Farah-Hart-Sherman), but such an embedding property does not depend on the ultrafilter because von Neumann algebras acting on separable Hilbert spaces are, roughly speaking, very small.\n\n"}
{"id": "2557627", "url": "https://en.wikipedia.org/wiki?curid=2557627", "title": "Curved space", "text": "Curved space\n\nCurved space often refers to a spatial geometry which is not \"flat\" where a flat space is described by Euclidean geometry. Curved spaces can generally be described by Riemannian geometry though some simple cases can be described in other ways. Curved spaces play an essential role in general relativity, where gravity is often visualized as curved space. The Friedmann-Lemaître-Robertson-Walker metric is a curved metric which forms the current foundation for the description of the expansion of space and shape of the universe.\n\nA very familiar example of a curved space is the surface of a sphere. While to our familiar outlook the sphere \"looks\" three-dimensional, if an object is constrained to lie on the surface, it only has two dimensions that it can move in. The surface of a sphere can be completely described by two dimensions since no matter how rough the surface may appear to be, it is still only a surface, which is the two-dimensional outside border of a volume. Even the surface of the Earth, which is fractal in complexity, is still only a two-dimensional boundary along the outside of a volume.\n\nOne of the defining characteristics of a curved space is its departure with the Pythagorean theorem. In a curved space \n\nThe Pythagorean relationship can often be restored by describing the space with an extra dimension.\nSuppose we have a non-euclidean three-dimensional space with coordinates formula_2. Because it is not flat \n\nBut if we now describe the three-dimensional space with \"four\" dimensions (formula_4) we can \"choose\" coordinates such that \n\nNote that the coordinate formula_6 is not the same as the coordinate formula_7.\n\nFor the choice of the 4D coordinates to be valid descriptors of the original 3D space it must have the same number of degrees of freedom. Since four coordinates have four degrees of freedom it must have a constraint placed on it. We can choose a constraint such that Pythagorean theorem holds in the new 4D space. That is \n\nThe constant can be positive or negative. For convenience we can choose the constant to be \n\nWe can now use this constraint to eliminate the artificial fourth coordinate formula_12. The differential of the constraining equation is \n\nPlugging formula_15 into the original equation gives\n\nThis form is usually not particularly appealing and so a coordinate transform is often applied: formula_17, formula_18, formula_19. With this coordinate transformation\n\nThe geometry of a n-dimensional space can also be described with Riemannian geometry. An isotropic and homogeneous space can be described by the metric:\nThis reduces to Euclidean space when formula_22. But a space can be said to be \"flat\" when the Weyl Tensor has all zero components. In three dimensions this condition is met when the Ricci Tensor (formula_23) is equal to the metric times the Ricci Scalar (formula_24, not to be confused with the R of the previous section). That is formula_25. Calculation of these components from the metric gives that \n\nThis gives the metric:\n\nwhere formula_29 can be zero, positive, or negative and is not limited to ±1.\n\nAn isotropic and homogeneous space can be described by the metric:\n\nIn the limit that the constant of curvature (formula_24) becomes infinitely large, a flat, Euclidean space is returned. It is essentially the same as setting formula_32 to zero. If formula_32 is not zero the space is not Euclidean. When formula_34 the space is said to be \"closed\" or elliptic. When formula_35 the space is said to be \"open\" or hyperbolic.\n\nTriangles which lie on the surface of an open space will have a sum of angles which is less than 180°. Triangles which lie on the surface of a closed space will have a sum of angles which is greater than 180°. The volume, however, is not formula_36.\n\n\n"}
{"id": "39807150", "url": "https://en.wikipedia.org/wiki?curid=39807150", "title": "D-Wave Two", "text": "D-Wave Two\n\nD-Wave Two (project code name Vesuvius) is the second commercially available quantum computer, and the successor to the first commercially available quantum computer, D-Wave One. Both computers were developed by Canadian company D-Wave Systems. The computers are not general purpose, but rather are designed for quantum annealing. Specifically, the computers are designed to use quantum annealing to solve a single type of problem known as quadratic unconstrained binary optimization. As of 2015, it is still heavily debated whether large-scale entanglement takes place in D-Wave Two, and whether current or future generations of D-Wave computers will have any advantage over classical computers.\n\nD-Wave Two boasts a CPU of about 512 qubits—an improvement over the D-Wave One series' CPUs of about 128 qubits The number of qubits can vary from chip to chip, due to variations in manufacturing. The increase in qubit count for the D-Wave Two was accomplished by tiling qubit pattern of the D-Wave One. This pattern, named \"chimera\" by D-Wave Systems, has a limited connectivity such that a given qubit can only interact with at most six other qubits. As with the D-Wave One, this restricted connectivity greatly limits the optimization problems that can be approached with the hardware.\n\nIn March 2013, several groups of researchers at the Adiabatic Quantum Computing workshop at the Institute of Physics in London produced evidence of quantum entanglement in D-Wave CPUs. In March 2014, researchers from University College London and the University of Southern California corroborated their findings; in their tests, the D-Wave Two exhibited the quantum physics outcome that it should while not showing three different classical physics outcomes.\n\nIn May 2013, Catherine McGeoch verified that D-Wave Two finds solutions to a synthetic benchmark set of Ising spin optimization problems. Boixo et al. (2014) evidenced that the D-Wave Two performs quantum annealing, but that a simulated annealing on a notebook computer also performs well. Jean Francois Puget of IBM compared computation on the D-Wave Two with IBM's CPLEX software.\n\nA D-Wave Two in the Quantum Artificial Intelligence Lab at the NASA Advanced Supercomputing Division of Ames Research Center is used for research into machine learning and related fields of study. NASA, Google, and the Universities Space Research Association (USRA) started the lab in 2013.\n\nIn July 2016, computer music researcher Alexis Kirke used a harmony algorithm developed for the D-Wave 2 live in a public musical performance for mezzo soprano and electronics in the UK.\n\n"}
{"id": "8887", "url": "https://en.wikipedia.org/wiki?curid=8887", "title": "Direct product", "text": "Direct product\n\nIn mathematics, one can often define a direct product of objects already known, giving a new one. This generalizes the Cartesian product of the underlying sets, together with a suitably defined structure on the product set. More abstractly, one talks about the product in category theory, which formalizes these notions.\n\nExamples are the product of sets (see Cartesian product), groups (described below), the product of rings and of other algebraic structures. The product of topological spaces is another instance.\n\nThere is also the direct sum – in some areas this is used interchangeably, in others it is a different concept.\n\n\nIn a similar manner, we can talk about the product of more than two objects, e.g. formula_18. We can even talk about product of infinitely many objects, e.g. formula_19.\n\nIn group theory one can define the direct product of two groups (\"G\", ∘) and (\"H\", ∙), denoted by \"G\" × \"H\". For abelian groups which are written additively, it may also be called the direct sum of two groups, denoted by formula_20.\n\nIt is defined as follows:\n(Note (\"G\", ∘) may be the same as (\"H\", ∙).)\n\nThis construction gives a new group. It has a normal subgroup isomorphic to \"G\" (given by the elements of the form (\"g\", 1)), and one isomorphic to \"H\" (comprising the elements (1, \"h\")).\n\nThe reverse also holds, there is the following recognition theorem: If a group \"K\" contains two normal subgroups \"G\" and \"H\", such that \"K\"= \"GH\" and the intersection of \"G\" and \"H\" contains only the identity, then \"K\" is isomorphic to \"G\" × \"H\". A relaxation of these conditions, requiring only one subgroup to be normal, gives the semidirect product.\n\nAs an example, take as \"G\" and \"H\" two copies of the unique (up to isomorphisms) group of order 2, \"C\": say {1, \"a\"} and {1, \"b\"}. Then \"C\"×\"C\" = {(1,1), (1,\"b\"), (\"a\",1), (\"a\",\"b\")}, with the operation element by element. For instance, (1,\"b\")*(\"a\",1) = (1*\"a\", \"b\"*1) = (\"a\",\"b\"), and (1,\"b\")*(1,\"b\") = (1,\"b\") = (1,1).\n\nWith a direct product, we get some natural group homomorphisms for free: the projection maps \ncalled the coordinate functions.\n\nAlso, every homomorphism \"f\" to the direct product is totally determined by its component functions \nformula_22.\n\nFor any group (\"G\", ∘), and any integer \"n\" ≥ 0, multiple application of the direct product gives the group of all \"n\"-tuples \"G\" (for \"n\" = 0 the trivial group). Examples:\n\nThe direct product for modules (not to be confused with the tensor product) is very similar to the one defined for groups above, using the Cartesian product with the operation of addition being componentwise, and the scalar multiplication just distributing over all the components. Starting from R we get Euclidean space R, the prototypical example of a real \"n\"-dimensional vector space. The direct product of R and R is R.\n\nNote that a direct product for a finite index formula_23 is identical to the direct sum formula_24. The direct sum and direct product differ only for infinite indices, where the elements of a direct sum are zero for all but for a finite number of entries. They are dual in the sense of category theory: the direct sum is the coproduct, while the direct product is the product.\n\nFor example, consider formula_25 and formula_26, the infinite direct product and direct sum of the real numbers. Only sequences with a finite number of non-zero elements are in \"Y\". For example, (1,0,0,0...) is in \"Y\" but (1,1,1,1...) is not. Both of these sequences are in the direct product \"X\"; in fact, \"Y\" is a proper subset of \"X\" (that is, \"Y\" ⊂ \"X\").\n\nThe direct product for a collection of topological spaces \"X\" for \"i\" in \"I\", some index set, once again makes use of the Cartesian product\n\nDefining the topology is a little tricky. For finitely many factors, this is the obvious and natural thing to do: simply take as a basis of open sets to be the collection of all Cartesian products of open subsets from each factor:\n\nThis topology is called the product topology. For example, directly defining the product topology on R by the open sets of R (disjoint unions of open intervals), the basis for this topology would consist of all disjoint unions of open rectangles in the plane (as it turns out, it coincides with the usual metric topology).\n\nThe product topology for infinite products has a twist, and this has to do with being able to make all the projection maps continuous and to make all functions into the product continuous if and only if all its component functions are continuous (i.e. to satisfy the categorical definition of product: the morphisms here are continuous functions): we take as a basis of open sets to be the collection of all Cartesian products of open subsets from each factor, as before, with the proviso that all but finitely many of the open subsets are the entire factor:\n\nThe more natural-sounding topology would be, in this case, to take products of infinitely many open subsets as before, and this does yield a somewhat interesting topology, the box topology. However it is not too difficult to find an example of bunch of continuous component functions whose product function is not continuous (see the separate entry box topology for an example and more). The problem which makes the twist necessary is ultimately rooted in the fact that the intersection of open sets is only guaranteed to be open for finitely many sets in the definition of topology.\n\nProducts (with the product topology) are nice with respect to preserving properties of their factors; for example, the product of Hausdorff spaces is Hausdorff; the product of connected spaces is connected, and the product of compact spaces is compact. That last one, called Tychonoff's theorem, is yet another equivalence to the axiom of choice.\n\nFor more properties and equivalent formulations, see the separate entry product topology.\n\nOn the Cartesian product of two sets with binary relations \"R\" and \"S\", define (\"a\", \"b\") T (\"c\", \"d\") as \"a\" \"R\" \"c\" and \"b\" \"S\" \"d\". If \"R\" and \"S\" are both reflexive, irreflexive, transitive, symmetric, or antisymmetric, relation \"T\" has the same property. Combining properties it follows that this also applies for being a preorder and being an equivalence relation. However, if \"R\" and \"S\" are total relations, \"T\" is in general not.\n\nIf Σ is a fixed signature, \"I\" is an arbitrary (possibly infinite) index set, and (A) is an indexed family of Σ algebras, the direct product A = ∏ A is a Σ algebra defined as follows:\nFor each , the \"i\"th projection is defined by . It is a surjective homomorphism between the Σ algebras A and A.\n\nAs a special case, if the index set the direct product of two Σ algebras A and A is obtained, written as . If Σ just contains one binary operation \"f\", the above definition of the direct product of groups is obtained, using the notation , , , , and . Similarly, the definition of the direct product of modules is subsumed here.\n\nThe direct product can be abstracted to an arbitrary category. In a general category, given a collection of objects \"A\" \"and\" a collection of morphisms \"p\" from \"A\" to \"A\" with \"i\" ranging in some index set \"I\", an object \"A\" is said to be a categorical product in the category if, for any object \"B\" and any collection of morphisms \"f\" from \"B\" to \"A\", there exists a unique morphism \"f\" from \"B\" to \"A\" such that \"f = p f\" and this object \"A\" is unique. This not only works for two factors, but arbitrarily (even infinitely) many.\n\nFor groups we similarly define the direct product of a more general, arbitrary collection of groups \"G\" for \"i\" in \"I\", \"I\" an index set. Denoting the Cartesian product of the groups by \"G\" we define multiplication on \"G\" with the operation of componentwise multiplication; and corresponding to the \"p\" in the definition above are the projection maps\n\nthe functions that take formula_31 to its \"i\"th component \"g\".\n\nSome authors draw a distinction between an internal direct product and an external direct product. If formula_32 and formula_33, then we say that \"X\" is an \"internal\" direct product (of \"A\" and \"B\"); if \"A\" and \"B\" are not subobjects, then we say that this is an \"external\" direct product.\n\nA metric on a Cartesian product of metric spaces, and a norm on a direct product of normed vector spaces, can be defined in various ways, see for example p-norm.\n\n"}
{"id": "181334", "url": "https://en.wikipedia.org/wiki?curid=181334", "title": "Discrete logarithm", "text": "Discrete logarithm\n\nIn the mathematics of the real numbers, the logarithm log \"a\" is a number \"x\" such that , for given numbers \"a\" and \"b\". Analogously, in any group \"G\", powers \"b\" can be defined for all integers \"k\", and the discrete logarithm log \"a\" is an integer \"k\" such that .\n\nDiscrete logarithms are quickly computable in a few special cases. However, no efficient method is known for computing them in general. Several important algorithms in public-key cryptography base their security on the assumption that the discrete logarithm problem over carefully chosen groups has no efficient solution.\n\nLet \"G\" be any group. Denote its group operation by multiplication and its identity element by 1. Let \"b\" be any element of \"G\". For any positive integer \"k\", the expression \"b\" denotes the product of \"b\" with itself \"k\" times:\nSimilarly, let \"b\" denote the product of \"b\" with itself \"k\" times. For \"k\" = 0, the \"k\"th power is the identity: .\n\nLet \"a\" also be an element of \"G\". An integer \"k\" that solves the equation is termed a discrete logarithm (or simply logarithm, in this context) of \"a\" to the base \"b\". One writes \"k\" = log \"a\".\n\nThe powers of 10 form an infinite subset \"G\" = {…, 0.001, 0.01, 0.1, 1, 10, 100, 1000, …} of the rational numbers. This set \"G\" is a cyclic group under multiplication, and 10 is a generator. For any element \"a\" of the group, one can compute log \"a\". For example, log 10000 = 4, and log 0.001 = −3. These are instances of the discrete logarithm problem.\n\nOther base-10 logarithms in the real numbers are not instances of the discrete logarithm problem, because they involve non-integer exponents. For example, the equation log 53 = 1.724276… means that 10 = 53. While integer exponents can be defined in any group using products and inverses, arbitrary real exponents in the real numbers require other concepts such as the exponential function.\n\nA similar example holds for any non-zero real number \"b\". The powers form a multiplicative subgroup \"G\" = {…, \"b\", \"b\", \"b\", 1, \"b\", \"b\", \"b\", …} of the non-zero real numbers. For any element \"a\" of \"G\", one can compute log \"a\".\n\nOne of the simplest settings for discrete logarithms is the group (Z). This is the group of multiplication modulo the prime \"p\". Its elements are congruence classes modulo \"p\", and the group product of two elements may be obtained by ordinary integer multiplication of the elements followed by reduction modulo \"p\".\n\nThe \"k\"th power of one of the numbers in this group may be computed by finding its \"k\"th power as an integer and then finding the remainder after division by \"p\". When the numbers involved are large, it is more efficient to reduce modulo \"p\" multiple times during the computation. Regardless of the specific algorithm used, this operation is called modular exponentiation. For example, consider (Z). To compute 3 in this group, compute 3 = 81, and then divide 81 by 17, obtaining a remainder of 13. Thus 3 = 13 in the group (Z).\n\nThe discrete logarithm is just the inverse operation. For example, consider the equation 3 ≡ 13 (mod 17) for \"k\". From the example above, one solution is \"k\" = 4, but it is not the only solution. Since 3 ≡ 1 (mod 17)—as follows from Fermat's little theorem—it also follows that if \"n\" is an integer then 3 ≡ 3 × (3) ≡ 13 × 1 ≡ 13 (mod 17). Hence the equation has infinitely many solutions of the form 4 + 16\"n\". Moreover, because 16 is the smallest positive integer \"m\" satisfying 3 ≡ 1 (mod 17), these are the only solutions. Equivalently, the set of all possible solutions can be expressed by the constraint that \"k\" ≡ 4 (mod 16).\n\nIn the special case where \"b\" is the identity element 1 of the group \"G\", the discrete logarithm log \"a\" is undefined for \"a\" other than 1, and every integer \"k\" is a discrete logarithm for \"a\" = 1.\n\nPowers obey the usual algebraic identity \"b\" = \"b\" \"b\". In other words, the function\ndefined by \"f\"(\"k\") = \"b\" is a group homomorphism from the integers Z under addition onto the subgroup \"H\" of \"G\" generated by \"b\". For all \"a\" in \"H\", log \"a\" exists. Conversely, log \"a\" does not exist for \"a\" that are not in \"H\".\n\nIf \"H\" is infinite, then log \"a\" is also unique, and the discrete logarithm amounts to a group isomorphism\n\nOn the other hand, if \"H\" is finite of size \"n\", then log \"a\" is unique only up to congruence modulo \"n\", and the discrete logarithm amounts to a group isomorphism\nwhere Z denotes the additive group of integers modulo \"n\".\n\nThe familiar base change formula for ordinary logarithms remains valid: If \"c\" is another generator of \"H\", then\n\nThe discrete logarithm problem is considered to be computationally intractable. That is, no efficient classical algorithm is known for computing discrete logarithms in general.\n\nA general algorithm for computing log \"a\" in finite groups \"G\" is to raise \"b\" to larger and larger powers \"k\" until the desired \"a\" is found. This algorithm is sometimes called \"trial multiplication\". It requires running time linear in the size of the group \"G\" and thus exponential in the number of digits in the size of the group. Therefore, it is an exponential-time algorithm, practical only for small groups \"G\".\n\nMore sophisticated algorithms exist, usually inspired by similar algorithms for integer factorization. These algorithms run faster than the naïve algorithm, some of them linear in the square root of the size of the group, and thus exponential in half the number of digits in the size of the group. However none of them run in polynomial time (in the number of digits in the size of the group).\n\n\nThere is an efficient quantum algorithm due to Peter Shor.\n\nEfficient classical algorithms also exist in certain special cases. For example, in the group of the integers modulo \"p\" under addition, the power \"b\" becomes a product \"bk\", and equality means congruence modulo \"p\" in the integers. The extended Euclidean algorithm finds \"k\" quickly.\n\nWhile computing discrete logarithms and factoring integers are distinct problems, they share some properties:\n\nThere exist groups for which computing discrete logarithms is apparently difficult. In some cases (e.g. large prime order subgroups of groups (Z)) there is not only no efficient algorithm known for the worst case, but the average-case complexity can be shown to be about as hard as the worst case using random self-reducibility.\n\nAt the same time, the inverse problem of discrete exponentiation is not difficult (it can be computed efficiently using exponentiation by squaring, for example). This asymmetry is analogous to the one between integer factorization and integer multiplication. Both asymmetries (and other possibly one-way functions) have been exploited in the construction of cryptographic systems.\n\nPopular choices for the group \"G\" in discrete logarithm cryptography are the cyclic groups (Z) (e.g. ElGamal encryption, Diffie–Hellman key exchange, and the Digital Signature Algorithm) and cyclic subgroups of elliptic curves over finite fields (\"see\" elliptic curve cryptography).\n\nWhile there is no publicly known algorithm for solving the discrete logarithm problem in general, the first three steps of the number field sieve algorithm only depend on the group \"G\", not on the specific elements of \"G\" whose finite log is desired. By precomputing these three steps for a specific group, one need only carry out the last step, which is much less computationally expensive than the first three, to obtain a specific logarithm in that group.\n\nIt turns out that much Internet traffic uses one of a handful of groups that are of order 1024 bits or less, e.g. cyclic groups with order of the Oakley primes specified in RFC 2409. The Logjam attack used this vulnerability to compromise a variety of Internet services that allowed the use of groups whose order was a 512-bit prime number, so called export grade.\n\nThe authors of the Logjam attack estimate that the much more difficult precomputation needed to solve the discrete log problem for a 1024-bit prime would be within the budget of a large national intelligence agency such as the U.S. National Security Agency (NSA). The Logjam authors speculate that precomputation against widely reused 1024 DH primes is behind claims in leaked NSA documents that NSA is able to break much of current cryptography.\n\n"}
{"id": "39721486", "url": "https://en.wikipedia.org/wiki?curid=39721486", "title": "Dynamical decoupling", "text": "Dynamical decoupling\n\nDynamical decoupling (DD) is an open-loop quantum control technique employed in quantum computing to suppress decoherence by taking advantage of rapid, time-dependent control modulation. In its simplest form, DD is implemented by periodic sequences of instantaneous control pulses, whose net effect is to approximately average the unwanted system-environment coupling to zero. \nDifferent schemes exist for designing DD protocols that use realistic bounded-strength control pulses, as well as for achieving high-order error suppression, and for making DD compatible with quantum gates. In spin systems in particular, commonly used protocols for dynamical decoupling include the Carr-Purcell and the Carr-Purcell-Meiboom-Gill schemes. They are based on the Hahn spin echo technique of applying periodic pulses to enable refocusing and hence extend the coherence times of qubits.\n\nPeriodic repetition of suitable high-order DD sequences may be employed to engineer a ‘stroboscopic saturation’ of qubit coherence, or coherence plateau, that can persist in the presence of realistic noise spectra and experimental control imperfections. This permits device-independent, high-fidelity data storage for computationally useful periods with bounded error probability.\n\nDynamical decoupling has also been studied in a classical context for two coupled pendulums whose oscillation frequencies are modulated in time.\n"}
{"id": "15767827", "url": "https://en.wikipedia.org/wiki?curid=15767827", "title": "Ergebnisse der Mathematik und ihrer Grenzgebiete", "text": "Ergebnisse der Mathematik und ihrer Grenzgebiete\n\nErgebnisse der Mathematik und ihrer Grenzgebiete/A Series of Modern Surveys in Mathematics is a series of scholarly monographs published by Springer Science+Business Media. The title literally means \"Results in mathematics and related areas\". Most of the books were published in German or English, but there were a few in French and Italian. There have been several sequences, or \"Folge\": the original series, neue Folge, and 3.Folge. Some of the most significant mathematical monographs of 20th century appeared in this series.\n\nThe series started in 1932 with publication of \"Knotentheorie\" by Kurt Reidemeister as \"Band 1\" (English: volume 1). There seems to have been double numeration in this sequence.\n\nThis sequence started in 1950 with the publication of \"Transfinite Zahlen\" by Heinz Bachmann. The volumes are consecutively numbered, designated as either \"Band\" or \"Heft\". A total of 100 volumes was published, often in multiple editions, but preserving the original numbering within the series.\n\nThe ISSN for this sequence is 0071-1136. As of February 2008, P. R. Halmos, P. J. Hilton, R. Remmert, and B. Szökefalvi-Nagy are listed on the series' website as the editors of the defunct 2. Folge.\n\nThis sequence started in 1983 with the publication of \"Galois module structure of algebraic integers\" by Albrecht Fröhlich.\n\nAs of February 2008, the editor-in-chief is R. Remmert.\n"}
{"id": "761055", "url": "https://en.wikipedia.org/wiki?curid=761055", "title": "Euler's Disk", "text": "Euler's Disk\n\nEuler's Disk, invented in 1990 by Joseph Bendik, is a trademark for a scientific educational toy, used to illustrate and study the dynamic system of a spinning disk on a flat or curved surface (such as a spinning coin), and has been the subject of a number of scientific papers. \n\nThe apparatus is known for a dramatic visualization of energy exchanges in three different processes tightly coupled. As the disk gradually decreases the azimuthal rotation, there is also a decrease in amplitude and increase in the frequency of the disk's axial precession senoidal curve. \n\nThe evolution of the axial precession senoidal curve is easily visualized in a slow motion video by looking at the side of the disk following a single point marked on the disc. The evolution of the rotation of the disk is easily visualized in slow motion by looking at the top of the disk following an arrow drawn on the disc representing its radius. \n\nAs the disk releases the initial energy given by the user and approaches a stopped condition, the disk seems to \"defy gravity\" through these dynamic exchanges of energy. Joseph Bendik named the toy after Leonhard Euler, who studied similar physics in the 18th century.\n\nThe commercially available toy consists of a heavy, thick chrome-plated steel disk and a rigid, slightly concave, mirrored base. Included holographic magnetic stickers can be attached to the disk, to enhance the visual effect of wobbling. These attachments are strictly decorative and may reduce the capacity to see and understand what processes are truly at work. \n\nThe disk, when spun on a flat surface, exhibits a spinning/rolling motion, slowly progressing through different rates and types of motion before coming to rest—most notably, the precession rate of the disk's axis of symmetry accelerates as the disk spins down. The rigid mirror is used to provide a suitable low-friction surface, with a slight concavity which keeps the spinning disk from \"wandering\" off a support surface. \n\nAn ordinary coin spun on a table, as with any disk spun on a relatively flat surface, exhibits essentially the same type of motion, but is normally more limited in the length of time before stopping. The commercially available Euler’s Disk toy provides a more effective demonstration of the phenomenon than more commonly found items, having an optimized aspect ratio and a precision polished, slightly rounded edge to maximize the spinning/rolling time.\n\nA spinning/rolling disk ultimately comes to rest, and it does so quite abruptly, the final stage of motion being accompanied by a whirring sound of rapidly increasing frequency. As the disk rolls, the point of rolling contact describes a circle that oscillates with a constant angular velocity formula_1. If the motion is non-dissipative (frictionless), formula_1 is constant, and the motion persists forever; this is contrary to observation, since formula_1 is not constant in real life situations. In fact, the precession rate of the axis of symmetry approaches a finite-time singularity modeled by a power law with exponent approximately −1/3 (depending on specific conditions).\n\nThere are two conspicuous dissipative effects: rolling friction when the coin slips along the surface, and air drag from the resistance of air. Experiments show that rolling friction is mainly responsible for the dissipation and behavior—experiments in a vacuum show that the absence of air affects behavior only slightly, while the behavior (precession rate) depends systematically on coefficient of friction. In the limit of small angle (i.e. immediately before the disk stops spinning), air drag (specifically, viscous dissipation) is the dominant factor, but prior to this end stage, rolling friction is the dominant effect.\n\nIn the early 2000s, research was sparked by an article in the April 20, 2000 edition of \"Nature\", where Keith Moffatt showed that viscous dissipation in the thin layer of air between the disk and the table would be sufficient to account for the observed abruptness of the settling process. He also showed that the motion concluded in a finite-time singularity. His first theoretical hypothesis was contradicted by subsequent research, which showed that rolling friction is actually the dominant factor.\n\nMoffatt showed that, as time formula_4 approaches a particular time formula_5 (which is mathematically a constant of integration), the viscous dissipation approaches infinity. The singularity that this implies is not realized in practice, because the magnitude of the vertical acceleration cannot exceed the acceleration due to gravity (the disk loses contact with its support surface). Moffatt goes on to show that the theory breaks down at a time formula_6 before the final settling time formula_5, given by:\n\nwhere formula_9 is the radius of the disk, formula_10 is the acceleration due to Earth's gravity, formula_11 the dynamic viscosity of air, and formula_12 the mass of the disk. For the commercially available Euler’s Disk toy (see link in \"External links\" below), formula_6 is about formula_14 seconds, at which time the angle between the coin and the surface, formula_15, is approximately 0.005 radians and the rolling angular velocity, formula_16, is about 500 Hz.\n\nUsing the above notation, the total spinning/rolling time is:\n\nwhere formula_18 is the initial inclination of the disk, measured in radians. Moffatt also showed that, if formula_19, the finite-time singularity in formula_16 is given by\n\nMoffatt's theoretical work inspired several other workers to experimentally investigate the dissipative mechanism of a spinning/rolling disk, with results that partially contradicted his explanation. These experiments used spinning objects and surfaces of various geometries (disks and rings), with varying coefficients of friction, both in air and in a vacuum, and used instrumentation such as high speed photography to quantify the phenomenon.\n\nIn the 30 November 2000 issue of \"Nature\", physicists Van den Engh, Nelson and Roach discuss experiments in which disks were spun in a vacuum. Van den Engh used a rijksdaalder, a Dutch coin, whose magnetic properties allowed it to be spun at a precisely determined rate. They found that slippage between the disk and the surface could account for observations, and the presence or absence of air only slightly affected the disk's behavior. They pointed out that Moffatt's theoretical analysis would predict a very long spin time for a disk in a vacuum, which was not observed.\n\nMoffatt responded with a generalized theory that should allow experimental determination of which dissipation mechanism is dominant, and pointed out that the dominant dissipation mechanism would always be viscous dissipation in the limit of small formula_15 (i.e., just before the disk settles). \n\nLater work at the University of Guelph by Petrie, Hunt and Gray showed that carrying out the experiments in a vacuum (pressure 0.1 pascal) did not significantly affect the energy dissipation rate. Petrie \"et al.\" also showed that the rates were largely unaffected by replacing the disk with a ring shape, and that the no-slip condition was satisfied for angles greater than 10°.\n\nOn several occasions during the 2007–2008 Writers Guild of America strike, talk show host Conan O'Brien would spin his wedding ring on his desk, trying to spin the ring for as long as possible. The quest to achieve longer and longer spin times led him to invite MIT professor Peter Fisher onto the show to experiment with the problem. Spinning the ring in a vacuum had no identifiable effect, while a Teflon spinning support surface gave a record time of 51 seconds, corroborating the claim that rolling friction is the primary mechanism for kinetic energy dissipation.\nVarious kinds of rolling friction as primary mechanism for energy dissipation have been studied by Leine who confirmed experimentally that the frictional resistance of the movement of the contact point over the rim of the disk is most likely the primary dissipation mechanism on a time-scale of seconds.\n\nThe toy's sound and motion has been captured on both radio and film, for example, in the movies, \"Snow Flake\" (flashing toy) and \"Pearl Harbor\" (sound for the torpedoes, also played during the Academy Awards show). Euler's Disk was featured on the TV show \"The Big Bang Theory\", season 10, episode 16, which aired February 16, 2017 (MSN.com Euler's Disk vs. Raj).\n\n\n"}
{"id": "12036119", "url": "https://en.wikipedia.org/wiki?curid=12036119", "title": "Gromov's inequality for complex projective space", "text": "Gromov's inequality for complex projective space\n\nIn Riemannian geometry, Gromov's optimal stable 2-systolic inequality is the inequality\n\nvalid for an arbitrary Riemannian metric on the complex projective space, where the optimal bound is attained\nby the symmetric Fubini–Study metric, providing a natural geometrisation of quantum mechanics. Here formula_2 is the stable 2-systole, which in this case can be defined as the infimum of the areas of rational 2-cycles representing the class of the complex projective line formula_3 in 2-dimensional homology.\n\nThe inequality first appeared in as Theorem 4.36.\n\nThe proof of Gromov's inequality relies on the Wirtinger inequality for exterior 2-forms.\n\nIn the special case n=2, Gromov's inequality becomes formula_5. This inequality can be thought of as an analog of Pu's inequality for the real projective plane formula_6. In both cases, the boundary case of equality is attained by the symmetric metric of the projective plane. Meanwhile, in the quaternionic case, the symmetric metric on formula_7 is not its systolically optimal metric. In other words, the manifold formula_7 admits Riemannian metrics with higher systolic ratio formula_9 than for its symmetric metric .\n\n\n"}
{"id": "27396845", "url": "https://en.wikipedia.org/wiki?curid=27396845", "title": "Heine–Stieltjes polynomials", "text": "Heine–Stieltjes polynomials\n\nIn mathematics, the Heine–Stieltjes polynomials or Stieltjes polynomials, introduced by , are polynomial solutions of a second-order Fuchsian equation, a differential equation all of whose singularities are regular. The Fuchsian equation has the form\n\nfor some polynomial \"V\"(\"z\") of degree at most \"N\" − 2, and if this has a polynomial solution \"S\" then \"V\" is called a Van Vleck polynomial (after Edward Burr Van Vleck) and \"S\" is called a Heine–Stieltjes polynomial.\n\nHeun polynomials are the special cases of Stieltjes polynomials when the differential equation has four singular points.\n\n"}
{"id": "579311", "url": "https://en.wikipedia.org/wiki?curid=579311", "title": "Image (mathematics)", "text": "Image (mathematics)\n\nIn mathematics, an image is the subset of a function's codomain which is the output of the function from a subset of its domain.\n\nEvaluating a function at each element of a subset \"X\" of the domain, produces a set called the \"image of X under or through\" the function. The inverse image or preimage of a particular subset \"S\" of the codomain of a function is the set of all elements of the domain that map to the members of \"S\".\n\nImage and inverse image may also be defined for general binary relations, not just functions.\n\nThe word \"image\" is used in three related ways. In these definitions, \"f\" : \"X\" → \"Y\" is a function from the set \"X\" to the set \"Y\".\n\nIf \"x\" is a member of \"X\", then \"f\"(\"x\") = \"y\" (the value of \"f\" when applied to \"x\") is the image of \"x\" under \"f\". \"y\" is alternatively known as the output of \"f\" for argument \"x\".\n\nThe image of a subset \"A\" ⊆ \"X\" under \"f\" is the subset \"f\"<nowiki>[</nowiki>\"A\"<nowiki>]</nowiki> ⊆ \"Y\" defined by (in set-builder notation):\n\nWhen there is no risk of confusion, \"f\"<nowiki>[</nowiki>\"A\"<nowiki>]</nowiki> is simply written as \"f\"(\"A\"). This convention is a common one; the intended meaning must be inferred from the context. This makes \"f\"[.] a function whose domain is the power set of \"X\" (the set of all subsets of \"X\"), and whose codomain is the power set of \"Y\". See Notation below.\n\nThe image \"f\"<nowiki>[</nowiki>\"X\"<nowiki>]</nowiki> of the entire domain \"X\" of \"f\" is called simply the image of \"f\".\n\nIf \"R\" is an arbitrary binary relation on \"X\"×\"Y\", the set { y∈\"Y\" | \"xRy\" for some \"x\"∈\"X\" } is called the image, or the range, of \"R\". Dually, the set { \"x\"∈\"X\" | \"xRy\" for some y∈\"Y\" } is called the domain of \"R\".\n\nLet \"f\" be a function from \"X\" to \"Y\". The preimage or inverse image of a set \"B\" ⊆ \"Y\" under \"f\" is the subset of \"X\" defined by\n\nThe inverse image of a singleton, denoted by \"f\"<nowiki>[</nowiki>{\"y\"}<nowiki>]</nowiki> or by \"f\"<nowiki>[</nowiki>\"y\"<nowiki>]</nowiki>, is also called the fiber over \"y\" or the level set of \"y\". The set of all the fibers over the elements of \"Y\" is a family of sets indexed by \"Y\".\n\nFor example, for the function \"f\"(\"x\") = \"x\", the inverse image of {4} would be {−2, 2}. Again, if there is no risk of confusion, we may denote \"f\"<nowiki>[</nowiki>\"B\"<nowiki>]</nowiki> by \"f\"(\"B\"), and think of \"f\" as a function from the power set of \"Y\" to the power set of \"X\". The notation \"f\" should not be confused with that for inverse function. The notation coincides with the usual one, though, for bijections, in the sense that the inverse image of \"B\" under \"f\" is the image of \"B\" under \"f\".\n\nThe traditional notations used in the previous section can be confusing. An alternative is to give explicit names for the image and preimage as functions between powersets:\n\n\n\n\n\nGiven a function \"f\" : \"X\" → \"Y\", for all subsets \"A\", \"A\", and \"A\" of \"X\" and all subsets \"B\", \"B\", and \"B\" of \"Y\" we have:\n\nThe results relating images and preimages to the (Boolean) algebra of intersection and union work for any collection of subsets, not just for pairs of subsets:\n(Here, \"S\" can be infinite, even uncountably infinite.)\n\nWith respect to the algebra of subsets, by the above we see that the inverse image function is a lattice homomorphism while the image function is only a semilattice homomorphism (it does not always preserve intersections).\n\n\n"}
{"id": "253568", "url": "https://en.wikipedia.org/wiki?curid=253568", "title": "Implied volatility", "text": "Implied volatility\n\nIn financial mathematics, the implied volatility of an option contract is that value of the volatility of the underlying instrument which, when input in an option pricing model (such as Black–Scholes) will return a theoretical value equal to the current market price of the option. A non-option financial instrument that has embedded optionality, such as an interest rate cap, can also have an implied volatility. Implied volatility, a forward-looking and subjective measure, differs from historical volatility because the latter is calculated from known past returns of a security. To understand where Implied Volatility stands in terms of the underlying, implied volatility rank is used to understand its implied volatility from a one year high and low IV. \n\nAn option pricing model, such as Black–Scholes, uses a variety of inputs to derive a theoretical value for an option. Inputs to pricing models vary depending on the type of option being priced and the pricing model used. However, in general, the value of an option depends on an estimate of the future realized price volatility, σ, of the underlying. Or, mathematically:\n\nwhere \"C\" is the theoretical value of an option, and \"f\" is a pricing model that depends on σ, along with other inputs.\n\nThe function \"f\" is monotonically increasing in σ, meaning that a higher value for volatility results in a higher theoretical value of the option. Conversely, by the inverse function theorem, there can be at most one value for σ that, when applied as an input to formula_2, will result in a particular value for \"C\".\n\nPut in other terms, assume that there is some inverse function \"g\" = \"f\", such that\n\nwhere formula_4 is the market price for an option. The value formula_5 is the volatility implied by the market price formula_4, or the implied volatility.\n\nIn general, it is not possible to give a closed form formula for implied volatility in terms of call price. However, in some cases (large strike, low strike, short expiry, large expiry) it is possible to give an asymptotic expansion of implied volatility in terms of call price.\n\nA European call option, formula_7, on one share of non-dividend-paying XYZ Corp. The option is struck at $50 and expires in 32 days. The risk-free interest rate is 5%. XYZ stock is currently trading at $51.25 and the current market price of formula_7 is $2.00. Using a standard Black–Scholes pricing model, the volatility implied by the market price formula_7 is 18.7%, or:\n\nTo verify, we apply the implied volatility back into the pricing model, \"f\" and we generate a theoretical value of $2.0004:\n\nwhich confirms our computation of the market implied volatility.\n\nIn general, a pricing model function, \"f\", does not have a closed-form solution for its inverse, \"g\". Instead, a root finding technique is used to solve the equation:\n\nWhile there are many techniques for finding roots, two of the most commonly used are Newton's method and Brent's method. Because options prices can move very quickly, it is often important to use the most efficient method when calculating implied volatilities.\n\nNewton's method provides rapid convergence; however, it requires the first partial derivative of the option's theoretical value with respect to volatility; i.e., formula_13, which is also known as \"vega\" (see The Greeks). If the pricing model function yields a closed-form solution for \"vega\", which is the case for Black–Scholes model, then Newton's method can be more efficient. However, for most practical pricing models, such as a binomial model, this is not the case and \"vega\" must be derived numerically. When forced to solve for \"vega\" numerically, one can use the Christopher and Salkin method or, for more accurate calculation of out-of-the-money implied volatilities, one can use the Corrado-Miller model.\n\nWith the arrival of Big Data and Data Science parametrising the implied volatility has taken central importance for the sake of coherent interpolation and extrapolation purposes. The classic models are the SABR and SVI model with their IVP extension.\n\nAs stated by Brian Byrne, the implied volatility of an option is a more useful measure of the option's relative value than its price. The reason is that the price of an option depends most directly on the price of its underlying asset. If an option is held as part of a delta neutral portfolio (that is, a portfolio that is hedged against small moves in the underlying's price), then the next most important factor in determining the value of the option will be its implied volatility.\nImplied volatility is so important that options are often quoted in terms of volatility rather than price, particularly between professional traders.\n\nA call option is trading at $1.50 with the underlying trading at $42.05. The implied volatility of the option is determined to be 18.0%. A short time later, the option is trading at $2.10 with the underlying at $43.34, yielding an implied volatility of 17.2%. Even though the option's price is higher at the second measurement, it is still considered cheaper based on volatility. \nThe reason is that the underlying needed to hedge the call option can be sold for a higher price.\n\nAnother way to look at implied volatility is to think of it as a price, not as a measure of future stock moves.\nIn this view, it simply is a more convenient way to communicate option prices than currency. Prices are different in nature from statistical quantities: one can estimate volatility of future underlying returns using any of a large number of estimation methods; however, the number one gets is not a price. A price requires two counterparties, a buyer, and a seller. Prices are determined by supply and demand. Statistical estimates depend on the time-series and the mathematical structure of the model used. \nIt is a mistake to confuse a price, which implies a transaction, with the result of a statistical estimation, which is merely what comes out of a calculation. Implied volatilities are prices: they have been derived from actual transactions. Seen in this light, it should not be surprising that implied volatilities might not conform to what a particular statistical model would predict.\n\nHowever, the above view ignores the fact that the values of implied volatilities depend on the model used to calculate them: different models applied to the same market option prices will produce different implied volatilities. Thus, if one adopts this view of implied volatility as a price, then one also has to concede that there is no unique implied-volatility-price and that a buyer and a seller in the same transaction might be trading at different \"prices\".\n\nIn general, options based on the same underlying but with different strike values and expiration times will yield different implied volatilities. This is generally viewed as evidence that an underlying's volatility is not constant but instead depends on factors such as the price level of the underlying, the underlying's recent price variance, and the passage of time. There exist few known parametrisation of the volatility surface (Schonbusher, SVI, and gSVI) as well as their de-arbitraging methodologies. See stochastic volatility and volatility smile for more information.\n\nVolatility instruments are financial instruments that track the value of implied volatility of other derivative securities. For instance, the CBOE Volatility Index (VIX) is calculated from a weighted average of implied volatilities of various options on the S&P 500 Index. There are also other commonly referenced volatility indices such as the VXN index (Nasdaq 100 index futures volatility measure), the QQV (QQQ volatility measure), IVX - Implied Volatility Index (an expected stock volatility over a future period for any of US securities and exchange-traded instruments), as well as options and futures derivatives based directly on these volatility indices themselves.\n\n\n\n"}
{"id": "13265337", "url": "https://en.wikipedia.org/wiki?curid=13265337", "title": "Kuratowski convergence", "text": "Kuratowski convergence\n\nIn mathematics, Kuratowski convergence is a notion of convergence for sequences (or, more generally, nets) of compact subsets of metric spaces, named after Kazimierz Kuratowski. Intuitively, the Kuratowski limit of a sequence of sets is where the sets \"accumulate\".\n\nLet (\"X\", \"d\") be a metric space, where \"X\" is a set and \"d\" is the function of distance between points of \"X\".\n\nFor any point \"x\" ∈ \"X\" and any non-empty compact subset \"A\" ⊆ \"X\", define the distance between the point and the subset:\n\nFor any sequence of such subsets \"A\" ⊆ \"X\", \"n\" ∈ N, the Kuratowski limit inferior (or lower closed limit) of \"A\" as \"n\" → ∞ is\n\nthe Kuratowski limit superior (or upper closed limit) of \"A\" as \"n\" → ∞ is\n\nIf the Kuratowski limits inferior and superior agree (i.e. are the same subset of \"X\"), then their common value is called the Kuratowski limit of the sets \"A\" as \"n\" → ∞ and denoted Lt\"A\".\n\nThe definitions for a general net of compact subsets of \"X\" go through \"mutatis mutandis\".\n\n\n\nFor metric spaces \"X\" we have the following:\n\n\n"}
{"id": "13305267", "url": "https://en.wikipedia.org/wiki?curid=13305267", "title": "LU reduction", "text": "LU reduction\n\nLU reduction is an algorithm related to LU decomposition. This term is usually used in the context of super computing and highly parallel computing. In this context it is used as a benchmarking algorithm, i.e. to provide a comparative measurement of speed for different computers. LU reduction is a special parallelized version of an LU decomposition algorithm, an example can be found in (Guitart 2001). The parallelized version usually distributes the work for a matrix row to a single processor and synchronizes the result with the whole matrix (Escribano 2000).\n\n"}
{"id": "20502609", "url": "https://en.wikipedia.org/wiki?curid=20502609", "title": "Local trace formula", "text": "Local trace formula\n\nIn mathematics, the local trace formula is a local analogue of the Arthur–Selberg trace formula that describes the character of the representation of \"G\"(F) on the discrete part of \"L\"(\"G\"(F)), for \"G\" a reductive algebraic group over a local field F.\n"}
{"id": "14188480", "url": "https://en.wikipedia.org/wiki?curid=14188480", "title": "Malgrange preparation theorem", "text": "Malgrange preparation theorem\n\nIn mathematics, the Malgrange preparation theorem is an analogue of the Weierstrass preparation theorem for smooth functions. It was conjectured by René Thom and proved by .\n\nSuppose that \"f\"(\"t\",\"x\") is a smooth complex function of \"t\"∈R and \"x\"∈R near the origin, and let \"k\" be the smallest integer such that\nThen one form of the preparation theorem states that near the origin \"f\" can be written as the product of a smooth function \"c\" that is nonzero at the origin and a smooth function that as a function of \"t\" is a polynomial of degree \"k\". In other words,\nwhere the functions \"c\" and \"a\" are smooth and \"c\" is nonzero at the origin.\n\nA second form of the theorem, occasionally called the Mather division theorem, is a sort of \"division with remainder\" theorem: it says that if \"f\" and \"k\" satisfy the conditions above and \"g\" is a smooth function near the origin, then we can write\nwhere \"q\" and \"r\" are smooth, and as a function of \"t\", \"r\" is a polynomial of degree less than \"k\". This means that\nfor some smooth functions \"r\"(\"x\").\n\nThe two forms of the theorem easily imply each other: the first form is the special case of the \"division with remainder\" form where \"g\" is \"t\", and the division with remainder form follows from the first form of the theorem as we may assume that \"f\" as a function of \"t\" is a polynomial of degree \"k\".\n\nIf the functions \"f\" and \"g\" are real, then the functions \"c\", \"a\", \"q\", and \"r\" can also be taken to be real. In the case of the Weierstrass preparation theorem these functions are uniquely determined by \"f\" and \"g\", but uniqueness no longer holds for the Malgrange preparation theorem.\n\nThe Malgrange preparation theorem can be deduced from the Weierstrass preparation theorem. The obvious way of doing this does not work: although smooth functions have a formal power series expansion at the origin, and the Weierstrass preparation theorem applies to formal power series, the formal power series will not usually converge to smooth functions near the origin. Instead one can use the idea of decomposing a smooth function as a sum of analytic functions by applying a partition of unity to its Fourier transform.\nFor a proof along these lines see or \n\nThe Malgrange preparation theorem can be restated as a theorem about modules over rings of smooth, real-valued germs. If \"X\" is a manifold, with \"p\"∈\"X\", let \"C\"(\"X\") denote the ring of real-valued germs of smooth functions at \"p\" on \"X\". Let \"M\"(\"X\") denote the unique maximal ideal of \"C\"(\"X\"), consisting of germs which vanish at p. Let \"A\" be a \"C\"(\"X\")-module, and let \"f\":\"X\" → \"Y\" be a smooth function between manifolds. Let \"q\" = \"f\"(\"p\"). \"f\" induces a ring homomorphism \"f\":\"C\"(Y) → \"C\"(\"X\") by composition on the right with \"f\". Thus we can view \"A\" as a \"C\"(\"Y\")-module. Then the Malgrange preparation theorem says that if \"A\" is a finitely-generated \"C\"(\"X\")-module, then \"A\" is a finitely-generated \"C\"(\"Y\")-module if and only if \"A\"/\"M\"(\"Y\")A is a finite-dimensional real vector space.\n\n"}
{"id": "30962576", "url": "https://en.wikipedia.org/wiki?curid=30962576", "title": "Mason–Stothers theorem", "text": "Mason–Stothers theorem\n\nThe Mason–Stothers theorem, or simply Mason's theorem, is a mathematical theorem about polynomials, analogous to the \"abc\" conjecture for integers. It is named after , who published it in 1981, and R. C. Mason, who rediscovered it shortly thereafter.\n\nThe theorem states:\nHere is the product of the distinct irreducible factors of . For algebraically closed fields it is the polynomial of minimum degree that has the same roots as ; in this case gives the number of distinct roots of .\n\n\n gave the following elementary proof of the Mason–Stothers theorem.\n\nStep 1. The condition implies that the Wronskians , , and are all equal. Write for their common value.\n\nStep 2. The condition that at least one of the derivatives , , or is nonzero and that , , and are coprime is used to show that is nonzero.\nFor example, if then so divides (as and are coprime) so (as unless is constant).\n\nStep 3. is divisible by each of the greatest common divisors , , and . Since these are coprime it is divisible by their product, and since is nonzero we get\n\nStep 4. Substituting in the inequalities\n(where the roots are taken in some algebraic closure) and\nwe find that\nwhich is what we needed to prove.\n\nThere is a natural generalization in which the ring of polynomials is replaced by a one-dimensional function field.\nLet be an algebraically closed field of characteristic 0, let be a smooth projective curve\nof genus , let\nand let\nThen\nHere the degree of a function in is the degree of\nthe map it induces from to P.\nThis was proved by Mason, with an alternative short proof published the same year by J. H. Silverman\n\nThere is a further generalization, due independently to J. F. Voloch\nand to\nW. D. Brownawell and D. W. Masser,\nthat gives an upper bound for -variable -unit\nequations provided that\nno subset of the are -linearly dependent. Under this assumption, they prove that\n\n"}
{"id": "2757224", "url": "https://en.wikipedia.org/wiki?curid=2757224", "title": "Multiple integral", "text": "Multiple integral\n\nThe multiple integral is a definite integral of a function of more than one real variable, for example, or . Integrals of a function of two variables over a region in are called double integrals, and integrals of a function of three variables over a region of are called triple integrals.\n\nJust as the definite integral of a positive function of one variable represents the area of the region between the graph of the function and the -axis, the double integral of a positive function of two variables represents the volume of the region between the surface defined by the function (on the three-dimensional Cartesian plane where ) and the plane which contains its domain. If there are more variables, a multiple integral will yield hypervolumes of multidimensional functions.\n\nMultiple integration of a function in variables: over a domain is most commonly represented by nested integral signs in the reverse order of execution (the leftmost integral sign is computed last), followed by the function and integrand arguments in proper order (the integral with respect to the rightmost argument is computed last). The domain of integration is either represented symbolically for every argument over each integral sign, or is abbreviated by a variable at the rightmost integral sign:\n\nSince the concept of an antiderivative is only defined for functions of a single real variable, the usual definition of the indefinite integral does not immediately extend to the multiple integral.\n\nFor , consider a so-called \"half-open\" -dimensional hyperrectangular domain , defined as:\n\nPartition each interval into a finite family of non-overlapping subintervals , with each subinterval closed at the left end, and open at the right end.\n\nThen the finite family of subrectangles given by\n\nis a partition of ; that is, the subrectangles are non-overlapping and their union is .\n\nLet be a function defined on . Consider a partition of as defined above, such that is a family of subrectangles and\n\nWe can approximate the total th-dimensional volume bounded below by the -dimensional hyperrectangle and above by the -dimensional graph of with the following Riemann sum:\n\nwhere is a point in and is the product of the lengths of the intervals whose Cartesian product is , also known as the measure of .\n\nThe diameter of a subrectangle is the largest of the lengths of the intervals whose Cartesian product is . The diameter of a given partition of is defined as the largest of the diameters of the subrectangles in the partition. Intuitively, as the diameter of the partition is restricted smaller and smaller, the number of subrectangles gets larger, and the measure of each subrectangle grows smaller. The function is said to be Riemann integrable if the limit\n\nexists, where the limit is taken over all possible partitions of of diameter at most .\n\nIf is Riemann integrable, is called the Riemann integral of over and is denoted\n\nFrequently this notation is abbreviated as\n\nwhere represents the -tuple and is the -dimensional volume differential.\n\nThe Riemann integral of a function defined over an arbitrary bounded -dimensional set can be defined by extending that function to a function defined over a half-open rectangle whose values are zero outside the domain of the original function. Then the integral of the original function over the original domain is defined to be the integral of the extended function over its rectangular domain, if it exists.\n\nIn what follows the Riemann integral in dimensions will be called the multiple integral.\n\nMultiple integrals have many properties common to those of integrals of functions of one variable (linearity, commutativity, monotonicity, and so on). One important property of multiple integrals is that the value of an integral is independent of the order of integrands under certain conditions. This property is popularly known as Fubini's theorem.\n\nIn the case of , the integral\n\nis the double integral of on , and if the integral\n\nis the triple integral of on .\n\nNotice that, by convention, the double integral has two integral signs, and the triple integral has three; this is a notational convention which is convenient when computing a multiple integral as an iterated integral, as shown later in this article.\n\nThe resolution of problems with multiple integrals consists, in most cases, of finding a way to reduce the multiple integral to an iterated integral, a series of integrals of one variable, each being directly solvable. For continuous functions, this is justified by Fubini's theorem. Sometimes, it is possible to obtain the result of the integration by direct examination without any calculations.\n\nThe following are some simple methods of integration:\n\nWhen the integrand is a constant function , the integral is equal to the product of and the measure of the domain of integration. If and the domain is a subregion of , the integral gives the area of the region, while if the domain is a subregion of , the integral gives the volume of the region.\n\nExample. Let and\n\nin which case\n\nsince by definition we have:\n\nWhen the domain of integration is symmetric about the origin with respect to at least one of the variables of integration and the integrand is odd with respect to this variable, the integral is equal to zero, as the integrals over the two halves of the domain have the same absolute value but opposite signs. When the integrand is even with respect to this variable, the integral is equal to twice the integral over one half of the domain, as the integrals over the two halves of the domain are equal.\n\nExample 1. Consider the function integrated over the domain\n\na disc with radius 1 centered at the origin with the boundary included.\n\nUsing the linearity property, the integral can be decomposed into three pieces:\n\nThe function is an odd function in the variable and the disc is symmetric with respect to the -axis, so the value of the first integral is 0. Similarly, the function is an odd function of , and is symmetric with respect to the -axis, and so the only contribution to the final result is that of the third integral. Therefore the original integral is equal to the area of the disk times 5, or 5.\n\nExample 2. Consider the function and as integration region the sphere with radius 2 centered at the origin,\nThe \"ball\" is symmetric about all three axes, but it is sufficient to integrate with respect to -axis to show that the integral is 0, because the function is an odd function of that variable.\n\nThis method is applicable to any domain for which:\nSuch a domain will be here called a \"normal domain\". Elsewhere in the literature, normal domains are sometimes called type I or type II domains, depending on which axis the domain is fibred over. In all cases, the function to be integrated must be Riemann integrable on the domain, which is true (for instance) if the function is continuous.\n\nIf the domain is normal with respect to the -axis, and is a continuous function; then and (both of which are defined on the interval ) are the two functions that determine . Then, by Fubini's theorem:\n\nIf is normal with respect to the -axis and is a continuous function; then and (both of which are defined on the interval ) are the two functions that determine . Again, by Fubini's theorem:\n\nIf is a domain that is normal with respect to the -plane and determined by the functions and , then\n\nThis definition is the same for the other five normality cases on . It can be generalized in a straightforward way to domains in R.\n\nThe limits of integration are often not easily interchangeable (without normality or with complex formulae to integrate). One makes a change of variables to rewrite the integral in a more \"comfortable\" region, which can be described in simpler formulae. To do so, the function must be adapted to the new coordinates.\n\nExample 1a. The function is ; if one adopts the substitution , therefore , one obtains the new function .\n\nThere exist three main \"kinds\" of changes of variable (one in , two in ); however, more general substitutions can be made using the same principle.\n\nIn if the domain has a circular symmetry and the function has some particular characteristics one can apply the \"transformation to polar coordinates\" (see the example in the picture) which means that the generic points in Cartesian coordinates switch to their respective points in polar coordinates. That allows one to change the shape of the domain and simplify the operations.\n\nThe fundamental relation to make the transformation is the following:\n\nExample 2a. The function is and applying the transformation one obtains\n\nExample 2b. The function is , in this case one has:\nusing the Pythagorean trigonometric identity (very useful to simplify this operation).\n\nThe transformation of the domain is made by defining the radius' crown length and the amplitude of the described angle to define the intervals starting from .\nExample 2c. The domain is , that is a circumference of radius 2; it's evident that the covered angle is the circle angle, so varies from 0 to 2, while the crown radius varies from 0 to 2 (the crown with the inside radius null is just a circle).\n\nExample 2d. The domain is , that is the circular crown in the positive half-plane (please see the picture in the example); describes a plane angle while varies from 2 to 3. Therefore the transformed domain will be the following rectangle:\n\nThe Jacobian determinant of that transformation is the following:\n\nwhich has been obtained by inserting the partial derivatives of , in the first column respect to and in the second respect to , so the differentials in this transformation become .\n\nOnce the function is transformed and the domain evaluated, it is possible to define the formula for the change of variables in polar coordinates:\n\nExample 2e. The function is and the domain is the same as in Example 2d. From the previous analysis of we know the intervals of (from 2 to 3) and of (from 0 to ). Now we change the function:\n\nfinally let's apply the integration formula:\n\nOnce the intervals are known, you have\n\nIn the integration on domains with a circular base can be made by the \"passage to cylindrical coordinates\"; the transformation of the function is made by the following relation:\n\nThe domain transformation can be graphically attained, because only the shape of the base varies, while the height follows the shape of the starting region.\n\nExample 3a. The region is (that is the \"tube\" whose base is the circular crown of Example 2d and whose height is 5); if the transformation is applied, this region is obtained:\n(that is, the parallelepiped whose base is similar to the rectangle in Example 2d and whose height is 5).\n\nBecause the component is unvaried during the transformation, the differentials vary as in the passage to polar coordinates: therefore, they become .\n\nFinally, it is possible to apply the final formula to cylindrical coordinates:\n\nThis method is convenient in case of cylindrical or conical domains or in regions where it is easy to individuate the \"z\" interval and even transform the circular base and the function.\n\nExample 3b. The function is and as integration domain this cylinder: . The transformation of in cylindrical coordinates is the following:\n\nwhile the function becomes\n\nFinally one can apply the integration formula:\n\ndeveloping the formula you have\n\nIn some domains have a spherical symmetry, so it's possible to specify the coordinates of every point of the integration region by two angles and one distance. It's possible to use therefore the \"passage to spherical coordinates\"; the function is transformed by this relation:\n\nPoints on the -axis do not have a precise characterization in spherical coordinates, so can vary between 0 and 2.\n\nThe better integration domain for this passage is obviously the sphere.\n\nExample 4a. The domain is (sphere with radius 4 and center at the origin); applying the transformation you get the region\n\nThe Jacobian determinant of this transformation is the following:\n\nThe differentials therefore are transformed to .\n\nThis yields the final integration formula:\n\nIt is better to use this method in case of spherical domains and in case of functions that can be easily simplified by the first fundamental relation of trigonometry extended to (see Example 4b); in other cases it can be better to use cylindrical coordinates (see Example 4c).\n\nThe extra and come from the Jacobian.\n\nIn the following examples the roles of and have been reversed.\n\nExample 4b. is the same region as in Example 4a and is the function to integrate. Its transformation is very easy:\n\nwhile we know the intervals of the transformed region from :\n\nWe therefore apply the integration formula:\n\nand, developing, we get\n\nExample 4c. The domain is the ball with center at the origin and radius ,\n\nand is the function to integrate.\n\nLooking at the domain, it seems convenient to adopt the passage to spherical coordinates, in fact, the intervals of the variables that delimit the new region are obviously:\n\nHowever, applying the transformation, we get\n\nApplying the formula for integration we obtain:\n\nwhich is very hard to solve. This problem will be solved by using the passage to cylindrical coordinates. The new intervals are\n\nthe interval has been obtained by dividing the ball into two hemispheres simply by solving the inequality from the formula of (and then directly transforming into ). The new function is simply . Applying the integration formula\n\nThen we get\n\nThanks to the passage to cylindrical coordinates it was possible to reduce the triple integral to an easier one-variable integral.\n\nSee also the differential volume entry in nabla in cylindrical and spherical coordinates.\n\nLet us assume that we wish to integrate a multivariable function over a region :\n\nFrom this we formulate the iterated integral\n\nThe inner integral is performed first, integrating with respect to and taking as a constant, as it is not the variable of integration. The result of this integral, which is a function depending only on , is then integrated with respect to .\n\nWe then integrate the result with respect to .\n\nSometimes, the order of integration is interchangeable, that is, integrating with respect to \"x\" first and integrating with respect to \"y\" first produce the same result. For example, doing the previous calculation with order reversed gives the same result:\n\nThe instances where the order is interchangeable is determined by Fubini's theorem.\n\nConsider the region (please see the graphic in the example):\nCalculate\n\nThis domain is normal with respect to both the \"x\"- and \"y\"-axes. To apply the formulae it is required to find the functions that determine \"D\" and the intervals over which these functions are defined. In this case the two functions are:\n\nwhile the interval is given by the intersections of the functions with \"x\" = 0, so the interval is [\"a\", \"b\"] = [0, 1] (normality has been chosen with respect to the \"x\"-axis for a better visual understanding).\n\nIt is now possible to apply the formula:\n\n(at first the second integral is calculated considering \"x\" as a constant). The remaining operations consist of applying the basic techniques of integration:\n\nIf we choose normality with respect to the \"y\"-axis we could calculate\n\nand obtain the same value.\nUsing the methods previously described, it is possible to calculate the volumes of some common solids.\n\nThis is in agreement with the formula for the volume of a prism\n\n\n\nIn case of unbounded domains or functions not bounded near the boundary of the domain, we have to introduce the double improper integral or the triple improper integral.\n\nFubini's theorem states that if\nthat is, if the integral is absolutely convergent, then the multiple integral will give the same result as the iterated integral,\nIn particular this will occur if is a bounded function and and are bounded sets.\n\nIf the integral is not absolutely convergent, care is needed not to confuse the concepts of \"multiple integral\" and \"iterated integral\", especially since the same notation is often used for either concept. The notation\n\nmeans, in some cases, an iterated integral rather than a true double integral. In an iterated integral, the outer integral\n\nis the integral with respect to of the following function of :\n\nA double integral, on the other hand, is defined with respect to area in the -plane. If the double integral exists, then it is equal to each of the two iterated integrals (either \" or \") and one often computes it by computing either of the iterated integrals. But sometimes the two iterated integrals exist when the double integral does not, and in some such cases the two iterated integrals are different numbers, i.e., one has\n\nThis is an instance of rearrangement of a conditionally convergent integral.\n\nOn the other hand, some conditions ensure that the two iterated integrals are equal even though the double integral need not exist. By the Fichtenholz–Lichtenstein theorem, if is bounded on and both iterated integrals exist, then they are equal. Moreover, existence of the inner integrals ensures existence of the outer integrals. The double integral need not exist in this case even as Lebesgue integral, according to Sierpiński.\n\nThe notation\n\nmay be used if one wishes to be emphatic about intending a double integral rather than an iterated integral.\n\nQuite generally, just as in one variable, one can use the multiple integral to find the average of a function over a given set. Given a set and an integrable function over , the average value of over its domain is given by\n\nwhere is the measure of .\n\nAdditionally, multiple integrals are used in many applications in physics. The examples below also show some variations in the notation.\n\nIn mechanics, the moment of inertia is calculated as the volume integral (triple integral) of the density weighed with the square of the distance from the axis:\n\nThe gravitational potential associated with a mass distribution given by a mass measure on three-dimensional Euclidean space is\n\nIf there is a continuous function representing the density of the distribution at , so that , where is the Euclidean volume element, then the gravitational potential is\n\nIn electromagnetism, Maxwell's equations can be written using multiple integrals to calculate the total magnetic and electric fields. In the following example, the electric field produced by a distribution of charges given by the volume charge density is obtained by a \"triple integral\" of a vector function:\n\nThis can also be written as an integral with respect to a signed measure representing the charge distribution.\n\n\n\n"}
{"id": "663673", "url": "https://en.wikipedia.org/wiki?curid=663673", "title": "NE (complexity)", "text": "NE (complexity)\n\nIn computational complexity theory, the complexity class NE is the set of decision problems that can be solved by a non-deterministic Turing machine in time O(\"k\") for some \"k\".\n\nNE, unlike the similar class NEXPTIME, is not closed under polynomial-time many-one reductions.\n\n"}
{"id": "3668693", "url": "https://en.wikipedia.org/wiki?curid=3668693", "title": "Nilmanifold", "text": "Nilmanifold\n\nIn mathematics, a nilmanifold is a differentiable manifold which has a transitive nilpotent group of diffeomorphisms acting on it. As such, a nilmanifold is an example of a homogeneous space and is diffeomorphic to the quotient space formula_1, the quotient of a nilpotent Lie group \"N\" modulo a closed subgroup \"H\". This notion was introduced by A. Mal'cev in 1951.\n\nIn the Riemannian category, there is also a good notion of a nilmanifold. A Riemannian manifold is called a homogeneous nilmanifold if there exist a nilpotent group of isometries acting transitively on it. The requirement that the transitive nilpotent group acts by isometries leads to the following rigid characterization: every homogeneous nilmanifold is isometric to a nilpotent Lie group with left-invariant metric (see Wilson ).\n\nNilmanifolds are important geometric objects and often arise as concrete examples with interesting properties; in Riemannian geometry these spaces always have mixed curvature, almost flat spaces arise as quotients of nilmanifolds, and compact nilmanifolds have been used to construct elementary examples of collapse of Riemannian metrics under the Ricci flow.\n\nIn addition to their role in geometry, nilmanifolds are increasingly being seen as having a role in arithmetic combinatorics (see Green–Tao ) and ergodic theory (see, e.g., Host–Kra ).\n\nA compact nilmanifold is a nilmanifold which is compact. One way to construct such spaces is to start with a simply connected nilpotent Lie group \"N\" and a discrete subgroup formula_2. If the subgroup formula_2 acts cocompactly (via right multiplication) on \"N\", then the quotient manifold formula_4 will be a compact nilmanifold. As Mal'cev has shown, every compact\nnilmanifold is obtained this way.\n\nSuch a subgroup formula_2 as above is called a lattice in \"N\". It is well known that a nilpotent Lie group admits a lattice if and only if its Lie algebra admits a basis with rational structure constants: this is Malcev's criterion. Not all nilpotent Lie groups admit lattices; for more details, see also Raghunathan.\n\nA compact Riemannian nilmanifold is a compact Riemannian manifold which is locally isometric to a nilpotent Lie group with left-invariant metric. These spaces are constructed as follows. Let formula_2 be a lattice in a simply connected nilpotent Lie group \"N\", as above. Endow \"N\" with a left-invariant (Riemannian) metric. Then the subgroup formula_7 acts by isometries on \"N\" via left-multiplication. Thus the quotient formula_8 is a compact space locally isometric to \"N\". Note: this space is naturally diffeomorphic to formula_9.\n\nCompact nilmanifolds also arise as principal bundles. For example, consider a 2-step nilpotent Lie group \"N\" which admits a lattice (see above). Let formula_10 be the commutator subgroup of \"N\". Denote by p the dimension of \"Z\" and by q the codimension of \"Z\"; i.e. the dimension of \"N\" is p+q. It is known (see Raghunathan) that formula_11 is a lattice in \"Z\". Hence, formula_12 is a \"p\"-dimensional compact torus. Since \"Z\" is central in \"N\", the group G acts on the compact nilmanifold formula_13 with quotient space formula_14. This base manifold \"M\" is a \"q\"-dimensional compact torus. It has been shown that ever principal torus bundle over a torus is of this form, see. More generally, a compact nilmanifold is torus bundle, over a torus bundle, over...over a torus.\n\nAs mentioned above, almost flat manifolds are intimately compact nilmanifolds. See that article for more information.\n\nHistorically, a complex nilmanifold meant a quotient of a complex nilpotent Lie group over\na cocompact lattice. An example of such a nilmanifold is an Iwasawa manifold. From the 1980s, another (more general) notion of a complex nilmanifold gradually replaced this one.\n\nAn almost complex structure on a real Lie algebra \"g\" is an endomorphism formula_15 which squares to\n−Id. This operator is called a complex structure if its eigenspaces, corresponding to eigenvalues\nformula_16, are subalgebras in formula_17. In this case, \"I\" defines a left-invariant complex structure on the corresponding Lie group. Such a manifold (\"G\",\"I\") is called a complex group manifold.\nIt is easy to see that every connected complex homogeneous manifold equipped with a free, transitive, holomorphic action by a real Lie group is obtained this way.\n\nLet \"G\" be a real, nilpotent Lie group. A complex nilmanifold is a quotient of a complex group manifold (\"G\",\"I\"), equipped with a left-invariant complex structure, by a discrete, cocompact lattice, acting from the right.\n\nComplex nilmanifolds are usually not homogeneous, as complex varieties.\n\nIn complex dimension 2, the only complex nilmanifolds are a complex torus and a Kodaira surface.\n\nCompact nilmanifolds (except a torus) are never homotopy formal. This implies immediately that compact nilmanifolds (except a torus) cannot\nadmit a Kähler structure (see also ).\n\nTopologically, all nilmanifolds can be obtained\nas iterated torus bundles over a torus. This is easily seen from a filtration by ascending central series.\n\nFrom the above definition of homogeneous nilmanifolds, it is clear that any nilpotent Lie group with left-invariant metric is a homogeneous nilmanifold. The most familiar nilpotent Lie groups are matrix groups whose diagonal entries are 1 and whose lower diagonal entries are all zeros.\n\nFor example, the Heisenberg group is a 2-step nilpotent Lie group. This nilpotent Lie group is also special in that it admits a compact quotient. The group formula_7 would be the upper triangular matrices with integral coefficients. The resulting nilmanifold is 3-dimensional. One possible fundamental domain is (isomorphic to) [0,1] with the faces identified in a suitable way. This is because an element formula_19 of the nilmanifold can be represented by the element formula_20 in the fundamental domain. Here formula_21 denotes the floor function of \"x\", and formula_22 the fractional part. The appearance of the floor function here is a clue to the relevance of nilmanifolds to additive combinatorics: the so-called bracket polynomials, or generalised polynomials, seem to be important in the development of higher-order Fourier analysis.\n\nA simpler example would be any abelian Lie group. This is because any such group is a nilpotent Lie group. For example, one can take the group of real numbers under addition, and the discrete, cocompact subgroup consisting of the integers. The resulting 1-step nilmanifold is the familiar circle formula_23. Another familiar example might be the compact 2-torus or Euclidean space under addition.\n\nA parallel construction based on solvable Lie groups produces a class of spaces called solvmanifolds. An important example of a solvmanifolds are Inoue surfaces, known in complex geometry.\n"}
{"id": "9808147", "url": "https://en.wikipedia.org/wiki?curid=9808147", "title": "Niven's constant", "text": "Niven's constant\n\nIn number theory, Niven's constant, named after Ivan Niven, is the largest exponent appearing in the prime factorization of any natural number \"n\" \"on average\". More precisely, if we define \"H\"(1) = 1 and \"H\"(\"n\") = the largest exponent appearing in the unique prime factorization of a natural number \"n\" > 1, then Niven's constant is given by\n\nwhere ζ(\"k\") is the value of the Riemann zeta function at the point \"k\" (Niven, 1969).\n\nIn the same paper Niven also proved that\n\nwhere \"h\"(1) = 1, \"h\"(\"n\") = the smallest exponent appearing in the unique prime factorization of each natural number \"n\" > 1, \"o\" is little o notation, and the constant \"c\" is given by\n\nand consequently that\n\n\n"}
{"id": "39381650", "url": "https://en.wikipedia.org/wiki?curid=39381650", "title": "NodeXL", "text": "NodeXL\n\nNodeXL Basic is a free and open-source network analysis and visualization software package for Microsoft Excel 2007/2010/2013/2016. NodeXL Pro is a fee-based fully featured version of NodeXL that includes access to social media network data importers, advanced network metrics, and automation. It is a popular package similar to other network visualization tools such as Pajek, UCINet, and Gephi.\nNodeXL is a set of prebuilt class libraries using a custom Windows Presentation Foundation control. Additional .NET assemblies can be developed as \"plug-ins\" to import data from outside data providers. Currently-implemented data providers for NodeXL include Facebook, Twitter, Wikipedia (the MediaWiki understructure), web hyperlinks, Microsoft Exchange Server.\n\nNodeXL is intended for users with little or no programming experience to allow them to collect, analyze, and visualize a variety of networks. NodeXL integrates into Microsoft Excel 2007, 2010, 2013 and 2016 and opens as a workbook with a variety of worksheets containing the elements of a graph structure such as edges and nodes. NodeXL can also import a variety of graph formats such as edgelists, adjacency matrices, GraphML, UCINet .dl, and Pajek .net.\n\nNodeXL imports UCINet and GraphML files, as well as Excel spreadsheets containing edge lists or adjacency matrices, into NodeXL workbooks. NodeXL also allows for quick collection of social media data via a set of import tools which can collect network data from e-mail, Twitter, YouTube, and Flickr. NodeXL requests the user's permission before collecting any personal data and focuses on the collection of publicly available data, such as Twitter statuses and follows relationships for users who have made their accounts public. These features allow NodeXL users to instantly get working on relevant social media data and integrate aspects of social media data collection and analysis into one tool.\n\nNodeXL workbooks contain four worksheets: Edges, Vertices, Groups, and Overall Metrics. The relevant data about entities in the graph and relationships between them are located in the appropriate worksheet in row format. For example, the edges worksheet contains a minimum of two columns, and each row has a minimum of two elements corresponding to the two vertices that make up an edge in the graph. Graph metrics and edge and vertex visual properties appear as additional columns in the respective worksheets. This representation allows the user to leverage the Excel spreadsheet to quickly edit existing node properties and to generate new ones, for instance by applying Excel formulas to existing columns.\n\nNodeXL contains a library of commonly used graph metrics: centrality, clustering coefficient, diameter. NodeXL differentiates between directed and undirected networks. NodeXL implements a variety of community detection algorithms to allow the user to automatically discover clusters in their social networks.\n\nNodeXL generates an interactive canvas for visualizing graphs. The project allows users to pick from several well-known Force-directed graph drawing layout algorithms such as Fruchterman-Reingold and Harel-Koren. NodeXL allows the user to multi-select, drag and drop nodes on the canvas and to manually edit their visual properties (size, color, and opacity). In addition, NodeXL allows users to map the visual properties of nodes and edges to metrics it calculates, and in general to any column in the edges and vertices worksheet.\n\nNodeXL has been used by news outlets like Foreign Policy to visualize the structure of conversations about political topics as well as organizations like the World Bank to analyze voting data. NodeXL has been used as an analytical tool in dozens of research papers in the social, information, and computer sciences as well as the focus of research in human computer interaction, data mining, and data visualization.\n\n\n\n\n"}
{"id": "37990843", "url": "https://en.wikipedia.org/wiki?curid=37990843", "title": "Noncommutative torus", "text": "Noncommutative torus\n\nIn mathematics, and more specifically in the theory of C*-algebras, the noncommutative tori \"A\", also known as irrational rotation algebras for irrational values of θ, form a family of noncommutative C*-algebras which generalize the algebra of continuous functions on the 2-torus. Many topological and geometric properties of the classical 2-torus have algebraic analogues for the noncommutative tori, and as such they are fundamental examples of a noncommutative space in the sense of Alain Connes.\n\nFor any real number \"θ\", the noncommutative torus \"A\" is the C*-subalgebra of \"B\"(\"L\"(S)), the algebra of bounded linear operators of square-integrable functions on the unit circle S of C, generated by the unitary elements \"U\" and \"V\", where \"U\"(\"f\")(\"z\")=\"zf\"(\"z\") and \"V\"(\"f\")(\"z\")=\"f\"(\"e\"\"z\"). A quick calculation shows that \"VU\" = \"e\"\"UV\".\n\n\nThe K-theory of \"A\" is Z in both even dimension and odd dimension, and so does not distinguish the irrational rotation algebras. But as an ordered group, \"K\" ≃ Z + \"θ\"Z. Therefore, two noncommutative tori \"A\" and \"A\" are isomorphic if and only if either \"θ\" + \"η\" or \"θ\" − \"η\" is an integer.\n\nTwo irrational rotation algebras \"A\" and \"A\" are strongly Morita equivalent if and only if \"θ\" and \"η\" are in the same orbit of the action of SL(2, Z) on R by fractional linear transformations. In particular, the noncommutative tori with θ rational are Morita equivalent to the classical torus. On the other hand, the noncommutative tori with θ irrational are simple C*-algebras.\n"}
{"id": "45609427", "url": "https://en.wikipedia.org/wiki?curid=45609427", "title": "Nuprl", "text": "Nuprl\n\nNuprl is a proof development system, providing computer-mediated analysis and proofs of formal mathematical statements, and tools for software verification and optimization. Originally developed in the 1980s by Robert Lee Constable and others, the system is now maintained by the PRL Project at Cornell University. The currently supported version, Nuprl 5, is also known as FDL (Formal Digital Library). Nuprl functions as an automated theorem proving system and can also be used to provide proof assistance.\n\nNuprl uses a type system based on Martin-Löf intuitionistic type theory to model mathematical statements in a digital library. Mathematical theories can be constructed and analyzed with a variety of editors, including a graphical user interface, a web-based editor, and an Emacs mode. A variety of evaluators and inference engines can operate on the statements in the library. Translators also allow statements to be manipulated with Java and OCaml programs. The overall system is controlled with a variant of ML.\n\nNuprl 5's architecture is described as \"distributed open architecture\" and intended primarily to be used as a web service rather than as standalone software. Those interested in using the web service, or migrating theories from older versions of Nuprl, can contact the email address given on the Nuprl System web page.\n\nNuprl was first released in 1984, and was first described in detail in the book \"Implementing Mathematics with the Nuprl Proof Development System\", published in 1986. Nuprl 2 was the first Unix version. Nuprl 3 provided machine proof for mathematical problems related to Girard's Paradox and Higman's lemma. Nuprl 4, the first version developed for the World Wide Web, was used to verify cache coherency protocols and other computer systems.\n\nThe current system architecture, implemented in Nuprl 5, was first proposed in a 2000 conference paper. A reference manual for Nuprl 5 was published in 2002. Nuprl has been the subject of many computer science publications, some as recent as 2014.\n\nBoth the JonPRL and RedPRL systems are also based on computational type theory. RedPRL is explicitly \"inspired by Nuprl\".\n\n"}
{"id": "58873065", "url": "https://en.wikipedia.org/wiki?curid=58873065", "title": "Olga Gil Medrano", "text": "Olga Gil Medrano\n\nOlga Gil Medrano (born 1956) is a Spanish mathematician who was president of the Royal Spanish Mathematical Society from 2006 to 2009.\nShe is a professor of mathematics at the University of Valencia, where she is also Vice-Rector for International Relations and Cooperation.\nHer mathematical research concerns differential geometry and geometric analysis; since 2000, she has also been interested in the dissemination of mathematics to the general public.\n\nGil was born in Burgos in 1956.\n\nAs an undergraduate student at the University of Valencia, Gil was advised to study engineering, but she ignored the advice, preferring mathematics and physics.\nShe earned a Ph.D. at the University of Valencia in 1982, with a dissertation on \"Certain Geometric and Topological Properties of Some Classes of Almost-Product Manifolds\" supervised by Antonio Martínez Naveira. She then studied for a doctorat de troisième cycle in France, at Pierre and Marie Curie University, which she completed in 1985. Her second dissertation, \"Sur le Problème de Yamabe concernat les variétés localement conformement plates\", was supervised by Thierry Aubin.\n\nAfter completing her second doctorate, she returned to Valencia as a faculty member.\nShe has been a member of the governing board of the University of Valencia since 2000. When she became president of the Royal Spanish Mathematical Society in 2006, she was the first woman elected to that position.\n\nIn September 2017, the University of Valencia hosted a workshop on geometric analysis in honor of Gil.\n\n"}
{"id": "55836870", "url": "https://en.wikipedia.org/wiki?curid=55836870", "title": "OpenQASM", "text": "OpenQASM\n\nOpen Quantum Assembly Language (OpenQASM; pronounced \"open kazm\") is an intermediate representation for quantum instructions. The language was first described in a paper published in July 2017, and source code was released as part of IBM's Quantum Information Software Kit (QISKit) for use with their IBM Q Experience cloud quantum computing platform. The language has similar qualities to traditional hardware description languages such as Verilog.\n\nThe following is an example of OpenQASM source code from the official library. The program adds two four-bit numbers.\n// quantum ripple-carry adder from Cuccaro et al, quant-ph/0410184\nOPENQASM 2.0;\ninclude \"qelib1.inc\";\ngate majority a,b,c \ngate unmaj a,b,c \nqreg cin[1];\nqreg a[4];\nqreg b[4];\nqreg cout[1];\ncreg ans[5];\n// set input states\nx a[0]; // a = 0001\nx b; // b = 1111\n// add a to b, storing result in b\nmajority cin[0],b[0],a[0];\nmajority a[0],b[1],a[1];\nmajority a[1],b[2],a[2];\nmajority a[2],b[3],a[3];\ncx a[3],cout[0];\nunmaj a[2],b[3],a[3];\nunmaj a[1],b[2],a[2];\nunmaj a[0],b[1],a[1];\nunmaj cin[0],b[0],a[0];\nmeasure b[0] -> ans[0];\nmeasure b[1] -> ans[1];\nmeasure b[2] -> ans[2];\nmeasure b[3] -> ans[3];\nmeasure cout[0] -> ans[4];\n"}
{"id": "12606685", "url": "https://en.wikipedia.org/wiki?curid=12606685", "title": "Oriented coloring", "text": "Oriented coloring\n\nIn graph theory, oriented graph coloring is a special type of graph coloring. Namely, it is\nan assignment of colors to vertices of an oriented graph that\n\nAn \"oriented chromatic number\" of a graph \"G\" is the least number of colors needed in an oriented coloring;\nit is usually denoted by formula_1. The same definition can be extended to undirected graphs, as well, by defining the oriented chromatic number of an undirected graph to be the largest oriented chromatic number of any of its orientations.\n\nThe oriented chromatic number of a directed 5-cycle is five. If the cycle is colored by four or fewer colors, then either two adjacent vertices have the same color, or two vertices two steps apart have the same color. In the latter case, the edges connecting these two vertices to the vertex between them are inconsistently oriented: both have the same pair of colors but with opposite orientations. Thus, no coloring with four or fewer colors is possible. However, giving each vertex its own unique color leads to a valid oriented coloring.\n\nAn oriented coloring can exist only for a directed graph with no loops or directed 2-cycles. For, a loop cannot have different colors at its endpoints, and a 2-cycle cannot have both of its edges consistently oriented between the same two colors. If these conditions are satisfied, then there always exists an oriented coloring, for instance the coloring that assigns a different color to each vertex.\n\nIf an oriented coloring is complete, in the sense that no two colors can be merged to produce a coloring with fewer colors, then it corresponds uniquely to a graph homomorphism into a tournament. The tournament has one vertex for each color in the coloring. For each pair of colors, there is an edge in the colored graph with those two colors at its endpoints, which lends its orientation to the edge in the tournament between the vertices corresponding to the two colors. Incomplete colorings may also be represented by homomorphisms into tournaments but in this case the correspondence between colorings and homomorphisms is not one-to-one.\n\nUndirected graphs of bounded genus, bounded degree, or bounded acyclic chromatic number also have bounded oriented chromatic number.\n"}
{"id": "354319", "url": "https://en.wikipedia.org/wiki?curid=354319", "title": "Outline of category theory", "text": "Outline of category theory\n\nThe following outline is provided as an overview of and guide to category theory, the area of study in mathematics that examines in an abstract way the properties of particular mathematical concepts, by formalising them as collections of \"objects\" and \"arrows\" (also called morphisms, although this term also has a specific, non category-theoretical sense), where these collections satisfy certain basic conditions. Many significant areas of mathematics can be formalised as categories, and the use of category theory allows many intricate and subtle mathematical results in these fields to be stated, and proved, in a much simpler way than without the use of categories.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "25198879", "url": "https://en.wikipedia.org/wiki?curid=25198879", "title": "Pizza theorem", "text": "Pizza theorem\n\nIn elementary geometry, the pizza theorem states the equality of two areas that arise when one partitions a disk in a certain way.\n\nLet \"p\" be an interior point of the disk, and let \"n\" be a number that is divisible by 4 and greater than or equal to 8. Form \"n\" sectors of the disk with equal angles by choosing an arbitrary line through \"p\", rotating the line times by an angle of radians, and slicing the disk on each of the resulting lines. Number the sectors consecutively in a clockwise or anti-clockwise fashion. Then the pizza theorem states that:\n\nThe pizza theorem is so called because it mimics a traditional pizza slicing technique. It shows that, if two people share a pizza sliced in this way by taking alternating slices, then they each get an equal amount of pizza.\n\nThe pizza theorem was originally proposed as a challenge problem by . The published solution to this problem, by Michael Goldberg, involved direct manipulation of the algebraic expressions for the areas of the sectors.\n\nThe requirement that the number of sectors be a multiple of four is necessary: as Don Coppersmith showed, dividing a disk into four sectors, or a number of sectors that is not divisible by four, does not in general produce equal areas. answered a problem of by providing a more precise version of the theorem that determines which of the two sets of sectors has greater area in the cases that the areas are unequal. Specifically, if the number of sectors is 2 (mod 8) and no slice passes through the center of the disk, then the subset of slices containing the center has smaller area than the other subset, while if the number of sectors is 6 (mod 8) and no slice passes through the center, then the subset of slices containing the center has larger area. An odd number of sectors is not possible with straight-line cuts, and a slice through the center causes the two subsets to be equal regardless of the number of sectors.\n\nAs note, an equal division of the pizza also leads to an equal division of its toppings, as long as each topping is distributed in a disk (not necessarily concentric with the whole pizza) that contains the central point \"p\" of the division into sectors.\n\n show that a pizza sliced in the same way as the pizza theorem, into a number \"n\" of sectors with equal angles where \"n\" is divisible by four, can also be shared equally among \"n\"/4 people. For instance, a pizza divided into 12 sectors can be shared equally by three people as well as by two; however, to accommodate all five of the Hirschhorns, a pizza would need to be divided into 20 sectors.\n\nOther mathematical results related to pizza-slicing involve the lazy caterer's sequence, a sequence of integers that counts the maximum number of pieces of pizza that one can obtain by a given number of straight slices, and the ham sandwich theorem, a result about slicing three-dimensional objects whose two-dimensional version implies that any pizza, no matter how misshapen, can have its area and its crust length simultaneously bisected by a single carefully chosen straight-line cut, and whose three-dimensional version implies that a plane cut exists that equally shares base, tomato and cheese.\n\n\n"}
{"id": "28752783", "url": "https://en.wikipedia.org/wiki?curid=28752783", "title": "PlusCal", "text": "PlusCal\n\nPlusCal (formerly called +CAL) is a formal specification language created by Leslie Lamport, which transpiles to TLA. In contrast to TLA's action-oriented focus on distributed systems, PlusCal most resembles an imperative programming language and is better-suited when specifying sequential algorithms. PlusCal was designed to replace pseudocode, retaining its simplicity while providing a formally-defined and verifiable language. A one-bit clock is written in PlusCal as follows:\n-- fair algorithm OneBitClock {\n\n\n"}
{"id": "44112540", "url": "https://en.wikipedia.org/wiki?curid=44112540", "title": "Rachel Kuske", "text": "Rachel Kuske\n\nRachel Kuske is an American-Canadian applied mathematician and Professor and Chair of Mathematics at the Georgia Institute of Technology.\n\nKuske received her PhD in Applied Mathematics from Northwestern University in 1992. From 1997 to 2002, she was Assistant Professor and then Associate Professor at the University of Minnesota. She is an expert on stochastic and nonlinear dynamics, mathematical modelling, asymptotic methods, and industrial mathematics.\n\nKuske was awarded a Sloan Fellowship in 1992 and was made a Canada Research Chair in 2002.\n\nIn 2011 Kuske was a recipient of the Canadian Mathematical Society Krieger–Nelson Prize, given to outstanding woman in mathematics in Canada.\n\nIn 2015 she became a fellow of the Society for Industrial and Applied Mathematics \"for contributions to the theory of stochastic and nonlinear dynamics and its application, and for promoting equity and diversity in mathematics.\"\n\n"}
{"id": "40725476", "url": "https://en.wikipedia.org/wiki?curid=40725476", "title": "Ramsey class", "text": "Ramsey class\n\nIn the area of mathematics known as Ramsey theory, a Ramsey class is one which satisfies a generalization of Ramsey's theorem.\n\nSuppose formula_1, formula_2 and formula_3 are structures and formula_4 is a positive integer. We denote by formula_5 the set of all subobjects formula_6 of formula_2 which are isomorphic to formula_1. We further denote by formula_9 the property that for all partitions formula_10 of formula_11 there exists a formula_12 and an formula_13 such that formula_14.\n\nSuppose formula_15 is a class of structures closed under isomorphism and substructures. We say the class formula_15 has the A-Ramsey property if for ever positive integer formula_4 and for every formula_18 there is a formula_19 such that formula_9 holds. If formula_15 has the formula_1-Ramsey property for all formula_23 then we say formula_15 is a Ramsey class.\n\nRamsey's theorem is equivalent to the statement that the class of all finite sets is a Ramsey class.\n"}
{"id": "22899297", "url": "https://en.wikipedia.org/wiki?curid=22899297", "title": "Rate of return on a portfolio", "text": "Rate of return on a portfolio\n\nThe rate of return on a portfolio is the ratio of income generated (whether realized or not) by a portfolio to the size of the portfolio. It is measured over a period of time, commonly a year.\n\nThe rate of return on a portfolio can be calculated either directly or indirectly, depending upon the particular type of data available.\n\nDirect historical measurement of the rate of return on a portfolio applies one of several alternative methods, such as for example the time-weighted method, or the modified Dietz method. It requires knowledge of the value of the portfolio at the start and end of the period of time under measurement, together with the external flows of value into and out of the portfolio at various times within the time period. For the time-weighted method, it is also necessary to know the value of the portfolio when these flows occur (i.e. either immediately after, or immediately before).\n\nThe rate of return on a portfolio can be calculated indirectly as the weighted average rate of return on the various assets within the portfolio. The weights are proportional to the value of the assets within the portfolio, to take into account what portion of the portfolio each individual return represents in calculating the contribution of that asset to the return on the portfolio.\n\nThis method is particularly useful for projecting into the future the rate of return on a portfolio, given projections of the rates of return on the constituents of the portfolio.\n\nThe indirect calculation of the rate of return on a portfolio can be expressed by the formula:\n\nwhich is the sum of the contributions formula_2, formula_3 where:\n\n\nNow suppose that 40% of the portfolio is in the mining stock (weighting for this stock \"A\" = 40%), 40% is in the child care centre (weighting for this stock \"A\" = 40%) and the remaining 20% is in the fishing company (weighting for this stock \"A\" = 20%). To determine the rate of return on this portfolio, first calculate the contribution of each asset to the return on the portfolio, by multiplying the weighting of each asset by its rate of return, and then add these contributions together:\n\n\nAdding together these percentage contributions gives 4% + 3.2% + 2.4% = 9.6%,\nresulting in a rate of return on this portfolio of 9.6%.\n\nIf there are any external flows or other transactions on the assets in the portfolio during the period of measurement, and also depending on the methodology used for calculating the returns and weights, discrepancies may arise between the direct measurement of the rate of return on a portfolio, and indirect measurement (described above).\n\n"}
{"id": "646931", "url": "https://en.wikipedia.org/wiki?curid=646931", "title": "Richard Taylor (mathematician)", "text": "Richard Taylor (mathematician)\n\nRichard Lawrence Taylor (born 19 May 1962) is a British and American mathematician working in the field of number theory. He is currently a professor of mathematics at Stanford University and the Institute for Advanced Study.\n\nTaylor received the 2014 Breakthrough Prize in Mathematics \"for numerous breakthrough results in the theory of automorphic forms, including the Taniyama–Weil conjecture, the local Langlands conjecture for general linear groups, and the Sato–Tate conjecture.\" He also received the 2007 Shaw Prize in Mathematical Sciences for his work on the Langlands program with Robert Langlands.\n\nHe received his BA from Clare College, Cambridge. During his time at Cambridge, he was president of The Archimedeans in 1981 and 1982, following the impeachment of his predecessor. He earned his PhD from Princeton University in 1988. From 1995 to 1996 he held the Savilian chair of geometry at Oxford University and Fellow of New College, Oxford, and later became the Herchel Smith Professor of Mathematics at Harvard University. He currently holds Robert and Luisa Fernholz Professorship at the Institute for Advanced Study.\n\nHe received the Whitehead Prize in 1990, the Fermat Prize, the Ostrowski Prize in 2001, the Cole Prize of the American Mathematical Society in 2002, and the Shaw Prize for Mathematics in 2007. He was also elected a Fellow of the Royal Society in 1995. In 2012 he became a fellow of the American Mathematical Society. In 2015 he was inducted into the National Academy of Sciences.\nHe was elected to the American Philosophical Society in 2018.\n\nOne of the two papers containing the published proof of Fermat's Last Theorem is a joint work of Taylor and Andrew Wiles.\n\nIn subsequent work, Taylor (along with Michael Harris) proved the local Langlands conjectures for GL(\"n\") over a number field. A simpler proof was suggested almost at the same time by Guy Henniart, and ten years later by Peter Scholze.\n\nTaylor, together with Christophe Breuil, Brian Conrad and Fred Diamond, completed the proof of the Taniyama–Shimura conjecture, by performing quite heavy technical computations in the case of additive reduction.\n\nIn 2008, Taylor, following the ideas of Michael Harris and building on his joint work with Laurent Clozel, Michael Harris, and Nick Shepherd-Barron, announced a proof of the Sato–Tate conjecture, for elliptic curves with non-integral j-invariant. This partial proof of the Sato–Tate conjecture uses Wiles's theorem about modularity of semistable elliptic curves.\n\nTaylor is the son of British physicist John C. Taylor. He is married, and has two children.\n\n"}
{"id": "37606787", "url": "https://en.wikipedia.org/wiki?curid=37606787", "title": "Run to completion scheduling", "text": "Run to completion scheduling\n\nRun-to-completion scheduling is a scheduling model in which each task runs until it either finishes, or explicitly yields control back to the scheduler. Run to completion systems typically have an event queue which is serviced either in strict order of admission by an event loop, or by an admission scheduler which is capable of scheduling events out of order, based on other constraints such as deadlines.\n\nSome preemptive multitasking scheduling systems behave as run-to-completion schedulers in regard to scheduling tasks at one particular process priority level, at the same time as those processes still preempt other lower priority tasks and are themselves preempted by higher priority tasks.\n\n"}
{"id": "19376148", "url": "https://en.wikipedia.org/wiki?curid=19376148", "title": "Stephen Hawking", "text": "Stephen Hawking\n\nStephen William Hawking (8 January 1942 – 14 March 2018) was an English theoretical physicist, cosmologist, and author, who was director of research at the Centre for Theoretical Cosmology at the University of Cambridge at the time of his death. He was the Lucasian Professor of Mathematics at the University of Cambridge between 1979 and 2009.\n\nHis scientific works included a collaboration with Roger Penrose on gravitational singularity theorems in the framework of general relativity and the theoretical prediction that black holes emit radiation, often called Hawking radiation. Hawking was the first to set out a theory of cosmology explained by a union of the general theory of relativity and quantum mechanics. He was a vigorous supporter of the many-worlds interpretation of quantum mechanics.\n\nHawking achieved commercial success with several works of popular science in which he discusses his own theories and cosmology in general. His book \"A Brief History of Time\" appeared on the British \"Sunday Times\" best-seller list for a record-breaking 237 weeks. Hawking was a fellow of the Royal Society (FRS), a lifetime member of the Pontifical Academy of Sciences, and a recipient of the Presidential Medal of Freedom, the highest civilian award in the United States. In 2002, Hawking was ranked number 25 in the BBC's poll of the 100 Greatest Britons.\nHawking had a rare early-onset slow-progressing form of motor neurone disease (also known as amyotrophic lateral sclerosis \"ALS\" or Lou Gehrig's disease) that gradually paralysed him over the decades. Even after the loss of his speech, he was still able to communicate through a speech-generating device, initially through use of a hand-held switch, and eventually by using a single cheek muscle. He died on 14 March 2018 at the age of 76.\n\nHawking was born on 8 January 1942 in Oxford to Frank (1905–1986) and Isobel Eileen Hawking (née Walker; 1915–2013). Hawking's mother was born into a family of doctors in Glasgow, Scotland. His wealthy paternal great-grandfather, from Yorkshire, had over-extended himself buying farm land and then going bankrupt in the great agricultural depression during the early 20th century. His paternal great-grandmother saved the family from financial ruin by opening a school in their home. Despite their families' financial constraints, both parents attended the University of Oxford, where Frank read medicine and Isobel read Philosophy, Politics and Economics. Isobel worked as a secretary for a medical research institute, and Frank was a medical researcher. Hawking had two younger sisters, Philippa and Mary, and an adopted brother, Edward Frank David (1955–2003).\n\nIn 1950, when Hawking's father became head of the division of parasitology at the National Institute for Medical Research, the family moved to St Albans, Hertfordshire. In St Albans, the family was considered highly intelligent and somewhat eccentric; meals were often spent with each person silently reading a book. They lived a frugal existence in a large, cluttered, and poorly maintained house and travelled in a converted London taxicab. During one of Hawking's father's frequent absences working in Africa, the rest of the family spent four months in Majorca visiting his mother's friend Beryl and her husband, the poet Robert Graves.\n\nHawking began his schooling at the Byron House School in Highgate, London. He later blamed its \"progressive methods\" for his failure to learn to read while at the school. In St Albans, the eight-year-old Hawking attended St Albans High School for Girls for a few months. At that time, younger boys could attend one of the houses.\n\nHawking attended two independent (i.e. fee-paying) schools, first Radlett School and from September 1952, St Albans School, after passing the eleven-plus a year early. The family placed a high value on education. Hawking's father wanted his son to attend the well-regarded Westminster School, but the 13-year-old Hawking was ill on the day of the scholarship examination. His family could not afford the school fees without the financial aid of a scholarship, so Hawking remained at St Albans. A positive consequence was that Hawking remained close to a group of friends with whom he enjoyed board games, the manufacture of fireworks, model aeroplanes and boats, and long discussions about Christianity and extrasensory perception. From 1958 on, with the help of the mathematics teacher Dikran Tahta, they built a computer from clock parts, an old telephone switchboard and other recycled components.\n\nAlthough known at school as \"Einstein\", Hawking was not initially successful academically. With time, he began to show considerable aptitude for scientific subjects and, inspired by Tahta, decided to read mathematics at university. Hawking's father advised him to study medicine, concerned that there were few jobs for mathematics graduates. He also wanted his son to attend University College, Oxford, his own \"alma mater\". As it was not possible to read mathematics there at the time, Hawking decided to study physics and chemistry. Despite his headmaster's advice to wait until the next year, Hawking was awarded a scholarship after taking the examinations in March 1959.\n\nHawking began his university education at University College, Oxford, in October 1959 at the age of 17. For the first 18 months, he was bored and lonelyhe found the academic work \"ridiculously easy\". His physics tutor, Robert Berman, later said, \"It was only necessary for him to know that something could be done, and he could do it without looking to see how other people did it.\" A change occurred during his second and third year when, according to Berman, Hawking made more of an effort \"to be one of the boys\". He developed into a popular, lively and witty college member, interested in classical music and science fiction. Part of the transformation resulted from his decision to join the college boat club, the University College Boat Club, where he coxed a rowing crew. The rowing coach at the time noted that Hawking cultivated a daredevil image, steering his crew on risky courses that led to damaged boats.\n\nHawking estimated that he studied about 1,000 hours during his three years at Oxford. These unimpressive study habits made sitting his finals a challenge, and he decided to answer only theoretical physics questions rather than those requiring factual knowledge. A first-class honours degree was a condition of acceptance for his planned graduate study in cosmology at the University of Cambridge. Anxious, he slept poorly the night before the examinations, and the final result was on the borderline between first- and second-class honours, making a \"viva\" (oral examination) necessary. Hawking was concerned that he was viewed as a lazy and difficult student. So, when asked at the oral to describe his plans, he said, \"If you award me a First, I will go to Cambridge. If I receive a Second, I shall stay in Oxford, so I expect you will give me a First.\" He was held in higher regard than he believed; as Berman commented, the examiners \"were intelligent enough to realise they were talking to someone far cleverer than most of themselves\". After receiving a first-class BA (Hons.) degree in physics and completing a trip to Iran with a friend, he began his graduate work at Trinity Hall, Cambridge, in October 1962.\n\nHawking's first year as a doctoral student was difficult. He was initially disappointed to find that he had been assigned Dennis William Sciama, one of the founders of modern cosmology, as a supervisor rather than noted Yorkshire astronomer Fred Hoyle, and he found his training in mathematics inadequate for work in general relativity and cosmology. After being diagnosed with motor neurone disease, Hawking fell into a depressionthough his doctors advised that he continue with his studies, he felt there was little point. His disease progressed more slowly than doctors had predicted. Although Hawking had difficulty walking unsupported, and his speech was almost unintelligible, an initial diagnosis that he had only two years to live proved unfounded. With Sciama's encouragement, he returned to his work. Hawking started developing a reputation for brilliance and brashness when he publicly challenged the work of Fred Hoyle and his student Jayant Narlikar at a lecture in June 1964.\n\nWhen Hawking began his graduate studies, there was much debate in the physics community about the prevailing theories of the creation of the universe: the Big Bang and Steady State theories. Inspired by Roger Penrose's theorem of a spacetime singularity in the centre of black holes, Hawking applied the same thinking to the entire universe; and, during 1965, he wrote his thesis on this topic. Hawking's thesis was approved in 1966. There were other positive developments: Hawking received a research fellowship at Gonville and Caius College at Cambridge; he obtained his PhD degree in applied mathematics and theoretical physics, specialising in general relativity and cosmology, in March 1966; and his essay \"Singularities and the Geometry of Space-Time\" shared top honours with one by Penrose to win that year's prestigious Adams Prize.\n\nIn his work, and in collaboration with Penrose, Hawking extended the singularity theorem concepts first explored in his doctoral thesis. This included not only the existence of singularities but also the theory that the universe might have started as a singularity. Their joint essay was the runner-up in the 1968 Gravity Research Foundation competition. In 1970 they published a proof that if the universe obeys the general theory of relativity and fits any of the models of physical cosmology developed by Alexander Friedmann, then it must have begun as a singularity. In 1969, Hawking accepted a specially created Fellowship for Distinction in Science to remain at Caius.\n\nIn 1970, Hawking postulated what became known as the second law of black hole dynamics, that the event horizon of a black hole can never get smaller. With James M. Bardeen and Brandon Carter, he proposed the four laws of black hole mechanics, drawing an analogy with thermodynamics. To Hawking's irritation, Jacob Bekenstein, a graduate student of John Wheeler, went further—and ultimately correctly—to apply thermodynamic concepts literally. In the early 1970s, Hawking's work with Carter, Werner Israel and David C. Robinson strongly supported Wheeler's no-hair theorem, one that states that no matter what the original material from which a black hole is created, it can be completely described by the properties of mass, electrical charge and rotation. His essay titled \"Black Holes\" won the Gravity Research Foundation Award in January 1971. Hawking's first book, \"The Large Scale Structure of Space-Time,\" written with George Ellis, was published in 1973.\n\nBeginning in 1973, Hawking moved into the study of quantum gravity and quantum mechanics. His work in this area was spurred by a visit to Moscow and discussions with Yakov Borisovich Zel'dovich and Alexei Starobinsky, whose work showed that according to the uncertainty principle, rotating black holes emit particles. To Hawking's annoyance, his much-checked calculations produced findings that contradicted his second law, which claimed black holes could never get smaller, and supported Bekenstein's reasoning about their entropy. His results, which Hawking presented from 1974, showed that black holes emit radiation, known today as Hawking radiation, which may continue until they exhaust their energy and evaporate. Initially, Hawking radiation was controversial. By the late 1970s and following the publication of further research, the discovery was widely accepted as a significant breakthrough in theoretical physics. Hawking was elected a Fellow of the Royal Society (FRS) in 1974, a few weeks after the announcement of Hawking radiation. At the time, he was one of the youngest scientists to become a Fellow.\n\nHawking was appointed to the Sherman Fairchild Distinguished visiting professorship at the California Institute of Technology (Caltech) in 1970. He worked with a friend on the faculty, Kip Thorne, and engaged him in a scientific wager about whether the X-ray source Cygnus X-1 was a black hole. The wager was an \"insurance policy\" against the proposition that black holes did not exist. Hawking acknowledged that he had lost the bet in 1990, a bet that was the first of several he was to make with Thorne and others. Hawking had maintained ties to Caltech, spending a month there almost every year since this first visit.\n\nHawking returned to Cambridge in 1975 to a more academically senior post, as reader in gravitational physics. The mid to late 1970s were a period of growing public interest in black holes and the physicists who were studying them. Hawking was regularly interviewed for print and television. He also received increasing academic recognition of his work. In 1975, he was awarded both the Eddington Medal and the Pius XI Gold Medal, and in 1976 the Dannie Heineman Prize, the Maxwell Prize and the Hughes Medal. He was appointed a professor with a chair in gravitational physics in 1977. The following year he received the Albert Einstein Medal and an honorary doctorate from the University of Oxford.\n\nIn 1979, Hawking was elected Lucasian Professor of Mathematics at the University of Cambridge. His inaugural lecture in this role was titled: \"Is the End in Sight for Theoretical Physics?\" and proposed N=8 Supergravity as the leading theory to solve many of the outstanding problems physicists were studying. His promotion coincided with a health crisis which led to his accepting, albeit reluctantly, some nursing services at home. At the same time, he was also making a transition in his approach to physics, becoming more intuitive and speculative rather than insisting on mathematical proofs. \"I would rather be right than rigorous\", he told Kip Thorne. In 1981, he proposed that information in a black hole is irretrievably lost when a black hole evaporates. This information paradox violates the fundamental tenet of quantum mechanics, and led to years of debate, including \"the Black Hole War\" with Leonard Susskind and Gerard 't Hooft.\n\nCosmological inflationa theory proposing that following the Big Bang, the universe initially expanded incredibly rapidly before settling down to a slower expansionwas proposed by Alan Guth and also developed by Andrei Linde. Following a conference in Moscow in October 1981, Hawking and Gary Gibbons organised a three-week Nuffield Workshop in the summer of 1982 on \"The Very Early Universe\" at Cambridge University, a workshop that focused mainly on inflation theory. Hawking also began a new line of quantum theory research into the origin of the universe. In 1981 at a Vatican conference, he presented work suggesting that there might be no boundaryor beginning or endingto the universe. He subsequently developed the research in collaboration with Jim Hartle, and in 1983 they published a model, known as the Hartle–Hawking state. It proposed that prior to the Planck epoch, the universe had no boundary in space-time; before the Big Bang, time did not exist and the concept of the beginning of the universe is meaningless. The initial singularity of the classical Big Bang models was replaced with a region akin to the North Pole. One cannot travel north of the North Pole, but there is no boundary thereit is simply the point where all north-running lines meet and end. Initially, the no-boundary proposal predicted a closed universe, which had implications about the existence of God. As Hawking explained, \"If the universe has no boundaries but is self-contained... then God would not have had any freedom to choose how the universe began.\"\n\nHawking did not rule out the existence of a Creator, asking in \"A Brief History of Time\" \"Is the unified theory so compelling that it brings about its own existence?\" In his early work, Hawking spoke of God in a metaphorical sense. In \"A Brief History of Time\" he wrote: \"If we discover a complete theory, it would be the ultimate triumph of human reasonfor then we should know the mind of God.\" In the same book he suggested that the existence of God was not necessary to explain the origin of the universe. Later discussions with Neil Turok led to the realisation that the existence of God was also compatible with an open universe.\n\nFurther work by Hawking in the area of arrows of time led to the 1985 publication of a paper theorising that if the no-boundary proposition were correct, then when the universe stopped expanding and eventually collapsed, time would run backwards. A paper by Don Page and independent calculations by Raymond Laflamme led Hawking to withdraw this concept. Honours continued to be awarded: in 1981 he was awarded the American Franklin Medal, and in the 1982 New Year Honours appointed a Commander of the Order of the British Empire (CBE). These awards did not significantly change Hawking's financial status, and motivated by the need to finance his children's education and home expenses, he decided in 1982 to write a popular book about the universe that would be accessible to the general public. Instead of publishing with an academic press, he signed a contract with Bantam Books, a mass market publisher, and received a large advance for his book. A first draft of the book, called \"A Brief History of Time\", was completed in 1984.\n\nOne of the first messages Hawking produced with his speech-generating device was a request for his assistant to help him finish writing \"A Brief History of Time\". Peter Guzzardi, his editor at Bantam, pushed him to explain his ideas clearly in non-technical language, a process that required many revisions from an increasingly irritated Hawking. The book was published in April 1988 in the US and in June in the UK, and it proved to be an extraordinary success, rising quickly to the top of best-seller lists in both countries and remaining there for months. The book was translated into many languages, and ultimately sold an estimated 9million copies. Media attention was intense, and a \"Newsweek\" magazine cover and a television special both described him as \"Master of the Universe\". Success led to significant financial rewards, but also the challenges of celebrity status. Hawking travelled extensively to promote his work, and enjoyed partying and dancing into the small hours. A difficulty refusing the invitations and visitors left him limited time for work and his students. Some colleagues were resentful of the attention Hawking received, feeling it was due to his disability. He received further academic recognition, including five more honorary degrees, the Gold Medal of the Royal Astronomical Society (1985), the Paul Dirac Medal (1987) and, jointly with Penrose, the prestigious Wolf Prize (1988). In the 1989 Birthday Honours, he was appointed a Companion of Honour (CH). He reportedly declined a knighthood in the late 1990s in objection to the UK's science funding policy.\n\nHawking pursued his work in physics: in 1993 he co-edited a book on Euclidean quantum gravity with Gary Gibbons and published a collected edition of his own articles on black holes and the Big Bang. In 1994, at Cambridge's Newton Institute, Hawking and Penrose delivered a series of six lectures that were published in 1996 as \"The Nature of Space and Time\". In 1997, he conceded a 1991 public scientific wager made with Kip Thorne and John Preskill of Caltech. Hawking had bet that Penrose's proposal of a \"cosmic censorship conjecture\"that there could be no \"naked singularities\" unclothed within a horizonwas correct. After discovering his concession might have been premature, a new and more refined wager was made. This one specified that such singularities would occur without extra conditions. The same year, Thorne, Hawking and Preskill made another bet, this time concerning the black hole information paradox. Thorne and Hawking argued that since general relativity made it impossible for black holes to radiate and lose information, the mass-energy and information carried by Hawking radiation must be \"new\", and not from inside the black hole event horizon. Since this contradicted the quantum mechanics of microcausality, quantum mechanics theory would need to be rewritten. Preskill argued the opposite, that since quantum mechanics suggests that the information emitted by a black hole relates to information that fell in at an earlier time, the concept of black holes given by general relativity must be modified in some way.\n\nHawking also maintained his public profile, including bringing science to a wider audience. A film version of \"A Brief History of Time\", directed by Errol Morris and produced by Steven Spielberg, premiered in 1992. Hawking had wanted the film to be scientific rather than biographical, but he was persuaded otherwise. The film, while a critical success, was not widely released. A popular-level collection of essays, interviews, and talks titled \"Black Holes and Baby Universes and Other Essays\" was published in 1993, and a six-part television series \"Stephen Hawking's Universe\" and a companion book appeared in 1997. As Hawking insisted, this time the focus was entirely on science.\n\nHawking continued his writings for a popular audience, publishing \"The Universe in a Nutshell\" in 2001, and \"A Briefer History of Time\", which he wrote in 2005 with Leonard Mlodinow to update his earlier works with the aim of making them accessible to a wider audience, and \"God Created the Integers\", which appeared in 2006. Along with at CERN and Jim Hartle, from 2006 on Hawking developed a theory of \"top-down cosmology\", which says that the universe had not one unique initial state but many different ones, and therefore that it is inappropriate to formulate a theory that predicts the universe's current configuration from one particular initial state. Top-down cosmology posits that the present \"selects\" the past from a superposition of many possible histories. In doing so, the theory suggests a possible resolution of the fine-tuning question.\n\nHawking continued to travel widely, including trips to Chile, Easter Island, South Africa, Spain (to receive the Fonseca Prize in 2008), Canada, and numerous trips to the United States. For practical reasons related to his disability, Hawking increasingly travelled by private jet, and by 2011 that had become his only mode of international travel.\n\nBy 2003, consensus among physicists was growing that Hawking was wrong about the loss of information in a black hole. In a 2004 lecture in Dublin, he conceded his 1997 bet with Preskill, but described his own, somewhat controversial solution to the information paradox problem, involving the possibility that black holes have more than one topology. In the 2005 paper he published on the subject, he argued that the information paradox was explained by examining all the alternative histories of universes, with the information loss in those with black holes being cancelled out by those without such loss. In January 2014, he called the alleged loss of information in black holes his \"biggest blunder\".\n\nAs part of another longstanding scientific dispute, Hawking had emphatically argued, and bet, that the Higgs boson would never be found. The particle was proposed to exist as part of the Higgs field theory by Peter Higgs in 1964. Hawking and Higgs engaged in a heated and public debate over the matter in 2002 and again in 2008, with Higgs criticising Hawking's work and complaining that Hawking's \"celebrity status gives him instant credibility that others do not have.\" The particle was discovered in July 2012 at CERN following construction of the Large Hadron Collider. Hawking quickly conceded that he had lost his bet and said that Higgs should win the Nobel Prize for Physics, which he did in 2013.\n\nIn 2007, Hawking and his daughter Lucy published \"George's Secret Key to the Universe\", a children's book designed to explain theoretical physics in an accessible fashion and featuring characters similar to those in the Hawking family. The book was followed by sequels in 2009, 2011, 2014 and 2016.\n\nIn 2002, following a UK-wide vote, the BBC included Hawking in their list of the 100 Greatest Britons. He was awarded the Copley Medal from the Royal Society (2006), the Presidential Medal of Freedom, which is America's highest civilian honour (2009), and the Russian Special Fundamental Physics Prize (2013).\n\nSeveral buildings have been named after him, including the Stephen W. Hawking Science Museum in San Salvador, ElSalvador, the Stephen Hawking Building in Cambridge, and the Stephen Hawking Centre at the Perimeter Institute in Canada. Appropriately, given Hawking's association with time, he unveiled the mechanical \"Chronophage\" (or time-eating) Corpus Clock at Corpus Christi College, Cambridge in September 2008.\n\nDuring his career, Hawking supervised 39 successful PhD students. One doctoral student did not successfully complete the PhD. As required by Cambridge University regulations, Hawking retired as Lucasian Professor of Mathematics in 2009. Despite suggestions that he might leave the United Kingdom as a protest against public funding cuts to basic scientific research, Hawking worked as director of research at the Cambridge University Department of Applied Mathematics and Theoretical Physics.\n\nOn 28 June 2009, as a tongue-in-cheek test of his 1992 conjecture that travel into the past is effectively impossible, Hawking held a party open to all, complete with hors d'oeuvres and iced champagne, but publicised the party only after it was over so that only time-travellers would know to attend; as expected, nobody showed up to the party.\n\nOn 20 July 2015, Hawking helped launch Breakthrough Initiatives, an effort to search for extraterrestrial life. Hawking created \"Stephen Hawking: Expedition New Earth\", a documentary on space colonisation, as a 2017 episode of \"Tomorrow's World\".\n\nIn August 2015, Hawking said that not all information is lost when something enters a black hole and there might be a possibility to retrieve information from a black hole according to his theory. In July 2017, Hawking was awarded an Honorary Doctorate from Imperial College London.\n\nHawking's final paper – \"A smooth exit from eternal inflation?\" – was published in the \"Journal of High Energy Physics\" on 27 April 2018.\n\nWhen Hawking was a graduate student at Cambridge, his relationship with Jane Wilde, a friend of his sister whom he had met shortly before his late 1963 diagnosis with motor neurone disease, continued to develop. The couple became engaged in October 1964Hawking later said that the engagement gave him \"something to live for\"and the two were married on 14 July 1965.\n\nDuring their first years of marriage, Jane lived in London during the week as she completed her degree, and they travelled to the United States several times for conferences and physics-related visits. The couple had difficulty finding housing that was within Hawking's walking distance to the Department of Applied Mathematics and Theoretical Physics (DAMTP). Jane began a PhD programme, and a son, Robert, was born in May 1967. A daughter, Lucy, was born in 1970. A third child, Timothy, was born in April 1979.\n\nHawking rarely discussed his illness and physical challenges, evenin a precedent set during their courtshipwith Jane. His disabilities meant that the responsibilities of home and family rested firmly on his wife's increasingly overwhelmed shoulders, leaving him more time to think about physics. Upon his appointment in 1974 to a year-long position at the California Institute of Technology in Pasadena, California, Jane proposed that a graduate or post-doctoral student live with them and help with his care. Hawking accepted, and Bernard Carr travelled with them as the first of many students who fulfilled this role. The family spent a generally happy and stimulating year in Pasadena.\n\nHawking returned to Cambridge in 1975 to a new home and a new job, as reader. Don Page, with whom Hawking had begun a close friendship at Caltech, arrived to work as the live-in graduate student assistant. With Page's help and that of a secretary, Jane's responsibilities were reduced so she could return to her thesis and her new interest in singing.\n\nBy December 1977, Jane had met organist Jonathan Hellyer Jones when singing in a church choir. Hellyer Jones became close to the Hawking family, and by the mid-1980s, he and Jane had developed romantic feelings for each other. According to Jane, her husband was accepting of the situation, stating \"he would not object so long as I continued to love him\". Jane and Hellyer Jones determined not to break up the family, and their relationship remained platonic for a long period.\n\nBy the 1980s, Hawking's marriage had been strained for many years. Jane felt overwhelmed by the intrusion into their family life of the required nurses and assistants. The impact of his celebrity was challenging for colleagues and family members, while the prospect of living up to a worldwide fairytale image was daunting for the couple. Hawking's views of religion also contrasted with her strong Christian faith and resulted in tension. In the late 1980s, Hawking had grown close to one of his nurses, Elaine Mason, to the dismay of some colleagues, caregivers, and family members, who were disturbed by her strength of personality and protectiveness. Hawking told Jane that he was leaving her for Mason, and departed the family home in February 1990. After his divorce from Jane in 1995, Hawking married Mason in September, declaring, \"It's wonderfulI have married the woman I love.\"\n\nIn 1999, Jane Hawking published a memoir, \"Music to Move the Stars\", describing her marriage to Hawking and its breakdown. Its revelations caused a sensation in the media but, as was his usual practice regarding his personal life, Hawking made no public comment except to say that he did not read biographies about himself. After his second marriage, Hawking's family felt excluded and marginalised from his life. For a period of about five years in the early 2000s, his family and staff became increasingly worried that he was being physically abused. Police investigations took place, but were closed as Hawking refused to make a complaint.\n\nIn 2006, Hawking and Mason quietly divorced, and Hawking resumed closer relationships with Jane, his children, and his grandchildren. Reflecting this happier period, a revised version of Jane's book called \"Travelling to Infinity: My Life with Stephen\" appeared in 2007, and was made into a film, \"The Theory of Everything\", in 2014.\n\nHawking had a rare early-onset slow-progressing form of motor neurone disease (also known as amyotrophic lateral sclerosis, \"ALS\", or Lou Gehrig's disease), that gradually paralysed him over the decades.\n\nHawking had experienced increasing clumsiness during his final year at Oxford, including a fall on some stairs and difficulties when rowing. The problems worsened, and his speech became slightly slurred and his family noticed the changes when he returned home for Christmas, and medical investigations were begun. The diagnosis of motor neurone disease came when Hawking was 21, in 1963. At the time, doctors gave him a life expectancy of two years.\n\nIn the late 1960s, Hawking's physical abilities declined: he began to use crutches and could no longer give lectures regularly. As he slowly lost the ability to write, he developed compensatory visual methods, including seeing equations in terms of geometry. The physicist Werner Israel later compared the achievements to Mozart composing an entire symphony in his head. Hawking was fiercely independent and unwilling to accept help or make concessions for his disabilities. He preferred to be regarded as \"a scientist first, popular science writer second, and, in all the ways that matter, a normal human being with the same desires, drives, dreams, and ambitions as the next person.\" His wife, Jane Hawking, later noted: \"Some people would call it determination, some obstinacy. I've called it both at one time or another.\" He required much persuasion to accept the use of a wheelchair at the end of the 1960s, but ultimately became notorious for the wildness of his wheelchair driving. Hawking was a popular and witty colleague, but his illness, as well as his reputation for brashness, distanced him from some.\n\nHawking's speech deteriorated, and by the late 1970s he could be understood by only his family and closest friends. To communicate with others, someone who knew him well would interpret his speech into intelligible speech. Spurred by a dispute with the university over who would pay for the ramp needed for him to enter his workplace, Hawking and his wife campaigned for improved access and support for those with disabilities in Cambridge, including adapted student housing at the university. In general, Hawking had ambivalent feelings about his role as a disability rights champion: while wanting to help others, he also sought to detach himself from his illness and its challenges. His lack of engagement in this area led to some criticism.\n\nDuring a visit to CERN on the border of France and Switzerland in mid-1985, Hawking contracted pneumonia, which in his condition was life-threatening; he was so ill that Jane was asked if life support should be terminated. She refused, but the consequence was a tracheotomy, which required round-the-clock nursing care and the removal of what remained of his speech. The National Health Service was ready to pay for a nursing home, but Jane was determined that he would live at home. The cost of the care was funded by an American foundation. Nurses were hired for the three shifts required to provide the round-the-clock support he required. One of those employed was Elaine Mason, who was to become Hawking's second wife.\n\nFor his communication, Hawking initially raised his eyebrows to choose letters on a spelling card, but in 1986 he received a computer program called the \"Equalizer\" from Walter Woltosz, CEO of Words Plus, who had developed an earlier version of the software to help his mother-in-law, who also suffered from ALS and had lost her ability to speak and write. In a method he used for the rest of his life, Hawking could now simply press a switch to select phrases, words or letters from a bank of about 2,500–3,000 that were scanned. The program was originally run on a desktop computer. Elaine Mason's husband, David, a computer engineer, adapted a small computer and attached it to his wheelchair. Released from the need to use somebody to interpret his speech, Hawking commented that \"I can communicate better now than before I lost my voice.\" The voice he used had an American accent and is no longer produced. Despite the later availability of other voices, Hawking retained this original voice, saying that he preferred it and identified with it. Originally, Hawking activated a switch using his hand and could produce up to 15 words a minute. Lectures were prepared in advance and were sent to the speech synthesizer in short sections to be delivered.\n\nHawking gradually lost the use of his hand, and in 2005 he began to control his communication device with movements of his cheek muscles, with a rate of about one word per minute. With this decline there was a risk of his developing locked-in syndrome, so Hawking collaborated with Intel researchers on systems that could translate his brain patterns or facial expressions into switch activations. After several prototypes that did not perform as planned, they settled on an adaptive word predictor made by the London-based startup SwiftKey, which used a system similar to his original technology. Hawking had an easier time adapting to the new system, which was further developed after inputting large amounts of Hawking's papers and other written materials and uses predictive software similar to other smartphone keyboards. By 2009 he could no longer drive his wheelchair independently, but the same people who created his new typing mechanics were working on a method to drive his chair using movements made by his chin. This proved difficult, since Hawking could not move his neck, and trials showed that while he could indeed drive the chair, the movement was sporadic and jumpy. Near the end of his life, Hawking experienced increased breathing difficulties, often resulting in his requiring the usage of a ventilator, and being regularly hospitalised.\n\nStarting in the 1990s, Hawking accepted the mantle of role model for disabled people, lecturing and participating in fundraising activities. At the turn of the century, he and eleven other luminaries signed the \"Charter for the Third Millennium on Disability\", which called on governments to prevent disability and protect the rights of the disabled. In 1999, Hawking was awarded the Julius Edgar Lilienfeld Prize of the American Physical Society.\n\nIn August 2012, Hawking narrated the \"Enlightenment\" segment of the 2012 Summer Paralympics opening ceremony in London. In 2013, the biographical documentary film \"Hawking\", in which Hawking himself is featured, was released. In September 2013, he expressed support for the legalisation of assisted suicide for the terminally ill. In August 2014, Hawking accepted the Ice Bucket Challenge to promote ALS/MND awareness and raise contributions for research. As he had pneumonia in 2013, he was advised not to have ice poured over him, but his children volunteered to accept the challenge on his behalf.\n\nIn late 2006, Hawking revealed in a BBC interview that one of his greatest unfulfilled desires was to travel to space; on hearing this, Richard Branson offered a free flight into space with Virgin Galactic, which Hawking immediately accepted. Besides personal ambition, he was motivated by the desire to increase public interest in spaceflight and to show the potential of people with disabilities. On 26 April 2007, Hawking flew aboard a specially-modified Boeing 727–200 jet operated by Zero-G Corp off the coast of Florida to experience weightlessness. Fears the manoeuvres would cause him undue discomfort proved groundless, and the flight was extended to eight parabolic arcs. It was described as a successful test to see if he could withstand the g-forces involved in space flight. At the time, the date of Hawking's trip to space was projected to be as early as 2009, but commercial flights to space did not commence before his death.\n\nHawking died at his home in Cambridge, England, early in the morning of 14 March 2018, at the age of 76. His family stated that he \"died peacefully\". He was eulogised by figures in science, entertainment, politics, and other areas. The Gonville and Caius College flag flew at half-mast and a book of condolences was signed by students and visitors. A tribute was made to Hawking in the closing speech by IPC President Andrew Parsons at the closing ceremony of the 2018 Paralympic Winter Games in Pyeongchang, South Korea.\n\nHawking's final broadcast interview, about the detection of gravitational waves resulting from the collision of two neutron stars, occurred in October 2017. His final words to the world appeared posthumously, in April 2018, in the form of a Smithsonian TV Channel documentary entitled, \"Leaving Earth: Or How to Colonize a Planet\". One of his final research studies, entitled \"A smooth exit from eternal inflation?\", about the origin of the universe, was published in the Journal of High Energy Physics in May 2018. Later, in October 2018, another of his final research studies, entitled \"Black Hole Entropy and Soft Hair\", was published, and dealt with the \"mystery of what happens to the information held by objects once they disappear into a black hole\". Also in October 2018, Hawking's last book, \"Brief Answers to the Big Questions\", a popular science book presenting his final comments on the most important questions facing humankind, was published.\nHawking was born on the 300th anniversary of Galileo's death and died on the 139th anniversary of Einstein's birth. His private funeral took place at 2 pm on the afternoon of 31 March 2018, at Great St Mary's Church, Cambridge. Guests at the funeral included Eddie Redmayne, Felicity Jones, and Queen guitarist and astrophysicist Brian May. Following the cremation, a service of thanksgiving was held at Westminster Abbey on 15 June 2018, after which his ashes were interred in the Abbey's nave, alongside the grave of Sir Isaac Newton and close to that of Charles Darwin.\n\nDuring the service, readings and tributes were made by Benedict Cumberbatch, who played Hawking in a BBC television film, astronaut Tim Peake, Astronomer Royal Martin Rees, and Nobel Prize winner Kip Thorne. Inscribed on his memorial stone are the words \"Here lies what was mortal of Stephen Hawking 1942 - 2018\" and his most famed equation. He directed, at least fifteen years before his death, that the Bekenstein–Hawking entropy equation be his epitaph.{k} = \\frac{1}{4} \\frac{A}{A_\\text{P}},</math>\n\nwhere formula_1 is the Planck area (the unit of area in quantum mechanics). Use of Planck units (in which formula_2 and formula_3 are set to unity) reduces it further to:\n\nJacob Bekenstein had conjectured the proportionality; Hawking confirmed it and established the constant of proportionality at formula_5. Calculations based on string theory, first carried out in 1995, have been found to yield the same result.\n\nThis relationship is conjectured to be valid not just for black holes, but also (since entropy is proportional to information) as an upper bound on the amount of information that can be contained in any volume of space, which has in turn spawned deeper reflections on the possible nature of reality.}} in June 2018, it was announced that Hawking's words, set to music by Greek composer Vangelis, are to be beamed into space from a European space agency satellite dish in Spain with the aim of reaching the nearest black hole, 1A 0620-00.\n\nOn 8 November 2018, an auction of 22 personal possessions of Stephen Hawking, including his doctoral thesis (\"Properties of Expanding Universes\", Ph.D. thesis, Cambridge University, 1965) and wheelchair, took place, and fetched about £1.8m (more than $2.3m). Proceeds from the auction sale of the wheelchair are to go to two charities, the Motor Neurone Disease Association and the Stephen Hawking Foundation,; proceeds from Hawking's other items are to go to his estate.\n\nIn 2006, Hawking posed an open question on the Internet: \"In a world that is in chaos politically, socially and environmentally, how can the human race sustain another 100 years?\", later clarifying: \"I don't know the answer. That is why I asked the question, to get people to think about it, and to be aware of the dangers we now face.\"\n\nHawking expressed concern that life on Earth is at risk from a sudden nuclear war, a genetically engineered virus, global warming, or other dangers humans have not yet thought of. Hawking has stated: “I regard it as almost inevitable that either a nuclear confrontation or environmental catastrophe will cripple the Earth at some point in the next 1,000 years”, and has considered an \"asteroid collision\" to be the biggest threat to the planet. Such a planet-wide disaster need not result in human extinction if the human race were to be able to colonise additional planets before the disaster. Hawking viewed spaceflight and the colonisation of space as necessary for the future of humanity.\n\nHawking stated that, given the vastness of the universe, aliens likely exist, but that contact with them should be avoided. He warned that aliens might pillage Earth for resources. In 2010 he said, \"If aliens visit us, the outcome would be much as when Columbus landed in America, which didn't turn out well for the Native Americans.\"\n\nHawking warned that superintelligent artificial intelligence could be pivotal in steering humanity's fate, stating that \"the potential benefits are huge... Success in creating AI would be the biggest event in human history. It might also be the last, unless we learn how to avoid the risks.\" However, he argued that we should be more frightened of capitalism exacerbating economic inequality than robots.\n\nHawking was concerned about the future emergence of a race of “superhumans” that would be able to design their own evolution and, as well, argued that computer viruses in today's world should be considered a new form of life, stating that \"maybe it says something about human nature, that the only form of life we have created so far is purely destructive. Talk about creating life in our own image.\"\n\nAt Google's Zeitgeist Conference in 2011, Hawking said that \"philosophy is dead\". He believed that philosophers \"have not kept up with modern developments in science\" and that scientists \"have become the bearers of the torch of discovery in our quest for knowledge\". He said that philosophical problems can be answered by science, particularly new scientific theories which \"lead us to a new and very different picture of the universe and our place in it\".\n\nHawking was an atheist and believed that \"the universe is governed by the laws of science\". He stated: \"There is a fundamental difference between religion, which is based on authority, [and] science, which is based on observation and reason. Science will win because it works.\" In an interview published in \"The Guardian\", Hawking regarded \"the brain as a computer which will stop working when its components fail\", and the concept of an afterlife as a \"fairy story for people afraid of the dark\". In 2011, narrating the first episode of the American television series \"Curiosity\" on the Discovery Channel, Hawking declared:\n\nIn September 2014, he joined the Starmus Festival as keynote speaker and declared himself an atheist. In an interview with \"El Mundo\", he said:\n\nIn addition, Hawking has stated:\nHawking was a longstanding Labour Party supporter. He recorded a tribute for the 2000 Democratic presidential candidate Al Gore, called the 2003 invasion of Iraq a \"war crime\", supported the academic boycott of Israel, campaigned for nuclear disarmament, and supported stem cell research, universal health care, and action to prevent climate change. In August 2014, Hawking was one of 200 public figures who were signatories to a letter to \"The Guardian\" expressing their hope that Scotland would vote to remain part of the United Kingdom in September's referendum on that issue. Hawking believed a United Kingdom withdrawal from the European Union (Brexit) would damage the UK's contribution to science as modern research needs international collaboration, and that free movement of people in Europe encourages the spread of ideas. Hawking was disappointed by Brexit and warned against envy and isolationism.\n\nHawking was greatly concerned over health care, and maintained that without the UK National Health Service, he could not have survived into his 70s. He stated, \"I have received excellent medical attention in Britain, and I felt it was important to set the record straight. I believe in universal health care. And I am not afraid to say so.\"\n\nHawking feared privatisation. He stated, \"The more profit is extracted from the system, the more private monopolies grow and the more expensive healthcare becomes. The NHS must be preserved from commercial interests and protected from those who want to privatise it.\" Hawking alleged ministers damaged the NHS, he blamed the Conservatives for cutting funding, weakening the NHS by privatisation, lowering staff morale through holding pay back and reducing social care. Hawking accused Jeremy Hunt of cherry picking evidence which Hawking maintained debased science.<ref name=\"bbc20/8/2017\"></ref> Hawking also stated, \"There is overwhelming evidence that NHS funding and the numbers of doctors and nurses are inadequate, and it is getting worse.\"\n\nIn June 2017, Hawking endorsed the Labour Party in the 2017 UK general election, citing the Conservatives' proposed cuts to the NHS. But he was also critical of Labour leader Jeremy Corbyn, expressing scepticism over whether the party could win a general election under him.\n\nHawking feared Donald Trump's policies on global warming could endanger the planet and make global warming irreversible. He said, \"Climate change is one of the great dangers we face, and it's one we can prevent if we act now. By denying the evidence for climate change, and pulling out of the Paris Agreement, Donald Trump will cause avoidable environmental damage to our beautiful planet, endangering the natural world, for us and our children.\" Hawking further stated that this could lead Earth \"to become like Venus, with a temperature of two hundred and fifty degrees, and raining sulphuric acid\".\n\nIn 1988, Stephen Hawking, Arthur C. Clarke and Carl Sagan were interviewed in \"God, the Universe and Everything Else\". They discussed the Big Bang theory, God and the possibility of extraterrestrial life.\n\nAt the release party for the home video version of the \"A Brief History of Time\", Leonard Nimoy, who had played Spock on \"Star Trek\", learned that Hawking was interested in appearing on the show. Nimoy made the necessary contact, and Hawking played a holographic simulation of himself in an episode of \"\" in 1993. The same year, his synthesizer voice was recorded for the Pink Floyd song \"Keep Talking\", and in 1999 for an appearance on \"The Simpsons\". Hawking appeared in documentaries titled \"The Real Stephen Hawking\" (2001), \"Stephen Hawking: Profile\" (2002) and \"Hawking\" (2013), and the documentary series \"Stephen Hawking, Master of the Universe\" (2008). Hawking also guest-starred in \"Futurama\" and \"The Big Bang Theory\".\n\nHawking allowed the use of his copyrighted voice in the biographical 2014 film \"The Theory of Everything\", in which he was portrayed by Eddie Redmayne in an Academy Award-winning role. Hawking was featured at the \"Monty Python Live (Mostly)\" in 2014. He was shown to sing an extended version of the Galaxy Song, after running down Brian Cox with his wheelchair, in a pre-recorded video.\n\nHawking used his fame to advertise products, including a wheelchair, National Savings, British Telecom, Specsavers, Egg Banking, and Go Compare. In 2015 he applied to trademark his name.\n\nBroadcast in March 2018 just a week or two before his death, Hawking was the voice of The Book Mark II on \"The Hitchhiker's Guide to the Galaxy\" radio series, and he was the guest of Neil deGrasse Tyson on \"StarTalk\".\n\nHawking received numerous awards and honours. Already early in the list, in 1974 he was elected a Fellow of the Royal Society (FRS). At that time, his nomination read:\n\nThe citation continues, \"Other important work by Hawking relates to the interpretation of cosmological observations and to the design of gravitational wave detectors.\"\n\nHawking received the 2015 BBVA Foundation Frontiers of Knowledge Award in Basic Sciences shared with Viatcheslav Mukhanov for discovering that the galaxies were formed from quantum fluctuations in the early Universe. At the 2016 Pride of Britain Awards, Hawking received the lifetime achievement award \"for his contribution to science and British culture\". After receiving the award from Prime Minister Theresa May, Hawking humorously requested that she not seek his help with Brexit.\n\nHawking was a member of the Advisory Board of the Starmus Festival, and had a major role in acknowledging and promoting science communication. The Stephen Hawking Medal for Science Communication is an annual award initiated in 2016 to honour members of the arts community for contributions that help build awareness of science. Recipients receive a medal bearing a portrait of Stephen Hawking by Alexei Leonov, and the other side represents an image of Leonov himself performing his famous space walk and the iconic \"Red Special\", Brian May's guitar.\n\nThe Starmus III Festival in 2016 was a tribute to Stephen Hawking and the book of all Starmus III lectures, \"Beyond the Horizon\", was also dedicated to him. The first recipients of the medals, which were awarded at the festival, were chosen by Hawking himself. They were composer Hans Zimmer, physicist Jim Al-Khalili, and the science documentary \"Particle Fever\".\n\nIn March 2018, it was announced that two Russian astronomers who discovered GRB180316A, a newborn black hole in the Ophiuchus constellation on 16 March 2018, had dedicated their find to Stephen Hawking, having discovered it two days after his death.\n\n\n\n\nCo-written with his daughter Lucy.\n\n\n\n\n"}
{"id": "40294", "url": "https://en.wikipedia.org/wiki?curid=40294", "title": "Stephen Smale", "text": "Stephen Smale\n\nStephen Smale (born July 15, 1930) is an American mathematician from Flint, Michigan. His research concerns topology, dynamical systems and mathematical economics. He was awarded the Fields Medal in 1966 and spent more than three decades on the mathematics faculty of the University of California, Berkeley (1960–1961 and 1964–1995).\n\nSmale entered the University of Michigan in 1948. Initially, he was a good student, placing into an honors calculus sequence taught by Bob Thrall and earning himself A's. However, his sophomore and junior years were marred with mediocre grades, mostly Bs, Cs and even an F in nuclear physics. However, with some luck, Smale was accepted as a graduate student at the University of Michigan's mathematics department. Yet again, Smale performed poorly in his first years, earning a C average as a graduate student. It was only when the department chair, Hildebrandt, threatened to kick Smale out that he began to work hard. Smale finally earned his Ph.D. in 1957, under Raoul Bott.\n\nSmale began his career as an instructor at the college at the University of Chicago. In 1958, he astounded the mathematical world with a proof of a sphere eversion. He then cemented his reputation with a proof of the Poincaré conjecture for all dimensions greater than or equal to 5, published in 1961; in 1962 he generalized the ideas in a 107-page paper that established the h-cobordism theorem.\n\nAfter having made great strides in topology, he then turned to the study of dynamical systems, where he made significant advances as well. His first contribution is the Smale horseshoe that started significant research in dynamical systems. He also outlined a research program carried out by many others. Smale is also known for injecting Morse theory into mathematical economics, as well as recent explorations of various theories of computation.\n\nIn 1998 he compiled a list of 18 problems in mathematics to be solved in the 21st century, known as Smale's problems. This list was compiled in the spirit of Hilbert's famous list of problems produced in 1900. In fact, Smale's list contains some of the original Hilbert problems, including the Riemann hypothesis and the second half of Hilbert's sixteenth problem, both of which are still unsolved. Other famous problems on his list include the Poincaré conjecture (now a theorem, proved by Grigori Perelman), the P = NP problem, and the Navier–Stokes equations, all of which have been designated Millennium Prize Problems by the Clay Mathematics Institute.\n\nEarlier in his career, Smale was involved in controversy over remarks he made regarding his work habits while proving the higher-dimensional Poincaré conjecture. He said that his best work had been done \"on the beaches of Rio\". This led to the withholding of his grant money from the NSF. He has been politically active in various movements in the past, such as the Free Speech movement and the movement against the Vietnam War. At one time he was subpoenaed by the House Un-American Activities Committee.\n\nIn 1960 Smale was appointed an associate professor of mathematics at the University of California, Berkeley, moving to a professorship at Columbia University the following year. In 1964 he returned to a professorship at UC Berkeley where he has spent the main part of his career. He retired from UC Berkeley in 1995 and took up a post as professor at the City University of Hong Kong. He also amassed over the years one of the finest private mineral collections in existence. Many of Smale's mineral specimens can be seen in the book—\"The Smale Collection: Beauty in Natural Crystals\".\n\nSince 2002 Smale is a Professor at the Toyota Technological Institute at Chicago; starting August 1, 2009, he is also a Distinguished University Professor at the City University of Hong Kong.\n\nIn 2007, Smale was awarded the Wolf Prize in mathematics.\n\n\n\n\n"}
{"id": "10388845", "url": "https://en.wikipedia.org/wiki?curid=10388845", "title": "Stream X-Machine", "text": "Stream X-Machine\n\nThe Stream X-machine (SXM) is a model of computation introduced by Gilbert Laycock in his 1993 PhD thesis, \"The Theory and Practice of Specification Based Software Testing\".\nBased on Samuel Eilenberg's X-machine, an extended finite state machine for processing data of the type \"X\", the Stream X-Machine is a kind of X-machine for processing a memory data type \"Mem\" with associated input and output streams \"In\"* and \"Out\"*, that is, where \"X\" = \"Out\"* × \"Mem\" × \"In\"*. The transitions of a Stream X-Machine are labelled by functions of the form φ: \"Mem\" × \"In\" → \"Out\" × \"Mem\", that is, which compute an output value and update the memory, from the current memory and an input value.\n\nAlthough the general X-machine had been identified in the 1980s as a potentially useful formal model for specifying software systems, it was not until the emergence of the Stream X-Machine that this idea could be fully exploited. Florentin Ipate and Mike Holcombe went on to develop a theory of \"complete\" functional testing, in which complex software systems with hundreds of thousands of states and millions of transitions could be decomposed into separate SXMs that could be tested exhaustively, with a guaranteed proof of correct integration.\n\nBecause of the intuitive interpretation of Stream X-Machines as \"processing agents with inputs and outputs\", they have attracted increasing interest, because of their utility in modelling real-world phenomena. The SXM model has important applications in fields as diverse as computational biology, software testing and agent-based computational economics.\n\nA Stream X-Machine (SXM) is an extended finite state machine with auxiliary memory, inputs and outputs. It is a variant of the general X-machine, in which the fundamental data type \"X\" = \"Out\"* × \"Mem\" × \"In\"*, that is, a tuple consisting of an output stream, the memory and an input stream. A SXM separates the \"control flow\" of a system from the \"processing\" carried out by the system. The control is modelled by a finite state machine (known as the \"associated automaton\") whose transitions are labelled with processing functions chosen from a set Φ (known as the \"type\" of the machine), which act upon the fundamental data type.\n\nEach processing function in Φ is a partial function, and can be considered to have the type φ: \"Mem\" × \"In\" → \"Out\" × \"Mem\", where \"Mem\" is the memory type, and \"In\" and \"Out\" are respectively the input and output types. In any given state, a transition is \"enabled\" if the domain of the associated function φ includes the next input value and the current memory state. If (at most) one transition is enabled in a given state, the machine is \"deterministic\". Crossing a transition is equivalent to applying the associated function φ, which consumes one input, possibly modifies the memory and produces one output. Each recognised path through the machine therefore generates a list φ ... φ of functions, and the SXM composes these functions together to generate a relation on the fundamental data type |φ ... φ|: \"X\" → \"X\".\n\nThe Stream X-Machine is a variant of X-machine in which the fundamental data type \"X\" = \"Out\"* × \"Mem\" × \"In\"*. In the original X-machine, the φ are general \"relations\" on \"X\". In the Stream X-Machine, these are usually restricted to \"functions\"; however the SXM is still only deterministic if (at most) one transition is enabled in each state.\n\nA general X-machine handles input and output using a prior encoding function α: \"Y\" → \"X\" for input, and a posterior decoding function β: \"X\" → \"Z\" for output, where \"Y\" and \"Z\" are respectively the input and output types. In a Stream X-Machine, these types are streams:\n\nand the encoding and decoding functions are defined as:\n\nwhere \"ins: In\"*, \"outs: Out\"* and \"mem\": \"Mem\". In other words, the machine is initialized with the whole of the input stream; and the decoded result is the whole of the output stream, provided the input stream is eventually consumed (otherwise the result is undefined).\n\nEach processing function in a SXM is given the abbreviated type φ: \"Mem\" × \"In\" → \"Out\" × \"Mem\". This can be mapped onto a general X-machine relation of the type φ: X → X if we treat this as computing:\n\nwhere codice_1 denotes concatenation of an element and a sequence. In other words, the relation extracts the head of the input stream, modifies memory and appends a value to the tail of the output stream.\n\nBecause of the above equivalence, attention may focus on the way a Stream X-Machine processes inputs into outputs, using an auxiliary memory. Given an initial memory state \"mem\" and an input stream \"ins\", the machine executes in a step-wise fashion, consuming one input at a time, and generating one output at a time. Provided that (at least) one recognised path \"path\" = φ ... φ exists leading to a state in which the input has been consumed, the machine yields a final memory state \"mem\" and an output stream \"outs\". In general, we can think of this as the relation computed by all recognised paths: | \"path\" | : \"In\"* → \"Out\"*. This is often called the \"behaviour\" of the Stream X-Machine.\n\nThe behaviour is deterministic, if (at most) one transition is enabled in each state. This property, and the ability to control how the machine behaves in a step-wise fashion in response to inputs and memory, makes it an ideal model for the specification of software systems. If the specification and implementation are both assumed to be Stream X-Machines, then the implementation may be tested for conformance to the specification machine, by observing the inputs and outputs at each step. Laycock first highlighted the utility of single-step processing with observations for testing purposes.\n\nHolcombe and Ipate developed this into a practical theory of software testing which was fully compositional, scaling up to very large systems. A proof of correct integration guarantees that testing each component and each integration layer separately corresponds to testing the whole system. This divide-and-conquer approach makes \"exhaustive\" testing feasible for large systems.\n\nThe testing method is described in a separate article on the Stream X-Machine testing methodology.\n\nStream X-Machines have been used in a number of different application areas.\n\n\n"}
{"id": "54455799", "url": "https://en.wikipedia.org/wiki?curid=54455799", "title": "Stronger uncertainty relations", "text": "Stronger uncertainty relations\n\nHeisenberg's uncertainty relation is one of the fundamental results in quantum mechanics. Later Robertson proved the uncertainty relation for two general non-commuting observables, which was strengthened by Schrödinger. However, the conventional uncertainty relation like the Roberstson-Schrödinger relation cannot give a non-trivial bound for the product of variances of two incompatible observables because the lower bound in the uncertainty inequalities can be null and hence trivial even for observables that are incompatible on the state of the system. The Heisenberg–Robertson–Schrödinger uncertainty relation was proved at the dawn of quantum formalism and is ever-present in the teaching and research on quantum mechanics. After about 85 years of existence of the uncertainty relation this problem was solved recently by Lorenzo Maccone and Arun K. Pati.\nThe standard uncertainty relations are expressed in terms of the product of variances of the measurement results of the observables formula_1 and formula_2, and the product can be null even when one of the two variances is different from zero. However, the stronger uncertainty relations due to Maccone and Pati provide different uncertainty relations, based on the sum of variances that are guaranteed to be nontrivial whenever the observables are incompatible on the state of the quantum system.\n\nThe Heisenberg–Robertson or Schrödinger uncertainty relations do not fully capture the incompatibility of observables in a given quantum state. The stronger uncertainty relations give non-trivial bounds on the sum of the variances for two incompatible observables. For two non-commuting observables formula_1 and formula_2 the first stronger uncertainty relation is given by \n\nwhere formula_6, formula_7, formula_8 is a vector that is orthogonal to the state of the system, i.e., formula_9 and \none should chose the sign of formula_10 so that this is a positive number.\n\nThe other non-trivial stronger uncertainty relation is given by\n\nwhere formula_12 is a unit vector orthogonal to formula_13.\nThe form of formula_14 implies that the right-hand side of the new uncertainty relation \nis nonzero unless formula_15 is an eigenstate of formula_16.\n\nOne can prove an improved version of the Heisenberg–Robertson uncertainty relation which reads as\n\nThe Heisenberg–Robertson uncertainty relation follows from the above uncertainty relation.\nIn quantum theory one should distinguish between the uncertainty relation and the uncertainty principle. The former refers solely to the preparation of the system which induces a spread in the measurement outcomes, and does not refer to the disturbance induced by the measurement. The uncertainty principle captures the measurement disturbance by the apparatus and the impossibility of joint measurements of incompatible observables. The Maccone–Pati uncertainty relations refer to preparation uncertainty relations. These relations set strong limitations for the nonexistence of common eigenstates for incompatible observables. The Maccone–Pati uncertainty relations have been experimentally tested for qutrit systems.\nIt is a curious fact that a basic result such as the Heisenberg–Robertson uncertainty relation within quantum mechanics has an important problem which, although probably being for long noticed by several teachers and researchers in the area, was solved so much time after its conception.\nThe new uncertainty relations may be far ranging because they not only capture the incompatibility of observables but also of quantities that are physically measurable (as variances can be measured in the experiment). Since the uncertainty relations have fundamental significance in quantum physics, this result may find a wide range of applications including quantum cryptography, quantum entanglement, quantum computation and physics in general.\n\n"}
{"id": "12698305", "url": "https://en.wikipedia.org/wiki?curid=12698305", "title": "Thomas Starkie", "text": "Thomas Starkie\n\nThomas Starkie (2 January 1782 – 15 April 1849) was an English lawyer and jurist. A talented mathematician in his youth, he especially contributed to the unsuccessful attempts to codify the English criminal law in the nineteenth century.\n\nBorn in Blackburn, Lancashire, Thomas was the eldest son of the Rev. Thomas Starkie, vicar of Blackburn, and his wife, Ann \"née\" Yatman. He was educated at Clitheroe Royal Grammar School and St John's College, Cambridge, from where he graduated in 1803 as senior wrangler and first Smith's prizeman. In the same year, he became a Fellow of St Catharine's College, Cambridge. In 1812 he married Lucy, eldest daughter of Rev. Thomas Dunham Whitaker which entailed that he resign his fellowship. The couple went on to parent five children.\n\nStarkie entered Lincoln's Inn as a pupil of Joseph Chitty and was called to the bar in 1810, proceeding to practise as a special pleader as well as on the northern circuit, and becoming a QC.\n\nIn 1823 he became Downing Professor of law at Cambridge though he had little success in attracting pupils with his poor presentations, a fate shared with his contemporary John Austin. He repeated his failure at the Inner Temple in 1833. However, in 1833, Starkie was appointed to the royal commission on a proposed English Criminal Code and spent the rest of his life on various commissions on reform and codification of the criminal law. He was not always popular with his colleagues, Henry Bellenden Ker calling him \"childish\" and \"desultory and wayward\".\n\nHe was also a sometime law reporter and author of the influential texts: \"A Practical Treatise on the Law of Slander, Libel, and Incidentally of Malicious Prosecutions\" (1812) and \"A Practical Treatise on the Law of Evidence\" (1824). In 1847, Starkie became a judge in the Clerkenwell small-debts court.\n\nHe died in his rooms in Downing College, Cambridge.\n\nStarkie's instincts were Tory and he opposed the Catholic Relief Act 1829. However, in 1840 he unsuccessfully stood as a Liberal Party candidate in Cambridge.\n\n"}
{"id": "12700262", "url": "https://en.wikipedia.org/wiki?curid=12700262", "title": "Toroidal and poloidal", "text": "Toroidal and poloidal\n\nThe earliest use of these terms cited by the Oxford English Dictionary (OED) is by Walter M. Elsasser (1946) in the context of the generation of the Earth's magnetic field by currents in the core, with \"toroidal\" being parallel to lines of latitude and \"poloidal\" being in the direction of the magnetic field (i.e. towards the poles).\n\nThe OED also records the later usage of these terms in the context of toroidally confined plasmas, as encountered in magnetic confinement fusion. In the plasma context, the toroidal direction is the long way around the torus, the corresponding coordinate being denoted by \"z\" in the slab approximation or formula_1 or formula_2 in magnetic coordinates; the poloidal direction is the short way around the torus, the corresponding coordinate being denoted by \"y\" in the slab approximation or formula_3 in magnetic coordinates. (The third direction, normal to the magnetic surfaces, is often called the \"radial direction\", denoted by \"x\" in the slab approximation and variously formula_4, formula_5, \"r\", formula_6, or \"s\" in magnetic coordinates.)\n\nAs a simple example from the physics of magnetically confined plasmas, consider an axisymmetric system with circular, concentric magnetic flux surfaces of radius formula_7 (a crude approximation to the magnetic field geometry in an early [Tokamak] but topologically equivalent to any toroidal magnetic confinement system with nested flux surfaces) and denote the toroidal angle by formula_1 and the poloidal angle by formula_3.\nThen the Toroidal/Poloidal coordinate system relates to standard Cartesian Coordinates by these transformation rules:\n\nwhere formula_13.\n\nThe natural choice geometrically is to take formula_14, giving the toroidal and poloidal directions shown by the arrows in the figure above, but this makes formula_15 a left-handed curvilinear coordinate system. As it is usually assumed in setting up \"flux coordinates\" for describing magnetically confined plasmas that the set formula_15 forms a \"right\"-handed coordinate system, formula_17, we must either reverse the poloidal direction by taking formula_18, or reverse the toroidal direction by taking formula_19. Both choices are used in the literature.\n\nTo study single particle motion in toroidally confined plasma devices, velocity and acceleration vectors must be known. Considering the natural choice formula_14, the unit vectors of toroidal and poloidal coordinates system formula_21 can be expressed as:\n\naccording to Cartesian coordinates. The position vector is expressed as:\n\nThe velocity vector is then given by:\n\nand the acceleration vector is:\n\n\n"}
{"id": "199701", "url": "https://en.wikipedia.org/wiki?curid=199701", "title": "Type system", "text": "Type system\n\nIn programming languages, a type system is a set of rules that assigns a property called type to the various constructs of a computer program, such as variables, expressions, functions or modules. These types formalize and enforce the otherwise implicit categories the programmer uses for algebraic data types, data structures, or other components (e.g. \"string\", \"array of float\", \"function returning boolean\"). The main purpose of a type system is to reduce possibilities for bugs in computer programs by defining interfaces between different parts of a computer program, and then checking that the parts have been connected in a consistent way. This checking can happen statically (at compile time), dynamically (at run time), or as a combination of static and dynamic checking. Type systems have other purposes as well, such as expressing business rules, enabling certain compiler optimizations, allowing for multiple dispatch, providing a form of documentation, etc.\n\nA type system associates a type with each computed value and, by examining the flow of these values, attempts to ensure or prove that no type errors can occur. The given type system in question determines exactly what constitutes a type error, but in general the aim is to prevent operations expecting a certain kind of value from being used with values for which that operation does not make sense (logic errors). Type systems are often specified as part of programming languages, and built into the interpreters and compilers for them; although the type system of a language can be extended by optional tools that perform added kinds of checks using the language's original type syntax and grammar.\n\nAn example of a simple type system is that of the C language. The portions of a C program are the function definitions. One function is invoked by another function. The interface of a function states the name of the function and a list of values that are passed to the function's code. The code of an invoking function states the name of the invoked, along with the names of variables that hold values to pass to it. During execution, the values are placed into temporary storage, then execution jumps to the code of the invoked function. The invoked function's code accesses the values and makes use of them. If the instructions inside the function are written with the assumption of receiving an integer value, but the calling code passed a floating-point value, then the wrong result will be computed by the invoked function. The C compiler checks the type declared for each variable sent, against the type declared for each variable in the interface of the invoked function. If the types do not match, the compiler throws a compile-time error.\n\nA compiler may also use the static type of a value to optimize the storage it needs and the choice of algorithms for operations on the value. In many C compilers the \"float\" data type, for example, is represented in 32 bits, in accord with the IEEE specification for single-precision floating point numbers. They will thus use floating-point-specific microprocessor operations on those values (floating-point addition, multiplication, etc.).\n\nThe depth of type constraints and the manner of their evaluation affect the \"typing\" of the language. A programming language may further associate an operation with various resolutions for each type, in the case of type polymorphism. Type theory is the study of type systems. The concrete types of some programming languages, such as integers and strings, depend on practical issues of computer architecture, compiler implementation, and language design.\n\nFormally, type theory studies type systems. A programming language must have occurrence to type check using the \"type system\" whether at compile time or runtime, manually annotated or automatically inferred. As Mark Manasse concisely put it:\nAssigning a data type, termed \"typing\", gives meaning to a sequence of bits such as a value in memory or some object such as a variable. The hardware of a general purpose computer is unable to discriminate between for example a memory address and an instruction code, or between a character, an integer, or a floating-point number, because it makes no intrinsic distinction between any of the possible values that a sequence of bits might \"mean\". Associating a sequence of bits with a type conveys that meaning to the programmable hardware to form a \"symbolic system\" composed of that hardware and some program.\n\nA program associates each value with at least one specific type, but it also can occur that one value is associated with many subtypes. Other entities, such as objects, modules, communication channels, and dependencies can become associated with a type. Even a type can become associated with a type. An implementation of a \"type system\" could in theory associate identifications called \"data type\" (a type of a value), \"class\" (a type of an object), and \"kind\" (a \"type of a type\", or metatype). These are the abstractions that typing can go through, on a hierarchy of levels contained in a system.\n\nWhen a programming language evolves a more elaborate type system, it gains a more finely grained rule set than basic type checking, but this comes at a price when the type inferences (and other properties) become undecidable, and when more attention must be paid by the programmer to annotate code or to consider computer-related operations and functioning. It is challenging to find a sufficiently expressive type system that satisfies all programming practices in a type safe manner.\n\nThe more type restrictions that are imposed by the compiler, the more \"strongly typed\" a programming language is. Strongly typed languages often require the programmer to make explicit conversions in contexts where an implicit conversion would cause no harm. Pascal's type system has been described as \"too strong\" because, for example, the size of an array or string is part of its type, making some programming tasks difficult. Haskell is also strongly typed but its types are automatically inferred so that explicit conversions are often (but not always) unnecessary.\n\nA programming language compiler can also implement a \"dependent type\" or an \"effect system\", which enables even more program specifications to be verified by a type checker. Beyond simple value-type pairs, a virtual \"region\" of code is associated with an \"effect\" component describing \"what\" is being done \"with what\", and enabling for example to \"throw\" an error report. Thus the symbolic system may be a \"type and effect system\", which endows it with more safety checking than type checking alone.\nWhether automated by the compiler or specified by a programmer, a type system makes program behavior illegal if outside the type-system rules. Advantages provided by programmer-specified type systems include:\n\nAdvantages provided by compiler-specified type systems include:\n\nA type error is an unintended condition which might manifest in multiple stages of a program's development. Thus a facility for detection of the error is needed in the type system. In some languages, such as Haskell, for which type inference is automated, lint might be available to its compiler to aid in the detection of error.\n\nType safety contributes to program correctness, but might only guarantee correctness at the cost of making the type checking itself an undecidable problem. In a \"type system\" with automated type checking a program may prove to run incorrectly yet be safely typed, and produce no compiler errors. Division by zero is an unsafe and incorrect operation, but a type checker running at compile time only does not scan for division by zero in most languages, and then it is left as a runtime error. To prove the absence of these more-general-than-types defects, other kinds of formal methods, collectively known as program analyses, are in common use. Alternatively, a sufficiently expressive type system, such as in dependently typed languages, can prevent these kinds of errors (for example, expressing \"the type of non-zero numbers\"). In addition software testing is an empirical method for finding errors that the type checker cannot detect.\n\nThe process of verifying and enforcing the constraints of types—\"type checking\"—may occur either at compile-time (a static check) or at run-time. If a language specification requires its typing rules strongly (i.e., more or less allowing only those automatic type conversions that do not lose information), one can refer to the process as \"strongly typed\", if not, as \"weakly typed\". The terms are not usually used in a strict sense.\n\nStatic type checking is the process of verifying the type safety of a program based on analysis of a program's text (source code). If a program passes a static type checker, then the program is guaranteed to satisfy some set of type safety properties for all possible inputs.\n\nStatic type checking can be considered a limited form of program verification (see type safety), and in a type-safe language, can be considered also an optimization. If a compiler can prove that a program is well-typed, then it does not need to emit dynamic safety checks, allowing the resulting compiled binary to run faster and to be smaller.\n\nStatic type checking for Turing-complete languages is inherently conservative. That is, if a type system is both \"sound\" (meaning that it rejects all incorrect programs) and \"decidable\" (meaning that it is possible to write an algorithm that determines whether a program is well-typed), then it must be \"incomplete\" (meaning there are correct programs, which are also rejected, even though they do not encounter runtime errors). For example, consider a program containing the code:\n\ncodice_2\n\nEven if the expression codice_3 always evaluates to codice_4 at run-time, most type checkers will reject the program as ill-typed, because it is difficult (if not impossible) for a static analyzer to determine that the codice_5 branch will not be taken. Conversely, a static type checker will quickly detect type errors in rarely used code paths. Without static type checking, even code coverage tests with 100% coverage may be unable to find such type errors. The tests may fail to detect such type errors, because the combination of all places where values are created and all places where a certain value is used must be taken into account.\n\nA number of useful and common programming language features cannot be checked statically, such as downcasting. Thus, many languages will have both static and dynamic type checking; the static type checker verifies what it can, and dynamic checks verify the rest.\n\nMany languages with static type checking provide a way to bypass the type checker. Some languages allow programmers to choose between static and dynamic type safety. For example, C# distinguishes between \"statically-typed\" and \"dynamically-typed\" variables. Uses of the former are checked statically, whereas uses of the latter are checked dynamically. Other languages allow writing code that is not type-safe; for example, in C, programmers can freely cast a value between any two types that have the same size, effectively subverting the type concept.\n\nFor a list of languages with static type checking, see .\n\nDynamic type checking is the process of verifying the type safety of a program at runtime. Implementations of dynamically type-checked languages generally associate each runtime object with a \"type tag\" (i.e., a reference to a type) containing its type information. This runtime type information (RTTI) can also be used to implement dynamic dispatch, late binding, downcasting, reflection, and similar features.\n\nMost type-safe languages include some form of dynamic type checking, even if they also have a static type checker. The reason for this is that many useful features or properties are difficult or impossible to verify statically. For example, suppose that a program defines two types, A and B, where B is a subtype of A. If the program tries to convert a value of type A to type B, which is known as downcasting, then the operation is legal only if the value being converted is actually a value of type B. Thus, a dynamic check is needed to verify that the operation is safe. This requirement is one of the criticisms of downcasting.\n\nBy definition, dynamic type checking may cause a program to fail at runtime. In some programming languages, it is possible to anticipate and recover from these failures. In others, type-checking errors are considered fatal.\n\nProgramming languages that include dynamic type checking but not static type checking are often called \"dynamically typed programming languages\". For a list of such languages, see .\n\nSome languages allow both static and dynamic typing (type checking), sometimes called \"soft\" typing. For example, Java and some other ostensibly statically typed languages support downcasting types to their subtypes, querying an object to discover its dynamic type, and other type operations that depend on runtime type information. More generally, most programming languages include mechanisms for dispatching over different 'kinds' of data, such as disjoint unions, subtype polymorphism, and variant types. Even when not interacting with type annotations or type checking, such mechanisms are materially similar to dynamic typing implementations. See programming language for more discussion of the interactions between static and dynamic typing.\n\nObjects in object-oriented languages are usually accessed by a reference whose static target type (or manifest type) is equal to either the object's run-time type (its latent type) or a supertype thereof. This is conformant with the Liskov substitution principle, which states that all operations performed on an instance of a given type can also be performed on an instance of a subtype. This concept is also known as subsumption. In some languages subtypes may also possess covariant or contravariant return types and argument types respectively.\n\nCertain languages, for example Clojure, Common Lisp, or Cython are dynamically type-checked by default, but allow programs to opt into static type checking by providing optional annotations. One reason to use such hints would be to optimize the performance of critical sections of a program. This is formalized by gradual typing. The programming environment \"DrRacket\", a pedagogic environment based on Lisp, and a precursor of the language Racket was also soft-typed.\n\nConversely, as of version 4.0, the C# language provides a way to indicate that a variable should not be statically type-checked. A variable whose type is codice_6 will not be subject to static type checking. Instead, the program relies on runtime type information to determine how the variable may be used.\n\nThe choice between static and dynamic typing requires certain trade-offs.\n\nStatic typing can find type errors reliably at compile time, which should increase the reliability of the delivered program. However, programmers disagree over how commonly type errors occur, resulting in further disagreements over the proportion of those bugs that are coded that would be caught by appropriately representing the designed types in code. Static typing advocates believe programs are more reliable when they have been well type-checked, whereas dynamic-typing advocates point to distributed code that has proven reliable and to small bug databases. The value of static typing, then, presumably increases as the strength of the type system is increased. Advocates of dependent typing, implemented in languages such as Dependent ML and Epigram, have suggested that almost all bugs can be considered type errors, if the types used in a program are properly declared by the programmer or correctly inferred by the compiler.\n\nStatic typing usually results in compiled code that executes faster. When the compiler knows the exact data types that are in use (which is necessary for static verification, either through declaration or inference) it can produce optimized machine code. Some dynamically typed languages such as Common Lisp allow optional type declarations for optimization for this reason.\n\nBy contrast, dynamic typing may allow compilers to run faster and interpreters to dynamically load new code, because changes to source code in dynamically typed languages may result in less checking to perform and less code to revisit. This too may reduce the edit-compile-test-debug cycle.\n\nStatically typed languages that lack type inference (such as C and Java prior to version 10) require that programmers declare the types that a method or function must use. This can serve as added program documentation, that is active and dynamic, instead of static. This allows a compiler to prevent it from drifting out of synchrony, and from being ignored by programmers. However, a language can be statically typed without requiring type declarations (examples include Haskell, Scala, OCaml, F#, and to a lesser extent C# and C++), so explicit type declaration is not a necessary requirement for static typing in all languages.\n\nDynamic typing allows constructs that some static type checking would reject as illegal. For example, \"eval\" functions, which execute arbitrary data as code, become possible. An \"eval\" function is possible with static typing, but requires advanced uses of algebraic data types. Further, dynamic typing better accommodates transitional code and prototyping, such as allowing a placeholder data structure (mock object) to be transparently used in place of a full data structure (usually for the purposes of experimentation and testing).\n\nDynamic typing typically allows duck typing (which enables easier code reuse). Many languages with static typing also feature duck typing or other mechanisms like generic programming that also enable easier code reuse.\n\nDynamic typing typically makes metaprogramming easier to use. For example, C++ templates are typically more cumbersome to write than the equivalent Ruby or Python code since C++ has stronger rules regarding type definitions (for both functions and variables). This forces a developer to write more boilerplate code for a template than a Python developer would need to. More advanced run-time constructs such as metaclasses and introspection are often harder to use in statically typed languages. In some languages, such features may also be used e.g. to generate new types and behaviors on the fly, based on run-time data. Such advanced constructs are often provided by dynamic programming languages; many of these are dynamically typed, although \"dynamic typing\" need not be related to \"dynamic programming languages\".\n\nLanguages are often colloquially referred to as \"strongly typed\" or \"weakly typed\". In fact, there is no universally accepted definition of what these terms mean. In general, there are more precise terms to represent the differences between type systems that lead people to call them \"strong\" or \"weak\".\n\nA third way of categorizing the type system of a programming language uses the safety of typed operations and conversions. Computer scientists consider a language \"type-safe\" if it does not allow operations or conversions that violate the rules of the type system.\n\nSome observers use the term \"memory-safe language\" (or just \"safe language\") to describe languages that do not allow programs to access memory that has not been assigned for their use. For example, a memory-safe language will check array bounds, or else statically guarantee (i.e., at compile time before execution) that array accesses out of the array boundaries will cause compile-time and perhaps runtime errors.\n\nConsider the following program of a language that is both type-safe and memory-safe:\n\nIn this example, the variable will have the value 42. Although this may not be what the programmer anticipated, it is a well-defined result. If were a different string, one that could not be converted to a number (e.g. \"Hello World\"), the result would be well-defined as well. Note that a program can be type-safe or memory-safe and still crash on an invalid operation; in fact, if a program encounters an operation that is not type-safe, terminating the program is often the only option.\n\nNow consider a similar example in C:\n\nIn this example will point to a memory address five characters beyond , equivalent to three characters after the terminating zero character of the string pointed to by . This is memory that the program is not expected to access. It may contain garbage data, and it certainly doesn't contain anything useful. As this example shows, C is neither a memory-safe nor a type-safe language.\n\nIn general, type-safety and memory-safety go hand in hand. For example, a language that supports pointer arithmetic and number-to-pointer conversions (like C) is neither memory-safe nor type-safe, because it allows arbitrary memory to be accessed as if it were valid memory of any type.\n\nFor more information, see memory safety.\n\nSome languages allow different levels of checking to apply to different regions of code. Examples include:\n\nAdditional tools such as lint and IBM Rational Purify can also be used to achieve a higher level of strictness.\n\nIt has been proposed, chiefly by Gilad Bracha, that the choice of type system be made independent of choice of language; that a type system should be a module that can be \"plugged\" into a language as needed. He believes this is advantageous, because what he calls mandatory type systems make languages less expressive and code more fragile. The requirement that types do not affect the semantics of the language is difficult to fulfill.\n\nOptional typing is related to gradual typing, but still distinct from it.\n\nThe term \"polymorphism\" refers to the ability of code (especially, functions or classes) to act on values of multiple types, or to the ability of different instances of the same data structure to contain elements of different types. Type systems that allow polymorphism generally do so in order to improve the potential for code re-use: in a language with polymorphism, programmers need only implement a data structure such as a list or an associative array once, rather than once for each type of element with which they plan to use it. For this reason computer scientists sometimes call the use of certain forms of polymorphism \"generic programming\". The type-theoretic foundations of polymorphism are closely related to those of abstraction, modularity and (in some cases) subtyping.\n\nIn \"duck typing\", a statement calling a method codice_11 on an object does not rely on the declared type of the object; only that the object, of whatever type, must supply an implementation of the method called, when called, at run-time.\n\nDuck typing differs from structural typing in that, if the \"part\" (of the whole module structure) needed for a given local computation is present \"at runtime\", the duck type system is satisfied in its type identity analysis. On the other hand, a structural type system would require the analysis of the whole module structure at compile time to determine type identity or type dependence.\n\nDuck typing differs from a nominative type system in a number of aspects. The most prominent ones are that for duck typing, type information is determined at runtime (as contrasted to compile time), and the name of the type is irrelevant to determine type identity or type dependence; only partial structure information is required for that for a given point in the program execution.\n\nDuck typing uses the premise that (referring to a value) \"if it walks like a duck, and quacks like a duck, then it is a duck\" (this is a reference to the duck test that is attributed to James Whitcomb Riley). The term may have been coined by Alex Martelli in a 2000 message to the comp.lang.python newsgroup (see Python).\n\nWhile one controlled experiment showed an increase in developer productivity for duck typing in single developer projects, other controlled experiments on API usability show the opposite.\n\nMany type systems have been created that are specialized for use in certain environments with certain types of data, or for out-of-band static program analysis. Frequently, these are based on ideas from formal type theory and are only available as part of prototype research systems.\n\nDependent types are based on the idea of using scalars or values to more precisely describe the type of some other value. For example, formula_1 might be the type of a formula_2 matrix. We can then define typing rules such as the following rule for matrix multiplication:\n\nformula_3\n\nwhere formula_4, formula_5, formula_6 are arbitrary positive integer values. A variant of ML called Dependent ML has been created based on this type system, but because type checking for conventional dependent types is undecidable, not all programs using them can be type-checked without some kind of limits. Dependent ML limits the sort of equality it can decide to Presburger arithmetic.\n\nOther languages such as Epigram make the value of all expressions in the language decidable so that type checking can be decidable. However, in general proof of decidability is undecidable, so many programs require hand-written annotations that may be very non-trivial. As this impedes the development process, many language implementations provide an easy way out in the form of an option to disable this condition. This, however, comes at the cost of making the type-checker run in an infinite loop when fed programs that do not type-check, causing the compilation to fail.\n\nLinear types, based on the theory of linear logic, and closely related to uniqueness types, are types assigned to values having the property that they have one and only one reference to them at all times. These are valuable for describing large immutable values such as files, strings, and so on, because any operation that simultaneously destroys a linear object and creates a similar object (such as 'codice_12') can be optimized \"under the hood\" into an in-place mutation. Normally this is not possible, as such mutations could cause side effects on parts of the program holding other references to the object, violating referential transparency. They are also used in the prototype operating system Singularity for interprocess communication, statically ensuring that processes cannot share objects in shared memory in order to prevent race conditions. The Clean language (a Haskell-like language) uses this type system in order to gain a lot of speed (compared to performing a deep copy) while remaining safe.\n\nIntersection types are types describing values that belong to \"both\" of two other given types with overlapping value sets. For example, in most implementations of C the signed char has range -128 to 127 and the unsigned char has range 0 to 255, so the intersection type of these two types would have range 0 to 127. Such an intersection type could be safely passed into functions expecting \"either\" signed or unsigned chars, because it is compatible with both types.\n\nIntersection types are useful for describing overloaded function types: For example, if \" → \" is the type of functions taking an integer argument and returning an integer, and \" → \" is the type of functions taking a float argument and returning a float, then the intersection of these two types can be used to describe functions that do one or the other, based on what type of input they are given. Such a function could be passed into another function expecting an \" → \" function safely; it simply would not use the \" → \" functionality.\n\nIn a subclassing hierarchy, the intersection of a type and an ancestor type (such as its parent) is the most derived type. The intersection of sibling types is empty.\n\nThe Forsythe language includes a general implementation of intersection types. A restricted form is refinement types.\n\nUnion types are types describing values that belong to \"either\" of two types. For example, in C, the signed char has a -128 to 127 range, and the unsigned char has a 0 to 255 range, so the union of these two types would have an overall \"virtual\" range of -128 to 255 that may be used partially depending on which union member is accessed. Any function handling this union type would have to deal with integers in this complete range. More generally, the only valid operations on a union type are operations that are valid on \"both\" types being unioned. C's \"union\" concept is similar to union types, but is not typesafe, as it permits operations that are valid on \"either\" type, rather than \"both\". Union types are important in program analysis, where they are used to represent symbolic values whose exact nature (e.g., value or type) is not known.\n\nIn a subclassing hierarchy, the union of a type and an ancestor type (such as its parent) is the ancestor type. The union of sibling types is a subtype of their common ancestor (that is, all operations permitted on their common ancestor are permitted on the union type, but they may also have other valid operations in common).\n\nExistential types are frequently used in connection with record types to represent modules and abstract data types, due to their ability to separate implementation from interface. For example, the type \"T = ∃X { a: X; f: (X → int); }\" describes a module interface that has a data member named \"a\" of type \"X\" and a function named \"f\" that takes a parameter of the \"same\" type \"X\" and returns an integer. This could be implemented in different ways; for example:\n\n\nThese types are both subtypes of the more general existential type T and correspond to concrete implementation types, so any value of one of these types is a value of type T. Given a value \"t\" of type \"T\", we know that \"t.f(t.a)\" is well-typed, regardless of what the abstract type \"X\" is. This gives flexibility for choosing types suited to a particular implementation while clients that use only values of the interface type—the existential type—are isolated from these choices.\n\nIn general it's impossible for the typechecker to infer which existential type a given module belongs to. In the above example intT { a: int; f: (int → int); } could also have the type ∃X { a: X; f: (int → int); }. The simplest solution is to annotate every module with its intended type, e.g.:\n\n\nAlthough abstract data types and modules had been implemented in programming languages for quite some time, it wasn't until 1988 that John C. Mitchell and Gordon Plotkin established the formal theory under the slogan: \"Abstract [data] types have existential type\". The theory is a second-order typed lambda calculus similar to System F, but with existential instead of universal quantification.\n\nGradual typing is a type system in which variables may be typed either at compile-time (which is static typing) or at run-time (which is dynamic typing), allowing software developers to choose either type paradigm as appropriate, from within a single language. In particular, gradual typing uses a special type named \"dynamic\" to represent statically-unknown types, and gradual typing replaces the notion of type equality with a new relation called \"consistency\" that relates the dynamic type to every other type. The consistency relation is symmetric but not transitive.\n\nMany static type systems, such as those of C and Java, require \"type declarations\": The programmer must explicitly associate each variable with a specific type. Others, such as Haskell's, perform \"type inference\": The compiler draws conclusions about the types of variables based on how programmers use those variables. For example, given a function that adds and together, the compiler can infer that and must be numbers – since addition is only defined for numbers. Thus, any call to elsewhere in the program that specifies a non-numeric type (such as a string or list) as an argument would signal an error.\n\nNumerical and string constants and expressions in code can and often do imply type in a particular context. For example, an expression might imply a type of floating-point, while might imply a list of integers – typically an array.\n\nType inference is in general possible, if it is decidable in the type theory in question. Moreover, even if inference is undecidable in general for a given type theory, inference is often possible for a large subset of real-world programs. Haskell's type system, a version of Hindley–Milner, is a restriction of System Fω to so-called rank-1 polymorphic types, in which type inference is decidable. Most Haskell compilers allow arbitrary-rank polymorphism as an extension, but this makes type inference undecidable. (Type checking is decidable, however, and rank-1 programs still have type inference; higher rank polymorphic programs are rejected unless given explicit type annotations.)\n\nSome languages like Perl 6 or C# have a unified type system. This means that all C# types including primitive types inherit from a single root object. Every type in C# inherits from the Object class. Java has several primitive types that are not objects. Java provides wrapper object types that exist together with the primitive types so developers can use either the wrapper object types or the simpler non-object primitive types.\n\nA type-checker for a statically typed language must verify that the type of any expression is consistent with the type expected by the context in which that expression appears. For example, in an assignment statement of the form codice_13,\nthe inferred type of the expression \"codice_14\" must be consistent with the declared or inferred type of the variable codice_15. This notion of consistency, called \"compatibility\", is specific to each programming language.\n\nIf the type of \"codice_14\" and the type of codice_15 are the same, and assignment is allowed for that type, then this is a valid expression. Thus, in the simplest type systems, the question of whether two types are compatible reduces to that of whether they are \"equal\" (or \"equivalent\"). Different languages, however, have different criteria for when two type expressions are understood to denote the same type. These different \"equational theories\" of types vary widely, two extreme cases being \"structural type systems\", in which any two types that describe values with the same structure are equivalent, and \"nominative type systems\", in which no two syntactically distinct type expressions denote the same type (\"i.e.\", types must have the same \"name\" in order to be equal).\n\nIn languages with subtyping, the compatibility relation is more complex. In particular, if codice_18 is a subtype of codice_19, then a value of type codice_18 can be used in a context where one of type codice_19 is expected, even if the reverse is not true. Like equivalence, the subtype relation is defined differently for each programming language, with many variations possible. The presence of parametric or ad hoc polymorphism in a language may also have implications for type compatibility.\n\n\n\n"}
{"id": "1508507", "url": "https://en.wikipedia.org/wiki?curid=1508507", "title": "Vector projection", "text": "Vector projection\n\nThe vector projection of a vector a on (or onto) a nonzero vector b (also known as the vector component or vector resolution of a in the direction of b) is the orthogonal projection of a onto a straight line parallel to b. It is a vector parallel to b, defined as\n\nwhere formula_2 is a scalar, called the scalar projection of a onto b, and b̂ is the unit vector in the direction of b.\nIn turn, the scalar projection is defined as\n\nwhich is equivalent to either\n\nor\n\nBy definition,\nHence,\n\nThe scalar projection a on b is a scalar which has a negative sign if 90 < \"θ\" ≤ 180 degrees. It coincides with the length |c| of the vector projection if the angle is smaller than 90°. More exactly:\n\n\nThe vector projection of a on b is a vector a which is either null or parallel to b. More exactly:\n\n\nThe vector rejection of a on b is a vector a which is either null or orthogonal to b. More exactly:\n\n\nThe orthogonal projection can be represented by a projection matrix. To project a vector onto the unit vector \"a\" = (\"a, a, a\"), it would need to be multiplied with this projection matrix:\n\nThe vector projection is an important operation in the Gram–Schmidt orthonormalization of vector space bases. It is also used in the Separating axis theorem to detect whether two convex shapes intersect.\n\nSince the notions of vector length and angle between vectors can be generalized to any \"n\"-dimensional inner product space, this is also true for the notions of orthogonal projection of a vector, projection of a vector onto another, and rejection of a vector from another. In some cases, the inner product coincides with the dot product. Whenever they don't coincide, the inner product is used instead of the dot product in the formal definitions of projection and rejection.\n\nFor a three-dimensional inner product space, the notions of projection of a vector onto another and rejection of a vector from another can be generalized to the notions of projection of a vector onto a plane, and rejection of a vector from a plane.\nThe projection of a vector on a plane is its orthogonal projection on that plane. The rejection of a vector from a plane is its orthogonal projection on a straight line which is orthogonal to that plane. Both are vectors. The first is parallel to the plane, the second is orthogonal. For a given vector and plane, the sum of projection and rejection is equal to the original vector.\n\nSimilarly, for inner product spaces with more than three dimensions, the notions of projection onto a vector and rejection from a vector can be generalized to the notions of projection onto a hyperplane, and rejection from a hyperplane.\n\nIn geometric algebra, they can be further generalized to the notions of projection and rejection of a general multivector onto/from any invertible \"k\"-blade.\n\n\n"}
{"id": "32370", "url": "https://en.wikipedia.org/wiki?curid=32370", "title": "Vector space", "text": "Vector space\n\nA vector space (also called a linear space) is a collection of objects called vectors, which may be added together and multiplied (\"scaled\") by numbers, called \"scalars\". Scalars are often taken to be real numbers, but there are also vector spaces with scalar multiplication by complex numbers, rational numbers, or generally any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called \"axioms\", listed below.\n\nEuclidean vectors are an example of a vector space. They represent physical quantities such as forces: any two forces (of the same type) can be added to yield a third, and the multiplication of a force vector by a real multiplier is another force vector. In the same vein, but in a more geometric sense, vectors representing displacements in the plane or in three-dimensional space also form vector spaces. Vectors in vector spaces do not necessarily have to be arrow-like objects as they appear in the mentioned examples: vectors are regarded as abstract mathematical objects with particular properties, which in some cases can be visualized as arrows.\n\nVector spaces are the subject of linear algebra and are well characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. Infinite-dimensional vector spaces arise naturally in mathematical analysis, as function spaces, whose vectors are functions. These vector spaces are generally endowed with additional structure, which may be a topology, allowing the consideration of issues of proximity and continuity. Among these topologies, those that are defined by a norm or inner product are more commonly used, as having a notion of distance between two vectors. This is particularly the case of Banach spaces and Hilbert spaces, which are fundamental in mathematical analysis.\n\nHistorically, the first ideas leading to vector spaces can be traced back as far as the 17th century's analytic geometry, matrices, systems of linear equations, and Euclidean vectors. The modern, more abstract treatment, first formulated by Giuseppe Peano in 1888, encompasses more general objects than Euclidean space, but much of the theory can be seen as an extension of classical geometric ideas like lines, planes and their higher-dimensional analogs.\n\nToday, vector spaces are applied throughout mathematics, science and engineering. They are the appropriate linear-algebraic notion to deal with systems of linear equations. They offer a framework for Fourier expansion, which is employed in image compression routines, and they provide an environment that can be used for solution techniques for partial differential equations. Furthermore, vector spaces furnish an abstract, coordinate-free way of dealing with geometrical and physical objects such as tensors. This in turn allows the examination of local properties of manifolds by linearization techniques. Vector spaces may be generalized in several ways, leading to more advanced notions in geometry and abstract algebra.\n\nThe concept of vector space will first be explained by describing two particular examples:\n\nThe first example of a vector space consists of arrows in a fixed plane, starting at one fixed point. This is used in physics to describe forces or velocities. Given any two such arrows, and , the parallelogram spanned by these two arrows contains one diagonal arrow that starts at the origin, too. This new arrow is called the \"sum\" of the two arrows and is denoted . In the special case of two arrows on the same line, their sum is the arrow on this line whose length is the sum or the difference of the lengths, depending on whether the arrows have the same direction. Another operation that can be done with arrows is scaling: given any positive real number , the arrow that has the same direction as , but is dilated or shrunk by multiplying its length by , is called \"multiplication\" of by . It is denoted . When is negative, is defined as the arrow pointing in the opposite direction, instead.\n\nThe following shows a few examples: if , the resulting vector has the same direction as , but is stretched to the double length of (right image below). Equivalently, is the sum . Moreover, has the opposite direction and the same length as (blue vector pointing down in the right image).\n\nA second key example of a vector space is provided by pairs of real numbers and . (The order of the components and is significant, so such a pair is also called an ordered pair.) Such a pair is written as . The sum of two such pairs and multiplication of a pair with a number is defined as follows:\nand \n\nThe first example above reduces to this one if the arrows are represented by the pair of Cartesian coordinates of their end points.\n\nIn this article, vectors are represented in boldface to distinguish them from scalars.\n\nA vector space over a field is a set  together with two operations that satisfy the eight axioms listed below.\n\n\nElements of are commonly called \"vectors\". Elements of  are commonly called \"scalars\".\n\nIn the two examples above, the field is the field of the real numbers and the set of the vectors consists of the planar arrows with fixed starting point and of pairs of real numbers, respectively.\n\nTo qualify as a vector space, the set  and the operations of addition and multiplication must adhere to a number of requirements called axioms. In the list below, let , and be arbitrary vectors in , and and scalars in .\n\nThese axioms generalize properties of the vectors introduced in the above examples. Indeed, the result of addition of two ordered pairs (as in the second example above) does not depend on the order of the summands:\nLikewise, in the geometric example of vectors as arrows, since the parallelogram defining the sum of the vectors is independent of the order of the vectors. All other axioms can be checked in a similar manner in both examples. Thus, by disregarding the concrete nature of the particular type of vectors, the definition incorporates these two and many more examples in one notion of vector space.\n\nSubtraction of two vectors and division by a (non-zero) scalar can be defined as\n\nWhen the scalar field is the real numbers , the vector space is called a \"real vector space\". When the scalar field is the complex numbers , the vector space is called a \"complex vector space\". These two cases are the ones used most often in engineering. The general definition of a vector space allows scalars to be elements of any fixed field . The notion is then known as an -\"vector spaces\" or a \"vector space over \". A field is, essentially, a set of numbers possessing addition, subtraction, multiplication and division operations. For example, rational numbers form a field.\n\nIn contrast to the intuition stemming from vectors in the plane and higher-dimensional cases, there is, in general vector spaces, no notion of nearness, angles or distances. To deal with such matters, particular types of vector spaces are introduced; see below.\n\nVector addition and scalar multiplication are operations, satisfying the closure property: and are in for all in , and , in . Some older sources mention these properties as separate axioms.\n\nIn the parlance of abstract algebra, the first four axioms are equivalent to requiring the set of vectors to be an abelian group under addition. The remaining axioms give this group an -module structure. In other words, there is a ring homomorphism from the field into the endomorphism ring of the group of vectors. Then scalar multiplication is defined as .\n\nThere are a number of direct consequences of the vector space axioms. Some of them derive from elementary group theory, applied to the additive group of vectors: for example, the zero vector of and the additive inverse of any vector are unique. Further properties follow by employing also the distributive law for the scalar multiplication, for example equals if and only if equals or equals .\n\nVector spaces stem from affine geometry via the introduction of coordinates in the plane or three-dimensional space. Around 1636, Descartes and Fermat founded analytic geometry by equating solutions to an equation of two variables with points on a plane curve. In 1804, to achieve geometric solutions without using coordinates, Bolzano introduced certain operations on points, lines and planes, which are predecessors of vectors. His work was then used in the conception of barycentric coordinates by Möbius in 1827. In 1828 C. V. Mourey suggested the existence of an algebra surpassing not only ordinary algebra but also two-dimensional algebra created by him searching a geometrical interpretation of complex numbers.\n\nThe definition of vectors was founded on Bellavitis' notion of the bipoint, an oriented segment of which one end is the origin and the other a target, then further elaborated with the presentation of complex numbers by Argand and Hamilton and the introduction of quaternions and biquaternions by the latter. They are elements in , , and ; their treatment as linear combinations can be traced back to Laguerre in 1867, who also defined systems of linear equations.\n\nIn 1857, Cayley introduced matrix notation, which allows for a harmonization and simplification of linear maps. Around the same time, Grassmann studied the barycentric calculus initiated by Möbius. He envisaged sets of abstract objects endowed with operations. In his work, the concepts of linear independence and dimension, as well as scalar products, are present. In fact, Grassmann's 1844 work extended a vector space of \"n\" dimensions to one of 2 dimensions by consideration of 2-vectors formula_1 and 3-vectors formula_2 called multivectors. This extension, called multilinear algebra, is governed by the rules of exterior algebra. Peano was the first to give the modern definition of vector spaces and linear maps in 1888.\n\nAn important development of vector spaces is due to the construction of function spaces by Lebesgue. This was later formalized by Banach and Hilbert, around 1920. At that time, algebra and the new field of functional analysis began to interact, notably with key concepts such as spaces of \"p\"-integrable functions and Hilbert spaces. Vector spaces, including infinite-dimensional ones, then became a firmly established notion, and many mathematical branches started making use of this concept.\n\nThe simplest example of a vector space over a field is the field itself, equipped with its standard addition and multiplication. More generally, a vector space can be composed of\n\"n\"-tuples (sequences of length ) of elements of , such as\nA vector space composed of all the -tuples of a field is known as a \"coordinate space\", usually denoted . The case is the above-mentioned simplest example, in which the field is also regarded as a vector space over itself. The case and was discussed in the introduction above.\n\nThe set of complex numbers , i.e., numbers that can be written in the form for real numbers and where is the imaginary unit, form a vector space over the reals with the usual addition and multiplication: and for real numbers , , , and . The various axioms of a vector space follow from the fact that the same rules hold for complex number arithmetic.\n\nIn fact, the example of complex numbers is essentially the same (i.e., it is \"isomorphic\") to the vector space of ordered pairs of real numbers mentioned above: if we think of the complex number as representing the ordered pair in the complex plane then we see that the rules for sum and scalar product correspond exactly to those in the earlier example.\n\nMore generally, field extensions provide another class of examples of vector spaces, particularly in algebra and algebraic number theory: a field containing a smaller field is an -vector space, by the given multiplication and addition operations of . For example, the complex numbers are a vector space over , and the field extension formula_3 is a vector space over . \n\nFunctions from any fixed set to a field also form vector spaces, by performing addition and scalar multiplication pointwise. That is, the sum of two functions and is the function given by\nand similarly for multiplication. Such function spaces occur in many geometric situations, when is the real line or an interval, or other subsets of . Many notions in topology and analysis, such as continuity, integrability or differentiability are well-behaved with respect to linearity: sums and scalar multiples of functions possessing such a property still have that property. Therefore, the set of such functions are vector spaces. They are studied in greater detail using the methods of functional analysis, see below. Algebraic constraints also yield vector spaces: the vector space is given by polynomial functions:\n\nSystems of homogeneous linear equations are closely tied to vector spaces. For example, the solutions of \nare given by triples with arbitrary , , and . They form a vector space: sums and scalar multiples of such triples still satisfy the same ratios of the three variables; thus they are solutions, too. Matrices can be used to condense multiple linear equations as above into one vector equation, namely\nwhere formula_4 is the matrix containing the coefficients of the given equations, is the vector , denotes the matrix product, and is the zero vector. In a similar vein, the solutions of homogeneous \"linear differential equations\" form vector spaces. For example,\nyields , where and are arbitrary constants, and is the natural exponential function.\n\n\"Bases\" allow one to represent vectors by a sequence of scalars called \"coordinates\" or \"components\". A basis is a (finite or infinite) set of vectors , for convenience often indexed by some index set , that spans the whole space and is linearly independent. \"Spanning the whole space\" means that any vector can be expressed as a finite sum (called a \"linear combination\") of the basis elements:\n\nwhere the are scalars, called the coordinates (or the components) of the vector with respect to the basis , and elements of . Linear independence means that the coordinates are uniquely determined for any vector in the vector space.\n\nFor example, the coordinate vectors , , to , form a basis of , called the standard basis, since any vector can be uniquely expressed as a linear combination of these vectors:\nThe corresponding coordinates , , , are just the Cartesian coordinates of the vector.\n\nEvery vector space has a basis. This follows from Zorn's lemma, an equivalent formulation of the Axiom of Choice. Given the other axioms of Zermelo–Fraenkel set theory, the existence of bases is equivalent to the axiom of choice. The ultrafilter lemma, which is weaker than the axiom of choice, implies that all bases of a given vector space have the same number of elements, or cardinality (cf. \"Dimension theorem for vector spaces\"). It is called the \"dimension\" of the vector space, denoted by dim \"V\". If the space is spanned by finitely many vectors, the above statements can be proven without such fundamental input from set theory.\n\nThe dimension of the coordinate space is , by the basis exhibited above. The dimension of the polynomial ring \"F\"[\"x\"] introduced above is countably infinite, a basis is given by , , , A fortiori, the dimension of more general function spaces, such as the space of functions on some (bounded or unbounded) interval, is infinite. Under suitable regularity assumptions on the coefficients involved, the dimension of the solution space of a homogeneous ordinary differential equation equals the degree of the equation. For example, the solution space for the above equation is generated by . These two functions are linearly independent over , so the dimension of this space is two, as is the degree of the equation.\n\nA field extension over the rationals can be thought of as a vector space over (by defining vector addition as field addition, defining scalar multiplication as field multiplication by elements of , and otherwise ignoring the field multiplication). The dimension (or degree) of the field extension over depends on . If satisfies some polynomial equation\nformula_5\nwith rational coefficients (in other words, if α is algebraic), the dimension is finite. More precisely, it equals the degree of the minimal polynomial having α as a root. For example, the complex numbers C are a two-dimensional real vector space, generated by 1 and the imaginary unit \"i\". The latter satisfies \"i\" + 1 = 0, an equation of degree two. Thus, C is a two-dimensional R-vector space (and, as any field, one-dimensional as a vector space over itself, C). If α is not algebraic, the dimension of Q(α) over Q is infinite. For instance, for α = π there is no such equation, in other words π is transcendental.\n\nThe relation of two vector spaces can be expressed by \"linear map\" or \"linear transformation\". They are functions that reflect the vector space structure—i.e., they preserve sums and scalar multiplication:\n\nAn \"isomorphism\" is a linear map such that there exists an inverse map , which is a map such that the two possible compositions and are identity maps. Equivalently, \"f\" is both one-to-one (injective) and onto (surjective). If there exists an isomorphism between \"V\" and \"W\", the two spaces are said to be \"isomorphic\"; they are then essentially identical as vector spaces, since all identities holding in \"V\" are, via \"f\", transported to similar ones in \"W\", and vice versa via \"g\".\nFor example, the \"arrows in the plane\" and \"ordered pairs of numbers\" vector spaces in the introduction are isomorphic: a planar arrow v departing at the origin of some (fixed) coordinate system can be expressed as an ordered pair by considering the \"x\"- and \"y\"-component of the arrow, as shown in the image at the right. Conversely, given a pair (\"x\", \"y\"), the arrow going by \"x\" to the right (or to the left, if \"x\" is negative), and \"y\" up (down, if \"y\" is negative) turns back the arrow v.\n\nLinear maps \"V\" → \"W\" between two vector spaces form a vector space Hom(\"V\", \"W\"), also denoted L(\"V\", \"W\"). The space of linear maps from \"V\" to \"F\" is called the \"dual vector space\", denoted \"V\". Via the injective natural map , any vector space can be embedded into its \"bidual\"; the map is an isomorphism if and only if the space is finite-dimensional.\n\nOnce a basis of is chosen, linear maps are completely determined by specifying the images of the basis vectors, because any element of \"V\" is expressed uniquely as a linear combination of them. If , a 1-to-1 correspondence between fixed bases of and gives rise to a linear map that maps any basis element of to the corresponding basis element of . It is an isomorphism, by its very definition. Therefore, two vector spaces are isomorphic if their dimensions agree and vice versa. Another way to express this is that any vector space is \"completely classified\" (up to isomorphism) by its dimension, a single number. In particular, any \"n\"-dimensional -vector space is isomorphic to . There is, however, no \"canonical\" or preferred isomorphism; actually an isomorphism is equivalent to the choice of a basis of , by mapping the standard basis of to , via . The freedom of choosing a convenient basis is particularly useful in the infinite-dimensional context; see below.\n\n\"Matrices\" are a useful notion to encode linear maps. They are written as a rectangular array of scalars as in the image at the right. Any \"m\"-by-\"n\" matrix \"A\" gives rise to a linear map from \"F\" to \"F\", by the following\nor, using the matrix multiplication of the matrix with the coordinate vector :\nMoreover, after choosing bases of and , \"any\" linear map is uniquely represented by a matrix via this assignment.\nThe determinant of a square matrix is a scalar that tells whether the associated map is an isomorphism or not: to be so it is sufficient and necessary that the determinant is nonzero. The linear transformation of corresponding to a real \"n\"-by-\"n\" matrix is orientation preserving if and only if its determinant is positive.\n\nEndomorphisms, linear maps , are particularly important since in this case vectors can be compared with their image under , . Any nonzero vector satisfying , where is a scalar, is called an \"eigenvector\" of with \"eigenvalue\" . Equivalently, is an element of the kernel of the difference (where Id is the identity map . If is finite-dimensional, this can be rephrased using determinants: having eigenvalue is equivalent to\nBy spelling out the definition of the determinant, the expression on the left hand side can be seen to be a polynomial function in , called the characteristic polynomial of . If the field is large enough to contain a zero of this polynomial (which automatically happens for algebraically closed, such as ) any linear map has at least one eigenvector. The vector space may or may not possess an eigenbasis, a basis consisting of eigenvectors. This phenomenon is governed by the Jordan canonical form of the map. The set of all eigenvectors corresponding to a particular eigenvalue of forms a vector space known as the \"eigenspace\" corresponding to the eigenvalue (and ) in question. To achieve the spectral theorem, the corresponding statement in the infinite-dimensional case, the machinery of functional analysis is needed, see below.\n\nIn addition to the above concrete examples, there are a number of standard linear algebraic constructions that yield vector spaces related to given ones. In addition to the definitions given below, they are also characterized by universal properties, which determine an object by specifying the linear maps from to any other vector space.\n\nA nonempty subset \"W\" of a vector space \"V\" that is closed under addition and scalar multiplication (and therefore contains the 0-vector of \"V\") is called a \"linear subspace\" of \"V\", or simply a \"subspace\" of \"V\", when the ambient space is unambiguously a vector space. Subspaces of \"V\" are vector spaces (over the same field) in their own right. The intersection of all subspaces containing a given set \"S\" of vectors is called its span, and it is the smallest subspace of \"V\" containing the set \"S\". Expressed in terms of elements, the span is the subspace consisting of all the linear combinations of elements of \"S\".\n\nA linear subspace of dimension 1 is a vector line. A linear subspace of dimension 2 is a vector plane. A linear subspace that contains all elements but one of a basis of the ambient space is a vector hyperplane. In a vector space of finite dimension , a vector hyperplane is thus a subspace of dimension .\n\nThe counterpart to subspaces are \"quotient vector spaces\". Given any subspace , the quotient space \"V\"/\"W\" (\"\"V\" modulo \"W\"\") is defined as follows: as a set, it consists of where v is an arbitrary vector in \"V\". The sum of two such elements and is and scalar multiplication is given by . The key point in this definition is that if and only if the difference of v and v lies in \"W\". This way, the quotient space \"forgets\" information that is contained in the subspace \"W\".\n\nThe kernel ker(\"f\") of a linear map consists of vectors v that are mapped to 0 in \"W\". Both kernel and image are subspaces of \"V\" and \"W\", respectively. The existence of kernels and images is part of the statement that the category of vector spaces (over a fixed field \"F\") is an abelian category, i.e. a corpus of mathematical objects and structure-preserving maps between them (a category) that behaves much like the category of abelian groups. Because of this, many statements such as the first isomorphism theorem (also called rank–nullity theorem in matrix-related terms)\nand the second and third isomorphism theorem can be formulated and proven in a way very similar to the corresponding statements for groups.\n\nAn important example is the kernel of a linear map for some fixed matrix \"A\", as above. The kernel of this map is the subspace of vectors x such that , which is precisely the set of solutions to the system of homogeneous linear equations belonging to \"A\". This concept also extends to linear differential equations\nIn the corresponding map\nthe derivatives of the function \"f\" appear linearly (as opposed to \"f\"′′(\"x\"), for example). Since differentiation is a linear procedure (i.e., and for a constant ) this assignment is linear, called a linear differential operator. In particular, the solutions to the differential equation form a vector space (over or ).\n\nThe \"direct product\" of vector spaces and the \"direct sum\" of vector spaces are two ways of combining an indexed family of vector spaces into a new vector space.\n\nThe \"direct product\" formula_10 of a family of vector spaces \"V\" consists of the set of all tuples (, which specify for each index \"i\" in some index set \"I\" an element v of \"V\". Addition and scalar multiplication is performed componentwise. A variant of this construction is the \"direct sum\" formula_11 (also called coproduct and denoted formula_12), where only tuples with finitely many nonzero vectors are allowed. If the index set \"I\" is finite, the two constructions agree, but in general they are different.\n\nThe \"tensor product\" , or simply , of two vector spaces \"V\" and \"W\" is one of the central notions of multilinear algebra which deals with extending notions such as linear maps to several variables. A map is called bilinear if \"g\" is linear in both variables v and w. That is to say, for fixed w the map is linear in the sense above and likewise for fixed v.\n\nThe tensor product is a particular vector space that is a \"universal\" recipient of bilinear maps \"g\", as follows. It is defined as the vector space consisting of finite (formal) sums of symbols called tensors\nsubject to the rules \n\nThese rules ensure that the map \"f\" from the to that maps a tuple to is bilinear. The universality states that given \"any\" vector space \"X\" and \"any\" bilinear map , there exists a unique map \"u\", shown in the diagram with a dotted arrow, whose composition with \"f\" equals \"g\": . This is called the universal property of the tensor product, an instance of the method—much used in advanced abstract algebra—to indirectly define objects by specifying maps from or to this object.\n\nFrom the point of view of linear algebra, vector spaces are completely understood insofar as any vector space is characterized, up to isomorphism, by its dimension. However, vector spaces \"per se\" do not offer a framework to deal with the question—crucial to analysis—whether a sequence of functions converges to another function. Likewise, linear algebra is not adapted to deal with infinite series, since the addition operation allows only finitely many terms to be added. Therefore, the needs of functional analysis require considering additional structures.\n\nA vector space may be given a partial order ≤, under which some vectors can be compared. For example, \"n\"-dimensional real space R can be ordered by comparing its vectors componentwise. Ordered vector spaces, for example Riesz spaces, are fundamental to Lebesgue integration, which relies on the ability to express a function as a difference of two positive functions\nwhere \"f\" denotes the positive part of \"f\" and \"f\" the negative part.\n\n\"Measuring\" vectors is done by specifying a norm, a datum which measures lengths of vectors, or by an inner product, which measures angles between vectors. Norms and inner products are denoted formula_13 and formula_14, respectively. The datum of an inner product entails that lengths of vectors can be defined too, by defining the associated norm formula_15. Vector spaces endowed with such data are known as \"normed vector spaces\" and \"inner product spaces\", respectively.\n\nCoordinate space \"F\" can be equipped with the standard dot product:\nIn R, this reflects the common notion of the angle between two vectors x and y, by the law of cosines:\nBecause of this, two vectors satisfying formula_18 are called orthogonal. An important variant of the standard dot product is used in Minkowski space: R endowed with the Lorentz product\nIn contrast to the standard dot product, it is not positive definite: formula_20 also takes negative values, for example for formula_21. Singling out the fourth coordinate—corresponding to time, as opposed to three space-dimensions—makes it useful for the mathematical treatment of special relativity.\n\nConvergence questions are treated by considering vector spaces \"V\" carrying a compatible topology, a structure that allows one to talk about elements being close to each other. Compatible here means that addition and scalar multiplication have to be continuous maps. Roughly, if x and y in \"V\", and \"a\" in \"F\" vary by a bounded amount, then so do and . To make sense of specifying the amount a scalar changes, the field \"F\" also has to carry a topology in this context; a common choice are the reals or the complex numbers.\n\nIn such \"topological vector spaces\" one can consider series of vectors. The infinite sum\ndenotes the limit of the corresponding finite partial sums of the sequence (\"f\") of elements of \"V\". For example, the \"f\" could be (real or complex) functions belonging to some function space \"V\", in which case the series is a function series. The mode of convergence of the series depends on the topology imposed on the function space. In such cases, pointwise convergence and uniform convergence are two prominent examples.\nA way to ensure the existence of limits of certain infinite series is to restrict attention to spaces where any Cauchy sequence has a limit; such a vector space is called complete. Roughly, a vector space is complete provided that it contains all necessary limits. For example, the vector space of polynomials on the unit interval [0,1], equipped with the topology of uniform convergence is not complete because any continuous function on [0,1] can be uniformly approximated by a sequence of polynomials, by the Weierstrass approximation theorem. In contrast, the space of \"all\" continuous functions on [0,1] with the same topology is complete. A norm gives rise to a topology by defining that a sequence of vectors v converges to v if and only if\nBanach and Hilbert spaces are complete topological vector spaces whose topologies are given, respectively, by a norm and an inner product. Their study—a key piece of functional analysis—focusses on infinite-dimensional vector spaces, since all norms on finite-dimensional topological vector spaces give rise to the same notion of convergence. The image at the right shows the equivalence of the 1-norm and ∞-norm on R: as the unit \"balls\" enclose each other, a sequence converges to zero in one norm if and only if it so does in the other norm. In the infinite-dimensional case, however, there will generally be inequivalent topologies, which makes the study of topological vector spaces richer than that of vector spaces without additional data.\n\nFrom a conceptual point of view, all notions related to topological vector spaces should match the topology. For example, instead of considering all linear maps (also called functionals) , maps between topological vector spaces are required to be continuous. In particular, the (topological) dual space consists of continuous functionals (or to ). The fundamental Hahn–Banach theorem is concerned with separating subspaces of appropriate topological vector spaces by continuous functionals.\n\n\"Banach spaces\", introduced by Stefan Banach, are complete normed vector spaces. \n\nA first example is the vector space formula_24 \nconsisting of infinite vectors with real entries \nformula_25\nwhose formula_26-norm \nformula_27\ngiven by \nThe topologies on the infinite-dimensional space \nformula_24 \nare inequivalent for different formula_26. \nE.g. the sequence of vectors \nformula_33, \ni.e. the first formula_34 components are formula_35, \nthe following ones are formula_36, converges to the zero vector for \nformula_37, \nbut does not for \nformula_38:\n\nMore generally than sequences of real numbers, functions \nformula_41\nare endowed with a norm that replaces the above sum by the Lebesgue integral\nThe space of integrable functions on a given domain \nformula_43 \n(for example an interval) satisfying \nformula_44,\nand equipped with this norm are called Lebesgue spaces, \ndenoted formula_45. \n\nThese spaces are complete. (If one uses the Riemann integral instead, the space is \"not\" complete, which may be seen as a justification for Lebesgue's integration theory.) \nConcretely this means that for any sequence of Lebesgue-integrable functions   \nformula_46  \nwith formula_47, \nsatisfying the condition\nthere exists a function formula_49 belonging to the vector space formula_45 such that\n\nImposing boundedness conditions not only on the function, but also on its derivatives leads to Sobolev spaces.\nComplete inner product spaces are known as \"Hilbert spaces\", in honor of David Hilbert.\nThe Hilbert space \"L\"(Ω), with inner product given by\nwhere formula_53 denotes the complex conjugate of \"g\"(\"x\"), is a key case.\n\nBy definition, in a Hilbert space any Cauchy sequence converges to a limit. Conversely, finding a sequence of functions \"f\" with desirable properties that approximates a given limit function, is equally crucial. Early analysis, in the guise of the Taylor approximation, established an approximation of differentiable functions \"f\" by polynomials. By the Stone–Weierstrass theorem, every continuous function on can be approximated as closely as desired by a polynomial. A similar approximation technique by trigonometric functions is commonly called Fourier expansion, and is much applied in engineering, see below. More generally, and more conceptually, the theorem yields a simple description of what \"basic functions\", or, in abstract Hilbert spaces, what basic vectors suffice to generate a Hilbert space \"H\", in the sense that the \"closure\" of their span (i.e., finite linear combinations and limits of those) is the whole space. Such a set of functions is called a \"basis\" of \"H\", its cardinality is known as the Hilbert space dimension. Not only does the theorem exhibit suitable basis functions as sufficient for approximation purposes, but together with the Gram–Schmidt process, it enables one to construct a basis of orthogonal vectors. Such orthogonal bases are the Hilbert space generalization of the coordinate axes in finite-dimensional Euclidean space.\n\nThe solutions to various differential equations can be interpreted in terms of Hilbert spaces. For example, a great many fields in physics and engineering lead to such equations and frequently solutions with particular physical properties are used as basis functions, often orthogonal. As an example from physics, the time-dependent Schrödinger equation in quantum mechanics describes the change of physical properties in time by means of a partial differential equation, whose solutions are called wavefunctions. Definite values for physical properties such as energy, or momentum, correspond to eigenvalues of a certain (linear) differential operator and the associated wavefunctions are called eigenstates. The spectral theorem decomposes a linear compact operator acting on functions in terms of these eigenfunctions and their eigenvalues.\nGeneral vector spaces do not possess a multiplication between vectors. A vector space equipped with an additional bilinear operator defining the multiplication of two vectors is an \"algebra over a field\". Many algebras stem from functions on some geometrical object: since functions with values in a given field can be multiplied pointwise, these entities form algebras. The Stone–Weierstrass theorem mentioned above, for example, relies on Banach algebras which are both Banach spaces and algebras.\n\nCommutative algebra makes great use of rings of polynomials in one or several variables, introduced above. Their multiplication is both commutative and associative. These rings and their quotients form the basis of algebraic geometry, because they are rings of functions of algebraic geometric objects.\n\nAnother crucial example are \"Lie algebras\", which are neither commutative nor associative, but the failure to be so is limited by the constraints ( denotes the product of and ):\nExamples include the vector space of \"n\"-by-\"n\" matrices, with , the commutator of two matrices, and , endowed with the cross product.\n\nThe tensor algebra T(\"V\") is a formal way of adding products to any vector space \"V\" to obtain an algebra. As a vector space, it is spanned by symbols, called simple tensors\nThe multiplication is given by concatenating such symbols, imposing the distributive law under addition, and requiring that scalar multiplication commute with the tensor product ⊗, much the same way as with the tensor product of two vector spaces introduced above. In general, there are no relations between and . Forcing two such elements to be equal leads to the symmetric algebra, whereas forcing yields the exterior algebra.\n\nWhen a field, is explicitly stated, a common term used is -algebra.\n\nVector spaces have many applications as they occur frequently in common circumstances, namely wherever functions with values in some field are involved. They provide a framework to deal with analytical and geometrical problems, or are used in the Fourier transform. This list is not exhaustive: many more applications exist, for example in optimization. The minimax theorem of game theory stating the existence of a unique payoff when all players play optimally can be formulated and proven using vector spaces methods. Representation theory fruitfully transfers the good understanding of linear algebra and vector spaces to other mathematical domains such as group theory.\n\nA \"distribution\" (or \"generalized function\") is a linear map assigning a number to each \"test\" function, typically a smooth function with compact support, in a continuous way: in the above terminology the space of distributions is the (continuous) dual of the test function space. The latter space is endowed with a topology that takes into account not only \"f\" itself, but also all its higher derivatives. A standard example is the result of integrating a test function \"f\" over some domain Ω:\nWhen the set consisting of a single point, this reduces to the Dirac distribution, denoted by δ, which associates to a test function \"f\" its value at the . Distributions are a powerful instrument to solve differential equations. Since all standard analytic notions such as derivatives are linear, they extend naturally to the space of distributions. Therefore, the equation in question can be transferred to a distribution space, which is bigger than the underlying function space, so that more flexible methods are available for solving the equation. For example, Green's functions and fundamental solutions are usually distributions rather than proper functions, and can then be used to find solutions of the equation with prescribed boundary conditions. The found solution can then in some cases be proven to be actually a true function, and a solution to the original equation (e.g., using the Lax–Milgram theorem, a consequence of the Riesz representation theorem).\n\nResolving a periodic function into a sum of trigonometric functions forms a \"Fourier series\", a technique much used in physics and engineering. The underlying vector space is usually the Hilbert space \"L\"(0, 2π), for which the functions sin \"mx\" and cos \"mx\" (\"m\" an integer) form an orthogonal basis. The Fourier expansion of an \"L\" function \"f\" is\n\nThe coefficients \"a\" and \"b\" are called Fourier coefficients of \"f\", and are calculated by the formulas\n\nIn physical terms the function is represented as a superposition of sine waves and the coefficients give information about the function's frequency spectrum. A complex-number form of Fourier series is also commonly used. The concrete formulae above are consequences of a more general mathematical duality called Pontryagin duality. Applied to the group R, it yields the classical Fourier transform; an application in physics are reciprocal lattices, where the underlying group is a finite-dimensional real vector space endowed with the additional datum of a lattice encoding positions of atoms in crystals.\n\nFourier series are used to solve boundary value problems in partial differential equations. In 1822, Fourier first used this technique to solve the heat equation. A discrete version of the Fourier series can be used in sampling applications where the function value is known only at a finite number of equally spaced points. In this case the Fourier series is finite and its value is equal to the sampled values at all points. The set of coefficients is known as the discrete Fourier transform (DFT) of the given sample sequence. The DFT is one of the key tools of digital signal processing, a field whose applications include radar, speech encoding, image compression. The JPEG image format is an application of the closely related discrete cosine transform.\n\nThe fast Fourier transform is an algorithm for rapidly computing the discrete Fourier transform. It is used not only for calculating the Fourier coefficients but, using the convolution theorem, also for computing the convolution of two finite sequences. They in turn are applied in digital filters and as a rapid multiplication algorithm for polynomials and large integers (Schönhage–Strassen algorithm).\n\nThe tangent plane to a surface at a point is naturally a vector space whose origin is identified with the point of contact. The tangent plane is the best linear approximation, or linearization, of a surface at a point. Even in a three-dimensional Euclidean space, there is typically no natural way to prescribe a basis of the tangent plane, and so it is conceived of as an abstract vector space rather than a real coordinate space. The \"tangent space\" is the generalization to higher-dimensional differentiable manifolds.\n\nRiemannian manifolds are manifolds whose tangent spaces are endowed with a suitable inner product. Derived therefrom, the Riemann curvature tensor encodes all curvatures of a manifold in one object, which finds applications in general relativity, for example, where the Einstein curvature tensor describes the matter and energy content of space-time. The tangent space of a Lie group can be given naturally the structure of a Lie algebra and can be used to classify compact Lie groups.\n\nA \"vector bundle\" is a family of vector spaces parametrized continuously by a topological space \"X\". More precisely, a vector bundle over \"X\" is a topological space \"E\" equipped with a continuous map \nsuch that for every \"x\" in \"X\", the fiber π(\"x\") is a vector space. The case dim is called a line bundle. For any vector space \"V\", the projection makes the product into a \"trivial\" vector bundle. Vector bundles over \"X\" are required to be locally a product of \"X\" and some (fixed) vector space \"V\": for every \"x\" in \"X\", there is a neighborhood \"U\" of \"x\" such that the restriction of π to π(\"U\") is isomorphic to the trivial bundle . Despite their locally trivial character, vector bundles may (depending on the shape of the underlying space \"X\") be \"twisted\" in the large (i.e., the bundle need not be (globally isomorphic to) the trivial bundle ). For example, the Möbius strip can be seen as a line bundle over the circle \"S\" (by identifying open intervals with the real line). It is, however, different from the cylinder , because the latter is orientable whereas the former is not.\n\nProperties of certain vector bundles provide information about the underlying topological space. For example, the tangent bundle consists of the collection of tangent spaces parametrized by the points of a differentiable manifold. The tangent bundle of the circle \"S\" is globally isomorphic to , since there is a global nonzero vector field on \"S\". In contrast, by the hairy ball theorem, there is no (tangent) vector field on the 2-sphere \"S\" which is everywhere nonzero. K-theory studies the isomorphism classes of all vector bundles over some topological space. In addition to deepening topological and geometrical insight, it has purely algebraic consequences, such as the classification of finite-dimensional real division algebras: R, C, the quaternions H and the octonions O.\n\nThe cotangent bundle of a differentiable manifold consists, at every point of the manifold, of the dual of the tangent space, the cotangent space. Sections of that bundle are known as differential one-forms.\n\n\"Modules\" are to rings what vector spaces are to fields: the same axioms, applied to a ring \"R\" instead of a field \"F\", yield modules. The theory of modules, compared to that of vector spaces, is complicated by the presence of ring elements that do not have multiplicative inverses. For example, modules need not have bases, as the Z-module (i.e., abelian group) Z/2Z shows; those modules that do (including all vector spaces) are known as free modules. Nevertheless, a vector space can be compactly defined as a module over a ring which is a field with the elements being called vectors. Some authors use the term \"vector space\" to mean modules over a division ring. The algebro-geometric interpretation of commutative rings via their spectrum allows the development of concepts such as locally free modules, the algebraic counterpart to vector bundles.\n\nRoughly, \"affine spaces\" are vector spaces whose origins are not specified. More precisely, an affine space is a set with a free transitive vector space action. In particular, a vector space is an affine space over itself, by the map\nIf \"W\" is a vector space, then an affine subspace is a subset of \"W\" obtained by translating a linear subspace \"V\" by a fixed vector ; this space is denoted by (it is a coset of \"V\" in \"W\") and consists of all vectors of the form for An important example is the space of solutions of a system of inhomogeneous linear equations\ngeneralizing the homogeneous case above. The space of solutions is the affine subspace where x is a particular solution of the equation, and \"V\" is the space of solutions of the homogeneous equation (the nullspace of \"A\").\n\nThe set of one-dimensional subspaces of a fixed finite-dimensional vector space \"V\" is known as \"projective space\"; it may be used to formalize the idea of parallel lines intersecting at infinity. Grassmannians and flag manifolds generalize this by parametrizing linear subspaces of fixed dimension \"k\" and flags of subspaces, respectively.\n\n\n\n\n\n\n\n\n"}
