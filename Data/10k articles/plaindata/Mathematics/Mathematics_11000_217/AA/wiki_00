{"id": "3431323", "url": "https://en.wikipedia.org/wiki?curid=3431323", "title": "Academy ratio", "text": "Academy ratio\n\nThe Academy ratio of 1.375:1 (abbreviated as 1.37:1) is an aspect ratio of a frame of 35mm film when used with 4-perf pulldown. It was standardized by the Academy of Motion Picture Arts and Sciences as the standard film aspect ratio in 1932, although similar-sized ratios were used as early as 1928.\n\nSilent films were shot at a 1.33 aspect ratio (also known as a 4:3 aspect ratio), with each frame using all of the negative space between the two rows of film perforations for a length of 4 perfs. The frame line between the silent film frames was very thin. When sound-on-film was introduced in the late 1920s, the soundtrack was recorded in a stripe running just inside of one set of the perforations and cut into the 1.33 image. This made the image area \"taller\", usually around 1.19, which was slightly disorienting to audiences used to the 1.33 frame and also presented problems for exhibitors with fixed-size screens and stationary projectors.\n\nFrom studio to studio, the common attempt to reduce the image back to a 1.33:1 ratio by decreasing the projector aperture in-house met with conflicting results. Each movie theater chain, furthermore, had its own designated house ratio. The first standards set for the new sound-on-film motion pictures were accepted in November 1929, when all major US studios agreed to compose for the Society of Motion Picture Engineers' (SMPE) designated size of returning to the aspect ratio of 1.3:1.\n\nFollowing this, Academy of Motion Picture Arts and Sciences (AMPAS) considered further alterations to this 1930 standard. Various dimensions were submitted, and the projector aperture plate opening size of 0.825 in × 0.600 in was agreed upon. The resulting 1.375:1 aspect ratio was then dubbed the \"Academy Ratio\". On May 9, 1932, the SMPE adopted the same projector aperture standard.\n\nAll studio films shot in 35mm from 1932 to 1952 were shot in the Academy ratio. However, following the widescreen \"revolution\" of 1953, it quickly became an obsolete production format. Within several months, all major studios started matting their non-anamorphic films in the projector to wider ratios such as 1.66, 1.75, and 1.85, the last of which is still considered a standard ratio along with anamorphic (2.39). 1.37:1 is not totally obsolete, nonetheless, and can still be found in selected recent films such as Wes Anderson's \"The Grand Budapest Hotel\", Paul Schrader's \"First Reformed\", Michel Hazanavicius's \"The Artist\", Gus Van Sant's \"Elephant\", Andrea Arnold's \"Fish Tank\", Kelly Reichardt's \"Meek's Cutoff\" and Paweł Pawlikowski's \"Ida\", as well on prints of Phil Lord and Christopher Miller's \"The Lego Movie\" intended for 1.78:1 exhibition (a 2.39:1 version was also made).\n\nThe Academy ratio is not created in the camera, which has continued to use the full frame silent aperture gate for all 4-perf spherical filming. Rather, it is created in the married print, when the optical soundtrack and frame lines are added. Though most non-anamorphic film prints with a soundtrack are now framed to one of the non-anamorphic widescreen ratios, from 1.66 to 1.85, some still retain Academy-sized frames. These frames are then cropped in the projector by means of aperture masks used in the projector's gate in conjunction with a wider lens than would be used for projecting Academy ratio films.\n\nDuring filming, using the 4-perf frame for widescreen framing when spherical lenses are used is sometimes considered to be wasteful in terms of the cost of film stock and processing, especially in the case of television, which does not require a film print. The 3-perf pulldown process was originally proposed in 1973, developed by Miklos Lente in 1976, and further developed by Rune Ericson in 1986 to solve this problem.\n\n"}
{"id": "37092233", "url": "https://en.wikipedia.org/wiki?curid=37092233", "title": "Adaptive sampling", "text": "Adaptive sampling\n\nAdaptive sampling is a technique used in computational molecular biology to efficiently simulate protein folding.\n\nProteins spend a large portion – nearly 96% in some cases – of their folding time \"waiting\" in various thermodynamic free energy minima. Consequently, a straightforward simulation of this process would spend a great deal of computation to this state, with the transitions between the states – the aspects of protein folding of greater scientific interest – taking place only rarely. Adaptive sampling exploits this property to simulate the protein's phase space in between these states. Using adaptive sampling, molecular simulations that previously would have taken decades can be performed in a matter of weeks.\n\nIf a protein folds through the metastable states A -> B -> C, researchers can calculate the length of the transition time between A and C by simulating the A -> B transition and the B -> C transition. The protein may fold through alternative routes which may overlap in part with the A -> B -> C pathway. Decomposing the problem in this manner is efficient because each step can be simulated in parallel.\n\nAdaptive sampling is used by the Folding@home distributed computing project in combination with Markov state models.\n\nWhile adaptive sampling is useful for short simulations, longer trajectories may be more helpful for certain types of biochemical problems.\n\n"}
{"id": "897776", "url": "https://en.wikipedia.org/wiki?curid=897776", "title": "Arm solution", "text": "Arm solution\n\nIn applied mathematics as used in the engineering field of robotics, an arm solution is a solution of equations that allow the calculation of the precise design parameters of a robot's arms in such a way as to enable it to make certain movements.\n\nA typical industrial robot is built with fixed length segments that are connected either at joints whose angles can be controlled, or along linear slides whose length can be controlled. If each angle and slide distance is known, the position and orientation of the end of the robot arm relative to the base can be computed with the simple trigonometry in robot control. \n\nGoing the other way — calculating the angles and slides needed to achieve a desired position and orientation — is much harder. The mathematical procedure for doing this is called an arm solution. For some robot designs, such as the Stanford arm, SCARA robot or cartesian coordinate robots, this can be done in closed form. Other robot designs require an iterative solution.\n\n\n"}
{"id": "60167", "url": "https://en.wikipedia.org/wiki?curid=60167", "title": "Average", "text": "Average\n\nIn colloquial language, an average is a single number taken as representative of a list of numbers. Different concepts of average are used in different contexts. Often \"average\" refers to the arithmetic mean, the sum of the numbers divided by how many numbers are being averaged. In statistics, mean, median, and mode are all known as measures of central tendency, and in colloquial usage any of these might be called an average value.\n\nThe most common type of average is the arithmetic mean. If \" n \" numbers are given, each number denoted by \"a\" (where \"i\" = 1,2, …, \"n\"), the arithmetic mean is the sum of the \"a\"s divided by \"n\" or\n\nThe arithmetic mean, often simply called the mean, of two numbers, such as 2 and 8, is obtained by finding a value A such that 2 + 8 = A + A. One may find that \"A\" = (2 + 8)/2 = 5. Switching the order of 2 and 8 to read 8 and 2 does not change the resulting value obtained for A. The mean 5 is not less than the minimum 2 nor greater than the maximum 8. If we increase the number of terms in the list to 2, 8, and 11, the arithmetic mean is found by solving for the value of \"A\" in the equation 2 + 8 + 11 = \"A\" + \"A\" + \"A\". One finds that \"A\" = (2 + 8 + 11)/3 = 7.\n\nAlong with the arithmetic mean above, the geometric mean and the harmonic mean are known collectively as the Pythagorean means.\n\nThe geometric mean of \"n\" positive numbers is obtained by multiplying them all together and then taking the \"n\"th root. In algebraic terms, the geometric mean of \"a\", \"a\", …, \"a\" is defined as\n\nGeometric mean can be thought of as the antilog of the arithmetic mean of the logs of the numbers.\n\nExample: Geometric mean of 2 and 8 is formula_3\n\nHarmonic mean for a non-empty collection of numbers \"a\", \"a\", …, \"a\", all different from 0, is defined as the reciprocal of the arithmetic mean of the reciprocals of the \"a\"s:\n\nOne example where the harmonic mean is useful is when examining the speed for a number of fixed-distance trips. For example, if the speed for going from point \"A\" to \"B\" was 60 km/h, and the speed for returning from \"B\" to \"A\" was 40 km/h, then the harmonic mean speed is given by\n\nA well known inequality concerning arithmetic, geometric, and harmonic means for any set of positive numbers is\n\nIt is easy to remember noting that the alphabetical order of the letters \"A\", \"G\", and \"H\" is preserved in the inequality. See Inequality of arithmetic and geometric means.\n\nThus for the above harmonic mean example: AM = 50, GM ≈ 49, and HM = 48 km/h.\n\nThe mode, the median, and the mid-range are often used in addition to the mean as estimates of central tendency in descriptive statistics. These can all be seen as minimizing variation by some measure; see .\nThe most frequently occurring number in a list is called the mode. For example, the mode of the list (1, 2, 2, 3, 3, 3, 4) is 3. It may happen that there are two or more numbers which occur equally often and more often than any other number. In this case there is no agreed definition of mode. Some authors say they are all modes and some say there is no mode.\n\nThe median is the middle number of the group when they are ranked in order. (If there are an even number of numbers, the mean of the middle two is taken.)\n\nThus to find the median, order the list according to its elements' magnitude and then repeatedly remove the pair consisting of the highest and lowest values until either one or two values are left. If exactly one value is left, it is the median; if two values, the median is the arithmetic mean of these two. This method takes the list 1, 7, 3, 13 and orders it to read 1, 3, 7, 13. Then the 1 and 13 are removed to obtain the list 3, 7. Since there are two elements in this remaining list, the median is their arithmetic mean, (3 + 7)/2 = 5.\n\nThe mid-range is the arithmetic mean of the highest and lowest values of a set.\n\nThe table of mathematical symbols explains the symbols used below.\n\nOther more sophisticated averages are: trimean, trimedian, and normalized mean, with their generalizations.\n\nOne can create one's own average metric using the generalized \"f\"-mean:\n\nwhere \"f\" is any invertible function. The harmonic mean is an example of this using \"f\"(\"x\") = 1/\"x\", and the geometric mean is another, using \"f\"(\"x\") = log \"x\".\n\nHowever, this method for generating means is not general enough to capture all averages. A more general method for defining an average takes any function \"g\"(\"x\", \"x\", …, \"x\") of a list of arguments that is continuous, strictly increasing in each argument, and symmetric (invariant under permutation of the arguments). The average \"y\" is then the value that, when replacing each member of the list, results in the same function value: . This most general definition still captures the important property of all averages that the average of a list of identical elements is that element itself. The function provides the arithmetic mean. The function (where the list elements are positive numbers) provides the geometric mean. The function (where the list elements are positive numbers) provides the harmonic mean.\n\nA type of average used in finance is the average percentage return. It is an example of a geometric mean. When the returns are annual, it is called the Compound Annual Growth Rate (CAGR). For example, if we are considering a period of two years, and the investment return in the first year is −10% and the return in the second year is +60%, then the average percentage return or CAGR, \"R\", can be obtained by solving the equation: . The value of \"R\" that makes this equation true is 0.2, or 20%. This means that the total return over the 2-year period is the same as if there had been 20% growth each year. Note that the order of the years makes no difference – the average percentage returns of +60% and −10% is the same result as that for −10% and +60%.\n\nThis method can be generalized to examples in which the periods are not equal. For example, consider a period of a half of a year for which the return is −23% and a period of two and a half years for which the return is +13%. The average percentage return for the combined period is the single year return, \"R\", that is the solution of the following equation: , giving an average return \"R\" of 0.0600 or 6.00%.\n\nGiven a time series such as daily stock market prices or yearly temperatures people often want to create a smoother series. This helps to show underlying trends or perhaps periodic behavior. An easy way to do this is the \"moving average\": one chooses a number \"n\" and creates a new series by taking the arithmetic mean of the first \"n\" values, then moving forward one place by dropping the oldest value and introducing a new value at the other end of the list, and so on. This is the simplest form of moving average. More complicated forms involve using a weighted average. The weighting can be used to enhance or suppress various periodic behavior and there is very extensive analysis of what weightings to use in the literature on filtering. In digital signal processing the term “moving average” is used even when the sum of the weights is not 1.0 (so the output series is a scaled version of the averages). The reason for this is that the analyst is usually interested only in the trend or the periodic behavior.\n\nThe first recorded time that the arithmetic mean was extended from 2 to n cases for the use of estimation was in the sixteenth century. From the late sixteenth century onwards, it gradually became a common method to use for reducing errors of measurement in various areas. At the time, astronomers wanted to know a real value from noisy measurement, such as the position of a planet or the diameter of the moon. Using the mean of several measured values, scientists assumed that the errors add up to a relatively small number when compared to the total of all measured values. The method of taking the mean for reducing observation errors was indeed mainly developed in astronomy. A possible precursor to the arithmetic mean is the mid-range (the mean of the two extreme values), used for example in Arabian astronomy of the ninth to eleventh centuries, but also in metallurgy and navigation.\n\nHowever, there are various older vague references to the use of the arithmetic mean (which are not as clear, but might reasonably have to do with our modern definition of the mean). In a text from the 4th century, it was written that (text in square brackets is a possible missing text that might clarify the meaning):\n\nEven older potential references exist. There are records that from about 700 BC, merchants and shippers agreed that damage to the cargo and ship (their \"contribution\" in case of damage by the sea) should be shared equally among themselves. This might have been calculated using the average, although there seem to be no direct record of the calculation.\n\nAccording to the \"Oxford English Dictionary\", \"few words have received more etymological investigation.\" In the 16th century \"average\" meant a customs duty, or the like, and was used in the Mediterranean area. It came to mean the cost of damage sustained at sea. From that came an \"average adjuster\" who decided how to apportion a loss between the owners and insurers of a ship and cargo.\n\nMarine damage is either \"particular average\", which is borne only by the owner of the damaged property, or general average, where the owner can claim a proportional contribution from all the parties to the marine venture. The type of calculations used in adjusting general average gave rise to the use of \"average\" to mean \"arithmetic mean\".\n\nA second English usage, documented as early as 1674 and sometimes spelled \"averish\", is as the residue and second growth of field crops, which were considered suited to consumption by draught animals (\"avers\").\n\nThe root is found in Arabic as \"awar\", in Italian as \"avaria\", in French as \"avarie\" and in Dutch as \"averij\". It is unclear in which language the word first appeared.\n\nThere is earlier (from at least the 11th century), unrelated use of the word. It appears to be an old legal term for a tenant's day labour obligation to a sheriff, probably anglicised from \"avera\" found in the English Domesday Book (1085).\n\n\n"}
{"id": "16046999", "url": "https://en.wikipedia.org/wiki?curid=16046999", "title": "BCK algebra", "text": "BCK algebra\n\nIn mathematics, BCI and BCK algebras are algebraic structures, introduced by Y. Imai, K. Iséki and S. Tanaka in 1966, that describe fragments of the propositional calculus involving implication known as BCI and BCK logics.\n\nAn algebra formula_1 of type formula_2 is called a \"BCI-algebra\" if, for any formula_3, it satisfies the following conditions. (Informally, we may read formula_4 as \"truth\" and formula_5 as \"formula_6 implies formula_7\".)\n\nA BCI-algebra formula_13 is called a \"BCK-algebra\" if it\nsatisfies the following condition:\n\nA partial order can then be defined as \"x\" ≤ \"y\" iff x * y = 0.\n\nA BCK-algebra is said to be \"commutative\" if it satisfies:\nIn a commutative BCK-algebra \"x\" * (\"x\" * \"y\") = \"x\" ∧ \"y\" is the greatest lower bound of \"x\" and \"y\" under the partial order ≤.\n\nA BCK-algebra is said to be bounded if it has a largest element, usually denoted by 1. In a bounded commutative BCK-algebra the least upper bound of two elements satisfies \"x\" ∨ \"y\" = 1 * ((1 * \"x\") ∧ (1 * \"y\")); that makes it a distributive lattice.\n\nEvery abelian group is a BCI-algebra, with * defined as group subtraction and 0 defined as the group identity.\n\nThe subsets of a set form a BCK-algebra, where A*B is the difference A\\B (the elements in A but not in B), and 0 is the empty set.\n\nA Boolean algebra is a BCK algebra if \"A\"*\"B\" is defined to be \"A\"∧¬\"B\" (\"A\" does not imply \"B\").\n\nThe bounded commutative BCK-algebras are precisely the MV-algebras.\n\n"}
{"id": "58528675", "url": "https://en.wikipedia.org/wiki?curid=58528675", "title": "Beckenbach Book Prize", "text": "Beckenbach Book Prize\n\nThe Beckenbach Book Prize, formerly known as the Mathematical Association of America Book Prize, is awarded to authors of distinguished, innovative books that have been published by the Mathematical Association of America (MAA). The prize was established in 1983 and first awarded in 1985. The award is $2500 for the honored author and is awarded on an irregular basis. \n\nThe recipients of the Beckenbach Book Prize are:\n"}
{"id": "27753031", "url": "https://en.wikipedia.org/wiki?curid=27753031", "title": "Circuit (computer science)", "text": "Circuit (computer science)\n\nIn theoretical computer science, a circuit is a model of computation in which input values proceed through a sequence of gates, each of which computes a function. Circuits of this kind provide a generalization of Boolean circuits and a mathematical model for digital logic circuits. Circuits are defined by the gates they contain and the values the gates can produce. For example, the values in a Boolean circuit are boolean values, and the circuit includes conjunction, disjunction, and negation gates. The values in an integer circuit are sets of integers and the gates compute set union, set intersection, and set complement, as well as the arithmetic operations addition and multiplication.\n\nA circuit is a triple formula_1, where\n\nThe vertices of the graph are called \"gates\". For each gate formula_10 of in-degree formula_6, the gate formula_10 can be labeled by an element formula_13 of formula_3 if and only if formula_13 is defined on formula_4.\n\nThe gates of in-degree 0 are called \"inputs\" or \"leaves\". The gates of out-degree 0 are called \"outputs\". If there is an edge from gate formula_10 to gate formula_18 in the graph formula_8 then formula_18 is called a \"child\" of formula_10. We suppose there is an order on the vertices of the graph, so we can speak of the formula_22th child of a gate when formula_22 is less than the out-degree of the gate.\n\nThe \"size\" of a circuit is the number of nodes of a circuit. The \"depth of a gate\" formula_10 is the length of the longest path in formula_8 beginning at formula_10 up to an output gate. In particular, the gates of out-degree 0 are the only gates of depth 1. The \"depth of a circuit\" is the maximum depth of any gate. \n\n\"Level formula_6\" is the set of all gates of depth formula_6. A \"levelled circuit\" is a circuit in which the edges to gates of depth formula_6 comes only from gates of depth formula_30 or from the inputs. In other words, edges only exist between adjacent levels of the circuit. The \"width\" of a levelled circuit is the maximum size of any level.\n\nThe exact value formula_31 of a gate formula_10 with in-degree formula_6 and label formula_34 is defined recursively for all gates formula_10.\nwhere each formula_37 is a parent of formula_10.\n\nThe value of the circuit is the value of each of the output gates.\n\nThe labels of the leaves can also be variables which take values in formula_2. If there are formula_40 leaves, then the circuit can be seen as a function from formula_41 to formula_2. It is then usual to consider a family of circuits formula_43, a sequence of circuits indexed by the integers where the circuit formula_44 has formula_40 variables. Families of circuits can thus be seen as functions from formula_46 to formula_2.\n\nThe notions of size, depth and width can be naturally extended to families of functions, becoming functions from formula_48 to formula_48; for example, formula_50 is the size of the formula_40th circuit of the family. \n\nComputing the output of a given Boolean circuit on a specific input is P-complete problem. If the input is an integer circuit, however, it is unknown whether this problem is decidable.\n\nCircuit complexity attempts to classify Boolean functions with respect to the size or depth of circuits that can compute them.\n\n\n"}
{"id": "20271419", "url": "https://en.wikipedia.org/wiki?curid=20271419", "title": "Divergence (computer science)", "text": "Divergence (computer science)\n\nIn computer science, a computation is said to diverge if it does not terminate or terminates in an (unobservable) exceptional state. Otherwise it is said to converge. In domains where computations are expected to be infinite, such as process calculi, a computation is said to diverge if it fails to be productive (always produces an action within a finite amount of time.)\n\nVarious subfields of computer science use varying, but mathematically precise, definitions of what it means for a computation to converge or diverge.\n\nIn abstract rewriting, an abstract rewriting system is called convergent if it is both confluent and terminating.\n\nThe notation \"t\" ↓ \"n\" means that \"t\" reduces to normal form \"n\" in zero or more reductions, \"t\"↓ means \"t\" reduces to some normal form in zero or more reductions, and \"t\"↑ means \"t\" does not reduce to a normal form; the latter is impossible in a terminating rewriting system.\n\nIn the lambda calculus an expression is divergent if it has no normal form.\n\nIn denotational semantics an object function \"f\" : \"A\" → \"B\" can be modelled as a mathematical function \"f\" : \"A\" ∪ {⊥} → \"B\" ∪ {⊥} where ⊥ (bottom) indicates that the object function or its argument diverges.\n\nIn the calculus of communicating sequential processes, divergence is a drastic situation where a process performs an endless series of hidden actions. For example, consider the following process, defined by CSP notation:\nThe traces of this process are defined as:\nNow, consider the following process, which conceals the \"tick\" event of the \"Clock\" process:\nBy definition, \"P\" is called a divergent process.\n\n\n"}
{"id": "24209919", "url": "https://en.wikipedia.org/wiki?curid=24209919", "title": "Errera graph", "text": "Errera graph\n\nIn the mathematical field of graph theory, the Errera graph is a graph with 17 vertices and 45 edges. Alfred Errera published it in 1921 as a counterexample to Kempe's erroneous proof of the four color theorem; it was named after Errera by .\n\nThe Errera graph is planar and has chromatic number 4, chromatic index 6, radius 3, diameter 4 and girth 3. All its vertices are of degree 5 or 6 and it is a 5-vertex-connected graph and a 5-edge-connected graph.\n\nThe Errera graph is not a vertex-transitive graph and its full automorphism group is isomorphic to the dihedral group of order 20, the group of symmetries of a decagon, including both rotations and reflections.\n\nThe characteristic polynomial of the Errera graph is formula_1.\n\nThe four color theorem states that the vertices of every planar graph can be colored with four colors, so that no two adjacent vertices have equal colors. An erroneous proof was published in 1879 by Alfred Kempe, but it was discovered to be erroneous by 1890. The four color theorem was not given a valid proof until 1976.\nKempe's proof can be translated into an algorithm to color planar graphs, which is also erroneous. Counterexamples to his proof were found in 1890 and 1896 (the Poussin graph), and later, the Fritsch graph and Soifer graph provided two smaller counterexamples.\nHowever, until the work of Kempe, these counterexamples did not show that the whole coloring algorithm fails. Rather, they assumed that all but one vertex of the graph had already been colored,\nand showed that Kempe's method (which purportedly would modify the coloring to extend it to the whole graphs) failed in those precolored instances. The Errera graph, on the other hand, provides a counterexample to Kempe's entire method. When this method is run on the Errera graph, starting with no vertices colored, it can fail to find a valid coloring for the whole graph.\nAdditionally, unlike the Poussin graph, all vertices in the Errera graph have degree five or more. Therefore, on this graph, it is impossible to avoid the problematic cases of Kempe's method by choosing lower-degree vertices.\n\nThe figure shows an example of how Kempe's proof can fail for this graph. In the figure, the adjacencies between regions of this map form the Errera graph, partially four-colored with the outer region uncolored. Kempe's erroneous proof follows the idea of extending partial colorings such as this one by recoloring Kempe chains,\nconnected subgraphs that have only two colors. Any such chain can be recolored, preserving the validity of the coloring, by swapping its two colors on all vertices of the chain.\nKempe's proof has different cases depending on whether the next vertex to be colored has three, four, or five neighbors and on how those neighbors are colored. In the case shown, the vertex to be colored next is the one corresponding to the outer region of the map.\nThis region cannot be colored directly, because it already has neighbors of all four different colors. The blue and yellow neighbors are connected by a single Kempe chain (shown by the dashed yellow lines in the image), preventing a swap from making them both blue or both yellow and freeing a color.\nSimilarly, the blue and green neighbors are connected by another Kempe chain (the dashed green lines). In such a case, Kempe's proof would try to simultaneously swap the colors on two Kempe chains, the left red-yellow chain and the right red-green chain (dashed red lines).\nThe blue-green chain blocks the left red-yellow chain from reaching the right side of the graph, and the blue-yellow chain blocks the right red-green chain from reaching the left, so it would seem that simultaneously swapping the colors on these two chains is a safe operation.\nBut because the blue-yellow and blue-green chains cross each other rather than staying separated, there is a region in the middle of the figure where the red-yellow and red-green chains can meet.\nWhen these two chains meet in the middle, the simultaneous swap causes adjacent yellow and green vertices in this middle area (such as the vertices represented by the upper yellow and green regions in the figure) to both become red, producing an invalid coloring.\n\nChemical graph theory concerns the graph-theoretic structure of molecules and other clusters of atoms. Both the Errera graph itself and its dual graph are relevant in this context.\n\nAtoms of metals such as gold can form clusters in which a central atom is surrounded by twelve more atoms, in the pattern of an icosahedron. Another, larger, type of cluster can be formed by coalescing two of these icosahedral clusters, so that the central atom of each cluster becomes one of the boundary atoms for the other cluster.\nThe resulting cluster of 19 atoms has two interior atoms (the centers of the two icosahedra) with 17 atoms in the outer shell in the pattern of the Errera graph.\n\nThe dual graph of the Errera graph is a fullerene with 30 vertices, designated in the chemistry literature as C(D) or F(D) to indicate its symmetry and distinguish it from other 30-vertex fullerenes.\nThis shape also plays a central role in the construction of higher-dimensional fullerenes.\n"}
{"id": "4557668", "url": "https://en.wikipedia.org/wiki?curid=4557668", "title": "Essentially surjective functor", "text": "Essentially surjective functor\n\nIn category theory, a functor \n\nis essentially surjective (or dense) if each object formula_2 of formula_3 is isomorphic to an object of the form formula_4 for some object formula_5 of formula_6. Any functor which is part of an equivalence is essentially surjective.\n"}
{"id": "1058719", "url": "https://en.wikipedia.org/wiki?curid=1058719", "title": "Harmonic spectrum", "text": "Harmonic spectrum\n\nA harmonic spectrum is a spectrum containing only frequency components whose frequencies are whole number multiples of the fundamental frequency; such frequencies are known as harmonics. \"The individual partials are not heard separately but are blended together by the ear into a single tone.\"\n\nIn other words, if formula_1 is the fundamental frequency, then a harmonic spectrum has the form \n\nA standard result of Fourier analysis is that a function has a harmonic spectrum if and only if it is periodic.\n\n"}
{"id": "43263796", "url": "https://en.wikipedia.org/wiki?curid=43263796", "title": "Hilbert–Kunz function", "text": "Hilbert–Kunz function\n\nIn algebra, the Hilbert–Kunz function of a local ring (\"R\", \"m\") of prime characteristic \"p\" is the function\nwhere \"q\" is a power of \"p\" and \"m\" is the ideal generated by the \"q\"-th powers of elements of the maximal ideal \"m\".\n\nThe notion was introduced by Ernst Kunz, who used it to characterize a regular ring as a Noetherian ring in which the Frobenius morphism is flat. If d is the dimension of the local ring, Monsky showed that f(q)/(q^d) is c+O(1/q) for some real constant c. This constant, the \"Hilbert-Kunz\" multiplicity\", is greater than or equal to 1. Watanabe and Yoshida strengthened some of Kunz's results, showing that in the unmixed case, the ring is regular precisely when c=1.\n\nHilbert–Kunz functions and multiplicities have been studied for their own sake. Brenner and Trivedi have treated local rings coming from the homogeneous co-ordinate rings of smooth projective curves, using techniques from algebraic geometry. Han, Monsky and Teixeira have treated diagonal hypersurfaces and various related hypersurfaces. But there is no known technique for determining the Hilbert–Kunz function or c in general. In particular the question of whether c is always rational wasn't settled until recently (by Brenner—it needn't be, and indeed can be transcendental). Hochster and Huneke related Hilbert-Kunz multiplicities to \"tight closure\" and Brenner and Monsky used Hilbert–Kunz functions to show that localization need not preserve tight closure. The question of how c behaves as the characteristic goes to infinity (say for a hypersurface defined by a polynomial with integer coefficients) has also received attention; once again open questions abound.\n\nA comprehensive overview is to be found in Craig Huneke's article \"Hilbert-Kunz multiplicities and the F-signature\" arXiv:1409.0467. This article is also found on pages 485-525 of the Springer volume \"Commutative Algebra: Expository Papers Dedicated to David Eisenbud on the Occasion of His 65th Birthday\", edited by Irena Peeva.\n\n"}
{"id": "51231053", "url": "https://en.wikipedia.org/wiki?curid=51231053", "title": "Homomorphic equivalence", "text": "Homomorphic equivalence\n\nIn the mathematics of graph theory, two graphs, \"G\" and \"H\", are called homomorphically equivalent if there exists a graph homomorphism formula_1 and a graph homomorphism formula_2. An example usage of this notion is that any two cores of a graph are homomorphically equivalent.\n\nHomomorphic equivalence also comes up in the theory of databases. Given a database schema, two instances I and J on it are called homomorphically equivalent if there exists an instance homomorphism formula_3 and an instance homomorphism formula_4.\n\nIn fact for any category \"C\", one can define homomorphic equivalence. It is used in the theory of accessible categories, where \"weak universality\" is the best one can hope for in terms of injectivity classes; see \n"}
{"id": "11000264", "url": "https://en.wikipedia.org/wiki?curid=11000264", "title": "Homomorphic secret sharing", "text": "Homomorphic secret sharing\n\nIn cryptography, homomorphic secret sharing is a type of secret sharing algorithm in which the secret is encrypted via homomorphic encryption. A homomorphism is a transformation from one algebraic structure into another of the same type so that the structure is preserved. Importantly, this means that for every kind of manipulation of the original data, there is a corresponding manipulation of the transformed data.\n\nHomomorphic secret sharing is used to transmit a secret to several recipients as follows:\n\n\nSuppose a community wants to perform an election, but they want to ensure that the vote-counters won't lie about the results. Using a kind of homomorphic secret sharing known as Shamir's secret sharing, each member of the community can put his vote into a form that can be split into pieces, then submit each piece to a different vote-counter. The pieces are designed so that the vote-counters can't predict how altering a piece of a vote will affect the whole vote; thus, vote-counters are discouraged from tampering with their pieces. When all votes have been received, the vote-counters combine all the pieces together, which allows them to reverse the alteration process and to recover the aggregate election results.\n\nIn detail, suppose we have an election with:\n\nAssume the election has two outcomes, so each member of the community can vote either \"yes\" or \"no\". We'll represent those votes numerically by +1 and -1, respectively.\n\n\nThis protocol works as long as not all of the formula_1 authorities are corrupt — if they were, then they could collaborate to reconstruct formula_2 for each voter and also subsequently alter the votes.\n\nThe protocol requires t+1 authorities to be completed, therefore in case there are N>t+1 authorities, N-t-1 authorities can be corrupted, which gives the protocol a certain degree of robustness.\n\nThe protocol manages the IDs of the voters (the IDs were submitted with the ballots) and therefore can verify that only legitimate voters have voted.\n\nUnder the assumptions on t:\n\nThe protocol implicitly prevents corruption of ballots.\nThis is because the authorities have no incentive to change the ballot since each authority has only a share of the ballot and has no knowledge how changing this share will affect the outcome.\n\n\n"}
{"id": "11756462", "url": "https://en.wikipedia.org/wiki?curid=11756462", "title": "Hybrid cryptosystem", "text": "Hybrid cryptosystem\n\nIn cryptography, a hybrid cryptosystem is one which combines the convenience of a public-key cryptosystem with the efficiency of a symmetric-key cryptosystem. Public-key cryptosystems are convenient in that they do not require the sender and receiver to share a common secret in order to communicate securely (among other useful properties). However, they often rely on complicated mathematical computations and are thus generally much more inefficient than comparable symmetric-key cryptosystems. In many applications, the high cost of encrypting long messages in a public-key cryptosystem can be prohibitive. This is addressed by hybrid systems by using a combination of both.\n\nA hybrid cryptosystem can be constructed using any two separate cryptosystems:\n\nThe hybrid cryptosystem is itself a public-key system, whose public and private keys are the same as in the key encapsulation scheme.\n\nNote that for very long messages the bulk of the work in encryption/decryption is done by the more efficient symmetric-key scheme, while the inefficient public-key scheme is used only to encrypt/decrypt a short key value.\n\nAll practical implementations of public key cryptography today employ the use of a hybrid system. Examples include the TLS protocol which uses a public-key mechanism for key exchange (such as Diffie-Hellman) and a symmetric-key mechanism for data encapsulation (such as AES). The OpenPGP (RFC 4880) file format and the PKCS #7 (RFC 2315) file format are other examples.\n\nTo encrypt a message addressed to Alice in a hybrid cryptosystem, Bob does the following:\n\nTo decrypt this hybrid ciphertext, Alice does the following:\n\nIf both the key encapsulation and data encapsulation schemes are secure against adaptive chosen ciphertext attacks, then the hybrid scheme inherits that property as well.\nHowever, it is possible to construct a hybrid scheme secure against adaptive chosen ciphertext attack even if the key encapsulation has a slightly weakened security definition (though the security of the data encapsulation must be slightly stronger).\n"}
{"id": "33731923", "url": "https://en.wikipedia.org/wiki?curid=33731923", "title": "Hyperoperation", "text": "Hyperoperation\n\nIn mathematics, the hyperoperation sequence is an infinite sequence of arithmetic operations (called \"hyperoperations\" in this context) that starts with a unary operation (the successor function with \"n\" = 0). The sequence continues with the binary operations of addition (\"n\" = 1), multiplication (\"n\" = 2), and exponentiation (\"n\" = 3).\n\nAfter that, the sequence proceeds with further binary operations extending beyond exponentiation, using right-associativity. For the operations beyond exponentiation, the \"n\"th member of this sequence is named by Reuben Goodstein after the Greek prefix of \"n\" suffixed with \"-ation\" (such as tetration (\"n\" = 4), pentation (\"n\" = 5), hexation (\"n\" = 6), etc.) and can be written as using \"n\" − 2 arrows in Knuth's up-arrow notation.\nEach hyperoperation may be understood recursively in terms of the previous one by:\n\nIt may also be defined according to the recursion rule part of the definition, as in Knuth's up-arrow version of the Ackermann function:\n\nThis can be used to easily show numbers much larger than those which scientific notation can, such as Skewes' number and googolplexplex (e.g. formula_3 is much larger than Skewes’ number and googolplexplex), but there are some numbers which even they cannot easily show, such as Graham's number and TREE(3).\n\nThis recursion rule is common to many variants of hyperoperations (see below in definition).\n\nThe \"hyperoperation sequence\" formula_4 is the sequence of binary operations formula_5, defined recursively as follows:\n\nFor \"n\" = 0, 1, 2, 3, this definition reproduces the basic arithmetic operations of successor (which is a unary operation), addition, multiplication, and exponentiation, respectively, as\n\nSo what will be the next operation after exponentiation? We defined multiplication so that formula_8, and defined exponentiation so that formula_9 so it seems logical to define the next operation, tetration, so that formula_10 with a tower of three 'a'. Analogously, the pentation of (a, 3) will be tetration(a, tetration(a, a)), with three \"a\" in it.\n\nThe H operations for \"n\" ≥ 3 can be written in Knuth's up-arrow notation as\n\nKnuth's notation could be extended to negative indices ≥ −2 in such a way as to agree with the entire hyperoperation sequence, except for the lag in the indexing:\n\nThe hyperoperations can thus be seen as an answer to the question \"what's next\" in the sequence: successor, addition, multiplication, exponentiation, and so on. Noting that\n\nthe relationship between basic arithmetic operations is illustrated, allowing the higher operations to be defined naturally as above. The parameters of the hyperoperation hierarchy are sometimes referred to by their analogous exponentiation term; so \"a\" is the base, \"b\" is the exponent (or \"hyperexponent\"), and \"n\" is the rank (or \"grade\")., and formula_14 is read as \"the \"b\"th \"n\"-ation of \"a\"\", e.g. formula_15 is read as \"the 9th tetration of 7\", and formula_16 is read as \"the 789th 123-ation of 456\".\n\nIn common terms, the hyperoperations are ways of compounding numbers that increase in growth based on the iteration of the previous hyperoperation. The concepts of successor, addition, multiplication and exponentiation are all hyperoperations; the successor operation (producing \"x\" + 1 from \"x\") is the most primitive, the addition operator specifies the number of times 1 is to be added to itself to produce a final value, multiplication specifies the number of times a number is to be added to itself, and exponentiation refers to the number of times a number is to be multiplied by itself.\n\nBelow is a list of the first seven (0th to 6th) hyperoperations (0⁰ is defined as 1.).\n\n\"H\"(0, \"b\") =\n\n\"H\"(1, \"b\") =\n\n\"H\"(\"a\", 0) =\n\n\"H\"(\"a\", 1) =\n\n\"H\"(\"a\", −1) =\n\n\"H\"(2, 2) =\n\nOne of the earliest discussions of hyperoperations was that of Albert Bennett in 1914, who developed some of the theory of \"commutative hyperoperations\" (see below). About 12 years later, Wilhelm Ackermann defined the function formula_17 which somewhat resembles the hyperoperation sequence.\n\nIn his 1947 paper, R. L. Goodstein introduced the specific sequence of operations that are now called \"hyperoperations\", and also suggested the Greek names tetration, pentation, etc., for the extended operations beyond exponentiation (because they correspond to the indices 4, 5, etc.). As a three-argument function, e.g., formula_18, the hyperoperation sequence as a whole is seen to be a version of the original Ackermann function formula_17 — recursive but not primitive recursive — as modified by Goodstein to incorporate the primitive successor function together with the other three basic operations of arithmetic (addition, multiplication, exponentiation), and to make a more seamless extension of these beyond exponentiation.\n\nThe original three-argument Ackermann function formula_20 uses the same recursion rule as does Goodstein's version of it (i.e., the hyperoperation sequence), but differs from it in two ways. First, formula_17 defines a sequence of operations starting from addition (\"n\" = 0) rather than the successor function, then multiplication (\"n\" = 1), exponentiation (\"n\" = 2), etc. Secondly, the initial conditions for formula_20 result in formula_23, thus differing from the hyperoperations beyond exponentiation. The significance of the \"b\" + 1 in the previous expression is that formula_24 = formula_25, where \"b\" counts the number of \"operators\" (exponentiations), rather than counting the number of \"operands\" (\"a\"s) as does the \"b\" in formula_26, and so on for the higher-level operations. (See the Ackermann function article for details.)\n\nThis is a list of notations that have been used for hyperoperations.\n\nIn 1928, Wilhelm Ackermann defined a 3-argument function formula_17 which gradually evolved into a 2-argument function known as the Ackermann function. The \"original\" Ackermann function formula_20 was less similar to modern hyperoperations, because his initial conditions start with formula_29 for all \"n\" > 2. Also he assigned addition to \"n\" = 0, multiplication to \"n\" = 1 and exponentiation to \"n\" = 2, so the initial conditions produce very different operations for tetration and beyond.\n\nAnother initial condition that has been used is formula_30 (where the base is constant formula_31), due to Rózsa Péter, which does not form a hyperoperation hierarchy.\n\nIn 1984, C. W. Clenshaw and F. W. J. Olver began the discussion of using hyperoperations to prevent computer floating-point overflows. Since then, many other authors have renewed interest in the application of hyperoperations to floating-point representation. (Since \"H\"(\"a\", \"b\") are all defined for \"b\" = -1.) While discussing tetration, Clenshaw \"et al.\" assumed the initial condition formula_32, which makes yet another hyperoperation hierarchy. Just like in the previous variant, the fourth operation is very similar to tetration, but offset by one.\n\nAn alternative for these hyperoperations is obtained by evaluation from left to right. Since\ndefine (with ° or subscript)\nwith\n\nThis was extended to ordinal numbers by Donner and Tarski, by :\n\nIt follows from Definition 1(i), Corollary 2(ii), and Theorem 9, that, for \"a\" ≥ 2 and \"b\" ≥ 1, that \n\nBut this suffers a kind of collapse, failing to form the \"power tower\" traditionally expected of hyperoperators:\n\nIf α ≥ 2 and γ ≥ 2,\n\nCommutative hyperoperations were considered by Albert Bennett as early as 1914, which is possibly the earliest remark about any hyperoperation sequence. Commutative hyperoperations are defined by the recursion rule\nwhich is symmetric in \"a\" and \"b\", meaning all hyperoperations are commutative. This sequence does not contain exponentiation, and so does not form a hyperoperation hierarchy.\n\n"}
{"id": "18722547", "url": "https://en.wikipedia.org/wiki?curid=18722547", "title": "Introduction to the Theory of Computation", "text": "Introduction to the Theory of Computation\n\nIntroduction to the Theory of Computation () is a standard textbook in theoretical computer science, written by Michael Sipser and first published by PWS Publishing in 1997.\n\n\n"}
{"id": "7987653", "url": "https://en.wikipedia.org/wiki?curid=7987653", "title": "Koszul algebra", "text": "Koszul algebra\n\nIn abstract algebra, a Koszul algebra formula_1 is a graded formula_2-algebra over which the ground field formula_2 has a linear minimal graded free resolution, \"i.e.\", there exists an exact sequence:\nIt is named after the French mathematician Jean-Louis Koszul.\n\nHere, formula_5 is the graded algebra formula_1 with grading shifted up by formula_7, \"i.e.\" formula_8. The exponents formula_9 refer to the formula_9-fold direct sum.\n\nWe can choose bases for the free modules in the resolution; then the maps can be written as matrices. For a Koszul algebra, the entries in the matrices are zero or linear forms.\n\nAn example of a Koszul algebra is a polynomial ring over a field, for which the Koszul complex is the minimal graded free resolution of the ground field. There are Koszul algebras whose ground fields have infinite minimal graded free resolutions, \"e.g\", formula_11\n\n\n"}
{"id": "50700194", "url": "https://en.wikipedia.org/wiki?curid=50700194", "title": "Linear difference equation", "text": "Linear difference equation\n\nIn mathematics and in particular dynamical systems, a linear difference equation or linear recurrence relation equates to a polynomial that is linear in the various iterates of a variable—that is, in the values of the elements of a sequence. The polynomial's linearity means that each of its terms has degree 0 or 1. Usually the context is the evolution of some variable over time, with the current time period or discrete moment in time denoted as , one period earlier denoted as , one period later as , etc.\n\nAn \"n\"-th order linear difference equation is one that can be written in terms of parameters \"a\" and \"b\" as\n\nor equivalently as\n\nThe equation is called \"homogeneous\" if formula_3 and \"inhomogeneous\" if formula_4. Since the longest time lag between iterates appearing in the equation is \"n\", this is an \"n\"-th order equation, where \"n\" could be any positive integer. When the longest lag is specified numerically so \"n\" does not appear notationally as the longest time lag, \"n\" is occasionally used instead of \"t\" to index iterates.\n\nIn the most general case the coefficients \"a\" and \"b\" could themselves be functions of time; however, this article treats the most common case, that of constant coefficients. If the coefficients \"a\" are polynomials in the equation is called a linear recurrence equation with polynomial coefficients. \n\nThe \"solution\" of such an equation is a function of time, and not of any iterate values, giving the value of the iterate at any time. To find the solution it is necessary to know the specific values (known as \"initial conditions\") of \"n\" of the iterates, and normally these are the \"n\" iterates that are oldest. The equation or its variable is said to be \"stable\" if from any set of initial conditions the variable's limit as time goes to infinity exists; this limit is called the \"steady state\".\n\nDifference equations are used in a variety of contexts, such as in economics to model the evolution through time of variables such as gross domestic product, the inflation rate, the exchange rate, etc. They are used in modeling such time series because values of these variables are only measured at discrete intervals. In econometric applications, linear difference equations are modeled with stochastic terms in the form of autoregressive (AR) models and in models such as vector autoregression (VAR) and autoregressive moving average (ARMA) models that combine AR with other features.\n\nSolving the homogeneous equation formula_5 involves first solving its characteristic equation\n\nfor its characteristic roots formula_7 \"i\" = 1, ..., \"n\". These roots can be solved for algebraically if \"n\" ≤ 4, but not necessarily otherwise. If the solution is to be used numerically, all the roots of this characteristic equation can be found by numerical methods. However, for use in a theoretical context it may be that the only information required about the roots is whether any of them are greater than or equal to 1 in absolute value.\n\nIt may be that all the roots are real or instead there may be some that are complex numbers. In the latter case, all the complex roots come in complex conjugate pairs.\n\nIf no characteristic roots share the same value, the solution of the homogeneous linear difference equation formula_5 can be written in terms of the characteristic roots as\n\nwhere the coefficients \"c\" can be found by invoking the initial conditions. Specifically, for each time period for which an iterate value is known, this value and its corresponding value of \"t\" can be substituted into the solution equation to obtain a linear equation in the \"n\" as-yet-unknown parameters; \"n\" such equations, one for each initial condition, can be solved simultaneously for the \"n\" parameter values. If all characteristic roots are real, then all the coefficient values \"c\" will also be real; but with non-real complex roots, in general some of these coefficients will also be non-real.\n\nIf there are complex roots, they come in pairs and so do the complex terms in the solution equation. If two of these complex terms are formula_10 and formula_11 the roots formula_12 can be written as\n\nwhere the non-subscript \"i\" is the imaginary unit and \"M\" is the modulus of the roots: formula_14 Then the two complex terms in the solution equation can be written as\n\nwhere formula_16 is the angle whose cosine is formula_17 and whose sine is formula_18 the last equality here made use of de Moivre's formula.\n\nNow the process of finding the coefficients formula_19 and formula_20 guarantees that they are also complex conjugates, which can be written as formula_21 Using this in the last equation gives this expression for the two complex terms in the solution equation:\n\nwhich can also be written as\n\nwhere formula_24 is the angle whose cosine is formula_25 and whose sine is formula_26\n\nDepending on the initial conditions, even with all roots real the iterates can experience a transitory tendency to go above and below the steady state value. But true cyclicality involves a permanent tendency to fluctuate, and this occurs if there is at least one pair of complex conjugate characteristic roots. This can be seen in the trigonometric form of their contribution to the solution equation, involving formula_27 and formula_28\n\nIn the second-order case, if the two roots are identical (formula_29), they can both be denoted as formula_30 and a solution may be of the form\n\nIf \"b\" ≠ 0, the equation\n\nis said to be non-homogeneous. To solve this equation it is convenient to convert it to homogeneous form, with no constant term. This is done by first finding the equation's \"steady state value\"—a value \"y\" * such that, if \"n\" successive iterates all had this value, so would all future values. This value is found by setting all values of \"y\" equal to \"y\" * in the difference equation, and solving, thus obtaining\n\nassuming the denominator is not 0. If it is zero, the steady state does not exist.\n\nGiven the steady state, the difference equation can be rewritten in terms of deviations of the iterates from the steady state, as\n\nwhich has no constant term, and which can be written more succinctly as\n\nwhere \"x\" equals \"y\"–\"y\" *. This is the homogeneous form.\n\nIf there is no steady state, the difference equation\n\ncan be combined with its equivalent form\n\nto obtain (by solving both for \"b\")\n\nin which like terms can be combined to give a homogeneous equation of one order higher than the original.\n\nIn the solution equation formula_9 a term with real characteristic roots converges to 0 as \"t\" grows indefinitely large if the absolute value of the characteristic root is less than 1. If the absolute value equals 1, the term will stay constant as \"t\" grows if the root is +1 but will fluctuate between two values if the root is –1. If the absolute value of the root is greater than 1 the term will become larger and larger over time. A pair of terms with complex conjugate characteristic roots will converge to 0 with dampening fluctuations if the absolute value of the modulus \"M\" of the roots is less than 1; if the modulus equals 1 then constant amplitude fluctuations in the combined terms will persist; and if the modulus is greater than 1, the combined terms will show fluctuations of ever-increasing magnitude.\n\nThus the evolving variable \"x\" will converge to 0 if all of the characteristic roots have magnitude less than 1. \n\nIf the largest root has absolute value 1, neither convergence to 0 nor divergence to infinity will occur. If all roots with magnitude 1 are real and positive, \"x\" will converge to the sum of their constant terms formula_40 unlike in the stable case, this converged value depends on the initial conditions: different starting points lead to different points in the long run. If any root is –1, its term will contribute permanent fluctuations between two values. If any of the unit-magnitude roots are complex then constant-amplitude fluctuations of \"x\" will persist. \n\nFinally, if any characteristic root has magnitude greater than 1, then \"x\" will diverge to infinity as time goes to infinity, or will fluctuate between increasingly large positive and negative values.\n\nA theorem of Issai Schur states that all roots have magnitude less than 1 (the stable case) if and only if a particular string of determinants are all positive.\n\nIf a non-homogeneous linear difference equation has been converted to homogeneous form which has been analyzed as above, then the stability and cyclicality properties of the original non-homogeneous equation will be the same as those of the derived homogeneous form, with convergence in the stable case being to the steady-state value \"y\"* instead of to 0.\n\nAn alternative solution method involves converting the \"n\"-th order difference equation to a first-order matrix difference equation. This is accomplished by writing formula_41 formula_42 formula_43 etc. Then the original single \"n\"-th order equation\n\ncan be replaced by this set of \"n\" first-order equations:\n\nWriting the vector formula_46 as formula_47 this can be put in matrix form as\n\nHere \"A\" is a matrix in which each row after the first has a single 1 with all other elements being 0, and \"B\" is a column vector with first element \"b\" and with the rest of its elements being 0.\n\nThis matrix equation can be solved using the methods in the article Matrix difference equation.\n\n"}
{"id": "147909", "url": "https://en.wikipedia.org/wiki?curid=147909", "title": "Linearity of differentiation", "text": "Linearity of differentiation\n\nIn calculus, the derivative of any linear combination of functions equals the same linear combination of the derivatives of the functions; this property is known as linearity of differentiation, the rule of linearity, or the superposition rule for differentiation. It is a fundamental property of the derivative that encapsulates in a single rule two simpler rules of differentiation, the sum rule (the derivative of the sum of two functions is the sum of the derivatives) and the constant factor rule (the derivative of a constant multiple of a function is the same constant multiple of the derivative). Thus it can be said that the act of differentiation is linear, or the differential operator is a linear operator.\n\nLet and be functions, with and constants. Now consider:\n\nBy the sum rule in differentiation, this is:\n\nBy the constant factor rule in differentiation, this reduces to:\n\nThis in turn leads to:\n\nOmitting the brackets, this is often written as:\n"}
{"id": "351549", "url": "https://en.wikipedia.org/wiki?curid=351549", "title": "List of algebraic geometry topics", "text": "List of algebraic geometry topics\n\nThis is a list of algebraic geometry topics, by Wikipedia page.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "11488892", "url": "https://en.wikipedia.org/wiki?curid=11488892", "title": "Michael H. Albert", "text": "Michael H. Albert\n\nMichael Henry Albert (born September 20, 1962) is a mathematician and computer scientist, originally from Canada, and currently a professor and the head of the computer science department at the University of Otago in Dunedin, New Zealand. His varied research interests include combinatorics and combinatorial game theory.\n\nHe received his B.Math in 1981 from the University of Waterloo. In that year Albert received the Rhodes Scholarship, and he completed his D. Phil. in 1984 at the University of Oxford. He then returned to the University of Waterloo. From 1987 to 1996 he was a professor at Carnegie Mellon University. Albert has been at the University of Otago since 1998.\n\nTogether with J.P. Grossman and Richard Nowakowski, Albert invented the game Clobber. Albert has also contributed to the Combinatorial Game Suite game analysis software, and is a coauthor of \"Lessons in Play: An Introduction to Combinatorial Game Theory\". Another significant topic of his research has been permutation patterns.\n\nAlbert is a keen bridge player, and has won tournaments internationally.\n\n"}
{"id": "49837457", "url": "https://en.wikipedia.org/wiki?curid=49837457", "title": "NQIT", "text": "NQIT\n\nNQIT (Networked Quantum Information Technologies) is a quantum computing research hub established in 2014 as part of the UK National Quantum Technologies Programme. NQIT is a consortium of 9 UK universities and 30 partners, which received funding of £38m over a 5-year period.\n\nBy the end of the 5-year programme NQIT aims to produce the Q20:20 engine, a demonstration of a scalable quantum computer demonstrator comprising an optically-linked network of 20 cells, each cell being a quantum processor with 20 matter qubits.\n\nThe UK National Quantum Technologies Programme was initiated by the UK Chancellor of the Exchequer, George Osborne in the Autumn Statement in 2013 in which he pledged a £270 million investment. A £120 million national network of four Quantum Technology Hubs was announced by Greg Clark in 2014. The NQIT Hub is led by a Director, Professor Ian Walmsley, who provides overall leadership and scientific vision, and two Co-Directors, Professor Dominic O’Brien, who leads the Systems Engineering, and Dr Tim Cook, who leads the Industrial User Engagement activities.\n\nNQIT is led by the University of Oxford and academic partners are the University of Bath, the University of Cambridge, the University of Edinburgh, the University of Leeds, the University of Southampton, the University of Strathclyde, the University of Sussex and the University of Warwick.\n\nWithin the University of Oxford, NQIT works across the Departments of Physics, Engineering, Computer Science and Materials.\n\nNQIT works with 30 industrial and government partners, including Aspen Electronics, the Centre for Quantum Technologies, Covesion Ltd, the Defence Science and Technology Laboratory, Element Six, ETSI, the Fraunhofer Institute for Telecommunications, Google, Lockheed Martin, M Squared Lasers, the UK National Physical Laboratory, Oxford Capital, Oxford Instruments, Pure Lifi, Raytheon UK, Rohde & Schwarz, Satellite Applications Catapult and Toshiba.\n\nNQIT's principal goal is to implement the Q20:20 engine: a hybrid matter/optical quantum computer involving twenty optically linked nodes, where each node is a small quantum processor of twenty qubits. Each processing node will be an ion trap, within which a small number of ions are held suspended in a vacuum and manipulated by laser and microwave systems. Each qubit is implemented within the internal hyperfine states of each ion, and control of the qubits is achieved optically via integrated lasers and through microwave manipulation. Quantum interlinks between the traps will be realized by single photon emissions, which are combined and measured by optical fibres, splitters, switches and detectors.\n\n"}
{"id": "1150998", "url": "https://en.wikipedia.org/wiki?curid=1150998", "title": "Nicolaus II Bernoulli", "text": "Nicolaus II Bernoulli\n\nNicolaus II Bernoulli, a.k.a. Niklaus Bernoulli, Nikolaus Bernoulli, (6 February 1695, Basel, Switzerland – 31 July 1726, St. Petersburg, Russia) was a Swiss mathematician as were his father Johann Bernoulli and one of his brothers, Daniel Bernoulli. He was one of the many prominent mathematicians in the Bernoulli family.\n\nNicolaus worked mostly on curves, differential equations, and probability. He was a friend and contemporary of Leonhard Euler, who studied under Nicolaus' father. He also contributed to fluid dynamics.\n\nHe was older brother of Daniel Bernoulli, to whom he also taught mathematics. Even in his youth he had learned several languages. From the age of 13, he studied mathematics and law at the University of Basel. In 1711 he received his Master's of Philosophy; in 1715 he received a Doctorate in Law. In 1716-17 he was a private tutor in Venice. From 1719 he had the Chair in Mathematics at the University of Padua, as the successor of Giovanni Poleni. He served as an assistant to his father, among other areas, in the correspondence over the priority dispute between Isaac Newton and Leibniz, and also in the priority dispute between his father and the English mathematician Brook Taylor. In 1720 he posed the problem of reciprocal orthogonal trajectories, which was intended as a challenge for the English Newtonians. From 1723 he was a law professor at the Berner Oberen Schule. In 1725 he together with his brother Daniel, with whom he was touring Italy and France at this time, was invited by Peter the Great to the newly founded St. Petersburg Academy. Eight months after his appointment he came down with a fever and died. His professorship was succeeded in 1727 by Leonhard Euler, whom the Bernoulli brothers had recommended. His early death cut short a promising career.\n\n\n"}
{"id": "14289175", "url": "https://en.wikipedia.org/wiki?curid=14289175", "title": "Nonhypotenuse number", "text": "Nonhypotenuse number\n\nIn mathematics, a nonhypotenuse number is a natural number whose square \"cannot\" be written as the sum of two nonzero squares. The name stems from the fact that an edge of length equal to a nonhypotenuse number \"cannot\" form the hypotenuse of a right angle triangle with integer sides.\n\nThe numbers 1, 2, 3 and 4 are all nonhypotenuse numbers. The number 5, however, is \"not\" a nonhypotenuse number as 5 equals 3 + 4.\n\nThe first fifty nonhypotenuse numbers are:\n\nAlthough nonhypotenuse numbers are common among small integers, they become more-and-more sparse for larger numbers. Yet, there are infinitely many nonhypotenuse numbers, and the number of nonhypotenuse numbers not exceeding a value \"x\" scales asymptotically with \"x\"/.\n\nThe nonhypotenuse numbers are those numbers that have no prime factors of the form 4\"k\"+1. Equivalently, any number that cannot be put into the form formula_1 where \"K\", \"m\", and \"n\" are all positive integers, is never a nonhypotenuse number. A number whose prime factors are not of the form 4\"k\"+1 cannot be the hypotenuse of a primitive triangle, but may still be the hypotenuse of a non-primitive triangle.\n\n"}
{"id": "50490527", "url": "https://en.wikipedia.org/wiki?curid=50490527", "title": "Parallel (operator)", "text": "Parallel (operator)\n\nThe parallel operator formula_1 (pronounced \"parallel\") is a mathematical function which is used especially as shorthand in electrical engineering. It represents the reciprocal value of a sum of reciprocal values and is defined by:\n\nformula_2 \n\nThat is, it gives half of the harmonic mean of two numbers a and b.\n"}
{"id": "49861510", "url": "https://en.wikipedia.org/wiki?curid=49861510", "title": "Paul Linden", "text": "Paul Linden\n\nPaul Frederick Linden (born 29 January 1947) is a mathematician specialising in fluid dynamics. He was the third G. I. Taylor Professor of Fluid Mechanics at the University of Cambridge and a fellow of Downing College. \n\nLinden earned his PhD from the University of Cambridge in 1972, under the supervision of Stewart Turner. His thesis was entitled \"The Effect of Turbulence and Shear on Salt Fingers\".\n\nLinden was elected a Fellow of the Royal Society (FRS) in 2007. His certificate of election reads: \n"}
{"id": "34218262", "url": "https://en.wikipedia.org/wiki?curid=34218262", "title": "Philip Ehrlich", "text": "Philip Ehrlich\n\nPhilip Ehrlich is Professor at Department of Philosophy of Ohio University. His main areas of interest are Logic, History of Mathematics, and Philosophy of Science.\n\n\n"}
{"id": "48781", "url": "https://en.wikipedia.org/wiki?curid=48781", "title": "Philosophiæ Naturalis Principia Mathematica", "text": "Philosophiæ Naturalis Principia Mathematica\n\nPhilosophiæ Naturalis Principia Mathematica (Latin for \"Mathematical Principles of Natural Philosophy\"), often referred to as simply the Principia , is a work in three books by Isaac Newton, in Latin, first published 5 July 1687. After annotating and correcting his personal copy of the first edition, Newton published two further editions, in 1713 and 1726. The \"Principia\" states Newton's laws of motion, forming the foundation of classical mechanics; Newton's law of universal gravitation; and a derivation of Kepler's laws of planetary motion (which Kepler first obtained empirically).\n\nThe \"Principia\" is considered one of the most important works in the history of science.\n\nThe French mathematical physicist Alexis Clairaut assessed it in 1747: \"The famous book of \"Mathematical Principles of Natural Philosophy\" marked the epoch of a great revolution in physics. The method followed by its illustrious author Sir Newton ... spread the light of mathematics on a science which up to then had remained in the darkness of conjectures and hypotheses.\"\n\nA more recent assessment has been that while acceptance of Newton's theories was not immediate, by the end of a century after publication in 1687, \"no one could deny that\" (out of the \"Principia\") \"a science had emerged that, at least in certain respects, so far exceeded anything that had ever gone before that it stood alone as the ultimate exemplar of science generally.\"\n\nIn formulating his physical theories, Newton developed and used mathematical methods now included in the field of calculus. But the language of calculus as we know it was largely absent from the \"Principia\"; Newton gave many of his proofs in a geometric form of infinitesimal calculus, based on limits of ratios of vanishing small geometric quantities. In a revised conclusion to the \"Principia\" (see \"General Scholium\"), Newton used his expression that became famous, \"Hypotheses non fingo\" (\"I formulate no hypotheses\").\n\nIn the preface of the \"Principia\", Newton wrote:\n\nThe \"Principia\" deals primarily with massive bodies in motion, initially under a variety of conditions and hypothetical laws of force in both non-resisting and resisting media, thus offering criteria to decide, by observations, which laws of force are operating in phenomena that may be observed. It attempts to cover hypothetical or possible motions both of celestial bodies and of terrestrial projectiles. It explores difficult problems of motions perturbed by multiple attractive forces. Its third and final book deals with the interpretation of observations about the movements of planets and their satellites.\n\nIt shows:\n\nThe opening sections of the \"Principia\" contain, in revised and extended form, nearly all of the content of Newton's 1684 tract \"De motu corporum in gyrum\".\n\nThe \"Principia\" begin with \"Definitions\" and \"Axioms or Laws of Motion\", and continues in three books:\n\nBook 1, subtitled \"De motu corporum\" (\"On the motion of bodies\") concerns motion in the absence of any resisting medium. It opens with a mathematical exposition of \"the method of first and last ratios\", a geometrical form of infinitesimal calculus.\nThe second section establishes relationships between centripetal forces and the law of areas now known as Kepler's second law (Propositions 1–3), and relates circular velocity and radius of path-curvature to radial force (Proposition 4), and relationships between centripetal forces varying as the inverse-square of the distance to the center and orbits of conic-section form (Propositions 5–10).\n\nPropositions 11–31 establish properties of motion in paths of eccentric conic-section form including ellipses, and their relation with inverse-square central forces directed to a focus, and include Newton's theorem about ovals (lemma 28).\n\nPropositions 43–45 are demonstration that in an eccentric orbit under centripetal force where the apse may move, a steady non-moving orientation of the line of apses is an indicator of an inverse-square law of force.\n\nBook 1 contains some proofs with little connection to real-world dynamics. But there are also sections with far-reaching application to the solar system and universe:\n\nPropositions 57–69 deal with the \"motion of bodies drawn to one another by centripetal forces\". This section is of primary interest for its application to the solar system, and includes Proposition 66 along with its 22 corollaries: here Newton took the first steps in the definition and study of the problem of the movements of three massive bodies subject to their mutually perturbing gravitational attractions, a problem which later gained name and fame (among other reasons, for its great difficulty) as the three-body problem.\n\nPropositions 70–84 deal with the attractive forces of spherical bodies. The section contains Newton's proof that a massive spherically symmetrical body attracts other bodies outside itself as if all its mass were concentrated at its centre. This fundamental result, called the Shell theorem, enables the inverse square law of gravitation to be applied to the real solar system to a very close degree of approximation.\n\nPart of the contents originally planned for the first book was divided out into a second book, which largely concerns motion through resisting mediums. Just as Newton examined consequences of different conceivable laws of attraction in Book 1, here he examines different conceivable laws of resistance; thus discusses resistance in direct proportion to velocity, and goes on to examine the implications of resistance in proportion to the square of velocity. Book 2 also discusses (in ) hydrostatics and the properties of compressible fluids. The effects of air resistance on pendulums are studied in , along with Newton's account of experiments that he carried out, to try to find out some characteristics of air resistance in reality by observing the motions of pendulums under different conditions. Newton compares the resistance offered by a medium against motions of globes with different properties (material, weight, size). In Section 8, he derives rules to determine the speed of waves in fluids and relates them to the density and condensation(Proposition 48; this would become very important in acoustics). He assumes that these rules apply equally to light and sound and estimates that the speed of sound is around 1088 feet per second and can increase depending on the amount of water in air.\n\nLess of Book 2 has stood the test of time than of Books 1 and 3, and it has been said that Book 2 was largely written on purpose to refute a theory of Descartes which had some wide acceptance before Newton's work (and for some time after). According to this Cartesian theory of vortices, planetary motions were produced by the whirling of fluid vortices that filled interplanetary space and carried the planets along with them. Newton wrote at the end of Book 2 his conclusion that the hypothesis of vortices was completely at odds with the astronomical phenomena, and served not so much to explain as to confuse them.\n\nBook 3, subtitled \"De mundi systemate\" (\"On the system of the world\"), is an exposition of many consequences of universal gravitation, especially its consequences for astronomy. It builds upon the propositions of the previous books, and applies them with further specificity than in Book 1 to the motions observed in the solar system. Here (introduced by Proposition 22, and continuing in Propositions 25–35) are developed several of the features and irregularities of the orbital motion of the Moon, especially the variation. Newton lists the astronomical observations on which he relies, and establishes in a stepwise manner that the inverse square law of mutual gravitation applies to solar system bodies, starting with the satellites of Jupiter and going on by stages to show that the law is of universal application. He also gives starting at Lemma 4 and Proposition 40 the theory of the motions of comets, for which much data came from John Flamsteed and Edmond Halley, and accounts for the tides, attempting quantitative estimates of the contributions of the Sun and Moon to the tidal motions; and offers the first theory of the precession of the equinoxes. Book 3 also considers the harmonic oscillator in three dimensions, and motion in arbitrary force laws.\n\nIn Book 3 Newton also made clear his heliocentric view of the solar system, modified in a somewhat modern way, since already in the mid-1680s he recognised the \"deviation of the Sun\" from the centre of gravity of the solar system. For Newton, \"the common centre of gravity of the Earth, the Sun and all the Planets is to be esteem'd the Centre of the World\", and that this centre \"either is at rest, or moves uniformly forward in a right line\". Newton rejected the second alternative after adopting the position that \"the centre of the system of the world is immoveable\", which \"is acknowledg'd by all, while some contend that the Earth, others, that the Sun is fix'd in that centre\". Newton estimated the mass ratios Sun:Jupiter and Sun:Saturn, and pointed out that these put the centre of the Sun usually a little way off the common center of gravity, but only a little, the distance at most \"would scarcely amount to one diameter of the Sun\".\n\nThe sequence of definitions used in setting up dynamics in the \"Principia\" is recognisable in many textbooks today. Newton first set out the definition of mass\n\nThis was then used to define the \"quantity of motion\" (today called momentum), and the principle of inertia in which mass replaces the previous Cartesian notion of \"intrinsic force\". This then set the stage for the introduction of forces through the change in momentum of a body. Curiously, for today's readers, the exposition looks dimensionally incorrect, since Newton does not introduce the dimension of time in rates of changes of quantities.\n\nHe defined space and time \"not as they are well known to all\". Instead, he defined \"true\" time and space as \"absolute\" and explained:\n\nTo some modern readers it can appear that some dynamical quantities recognised today were used in the \"Principia\" but not named. The mathematical aspects of the first two books were so clearly consistent that they were easily accepted; for example, Locke asked Huygens whether he could trust the mathematical proofs, and was assured about their correctness.\n\nHowever, the concept of an attractive force acting at a distance received a cooler response. In his notes, Newton wrote that the inverse square law arose naturally due to the structure of matter. However, he retracted this sentence in the published version, where he stated that the motion of planets is consistent with an inverse square law, but refused to speculate on the origin of the law. Huygens and Leibniz noted that the law was incompatible with the notion of the aether. From a Cartesian point of view, therefore, this was a faulty theory. Newton's defence has been adopted since by many famous physicists—he pointed out that the mathematical form of the theory had to be correct since it explained the data, and he refused to speculate further on the basic nature of gravity. The sheer number of phenomena that could be organised by the theory was so impressive that younger \"philosophers\" soon adopted the methods and language of the \"Principia\".\n\nPerhaps to reduce the risk of public misunderstanding, Newton included at the beginning of Book 3 (in the second (1713) and third (1726) editions) a section entitled \"Rules of Reasoning in Philosophy.\" In the four rules, as they came finally to stand in the 1726 edition, Newton effectively offers a methodology for handling unknown phenomena in nature and reaching towards explanations for them. The four Rules of the 1726 edition run as follows (omitting some explanatory comments that follow each):\n\nRule 1: \"We are to admit no more causes of natural things than such as are both true and sufficient to explain their appearances.\"\n\nRule 2: \"Therefore to the same natural effects we must, as far as possible, assign the same causes.\"\n\nRule 3: \"The qualities of bodies, which admit neither intensification nor remission of degrees, and which are found to belong to all bodies within the reach of our experiments, are to be esteemed the universal qualities of all bodies whatsoever.\"\n\nRule 4: \"In experimental philosophy we are to look upon propositions inferred by general induction from phenomena as accurately or very nearly true, not withstanding any contrary hypothesis that may be imagined, till such time as other phenomena occur, by which they may either be made more accurate, or liable to exceptions.\"\n\nThis section of Rules for philosophy is followed by a listing of 'Phenomena', in which are listed a number of mainly astronomical observations, that Newton used as the basis for inferences later on, as if adopting a consensus set of facts from the astronomers of his time.\n\nBoth the 'Rules' and the 'Phenomena' evolved from one edition of the \"Principia\" to the next. Rule 4 made its appearance in the third (1726) edition; Rules 1–3 were present as 'Rules' in the second (1713) edition, and predecessors of them were also present in the first edition of 1687, but there they had a different heading: they were not given as 'Rules', but rather in the first (1687) edition the predecessors of the three later 'Rules', and of most of the later 'Phenomena', were all lumped together under a single heading 'Hypotheses' (in which the third item was the predecessor of a heavy revision that gave the later Rule 3).\n\nFrom this textual evolution, it appears that Newton wanted by the later headings 'Rules' and 'Phenomena' to clarify for his readers his view of the roles to be played by these various statements.\n\nIn the third (1726) edition of the \"Principia\", Newton explains each rule in an alternative way and/or gives an example to back up what the rule is claiming. The first rule is explained as a philosophers' principle of economy. The second rule states that if one cause is assigned to a natural effect, then the same cause so far as possible must be assigned to natural effects of the same kind: for example respiration in humans and in animals, fires in the home and in the Sun, or the reflection of light whether it occurs terrestrially or from the planets. An extensive explanation is given of the third rule, concerning the qualities of bodies, and Newton discusses here the generalisation of observational results, with a caution against making up fancies contrary to experiments, and use of the rules to illustrate the observation of gravity and space.\n\nIsaac Newton’s statement of the four rules revolutionised the investigation of phenomena. With these rules, Newton could in principle begin to address all of the world’s present unsolved mysteries. He was able to use his new analytical method to replace that of Aristotle, and he was able to use his method to tweak and update Galileo’s experimental method. The re-creation of Galileo's method has never been significantly changed and in its substance, scientists use it today.\n\nThe \"General Scholium\" is a concluding essay added to the second edition, 1713 (and amended in the third edition, 1726). It is not to be confused with the \"General Scholium\" at the end of Book 2, Section 6, which discusses his pendulum experiments and resistance due to air, water, and other fluids.\n\nHere Newton used what became his famous expression Hypotheses non fingo, \"I formulate no hypotheses\", in response to criticisms of the first edition of the \"Principia\". (\"<nowiki>'Fingo'</nowiki>\" is sometimes nowadays translated 'feign' rather than the traditional 'frame'.) Newton's gravitational attraction, an invisible force able to act over vast distances, had led to criticism that he had introduced \"occult agencies\" into science. Newton firmly rejected such criticisms and wrote that it was enough that the phenomena implied gravitational attraction, as they did; but the phenomena did not so far indicate the cause of this gravity, and it was both unnecessary and improper to frame hypotheses of things not implied by the phenomena: such hypotheses \"have no place in experimental philosophy\", in contrast to the proper way in which \"particular propositions are inferr'd from the phenomena and afterwards rendered general by induction\".\n\nNewton also underlined his criticism of the vortex theory of planetary motions, of Descartes, pointing to its incompatibility with the highly eccentric orbits of comets, which carry them \"through all parts of the heavens indifferently\".\n\nNewton also gave theological argument. From the system of the world, he inferred the existence of a Lord God, along lines similar to what is sometimes called the argument from intelligent or purposive design. It has been suggested that Newton gave \"an oblique argument for a unitarian conception of God and an implicit attack on the doctrine of the Trinity\", but the General Scholium appears to say nothing specifically about these matters.\n\nIn January 1684, Edmond Halley, Christopher Wren and Robert Hooke had a conversation in which Hooke claimed to not only have derived the inverse-square law, but also all the laws of planetary motion. Wren was unconvinced, Hooke did not produce the claimed derivation although the others gave him time to do it, and Halley, who could derive the inverse-square law for the restricted circular case (by substituting Kepler's relation into Huygens' formula for the centrifugal force) but failed to derive the relation generally, resolved to ask Newton.\n\nHalley's visits to Newton in 1684 thus resulted from Halley's debates about planetary motion with Wren and Hooke, and they seem to have provided Newton with the incentive and spur to develop and write what became \"Philosophiae Naturalis Principia Mathematica\". Halley was at that time a Fellow and Council member of the Royal Society in London (positions that in 1686 he resigned to become the Society's paid Clerk). Halley's visit to Newton in Cambridge in 1684 probably occurred in August. When Halley asked Newton's opinion on the problem of planetary motions discussed earlier that year between Halley, Hooke and Wren, Newton surprised Halley by saying that he had already made the derivations some time ago; but that he could not find the papers. (Matching accounts of this meeting come from Halley and Abraham De Moivre to whom Newton confided.) Halley then had to wait for Newton to 'find' the results, but in November 1684 Newton sent Halley an amplified version of whatever previous work Newton had done on the subject. This took the form of a 9-page manuscript, \"De motu corporum in gyrum\" (\"Of the motion of bodies in an orbit\"): the title is shown on some surviving copies, although the (lost) original may have been without title.\n\nNewton's tract \"De motu corporum in gyrum\", which he sent to Halley in late 1684, derived what are now known as the three laws of Kepler, assuming an inverse square law of force, and generalised the result to conic sections. It also extended the methodology by adding the solution of a problem on the motion of a body through a resisting medium. The contents of \"De motu\" so excited Halley by their mathematical and physical originality and far-reaching implications for astronomical theory, that he immediately went to visit Newton again, in November 1684, to ask Newton to let the Royal Society have more of such work. The results of their meetings clearly helped to stimulate Newton with the enthusiasm needed to take his investigations of mathematical problems much further in this area of physical science, and he did so in a period of highly concentrated work that lasted at least until mid-1686.\n\nNewton's single-minded attention to his work generally, and to his project during this time, is shown by later reminiscences from his secretary and copyist of the period, Humphrey Newton. His account tells of Isaac Newton's absorption in his studies, how he sometimes forgot his food, or his sleep, or the state of his clothes, and how when he took a walk in his garden he would sometimes rush back to his room with some new thought, not even waiting to sit before beginning to write it down. Other evidence also shows Newton's absorption in the \"Principia\": Newton for years kept up a regular programme of chemical or alchemical experiments, and he normally kept dated notes of them, but for a period from May 1684 to April 1686, Newton's chemical notebooks have no entries at all. So it seems that Newton abandoned pursuits to which he was normally dedicated, and did very little else for well over a year and a half, but concentrated on developing and writing what became his great work.\n\nThe first of the three constituent books was sent to Halley for the printer in spring 1686, and the other two books somewhat later. The complete work, published by Halley at his own financial risk, appeared in July 1687. Newton had also communicated \"De motu\" to Flamsteed, and during the period of composition he exchanged a few letters with Flamsteed about observational data on the planets, eventually acknowledging Flamsteed's contributions in the published version of the \"Principia\" of 1687.\n\nThe process of writing that first edition of the \"Principia\" went through several stages and drafts: some parts of the preliminary materials still survive, while others are lost except for fragments and cross-references in other documents.\n\nSurviving materials show that Newton (up to some time in 1685) conceived his book as a two-volume work. The first volume was to be titled \"De motu corporum, Liber primus\", with contents that later appeared in extended form as Book 1 of the \"Principia\".\n\nA fair-copy draft of Newton's planned second volume \"De motu corporum, Liber secundus\" survives, its completion dated to about the summer of 1685. It covers the application of the results of \"Liber primus\" to the Earth, the Moon, the tides, the solar system, and the universe; in this respect it has much the same purpose as the final Book 3 of the \"Principia\", but it is written much less formally and is more easily read.\nIt is not known just why Newton changed his mind so radically about the final form of what had been a readable narrative in \"De motu corporum, Liber secundus\" of 1685, but he largely started afresh in a new, tighter, and less accessible mathematical style, eventually to produce Book 3 of the \"Principia\" as we know it. Newton frankly admitted that this change of style was deliberate when he wrote that he had (first) composed this book \"in a popular method, that it might be read by many\", but to \"prevent the disputes\" by readers who could not \"lay aside the[ir] prejudices\", he had \"reduced\" it \"into the form of propositions (in the mathematical way) which should be read by those only, who had first made themselves masters of the principles established in the preceding books\". The final Book 3 also contained in addition some further important quantitative results arrived at by Newton in the meantime, especially about the theory of the motions of comets, and some of the perturbations of the motions of the Moon.\n\nThe result was numbered Book 3 of the \"Principia\" rather than Book 2, because in the meantime, drafts of \"Liber primus\" had expanded and Newton had divided it into two books. The new and final Book 2 was concerned largely with the motions of bodies through resisting mediums.\n\nBut the \"Liber secundus\" of 1685 can still be read today. Even after it was superseded by Book 3 of the \"Principia\", it survived complete, in more than one manuscript. After Newton's death in 1727, the relatively accessible character of its writing encouraged the publication of an English translation in 1728 (by persons still unknown, not authorised by Newton's heirs). It appeared under the English title \"A Treatise of the System of the World\". This had some amendments relative to Newton's manuscript of 1685, mostly to remove cross-references that used obsolete numbering to cite the propositions of an early draft of Book 1 of the \"Principia\". Newton's heirs shortly afterwards published the Latin version in their possession, also in 1728, under the (new) title \"De Mundi Systemate\", amended to update cross-references, citations and diagrams to those of the later editions of the \"Principia\", making it look superficially as if it had been written by Newton after the \"Principia\", rather than before. The \"System of the World\" was sufficiently popular to stimulate two revisions (with similar changes as in the Latin printing), a second edition (1731), and a 'corrected' reprint of the second edition (1740).\n\nThe text of the first of the three books of the \"Principia\" was presented to the Royal Society at the close of April 1686. Hooke made some priority claims (but failed to substantiate them), causing some delay. When Hooke's claim was made known to Newton, who hated disputes, Newton threatened to withdraw and suppress Book 3 altogether, but Halley, showing considerable diplomatic skills, tactfully persuaded Newton to withdraw his threat and let it go forward to publication. Samuel Pepys, as President, gave his imprimatur on 30 June 1686, licensing the book for publication. The Society had just spent its book budget on a \"History of Fishes\", and the cost of publication was borne by Edmund Halley (who was also then acting as publisher of the \"Philosophical Transactions of the Royal Society\"): the book appeared in summer 1687.\n\nNicolaus Copernicus had moved the Earth away from the center of the universe with the heliocentric theory for which he presented evidence in his book \"De revolutionibus orbium coelestium\" (\"On the revolutions of the heavenly spheres\") published in 1543. The structure was completed when Johannes Kepler wrote the book \"Astronomia nova\" (\"A new astronomy\") in 1609, setting out the evidence that planets move in elliptical orbits with the sun at one focus, and that planets do not move with constant speed along this orbit. Rather, their speed varies so that the line joining the centres of the sun and a planet sweeps out equal areas in equal times. To these two laws he added a third a decade later, in his book \"Harmonices Mundi\" (\"Harmonies of the world\"). This law sets out a proportionality between the third power of the characteristic distance of a planet from the sun and the square of the length of its year.\n\nThe foundation of modern dynamics was set out in Galileo's book \"Dialogo sopra i due massimi sistemi del mondo\" (\"Dialogue on the two main world systems\") where the notion of inertia was implicit and used. In addition, Galileo's experiments with inclined planes had yielded precise mathematical relations between elapsed time and acceleration, velocity or distance for uniform and uniformly accelerated motion of bodies.\n\nDescartes' book of 1644 \"Principia philosophiae\" (\"Principles of philosophy\") stated that bodies can act on each other only through contact: a principle that induced people, among them himself, to hypothesize a universal medium as the carrier of interactions such as light and gravity—the aether. Newton was criticized for apparently introducing forces that acted at distance without any medium. Not until the development of particle theory was Descartes' notion vindicated when it was possible to describe all interactions, like the strong, weak, and electromagnetic fundamental interactions, using mediating gauge bosons and gravity through hypothesized gravitons. Although he was mistaken in his treatment of circular motion, this effort was more fruitful in the short term when it led others to identify circular motion as a problem raised by the principle of inertia. Christiaan Huygens solved this problem in the 1650s and published it much later in 1673 in his book \"Horologium oscillatorium sive de motu pendulorum\".\n\nNewton had studied these books, or, in some cases, secondary sources based on them, and taken notes entitled \"Quaestiones quaedam philosophicae\" (\"Questions about philosophy\") during his days as an undergraduate. During this period (1664–1666) he created the basis of calculus, and performed the first experiments in the optics of colour. At this time, his proof that white light was a combination of primary colours (found via prismatics) replaced the prevailing theory of colours and received an overwhelmingly favourable response, and occasioned bitter disputes with Robert Hooke and others, which forced him to sharpen his ideas to the point where he already composed sections of his later book \"Opticks\" by the 1670s in response. Work on calculus is shown in various papers and letters, including two to Leibniz. He became a fellow of the Royal Society and the second Lucasian Professor of Mathematics (succeeding Isaac Barrow) at Trinity College, Cambridge.\n\nIn the 1660s Newton studied the motion of colliding bodies, and deduced that the centre of mass of two colliding bodies remains in uniform motion. Surviving manuscripts of the 1660s also show Newton's interest in planetary motion and that by 1669 he had shown, for a circular case of planetary motion, that the force he called 'endeavour to recede' (now called centrifugal force) had an inverse-square relation with distance from the center. After his 1679–1680 correspondence with Hooke, described below, Newton adopted the language of inward or centripetal force. According to Newton scholar J Bruce Brackenridge, although much has been made of the change in language and difference of point of view, as between centrifugal or centripetal forces, the actual computations and proofs remained the same either way. They also involved the combination of tangential and radial displacements, which Newton was making in the 1660s. The difference between the centrifugal and centripetal points of view, though a significant change of perspective, did not change the analysis. Newton also clearly expressed the concept of linear inertia in the 1660s: for this Newton was indebted to Descartes' work published 1644.\n\nHooke published his ideas about gravitation in the 1660s and again in 1674. He argued for an attracting principle of gravitation in \"Micrographia\" of 1665, in a 1666 Royal Society lecture \"On gravity\", and again in 1674, when he published his ideas about the \"System of the World\" in somewhat developed form, as an addition to \"An Attempt to Prove the Motion of the Earth from Observations\". Hooke clearly postulated mutual attractions between the Sun and planets, in a way that increased with nearness to the attracting body, along with a principle of linear inertia. Hooke's statements up to 1674 made no mention, however, that an inverse square law applies or might apply to these attractions. Hooke's gravitation was also not yet universal, though it approached universality more closely than previous hypotheses. Hooke also did not provide accompanying evidence or mathematical demonstration. On these two aspects, Hooke stated in 1674: \"Now what these several degrees [of gravitational attraction] are I have not yet experimentally verified\" (indicating that he did not yet know what law the gravitation might follow); and as to his whole proposal: \"This I only hint at present\", \"having my self many other things in hand which I would first compleat, and therefore cannot so well attend it\" (i.e., \"prosecuting this Inquiry\").\n\nIn November 1679, Hooke began an exchange of letters with Newton, of which the full text is now published. Hooke told Newton that Hooke had been appointed to manage the Royal Society's correspondence, and wished to hear from members about their researches, or their views about the researches of others; and as if to whet Newton's interest, he asked what Newton thought about various matters, giving a whole list, mentioning \"compounding the celestial motions of the planets of a direct motion by the tangent and an attractive motion towards the central body\", and \"my hypothesis of the lawes or causes of springinesse\", and then a new hypothesis from Paris about planetary motions (which Hooke described at length), and then efforts to carry out or improve national surveys, the difference of latitude between London and Cambridge, and other items. Newton's reply offered \"a fansy of my own\" about a terrestrial experiment (not a proposal about celestial motions) which might detect the Earth's motion, by the use of a body first suspended in air and then dropped to let it fall. The main point was to indicate how Newton thought the falling body could experimentally reveal the Earth's motion by its direction of deviation from the vertical, but he went on hypothetically to consider how its motion could continue if the solid Earth had not been in the way (on a spiral path to the centre). Hooke disagreed with Newton's idea of how the body would continue to move. A short further correspondence developed, and towards the end of it Hooke, writing on 6 January 1680 to Newton, communicated his \"supposition ... that the Attraction always is in a duplicate proportion to the Distance from the Center Reciprocall, and Consequently that the Velocity will be in a subduplicate proportion to the Attraction and Consequently as Kepler Supposes Reciprocall to the Distance.\" (Hooke's inference about the velocity was actually incorrect.)\n\nIn 1686, when the first book of Newton's \"Principia\" was presented to the Royal Society, Hooke claimed that Newton had obtained from him the \"notion\" of \"the rule of the decrease of Gravity, being reciprocally as the squares of the distances from the Center\". At the same time (according to Edmond Halley's contemporary report) Hooke agreed that \"the Demonstration of the Curves generated therby\" was wholly Newton's.\n\nA recent assessment about the early history of the inverse square law is that \"by the late 1660s,\" the assumption of an \"inverse proportion between gravity and the square of distance was rather common and had been advanced by a number of different people for different reasons\". Newton himself had shown in the 1660s that for planetary motion under a circular assumption, force in the radial direction had an inverse-square relation with distance from the center. Newton, faced in May 1686 with Hooke's claim on the inverse square law, denied that Hooke was to be credited as author of the idea, giving reasons including the citation of prior work by others before Hooke. Newton also firmly claimed that even if it had happened that he had first heard of the inverse square proportion from Hooke, which it had not, he would still have some rights to it in view of his mathematical developments and demonstrations, which enabled observations to be relied on as evidence of its accuracy, while Hooke, without mathematical demonstrations and evidence in favour of the supposition, could only guess (according to Newton) that it was approximately valid \"at great distances from the center\".\n\nThe background described above shows there was basis for Newton to deny deriving the inverse square law from Hooke. On the other hand, Newton did accept and acknowledge, in all editions of the \"Principia\", that Hooke (but not exclusively Hooke) had separately appreciated the inverse square law in the solar system. Newton acknowledged Wren, Hooke and Halley in this connection in the Scholium to Proposition 4 in Book 1. Newton also acknowledged to Halley that his correspondence with Hooke in 1679–80 had reawakened his dormant interest in astronomical matters, but that did not mean, according to Newton, that Hooke had told Newton anything new or original: \"yet am I not beholden to him for any light into that business but only for the diversion he gave me from my other studies to think on these things & for his dogmaticalness in writing as if he had found the motion in the Ellipsis, which inclined me to try it ...\".) Newton's reawakening interest in astronomy received further stimulus by the appearance of a comet in the winter of 1680/1681, on which he corresponded with John Flamsteed.\n\nIn 1759, decades after the deaths of both Newton and Hooke, Alexis Clairaut, mathematical astronomer eminent in his own right in the field of gravitational studies, made his assessment after reviewing what Hooke had published on gravitation. \"One must not think that this idea ... of Hooke diminishes Newton's glory\", Clairaut wrote; \"The example of Hooke\" serves \"to show what a distance there is between a truth that is glimpsed and a truth that is demonstrated\".\n\nSince only between 250 and 400 copies were printed by the Royal Society, the first edition is very rare. Several rare-book collections contain first edition and other early copies of Newton's \"Principia Mathematica\", including:\n\n\nIn 2016, a first edition sold for $3.7 million.\nA facsimile edition (based on the 3rd edition of 1726 but with variant readings from earlier editions and important annotations) was published in 1972 by Alexandre Koyré and I. Bernard Cohen.\nTwo later editions were published by Newton:\n\nNewton had been urged to make a new edition of the \"Principia\" since the early 1690s, partly because copies of the first edition had already become very rare and expensive within a few years after 1687. Newton referred to his plans for a second edition in correspondence with Flamsteed in November 1694: Newton also maintained annotated copies of the first edition specially bound up with interleaves on which he could note his revisions; two of these copies still survive: but he had not completed the revisions by 1708, and of two would-be editors, Newton had almost severed connections with one, Nicolas Fatio de Duillier, and the other, David Gregory seems not to have met with Newton's approval and was also terminally ill, dying later in 1708. Nevertheless, reasons were accumulating not to put off the new edition any longer. Richard Bentley, master of Trinity College, persuaded Newton to allow him to undertake a second edition, and in June 1708 Bentley wrote to Newton with a specimen print of the first sheet, at the same time expressing the (unfulfilled) hope that Newton had made progress towards finishing the revisions. It seems that Bentley then realised that the editorship was technically too difficult for him, and with Newton's consent he appointed Roger Cotes, Plumian professor of astronomy at Trinity, to undertake the editorship for him as a kind of deputy (but Bentley still made the publishing arrangements and had the financial responsibility and profit). The correspondence of 1709–1713 shows Cotes reporting to two masters, Bentley and Newton, and managing (and often correcting) a large and important set of revisions to which Newton sometimes could not give his full attention. Under the weight of Cotes' efforts, but impeded by priority disputes between Newton and Leibniz, and by troubles at the Mint, Cotes was able to announce publication to Newton on 30 June 1713. Bentley sent Newton only six presentation copies; Cotes was unpaid; Newton omitted any acknowledgement to Cotes.\n\nAmong those who gave Newton corrections for the Second Edition were: Firmin Abauzit, Roger Cotes and David Gregory. However, Newton omitted acknowledgements to some because of the priority disputes. John Flamsteed, the Astronomer Royal, suffered this especially.\n\nThe Second Edition was the basis of the first edition to be printed abroad, which appeared in Amsterdam in 1714.\n\nThe third edition was published 25 March 1726, under the stewardship of \"Henry Pemberton, M.D., a man of the greatest skill in these matters...\"; Pemberton later said that this recognition was worth more to him than the two hundred guinea award from Newton.\n\nIn 1739–42, two French priests, Pères Thomas LeSeur and François Jacquier (of the Minim order, but sometimes erroneously identified as Jesuits), produced with the assistance of J.-L. Calandrini an extensively annotated version of the \"Principia\" in the 3rd edition of 1726. Sometimes this is referred to as the \"Jesuit edition\": it was much used, and reprinted more than once in Scotland during the 19th century.\n\nÉmilie du Châtelet also made a translation of Newton's Principia into French. Unlike LeSeur and Jacquier's edition, hers was a complete translation of Newton's three books and their prefaces. She also included a Commentary section where she fused the three books into a much clearer and easier to understand summary. She included an analytical section where she applied the new mathematics of calculus to Newton's most controversial theories. Previously, geometry was the standard mathematics used to analyse theories. Du Châtelet's translation is the only complete one to have been done in French and hers remains the standard French translation to this day.\n\nTwo full English translations of Newton's \"Principia\" have appeared, both based on Newton's 3rd edition of 1726.\n\nThe first, from 1729, by Andrew Motte, was described by Newton scholar I. Bernard Cohen (in 1968) as \"still of enormous value in conveying to us the sense of Newton's words in their own time, and it is generally faithful to the original: clear, and well written\". The 1729 version was the basis for several republications, often incorporating revisions, among them a widely used modernised English version of 1934, which appeared under the editorial name of Florian Cajori (though completed and published only some years after his death). Cohen pointed out ways in which the 18th-century terminology and punctuation of the 1729 translation might be confusing to modern readers, but he also made severe criticisms of the 1934 modernised English version, and showed that the revisions had been made without regard to the original, also demonstrating gross errors \"that provided the final impetus to our decision to produce a wholly new translation\".\n\nThe second full English translation, into modern English, is the work that resulted from this decision by collaborating translators I. Bernard Cohen, Anne Whitman, and Julia Budenz; it was published in 1999 with a guide by way of introduction.\n\nWilliam H. Donahue has published a translation of the work's central argument, published in 1996, along with expansion of included proofs and ample commentary. The book was developed as a textbook for classes at St. John's College and the aim of this translation is to be faithful to the Latin text.\n\nIn 2014, British astronaut Tim Peake named his upcoming mission to the International Space Station \"Principia\" after the book, in \"honour of Britain's greatest scientist\". Tim Peake's \"Principia\" launched on December 15, 2015 aboard Soyuz TMA-19M.\n\n\n\n\n\n\n\n\n"}
{"id": "56127111", "url": "https://en.wikipedia.org/wiki?curid=56127111", "title": "Po-Shen Loh", "text": "Po-Shen Loh\n\nPo-Shen Loh (born June 18, 1982) is an associate professor of mathematics at Carnegie Mellon University and currently the national coach of the United States' International Math Olympiad team. Under his coaching, the team won the competition in 2015, 2016, and 2018, their first victories since 1994. He had previously won a silver medal for the US as a participant in 1999. Loh runs a popular course to train students for the William Lowell Putnam Mathematical Competition known as Putnam Seminar, and is the founder of the educational website Expii. In alternating semesters he teaches CMU's undergraduate course on discrete mathematics and the graduate seminar on extremal combinatorics. \n"}
{"id": "637733", "url": "https://en.wikipedia.org/wiki?curid=637733", "title": "Pockels effect", "text": "Pockels effect\n\nThe Pockels effect (after Friedrich Carl Alwin Pockels who studied the effect in 1893), or Pockels electro-optic effect, changes or produces birefringence in an optical medium induced by an electric field. In the Pockels effect, also known as the linear electro-optic effect, the birefringence is proportional to the electric field. In the Kerr effect, the refractive index change (birefringence) is proportional to the square of the field. The Pockels effect occurs only in crystals that lack inversion symmetry, such as lithium niobate, and in other noncentrosymmetric media such as electric-field poled polymers or glasses.\n\nPockels cells are voltage-controlled wave plates. The Pockels effect is the basis of the operation of Pockels cells. Pockels cells may be used to rotate the polarization of a beam that passes through. See applications below for uses.\n\nA transverse Pockels cell consists of two crystals in opposite orientation, which together give a zero-order wave plate when the voltage is turned off. This is often not perfect and drifts with temperature. But the mechanical alignment of the crystal axis is not so critical and is often done by hand without screws; while misalignment leads to some energy in the wrong ray (either \"e\" or \"o\"for example, horizontal or vertical), in contrast to the longitudinal case, the loss is not amplified through the length of the crystal.\n\nThe electric field can be applied to the crystal medium either longitudinally or transversely to the light beam. Longitudinal Pockels cells need transparent or ring electrodes. Transverse voltage requirements can be reduced by lengthening the crystal.\n\nAlignment of the crystal axis with the ray axis is critical. Misalignment leads to birefringence and to a large phase shift across the long crystal. This leads to polarization rotation if the alignment is not exactly parallel or perpendicular to the polarization.\n\nBecause of the high relative dielectric constant of ε ≈ 36 inside the crystal, changes in the electric field propagate at a speed of only \"c\"/6. Fast non-fiber optic cells are thus embedded into a matched transmission line. Putting it at the end of a transmission line leads to reflections and doubled switching time. The signal from the driver is split into parallel lines that lead to both ends of the crystal. When they meet in the crystal, their voltages add up.\nPockels cells for fibre optics may employ a traveling wave design to reduce current requirements and increase speed.\n\nUsable crystals also exhibit the piezoelectric effect to some degree (RTP has the lowest, BBO and lithium niobate are high). After a voltage change, sound waves start propagating from the sides of the crystal to the middle. This is important not for pulse pickers, but for boxcar windows. Guard space between the light and the faces of the crystals needs to be larger for longer holding times.\nBehind the sound wave the crystal stays deformed in the equilibrium position for the high electric field.\nThis increases the polarization. Due to the growing of the polarized volume the electric field in the crystal in front of the wave increases\nlinearly, or the driver has to provide a constant current leakage.\n\nThe driver must withstand the doubled voltage returned to it. Pockels cells behave like a capacitor. When switching these to high voltage, a high charge is needed; consequently, 3 ns switching requires about 40 A for a 5 mm aperture.\nShorter cables reduce the amount of charge wasted in transporting current to the cell.\n\nThe driver may employ many transistors connected parallel and serial.\nThe transistors are floating and need DC isolation for their gates.\nTo do this, the gate signal is connected via optical fiber, or the gates are driven by a large transformer.\nIn this case, careful compensation for feedback is needed to prevent oscillation.\n\nThe driver may employ a cascade of transistors and a triode.\nIn a classic, commercial circuit the last transistor is an IRF830 MOSFET and the triode is an Eimac Y690 triode.\nThe setup with a single triode has the lowest capacity; this even justifies turning off the cell by applying the double voltage.\nA resistor ensures the leakage current needed by the crystal and later to recharge the storage capacitor.\nThe Y690 switches up to 10 kV and the cathode delivers 40 A if the grid is on +400 V.\nIn this case the grid current is 8 A and the input impedance is thus 50 ohms, which matches standard coaxial cables, and the MOSFET can thus be placed remotely. Some of the 50 ohms are spent on an additional resistor which pulls the bias on −100 V.\nThe IRF can switch 500 volts. It can deliver 18 A pulsed.\nIts leads function as an inductance, a storage capacitor is employed, the 50 ohm coax cable is connected, the MOSFET has an internal resistance,\nand in the end this is a critically damped RLC circuit, which is fired by a pulse to the gate of the MOSFET.\n\nThe gate needs 5 V pulses (range: ±20 V) while provided with 22 nC.\nThus the current gain of this transistor is one for 3 ns switching, but it still has voltage gain.\nThus it could theoretically also be used in common gate configuration and not in common source configuration.\nTransistors, which switch 40 V are typically faster, so in the previous stage a current gain is possible.\n\nPockels cells are used in a variety of scientific and technical applications. A Pockels cell, combined with a polarizer, can be used for switching between no optical rotation and 90° rotation creates a fast shutter capable of \"opening\" and \"closing\" in nanoseconds. The same technique can be used to impress information on the beam by modulating the rotation between 0° and 90°; the exiting beam's intensity, when viewed through the polarizer, contains an amplitude-modulated signal. This modulated signal can be used for time-resolved electric field measurements when a crystal is exposed to an unknown electric field.\n\nPockels cells are used for preventing the feedback of a laser cavity by using a polarizing prism. This prevents optical amplification by directing light of a certain polarization out of the cavity. Because of this, the gain medium is pumped to a highly excited state. When the medium has become saturated by energy, the Pockels cell is switched, and the intracavity light is allowed to exit. This creates a very fast, high-intensity pulse. Q-switching, chirped pulse amplification, and cavity dumping use this technique.\n\nPockels cells can be used for quantum key distribution by polarizing photons.\n\nPockels cells in conjunction with other EO elements can be combined to form electro-optic probes.\n\nA Pockels cell was used by MCA Disco-Vision (DiscoVision) engineers in the optical videodisc mastering system. Light from an argon-ion laser was passed through the Pockels cell to create pulse modulations corresponding to the original FM video and audio signals to be recorded on the master videodisc. MCA used the Pockels cell in videodisc mastering until the sale to Pioneer Electronics. To increase the quality of the recordings, MCA patented a Pockels cell stabilizer that reduced the second-harmonic distortion that could be created by the Pockels cell during mastering. MCA used either a DRAW (Direct Read After Write) mastering system or a photoresist system. The DRAW system was originally preferred, since it didn't require clean-room conditions during disc recording and allowed instant quality checking during mastering. The original single-sided test pressings from 1976/77 were mastered with the DRAW system as were the \"educational\", non-feature titles at the format's release in December 1978.\n\nPockels cells are used in two-photon microscopy.\n\n\n"}
{"id": "10650443", "url": "https://en.wikipedia.org/wiki?curid=10650443", "title": "Preimage theorem", "text": "Preimage theorem\n\nIn mathematics, particularly in the field of differential topology, the preimage theorem is a variation of the implicit function theorem concerning the preimage of particular points in a manifold under the action of a smooth map.\n\n\"Definition.\" Let formula_1 be a smooth map between manifolds. We say that a point formula_2 is a \"regular value of f\" if for all formula_3 the map formula_4 is surjective. Here, formula_5 and formula_6 are the tangent spaces of X and Y at the points x and y.\n\"Theorem.\" Let formula_1 be a smooth map, and let formula_2 be a regular value of \"f\"; then formula_9 is a submanifold of X. If formula_10, then the codimension of formula_9 is equal to the dimension of Y. Also, the tangent space of formula_9 at formula_13 is equal to formula_14.\n"}
{"id": "3090886", "url": "https://en.wikipedia.org/wiki?curid=3090886", "title": "Proofs of quadratic reciprocity", "text": "Proofs of quadratic reciprocity\n\nIn number theory, the law of quadratic reciprocity, like the Pythagorean theorem, has lent itself to an unusual number of proofs. Several hundred proofs of the law of quadratic reciprocity have been found.\n\nOf relatively elementary, combinatorial proofs, there are two which apply types of double counting. One by Gotthold Eisenstein counts lattice points. Another applies Zolotarev's lemma to formula_1 \"expressed by the Chinese remainder theorem as\" formula_2 \"and calculates the signature of a permutation.\"\n\nEisenstein's proof of quadratic reciprocity is a simplification of Gauss's third proof. It is more geometrically intuitive and requires less technical manipulation.\n\nThe point of departure is \"Eisenstein's lemma\", which states that for distinct odd primes \"p\", \"q\",\nwhere formula_4 denotes the floor function (the largest integer less than or equal to \"x\"), and where the sum is taken over the \"even\" integers \"u\" = 2, 4, 6, ..., \"p\"−1. For example,\nThis result is very similar to Gauss's lemma, and can be proved in a similar fashion (proof given below).\n\nUsing this representation of (\"q\"/\"p\"), the main argument is quite elegant. The sum formula_6 counts the number of lattice points with even \"x\"-coordinate in the interior of the triangle ABC in the following diagram:\n\nBecause each column has an even number of points (namely \"q\"−1 points), the number of such lattice points in the region BCYX is the same \"modulo 2\" as the number of such points in the region CZY:\n\nThen by flipping the diagram in both axes, we see that the number of points with even \"x\"-coordinate inside CZY is the same as the number of points inside AXY having \"odd\" \"x\"-coordinates:\n\nThe conclusion is that\nwhere μ is the \"total\" number of lattice points in the interior of AYX. Switching \"p\" and \"q\", the same argument shows that\nwhere ν is the number of lattice points in the interior of WYA. Since there are no lattice points on the line AY itself (because \"p\" and \"q\" are relatively prime), and since the total number of points in the rectangle WYXA is\nwe obtain finally\n\nFor an even integer \"u\" in the range 1 ≤ \"u\" ≤ \"p\"−1, denote by \"r\"(\"u\") the least positive residue of \"qu\" modulo \"p\". (For example, for \"p\" = 11, \"q\" = 7, we allow \"u\" = 2, 4, 6, 8, 10, and the corresponding values of \"r\"(\"u\") are 3, 6, 9, 1, 4.) The numbers (−1)\"r\"(\"u\"), again treated as least positive residues modulo \"p\", are all \"even\" (in our running example, they are 8, 6, 2, 10, 4.) Furthermore, they are all distinct, because if (−1)\"r\"(\"u\") ≡ (−1)\"r\"(\"t\") (mod \"p\"), then we may divide out by \"q\" to obtain \"u\" ≡ ±\"t\" (mod \"p\"). This forces \"u\" ≡ \"t\" (mod \"p\"), because both \"u\" and \"t\" are \"even\", whereas \"p\" is odd. Since there exactly (\"p\"−1)/2 of them and they are distinct, they must be simply a rearrangement of the even integers 2, 4, ..., \"p\"−1. Multiplying them together, we obtain\nDividing out successively by 2, 4, ..., \"p\"−1 on both sides (which is permissible since none of them are divisible by \"p\") and rearranging, we have\nOn the other hand, by the definition of \"r\"(\"u\") and the floor function,\nand so since \"p\" is odd and \"u\" is even, we see that formula_14 and \"r\"(\"u\") are congruent modulo 2. Finally this shows that\nWe are finished because the left hand side is just an alternative expression for (\"q\"/\"p\").\n\nThe proof of Quadratic Reciprocity using Gauss sums is one of the more common and classic proofs. These proofs work by comparing computations of single values in two different ways, one using Euler's Criterion and the other using the Binomial theorem. As an example of how Euler's criterion is used, we can use it to give a quick proof of the first supplemental case of determining formula_16 for an odd prime \"p\": By Euler's criterion formula_17 , but since both sides of the equivalence are ±1 and \"p\" is odd, we can deduce that formula_18.\n\nLet formula_19, a primitive 8th root of unity and set formula_20. Since formula_21 and formula_22 we see that formula_23. Because formula_24 is an algebraic integer, if \"p\" is an odd prime it makes sense to talk about it modulo \"p\". (Formally we are considering the commutative ring formed by factoring the algebraic integers formula_25 with the ideal generated by \"p\". Because formula_26 is not an algebraic integer, 1, 2, ..., \"p\" are distinct elements of formula_27.) Using Euler's criterion, it follows that formula_28We can then say that formula_29But we can also compute formula_30 using the binomial theorem. Because the cross terms in the binomial expansion all contain factors of \"p\", we find that formula_31. We can evaluate this more exactly by breaking this up into two cases \nThese are the only options for a prime modulo 8 and both of these cases can be computed using the exponential form formula_34. We can write this succinctly for all odd primes \"p\" as formula_35Combining these two expressions for formula_36 and multiplying through by formula_24 we find that formula_38. Since both formula_39 and formula_40 are ±1 and 2 is invertible modulo \"p\", we can conclude that formula_41\n\nThe idea for the general proof follows the above supplemental case: Find an algebraic integer that somehow encodes the Legendre symbols for \"p\", then find a relationship between Legendre symbols by computing the \"q\"th power of this algebraic integer modulo \"q\" in two different ways, one using Euler's criterion the other using the binomial theorem.\n\nLet formula_42where formula_43 is a primitive \"p\"th root of unity. This is a Quadratic Gauss Sum. A fundamental property of these Gauss sums is that formula_44where formula_45. To put this in context of the next proof, the individual elements of the Gauss sum are in the cyclotomic field formula_46 but the above formula shows that the sum itself is a generator of the unique quadratic field contained in \"L\". Again, since the quadratic Gauss sum is an algebraic integer, we can use modular arithmetic with it. Using this fundamental formula and Euler's criterion we find thatformula_47Thereforeformula_48Using the binomial theorem, we also find that formula_49, If we let \"a\" be a multiplicative inverse of formula_50, then we can rewrite this sum as formula_51 using the substitution formula_52, which doesn't affect the range of the sum. Since formula_53, we can then writeformula_54Using these two expressions for formula_55, and multiplying through by formula_56 givesformula_57Since formula_58 is invertible modulo \"q\", and the Legendre symbols are either ±1, we can then conclude thatformula_59\n\nThe proof presented here is by no means the simplest known; however, it is quite a deep one, in the sense that it motivates some of the ideas of Artin reciprocity.\n\nSuppose that \"p\" is an odd prime. The action takes place inside the cyclotomic field\nformula_60\nwhere ζ is a primitive \"p\" root of unity. The basic theory of cyclotomic fields informs us that there is a canonical isomorphism\nwhich sends the automorphism σ satisfying formula_62 to the element formula_63 In particular, this isomorphism is injective because the multiplicative group of a field is a cyclic group: formula_64.\n\nNow consider the subgroup \"H\" of \"squares\" of elements of \"G\". Since \"G\" is cyclic, \"H\" has index 2 in \"G\", so the subfield corresponding to \"H\" under the Galois correspondence must be a \"quadratic\" extension of Q. (In fact it is the \"unique\" quadratic extension of Q contained in \"L\".) The Gaussian period theory determines which one; it turns out to be formula_65, where\n\nAt this point we start to see a hint of quadratic reciprocity emerging from our framework. On one hand, the image of \"H\" in formula_67 consists precisely of the (nonzero) \"quadratic residues modulo p\". On the other hand, \"H\" is related to an attempt to take the \"square root of p\" (or possibly of −\"p\"). In other words, if now \"q\" is a prime (different from \"p\"), we have shown that\n\nIn the ring of integers formula_69, choose any unramified prime ideal β of lying over \"q\", and let formula_70 be the Frobenius automorphism associated to β; the characteristic property of formula_71 is that\n\nThe key fact about formula_71 that we need is that for any subfield \"K\" of \"L\",\nIndeed, let δ be any ideal of \"O\" below β (and hence above \"q\"). Then, since formula_75 for any formula_76, we see that formula_77 is a Frobenius for δ. A standard result concerning formula_71 is that its order is equal to the corresponding inertial degree; that is,\nThe left hand side is equal to 1 if and only if φ fixes \"K\", and the right hand side is equal to one if and only \"q\" splits completely in \"K\", so we are done.\n\nNow, since the \"p\" roots of unity are distinct modulo β (i.e. the polynomial \"X\" − 1 is separable in characteristic \"q\"), we must have\nthat is, formula_71 coincides with the automorphism σ defined earlier. Taking \"K\" to be the quadratic field in which we are interested, we obtain the equivalence\n\nFinally we must show that\nOnce we have done this, the law of quadratic reciprocity falls out immediately since\nand\nfor formula_86.\n\nTo show the last equivalence, suppose first that formula_87 In this case, there is some integer \"x\" (not divisible by \"q\") such that formula_88 say formula_89 for some integer \"c\". Let\nformula_90 and consider the ideal formula_91 of \"K\". It certainly divides the principal ideal (\"q\"). It cannot be equal to (\"q\"), since formula_92 is not divisible by \"q\". It cannot be the unit ideal, because then\nis divisible by \"q\", which is again impossible. Therefore (\"q\") must split in \"K\".\n\nConversely, suppose that (\"q\") splits, and let β be a prime of \"K\" above \"q\". Then formula_94 so we may choose some\nActually, since formula_96 elementary theory of quadratic fields implies that the ring of integers of \"K\" is precisely formula_97 so the denominators of \"a\" and \"b\" are at worst equal to 2. Since \"q\" ≠ 2, we may safely multiply \"a\" and \"b\" by 2, and assume that formula_98 where now \"a\" and \"b\" are in Z. In this case we have\nso formula_100 However, \"q\" cannot divide \"b\", since then also \"q\" divides \"a\", which contradicts our choice of formula_101 Therefore, we may divide by \"b\" modulo \"q\", to obtain formula_102 as desired.\n\nEvery textbook on elementary number theory (and quite a few on algebraic number theory) has a proof of quadratic reciprocity. Two are especially noteworthy:\n\n\n"}
{"id": "11908435", "url": "https://en.wikipedia.org/wiki?curid=11908435", "title": "Pseudo-monotone operator", "text": "Pseudo-monotone operator\n\nIn mathematics, a pseudo-monotone operator from a reflexive Banach space into its continuous dual space is one that is, in some sense, almost as well-behaved as a monotone operator. Many problems in the calculus of variations can be expressed using operators that are pseudo-monotone, and pseudo-monotonicity in turn implies the existence of solutions to these problems.\n\nLet (\"X\", || ||) be a reflexive Banach space. A map \"T\" : \"X\" → \"X\" from \"X\" into its continuous dual space \"X\" is said to be pseudo-monotone if \"T\" is a bounded operator (not necessarily continuous) and if whenever\n\n(i.e. \"u\" converges weakly to \"u\") and\n\nit follows that, for all \"v\" ∈ \"X\",\n\nUsing a very similar proof to that of the Browder-Minty theorem, one can show the following:\n\nLet (\"X\", || ||) be a real, reflexive Banach space and suppose that \"T\" : \"X\" → \"X\" is bounded, coercive and pseudo-monotone. Then, for each continuous linear functional \"g\" ∈ \"X\", there exists a solution \"u\" ∈ \"X\" of the equation \"T\"(\"u\") = \"g\".\n\n"}
{"id": "24257124", "url": "https://en.wikipedia.org/wiki?curid=24257124", "title": "Randić's molecular connectivity index", "text": "Randić's molecular connectivity index\n\nThe Randić index, also known as the connectivity index, of a graph is the sum of bond contributions formula_1 where formula_2 and formula_3 are\nthe degrees of the vertices making bond \"i\" ~ \"j\".\n\nThis graph invariant was introduced by Milan Randić in 1975. It is often used in chemoinformatics for investigations of organic compounds.\n\n"}
{"id": "3157750", "url": "https://en.wikipedia.org/wiki?curid=3157750", "title": "Redfield ratio", "text": "Redfield ratio\n\nRedfield ratio or Redfield stoichiometry is the atomic ratio of carbon, nitrogen and phosphorus found in phytoplankton and throughout the deep oceans. This empirically developed stoichiometric ratio was originally found to be C:N:P = 106:16:1. This term is named after the American oceanographer Alfred C. Redfield, who first described this ratio in an article written in 1934 (Redfield 1934). As a Harvard physiologist, Redfield participated in several voyages on board the research vessel Atlantis. Alfred Redfield analyzed thousands of samples of marine biomass across all of the ocean regions. From this research he found that globally the elemental composition of marine organic matter (dead and living) was remarkably constant across all of the regions. The stoichiometric ratios of carbon, nitrogen, phosphorus remain relatively consistent from both the coastal to open ocean regions.\n\nFor his 1934 paper, Alfred Redfield analyzed nitrate and phosphate data for Atlantic, Indian, Pacific oceans and Barents Sea, including data published by other researchers. In addition, Redfield analyzed data for C, N, and P content in marine plankton, including data collected by other researchers as early as 1898.\n\nRedfield’s analysis of the empirical data led to him to a startling discovery: across and within the three oceans and Barents Sea, seawater had the N:P atomic ratio near 20:1 (later corrected to 16:1), and was very similar to the average N:P in plankton. Redfield seemed to be deeply puzzled that “the definite correlation exists between the quantity of nitrate and phosphate occurring in any sample” and thought that “it is pertinent to inquire how these proportions agree with those actually found in various members of the plankton community.” (Redfield 1934)\n\nRedfield foresaw that “relation between the concentration of the various organic derivatives in sea water and the chemical composition of plankton would provide a valuable tool in the analysis of many oceanographic problems.” (Redfield 1934)\n\nUnderstanding that the problem is akin to the classical the chicken or the egg causality dilemma, Redfield proposed two mutually non-exclusive mechanisms:\n\nI) The N:P in plankton tends to the N:P composition of seawater. Specifically, phytoplankton species with different N and P requirements compete and, as the result, balance each other so that “the ratio of the elements in the plankton as a whole might come to reflect the ratio of the nutrient substances in sea water rather closely” (Redfield 1934).\n\nII) The N:P in seawater “must tend to approach that characteristic of protoplasm in general” (Redfield 1934). Furthermore, Redfield proposed thermostat like scenario in which the activities of nitrogen fixers and denitrifiers keep the nitrate to phosphate ratio in the seawater near the requirements in the protoplasm. Considering that at the time little was known about the composition of “protoplasm,” Redfield did not attempt to explain why its N:P ratio should be approximately 16:1.\n\nIn 1958, almost a quarter century after first discovering the ratios, Redfield leaned toward the latter mechanism proposing in his seminal manuscript the idea of \"the biological control of chemical factors\" in the ocean (Redfield, 1958). Redfield proposed that the ratio of Nitrogen to Phosphorus in plankton resulted in the global ocean having a remarkably similar ratio of dissolved nitrate to phosphate (16:1). He considered how the cycles of not just N and P but also C and O could interact to result in this match.\n\nThe research that resulted in this ratio has become a fundamental feature in the understanding of the biogeochemical cycles of the oceans, and one of the key tenets of biogeochemistry. The Redfield ratio is instrumental in estimating carbon and nutrient fluxes in global circulation models. They also help in determining which nutrients are limiting in a localized system, if there is a limiting nutrient. The ratio can also be used to understand the formation of phytoplankton blooms and subsequently hypoxia by comparing the ratio between different regions, such as a comparison of the Redfield Ratio of the Mississippi River to the ratio of the northern Gulf of Mexico.\n\nRedfield discovered the remarkable congruence between the chemistry of the deep ocean and the chemistry of living things such as phytoplankton in the surface ocean. Both have N:P ratios of about 16:1 in terms of atoms. When nutrients are not limiting, the molar elemental ratio C:N:P in most phytoplankton is 106:16:1. Redfield thought it wasn't purely coincidental that the vast oceans would have a chemistry perfectly suited to the requirements of living organisms.\n\nIn the ocean a large portion of the biomass is found to be nitrogen-rich plankton. Many of these plankton are consumed by other plankton biomass which have similar chemical compositions. This results in a similar nitrogen to phosphorus ratio, on average, for all the plankton throughout the world’s ocean, empirically found to be averaging approximately 16:1. When these organisms sink into the ocean interior, their energy-rich bodies are consumed by bacteria that, in aerobic conditions, oxidize the organic matter to form dissolved inorganic nutrients, mainly carbon dioxide, nitrate, and phosphate.\n\nThat the nitrate to phosphate ratio in the interior of all of the major ocean basins is highly similar is possibly due to the residence times of these elements in the ocean relative to the oceans circulation time, roughly 100 000 years for phosphorus and 2000 years for nitrogen. The fact that the residence times of these elements are greater than the mixing times of the oceans (~ 1000 years) can result in the ratio of nitrate to phosphate in the ocean interior remaining fairly constant.\n\nWhile such arguments can potentially explain why the ratios are fairly constant, they do not address the question why the N:P ratio is nearly 16 and not some other number.\n\nThe Redfield ratio was initially derived empirically from measurements of the elemental composition of plankton in addition to the nitrate and phosphate content of seawater collected from a few stations in the Atlantic Ocean. This was later supported by hundreds of independent measurements. However, looking at the composition of individual species of phytoplankton grown under nitrogen or phosphorus limitation shows that this nitrogen to phosphorus ratio can vary anywhere from 6:1 to 60:1. While understanding this problem, Redfield never attempted to explain it with the exception of noting that the N:P ratio of inorganic nutrients in the ocean interior was an average with small scale variability to be expected.\n\nAlthough the Redfield ratio is remarkably stable in the deep ocean, phytoplankton may have large variations in the C:N:P composition, and their life strategy play a role in the C:N:P ratio, which has made some researchers speculate that the Redfield ratio perhaps is a general average rather than specific requirement for phytoplankton growth (e.g., Arrigo 2005). However, the Redfield ratio was recently found to be related to a homeostatic protein-to-rRNA ratio fundamentally present in both prokaryotes and eukaryotes (Loladze and Elser 2011). Furthermore, the Redfield ratio has been shown to vary at different spatial scales as well as average slightly higher (166:20:1) than Redfield's original estimate (Sterner et al. 2008).\n\nDespite reports that the elemental composition of organisms such as marine phytoplankton in an oceanic region do not conform to the canonical Redfield ratio, the fundamental concept of this ratio continues to remain valid and useful.\n\nSome feel that there are other elements, such as potassium, sulfur, zinc, copper, and iron are also important in the ocean chemistry. In particular, iron (Fe) was considered of great importance as early biological oceanographers hypothesized that iron may also be a limiting factor for primary production in the ocean. As a result an extended Redfield ratio was developed to include this as part of this balance. This new stoichiometric ratio states that the ratio should be 106 C:16 N:1 P:0.1-0.001 Fe. The variation in iron is the result of “…iron contamination on ships and in labs is large and difficult to control. No one has been able to beat this nearly insuperable combination of difficulties.” (Broecker and Peng (1982)). It is this contamination that resulted in early evidence suggesting that iron concentrations were high and not a limiting factor in marine primary production.\n\nDiatoms need, among other nutrients, silicic acid to create biogenic silica for their frustules (cell walls). As a result of this the Redfield-Brzezinski nutrient ratio was proposed for diatoms and stated to be C:Si:N:P = 106:15:16:1 (Brzezinski, 1985).\n\n\nIn 2014, an article was released in the Scientific Data journal, which aggregated Redfield ratios measurements from observational cruises around the world from 1970 to 2010. This article gave a large database usable to study the evolution of particular phosphorus, carbon and nitrogen across sea stations and time.\n\n"}
{"id": "33590159", "url": "https://en.wikipedia.org/wiki?curid=33590159", "title": "René Taton", "text": "René Taton\n\nRené Taton (4 April 1915 – 9 August 2004) was a French author, historian of science, and long co-editor (along with Suzanne Delorme) of the \"Revue d'histoire des sciences\".\n\nHe was born on 4 April 1915 in L'Échelle.\n\nHe died on 9 August 2004 in Ajaccio.\n\nIn 1935, he became a student of École normale supérieure de Saint-Cloud.\n\nHe was the co-editor of \"Journal of history of science\".\n\nHis \"General History of Science (last edition in 1996, PUF, Quadriga)\" is a major reference in the field of history of science\n\n\n"}
{"id": "44457082", "url": "https://en.wikipedia.org/wiki?curid=44457082", "title": "Robustness of complex networks", "text": "Robustness of complex networks\n\nRobustness, the ability to withstand failures and perturbations, is a critical attribute of many complex systems including complex networks.\n\nThe study of robustness in complex networks is important for many fields.\nIn ecology, robustness is an important attribute of ecosystems, and can give insight into the reaction to disturbances such as the extinction of species. For biologists, network robustness can help the study of diseases and mutations, and how to recover from some mutations. In economics, network robustness principles can help understanding of the stability and risks of banking systems. And in engineering, network robustness can help to evaluate the resilience of infrastructure networks such as the Internet or power grids.\n\nThe focus of robustness in complex networks is the response of the network to the removal of nodes or links. The mathematical model of such a process can be thought of as an inverse percolation process. Percolation theory models the process of randomly placing pebbles on an n-dimensional lattice with probability p, and predicts the sudden formation of a single large cluster at a critical probability formula_1. In percolation theory this cluster is named the percolating cluster. This phenomenon is quantified in percolation theory by a number of quantities, for example the average cluster size formula_2. This quantity represents the average size of all finite clusters and is given by the following equation.\n\nformula_3\n\nWe can see the average cluster size suddenly diverges around the critical probability, indicating the formation of a single large cluster. It is also important to note that the exponent formula_4 is universal for all lattices, while formula_1 is not. This is important as it indicates a universal phase transition behavior, at a point dependent on the topology. The problem of robustness in complex networks can be seen as starting with the percolating cluster, and removing a critical fraction of the pebbles for the cluster to break down. Analogous to the formation of the percolation cluster in percolation theory, the breaking down of a complex network happens abruptly during a phase transition at some critical fraction of nodes removed.\n\nThe mathematical derivation for the threshold at which a complex network will lose its giant component is based on the Molloy–Reed criterion.\n\nformula_6\n\nThe Molloy–Reed criterion is derived from the basic principle that in order for a giant component to exist, on average each node in the network must have at least two links. This is analogous to each person holding two others' hands in order to form a chain. Using this criterion and an involved mathematical proof, one can derive a critical threshold for the fraction of nodes needed to be removed for the breakdown of the giant component of a complex network.\n\nformula_7\n\nAn important property of this finding is that the critical threshold is only dependent on the first and second moment of the degree distribution and is valid for an arbitrary degree distribution.\n\nUsing formula_8 for an Erdős–Rényi (ER) random graph, one can re-express the critical point for a random network.\n\nformula_9\n\nAs a random network gets denser, the critical threshold increases, meaning a higher fraction of the nodes must be removed to disconnect the giant component.\n\nBy re-expressing the critical threshold as a function of the gamma exponent for a scale-free network, we can draw a couple of important conclusions regarding scale-free network robustness.\n\nformula_10\n\nFor gamma greater than 3, the critical threshold only depends on gamma and the minimum degree, and in this regime the network acts like a random network breaking when a finite fraction of its nodes are removed. For gamma less than 3, formula_11 diverges in the limit as N trends toward infinity. In this case, for large scale-free networks, the critical threshold approaches 1. This essentially means almost all nodes must be removed in order to destroy the giant component, and large scale-free networks are very robust with regard to random failures. One can make intuitive sense of this conclusion by thinking about the heterogeneity of scale-free networks and of the hubs in particular. Because there are relatively few hubs, they are less likely to be removed through random failures while small low-degree nodes are more likely to be removed. Because the low-degree nodes are of little importance in connecting the giant component, their removal has little impact.\n\nAlthough scale-free networks are resilient to random failures, we might imagine them being quite vulnerable to targeted hub removal. In this case we consider the robustness of scale free networks in response to targeted attacks, performed with thorough prior knowledge of the network topology. By considering the changes induced by the removal of a hub, specifically the change in the maximum degree and the degrees of the connected nodes, we can derive another formula for the critical threshold considering targeted attacks on a scale free network.\n\nformula_12\n\nThis equation cannot be solved analytically, but can be graphed numerically. To summarize the important points, when gamma is large, the network acts as a random network, and attack robustness become similar to random failure robustness of a random network. However, when gamma is smaller, the critical threshold for attacks on scale-free networks becomes relatively small, indicating a weakness to targeted attacks.\n\nFor more detailed information on the attack tolerance of complex networks please see the attack tolerance page.\n\nAn important aspect of failures in many networks is that a single failure in one node might induce failures in neighboring nodes. When a small number of failures induces more failures, resulting in a large number of failures relative to the network size, a cascading failure has occurred. There are many models for cascading failures. These models differ in many details, and model different physical propagation phenomenon from power failures to information flow over Twitter, but have some shared principals. Each model focuses on some sort of propagation or cascade, there is some threshold determining when a node will fail or activate and contribute towards propagation, and there is some mechanism defined by which propagation will be directed when nodes fail or activate. All of these models predict some critical state, in which the distribution of the size of potential cascades matches a power law, and the exponent is uniquely determined by the degree exponent of the underlying network. Because of the differences in the models and the consensus of this result, we are led to believe the underlying phenomenon is universal and model-independent.\n\nFor more detailed information on modeling cascading failures, see the global cascades model page.\n"}
{"id": "2031353", "url": "https://en.wikipedia.org/wiki?curid=2031353", "title": "Schanuel's conjecture", "text": "Schanuel's conjecture\n\nIn mathematics, specifically transcendental number theory, Schanuel's conjecture is a conjecture made by Stephen Schanuel in the 1960s concerning the transcendence degree of certain field extensions of the rational numbers.\n\nThe conjecture is as follows:\n\nThe conjecture can be found in Lang (1966).\n\nThe conjecture, if proven, would generalize most known results in transcendental number theory. The special case where the numbers \"z\"...,\"z\" are all algebraic is the Lindemann–Weierstrass theorem. If, on the other hand, the numbers are chosen so as to make exp(\"z\")...,exp(\"z\") all algebraic then one would prove that linearly independent logarithms of algebraic numbers are algebraically independent, a strengthening of Baker's theorem.\n\nThe Gelfond–Schneider theorem follows from this strengthened version of Baker's theorem, as does the currently unproven four exponentials conjecture.\n\nSchanuel's conjecture, if proved, would also settle whether numbers such as \"e\" + π and \"e\" are algebraic or transcendental, and prove that \"e\" and π are algebraically independent simply by setting \"z\" = 1 and \"z\" = π\"i\", and using Euler's identity.\n\nEuler's identity states that \"e\" + 1 = 0. If Schanuel's conjecture is true then this is, in some precise sense involving exponential rings, the \"only\" relation between \"e\", π, and \"i\" over the complex numbers.\n\nAlthough ostensibly a problem in number theory, the conjecture has implications in model theory as well. Angus Macintyre and Alex Wilkie, for example, proved that the theory of the real field with exponentiation, , is decidable provided Schanuel's conjecture is true. In fact they only needed the real version of the conjecture, defined below, to prove this result, which would be a positive solution to Tarski's exponential function problem.\n\nThe converse Schanuel conjecture is the following statement: \n\nA version of Schanuel's conjecture for formal power series, also by Schanuel, was proven by James Ax in 1971. It states:\n\nAs stated above, the decidability of follows from the real version of Schanuel's conjecture which is as follows:\nA related conjecture called the uniform real Schanuel's conjecture essentially says the same but puts a bound on the integers \"m\". The uniform real version of the conjecture is equivalent to the standard real version. Macintyre and Wilkie showed that a consequence of Schanuel's conjecture, which they dubbed the Weak Schanuel's conjecture, was equivalent to the decidability of . This conjecture states that there is a computable upper bound on the norm of non-singular solutions to systems of exponential polynomials; this is, non-obviously, a consequence of Schanuel's conjecture for the reals.\n\nIt is also known that Schanuel's conjecture would be a consequence of conjectural results in the theory of motives. There Grothendieck's period conjecture for an abelian variety \"A\" states that the transcendence degree of its period matrix is the same as the dimension of the associated Mumford–Tate group, and what is known by work of Pierre Deligne is that the dimension is an upper bound for the transcendence degree. Bertolin has shown how a generalised period conjecture includes Schanuel's conjecture.\n\nWhile a proof of Schanuel's conjecture with number theoretic tools seems a long way off, connections with model theory have prompted a surge of research on the conjecture.\n\nIn 2004, Boris Zilber systematically constructed exponential fields \"K\" that are algebraically closed and of characteristic zero, and such that one of these fields exists for each uncountable cardinality. He axiomatised these fields and, using Hrushovski's construction and techniques inspired by work of Shelah on categoricity in infinitary logics, proved that this theory of \"pseudo-exponentiation\" has a unique model in each uncountable cardinal. Schanuel's conjecture is part of this axiomatisation, and so the natural conjecture that the unique model of cardinality continuum is actually isomorphic to the complex exponential field implies Schanuel's conjecture. In fact, Zilber showed that this conjecture holds if and only if both Schanuel's conjecture and another unproven condition on the complex exponentiation field, which Zilber calls exponential-algebraic closedness, hold.\n"}
{"id": "8187273", "url": "https://en.wikipedia.org/wiki?curid=8187273", "title": "Schur test", "text": "Schur test\n\nIn mathematical analysis, the Schur test, named after German mathematician Issai Schur, is a bound on the formula_1 operator norm of an integral operator in terms of its Schwartz kernel (see Schwartz kernel theorem).\n\nHere is one version. Let formula_2 be two measurable spaces (such as formula_3). Let formula_4 be an integral operator with the non-negative Schwartz kernel formula_5, formula_6, formula_7:\n\nIf there exist real functions formula_9 and formula_10 and numbers formula_11 such that\n\nfor almost all formula_13 and\n\nfor almost all formula_15, then formula_4 extends to a continuous operator formula_17 with the operator norm\n\nSuch functions formula_19, formula_20 are called the Schur test functions.\n\nIn the original version, formula_4 is a matrix and formula_22.\n\nA common usage of the Schur test is to take formula_23 Then we get:\n\nThis inequality is valid no matter whether the Schwartz kernel formula_5 is non-negative or not.\n\nA similar statement about formula_26 operator norms is known as Young's inequality for integral operators:\n\nif\n\nwhere formula_28 satisfies formula_29, for some formula_30, then the operator formula_31 extends to a continuous operator formula_32, with formula_33\n\nUsing the Cauchy–Schwarz inequality and the inequality (1), we get:\n\nIntegrating the above relation in formula_35, using Fubini's Theorem, and applying the inequality (2), we get:\n\nIt follows that formula_37 for any formula_38.\n\n"}
{"id": "4015574", "url": "https://en.wikipedia.org/wiki?curid=4015574", "title": "Sicherman dice", "text": "Sicherman dice\n\nSicherman dice are the only pair of 6-sided dice that are not normal dice, bear only positive integers, and have the same probability distribution for the sum as normal dice.\n\nThe faces on the dice are numbered 1, 2, 2, 3, 3, 4 and 1, 3, 4, 5, 6, 8.\n\nA standard exercise in elementary combinatorics is to calculate the number of ways of rolling any given value with a pair of fair six-sided dice (by taking the sum of the two rolls). The table shows the number of such ways of rolling a given value formula_1: \n\nCrazy dice is a mathematical exercise in elementary combinatorics, involving a re-labeling of the faces of a pair of six-sided dice to reproduce the same frequency of sums as the standard labeling. The Sicherman dice are crazy dice that are re-labeled with only positive integers. \n\nThe table below lists all possible totals of dice rolls with standard dice and Sicherman dice. One Sicherman die is coloured for clarity: 1–2–\"2\"–3–\"3\"–4, and the other is all black, 1–3–4–5–6–8.\nThe Sicherman dice were discovered by George Sicherman of Buffalo, New York and were originally reported by Martin Gardner in a 1978 article in \"Scientific American\".\n\nThe numbers can be arranged so that all pairs of numbers on opposing sides sum to equal numbers, 5 for the first and 9 for the second.\n\nLater, in a letter to Sicherman, Gardner mentioned that a magician he knew had anticipated Sicherman's discovery. For generalizations of the Sicherman dice to more than two dice and noncubical dice, see Broline (1979), Gallian and Rusin (1979), Brunson and Swift (1997/1998), and Fowler and Swift (1999).\n\nLet a \"canonical\" \"n\"-sided die be an \"n\"-hedron whose faces are marked with the integers [1,n] such that the probability of throwing each number is 1/\"n\". Consider the canonical cubical (six-sided) die. The generating function for the throws of such a die is formula_2. The product of this polynomial with itself yields the generating function for the throws of a pair of dice: formula_3. From the theory of cyclotomic polynomials, we know that\nwhere \"d\" ranges over the divisors of \"n\" and formula_5 is the \"d\"-th cyclotomic polynomial. We note also that\nWe therefore derive the generating function of a single \"n\"-sided canonical die as being\nformula_8 and is canceled. Thus the factorization of the generating function of a six-sided canonical die is\nThe generating function for the throws of two dice is the product of two copies of each of these factors. How can we partition them to form two legal dice whose spots are not arranged traditionally? Here \"legal\" means that the coefficients are non-negative and sum to six, so that each die has six sides and every face has at least one spot. (That is, the generating function of each die must be a polynomial p(x) with positive coefficients, and with p(0) = 0 and p(1) = 6.) \nOnly one such partition exists:\nand\nThis gives us the distribution of spots on the faces of a pair of Sicherman dice as being {1,2,2,3,3,4} and {1,3,4,5,6,8}, as above.\n\nThis technique can be extended for dice with an arbitrary number of sides.\n\n\n"}
{"id": "77173", "url": "https://en.wikipedia.org/wiki?curid=77173", "title": "South", "text": "South\n\nSouth is one of the four cardinal directions or compass points. South is the opposite of north and is perpendicular to the east and west.\n\nThe word \"south\" comes from Old English \"sūþ\", from earlier Proto-Germanic \"*sunþaz\" (\"south\"), possibly related to the same Proto-Indo-European root that the word \"sun\" derived from.\n\nBy convention, the bottom side of a map is south, although reversed maps exist that defy this convention. To go south using a compass for navigation, set a bearing or azimuth of 180°. Alternatively, in the Northern Hemisphere outside the tropics, the Sun will be roughly in the south at midday.\n\nTrue south is the direction towards the sun end of the axis about which the Earth rotates, called the South Pole. The South Pole is located in Antarctica. Magnetic south is the direction towards the south magnetic pole, some distance away from the south geographic pole.\n\nRoald Amundsen, from Norway, was the first to reach the South Pole, on 14 December 1911, after Ernest Shackleton from the UK was forced to turn back some distance short.\n\nThe Global South refers to the socially and economically less-developed southern half of the globe. 95% of the Global North has enough food and shelter, and a functioning education system. In the South, on the other hand, only 5% of the population has enough food and shelter. It \"lacks appropriate technology, it has no political stability, the economies are disarticulated, and their foreign exchange earnings depend on primary product exports\".\n\nUse of the term \"South\" may also be country-relative, particularly in cases of noticeable economic or cultural divide. For example, the Southern United States, separated from the Northeastern United States by the Mason–Dixon line, or the South of England, which is politically and economically unmatched with all of the North of England.\n\nSouthern Cone is the name that is often referred to as the southernmost area of South America that, in the form of an inverted \"cone\", almost like a large peninsula, encompasses Argentina, Chile, Paraguay, Uruguay and the entire South of Brazil (Brazilian states of Rio Grande do Sul, Santa Catarina, Paraná and São Paulo). Rarely does the meaning broaden to Bolivia, and in the most restricted sense it only covers Chile, Argentina and Uruguay.\n\nThe country of South Africa is so named because of its location at the southern tip of Africa. Upon formation the country was named the Union of South Africa in English, reflecting its origin from the unification of four formerly separate British colonies. Australia derives its name from the Latin Terra Australis (\"Southern Land\"), a name used for a hypothetical continent in the Southern Hemisphere since ancient times.\n\nIn the card game bridge, one of the players is known for scoring purposes as South. South partners with North and plays against East and West.\n\nIn Greek religion, Notos, was the south wind and bringer of the storms of late summer and autumn.\n"}
{"id": "15958803", "url": "https://en.wikipedia.org/wiki?curid=15958803", "title": "Splitting principle", "text": "Splitting principle\n\nIn mathematics, the splitting principle is a technique used to reduce questions about vector bundles to the case of line bundles.\n\nIn the theory of vector bundles, one often wishes to simplify computations, say of Chern classes. Often computations are well understood for line bundles and for direct sums of line bundles. In this case the splitting principle can be quite useful. \n\nThe theorem above holds for complex vector bundles and integer coefficients or for real vector bundles with formula_1 coefficients. In the complex case, the line bundles formula_2 or their first characteristic classes are called Chern roots. \n\nThe fact that formula_3 is injective means that any equation which holds in formula_4 (say between various Chern classes) also holds in formula_5. \n\nThe point is that these equations are easier to understand for direct sums of line bundles than for arbitrary vector bundles, so equations should be understood in formula_6 and then pushed down to formula_7.\n\nUnder the splitting principle, characteristic classes for complex vector bundles correspond to symmetric polynomials in the first Chern classes of complex line bundles; these are the Chern classes.\n\n\n"}
{"id": "27576", "url": "https://en.wikipedia.org/wiki?curid=27576", "title": "Statistical model", "text": "Statistical model\n\nA statistical model is a mathematical model that embodies a set of statistical assumptions concerning the generation of some sample data and similar data from a larger population. A statistical model represents, often in considerably idealized form, the data-generating process.\n\nThe assumptions embodied by a statistical model describe a set of probability distributions, some of which are assumed to adequately approximate the distribution from which a particular data set is sampled. The probability distributions inherent in statistical models are what distinguishes statistical models from other, non-statistical, mathematical models.\n\nA statistical model is usually specified by mathematical equations that relate one or more random variables and possibly other non-random variables. As such, a statistical model is \"a formal representation of a theory\" (Herman Adèr quoting Kenneth Bollen).\n\nAll statistical hypothesis tests and all statistical estimators are derived from statistical models. More generally, statistical models are part of the foundation of statistical inference.\n\nInformally, a statistical model can be thought of as a statistical assumption (or set of statistical assumptions) with a certain property: that the assumption allows us to calculate the probability of any event. As an example, consider a pair of ordinary six-sided dice. We will study two different statistical assumptions about the dice.\n\nThe first statistical assumption is this: for each of the dice, the probability of each face (1, 2, 3, 4, 5, and 6) coming up is . From that assumption, we can calculate the probability of both dice coming up 1:    More generally, we can calculate the probability of any event: e.g. (1 and 2) or (3 and 3) or (4 and 6). \n\nThe alternative statistical assumption is this: for each of the dice, the probability of the face 1 coming up is (because the dice are weighted). From that assumption, we can calculate the probability of both dice coming up 1:    We cannot, however, calculate the probability of any other nontrivial event. \n\nThe first statistical assumption constitutes a statistical model: because with the assumption alone, we can calculate the probability of any event. The alternative statistical assumption does \"not\" constitute a statistical model: because with the assumption alone, we cannot calculate the probability of every event.\n\nIn the example above, with the first assumption, calculating the probability of an event is easy. With some other examples, though, the calculation can be difficult, or even impractical (e.g. it might require millions of years of computation). For an assumption to constitute a statistical model, such difficulty is acceptable: doing the calculation does not need to be practicable, just theoretically possible.\n\nIn mathematical terms, a statistical model is usually thought of as a pair (formula_1), where formula_2 is the set of possible observations, i.e. the sample space, and formula_3 is a set of probability distributions on formula_2.\n\nThe intuition behind this definition is as follows. It is assumed that there is a \"true\" probability distribution induced by the process that generates the observed data. We choose formula_3 to represent a set (of distributions) which contains a distribution that adequately approximates the true distribution. Note that we do not require that formula_3 contains the true distribution, and in practice that is rarely the case. Indeed, as Burnham & Anderson state, \"A model is a simplification or approximation of reality and hence will not reflect all of reality\"—whence the saying \"all models are wrong\".\n\nThe set formula_3 is almost always parameterized: formula_8. The set formula_9 defines the parameters of the model. A parameterization is generally required to have distinct parameter values give rise to distinct distributions, i.e. formula_10 must hold (in other words, it must be injective). A parameterization that meets the requirement is said to be \"identifiable\".\n\nSuppose that we have a population of school children, with the ages of the children distributed uniformly, in the population. The height of a child will be stochastically related to the age: e.g. when we know that a child is of age 7, this influences the chance of the child being 5 feet tall. We could formalize that relationship in a linear regression model, like this: \nheight = \"b\" + \"b\"age + ε, where \"b\" is the intercept, \"b\" is a parameter that age is multiplied by in obtaining a prediction of height, ε is the error term, and \"i\" identifies the child. This implies that height is predicted by age, with some error.\n\nAn admissible model must be consistent with all the data points. Thus, a straight line (height = \"b\" + \"b\"age) cannot be the equation for a model of the data—unless it exactly fits all the data points, i.e. all the data points lie perfectly on the line. The error term, ε, must be included in the equation, so that the model is consistent with all the data points.\n\nTo do statistical inference, we would first need to assume some probability distributions for the ε. For instance, we might assume that the ε distributions are i.i.d. Gaussian, with zero mean. In this instance, the model would have 3 parameters: \"b\", \"b\", and the variance of the Gaussian distribution.\n\nWe can formally specify the model in the form (formula_1) as follows. The sample space, formula_2, of our model comprises the set of all possible pairs (age, height). Each possible value of formula_13 = (\"b\", \"b\", \"σ\") determines a distribution on formula_2; denote that distribution by formula_15. If formula_9 is the set of all possible values of formula_13, then formula_8. (The parameterization is identifiable, and this is easy to check.)\n\nIn this example, the model is determined by (1) specifying formula_2 and (2) making some assumptions relevant to formula_3. There are two assumptions: that height can be approximated by a linear function of age; that errors in the approximation are distributed as i.i.d. Gaussian. The assumptions are sufficient to specify formula_3—as they are required to do.\n\nA statistical model is a special class of mathematical model. What distinguishes a statistical model from other mathematical models is that a statistical model is non-deterministic. Thus, in a statistical model specified via mathematical equations, some of the variables do not have specific values, but instead have probability distributions; i.e. some of the variables are stochastic. In the example above, ε is a stochastic variable; without that variable, the model would be deterministic.\n\nStatistical models are often used even when the physical process being modeled is deterministic. For instance, coin tossing is, in principle, a deterministic process; yet it is commonly modeled as stochastic (via a Bernoulli process).\n\nThere are three purposes for a statistical model, according to Konishi & Kitagawa.\n\nSuppose that we have a statistical model (formula_1) with formula_8. The model is said to be \"parametric\" if formula_9 has a finite dimension. In notation, we write that formula_25 where is a positive integer (formula_26 denotes the real numbers; other sets can be used, in principle). Here, is called the dimension of the model.\n\nAs an example, if we assume that data arise from a univariate Gaussian distribution, then we are assuming that \nIn this example, the dimension, , equals 2.\n\nAs another example, suppose that the data consists of points (, ) that we assume are distributed according to a straight line with i.i.d. Gaussian residuals (with zero mean). Then the dimension of the statistical model is 3: the intercept of the line, the slope of the line, and the variance of the distribution of the residuals. (Note that in geometry, a straight line has dimension 1.)\n\nAlthough formally formula_28 is a single parameter that has dimension , it is sometimes regarded as comprising separate parameters. For example, with the univariate Gaussian distribution, formula_13 is a single parameter with dimension 2, but it is sometimes regarded as comprising 2 separate parameters—the mean and the standard deviation.\n\nA statistical model is \"nonparametric\" if the parameter set formula_9 is infinite dimensional. A statistical model is \"semiparametric\" if it has both finite-dimensional and infinite-dimensional parameters. Formally, if is the dimension of formula_9 and is the number of samples, both semiparametric and nonparametric models have formula_32 as formula_33. If formula_34 as formula_33, then the model is semiparametric; otherwise, the model is nonparametric.\n\nParametric models are by far the most commonly used statistical models. Regarding semiparametric and nonparametric models, Sir David Cox has said, \"These typically involve fewer assumptions of structure and distributional form but usually contain strong assumptions about independencies\".\n\nTwo statistical models are nested if the first model can be transformed into the second model by imposing constraints on the parameters of the first model. As an example, the set of all Gaussian distributions has, nested within it, the set of zero-mean Gaussian distributions: we constrain the mean in the set of all Gaussian distributions to get the zero-mean distributions. As a second example, the quadratic model \nhas, nested within it, the linear model \n—we constrain the parameter to equal 0.\n\nIn both those examples, the first model has a higher dimension than the second model (for the first example, the zero-mean model has dimension 1). Such is often, but not always, the case. As a different example, the set of positive-mean Gaussian distributions, which has dimension 2, is nested within the set of all Gaussian distributions.\n\nIt is assumed that there is a \"true\" probability distribution underlying the observed data, induced by the process that generated the data. The main goal of model selection is to make statements about which elements of formula_3 are most likely to adequately approximate the true distribution.\n\nModels can be compared to each other by exploratory data analysis or confirmatory data analysis. In exploratory analysis, a variety of models are formulated and an assessment is performed of how well each one describes the data. In confirmatory analysis, a previously formulated model or models are compared to the data. Common criteria for comparing models include \"R\", Bayes factor, and the likelihood-ratio test together with its generalization relative likelihood.\n\nKonishi & Kitagawa state: \"The majority of the problems in statistical inference can be considered to be problems related to statistical modeling. They are typically formulated as comparisons of several statistical models.\" Relatedly, Sir David Cox has said, \"How [the] translation from subject-matter problem to statistical model is done is often the most critical part of an analysis\".\n\n\n"}
{"id": "58885735", "url": "https://en.wikipedia.org/wiki?curid=58885735", "title": "The Indispensability of Mathematics", "text": "The Indispensability of Mathematics\n\nThe Indispensability of Mathematics is a 2001 book by Mark Colyvan in which he examines the Quine–Putnam indispensability thesis in the philosophy of mathematics. This thesis is based on the premise that mathematical entities are placed on the same ontological foundation as other theoretical entities indispensable to our best scientific theories.\n\n"}
{"id": "19374170", "url": "https://en.wikipedia.org/wiki?curid=19374170", "title": "Timeline of numerals and arithmetic", "text": "Timeline of numerals and arithmetic\n\nA timeline of numerals and arithmetic\n\n\n\n\n\n\n\n\n"}
{"id": "18096507", "url": "https://en.wikipedia.org/wiki?curid=18096507", "title": "Transportation Science", "text": "Transportation Science\n\nTransportation Science is a peer-reviewed academic journal published by the Institute for Operations Research and the Management Sciences (INFORMS). The studies published in the journal apply operations research techniques to problems in the full range of transportation sectors, including air travel, rail transport, commuter lines, and vehicular travel/traffic. From time to time, studies published in \"Transportation Science\" are featured in mainstream publications.\n"}
{"id": "31883", "url": "https://en.wikipedia.org/wiki?curid=31883", "title": "Uncertainty principle", "text": "Uncertainty principle\n\nIn quantum mechanics, the uncertainty principle (also known as Heisenberg's uncertainty principle) is any of a variety of mathematical inequalities asserting a fundamental limit to the precision with which certain pairs of physical properties of a particle, known as complementary variables, such as position \"x\" and momentum \"p\", can be known.\n\nIntroduced first in 1927, by the German physicist Werner Heisenberg, it states that the more precisely the position of some particle is determined, the less precisely its momentum can be known, and vice versa. The formal inequality relating the standard deviation of position \"σ\" and the standard deviation of momentum \"σ\" was derived by Earle Hesse Kennard later that year and by Hermann Weyl in 1928:\n\nwhere is the reduced Planck constant, ).\n\nHistorically, the uncertainty principle has been confused with a somewhat similar effect in physics, called the observer effect, which notes that measurements of certain systems cannot be made without affecting the systems, that is, without changing something in a system. Heisenberg utilized such an observer effect at the quantum level (see below) as a physical \"explanation\" of quantum uncertainty. It has since become clearer, however, that the uncertainty principle is inherent in the properties of all wave-like systems, and that it arises in quantum mechanics simply due to the matter wave nature of all quantum objects. Thus, \"the uncertainty principle actually states a fundamental property of quantum systems and is not a statement about the observational success of current technology\". It must be emphasized that \"measurement\" does not mean only a process in which a physicist-observer takes part, but rather any interaction between classical and quantum objects regardless of any observer.\n\nSince the uncertainty principle is such a basic result in quantum mechanics, typical experiments in quantum mechanics routinely observe aspects of it. Certain experiments, however, may deliberately test a particular form of the uncertainty principle as part of their main research program. These include, for example, tests of number–phase uncertainty relations in superconducting or quantum optics systems. Applications dependent on the uncertainty principle for their operation include extremely low-noise technology such as that required in gravitational wave interferometers.\n\nThe uncertainty principle is not readily apparent on the macroscopic scales of everyday experience. So it is helpful to demonstrate how it applies to more easily understood physical situations. Two alternative frameworks for quantum physics offer different explanations for the uncertainty principle. The wave mechanics picture of the uncertainty principle is more visually intuitive, but the more abstract matrix mechanics picture formulates it in a way that generalizes more easily.\n\nMathematically, in wave mechanics, the uncertainty relation between position and momentum arises because the expressions of the wavefunction in the two corresponding orthonormal bases in Hilbert space are Fourier transforms of one another (i.e., position and momentum are conjugate variables). A nonzero function and its Fourier transform cannot both be sharply localized. A similar tradeoff between the variances of Fourier conjugates arises in all systems underlain by Fourier analysis, for example in sound waves: A pure tone is a sharp spike at a single frequency, while its Fourier transform gives the shape of the sound wave in the time domain, which is a completely delocalized sine wave. In quantum mechanics, the two key points are that the position of the particle takes the form of a matter wave, and momentum is its Fourier conjugate, assured by the de Broglie relation , where is the wavenumber.\n\nIn matrix mechanics, the mathematical formulation of quantum mechanics, any pair of non-commuting self-adjoint operators representing observables are subject to similar uncertainty limits. An eigenstate of an observable represents the state of the wavefunction for a certain measurement value (the eigenvalue). For example, if a measurement of an observable is performed, then the system is in a particular eigenstate of that observable. However, the particular eigenstate of the observable need not be an eigenstate of another observable : If so, then it does not have a unique associated measurement for it, as the system is not in an eigenstate of that observable.\n\nAccording to the de Broglie hypothesis, every object in the universe is a wave, i.e., a situation which gives rise to this phenomenon. The position of the particle is described by a wave function formula_1. The time-independent wave function of a single-moded plane wave of wavenumber \"k\" or momentum \"p\" is\n\nThe Born rule states that this should be interpreted as a probability density amplitude function in the sense that the probability of finding the particle between \"a\" and \"b\" is\n\nIn the case of the single-moded plane wave, formula_4 is a uniform distribution. In other words, the particle position is extremely uncertain in the sense that it could be essentially anywhere along the wave packet. \n\nOn the other hand, consider a wave function that is a sum of many waves, which we may write this as\n\nwhere \"A\" represents the relative contribution of the mode \"p\" to the overall total. The figures to the right show how with the addition of many plane waves, the wave packet can become more localized. We may take this a step further to the continuum limit, where the wave function is an integral over all possible modes\n\nwith formula_7 representing the amplitude of these modes and is called the wave function in momentum space. In mathematical terms, we say that formula_7 is the \"Fourier transform\" of formula_9 and that \"x\" and \"p\" are conjugate variables. Adding together all of these plane waves comes at a cost, namely the momentum has become less precise, having become a mixture of waves of many different momenta.\n\nOne way to quantify the precision of the position and momentum is the standard deviation \"σ\". Since formula_4 is a probability density function for position, we calculate its standard deviation.\n\nThe precision of the position is improved, i.e. reduced σ, by using many plane waves, thereby weakening the precision of the momentum, i.e. increased σ. Another way of stating this is that σ and σ have an inverse relationship or are at least bounded from below. This is the uncertainty principle, the exact limit of which is the Kennard bound. Click the \"show\" button below to see a semi-formal derivation of the Kennard inequality using wave mechanics.\n(Ref ) \n\nIn matrix mechanics, observables such as position and momentum are represented by self-adjoint operators. When considering pairs of observables, an important quantity is the \"commutator\". For a pair of operators and , one defines their commutator as\nIn the case of position and momentum, the commutator is the canonical commutation relation\n\nThe physical meaning of the non-commutativity can be understood by considering the effect of the commutator on position and momentum eigenstates. Let formula_13 be a right eigenstate of position with a constant eigenvalue . By definition, this means that formula_14 Applying the commutator to formula_13 yields\nwhere is the identity operator.\n\nSuppose, for the sake of proof by contradiction, that formula_13 is also a right eigenstate of momentum, with constant eigenvalue . If this were true, then one could write\nOn the other hand, the above canonical commutation relation requires that\nThis implies that no quantum state can simultaneously be both a position and a momentum eigenstate.\n\nWhen a state is measured, it is projected onto an eigenstate in the basis of the relevant observable. For example, if a particle's position is measured, then the state amounts to a position eigenstate. This means that the state is \"not\" a momentum eigenstate, however, but rather it can be represented as a sum of multiple momentum basis eigenstates. In other words, the momentum must be less precise. This precision may be quantified by the standard deviations, \n\nAs in the wave mechanics interpretation above, one sees a tradeoff between the respective precisions of the two, quantified by the uncertainty principle.\n\nThe most common general form of the uncertainty principle is the \"Robertson uncertainty relation\".\n\nFor an arbitrary Hermitian operator formula_22 we can associate a standard deviation\n\nwhere the brackets formula_24 indicate an expectation value. For a pair of operators formula_25 and formula_26, we may define their \"commutator\" as\n\nIn this notation, the Robertson uncertainty relation is given by\n\nThe Robertson uncertainty relation immediately follows from a slightly stronger inequality, the \"Schrödinger uncertainty relation\",\n\nwhere we have introduced the \"anticommutator\",\n\nSince the Robertson and Schrödinger relations are for general operators, the relations can be applied to any two observables to obtain specific uncertainty relations. A few of the most common relations found in the literature are given below.\nSuppose we consider a quantum particle on a ring, where the wave function depends on an angular variable formula_41, which we may take to lie in the interval formula_42. Define \"position\" and \"momentum\" operators formula_25 and formula_26 by\n\nand\n\nwhere we impose periodic boundary conditions on formula_26. Note that the definition of formula_25 depends on our choice to have formula_41 range from 0 to formula_50. These operators satisfy the usual commutation relations for position and momentum operators, formula_51.\n\nNow let formula_52 be any of the eigenstates of formula_26, which are given by formula_54. Note that these states are normalizable, unlike the eigenstates of the momentum operator on the line. Note also that the operator formula_25 is bounded, since formula_41 ranges over a bounded interval. Thus, in the state formula_52, the uncertainty of formula_58 is zero and the uncertainty of formula_59 is finite, so that \nAlthough this result appears to violate the Robertson uncertainty principle, the paradox is resolved when we note that formula_52 is not in the domain of the operator formula_62, since multiplication by formula_41 disrupts the periodic boundary conditions imposed on formula_26. Thus, the derivation of the Robertson relation, which requires formula_65 and formula_66 to be defined, does not apply. (These also furnish an example of operators satisfying the canonical commutation relations but not the Weyl relations.)\n\nFor the usual position and momentum operators formula_67 and formula_68 on the real line, no such counterexamples can occur. As long as formula_69 and formula_70 are defined in the state formula_52, the Heisenberg uncertainty principle holds, even if formula_52 fails to be in the domain of formula_73 or of formula_74.\n\nConsider a one-dimensional quantum harmonic oscillator (QHO). It is possible to express the position and momentum operators in terms of the creation and annihilation operators:\n\nUsing the standard rules for creation and annihilation operators on the eigenstates of the QHO,\nthe variances may be computed directly,\nThe product of these standard deviations is then\n\nIn particular, the above Kennard bound is saturated for the ground state , for which the probability density is just the normal distribution.\n\nIn a quantum harmonic oscillator of characteristic angular frequency ω, place a state that is offset from the bottom of the potential by some displacement \"x\" as\nwhere Ω describes the width of the initial state but need not be the same as ω. Through integration over the , we can solve for the -dependent solution. After many cancelations, the probability densities reduce to\nwhere we have used the notation formula_85 to denote a normal distribution of mean μ and variance σ. Copying the variances above and applying trigonometric identities, we can write the product of the standard deviations as\n\nFrom the relations\n\nwe can conclude the following: (the right most equality holds only when Ω = \"ω\") .\n\nA coherent state is a right eigenstate of the annihilation operator,\nwhich may be represented in terms of Fock states as\n\nOne expects that the factor may be replaced by , \nwhich is only known if either or is convex.\n\nThe mathematician G. H. Hardy formulated the following uncertainty principle: it is not possible for and to both be \"very rapidly decreasing\". Specifically, if in formula_91 is such that\nand\n\nthen, if , while if , then there is a polynomial of degree such that\n\nThis was later improved as follows: if formula_96 is such that\n\nthen\nwhere is a polynomial of degree and is a real positive definite matrix.\n\nThis result was stated in Beurling's complete works without proof and proved in Hörmander (the case formula_99) and Bonami, Demange, and Jaming for the general case. Note that Hörmander–Beurling's version implies the case in Hardy's Theorem while the version by Bonami–Demange–Jaming covers the full strength of Hardy's Theorem. A different proof of Beurling's theorem based on Liouville's theorem appeared in\nref.\n\nA full description of the case as well as the following extension to Schwartz class distributions appears in ref.\n\nTheorem. If a tempered distribution formula_100 is such that\n\nand\nthen\nfor some convenient polynomial and real positive definite matrix of type .\n\nWerner Heisenberg formulated the uncertainty principle at Niels Bohr's institute in Copenhagen, while working on the mathematical foundations of quantum mechanics.\n\nIn 1925, following pioneering work with Hendrik Kramers, Heisenberg developed matrix mechanics, which replaced the ad hoc old quantum theory with modern quantum mechanics. The central premise was that the classical concept of motion does not fit at the quantum level, as electrons in an atom do not travel on sharply defined orbits. Rather, their motion is smeared out in a strange way: the Fourier transform of its time dependence only involves those frequencies that could be observed in the quantum jumps of their radiation.\n\nHeisenberg's paper did not admit any unobservable quantities like the exact position of the electron in an orbit at any time; he only allowed the theorist to talk about the Fourier components of the motion. Since the Fourier components were not defined at the classical frequencies, they could not be used to construct an exact trajectory, so that the formalism could not answer certain overly precise questions about where the electron was or how fast it was going.\n\nIn March 1926, working in Bohr's institute, Heisenberg realized that the non-commutativity implies the uncertainty principle. This implication provided a clear physical interpretation for the non-commutativity, and it laid the foundation for what became known as the Copenhagen interpretation of quantum mechanics. Heisenberg showed that the commutation relation implies an uncertainty, or in Bohr's language a complementarity. Any two variables that do not commute cannot be measured simultaneously—the more precisely one is known, the less precisely the other can be known. Heisenberg wrote:It can be expressed in its simplest form as follows: One can never know with perfect accuracy both of those two important factors which determine the movement of one of the smallest particles—its position and its velocity. It is impossible to determine accurately \"both\" the position and the direction and speed of a particle \"at the same instant\".\n\nIn his celebrated 1927 paper, \"Über den anschaulichen Inhalt der quantentheoretischen Kinematik und Mechanik\" (\"On the Perceptual Content of Quantum Theoretical Kinematics and Mechanics\"), Heisenberg established this expression as the minimum amount of unavoidable momentum disturbance caused by any position measurement, but he did not give a precise definition for the uncertainties Δx and Δp. Instead, he gave some plausible estimates in each case separately. In his Chicago lecture he refined his principle:\n\nKennard in 1927 first proved the modern inequality:\n\nwhere , and , are the standard deviations of position and momentum. Heisenberg only proved relation () for the special case of Gaussian states.\n\nThroughout the main body of his original 1927 paper, written in German, Heisenberg used the word, \"Ungenauigkeit\" (\"indeterminacy\"),\nto describe the basic theoretical principle. Only in the endnote did he switch to the word, \"Unsicherheit\" (\"uncertainty\"). When the English-language version of Heisenberg's textbook, \"The Physical Principles of the Quantum Theory\", was published in 1930, however, the translation \"uncertainty\" was used, and it became the more commonly used term in the English language thereafter.\n\nThe principle is quite counter-intuitive, so the early students of quantum theory had to be reassured that naive measurements to violate it were bound always to be unworkable. One way in which Heisenberg originally illustrated the intrinsic impossibility of violating the uncertainty principle is by utilizing the observer effect of an imaginary microscope as a measuring device.\n\nHe imagines an experimenter trying to measure the position and momentum of an electron by shooting a photon at it.\n\nThe combination of these trade-offs implies that no matter what photon wavelength and aperture size are used, the product of the uncertainty in measured position and measured momentum is greater than or equal to a lower limit, which is (up to a small numerical factor) equal to Planck's constant. Heisenberg did not care to formulate the uncertainty principle as an exact limit (which is elaborated below), and preferred to use it instead, as a heuristic quantitative statement, correct up to small numerical factors, which makes the radically new noncommutativity of quantum mechanics inevitable.\n\nThe Copenhagen interpretation of quantum mechanics and Heisenberg's Uncertainty Principle were, in fact, seen as twin targets by detractors who believed in an underlying determinism and realism. According to the Copenhagen interpretation of quantum mechanics, there is no fundamental reality that the quantum state describes, just a prescription for calculating experimental results. There is no way to say what the state of a system fundamentally is, only what the result of observations might be.\n\nAlbert Einstein believed that randomness is a reflection of our ignorance of some fundamental property of reality, while Niels Bohr believed that the probability distributions are fundamental and irreducible, and depend on which measurements we choose to perform. Einstein and Bohr debated the uncertainty principle for many years.\n\nWolfgang Pauli called Einstein's fundamental objection to the uncertainty principle \"the ideal of the detached observer\" (phrase translated from the German):\n\nThe first of Einstein's thought experiments challenging the uncertainty principle went as follows:\n\nBohr's response was that the wall is quantum mechanical as well, and that to measure the recoil to accuracy , the momentum of the wall must be known to this accuracy before the particle passes through. This introduces an uncertainty in the position of the wall and therefore the position of the slit equal to , and if the wall's momentum is known precisely enough to measure the recoil, the slit's position is uncertain enough to disallow a position measurement.\n\nA similar analysis with particles diffracting through multiple slits is given by Richard Feynman.\n\nBohr was present when Einstein proposed the thought experiment which has become known as Einstein's box. Einstein argued that \"Heisenberg's uncertainty equation implied that the uncertainty in time was related to the uncertainty in energy, the product of the two being related to Planck's constant.\" Consider, he said, an ideal box, lined with mirrors so that it can contain light indefinitely. The box could be weighed before a clockwork mechanism opened an ideal shutter at a chosen instant to allow one single photon to escape. \"We now know, explained Einstein, precisely the time at which the photon left the box.\" \"Now, weigh the box again. The change of mass tells the energy of the emitted light. In this manner, said Einstein, one could measure the energy emitted and the time it was released with any desired precision, in contradiction to the uncertainty principle.\"\n\nBohr spent a sleepless night considering this argument, and eventually realized that it was flawed. He pointed out that if the box were to be weighed, say by a spring and a pointer on a scale, \"since the box must move vertically with a change in its weight, there will be uncertainty in its vertical velocity and therefore an uncertainty in its height above the table. ... Furthermore, the uncertainty about the elevation above the earth's surface will result in an uncertainty in the rate of the clock,\" because of Einstein's own theory of gravity's effect on time.\n\"Through this chain of uncertainties, Bohr showed that Einstein's light box experiment could not simultaneously measure exactly both the energy of the photon and the time of its escape.\"\n\nBohr was compelled to modify his understanding of the uncertainty principle after another thought experiment by Einstein. In 1935, Einstein, Podolsky and Rosen (see EPR paradox) published an analysis of widely separated entangled particles. Measuring one particle, Einstein realized, would alter the probability distribution of the other, yet here the other particle could not possibly be disturbed. This example led Bohr to revise his understanding of the principle, concluding that the uncertainty was not caused by a direct interaction.\n\nBut Einstein came to much more far-reaching conclusions from the same thought experiment. He believed the \"natural basic assumption\" that a complete description of reality would have to predict the results of experiments from \"locally changing deterministic quantities\" and therefore would have to include more information than the maximum possible allowed by the uncertainty principle.\n\nIn 1964, John Bell showed that this assumption can be falsified, since it would imply a certain inequality between the probabilities of different experiments. Experimental results confirm the predictions of quantum mechanics, ruling out Einstein's basic assumption that led him to the suggestion of his \"hidden variables\". These hidden variables may be \"hidden\" because of an illusion that occurs during observations of objects that are too large or too small. This illusion can be likened to rotating fan blades that seem to pop in and out of existence at different locations and sometimes seem to be in the same place at the same time when observed. This same illusion manifests itself in the observation of subatomic particles. Both the fan blades and the subatomic particles are moving so fast that the illusion is seen by the observer. Therefore, it is possible that there would be predictability of the subatomic particles behavior and characteristics to a recording device capable of very high speed tracking...Ironically this fact is one of the best pieces of evidence supporting Karl Popper's philosophy of invalidation of a theory by falsification-experiments. That is to say, here Einstein's \"basic assumption\" became falsified by experiments based on Bell's inequalities. For the objections of Karl Popper to the Heisenberg inequality itself, see below.\n\nWhile it is possible to assume that quantum mechanical predictions are due to nonlocal, hidden variables, and in fact David Bohm invented such a formulation, this resolution is not satisfactory to the vast majority of physicists. The question of whether a random outcome is predetermined by a nonlocal theory can be philosophical, and it can be potentially intractable. If the hidden variables are not constrained, they could just be a list of random digits that are used to produce the measurement outcomes. To make it sensible, the assumption of nonlocal hidden variables is sometimes augmented by a second assumption—that the size of the observable universe puts a limit on the computations that these variables can do. A nonlocal theory of this sort predicts that a quantum computer would encounter fundamental obstacles when attempting to factor numbers of approximately 10,000 digits or more; a potentially achievable task in quantum mechanics.\n\nKarl Popper approached the problem of indeterminacy as a logician and metaphysical realist. He disagreed with the application of the uncertainty relations to individual particles rather than to ensembles of identically prepared particles, referring to them as \"statistical scatter relations\". In this statistical interpretation, a \"particular\" measurement may be made to arbitrary precision without invalidating the quantum theory. This directly contrasts with the Copenhagen interpretation of quantum mechanics, which is non-deterministic but lacks local hidden variables.\n\nIn 1934, Popper published \"Zur Kritik der Ungenauigkeitsrelationen\" (\"Critique of the Uncertainty Relations\") in \"Naturwissenschaften\", and in the same year \"Logik der Forschung\" (translated and updated by the author as \"The Logic of Scientific Discovery\" in 1959), outlining his arguments for the statistical interpretation. In 1982, he further developed his theory in \"Quantum theory and the schism in Physics\", writing:\n[Heisenberg's] formulae are, beyond all doubt, derivable \"statistical formulae\" of the quantum theory. But they have been \"habitually misinterpreted\" by those quantum theorists who said that these formulae can be interpreted as determining some upper limit to the \"precision of our measurements\". [original emphasis]\n\nPopper proposed an experiment to falsify the uncertainty relations, although he later withdrew his initial version after discussions with Weizsäcker, Heisenberg, and Einstein; this experiment may have influenced the formulation of the EPR experiment.\n\nThe many-worlds interpretation originally outlined by Hugh Everett III in 1957 is partly meant to reconcile the differences between Einstein's and Bohr's views by replacing Bohr's wave function collapse with an ensemble of deterministic and independent universes whose \"distribution\" is governed by wave functions and the Schrödinger equation. Thus, uncertainty in the many-worlds interpretation follows from each observer within any universe having no knowledge of what goes on in the other universes.\n\nSome scientists including Arthur Compton and Martin Heisenberg have suggested that the uncertainty principle, or at least the general probabilistic nature of quantum mechanics, could be evidence for the two-stage model of free will. One critique, however, is that apart from the basic role of quantum mechanics as a foundation for chemistry, nontrivial biological mechanisms requiring quantum mechanics are unlikely, due to the rapid decoherence time of quantum systems at room temperature. The standard view, however, is that this decoherence is overcome by both screening and decoherence-free subspaces found in biological cells.\n\nThere is reason to believe that violating the uncertainty principle also strongly implies the violation of the second law of thermodynamics.\n\n"}
{"id": "10681913", "url": "https://en.wikipedia.org/wiki?curid=10681913", "title": "Whitehead Prize", "text": "Whitehead Prize\n\nThe Whitehead Prize is awarded yearly by the London Mathematical Society to multiple mathematicians working in the United Kingdom who are at an early stage of their career. The prize is named in memory of homotopy theory pioneer J. H. C. Whitehead.\n\nMore specifically, people being considered for the award must be resident in the United Kingdom on 1 January of the award year or must have been educated in the United Kingdom. Also, the candidates must have less than 15 years of work at the postdoctorate level and must not have received any other prizes from the Society.\n\nSince the inception of the prize, no more than two could be awarded per year, but in 1999 this was increased to four “to allow for the award of prizes across the whole of mathematics, including applied mathematics, mathematical physics, and mathematical aspects of computer science.”\n\nThe Senior Whitehead Prize has similar residence requirements and rules concerning prior prizes, but is intended to recognize more experienced mathematicians.\n\n\n\n"}
