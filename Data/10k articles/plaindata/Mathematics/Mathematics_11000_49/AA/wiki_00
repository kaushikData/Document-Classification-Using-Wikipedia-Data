{"id": "5075473", "url": "https://en.wikipedia.org/wiki?curid=5075473", "title": "A-paracompact space", "text": "A-paracompact space\n\nIn mathematics, in the field of topology, a topological space is said to be a-paracompact if every open cover of the space has a locally finite refinement. In contrast to the definition of paracompactness, the refinement is not required to be open.\n\nEvery paracompact space is a-paracompact, and in regular spaces the two notions coincide.\n"}
{"id": "6339424", "url": "https://en.wikipedia.org/wiki?curid=6339424", "title": "ABC (stream cipher)", "text": "ABC (stream cipher)\n\nIn cryptography, ABC is a stream cypher algorithm developed by Vladimir Anashin, Andrey Bogdanov, Ilya Kizhvatov, and Sandeep Kumar. It has been submitted to the eSTREAM Project of the eCRYPT network. \n"}
{"id": "1341579", "url": "https://en.wikipedia.org/wiki?curid=1341579", "title": "Angle of parallelism", "text": "Angle of parallelism\n\nIn hyperbolic geometry, the angle of parallelism formula_1, is the angle at one vertex of a right hyperbolic triangle that has two asymptotic parallel sides. The angle depends on the segment length \"a\" between the right angle and the vertex of the angle of parallelism. \n\nGiven a point off of a line, if we drop a perpendicular to the line from the point, then \"a\" is the distance along this perpendicular segment, and \"φ\" or formula_1 is the least angle such that the line drawn through the point at that angle does not intersect the given line. Since two sides are asymptotic parallel,\n\nThere are five equivalent expressions that relate \" formula_4\" and \"a\":\n\nwhere sinh, cosh, tanh, sech and csch are hyperbolic functions and gd is the Gudermannian function.\n\nJános Bolyai discovered a construction which gives the asymptotic parallel \"s\" to a line \"r\" passing through a point \"A\" not on \"r\". Drop a perpendicular from \"A\" onto \"B\" on \"r\". Choose any point \"C\" on \"r\" different from \"B\". Erect a perpendicular \"t\" to \"r\" at \"C\". Drop a perpendicular from \"A\" onto \"D\" on \"t\". Then length \"DA\" is longer than \"CB\", but shorter than \"CA\". Draw a circle around \"C\" with radius equal to \"DA\". It will intersect the segment \"AB\" at a point \"E\". Then the angle \"BEC\" is independent of the length \"BC\", depending only on \"AB\"; it is the angle of parallelism. Construct \"s\" through \"A\" at angle \"BEC\" from \"AB\".\n\nSee Trigonometry of right triangles for the formulas used here.\n\nThe angle of parallelism was developed in 1840 in the German publication \"Geometrische Untersuchungen zur Theory der Parallellinien\" by Nicolai Lobachevsky.\n\nThis publication became widely known in English after the Texas professor G. B. Halsted produced a translation in 1891. (\"Geometrical Researches on the Theory of Parallels\")\n\nThe following passages define this pivotal concept in hyperbolic geometry:\n\nIn the Poincaré half-plane model of the hyperbolic plane (see Hyperbolic motions), one can establish the relation of \"φ\" to \"a\" with Euclidean geometry. Let \"Q\" be the semicircle with diameter on the \"x\"-axis that passes through the points (1,0) and (0,\"y\"), where \"y\" > 1. Since \"Q\" is tangent to the unit semicircle centered at the origin, the two semicircles represent \"parallel hyperbolic lines\". The \"y\"-axis crosses both semicircles, making a right angle with the unit semicircle and a variable angle \"φ\" with \"Q\". The angle at the center of \"Q\" subtended by the radius to (0, \"y\") is also \"φ\" because the two angles have sides that are perpendicular, left side to left side, and right side to right side. The semicircle \"Q\" has its center at (\"x\", 0), \"x\" < 0, so its radius is 1 − \"x\". Thus, the radius squared of \"Q\" is\n\nhence\n\nThe metric of the Poincaré half-plane model of hyperbolic geometry parametrizes distance on the ray {(0, \"y\") : \"y\" > 0 } with logarithmic measure. Let log \"y\" = \"a\", so \"y\" = e where e is the base of the natural logarithm. Then \nthe relation between \"φ\" and \"a\" can be deduced from the triangle {(\"x\", 0), (0, 0), (0, \"y\")}, for example:\n\n"}
{"id": "2983356", "url": "https://en.wikipedia.org/wiki?curid=2983356", "title": "Bred vector", "text": "Bred vector\n\nIn applied mathematics, bred vectors are perturbations, related to Lyapunov vectors, that capture fast-growing dynamical instabilities of the solution of a numerical model. They are used, for example, as initial perturbations for ensemble forecasting in numerical weather prediction. They were introduced by Zoltan Toth and Eugenia Kalnay.\n\nBred vectors are created by adding initially random perturbations to a nonlinear model. The control (unperturbed) and the perturbed models are integrated in time, and periodically the control solution is subtracted from the perturbed solution. This difference is the bred vector. The vector is scaled to be the same size as the initial perturbation, and is then added back to the control to create the new perturbed initial condition. After a short transient period, this \"breeding\" process creates bred vectors dominated by the naturally fastest-growing instabilities of the evolving control solution.\n\n"}
{"id": "970031", "url": "https://en.wikipedia.org/wiki?curid=970031", "title": "Byzantine fault tolerance", "text": "Byzantine fault tolerance\n\nByzantine fault tolerance (BFT) is the dependability of a fault-tolerant computer system, particularly distributed computing systems, where components may fail and there is imperfect information on whether a component has failed. In a \"Byzantine failure\", a component such as a server can inconsistently appear both failed and functioning to failure-detection systems, presenting different symptoms to different observers. \n\nIt is difficult for the other components to declare it failed and shut it out of the network, because they need to first reach a consensus regarding which component has failed in the first place. The term is derived from the Byzantine Generals' Problem, where actors must agree on a concerted strategy to avoid catastrophic system failure, but some of the actors are unreliable. Byzantine fault tolerance has been also referred to with the phrases interactive consistency or source congruency, error avalanche, Byzantine agreement problem, Byzantine generals problem, and Byzantine failure.\n\nA Byzantine fault is any fault presenting different symptoms to different observers. A Byzantine failure is the loss of a system service due to a Byzantine fault in systems that require consensus.\n\nThe objective of Byzantine fault tolerance is to be able to defend against failures of system components with or without symptoms that prevent other components of the system from reaching an agreement among themselves, where such an agreement is needed for the correct operation of the system.\n\nRemaining correctly operational components of a Byzantine fault tolerant system will be able to continue providing the system's service as originally intended, assuming there are sufficiently many accurately operating components to maintain the service.\n\nByzantine failures are considered the most general and most difficult class of failures among the failure modes. The so-called fail-stop failure mode occupies the simplest end of the spectrum. Whereas fail-stop failure mode simply means that the only way to fail is a node crash, detected by other nodes, Byzantine failures imply no restrictions, which means that the failed node can generate arbitrary data, pretending to be a correct one. Thus, Byzantine failures can confuse failure detection systems, which makes fault tolerance difficult. Despite the analogy, a Byzantine failure is not necessarily a security problem involving hostile human interference: it can arise purely from electrical faults.\n\nThe terms fault and failure are used here according to the standard definitions originally created by a joint committee on \"Fundamental Concepts and Terminology\" formed by the IEEE Computer Society's Technical Committee on Dependable Computing and Fault-Tolerance and IFIP Working Group 10.4 on Dependable Computing and Fault Tolerance. A version of these definitions is also described in the Dependability Wikipedia page.\n\n\"Byzantine\" refers to the Byzantine Generals' Problem, an agreement problem (described by Leslie Lamport, Robert Shostak and Marshall Pease in their 1982 paper, \"The Byzantine Generals Problem\") in which a group of generals, each commanding a portion of the Byzantine army, encircle a city. These generals wish to formulate a plan for attacking the city. In its simplest form, the generals must decide only whether to attack or retreat. Some generals may prefer to attack, while others prefer to retreat. The important thing is that every general agree on a common decision, for a halfhearted attack by a few generals would become a rout, and would be worse than either a coordinated attack or a coordinated retreat.\n\nThe problem is complicated by the presence of treacherous generals who may not only cast a vote for a suboptimal strategy, they may do so selectively. For instance, if nine generals are voting, four of whom support attacking while four others are in favor of retreat, the ninth general may send a vote of retreat to those generals in favor of retreat, and a vote of attack to the rest. Those who received a retreat vote from the ninth general will retreat, while the rest will attack (which may not go well for the attackers). The problem is complicated further by the generals being physically separated and having to send their votes via messengers who may fail to deliver votes or may forge false votes.\n\nByzantine fault tolerance can be achieved if the loyal (non-faulty) generals have a majority agreement on their strategy. There can be a default vote value given to missing messages. For example, missing messages can be given the value <Null>. Further, if the agreement is that the <Null> votes are in the majority, a pre-assigned default strategy can be used (e.g., retreat).\n\nThe typical mapping of this story onto computer systems is that the computers are the generals and their digital communication system links are the messengers. Although the problem is formulated in the analogy as a decision-making and security problem, in electronics, it cannot be solved simply by cryptographic digital signatures, because failures such as incorrect voltages can propagate through the encryption process. Thus, a component may appear functioning to one component and faulty to another, which prevents forming a consensus whether the component is faulty or not.\n\nSeveral examples of Byzantine failures that have occurred are given in two equivalent journal papers. These and other examples are described on the NASA DASHlink web pages. These web pages also describe some phenomenology that can cause Byzantine faults.\n\nByzantine errors were observed infrequently and at irregular points during endurance testing for the newly constructed \"Virginia\" class submarines, at least through 2005 (when the issues were publicly reported).\n\nA similar problem faces honeybee swarms. They have to find a new home, and the many scouts and wider participants have to reach consensus about which of perhaps several candidate homes to fly to. And then they all have to fly there, with their queen.\n\nSeveral solutions were described by Lamport, Shostak, and Pease in 1982. They began by noting that the Generals' Problem can be reduced to solving a \"Commander and Lieutenants\" problem where loyal Lieutenants must all act in unison and that their action must correspond to what the Commander ordered in the case that the Commander is loyal.\n\nSeveral system architectures were designed c. 1980 that implemented Byzantine fault tolerance. These include: Draper's FTMP,\nHoneywell's MMFCS,\nand SRI's SIFT.\n\nIn 1999, Miguel Castro and Barbara Liskov introduced the \"Practical Byzantine Fault Tolerance\" (PBFT) algorithm, which provides high-performance Byzantine state machine replication, processing thousands of requests per second with sub-millisecond increases in latency.\n\nAfter PBFT, several BFT protocols were introduced to improve its robustness and performance. For instance, Q/U,\nHQ,\nZyzzyva,\nand ABsTRACTs\n, etc., addressed the performance and cost issues; whereas other protocols, like Aardvark\nand RBFT\n, addressed its robustness issues. Furthermore, Adapt tried to make use of existing BFT protocols, through switching between them in an adaptive way, to improve system robustness and performance as the underlying conditions change. Furthermore, BFT protocols were introduced that leverage trusted components to reduce the number of replicas, e.g., A2M-PBFT-EA and MinBFT.\n\nUpRight is an open source library for constructing services that tolerate both crashes (\"up\") and Byzantine behaviors (\"right\") that incorporates many of these protocols' innovations.\n\nIn addition to PBFT and UpRight, there is the BFT-SMaRt library, a high-performance Byzantine fault-tolerant state machine replication library developed in Java. This library implements a protocol very similar to PBFT's, plus complementary protocols which offer state transfer and on-the-fly reconfiguration of hosts. BFT-SMaRt is the most recent effort to implement state machine replication, still being actively maintained.\n\nArchistar utilizes a slim BFT layer for communication. It prototypes a secure multi-cloud storage system using Java licensed under LGPLv2. Focus lies on simplicity and readability, it aims to be the foundation for further research projects.\n\nAskemos is a concurrent, garbage-collected, persistent programming platform atop of replicated state machines which tolerates Byzantine faults. It prototypes an execution environment facilitating Smart contracts.\n\nTendermint is general purpose software for BFT state machine replication. Using a socket protocol, it enables state machines to be written in any programming language, and provides\nmeans for the state machine to influence elements of the consensus, such as the list of active processes. Tendermint is implemented in the style of a blockchain, which amortizes the overhead of BFT and allows for faster recovery from failure.\n\nOne example of BFT in use is bitcoin, a peer-to-peer digital cash system. The bitcoin network works in parallel to generate a blockchain with proof-of-work allowing the system to overcome Byzantine failures and reach a coherent global view of the system's state.\n\nSome aircraft systems, such as the Boeing 777 Aircraft Information Management System (via its ARINC 659 SAFEbus network),\n\nthe Boeing 777 flight control system,\nand the Boeing 787 flight control systems use Byzantine fault tolerance; because these are real-time systems, their Byzantine fault tolerance solutions must have very low latency. For example, SAFEbus can achieve Byzantine fault tolerance within the order of a microsecond of added latency.\n\nSome spacecraft flight systems such as that of the SpaceX Dragon consider Byzantine fault tolerance in their design.\n\nByzantine fault tolerance mechanisms use components that repeat an incoming message (or just its signature) to other recipients of that incoming message. All these mechanisms make the assumption that the act of repeating a message blocks the propagation of Byzantine symptoms. For systems that have a high degree of safety or security criticality, these assumptions must be proven to be true to an acceptable level of fault coverage. When providing proof through testing, one difficulty is creating a sufficiently wide range of signals with Byzantine symptoms. Such testing likely will require specialized fault injectors.\n\n"}
{"id": "151040", "url": "https://en.wikipedia.org/wiki?curid=151040", "title": "CPT symmetry", "text": "CPT symmetry\n\nCharge, parity, and time reversal symmetry is a fundamental symmetry of physical laws under the simultaneous transformations of charge conjugation (C), parity transformation (P), and time reversal (T). CPT is the only combination of C, P, and T that is observed to be an exact symmetry of nature at the fundamental level. The CPT theorem says that CPT symmetry holds for all physical phenomena, or more precisely, that any Lorentz invariant local quantum field theory with a Hermitian Hamiltonian must have CPT symmetry.\n\nThe CPT theorem appeared for the first time, implicitly, in the work of Julian Schwinger in 1951 to prove the connection between spin and statistics. In 1954, Gerhart Lüders and Wolfgang Pauli derived more explicit proofs, so this theorem is sometimes known as the Lüders–Pauli theorem. At about the same time, and independently, this theorem was also proved by John Stewart Bell. These proofs are based on the principle of Lorentz invariance and the principle of locality in the interaction of quantum fields. Subsequently, Res Jost gave a more general proof in the framework of axiomatic quantum field theory.\n\nEfforts during the late 1950s revealed the violation of P-symmetry by phenomena that involve the weak force, and there were well-known violations of C-symmetry as well. For a short time, the CP-symmetry was believed to be preserved by all physical phenomena, but that was later found to be false too, which implied, by CPT invariance, violations of T-symmetry as well.\n\nConsider a Lorentz boost in a fixed direction \"z\". This can be interpreted as a rotation of the time axis into the \"z\" axis, with an imaginary rotation parameter. If this rotation parameter were real, it would be possible for a 180° rotation to reverse the direction of time and of \"z\". Reversing the direction of one axis is a reflection of space in any number of dimensions. If space has 3 dimensions, it is equivalent to reflecting all the coordinates, because an additional rotation of 180° in the \"x-y\" plane could be included.\n\nThis defines a CPT transformation if we adopt the Feynman-Stueckelberg interpretation of antiparticles as the corresponding particles traveling backwards in time. This interpretation requires a slight analytic continuation, which is well-defined only under the following assumptions:\n\nWhen the above hold, quantum theory can be extended to a Euclidean theory, defined by translating all the operators to imaginary time using the Hamiltonian. The commutation relations of the Hamiltonian, and the Lorentz generators, guarantee that Lorentz invariance implies rotational invariance, so that any state can be rotated by 180 degrees.\n\nSince a sequence of two CPT reflections is equivalent to a 360-degree rotation, fermions change by a sign under two CPT reflections, while bosons do not. This fact can be used to prove the spin-statistics theorem.\n\nThe implication of CPT symmetry is that a \"mirror-image\" of our universe — with all objects having their positions reflected by an arbitrary plane (corresponding to a parity inversion), all momenta reversed (corresponding to a time inversion) and with all matter replaced by antimatter (corresponding to a charge inversion)—would evolve under exactly our physical laws. The CPT transformation turns our universe into its \"mirror image\" and vice versa. CPT symmetry is recognized to be a fundamental property of physical laws.\n\nIn order to preserve this symmetry, every violation of the combined symmetry of two of its components (such as CP) must have a corresponding violation in the third component (such as T); in fact, mathematically, these are the same thing. Thus violations in T symmetry are often referred to as CP violations.\n\nThe CPT theorem can be generalized to take into account pin groups.\n\nIn 2002 Oscar Greenberg published an apparent proof that CPT violation implies the breaking of Lorentz symmetry. If correct, this would imply that any study of CPT violation also includes Lorentz violation. However, Chaichian \"et al\" later disputed the validity of Greenberg's result. Greenberg replied that the model used in their paper meant that their \"proposed objection was not relevant to my result\".\n\nThe overwhelming majority of experimental searches for Lorentz violation have yielded negative results. A detailed tabulation of these results was given in 2011 by Kostelecky and Russell.\n\n\n\n"}
{"id": "1081339", "url": "https://en.wikipedia.org/wiki?curid=1081339", "title": "Chisini mean", "text": "Chisini mean\n\nIn mathematics, a function \"f\" of \"n\" variables \n\nleads to a Chisini mean \"M\" if for every vector <x ... x>, there exists a unique \"M\" such that \n\nThe arithmetic, harmonic, geometric, generalised, Heronian and quadratic means are all Chisini means, as are their weighted variants.\n\nThey were introduced by Oscar Chisini in 1929.\n\n\n"}
{"id": "7585515", "url": "https://en.wikipedia.org/wiki?curid=7585515", "title": "Classifier (UML)", "text": "Classifier (UML)\n\nA classifier is a category of Unified Modeling Language (UML) elements that have some common features, such as attributes or methods.\n\nA classifier is an abstract metaclass classification concept that serves as a mechanism to show interfaces, classes, datatypes and components.\n\nA classifier describes a set of instances that have common behavioral and structural features (operations and attributes, respectively).\n\nA classifier is a namespace whose members can specify a generalization hierarchy by referencing its general classifiers.\n\nA classifier is a type and can own generalizations, thereby making it possible to define generalization relationships to other classifiers.\n\nA classifier is a redefinable element, as it is possible to redefine nested classifiers.\n\nAll objects that can have instances are classifiers.\n\n\n\n"}
{"id": "59133525", "url": "https://en.wikipedia.org/wiki?curid=59133525", "title": "Compound of tesseract and 16-cell", "text": "Compound of tesseract and 16-cell\n\nIn 4-dimensional geometry, the tesseract 16-cell compound is a polytope compound composed of a regular tesseract and dual regular 16-cell. A \"compound polytope\" is a figure that is composed of several polytopes sharing a common center. The outer vertices of a compound can be connected to form a convex polytope called the convex hull. The compound is a facetting of the convex hull.\n\nIn 4-polytope compounds constructed as dual pairs, cells and vertices swap positions and faces and edges swap positions. Because of this the number of cells and vertices are equal, as are faces and edges. Mid-edges of the tesseract cross mid-face in the 16-cell, and vice versa.\n\nIt can be seen as the 4-dimensional analogue of a compound of cube and octahedron.\n\nThe 24 Cartesian coordinates of the vertices of the compound are.\n\nThese are the first two vertex sets of the stellations of a 16-cell.\n\nThe convex hull is the self-dual regular 24-cell, which is also a \"rectified 16-cell\". This makes it a faceting of the 24-cell.\n\nThe intersection of the tesseract and 16-cell compound is the uniform bitruncated tesseract: = ∩ .\n\n\n\n"}
{"id": "2361708", "url": "https://en.wikipedia.org/wiki?curid=2361708", "title": "David E. Shaw", "text": "David E. Shaw\n\nDavid Elliot Shaw (born March 29, 1951) is an American investor, computer scientist, and hedge fund manager. He founded D. E. Shaw & Co., a hedge fund company which was once described by \"Fortune\" magazine as \"the most intriguing and mysterious force on Wall Street\". A former faculty member in the computer science department at Columbia University, Shaw made his fortune exploiting inefficiencies in financial markets with the help of state-of-the-art high speed computer networks. In 1996, \"Fortune\" magazine referred to him as \"King Quant\" because of his firm's pioneering role in high-speed quantitative trading. In 2001, Shaw turned to full-time scientific research in computational biochemistry, more specifically molecular dynamics simulations of proteins.\n\nShaw received a bachelor's degree \"summa cum laude\" from the University of California, San Diego and obtained his PhD from Stanford University in 1980, then became a faculty member of the Department of Computer Science at Columbia University. While at Columbia, Shaw conducted research in massively parallel computing with the NON-VON supercomputer. This supercomputer was composed of processing elements in a tree structure meant to be used for fast relational database searches. Earlier in his career, he founded Stanford Systems Corporation.\n\nIn 1986, he joined Morgan Stanley, as vice president for technology in Nunzio Tartaglia's automated proprietary trading group. In 1994, Shaw was appointed by President Clinton to the President's Council of Advisors on Science and Technology, where he was chairman of the Panel on Educational Technology. In 2000, he was elected to the board of directors of the American Association for the Advancement of Science served as its treasurer 2000-2010. In 2007, Shaw was elected as a fellow of the American Academy of Arts and Sciences. In 2009, he was appointed by President Obama again to the President's Council of Advisors on Science and Technology. In 2012, he was elected to the National Academy of Engineering and in 2014 was elected to the National Academy of Sciences.\n\nIn 1988 he started his own hedge fund, \"D. E. Shaw & Co.\", which employed proprietary algorithms for securities trading. In 2018, \"Forbes\" estimated his wealth at $6.2 billion. He is also a Senior Research Fellow at the Center for Computational Biology and Bioinformatics at Columbia University and an Adjunct Professor of Biomedical Informatics at Columbia's medical school. Shaw is Chief Scientist of D. E. Shaw Research, which conducts interdisciplinary research in the field of computational biochemistry.\n\nAccording to the Institutional Investor’s \"Alpha\" magazine's annual ranking for 2014, D. E. Shaw, who made $530 million in 2014, and James H. Simons of Renaissance Technologies who made $1.2 billion were among the top 25 earners in the hedge fund industry. They are both \"quantitative strategists who founded firms that build algorithms for trading.\"\n\nShaw has donated US$2.25 million to Priorities USA Action, a super PAC supporting Democratic presidential candidate Hillary Clinton.\n\nThrough the Shaw Family Endowment Fund, by 2014 he and his wife have donated $1 million to Organizing for Action, $400,000 to the Stephen Wise Free Synagogue, $400,000 to Memorial Sloan Kettering Cancer Center, $1 million to Yale University, $800,000 to the Horace Mann School, $1 million to Stanford University, and $1 million to Harvard University. Shaw is also on the board of the American Association for the Advancement of Science.\n\nShaw is married to personal finance commentator and journalist Beth Kobliner. They are members of the Stephen Wise Free Synagogue in New York.\n\n\n"}
{"id": "661384", "url": "https://en.wikipedia.org/wiki?curid=661384", "title": "Dependability", "text": "Dependability\n\nIn systems engineering, dependability is a measure of a system's availability, reliability, and its maintainability, and maintenance support performance, and, in some cases, other characteristics such as durability, safety and security. In software engineering, dependability is the ability to provide services that can defensibly be trusted within a time-period. This may also encompass mechanisms designed to increase and maintain the dependability of a system or software.\n\nThe International Electrotechnical Commission (IEC), via its Technical Committee TC 56 develops and maintains international standards that provide systematic methods and tools for dependability assessment and management of equipment, services, and systems throughout their life cycles.\n\nDependability can be broken down into three elements:\n\nSome sources hold that word was coined in the nineteen-teens in Dodge Brothers automobile print advertising. But the word predates that period, with the Oxford English Dictionary finding its first use in 1901.\n\nAs interest in fault tolerance and system reliability increased in the 1960s and 1970s, dependability came to be a measure of [x] as measures of reliability came to encompass additional measures like safety and integrity. In the early 1980s, Jean-Claude Laprie thus chose \"dependability\" as the term to encompass studies of fault tolerance and system reliability without the extension of meaning inherent in \"reliability\".\n\nThe field of dependability has evolved from these beginnings to be an internationally active field of research fostered by a number of prominent international conferences, notably the International Conference on Dependable Systems and Networks, the International Symposium on Reliable Distributed Systems and the International Symposium on Software Reliability Engineering.\n\nTraditionally, dependability for a system incorporates availability, reliability, maintainability but since the 1980s, safety and security have been added to measures of dependability.\n\nAttributes are qualities of a system. These can be assessed to determine its overall dependability using Qualitative or Quantitative measures. Avizienis et al. define the following Dependability Attributes:\n\n\nAs these definitions suggested, only Availability and Reliability are quantifiable by direct measurements whilst others are more subjective. For instance Safety cannot be measured directly via metrics but is a subjective assessment that requires judgmental information to be applied to give a level of confidence, whilst Reliability can be measured as failures over time.\n\nConfidentiality, i.e. \"the absence of unauthorized disclosure of information\" is also used when addressing security. Security is a composite of Confidentiality, Integrity, and Availability. Security is sometimes classed as an attribute but the current view is to aggregate it together with dependability and treat Dependability as a composite term called Dependability and Security.\n\nPractically, applying security measures to the appliances of a system generally improves the dependability by limiting the number of externally originated errors.\n\nThreats are things that can affect a system and cause a drop in Dependability. There are three main terms that must be clearly understood: \n\n\nIt is important to note that Failures are recorded at the system boundary. They are basically Errors that have propagated to the system boundary and have become observable.\nFaults, Errors and Failures operate according to a mechanism. This mechanism is sometimes known as a Fault-Error-Failure chain. As a general rule a fault, when activated, can lead to an error (which is an invalid state) and the invalid state generated by an error may lead to another error or a failure (which is an observable deviation from the specified behaviour at the system boundary).\n\nOnce a fault is activated an error is created. An error may act in the same way as a fault in that it can create further error conditions, therefore an error may propagate multiple times within a system boundary without causing an observable failure. If an error propagates outside the system boundary a failure is said to occur. A failure is basically the point at which it can be said that a service is failing to meet its specification. Since the output data from one service may be fed into another, a failure in one service may propagate into another service as a fault so a chain can be formed of the form: Fault leading to Error leading to Failure leading to Error, etc.\n\nSince the mechanism of a Fault-Error-Chain is understood it is possible to construct means to break these chains and thereby increase the dependability of a system.\nFour means have been identified so far:\n\nFault Prevention deals with preventing faults being incorporated into a system. This can be accomplished by use of development methodologies and good implementation techniques.\n\nFault Removal can be sub-divided into two sub-categories: Removal During Development and Removal During Use.<br>\nRemoval during development requires verification so that faults can be detected and removed before a system is put into production. Once systems have been put into production a system is needed to record failures and remove them via a maintenance cycle.\n\nFault Forecasting predicts likely faults so that they can be removed or their effects can be circumvented.\n\nFault Tolerance deals with putting mechanisms in place that will allow a system to still deliver the required service in the presence of faults, although that service may be at a degraded level.\n\nDependability means are intended to reduce the number of failures presented to the user of a system. Failures are traditionally recorded over time and it is useful to understand how their frequency is measured so that the effectiveness of means can be assessed.11\n\nRecent works, such upon dependability take benefit of structured information systems, e.g. with SOA, to introduce a more efficient ability, the survivability, thus taking into account the degraded services that an Information System sustains or resumes after a non-maskable failure.\n\nThe flexibility of current frameworks encourage system architects to enable reconfiguration mechanisms that refocus the available, safe resources to support the most critical services rather than over-provisioning to build failure-proof system.\n\nWith the generalisation of networked information systems, accessibility was introduced to give greater importance to users' experience.\n\nTo take into account the level of performance, the measurement of performability is defined as \"quantifying how well the object system performs in the presence of faults over a specified period of time\".\n\n\n\n\n\n"}
{"id": "7588072", "url": "https://en.wikipedia.org/wiki?curid=7588072", "title": "Dispersion point", "text": "Dispersion point\n\nIn topology, a dispersion point or explosion point is a point in a topological space the removal of which leaves the space highly disconnected.\n\nMore specifically, if \"X\" is a connected topological space containing the point \"p\" and at least two other points, \"p\" is a dispersion point for \"X\" if and only if formula_1 is totally disconnected (every subspace is disconnected, or, equivalently, every connected component is a single point). If \"X\" is connected and formula_1 is totally separated (for each two points \"x\" and \"y\" there exists a clopen set containing \"x\" and not containing \"y\") then \"p\" is an explosion point. A space can have at most one dispersion point or explosion point. Every totally separated space is totally disconnected, so every explosion point is a dispersion point.\n\nThe Knaster–Kuratowski fan has a dispersion point; any space with the particular point topology has an explosion point.\n\nIf \"p\" is an explosion point for a space \"X\", then the totally separated space formula_1 is said to be \"pulverized\".\n\n"}
{"id": "48030765", "url": "https://en.wikipedia.org/wiki?curid=48030765", "title": "Electro-olfactography", "text": "Electro-olfactography\n\nElectro-olfactography or electroolfactography (EOG) is a type of electrography (electrophysiologic test) that aids the study of olfaction (the sense of smell). It measures and records the changing electrical potentials of the olfactory epithelium, in a way similar to how other forms of electrography (such as ECG, EEG, and EMG) measure and record other bioelectric activity.\n\nElectro-olfactography has been used for decades to advance the basic science of smell, although the advances in molecular biology in recent decades have expanded olfactory science beyond the knowledge that the electrical recordings of electro-olfactography alone could provide. Electro-olfactography is closely related to electroantennography, the electrography of insect antennae olfaction.\nNeuroscientist David Ottoson (1918-2001) discovered the electro-olfactogram (EOG) and analysed its properties in great detail.\n\n"}
{"id": "24947285", "url": "https://en.wikipedia.org/wiki?curid=24947285", "title": "Fast-growing hierarchy", "text": "Fast-growing hierarchy\n\nIn computability theory, computational complexity theory and proof theory, a fast-growing hierarchy (also called an extended Grzegorczyk hierarchy) is an ordinal-indexed family of rapidly increasing functions \"f\": N → N (where N is the set of natural numbers {0, 1, ...}, and α ranges up to some large countable ordinal). A primary example is the Wainer hierarchy, or Löb–Wainer hierarchy, which is an extension to all α < ε. Such hierarchies provide a natural way to classify computable functions according to rate-of-growth and computational complexity.\n\nLet μ be a large countable ordinal such that a fundamental sequence (a strictly increasing sequence of ordinals whose supremum is a limit ordinal) is assigned to every limit ordinal less than μ. A fast-growing hierarchy of functions \"f\": N → N, for α < μ, is then defined as follows:\n\n\nHere \"f\"(\"n\") = \"f\"(\"f\"(...(\"f\"(\"n\"))...)) denotes the \"n\" iterate of \"f\" applied to \"n\", and α[\"n\"] denotes the \"n\" element of the fundamental sequence assigned to the limit ordinal α. (An alternative definition takes the number of iterations to be \"n\"+1, rather than \"n\", in the second line above.)\n\nThe initial part of this hierarchy, comprising the functions \"f\" with \"finite\" index (i.e., α < ω), is often called the Grzegorczyk hierarchy because of its close relationship to the Grzegorczyk hierarchy; note, however, that the former is here an indexed family of functions \"f\", whereas the latter is an indexed family of \"sets\" of functions formula_4. (See Points of Interest below.)\n\nGeneralizing the above definition even further, a fast iteration hierarchy is obtained by taking \"f\" to be any increasing function g: N → N.\n\nFor limit ordinals not greater than ε, there is a straightforward natural definition of the fundamental sequences (see the Wainer hierarchy below), but beyond ε the definition is much more complicated. However, this is possible well beyond the Feferman–Schütte ordinal, Γ, up to at least the Bachmann–Howard ordinal. Using Buchholz psi functions one can extend this definition easily to the ordinal of transfinitely iterated formula_5-comprehension (see Analytical hierarchy).\n\nA fully specified extension beyond the recursive ordinals is thought to be unlikely; e.g., Prӧmel \"et al.\" [1991](p. 348) note that in such an attempt \"there would even arise problems in ordinal notation\".\n\nThe Wainer hierarchy is the particular fast-growing hierarchy of functions \"f\" (α ≤ ε) obtained by defining the fundamental sequences as follows [Gallier 1991][Prӧmel, et al., 1991]:\n\nFor limit ordinals λ < ε, written in Cantor normal form,\n\n\nand\n\n\nSome authors use slightly different definitions (e.g., ω[\"n\"] = ω(\"n+1\"), instead of ω\"n\"), and some define this hierarchy only for α < ε (thus excluding \"f\" from the hierarchy).\n\nTo continue beyond ε, see the Fundamental sequences for the Veblen hierarchy.\n\nFollowing are some relevant points of interest about fast-growing hierarchies:\n\n\nThe functions at finite levels (α < ω) of any fast-growing hierarchy coincide with those of the Grzegorczyk hierarchy: (using hyperoperation)\n\n\nBeyond the finite levels are the functions of the Wainer hierarchy (ω ≤ α ≤ ε):\n\n\n\n\n\n"}
{"id": "17324351", "url": "https://en.wikipedia.org/wiki?curid=17324351", "title": "Generalized Gauss–Newton method", "text": "Generalized Gauss–Newton method\n\nThe generalized Gauss–Newton method is a generalization of the least-squares method originally described by Carl Friedrich Gauss and of Newton's method due to Isaac Newton to the case of constrained nonlinear least-squares problems.\n"}
{"id": "3141761", "url": "https://en.wikipedia.org/wiki?curid=3141761", "title": "Glossary of arithmetic and diophantine geometry", "text": "Glossary of arithmetic and diophantine geometry\n\nThis is a glossary of arithmetic and diophantine geometry in mathematics, areas growing out of the traditional study of Diophantine equations to encompass large parts of number theory and algebraic geometry. Much of the theory is in the form of proposed conjectures, which can be related at various levels of generality.\n\nDiophantine geometry in general is the study of algebraic varieties \"V\" over fields \"K\" that are finitely generated over their prime fields—including as of special interest number fields and finite fields—and over local fields. Of those, only the complex numbers are algebraically closed; over any other \"K\" the existence of points of \"V\" with coordinates in \"K\" is something to be proved and studied as an extra topic, even knowing the geometry of \"V\".\n\n\"Arithmetical\" or \"arithmetic\" (algebraic) geometry is a field with a less elementary definition. After the advent of scheme theory it could reasonably be defined as the study of Alexander Grothendieck's schemes \"of finite type\" over the spectrum of the ring of integers Z. This point of view has been very influential for around half a century; it has very widely been regarded as fulfilling Leopold Kronecker's ambition to have number theory operate only with rings that are quotients of polynomial rings over the integers (to use the current language of commutative algebra). In fact scheme theory uses all sorts of auxiliary constructions that do not appear at all 'finitistic', so that there is little connection with 'constructivist' ideas as such. That scheme theory may not be the last word appears from continuing interest in the 'infinite primes' (the real and complex local fields), which do not come from prime ideals as the p-adic numbers do.\n\n\n\n"}
{"id": "19590493", "url": "https://en.wikipedia.org/wiki?curid=19590493", "title": "Gosper's algorithm", "text": "Gosper's algorithm\n\nIn mathematics, Gosper's algorithm is a procedure for finding sums of hypergeometric terms that are themselves hypergeometric terms. That is: suppose we have \"a\"(1) + ... + \"a\"(\"n\") = \"S\"(\"n\") − \"S\"(0), where \"S\"(\"n\") is a hypergeometric term (i.e., \"S\"(\"n\" + 1)/\"S\"(\"n\") is a rational function of \"n\"); then necessarily \"a\"(\"n\") is itself a hypergeometric term, and given the formula for \"a\"(\"n\") Gosper's algorithm finds that for \"S(\"n\").\n\nStep 1: Find a polynomial \"p\" such that, writing \"b\"(\"n\") = \"a\"(\"n\")/\"p\"(\"n\"), the ratio \"b\"(\"n\")/\"b\"(\"n\" − 1) has the form \"q\"(\"n\")/\"r\"(\"n\") where \"q\" and \"r\" are polynomials and no \"q\"(\"n\") has a nontrivial factor with \"r\"(\"n\" + \"j\") for \"j\" = 0, 1, 2, ... . (This is always possible, whether or not the series is summable in closed form.)\n\nStep 2: Find a polynomial \"ƒ\" such that \"S\"(\"n\") = \"q\"(\"n\" + 1)/\"p\"(\"n\") \"ƒ\"(\"n\") \"a\"(\"n\"). If the series is summable in closed form then clearly a rational function \"ƒ\" with this property exists; in fact it must always be a polynomial, and an upper bound on its degree can be found. Determining \"ƒ\" (or finding that there is no such \"ƒ\") is then a matter of solving a system of linear equations.\n\nGosper's algorithm can be used to discover Wilf–Zeilberger pairs, where they exist. Suppose that \"F\"(\"n\" + 1, \"k\") − \"F\"(\"n\", \"k\") = \"G\"(\"n\", \"k\" + 1) − \"G\"(\"n\", \"k\") where \"F\" is known but \"G\" is not. Then feed \"a\"(\"k\") := \"F\"(\"n\" + 1, \"k\") − \"F\"(\"n\", \"k\") into Gosper's algorithm. (Treat this as a function of k whose coefficients happen to be functions of n rather than numbers; everything in the algorithm works in this setting.) If it successfully finds \"S\"(\"k\") with \"S\"(\"k\") − \"S\"(\"k\" − 1) = \"a\"(\"k\"), then we are done: this is the required \"G\". If not, there is no such \"G\".\n\nGosper's algorithm finds (where possible) a hypergeometric closed form for the \"indefinite\" sum of hypergeometric terms. It can happen that there is no such closed form, but that the sum over \"all\" \"n\", or some particular set of values of n, has a closed form. This question is only meaningful when the coefficients are themselves functions of some other variable. So, suppose a(n,k) is a hypergeometric term in both \"n\" and \"k\": that is, \"a\"(\"n\", \"k\")/\"a\"(\"n\" − 1,\"k\") and \"a\"(\"n\", \"k\")/\"a\"(\"n\", \"k\" − 1) are rational functions of \"n\" and \"k\". Then Zeilberger's algorithm and Petkovšek's algorithm may be used to find closed forms for the sum over \"k\" of \"a\"(\"n\", \"k\").\n\nBill Gosper discovered this algorithm in the 1970s while working on the Macsyma computer algebra system at SAIL and MIT.\n\n"}
{"id": "5852751", "url": "https://en.wikipedia.org/wiki?curid=5852751", "title": "Grothendieck connection", "text": "Grothendieck connection\n\nIn algebraic geometry and synthetic differential geometry, a Grothendieck connection is a way of viewing connections in terms of descent data from infinitesimal neighbourhoods of the diagonal.\n\nThe Grothendieck connection is a generalization of the Gauss–Manin connection constructed in a manner analogous to that in which the Ehresmann connection generalizes the Koszul connection. The construction itself must satisfy a requirement of \"geometric invariance\", which may be regarded as the analog of covariance for a wider class of structures including the schemes of algebraic geometry. Thus the connection in a certain sense must live in a natural sheaf on a Grothendieck topology. In this section, we discuss how to describe an Ehresmann connection in sheaf-theoretic terms as a Grothendieck connection.\n\nLet \"M\" be a manifold and π : \"E\" → \"M\" a surjective submersion, so that \"E\" is a manifold fibred over \"M\". Let J(\"M\",\"E\") be the first-order jet bundle of sections of \"E\". This may be regarded as a bundle over \"M\" or a bundle over the total space of \"E\". With the latter interpretation, an Ehresmann connection is a section of the bundle (over \"E\") J(\"M\",\"E\") → \"E\". The problem is thus to obtain an intrinsic description of the sheaf of sections of this vector bundle.\n\nGrothendieck's solution is to consider the diagonal embedding Δ : \"M\" → \"M\" × \"M\". The sheaf \"I\" of ideals of Δ in \"M\" × \"M\" consists of functions on \"M\" × \"M\" which vanish along the diagonal. Much of the infinitesimal geometry of \"M\" can be realized in terms of \"I\". For instance, Δ (\"I\"/\"I\") is the sheaf of sections of the cotangent bundle. One may define a \"first-order infinitesimal neighborhood\" \"M\" of Δ in \"M\" × \"M\" to be the subscheme corresponding to the sheaf of ideals \"I\". (See below for a coordinate description.) \n\nThere are a pair of projections \"p\", \"p\" : \"M\" × \"M\" → \"M\" given by projection the respective factors of the Cartesian product, which restrict to give projections \"p\", \"p\" : \"M\" → \"M\". One may now form the pullback of the fibre space \"E\" along one or the other of \"p\" or \"p\". In general, there is no canonical way to identify \"p\"\"E\" and \"p\"\"E\" with each other. A Grothendieck connection is a specified isomorphism between these two spaces. One may proceed to define curvature and p-curvature of a connection in the same language.\n\n"}
{"id": "54388740", "url": "https://en.wikipedia.org/wiki?curid=54388740", "title": "Helene Stähelin", "text": "Helene Stähelin\n\nHelene Stähelin (18 July 1891 Wintersingen – 30 December 1970 Basel) was a Swiss mathematician, teacher, and peace activist. \nBetween 1948 and 1967, she was president of the Swiss section of the Women's International League for Peace and Freedom and its representant in the Swiss Peace Council.\n\nShe was one of twelve childs of the parson Gustav Stähelin (1858–1934) and his wife Luise, née Lieb. In 1894, the family moved from Wintersingen to Allschwil.\nHelene Stähelin attended the \"Töchterschule\" Basel and the Universities Basel and Göttingen.\nIn 1922, she became teacher of mathematics and natural sciences at the \"Töchterinstitut\" in Ftan.\nIn 1924, she obtained her \"Dr.phil.\" degree\nfrom Basel University for her dissertation \"Die charakteristischen Zahlen analytischer Kurven auf dem Kegel zweiter Ordnung und ihrer Studyschen Bildkurven\", advised by Hans Mohrmann and .\nIn 1926, she became a member of the Swiss Mathematical Society.\nBetween 1934 and 1956, Helene Stähelin worked as teacher at the Protestant secondary school in Zug.\nAfter her pensioning she returned to Basel, where she assisted for several years to Otto Spiess' editing the Bernoulli family letters.\n\nBeing a pacifist, Helene Stähelin committed herself to the Women's International League for Peace and Freedom (\"Internationale Frauenliga für Frieden und Freiheit\", IFFF) and its struggle against scientific warfare.\nShe was president of the IFFF's Swiss section in 1947–1967, when the main issues were the United Nations Organization, nuclear weapons, and the Vietnam War.\nDue to her peace activism, she was watched by Swiss authorities in the mid 1950s,\nher file at the was kept secret until 1986.\nHelene Stähelin also was active towards Women's suffrage in Switzerland,\nwhich was, however, not gained during her lifetime.\n"}
{"id": "310914", "url": "https://en.wikipedia.org/wiki?curid=310914", "title": "Hodge star operator", "text": "Hodge star operator\n\nIn mathematics, the Hodge star operator or Hodge star is a linear map introduced by W. V. D. Hodge. It is defined on the exterior algebra of a finite-dimensional oriented vector space endowed with a nondegenerate symmetric bilinear form. The result when applied to an element of the algebra is called the element's Hodge dual.\n\nFor example, in 3-dimensional Euclidean space, every oriented plane has an unique normal vector, and every vector can be used to define a plane perpendicular to that vector. The Hodge star can be thought of as generalizing this relationship: in general, for an -dimensional vector space, the Hodge star maps -vectors to -vectors and vice versa.\n\nSuppose that is the dimension of the oriented inner product space and is an integer such that , then the Hodge star operator establishes a one-to-one mapping from the space of -vectors to the space of -vectors. The image of a -vector under this mapping is called its \"Hodge dual\". The former space, of -vectors, has dimension formula_1, while the latter has dimension formula_2, which are equal by the symmetry of the binomial coefficients. Equal-dimensional vector spaces are always isomorphic, but not necessarily in a natural or canonical way. In this case, however, Hodge duality exploits the nondegenerate symmetric bilinear form, hereafter referred to as the \"inner product\" (though it might not be positive definite), and a choice of orientation to single out a unique isomorphism, which parallels the combinatorial symmetry of binomial coefficients. This in turn induces an inner product on the space of -vectors. The naturalness of this definition means the duality can play a role in differential geometry.\n\nThe first interesting case is on three-dimensional Euclidean space . In this context the relevant row of Pascal's triangle reads\n\nand the Hodge star sets up an isomorphism between the two three-dimensional spaces, which are and the image space of the exterior product acting on pairs of vectors in . See \"\" for details. In this case the Hodge star allows the definition of the cross product of traditional vector calculus in terms of the exterior product. While the properties of the cross product are special to three dimensions, the Hodge star applies to an arbitrary number of dimensions.\n\nSince the space of alternating linear forms in arguments on a vector space is naturally isomorphic to the dual of the space of -vectors over that vector space, the Hodge star can be defined for these spaces as well. As with most constructions from linear algebra, the Hodge star can then be extended to a vector bundle. Thus a context in which the Hodge star is very often seen is the exterior algebra of the cotangent bundle, the space of differential forms on a manifold, where it can be used to construct the codifferential from the exterior derivative, and thus the Laplace–de Rham operator, which leads to the Hodge decomposition of differential forms in the case of compact Riemannian manifolds.\n\nThe Hodge star operator on a vector space with an inner product is a linear operator on the exterior algebra of , mapping -vectors to -vectors where , for . It has the following property, which defines it completely: given two -vectors ,\n\nwhere formula_4 denotes the inner product on -vectors and is the preferred unit -vector.\n\nThe inner product formula_4 on -vectors is extended from that on by requiring that for any decomposable -vectors formula_6 and formula_7 it equals the Gram determinant\n\nThe unit -vector is unique up to a sign. The preferred choice of defines an orientation on .\n\nLet be a vector space, with an inner product formula_9. The Riesz representation theorem states that for every continuous (even in the infinite-dimensional case) linear functional formula_10 there exists a unique vector in such that formula_11 for all in . The map formula_12 given by formula_13 is an isomorphism. (If is complex, the map is \"conjugate linear\" as opposed to complex linear.) This holds for \"all\" vector spaces with an inner product, and can be used to explain the Hodge star.\n\nLet be an -dimensional vector space with basis formula_14. For , consider the exterior power spaces formula_15 and formula_16. For\n\nwe have\n\nThere is, up to a scalar, only one -vector (an -form), namely formula_19. In other words, formula_20 must be a scalar multiple of formula_19 for all formula_22 and formula_23.\n\nConsider a \"fixed\" formula_22. There exists a unique linear function\n\nsuch that\n\nThis formula_27 is the scalar multiple mentioned in the previous paragraph. If formula_28 denotes the inner product on -vectors, then there exists a unique -vector, say\n\nsuch that\n\nThis -vector is the Hodge dual of , and is the image of the formula_31 under the isomorphism induced by the inner product,\n\nThus,\n\nGiven an orthonormal basis formula_34 ordered such that formula_35, for a positive-definite metric, we see that\n\nwhere formula_37 is an even permutation of \n\nOf these formula_38 relations, only formula_39 are independent. The first one in the usual lexicographical order reads\n\nUsing tensor index notation, the Hodge dual of an arbitrary wedge product of one-forms is given by the following:\n\nThe symbol formula_42 is the Levi-Civita symbol defined so that formula_43 and formula_44 is the inverse metric. Note that the factorial formula_45 had to be inserted to account for double counting, and cancels out if one orders the summation indices so that formula_46. The absolute value of the determinant is necessary since it may be negative, e.g. for tangent spaces to Lorentzian manifolds.\n\nAn arbitrary differential form can be expanded into its components as follows:\n\nThe factorial formula_48 is again included to account for double counting. We would like to define the dual of the component formula_49 so that the Hodge dual of the form is given by \n\nThus using the expression for the Hodge dual of formula_51 given at the beginning of this section we find\n\nIt is understood that indices are raised and lowered using the same inner product formula_53 as in the definition of the Levi-Civita tensor. Although one can apply this expression to any tensor formula_54, the result is antisymmetric, since contraction with the completely anti-symmetric Levi-Civita symbol cancels all but the totally antisymmetric part of the tensor. It is thus equivalent to antisymmetrization followed by applying the Hodge star.\n\nIn two dimensions with the normalized Euclidean metric and orientation given by the ordering , the Hodge star on -forms is given by\n\nThe complex plane has the remarkable property that it is invariant under holomorphic changes of coordinate.\nIf is a holomorphic function of , then by the Cauchy–Riemann equations and . In the new coordinates\n\nso that\n\nproving the claimed invariance.\n\nA common example of the star operator is the case , when it can be taken as the correspondence between the vectors and the skew-symmetric matrices of that size. This is used implicitly in vector calculus, for example to create the cross product vector from the wedge product of two vectors. Specifically, for Euclidean R, one easily finds that\n\nwhere , and are the standard orthonormal differential one-forms on R. The Hodge star in this case clearly relates the cross-product to the wedge product in three dimensions. A detailed presentation not restricted to differential geometry is provided next.\n\nApplied to three dimensions, the Hodge star provides an isomorphism between axial vectors and bivectors, so each axial vector a is associated with a bivector and vice versa, that is:\n\nwhere is the Hodge star. These dual relations can be implemented using multiplication by the unit pseudoscalar in , (the vectors are an orthonormal basis in three dimensional Euclidean space) according to the relations:\n\nThe dual of a vector is obtained by multiplication by , as established using the properties of the geometric product of the algebra as follows:\n\nand also, in the dual space spanned by \n\nIn establishing these results, the identities are used:\n\nand:\n\nThese relations between the dual and apply to any vectors. Here they are applied to relate the axial vector created as the cross product to the bivector-valued exterior product of two polar (that is, not axial) vectors and ; the two products can be written as determinants expressed in the same way:\n\nusing the notation . These expressions show these two types of vector are Hodge duals:\n\nas a result of the relations:\n\nwith cyclic,\n\nand:\n\nalso with cyclic.\n\nUsing the implementation of based upon , the commonly used relations are:\n\nIn case , the Hodge star acts as an endomorphism of the second exterior power (i.e. it maps two-forms to two-forms, since ). It is an involution, so it splits it into \"self-dual\" and \"anti-self-dual\" subspaces, on which it acts respectively as and .\n\nAnother useful example is Minkowski spacetime with metric signature and coordinates where (using formula_73)\n\nfor one-forms while\n\nfor two-forms. Because their determinants are the same in both and , the signs of the Minkowski space two-form duals depend only on the chosen orientation.\n\nAn easy rule to remember for the above Hodge operations is that given a form formula_54, its Hodge dual formula_77 may be obtained by writing the components not involved in formula_54 in an order such that formula_79. An extra minus sign will enter only if formula_54 does not contain formula_81. (The latter convention stems from the choice for the metric signature. For , one puts a minus sign only if formula_54 involves formula_81.)\n\nThe Hodge star induces an inner product on the space of -vectors, that is, on the exterior algebra of . Given two -vectors and , one has\n\nwhere is the normalised -form (i.e. ). In the calculus of exterior differential forms on a pseudo-Riemannian manifold of dimension , the normalised -form is called the volume form and can be written as\n\nwhere formula_86 is the matrix of components of the metric tensor on the manifold in the coordinate basis.\n\nIf an inner product is given on formula_87, then this equation can be regarded as an alternative definition of the Hodge star.\n\nThe ordered wedge products of k distinct orthonormal basis vectors of form an \"orthonormal\" basis on each subspace formula_88 of the exterior algebra of .\n\nApplying the Hodge star twice leaves a -vector unchanged, up to sign. Given a -vector in in an -dimensional space , one has\n\nwhere is related to the signature of the inner product on . Specifically, is the sign of the determinant of the matrix representation of the inner product tensor with respect to any basis. Thus, for example, if and the signature of the inner product is either or then . For Riemannian manifolds (including Euclidean spaces), the signature is always positive, and so .\n\nNote that the above identity implies that the inverse of can be given as\n\nNote that if is odd then is even for any , whereas if is even then has the parity of . Therefore:\n\nwhere is the degree of the element operated on.\n\nApplying the construction above to each cotangent space of an -dimensional oriented Riemannian or pseudo-Riemannian manifold, we can obtain an object known as the Hodge dual of a -form. To be explicit –\n\nFor any -form we define as the unique -form satisfying\n\nfor every -form (here the inner product on forms and the volume form are induced by the Riemannian metric tensor in the usual way; greater understanding of these objects can be found by learning about the inner product on -forms and the volume form).\n\nThe Hodge star is thus related to the inner product on -forms by the formula:\n\nfor -forms and . (Note that we can also see this as an inner product on sections of formula_94. The set of sections is frequently denoted as formula_95. Each element of formula_96 is a -form.)\n\nMore generally, in the non-oriented case, one can define the Hodge star of a -form as a -pseudo differential form; that is, a differential form with values in the canonical line bundle.\n\nThe most important application of the Hodge star on manifolds is to define the codifferential on -forms. Let\n\nwhere is the exterior derivative or differential, and for Riemannian manifolds.\n\nwhile\n\nThe codifferential is not an antiderivation on the exterior algebra, in contrast to the exterior derivative.\n\nThe codifferential is the adjoint of the exterior derivative, in that\n\nwhere is a -form and a -form. This identity follows from Stokes' theorem for smooth forms, when\n\ni.e. when has empty boundary or when or has zero boundary values (of course, true adjointness follows after continuous continuation to the appropriate topological vector spaces as closures of the spaces of smooth forms).\n\nNotice that since the differential satisfies , the codifferential has the corresponding property\n\nThe Laplace–deRham operator is given by\n\nand lies at the heart of Hodge theory. It is symmetric:\n\nand non-negative:\n\nThe Hodge star sends harmonic forms to harmonic forms. As a consequence of the Hodge theory, the de Rham cohomology is naturally isomorphic to the space of harmonic -forms, and so the Hodge star induces an isomorphism of cohomology groups\n\nwhich in turn gives canonical identifications via Poincaré duality of with its dual space.\n\nThe combination of the operator and the exterior derivative generates the classical operators , , and , in three-dimensional Euclidean space. This works out as follows: can take a 0-form (function) to a 1-form, a 1-form to a 2-form, and a 2-form to a 3-form (applied to a 3-form it just gives zero). For a 0-form, formula_107, the first case written out in components is identifiable as the grad operator:\n\nThe second case followed by is an operator on 1-forms (formula_109) that in components is the curl operator:\n\nApplying the Hodge star gives:\n\nThe final case prefaced and followed by , takes a 1-form (formula_109) to a 0-form (function); written out in components it is the divergence operator:\n\nOne advantage of this expression is that the identity , which is true in all cases, sums up two others, namely that and . In particular, Maxwell's equations take on a particularly simple and elegant form, when expressed in terms of the exterior derivative and the Hodge star.\n\nOne can also obtain the Laplacian. Using the information above and the fact that then for a 0-form, formula_107:\n\n"}
{"id": "36234984", "url": "https://en.wikipedia.org/wiki?curid=36234984", "title": "Holonomic basis", "text": "Holonomic basis\n\nIn mathematics and mathematical physics, a coordinate basis or holonomic basis for a differentiable manifold is a set of basis vector fields defined at every point of a region of the manifold as\nwhere is the infinitesimal displacement vector between the point and a nearby point\n\nIt is possible to make an association between such a basis and directional derivative operators. Given a parameterized curve on the manifold defined by with the tangent vector , where , and a function defined in a neighbourhood of , the variation of along can be written as\nSince we have that , the identification is often made between a coordinate basis vector and the partial derivative operator , under the interpretation of all vector relations as equalities between operators acting on scalar quantities.\n\nA local condition for a basis to be holonomic is that all mutual Lie derivatives vanish:\n\nA basis that is not holonomic is called a non-holonomic or non-coordinate basis.\n\nGiven a metric tensor on a manifold , it is in general not possible to find a coordinate basis that is orthonormal in any open region of . An obvious exception is when is the real coordinate space considered as a manifold with being the Euclidean metric at every point.\n\n"}
{"id": "1966001", "url": "https://en.wikipedia.org/wiki?curid=1966001", "title": "Isotropic manifold", "text": "Isotropic manifold\n\nIn mathematics, an isotropic manifold is a manifold in which the geometry does not depend on directions. Formally, we say that a Riemannian manifold formula_1 is isotropic if for any point formula_2 and unit vectors formula_3, there is an isometry formula_4 of formula_5 with formula_6 and formula_7. Every complete isotropic manifold is homogeneous, i.e. for any formula_8 there is an isometry formula_4 of formula_5 with formula_11 This can be seen by considering a geodesic formula_12 from formula_13 to formula_14 and taking the isometry which fixes formula_15 and maps formula_16 to formula_17\nThe simply-connected space forms (the n-sphere, hyperbolic space, and formula_18) are isotropic. It is not true in general that any constant curvature manifold is isotropic; for example, the flat torus formula_19 is not isotropic. This can be seen by noting that any isometry of formula_20 which fixes a point formula_21 must lift to an isometry of formula_22 which fixes a point and preserves formula_23; thus the group of isometries of formula_20 which fix formula_13 is discrete. Moreover, it can be seen that no oriented surface with constant curvature and negative Euler characteristic is isotropic.\n\nMoreover, there are isotropic manifolds which do not have constant curvature, such as the complex projective space formula_26 (formula_27) equipped with the Fubini-Study metric.\n\nFurther examples of isotropic manifolds are given by the rank one symmetric spaces, including the projective spaces formula_28, formula_26, formula_30, and formula_31, as well as their noncompact hyperbolic analogues.\n\nA manifold can be homogeneous but not isotropic, such as the flat torus formula_20 or formula_33 with the product metric.\n\n"}
{"id": "50501311", "url": "https://en.wikipedia.org/wiki?curid=50501311", "title": "Kengo Hirachi", "text": "Kengo Hirachi\n\nKengo Hirachi (平地 健吾 \"Hirachi Kengo\", born 30 November 1964) is a Japanese mathematician, specializing in CR geometry and mathematical analysis.\n\nHirachi received from Osaka University his B.S. in 1987, his M.S. in 1989, and his Dr.Sci., advised by Gen Komatsu, in 1994 with dissertation \"The second variation of the Bergman kernel for ellipsoids\". He was a research assistant from 1989 to 1996 and a lecturer from 1996 to 2000 at Osaka University. He was an associate professor from 2000 to 2010 and a full professor from 2010 to the present at the University of Tokyo. He was a visiting professor at the Mathematical Sciences Research Institute from October 1995 to September 1996, at the Erwin Schrödinger Institute for Mathematical Physics from March 2004 to April 2004, at Princeton University from October 2004 to July 2005, and at the Institute for Advanced Study from January 2009 to April 2009.\n\n\n"}
{"id": "35221526", "url": "https://en.wikipedia.org/wiki?curid=35221526", "title": "Lancelot Stephen Bosanquet", "text": "Lancelot Stephen Bosanquet\n\nLancelot Stephen Bosanquet (26 December 1903 St. Stephen's-by-Saltash, Cornwall, England – 10 January 1984 Cambridge) was a British mathematician who worked in analysis, especially Fourier series.\n\n"}
{"id": "54494563", "url": "https://en.wikipedia.org/wiki?curid=54494563", "title": "Limit and colimit of presheaves", "text": "Limit and colimit of presheaves\n\nIn category theory, a branch of mathematics, a limit or a colimit of presheaves on a category \"C\" is a limit or colimit in the functor category formula_1.\n\nThe category formula_2 admits small limits and small colimits. Explicitly, if formula_3 is a functor from a small category \"I\" and \"U\" is an object in \"C\", then formula_4 is computed pointwise:\n\nThe same is true for small limits. Concretely this means that, for example, a fiber product exists and is computed pointwise.\n\nWhen \"C\" is small, by the Yoneda lemma, one can view \"C\" as the full subcategory of formula_2. If formula_7 is a functor, if formula_8 is a functor from a small category \"I\" and if the colimit formula_9 in formula_2 is representable; i.e., isomorphic to an object in \"C\", then, in \"D\",\n\nThe density theorem states that every presheaf is a colimit of representable presheaves.\n"}
{"id": "4587078", "url": "https://en.wikipedia.org/wiki?curid=4587078", "title": "Longest increasing subsequence", "text": "Longest increasing subsequence\n\nIn computer science, the longest increasing subsequence problem is to find a subsequence of a given sequence in which the subsequence's elements are in sorted order, lowest to highest, and in which the subsequence is as long as possible. This subsequence is not necessarily contiguous, or unique.\nLongest increasing subsequences are studied in the context of various disciplines related to mathematics, including algorithmics, random matrix theory, representation theory, and physics. The longest increasing subsequence problem is solvable in time O(\"n\" log \"n\"), where \"n\" denotes the length of the input sequence.\n\nIn the first 16 terms of the binary Van der Corput sequence\na longest increasing subsequence is\nThis subsequence has length six; the input sequence has no seven-member increasing subsequences. The longest increasing subsequence in this example is not unique: for instance,\nare other increasing subsequences of equal length in the same input sequence.\n\nThe longest increasing subsequence problem is closely related to the longest common subsequence problem, which has a quadratic time dynamic programming solution: the longest increasing subsequence of a sequence \"S\" is the longest common subsequence of \"S\" and \"T\", where \"T\" is the result of sorting \"S\". However, for the special case in which the input is a permutation of the integers 1, 2, ..., \"n\", this approach can be made much more efficient, leading to time bounds of the form O(\"n\" log log \"n\").\n\nThe largest clique in a permutation graph is defined by the longest decreasing subsequence of the permutation that defines the graph; the longest decreasing subsequence is equivalent in computational complexity, by negation of all numbers, to the longest increasing subsequence. Therefore, longest increasing subsequence algorithms can be used to solve the clique problem efficiently in permutation graphs.\n\nIn the Robinson–Schensted correspondence between permutations and Young tableaux, the length of the first row of the tableau corresponding to a permutation equals the length of the longest increasing subsequence of the permutation, and the length of the first column equals the length of the longest decreasing subsequence.\n\nThe algorithm outlined below solves the longest increasing subsequence problem efficiently with arrays and binary searching. \nIt processes the sequence elements in order, maintaining the longest increasing subsequence found so far. Denote the sequence values as X[0], X[1], etc. Then, after processing X[\"i\"], the algorithm will have stored values in two arrays:\nIn addition the algorithm stores a variable L representing the length of the longest increasing subsequence found so far. Because the algorithm below uses zero-based numbering, for clarity M is padded with M[0], which goes unused so that M[\"j\"] corresponds to a subsequence of length \"j\". A real implementation can skip M[0] and adjust the indices accordingly.\n\nNote that, at any point in the algorithm, the sequence\nis increasing. For, if there is an increasing subsequence of length \"j\" ≥ 2 ending at X[M[\"j\"]], then there is also a subsequence of length \"j\"-1 ending at a smaller value: namely the one ending at X[P[M[\"j\"]]]. Thus, we may do binary searches in this sequence in logarithmic time.\n\nThe algorithm, then, proceeds as follows:\n\nBecause the algorithm performs a single binary search per sequence element, its total time can be expressed using Big O notation as O(\"n\" log \"n\"). discusses a variant of this algorithm, which he credits to Donald Knuth; in the variant that he studies, the algorithm tests whether each value X[\"i\"] can be used to extend the current longest increasing sequence, in constant time, prior to doing the binary search. With this modification, the algorithm uses at most comparisons in the worst case, which is optimal for a comparison-based algorithm up to the constant factor in the O(\"n\") term.\n\nAccording to the Erdős–Szekeres theorem, any sequence of \"n\"+1 distinct integers has an increasing or a decreasing subsequence of length For inputs in which each permutation of the input is equally likely, the expected length of the longest increasing subsequence is approximately 2.\nIn the limit as \"n\" approaches infinity, the length of the longest increasing subsequence of a randomly permuted sequence of \"n\" items has a distribution approaching the Tracy–Widom distribution, the distribution of the largest eigenvalue of a random matrix in the Gaussian unitary ensemble.\n\nThe longest increasing subsequence has also been studied in the setting of online algorithms, in which the elements of a sequence of independent random variables with continuous distribution \"F\" – or alternatively the elements of a random permutation – are presented one at a time to an algorithm that must decide whether to include or exclude each element, without knowledge of the later elements. In this variant of the problem, which allows for interesting applications in several contexts, it is possible to devise an optimal selection procedure that, given a random sample of size \"n\" as input, will generate an increasing sequence with maximal expected length of size approximately .\n\nThe length of the increasing subsequence selected by this optimal procedure has variance approximately equal to \"/3\", and its limiting distribution is asymptotically normal after the usual centering and scaling.\nThe same asymptotic results hold with more precise bounds for the corresponding problem in the setting of a Poisson arrival process.\nA further refinement in the Poisson process setting is given through the proof of a central limit theorem for the optimal selection process\nwhich holds, with a suitable normalization, in a more complete sense than one would expect. The proof yields not only the \"correct\" functional limit theorem\nbut also the (singular) covariance matrix of the three-dimensional process summarizing all interacting processes.\n\n"}
{"id": "4350138", "url": "https://en.wikipedia.org/wiki?curid=4350138", "title": "Loop theorem", "text": "Loop theorem\n\nIn mathematics, in the topology of 3-manifolds, the loop theorem is a generalization of Dehn's lemma. The loop theorem was first proven by Christos Papakyriakopoulos in 1956, along with Dehn's lemma and the Sphere theorem.\n\nA simple and useful version of the loop theorem states that if there is a map\n\nwith formula_2 not nullhomotopic in formula_3, then there is an embedding with the same property.\n\nThe following version of the loop theorem, due to John Stallings, is given in the standard 3-manifold treatises (such as Hempel or Jaco):\n\nLet formula_4 be a 3-manifold and let formula_5\nbe a connected surface in formula_6. Let formula_7 be a normal subgroup such that formula_8.\nLet\n\nbe a continuous map such that\n\nand\n\nThen there exists an embedding\n\nsuch that\n\nand\n\nFurthermore if one starts with a map \"f\" in general position, then for any neighborhood U of the singularity set of \"f\", we can find such a \"g\" with image lying inside the union of image of \"f\" and U.\n\nStalling's proof utilizes an adaptation, due to Whitehead and Shapiro, of Papakyriakopoulos' \"tower construction\". The \"tower\" refers to a special sequence of coverings designed to simplify lifts of the given map. The same tower construction was used by Papakyriakopoulos to prove the sphere theorem (3-manifolds), which states that a nontrivial map of a sphere into a 3-manifold implies the existence of a nontrivial \"embedding\" of a sphere. There is also a version of Dehn's lemma for minimal discs due to Meeks and S.-T. Yau, which also crucially relies on the tower construction.\n\nA proof not utilizing the tower construction exists of the first version of the loop theorem. This was essentially done 30 years ago by Friedhelm Waldhausen as part of his solution to the word problem for Haken manifolds; although he recognized this gave a proof of the loop theorem, he did not write up a detailed proof. The essential ingredient of this proof is the concept of Haken hierarchy. Proofs were later written up, by Klaus Johannson, Marc Lackenby, and Iain Aitchison with Hyam Rubinstein.\n\n"}
{"id": "1464307", "url": "https://en.wikipedia.org/wiki?curid=1464307", "title": "Luigi Guido Grandi", "text": "Luigi Guido Grandi\n\nDom Guido Grandi, O.S.B. Cam. (October 1, 1671 – July 4, 1742) was an Italian monk, priest, philosopher, theologian, mathematician, and engineer.\n\nGrandi was born on Oct. 1, 1671 in Cremona, Italy and christened Luigi Francesco Lodovico. When he was of age, he was educated at the Jesuit college there. After he completed his studies there in 1687, he entered the novitiate of the Camaldolese monks at Ferrara and took the name of Guido. In 1693 he was sent to the Monastery of St. Gregory the Great, the Camaldolese house in Rome, to complete his studies in philosophy and theology in preparation for Holy Orders. A year later, Grandi was assigned as professor of both fields at the Camaldolese Monastery of St. Mary of the Angels in Florence. It appears that it was during this period of his life that he took an interest in mathematics. He did his research privately, however, as he was appointed professor of philosophy at St. Gregory Monastery in 1700, subsequently holding a post in the same field in Pisa.\n\nBy 1707, however, Dom Grandi had developed such a reputation in the field of mathematics that he was named court mathematician to the Grand Duke of Tuscany, Cosimo III de Medici. In that post, he also worked as an engineer, being appointed Superintendent of Water for the Duchy, and in that capacity he was involved in the drainage of the Chiana Valley.In 1709 he visited England where he clearly impressed his colleagues there, as he was elected a Fellow of the Royal Society. The University of Pisa named him Professor of Mathematics in 1714. It was there that he died on 4 July 1742.\n\nIn 1701 Grandi published a study of the conical loxodrome, followed by a study in 1703 of the curve which he named \"versiera\", from the (to turn). This curve was later studied by one of the few women scientists to achieve a degree, Maria Gaetana Agnesi. Through a mistranslation by the translator of her work into English who mistook the term \"witch\" () for Grandi's term, this curve became known in English as the witch of Agnesi. It was through his studies on this curve that Grandi helped introduce Leibniz' ideas on calculus to Italy.\n\nIn mathematics Grandi is best known for his work \"Flores geometrici\" (1728), studying the rose curve, a curve which has the shape of a petalled flower, and for Grandi's series. He named the rose curve \"rhodonea\". He also contributed to the \"Note on the Treatise of Galileo Concerning Natural Motion\" in the first Florentine edition of Galileo Galilei's works.\n\n\n"}
{"id": "577301", "url": "https://en.wikipedia.org/wiki?curid=577301", "title": "Magnitude (mathematics)", "text": "Magnitude (mathematics)\n\nIn mathematics, magnitude is the size of a mathematical object, a property which determines whether the object is larger or smaller than other objects of the same kind. More formally, an object's magnitude is the displayed result of an ordering (or ranking) of the class of objects to which it belongs.\n\nThe Greeks distinguished between several types of magnitude, including:\n\nThey proved that the first two could not be the same, or even isomorphic systems of magnitude. They did not consider negative magnitudes to be meaningful, and \"magnitude\" is still chiefly used in contexts in which zero is either the smallest size or less than all possible sizes.\n\nThe magnitude of any number is usually called its \"absolute value\" or \"modulus\", denoted by |\"x\"|.\n\nThe absolute value of a real number \"r\" is defined by:\n\nAbsolute value may be thought of as the number's distance from zero on the real number line. For example, the absolute value of both 70 and −70 is 70.\n\nA complex number \"z\" may be viewed as the position of a point \"P\" in a 2-dimensional space, called the complex plane. The absolute value or modulus of \"z\" may be thought of as the distance of \"P\" from the origin of that space. The formula for the absolute value of is similar to that for the Euclidean norm of a vector in a 2-dimensional Euclidean space:\n\nwhere the real numbers \"a\" and \"b\" are the real part and the imaginary part of \"z\", respectively. For instance, the modulus of is formula_4. Alternatively, the magnitude of a complex number \"z\" may be defined as the square root of the product of itself and its complex conjugate, \"z\", where for any complex number , its complex conjugate is . \n\nA Euclidean vector represents the position of a point \"P\" in a Euclidean space. Geometrically, it can be described as an arrow from the origin of the space (vector tail) to that point (vector tip). Mathematically, a vector x in an \"n\"-dimensional Euclidean space can be defined as an ordered list of \"n\" real numbers (the Cartesian coordinates of \"P\"): \"x\" = [\"x\", \"x\", ..., \"x\"]. Its magnitude or length is most commonly defined as its Euclidean norm (or Euclidean length):\nFor instance, in a 3-dimensional space, the magnitude of [3, 4, 12] is 13 because formula_8\nThis is equivalent to the square root of the dot product of the vector by itself:\n\nThe Euclidean norm of a vector is just a special case of Euclidean distance: the distance between its tail and its tip. Two similar notations are used for the Euclidean norm of a vector \"x\":\nA disadvantage of the second notation is that it is also used to denote the absolute value of scalars and the determinants of matrices and therefore can be ambiguous.\n\nBy definition, all Euclidean vectors have a magnitude (see above). However, the notion of magnitude cannot be applied to all kinds of vectors.\n\nA function that maps objects to their magnitudes is called a norm. A vector space endowed with a norm, such as the Euclidean space, is called a normed vector space. Not all vector spaces are normed.\n\nIn a pseudo-Euclidean space, the magnitude of a vector is the value of the quadratic form for that vector.\n\nWhen comparing magnitudes, a logarithmic scale is often used. Examples include the loudness of a sound (measured in decibels), the brightness of a star, and the Richter scale of earthquake intensity. Logarithmic magnitudes can be negative. It is not meaningful to simply add or subtract them.\n\nOrders of magnitude denote differences in numeric quantities, usually measurements, by a factor of 10—that is, a difference of one digit in the location of the decimal point.\n\n"}
{"id": "18578839", "url": "https://en.wikipedia.org/wiki?curid=18578839", "title": "Mark J. Ablowitz", "text": "Mark J. Ablowitz\n\nMark Jay Ablowitz (born June 5, 1945, New York) is a professor in the department of Applied Mathematics at the University of Colorado at Boulder, Colorado. He was born in New York City. \n\nAblowitz received his Bachelor of Science degree in Mechanical Engineering from University of Rochester, and completed his Ph.D. in Mathematics under the supervision of David Benney at Massachusetts Institute of Technology in 1971.\n\nAblowitz was an assistant professor of Mathematics at Clarkson University during 1971–1975 and an associate professor during 1975–1976. He visited the Program in Applied Mathematics founded by Ahmed Cemal Eringen at Princeton University during 1977–1978. He was a professor of Mathematics at Clarkson during 1976-1985 where he became the Chairman of the Department of Mathematics and Computer Science in 1979. In July 1, 1985, he was appointed as the Dean of Science of Clarkson University and served there until he joined to the department of Applied Mathematics (APPM) at University of Colorado Boulder in June 30, 1989.\n\n\n"}
{"id": "13410380", "url": "https://en.wikipedia.org/wiki?curid=13410380", "title": "Mathematics and fiber arts", "text": "Mathematics and fiber arts\n\nIdeas from Mathematics have been used as inspiration for fiber arts including quilt making, knitting, cross-stitch, crochet, embroidery and weaving. A wide range of mathematical concepts have been used as inspiration including topology, graph theory, number theory and algebra. Some techniques such as counted-thread embroidery are naturally geometrical; other kinds of textile provide a ready means for the colorful physical expression of mathematical concepts.\n\nThe IEEE Spectrum has organized a number of competitions on quilt block design, and several books have been published on the subject. Notable quiltmakers include Diana Venters and Elaine Ellison, who have written a book on the subject \"Mathematical Quilts: No Sewing Required\". Examples of mathematical ideas used in the book as the basis of a quilt include the golden rectangle, conic sections, Leonardo da Vinci's Claw, the Koch curve, the Clifford torus, San Gaku, Mascheroni's cardioid, Pythagorean triples, spidrons, and the six trigonometric functions.\n\nKnitted mathematical objects include the Platonic solids, Klein bottles and Boy's surface.\nThe Lorenz manifold and the hyperbolic plane have been crafted using crochet. Knitted and crocheted tori have also been constructed depicting toroidal embeddings of the complete graph \"K\" and of the Heawood graph. The crocheting of hyperbolic planes has been popularized by the Institute For Figuring; a book by Daina Taimina on the subject, \"Crocheting Adventures with Hyperbolic Planes\", won the 2009 Bookseller/Diagram Prize for Oddest Title of the Year.\n\nEmbroidery techniques such as counted-thread embroidery including cross-stitch and some canvas work methods such as Bargello (needlework) make use of the natural pixels of the weave, lending themselves to geometric designs.\n\nAda Dietz (1882 – 1950) was an American weaver best known for her 1949 monograph \"Algebraic Expressions in Handwoven Textiles\", which defines weaving patterns based on the expansion of multivariate polynomials.\n\nMargaret Greig was a mathematician who articulated the mathematics of worsted spinning.\n\nThe silk scarves from DMCK Designs' 2013 collection are all based on Douglas McKenna's space-filling curve patterns. The designs are either generalized Peano curves, or based on a new space-filling construction technique.\n\nThe Issey Miyake Fall-Winter 2010–2011 ready-to-wear collection featured designs from a collaboration between fashion designer Dai Fujiwara and mathematician William Thurston. The designs were inspired by Thurston's geometrization conjecture, the statement that every 3-manifold can be decomposed into pieces with one of eight different uniform geometries, a proof of which had been sketched in 2003 by Grigori Perelman as part of his proof of the Poincaré conjecture.\n\n\n"}
{"id": "1072006", "url": "https://en.wikipedia.org/wiki?curid=1072006", "title": "Median (geometry)", "text": "Median (geometry)\n\nIn geometry, a median of a triangle is a line segment joining a vertex to the midpoint of the opposing side, bisecting it. Every triangle has exactly three medians, one from each vertex, and they all intersect each other at the triangle's centroid. In the case of isosceles and equilateral triangles, a median bisects any angle at a vertex whose two adjacent sides are equal in length.\n\nThe concept of a median extends to tetrahedra.\n\nEach median of a triangle passes through the triangle's centroid, which is the center of mass of an infinitely thin object of uniform density coinciding with the triangle. Thus the object would balance on the intersection point of the medians. The centroid is twice as close along any median to the side that the median intersects as it is to the vertex it emanates from.\n\nEach median divides the area of the triangle in half; hence the name, and hence a triangular object of uniform density would balance on any median. (Any other lines which divide the area of the triangle into two equal parts do not pass through the centroid.) The three medians divide the triangle into six smaller triangles of equal area. \n\nConsider a triangle \"ABC\". Let \"D\" be the midpoint of formula_1, \"E\" be the midpoint of formula_2, \"F\" be the midpoint of formula_3, and \"O\" be the centroid (most commonly denoted \"G\").\n\nBy definition, formula_4. Thus formula_5 and formula_6, where formula_7 represents the area of triangle formula_8 ; these hold because in each case the two triangles have bases of equal length and share a common altitude from the (extended) base, and a triangle's area equals one-half its base times its height.\n\nWe have:\n\nThus, formula_11 and formula_12\n\nSince formula_13, therefore, formula_14.\nUsing the same method, one can show that formula_15.\n\nIn 2014 Lee Sallows discovered the following theorem:\n\nThe lengths of the medians can be obtained from Apollonius' theorem as:\n\nwhere \"a\", \"b\" and \"c\" are the sides of the triangle with respective medians \"m\", \"m\", and \"m\" from their midpoints.\n\nThus we have the relationships:\n\nThe centroid divides each median into parts in the ratio 2:1, with the centroid being twice as close to the midpoint of a side as it is to the opposite vertex.\n\nFor any triangle with sides formula_22 and medians formula_23 \n\nand\n\nThe medians from sides of lengths \"a\" and \"b\" are perpendicular if and only if formula_26\n\nThe medians of a right triangle with hypotenuse \"c\" satisfy formula_27\n\nAny triangle's area \"T\" can be expressed in terms of its medians formula_28, and formula_29 as follows. Denoting their semi-sum as σ, we have\n\nA tetrahedron is a three-dimensional object having four triangular faces. A line segment joining a vertex of a tetrahedron with the centroid of the opposite face is called a \"median\" of the tetrahedron. There are four medians, and they are all concurrent at the \"centroid\" of the tetrahedron. As in the two-dimensional case, the centroid of the tetrahedron is the center of mass. However contrary to the two-dimensional case the centroid divides the medians not in a 2:1 ratio but in a 3:1 ratio (Commandino's theorem).\n\n\n"}
{"id": "23504223", "url": "https://en.wikipedia.org/wiki?curid=23504223", "title": "Michael J. Fischer", "text": "Michael J. Fischer\n\nMichael John Fischer (born 1942) is a computer scientist who works in the fields of distributed computing, parallel computing, cryptography, algorithms and data structures, and computational complexity.\n\nFischer was born in 1942 in Ann Arbor, Michigan, USA. He received his BSc degree in mathematics from the University of Michigan in 1963. Fischer did his MA and PhD studies in applied mathematics at Harvard University; he received his MA degree in 1965 and PhD in 1968. Fischer's PhD supervisor at Harvard was Sheila Greibach.\n\nAfter receiving his PhD, Fischer was an assistant professor of computer science at Carnegie-Mellon University in 1968–1969, an assistant professor of mathematics at Massachusetts Institute of Technology (MIT) in 1969–1973, and an associate professor of electrical engineering at MIT in 1973–1975. At MIT he supervised doctoral students who became prominent computer scientists, including David S. Johnson, Frances Yao, and Michael Hammer.\n\nIn 1975, Fischer was nominated as a professor of computer science at the University of Washington. Since 1981, he has been a professor of computer science at Yale University. Fischer served as the editor-in-chief of the Journal of the ACM in 1982–1986. He was inducted as a Fellow of the Association for Computing Machinery (ACM) in 1996.\n\nFischer is famous for his contributions in the field of distributed computing. His 1985 work with Nancy A. Lynch and Michael S. Paterson on consensus problems received the PODC Influential-Paper Award in 2001. Their work showed that in an asynchronous distributed system, consensus is impossible if there is one processor that crashes. Jennifer Welch writes that “This result has had a monumental impact in distributed computing, both theory and practice. Systems designers were motivated to clarify their claims concerning under what circumstances the systems work.”\n\nFischer was the program chairman of the first Symposium on Principles of Distributed Computing (PODC) in 1982; nowadays, PODC is the leading conference in the field. In 2003, the distributed computing community honoured Fischer's 60th birthday by organising a lecture series during the 22nd PODC, with Leslie Lamport, Nancy Lynch, Albert R. Meyer, and Rebecca Wright as speakers.\n\nIn 1980, Fischer and Richard E. Ladner presented a parallel algorithm for computing prefix sums efficiently. They show how to construct a circuit that computes the prefix sums; in the circuit, each node performs an addition of two numbers. With their construction, one can choose a trade-off between the circuit depth and the number of nodes. However, the same circuit designs were already studied much earlier in Soviet mathematics.\n\nFischer has done multifaceted work in theoretical computer science in general. Fischer's early work, including his PhD thesis, focused on parsing and formal grammars. One of Fischer's most-cited works deals with string matching. Already during his years at Michigan, Fischer studied disjoint-set data structures together with Bernard Galler.\n\nFischer is one of the pioneers in electronic voting. In 1985, Fischer and his student Josh Cohen Benaloh presented one of the first electronic voting schemes.\n\nOther contributions related to cryptography include the study of key exchange problems and a protocol for oblivious transfer. In 1984, Fischer, Silvio Micali, and Charles Rackoff presented an improved version of Michael O. Rabin's protocol for oblivious transfer.\n\n\n"}
{"id": "37793177", "url": "https://en.wikipedia.org/wiki?curid=37793177", "title": "Monodomain model", "text": "Monodomain model\n\nThe monodomain model is a reduction of the bidomain model of the electrical propagation in myocardial tissue.\nThe reduction comes from assuming that the intra- and extracellular domains have equal anisotropy ratios.\nAlthough not as physiologically accurate as the bidomain model, it is still adequate in some cases, and has reduced complexity.\n\nThe monodomain model can be formulated as follows\n\nwhere formula_2 is the intracellular conductivity tensor, formula_3 is the transmembrane potential, formula_4 is the transmembrane ionic current per unit area, formula_5 is the membrane conductivity per unit area, formula_6 is the intra- to extracellular conductivity ratio, and formula_7 is the membrane surface area per unit volume (of tissue).\n\nThe bidomain model can be written as\n\nAssuming equal anisotropy ratios, i.e. formula_9, the second equation can be written\n\nInserting this into the first bidomain equation gives\n"}
{"id": "427282", "url": "https://en.wikipedia.org/wiki?curid=427282", "title": "Mutual information", "text": "Mutual information\n\nIn probability theory and information theory, the mutual information (MI) of two random variables is a measure of the mutual dependence between the two variables. More specifically, it quantifies the \"amount of information\" (in units such as shannons, commonly called bits) obtained about one random variable through observing the other random variable. The concept of mutual information is intricately linked to that of entropy of a random variable, a fundamental notion in information theory that quantifies the expected \"amount of information\" held in a random variable.\n\nNot limited to real-valued random variables like the correlation coefficient, MI is more general and determines how similar the joint distribution formula_1 is to the products of factored marginal distribution formula_2. MI is the expected value of the pointwise mutual information (PMI).\n\nFormally, the mutual information of two discrete random variables formula_3 and formula_4 can be defined as:\n\nwhere formula_1 is the joint probability function of formula_3 and formula_4, and formula_9 and formula_10 are the marginal probability distribution functions of formula_3 and formula_4 respectively.\n\nIn the case of continuous random variables, the summation is replaced by a definite double integral:\n\nwhere formula_1 is now the joint probability \"density\" function of formula_3 and formula_4, and formula_9 and formula_10 are the marginal probability density functions of formula_3 and formula_4 respectively.\n\nIf the log base 2 is used, the units of mutual information are bits.\n\nIntuitively, mutual information measures the information that formula_3 and formula_4 share: It measures how much knowing one of these variables reduces uncertainty about the other. For example, if formula_3 and formula_4 are independent, then knowing formula_3 does not give any information about formula_4 and vice versa, so their mutual information is zero. At the other extreme, if formula_3 is a deterministic function of formula_4 and formula_4 is a deterministic function of formula_3 then all information conveyed by formula_3 is shared with formula_4: knowing formula_3 determines the value of formula_4 and vice versa. As a result, in this case the mutual information is the same as the uncertainty contained in formula_4 (or formula_3) alone, namely the entropy of formula_4 (or formula_3). Moreover, this mutual information is the same as the entropy of formula_3 and as the entropy of formula_4. (A very special case of this is when formula_3 and formula_4 are the same random variable.)\n\nMutual information is a measure of the inherent dependence expressed in the joint distribution of formula_3 and formula_4 relative to the joint distribution of formula_3 and formula_4 under the assumption of independence. Mutual information therefore measures dependence in the following sense: formula_47 if and only if formula_3 and formula_4 are independent random variables. This is easy to see in one direction: if formula_3 and formula_4 are independent, then formula_52, and therefore:\n\nMoreover, mutual information is nonnegative (i.e. formula_54 see below) and symmetric (i.e. formula_55 see below).\n\nUsing Jensen's inequality on the definition of mutual information we can show that formula_56 is non-negative, i.e.\n\nMutual information can be equivalently expressed as\n\nwhere formula_60 and formula_61 are the marginal entropies, Η(\"X\"|\"Y\") and Η(\"Y\"|\"X\") are the conditional entropies, and Η(\"X\",\"Y\") is the joint entropy of \"X\" and \"Y\". Note the analogy to the union, difference, and intersection of two sets, as illustrated in the Venn diagram. In terms of a communication channel in which the output formula_4 is a noisy version of the input formula_3, these relations are summarised in the figure below.\nBecause formula_64 is non-negative, consequently, formula_65. Here we give the detailed deduction of formula_66:\n\nThe proofs of the other identities above are similar.\n\nIntuitively, if entropy Η(\"Y\") is regarded as a measure of uncertainty about a random variable, then Η(\"Y\"|\"X\") is a measure of what \"X\" does \"not\" say about \"Y\". This is \"the amount of uncertainty remaining about \"Y\" after \"X\" is known\", and thus the right side of the second of these equalities can be read as \"the amount of uncertainty in \"Y\", minus the amount of uncertainty in \"Y\" which remains after \"X\" is known\", which is equivalent to \"the amount of uncertainty in \"Y\" which is removed by knowing \"X\"\". This corroborates the intuitive meaning of mutual information as the amount of information (that is, reduction in uncertainty) that knowing either variable provides about the other.\n\nNote that in the discrete case formula_68 and therefore formula_69. Thus formula_70, and one can formulate the basic principle that a variable contains at least as much information about itself as any other variable can provide. This parallels a similar result about .\n\nMutual information can also be expressed as a Kullback–Leibler divergence of the product of the marginal distributions, formula_2, of the two random variables formula_3 and formula_4, from the random variables' joint distribution, formula_1:\n\nFurthermore, let \"p\"(\"x\"|\"y\") = \"p\"(\"x\", \"y\") / \"p\"(\"y\"). Then\n\nNote that here the Kullback–Leibler divergence involves integration with respect to the random variable formula_3 only and the expression formula_78 is now a random variable in formula_4. Thus mutual information can also be understood as the expectation of the Kullback–Leibler divergence of the univariate distribution formula_9 of formula_3 from the conditional distribution formula_82 of formula_3 given formula_4: the more different the distributions formula_82 and formula_9 are on average, the greater the information gain.\n\nIt is well-understood how to do Bayesian estimation of the mutual information\nof a joint distribution based on samples of that distribution. The\nfirst work to do this, which also showed how to do Bayesian estimation of many\nother information-theoretic besides mutual information, was . Subsequent researchers have rederived \nand extended \nthis analysis. See \nfor a recent paper based on a prior specifically tailored to estimation of mutual\ninformation per se.\n\nThe Kullbeck-Leibler divergence formulation of the mutual information is predicated on that one is interested in comparing \"p(x,y)\" to the fully factorized outer product \"p(x)p(y)\". In many problems, such as non-negative matrix factorization, one is interested in less extreme factorizations; specifically, one wishes to compare \"p(x,y)\" to a low-rank matrix approximation in some unknown variable \"w\"; that is, to what degree one might have\nAlternately, one might be interested in knowing how much more information \"p(x,y)\" carries over its factorization. In such a case, the excess information that the full distribution \"p(x,y)\" carries over the matrix factorization is given by the Kullbeck-Leibler divergence\nThe conventional definition of the mutual information is recovered in the extreme case that the process \"W\" has only one value for \"w\".\n\nSeveral variations on mutual information have been proposed to suit various needs. Among these are normalized variants and generalizations to more than two variables.\n\nMany applications require a metric, that is, a distance measure between pairs of points. The quantity\n\nsatisfies the properties of a metric (triangle inequality, non-negativity, indiscernability and symmetry). This distance metric is also known as the variation of information.\n\nIf formula_90 are discrete random variables then all the entropy terms are non-negative, so formula_91 and one can define a normalized distance\n\nThe metric \"D\" is a universal metric, in that if any other distance measure places \"X\" and \"Y\" close-by, then the \"D\" will also judge them close.\n\nPlugging in the definitions shows that\n\nIn a set-theoretic interpretation of information (see the figure for Conditional entropy), this is effectively the Jaccard distance between \"X\" and \"Y\".\n\nFinally,\n\nis also a metric.\n\nSometimes it is useful to express the mutual information of two random variables conditioned on a third.\n\nwhich can be simplified as\nConditioning on a third random variable may either increase or decrease the mutual information, but it is always true that\nfor discrete, jointly distributed random variables \"X\", \"Y\", \"Z\". This result has been used as a basic building block for proving other inequalities in information theory.\n\nSeveral generalizations of mutual information to more than two random variables have been proposed, such as total correlation and interaction information. If Shannon entropy is viewed as a signed measure in the context of information diagrams, as explained in the article \"Information theory and measure theory\", then the only definition of multivariate mutual information that makes sense is as follows:\n\nand for formula_99\n\nwhere (as above) we define\n\nApplying information diagrams blindly to derive the above definition has been criticised, and indeed it has found rather limited practical application since it is difficult to visualize or grasp the significance of this quantity for a large number of random variables. It can be zero, positive, or negative for any odd number of variables formula_102\n\nOne high-dimensional generalization scheme which maximizes the mutual information between the joint distribution and other target variables is found to be useful in feature selection.\n\nMutual information is also used in the area of signal processing as a measure of similarity between two signals. For example, FMI metric is an image fusion performance measure that makes use of mutual information in order to measure the amount of information that the fused image contains about the source images. The Matlab code for this metric can be found at.\n\nDirected information, formula_103, measures the amount of information that flows from the process formula_104 to formula_105, where formula_104 denotes the vector formula_107 and formula_105 denotes formula_109. The term \"directed information\" was coined by James Massey and is defined as\n\nNote that if \"n\" = 1, the directed information becomes the mutual information. Directed information has many applications in problems where causality plays an important role, such as capacity of channel with feedback.\n\nNormalized variants of the mutual information are provided by the \"coefficients of constraint\", uncertainty coefficient or proficiency:\n\nThe two coefficients are not necessarily equal. In some cases a symmetric measure may be desired, such as the following \"redundancy\" measure:\n\nwhich attains a minimum of zero when the variables are independent and a maximum value of\n\nwhen one variable becomes completely redundant with the knowledge of the other. See also \"Redundancy (information theory)\". Another symmetrical measure is the \"symmetric uncertainty\" , given by\n\nwhich represents the harmonic mean of the two uncertainty coefficients formula_115.\n\nIf we consider mutual information as a special case of the total correlation or dual total correlation, the normalized version are respectively,\n\nThis normalized version also known as Information Quality Ratio (IQR) which quantifies the amount of information of a variable based on another variable against total uncertainty:\n\nThere's a normalization which derives from first thinking of mutual information as an analogue to covariance (thus Shannon entropy is analogous to variance). Then the normalized mutual information is calculated akin to the Pearson correlation coefficient,\n\nIn the traditional formulation of the mutual information,\n\neach \"event\" or \"object\" specified by formula_121 is weighted by the corresponding probability formula_122. This assumes that all objects or events are equivalent \"apart from\" their probability of occurrence. However, in some applications it may be the case that certain objects or events are more \"significant\" than others, or that certain patterns of association are more semantically important than others.\n\nFor example, the deterministic mapping formula_123 may be viewed as stronger than the deterministic mapping formula_124, although these relationships would yield the same mutual information. This is because the mutual information is not sensitive at all to any inherent ordering in the variable values (, , ), and is therefore not sensitive at all to the form of the relational mapping between the associated variables. If it is desired that the former relation—showing agreement on all variable values—be judged stronger than the later relation, then it is possible to use the following \"weighted mutual information\" .\n\nwhich places a weight formula_126 on the probability of each variable value co-occurrence, formula_1. This allows that certain probabilities may carry more or less significance than others, thereby allowing the quantification of relevant \"holistic\" or \"Prägnanz\" factors. In the above example, using larger relative weights for formula_128, formula_129, and formula_130 would have the effect of assessing greater \"informativeness\" for the relation formula_123 than for the relation formula_124, which may be desirable in some cases of pattern recognition, and the like. This weighted mutual information is a form of weighted KL-Divergence, which is known to take negative values for some inputs, and there are examples where the weighted mutual information also takes negative values.\n\nA probability distribution can be viewed as a partition of a set. One may then ask: if a set were partitioned randomly, what would the distribution of probabilities be? What would the expectation value of the mutual information be? The adjusted mutual information or AMI subtracts the expectation value of the MI, so that the AMI is zero when two different distributions are random, and one when two distributions are identical. The AMI is defined in analogy to the adjusted Rand index of two different partitions of a set.\n\nUsing the ideas of Kolmogorov complexity, one can consider the mutual information of two sequences independent of any probability distribution:\n\nTo establish that this quantity is symmetric up to a logarithmic factor (formula_134) requires the chain rule for Kolmogorov complexity . Approximations of this quantity via compression can be used to define a distance measure to perform a hierarchical clustering of sequences without having any domain knowledge of the sequences .\n\nUnlike correlation coefficients, such as the product moment correlation coefficient, mutual information contains information about all dependence—linear and nonlinear—and not just linear dependence as the correlation coefficient measures. However, in the narrow case that the joint distribution for \"X\" and \"Y\" is a bivariate normal distribution (implying in particular that both marginal distributions are normally distributed), there is an exact relationship between \"I\" and the correlation coefficient formula_135 .\n\nThe equation above can be derived as follows for a bivariate Gaussian:\n\nTherefore, \n\nWhen \"X\" and \"Y\" are limited to be in a discrete number of states, observation data is summarized in a contingency table, with row variable \"X\" (or \"i\") and column variable \"Y\" (or \"j\"). Mutual information is one of the measures of association or correlation between the row and column variables. Other measures of association include Pearson's chi-squared test statistics, G-test statistics, etc. In fact, mutual information is equal to G-test statistics divided by 2\"N\", where \"N\" is the sample size.\n\nIn many applications, one wants to maximize mutual information (thus increasing dependencies), which is often equivalent to minimizing conditional entropy. Examples include:\n\n\n"}
{"id": "3166399", "url": "https://en.wikipedia.org/wiki?curid=3166399", "title": "Nicolaas Govert de Bruijn", "text": "Nicolaas Govert de Bruijn\n\nNicolaas Govert (Dick) de Bruijn (; 9 July 1918 – 17 February 2012) was a Dutch mathematician, noted for his many contributions in the fields of analysis, number theory, combinatorics and logic.\n\nBorn in The Hague, De Bruijn received his MA in Mathematics at the Leiden University in 1941. He received his PhD in 1943 from Vrije Universiteit Amsterdam with a thesis entitled \"Over modulaire vormen van meer veranderlijken\" advised by Jurjen Ferdinand Koksma.\n\nDe Bruijn started his academic career at the University of Amsterdam, where he was Professor of Mathematics from 1952 to 1960. In 1960 he moved to the Technical University Eindhoven where he was Professor of Mathematics until his retirement in 1984. Among his graduate students were Johannes Runnenburg (1960), Antonius Levelt (1961), S. Ackermans (1964), Jozef Beenakker (1966), W. van der Meiden (1967), Matheus Hautus (1970), Robert Nederpelt Lazarom (1973), Lambert van Benthem Jutting (1977), A. Janssen (1979), Diederik van Daalen (1980), and Harmannus Balsters (1986).\n\nIn 1957 he was appointed member of the Royal Netherlands Academy of Arts and Sciences. He was Knighted with the Order of the Netherlands Lion.\n\nDe Bruijn covered many areas of mathematics. He is especially noted for:\n\nHe wrote one of the standard books in advanced asymptotic analysis (De Bruijn, 1958).\n\nIn the late sixties, he designed the Automath language for representing mathematical proofs, so that they could be verified automatically (see automated theorem checking). Shortly before his death, he had been working on models for the human brain.\n\nBooks, a selection:\n\nArticles, a selection:\n\n\n"}
{"id": "41727746", "url": "https://en.wikipedia.org/wiki?curid=41727746", "title": "Octahedral pyramid", "text": "Octahedral pyramid\n\nIn 4-dimensional geometry, the octahedral pyramid is bounded by one octahedron on the base and 8 triangular pyramid cells which meet at the apex. Since an octahedron has a circumradius divided by edge length less than one, the triangular pyramids can be made with regular faces (as regular tetrahedrons) by computing the appropriate height.\n\nThe regular 16-cell has \"octahedral pyramids\" around every vertex, with the octahedron passing through the center of the 16-cell.\n\nThe octahedral pyramid is the vertex figure for a truncated 5-orthoplex, .\n\nThe graph of the octahedral pyramid is the only possible minimal counterexample to Negami's conjecture, that the connected graphs with planar covers are themselves projective-planar.\n\nThe dual to the octahedral pyramid is a cubic pyramid, seen as an cubic base, and 6 square pyramids meeting at an apex.\n\nThe square-pyramidal pyramid, ( ) ∨ [( ) ∨ {4}], is a bisected octahedral pyramid. It has a square pyramid base, and 4 tetrahedrons along with another one more square pyramid meeting at the apex. It can also be seen in an edge-centered projection as a square bipyramid with four tetrahedra wrapped around the common edge. If the height of the two apexes are the same, it can be given a higher symmetry name [( ) ∨ ( )] ∨ {4} = { } ∨ {4}, joining an edge to a perpendicular square.\n\nThe \"square-pyramidal pyramid\" can be distorted into a \"rectangular-pyramidal pyramid\", { } ∨ [{ } × { }] or a \"rhombic-pyramidal pyramid\", { } ∨ [{ } + { }], or other lower symmetry forms.\n\nThe \"square-pyramidal pyramid\" exists as a vertex figure in uniform polytopes of the form , including the bitruncated 5-orthoplex and bitruncated tesseractic honeycomb.\n\n"}
{"id": "51088824", "url": "https://en.wikipedia.org/wiki?curid=51088824", "title": "Olimpíada de Matemática do Grande ABC", "text": "Olimpíada de Matemática do Grande ABC\n\nThe Olimpíada de Matemática do Grande ABC (English:\"Grande ABC Mathematical Olympiad\"), or OMABC is a mathematical competition for pre-collegiate Brazilian students of Grande ABC region, composed by the following cities: \n\n\nThe Faculdade de Ciências Exatas e Tecnológicas da Universidade Metodista de São Paulo is the main organizator of this event, who create the tests and correct then. The main purpose of this olympiad is improve the mathematical knowledge, encouraging the study and research in scientific areas., and contributing to participate in national mathematical competitions, like Olimpíada Brasileira de Matemática das Escolas Públicas and Olimpíada Brasileira de Matemática. The first edition was held in 2004.\n\nThe participants are ranked based on their individual scores. Medals are awarded to the highest ranked participants, such that slightly less than half of them receive a medal. Subsequently, the cutoffs (minimum scores required to receive a gold, silver or bronze medal respectively) are chosen such that the ratio of gold to silver to bronze medals awarded approximates 1 : 2 : 3.\n\n\nSpecial prizes are awarded for the schools:\n\n\n Educandário Santo Antônio Colégio Harmonia Colégio Ábaco Colégio Eduardo Gomes Colégio Liceu Jardim\n\n"}
{"id": "46886526", "url": "https://en.wikipedia.org/wiki?curid=46886526", "title": "Phenotypic disease network (PDN)", "text": "Phenotypic disease network (PDN)\n\nThe first phenotypic disease network was constructed by Hidalgo et al. (2009) to help understand the origins of many diseases and the links between them. Hidalgo et al. (2009) defined diseases as specific sets of phenotypes that affect one or several physiological systems, and compiled data on pairwise comorbidity correlations for more than 10,000 diseases reconstructed from over 30 million medical records. Hidalgo et al. (2009) presented their data in the form of a network with diseases as the nodes and comorbidity correlations as the links. Intuitively, the phenotypic disease network (PDN) can be seen as a map of the phenotypic space whose structure can contribute to the understanding of disease progression.\n\nDuring the last decade, several papers were published that aim at understanding the origins and interrelatedness of diseases using the analytical tools of network science. Interactions between disease-associated genes, proteins, and gene expressions have been explored.\nHowever, phenotypic information was essentially overlooked, despite the fact that there exist extensive, high-quality data on it in the form of clinical histories, until the seminal paper of Hidalgo et al. (2009) introducing the human phenotypic disease network.\n\nHidalgo et al. (2009) used Medicare hospital claims based on the MedPAR records on hospitalizations for the period 1990-1993. For the 32 million elderly Americans aged 65 or older enrolled\nin Medicare and alive for the entire study period, there were approximately 32,000,000 inpatient claims, belonging to about 13,000 individuals. The dataset consisted of mainly white patients over 65 years old living in an industrialized country which imposed some limitations on the study; for example, many infectious diseases or pregnancy related conditions did not appear in the data at all.\n\nComorbidity measures are used to measure the \"distance\" between two diseases. The relative risk (RR) of observing disease \"i\" and \"j\" affecting the same patient is given by\nformula_1\nwhere formula_2 denotes the number of patients affected by both diseases, \"N\" is the total number of patients in the population, and formula_3 and formula_4 are the prevalences of diseases \"i\" and \"j\", respectively. \nThe formula_5-correlation (Pearson correlation for binary measures) can be expressed as \nformula_6\nBoth measures have inherent biases: RR overestimates relationships involving rare diseases and underestimates the comorbidity between highly prevalent diseases, while the formula_5-correlation is accurate in describing comorbidity between diseases with similar prevalence but underestimates the comorbidity between rare and common diseases. In the PDN, nodes are disease phenotypes and links connect those phenotypes that have significant comorbidity correlation according to the RR and formula_5-correlation.\nConsidering the complementary biases of these two measures, Hidalgo et al. (2009) constructed a separate PDN for each.\n\nComparing the average correlation between illnesses diagnosed in the first two visits and those diagnosed later (during the third and fourth visits for patients with a total of four visits) to the average correlation in a randomized control case, inter-visit correlations were found to be significantly larger than those that would occur by chance alone, pointing to the fact that patients develop diseases that are close in the PDN to those they alread have.\n\nHidalgo et al. (2009) also established a connection between that the mortality associated with a given disease and its connectivity in the PDN. Diseases that are preceded by others are usually more connected than those that precede others, and they tend to be more lethal.\nThat is to say, patients that have a disease that is more connected in the network face higher mortality rates that those patients who have less connected conditions.\n\nIn the Hidalgo et al. (2009) study, disease progression was found to be different across genders and ethnicities. For example, hypertension, diabetes, and renal disorders tend to be more comorbid in white males, while heart disease, infactions, pulmonary complications and hypercholesterolemia are more comorbid in white males.\n\n\n"}
{"id": "52031076", "url": "https://en.wikipedia.org/wiki?curid=52031076", "title": "Power law of cache misses", "text": "Power law of cache misses\n\nA power law is a mathematical relationship between two quantities in which one is directly proportional to some power of the other. The power law for cache misses was first established by C. K. Chow in his 1974 paper, supported by experimental data on hit ratios for stack processing by Richard Mattson in 1971. The power law of cache misses can be used to narrow down the cache sizes to practical ranges, given a tolerable miss rate, as one of the early steps while designing the cache hierarchy for a uniprocessor system.\n\nThe power law for cache misses can be stated as\n\nwhere \"M\" is the miss rate for a cache of size \"C\" and \"M\" is the miss rate of a baseline cache. The exponent \"α\" is workload-specific and typically ranges from 0.3 to 0.7.\n\nThe power law can only give an estimate of the miss rate only up to a certain value of cache size. A large enough cache eliminates capacity misses and increasing the cache size further will not reduce the miss rate any further, contrary to the power law's prediction.\n\nThe validity of the power law of cache misses also depends on the size of working memory set in a given process and also on the temporal re-reference pattern of cache blocks in a process. If a process has a small working memory set relative to the cache size, capacity misses are unlikely and the power law does not hold.\n\nAlthough conflict misses reduce as associativity increases, Hartstein et al. showed that the power law holds irrespective of set associativity.\n\nHartstein et al. plotted the number of cache block re-accesses versus their re-reference times for a large number of workloads and found that most also follow an exponential relationship.\n\nwhere \"R\"(\"t\") is the rate of re-referencing. It was found that the exponent \"β\" ranged between 1.7 and 1.3. Theoretically, it was proved that the power laws of cache re-reference and cache miss rate are related by the equation formula_3. This means that for workloads that do not follow the re-reference power law, the power law of cache misses does not hold true.\n\nIn a multilevel cache hierarchy, the miss pattern of the higher level cache becomes the re-reference pattern of the immediate lower level cache. Hartstein et al. found that whereas the cache misses for lower levels do not follow a strict power law, as long as the lower level cache is considerably larger than the higher level cache, the miss rate function can be approximated to the power law.\n\n"}
{"id": "24334988", "url": "https://en.wikipedia.org/wiki?curid=24334988", "title": "Proofs of convergence of random variables", "text": "Proofs of convergence of random variables\n\nThis article is supplemental for “Convergence of random variables” and provides proofs for selected results.\n\nSeveral results will be established using the portmanteau lemma: A sequence {\"X\"} converges in distribution to \"X\" if and only if any of the following conditions are met:\nProof: If {\"X\"} converges to \"X\" almost surely, it means that the set of points {ω: lim \"X\"(ω) ≠ \"X\"(ω)} has measure zero; denote this set \"O\". Now fix ε > 0 and consider a sequence of sets\n\nThis sequence of sets is decreasing: \"A\" ⊇ \"A\" ⊇ ..., and it decreases towards the set\n\nFor this decreasing sequence of events, their probabilities are also a decreasing sequence, and it decreases towards the Pr(\"A\"); we shall show now that this number is equal to zero. Now any point ω in the complement of \"O\" is such that lim \"X\"(ω) = \"X\"(ω), which implies that |\"X\"(ω) − \"X\"(ω)| < ε for all \"n\" greater than a certain number \"N\". Therefore, for all \"n\" ≥ \"N\" the point ω will not belong to the set \"A\", and consequently it will not belong to \"A\". This means that \"A\" is disjoint with \"O\", or equivalently, \"A\" is a subset of \"O\" and therefore Pr(\"A\") = 0.\n\nFinally, consider\nwhich by definition means that \"X\" converges in probability to \"X\".\n\nIf \"X\" are independent random variables assuming value one with probability 1/\"n\" and zero otherwise, then \"X\" converges to zero in probability but not almost surely. This can be verified using the Borel–Cantelli lemmas.\n\nLemma. Let \"X\", \"Y\" be random variables, let \"a\" be a real number and ε > 0. Then\n\nProof of lemma:\n\nProof of the theorem: Recall that in order to prove convergence in distribution, one must show that the sequence of cumulative distribution functions converges to the \"F\" at every point where \"F\" is continuous. Let \"a\" be such a point. For every ε > 0, due to the preceding lemma, we have:\n\nSo, we have\n\nTaking the limit as \"n\" → ∞, we obtain:\nwhere \"F\"(\"a\") = Pr(\"X\" ≤ \"a\") is the cumulative distribution function of \"X\". This function is continuous at \"a\" by assumption, and therefore both \"F\"(\"a\"−ε) and \"F\"(\"a\"+ε) converge to \"F\"(\"a\") as ε → 0. Taking this limit, we obtain\nwhich means that {\"X\"} converges to \"X\" in distribution.\n\nThe implication follows for when \"X\" is a random vector by using this property proved later on this page and by taking \"Y = X\".\n\nProof: Fix ε > 0. Let \"B\"(\"c\") be the open ball of radius ε around point \"c\", and \"B\"(\"c\") its complement. Then\nBy the portmanteau lemma (part C), if \"X\" converges in distribution to \"c\", then the limsup of the latter probability must be less than or equal to Pr(\"c\" ∈ \"B\"(\"c\")), which is obviously equal to zero. Therefore,\n\nwhich by definition means that \"X\" converges to \"c\" in probability.\n\nProof: We will prove this theorem using the portmanteau lemma, part B. As required in that lemma, consider any bounded function \"f\" (i.e. |\"f\"(\"x\")| ≤ \"M\") which is also Lipschitz:\n\nTake some ε > 0 and majorize the expression |E[\"f\"(\"Y\")] − E[\"f\"(\"X\")]| as\n\n(here 1 denotes the indicator function; the expectation of the indicator function is equal to the probability of corresponding event). Therefore,\nIf we take the limit in this expression as \"n\" → ∞, the second term will go to zero since {\"Y−X\"} converges to zero in probability; and the third term will also converge to zero, by the portmanteau lemma and the fact that \"X\" converges to \"X\" in distribution. Thus\nSince ε was arbitrary, we conclude that the limit must in fact be equal to zero, and therefore E[\"f\"(\"Y\")] → E[\"f\"(\"X\")], which again by the portmanteau lemma implies that {\"Y\"} converges to \"X\" in distribution. QED.\n\nProof: We will prove this statement using the portmanteau lemma, part A.\n\nFirst we want to show that (\"X\", \"c\") converges in distribution to (\"X\", \"c\"). By the portmanteau lemma this will be true if we can show that E[\"f\"(\"X\", \"c\")] → E[\"f\"(\"X\", \"c\")] for any bounded continuous function \"f\"(\"x\", \"y\"). So let \"f\" be such arbitrary bounded continuous function. Now consider the function of a single variable \"g\"(\"x\") := \"f\"(\"x\", \"c\"). This will obviously be also bounded and continuous, and therefore by the portmanteau lemma for sequence {\"X\"} converging in distribution to \"X\", we will have that E[\"g\"(\"X\")] → E[\"g\"(\"X\")]. However the latter expression is equivalent to “E[\"f\"(\"X\", \"c\")] → E[\"f\"(\"X\", \"c\")]”, and therefore we now know that (\"X\", \"c\") converges in distribution to (\"X\", \"c\").\n\nSecondly, consider |(\"X\", \"Y\") − (\"X\", \"c\")| = |\"Y\" − \"c\"|. This expression converges in probability to zero because \"Y\" converges in probability to \"c\". Thus we have demonstrated two facts:\nBy the property proved earlier, these two facts imply that (\"X\", \"Y\") converge in distribution to (\"X\", \"c\").\n\nProof:\nEach of the probabilities on the right-hand side converge to zero as \"n\" → ∞ by definition of the convergence of {\"X\"} and {\"Y\"} in probability to \"X\" and \"Y\" respectively. Taking the limit we conclude that the left-hand side also converges to zero, and therefore the sequence {(\"X\", \"Y\")} converges in probability to {(\"X\", \"Y\")}.\n\n"}
{"id": "716969", "url": "https://en.wikipedia.org/wiki?curid=716969", "title": "Rational pricing", "text": "Rational pricing\n\nRational pricing is the assumption in financial economics that asset prices (and hence asset pricing models) will reflect the arbitrage-free price of the asset as any deviation from this price will be \"arbitraged away\". This assumption is useful in pricing fixed income securities, particularly bonds, and is fundamental to the pricing of derivative instruments.\n\nArbitrage is the practice of taking advantage of a state of imbalance between two (or possibly more) markets. Where this mismatch can be exploited (i.e. after transaction costs, storage costs, transport costs, dividends etc.) the arbitrageur can \"lock in\" a risk-free profit by purchasing and selling simultaneously in both markets.\n\nIn general, arbitrage ensures that \"the law of one price\" will hold; arbitrage also equalises the prices of assets with identical cash flows, and sets the price of assets with known future cash flows.\n\nThe same asset must trade at the same price on all markets (\"the law of one price\").\nWhere this is not true, the arbitrageur will:\n\nTwo assets with identical cash flows must trade at the same price.\nWhere this is not true, the arbitrageur will:\n\nAn asset with a known price in the future must today trade at that price discounted at the risk free rate.\n\nNote that this condition can be viewed as an application of the above, where the two assets in question are the asset to be delivered and the risk free asset.\n\n(a) where the discounted future price is \"higher\" than today's price:\n\n(b) where the discounted future price is \"lower\" than today's price:\n\nIt will be noted that (b) is only possible for those holding the asset but not needing it until the future date. There may be few such parties if short-term demand exceeds supply, leading to backwardation.\n\nRational pricing is one approach used in pricing fixed rate bonds. Here, each cash flow can be matched by trading in (a) some multiple of a zero-coupon bond corresponding to the coupon date, and of equivalent credit worthiness (if possible, from the same issuer as the bond being valued) with the corresponding maturity, or (b) in a corresponding strip and ZCB.\n\nGiven that the cash flows can be replicated, the price of the bond must today equal the sum of each of its cash flows discounted at the same rate as each ZCB, as above. Were this not the case, arbitrage would be possible and would bring the price back into line with the price based on ZCBs; see Bond valuation#Arbitrage-free pricing approach\n\nThe pricing formula is as below, where each cash flow formula_1 is discounted at the rate formula_2 that matches the coupon date:\n\nOften, the formula is expressed as formula_4, using prices instead of rates, as prices are more readily available.\n\n\"See also Fixed income arbitrage; Bond credit rating.\"\n\nA derivative is an instrument that allows for buying and selling of the same asset on two markets – the spot market and the derivatives market. Mathematical finance assumes that any imbalance between the two markets will be arbitraged away. Thus, in a correctly priced derivative contract, the derivative price, the strike price (or reference rate), and the spot price will be related such that arbitrage is not possible. See Fundamental theorem of arbitrage-free pricing.\n\nIn a futures contract, for no arbitrage to be possible, the price paid on delivery (the forward price) must be the same as the cost (including interest) of buying and storing the asset. In other words, the rational forward price represents the expected future value of the underlying discounted at the risk free rate (the \"asset with a known future-price\", as above); see Spot–future parity. Thus, for a simple, non-dividend paying asset, the value of the future/forward, formula_5, will be found by accumulating the present value formula_6 at time formula_7 to maturity formula_8 by the rate of risk-free return formula_9.\n\nThis relationship may be modified for storage costs, dividends, dividend yields, and convenience yields; see futures contract pricing.\n\nAny deviation from this equality allows for arbitrage as follows.\n\n\n\nAs above, where the value of an asset in the future is known (or expected), this value can be used to determine the asset's rational price today. In an option contract, however, exercise is dependent on the price of the underlying, and hence payment is uncertain. Option pricing models therefore include logic that either \"locks in\" or \"infers\" this future value; both approaches deliver identical results. Methods that lock-in future cash flows assume \"arbitrage free pricing\", and those that infer expected value assume \"risk neutral valuation\".\n\nTo do this, (in their simplest, though widely used form) both approaches assume a \"binomial model\" for the behavior of the underlying instrument, which allows for only two states – up or down. If S is the current price, then in the next period the price will either be \"S up\" or \"S down\". Here, the value of the share in the up-state is S × u, and in the down-state is S × d (where u and d are multipliers with d < 1 < u and assuming d < 1+r < u; see the binomial options model). Then, given these two states, the \"arbitrage free\" approach creates a position that has an identical value in either state – the cash flow in one period is therefore known, and arbitrage pricing is applicable. The risk neutral approach infers expected option value from the intrinsic values at the later two nodes.\n\nAlthough this logic appears far removed from the Black–Scholes formula and the lattice approach in the Binomial options model, it in fact underlies both models; see The Black–Scholes PDE. The assumption of binomial behaviour in the underlying price is defensible as the number of time steps between today (valuation) and exercise increases, and the period per time-step is correspondingly short. The Binomial options model allows for a high number of very short time-steps (if coded correctly), while Black–Scholes, in fact, models a continuous process.\n\nThe examples below have shares as the underlying, but may be generalised to other instruments. The value of a put option can be derived as below, or may be found from the value of the call using put-call parity.\n\nHere, the future payoff is \"locked in\" using either \"delta hedging\" or the \"replicating portfolio\" approach. As above, this payoff is then discounted, and the result is used in the valuation of the option today.\n\nIt is possible to create a position consisting of Δ shares and 1 call sold, such that the position's value will be identical in the \"S up\" and \"S down\" states, and hence known with certainty (see Delta hedging). This certain value corresponds to the forward price above (\"An asset with a known future price\"), and as above, for no arbitrage to be possible, the present value of the position must be its expected future value discounted at the risk free rate, r. The value of a call is then found by equating the two.\n\n\nIt is possible to create a position consisting of Δ shares and $B borrowed at the risk free rate, which will produce identical cash flows to one option on the underlying share. The position created is known as a \"replicating portfolio\" since its cash flows replicate those of the option. As shown above (\"Assets with identical cash flows\"), in the absence of arbitrage opportunities, since the cash flows produced are identical, the price of the option today must be the same as the value of the position today.\n\n\nNote that there is no discounting here  – the interest rate appears only as part of the construction. This approach is therefore used in preference to others where it is not clear whether the risk free rate may be applied as the discount rate at each decision point, or whether, instead, a premium over risk free, differing by state, would be required. The best example of this would be under Real options analysis where managements' actions actually change the risk characteristics of the project in question, and hence the Required rate of return could differ in the up- and down-states. Here, in the above formulae, we then have: \"Δ × \"S up\" - B × (1 + r up)...\" and \"Δ × \"S down\" - B × (1 + r down)...\" . See Real options valuation#Technical considerations. (Another case where the modelling assumptions may depart from rational pricing is the valuation of employee stock options.)\n\nHere the value of the option is calculated using the risk neutrality assumption. Under this assumption, the \"expected value\" (as opposed to \"locked in\" value) is discounted. The expected value is calculated using the intrinsic values from the later two nodes: \"Option up\" and \"Option down\", with u and d as price multipliers as above. These are then weighted by their respective probabilities: \"probability\" p of an up move in the underlying, and \"probability\" (1-p) of a down move. The expected value is then discounted at r, the risk-free rate.\n\n\nNote that above, the risk neutral formula does not refer to the expected or forecast return of the underlying, nor its volatility  – p as solved, relates to the risk-neutral measure as opposed to the actual probability distribution of prices. Nevertheless, both arbitrage free pricing and risk neutral valuation deliver identical results. In fact, it can be shown that \"delta hedging\" and \"risk-neutral valuation\" use identical formulae expressed differently. Given this equivalence, it is valid to assume \"risk neutrality\" when pricing derivatives. See fundamental theorem of arbitrage-free pricing.\n\nRational pricing underpins the logic of swap valuation. Here, two counterparties \"swap\" obligations, effectively exchanging cash flow streams calculated against a notional principal amount, and the value of the swap is the present value (PV) of both sets of future cash flows \"netted off\" against each other.\n\nNote that since the 2007–2012 global financial crisis, pricing is under a \"multi-curve\" framework, whereas previously it was off a single, \"self discounting\", curve; see Financial economics#Derivative pricing for context. Of course, under both approaches, pricing must be arbitrage free, and the logic below therefore holds under either, although see Interest rate swap#Valuation and pricing for formulae.\n\nTo be arbitrage free, the terms of a swap contract are such that, initially, the \"Net\" present value of these future cash flows is equal to zero; see swap valuation. For example, consider the valuation of a fixed-to-floating Interest rate swap where Party A pays a fixed rate, and Party B pays a floating rate. Here, the \"fixed rate\" would be such that the present value of future fixed rate payments by Party A is equal to the present value of the \"expected\" future floating rate payments (i.e. the NPV is zero). Were this not the case, an arbitrageur, C, could:\n\nOnce traded, swaps can also be priced using rational pricing. For example, the Floating leg of an interest rate swap can be \"decomposed\" into a series of forward rate agreements. Here, since the swap has identical payments to the FRA, arbitrage free pricing must apply as above – i.e. the value of this leg is equal to the value of the corresponding FRAs. Similarly, the \"receive-fixed\" leg of a swap can be valued by comparison to a bond with the same schedule of payments. (Relatedly, given that their underlyings have the same cash flows, bond options and swaptions are equatable.) See Swap (finance)#Using bond prices.\n\nThe arbitrage pricing theory (APT), a general theory of asset pricing, has become influential in the pricing of shares. APT holds that the expected return of a financial asset can be modelled as a linear function of various macro-economic factors, where sensitivity to changes in each factor is represented by a factor specific beta coefficient:\n\nThe model derived rate of return will then be used to price the asset correctly – the asset price should equal the expected end of period price discounted at the rate implied by model. If the price diverges, arbitrage should bring it back into line. Here, to perform the arbitrage, the investor \"creates\" a correctly priced asset (a \"synthetic\" asset), a \"portfolio\" with the same net-exposure to each of the macroeconomic factors as the mispriced asset but a different expected return. See the arbitrage pricing theory article for detail on the construction of the portfolio. The arbitrageur is then in a position to make a risk free profit as follows:\n\n\n\nNote that under \"true arbitrage\", the investor locks-in a \"guaranteed\" payoff, whereas under APT arbitrage, the investor locks-in a positive \"expected\" payoff. The APT thus assumes \"arbitrage in expectations\" — i.e. that arbitrage by investors will bring asset prices back into line with the returns expected by the model.\n\nThe capital asset pricing model (CAPM) is an earlier, (more) influential theory on asset pricing. Although based on different assumptions, the CAPM can, in some ways, be considered a \"special case\" of the APT; specifically, the CAPM's security market line represents a single-factor model of the asset price, where beta is exposure to changes in the \"value of the market\" as a whole.\n\nClassical valuation methods like the Black-Scholes model or the Merton model cannot account for systemic counterparty risk which is present in systems with financial interconnectedness.\nMore details regarding risk-neutral, arbitrage-free asset and derivative valuation can be found in Wikipedia's systemic risk\narticle (see also valuation under systemic risk).\n\n\nArbitrage free pricing\n\nRisk neutrality and arbitrage free pricing\n\nApplication to derivatives\n"}
{"id": "27859", "url": "https://en.wikipedia.org/wiki?curid=27859", "title": "Sphere", "text": "Sphere\n\nA sphere (from Greek σφαῖρα — \"sphaira\", \"globe, ball\") is a perfectly round geometrical object in three-dimensional space that is the surface of a completely round ball (viz., analogous to the circular objects in two dimensions, where a \"circle\" circumscribes its \"disk\").\n\nLike a circle in a two-dimensional space, a sphere is defined mathematically as the set of points that are all at the same distance from a given point, but in a three-dimensional space. This distance is the radius of the ball, which is made up from all points with a distance less than from the given point, which is the center of the mathematical ball. These are also referred to as the radius and center of the sphere, respectively. The longest straight line through the ball, connecting two points of the sphere, passes through the center and its length is thus twice the radius; it is a diameter of both the sphere and its ball.\n\nWhile outside mathematics the terms \"sphere\" and \"ball\" are sometimes used interchangeably, in mathematics the above distinction is made between a \"sphere\", which is a two-dimensional closed surface, embedded in a three-dimensional Euclidean space, and a \"ball\", which is a three-dimensional shape that includes the sphere and everything \"inside\" the sphere (a \"closed ball\"), or, more often, just the points \"inside\", but \"not on\" the sphere (an \"open ball\"). This distinction has not always been maintained and especially older mathematical references talk about a sphere as a solid. This is analogous to the situation in the plane, where the terms \"circle\" and \"disk\" can also be confounded.\n\nIn analytic geometry, a sphere with center and radius is the locus of all points such that\n\nLet be real numbers with and put\nThen the equation\nhas no real points as solutions if formula_4 and is called the equation of an imaginary sphere. If formula_5 the only solution of formula_6 is the point formula_7 and the equation is said to be the equation of a point sphere. Finally, in the case formula_8, formula_6 is an equation of a sphere whose center is formula_10 and whose radius is formula_11.\n\nIf in the above equation is zero then is the equation of a plane. Thus, a plane may be thought of as a sphere of infinite radius whose center is a point at infinity.\n\nThe points on the sphere with radius formula_12 and center formula_13 can be parameterized via\n\nA sphere of any radius centered at zero is an integral surface of the following differential form:\n\nThis equation reflects that position and velocity vectors of a point, and , traveling on the sphere are always orthogonal to each other.\n\nA sphere can also be constructed as the surface formed by rotating a circle about any of its diameters. Since a circle is a special type of ellipse, a sphere is a special type of ellipsoid of revolution. Replacing the circle with an ellipse rotated about its major axis, the shape becomes a prolate spheroid; rotated about the minor axis, an oblate spheroid.\n\nIn three dimensions, the volume inside a sphere (that is, the volume of a ball, but classically referred to as the volume of a sphere) is \nwhere is the radius of the sphere. Archimedes first derived this formula, by showing that the volume inside a sphere is twice the volume between the sphere and the circumscribed cylinder of that sphere (having the height and diameter equal to the diameter of the sphere). This assertion can be obtained from Cavalieri's principle. This formula can also be derived using integral calculus, i.e. disk integration to sum the volumes of an infinite number of circular disks of infinitesimally small thickness stacked side by side and centered along the -axis from to , assuming the sphere of radius is centered at the origin. \n\nAt any given , the incremental volume () equals the product of the cross-sectional area of the disk at and its thickness ():\n\nThe total volume is the summation of all incremental volumes:\n\nIn the limit as approaches zero this equation becomes:\n\nAt any given , a right-angled triangle connects , and to the origin; hence, applying the Pythagorean theorem yields:\n\nUsing this substitution gives\n\nthat can be evaluated to give the result\n\nAlternatively, this formula is found using spherical coordinates, with volume element\nso\nFor most practical purposes, the volume inside a sphere inscribed in a cube can be approximated as 52.4% of the volume of the cube, since , where is the diameter of the sphere and also the length of a side of the cube and  ≈ 0.5236. For example, a sphere with diameter 1 meter has 52.4% the volume of a cube with edge length 1 meter, or about 0.524 m.\n\nThe surface area of a sphere of radius is:\n\nArchimedes first derived this formula from the fact that the projection to the lateral surface of a circumscribed cylinder is area-preserving. Another approach to obtaining the formula comes from the fact that it equals the derivative of the formula for the volume with respect to because the total volume inside a sphere of radius can be thought of as the summation of the surface area of an infinite number of spherical shells of infinitesimal thickness concentrically stacked inside one another from radius 0 to radius . At infinitesimal thickness the discrepancy between the inner and outer surface area of any given shell is infinitesimal, and the elemental volume at radius is simply the product of the surface area at radius and the infinitesimal thickness.\n\nAt any given radius , the incremental volume () equals the product of the surface area at radius () and the thickness of a shell ():\n\nThe total volume is the summation of all shell volumes:\n\nIn the limit as approaches zero this equation becomes:\n\nSubstitute :\n\nDifferentiating both sides of this equation with respect to yields as a function of :\n\nThis is generally abbreviated as:\nwhere is now considered to be the fixed radius of the sphere.\n\nAlternatively, the area element on the sphere is given in spherical coordinates by . In Cartesian coordinates, the area element is\nFor more generality, see area element.\n\nThe total area can thus be obtained by integration:\n\nThe sphere has the smallest surface area of all surfaces that enclose a given volume, and it encloses the largest volume among all closed surfaces with a given surface area. The sphere therefore appears in nature: for example, bubbles and small water drops are roughly spherical because the surface tension locally minimizes surface area.\n\nThe surface area relative to the mass of a ball is called the specific surface area and can be expressed from the above stated equations as\nwhere is the density (the ratio of mass to volume).\n\nA sphere is uniquely determined by four points that are not coplanar. More generally, a sphere is uniquely determined by four conditions such as passing through a point, being tangent to a plane, etc. This property is analogous to the property that three non-collinear points determine a unique circle in a plane.\n\nConsequently, a sphere is uniquely determined by (that is, passes through) a circle and a point not in the plane of that circle.\n\nBy examining the common solutions of the equations of two spheres, it can be seen that two spheres intersect in a circle and the plane containing that circle is called the radical plane of the intersecting spheres. Although the radical plane is a real plane, the circle may be imaginary (the spheres have no real point in common) or consist of a single point (the spheres are tangent at that point).\n\nThe angle between two spheres at a real point of intersection is the dihedral angle determined by the tangent planes to the spheres at that point. Two spheres intersect at the same angle at all points of their circle of intersection. They intersect at right angles (are orthogonal) if and only if the squares of the distance between their centers is equal to the sum of the squares of their radii.\n\nIf and are the equations of two distinct spheres then \nis also the equation of a sphere for arbitrary values of the parameters and . The set of all spheres satisfying this equation is called a pencil of spheres determined by the original two spheres. In this definition a sphere is allowed to be a plane (infinite radius, center at infinity) and if both the original spheres are planes then all the spheres of the pencil are planes, otherwise there is only one plane (the radical plane) in the pencil.\n\nIf the pencil of spheres does not consist of all planes, then there are three types of pencils:\n\nAll the tangent lines from a fixed point of the radical plane to the spheres of a pencil have the same length.\n\nThe radical plane is the locus of the centers of all the spheres that are orthogonal to all the spheres in a pencil. Moreover, a sphere orthogonal to any two spheres of a pencil of spheres is orthogonal to all of them and its center lies in the radical plane of the pencil.\n\nPairs of points on a sphere that lie on a straight line through the sphere's center are called antipodal points. A great circle is a circle on the sphere that has the same center and radius as the sphere and, consequently, divides it into two equal parts. The plane sections of a sphere are called \"spheric sections\". They are all circles and those that are not great circles are called \"small circles\". \n\nThe shortest distance along the surface between two distinct non-antipodal points on the sphere is the length of the smaller of the two arcs on the unique great circle that includes the two points. Equipped with this \"great-circle distance\", a great circle becomes the Riemannian circle.\n\nIf a particular point on a sphere is (arbitrarily) designated as its \"north pole\", then the corresponding antipodal point is called the \"south pole\", and the equator is the great circle that is equidistant to them. Great circles through the two poles are called lines (or meridians) of longitude, and the line connecting the two poles is called the axis of rotation. Circles on the sphere that are parallel to the equator are lines of latitude. This terminology is also used for such approximately spheroidal astronomical bodies as the planet Earth (see geoid).\n\nAny plane that includes the center of a sphere divides it into two equal hemispheres. Any two intersecting planes that include the center of a sphere subdivide the sphere into four lunes or biangles, the vertices of which all coincide with the antipodal points lying on the line of intersection of the planes.\n\nThe antipodal quotient of the sphere is the surface called the real projective plane, which can also be thought of as the northern hemisphere with antipodal points of the equator identified.\n\nThe hemisphere is conjectured to be the optimal (least area) isometric filling of the Riemannian circle.\n\nSpheres can be generalized to spaces of any number of dimensions. For any natural number , an \"-sphere,\" often written as , is the set of points in ()-dimensional Euclidean space that are at a fixed distance from a central point of that space, where is, as before, a positive real number. In particular:\n\nSpheres for are sometimes called hyperspheres.\n\nThe -sphere of unit radius centered at the origin is denoted and is often referred to as \"the\" -sphere. Note that the ordinary sphere is a 2-sphere, because it is a 2-dimensional surface (which is embedded in 3-dimensional space).\n\nThe surface area of the unit ()-sphere is\n\nwhere is Euler's gamma function.\n\nAnother expression for the surface area is\n\nand the volume is the surface area times or\n\nGeneral recursive formulas also exist for the volume of an -ball.\n\nMore generally, in a metric space , the sphere of center and radius is the set of points such that .\n\nIf the center is a distinguished point that is considered to be the origin of , as in a normed space, it is not mentioned in the definition and notation. The same applies for the radius if it is taken to equal one, as in the case of a unit sphere.\n\nUnlike a ball, even a large sphere may be an empty set. For example, in with Euclidean metric, a sphere of radius is nonempty only if can be written as sum of squares of integers.\n\nIn topology, an -sphere is defined as a space homeomorphic to the boundary of an -ball; thus, it is homeomorphic to the Euclidean -sphere, but perhaps lacking its metric.\n\nThe -sphere is denoted . It is an example of a compact topological manifold without boundary. A sphere need not be smooth; if it is smooth, it need not be diffeomorphic to the Euclidean sphere.\n\nThe Heine–Borel theorem implies that a Euclidean -sphere is compact. The sphere is the inverse image of a one-point set under the continuous function . Therefore, the sphere is closed. is also bounded; therefore it is compact.\n\nRemarkably, it is possible to turn an ordinary sphere inside out in a three-dimensional space with possible self-intersections but without creating any crease, in a process called sphere eversion.\n\nThe basic elements of Euclidean plane geometry are points and lines. On the sphere, points are defined in the usual sense. The analogue of the \"line\" is the geodesic, which is a great circle; the defining characteristic of a great circle is that the plane containing all its points also passes through the center of the sphere. Measuring by arc length shows that the shortest path between two points lying on the sphere is the shorter segment of the great circle that includes the points.\n\nMany theorems from classical geometry hold true for spherical geometry as well, but not all do because the sphere fails to satisfy some of classical geometry's postulates, including the parallel postulate. In spherical trigonometry, angles are defined between great circles. Spherical trigonometry differs from ordinary trigonometry in many respects. For example, the sum of the interior angles of a spherical triangle always exceeds 180 degrees. Also, any two similar spherical triangles are congruent.\n\nIn their book \"Geometry and the Imagination\" David Hilbert and Stephan Cohn-Vossen describe eleven properties of the sphere and discuss whether these properties uniquely determine the sphere. Several properties hold for the plane, which can be thought of as a sphere with infinite radius. These properties are:\n\n\n"}
{"id": "3784565", "url": "https://en.wikipedia.org/wiki?curid=3784565", "title": "Ternary relation", "text": "Ternary relation\n\nIn mathematics, a ternary relation or triadic relation is a finitary relation in which the number of places in the relation is three. Ternary relations may also be referred to as 3-adic, 3-ary, 3-dimensional, or 3-place.\n\nJust as a binary relation is formally defined as a set of \"pairs\", i.e. a subset of the Cartesian product of some sets \"A\" and \"B\", so a ternary relation is a set of triples, forming a subset of the Cartesian product of three sets \"A\", \"B\" and \"C\".\n\nAn example of a ternary relation in elementary geometry can be given on triples of points, where a triple is in the relation if the three points are collinear. Another geometric example can be obtained by considering triples consisting of two points and a line, where a triple is in the ternary relation if the two points determine (are incident with) the line.\n\nA function in two variables, mapping two values from sets \"A\" and \"B\", respectively, to a value in \"C\" associates to every pair (\"a\",\"b\") in an element \"ƒ\"(\"a\", \"b\") in \"C\". Therefore, its graph consists of pairs of the form . Such pairs in which the first element is itself a pair are often identified with triples. This makes the graph of \"ƒ\" a ternary relation between \"A\", \"B\" and \"C\", consisting of all triples , satisfying , , and \n\nGiven any set \"A\" whose elements are arranged on a circle, one can define a ternary relation \"R\" on \"A\", i.e. a subset of \"A\" = , by stipulating that holds if and only if the elements \"a\", \"b\" and \"c\" are pairwise different and when going from \"a\" to \"c\" in a clockwise direction one passes through \"b\". For example, if \"A\" = { } represents the hours on a clock face, then holds and does not hold.\n\nThe ordinary congruence of arithmetics\nwhich holds for three integers \"a\", \"b\", and \"m\" if and only if \"m\" divides \"a\" − \"b\", formally may be considered as a ternary relation. However, usually, this instead is considered as a family of binary relations between the \"a\" and the \"b\", indexed by the modulus \"m\". For each fixed \"m\", indeed this binary relation has some natural properties, like being an equivalence relation; while the combined ternary relation in general is not studied as one relation.\n\nA \"typing relation\" formula_2 indicates that formula_3 is a term of type formula_4 in context formula_5, and is thus a ternary relation between contexts, terms and types.\n\nA ternary relation formula_6 can be defined in the category of binary relations using composition of relations \"AB\" and inclusion \"AB\" ⊆ \"C\". Within the calculus of relations each relation \"A\" has a converse relation \"A\" and a complement relation formula_7 Using these involutions, Augustus De Morgan and Ernst Schröder showed that formula_6is equivalent to formula_9 and also equivalent to formula_10 The mutual equivalences of these forms, constructed from the ternary are called the Schröder rules.\n\n"}
{"id": "44249205", "url": "https://en.wikipedia.org/wiki?curid=44249205", "title": "Thomas W. Tucker", "text": "Thomas W. Tucker\n\nThomas William Tucker (born July 15, 1945) is an American mathematician, the Charles Hetherington Professor of Mathematics at Colgate University, and an expert in the area of topological graph theory.\n\nTucker did his undergraduate studies at Harvard University, graduating in 1967, and obtained his Ph.D. from Dartmouth College in 1971, under the supervision of Edward Martin Brown. \n\nTucker's father, Albert W. Tucker, was also a professional mathematician, and his brother, Alan Tucker, and son, Thomas J. Tucker, are also professional mathematicians.\n"}
{"id": "49188683", "url": "https://en.wikipedia.org/wiki?curid=49188683", "title": "Trillium theorem", "text": "Trillium theorem\n\nIn Euclidean geometry, the trillium theorem – (from , literally 'lemma about trident', , literally 'theorem of trillium' or 'theorem of trefoil') is a statement about properties of inscribed and circumscribed circles and their relations.\n\nLet be an arbitrary triangle. Let be its incenter and let be the point where line (the angle bisector of ) crosses the circumcircle of . Then, the theorem states that is equidistant from , , and .\nEquivalently:\nA fourth point, the excenter of relative to , also lies at the same distance from , diametrally opposite from .\n\nBy the inscribed angle theorem,\n\nSince formula_2 is angle bisector,\n\nThis theorem can be used to reconstruct a triangle starting from the locations only of one vertex, the incenter, and the circumcenter of the triangle.\nFor, let be the given vertex, be the incenter, and be the circumcenter. This information allows the successive construction of:\nHowever, for some triples of points , , and , this construction may fail, either because line is tangent to the circumcircle or because the two circles do not have two crossing points. It may also produce a triangle for which the given point is an excenter rather than the incenter. In these cases, there can be no triangle having as vertex, as incenter, and as circumcenter.\n\nOther triangle reconstruction problems, such as the reconstruction of a triangle from a vertex, incenter, and center of its nine-point circle, can be solved by reducing the problem to the case of a vertex, incenter, and circumcenter.\n\nLet and be any two of the four points given by the incenter and the three excenters of a triangle . Then and are collinear with one of the three triangle vertices. The circle with as diameter passes through the other two vertices and is centered on the circumcircle of . When one of or is the incenter, this is the trillium theorem, with line as the (internal) angle bisector of one of the triangle's angles. However, it is also true when and are both excenters; in this case, line is the external angle bisector of one of the triangle's angles.\n\n"}
{"id": "40256418", "url": "https://en.wikipedia.org/wiki?curid=40256418", "title": "Versor (physics)", "text": "Versor (physics)\n\nIn geometry and physics, the versor of an axis or of a vector is a unit vector indicating its direction. \n\nThe versor of a Cartesian axis is also known as a standard basis vector. The versor of a vector is also known as a normalized vector.\n\nThe versors of the axes of a Cartesian coordinate system are the unit vectors codirectional with the axes of that system. \nEvery Euclidean vector a in a \"n\"-dimensional Euclidean space () can be represented as a linear combination of the \"n\" versors of the corresponding Cartesian coordinate system. For instance, in a three-dimensional space (), there are three versors:\n\nThey indicate the direction of the Cartesian axes \"x\", \"y\", and \"z\", respectively. In terms of these, any vector a can be represented as \n\nwhere a, a, a are called the vector components (or vector projections) of a on the Cartesian axes \"x\", \"y\", and \"z\" (see figure), while \"a\", \"a\", \"a\" are the respective scalar components (or scalar projections).\n\nIn linear algebra, the set formed by these \"n\" versors is typically referred to as the standard basis of the corresponding Euclidean space, and each of them is commonly called a standard basis vector.\n\nA hat above the symbol of a versor is sometimes used to emphasize its status as a unit vector (e.g., formula_5). \n\nIn most contexts it can be assumed that i, j, and k, (or formula_6 formula_7 and formula_8) are versors of a 3-D Cartesian coordinate system. The notations formula_9, formula_10, formula_11, or formula_12, with or without hat, are also used, particularly in contexts where i, j, k might lead to confusion with another quantity. This is recommended, for instance, when index symbols such as \"i\", \"j\", \"k\" are used to identify an element of a set of variables.\n\nThe versor (or normalized vector) formula_13 of a non-zero vector formula_14 is the unit vector codirectional with formula_14:\n\nwhere formula_17 is the norm (or length) of formula_14. Notice that a versor lost the units of the original vector. For instance, if we have the vector formula_19, then formula_20 and\n\nformula_21\n\nYou can notice that formula_22 is a dimensionless quantity.\n"}
{"id": "1053747", "url": "https://en.wikipedia.org/wiki?curid=1053747", "title": "Vladimir Drinfeld", "text": "Vladimir Drinfeld\n\nVladimir Gershonovich Drinfeld (; ; born February 14, 1954), surname also romanized as Drinfel'd, is a Ukrainian-American mathematician, currently working at the University of Chicago.\n\nDrinfeld's work connected algebraic geometry over finite fields with number theory, especially the theory of automorphic forms, through the notions of elliptic module and the theory of the geometric Langlands correspondence. Drinfeld introduced the notion of a quantum group (independently discovered by Michio Jimbo at the same time) and made important contributions to mathematical physics, including the ADHM construction of instantons, algebraic formalism of the quantum inverse scattering method, and the Drinfeld–Sokolov reduction in the theory of solitons.\n\nHe was awarded the Fields Medal in 1990.\nIn 2016, he was elected to the National Academy of Sciences. In 2018 he received the Wolf Prize in Mathematics.\n\nDrinfeld was born in Kharkiv, Ukrainian SSR, Soviet Union in 1954. In 1969, at the age of 15, Drinfeld represented the Soviet Union at the International Mathematics Olympiad in Bucharest, Romania, and won a gold medal with the full score of 40 points. He was, at the time, the youngest participant to achieve a perfect score, and has since only been surpassed by Sergei Konyagin (1972) and Noam Elkies (1981). Drinfeld entered Moscow State University in the same year and graduated from it in 1974. Drinfeld was awarded the Candidate of Sciences degree in 1978 and the Doctor of Sciences degree from the Steklov Institute of Mathematics in 1988. He was awarded the Fields Medal in 1990. From 1981 till 1999 he worked at the Verkin Institute for Low Temperature Physics and Engineering (Department of Mathematical Physics). Drinfeld moved to the United States in 1999 and has been working at the University of Chicago since January 1999.\n\nIn 1974, at the age of twenty, Drinfeld announced a proof of the Langlands conjectures for GL over a global field of positive characteristic. In the course of proving the conjectures, Drinfeld introduced a new class of objects that he called \"elliptic modules\" (now known as Drinfeld modules). Later, in 1983, Drinfeld published a short article that expanded the scope of the Langlands conjectures. The Langlands conjectures, when published in 1967, could be seen as a sort of non-abelian class field theory. It postulated the existence of a natural one-to-one correspondence between Galois representations and some automorphic forms. The \"naturalness\" is guaranteed by the essential coincidence of L-functions. However, this condition is purely arithmetic and cannot be considered for a general one-dimensional function field in a straightforward way. Drinfeld pointed out that instead of automorphic forms one can consider automorphic perverse sheaves or automorphic D-modules. \"Automorphicity\" of these modules and the Langlands correspondence could be then understood in terms of the action of Hecke operators.\n\nDrinfeld has also done much work in mathematical physics. In collaboration with his advisor Yuri Manin, he constructed the moduli space of Yang–Mills instantons, a result that was proved independently by Michael Atiyah and Nigel Hitchin. Drinfeld coined the term \"quantum group\" in reference to Hopf algebras that are deformations of simple Lie algebras, and connected them to the study of the Yang–Baxter equation, which is a necessary condition for the solvability of statistical mechanical models. He also generalized Hopf algebras to quasi-Hopf algebras and introduced the study of Drinfeld twists, which can be used to factorize the R-matrix corresponding to the solution of the Yang–Baxter equation associated with a quasitriangular Hopf algebra.\n\nDrinfeld has also collaborated with Alexander Beilinson to rebuild the theory of vertex algebras in a coordinate-free form, which have become increasingly important to two-dimensional conformal field theory, string theory, and the geometric Langlands program. Drinfeld and Beilinson published their work in 2004 in a book titled \"Chiral Algebras.\"\n\n\n\n"}
{"id": "2056815", "url": "https://en.wikipedia.org/wiki?curid=2056815", "title": "Void type", "text": "Void type\n\nThe Void type, in several programming languages derived from C and Algol68, is the type for the result of a function that returns normally, but does not provide a result value to its caller. Usually such functions are called for their side effects, such as performing some task or writing to their output parameters. The usage of the void type in such context is comparable to procedures in Pascal and syntactic constructs which define subroutines in Visual Basic. It is also similar to the unit type used in functional programming languages and type theory. See Unit type#In programming languages for a comparison.\n\nC and C++ also support the pointer to void type (specified as codice_1), but this is an unrelated notion. Variables of this type are pointers to data of an \"unspecified\" type, so in this context (but not the others) codice_1 acts roughly like a universal or top type. A program can probably convert a pointer to any type of data (except a function pointer) to a pointer to void and back to the original type without losing information, which makes these pointers useful for polymorphic functions. The C language standard does not guarantee that the different pointer types have the same size.\n\nA function with void result type ends either by reaching the end of the function or by executing a return statement with no returned value. The void type may also appear as the sole argument of a function prototype to indicate that the function takes no arguments. Note that despite the name, in all of these situations, the void type serves as a unit type, not as a zero or bottom type (which \nis sometimes confusingly called the \"void type\"), even though unlike a real unit type which is a singleton, the void type lacks a way to represent its value and the language does not provide any way to declare an object or represent a value with type codice_3.\n\nIn the earliest versions of C, functions with no specific result defaulted to a return type of codice_4 and functions with no arguments simply had empty argument lists. Pointers to untyped data were declared as integers or pointers to codice_5. Some early C compilers had the feature, now seen as an annoyance, of generating a warning on any function call that did not use the function's returned value. Old code sometimes casts such function calls to void to suppress this warning. By the time Bjarne Stroustrup began his work on C++ in 1979–1980, void and void pointers were part of the C language dialect supported by AT&T-derived compilers.\n\nThe explicit use of void vs. giving no arguments in a function prototype has different semantics in C and C++, as detailed in the following table:\n\nA C prototype taking no arguments, e.g. codice_6 above, has been deprecated in C99, however.\n\nQuite contrary to C++, in the functional programming language Haskell the void type denotes the empty type, which has no inhabitants . A function into the void type does not return results, and a side-effectful program with type signature codice_7 does not terminate, or crashes. In particular, there are no total functions into the void type.\n"}
