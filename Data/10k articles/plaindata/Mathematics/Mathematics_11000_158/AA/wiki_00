{"id": "37631784", "url": "https://en.wikipedia.org/wiki?curid=37631784", "title": "228 (number)", "text": "228 (number)\n\n228 (two hundred [and] twenty-eight) is the natural number following 227 and preceding 229.\n228 is a refactorable number,\nand a practical number.\nThere are 228 matchings in a ladder graph with five rungs.\n228 is the smallest even number \"n\" such that the numerator of the \"n\"th Bernoulli number is divisible by a nontrivial square number that is relatively prime to \"n\".\n\nThe binary form of 228 contains all the two digit binary numbers in sequence from highest to lowest (11 10 01 00).\n"}
{"id": "45012210", "url": "https://en.wikipedia.org/wiki?curid=45012210", "title": "Antiprism graph", "text": "Antiprism graph\n\nIn the mathematical field of graph theory, an antiprism graph is a graph that has one of the antiprisms as its skeleton. An \"n\"-sided antiprism has 2\"n\" vertices and 4\"n\" edges. They are regular, polyhedral (and therefore by necessity also 3-vertex-connected, vertex-transitive, and planar graphs), and also Hamiltonian graphs.\n\nThe first graph in the sequence, the octahedral graph, has 6 vertices and 12 edges. Later graphs in the sequence may be named after the type of antiprism they correspond to:\n\n\nAlthough geometrically the star polygons also form the faces of a different sequence of (self-intersecting) antiprisms, the star antiprisms, they do not form a different sequence of graphs.\n\nAn antiprism graph is a special case of a circulant graph, Ci(2,1). \n\nOther infinite sequences of polyhedral graph formed in a similar way from polyhedra with regular-polygon bases include the prism graphs (graphs of prisms) and wheel graphs (graphs of pyramids). Other vertex-transitive polyhedral graphs include the Archimedean graphs.\n"}
{"id": "24732291", "url": "https://en.wikipedia.org/wiki?curid=24732291", "title": "Balinski's theorem", "text": "Balinski's theorem\n\nIn polyhedral combinatorics, a branch of mathematics, Balinski's theorem is a statement about the graph-theoretic structure of three-dimensional polyhedra and higher-dimensional polytopes. It states that, if one forms an undirected graph from the vertices and edges of a convex \"d\"-dimensional polyhedron or polytope (its skeleton), then the resulting graph is at least \"d\"-vertex-connected: the removal of any \"d\" − 1 vertices leaves a connected subgraph. For instance, for a three-dimensional polyhedron, even if two of its vertices (together with their incident edges) are removed, for any pair of vertices there will still exist a path of vertices and edges connecting the pair.\n\nBalinski's theorem is named after mathematician Michel Balinski, who published its proof in 1961, although the three-dimensional case dates back to the earlier part of the 20th century and the discovery of Steinitz's theorem that the graphs of three-dimensional polyhedra are exactly the three-connected planar graphs.\n\nBalinski proves the result based on the correctness of the simplex method for finding the minimum or maximum of a linear function on a convex polytope (the linear programming problem). The simplex method starts at an arbitrary vertex of the polytope and repeatedly moves towards an adjacent vertex that improves the function value; when no improvement can be made, the optimal function value has been reached.\n\nIf \"S\" is a set of fewer than \"d\" vertices to be removed from the graph of the polytope, Balinski adds one more vertex \"v\" to \"S\" and finds a linear function ƒ that has the value zero on the augmented set but is not identically zero on the whole space. Then, any remaining vertex at which ƒ is non-negative (including \"v\") can be connected by simplex steps to the vertex with the maximum value of ƒ, while any remaining vertex at which ƒ is non-positive (again including \"v\") can be similarly connected to the vertex with the minimum value of ƒ. Therefore, the entire remaining graph is connected.\n"}
{"id": "4533924", "url": "https://en.wikipedia.org/wiki?curid=4533924", "title": "Binary multiplier", "text": "Binary multiplier\n\nA binary multiplier is an electronic circuit used in digital electronics, such as a computer, to multiply two binary numbers. It is built using binary adders.\n\nA variety of techniques can be used to implement a digital multiplier. Most techniques involve computing a set of \"partial products\", and then summing the partial products together. This process is similar to the method taught to primary schoolchildren for conducting long multiplication on base-10 integers, but has been modified here for application to a base-2 (binary) numeral system.\n\nBetween 1947-1949 Arthur Alec Robinson worked for English Electric Ltd, as a student apprentice, and then as a development engineer. Crucially during this period he studied for a PhD degree at the University of Manchester, where he worked on the design of the hardware multiplier for the early Mark 1 computer.\n\nHowever, until the late 1970s, most minicomputers did not have a multiply instruction, and so programmers used a \"multiply routine\"\nwhich repeatedly shifts and accumulates partial results,\noften written using loop unwinding. Mainframe computers had multiply instructions, but they did the same sorts of shifts and adds as a \"multiply routine\".\n\nEarly microprocessors also had no multiply instruction. Though the multiply instruction is usually associated with the 16-bit microprocessor generation, \nat least two \"enhanced\" 8-bit micro have a multiply instruction: the Motorola 6809, introduced in 1978, and Intel MCS-51 family, developed in 1980, and later the modern Atmel AVR 8-bit microprocessors present in the ATMega, ATTiny and ATXMega microcontrollers.\n\nAs more transistors per chip became available due to larger-scale integration, it became possible to put enough adders on a single chip to sum all the partial products at once, rather than reuse a single adder to handle each partial product one at a time.\n\nBecause some common digital signal processing algorithms spend most of their time multiplying, digital signal processor designers sacrifice a lot of chip area in order to make the multiply as fast as possible; a single-cycle multiply–accumulate unit often used up most of the chip area of early DSPs.\n\nThe method taught in school for multiplying decimal numbers is based on calculating partial products, shifting them to the left and then adding them together. The most difficult part is to obtain the partial products, as that involves multiplying a long number by one digit (from 0 to 9):\n\nA binary computer does exactly the same, but with binary numbers. In binary encoding each long number is multiplied by one digit (either 0 or 1), and that is much easier than in decimal, as the product by 0 or 1 is just 0 or the same number. Therefore, the multiplication of two binary numbers comes down to calculating partial products (which are 0 or the first number), shifting them left, and then adding them together (a binary addition, of course):\n\nThis is much simpler than in the decimal system, as there is no table of multiplication to remember: just shifts and adds.\n\nThis method is mathematically correct and has the advantage that a small CPU may perform the multiplication by using the shift and add features of its arithmetic logic unit rather than a specialized circuit. The method is slow, however, as it involves many intermediate additions. These additions take a lot of time. Faster multipliers may be engineered in order to do fewer additions; a modern processor can multiply two 64-bit numbers with 6 additions (rather than 64), and can do several steps in parallel. \n\nThe second problem is that the basic school method handles the sign with a separate rule (\"+ with + yields +\", \"+ with − yields −\", etc.). Modern computers embed the sign of the number in the number itself, usually in the two's complement representation. That forces the multiplication process to be adapted to handle two's complement numbers, and that complicates the process a bit more. Similarly, processors that use ones' complement, sign-and-magnitude, IEEE-754 or other binary representations require specific adjustments to the multiplication process.\n\nFor example, suppose we want to multiply two unsigned eight bit integers together: \"a\"[7:0] and \"b\"[7:0]. We can produce eight partial products by performing eight one-bit multiplications, one for each bit in multiplicand \"a\":\n\nwhere <nowiki>{8{a[0]}}</nowiki> means repeating a[0] (the 0th bit of a) 8 times (Verilog notation).\n\nTo produce our product, we then need to add up all eight of our partial products, as shown here:\n\nIn other words, \"P\"[15:0] is produced by summing \"p\"0, \"p\"1 « 1, \"p\"2 « 2, and so forth, to produce our final unsigned 16-bit product.\n\nIf \"b\" had been a signed integer instead of an unsigned integer, then the partial products would need to have been sign-extended up to the width of the product before summing. If \"a\" had been a signed integer, then partial product \"p7\" would need to be subtracted from the final sum, rather than added to it.\n\nThe above array multiplier can be modified to support two's complement notation signed numbers by inverting several of the product terms and inserting a one to the left of the first partial product term:\n\nWhere ~p represents the complement (opposite value) of p.\n\nThere are a lot of simplifications in the bit array above that are not shown and are not obvious. The sequences of one complemented bit followed by noncomplemented bits are implementing a two's complement trick to avoid sign extension. The sequence of p7 (noncomplemented bit followed by all complemented bits) is because we're subtracting this term so they were all negated to start out with (and a 1 was added in the least significant position). For both types of sequences, the last bit is flipped and an implicit -1 should be added directly below the MSB. When the +1 from the two's complement negation for p7 in bit position 0 (LSB) and all the -1's in bit columns 7 through 14 (where each of the MSBs are located) are added together, they can be simplified to the single 1 that \"magically\" is floating out to the left. For an explanation and proof of why flipping the MSB saves us the sign extension, see a computer arithmetic book.\n\nOlder multiplier architectures employed a shifter and accumulator to sum each partial product, often one partial product per cycle, trading off speed for die area. Modern multiplier architectures use the (Modified) Baugh–Wooley algorithm, Wallace trees, or Dadda multipliers to add the partial products together in a single cycle. The performance of the Wallace tree implementation is sometimes improved by \"modified\" Booth encoding one of the two multiplicands, which reduces the number of partial products that must be summed.\n\n\n"}
{"id": "43140009", "url": "https://en.wikipedia.org/wiki?curid=43140009", "title": "Bruno Courcelle", "text": "Bruno Courcelle\n\nBruno Courcelle is a French mathematician and computer scientist, best known for Courcelle's theorem in graph theory.\n\nCourcelle earned his Ph.D. in 1976 from the French Institute for Research in Computer Science and Automation, then called IRIA, under the supervision of Maurice Nivat. He then joined the Laboratoire Bordelais de Recherche en Informatique (LaBRI) at the University of Bordeaux 1, where he remained for the rest of his career. He has been a senior member of the Institut Universitaire de France since 2007.\n\nA workshop in honor of Courcelle's retirement was held in Bordeaux in 2012.\n\nHe is known for Courcelle's theorem, which combines second order logic, the theory of formal languages, and tree decompositions of graphs to show that a wide class of algorithmic problems in graph theory have efficient solutions.\n\n"}
{"id": "542587", "url": "https://en.wikipedia.org/wiki?curid=542587", "title": "Coaxial", "text": "Coaxial\n\nIn geometry, coaxial means that two or more three-dimensional linear forms share a common axis. Thus, it is concentric in three-dimensional, linear forms.\n\nA coaxial cable, as a common example, is a three-dimensional linear structure. It has a wire conductor in the centre (D), a circumferential outer conductor (B), and an insulating medium called the dielectric (C) separating these two conductors. The outer conductor is usually sheathed in a protective PVC outer jacket (A). All these have a common axis.\n\nThe dimension and material of the conductors and insulation determine the cable's characteristic impedance and attenuation at various frequencies.\n\nIn loudspeaker design, coaxial speakers are a loudspeaker system in which the individual driver units radiate sound from the same point or axis.\n\nA coaxial weapon mount places two weapons on [roughly] the same axis – as the weapons are usually side-by-side or one on top of the other, they are technically par-axial rather than coaxial, however the distances involved mean that they are effectively coaxial as far as the operator is concerned.\n"}
{"id": "1979078", "url": "https://en.wikipedia.org/wiki?curid=1979078", "title": "Color model", "text": "Color model\n\nA color model is an abstract mathematical model describing the way colors can be represented as tuples of numbers, typically as three or four values or color components. When this model is associated with a precise description of how the components are to be interpreted (viewing conditions, etc.), the resulting set of colors is called \"color space.\" This section describes ways in which human color vision can be modeled.\n\nOne can picture this space as a region in three-dimensional Euclidean space if one identifies the \"x\", \"y\", and \"z\" axes with the stimuli for the long-wavelength (\"L\"), medium-wavelength (\"M\"), and short-wavelength (\"S\") light receptors. The origin, (\"S\",\"M\",\"L\") = (0,0,0), corresponds to black. White has no definite position in this diagram; rather it is defined according to the color temperature or white balance as desired or as available from ambient lighting. The human color space is a horse-shoe-shaped cone such as shown here (see also CIE chromaticity diagram below), extending from the origin to, in principle, infinity. In practice, the human color receptors will be saturated or even be damaged at extremely high light intensities, but such behavior is not part of the CIE color space and neither is the changing color perception at low light levels (see: Kruithof curve). \nThe most saturated colors are located at the outer rim of the region, with brighter colors farther removed from the origin. As far as the responses of the receptors in the eye are concerned, there is no such thing as \"brown\" or \"gray\" light. The latter color names refer to orange and white light respectively, with an intensity that is lower than the light from surrounding areas. One can observe this by watching the screen of an overhead projector during a meeting: one sees black lettering on a white background, even though the \"black\" has in fact not become darker than the white screen on which it is projected before the projector was turned on. The \"black\" areas have not actually become darker but appear \"black\" relative to the higher intensity \"white\" projected onto the screen around it. See also color constancy.\n\nThe human tristimulus space has the property that additive mixing of colors corresponds to the adding of vectors in this space. This makes it easy to, for example, describe the possible colors (gamut) that can be constructed from the red, green, and blue primaries in a computer display.\n\nOne of the first mathematically defined color spaces is the CIE XYZ color space (also known as CIE 1931 color space), created by the International Commission on Illumination in 1931. These data were measured for human observers and a 2-degree field of view. In 1964, supplemental data for a 10-degree field of view were published.\n\nNote that the tabulated sensitivity curves have a certain amount of arbitrariness in them. The shapes of the individual X, Y and Z sensitivity curves can be measured with a reasonable accuracy. However, the overall luminosity function (which in fact is a weighted sum of these three curves) is subjective, since it involves asking a test person whether two light sources have the same brightness, even if they are in completely different colors. Along the same lines, the relative magnitudes of the X, Y, and Z curves are arbitrarily chosen to produce equal areas under the curves. One could as well define a valid color space with an X sensitivity curve that has twice the amplitude. This new color space would have a different shape. The sensitivity curves in the CIE 1931 and 1964 xyz color space are scaled to have equal areas under the curves.\n\nSometimes XYZ colors are represented by the luminance, Y, and chromaticity coordinates \"x\" and \"y\", defined by:\n\nMathematically, \"x\" and \"y\" are projective coordinates and the colors of the chromaticity diagram occupy a region of the real projective plane. Because the CIE sensitivity curves have equal areas under the curves, light with a flat energy spectrum corresponds to the point (\"x\",\"y\") = (0.333,0.333).\n\nThe values for \"X\", \"Y\", and \"Z\" are obtained by integrating the product of the spectrum of a light beam and the published color-matching functions.\n\nMedia that transmit light (such as television) use additive color mixing with primary colors of red, green, and blue, each of which stimulates one of the three types of the eye's color receptors with as little stimulation as possible of the other two. This is called \"RGB\" color space. Mixtures of light of these primary colors cover a large part of the human color space and thus produce a large part of human color experiences. This is why color television sets or color computer monitors need only produce mixtures of red, green and blue light. See Additive color.\n\nOther primary colors could in principle be used, but with red, green and blue the largest portion of the human color space can be captured. Unfortunately there is no exact consensus as to what loci in the chromaticity diagram the red, green, and blue colors should have, so the same RGB values can give rise to slightly different colors on different screens.\n\nIt is possible to achieve a large range of colors seen by humans by combining cyan, magenta, and yellow transparent dyes/inks on a white substrate. These are the \"subtractive\" primary colors. Often a fourth ink, black, is added to improve reproduction of some dark colors. This is called the \"CMY\" or \"CMYK\" color space.\n\nThe cyan ink absorbs red light but transmits green and blue, the magenta ink absorbs green light but transmits red and blue, and the yellow ink absorbs blue light but transmits red and green. The white substrate reflects the transmitted light back to the viewer. Because in practice the CMY inks suitable for printing also reflect a little bit of color, making a deep and neutral black impossible, the K (black ink) component, usually printed last, is needed to compensate for their deficiencies. Use of a separate black ink is also economically driven when a lot of black content is expected, e.g. in text media, to reduce simultaneous use of the three colored inks. The dyes used in traditional color photographic prints and slides are much more perfectly transparent, so a K component is normally not needed or used in those media.\nA number of color models exist in which colors are fit into conic, cylindrical or spherical shapes, with neutrals running from black to white in a central axis, and hues corresponding to angles around that axis. Arrangements of this type date back to the 18th century, and continue to be developed in the most modern and scientific models.\n\nDifferent color theorists have each designed unique color solids. Many are in the shape of a sphere, whereas others are warped three-dimensional ellipsoid figures—these variations being designed to express some aspect of the relationship of the colors more clearly. The color spheres conceived by Phillip Otto Runge and Johannes Itten are typical examples and prototypes for many other color solid schematics. The models of Runge and Itten are basically identical, and form the basis for the description below.\n\nPure, saturated hues of equal brightness are located around the equator at the periphery of the color sphere. As in the color wheel, contrasting (or complementary) hues are located opposite each other. Moving toward the center of the color sphere on the equatorial plane, colors become less and less saturated, until all colors meet at the central axis as a neutral gray. Moving vertically in the color sphere, colors become lighter (toward the top) and darker (toward the bottom). At the upper pole, all hues meet in white; at the bottom pole, all hues meet in black. \n\nThe vertical axis of the color sphere, then, is gray all along its length, varying from black at the bottom to white at the top. All pure (saturated) hues are located on the surface of the sphere, varying from light to dark down the color sphere. All impure (unsaturated hues, created by mixing contrasting colors) comprise the sphere's interior, likewise varying in brightness from top to bottom.\n\nHSL and HSV are both cylindrical geometries, with hue, their angular dimension, starting at the red primary at 0°, passing through the green primary at 120° and the blue primary at 240°, and then wrapping back to red at 360°. In each geometry, the central vertical axis comprises the \"neutral\", \"achromatic\", or \"gray\" colors, ranging from black at lightness 0 or value 0, the bottom, to white at lightness 1 or value 1, the top.\n\nMost televisions, computer displays, and projectors produce colors by combining red, green, and blue light in varying intensities—the so-called RGB additive primary colors. However, the relationship between the constituent amounts of red, green, and blue light and the resulting color is unintuitive, especially for inexperienced users, and for users familiar with subtractive color mixing of paints or traditional artists’ models based on tints and shades.\n\nIn an attempt to accommodate more traditional and intuitive color mixing models, computer graphics pioneers at PARC and NYIT developed the HSV model in the mid-1970s, formally described by Alvy Ray Smith in the August 1978 issue of \"Computer Graphics\". In the same issue, Joblove and Greenberg described the HSL model—whose dimensions they labeled \"hue\", \"relative chroma\", and \"intensity\"—and compared it to HSV. Their model was based more upon how colors are organized and conceptualized in human vision in terms of other color-making attributes, such as hue, lightness, and chroma; as well as upon traditional color mixing methods—e.g., in painting—that involve mixing brightly colored pigments with black or white to achieve lighter, darker, or less colorful colors.\n\nThe following year, 1979, at SIGGRAPH, Tektronix introduced graphics terminals using HSL for color designation, and the Computer Graphics Standards Committee recommended it in their annual status report. These models were useful not only because they were more intuitive than raw RGB values, but also because the conversions to and from RGB were extremely fast to compute: they could run in real time on the hardware of the 1970s. Consequently, these models and similar ones have become ubiquitous throughout image editing and graphics software since then.\n\nAnother influential older cylindrical color model is the early-20th-century Munsell color system. Albert Munsell began with a spherical arrangement in his 1905 book \"A Color Notation\", but he wished to properly separate color-making attributes into separate dimensions, which he called \"hue\", \"value\", and \"chroma\", and after taking careful measurements of perceptual responses, he realized that no symmetrical shape would do, so he reorganized his system into a lumpy blob.\n\nMunsell’s system became extremely popular, the de facto reference for American color standards—used not only for specifying the color of paints and crayons, but also, e.g., electrical wire, beer, and soil color—because it was organized based on perceptual measurements, specified colors via an easily learned and systematic triple of numbers, because the color chips sold in the \"Munsell Book of Color\" covered a wide gamut and remained stable over time (rather than fading), and because it was effectively marketed by Munsell’s Company. In the 1940s, the Optical Society of America made extensive measurements, and adjusted the arrangement of Munsell colors, issuing a set of \"renotations\". The trouble with the Munsell system for computer graphics applications is that its colors are not specified via any set of simple equations, but only via its foundational measurements: effectively a lookup table. Converting from requires interpolating between that table’s entries, and is extremely computationally expensive in comparison with converting from or which only requires a few simple arithmetic operations.\n\nIn densitometry, a model quite similar to the hue defined above is used for describing colors of CMYK process inks. In 1953, Frank Preucil developed two geometric arrangements of hue, the \"Preucil hue circle\" and the \"Preucil hue hexagon\", analogous to our \"H\" and \"H\", respectively, but defined relative to idealized cyan, yellow, and magenta ink colors. The \"Preucil \"hue error\"\" of an ink indicates the difference in the \"hue circle\" between its color and the hue of the corresponding idealized ink color. The \"grayness\" of an ink is , where \"m\" and \"M\" are the minimum and maximum among the amounts of idealized cyan, magenta, and yellow in a density measurement.\n\nThe Swedish Natural Color System (NCS), widely used in Europe, takes a similar approach to the Ostwald bicone at right. Because it attempts to fit color into a familiarly shaped solid based on \"phenomenological\" instead of photometric or psychological characteristics, it suffers from some of the same disadvantages as HSL and HSV: in particular, its lightness dimension differs from perceived lightness, because it forces colorful yellow, red, green, and blue into a plane.\n\nThe International Commission on Illumination (CIE) developed the XYZ model for describing the colors of light spectra in 1931, but its goal was to match human visual metamerism, rather than to be perceptually uniform, geometrically. In the 1960s and 70s, attempts were made to transform XYZ colors into a more relevant geometry, influenced by the Munsell system. These efforts culminated in the 1976 CIELUV and CIELAB models. The dimensions of these models— and , respectively—are cartesian, based on the opponent process theory of color, but both are also often described using polar coordinates— and , respectively—where \"L\"* is lightness, \"C\"* is chroma, and \"h\"* is hue angle. Officially, both CIELAB and CIELUV were created for their color difference metrics ∆\"E\"* and ∆\"E\"*, particularly for use defining color tolerances, but both have become widely used as color order systems and color appearance models, including in computer graphics and computer vision. For example, gamut mapping in ICC color management is usually performed in CIELAB space, and Adobe Photoshop includes a CIELAB mode for editing images. CIELAB and CIELUV geometries are much more perceptually relevant than many others such as RGB, HSL, HSV, YUV/YIQ/YCbCr or XYZ, but are not perceptually perfect, and in particular have trouble adapting to unusual lighting conditions.\n\nThe HCL color space seems to be synonymous with CIELCH.\n\nThe CIE’s most recent model, CIECAM02 (CAM stands for \"color appearance model\"), is more theoretically sophisticated and computationally complex than earlier models. Its aims are to fix several of the problems with models such as CIELAB and CIELUV, and to explain not only responses in carefully controlled experimental environments, but also to model the color appearance of real-world scenes. Its dimensions \"J\" (lightness), \"C\" (chroma), and \"h\" (hue) define a polar-coordinate geometry.\n\nThere are various types of color systems that classify color and analyse their effects. The American Munsell color system devised by Albert H. Munsell is a famous classification that organises various colors into a color solid based on hue, saturation and value. Other important color systems include the Swedish Natural Color System (NCS), the Optical Society of America's Uniform Color Space (OSA-UCS), and the Hungarian Coloroid system developed by Antal Nemcsics from the Budapest University of Technology and Economics. Of those, the NCS is based on the opponent-process color model, while the Munsell, the OSA-UCS and the Coloroid attempt to model color uniformity. The American Pantone and the German RAL commercial color-matching systems differ from the previous ones in that their color spaces are not based on an underlying color model.\n\nWe also use \"color model\" to indicate a model or mechanism of color vision for explaining how color signals are processed from visual cones to ganglion cells. For simplicity, we call these models color mechanism models. The classical color mechanism models are Young–Helmholtz's trichromatic model and Hering's opponent-process model. Though these two theories were initially thought to be at odds, it later came to be understood that the mechanisms responsible for color opponency receive signals from the three types of cones and process them at a more complex level.\n\nVertebrate animals were primitively tetrachromatic. They possessed four types of cones—long, mid, short wavelength cones, and ultraviolet sensitive cones. Today, fish, amphibians, reptiles and birds are all tetrachromatic. Placental mammals lost both the mid and short wavelength cones. Thus, most mammals do not have complex color vision—they are dichromatic but they are sensitive to ultraviolet light, though they cannot see its colors. Human trichromatic color vision is a recent evolutionary novelty that first evolved in the common ancestor of the Old World Primates. Our trichromatic color vision evolved by duplication of the long wavelength sensitive opsin, found on the X chromosome. One of these copies evolved to be sensitive to green light and constitutes our mid wavelength opsin. At the same time, our short wavelength opsin evolved from the ultraviolet opsin of our vertebrate and mammalian ancestors.\n\nHuman red-green color blindness occurs because the two copies of the red and green opsin genes remain in close proximity on the X chromosome. Because of frequent recombination during meiosis, these gene pairs can get easily rearranged, creating versions of the genes that do not have distinct spectral sensitivities.\n\n\n\n"}
{"id": "52622473", "url": "https://en.wikipedia.org/wiki?curid=52622473", "title": "Contraction morphism", "text": "Contraction morphism\n\nIn algebraic geometry, a contraction morphism is a surjective projective morphism formula_1 between normal projective varieties (or projective schemes) such that formula_2 or, equivalently, the geometric fibers are all connected (Zariski's connectedness theorem). It is also commonly called an algebraic fiber space, as it is an analog of a fiber space in algebraic topology.\n\nBy the Stein factorization, any surjective projective morphism is a contraction morphism followed by a finite morphism.\n\nExamples include ruled surfaces and Mori fiber spaces.\n\nThe following perspective is crucial in birational geometry (in particular in Mori's minimal model program).\n\nLet \"X\" be a projective variety and formula_3 the closure of the span of irreducible curves on \"X\" in formula_4 = the real vector space of numerical equivalence classes of real 1-cycles on \"X\". Given a face \"F\" of formula_3, the contraction morphism associated to \"F\", if it exists, is a contraction morphism formula_1 to some projective variety \"Y\" such that for each irreducible curve formula_7, formula_8 is a point if and only if formula_9. The basic question is which face \"F\" gives rise to such a contraction morphism (cf. cone theorem).\n\n\n"}
{"id": "47790980", "url": "https://en.wikipedia.org/wiki?curid=47790980", "title": "Control variable (programming)", "text": "Control variable (programming)\n\nA control variable in computer programming is a program variable that is used to regulate the flow of control of the program.\n\nIn definite iteration, control variables are variables which are successively assigned (or bound to) values from a predetermined sequence of values.\n\nIn some programming languages control variables are just ordinary variables used for manipulating the program flow. This is the case of C, Fortran, and Pascal, which allow for control variables to have their values changed within the loop body. However, some languages have special rules for control variables. In Ada, for instance, the control variable of the for loop must remain constant within the loop body.\n"}
{"id": "21653957", "url": "https://en.wikipedia.org/wiki?curid=21653957", "title": "Definitions of mathematics", "text": "Definitions of mathematics\n\nMathematics has no generally accepted definition. Different schools of thought, particularly in philosophy, have put forth radically different definitions. All are controversial.\n\nAristotle defined mathematics as:\n\nThe science of quantity.\n\nIn Aristotle's classification of the sciences, discrete quantities were studied by arithmetic, continuous quantities by geometry.\n\nAuguste Comte's definition tried to explain the role of mathematics in coordinating phenomena in all other fields:\n\nThe science of indirect measurement. \n\nThe \"indirectness\" in Comte's definition refers to determining quantities that cannot be measured directly, such as the distance to planets or the size of atoms, by means of their relations to quantities that can be measured directly.\n\nThe preceding kinds of definitions, which had prevailed since Aristotle's time, were abandoned in the 19th century as new branches of mathematics were developed, which bore no obvious relation to measurement or the physical world, such as group theory, projective geometry, and non-Euclidean geometry. As mathematicians pursued greater rigor and more-abstract foundations, some proposed definitions purely in terms of logic:\n\nMathematics is the science that draws necessary conclusions. \n\nAll Mathematics is Symbolic Logic. \n\nPeirce did not think that mathematics is the same as logic, since he thought mathematics makes only hypothetical assertions, not categorical ones. Russell's definition, on the other hand, expresses the logicist philosophy of mathematics without reservation. Competing philosophies of mathematics put forth different definitions.\n\nOpposing the completely deductive character of logicism, intuitionism emphasizes the construction of ideas in the mind. Here is an intuitionist definition:\n\nMathematics is mental activity which consists in carrying out, one after the other, those mental constructions which are inductive and effective.\n\nmeaning that by combining fundamental ideas, one reaches a definite result.\n\nFormalism denies both physical and mental meaning to mathematics, making the symbols and rules themselves the object of study. A formalist definition:\n\nMathematics is the manipulation of the meaningless symbols of a first-order language according to explicit, syntactical rules.\n\nStill other approaches emphasize pattern, order, or structure. For example:\n\nMathematics is the classification and study of all possible patterns. \n\nYet another approach makes abstraction the defining criterion:\n\nMathematics is a broad-ranging field of study in which the properties and interactions of idealized objects are examined. \n\nMost contemporary reference works define mathematics by summarizing its main topics and methods:\n\nThe abstract science which investigates deductively the conclusions implicit in the elementary conceptions of spatial and numerical relations, and which includes as its main divisions geometry, arithmetic, and algebra. \n\nThe study of the measurement, properties, and relationships of quantities and sets, using numbers and symbols. \n\nThe science of structure, order, and relation that has evolved from elemental practices of counting, measuring, and describing the shapes of objects. \n\nBertrand Russell wrote this famous tongue-in-cheek definition, describing the way all terms in mathematics are ultimately defined by reference to undefined terms:\n\nThe subject in which we never know what we are talking about, nor whether what we are saying is true. \n\nMany other attempts to characterize mathematics have led to humor or poetic prose:\n\nA mathematician is a blind man in a dark room looking for a black cat which isn't there. \n\nA mathematician, like a painter or poet, is a maker of patterns. If his patterns are more permanent than theirs, it is because they are made with ideas. \n\nMathematics is the art of giving the same name to different things. \n\nMathematics is the science of skillful operations with concepts and rules invented just for this purpose. [this purpose being the skillful operation ...] \n\nMathematics is not a book confined within a cover and bound between brazen clasps, whose contents it needs only patience to ransack; it is not a mine, whose treasures may take long to reduce into possession, but which fill only a limited number of veins and lodes; it is not a soil, whose fertility can be exhausted by the yield of successive harvests; it is not a continent or an ocean, whose area can be mapped out and its contour defined: it is limitless as that space which it finds too narrow for its aspirations; its possibilities are as infinite as the worlds which are forever crowding in and multiplying upon the astronomer's gaze; it is as incapable of being restricted within assigned boundaries or being reduced to definitions of permanent validity, as the consciousness of life, which seems to slumber in each monad, in every atom of matter, in each leaf and bud cell, and is forever ready to burst forth into new forms of vegetable and animal existence. \n\nWhat is mathematics? What is it for? What are mathematicians doing nowadays? Wasn't it all finished long ago? How many new numbers can you invent anyway? Is today's mathematics just a matter of huge calculations, with the mathematician as a kind of zookeeper, making sure the precious computers are fed and watered? If it's not, what is it other than the incomprehensible outpourings of superpowered brainboxes with their heads in the clouds and their feet dangling from the lofty balconies of their ivory towers? Mathematics is all of these, and none. Mostly, it's just different. It's not what you expect it to be, you turn your back for a moment and it's changed. It's certainly not just a fixed body of knowledge, its growth is not confined to inventing new numbers, and its hidden tendrils pervade every aspect of modern life. \n\n\n"}
{"id": "151925", "url": "https://en.wikipedia.org/wiki?curid=151925", "title": "Del", "text": "Del\n\nDel, or nabla, is an operator used in mathematics, in particular in vector calculus, as a vector differential operator, usually represented by the nabla symbol ∇. When applied to a function defined on a one-dimensional domain, it denotes its standard derivative as defined in calculus. When applied to a field (a function defined on a multi-dimensional domain), it may denote the gradient (locally steepest slope) of a scalar field (or sometimes of a vector field, as in the Navier–Stokes equations), the divergence of a vector field, or the curl (rotation) of a vector field, depending on the way it is applied.\n\nStrictly speaking, del is not a specific operator, but rather a convenient mathematical notation for those three operators, that makes many equations easier to write and remember. The del symbol can be interpreted as a vector of partial derivative operators, and its three possible meanings—gradient, divergence, and curl—can be formally viewed as the product with a scalar, a dot product, and a cross product, respectively, of the del \"operator\" with the field. These formal products do not necessarily commute with other operators or products. These three uses, detailed below, are summarized as:\n\nIn the Cartesian coordinate system R with coordinates formula_4 and standard basis formula_5, del is defined in terms of partial derivative operators as\n\nIn three-dimensional Cartesian coordinate system R with coordinates formula_7 and standard basis or unit vectors of axes formula_8, del is written as\n\nDel can also be expressed in other coordinate systems, see for example del in cylindrical and spherical coordinates.\n\nDel is used as a shorthand form to simplify many long mathematical expressions. It is most commonly used to simplify expressions for the gradient, divergence, curl, directional derivative, and Laplacian.\n\nThe vector derivative of a scalar field formula_10 is called the gradient, and it can be represented as:\n\nIt always points in the direction of greatest increase of formula_10, and it has a magnitude equal to the maximum rate of increase at the point—just like a standard derivative. In particular, if a hill is defined as a height function over a plane formula_13, the gradient at a given location will be a vector in the xy-plane (visualizable as an arrow on a map) pointing along the steepest direction. The magnitude of the gradient is the value of this steepest slope.\n\nIn particular, this notation is powerful because the gradient product rule looks very similar to the 1d-derivative case:\n\nHowever, the rules for dot products do not turn out to be simple, as illustrated by:\n\nThe divergence of a vector field\nformula_16 is a scalar function that can be represented as:\n\nThe divergence is roughly a measure of a vector field's increase in the direction it points; but more accurately, it is a measure of that field's tendency to converge toward or repel from a point.\n\nThe power of the del notation is shown by the following product rule:\n\nThe formula for the vector product is slightly less intuitive, because this product is not commutative:\n\nThe curl of a vector field formula_20 is a vector function that can be represented as:\n\nThe curl at a point is proportional to the on-axis torque that a tiny pinwheel would be subjected to if it were centred at that point.\n\nThe vector product operation can be visualized as a pseudo-determinant:\n\nAgain the power of the notation is shown by the product rule:\n\nUnfortunately the rule for the vector product does not turn out to be simple:\n\nThe directional derivative of a scalar field formula_25 in the direction\nformula_26 is defined as:\n\nThis gives the rate of change of a field formula_10 in the direction of formula_29. In operator notation, the element in parentheses can be considered a single coherent unit; fluid dynamics uses this convention extensively, terming it the convective derivative—the \"moving\" derivative of the fluid.\n\nNote that formula_30 is an operator that takes scalar to a scalar. It can be extended to operate on a vector, by separately operate on each of its components.\n\nThe Laplace operator is a scalar operator that can be applied to either vector or scalar fields; for cartesian coordinate systems it is defined as:\nand the definition for more general coordinate systems is given in vector Laplacian.\n\nThe Laplacian is ubiquitous throughout modern mathematical physics, appearing for example in Laplace's equation, Poisson's equation, the heat equation, the wave equation, and the Schrödinger equation.\n\nDel can also be applied to a vector field with the result being a tensor. The tensor derivative of a vector field formula_32 (in three dimensions) is a 9-term second-rank tensor – that is, a 3×3 matrix – but can be denoted simply as formula_33, where formula_34 represents the dyadic product. This quantity is equivalent to the transpose of the Jacobian matrix of the vector field with respect to space. The divergence of the vector field can then be expressed as the trace of this matrix.\n\nFor a small displacement formula_35, the change in the vector field is given by:\n\nFor vector calculus:\n\nFor matrix calculus (for which formula_38 can be written formula_39):\n\nAnother relation of interest (see e.g. \"Euler equations\") is the following, where formula_41 is the outer product tensor:\n\nWhen del operates on a scalar or vector, either a scalar or vector is returned. Because of the diversity of vector products (scalar, dot, cross) one application of del already gives rise to three major derivatives: the gradient (scalar product), divergence (dot product), and curl (cross product). Applying these three sorts of derivatives again to each other gives five possible second derivatives, for a scalar field \"f\" or a vector field v; the use of the scalar Laplacian and vector Laplacian gives two more:\n\nThese are of interest principally because they are not always unique or independent of each other. As long as the functions are well-behaved, two of them are always zero:\n\nTwo of them are always equal:\n\nThe 3 remaining vector derivatives are related by the equation:\n\nAnd one of them can even be expressed with the tensor product, if the functions are well-behaved:\n\nMost of the above vector properties (except for those that rely explicitly on del's differential properties—for example, the product rule) rely only on symbol rearrangement, and must necessarily hold if the del symbol is replaced by any other vector. This is part of the value to be gained in notationally representing this operator as a vector.\n\nThough one can often replace del with a vector and obtain a vector identity, making those identities mnemonic, the reverse is \"not\" necessarily reliable, because del does not commute in general.\n\nA counterexample that relies on del's failure to commute:\n\nA counterexample that relies on del's differential properties:\n\nCentral to these distinctions is the fact that del is not simply a vector; it is a vector operator. Whereas a vector is an object with both a magnitude and direction, del has neither a magnitude nor a direction until it operates on a function.\n\nFor that reason, identities involving del must be derived with care, using both vector identities and \"differentiation\" identities such as the product rule.\n\n\n\n"}
{"id": "2976052", "url": "https://en.wikipedia.org/wiki?curid=2976052", "title": "Distribution (differential geometry)", "text": "Distribution (differential geometry)\n\nIn differential geometry, a discipline within mathematics, a distribution is a subset of the tangent bundle of a manifold satisfying certain properties. Distributions are used to build up notions of integrability, and specifically of a foliation of a manifold.\n\nEven though they share the same name, distributions we discuss in this article have nothing to do with distributions in the sense of analysis.\n\nLet formula_1 be a formula_2 manifold of dimension formula_3, and let formula_4. Suppose that for each formula_5, we assign an formula_6-dimensional subspace formula_7 of the tangent space in such a way that for a neighbourhood formula_8 of formula_9 there exist formula_6 linearly independent smooth vector fields formula_11 such that for any point formula_12, span formula_13 We let formula_14 refer to the collection of all the formula_15 for all formula_5 and we then call formula_14 a \"distribution\" of dimension formula_6 on formula_1, or sometimes a \"formula_2 formula_6-plane distribution\" on formula_22 The set of smooth vector fields formula_23 is called a \"local basis\" of formula_24\n\nWe say that a distribution formula_14 on formula_1 is \"involutive\" if for every point formula_5 there exists a local basis formula_23 of the distribution in a neighbourhood of formula_9 such that for all formula_30, formula_31 (the Lie bracket of two vector fields) is in the span of formula_32 That is, if formula_31 is a linear combination of formula_32 Normally this is written as formula_35\n\nInvolutive distributions are the tangent spaces to foliations. Involutive distributions are important in that they satisfy the conditions of the Frobenius theorem, and thus lead to integrable systems.\n\nA related idea occurs in Hamiltonian mechanics: two functions \"f\" and \"g\" on a symplectic manifold are said to be in mutual involution if their Poisson bracket vanishes.\n\nA generalized distribution, or Stefan-Sussmann distribution, is similar to a distribution, but the subspaces formula_36 are not required to all be of the same dimension. The definition requires that the formula_15 are determined locally by a set of vector fields, but these will no longer be linearly independent everywhere. It is not hard to see that the dimension of formula_15 is lower semicontinuous, so that at special points the dimension is lower than at nearby points.\n\nOne class of examples is furnished by a non-free action of a Lie group on a manifold, the vector fields in question being the infinitesimal generators of the group action (a free action gives rise to a genuine distribution). Another arises in dynamical systems, where the set of vector fields in the definition is the set of vector fields that commute with a given one. There are also examples and applications in Control theory, where the generalized distribution represents infinitesimal constraints of the system.\n\n"}
{"id": "47197348", "url": "https://en.wikipedia.org/wiki?curid=47197348", "title": "Dose-fractionation theorem", "text": "Dose-fractionation theorem\n\nHegerl and Hoppe. have pointed out that the total dose required to achieve statistical significance for each voxel of a computed 3D reconstruction is the same as that required to obtain a single 2D image of that isolated voxel, at the same level of statistical significance. Thus a statistically significant 3D image can be computed from statistically insignificant projections, as long as the total dose that is distributed among these projections is high enough that it would have resulted in a statistically significant projection, if applied to only one image.\n"}
{"id": "1303196", "url": "https://en.wikipedia.org/wiki?curid=1303196", "title": "Edward G. Coffman Jr.", "text": "Edward G. Coffman Jr.\n\nEdward Grady \"Ed\" Coffman Jr. is a computer scientist. He began his career as a systems programmer at the System Development Corporation (SDC) during the period 1958–65. His PhD in Engineering at UCLA in 1966 was followed by a series of positions at Princeton University (1966–69), The Pennsylvania State University (1970–76), Columbia University (1976–77), and the University of California, Santa Barbara (1977–79). In 1979, he joined the Mathematics Center at Bell Laboratories where he stayed until his retirement as a Distinguished Member of Technical Staff 20 years later. After a one-year stint at the New Jersey Institute of Technology, he returned to Columbia University in 2000 with appointments in Computer Science, Electrical Engineering, and Industrial Engineering and Operations Research. He retired from teaching in 2008 and is now a Professor Emeritus still fully engaged in research and in professional activities.\n\nCoffman is best known for his seminal research together with his international collaborations, measured in part by some 150 co-authors in his collection of publications. His work can be found in over 180 articles in technical journals devoted to original research contributions. He published 4 graduate-level text books, and papers in the proceedings of some 250 conferences and workshops, most of these being preliminary versions of journal articles. In his research, Coffman has been a generalist following many parallel paths in engineering and applied mathematics. The directions he has taken have drawn on the tools of combinatorial optimization and the theory of algorithms, along with those of applied probability and stochastic processes. The processes studied include those in the theories of scheduling, bin packing, sequential selection, graphs, and dynamic allocation, along with those in queueing, polling, reservation, moving-server, networking, and distributed local-rule systems (e.g. cellular automata). His contributions have been divided between mathematical foundations and the design and analysis of approximation algorithms providing the basis for engineering solutions to NP-hard problems. Computer and network engineering applications have been broad in scope; a partial list includes research addressing problems in the scheduling and storage allocation functions of computer operating systems, storage architectures, data structures, computer timing problems such as deadlocks and synchronization, Internet congestion, peer-to-peer file sharing networks, stream merging, self-assembly processes of molecular computing, minimalist algorithms in sensor networks, optical burst switching, and dynamic spectrum management in cognitive networks. The list expands greatly when including the myriad applications in industrial engineering and operations research of Coffman's research in scheduling and bin-packing theory in one and two dimensions. As of November 11, 2015, his works have been cited 13,597 times, and he has an h-index of 49.\n\nCoffman has been active professionally serving on several editorial boards, dozens of technical program committees, setting research agendas in workshops of the National Research Council, co-founding the Symposium on Operating Systems Principles, and the special interest groups on performance evaluation of both ACM and IFIPS.\n\n\n"}
{"id": "56891926", "url": "https://en.wikipedia.org/wiki?curid=56891926", "title": "Evidence lower bound", "text": "Evidence lower bound\n\nIn statistics, the evidence lower bound (ELBO, also variational lower bound) is the difference between the distribution of a latent variable and the distribution of the respective observed variable (See Kullback–Leibler divergence)\n"}
{"id": "25262819", "url": "https://en.wikipedia.org/wiki?curid=25262819", "title": "Exceptional isomorphism", "text": "Exceptional isomorphism\n\nIn mathematics, an exceptional isomorphism, also called an accidental isomorphism, is an isomorphism between members \"a\" and \"b\" of two families, usually infinite, of mathematical objects, that is not an example of a pattern of such isomorphisms. These coincidences are at times considered a matter of trivia, but in other respects they can give rise to other phenomena, notably exceptional objects. In the following, coincidences are listed wherever they occur.\n\nThe exceptional isomorphisms between the series of finite simple groups mostly involve projective special linear groups and alternating groups, and are:\n\nIn addition to the aforementioned, there are some isomorphisms involving SL, PSL, GL, PGL, and the natural maps between these. For example, the groups over formula_6 have a number of exceptional isomorphisms:\n\nThere are coincidences between alternating groups and small groups of Lie type:\nThese can all be explained in a systematic way by using linear algebra (and the action of formula_15 on affine formula_16-space)\nto define the isomorphism going from the right side to the left side. (The above isomorphisms for formula_17 and formula_18 are linked via the exceptional isomorphism formula_19.)\nThere are also some coincidences with symmetries of regular polyhedra: the alternating group A agrees with the icosahedral group (itself an exceptional object), and the double cover of the alternating group A is the binary icosahedral group.\n\nCyclic groups of small order especially arise in various ways, for instance:\n\nThe spheres \"S\", \"S\", and \"S\" admit group structures, which can be described in many ways:\n\nThere are some exceptional isomorphisms of Coxeter diagrams, yielding isomorphisms of the corresponding Coxeter groups and of polytopes realizing the symmetries. These are:\n\nClosely related ones occur in Lie theory for Dynkin diagrams.\n\nIn low dimensions, there are isomorphisms among the classical Lie algebras and among the classical Lie groups called \"accidental isomorphisms\". For instance, there are isomorphisms between low-dimensional spin groups and certain classical Lie groups, due to low-dimensional isomorphisms between the root systems of the different families of simple Lie algebras, visible as isomorphisms of the corresponding Dynkin diagrams:\n\n"}
{"id": "294056", "url": "https://en.wikipedia.org/wiki?curid=294056", "title": "Extensionality", "text": "Extensionality\n\nIn logic, extensionality, or extensional equality, refers to principles that judge objects to be equal if they have the same external properties. It stands in contrast to the concept of intensionality, which is concerned with whether the internal definitions of objects are the same.\n\nConsider the two functions \"f\" and \"g\" mapping from and to natural numbers, defined as follows:\n\nThese functions are extensionally equal; given the same input, both functions always produce the same value. But the definitions of the functions are not equal, and in that intensional sense the functions are not the same. \n\nSimilarly, in natural language there are many predicates (relations) that are intensionally different but are extensionally identical. For example, suppose that a town has one person named Joe, who is also the oldest person in the town. Then, the two argument predicates \"has one person named\", \"is the oldest person in\" are intensionally distinct, but extensionally equal for \"Joe\" in that \"town\" now.\n\nThe extensional definition of function equality, discussed above, is commonly used in mathematics. Sometimes additional information is attached to a function, such as an explicit codomain, in which case two functions must not only agree on all values, but must also have the same codomain, in order to be equal. \n\nA similar extensional definition is usually employed for relations: two relations are said to be equal if they have the same extensions.\n\nIn set theory, the axiom of extensionality states that two sets are equal if and only if they contain the same elements. In mathematics formalized in set theory, it is common to identify relations—and, most importantly, functions—with their extension as stated above, so that it is impossible for two relations or functions with the same extension to be distinguished.\n\nOther mathematical objects are also constructed in such a way that the intuitive notion of \"equality\" agrees with set-level extensional equality; thus, equal ordered pairs have equal elements, and elements of a set which are related by an equivalence relation belong to the same equivalence class.\n\nType-theoretical foundations of mathematics are generally \"not\" extensional in this sense, and setoids are commonly used to maintain a difference between intensional equality and a more general equivalence relation (which generally has poor constructibility or decidability properties).\n\n\n"}
{"id": "390404", "url": "https://en.wikipedia.org/wiki?curid=390404", "title": "Fano plane", "text": "Fano plane\n\nIn finite geometry, the Fano plane (after Gino Fano) is the finite projective plane of order 2. It is the finite projective plane with the smallest possible number of points and lines: 7 points and 7 lines, with 3 points on every line and 3 lines through every point. The standard notation for this plane, as a member of a family of projective spaces, is where stands for \"Projective Geometry\", the first parameter is the geometric dimension and the second parameter is the order.\n\nThe Fano plane is an example of a finite incidence structure, so many of its properties can be established using combinatorical techniques and other tools used in the study of incidence geometries. Since it is a projective space, algebraic techniques can also be effective tools in its study.\n\nThe Fano plane can be constructed via linear algebra as the projective plane over the finite field with two elements. One can similarly construct projective planes over any other finite field, with the Fano plane being the smallest.\n\nUsing the standard construction of projective spaces via homogeneous coordinates, the seven points of the Fano plane may be labeled with the seven non-zero ordered triples of binary digits 001, 010, 011, 100, 101, 110, and 111. This can be done in such a way that for every two points \"p\" and \"q\", the third point on line \"pq\" has the label formed by adding the labels of \"p\" and \"q\" modulo 2. In other words, the points of the Fano plane correspond to the non-zero points of the finite vector space of dimension 3 over the finite field of order 2.\n\nDue to this construction, the Fano plane is considered to be a Desarguesian plane, even though the plane is too small to contain a non-degenerate Desargues configuration (which requires 10 points and 10 lines).\n\nThe lines of the Fano plane may also be given homogeneous coordinates, again using non-zero triples of binary digits. With this system of coordinates, a point is incident to a line if the coordinate for the point and the coordinate for the line have an even number of positions at which they both have nonzero bits: for instance, the point 101 belongs to the line 111, because they have nonzero bits at two common positions. In terms of the underlying linear algebra, a point belongs to a line if the inner product of the vectors representing the point and line is zero.\n\nThe lines can be classified into three types.\n\nAlternatively, the 7 points of the plane correspond to the 7 non-identity elements of the group (\"Z\") = \"Z\" × \"Z\" × \"Z\". The lines of the plane correspond to the subgroups of order 4, isomorphic to \"Z\" × \"Z\". The automorphism group GL(3,2) of the group (\"Z\") is that of the Fano plane, and has order 168.\n\nAs with any incidence structure, the Levi graph of the Fano plane is a bipartite graph, the vertices of one part representing the points and the other representing the lines, with two vertices joined if the corresponding point and line are incident. This particular graph is a connected cubic graph (regular of degree 3), has girth 6 and each part contains 7 vertices. It is the Heawood graph, the unique 6-cage.\n\nA permutation of the seven points of the Fano plane that carries collinear points (points on the same line) to collinear points (in other words, it \"preserves collinearity\") is called a \"collineation\", \"automorphism\", or \"symmetry\" of the plane. The full collineation group (or automorphism group, or symmetry group) is the projective linear group PGL(3,2) which in this case is isomorphic to the projective special linear group PSL(3,2), and the general linear group GL(3,2) (which is equal to PGL(3,2), because the field has only one nonzero element) and is also isomorphic to PSL(2,7). It consists of 168 different permutations.\n\nAs a permutation group acting on the points of the plane, the collineation group is doubly transitive meaning that any ordered pair of points can be mapped by at least one collineation to any other ordered pair of points.\n\nCollineations may also be viewed as the color preserving automorphisms of the Heawood graph.\nA bijection between the point set and the line set that preserves incidence is called a \"duality\" and a duality of order two is called a \"polarity\".\n\nDualities can be viewed in the context of the Heawood graph as color reversing automorphisms. An example of a polarity is given by reflection through a vertical line that bisects the Heawood graph representation given on the right. The existence of this polarity shows that the Fano plane is \"self-dual\". This is also an immediate consequence of the symmetry between points and lines in the definition of the incidence relation in terms of homogeneous coordinates, as detailed in an earlier section.\n\nWith the nimber labels of the accompanying figure, the collineation group of the Fano plane, presented as a permutation group can be generated by the permutations given in cyclic notation by:\n\nThe collineation group is made up of 6 conjugacy classes.<br>\nAll cycle structures except the 7-cycle uniquely define a conjugacy class:\nThe 48 permutations with a complete 7-cycle form two distinct conjugacy classes with 24 elements:\n\nSee for a complete list.\n\nHence, by the Pólya enumeration theorem, the number of inequivalent colorings of the Fano plane with \"n\" colors is:\n\nIn any projective plane a set of four points, no three of which are collinear, and the six lines joining pairs of these points is a configuration known as a complete quadrangle. The lines are called \"sides\" and pairs of sides that do not meet at one of the four points are called \"opposite sides\". The points at which opposite sides meet are called \"diagonal points\" and there are three of them.\n\nIf this configuration lies in a projective plane and the three diagonal points are collinear, then the seven points and seven lines of the expanded configuration form a subplane of the projective plane that is isomorphic to the Fano plane and is called a \"Fano subplane\". \n\nA famous result, due to Andrew M. Gleason states that if every complete quadrangle in a finite projective plane extends to a Fano subplane (that is, has collinear diagonal points) then the plane is Desarguesian. Gleason called any projective plane satisfying this condition a \"Fano plane\" thus creating some confusion with modern terminology. To compound the confusion, \"Fano's axiom\" states that the diagonal points of a complete quadrangle are \"never\" collinear, a condition that holds in the Euclidean and real projective planes. Thus, what Gleason called Fano planes do not satisfy Fano's axiom.\n\nThe Fano plane contains the following numbers of configurations of points and lines of different types. For each type of configuration, the number of copies of configuration multiplied by the number of symmetries of the plane that keep the configuration unchanged is equal to 168, the size of the entire collineation group, provided each copy can be mapped to any other copy (see Orbit-Stabiliser theorem). Since the Fano plane is self-dual, these configurations come in dual pairs and it can be shown that the number of collineations fixing a configuration equals the number of collineations that fix its dual configuraton.\n\nThe Fano plane is an example of an -configuration, that is, a set of points and lines with three points on each line and three lines through each point. The Fano plane, a (7)-configuration, is unique and is the smallest such configuration. According to a theorem by Steinitz, configurations of this type can be realized in the Euclidean plane having at most one curved line (all other lines lying on Euclidean lines).\n\nThe Fano plane is a small symmetric block design, specifically a 2-(7,3,1)-design. The points of the design are the points of the plane, and the blocks of the design are the lines of the plane. As such it is a valuable example in (block) design theory.\n\nWith the points labelled 0, 1, 2, ..., 6 the lines (as point sets) are the translates of the (7, 3, 1) planar difference set given by {0, 1, 3} in the group formula_3 With the lines labeled \"ℓ\", ...,\"ℓ\" the incidence matrix (table) is given by:\n\nThe Fano plane, as a block design, is a Steiner triple system. As such, it can be given the structure of a quasigroup. This quasigroup coincides with the multiplicative structure defined by the unit octonions \"e\", \"e\", ..., \"e\" (omitting 1) if the signs of the octonion products are ignored .\n\nThe Fano plane is one of the important examples in the structure theory of matroids. Excluding the Fano plane as a matroid minor is necessary to characterize several important classes of matroids, such as regular, graphic, and cographic ones.\n\nIf you break one line apart into three 2-point lines you obtain the \"non-Fano configuration\", which can be embedded in the real plane. It is another important example in matroid theory, as it must be excluded for many theorems to hold.\n\nThe Fano plane can be extended in a third dimension to form a three-dimensional projective space, denoted by PG(3,2).\nIt has 15 points, 35 lines, and 15 planes and is the smallest three-dimensional projective space. It also has the following properties:\n\n\n\n\n"}
{"id": "3095929", "url": "https://en.wikipedia.org/wiki?curid=3095929", "title": "Fano variety", "text": "Fano variety\n\nIn algebraic geometry, a Fano variety, introduced in , is a complete variety \"X\" whose anticanonical bundle \"K\" is ample. In this definition, one could assume that \"X\" is smooth over a field, but the minimal model program has also led to the study of Fano varieties with various types of singularities, such as terminal or klt singularities.\n\n\nThe existence of some ample line bundle on \"X\" is equivalent to \"X\" being a projective variety, so a Fano variety is always projective. For a Fano variety \"X\" over the complex numbers, the Kodaira vanishing theorem implies that the sheaf cohomology groups formula_1 of the structure sheaf vanish for formula_2. In particular, the Todd genus formula_3 automatically equals 1. The formula_4 cases of this vanishing statement also tell us that the first Chern class induces an isomorphism formula_5. \n\nBy Yau's solution of the Calabi conjecture, a smooth complex variety admits Kähler metrics of positive\nRicci curvature if and only if it is Fano. Myers' theorem therefore tells us that the universal cover of a Fano manifold is compact, and so can only be a finite covering. However, we have just seen that the Todd genus of a Fano manifold must equal 1. Since this would also apply to the manifold's universal cover, and since the Todd genus is multiplicative under finite covers, it follows that any Fano manifold is simply connected. \n\nA much easier fact is that every Fano variety has Kodaira dimension −∞.\n\nCampana and Kollár–Miyaoka–Mori showed that a smooth Fano variety over an algebraically closed field is rationally chain connected; that is, any two closed points can be connected by a chain of rational curves. \nKollár–Miyaoka–Mori also showed that the smooth Fano varieties of a given dimension over an algebraically closed field of characteristic zero form a bounded family, meaning that they are classified by the points of finitely many algebraic varieties. In particular, there are only finitely many deformation classes of Fano varieties of each dimension. In this sense, Fano varieties are much more special than other classes of varieties such as varieties of general type.\n\nThe following discussion concerns smooth Fano varieties over the complex numbers.\n\nA Fano curve is isomorphic to the projective line.\n\nA Fano surface is also called a del Pezzo surface. Every del Pezzo surface is isomorphic to either P × P or to the projective plane blown up in at most 8 points, which must be in general position. As a result, they are all rational.\n\nIn dimension 3, there are smooth complex Fano varieties which are not rational, for example cubic 3-folds in P (by Clemens - Griffiths) and quartic 3-folds in P (by Iskovskikh - Manin). classified the smooth Fano 3-folds with second Betti number 1 into 17 classes, and classified the smooth ones with second Betti number at least 2, finding 88 deformation classes. A detailed summary of the classification of smooth Fano 3-folds is given in .\n\n\n"}
{"id": "49728689", "url": "https://en.wikipedia.org/wiki?curid=49728689", "title": "Flow-sensitive typing", "text": "Flow-sensitive typing\n\nIn programming language theory, flow-sensitive typing (or flow typing) is a type system where the type of an expression depends on its position in the control flow.\n\nIn statically typed languages, a type of an expression is determined by the types of the sub-expressions that compose it. However, in flow-sensitive typing, an expression's type may be updated to a more specific type if it follows a statement that validates its type. The type is determined by using type inference and type information is carried using algebraic data types.\n\nSee the following example in Ceylon which illustrates the concept:\n// Object? means the variable \"name\" is of type Object or else null\nvoid hello(Object? name) {\nhello(null);\nhello(1);\nhello(\"John Doe\");\nWhich outputs:\n\nHello, object 1!\nHello, John Doe!\n\nKotlin uses very flexible flow typing:\nfun hello(obj: Any) {\n\nhello(\"Mooooo\")\n\nThis technique coupled with type inference reduces the need for writing type annotations for all variables or to do type casting, like is seen with dynamic languages that use duck typing. It reduces verbosity and makes up for terser code, easier to read and modify.\n\nIt can also help language implementers to provide faster implementations for dynamic languages by statically predicting the type of objects.\n\nFinally, it increases type safety and can prevent problems due to null pointers, labeled by C.A.R. Hoare—the null reference inventor—as \"the billion dollar mistake\"\n\nWhiley, created by David J. Pearce, was the first language to make use of flow-sensitive typing in 2009.\n\nSince this introduction, other languages have made use of it, namely Ceylon, Kotlin, TypeScript and Facebook Flow.\n"}
{"id": "21687353", "url": "https://en.wikipedia.org/wiki?curid=21687353", "title": "Frink ideal", "text": "Frink ideal\n\nIn mathematics, a Frink ideal, introduced by Orrin Frink, is a certain kind of subset of a partially ordered set.\n\nLU(\"A\") is the set of all common lower bounds of the set of all common upper bounds of the subset \"A\" of a partially ordered set.\n\nA subset \"I\" of a partially ordered set (\"P\", ≤) is a Frink ideal, if the following condition holds:\n\nFor every finite subset \"S\" of \"P\", \"S\" formula_1 \"I\" implies that LU(\"S\") formula_1 \"I\".\n\nA subset \"I\" of a partially ordered set (\"P\",≤) is a normal ideal or a cut if LU(\"I\") formula_1 \"I\".\n\n\n\n"}
{"id": "564004", "url": "https://en.wikipedia.org/wiki?curid=564004", "title": "Graph reduction", "text": "Graph reduction\n\nIn computer science, graph reduction implements an efficient version of non-strict evaluation, an evaluation strategy where the arguments to a function are not immediately evaluated. This form of non-strict evaluation is also known as lazy evaluation and used in functional programming languages. The technique was first developed by Chris Wadsworth in 1971.\n\nA simple example of evaluating an arithmetic expression follows:\n\nThe above reduction sequence employs a strategy known as outermost tree reduction. The same expression can be evaluated using innermost tree reduction, yielding the reduction sequence:\n\nNotice that the reduction order is made explicit by the addition of parentheses. This expression could also have been simply evaluated right to left, because addition is an associative operation.\n\nRepresented as a tree, the expression above looks like this:\n\nThis is where the term tree reduction comes from. When represented as a tree, we can think of innermost reduction as working from the bottom up, while outermost works from the top down.\n\nThe expression can also be represented as a directed acyclic graph, allowing sub-expressions to be shared:\n\nAs for trees, outermost and innermost reduction also applies to graphs. Hence we have graph reduction.\n\nNow evaluation with outermost graph reduction can proceed as follows:\n\nNotice that evaluation now only requires four steps. Outermost graph reduction is referred to as lazy evaluation and innermost graph reduction is referred to as eager evaluation.\n\nCombinator graph reduction is a fundamental implementation technique for functional programming languages, in which a program is converted into a combinator representation which is mapped to a directed graph data structure in computer memory, and program execution then consists of rewriting parts of this graph (\"reducing\" it) so as to move towards useful results.\n\nThe concept of a graph reduction that allows evaluated values to be shared was first developed by Chris Wadsworth in his 1971 Ph.D. dissertation. This dissertation was cited by Peter Henderson and James H. Morris Jr. in 1976 paper, “A lazy evaluator” that introduced the notion of lazy evaluation. In 1976 David Turner incorporated lazy evaluation into SASL using combinators.\nSASL was an early functional programming language first developed by Turner in 1972.\n\n\n"}
{"id": "10966810", "url": "https://en.wikipedia.org/wiki?curid=10966810", "title": "Grundy's game", "text": "Grundy's game\n\nGrundy's game is a two-player mathematical game of strategy. The starting configuration is a single heap of objects, and the two players take turn splitting a single heap into two heaps of different sizes. The game ends when only heaps of size two and smaller remain, none of which can be split unequally. The game is usually played as a \"normal play\" game, which means that the last person who can make an allowed move wins.\n\nA normal play game starting with a single heap of 8 is a win for the first player provided he does start by splitting the heap into heaps of 7 and 1:\nPlayer 2 now has three choices: splitting the 7-heap into 6 + 1, 5 + 2, or 4 + 3. In each of these cases, player 1 can ensure that on the next move he hands back to his opponent a heap of size 4 plus heaps of size 2 and smaller:\nNow player 2 has to split the 4-heap into 3 + 1, and player 1 subsequently splits the 3-heap into 2 + 1:\n\nThe game can be analysed using the Sprague–Grundy theorem. This requires the heap sizes in the game to be mapped onto equivalent nim heap sizes. This mapping is captured in the On-Line Encyclopedia of Integer Sequences as :\n\nUsing this mapping, the strategy for playing the game Nim can also be used for Grundy's game. Whether the sequence of nim-values of Grundy's game ever becomes periodic is an unsolved problem. Elwyn Berlekamp, John Horton Conway and Richard Guy have conjectured that the sequence does become periodic eventually, but despite the calculation of the first 2 values by Achim Flammenkamp, the question has not been resolved.\n\n\n"}
{"id": "14263", "url": "https://en.wikipedia.org/wiki?curid=14263", "title": "Horner's method", "text": "Horner's method\n\nIn mathematics, Horner's rule (also known as Horner scheme or Horner's method) is a way of expressing and evaluating polynomials, which optimizes the needed number of arithmetic operations. \n\nMore precisely Horner's rule consists of expressing the polynomial\nas\nThis allows evaluating a polynomial of degree with only multiplications and additions. This is optimal, since there are polynomials of degree that cannot be evaluated with fewer arithmetic operations.\n\nHorner's method may also refer to the use of Horner's rule in the process of solving a polynomial equation with Newton's method. \n\nThese methods are named after the British mathematician William George Horner, although they were known before him by Paolo Ruffini , six hundred years earlier, by the Chinese mathematician Qin Jiushao and seven hundred years earlier, by the Persian mathematician Sharaf al-Dīn al-Ṭūsī.\n\nGiven the polynomial\n\nwhere formula_4 are real numbers, we wish to evaluate the polynomial at a specific value of formula_5, say formula_6.\n\nTo accomplish this, we define a new sequence of constants as follows:\n\nThen formula_8 is the value of formula_9.\n\nTo see why this works, note that the polynomial can be written in the form\n\nThus, by iteratively substituting the formula_11 into the expression,\n\nEvaluate formula_13 for formula_14\n\nWe use synthetic division as follows:\n\nThe entries in the third row are the sum of those in the first two. Each entry in the second row is the product of the \"x\"-value (3 in this example) with the third-row entry immediately to the left. The entries in the first row are the coefficients of the polynomial to be evaluated. Then the remainder of formula_15 on division by formula_16 is 5.\n\nBut by the polynomial remainder theorem, we know that the remainder is formula_17. Thus formula_18\n\nIn this example, if formula_19 we can see that formula_20, the entries in the third row. So, synthetic division is based on Horner's method.\n\nAs a consequence of the polynomial remainder theorem, the entries in the third row are the coefficients of the second-degree polynomial, the quotient of formula_15 on division by formula_22. \nThe remainder is 5. This makes Horner's method useful for polynomial long division.\n\nDivide formula_23 by formula_24:\n\nThe quotient is formula_25.\n\nLet formula_26 and formula_27. Divide formula_28 by formula_29 using Horner's method.\n\nThe third row is the sum of the first two rows, divided by 2. Each entry in the second row is the product of 1 with the third-row entry to the left. The answer is\n\nHorner's method is a fast, code-efficient method for multiplication and division of binary numbers on a microcontroller with no hardware multiplier. One of the binary numbers to be multiplied is represented as a trivial polynomial, where (using the above notation) \"a\" = 1, and \"x\" = 2. Then, \"x\" (or \"x\" to some power) is repeatedly factored out. In this binary numeral system (base 2), \"x\" = 2, so powers of 2 are repeatedly factored out.\n\nFor example, to find the product of two numbers (0.15625) and \"m\":\n\nTo find the product of two binary numbers \"d\" and \"m\":\n\nIn general, for a binary number with bit values (formula_32) the product is\nAt this stage in the algorithm, it is required that terms with zero-valued coefficients are dropped, so that only binary coefficients equal to one are counted, thus the problem of multiplication or division by zero is not an issue, despite this implication in the factored equation:\n\nThe denominators all equal one (or the term is absent), so this reduces to\nor equivalently (as consistent with the \"method\" described above)\n\nIn binary (base-2) math, multiplication by a power of 2 is merely a register shift operation. Thus, multiplying by 2 is calculated in base-2 by an arithmetic shift. The factor (2) is a right arithmetic shift, a (0) results in no operation (since 2 = 1 is the multiplicative identity element), and a (2) results in a left arithmetic shift.\nThe multiplication product can now be quickly calculated using only arithmetic shift operations, addition and subtraction.\n\nThe method is particularly fast on processors supporting a single-instruction shift-and-addition-accumulate. Compared to a C floating-point library, Horner's method sacrifices some accuracy, however it is nominally 13 times faster (16 times faster when the \"canonical signed digit\" (CSD) form is used) and uses only 20% of the code space.\n\nUsing Horner's method in combination with Newton's method, it is possible to approximate the real roots of a polynomial. The algorithm works as follows. Given a polynomial formula_37 of degree formula_38 with zeros formula_39 make some initial guess formula_40 such that formula_41. Now iterate the following two steps:\n\n1. Using Newton's method, find the largest zero formula_42 of formula_37 using the guess formula_6.\n\n2. Using Horner's method, divide out formula_45 to obtain formula_46. Return to step 1 but use the polynomial formula_46 and the initial guess formula_42.\n\nThese two steps are repeated until all real zeros are found for the polynomial. If the approximated zeros are not precise enough, the obtained values can be used as initial guesses for Newton's method but using the full polynomial rather than the reduced polynomials.\n\nConsider the polynomial\n\nwhich can be expanded to\n\nFrom the above we know that the largest root of this polynomial is 7 so we are able to make an initial guess of 8. Using Newton's method the first zero of 7 is found as shown in black in the figure to the right. Next formula_51 is divided by formula_52 to obtain\n\nwhich is drawn in red in the figure to the right. Newton's method is used to find the largest zero of this polynomial with an initial guess of 7. The largest zero of this polynomial which corresponds to the second largest zero of the original polynomial is found at 3 and is circled in red. The degree 5 polynomial is now divided by formula_54 to obtain\n\nwhich is shown in yellow. The zero for this polynomial is found at 2 again using Newton's method and is circled in yellow. Horner's method is now used to obtain\n\nwhich is shown in green and found to have a zero at −3. This polynomial is further reduced to\n\nwhich is shown in blue and yields a zero of −5. The final root of the original polynomial may be found by either using the final zero as an initial guess for Newton's method, or by reducing formula_58 and solving the linear equation. As can be seen, the expected roots of −8, −5, −3, 2, 3, and 7 were found.\n\nThe following Octave code was used in the example above to implement Horner's method.\n\nThe following Python code implements Horner's method.\n\nThe following C code implements Horner's method.\nHere is a slightly optimized version using explicit fused Multiply–accumulate operation, often execute faster than the above when running on a computer built with a processor supporting FMA instruction:\n\nThe following C# code implements Horner's method.\n\nHorner's method can be used to convert between different positional numeral systems – in which case \"x\" is the base of the number system, and the \"a\" coefficients are the digits of the base-\"x\" representation of a given number – and can also be used if \"x\" is a matrix, in which case the gain in computational efficiency is even greater. In fact, when \"x\" is a matrix, further acceleration is possible which exploits the structure of matrix multiplication, and only formula_59 instead of \"n\" multiplies are needed (at the expense of requiring more storage) using the 1973 method of Paterson and Stockmeyer.\n\nEvaluation using the monomial form of a degree-\"n\" polynomial requires at most \"n\" additions and (\"n\" + \"n\")/2 multiplications, if powers are calculated by repeated multiplication and each monomial is evaluated individually. (This can be reduced to \"n\" additions and 2\"n\" − 1 multiplications by evaluating the powers of \"x\" iteratively.) If numerical data are represented in terms of digits (or bits), then the naive algorithm also entails storing approximately 2\"n\" times the number of bits of \"x\" (the evaluated polynomial has approximate magnitude \"x\", and one must also store \"x\" itself). By contrast, Horner's method requires only \"n\" additions and \"n\" multiplications, and its storage requirements are only \"n\" times the number of bits of \"x\". Alternatively, Horner's method can be computed with \"n\" fused multiply–adds. Horner's method can also be extended to evaluate the first \"k\" derivatives of the polynomial with \"kn\" additions and multiplications.\n\nHorner's method is optimal, in the sense that any algorithm to evaluate an arbitrary polynomial must use at least as many operations. Alexander Ostrowski proved in 1954 that the number of additions required is minimal. Victor Pan proved in 1966 that the number of multiplications is minimal. However, when \"x\" is a matrix, Horner's method is not optimal.\n\nThis assumes that the polynomial is evaluated in monomial form and no preconditioning of the representation is allowed, which makes sense if the polynomial is evaluated only once. However, if preconditioning is allowed and the polynomial is to be evaluated many times, then faster algorithms are possible. They involve a transformation of the representation of the polynomial. In general, a degree-\"n\" polynomial can be evaluated using only +2 multiplications and \"n\" additions.\n\nA disadvantage of Horner's rule is that all of the operations are sequentially dependent, so it is not possible to take advantage of instruction level parallelism on modern computers. In most applications where the efficiency of polynomial evaluation matters, many low-order polynomials are evaluated simultaneously (for each pixel or polygon in computer graphics, or for each grid square in a numerical simulation), so it is not necessary to find parallelism within a single polynomial evaluation.\n\nIf, however, one is evaluating a single polynomial of very high order, it may be useful to break it up as follows:\n\nMore generally, the summation can be broken into \"k\" parts:\nwhere the inner summations may be evaluated using separate parallel instances of Horner's method. This requires slightly more operations than the basic Horner's method, but allows \"k\"-way SIMD execution of most of them.\n\nHorner's method can be modified to compute the divided difference formula_62 Given the polynomial (as before)\n\nproceed as follows\n\nAt completion, we have\nThis computation of the divided difference is subject to less\nround-off error than evaluating formula_51 and formula_67 separately, particularly when\nformula_68. Substituting\nformula_69 in this method gives formula_70, the derivative of formula_51.\n\nHorner's paper entitled \"A new method of solving numerical equations of all orders, by continuous approximation\" was read before the Royal Society of London, at its meeting on July 1, 1819, with Davies Gilbert, Vice-President and Treasurer, in the chair; this was the final meeting of the session before the Society adjorned for its Summer recess. When a sequel was read before the Society in 1823, it was again at the final meeting of the session. On both occasions, papers by James Ivory, FRS, were also read. In 1819, it was Horner's paper that got through to publication in the \"Philosophical Transactions\". later in the year, Ivory's paper falling by the way, despite Ivory being a Fellow; in 1823, when a total of ten papers were read, fortunes as regards publication, were reversed. But Gilbert, who had strong connections with the West of England and may have had social contact with Horner, resident as Horner was in Bristol and Bath, published his own survey of Horner-type methods earlier in 1823.\n\nHorner's paper in Part II of \"Philosophical Transactions of the Royal Society of London\" for 1819 was warmly and expansively welcomed by a reviewer in the issue of \"The Monthly Review: or, Literary Journal\" for April, 1820; in comparison, a technical paper by Charles Babbage is dismissed curtly in this review. However, the reviewer noted that another, similar method had also recently been published by the architect and mathematical expositor, Peter Nicholson. This theme is developed in a further review of some of Nicholson's books in the issue of \"The Monthly Review\" for December, 1820, which in turn ends with notice of the appearance of a booklet by Theophilus Holdred, from whom Nicholson acknowledges he obtained the gist of his approach in the first place, although claiming to have improved upon it. The sequence of reviews is concluded in the issue of \"The Monthly Review\" for September, 1821, with the reviewer concluding that whereas Holdred was the first person to discover a direct and general practical solution of numerical equations, he had not reduced it to its simplest form by the time of Horner's publication, and saying that had Holdred published forty years earlier when he first discovered his method, his contribution could be more easily recognized. The reviewer is exceptionally well-informed, even having sighted Horner's preparatory correspondence with Peter Barlow in 1818, seeking work of Budan. The Bodlean Library, Oxford has the Editor's annotated copy of \"The Monthly Review\" from which it is clear that the most active reviewer in mathematics in 1814 and 1815 (the last years for which this information has been published) was none other than Peter Barlow,one of the foremost specialists on approximation theory of the period, suggesting that it was Barlow, who wrote this sequence of reviews. As it also happened, Henry Atkinson, of Newcastle, devised a similar approximation scheme in 1809; he had consulted his fellow Geordie, Charles Hutton, another specialist and a senior colleague of Barlow at the Royal Military Academy, Woolwich, only to be advised that, while his work was publishable, it was unlikely to have much impact. J. R. Young, writing in the mid-1830s, concluded that Holdred's first method replicated Atkinson's while his improved method was only added to Holdred's booklet some months after its first appearance in 1820, when Horner's paper was already in circulation.\n\nThe feature of Horner's writing that most distinguishes it from his English contemporaries is the way he draws on the Continental literature, notably the work of Arbogast. The advocacy, as well as the detraction, of Horner's Method has this as an unspoken subtext. Quite how he gained that familiarity has not been determined. Horner is known to have made a close reading of John Bonneycastle's book on algebra. Bonneycastle recognizes that Arbogast has the general, combinatorial expression for the reversion of series, a project going back at least to Newton. But Bonneycastle's main purpose in mentioning Arbogast is not to praise him, but to observe that Arbogast's notation is incompatible with the approach he adopts. The gap in Horner's reading was the work of Paolo Ruffini, except that, as far as awareness of Ruffini goes, citations of Ruffini's work by authors, including medical authors, in \"Philosophical Transactions\" speak volumes: there are none - Ruffini's name only appears in 1814, recording a work he donated to the Royal Society. Ruffini might have done better if his work had appeared in French, as had Malfatti's Problem in the reformulation of Joseph Diaz Gergonne, or had he written in French, as had , a source quoted by Bonneycastle on series reversion (today, Cagnoli is in the Italian Wikipedia, as shown, but has yet to make it into either French or English).\n\nFuller showed that the method in Horner's 1819 paper differs from what afterwards became known as 'Horner's method' and that in consequence the priority for this method should go to Holdred (1920). This view may be compared with the remarks concerning the works of Horner and Holdred in the previous paragraph. Fuller also takes aim at Augustus De Morgan. Precocious though Augustus de Morgan was, he was not the reviewer for \"The Monthly Review\", while several others - Thomas Stephens Davies, J. R. Young, Stephen Fenwick, T. T. Wilkinson - wrote Horner firmly into their records, not least Horner himself, as he published extensively up until the year of his death in 1837. His paper in 1819 was one that would have been difficult to miss. In contrast, the only other mathematical sighting of Holdred is a single named contribution to \"The Gentleman's Mathematical Companion\", an answer to a problem.\n\nIt is questionable to what extent it was De Morgan's advocacy of Horner's priority in discovery that led to \"Horner's method\" being so called in textbooks, but it is true that those suggesting this tend themselves to know of Horner largely through intermediaries, of whom De Morgan made himself a prime example. However, this method \"qua\" method was known long before Horner. In reverse chronological order, Horner's method was already known to:\n\n\nHowever, this observation on its own masks significant differences in conception and also, as noted with Ruffini's work, issues of accessibility.\n\nQin Jiushao, in his \"Shu Shu Jiu Zhang\" (\"Mathematical Treatise in Nine Sections\"; 1247), presents a portfolio of methods of Horner-type for solving polynomial equations, which was based on earlier works of the 11th century Song dynasty mathematician Jia Xian; for example, one method is specifically suited to bi-quintics, of which Qin gives an instance, in keeping with the then Chinese custom of case studies. The first person writing in English to note the connection with Horner's method was Alexander Wylie, writing in \"The North China Herald\" in 1852; perhaps conflating and misconstruing different Chinese phrases, Wylie calls the method \"Harmoniously Alternating Evolution\" (which does not agree with his Chinese, \"linglong kaifang\", not that at that date he uses pinyin), working the case of one of Qin's quartics and giving, for comparison, the working with Horner's method. Yoshio Mikami in \"Development of Mathematics in China and Japan\" published in Leipzig in 1913, gave a detailed description of Qin's method, using the quartic illustrated to the above right in a worked example; he wrote: \"who can deny the fact of Horner's illustrious process being used in China at least nearly six long centuries earlier than in Europe ... We of course don't intend in any way to ascribe Horner's invention to a Chinese origin, but the lapse of time sufficiently makes it not altogether impossible that the Europeans could have known of the Chinese method in a direct or indirect way.\". However, as Mikami is also aware, it was \"not altogether impossible\" that a related work, \"Si Yuan Yu Jian\" (\"Jade Mirror of the Four Unknowns; 1303)\" by Zhu Shijie might make the shorter journey across to Japan, but seemingly it never did, although another work of Zhu, \"Suan Xue Qi Meng\", had a seminal influence on the development of traditional mathematics in the Edo period, starting in the mid-1600s. Ulrich Libbrecht (at the time teaching in school, but subsequently a professor of comparative philosophy) gave a detailed description in his doctoral thesis of Qin's method, he concluded: \"It is obvious that this procedure is a Chinese invention...the method was not known in India\". He said, Fibonacci probably learned of it from Arabs, who perhaps borrowed from the Chinese. Here, the problems is that there is no more evidence for this speculation than there is of the method being known in India. Of course, the extraction of square and cube roots along similar lines is already discussed by Liu Hui in connection with Problems IV.16 and 22 in \"Jiu Zhang Suan Shu\", while Wang Xiaotong in the 7th century supposes his readers can solve cubics by an approximation method described in his book Jigu Suanjing.\n\n\n\n"}
{"id": "37466657", "url": "https://en.wikipedia.org/wiki?curid=37466657", "title": "Igusa variety", "text": "Igusa variety\n\nIn mathematics, an Igusa curve is (roughly) a coarse moduli space of elliptic curves in characteristic \"p\" with a level \"p\" Igusa structure, where an Igusa structure on an elliptic curve \"E\" is roughly a point of order \"p\" of \"E\" generating the kernel of \"V\":\"E\" → \"E\". An Igusa variety is a higher-dimensional analogue of an Igusa curve. Igusa curves were studied by and Igusa varieties were introduced by .\n\n"}
{"id": "4116488", "url": "https://en.wikipedia.org/wiki?curid=4116488", "title": "Initial algebra", "text": "Initial algebra\n\nIn mathematics, an initial algebra is an initial object in the category of formula_1-algebras for a given endofunctor formula_1. This initiality provides a general framework for induction and recursion. \n\nFor instance, consider the endofunctor formula_3 on the category of sets, where formula_4 is the one-point (singleton) set, the terminal object in the category. An algebra for this endofunctor is a set formula_5 (called the \"carrier\" of the algebra) together with a point formula_6 and a function formula_7. The set of natural numbers is the carrier of the initial such algebra: the point is zero and the function is the successor map. \n\nFor a second example, consider the endofunctor formula_8 on the category of sets, where formula_9 is the set of natural numbers. An algebra for this endofunctor is a set formula_10 together with a point formula_6 and a function formula_12. The set of finite lists of natural numbers is the initial such algebra. The point is the empty list, and the function is cons, taking a number and a finite list, and returning a new finite list with the number at the head.\n\nIn categories with binary coproducts, the definitions just given are equivalent to the usual definitions of a natural number object and a list object, respectively.\n\nDually, a final coalgebra is a terminal object in the category of formula_1-coalgebras. The finality provides a general framework for coinduction and corecursion.\n\nFor example, using the same functor formula_3 as before, a coalgebra is a set formula_5 together with a truth-valued test function formula_16 and a partial function formula_17 whose domain is formed by those formula_18 for which formula_19. The set formula_20 consisting of the natural numbers extended with a new element formula_21 is the carrier of the final coalgebra in the category, where formula_22 is the test for zero: formula_23 and formula_24, and formula_25 is the predecessor function (the inverse of the successor function) on the positive naturals, but acts like the identity on the new element formula_21: formula_27, formula_28. This set formula_20 that is the carrier of the final coalgebra of formula_3 is known as the set of conatural numbers.\n\nFor a second example, consider the same functor formula_31 as before. In this case the carrier of the final coalgebra consists of all lists of natural numbers, finite as well as infinite. The operations are a test function testing whether a list is empty, and a deconstruction function defined on nonempty lists returning a pair consisting of the head and the tail of the input list.\n\n\nConsider the endofunctor formula_32 sending formula_5 to formula_34. Define\nand\nThen the set formula_9 of natural numbers together with the function formula_38 is an initial formula_1-algebra. The initiality (the universal property for this case) is not hard to establish; the unique homomorphism to an arbitrary formula_1-algebra formula_41, for formula_42 an element of formula_43 and formula_44 a function on formula_43, is the function sending the natural number formula_46 to formula_47, that is, formula_48, the formula_46-fold application of formula_25 to formula_51.\n\nVarious finite data structures used in programming, such as lists and trees, can be obtained as initial algebras of specific endofunctors.\nWhile there may be several initial algebras for a given endofunctor, they are unique up to isomorphism, which informally means that the \"observable\" properties of a data structure can be adequately captured by defining it as an initial algebra.\n\nTo obtain the type formula_52 of lists whose elements are members of set formula_43, consider that the list-forming operations are:\n\n\nCombined into one function, they give:\n\n\nwhich makes this an formula_1-algebra for the endofunctor formula_1 sending formula_5 to formula_60. It is, in fact, \"the\" initial formula_1-algebra. Initiality is established by the function known as \"foldr\" in functional programming languages such as Haskell and ML.\n\nLikewise, binary trees with elements at the leaves can be obtained as the initial algebra\n\n\nTypes obtained this way are known as algebraic data types.\n\nTypes defined by using least fixed point construct with functor formula_1 can be regarded as an initial formula_1-algebra, provided that parametricity holds for the type.\n\nIn a dual way, similar relationship exists between notions of greatest fixed point and terminal F-coalgebra, with applications to coinductive types. These can be used for allowing potentially infinite objects while maintaining strong normalization property. In the strongly normalizing Charity programming language (i.e. each program terminates), coinductive data types can be used achieving surprising results, e.g. defining lookup constructs to implement such “strong” functions like the Ackermann function.\n\n\n"}
{"id": "45323078", "url": "https://en.wikipedia.org/wiki?curid=45323078", "title": "Jason Behrstock", "text": "Jason Behrstock\n\nJason Behrstock is a mathematician at City University of New York known for his work in geometric group theory and low-dimensional topology.\n\nBehrstock was born in California and was educated in California's public school system. He received his Ph.D. from State University of New York at Stony Brook in 2004. He went to work at Columbia University and the University of Utah before his time at Lehman College, City University of New York.\n\n\n"}
{"id": "44357173", "url": "https://en.wikipedia.org/wiki?curid=44357173", "title": "Kaplan–Yorke conjecture", "text": "Kaplan–Yorke conjecture\n\nIn applied mathematics, the Kaplan–Yorke conjecture concerns the dimension of an attractor, using Lyapunov exponents. By arranging the Lyapunov exponents in order from largest to smallest formula_1, let \"j\" be the index for which\n\nand\n\nThen the conjecture is that the dimension of the attractor is\n\nEspecially for chaotic systems, the Kaplan–Yorke conjecture is a useful tool in order to determine the fractal dimension of the corresponding attractor.\n\n\n"}
{"id": "37704906", "url": "https://en.wikipedia.org/wiki?curid=37704906", "title": "Koopman–von Neumann classical mechanics", "text": "Koopman–von Neumann classical mechanics\n\nThe Koopman–von Neumann mechanics is a description of classical mechanics in terms of Hilbert space, introduced by Bernard Koopman and John von Neumann in 1931 and 1932.\n\nAs Koopman and von Neumann demonstrated, a Hilbert space of complex, square integrable wavefunctions can be defined in which classical mechanics can be formulated as an operatorial theory similar to quantum mechanics.\n\nStatistical mechanics describes macroscopic systems in terms of statistical ensembles, such as the macroscopic properties of an ideal gas. Ergodic theory is a branch of mathematics arising from the study of statistical mechanics.\n\nThe origins of Koopman–von Neumann (KvN) theory are tightly connected with the rise of ergodic theory as an independent branch of mathematics, in particular with Boltzmann's ergodic hypothesis.\n\nIn 1931 Koopman and André Weil independently observed that the phase space of the classical system can be converted into a Hilbert space by postulating a natural integration rule over the points of the phase space as the definition of the scalar product, and that this transformation allows drawing of interesting conclusions about the evolution of physical observables from Stone's theorem, which had been proved shortly before. This finding inspired von Neumann to apply the novel formalism to the ergodic problem. Already in 1932 he completed the operator reformulation of quantum mechanics currently known as Koopman–von Neumann theory. Subsequently, he published several seminal results in modern ergodic theory including the proof of his mean ergodic theorem\".\n\nIn the approach of Koopman and von Neumann (KvN), dynamics in phase space is described by a (classical) probability density, recovered from an underlying wavefunction – the Koopman–von Neumann wavefunction – as the square of its absolute value (more precisely, as the amplitude multiplied with its own complex conjugate). This stands in analogy to the Born rule in quantum mechanics. In the KvN framework, observables are represented by commuting self-adjoint operators acting on the Hilbert space of KvN wavefunctions. The commutativity physically implies that all observables are simultaneously measurable. Contrast this with quantum mechanics, where observables need not commute, which underlines the uncertainty principle, Kochen–Specker theorem, and Bell inequalities.\n\nThe KvN wavefunction is postulated to evolve according to exactly the same Liouville equation as the classical probability density. From this postulate it can be shown that indeed probability density dynamics is recovered.\n\nConversely, it is possible to start from operator postulates, similar to the Hilbert space axioms of quantum mechanics, and derive the equation of motion by specifying how expectation values evolve.\n\nThe relevant axioms are that as in quantum mechanics (i) the states of a system are represented by normalized vectors of a complex Hilbert space, and the observables are given by self-adjoint operators acting on that space, (ii) the expectation value of an observable is obtained in the manner as the expectation value in quantum mechanics, (iii) the probabilities of measuring certain values of some observables are calculated by the Born rule, and (iv) the state space of a composite system is the tensor product of the subsystem's spaces.\n\nIn the Hilbert space and operator formulation of classical mechanics, the Koopman von Neumann–wavefunction takes the form of a superposition of eigenstates, and measurement collapses the KvN wavefunction to the eigenstate which is associated the measurement result, in analogy to the wave function collapse of quantum mechanics.\n\nHowever, it can be shown that for Koopman–von Neumann classical mechanics \"non-selective measurements\" leave the KvN wavefunction unchanged.\n\nThe KvN dynamical equation () and Liouville equation () are first-order linear partial differential equations. One recovers Newton's laws of motion by applying the method of characteristics to either of these equations. Hence, the key difference between KvN and Liouville mechanics lies in weighting individual trajectories: Arbitrary weights, underlying the classical wave function, can be utilized in the KvN mechanics, while only positive weights, representing the probability density, are permitted in the Liouville mechanics (see this scheme).\n\nBeing explicitly based on the Hilbert space language, the KvN classical mechanics adopts many techniques from quantum mechanics, for example, perturbation and diagram techniques as well as functional integral methods. The KvN approach is very general, and it has been extended to dissipative systems, relativistic mechanics, and classical field theories.\n\nThe KvN approach is fruitful in studies on the quantum-classical correspondence as it reveals that the Hilbert space formulation is not exclusively quantum mechanical. Even Dirac spinors are not exceptionally quantum as they are utilized in the relativistic generalization of the KvN mechanics. Similarly as the more well-known phase space formulation of quantum mechanics, the KvN approach can be understood as an attempt to bring classical and quantum mechanics into a common mathematical framework. In fact, the time evolution of the Wigner function approaches, in the classical limit, the time evolution of the KvN wavefunction of a classical particle. However, a mathematical resemblance to quantum mechanics does not imply the presence of hallmark quantum effects. In particular, impossibility of double-slit experiment and Aharonov–Bohm effect are explicitly demonstrated in the KvN framework.\n\n\n"}
{"id": "19287170", "url": "https://en.wikipedia.org/wiki?curid=19287170", "title": "Középiskolai Matematikai és Fizikai Lapok", "text": "Középiskolai Matematikai és Fizikai Lapok\n\nKözépiskolai Matematikai és Fizikai Lapok [\"Mathematical and Physical Journal for Secondary Schools\"] (KöMaL) is a Hungarian mathematics and physics journal for high school students. It was founded by Dániel Arany, a high school teacher from Győr, Hungary and has been continually published since 1893.\n\nKöMaL has been organizing various renowned correspondence competitions for high school students, making a major contribution to Hungarian high school education. Since the early 1970s, all of the problems in the \"KöMaL\" journal have been translated into English; published solutions, however, are not typically translated. A 100-year archive of issues is provided online.\n\nThe journal has been a source of inspiration for the United States of America Mathematical Talent Search.\n\n"}
{"id": "8031241", "url": "https://en.wikipedia.org/wiki?curid=8031241", "title": "Language equation", "text": "Language equation\n\nLanguage equations are mathematical statements that resemble numerical equations, but the variables assume values of formal languages rather than numbers. Instead of arithmetic operations in numerical equations, the variables are joined by language operations. Among the most common operations on two languages \"A\" and \"B\" are the set union \"A\" ∪ \"B\", the set intersection \"A\" ∩ \"B\", and the concatenation \"A\"⋅\"B\". Finally, as an operation taking a single operand, the set \"A\" denotes the Kleene star of the language \"A\". Therefore language equations can be used to represent formal grammars, since the languages generated by the grammar must be the solution of a system of language equations.\n\nGinsburg and Rice\ngave an alternative definition of context-free grammars by language equations. To every context-free grammar formula_1, is associated a system of equations in variables formula_2. Each variable formula_3 is an unknown language over formula_4 and is defined by equation formula_5 where formula_6, ..., formula_7 are all productions for formula_8. Ginsburg and Rice used a fixed-point iteration argument to show that a solution always exists, and proved that the assignment formula_9 is the \"least solution\" to this system, i.e. any other solution must be a componentwise subset of this one.\n\nLanguage equations with added intersection analogously correspond to conjunctive grammars.\n\nBrzozowski and Leiss studied \"left language equations\" where every concatenation is with a singleton constant language on the left, e.g. formula_10 with variable formula_8, but not formula_12 nor formula_13. Each equation is of the form formula_14 with one variable on the right-hand side. Every nondeterministic finite automaton has such corresponding equation using left-concatenation and union, see Fig. 1. If intersection operation is allowed, equations correspond to alternating finite automata.\n\nBaader and Narendran studied equations formula_15 using left-concatenation and union and proved that their satisfiability problem is EXPTIME-complete.\n\nConway proposed the following problem: given a constant finite language formula_16, is the greatest solution of equation formula_17 always regular? This problem was studied by Karhumäki and Petre who gave an affirmative answer in a special case. A strongly negative answer to Conway's problem was given by Kunc who constructed a finite language formula_16 such that the greatest solution of this equation is not recursively enumerable.\n\nKunc also proved that the greatest solution of inequality formula_19 is always regular.\n\nLanguage equations with concatenation and Boolean operations were first studied by Parikh, Chandra, Halpern and Meyer\n\nFor a one-letter alphabet, Leiss discovered the first language equation with a nonregular solution, using complementation and concatenation operations. Later, Jeż showed that non-regular unary languages can be defined by language equations with union, intersection and concatenation, equivalent to conjunctive grammars. By this method Jeż and Okhotin proved that every recursive unary language is a unique solution of some equation.\n\n\n"}
{"id": "45603435", "url": "https://en.wikipedia.org/wiki?curid=45603435", "title": "Lie algebra extension", "text": "Lie algebra extension\n\nIn the theory of Lie groups, Lie algebras and their representation theory, a Lie algebra extension is an enlargement of a given Lie algebra by another Lie algebra . Extensions arise in several ways. There is the trivial extension obtained by taking a direct sum of two Lie algebras. Other types are the split extension and the central extension. Extensions may arise naturally, for instance, when forming a Lie algebra from projective group representations. Such a Lie algebra will contain central charges.\n\nStarting with a polynomial loop algebra over finite-dimensional simple Lie algebra and performing two extensions, a central extension and an extension by a derivation, one obtains a Lie algebra which is isomorphic with an untwisted affine Kac–Moody algebra. Using the centrally extended loop algebra one may construct a current algebra in two spacetime dimensions. The Virasoro algebra is the universal central extension of the Witt algebra.\n\nCentral extensions are needed in physics, because the symmetry group of a quantized system usually is a central extension of the classical symmetry group, and in the same way the corresponding symmetry Lie algebra of the quantum system is, in general, a central extension of the classical symmetry algebra. Kac–Moody algebras have been conjectured to be symmetry groups of a unified superstring theory. The centrally extended Lie algebras play a dominant role in quantum field theory, particularly in conformal field theory, string theory and in M-theory.\n\nA large portion towards the end is devoted to background material for applications of Lie algebra extensions, both in mathematics and in physics, in areas where they are actually useful. A parenthetical link, (background material), is provided where it might be beneficial.\n\nDue to the Lie correspondence, the theory, and consequently the history of Lie algebra extensions, is tightly linked to the theory and history of group extensions. A systematic study of group extensions was performed by the Austrian mathematician Otto Schreier in 1923 in his PhD. thesis and later published. The problem posed for his thesis by Otto Hölder was \"given two groups and , find all groups having a normal subgroup isomorphic to such that the factor group is isomorphic to \".\n\nLie algebra extensions are most interesting and useful for infinite-dimensional Lie algebras. In 1967, Victor Kac and Robert Moody independently generalized the notion of classical Lie algebras, resulting in a new theory of infinite-dimensional Lie algebras, now called Kac–Moody algebras. They generalize the finite-dimensional simple Lie algebras and can often concretely be constructed as extensions.\n\nNotational abuse to be found below includes for the exponential map given an argument, writing for the element in a direct product ( is the identity in ), and analogously for Lie algebra direct sums (where also and are used interchangeably). Likewise for semidirect products and semidirect sums. Canonical injections (both for groups and Lie algebras) are used for implicit identifications. Furthermore, if , , ..., are groups, then the default names for elements of , , ..., are , , ..., and their Lie algebras are , , ... . The default names for elements of , , ..., are , , ... (just like for the groups!), partly to save scarce alphabetical resources but mostly to have a uniform notation.\n\nLie algebras that are ingredients in an extension will, without comment, be taken to be over the same field.\n\nThe summation convention applies, including sometimes when the indices involved are both upstairs or both downstairs.\n\nCaveat: Not all proofs and proof outlines below have universal validity. The main reason is that the Lie algebras are often infinite-dimensional, and then there may or may not be a Lie group corresponding to the Lie algebra. Moreover, even if such a group exists, it may not have the \"usual\" properties, e.g. the exponential map might not exist, and if it does, it might not have all the \"usual\" properties. In such cases, it is questionable whether the group should be endowed with the \"Lie\" qualifier. The literature is not uniform. For the explicit examples, the relevant structures are supposedly in place.\n\nLie algebra extensions are formalized in terms of short exact sequences. A short exact sequence is an exact sequence of length three,\n\nsuch that is a monomorphism, is an epimorphism, and . From these properties of exact sequences, it follows that (the image of) is an ideal in . Moreover, \nbut it is not necessarily the case that is isomorphic to a subalgebra of . This construction mirrors the analogous constructions in the closely related concept of group extensions.\n\nIf the situation in prevails, non-trivially and for Lie algebras over the same field, then one says that is an extension of by .\n\nThe defining property may be reformulated. The Lie algebra is an extension of by if\n\nis exact. Here the zeros on the ends represent the zero Lie algebra (containing the null vector only) and the maps are the obvious ones; maps to and maps all elements of to . With this definition, it follows automatically that is a monomorphism and is an epimorphism.\n\nAn extension of by is not necessarily unique. Let denote two extensions and let the primes below have the obvious interpretation. Then, if there exists a Lie algebra isomorphism such that\n\nthen the extensions and are said to be equivalent extensions. Equivalence of extensions is an equivalence relation.\n\nA Lie algebra extension\nis trivial if there is a subspace such that and is an ideal in .\n\nA Lie algebra extension\nis split if there is a subspace such that as a vector space and is a subalgebra in .\n\nAn ideal is a subalgebra, but a subalgebra is not necessarily and ideal. A trivial extension is thus a split extension.\n\nCentral extensions of a Lie algebra by an abelian Lie algebra can be obtained with the help of a so-called (nontrivial) 2-cocycle (background) on . Non-trivial 2-cocycles occur in the context of projective representations (background) of Lie groups. This is alluded to further down.\n\nA Lie algebra extension\nis a central extension if is contained in the center of .\n\nProperties\n\nThe map satisfies\nTo see this, use the definition of on the left hand side, then use the linearity of . Use Jacobi identity on to get rid of half of the six terms. Use the definition of again on terms sitting inside three Lie brackets, bilinearity of Lie brackets, and the Jacobi identity on , and then finally use on the three remaining terms that and that so that brackets to zero with everything.\nIt then follows that satisfies the corresponding relation, and if in addition is one-dimensional, then is a 2-cocycle on (via a trivial correspondence of with the underlying field).\n\nA central extension\nis universal if for every other central extension\nthere exist \"unique\" homomorphisms formula_10 and formula_11 such that the diagram\n\ncommutes, i.e. and . By universality, it is easy to conclude that such universal central extensions are unique up to isomorphism.\n\nLet be Lie algebras over the same field . Define\nand define addition pointwise on . Scalar multiplication is defined by\nWith these definitions, is a vector space over . With the Lie bracket\n\nIt is clear that holds as an exact sequence. This extension of by is called a trivial extension. It is, of course, nothing else than the Lie algebra direct sum. By symmetry of definitions, is an extension of by as well, but . It is clear from that the subalgebra is an ideal (Lie algebra). This property of the direct sum of Lie algebras is promoted to the definition of a trivial extension.\n\nInspired by the construction of a semidirect product (background) of groups using a homomorphism , one can make the corresponding construct for Lie algebras.\n\nIf is a Lie algebra homomorphism, then define a Lie bracket on by\n\nWith this Lie bracket, the Lie algebra so obtained is denoted and is called the semidirect sum of and .\n\nBy inspection of one sees that is a subalgebra of and is an ideal in . Define by and by . It is clear that . Thus is a Lie algebra extension of by .\n\nAs with the trivial extension, this property generalizes to the definition of a split extension.\n\nExample\nLet be the Lorentz group and let denote the translation group in 4 dimensions, isomorphic to , and consider the multiplication rule of the Poincaré group \n(where and are identified with their images in ). From it follows immediately that, in the Poincaré group, . Thus every Lorentz transformation corresponds to an automorphism of with inverse and is clearly a homomorphism. Now define\nendowed with multiplication given by . Unwinding the definitions one finds that the multiplication is the same as the multiplication one started with and it follows that . From follows that and then from it follows that .\n\nLet be a derivation (background) of and denote by the one-dimensional Lie algebra spanned by . Define the Lie bracket on by\n\nIt is obvious from the definition of the bracket that is and ideal in in and that is a subalgebra of . Furthermore, is complementary to in . Let be given by and by . It is clear that . Thus is a split extension of by . Such an extension is called extension by a derivation.\n\nIf is defined by , then is a Lie algebra homomorphism into . Hence this construction is a special case of a semidirect sum, for when starting from and using the construction in the preceding section, the same Lie brackets result.\n\nIf is a 2-cocycle (background) on a Lie algebra and is any one-dimensional vector space, let (vector space direct sum) and define a Lie bracket on by\n\nHere is an arbitrary but fixed element of . Antisymmetry follows from antisymmetry of the Lie bracket on and antisymmetry of the 2-cocycle. The Jacobi identity follows from the corresponding properties of and of . Thus is a Lie algebra. Put and it follows that . Also, it follows with and that . Hence is a central extension of by . It is called extension by a 2-cocycle.\n\nBelow follows some results regarding central extensions and 2-cocycles.\n\nTheorem\nLet and be cohomologous 2-cocycles on a Lie algebra and let and be respectively the central extensions constructed with these 2-cocycles. Then the central extensions and are equivalent extensions.\nProof\nBy definition, . Define\nIt follows from the definitions that is a Lie algebra isomorphism and holds.\n\nCorollary\nA cohomology class defines a central extension of which is unique up to isomorphism.\n\nThe trivial 2-cocycle gives the trivial extension, and since a 2-coboundary is cohomologous with the trivial 2-cocycle, one has \nCorollary\nA central extension defined by a coboundary is equivalent with a trivial central extension.\n\nTheorem<br>\nA finite-dimensional simple Lie algebra has only trivial central extensions.<br>\nProof<br>\nSince every central extension comes from a 2-cocycle , it suffices to show that every 2-cocycle is a coboundary. Suppose is a 2-cocycle on . The task is to use this 2-cocycle to manufacture a 1-cochain such that .\n\nThe first step is to for each use to define a linear map . But the linear maps are elements of . This suffices to express in terms of , using the isomorphism . Next, a linear map is defined that turns out to be a derivation. Since all derivations are inner, one has for some . An expression for in terms of and is obtained. Thus set, trusting that is a derivation,\n\nLet be the 1-cochain defined by\nThen\nshowing that is a coboundary. By the previous results, any central extension is trivial.\n\nTo verify that actually is a derivation, first note that it is linear since is, then compute\nBy appeal to the non-degeneracy of , the left arguments of are equal on the far left and far right.\n\nThe observation that one can define a derivation , given a symmetric non-degenerate associative form and a 2-cocycle , by\nor using the symmetry of and the antisymmetry of ,\nleads to a corollary.\n\nCorollary<br>\nLet be a non-degenerate symmetric associative bilinear form and let be a derivation satisfying\nthen defined by\nis a 2-cocycle.\n\nProof\nThe condition on ensures the antisymmetry of . The Jacobi identity for 2-cocycles follows starting with\nusing symmetry of the form, the antisymmetry of the bracket, and once again the definition of in terms of .\n\nIf is the Lie algebra of a Lie group and is a central extension of , one may ask whether there is a Lie group with Lie algebra . The answer is, by Lie's third theorem affirmative. But is there a \"central extension\" of with Lie algebra ? The answer to this question requires some machinery, and can be found in .\n\nThe \"negative\" result of the preceding theorem indicates that one must, at least for semisimple Lie algebras, go to infinite-dimensional Lie algebras to find useful applications of central extensions. There are indeed such. Here will be presented affine Kac–Moody algebras and Virasoro algebras. These are extensions of polynomial loop-algebras and the Witt algebra respectively.\n\nLet be a polynomial loop algebra (background),\nwhere is a complex finite-dimensional simple Lie algebra. The goal is to find a central extension of this algebra. Two of the theorems apply. On the one hand, if there is a 2-cocycle on , then a central extension may be defined. On the other hand, if this 2-cocycle is acting on the part (only), then the resulting extension is trivial. Moreover, derivations acting on (only) cannot be used for definition of a 2-cocycle either because these derivations are all inner and the same problem results. One therefore looks for derivations on . One such set of derivations is \n\nIn order to manufacture a non-degenerate bilinear associative antisymmetric form on , attention is focused first on restrictions on the arguments, with fixed. It is a theorem that \"every\" form satisfying the requirements is a multiple of the Killing form on . This requires\nSymmetry of implies\nand associativity yields\nWith one sees that . This last condition implies the former. Using this fact, define . The defining equation then becomes\nFor every the definition\ndoes define a symmetric associative bilinear form\nBut these forms the basis of a vector space in which every form has the right properties.\n\nReturning to the derivations at hand and the condition\none sees, using the definitions, that\nor, with ,\nThis (and the antisymmetry condition) holds if , in particular it holds when .\n\nThus chose and . With these choices, the premises in the corollary are satisfied. The 2-cocycle defined by\nis finally employed to define a central extension of ,\nwith Lie bracket\nFor basis elements, suitably normalized and with antisymmetric structure constants, one has\nThis is a universal central extension of the polynomial loop algebra.\n\nA note on terminology\nIn physics terminology, the algebra of above might pass for a Kac–Moody algebra, whilst it will probably not in mathematics terminology. An additional dimension, an extension by a derivation is required for this. Nonetheless, if, in a physical application, the eigenvalues of or its representative are interpreted as (ordinary) quantum numbers, the additional superscript on the generators is referred to as the level. It is an additional quantum number. An additional operator whose eigenvalues are precisely the levels is introduced further below.\n\nAs an application of a central extension of polynomial loop algebra, a current algebra of a quantum field theory is considered (background). Suppose one has a current algebra, with the interesting commutator being\nwith a Schwinger term. To construct this algebra mathematically, let be the centrally extended polynomial loop algebra of the previous section with\nas one of the commutation relations, or, with a switch of notation () with a factor of under the physics convention,\n\nDefine using elements of ,\nOne notes that\nso that it is defined on a circle. Now compute the commutator,\n\nFor simplicity, switch coordinates so that and use the commutation relations,\n\nNow employ the Poisson summation formula,\nfor in the interval and differentiate it to yield\nand finally\nor\nsince the delta functions arguments only ensure that the arguments of the left and right arguments of the commutator are equal (formally ).\n\nBy comparison with , this is a current algebra in two spacetime dimensions, \"including a Schwinger term\", with the space dimension curled up into a circle. In the classical setting of quantum field theory, this is perhaps of little use, but with the advent of string theory where fields live on world sheets of strings, and spatial dimensions are curled up, there may be relevant applications.\n\nThe derivation used in the construction of the 2-cocycle in the previous section can be extended to a derivation on the centrally extended polynomial loop algebra, here denoted by in order to realize a Kac–Moody algebra (background). Simply set\nNext, define as a vector space\nThe Lie bracket on is, according to the standard construction with a derivation, given on a basis by\n\nFor convenience, define\nIn addition, assume the basis on the underlying finite-dimensional simple Lie algebra has been chosen so that the structure coefficients are antisymmetric in all indices and that the basis is appropriately normalized. Then one immediately through the definitions verifies the following commutation relations.\n\nThese are precisely the short-hand description of an untwisted affine Kac–Moody algebra. To recapitulate, begin with a finite-dimensional simple Lie algebra. Define a space of formal Laurent polynomials with coefficients in the finite-dimensional simple Lie algebra. With the support of a symmetric non-degenerate alternating bilinear form and a derivation, a 2-cocycle is defined, subsequently used in the standard prescription for a central extension by a 2-cocycle. Extend the derivation to this new space, use the standard prescription for a split extension by a derivation and an untwisted affine Kac–Moody algebra obtains.\n\nThe purpose is to construct the Virasoro algebra, due to Miguel Angel Virasoro, as a central extension by a 2-cocycle of the Witt algebra (background). The Jacobi identity for 2-cocycles yields\n\nLetting and using antisymmetry of one obtains\n\nIn the extension, the commutation relations for the element are\nIt is desirable to get rid of the central charge on the right hand side. To do this define\nThen, using as a 1-cochain, \nso with this 2-cocycle, equivalent to the previous one, one has\nWith this new 2-cocycle (skip the prime) the condition becomes\nand thus\nwhere the last condition is due to the antisymmetry of the Lie bracket. With this, and with (cutting out a \"plane\" in ), yields\n\nthat with (cutting out a \"line\" in ) becomes\nThis is a difference equation generally solved by\nThe commutator in the extension on elements of is then\nWith it is possible to change basis (or modify the 2-cocycle by a 2-coboundary) so that\nwith the central charge absent altogether, and the extension is hence trivial. (This was not (generally) the case with the previous modification, where only obtained the original relations.) With the following change of basis,\nthe commutation relations take the form\nshowing that the part linear in is trivial. It also shows that is one-dimensional (corresponding to the choice of ). The conventional choice is to take and still retaining freedom by absorbing an arbitrary factor in the arbitrary object . The Virasoro algebra is then\nwith commutation relations\n\nThe relativistic classical open string (background) is subject to quantization. This roughly amounts to taking the position and the momentum of the string and promoting them to operators on the space of states of open strings. Since strings are extended objects, this results in a continuum of operators depending on the parameter . The following commutation relations are postulated in the Heisenberg picture.\nAll other commutators vanish.\n\nBecause of the continuum of operators, and because of the delta functions, it is desirable to express these relations instead in terms of the quantized versions of the Virasoro modes, the Virasoro operators. These are calculated to satisfy\nThey are interpreted as creation and annihilation operators acting on Hilbert space, increasing or decreasing the quantum of their respective modes. If the index is negative, the operator is a creation operator, otherwise it is an annihilation operator. (If it is zero, it is proportional to the total momentum operator.) In view of the fact that the light cone plus and minus modes were expressed in terms of the transverse Virasoro modes, one must consider the commutation relations between the Virasoro operators. These were classically defined (then modes) as\n\nSince, in the quantized theory, the alphas are operators, the ordering of the factors matter. In view of the commutation relation between the mode operators, it will only matter for the operator (for which ). is chosen normal ordered,\nwhere is a possible ordering constant. One obtains after a somewhat lengthy calculation the relations\nIf one would allow for above, then one has precisely the commutation relations of the Witt algebra. Instead one has\nupon identification of the generic central term as times the identity operator, this is the Virasoro algebra, the universal central extension of the Witt algebra.\n\nThe operator enters the theory as the Hamiltonian, modulo an additive constant. Moreover, the Virasoro operators enter into the definition of the Lorentz generators of the theory. It is perhaps the most important algebra in string theory. The consistency of the Lorentz generators, by the way, fixes the spacetime dimensionality to 26. While this theory presented here (for relative simplicity of exposition) is unphysical, or at the very least incomplete (it has, for instance, no fermions) the Virasoro algebra arises in the same way in the more viable superstring theory and M-theory.\n\nA projective representation of a Lie group (background) can be used to define a so-called group extension .\n\nIn quantum mechanics, Wigner's theorem asserts that if is a symmetry group, then it will be represented projectively on Hilbert space by unitary or antiunitary operators. This is often dealt with by passing to the universal covering group of and take it as the symmetry group. This works nicely for the rotation group and the Lorentz group , but it does not work when the symmetry group is the Galilean group. In this case one has to pass to its central extension, the Bargmann group, which is the symmetry group of the Schrödinger equation. Likewise, if , the group of translations in position and momentum space, one has to pass to its central extension, the Heisenberg group.\n\nLet be the 2-cocycle on induced by . Define\nas a set and let the multiplication be defined by\nAssociativity holds since is a 2-cocycle on . One has for the unit element\nand for the inverse\n\nThe set is an abelian subgroup of . This means that is not semisimple. The center of , includes this subgroup. The center may be larger.\n\nAt the level of Lie algebras it can be shown that the Lie algebra of is given by\nas a vector space and endowed with the Lie bracket\nHere is a 2-cocycle on . This 2-cocycle can be obtained from albeit in a highly nontrivial way.\n\nNow by using the projective representation one may define a map by\nIt has the properties\nso is a bona fide representation of .\n\nIn the context of Wigner's theorem, the situation may be depicted as such (replace by ); let denote the unit sphere in Hilbert space , and let be its inner product. Let denote ray space and the ray product. Let moreover a wiggly arrow denote a group action. Then the diagram\n\ncommutes, i.e.\nMoreover, in the same way that is a symmetry of preserving , is a symmetry of preserving . The fibers of are all circles. These circles are left invariant under the action of . The action of on these fibers is transitive with no fixed point. The conclusion is that is a principal fiber bundle over with structure group .\nIn order to adequately discuss extensions, structure that goes beyond the defining properties of a Lie algebra is needed. Rudimentary facts about these are collected here for quick reference.\n\nA derivation on a Lie algebra is a map\nsuch that the Leibniz rule\nholds. The set of derivations on a Lie algebra is denoted . It is itself a Lie algebra under the Lie bracket\nIt is the Lie algebra of the group of automorphisms of . One has to show\nIf the rhs holds, differentiate and set implying that the lhs holds. If the lhs holds , write the rhs as\nand differentiate the rhs of this expression. It is, using , identically zero. Hence the rhs of this expression is independent of and equals its value for , which is the lhs of this expression.\n\nIf , then , acting by , is a derivation. The set is the set of inner derivations on . For finite-dimensional simple Lie algebras all derivations are inner derivations.\n\nConsider two Lie groups and and , the automorphism group of . The latter is the group of isomorphisms of . If there is a Lie group homomorphism , then for each there is a with the property . Denote with the \"set\" and define multiplication by \n\nThen is a group with identity and the inverse is given by . Using the expression for the inverse and equation it is seen that is normal in . Denote the group with this semidirect product as .\n\nConversely, if is a given semidirect product expression of the group , then by definition is normal in and for each where and the map is a homomorphism.\n\nNow make use of the Lie correspondence. The maps each induce, at the level of Lie algebras, a map . This map is computed by\n\nFor instance, if and are both subgroups of a larger group and , then \n\nand one recognizes as the adjoint action of on restricted to . Now <nowiki>[</nowiki> if is finite-dimensional<nowiki>]</nowiki> is a homomorphism, and appealing once more to the Lie correspondence, there is a unique Lie algebra homomorphism . This map is (formally) given by\nfor example, if , then (formally)\nwhere a relationship between and the adjoint action rigorously proved in here is used.\n\nLie algebra<br>\nThe Lie algebra is, as a vector space, . This is clear since generates and . The Lie bracket is given by\n\nTo compute the Lie bracket, begin with a surface in parametrized by and . Elements of in are decorated with a bar, and likewise for .\n\nOne has \nand\nby and thus\n\nNow differentiate this relationship with respect to and evaluate at $:\nand\nby and thus\n\nFor the present purposes, consideration of a limited portion of the theory Lie algebra cohomology suffices. The definitions are not the most general possible, or even the most common ones, but the objects they refer to are authentic instances of more the general definitions.\n\n2-cocycles<br>\nThe objects of primary interest are the 2-cocycles on , defined as bilinear alternating functions, \nthat are alternating,\nand having a property resembling the Jacobi identity called the Jacobi identity for 2-cycles,\n\nThe set of all 2-cocycles on is denoted .\n\n2-cocycles from 1-cochains<br>\nSome 2-cocycles can be obtained from 1-cochains. A 1-cochain on is simply a linear map,\nThe set of all such maps is denoted and, of course (in at least the finite-dimensional case) . Using a 1-cochain , a 2-cocycle may be defined by\nThe alternating property is immediate and the Jacobi identity for 2-cocycles is (as usual) shown by writing it out and using the definition and properties of the ingredients (here the Jacobi identity on and the linearity of ). The linear map is called the coboundary operator (here restricted to ).\n\nThe second cohomology group<br>\nDenote the image of of by . The quotient\nis called the second cohomology group of . Elements of are equivalence classes of 2-cocycles and two\n2-cocycles and are called equivalent cocycles if they differ by a 2-coboundary, i.e. if for some . Equivalent\n2-cocycles are called cohomologous. The equivalence class of is denoted .\n\nThese notions generalize in several directions. For this, see the main articles.\n\nLet be a Hamel basis for . Then each has a unique expression as\nfor some indexing set of suitable size. In this expansion, only finitely many are nonzero. In the sequel it is (for simplicity) assumed that the basis is countable, and Latin letters are used for the indices and the indexing set can be taken to be . One immediately has\nfor the basis elements, where the summation symbol has been rationalized away, the summation convention applies. The placement of the indices in the structure constants (up or down) is immaterial. The following theorem is useful:\n\nTheorem:There is a basis such that the structure constants are antisymmetric in all indices if and only if the Lie algebra is a direct sum of simple compact Lie algebras and Lie algebras. This is the case if and only if there is a real positive definite metric on satisfying the invariance condition\nin any basis. This last condition is necessary on physical grounds for non-Abelian gauge theories in quantum field theory. Thus one can produce an infinite list of possible gauge theories using the Cartan catalog of simple Lie algebras on their compact form (i.e., , etc. One such gauge theory is the gauge theory of the standard model with Lie algebra .\n\nThe Killing form is a symmetric bilinear form on defined by\nHere is viewed as a matrix operating on the vector space . The key fact needed is that if is semisimple, then, by Cartan's criterion, is non-degenerate. In such a case may be used to identify and . If , then there is a such that\nThis is resembling Riesz representation theorem and the proof is virtually the same. The Killing form has the property\nwhich is referred to as associativity. By defining and expanding the inner brackets in terms of structure constants, one finds that the Killing form satisfies the invariance condition of above.\n\nA loop group is taken as a group of smooth maps from the unit circle into a Lie group with the group structure defined by the group structure on . The Lie algebra of a loop group is then a vector space of mappings from into the Lie algebra of . Any subalgebra of such a Lie algebra is referred to as a loop algebra. Attention here is focused on polynomial loop algebras of the form\nTo see this, consider elements near the identity in for in the loop group, expressed in a basis for \nwhere the are real and small and the implicit sum is over the dimension of .\nNow write\nto obtain\nThus the functions\nconstitute the Lie algebra.\nA little thought confirms that these are loops in as goes from to . The operations are the ones defined pointwise by the operations in . This algebra is isomorphic with the algebra\nwhere is the algebra of Laurent polynomials,\nThe Lie bracket is\nIn this latter view the elements can be considered as polynomials with (constant!) coefficients in . In terms of a basis and structure constants,\n\nIt is also common to have a different notation,\nwhere the omission of should be kept in mind to avoid confusion; the elements really are functions . The Lie bracket is then\n^kT^{m+n}_k,</math>\nwhich is recognizable as one of the commutation relations in an untwisted affine Kac–Moody algebra, to be introduced later, \"without\" the central term. With , a subalgebra isomorphic to is obtained. It generates (as seen by tracing backwards in the definitions) the set of constant maps from into , which is obviously isomorphic with when is onto (which is the case when is compact. If is compact, then a basis for may be chosen such that the are skew-Hermitian. As a consequence, \nSuch a representation is called unitary because the representatives\nare unitary. Here, the minus on the lower index of is conventional, the summation convention applies, and the is (by the definition) buried in the s in the right hand side.\n\nCurrent algebras arise in quantum field theories as a consequence of global gauge symmetry. Conserved currents occur in classical field theories whenever the Lagrangian respects a continuous symmetry. This is the content of Noether's theorem. Most (perhaps all) modern quantum field theories can be formulated in terns of classical Lagrangians (prior to quantization), so Noether's theorem applies in the quantum case as well. Upon quantization, the conserved currents are promoted to position dependent operators on Hilbert space. These operators are subject to commutation relations, generally forming an infinite-dimensional Lie algebra. A model illustrating this is presented below.\n\nTo enhance the flavor of physics, factors of will appear here and there as opposed to in the mathematical conventions.\n\nConsider a column vector of scalar fields . Let the Lagrangian density be\n\nThis Lagrangian is invariant under the transformation\n\nwhere are generators of either or a closed subgroup thereof, satisfying\n\nNoether's theorem asserts the existence of conserved currents,\n\nwhere is the momentum canonically conjugate to .\nThe reason these currents are said to be \"conserved\" is because\nand consequently\n\nthe charge associated to the charge density is constant in time. This (so far classical) theory is quantized promoting the fields and their conjugates to operators on Hilbert space and by postulating (bosonic quantization) the commutation relations\nThe currents accordingly become operators They satisfy, using the above postulated relations, the definitions and integration over space, the commutation relations\nwhere the speed of light and the reduced Planck's constant have been set to unity. The last commutation relation does \"not\" follow from the postulated commutation relations (these are fixed only for , not for ), except for For the Lorentz transformation behavior is used to deduce the conclusion. The next commutator to consider is\nThe presence of the delta functions and their derivatives is explained by the requirement of microcausality that implies that the commutator vanishes when . Thus the commutator must be a distribution supported at . The first term is fixed due to the requirement that the equation should, when integrated over , reduce to the last equation before it. The following terms are the Schwinger terms. They integrate to zero, but it can be shown quite generally that they must be nonzero.\nConsider a conserved current\n\nwith a generic Schwinger term\nBy taking the vacuum expectation value (VEV),\none finds\nwhere and Heisenberg's equation of motion have been used as well as and its conjugate.\n\nMultiply this equation by and integrate with respect to and over all space, using integration by parts, and one finds\nNow insert a complete set of states, \nHere hermiticity of and the fact that not all matrix elements of between the vacuum state and the states from a complete set can be zero.\n\nLet be an -dimensional complex simple Lie algebra with a dedicated suitable normalized basis such that the structure constants are antisymmetric in all indices with commutation relations\nAn untwisted affine Kac–Moody algebra is obtained by copying the basis for each (regarding the copies as distinct), setting\nas a vector space and assigning the commutation relations\n\nIf , then the subalgebra spanned by the is obviously identical to the polynomial loop algebra of above.\n\nThe Witt algebra, named after Ernst Witt, is the complexification of the Lie algebra of smooth vector fields on the circle . In coordinates, such vector fields may be written\nand the Lie bracket is the Lie bracket of vector fields, on simply given by\nThe algebra is denoted .\nA basis for is given by the set\nThis basis satisfies\n^nd_n = (l-m)\\delta_{l+m}^nd_n,\\quad l,m,n \\in \\mathbb Z.</math>\n\nThis Lie algebra has a useful central extension, the Virasoro algebra. It has dimensional subalgebras isomorphic with and . For each , the set spans a subalgebra isomorphic to .\nFor one has\nThese are the commutation relations of with\nThe groups and are isomorphic under the map\nand the same map holds at the level of Lie algebras due to the properties of the exponential map.\nA basis for is given, see classical group, by\nNow compute\nThe map preserves brackets and there are thus Lie algebra isomorphisms between the subalgebra of spanned by with \"real\" coefficients, and . The same holds for \"any\" subalgebra spanned by , this follows from a simple rescaling of the elements (on either side of the isomorphisms).\n\nIf is a matrix Lie group, then elements of the Lie algebra can be given by\nwhere is a differentiable path in that goes through the identity element at . Commutators of elements of the Lie algebra can be computed using two paths, and the group commutator,\n\nLikewise, given a group representation , its Lie algebra is computed by\nThen there is a Lie algebra between and isomorphism sending bases to bases, so that is a faithful representation of .\n\nIf however is a projective representation, i.e. a representation up to a phase factor,then the Lie algebra, as computed from the group representation, is \"not\" isomorphic to . In a projective representation the multiplication rule reads\nThe function ,often required to be smooth, satisfies\nIt is called a 2-cocycle on .\n\nOne has\nbecause both and evaluate to the identity at . For an explanation of the phase factors , see Wigner's theorem. The commutation relations in for a basis,\nbecome in \nso in order for to be closed under the bracket (and hence have a chance of actually being a Lie algebra) a central charge must be included.\n\nA classical relativistic string traces out a world sheet in spacetime, just like a point particle traces out a world line. This world sheet can locally be parametrized using two parameters and . Points in spacetime can, in the range of the parametrization, be written . One uses a capital to denote points in spacetime actually being on the world sheet of the string. Thus the string parametrization is given by . The inverse of the parametrization provides a local coordinate system on the world sheet in the sense of manifolds.\n\nThe equations of motion of a classical relativistic string derived in the Lagrangian formalism from the Nambu–Goto action are\nA dot \"over\" a quantity denotes differentiation with respect to and a prime differentiation with respect to . A dot \"between\" quantities denotes the relativistic inner product.\n\nThese rather formidable equations simplify considerably with a clever choice of parametrization called the light cone gauge. In this gauge, the equations of motion become\nthe ordinary wave equation. The price to be paid is that the light cone gauge imposes constraints,\nso that one cannot simply take arbitrary solutions of the wave equation to represent the strings. The strings considered here are open strings, i.e. they don't close up on themselves. This means that the Neumann boundary conditions have to be imposed on the endpoints. With this, the general solution of the wave equation (excluding constraints) is given by \nwhere is the slope parameter of the string (related to the string tension). The quantities and are (roughly) string position from the initial condition and string momentum. If all the are zero, the solution represents the motion of a classical point particle.\n\nThis is rewritten, first defining\nand then writing\n\nIn order to satisfy the constraints, one passes to light cone coordinates. For , where is the number of \"space\" dimensions, set \n\nNot all are independent. Some are zero (hence missing in the equations above), and the \"minus coefficients\" satisfy\nThe quantity on the left is given a name, \nthe transverse Virasoro mode.\n\nWhen the theory is quantized, the alphas, and hence the become operators.\n\n\n\n"}
{"id": "36266522", "url": "https://en.wikipedia.org/wiki?curid=36266522", "title": "Lindsey–Fox algorithm", "text": "Lindsey–Fox algorithm\n\nThe Lindsey–Fox algorithm, named after Pat Lindsey and Jim Fox, is a numerical algorithm for finding the roots or zeros of a high-degree polynomial with real coefficients over the complex field. It is particularly designed for random coefficients but also works well on polynomials with coefficients from samples of speech, seismic signals, and other measured phenomena. A Matlab implementation of this has factored polynomials of degree over a million on a desk top computer.\n\nThe Lindsey–Fox algorithm uses the FFT (fast Fourier transform) to very efficiently conduct a grid search in the complex plane to find accurate approximations to the \"N\" roots (zeros) of an \"N\"th-degree polynomial. The power of this grid search allows a new polynomial factoring strategy that has proven to be very effective for a certain class of polynomials. This algorithm was conceived of by Pat Lindsey and implemented by Jim Fox in a package of computer programs created to factor high-degree polynomials. It was originally designed and has been further developed to be particularly suited to polynomials with real, random coefficients. In that form, it has proven to be very successful by factoring thousands of polynomials of degrees from one thousand to hundreds of thousand as well as several of degree one million and one each of degree two million and four million. In addition to handling very high degree polynomials, it is accurate, fast, uses minimum memory, and is programmed in the widely available language, Matlab. There are practical applications, often cases where the coefficients are samples of some natural signal such as speech or seismic signals, where the algorithm is appropriate and useful. However, it is certainly possible to create special, ill-conditioned polynomials that it cannot factor, even low degree ones. The basic ideas of the algorithm were first published by Lindsey and Fox in 1992 and reprinted in 1996.  After further development, other papers were published in 2003 and an on-line booklet.  The program was made available to the public in March 2004 on the Rice University web site.  A more robust version-2 was released in March 2006 and updated later in the year.\n\nThe strategy implemented in the Lindsey–Fox algorithm to factor polynomials is organized in three stages. The first evaluates the polynomial over a grid on the complex plane and conducts a direct search for potential zeros. The second stage takes these potential zeros and “polishes” them by applying Laguerre's method to bring them close to the actual zeros of the polynomial. The third stage multiplies these zeros together or “unfactors” them to create a polynomial that is verified against the original. If some of the zeros were not found, the original polynomial is “deflated” by dividing it by the polynomial created from the found zeros. This quotient polynomial will generally be of low order and can be factored by conventional methods with the additional, new zeros added to the set of those first found. If there are still missing zeros, the deflation is carried out until all are found or the whole program needs to be restarted with a finer grid. This system has proven to be fast, accurate, and robust on the class of polynomials with real, random coefficients and other similar, well-conditioned polynomials.\n\n\n\n\nStage one is the reason this algorithm is so efficient and is what sets it apart from most other factoring algorithms. Because the FFT (fast Fourier transform) is used to evaluate the polynomial, a fast evaluation over a dense grid in the complex plane is possible. In order to use the FFT, the grid is structured in polar coordinates. In the first phase of this stage, a grid is designed with concentric circles of a particular radius intersected by a set of radial lines. The positions and spacing of the radial lines and the circles are chosen to give a grid that will hopefully separate the expected roots. Because the zeros of a polynomial with random coefficients have a fairly uniform angular distribution and are clustered close to the unit circle, it is possible to design an evaluation grid that fits the expected root density very well. In the second phase of this stage, the polynomial is evaluated at the nodes of the grid using the fast Fourier transform (FFT). It is because of the extraordinary efficiency and accuracy of the FFT that a direct evaluation is possible. In the third phase of this first stage, a search is conducted over all of the 3 by 3 node cells formed in the grid. For each 3 by 3 cell (see Figure below), if the value of the polynomial at the center node of the cell (the \"x\") is less than the values at all 8 of the nodes on the edges of the cell (the \"o's\"), the center is designated a candidate zero. This rule is based on the “Minimum Modulus Theorem” which states that if a relative minimum of the absolute value of an analytic function over an open region exists, the minimum must be a zero of the function. Finally, this set of prospective zeros is passed to the second stage. The number is usually slightly larger than the degree because some were found twice or mistakes were made. The number could be less if some zeros were missed. \nStage two is more traditional than the other two. It “polishes” each of the prospective zeros found by the grid search. The first phase consists of applying an iterative algorithm to improve the accuracy of the location found by the grid search. In earlier versions of the program, Newton’s method was used but analysis and experiment showed that Laguerre's method was both more robust and more accurate. Even though it required more calculation than Newton’s method for each iteration, it converged in fewer iterations. The second phase of the second stage checks for duplications. A “fuzzy” uniqueness test is applied to each zero to eliminate any cases where on two or more prospective zeros, iterations converged to the same zero. If the number of unique, polished zeros is less than the degree of the polynomial, deflation later will be necessary. If the number is greater, some error has occurred. This stage consumes the largest part of the execution time of the total factorization, but it is crucial to the final accuracy of the roots. One of the two criteria for success in factoring a polynomial is that each root must have been successfully polished against the original polynomial.\n\nStage three has several phases and possible iterations or even restarting. The first phase of the third stage takes all of the unique, polished zeros that were found in the first two stages and multiplies them together into the coefficient form of a candidate polynomial (“unfactors” the zeros). If the degree of this reconstructed polynomial is the same as that of the original polynomial and if the difference in their coefficients is small, the factorization is considered successful. Often, however, several zeros were missed by the grid search and polish processes of stage one and two, or the uniqueness test discarded a legitimate zero (perhaps it is multiple), so the original polynomial is “deflated” (divided) by the reconstructed polynomial and the resulting (low degree) quotient is factored for the missing zeros. If that doesn’t find them all, the deflation process is repeated until they are all found. This allows the finding of multiple roots (or very tightly clustered roots), even if some of them were discarded earlier. If, in the unusual case, these further iterations of deflation do not find all of the missing zeros, a new, finer grid is constructed and the whole process started again at stage one. More details on the third stage are in another module.\n\nMultiple order and clustered roots are unusual in random coefficient polynomials. But, if they happen or if factoring an ill-conditioned polynomial is attempted, the roots will be found with the Lindsey–Fox program in most cases but with reduced accuracy. If there are multiple order zeros (Mth order with M not too high), the grid search will find them, but with multiplicity one. The polishing will converge to the multiple order root but not as fast as to a distinct root. In the case of a cluster with \"Q\" zeros that fall within a single cell, they are erroneously identified as a single zero and the polishing will converge to only one of them. In some cases, two zeros can be close to each other in adjacent cells and polish to the same point. In all of these cases, after the uniqueness test and deflation, the quotient polynomial will contain a \"M\" − 1 order zero and/or \"Q\" − 1 zeros clustered together. Each of these zeros will be found after \"M\" − 1 or \"Q\" − 1 deflations. There can be problems here because Laguerre’s polishing algorithm is not as accurate and does not converge as fast for a multiple zero and it may even diverge when applied to tightly clustered zeros. Also, the condition of the quotient polynomial will be poorer when multiple and clustered zeros are involved. If multiple order zeros are extremely far from the unit circle, the special methods for handling multiple roots developed by Zhonggang Zeng are used. Zeng’s method is powerful but slow, and hence only used in special cases [6]. References\n\nSuccessful completion of the factoring of a polynomial requires matching zeros on the complex plane measured by the convergence of Laguerre’s algorithm on each of the zeros. It also requires matching the polynomial reconstructed from the found zeros with the original polynomial by measuring the maximum difference in each coefficient.\n\nBecause the FFT is such an efficient means of evaluating the polynomial, a very fine grid can be used which will separate all or almost all of the zeros in a reasonable time. And because of the fineness of the grid, the starting point is close to the actual zero and the polishing almost always converges in a small number of steps (convergence is often a serious problem in traditional approaches). And because the searching and polishing is done on the original polynomial, they can be done on each root simultaneously on a parallel architecture computer. Because the searching is done on a 3 by 3 cell of the grid, no more that three concentric circles of the grid need be kept in memory at a time, i.e., it is not necessary to have the entire grid in memory. And, some parallelization of the FFT calculations can be done.\n\nDeflation is often a major source of error or failure in a traditional iterative algorithm. Here, because of the good starting points and the powerful polisher, very few stages of deflation are generally needed and they produce a low order quotient polynomial that is generally well-conditioned. Moreover, to control error, the unfactoring (multiplying the found roots together) is done in the FFT domain (for degree larger than 500) and the deflation is done partly in the FFT domain and partly in the coefficient domain, depending on a combination of stability, error accumulation, and speed factors.\n\nFor random coefficient polynomials, the number of zeros missed by the grid search and polish stages ranges from 0 to 10 or occasionally more. In factoring one 2 million degree polynomial, the search and polish stages found all 2 million zeros in one grid search and required no deflation which shows the power of the grid search on this class of polynomial. When deflation is needed, one pass is almost always sufficient. However, if you have a multiple zero or two (or more) very, very closely spaced zeros, the uniqueness test will discard a legitimate zero but it will be found by later deflation. Stage three has enough tests and alternatives to handle almost all possible conditions. But, by the very definition of random coefficients, it is hard to absolutely guarantee success.\n\nThe timings of the Lindsey–Fox program and examples of root distributions of polynomials with random coefficients are here.\n"}
{"id": "39482629", "url": "https://en.wikipedia.org/wiki?curid=39482629", "title": "List of things named after Ernst Witt", "text": "List of things named after Ernst Witt\n\nThese are things named after Ernst Witt, a German mathematician.\n"}
{"id": "168905", "url": "https://en.wikipedia.org/wiki?curid=168905", "title": "Mathematical folklore", "text": "Mathematical folklore\n\nAs the term is understood by mathematicians, folk mathematics or mathematical folklore is the body of theorems, definitions, proofs, or mathematical facts or techniques that circulate among mathematicians by word of mouth but have not appeared in print, either in books or in scholarly journals. Knowledge of folklore is the coin of the realm of academic mathematics.\n\nQuite important at times for researchers are folk theorems, which are results known, at least to experts in a field, and considered to have established status, but not published in complete form. Sometimes these are only alluded to in the public literature. \nAn example is a book of exercises, described on the back cover:\nAnother distinct category is wellknowable mathematics, a term introduced by John Conway. This consists of matters that are known and factual, but not in active circulation in relation with current research. Both of these concepts are attempts to describe the actual context in which research work is done.\n\nSome people, principally non-mathematicians, use the term \"folk mathematics\" to refer to the informal mathematics studied in many ethno-cultural studies of mathematics.\n\nMathematical folklore may also refer to unusual (and possibly apocryphal) stories or jokes involving mathematicians or mathematics that are told verbally in mathematics departments. Compilations include tales collected in G. H. Hardy's \"A Mathematician's Apology\" and ; examples include:\n\n"}
{"id": "200877", "url": "https://en.wikipedia.org/wiki?curid=200877", "title": "Maze generation algorithm", "text": "Maze generation algorithm\n\nMaze generation algorithms are automated methods for the creation of mazes.\n\nA maze can be generated by starting with a predetermined arrangement of cells (most commonly a rectangular grid but other arrangements are possible) with wall sites between them. This predetermined arrangement can be considered as a connected graph with the edges representing possible wall sites and the nodes representing cells. The purpose of the maze generation algorithm can then be considered to be making a subgraph in which it is challenging to find a route between two particular nodes.\n\nIf the subgraph is not connected, then there are regions of the graph that are wasted because they do not contribute to the search space. If the graph contains loops, then there may be multiple paths between the chosen nodes. Because of this, maze generation is often approached as generating a random spanning tree. Loops, which can confound naive maze solvers, may be introduced by adding random edges to the result during the course of the algorithm.\n\nThe animation shows the maze generation steps for a \ngraph that is not on a rectangular grid.\nFirst, the computer creates a random planar graph G\nshown in blue, and its dual F\nshown in yellow. Second, computer traverses F using a chosen\nalgorithm, such as a depth-first search, coloring the path red.\nDuring the traversal, whenever a red edge crosses over a blue edge,\nthe blue edge is removed.\nFinally, when all vertices of F have been visited, F is erased\nand two edges from G, one for the entrance and one for the exit, are removed.\n\nThis algorithm is a randomized version of the depth-first search algorithm. Frequently implemented with a stack, this approach is one of the simplest ways to generate a maze using a computer. Consider the space for a maze being a large grid of cells (like a large chess board), each cell starting with four walls. Starting from a random cell, the computer then selects a random neighbouring cell that has not yet been visited. The computer removes the wall between the two cells and marks the new cell as visited, and adds it to the stack to facilitate backtracking. The computer continues this process, with a cell that has no unvisited neighbours being considered a dead-end. When at a dead-end it backtracks through the path until it reaches a cell with an unvisited neighbour, continuing the path generation by visiting this new, unvisited cell (creating a new junction). This process continues until every cell has been visited, causing the computer to backtrack all the way back to the beginning cell. We can be sure every cell is visited.\n\nAs given above this algorithm involves deep recursion which may cause stack overflow issues on some computer architectures. The algorithm can be rearranged into a loop by storing backtracking information in the maze itself. This also provides a quick way to display a solution, by starting at any given point and backtracking to the beginning.\n\nMazes generated with a depth-first search have a low branching factor and contain many long corridors, because the algorithm explores as far as possible along each branch before backtracking.\n\nThe depth-first search algorithm of maze generation is frequently implemented using backtracking:\n\n\nThis algorithm is a randomized version of Kruskal's algorithm.\n\n\nThere are several data structures that can be used to model the sets of cells. An efficient implementation using a disjoint-set data structure can perform each union and find operation on two sets in nearly constant amortized time (specifically, formula_1 time; formula_2 for any plausible value of formula_3), so the running time of this algorithm is essentially proportional to the number of walls available to the maze.\n\nIt matters little whether the list of walls is initially randomized or if a wall is randomly chosen from a nonrandom list, either way is just as easy to code.\n\nBecause the effect of this algorithm is to produce a minimal spanning tree from a graph with equally weighted edges, it tends to produce regular patterns which are fairly easy to solve.\n\nThis algorithm is a randomized version of Prim's algorithm.\n\n\nIt will usually be relatively easy to find the way to the starting cell, but hard to find the way anywhere else.\n\nNote that simply running classical Prim's on a graph with random edge weights would create mazes stylistically identical to Kruskal's, because they are both minimal spanning tree algorithms. Instead, this algorithm introduces stylistic variation because the edges closer to the starting point have a lower effective weight.\n\nAlthough the classical Prim's algorithm keeps a list of edges, for maze generation we could instead maintain a list of adjacent cells. If the randomly chosen cell has multiple edges that connect it to the existing maze, select one of these edges at random. This will tend to branch slightly more than the edge-based version above.\n\nAll the above algorithms have biases of various sorts: depth-first search is biased toward long corridors, while Kruskal's/Prim's algorithms are biased toward many short dead ends. Wilson's algorithm, on the other hand, generates an \"unbiased\" sample from the uniform distribution over all mazes, using loop-erased random walks.\n\nWe begin the algorithm by initializing the maze with one cell chosen arbitrarily. Then we start at a new cell chosen arbitrarily, and perform a random walk until we reach a cell already in the maze—however, if at any point the random walk reaches its own path, forming a loop, we erase the loop from the path before proceeding. When the path reaches the maze, we add it to the maze. Then we perform another loop-erased random walk from another arbitrary starting cell, repeating until all cells have been filled.\n\nThis procedure remains unbiased no matter which method we use to arbitrarily choose starting cells. So we could always choose the first unfilled cell in (say) left-to-right, top-to-bottom order for simplicity.\n\nMazes can be created with \"recursive division\", an algorithm which works as follows: Begin with the maze's space with no walls. Call this a chamber. Divide the chamber with a randomly positioned wall (or multiple walls) where each wall contains a randomly positioned passage opening within it. Then recursively repeat the process on the subchambers until all chambers are minimum sized. This method results in mazes with long straight walls crossing their space, making it easier to see which areas to avoid.\n\nFor example, in a rectangular maze, build at random points two walls that are perpendicular to each other. These two walls divide the large chamber into four smaller chambers separated by four walls. Choose three of the four walls at random, and open a one cell-wide hole at a random point in each of the three. Continue in this manner recursively, until every chamber has a width of one cell in either of the two directions.\n\nOther algorithms exist that require only enough memory to store one line of a 2D maze or one plane of a 3D maze. They prevent loops by storing which cells in the current line are connected through cells in the previous lines, and never remove walls between any two cells already connected.\n\nMost maze generation algorithms require maintaining relationships between cells within it, to ensure the end result will be solvable. Valid simply connected mazes can however be generated by focusing on each cell independently. A binary tree maze is a standard orthogonal maze where each cell always has a passage leading up or leading left, but never both. To create a binary tree maze, for each cell flip a coin to decide whether to add a passage leading up or left. Always pick the same direction for cells on the boundary, and the end result will be a valid simply connected maze that looks like a binary tree, with the upper left corner its root.\n\nA related form of flipping a coin for each cell is to create an image using a random mix of forward slash and backslash characters. This doesn't generate a valid simply connected maze, but rather a selection of closed loops and unicursal passages. (The manual for the Commodore 64 presents a BASIC program using this algorithm, but using PETSCII diagonal line graphic characters instead for a smoother graphic appearance.)\n\nCertain types of cellular automata can be used to generate mazes. Two well-known such cellular automata, Maze and Mazectric, have rulestrings B3/S12345 and B3/S1234. In the former, this means that cells survive from one generation to the next if they have at least one and at most five neighbours. In the latter, this means that cells survive if they have one to four neighbours. If a cell has exactly three neighbours, it is born. It is similar to Conway's Game of Life in that patterns that do not have a living cell adjacent to 1, 4, or 5 other living cells in any generation will behave identically to it. However, for large patterns, it behaves very differently from Life.\n\nFor a random starting pattern, these maze-generating cellular automata will evolve into complex mazes with well-defined walls outlining corridors. Mazecetric, which has the rule B3/S1234 has a tendency to generate longer and straighter corridors compared with Maze, with the rule B3/S12345. Since these cellular automaton rules are deterministic, each maze generated is uniquely determined by its random starting pattern. This is a significant drawback since the mazes tend to be relatively predictable.\n\nLike some of the graph-theory based methods described above, these cellular automata typically generate mazes from a single starting pattern; hence it will usually be relatively easy to find the way to the starting cell, but harder to find the way anywhere else.\n\nExample implementation of a variant of Prim's algorithm in Python/NumPy. Prim's algorithm above starts with a grid full of walls and grows a single component of pathable tiles. In this example, we start with an open grid and grow multiple components of walls.\n\nThis algorithm works by creating n (density) islands of length p (complexity). An island is created by choosing a random starting point with odd coordinates, then a random direction is chosen. If the cell two steps in the direction is free, then a wall is added at both one step and two steps in this direction. The process is iterated for n steps for this island. p islands are created. n and p are expressed as float to adapt them to the size of the maze. With a low complexity, islands are very small and the maze is easy to solve. With low density, the maze has more \"big empty rooms\".\n\nThe code below is an example of depth-first search maze generator in C. \n\n"}
{"id": "25626903", "url": "https://en.wikipedia.org/wiki?curid=25626903", "title": "Measure algebra", "text": "Measure algebra\n\nIn mathematics, a measure algebra is a Boolean algebra with a countably additive positive measure. A probability measure on a measure space gives a measure algebra on the Boolean algebra of measurable sets modulo null sets.\n\nA measure algebra is a Boolean algebra \"B\" with a measure \"m\", which is a real-valued function on \"B\" such that:\n"}
{"id": "35265777", "url": "https://en.wikipedia.org/wiki?curid=35265777", "title": "Michael G. Crandall", "text": "Michael G. Crandall\n\nMichael Grain Crandall (born November 29, 1940, in Baton Rouge, Louisiana) is an American mathematician, specializing in differential equations.\n\nIn 1962 Crandall earned a baccalaureate in engineering physics from University of California, Berkeley, changed to mathematics, earning a master's in 1964 and a PhD in 1965 under Heinz Cordes at Berkeley, with a thesis that solved a problem in celestial mechanics posed by Carl Ludwig Siegel; the thesis title is \"Two families of plane solutions of the four body problem\". In 1965 he was an instructor at Berkeley, in 1966 an assistant professor at Stanford University and from 1969 at the University of California, Los Angeles (UCLA), where he was a professor from 1973 to 1976. From 1974 to 1984 he was a professor at the Mathematics Research Center at the University of Wisconsin–Madison, from 1984 to 1990 as Hille-Professor of Mathematics. From 1988 until his retirement he was a professor at the University of California, Santa Barbara. Crandall was several times a visiting professor at the University of Paris, where he received an honorary doctorate in 1999. His legacy of contributions contains all but not limited to: Banach solutions in Euclidean spaces, Fourier transforms of planar variables, PDE concepts and iterations for sequence analysis, semigroup transform solutions, differential harmonic study of divergent hyperbole, physical transformations of finite Jacobian entities, unique harmonic populations in convergent contexts, application of abstract existence principles on non-linear contexts, normalized vector sequencing in multi-dimensional parallax geometries, and the mathematical equivalence study of topographical dissimilar nodes using traditional non-linear surfacing theories to produce distinct solutions in the realm of differential multi-variable applications.\n\nCrandall works primarily on partial differential equations, e.g., with bifurcation theory, evolution equations, generation of semigroups of transformations on Banach spaces and the theory of Hamilton–Jacobi equations. With Pierre-Louis Lions he did research on the viscosity solutions of partial differential equations.\n\nIn 2000 he was elected a member of the American Academy of Arts and Sciences. In 1999 he received the Leroy P. Steele Prize. In 1974 he was an Invited Lecturer (on \"Semigroups of nonlinear equations and evolution equations\") at the International Congress of Mathematicians in Vancouver. In 2012 he became a fellow of the American Mathematical Society.\n\nAmong his doctoral students is Lawrence C. Evans.\n\n"}
{"id": "18739787", "url": "https://en.wikipedia.org/wiki?curid=18739787", "title": "Nassif Ghoussoub", "text": "Nassif Ghoussoub\n\nNassif A. Ghoussoub, , is a Canadian mathematician working in the fields of non-linear analysis and partial differential equations. He is a Professor of Mathematics and a Distinguished University Scholar at the University of British Columbia.\n\nGhoussoub was born to Lebanese parents in Western Africa (now Mali). \n\nHe completed his doctorat 3ème cycle (PhD) in 1975, and a Doctorat d'Etat in 1979 at the Pierre and Marie Curie University, where his advisors were Gustave Choquet and Antoine Brunel. \n\nGhoussoub completed his post-doctoral fellowship at the Ohio State University during 1976-77. He then joined the University of British Columbia, where he currently holds a position of Professor of Mathematics and a Distinguished University Scholar. Ghoussoub is known for his work in functional analysis, non-linear analysis and partial differential equations.\n\nHe was vice-president of the Canadian Mathematical Society from 1994 to 1996, the founding director of the Pacific Institute for the Mathematical Sciences (PIMS) for the period 1996–2003, the co-editor-in-chief of the Canadian Journal of Mathematics during 1993-2002, a co-founder of the MITACS Network of Centres of Excellence, and is the founder and current scientific director of the Banff International Research Station (BIRS). In 1994, Ghoussoub became a fellow of the Royal Society of Canada, and in 2012, a fellow of the American Mathematical Society. \n\nGhoussoub has been awarded multiple awards and distinctions, including the Coxeter-James prize in 1990, and the Jeffrey-Williams prize in 2007. He holds honorary doctorates from the Université Paris-Dauphine (France), and the University of Victoria (Canada). He was awarded the Queen Elizabeth II Diamond Jubilee Medal in 2012, and appointed to the Order of Canada in 2015, with the grade of officer for contributions to mathematics, research and education. \n\nIn 2018, Ghoussoub was elected a faculty representative on the University of British Columbia's Board of Governors. He will serve until February 29, 2020. Ghoussoub has previously served two consecutive terms in this role from 2008 to 2014.\n\nGhoussoub's scholarly work has been cited over 5,000 times, and has an h-index of 37.\n\n\n\n\n\n"}
{"id": "235029", "url": "https://en.wikipedia.org/wiki?curid=235029", "title": "Nth root", "text": "Nth root\n\nIn mathematics, an \"n\"th root of a number \"x\", where \"n\" is usually assumed to be a positive integer, is a number \"r\" which, when raised to the power \"n\" yields \"x\":\nwhere \"n\" is the \"degree\" of the root. A root of degree 2 is called a \"square root\" and a root of degree 3, a \"cube root\". Roots of higher degree are referred by using ordinal numbers, as in \"fourth root\", \"twentieth root\", etc.\n\nFor example:\n\nAny non-zero number, considered as complex number, has \"n\" different \"complex roots of degree \"n\"\" (\"n\"th roots), including those with zero imaginary part, i.e. any real roots. The root of \"0\" is zero for all degrees \"n\", since . In particular, if \"n\" is even and \"x\" is a positive real number, one of its \"n\"th roots is positive, one is negative, and the rest (when \"n\" > 2) are complex but not real; if \"n\" is even and \"x\" is a negative real, none of the \"n\"th roots is real. If \"n\" is odd and \"x\" is real, one \"n\"th root is real and has the same sign as \"x\", while the other (\"n\" − 1) roots are not real. Finally, if \"x\" is not real, then none of its \"n\"th roots is real.\n\nRoots are usually written using the radical symbol or \"radix\" with formula_2 denoting the principal square root of formula_3, formula_4 denoting the principal cube root, formula_5 denoting the principal fourth root, and so on. In the expression formula_6, \"n\" is called the \"index\", formula_7 is the \"radical sign\" or \"radix\", and formula_3 is called the \"radicand\". Since the radical symbol denotes a function, it is defined to return only one result for a given argument formula_3, which is called the principal \"n\"th root of formula_3. Conventionally, a real root, preferably non-negative, if there is one, is designated as the principal \"n\"th root.\n\nA complementary definition of \"principal root\" (though not formally defined or universally accepted) is to say that it is always the complex root that has the least value of the argument among all roots; here “argument” is bound to formula_11 and means the counterclockwise angle in radian between the positive real axis and the line joining the complex number to the origin.\n\nFor example:\n\nAn unresolved root, especially one using the radical symbol, is sometimes referred to as a \"surd\" or a \"radical\". Any expression containing a radical, whether it is a square root, a cube root, or a higher root, is called a \"radical expression\", and if it contains no transcendental functions or transcendental numbers it is called an algebraic expression.\n\nIn calculus, roots are treated as special cases of exponentiation, where the exponent is a fraction:\nRoots are particularly important in the theory of infinite series; the root test determines the radius of convergence of a power series. Roots can also be defined for complex numbers, and the complex roots of 1 (the roots of unity) play an important role in higher mathematics. Galois theory can be used to determine which algebraic numbers can be expressed using roots and to prove the Abel–Ruffini theorem, which states that a general polynomial equation of degree five or higher cannot be solved using roots alone; this result is also known as \"the insolubility of the quintic\".\n\nAn archaic term for the operation of taking \"n\"th roots is \"radication\". \n\nAn \"n\"th root of a number \"x\", where \"n\" is a positive integer, is any of the \"n\" real or complex numbers \"r\" whose \"n\"th power is \"x\":\nEvery positive real number \"x\" has a single positive \"n\"th root, called the principal \"n\"th root, which is written formula_6. For \"n\" equal to 2 this is called the principal square root and the \"n\" is omitted. The \"n\"th root can also be represented using exponentiation as \"x\".\n\nFor even values of \"n\", positive numbers also have a negative \"n\"th root, while negative numbers do not have a real \"n\"th root. For odd values of \"n\", every negative number \"x\" has a real negative \"n\"th root. For example, −2 has a real 5th root, formula_29 but −2 does not have any real 6th roots.\n\nEvery non-zero number \"x\", real or complex, has \"n\" different complex number \"n\"th roots. (In the case \"x\" is real, this count includes any real \"n\"th roots.) The only complex root of 0 is 0.\n\nThe \"n\"th roots of almost all numbers (all integers except the \"n\"th powers, and all rationals except the quotients of two \"n\"th powers) are irrational. For example,\n\nAll \"n\"th roots of integers are algebraic numbers.\n\nThe term \"surd\" traces back to al-Khwārizmī (c. 825), who referred to rational and irrational numbers as \"audible\" and \"inaudible\", respectively. This later led to the Arabic word \"\" (\"asamm\", meaning \"deaf\" or \"dumb\") for \"irrational number\" being translated into Latin as \"surdus\" (meaning \"deaf\" or \"mute\"). Gerard of Cremona (c. 1150), Fibonacci (1202), and then Robert Recorde (1551) all used the term to refer to \"unresolved irrational roots\".\n\nA square root of a number \"x\" is a number \"r\" which, when squared, becomes \"x\":\nEvery positive real number has two square roots, one positive and one negative. For example, the two square roots of 25 are 5 and −5. The positive square root is also known as the principal square root, and is denoted with a radical sign:\n\nSince the square of every real number is a positive real number, negative numbers do not have real square roots. However, for every negative real number there are two imaginary square roots. For example, the square roots of −25 are 5\"i\" and −5\"i\", where \"i\" represents a number whose square is .\n\nA cube root of a number \"x\" is a number \"r\" whose cube is \"x\":\nEvery real number \"x\" has exactly one real cube root, written formula_4. For example,\nEvery real number has two additional complex cube roots.\n\nExpressing the degree of an \"n\"th root in its exponent form, as in formula_37, makes it easier to manipulate powers and roots.\n\nEvery positive real number has exactly one positive real \"n\"th root, and so the rules for operations with surds involving positive radicands formula_39 are straightforward within the real numbers:\n\nSubtleties can occur when taking the \"n\"th roots of negative or complex numbers. For instance:\n\nSince the rule formula_43 strictly holds for non-negative real radicands only, its application leads to the inequality in the first step above.\n\nA non-nested radical expression is said to be in simplified form if\n\nFor example, to write the radical expression formula_44 in simplified form, we can proceed as follows. First, look for a perfect square under the square root sign and remove it:\nNext, there is a fraction under the radical sign, which we change as follows:\nFinally, we remove the radical from the denominator as follows:\n\nWhen there is a denominator involving surds it is always possible to find a factor to multiply both numerator and denominator by to simplify the expression. For instance using the factorization of the sum of two cubes:\n\nSimplifying radical expressions involving nested radicals can be quite difficult. It is not obvious for instance that:\n\nThe above can be derived through:\n\nThe radical or root may be represented by the infinite series:\n\nwith formula_52. This expression can be derived from the binomial series.\n\nThe \"n\"th root of an integer \"k\" is only an integer if \"k\" is the product of \"n\"th powers of integers. In all other cases the \"n\"th root of an integer is an irrational number. For instance, the fifth root of\n\nand the fifth root of 34 is\n\nwhere here the dots signify not only that the decimal expression does not end after a finite number of digits, but also that the digits never enter a repeating pattern, because the number is irrational.\n\nSince for positive real numbers and the equality formula_55 holds, the above property can be extended to positive rational numbers. Let formula_56, with and coprime and positive integers, be a rational number, has a rational \"n\"th root, if both positive have an integer \"n\"th root, i.e., formula_57 is the product of \"n\"th powers of rational numbers. If one or both \"n\"th roots of or are irrational, the quotient is irrational, too.\n\nThe \"n\"th root of a number \"A\" can be computed by the \"n\"th root algorithm, a special case of Newton's method. Start with an initial guess \"x\" and then iterate using the recurrence relation\nuntil the desired precision is reached.\n\nDepending on the application, it may be enough to use only the first Newton approximant:\nFor example, to find the fifth root of 34, note that 2 = 32 and thus take \"x\" = 2, \"n\" = 5 and \"y\" = 2 in the above formula. This yields\nThe error in the approximation is only about 0.03%.\n\nNewton's method can be modified to produce a generalized continued fraction for the \"n\"th root which can be modified in various ways as described in that article. For example:\n\nIn the case of the fifth root of 34 above (after dividing out selected common factors):\n\nBuilding on the digit-by-digit calculation of a square root, it can be seen that the formula used there, formula_64, or formula_65, follows a pattern involving Pascal's triangle. For the \"n\"th root of a number formula_66 is defined as the value of element formula_67 in row formula_68 of Pascal's Triangle such that formula_69, we can rewrite the expression as formula_70. For convenience, call the result of this expression formula_71. Using this more general expression, any positive principal root can be computed, digit-by-digit, as follows.\n\nWrite the original number in decimal form. The numbers are written similar to the long division algorithm, and, as in long division, the root will be written on the line above. Now separate the digits into groups of digits equating to the root being taken, starting from the decimal point and going both left and right. The decimal point of the root will be above the decimal point of the square. One digit of the root will appear above each group of digits of the original number.\n\nBeginning with the left-most group of digits, do the following procedure for each group:\n\n\nFind the square root of 152.2756.\n\nFind the cube root of 4192 to the nearest hundredth.\n\nThe principal \"n\"th root of a positive number can be computed using logarithms. Starting from the equation that defines \"r\" as an \"n\"th root of \"x\", namely formula_80 with \"x\" positive and therefore its principal root \"r\" also positive, one takes logarithms of both sides (any base of the logarithm will do) to obtain\n\nThe root \"r\" is recovered from this by taking the antilog:\n\nFor the case in which \"x\" is negative and \"n\" is odd, there is one real root \"r\" which is also negative. This can be found by first multiplying both sides of the defining equation by −1 to obtain formula_83 then proceeding as before to find |\"r\"|, and using .\n\nThe ancient Greek mathematicians knew how to use compass and straightedge to construct a length equal to the square root of a given length. In 1837 Pierre Wantzel proved that an \"n\"th root of a given length cannot be constructed if \"n\" is not a power of 2.\n\nEvery complex number other than 0 has \"n\" different \"n\"th roots.\n\nThe two square roots of a complex number are always negatives of each other. For example, the square roots of are and , and the square roots of are\nIf we express a complex number in polar form, then the square root can be obtained by taking the square root of the radius and halving the angle:\nA \"principal\" root of a complex number may be chosen in various ways, for example\nwhich introduces a branch cut in the complex plane along the positive real axis with the condition , or along the negative real axis with .\n\nUsing the first(last) branch cut the principal square root formula_87 maps formula_88 to the half plane with non-negative imaginary(real) part. The last branch cut is presupposed in mathematical software like Matlab or Scilab.\n\nThe number 1 has \"n\" different \"n\"th roots in the complex plane, namely\nwhere\nThese roots are evenly spaced around the unit circle in the complex plane, at angles which are multiples of formula_91. For example, the square roots of unity are 1 and −1, and the fourth roots of unity are 1, formula_67, −1, and formula_93.\n\nEvery complex number has \"n\" different \"n\"th roots in the complex plane. These are\n\nwhere \"η\" is a single \"n\"th root, and 1, \"ω\", \"ω\", ... \"ω\" are the \"n\"th roots of unity. For example, the four different fourth roots of 2 are\n\nIn polar form, a single \"n\"th root may be found by the formula\n\nHere \"r\" is the magnitude (the modulus, also called the absolute value) of the number whose root is to be taken; if the number can be written as \"a+bi\" then formula_97. Also, formula_98 is the angle formed as one pivots on the origin counterclockwise from the positive horizontal axis to a ray going from the origin to the number; it has the properties that formula_99 formula_100 and formula_101\n\nThus finding \"n\"th roots in the complex plane can be segmented into two steps. First, the magnitude of all the \"n\"th roots is the \"n\"th root of the magnitude of the original number. Second, the angle between the positive horizontal axis and a ray from the origin to one of the \"n\"th roots is formula_102, where formula_98 is the angle defined in the same way for the number whose root is being taken. Furthermore, all \"n\" of the \"n\"th roots are at equally spaced angles from each other.\n\nIf \"n\" is even, a complex number's \"n\"th roots, of which there are an even number, come in additive inverse pairs, so that if a number \"r\" is one of the \"n\"th roots then \"r\" = –\"r\" is another. This is because raising the latter's coefficient –1 to the \"n\"th power for even \"n\" yields 1: that is, (–\"r\") = (–1) × \"r\" = \"r\".\n\nAs with square roots, the formula above does not define a continuous function over the entire complex plane, but instead has a branch cut at points where \"θ\" / \"n\" is discontinuous.\n\nIt was once conjectured that all polynomial equations could be solved algebraically (that is, that all roots of a polynomial could be expressed in terms of a finite number of radicals and elementary operations). However, while this is true for third degree polynomials (cubics) and fourth degree polynomials (quartics), the Abel–Ruffini theorem (1824) shows that this is not true in general when the degree is 5 or greater. For example, the solutions of the equation\n\ncannot be expressed in terms of radicals. (\"cf.\" quintic equation)\n\n"}
{"id": "46179348", "url": "https://en.wikipedia.org/wiki?curid=46179348", "title": "Otfrid Mittmann", "text": "Otfrid Mittmann\n\nOtfrid Mittmann (27 December 1908 in Ruda Śląska — 10 August 1998 in Rheinbach) was a German mathematician.\nStarting in 1927, he studied mathematics and natural sciences in Göttingen and Leipzig, and got his Ph.D. in Apr 1935. He joined the Nazi movement in Oct 1929. and published on statistical aspects of Nazi eugenics. After the war, he published in Göttingen and Bonn.\n\n"}
{"id": "24608545", "url": "https://en.wikipedia.org/wiki?curid=24608545", "title": "Plateau principle", "text": "Plateau principle\n\nThe plateau principle is a mathematical model or scientific law originally developed to explain the time course of drug action. The principle has wide applicability in pharmacology, physiology, nutrition, biochemistry and system dynamics. It applies whenever a drug or nutrient is infused or ingested at a relatively constant rate and when a constant fraction is eliminated during each time interval. Under these conditions, any change in the rate of infusion leads to an exponential increase or decrease until a new level is achieved. This behavior is also called an approach to steady state because rather than causing an indefinite increase or decrease, a natural balance is achieved when the rate of infusion or production is balanced by the rate of loss.\n\nAn especially important use of the plateau principle is to study the renewal of tissue constituents in the human and animal body. In adults, daily synthesis of tissue constituents is nearly constant, and most constituents are removed with a first order reaction rate. Applicability of the plateau principle was recognized during radiotracer studies of protein turnover in the 1940s by Rudolph Schoenheimer and David Rittenberg. Unlike the case with drugs, the initial amount of tissue or tissue protein is not zero because daily synthesis offsets daily elimination. In this case, the model is also said to approach a steady state with exponential or logarithmic kinetics. Constituents that change in this manner are said to have a biological half-life.\n\nA practical application of the plateau principle is that most people have experienced \"plateauing\" during regimens for weight management or training for sports. After a few weeks of progress, one seems unable to continue gaining in ability or losing weight. This outcome results from the same underlying quantitative model. This entry will describe the popular concepts as well as development of the plateau principle as a scientific, mathematical model.\n\nIn the sciences, the broadest application of the plateau principle is creating realistic time signatures for change in kinetic models (see Mathematical model). One example of this principle is the long time required to effectively change human body composition. Theoretical studies have shown that many months of consistent physical training and food restriction are needed to bring about permanent weight stability in people who were previously overweight.\n\nMost drugs are eliminated from the blood plasma with first order kinetics. For this reason, when a drug is introduced into the body at a constant rate by intravenous therapy, it approaches a new steady concentration in the blood at a rate defined by its half-life. Similarly, when the intravenous infusion is ended, the drug concentration decreases exponentially and reaches an undetectable level after 5–6 half-lives have passed. If the same drug is administered as a bolus (medicine) with a single injection, peak concentration is achieved almost immediately and then the concentration declines exponentially.\n\nMost drugs are taken by mouth. In this case, the assumption of constant infusion is only approximated as doses are repeated over the course of several days. The plateau principle still applies but more complex models are required to account for the route of administration.\n\nDerivation of equations that describe the time course of change for a system with zero-order input and first-order elimination are presented in exponential decay and biological half-life and in scientific literature.,\n\n\nThe relationship between the elimination rate constant and half-life is given by the following equation:\n\nBecause ln 2 equals 0.693, the half-life is readily calculated from the elimination rate constant. Half-life has units of time, and the elimination rate constant has units of 1/time, e.g., per hour or per day.\n\nAn equation can be used to forecast the concentration of a compound at any future time when the fractional degration rate and steady state concentration are known:\n\n\nThe exponential function in parentheses corresponds to the fraction of total change that has been achieved as time passes and the difference between \"C\" and \"C\" equals the total amount of change. Finally, at steady state, the concentration is expected to equal the rate of synthesis, production or infusion divided by the first order elimination constant.\n\n\nAlthough these equations were derived to assist with predicting the time course of drug action, the same equation can be used for any substance or quantity that is being produced at a measurable rate and degraded with first-order kinetics. Because the equation applies in many instances of mass balance, it has very broad applicability in addition to pharmacokinetics. The most important inference derived from the steady state equation and the equation for fractional change over time is that the elimination rate constant (\"k\") or the sum of rate constants that apply in a model determine the time course for change in mass when a system is perturbed (either by changing the rate of inflow or production, or by changing the elimination rate(s)).\n\nWhen experimental data are available, the normal procedure for estimating rate parameters such as \"k\" and \"C\" is to minimize the sum of squares of differences between observed data and values predicted based on initial estimates of the rate constant and steady state value. This can be done using any software package that contains a curve fitting routine. An example of this methodology implemented with spreadsheet software has been reported. The same article reports a method that requires only 3 equally spaced data points to obtain estimates for kinetic parameters. Spreadsheets that compare these methods are available. Three point method explained\n\nDr. Wilbur O. Atwater, who developed the first database of food composition in the United States, recognized that the response to excessive or insufficient nutrient intake included an adjustment in efficiency that would result in a plateau. He observed: \"It has been found by numerous experiments that when the nutrients are fed in large excess, the body may continue for a time to store away part of the extra material, but after it has accumulated a certain amount, it refuses to take on more, and the daily consumption equals the supply even when this involves great waste.\"\n\nIn general, no essential nutrient is produced in the body. Nutrient kinetics therefore follow the plateau principle with the distinction that most are ingested by mouth and the body must contain an amount adequate for health. The plateau principle is important in determining how much time is needed to produce a deficiency when intake is insufficient. Because of this, pharmacokinetic considerations should be part of the information needed to set a dietary reference intake for essential nutrients.\n\nThe blood plasma concentration of vitamin C or ascorbic acid as a function of dose attains a plateau with a half-life of about 2 weeks. Bioavailability of vitamin C is highest at dosages below 200 mg per day. Above 500 mg, nearly all of excess vitamin C is excreted through urine.\n\nVitamin D metabolism is complex because the provitamin can be formed in the skin by ultraviolet irradiation or obtained from the diet. Once hydroxylated, the vitamin has a half-life of about 2 months.\nVarious studies have suggested that current intakes are inadequate for optimum bone health and much current research is aimed at determining recommendations for obtaining adequate circulating vitamin D and calcium while also minimizing potential toxicity.\n\nMany healthful qualities of foods and beverages may be related to the content of phytochemicals (see List of phytochemicals in food). Prime examples are flavonoids found in green tea, berries, cocoa and spice as well as in the skins and seeds of apples, onions and grapes.\n\nInvestigations into healthful benefits of phytochemicals follow exactly the same principles of pharmacokinetics that are required to study drug therapy. The initial concentration of any non-nutritive phytochemical in the blood plasma is zero unless a person has recently ingested a food or beverage. For example, as increasing amounts of green tea extract are consumed, a graded increase in plasma catechin can be measured, and the major compound is eliminated with a half-life of about 5 hours. Other considerations that must be evaluated include whether the ingested compound interacts favorably or unfavorably with other nutrients or drugs, and whether there is evidence for a threshold or toxicity at higher levels of intake.\n\nIt is especially common for people who are trying to lose weight to experience plateaus after several weeks of successful weight reduction. The plateau principle suggests that this leveling off is a sign of success. Basically, as one loses weight, less food energy is required to maintain the resting metabolic rate, which makes the initial regimen less effective. The idea of weight plateaus has been discussed for subjects who are participating in a calorie restriction experiment Food energy is expended largely through work done against gravity (see Joule), so weight reduction lessens the effectiveness of a given workout. In addition, a trained person has greater skill and therefore greater efficiency during a workout. Remedies include increasing the workout intensity or length and reducing portion sizes of meals more than may have been done initially.\n\nThe fact that weight loss and dieting reduce the metabolic rate is supported by research. In one study, heat production was reduced 30% in obese men after a weight loss program, and this led to resistance to further lose body weight. Whether body mass increases or decreases, adjustments in the thermic effect of food, resting energy expenditure, and non-resting energy expenditure all oppose further change.\n\nAny athlete who has trained for a sport has probably experienced plateaus, and this has given rise to various strategies to continue improving. Voluntary Skeletal muscle is in balance between the amount of muscle synthesized or renewed each day and the amount that is degraded. Muscle fibers respond to repetition and load, and increased training causes the quantity of exercised muscle fiber to increase exponentially (simply meaning that the greatest gains are seen during the first weeks of training). Successful training produces hypertrophy of muscle fibers as an adaptation to the training regimen. In order to make further gains, greater workout intensity is required with heavier loads and more repetitions, although improvement in skill can contribute to gains in ability.\n\nWhen a bodily constituent adjusts exponentially over time, it usually attains a new stable level as a result of the plateau principle. The new level may be higher than the initial level (hypertrophy) in the case of strength training or lower in the case of dieting or disuse atrophy. This adjustment contributes to homeostasis but does not require feedback regulation. Gradual, asymptotic approach to a new balance between synthesis and degradation produces a stable level. Because of this, the plateau principle is sometimes called the stability principle. Mathematically, the result is linear dynamics despite the fact that most biological processes are non-linear (see Nonlinear system) if considered over a very broad range of inputs.\n\nData from the Minnesota Starvation Experiment by Ancel Keys and others demonstrate that during food restriction, total body mass, fat mass and lean body mass follow an exponential approach to a new steady state. The observation that body mass changes exponentially during partial or complete starvation seems to be a general feature of adaptation to energy restriction.\n\nEach cell produces thousands of different kinds of protein and enzymes. One of the key methods of cellular regulation is to change the rate of transcription of messenger RNA, which gives rise to a change in the rate of synthesis for the protein that the messenger RNA encodes. The plateau principle explains why the concentration of different enzymes increases at unique rates in response to a single hormone. Because each enzyme is degraded with at a unique rate (each has a different half-life), the rate of change differs even when the same stimulus is applied. This principle has been demonstrated for the response of liver enzymes that degrade amino acids to cortisone, which is a catabolic hormone.\n\nThe method of approach to steady state has also been used to analyze the change in messenger RNA levels when synthesis or degradation changes, and a model has also been reported in which the plateau principle is used to connect the change in messenger RNA synthesis to the expected change in protein synthesis and concentration as a function of time.\n\nExcessive gain in body weight contributes to the metabolic syndrome, which may include elevated fasting blood sugar (or glucose), resistance to the action of insulin, elevated low-density lipoprotein (LDL cholesterol) or decreased high-density lipoprotein (HDL cholesterol), and elevated blood pressure. Although obesity by itself is not considered a disease, it increases the risk for Diabetes mellitus type II. Because body mass, fat mass and fat free mass all change exponentially during weight reduction, it is a reasonable hypothesis to expect that symptoms of metabolic syndrome will also adjust exponentially towards normal values.\n\nScientists have evaluated turnover of bodily constituents using radiotracer methods and stable isotopes. If given orally, the tracers are absorbed and move into the blood plasma, and are then distributed throughout the bodily tissues. In such studies, a multi-compartment model is required to analyze turnover by Isotopic labeling. The isotopic marker is called a tracer and the material being analyzed is the tracee.\n\nIn studies with humans, blood plasma is the only tissue that can be easily sampled. A common procedure is to analyze the dynamics by assuming that changes can be attributed to a sum of exponentials. A single mathematical compartment is usually assumed to follow first-order kinetics in accord with the plateau principle. There are many examples of this kind of analysis in nutrition, for example, in the study of metabolism of zinc, and carotenoids.\n\nThe commonest assumption in compartmental modeling is that material in a homogeneous compartment behaves exponentially. However, this assumption is sometimes modified to include a saturable response that follows Michaelis-Menten kinetics or a related model called a Hill equation. When the material in question is present at a concentration near the \"K\", it often behaves with pseudo first-order kinetics (see Rate equation) and the plateau principle applies despite the fact that the model is non-linear.\n\nCompartmental modeling in biomedical sciences primarily originated from the need to study metabolism by using tracers. In contrast, System dynamics originated as a simple method of developing mathematical models by Jay Wright Forrester and colleagues. System dynamics represents a compartment or pool as a stock and movement among compartments as flows. In general, the rate of flow depends on the amount of material in the stock to which it is connected. It is common to represent this dependence as a constant proportion (or first order) using a connector element in the model.\n\nSystem dynamics is one application of the field of Control theory. In the biomedical field, one of the strongest advocates for computer-based analysis of physiological problems was Dr. Arthur Guyton. For example, system dynamics has been used to analyze the problem of body weight regulation. Similar methods have been used to study the spread of epidemics (see Compartmental models in epidemiology).\n\nSoftware that solves systems of equations required for compartmental modeling and system dynamics makes use of Finite difference methods to represent a set of Ordinary differential equations. An expert appraisal of the different types of dynamic behavior that can be developed by application of the plateau principle to the field of system dynamics has been published.\n\n"}
{"id": "385944", "url": "https://en.wikipedia.org/wiki?curid=385944", "title": "Pontryagin class", "text": "Pontryagin class\n\nIn mathematics, the Pontryagin classes, named for Lev Pontryagin, are certain characteristic classes. The Pontryagin class lies in cohomology groups with degree a multiple of four. It applies to real vector bundles.\n\nGiven a real vector bundle \"E\" over \"M\", its \"k\"-th Pontryagin class \"p\"(\"E\") is defined as \nwhere:\n\nThe rational Pontryagin class \"p\"(\"E\", Q) is defined to be the image of \"p\"(\"E\") in \"H\"(\"M\", Q), the 4\"k\"-cohomology group of \"M\" with rational coefficients.\n\nThe total Pontryagin class \nis (modulo 2-torsion) multiplicative with respect to \nWhitney sum of vector bundles, i.e., \nfor two vector bundles \"E\" and \"F\" over \"M\". In terms of the individual Pontryagin classes \"p\", \nand so on.\n\nThe vanishing of the Pontryagin classes and Stiefel–Whitney classes of a vector bundle does not guarantee that the vector bundle is trivial. For example, up to vector bundle isomorphism, there is a unique nontrivial rank 10 vector bundle \"E\" over the 9-sphere. (The clutching function for \"E\" arises from the homotopy group π(O(10)) = Z/2Z.) The Pontryagin classes and Stiefel-Whitney classes all vanish: the Pontryagin classes don't exist in degree 9, and the Stiefel–Whitney class \"w\" of \"E\" vanishes by the Wu formula \"w\" = \"w\"\"w\" + Sq(\"w\"). Moreover, this vector bundle is stably nontrivial, i.e. the Whitney sum of \"E\" with any trivial bundle remains nontrivial. \n\nGiven a 2\"k\"-dimensional vector bundle \"E\" we have \nwhere \"e\"(\"E\") denotes the Euler class of \"E\", and formula_6 denotes the cup product of cohomology classes. \n\nAs was shown by Shiing-Shen Chern and André Weil around 1948, the rational Pontryagin classes \ncan be presented as differential forms which depend polynomially on the curvature form of a vector bundle. This Chern–Weil theory revealed a major connection between algebraic topology and global differential geometry.\n\nFor a vector bundle \"E\" over a \"n\"-dimensional differentiable manifold \"M\" equipped with a connection, the total Pontryagin class is expressed as \n"}
{"id": "6812823", "url": "https://en.wikipedia.org/wiki?curid=6812823", "title": "Product metric", "text": "Product metric\n\nIn mathematics, a product metric is a metric on the Cartesian product of finitely many metric spaces formula_1 which metrizes the product topology. The most prominent product metrics are the \"p\" product metrics for a fixed formula_2 :\nIt is defined as the \"p\" norm of the \"n\"-vector of the distances measured in \"n\" subspaces:\n\nFor formula_4 this metric is also called the sup metric:\n\nFor Euclidean spaces, using the L norm gives rise to the Euclidean metric in the product space; however, any other choice of \"p\" will lead to a topologically equivalent metric space. In the category of metric spaces (with Lipschitz maps having Lipschitz constant 1), the product (in the category theory sense) uses the sup metric.\n\nFor Riemannian manifolds formula_6 and formula_7, the product metric formula_8 on formula_9 is defined by \n\nfor formula_11 under the natural identification formula_12.\n\n"}
{"id": "30495939", "url": "https://en.wikipedia.org/wiki?curid=30495939", "title": "Quasitoric manifold", "text": "Quasitoric manifold\n\nIn mathematics, a quasitoric manifold is a topological analogue of the nonsingular projective toric variety of algebraic geometry. A smooth formula_1-dimensional manifold is a quasitoric manifold if it admits a smooth, locally standard action of an formula_2-dimensional torus, with orbit space an formula_2-dimensional simple convex polytope.\n\nQuasitoric manifolds were introduced in 1991 by M. Davis and T. Januszkiewicz, who called them \"toric manifolds\". However, the term \"quasitoric manifold\" was eventually adopted to avoid confusion with the class of compact smooth toric varieties, which are known to algebraic geometers as toric manifolds.\n\nQuasitoric manifolds are studied in a variety of contexts in algebraic topology, such as complex cobordism theory, and the other oriented cohomology theories.\n\nDenote the formula_4-th subcircle of the formula_2-torus formula_6 by formula_7 so that formula_8. Then coordinate-wise multiplication of formula_6 on formula_10 is called the standard representation.\n\nGiven open sets formula_11 in formula_12 and formula_13 in formula_10, that are closed under the action of formula_6, a formula_16-action on formula_12 is defined to be locally isomorphic to the standard representation if formula_18, for all formula_19 in formula_6, formula_21 in formula_11, where formula_23 is a homeomorphism formula_24, and formula_25 is an automorphism of formula_6.\n\nGiven a simple convex polytope formula_27 with formula_28 facets, a formula_6-manifold formula_12 is a quasitoric manifold over formula_27 if,\n\n\nThe definition implies that the fixed points of formula_12 under the formula_6-action are mapped to the vertices of formula_27 by formula_43, while points where the action is free project to the interior of the polytope.\n\nA quasitoric manifold can be described in terms of a dicharacteristic function and an associated dicharacteristic matrix. In this setting it is useful to assume that the facets formula_44 of formula_27 are ordered so that the intersection formula_46 is a vertex formula_47 of formula_48, called the initial vertex.\n\nA dicharacteristic function is a homomorphism formula_49, such that if formula_50 is a codimension-formula_51 face of formula_27, then formula_53 is a monomorphism on restriction to the subtorus formula_54 in formula_55.\n\nThe restriction of λ to the subtorus formula_56 corresponding to the initial vertex formula_47 is an isomorphism, and so formula_58 can be taken to be a basis for the Lie algebra of formula_6. The epimorphism of Lie algebras associated to λ may be described as a linear transformation formula_60, represented by the formula_61 dicharacteristic matrix formula_62 given by\n\nThe formula_4th column of formula_62 is a primitive vector formula_66 in formula_67, called the facet vector. As each facet vector is primitive, whenever the facets formula_68 meet in a vertex, the corresponding columns formula_69 form a basis of formula_67, with determinant equal to formula_71. The isotropy subgroup associated to each facet formula_72 is described by\n\nfor some formula_74 in formula_75.\n\nIn their original treatment of quasitoric manifolds, Davis and Januskiewicz introduced the notion of a characteristic function that mapped each facet of the polytope to a vector determining the isotropy subgroup of the facet, but this is only defined up to sign. In more recent studies of quasitoric manifolds, this ambiguity has been removed by the introduction of the dicharacteristic function and its insistence that each circle formula_76 be oriented, forcing a choice of sign for each vector formula_77. The notion of the dicharacteristic function was originally introduced V. Buchstaber and N. Ray to enable the study of quasitoric manifolds in complex cobordism theory. This was further refined by introducing the ordering of the facets of the polytope to define the initial vertex, which eventually leads to the above neat representation of the dicharacteristic matrix formula_62 as formula_79, where formula_80 is the identity matrix and formula_81 is an formula_82 submatrix.\n\nThe kernel formula_83 of the dicharacteristic function acts freely on the moment angle complex formula_84, and so defines a principal formula_83-bundle formula_86 over the resulting quotient space formula_12. This quotient space can be viewed as\n\nwhere pairs formula_89, formula_90 of formula_91 are identified if and only if formula_92 and formula_93 is in the image of formula_53 on restriction to the subtorus formula_95 that corresponds to the unique face formula_96 of formula_27 containing the point formula_98, for some formula_99.\n\nIt can be shown that any quasitoric manifold formula_12 over formula_27 is equivariently diffeomorphic to a quasitoric manifold of the form of the quotient space above.\n\n\nThe moment angle complex formula_109 is the formula_110-sphere formula_111, the kernel formula_83 is the diagonal subgroup formula_113, so the quotient of formula_109 under the action of formula_83 is formula_103.\n\n\nfor integers formula_124.\n\nThe moment angle complex formula_125 is a product of formula_2 copies of 3-sphere embedded in formula_127, the kernel formula_83 is given by\n\nso that the quotient of formula_125 under the action of formula_83 is the formula_2-th stage of a Bott tower. The integer values formula_124 are the tensor powers of the line bundles whose product is used in the iterated sphere-bundle construction of the Bott tower.\n\nCanonical complex line bundles formula_134 over formula_12 given by\n\ncan be associated with each facet formula_72 of formula_27, for formula_139, where formula_83 acts on formula_141, by the restriction of formula_83 to the formula_4-th subcircle of formula_55 embedded in formula_145. These bundles are known as the facial bundles associated to the quasitoric manifold. By the definition of formula_12, the preimage of a facet formula_147 is a formula_148-dimensional quasitoric facial submanifold formula_149 over formula_72, whose isotropy subgroup is the restriction of formula_53 on the subcircle formula_7 of formula_55. Restriction of formula_134 to formula_149 gives the normal 2-plane bundle of the embedding of formula_149 in formula_12.\n\nLet formula_158 in formula_159 denote the first Chern class of formula_134. The integral cohomology ring formula_161 is generated by formula_158, for formula_139, subject to two sets of relations. The first are the relations generated by the Stanley–Reisner ideal of formula_27; linear relations determined by the dicharacterstic function comprise the second set:\n\nTherefore only formula_166 are required to generate formula_161 multiplicatively.\n\n\n"}
{"id": "43821526", "url": "https://en.wikipedia.org/wiki?curid=43821526", "title": "The Theoretical Minimum", "text": "The Theoretical Minimum\n\nThe Theoretical Minimum: What You Need to Know to Start Doing Physics () is a popular science book by Leonard Susskind and George Hrabovsky. The book was initially published on January 29, 2013 by Basic Books.\n\nThe book is a mathematical introduction to various theoretical physics concepts, such as principle of least action, Lagrangian mechanics, Hamiltonian mechanics, Poisson brackets, and electromagnetism. It is the first book in a series called \"The Theoretical Minimum\", based on Stanford Continuing Studies courses taught by world renowned physicist Leonard Susskind. The courses collectively teach everything required to gain a basic understanding of each area of modern physics, including much of the fundamental mathematics.\n\nThe book, also published in 2014 by Penguin Books under the title Classical Mechanics: The Theoretical Minimum (), is complemented by video recordings of the complete lectures which are available on-line. There is also a supplemental website for the book.\n\nThe second book in the series, by Leonard Susskind and Art Friedman, was published in 2014 by Basic Books under the title Quantum Mechanics: The Theoretical Minimum ().Video recordings of the complete lectures are available on-line.\nThe third book in the series, by Leonard Susskind and Art Friedman, was published in 2017. This covers Special Relativity and Classical Field Theory\n\nLectures in the remaining three courses, on\nare available on-line as video recordings.\n\nFurther lecture courses in the Theoretical Minimum series have been delivered by Prof. Susskind, on\nand these are also available on-line as video recordings.\n\n"}
{"id": "6631661", "url": "https://en.wikipedia.org/wiki?curid=6631661", "title": "Transportation theory (mathematics)", "text": "Transportation theory (mathematics)\n\nIn mathematics and economics, transportation theory or transport theory is a name given to the study of optimal transportation and allocation of resources. The problem was formalized by the French mathematician Gaspard Monge in 1781.\n\nIn the 1920s A.N. Tolstoi was one of the first to study the transportation problem mathematically. In 1930, in the collection \"Transportation Planning Volume I\" for the National Commissariat of Transportation of the Soviet Union, he published a paper \"Methods of Finding the Minimal Kilometrage in Cargo-transportation in space\".\n\nMajor advances were made in the field during World War II by the Soviet mathematician and economist Leonid Kantorovich. Consequently, the problem as it is stated is sometimes known as the Monge–Kantorovich transportation problem. The linear programming formulation of the transportation problem is also known as the Hitchcock–Koopmans transportation problem. A similar problem was solved in 1917 by the Japanese mathematician Soichi Kakeya.\n\nSuppose that we have a collection of \"n\" mines mining iron ore, and a collection of \"n\" factories which use the iron ore that the mines produce. Suppose for the sake of argument that these mines and factories form two disjoint subsets \"M\" and \"F\" of the Euclidean plane R. Suppose also that we have a \"cost function\" \"c\" : R × R → [0, ∞), so that \"c\"(\"x\", \"y\") is the cost of transporting one shipment of iron from \"x\" to \"y\". For simplicity, we ignore the time taken to do the transporting. We also assume that each mine can supply only one factory (no splitting of shipments) and that each factory requires precisely one shipment to be in operation (factories cannot work at half- or double-capacity). Having made the above assumptions, a \"transport plan\" is a bijection \"T\" : \"M\" → \"F\".\nIn other words, each mine \"m\" ∈ \"M\" supplies precisely one factory \"T\"(\"m\") ∈ \"F\" and each factory is supplied by precisely one mine. \nWe wish to find the \"optimal transport plan\", the plan \"T\" whose \"total cost\"\n\nis the least of all possible transport plans from \"M\" to \"F\". This motivating special case of the transportation problem is an instance of the assignment problem.\nMore specifically, it is equivalent to finding a minimum weight matching in a bipartite graph.\n\nThe following simple example illustrates the importance of the cost function in determining the optimal transport plan. Suppose that we have \"n\" books of equal width on a shelf (the real line), arranged in a single contiguous block. We wish to rearrange them into another contiguous block, but shifted one book-width to the right. Two obvious candidates for the optimal transport plan present themselves:\nIf the cost function is proportional to Euclidean distance (\"c\"(\"x\", \"y\") = α|\"x\" − \"y\"|) then these two candidates are \"both\" optimal. If, on the other hand, we choose the strictly convex cost function proportional to the square of Euclidean distance (\"c\"(\"x\", \"y\") = α|\"x\" − \"y\"|), then the \"many small moves\" option becomes the unique minimizer.\n\nThe transportation problem as it is stated in modern or more technical literature looks somewhat different because of the development of Riemannian geometry and measure theory. The mines-factories example, simple as it is, is a useful reference point when thinking of the abstract case. In this setting, we allow the possibility that we may not wish to keep all mines and factories open for business, and allow mines to supply more than one factory, and factories to accept iron from more than one mine.\n\nLet \"X\" and \"Y\" be two separable metric spaces such that any probability measure on \"X\" (or \"Y\") is a Radon measure (i.e. they are Radon spaces). Let \"c\" : \"X\" × \"Y\" → [0, ∞] be a Borel-measurable function. Given probability measures μ on \"X\" and ν on \"Y\", Monge's formulation of the optimal transportation problem is to find a transport map \"T\" : \"X\" → \"Y\" that realizes the infimum\n\nwhere \"T\"(μ) denotes the push forward of μ by \"T\". A map \"T\" that attains this infimum (\"i.e.\" makes it a minimum instead of an infimum) is called an \"optimal transport map\".\n\nMonge's formulation of the optimal transportation problem can be ill-posed, because sometimes there is no \"T\" satisfying \"T\"(μ) = ν: this happens, for example, when μ is a Dirac measure but ν is not.\n\nWe can improve on this by adopting Kantorovich's formulation of the optimal transportation problem, which is to find a probability measure γ on \"X\" × \"Y\" that attains the infimum\n\nwhere Γ(μ, ν) denotes the collection of all probability measures on \"X\" × \"Y\" with marginals μ on \"X\" and ν on \"Y\". It can be shown that a minimizer for this problem always exists when the cost function \"X\" is lower semi-continuous and Γ(μ, ν) is a tight collection of measures (which is guaranteed for Radon spaces \"X\" and \"Y\"). (Compare this formulation with the definition of the Wasserstein metric \"W\" on the space of probability measures.) A gradient descent formulation for the solution of the Monge-Kantorovich problem was given by Sigurd Angenent, Steven Haker, and Allen Tannenbaum.\n\nThe minimum of the Kantorovich problem is equal to\n\nwhere the supremum runs over all pairs of bounded and continuous functions formula_5 and formula_6 such that\n\nFor formula_8, let formula_9 denote the collection of probability measures on formula_10 that have finite \"formula_11\"-th moment. Let formula_12 and let formula_13, where formula_14 is a convex function.\n\nThe proof of this solution appears in.\n\nLet \"formula_21\" be a separable Hilbert space. Let formula_22 denote the collection of probability measures on \"formula_21\" such that have finite formula_11-th moment; let formula_25 denote those elements formula_26 that are Gaussian regular: if formula_27 is any strictly positive Gaussian measure on formula_21 and formula_29, then formula_30 also.\n\nLet formula_31, formula_32, formula_33 for formula_34. Then the Kantorovich problem has a unique solution formula_35, and this solution is induced by an optimal transport map: i.e., there exists a Borel map formula_36 such that\n\nMoreover, if formula_38 has bounded support, then\n\nfor formula_15-almost all \"formula_41\" for some locally Lipschitz, \"c\"-concave and maximal Kantorovich potential formula_42. (Here formula_43 denotes the Gâteaux derivative of formula_42.)\n\nThe Monge-Kantorovich optimal transport has found applications in wide range in different fields. Among them are:\n\n"}
{"id": "21711296", "url": "https://en.wikipedia.org/wiki?curid=21711296", "title": "Valery Glivenko", "text": "Valery Glivenko\n\nValery Ivanovich Glivenko (, ; 2 January 1897 (Gregorian calendar) / 21 December 1896 (Julian calendar) in Kiev – 15 February 1940 in Moscow) was a Ukrainian Soviet mathematician. He worked in foundations of mathematics, real analysis, probability theory, and mathematical statistics. He taught at Moscow Industrial Pedagogical Institute until his death at age 43. Most of Glivenko's work was published in French.\n\n\n\n"}
