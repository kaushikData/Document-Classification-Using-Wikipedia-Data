{"id": "399064", "url": "https://en.wikipedia.org/wiki?curid=399064", "title": "43 (number)", "text": "43 (number)\n\n43 (forty-three) is the natural number following 42 and preceding 44.\n\nForty-three is the 14th smallest prime number. The previous is forty-one, with which it comprises a twin prime, and the next is forty-seven. 43 is the smallest prime that is not a Chen prime. It is also the third Wagstaff prime.\n\n43 is the fourth term of Sylvester's sequence, one more than the product of the previous terms (2 × 3 × 7).\n\n43 is a centered heptagonal number.\n\nLet \"a\" = \"a\" = 1, and thenceforth \"a\" = (\"a\" + \"a\" + ... + \"a\"). This sequence continues 1, 1, 2, 3, 5, 10, 28, 154... . \"a\" is the first term of this sequence that is not an integer.\n\n43 is a Heegner number.\n\n43 is the largest prime which dives the order of the Janko group J.\n\n43 is a repdigit in base 6 (111).\n\n43 is the largest natural number that is not a (original) McNugget number.\n\n43 is the smallest prime number expressible as the sum of 2, 3, 4, or 5 different primes:\n\nWhen taking the first six terms of the Taylor series for computing e, one obtains\n\nwhich is also five minus the fifth harmonic number.\n\nEvery solvable configuration of the Fifteen puzzle can be solved in no more than 43 multi-tile moves (i.e. when moving two or three tiles at once is counted as one move).\n\n\n\nIn auto racing:\n\n\n\nForty-three is:\n\n\n\n"}
{"id": "399104", "url": "https://en.wikipedia.org/wiki?curid=399104", "title": "48 (number)", "text": "48 (number)\n\n48 (forty-eight) is the natural number following 47 and preceding 49. It is one third of a gross, or four dozens.\n\nForty-eight is the double factorial of 6, a highly composite number. Like all other multiples of 6, it is a semiperfect number. 48 is the second 17-gonal number.\n\n48 is the smallest number with exactly ten divisors.\n\nThere are 11 solutions to the equation φ(\"x\") = 48, namely 65, 104, 105, 112, 130, 140, 144, 156, 168, 180 and 210. This is more than any integer below 48, making 48 a highly totient number.\n\nSince the greatest prime factor of 48 + 1 = 2305 is 461, which is clearly more than twice 48, 48 is a Størmer number.\n\n48 is a Harshad number in base 10. It has 24, 2, 12, and 4 as factors.\n\n\n\n\n\n\nForty-eight may also refer to:\n"}
{"id": "20967906", "url": "https://en.wikipedia.org/wiki?curid=20967906", "title": "ATS theorem", "text": "ATS theorem\n\nIn mathematics, the ATS theorem is the theorem on the approximation of a\ntrigonometric sum by a shorter one. The application of the ATS theorem in certain problems of mathematical and theoretical physics can be very helpful.\n\nIn some fields of mathematics and mathematical physics, sums of the form\n\nare under study.\n\nHere formula_2 and formula_3 are real valued functions of a real\nargument, and formula_4\nSuch sums appear, for example, in number theory in the analysis of the\nRiemann zeta function, in the solution of problems connected with\ninteger points in the domains on plane and in space, in the study of the\nFourier series, and in the solution of such differential equations as the wave equation, the potential equation, the heat conductivity equation.\n\nThe problem of approximation of the series (1) by a suitable function was studied already by Euler and \nPoisson.\n\nWe shall define\nthe length of the sum formula_5\nto be the number formula_6 \n(for the integers formula_7 and formula_8 this is the number of the summands in formula_5).\n\nUnder certain conditions on formula_2 and formula_3\nthe sum formula_5 can be\nsubstituted with good accuracy by another sum formula_13\n\nwhere the length formula_15 is far less than formula_16\n\nFirst relations of the form\n\nwhere formula_18 formula_19 are the sums (1) and (2) respectively, formula_20 is\na remainder term, with concrete functions formula_2 and formula_22\nwere obtained by G. H. Hardy and J. E. Littlewood,\nwhen they\ndeduced approximate functional equation for the Riemann zeta function\nformula_23 and by I. M. Vinogradov, in the study of\nthe amounts of integer points in the domains on plane.\nIn general form the theorem\nwas proved by J. Van der Corput, (on the recent\nresults connected with the Van der Corput theorem one can read at\n\nIn every one of the above-mentioned works,\nsome restrictions on the functions\nformula_2 and formula_3 were imposed. With\nconvenient (for applications) restrictions on\nformula_2 and formula_22 the theorem was proved by A. A. Karatsuba in (see also,).\n\n[1]. \"For\" formula_28\n\"or\" formula_29 \"the record\"\n\nformula_30\n\" means that there are the constants\" formula_31\n\"and\" formula_32\n\"such that\"\n\n[2]. \"For a real number\" formula_34 \"the record\"\nformula_35 \"means that\"\n\n\"where\"\n\n\"is the fractional part of\" formula_38\n\n\"Let the real functions\" \"ƒ\"(\"x\") \"and\" formula_2 \"satisfy on the segment\" [\"a\", \"b\"] \"the following conditions:\"\n\n1) formula_40 \"and\" formula_41 \"are continuous;\"\n\n2) \"there exist numbers\"\nformula_42 formula_43 \"and\" formula_44 \"such that\"\n\n\"Then, if we define the numbers\" formula_47 \"from the equation\"\n\n\"we have\"\n\n\"where\"\n\nformula_52\n\nThe most simple variant of the formulated theorem is the statement, which is called in the literature the Van der Corput lemma.\n\n\"Let\" formula_3 \"be a real differentiable function in the interval\"\nformula_56 \"moreover, inside of this interval, its derivative\"\nformula_57 \"is a monotonic and a sign-preserving function, and for the constant\" formula_58 \"such that\" formula_59 \"satisfies the inequality\"\nformula_60 \"Then\"\n\n\"where\" formula_62\n\nIf the parameters formula_7 and formula_64 are integers, then it is possible to substitute the last relation by the following ones:\n\nwhere formula_62\n\nOn the applications of ATS to the problems of physics see,; see also.\n"}
{"id": "33817092", "url": "https://en.wikipedia.org/wiki?curid=33817092", "title": "Anne Condon", "text": "Anne Condon\n\nAnne Elizabeth Condon, is an Irish-Canadian computer scientist, professor, and former head of the Computer Science Department of the University of British Columbia. Her research focuses on computational complexity theory, DNA computing, and bioinformatics. She has also held the NSERC/General Motors Canada Chair for Women in Science and Engineering from 2004 to 2009, and has worked to improve the success of women in the sciences and engineering.\n\nCondon did her undergraduate studies at University College Cork, earning a bachelor's degree in 1982. She moved to the University of Washington for her graduate studies, receiving her doctorate in 1987 under the supervision of Richard E. Ladner. She then joined the faculty at the University of Wisconsin–Madison, and remained there until her 1999 move to UBC.\n\nCondon won an ACM Distinguished Dissertation award (honorable mention)\nfor her thesis research. In 2010, the Association for Computing Machinery named her an ACM Fellow \"for contributions to\ncomplexity theory and leadership in advancing women in\ncomputing\". In the same year, she\nalso won the A. Nico Habermann Award of the Computing Research Association for \"long-standing and impactful service toward the goal\nof increasing the participation of women in computer science\nresearch.\" She is also\na winner of the University College Cork Distinguished Alumna\nAward, the University of Washington CSE Alumni\nAchievement Award.,\nand the 2012 University of Washington College of Engineering Diamond\nAward for Distinguished Achievement in Academia. She was the 2014 winner of the Grace Hopper Celebration of Women in Computing Technical Leadership ABIE Award\n\nCondon was elected a fellow of the Royal Society of Canada in 2012.\n\n\n"}
{"id": "37296018", "url": "https://en.wikipedia.org/wiki?curid=37296018", "title": "Birkhoff–Kellogg invariant-direction theorem", "text": "Birkhoff–Kellogg invariant-direction theorem\n\nIn functional analysis, the Birkhoff–Kellogg invariant-direction theorem, named after G. D. Birkhoff and O. D. Kellogg, is a generalization of the Brouwer fixed-point theorem. The theorem states that:\n\nLet \"U\" be a bounded open neighborhood of 0 in an infinite-dimensional normed linear space \"V\", and let \"F\":∂\"U\" → \"V\" be a compact map satisfying ||\"F\"(\"x\")|| ≥ α for some α > 0 for all \"x\" in ∂\"U\". Then \"F\" has an invariant direction, \"i.e.\", there exist some \"x\" and some \"λ\" > 0 satisfying \"x\" = \"λF\"(\"x\").\n\nThe Birkhoff–Kellogg theorem and its generalizations by Schauder and Leray have applications to partial differential equations.\n"}
{"id": "50650284", "url": "https://en.wikipedia.org/wiki?curid=50650284", "title": "Boolean Pythagorean triples problem", "text": "Boolean Pythagorean triples problem\n\nThe Boolean Pythagorean triples problem is a problem relating to Pythagorean triples which was solved using a computer-assisted proof in May 2016.\n\nThis problem is from Ramsey theory and asks if it is possible to color each of the positive integers either red or blue, so that no Pythagorean triple of integers \"a\", \"b\", \"c\", satisfying formula_1 are all the same color. For example, in the Pythagorean triple 3, 4 and 5 (formula_2), if 3 and 4 are colored red, then 5 must be colored blue.\n\nMarijn Heule, Oliver Kullmann and Victor Marek investigated the problem, and showed that such a coloring is impossible. Up to the number 7824 it is possible to color the numbers such that all Pythagorean triples are admissible, but the proof shows that no such coloring can be extended to also color the number 7825. The actual statement of the theorem proved is\nThere are 2 colorings for the numbers up to 7825. These possible colorings were logically and algorithmically narrowed down to around a trillion (still highly complex) cases, and those were examined using a Boolean satisfiability solver. Creating the proof took about 4 years of CPU time over two days on the Stampede supercomputer at the Texas Advanced Computing Center and generated a 200 terabyte propositional proof, which was compressed to 68 gigabytes.\n\nThe paper describing the proof was published on arXiv on 3 May 2016, and has been accepted for the SAT 2016 conference, where it won the best paper award.\n\nIn the 1980s Ronald Graham offered a $100 prize for the solution of the problem, which has now been awarded to Marijn Heule.\n"}
{"id": "714069", "url": "https://en.wikipedia.org/wiki?curid=714069", "title": "Church–Turing–Deutsch principle", "text": "Church–Turing–Deutsch principle\n\nIn computer science and quantum physics, the Church–Turing–Deutsch principle (CTD principle) is a stronger, physical form of the Church–Turing thesis formulated by David Deutsch in 1985.\n\nThe principle states that a universal computing device can simulate every physical process.\n\nThe principle was stated by Deutsch in 1985 with respect to finitary machines and processes. He observed that classical physics, which makes use of the concept of real numbers, cannot be simulated by a Turing machine, which can only represent computable reals. Deutsch proposed that quantum computers may actually obey the CTD principle, assuming that the laws of quantum physics can completely describe every physical process.\n\nAn earlier version of this thesis for classical computers was stated by Alan Turing's friend and student Robin Gandy in 1980.\n\n\n"}
{"id": "11503412", "url": "https://en.wikipedia.org/wiki?curid=11503412", "title": "Combinatorial commutative algebra", "text": "Combinatorial commutative algebra\n\nCombinatorial commutative algebra is a relatively new, rapidly developing mathematical discipline. As the name implies, it lies at the intersection of two more established fields, commutative algebra and combinatorics, and frequently uses methods of one to address problems arising in the other. Less obviously, polyhedral geometry plays a significant role.\n\nOne of the milestones in the development of the subject was Richard Stanley's 1975 proof of the Upper Bound Conjecture for simplicial spheres, which was based on earlier work of Melvin Hochster and Gerald Reisner. While the problem can be formulated purely in geometric terms, the methods of the proof drew on commutative algebra techniques.\n\nA signature theorem in combinatorial commutative algebra is the characterization of \"h\"-vectors of simplicial polytopes conjectured in 1970 by Peter McMullen. Known as the \"g\"-theorem, it was proved in 1979 by Stanley (necessity of the conditions, algebraic argument) and by Louis Billera and Carl W. Lee (sufficiency, combinatorial and geometric construction). A major open question is the extension of this characterization from simplicial polytopes to simplicial spheres, the \"g\"-conjecture.\n\n\n\nA foundational paper on Stanley–Reisner complexes by one of the pioneers of the theory:\n\nThe first book is a classic (first edition published in 1983):\n\nVery influential, and well written, textbook-monograph:\n\nAdditional reading:\n\nA recent addition to the growing literature in the field, contains exposition of current research topics:\n\n"}
{"id": "33827219", "url": "https://en.wikipedia.org/wiki?curid=33827219", "title": "Compartmental modelling of dendrites", "text": "Compartmental modelling of dendrites\n\nCompartmental modelling of dendrites deals with multi-compartment modelling of the dendrites, to make the understanding of the electrical behavior of complex dendrites easier. Basically, compartmental modelling of dendrites is a very helpful tool to develop new biological neuron models. Dendrites are very important because they occupy the most membrane area in many of the neurons and give the neuron an ability to connect to thousands of other cells. Originally the dendrites were thought to have constant conductance and current but now it has been understood that they may have active Voltage-gated ion channels, which influences the firing properties of the neuron and also the response of neuron to synaptic inputs. Many mathematical models have been developed to understand the electric behavior of the dendrites. Dendrites tend to be very branchy and complex, so the compartmental approach to understand the electrical behavior of the dendrites makes it very useful.\n\nConsider a two-compartmental model with the compartments viewed as isopotential cylinders with radius formula_1 and length formula_2. \n\nEach dendridic section is subdivided into segments, which are typically seen as uniform circular cylinders or tapered circular cylinders. In the traditional compartmental model, point process location is determined only to an accuracy of half segment length. This will make the model solution particularly sensitive to segment boundaries. The accuracy of the traditional approach for this reason is O(1/n) when a point current and synaptic input is present. Usually the trans-membrane current where the membrane potential is known is represented in the model at points, or nodes and is assumed to be at the center. The new approach partitions the effect of the input by distributing it to the boundaries of the segment. Hence any input is partitioned between the nodes at the proximal and distal boundaries of the segment. Therefore, this procedure makes sure that the solution obtained is not sensitive to small changes in location of these boundaries because it affects how the input is partitioned between the nodes. When these compartments are connected with continuous potentials and conservation of current at segment boundaries then a new compartmental model of a new mathematical form is obtained. This new approach also provides a model identical to the traditional model but an order more accurate. This model increases the accuracy and precision by an order of magnitude than that is achieved by point process input.\n\nDendrites and axons are considered to be continuous (cable-like), rather than series of compartments.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "3407326", "url": "https://en.wikipedia.org/wiki?curid=3407326", "title": "Corresponding sides and corresponding angles", "text": "Corresponding sides and corresponding angles\n\nIn geometry, the tests for congruence and similarity involve comparing corresponding sides and corresponding angles of polygons. In these tests, each side and each angle in one polygon is paired with a side or angle in the second polygon, taking care to preserve the order of adjacency. \n\nFor example, if one polygon has sequential sides \"a\", \"b\", \"c\", \"d\", and \"e\" and the other has sequential sides \"v\", \"w\", \"x\", \"y\", and \"z\", and if \"b\" and \"w\" are corresponding sides, then side \"a\" (adjacent to \"b\") must correspond to either \"v\" or \"x\" (both adjacent to \"w\"). If \"a\" and \"v\" correspond to each other, then \"c\" corresponds to \"x\", \"d\" corresponds to \"y\", and \"e\" corresponds to \"z\"; hence the \"i\" element of the sequence \"abcde\" corresponds to the \"i\" element of the sequence \"vwxyz\" for \"i\" = 1, 2, 3, 4, 5. On the other hand, if in addition to \"b\" corresponding to \"w\" we have \"c\" corresponding to \"v\", then the \"i\" element of \"abcde\" corresponds to the \"i\" element of the reverse sequence \"xwvzy\".\n\nCongruence tests look for all pairs of corresponding sides to be equal in length, though except in the case of the triangle this is not sufficient to establish congruence (as exemplified by a square and a rhombus that have the same side length). Similarity tests look at whether the ratios of the lengths of each pair of corresponding sides are equal, though again this is not sufficient. In either case equality of corresponding angles is also necessary; equality (or proportionality) of corresponding sides combined with equality of corresponding angles is necessary and sufficient for congruence (or similarity). The corresponding angles as well as the corresponding sides are defined as appearing in the same sequence, so for example if in a polygon with the side sequence \"abcde\" and another with the corresponding side sequence \"vwxyz\" we have vertex angle \"A\" appearing between sides \"a\" and \"b\" then its corresponding vertex angle \"V\" must appear between sides \"v\" and \"w\".\n"}
{"id": "663786", "url": "https://en.wikipedia.org/wiki?curid=663786", "title": "Crystallographic point group", "text": "Crystallographic point group\n\nIn crystallography, a crystallographic point group is a set of symmetry operations, like rotations or reflections, that leave a central point fixed while moving other directions and faces of the crystal to the positions of features of the same kind. For a periodic crystal (as opposed to a quasicrystal), the group must also be consistent with maintenance of the three-dimensional translational symmetry that defines crystallinity. The macroscopic properties of a crystal would look exactly the same before and after any of the operations in its point group. In the classification of crystals, each point group is also known as a crystal class.\n\nThere are infinitely many three-dimensional point groups. However, the crystallographic restriction of the infinite families of general point groups results in there being only 32 crystallographic point groups. These 32 point groups are one-and-the same as the 32 types of morphological (external) crystalline symmetries derived in 1830 by Johann Friedrich Christian Hessel from a consideration of observed crystal forms. \n\nThe point group of a crystal, among other things, determines directional variation of the physical properties that arise from its structure, including optical properties such as whether it is birefringent, or whether it shows the Pockels effect.\n\nThe point groups are denoted by their component symmetries. There are a few standard notations used by crystallographers, mineralogists, and physicists.\n\nFor the correspondence of the two systems below, see crystal system.\n\nIn Schoenflies notation, point groups are denoted by a letter symbol with a subscript. The symbols used in crystallography mean the following:\n\n\nDue to the crystallographic restriction theorem, \"n\" = 1, 2, 3, 4, or 6 in 2- or 3-dimensional space.\n\"D\" and \"D\" are actually forbidden because they contain improper rotations with n=8 and 12 respectively. The 27 point groups in the table plus \"T\", \"T\", \"T\", \"O\" and \"O\" constitute 32 crystallographic point groups.\n\nAn abbreviated form of the Hermann–Mauguin notation commonly used for space groups also serves to describe crystallographic point groups. Group names are\n\n\n"}
{"id": "7963", "url": "https://en.wikipedia.org/wiki?curid=7963", "title": "Disjunctive syllogism", "text": "Disjunctive syllogism\n\nIn classical logic, disjunctive syllogism (historically known as modus tollendo ponens (MTP), Latin for \"mode that affirms by denying\") is a valid argument form which is a syllogism having a disjunctive statement for one of its premises.\n\nAn example in English:\n\nIn propositional logic, disjunctive syllogism (also known as disjunction elimination and or elimination, or abbreviated ∨E), is a valid rule of inference. If we are told that at least one of two statements is true; and also told that it is not the former that is true; we can infer that it has to be the latter that is true. If \"P\" is true or \"Q\" is true and \"P\" is false, then \"Q\" is true. The reason this is called \"disjunctive syllogism\" is that, first, it is a syllogism, a three-step argument, and second, it contains a logical disjunction, which simply means an \"or\" statement. \"P or Q\" is a disjunction; P and Q are called the statement's \"disjuncts\". The rule makes it possible to eliminate a disjunction from a logical proof. It is the rule that:\n\nwhere the rule is that whenever instances of \"formula_2\", and \"formula_3\" appear on lines of a proof, \"formula_4\" can be placed on a subsequent line.\n\nDisjunctive syllogism is closely related and similar to hypothetical syllogism, in that it is also type of syllogism, and also the name of a rule of inference. It is also related to the law of noncontradiction, one of the .\n\nThe \"disjunctive syllogism\" rule may be written in sequent notation:\n\nwhere formula_6 is a metalogical symbol meaning that formula_4 is a syntactic consequence of formula_2, and formula_9 in some logical system;\n\nand expressed as a truth-functional tautology or theorem of propositional logic:\n\nwhere formula_11, and formula_4 are propositions expressed in some formal system.\n\nHere is an example:\n\nHere is another example:\n\nPlease observe that the disjunctive syllogism works whether 'or' is considered 'exclusive' or 'inclusive' disjunction. See below for the definitions of these terms.\n\nThere are two kinds of logical disjunction:\n\n\nThe widely used English language concept of \"or\" is often ambiguous between these two meanings, but the difference is pivotal in evaluating disjunctive arguments.\n\nThis argument:\n\nis valid and indifferent between both meanings. However, only in the \"exclusive\" meaning is the following form valid:\n\nHowever, if the fact is true it does not commit the fallacy.\n\nWith the \"inclusive\" meaning you could draw no conclusion from the first two premises of that argument. See affirming a disjunct.\n\nUnlike \"modus ponendo ponens\" and \"modus ponendo tollens\", with which it should not be confused, disjunctive syllogism is often not made an explicit rule or axiom of logical systems, as the above arguments can be proven with a (slightly devious) combination of reductio ad absurdum and disjunction elimination.\n\nOther forms of syllogism include:\n\nDisjunctive syllogism holds in classical propositional logic and intuitionistic logic, but not in some paraconsistent logics.\n\n"}
{"id": "2575903", "url": "https://en.wikipedia.org/wiki?curid=2575903", "title": "Duration calculus", "text": "Duration calculus\n\nDuration calculus (DC) is an interval logic for real-time systems. It was originally developed by Zhou Chaochen with the help of Anders P. Ravn and C. A. R. Hoare on the European ESPRIT Basic Research Action (BRA) \"ProCoS\" project on \"Provably Correct Systems\".\n\nDC is mainly useful at the requirements level of the software development process for real-time systems. Some tools are available (e.g., DCVALID, IDLVALID, etc.). Subsets of Duration Calculus have been studied (e.g., using discrete time rather than continuous time). DC is especially espoused by UNU-IIST in Macau and the Tata Institute of Fundamental Research in Mumbai, which are major centres of excellence for the approach.\n\n\n"}
{"id": "33185800", "url": "https://en.wikipedia.org/wiki?curid=33185800", "title": "Eisenstein integral", "text": "Eisenstein integral\n\nIn mathematical representation theory, the Eisenstein integral is an integral introduced by in the representation theory of semisimple Lie groups, analogous to Eisenstein series in the theory of automorphic forms.\n\n defined the Eisenstein integral by\nwhere:\n\n"}
{"id": "46930681", "url": "https://en.wikipedia.org/wiki?curid=46930681", "title": "Electrical Impedance and Diffuse Optical Tomography Reconstruction Software", "text": "Electrical Impedance and Diffuse Optical Tomography Reconstruction Software\n\nEIDORS is an open-source software tool box written mainly in MATLAB/GNU Octave designed primarily for image reconstruction from electrical impedance tomography(EIT) data, in a biomedical, industrial or geophysical setting. The name was originally an acronym for Electrical Impedance Tomography and Diffuse Optical Reconstruction Software. While the name reflects the original intention to cover image reconstruction of data from the mathematically similar near infra red diffuse optical imaging, to date there has been little development in that area.\n\nThe project was launched in 1999 with a Matlab code for 2D EIT reconstruction which had its origin in the PhD thesis of Marko Vauhkonen and the work of his supervisor Jari Kaipio at the University of Kuopio. While Kuopio also developed a three dimensional EIT code this was not released as open-source. Instead the three dimensional version of EIDORS was developed from work done at UMIST (now University of Manchester) by Nick Polydorides and William Lionheart.\n\nThe forward models in EIDORS use the finite element method and this requires mesh generation for sometimes irregular objects (such as human bodies), and the meshing needs to reflect the electrodes used to drive and measure current in EIT. For this purpose an interface was developed to the Netgen Mesh Generator.\n\nAs the project grew there was a desire to incorporate forward modelling and reconstruction code from a variety of groups and Andy Adler and Lionheart developed a more extensible software system. The most recent version is 3.8, released in May, 2015.\n\nThe EIDORS project also includes a repository of EIT data distributed under open-source licenses.\n\nEIDORS has been extensively used in biomedical applications of EIT, including lung imaging, measuring cardiac output. It has been used for investigation of imaging electrical activity in the brain, and monitoring conductivity changes during radio-frequency ablation. Outside medical imaging the toolbox has been used in process tomography, geophysics and materials science.\n\n"}
{"id": "34769960", "url": "https://en.wikipedia.org/wiki?curid=34769960", "title": "Exportation (logic)", "text": "Exportation (logic)\n\nExportation is a valid rule of replacement in propositional logic. The rule allows conditional statements having conjunctive antecedents to be replaced by statements having conditional consequents and vice versa in logical proofs. It is the rule that:\n\nformula_1\n\nWhere \"formula_2\" is a metalogical symbol representing \"can be replaced in a proof with.\"\n\nThe \"exportation\" rule may be written in sequent notation:\n\nwhere formula_4 is a metalogical symbol meaning that formula_5 is a syntactic equivalent of formula_6 in some logical system;\n\nor in rule form:\n\nwhere the rule is that wherever an instance of \"formula_9\" appears on a line of a proof, it can be replaced with \"formula_10\" and vice versa;\n\nor as the statement of a truth-functional tautology or theorem of propositional logic:\n\nwhere formula_12, formula_13, and formula_14 are propositions expressed in some logical system.\n\nAt any time, if P→Q is true, it can be replaced by P→(P∧Q). <br>\nOne possible case for P→Q is for P to be true and Q to be true; thus P∧Q is also true, and P→(P∧Q) is true. <br> \nAnother possible case sets P as false and Q as true. Thus, P∧Q is false and P→(P∧Q) is false; false→false is true. <br>\nThe last case occurs when both P and Q are false. Thus, P∧Q is false and P→(P∧Q) is true.\n\nIt rains and the sun shines implies that there is a rainbow. <br>\nThus, if it rains, then the sun shines implies that there is a rainbow.\n\nThe following proof uses Material Implication, double negation, De Morgan's Laws, the negation of the conditional statement, the Associative Property of conjunction, the negation of another conditional statement, and double negation again, in that order to derive the result.\n\nExportation is associated with Currying via the Curry–Howard correspondence.\n"}
{"id": "42334633", "url": "https://en.wikipedia.org/wiki?curid=42334633", "title": "Financial condition report", "text": "Financial condition report\n\nIn accounting, a financial condition report (FCR) is a report on the solvency condition of an insurance company that takes into account both the current financial status, as reflected in the balance sheet, and an assessment of the ability of the company to survive future risk scenarios. Risk assessment in a FCR involves \"dynamic solvency testing\", a type of dynamic financial analysis that simulates management response to risk scenarios, to test whether a company could remain solvent in the face of deteriorating economic conditions or major disasters. Dynamic solvency testing may involve both \"deterministic projections\", based on known risks, and \"stochastic projections\" that include random risk events.\n\nFCRs are a part of the statutory reporting requirements for life insurance companies in Pakistan, Australia, Ghana and non-life insurance companies in Canada. FCRs are also required in the UK under the \"Financial Services and Markets Act\" of 2000 and in Ireland. While many consider FCRs a type of compliance report, they can also be useful for corporate management to identify weaknesses in risk strategy, to test diversification of risk through, e.g., reinsurance, and to set fair market pricing for options.\n"}
{"id": "471365", "url": "https://en.wikipedia.org/wiki?curid=471365", "title": "Fuchsian group", "text": "Fuchsian group\n\nIn mathematics, a Fuchsian group is a discrete subgroup of PSL(2,R). The group PSL(2,R) can be regarded as a group of isometries of the hyperbolic plane, or conformal transformations of the unit disc, or conformal transformations of the upper half plane, so a Fuchsian group can be regarded as a group acting on any of these spaces. There are some variations of the definition: sometimes the Fuchsian group is assumed to be finitely generated, sometimes it is allowed to be a subgroup of PGL(2,R) (so that it contains orientation-reversing elements), and sometimes it is allowed to be a Kleinian group (a discrete group of PSL(2,C)) that is conjugate to a subgroup of PSL(2,R).\n\nFuchsian groups are used to create Fuchsian models of Riemann surfaces. In this case, the group may be called the Fuchsian group of the surface. In some sense, Fuchsian groups do for non-Euclidean geometry what crystallographic groups do for Euclidean geometry. Some Escher graphics are based on them (for the \"disc model\" of hyperbolic geometry).\n\nGeneral Fuchsian groups were first studied by , who was motivated by the paper , and therefore named them after Lazarus Fuchs.\n\nLet H = {\"z\" in C : Im(\"z\") > 0} be the upper half-plane. Then H is a model of the hyperbolic plane when endowed with the metric\n\nThe group PSL(2,R) acts on H by linear fractional transformations (also known as Möbius transformations):\n\nThis action is faithful, and in fact PSL(2,R) is isomorphic to the group of all orientation-preserving isometries of H.\n\nA Fuchsian group Γ may be defined to be a subgroup of PSL(2,R), which acts discontinuously on H. That is,\n\n\nAn equivalent definition for Γ to be Fuchsian is that Γ be a discrete group, which means that:\n\n\nAlthough discontinuity and discreteness are equivalent in this case, this is not generally true for the case of an arbitrary group of conformal homeomorphisms acting on the full Riemann sphere (as opposed to H). Indeed, the Fuchsian group PSL(2,Z) is discrete but has accumulation points on the real number line Im \"z\" = 0: elements of PSL(2,Z) will carry \"z\" = 0 to every rational number, and the rationals Q are dense in R.\n\nA linear fractional transformation defined by a matrix from PSL(2,C) will preserve the Riemann sphere P(C) = C ∪ ∞, but will send the upper-half plane H to some open disk Δ. Conjugating by such a transformation will send a discrete subgroup of PSL(2,R) to a discrete subgroup of PSL(2,C) preserving Δ.\n\nThis motivates the following definition of a Fuchsian group. Let Γ ⊂ PSL(2,C) act invariantly on a proper, open disk Δ ⊂ C ∪ ∞, that is, Γ(Δ) = Δ. Then Γ is Fuchsian if and only if any of the following three equivalent properties hold:\n\n\nThat is, any one of these three can serve as a definition of a Fuchsian group, the others following as theorems. The notion of an invariant proper subset Δ is important; the so-called Picard group PSL(2,Z[\"i\"]) is discrete but does not preserve any disk in the Riemann sphere. Indeed, even the modular group PSL(2,Z), which \"is\" a Fuchsian group, does not act discontinuously on the real number line; it has accumulation points at the rational numbers. Similarly, the idea that Δ is a proper subset of the region of discontinuity is important; when it is not, the subgroup is called a Kleinian group.\n\nIt is most usual to take the invariant domain Δ to be either the open unit disk or the upper half-plane.\n\nBecause of the discrete action, the orbit Γ\"z\" of a point \"z\" in the upper half-plane under the action of Γ has no accumulation points in the upper half-plane. There may, however, be limit points on the real axis. Let Λ(Γ) be the limit set of Γ, that is, the set of limit points of Γ\"z\" for \"z\" ∈ H. Then Λ(Γ) ⊆ R ∪ ∞. The limit set may be empty, or may contain one or two points, or may contain an infinite number. In the latter case, there are two types:\n\nA Fuchsian group of the first type is a group for which the limit set is the closed real line R ∪ ∞. This happens if the quotient space H/Γ has finite volume, but there are Fuchsian groups of the first kind of infinite covolume.\n\nOtherwise, a Fuchsian group is said to be of the second type. Equivalently, this is a group for which the limit set is a perfect set that is nowhere dense on R ∪ ∞. Since it is nowhere dense, this implies that any limit point is arbitrarily close to an open set that is not in the limit set. In other words, the limit set is a Cantor set.\n\nThe type of a Fuchsian group need not be the same as its type when considered as a Kleinian group: in fact, all Fuchsian groups are Kleinian groups of type 2, as their limit sets (as Kleinian groups) are proper subsets of the Riemann sphere, contained in some circle.\n\nAn example of a Fuchsian group is the modular group, PSL(2,Z). This is the subgroup of PSL(2,R) consisting of linear fractional transformations\n\nwhere \"a\", \"b\", \"c\", \"d\" are integers. The quotient space H/PSL(2,Z) is the moduli space of elliptic curves.\n\nOther Fuchsian groups include the groups Γ(\"n\") for each integer \"n\" > 0. Here Γ(\"n\") consists of linear fractional transformations of the above form where the entries of the matrix\n\nare congruent to those of the identity matrix modulo \"n\".\n\nA co-compact example is the (ordinary, rotational) (2,3,7) triangle group, containing the Fuchsian groups of the Klein quartic and of the Macbeath surface, as well as other Hurwitz groups. More generally, any hyperbolic von Dyck group (the index 2 subgroup of a triangle group, corresponding to orientation-preserving isometries) is a Fuchsian group.\n\nAll these are Fuchsian groups of the first kind.\n\n\nIf \"h\" is a hyperbolic element, the translation length \"L\" of its action in the upper half-plane is related to the trace of \"h\" as a 2×2 matrix by the relation\n\nA similar relation holds for the systole of the corresponding Riemann surface, if the Fuchsian group is torsion-free and co-compact.\n\n\n"}
{"id": "301504", "url": "https://en.wikipedia.org/wiki?curid=301504", "title": "Functional derivative", "text": "Functional derivative\n\nIn the calculus of variations, a field of mathematical analysis, the functional derivative (or variational derivative) relates a change in a functional to a change in a function on which the functional depends.\n\nIn the calculus of variations, functionals are usually expressed in terms of an integral of functions, their arguments, and their derivatives. In an integral of a functional, if a function is varied by adding to it another function that is arbitrarily small, and the resulting integrand is expanded in powers of , the coefficient of in the first order term is called the functional derivative.\n\nFor example, consider the functional\nwhere . If is varied by adding to it a function , and the resulting integrand is expanded in powers of , then the change in the value of to first order in can be expressed as follows:\nThe coefficient of , denoted as , is called the functional derivative of with respect to at the point . For this example functional, the functional derivative is the left hand side of the Euler-Lagrange equation,\n\nIn this section, the functional derivative is defined. Then the functional differential is defined in terms of the functional derivative.\n\nGiven a manifold representing (continuous/smooth) functions (with certain boundary conditions etc.), and a functional defined as \n\nthe functional derivative of \"ρ\"], denoted , is defined by\n\nwhere formula_6 is an arbitrary function. The quantity is called the variation of . In other words,\n\nis a linear functional, so by the Riesz–Markov–Kakutani representation theorem, this functional is given by integration against some measure.\nThen is defined to be the Radon–Nikodym derivative of this measure.\n\nWe think of the function as the gradient of at the point and \nas the directional derivative at point in the direction of . Then analogous to vector calculus, the inner product with the gradient gives the directional derivative.\n\nThe differential (or variation or first variation) of the functional formula_9 is \nHeuristically, formula_6 is the change in formula_12, so we 'formally' have formula_13, and then\nthis is similar in form to the total differential of a function formula_14,\nwhere formula_16 are independent variables. \nComparing the last two equations, the functional derivative formula_17 has a role similar to that of the partial derivative formula_18, where the variable of integration formula_19 is like a continuous version of the summation index formula_20.\n\nThe definition of a functional derivative may be made more mathematically precise and formal by defining the space of functions more carefully. For example, when the space of functions is a Banach space, the functional derivative becomes known as the Fréchet derivative, while one uses the Gâteaux derivative on more general locally convex spaces. Note that Hilbert spaces are special cases of Banach spaces. The more formal treatment allows many theorems from ordinary calculus and analysis to be generalized to corresponding theorems in functional analysis, as well as numerous new theorems to be stated.\n\nLike the derivative of a function, the functional derivative satisfies the following properties, where [\"ρ\"] and [\"ρ\"] are functionals:\nwhere are constants.\n\n\n\nWe give a formula to determine functional derivatives for a common class of functionals that can be written as the integral of a function and its derivatives. This is a generalization of the Euler–Lagrange equation: indeed, the functional derivative was introduced in physics within the derivation of the Lagrange equation of the second kind from the principle of least action in Lagrangian mechanics (18th century). The first three examples below are taken from density functional theory (20th century), the fourth from statistical mechanics (19th century).\n\nGiven a functional \nand a function () that vanishes on the boundary of the region of integration, from a previous section Definition,\n\nThe second line is obtained using the total derivative, where \"ρ\" is a derivative of a scalar with respect to a vector. The third line was obtained by use of a product rule for divergence. The fourth line was obtained using the divergence theorem and the condition that on the boundary of the region of integration. Since is also an arbitrary function, applying the fundamental lemma of calculus of variations to the last line, the functional derivative is\n\nwhere \"ρ\" = \"ρ\"() and , \"ρ\", ∇\"ρ\"). This formula is for the case of the functional form given by [\"ρ\"] at the beginning of this section. For other functional forms, the definition of the functional derivative can be used as the starting point for its determination. (See the example Coulomb potential energy functional.)\n\nThe above equation for the functional derivative can be generalized to the case that includes higher dimensions and higher order derivatives. The functional would be,\n\nwhere the vector , and is a tensor whose components are partial derivative operators of order , \n\nAn analogous application of the definition of the functional derivative yields\n\nIn the last two equations, the components of the tensor formula_31 are partial derivatives of with respect to partial derivatives of \"ρ\",\n\nand the tensor scalar product is,\n\nThe Thomas–Fermi model of 1927 used a kinetic energy functional for a noninteracting uniform electron gas in a first attempt of density-functional theory of electronic structure:\nSince the integrand of [\"ρ\"] does not involve derivatives of \"ρ\", the functional derivative of [\"ρ\"] is,\n\nFor the electron-nucleus potential, Thomas and Fermi employed the Coulomb potential energy functional\n\nApplying the definition of functional derivative,\n\nSo,\n\nFor the classical part of the electron-electron interaction, Thomas and Fermi employed the Coulomb potential energy functional\nFrom the definition of the functional derivative, \nThe first and second terms on the right hand side of the last equation are equal, since and in the second term can be interchanged without changing the value of the integral. Therefore,\nand the functional derivative of the electron-electron coulomb potential energy functional [\"ρ\"] is,\n\nThe second functional derivative is\n\nIn 1935 von Weizsäcker proposed to add a gradient correction to the Thomas-Fermi kinetic energy functional to make it suit better a molecular electron cloud:\nwhere\nUsing a previously derived formula for the functional derivative,\nand the result is,\n\nThe entropy of a discrete random variable is a functional of the probability mass function.\n\nThus,\n\nThus,\n\nLet\n\nUsing the delta function as a test function,\n\nThus,\n\nThis is particularly useful in calculating the correlation functions from the partition function in quantum field theory.\n\nA function can be written in the form of an integral like a functional. For example,\nSince the integrand does not depend on derivatives of \"ρ\", the functional derivative of \"ρ\" is,\n\nThe functional derivative of the iterated function formula_56 is given by:\n\nand \n\nIn general:\n\nPutting in N=0 gives:\n\nIn physics, it's common to use the Dirac delta function formula_61 in place of a generic test function formula_62, for yielding the functional derivative at the point formula_63 (this is a point of the whole functional derivative as a partial derivative is a component of the gradient):\n\nThis works in cases when formula_65 formally can be expanded as a series (or at least up to first order) in formula_66. The formula is however not mathematically rigorous, since formula_67 is usually not even defined.\n\nThe definition given in a previous section is based on a relationship that holds for all test functions , so one might think that it should hold also when is chosen to be a specific function such as the delta function. However, the latter is not a valid test function (it is not even a proper function).\n\nIn the definition, the functional derivative describes how the functional formula_68 changes as a result of a small change in the entire function formula_69. The particular form of the change in formula_69 is not specified, but it should stretch over the whole interval on which formula_19 is defined. Employing the particular form of the perturbation given by the delta function has the meaning that formula_69 is varied only in the point formula_63. Except for this point, there is no variation in formula_69.\n\n"}
{"id": "34802284", "url": "https://en.wikipedia.org/wiki?curid=34802284", "title": "Gauss's Pythagorean right triangle proposal", "text": "Gauss's Pythagorean right triangle proposal\n\nGauss's Pythagorean right triangle proposal is an idea attributed to Carl Friedrich Gauss for a method to signal extraterrestrial beings by constructing an immense right triangle and three squares on the surface of the Earth. The shapes would be a symbolical representation of the Pythagorean theorem, large enough to be seen from the Moon or Mars.\n\nAlthough credited in numerous sources as originating with Gauss, with exact details of the proposal set out, the specificity of detail, and even whether Gauss made the proposal, have been called into question. Many of the earliest sources do not actually name Gauss as the originator, instead crediting a \"German Astronomer\" or using other nonspecific descriptors, and in some cases naming a different author entirely. The details of the proposal also change significantly upon different retellings. Nevertheless, Gauss's writings reveal a belief and interest in finding a method to contact extraterrestrial life, and that he did, at the least, propose using amplified light using a heliotrope, his own 1818 invention, to signal supposed inhabitants of the Moon.\n\nCarl Friedrich Gauss is credited with an 1820 proposal for a method to signal extraterrestrial beings in the form of drawing an immense right triangle and three squares on the surface of the Earth, intended as a symbolical representation of the Pythagorean theorem, large enough to be seen from the Moon or Mars. Details vary between sources, but typically the \"drawing\" was to be constructed on the Siberian tundra, and made up of vast strips of pine forest forming the right triangle's borders, with the interior of the drawing and exterior squares composed of fields of wheat. Gauss is said to have been convinced that Mars harbored intelligent life and that this geometric figure, invoking the Pythagorean theorem through the squares on the outside borders (sometimes called a \"windmill diagram\", as originated by Euclid), would demonstrate to such alien observers the reciprocal existence of intelligent life on Earth and its grounding in mathematics. Wheat was said to be chosen by Gauss for contrast with the pine tree borders \"because of its uniform color\".\n\nThe specificity of the proposal's details as it appears in most later sources—even its attribution to Gauss—is called into question in University of Notre Dame Professor Michael J. Crowe's 1986 book, \"The Extraterrestrial Life Debate, 1750–1900\", in which he surveys the origins of the Gauss proposal and observes that:The history of this proposal ... can be traced through two dozen or more pluralist writings reaching back to the first half of the nineteenth century. When this is done, however, it turns out that the story exists in almost as many forms as its retellings. Furthermore, these versions share one characteristic: Never is reference supplied to where in the writings of Gauss ... the [proposal] appear[s]!Some early sources explored by Crowe for the attribution and form of Gauss's proposal include Austrian astronomer, Joseph Johann Littrow's statement in \"Wunder des Himmels\" that \"one of our most distinguished geometers\" proposed that a geometric figure \"for example the well known so-called square of the hypotenuse, be laid out on a large scale, say on a particular broad plain of the earth\". and Patrick Scott's \"Love in the Moon\", in which a \"learned man\" is described as proposing a signal formed by a \"great plantation of tree\" in the form of \"47th Proposition of Euclid\" in \"the great African dessert \". In \"Chambers's Edinburgh Journal\" it was written that a Russian savant had proposed to \"communicate with the moon by cutting a large figure of the forty-seventh proposition of Euclid on the plains of Siberia, which, he said, any fool would understand\".\n\nIn the writings of astronomers Asaph Hall and of Norman Lockyer, each refer separately to a \"German Astronomer\" who proposed the method of contact be by \"fire signals\" from Siberia. Writing in 1902, Simon Newcomb placed the origin of a Siberian triangle \"several hundred miles in extent\" not with Gauss, but at the feet of German astronomer Franz Xaver von Zach. In lectures presented by François Arago at the Paris Observatory, he named Siberia as the location of an extraterrestrial signaling project advanced by an unnamed \"German geometer\", but that the signaling method was to be through the use of mirrors, rather than any large symbol drawn upon the Earth. Despite this version's departure from a geometric figure, the appearance of mirrors as a signaling device has a connection with Gauss's background. Gauss invented the Heliotrope in 1818, an instrument that uses a mirror to reflect sunlight in a manner allowing a square mirror to be seen away even in sunny weather.\n\nGauss wrote of the heliotrope's potential as a celestial signaling device in a March 25, 1822 letter to Heinrich Olbers, by which he reveals a belief and interest in finding a method to contact extraterrestrial life: \"With 100 separate mirrors, each of 16 square feet, used conjointly, one would be able to send good heliotrope-light to the moon ... This would be a discovery even greater than that of America, if we could get in touch with our neighbors on the moon.\" Finally, in the October 1826 issue of the \"Edinburgh New Philosophical Journal\" an unnamed author wrote that in a conversation with Franz von Gruithuisen, Gauss stated words to the effect that \"the plan of erecting a geometrical figure on the plains of Siberia corresponded with his opinion, because, according to his view a correspondence with the inhabitants of the moon could only be begun by means of such mathematical contemplations and ideas, which we and they have in common.\" Crowe concluded in sum that his review of earliest sources failed to confirm the detail of the proposal and Gauss as its author, but that his origination of the idea was not unlikely given the existing evidence.\n"}
{"id": "22844401", "url": "https://en.wikipedia.org/wiki?curid=22844401", "title": "Homotopy analysis method", "text": "Homotopy analysis method\n\nThe homotopy analysis method (HAM) is a semi-analytical technique to solve nonlinear ordinary/partial differential equations. The homotopy analysis method employs the concept of the homotopy from topology to generate a convergent series solution for nonlinear systems. This is enabled by utilizing a homotopy-Maclaurin series to deal with the nonlinearities in the system.\n\nThe HAM was first devised in 1992 by Liao Shijun of Shanghai Jiaotong University in his PhD dissertation and further modified in 1997 to introduce a non-zero auxiliary parameter, referred to as the convergence-control parameter, c, to construct a homotopy on a differential system in general form. The convergence-control parameter is a non-physical variable that provides a simple way to verify and enforce convergence of a solution series. The capability of the HAM to naturally show convergence of the series solution is unusual in analytical and semi-analytic approaches to nonlinear partial differential equations.\n\nThe HAM distinguishes itself from various other analytical methods in four important aspects. First, it is a series expansion method that is not directly dependent on small or large physical parameters. Thus, it is applicable for not only weakly but also strongly nonlinear problems, going beyond some of the inherent limitations of the standard perturbation methods. Second, the HAM is a unified method for the Lyapunov artificial small parameter method, the delta expansion method, the Adomian decomposition method, and the homotopy perturbation method. The greater generality of the method often allows for strong convergence of the solution over larger spatial and parameter domains. Third, the HAM gives excellent flexibility in the expression of the solution and how the solution is explicitly obtained. It provides great freedom to choose the basis functions of the desired solution and the corresponding auxiliary linear operator of the homotopy. Finally, unlike the other analytic approximation techniques, the HAM provides a simple way to ensure the convergence of the solution series.\n\nThe homotopy analysis method is also able to combine with other techniques employed in nonlinear differential equations such as spectral methods and Padé approximants. It may further be combined with computational methods, such as the boundary element method to allow the linear method to solve nonlinear systems. Different from the numerical technique of homotopy continuation, the homotopy analysis method is an analytic approximation method as opposed to a discrete computational method. Further, the HAM uses the homotopy parameter only on a theoretical level to demonstrate that a nonlinear system may be split into an infinite set of linear systems which are solved analytically, while the continuation methods require solving a discrete linear system as the homotopy parameter is varied to solve the nonlinear system.\n\nIn the last twenty years, the HAM has been applied to solve a growing number of nonlinear ordinary/partial differential equations in science, finance, and engineering. \nFor example, multiple steady-state resonant waves in deep and finite water depth were found with the wave resonance criterion of arbitrary number of traveling gravity waves; this agreed with Phillips' criterion for four waves with small amplitude. Further, a unified wave model applied with the HAM, admits not only the traditional smooth progressive periodic/solitary waves, but also the progressive solitary waves with peaked crest in finite water depth. This model shows peaked solitary waves are consistent solutions along with the known smooth ones. Additionally, the HAM has been applied to many other nonlinear problems such as nonlinear heat transfer, the limit cycle of nonlinear dynamic systems, the American put option, the exact Navier–Stokes equation, the option pricing under stochastic volatility, the electrohydrodynamic flows, the Poisson–Boltzmann equation for semiconductor devices, and others.\n\nConsider a general nonlinear differential equation\n\nwhere formula_2 is a nonlinear operator. Let formula_3 denote an auxiliary linear operator, \"u\"(\"x\") an initial guess of \"u\"(\"x\"), and \"c\" a constant (called the convergence-control parameter), respectively. Using the embedding parameter \"q\" ∈ [0,1] from homotopy theory, one may construct a family of equations,\n\ncalled the zeroth-order deformation equation, whose solution varies continuously with respect to the embedding parameter \"q\" ∈ [0,1]. This is the linear equation\n\nwith known initial guess \"U\"(\"x\"; 0) = \"u\"(\"x\") when \"q\" = 0, but is equivalent to the original nonlinear equation formula_6, when \"q\" = 1, i.e. \"U\"(\"x\"; 1) = \"u\"(\"x\")). Therefore, as \"q\" increases from 0 to 1, the solution \"U\"(\"x\"; \"q\") of the zeroth-order deformation equation varies (or deforms) from the chosen initial guess \"u\"(\"x\") to the solution \"u\"(\"x\") of the considered nonlinear equation.\n\nExpanding \"U\"(\"x\"; \"q\") in a Taylor series about \"q\" = 0, we have the homotopy-Maclaurin series\n\nAssuming that the so-called convergence-control parameter \"c\" of the zeroth-order deformation equation is properly chosen that the above series is convergent at \"q\" = 1, we have the homotopy-series solution\n\nFrom the zeroth-order deformation equation, one can directly derive the governing equation of \"u\"(\"x\")\n\ncalled the \"m\"-order deformation equation, where formula_10 and formula_11 for \"k\" > 1, and the right-hand side \"R\" is dependent only upon the known results \"u\", \"u\", ..., \"u\" and can be obtained easily using computer algebra software. In this way, the original nonlinear equation is transferred into an infinite number of linear ones, but without the assumption of any small/large physical parameters.\n\nSince the HAM is based on a homotopy, one has great freedom to choose the initial guess \"u\"(\"x\"), the auxiliary linear operator formula_3, and the convergence-control parameter \"c\" in the zeroth-order deformation equation. Thus, the HAM provides the mathematician freedom to choose the equation-type of the high-order deformation equation and the base functions of its solution. The optimal value of the convergence-control parameter \"c\" is determined by the minimum of the squared residual error of governing equations and/or boundary conditions after the general form has been solved for the chosen initial guess and linear operator. Thus, the convergence-control parameter \"c\" is a simple way to guarantee the convergence of the homotopy series solution and differentiates the HAM from other analytic approximation methods. The method overall gives a useful generalization of the concept of homotopy.\n\nThe HAM is an analytic approximation method designed for the computer era with the goal of \"computing with functions instead of numbers.\" In conjunction with a computer algebra system such as Mathematica or Maple, one can gain analytic approximations of a highly nonlinear problem to arbitrarily high order by means of the HAM in only a few seconds. Inspired by the recent successful applications of the HAM in different fields, a Mathematica package based on the HAM, called BVPh, has been made available online for solving nonlinear boundary-value problems . BVPh is a solver package for highly nonlinear ODEs with singularities, multiple solutions, and multipoint boundary conditions in either a finite or an infinite interval, and includes support for certain types of nonlinear PDEs. Another HAM-based Mathematica code, APOh, has been produced to solve for an explicit analytic approximation of the optimal exercise boundary of American put option, which is also available online .\n\nThe HAM has recently been reported to be useful for obtaining analytical solutions for nonlinear frequency response equations. Such solutions are able to capture various nonlinear behaviors such as hardening-type, softening-type or mixed behaviors of the oscillator. These analytical equations are also useful in prediction of chaos in nonlinear systems.\n\n"}
{"id": "843071", "url": "https://en.wikipedia.org/wiki?curid=843071", "title": "Indian numbering system", "text": "Indian numbering system\n\nThe Indian numbering system is used in the Indian subcontinent (Bangladesh, India, Nepal, Maldives, Pakistan and Sri Lanka) and in Burma. The terms \"lakh\" (100,000 or 1,00,000 in the Indian system) and \"crore\" (10,000,000 or 1,00,00,000 in the Indian system) are used in Indian English to express large numbers. For example, in India 150,000 rupees becomes 1.5 lakh rupees, written as 1,50,000 or INR 1,50,000, while 30,000,000 (thirty million) rupees becomes 3 crore rupees, written as 3,00,00,000 with commas at the thousand, \"lakh\", and \"crore\" levels, and 1,000,000,000 (one billion) rupees (one hundred crore rupees or one arab अरब ) is written 1,00,00,00,000. While there are specific terms for numbers larger than 1 crore, these are not commonly used, and most practitioners are not familiar with these. In common parlance, the thousand, lakh, crore terminology repeats for larger numbers. Thus 1,000,000,000,000 (one trillion) becomes 1 lakh crore, and is written as 10,00,00,00,00,000.\n\nThe Indian numbering is equal to the western numbering system from ones then tens then hundreds then thousand then ten thousand. After ten thousand, the Indian and the western numbering diverge. According to the Indian numbering system, after ten thousand, the next power of ten is one lakh, then ten lakh, then one crore, then ten crore, and so on. According to the Western numbering system, after ten thousand, the next power of ten is referred to as one hundred thousand, then one million, then ten million, then one hundred million, and so on. The two numbering systems are not functionally different — 10,000 (Indian system) = 10,000 (Western system) — but they assign different names to the same number.\n\nThe Indian numbering system uses separators differently from the international norm. Instead of grouping digits by threes as in the international system, the Indian numbering system groups the rightmost three digits together (until the hundreds place), and thereafter groups by sets of two digits. One trillion would thus be written as 10,00,00,00,00,000 or one lakh crore (or 10 kharab). This makes the number convenient to read using thousand, lakh, crore terminology. Thus:\nThis accords with the Indian numbering system, which has units for thousands, hundreds of thousands, tens of millions, etc.\n\nThe table below follows the short scale usage of one billion being one thousand million. In India, Bangladesh and Pakistan, following former British usage, the long scale was used, with one billion equivalent to one million million.\n\nThere are various systems of numeration found in various ancient Vedic literatures of India. The following table gives one such system used in the Tamil ancient numbering system.\n"}
{"id": "27808556", "url": "https://en.wikipedia.org/wiki?curid=27808556", "title": "Integer circuit", "text": "Integer circuit\n\nIn computational complexity theory, an integer circuit is a circuit model of computation in which inputs to the circuit are sets of integers and each gate of the circuit computes either a set operation or an arithmetic operation on its input sets.\n\nAs an algorithmic problem, the possible questions are to find if a given integer is an element of the output node or if two circuits compute the same set. The decidability is still an open question, but there are results on restriction of thoses circuits. Finding answers to some questions about this model could serve as a proof to many important mathematical conjectures, like Goldbach's conjecture. \n\nIt is a natural extension of the circuits over sets of natural numbers when the considered set contains also negative integers, the definitions, which does not change, will not be repeated on this page. Only the differences will be mentioned.\n\nThe membership problem is the problem of deciding, given an integer circuit \"C\", an input to the circuit \"X\", and a specific integer \"n\", whether the integer \"n\" is in the output of the circuit \"C\" when provided with input \"X\". The computational complexity of this problem depends on the type of gates allowed in the circuit \"C\". The table below summarizes the computational complexity of the membership problem for various classes of integer circuits.\nHere, MFformula_1(O) denotes the classes defined by O-formulae, which are O-circuits with maximal fan-out 1.\n"}
{"id": "157972", "url": "https://en.wikipedia.org/wiki?curid=157972", "title": "John Colenso", "text": "John Colenso\n\nJohn William Colenso (24 January 1814 – 20 June 1883) was a British mathematician, theologian, Biblical scholar and social activist, who was the first Church of England Bishop of Natal.\n\nColenso was born at St Austell, Cornwall, on 24 January 1814. His father (John William Colenso) invested his capital into a mineral works in Pentewan, Cornwall, but the speculation proved to be ruinous when the investment was lost following a sea flood. His cousin was William Colenso, a missionary in New Zealand.\n\nFamily financial problems meant that Colenso had to take a job as an usher in a private school before he could attend university. These earnings and a loan of £30 raised by his relatives paid for his first year at St John's College, Cambridge where he was a sizar scholar. In 1836 he was Second Wrangler and Smith's Prizeman at Cambridge, and in 1837 he became fellow of St John's. Two years later he went to Harrow School as mathematical tutor, but the step proved an unfortunate one. The school was at its lowest ebb, and Colenso not only had few pupils, but lost most of his property in a fire. He returned to Cambridge burdened by an enormous debt of £5,000. However, within a relatively short period of time he paid off this debt by diligent tutoring and the sale to Longmans of his copyright interest in the highly successful and widely read manuals he had written on algebra (in 1841) and arithmetic (in 1843).\n\nColenso's early theological thinking was heavily influenced by Frederick Maurice to whom he was introduced by his wife and by Samuel Taylor Coleridge.\n\nIn 1846 he became rector of Forncett St Mary, Norfolk, and in 1853 he was recruited by the Bishop of Cape Town, Robert Gray, to be the first Bishop of Natal.\n\nColenso was a significant figure in the history of the published word in nineteenth century South Africa. He first wrote a short but vivid account of his initial journeying in Natal, \"Ten Weeks in Natal: A Journal of a First Tour of Visitation Among the Colonists and Zulu Kaffirs of Natal\" (Cambridge, 1855). Using the printing press he brought to his missionary station at Ekukhanyeni in Natal, and with William Ngidi he published the first Zulu Grammar and English/Zulu dictionary. His 1859 journey across Zululand to visit Mpande (the then Zulu King) and meet with Cetshwayo (Mpande's son and the Zulu King at the time of the Zulu War) was recorded in his book \"First Steps of the Zulu Mission\". The same journey was also described in the first book written by native South Africans in Zulu – \"Three Native Accounts\" by Magema Fuze, Ndiyane and William Ngidi. He also translated the New Testament and other portions of Scripture into Zulu.\n\nThrough the influence of his talented and well-educated wife, Colenso became one of only a handful of theologians to embrace Frederick Maurice, who was raised a Unitarian but joined the Church of England to help it \"purify and elevate the mind of the nation\".\n\nBefore his missionary career Colenso's volume of sermons dedicated to Frederick Maurice signalled the critical approach he would later apply to biblical interpretation and the baleful impact on native Africans of colonial expansion in southern Africa.\n\nColenso first courted controversy with the publication in 1855 of his \"Remarks on the Proper Treatment of Polygamy\"; one of the most cogent Christian-based arguments for tolerance of polygamy.\n\nColenso's experiences in Natal informed his development as a religious thinker. In his commentary upon St Paul's \"Epistle to the Romans\" (1861) he countered the doctrine of eternal punishment and the contention that Holy Communion was a precondition to salvation. Colenso, as a missionary, would not preach that the ancestors of newly Christianised Africans were condemned to eternal damnation. The thought provoking questions put to him by students at his missionary station encouraged him to re-examine the contents of the \"Pentateuch\" and the \"Book of Joshua\" and question whether certain sections of these books should be understood as literally or historically accurate. His conclusions, positive and negative, were published in a series of treatises on the \"Pentateuch\" and the \"Book of Joshua\", over a period of time from 1862 to 1879. The publication of these volumes created a scandal in England and were the cause of a number of anguished and patronising counter-blasts from those (clergy and laity alike) who refused to countenance the possibility of biblical fallibility. Colenso's work attracted the notice of biblical scholars on the continent such as Abraham Kuenen and played an important contribution in the development of biblical scholarship\n\nColenso's biblical criticism and his high-minded views about the treatment of African natives created a frenzy of alarm and opposition from the High Church party in South Africa and in England. As controversy raged in England, the South African bishops headed by Bishop Gray pronounced Colenso's deposition in December 1863. Colenso, who had refused to appear before this tribunal otherwise than by sending a proxy protest (delivered by his friend Wilhelm Bleek), appealed to the Judicial Committee of the Privy Council in London. The Privy Council eventually decided that the Bishop of Cape Town had no coercive jurisdiction and no authority to interfere with the Bishop of Natal. In view of this finding of \"ultra vires\" there was no opinion given upon the allegations of heresy made against Colenso. The first Lambeth Conference was convened in 1867 to address concerns raised by the Privy Council's decision in favour of Colenso.\n\nHis adversaries, though unable to remove him from his episcopal office, succeeded in restricting his ability to preach both in Natal and in England. Bishop Gray not only excommunicated him but consecrated a rival bishop (W.K. Macrorie), who took the title of \"Bishop of Maritzburg\" (the latter a common name for Pietermaritzburg). The contributions of the missionary societies were withdrawn, but an attempt to deprive him of his episcopal income and the control of St Peter's Cathedral in Pietermaritzburg was frustrated by another court ruling. Colenso, encouraged by a handsome testimonial raised in England to which many clergymen subscribed, returned to his diocese. A rival cathedral was built but it has long been sold and moved. The new Cathedral of the Nativity, beside St Peter's, honours both Bishop Colenso and Bishop Macrorie in the names it has given to its halls.\n\nColenso devoted the latter years of his life to further labours as a biblical commentator and as an advocate for native Africans in Natal and Zululand who had been unjustly treated by the colonial regime in Natal. In 1874 he took up the cause of Langalibalele and the Hlubi and Ngwe tribes in representations to the Colonial Secretary, Lord Carnarvon. Langalibalele had been falsely accused of rebellion in 1873 and, following a charade of a trial, was found guilty and imprisoned on Robben Island. In taking the side of Langalibalele against the Colonial regime in Natal and Theophilus Shepstone, the Secretary for Native Affairs, Colenso found himself even further estranged from colonial society in Natal.\n\nColenso's concern about the misleading information that was being provided to the Colonial Secretary in London by Shepstone and the Governor of Natal prompted him to devote much of the final part of his life to championing the cause of the Zulus against Boer oppression and official encroachments. He was a prominent critic of Sir Bartle Frere's efforts to depict the Zulu kingdom as a threat to Natal. Following the conclusion of the Anglo-Zulu War he interceded on behalf of Cetshwayo with the British government and succeeded in getting him released from Robben Island and returned to Zululand.\n\nHe was known as Sobantu (father of the people) to the native Africans in Natal and had a close relationship with members of the Zulu royal family; one of whom, Mkhungo (a son of Mpande), was taught at his school in Bishopstowe. After his death his wife and daughters continued his work supporting the Zulu cause and the organisation that eventually became the African National Congress.\n\nColenso was a polygenist; he believed in CoAdamism that races had been created separately. Colenso pointed to monuments and artifacts in Egypt to debunk monogenist beliefs that all races came from the same stock. Ancient Egyptian representations of races for example showed exactly how the races looked today. Egyptological evidence indicated the existence of remarkable permanent differences in the shape of the skull, bodily form, colour and physiognomy between different races which are difficult to reconcile with biblical monogenesis. Colenso believed that racial variation between races was so great, that there was no way all the races could have come from the same stock just a few thousand years ago, he was unconvinced that the climate could change racial variation, he also with other biblical polygenists believed that monogenists had interpreted the bible wrongly.\nColenso said “It seems most probable that the human race, as it now exists, had really sprung from more than one pair”. Colenso denied that polygenism caused any kind of racist attitudes or practices, like many other polygenists he claimed that monogenesis was the cause of slavery and racism. Colenso claimed that each race had sprung from a different pair of parents, and that all races had been created equal by God.\n\nColenso died at Durban on 20 June 1883. His daughter Frances Ellen Colenso (1849–1887) published two books on the relations of the Zulus to the British (\"History of the Zulu War and Its Origin\" in 1880 and \"The Ruin of Zululand\" in 1885) that explained recent events in Zululand from a pro-Zulu perspective. His oldest daughter, Harriette E Colenso (b. 1847), took up Colenso's mantle as advocate for the Zulus in opposition to their treatment by the authorities appointed by Natal, especially in the case of Dinizulu in 1888–1889 and in 1908–1909.\n\nColenso married Sarah Frances Bunyon in 1846, and they had five children, Harriet Emily, Frances Ellen, Robert John, Francis \"Frank\" Ernest, and Agnes. (In the marriage register, her name is spelt Bunyan. There had long been variations in the spelling of a surname that goes back at least to the 12th century in England and in Normandy.)\n\n\n\n"}
{"id": "54663684", "url": "https://en.wikipedia.org/wiki?curid=54663684", "title": "Knower paradox", "text": "Knower paradox\n\nThe knower paradox is a paradox belonging to the family of the paradoxes of self-reference (like the liar paradox). Informally, it consists in considering a sentence saying of itself that it is not known, and apparently deriving the contradiction that such sentence is both not known and known. \n\nA version of the paradox occurs already in chapter 9 of Thomas Bradwardine’s \"Insolubilia\". In the wake of the modern discussion of the paradoxes of self-reference, the paradox has been rediscovered (and dubbed with its current name) by the US logicians and philosophers David Kaplan and Richard Montague, and is now considered an important paradox in the area. The paradox bears connections with other epistemic paradoxes such as the hangman paradox and the paradox of knowability.\n\nThe notion of knowledge seems to be governed by the principle that knowledge is factive:\n\n(where we use single quotes to refer to the linguistic expression inside the quotes and where 'is known' is short for 'is known by someone at some time'). It also seems to be governed by the principle that proof yields knowledge:\n\nConsider however the sentence:\n\nAssume for reductio ad absurdum that (K) is known. Then, by (KF), (K) is not known, and so, by \"reductio ad absurdum\", (K) is not known. Now, this conclusion, which is the sentence (K) itself, depends on no undischarged assumptions, and so has just been proved. Therefore, by (PK), we can further conclude that (K) is known. Putting the two conclusions together, we have the contradiction that (K) is both not known and known.\n\nSince, given the diagonal lemma, every sufficiently strong theory will have to accept something like (K), absurdity can only be avoided either by rejecting one of the two principles of knowledge (KF) and (PK) or by rejecting classical logic (which validates the reasoning from (KF) and (PK) to absurdity). The first kind of strategy subdivides in several alternatives. One approach takes its inspiration from the hierarchy of truth predicates familiar from Alfred Tarski's work on the Liar paradox and constructs a similar hierarchy of knowledge predicates. Another approach upholds a single knowledge predicate but takes the paradox to call into doubt either the unrestricted validity of (PK) or at least knowledge of (KF). The second kind of strategy also subdivides in several alternatives. One approach rejects the law of excluded middle and consequently \"reductio ad absurdum\". Another approach upholds \"reductio ad absurdum\" and thus accepts the conclusion that (K) is both not known and known, thereby rejecting the law of non-contradiction.\n\n"}
{"id": "17766039", "url": "https://en.wikipedia.org/wiki?curid=17766039", "title": "Lee distance", "text": "Lee distance\n\nIn coding theory, the Lee distance is a distance between two strings formula_1 and formula_2 of equal length \"n\" over the \"q\"-ary alphabet {0, 1, …, \"q\" − 1} of size \"q\" ≥ 2.\nIt is a metric, defined as\n\nConsidering the alphabet as the additive group Z, the Lee distance between two single letters formula_4 and formula_5 is the length of shortest path in the Cayley graph (which is circular since the group is cyclic) between them.\n\nIf formula_6 or formula_7 the Lee distance coincides with the Hamming distance, because both distances are 0 for two single equal symbols and 1 for two single non-equal symbols. For formula_8 this is not the case anymore, the Lee distance can become bigger than 1.\n\nThe metric space induced by the Lee distance is a discrete analog of the elliptic space.\n\nIf \"q\" = 6, then the Lee distance between 3140 and 2543 is 1 + 2 + 0 + 3 = 6.\n\nThe Lee distance is named after C. Y. Lee. It is applied for phase modulation while the Hamming distance is used in case of orthogonal modulation.\n\nThe Berlekamp code is an example of code in the Lee metric. Other significant examples are the Preparata code and Kerdock code; these codes are non-linear when considered over a field, but are linear over a ring.\n\nAlso, there exists a Gray isometry (bijection preserving weight) between formula_9 with the Lee weight and formula_10 with the Hamming weight.\n\n"}
{"id": "1893663", "url": "https://en.wikipedia.org/wiki?curid=1893663", "title": "Logical shift", "text": "Logical shift\n\nIn computer science, a logical shift is a bitwise operation that shifts all the bits of its operand. The two base variants are the logical left shift and the logical right shift. This is further modulated by the number of bit positions a given value shall be shifted, such as \"shift left by 1\" or \"shift right by n\". Unlike an arithmetic shift, a logical shift does not preserve a number's sign bit or distinguish a number's exponent from its significand (mantissa); every bit in the operand is simply moved a given number of bit positions, and the vacant bit-positions are filled, usually with zeros, and possibly ones (contrast with a circular shift).\n\nA logical shift is often used when its operand is being treated as a sequence of bits instead of as a number.\n\nLogical shifts can be useful as efficient ways to perform multiplication or division of unsigned integers by powers of two. Shifting left by \"n\" bits on a signed or unsigned binary number has the effect of multiplying it by 2. Shifting right by \"n\" bits on an \"unsigned\" binary number has the effect of dividing it by 2 (rounding towards 0).\n\nLogical right shift differs from arithmetic right shift. Thus, many languages have different operators for them. For example, in Java and JavaScript, the logical right shift operator is »>, but the arithmetic right shift operator is ». (Java has only one left shift operator («), because left shift via logic and arithmetic have the same effect.)\n\nThe programming languages C, C++, and Go, however, have only one right shift operator, ». Most C and C++ implementations, and Go, choose which right shift to perform depending on the type of integer being shifted: signed integers are shifted using the arithmetic shift, and unsigned integers are shifted using the logical shift.\n\nAll currently relevant C standards (ISO/IEC 9899:1999 to 2011) leave a definition gap for cases where the number of shifts is equal to or bigger than the number of bits in the operands in a way that the result is undefined. This helps allow C compilers to emit efficient code for various platforms by allowing direct use of the native shift instructions which have differing behavior. For example, shift-left-word in PowerPC chooses the more-intuitive behavior where shifting by the bit width or above gives zero, whereas SHL in x86 chooses to mask the shift amount to the lower bits \"to reduce the maximum execution time of the instructions\", and as such a shift by the bit width doesn't change the value.\n\nSome languages, such as the .NET Framework and LLVM, also leave shifting by the bit width and above \"unspecified\" (.NET) or \"undefined\" (LLVM). Others choose to specify the behavior of their most common target platforms, such as C# which specifies the x86 behavior.\n\nIf the bit sequence 0001 0111 (decimal 23) is logically shifted by one bit position, then:\n\nNote: MSB = Most Significant Bit, \nLSB = Least Significant Bit\n"}
{"id": "1088425", "url": "https://en.wikipedia.org/wiki?curid=1088425", "title": "Mahler measure", "text": "Mahler measure\n\nIn mathematics, the Mahler measure formula_1 of a polynomial formula_2 with complex coefficients is defined as\nwhere formula_2 factorizes over the complex numbers formula_5 as\n\nThe Mahler measure can be viewed as a kind of height function. Using Jensen's formula, it can be proved that this measure is also equal to the geometric mean of formula_7 for formula_8 on the unit circle (i.e., formula_9):\n\nBy extension, the Mahler measure of an algebraic number formula_11 is defined as the Mahler measure of the minimal polynomial of formula_11 over formula_13. In particular, if formula_11 is a Pisot number or a Salem number, then its Mahler measure is simply formula_11.\n\nThe Mahler measure is named after the German-born Australian mathematician Kurt Mahler.\n\n\nThe Mahler measure formula_1 of a multi-variable polynomial formula_30 is defined similarly by the formula\n\nIt inherits the above three properties of the Mahler measure for a one-variable polynomial.\n\nThe multi-variable Mahler measure has been shown, in some cases, to be related to special values\nof zeta-functions and formula_32-functions. For example, in 1981, Smyth proved the formulas\nwhere formula_34 is the Dirichlet L-function, and\nwhere formula_36 is the Riemann zeta function. Here formula_37 is called the \"logarithmic Mahler measure\".\n\nFrom the definition, the Mahler measure is viewed as the integrated values of polynomials over the torus (also see Lehmer's conjecture). If formula_20 vanishes on the torus formula_39, then the convergence of the integral defining formula_1 is not obvious, but it is known that formula_1 does converge and is equal to a limit of one-variable Mahler measures, which had been conjectured by Boyd.\n\nThis is formulated as follows: Let formula_42 denote the integers and define formula_43 . If formula_44 is a polynomial in formula_45 variables and formula_46 define the polynomial formula_47 of one variable by\n\nand define formula_49 by\n\nwhere formula_51 .\n\nTheorem (Lawton) : Let formula_44 be a polynomial in \"N\" variables with complex coefficients. Then the following limit is valid (even if the condition that formula_53 is relaxed):\n\nBoyd provided more general statements than the above theorem. He pointed out that the classical Kronecker's theorem, which characterizes monic polynomials with integer coefficients all of whose roots are inside the unit disk, can be regarded as characterizing those polynomials of one variable whose measure is exactly 1, and that this result extends to polynomials in several variables.\n\nDefine an \"extended cyclotomic polynomial\" to be a polynomial of the form\nwhere formula_56 is the \"m\"-th cyclotomic polynomial, the formula_57 are integers, and the formula_58 are chosen minimally so that formula_59 is a polynomial in the formula_60. Let formula_61 be the set of polynomials that are products of monomials formula_62 and extended cyclotomic polynomials.\n\nTheorem (Boyd) : Let formula_63 be a polynomial with integer coefficients. Then formula_64 if and only if formula_65 is an element of formula_61.\n\nThis led Boyd to consider the set of values\nand the union formula_68. He made the far-reaching conjecture that the set of formula_69 is a closed subset of formula_70. An immediate consequence of this conjecture would be the truth of Lehmer's conjecture, albeit without an explicit lower bound. As Smyth's result suggests that formula_71 , Boyd further conjectures that\n\n\n\n"}
{"id": "33963415", "url": "https://en.wikipedia.org/wiki?curid=33963415", "title": "Maillet's determinant", "text": "Maillet's determinant\n\nIn mathematics, Maillet's determinant \"D\" is the determinant of the matrix introduced by whose entries are \"R\"(\"s/r\") for \"s\",\"r\" = 1, 2, ..., (\"p\" – 1)/2 ∈ Z/\"p\"Z for an odd prime \"p\", where and \"R\"(\"a\") is the least positive residue of \"a\" mod \"p\" . calculated the determinant \"D\" for \"p\" = 3, 5, 7, 11, 13 and found that in these cases it is given by (–\"p\"), and conjectured that it is given by this formula in general. showed that this conjecture is incorrect; the determinant in general is given by \"D\" = (–\"p\")\"h\", where \"h\" is the first factor of the class number of the cyclotomic field generated by \"p\"th roots of 1, which happens to be 1 for \"p\" less than 23. In particular this verifies Maillet's conjecture that the determinant is always non-zero. Chowla and Weil had previously found the same formula but did not publish it.\nTheir results have been extended to all non-prime odd numbers by K. Wang(1982).\n\n"}
{"id": "48768665", "url": "https://en.wikipedia.org/wiki?curid=48768665", "title": "Non-malleable codes", "text": "Non-malleable codes\n\nThe notion of non-malleable codes was introduced in 2010 by Dziembowski, Pietrzak, and Wichs, for relaxing the notion of error-correction and error-detection. Informally, a code is non-malleable if the message contained in a modified code-word is either the original message, or a completely unrelated value. Non-malleable codes provide a useful and meaningful security guarantee in situations where traditional error-correction and error-detection is impossible; for example, when the attacker can completely overwrite the encoded message. Although such codes do not exist if the family of \"tampering functions\" F is completely unrestricted, they are known to exist for many broad tampering families F.\n\nTo know the operation schema of non-malleable code, we have to have a knowledge of the basic experiment it based on. The following is the three step method of tampering experiment.\n\nThe tampering experiment can be used to model several interesting real-world settings, such as data transmitted over a noisy channel, or adversarial tampering of data stored in the memory of a physical device. Having this experimental base, we would like to build special encoding/decoding procedures formula_12, which give us some meaningful guarantees about the results of the above tampering experiment, for large and interesting families formula_13 of tampering functions. The following are several possibilities for the type of guarantees that we may hope for.\n\nOne very natural guarantee, called error-correction, would be to require that for any tampering function and any \"source-message s\", the tampering experiment always produces the correct decoded message formula_14.\n\nA weaker guarantee, called error-detection, requires that the tampering-experiment always results in either the correct value formula_14 or a special symbol formula_16 indicating that tampering has been detected. This notion of error-detection is a weaker guarantee than error-correction, and achievable for larger F of tampering functions.\n\nA non-malleable code ensures that either the tampering experiment results in a correct decoded-message formula_14, or the decoded-message formula_10 is completely independent of and unrelated to the \"source-message\" formula_1. In other word, the notion of non-malleability for codes is similar, in spirit, to notions of non-malleability for cryptographic primitives (such as encryption2, commitments and zero-knowledge proofs), introduced by the seminal work of Dolev, Dwork and Naor.\n\nCompared to error correction or error detection, the \"right\" formalization of non-malleable codes is somewhat harder to define. Let formula_20 be a random variable for the value of the decoded-message, which results when we run the tampering experiment with source-message formula_1 and tampering-function formula_22, over the randomness of the encoding procedure. Intuitively, we wish to say that the distribution of formula_20 is independent of the encoded message formula_1. Of course, we also want to allow for the case where the tampering experiment results in formula_14 (for example, if the tampering function is identity), which clearly depends on formula_1.\n\nThus, we require that for every tampering-function formula_5, there exists a distribution formula_28 which outputs either concrete values formula_10 or a special same formula_30 symbol, and faithfully models the distribution of formula_20 for all formula_1 in the following sense: for every source message formula_1, the distributions of formula_20 and formula_28 are statistically close when the formula_30 symbol is interpreted as formula_1. That is, formula_28 correctly simulates the \"outcome\" of the tampering-experiment with a function formula_5 without knowing the source-messages formula_1, but it is allowed some ambiguity by outputting a same formula_30 symbol to indicate that the decoded-message should be the same as the source-message, without specifying what the exact value is. The fact that formula_28 depends on only formula_22 and not on formula_1, shows that the outcome of formula_20 is independent of formula_1, exempting equality.\n\nNotice that non-malleability is a weaker guarantee than error correction/detection; the latter ensure that any change in the code-word can be corrected or at least detected by the decoding procedure, whereas the former does allow the message to be modified, but only to an unrelated value. However, when studying error correction/detection we usually restrict ourselves to limited forms of tampering which preserve some notion of distance (e.g., usually hamming distance) between the original and tampered code-word. \nFor example, it is already impossible to achieve error correction/detection for the simple family of functions formula_47 which, for every constant formula_6, includes a \"constant\" function formula_49 that maps all inputs to formula_6. There is always some function in formula_47 that maps everything to a valid code-word formula_6. In contrast, it is trivial to construct codes that are non-malleable w.r.t formula_47, as the output of a constant function is clearly independent of its input. The prior works on non-malleable codes show that one can construct non-malleable codes for highly complex tampering function families formula_13 for which error correction/detection can not be achievable.\n\nAs one very concrete example, we study non-malleability with respect to the family of functions formula_22 which specify, for each bit of the code-word formula_3, whether to keep it as is, flip it, set it to 0, set it to 1. That is, each bit of the code-word is modified arbitrarily but independently of the value of the other bits of the code-word. We call this the “bit-wise independent tampering” family formula_57. Note that this family contains constant functions formula_47 and constant-error functions formula_59 as subsets. Therefore, as we have mentioned, error-correction and error-detection cannot be achieved w.r.t. this family. Nevertheless, the following can show an efficient non-malleable code for this powerful family.\n\nWith formula_57 we denote the family which contains all tampering functions that tamper every bit independently. Formally, this family contains all functions <math>f_i: \\left\\\n"}
{"id": "49914674", "url": "https://en.wikipedia.org/wiki?curid=49914674", "title": "Online optimization", "text": "Online optimization\n\nOnline optimization is a field of optimization theory, more popular in computer science and operations research, that deals with optimization problems having no or incomplete knowledge of the future (online). These kind of problems are denoted as online problems and are seen as opposed to the classical optimization problems where complete information is assumed (offline). The research on online optimization can be distinguished into online problems where multiple decisions are made sequentially based on a piece-by-piece input and those where a decision is made only once. A famous online problem where a decision is made only once is the Ski rental problem. In general, the output of an online algorithm is compared to the solution of a corresponding offline algorithm which is necessarily always optimal and knows the entire input in advance (competitive analysis).\n\nIn many situations, present decisions (for example, resources allocation) must be made with incomplete knowledge of the future or distributional assumptions on the future are not reliable. In such cases, online optimization can be used, which is different from other approaches such as robust optimization, stochastic optimization and Markov decision processes.\n\nA problem exemplifying the concepts of online algorithms is the Canadian traveller problem. The goal of this problem is to minimize the cost of reaching a target in a weighted graph where some of the edges are unreliable and may have been removed from the graph. However, that an edge has been removed (\"failed\") is only revealed to \"the traveller\" when she/he reaches one of the edge's endpoints. The worst case for this problem is simply that all of the unreliable edges fail and the problem reduces to the usual shortest path problem. An alternative analysis of the problem can be made with the help of competitive analysis. For this method of analysis, the offline algorithm knows in advance which edges will fail and the goal is to minimize the ratio between the online and offline algorithms' performance. This problem is PSPACE-complete.\n\nThere are many formal problems that offer more than one \"online algorithm\" as solution:\n"}
{"id": "44116080", "url": "https://en.wikipedia.org/wiki?curid=44116080", "title": "POODLE", "text": "POODLE\n\nThe POODLE attack (which stands for \"Padding Oracle On Downgraded Legacy Encryption\") is a man-in-the-middle exploit which takes advantage of Internet and security software clients' fallback to SSL 3.0. If attackers successfully exploit this vulnerability, on average, they only need to make 256 SSL 3.0 requests to reveal one byte of encrypted messages. Bodo Möller, Thai Duong and Krzysztof Kotowicz from the Google Security Team discovered this vulnerability; they disclosed the vulnerability publicly on October 14, 2014 (despite the paper being dated \"September 2014\" ). On December 8, 2014 a variation of the POODLE vulnerability that affected TLS was announced.\n\nThe CVE-ID associated with the original POODLE attack is .\nF5 Networks filed for as well, see POODLE attack against TLS section below.\n\nPOODLE exemplifies a vulnerability that succeeds because of a mechanism designed for reducing security for the sake of interoperability. When designing systems in domains with high levels of fragmentation, then, extra care is appropriate. In such domains, graceful security degradation may become common.\n\nTo mitigate the POODLE attack, one approach is to completely disable SSL 3.0 on the client side and the server side. However, some old clients and servers do not support TLS 1.0 and above. Thus, the authors of the paper on POODLE attacks also encourage browser and server implementation of TLS_FALLBACK_SCSV, which will make downgrade attacks impossible.\n\nAnother mitigation is to implement \"anti-POODLE record splitting\". It splits the records into several parts and ensures none of them can be attacked. However the problem of the splitting is that, though valid according to the specification, it may also cause compatibility issues due to problems in server-side implementations.\n\nA full list of browser versions and levels of vulnerability to different attacks (including POODLE) can be found in the article Transport Layer Security. \n\nOpera 25 implemented this mitigation in addition to TLS_FALLBACK_SCSV.\n\nGoogle's Chrome browser and their servers had already supported TLS_FALLBACK_SCSV. Google stated in October 2014 it was planning to remove SSL 3.0 support from their products completely within a few months. Fallback to SSL 3.0 has been disabled in Chrome 39, released in November 2014. SSL 3.0 has been disabled by default in Chrome 40, released in January 2015.\n\nMozilla disabled SSL 3.0 in Firefox 34 and ESR 31.3, which were released in December 2014, and added support of TLS_FALLBACK_SCSV in Firefox 35.\n\nMicrosoft published a security advisory to explain how to disable SSL 3.0 in Internet Explorer and Windows OS, and on October 29, 2014, Microsoft released a fix which disables SSL 3.0 in Internet Explorer on Windows Vista / Server 2003 and above and announced a plan to disable SSL 3.0 by default in their products and services within a few months. Microsoft disabled fallback to SSL 3.0 in Internet Explorer 11 for Protect Mode sites on February 10, 2015, and for other sites on April 14, 2015.\n\nApple's Safari (on OS X 10.8, iOS 8.1 and later) mitigated against POODLE by removing support for all CBC protocols in SSL 3.0, however, this left RC4 which is also completely broken by the RC4 attacks in SSL 3.0.. POODLE was completely mitigated in OS X 10.11 (El Capitan 2015) and iOS 9 (2015). \n\nTo prevent the POODLE attack, some web services dropped support of SSL 3.0. Examples include CloudFlare and Wikimedia.\n\nNetwork Security Services version 3.17.1 (released on October 3, 2014) and 3.16.2.3 (released on October 27, 2014) introduced support for TLS_FALLBACK_SCSV, and NSS will disable SSL 3.0 by default in April 2015. OpenSSL versions 1.0.1j, 1.0.0o and 0.9.8zc, released on October 15, 2014, introduced support for TLS_FALLBACK_SCSV. LibreSSL version 2.1.1, released on October 16, 2014, disabled SSL 3.0 by default.\n\nA new variant of the original POODLE attack was announced on December 8, 2014. This attack exploits implementation flaws of CBC encryption mode in the TLS 1.0 - 1.2 protocols. Even though TLS specifications require servers to check the padding, some implementations fail to validate it properly, which makes some servers vulnerable to POODLE even if they disable SSL 3.0. SSL Pulse showed \"about 10% of the servers are vulnerable to the POODLE attack against TLS\" before this vulnerability is announced. The CVE-ID for F5 Networks' implementation bug is . The entry in NIST's NVD states that this CVE-ID is to be used only for F5 Networks' implementation of TLS, and that other vendors whose products have the same failure to validate the padding mistake in their implementations like A10 Networks and Cisco Systems need to issue their own CVE-IDs for their implementation errors because this is not a flaw in the protocol itself and is a flaw in the protocol's implementation.\n\nThe POODLE attack against TLS was found to be easier to initiate than the initial POODLE attack against SSL. There is no need to downgrade clients to SSL 3.0, meaning fewer steps are needed to execute a successful attack. \n\n"}
{"id": "12242679", "url": "https://en.wikipedia.org/wiki?curid=12242679", "title": "Ping-pong scheme", "text": "Ping-pong scheme\n\nAlgorithms said to employ a Ping-Pong scheme exist in different fields of Software Engineering. They are characterized by an alternation between two entities. In the examples described below, these entities are communication partners, network paths or file blocks.\n\nIn most database management systems durable database transactions are supported through a log file. However, multiple writes to the same page of that file can produce a slim chance of data loss. Assuming for simplicity that the log file is organized in pages whose size matches the block size of its underlying medium, the following problem can occur:\n\nIf the very last page of the log file is only partially filled with data and has to be written to permanent storage in this state, the very same page will have to be overwritten during the next write operation. If a crash happens during that later write operation, previously stored log data may be lost.\n\nThe Ping-Pong scheme described in \"Transaction Processing\" eliminates this problem by alternately writing the contents of said (logical) last page to two different physical pages inside the log file (the actual last page \"i\" and its empty successor \"i+1\"). Once said logical log page is no longer the last page (i.e. it is completely filled with log data), it is written one last time to the regular physical position (\"i\") inside the log file.\n\nThis scheme requires the usage of time stamps for each page in order to distinguish the most recent version of the logical last page one from its predecessor.\n\nA functionality which lets a computer A find out whether a computer B is reachable and responding is built into the Internet Control Message Protocol (ICMP). Through an \"echo request\" Computer A asks B to send back an \"Echo response\". These two messages are also sometimes called \"ping\" and \"pong\".\n\nIn Routing, a Ping-Pong scheme is a simple algorithm for distributing data packets across\ntwo paths.\n\nIf you had two paths codice_1 and codice_2, then the algorithm\nwould randomly start with one of the paths and then switch back and forth \nbetween the two.\n\nIf you were to get the next path from a function call, it would look like\nthis in Python:\n"}
{"id": "56343400", "url": "https://en.wikipedia.org/wiki?curid=56343400", "title": "Polynomial functor (type theory)", "text": "Polynomial functor (type theory)\n\nIn type theory, a polynomial functor (or container functor) is a kind of endofunctor of a category of types that is intimately related to the concept of inductive and coinductive types. Specifically, all W-types (resp. M-types) are (isomorphic to) initial algebras (resp. final coalgebras) of such functors.\n\nPolynomial functors have been studied in the more general setting of a pretopos with Σ-types, this article deals only with the applications of this concept inside the category of types of a Martin-Löf style type theory.\n\nLet be a universe of types, let : , and let : → be a family of types indexed by . The pair (, ) is sometimes called a signature or a container. The polynomial functor associated to the container (, ) is defined as follows:\nAny functor naturally isomorphic to is called a container functor. The action of on functions is defined by \nNote that this assignment is not only truly functorial in extensional type theories (see #Properties).\n\nIn intensional type theories, such functions are not truly functors, because the universe type is not strictly a category (the field of homotopy type theory is dedicated to exploring how the universe type behaves more like a higher category). However, it is functorial up to propositional equalities, that is, the following identity types are inhabited:\nfor any functions and and any type , where formula_4 is the identity function on the type .\n\n\n"}
{"id": "23855137", "url": "https://en.wikipedia.org/wiki?curid=23855137", "title": "Proofs of elementary ring properties", "text": "Proofs of elementary ring properties\n\nThe following proofs of elementary ring properties use only the axioms that define a mathematical ring:\n\nTheorem: formula_1\n\nTheorem: A ring formula_2 is the zero ring (that is, consists of precisely one element) if and only if formula_3.\n\nTheorem: formula_4\n\nTheorem: formula_5\n"}
{"id": "707404", "url": "https://en.wikipedia.org/wiki?curid=707404", "title": "Quantum circuit", "text": "Quantum circuit\n\nIn quantum information theory, a quantum circuit is a model for quantum computation in which a computation is a sequence of quantum gates, which are reversible transformations on a quantum mechanical analog of an \"n\"-bit register. This analogous structure is referred to as an \"n\"-qubit register.\n\nThe elementary logic gates of a classical computer, other than the NOT gate, are not reversible. Thus, for instance, for an AND gate one cannot recover the two input bits from the output bit; for example, if the output bit is 0, we cannot tell from this whether the input bits are 0,1 or 1,0 or 0,0.\n\nHowever, reversible gates in classical computers are easily constructed for bit strings of any length; moreover, these are actually of practical interest, since irreversible gates must always increase physical entropy. A reversible gate is a reversible function on \"n\"-bit data that returns \"n\"-bit data, where an \"n\"-bit data is a string of bits \"x\",\"x\", ...,\"x\" of length \"n\". The set of \"n\"-bit data is the space {0,1}, which consists of 2 strings of 0's and 1's.\n\nMore precisely: an \"n\"-bit reversible gate is a bijective mapping \"f\" from the set {0,1} of \"n\"-bit data onto itself.\nAn example of such a reversible gate \"f\" is a mapping that applies a fixed permutation to its inputs.\nFor reasons of practical engineering, one typically studies gates only for small values of \"n\", e.g. \"n\"=1, \"n\"=2 or \"n\"=3. These gates can be easily described by tables.\n\nTo define quantum gates, we first need to specify the quantum replacement of an \"n\"-bit datum. The \"quantized version\" of classical \"n\"-bit space {0,1} is the Hilbert space\n\nThis is by definition the space of complex-valued functions on {0,1} and is naturally an inner product space. This space can also be regarded as consisting of linear superpositions of classical bit strings. Note that \"H\" is a vector space over the complex numbers of dimension 2. The elements of this space are called \"n\"-qubits.\n\nUsing Dirac ket notation, if \"x\",\"x\", ...,\"x\" is a classical bit string, then \nis a special \"n\"-qubit corresponding to the function which maps this classical bit string to 1 and maps all other bit strings to 0; these 2 special \"n\"-qubits are called \"computational basis states\". All \"n\"-qubits are complex linear combinations of these computational basis states.\n\nQuantum logic gates, in contrast to classical logic gates, are always reversible. One requires a special kind of reversible function, namely a unitary mapping, that is, a linear transformation of a complex inner product space that preserves the Hermitian inner product. An \"n\"-qubit (reversible) quantum gate is a unitary mapping \"U\" from the space \"H\" of \"n\"-qubits onto itself.\n\nTypically, we are only interested in gates for small values of \"n\".\n\nA reversible \"n\"-bit classical logic gate gives rise to a reversible \"n\"-bit quantum gate as follows: to each reversible \"n\"-bit logic gate \"f\" corresponds a quantum gate \"W\" defined as follows:\nNote that \"W\" permutes the computational basis states.\n\nOf particular importance is the controlled NOT gate (also called CNOT gate) \"W\" defined on a quantized 2 qubit. Other examples of quantum logic gates derived from classical ones are the Toffoli gate and the Fredkin gate.\n\nHowever, the Hilbert-space structure of the qubits permits many quantum gates that are not induced by classical ones. For example, a relative phase shift is a 1 qubit gate given by multiplication by the unitary matrix:\nso \n\nAgain, we consider first \"reversible\" classical computation. Conceptually, there is no difference between a reversible \"n\"-bit circuit and a reversible \"n\"-bit logic gate: either one is just an invertible function on the space of \"n\" bit data. However, as mentioned in the previous section, for engineering reasons we would like to have a small number of simple reversible gates, that can be put together to assemble any reversible circuit.\n\nTo explain this assembly process, suppose we have a reversible \"n\"-bit gate \"f\" and a reversible \"m\"-bit gate \"g\". Putting them together means producing a new circuit by connecting some set of \"k\" outputs of \"f\" to some set of \"k\" inputs of \"g\" as in the figure below. In that figure \"n\"=5, \"k\" =3 and \"m\" = 7. The resulting circuit is also reversible and operates on \"n\"+\"m\"−\"k\" bits.\n\nWe will refer to this scheme as a \"classical assemblage\" (This concept corresponds to a technical definition in Kitaev's pioneering paper cited below). In composing these reversible machines, it is important to ensure that the intermediate machines are also reversible. This condition assures that \"intermediate\" \"garbage\" is not created (the net physical effect would be to increase entropy, which is one of the motivations for going through this exercise).\n\nNow it is possible to show that the Toffoli gate is a universal gate. This means that given any reversible classical \"n\"-bit circuit \"h\", we can construct a classical assemblage of Toffoli gates in the above manner to produce an (\"n\"+\"m\")-bit circuit \"f\" such that\nwhere there are \"m\" underbraced zeroed inputs and \nNotice that the end result always has a string of \"m\" zeros as the ancilla bits. No \"rubbish\" is ever produced, and so this computation is indeed one that, in a physical sense, generates no entropy. This issue is carefully discussed in Kitaev's article.\n\nMore generally, any function \"f\" (bijective or not) can be simulated by a circuit of Toffoli gates. Obviously, if the mapping fails to be injective, at some point in the simulation (for example as the last step) some \"garbage\" has to be produced.\n\nFor quantum circuits a similar composition of qubit gates can be defined. That is, associated to any \"classical assemblage\" as above, we can produce a reversible quantum circuit when in place of \"f\" we have an \"n\"-qubit gate \"U\" and in place of \"g\" we have an \"m\"-qubit gate \"W\". See illustration below:\n\nThe fact that connecting gates this way gives rise to a unitary mapping on \"n\"+\"m\"−\"k\" qubit space is easy to check. It should also be noted that in a real quantum computer the physical connection between the gates is a major engineering challenge, since it is one of the places where decoherence may occur.\n\nThere are also universality theorems for certain sets of well-known gates; such a universality theorem exists, for instance, for the pair consisting of the single qubit phase gate \"U\" mentioned above (for a suitable value of θ), together with the 2-qubit CNOT gate \"W\". However, the universality theorem for the quantum case is somewhat weaker than the one for the classical case; it asserts only that any reversible \"n\"-qubit circuit can be \"approximated\" arbitrarily well by circuits assembled from these two elementary gates. Note that there are uncountably many possible single qubit phase gates, one for every possible angle θ, so they cannot all be represented by a finite circuit constructed from {\"U\", \"W\")}.\n\nSo far we have not shown how quantum circuits are used to perform computations. Since many important numerical problems reduce to computing a unitary transformation \"U\" on a finite-dimensional space (the celebrated discrete Fourier transform \nbeing a prime example), one might expect that some quantum circuit could be designed to carry out the transformation \"U\". In principle, one needs only to prepare an \"n\" qubit state ψ as an appropriate superposition of computational basis states for the input and measure the output \"U\"ψ. Unfortunately, there are two problems with this:\n\n\nThis does not prevent quantum circuits for the discrete Fourier transform from being used as intermediate steps in other quantum circuits, but the use is more subtle. In fact quantum computations are \"probabilistic\".\n\nWe now provide a mathematical model for how quantum circuits can simulate\n\"probabilistic\" but classical computations. Consider an \"r\"-qubit circuit \"U\" with\nregister space \"H\". \"U\" is thus a unitary map\n\nIn order to associate this circuit to a classical mapping on bitstrings, we specify\n\n\nThe contents \"x\" = \"x\", ..., \"x\" of\nthe classical input register are used to initialize the qubit\nregister in some way. Ideally, this would be done with the computational basis\nstate \nwhere there are \"r\"-\"m\" underbraced zeroed inputs. Nevertheless,\nthis perfect initialization is completely unrealistic. Let us assume\ntherefore that the initialization is a mixed state given by some density operator \"S\" which is near the idealized input in some appropriate metric, e.g.\n\nSimilarly, the output register space is related to the qubit register, by a \"Y\"\nvalued observable \"A\". Note that observables in quantum mechanics are usually defined in\nterms of \"projection valued measures\" on R; if the variable\nhappens to be discrete, the projection valued measure reduces to a\nfamily {E} indexed on some parameter λ\nranging over a countable set. Similarly, a \"Y\" valued observable,\ncan be associated with a family of pairwise orthogonal projections\n\nGiven a mixed state \"S\", there corresponds a probability measure on \"Y\"\ngiven by \n\nThe function \"F\":\"X\" → \"Y\" is computed by a circuit\n\"U\":\"H\" → \"H\" to within ε if and only if\nfor all bitstrings \"x\" of length \"m\"\n\nNow \nso that\n\nTheorem. If ε+ δ <1/2, then the probability distribution \non \"Y\" can be used to determine \"F\"(\"x\") with an arbitrarily small probability of error by majority sampling, for a sufficiently large sample size. Specifically, take \"k\" independent samples from the probability distribution Pr on \"Y\" and choose a value on which more than half of the samples agree. The probability that the value \"F\"(\"x\") is sampled more than \"k\"/2 times is at least\nwhere γ = 1/2 -ε - δ.\n\nThis follows by applying the Chernoff bound.\n\n\n"}
{"id": "4937732", "url": "https://en.wikipedia.org/wiki?curid=4937732", "title": "Set theory of the real line", "text": "Set theory of the real line\n\nSet theory of the real line is an area of mathematics concerned with the application of set theory to aspects of the real numbers.\n\nFor example, one knows that all countable sets of reals are null, i.e. have Lebesgue measure 0; one might therefore ask the least possible size of a set\nwhich is not Lebesgue null. This invariant is called the uniformity of the ideal of null sets, denoted formula_1. There are many such invariants associated with this and other ideals, e.g. the ideal of meagre sets, plus more which do not have a characterisation in terms of ideals. If the continuum hypothesis (CH) holds, then all such invariants are equal to formula_2, the least uncountable cardinal. For example, we know formula_1 is uncountable, but being the size of some set of reals under CH it can be at most formula_2.\n\nOn the other hand, if one assumes Martin's Axiom (MA) all common invariants are \"big\", that is equal to formula_5, the cardinality of the continuum. Martin's Axiom is consistent with formula_6. In fact one should view Martin's Axiom as a forcing axiom that negates the need to do specific forcings of a certain class (those satisfying the ccc, since the consistency of MA with large continuum is proved by doing all such forcings (up to a certain size shown to be sufficient). Each invariant can be made large by some ccc forcing, thus each is big given MA.\n\nIf one restricts to specific forcings, some invariants will become big while others remain small. Analysing these effects is the major work of the area, seeking to determine which inequalities between invariants are provable and which are inconsistent with ZFC. The inequalities among the ideals of measure (null sets) and category (meagre sets) are captured in Cichon's diagram. Seventeen models (forcing constructions) were produced during the 1980s, starting with work of Arnold Miller, to demonstrate that no other inequalities are provable. These are analysed in detail in the book by Tomek Bartoszynski and Haim Judah, two of the eminent workers in the field.\n\nOne curious result is that if you can cover the real line with formula_7 meagre sets (where formula_8) then formula_9; conversely if you can cover the real line with formula_7 null sets then the least non-meagre set has size at least formula_7; both of these results follow from the existence of a decomposition of formula_12 as the union of a meagre set and a null set.\n\nOne of the last great unsolved problems of the area was the consistency of\n\nproved in 1998 by Saharon Shelah.\n\n\n"}
{"id": "192753", "url": "https://en.wikipedia.org/wiki?curid=192753", "title": "Seven Bridges of Königsberg", "text": "Seven Bridges of Königsberg\n\nThe Seven Bridges of Königsberg is a historically notable problem in mathematics. Its negative resolution by Leonhard Euler in 1736 laid the foundations of graph theory and prefigured the idea of topology.\n\nThe city of Königsberg in Prussia (now Kaliningrad, Russia) was set on both sides of the Pregel River, and included two large islands - Kneiphof and Lomse - which were connected to each other, or to the two mainland portions of the city, by seven bridges. The problem was to devise a walk through the city that would cross each of those bridges once and only once. \n\nBy way of specifying the logical task unambiguously, solutions involving either\nare explicitly unacceptable.\n\nEuler proved that the problem has no solution. The difficulty he faced was the development of a suitable technique of analysis, and of subsequent tests that established this assertion with mathematical rigor.\n\nFirst, Euler pointed out that the choice of route inside each land mass is irrelevant. The only important feature of a route is the sequence of bridges crossed. This allowed him to reformulate the problem in abstract terms (laying the foundations of graph theory), eliminating all features except the list of land masses and the bridges connecting them. In modern terms, one replaces each land mass with an abstract \"vertex\" or node, and each bridge with an abstract connection, an \"edge\", which only serves to record which pair of vertices (land masses) is connected by that bridge. The resulting mathematical structure is called a graph.\n\nSince only the connection information is relevant, the shape of pictorial representations of a graph may be distorted in any way, without changing the graph itself. Only the existence (or absence) of an edge between each pair of nodes is significant. For example, it does not matter whether the edges drawn are straight or curved, or whether one node is to the left or right of another.\n\nNext, Euler observed that (except at the endpoints of the walk), whenever one enters a vertex by a bridge, one leaves the vertex by a bridge. In other words, during any walk in the graph, the number of times one enters a non-terminal vertex equals the number of times one leaves it. Now, if every bridge has been traversed exactly once, it follows that, for each land mass (except for the ones chosen for the start and finish), the number of bridges touching that land mass must be \"even\" (half of them, in the particular traversal, will be traversed \"toward\" the landmass; the other half, \"away\" from it). However, all four of the land masses in the original problem are touched by an \"odd\" number of bridges (one is touched by 5 bridges, and each of the other three is touched by 3). Since, at most, two land masses can serve as the endpoints of a walk, the proposition of a walk traversing each bridge once leads to a contradiction.\n\nIn modern language, Euler shows that the possibility of a walk through a graph, traversing each edge exactly once, depends on the degrees of the nodes. The degree of a node is the number of edges touching it. Euler's argument shows that a necessary condition for the walk of the desired form is that the graph be connected and have exactly zero or two nodes of odd degree. This condition turns out also to be sufficient—a result stated by Euler and later proved by Carl Hierholzer. Such a walk is now called an \"Eulerian path\" or \"Euler walk\" in his honor. Further, if there are nodes of odd degree, then any Eulerian path will start at one of them and end at the other. Since the graph corresponding to historical Königsberg has four nodes of odd degree, it cannot have an Eulerian path.\n\nAn alternative form of the problem asks for a path that traverses all bridges and also has the same starting and ending point. Such a walk is called an \"Eulerian circuit\" or an \"Euler tour\". Such a circuit exists if, and only if, the graph is connected, and there are no nodes of odd degree at all. All Eulerian circuits are also Eulerian paths, but not all Eulerian paths are Eulerian circuits.\n\nEuler's work was presented to the St. Petersburg Academy on 26 August 1735, and published as \"Solutio problematis ad geometriam situs pertinentis\" (The solution of a problem relating to the geometry of position) in the journal \"Commentarii academiae scientiarum Petropolitanae\" in 1741. It is available in English in The World of Mathematics.\n\nIn the history of mathematics, Euler's solution of the Königsberg bridge problem is considered to be the first theorem of graph theory and the first true proof in the theory of networks, a subject now generally regarded as a branch of combinatorics. Combinatorial problems of other types had been considered since antiquity.\n\nIn addition, Euler's recognition that the key information was the number of bridges and the list of their endpoints (rather than their exact positions) presaged the development of topology. The difference between the actual layout and the graph schematic is a good example of the idea that topology is not concerned with the rigid shape of objects.\n\nHence, as Euler recognized, the \"geometry of position\" is not about \"measurements and calculations\" but about something more general. That called in question the traditional Aristotelian view that mathematics is the \"science of quantity\". Though that view fits arithmetic and Euclidean geometry, it did not fit topology and the more abstract structural features studied in modern mathematics.\n\nPhilosophers have noted that Euler's proof is not about an abstraction or a model of reality, but directly about the real arrangement of bridges. Hence the certainty of mathematical proof can apply directly to reality.\n\nThe classic statement of the problem, given above, uses unidentified nodes—that is, they are all alike except for the way in which they are connected. There is a variation in which the nodes are identified—each node is given a unique name or color.\nThe northern bank of the river is occupied by the \"Schloß\", or castle, of the \"Blue Prince\"; the southern by that of the Red Prince. The east bank is home to the Bridge's \"Ritcher\", or church; and on the small island in the center is a \"Gasthaus\", or inn.\n\nIt is understood that the problems to follow should be taken in order, and begin with a statement of the original problem:\n\nIt being customary among the townsmen, after some hours in the \"Gasthaus\", to attempt to walk the bridges, many have returned for more refreshment claiming success. However, none have been able to repeat the feat by the light of day.\n\nBridge 8: \"The Blue Prince\", having analyzed the town's bridge system by means of graph theory, concludes that the bridges cannot be walked. He contrives a stealthy plan to build an eighth bridge so that he can begin in the evening at his \"Schloß\", walk the bridges, and end at the \"Gasthaus\" to brag of his victory. Of course, he wants the \"Red Prince\" to be unable to recur the triumph from the \"Red Castle\". \"Where does the Blue Prince build the eighth bridge?\"\n\nBridge 9: \"The Red Prince\", infuriated by his brother's Gordian solution to the problem, wants to build a ninth bridge, enabling \"him\" to begin at his \"Schloß\", walk the bridges, and end at the \"Gasthaus\" to rub dirt in his brother's face. As an extra bit of revenge, his brother should then no longer be able to walk the bridges starting at his \"Schloß\" and ending at the \"Gasthaus\" as before. \"Where does the Red Prince build the ninth bridge?\"\n\nBridge 10: \"The Bishop\" has watched this furious bridge-building with dismay. It upsets the town's \"Weltanschauung\" and, worse, contributes to excessive drunkenness. He wants to build a tenth bridge that allows \"all\" the inhabitants to walk the bridges and return to their own beds. \"Where does the Bishop build the tenth bridge?\"\n\nReduce the city, as before, to a graph. Color each node. As in the classic problem, no Euler walk is possible; coloring does not affect this. All four nodes have an odd number of edges.\n\nBridge 8: Euler walks are possible if exactly zero or two nodes have an odd number of edges. If we have 2 nodes with an odd number of edges, the walk must begin at one such node and end at the other. Since there are only 4 nodes in the puzzle, the solution is simple. The walk desired must begin at the blue node and end at the orange node. Thus, a new edge is drawn between the other two nodes. Since they each formerly had an odd number of edges, they must now have an even number of edges, fulfilling all conditions. This is a change in parity from an odd to even degree.\n\nBridge 9: The 9th bridge is easy once the 8th is solved. The desire is to enable the red castle and forbid the blue castle as a starting point; the orange node remains the end of the walk and the white node is unaffected. To change the parity of both red and blue nodes, draw a new edge between them.\n\nBridge 10: The 10th bridge takes us in a slightly different direction. The Bishop wishes every citizen to return to his starting point. This is an Euler circuit and requires that all nodes be of even degree. After the solution of the 9th bridge, the red and the orange nodes have odd degree, so their parity must be changed by adding a new edge between them.\n\nTwo of the seven original bridges did not survive the bombing of Königsberg in World War II. Two others were later demolished and replaced by a modern highway. The three other bridges remain, although only two of them are from Euler's time (one was rebuilt in 1935). Thus, , there are five bridges remaining that were involved in Euler's problem.\n\nIn terms of graph theory, two of the nodes now have degree 2, and the other two have degree 3. Therefore, an Eulerian path is now possible, but it must begin on one island and end on the other.\n\nThe University of Canterbury in Christchurch has incorporated a model of the bridges into a grass area between the old Physical Sciences Library and the Erskine Building, housing the Departments of Mathematics, Statistics and Computer Science. The rivers are replaced with short bushes and the central island sports a stone tōrō. Rochester Institute of Technology has incorporated the puzzle into the pavement in front of the Gene Polisseni Center, an ice hockey arena that opened in 2014.\n\n\n"}
{"id": "29861804", "url": "https://en.wikipedia.org/wiki?curid=29861804", "title": "Shlomo Moran", "text": "Shlomo Moran\n\nShlomo Moran (; born 1947) is an Israeli computer scientist, the Bernard Elkin Chair in Computer Science at the Technion – Israel Institute of Technology in Haifa, Israel.\n\nMoran received his Ph.D. in 1979 from the Technion, under the supervision of Azaria Paz; his dissertation was entitled \"NP Optimization Problems and their Approximation\".\n\nSeveral PhD students of Moran joined the academia as well, including Shlomi Dolev, Ilan Gronau, Shay Kutten, and Gadi Taubenfeld.\n\nIn 1993 he shared the Gödel Prize with László Babai, Shafi Goldwasser, Silvio Micali, and Charles Rackoff for their work on Arthur–Merlin protocols and interactive proof systems.\n\n"}
{"id": "2384605", "url": "https://en.wikipedia.org/wiki?curid=2384605", "title": "Shooting ratio", "text": "Shooting ratio\n\nThe shooting ratio in filmmaking and television production is the ratio between the total duration of its footage created for possible use in a project and that which appears in its final cut.<ref>"}
{"id": "3078904", "url": "https://en.wikipedia.org/wiki?curid=3078904", "title": "Spec Sharp", "text": "Spec Sharp\n\nSpec# is a programming language with specification language features that extends the capabilities of the C# programming language with Eiffel-like contracts, including object invariants, preconditions and postconditions. Like ESC/Java, it includes a static checking tool based on a theorem prover that is able to statically verify many of these invariants. It also includes a variety of other minor extensions to the language, such as non-null reference types.\n\nThe \"code contracts\" API in the .NET Framework 4.0 has evolved with Spec#.\n\nMicrosoft Research developed both Spec# and C#; in turn, Spec# serves as the foundation of the Sing# programming language, which Microsoft Research also developed.\n\nSpec# extends the core C# programming language with features such as:\n\nThis example shows two of the basic structures that are used when adding contracts to your code (try Spec# in your browser).\n\n\nSing# is a superset of Spec#. Microsoft Research developed Spec#, and later extended it into Sing# in order to develop the Singularity operating system. Sing# augments the capabilities of Spec# with support for channels and low-level programming language constructs, which are necessary for implementing system software. Sing# is type-safe. The semantics of message-passing primitives in Sing# are defined by formal and written contracts.\n\n\n\n"}
{"id": "27172315", "url": "https://en.wikipedia.org/wiki?curid=27172315", "title": "Steven G. Krantz", "text": "Steven G. Krantz\n\nSteven George Krantz (born February 3, 1951) is an American scholar, mathematician, and writer. He has authored more than 235 research papers and more than 125 books. Additionally, Krantz has edited journals such as the \"Notices of the American Mathematical Society\" and \"The Journal of Geometric Analysis\".\n\nSteven Krantz grew up in Redwood City, California and graduated from Sequoia High School in class of 1967. \n\nKrantz was an undergraduate at the University of California, Santa Cruz (UCSC), graduating with \"summa cum laude\" in 1971. In the math department at UCSC his teachers included Nick Burgoyne, Marvin Greenberg, Ed Landesman, and Stan Philipp. Krantz obtained his Ph.D. in mathematics from Princeton University in 1974 under the direction of Elias M. Stein and Joseph J. Kohn. Other influencers included Fred Almgren, Robert Gunning, and Ed Nelson.\n\nAmong Krantz's research interests include: several complex variables, harmonic analysis, partial differential equations, differential geometry, interpolation of operators, Lie theory, smoothness of functions, convexity theory, the corona problem, the inner functions problem, Fourier analysis, singular integrals, Lusin area integrals, Lipschitz spaces, finite difference operators, Hardy spaces, functions of bounded mean oscillation, geometric measure theory, sets of positive reach, the implicit function theorem, approximation theory, real analytic functions, analysis on the Heisenberg group, complex function theory, and real analysis. \n\nHe applied wavelet analysis to plastic surgery, creating software for facial recognition. Krantz has also written software for the pharmaceutical industry.\n\nKrantz has worked on the inhomogeneous Cauchy–Riemann equations (he obtained the first sharp estimates in a variety of nonisotropic norms), on separate smoothness of functions (most notably with hypotheses about smoothness along integral curves of vector fields), on analysis on the Heisenberg group and other nilpotent Lie groups, on harmonic analysis in several complex variables, on the function theory of several complex variables, on the harmonic analysis of several real variables, on partial differential equations, on complex geometry, on the automorphism groups of domains in complex space, and on the geometry of complex domains. He has worked with Siqi Fu, Robert E. Greene, Alexander Isaev and Kang-Tae Kim on the Bergman kernel, the Bergman metric, and automorphism groups of domains; with Song-Ying Li on the harmonic analysis of several complex variables; and with Marco Peloso on harmonic analysis, the inhomogeneous Cauchy–Riemann equations, Hodge theory, and the analysis of the worm domain. Krantz's book on the geometry of complex domains, written jointly with Robert E. Greene and Kang-Tae Kim, appeared in 2011.\n\nKrantz's monographs include \"Function Theory of Several Complex Variables\", \"Complex Analysis: The Geometric Viewpoint\", \"A Primer of Real Analytic Functions\" (joint with Harold R. Parks), \"The Implicit Function Theorem\" (joint with Harold Parks), \"Geometric Integration Theory\" (joint with Harold Parks), and \"The Geometry of Complex Domains\" (joint with Kang-Tae Kim and Robert E. Greene). His book \"The Proof is in the Pudding: A Look at the Changing Nature of Mathematical Proof\" looks at the history and evolving nature of the proof concept. Krantz's latest book, \"A Mathematician Comes of Age\", published by the Mathematical Association of America, is an exploration of the concept of mathematical maturity.\n\nKrantz is author of textbooks and popular books. His books \"Mathematical Apocrypha\" and \"Mathematical Apocrypha Redux\" are collections of anecdotes about famous mathematicians. Krantz's book \"An Episodic History of Mathematics: Mathematical Culture through Problem Solving\" is a blend of history and problem solving. \"A Mathematician's Survival Guide\" and \"The Survival of a Mathematician\" are about how to get into the mathematics profession and how to survive in the mathematics profession. Krantz's new book with Harold R. Parks titled \"A Mathematical Odyssey: Journey from the Real to the Complex\" is an entree to mathematics for the layman. His book \"I, Mathematician\" (joint with Peter Casazza and Randi D. Ruden) is a study, with contributions from many mathematicians, of how mathematicians think of themselves and how others think of mathematicians. The book \"The Theory and Practice of Conformal Geometry\" is a study of\nclassical conformal geometry in the complex plane, and is the first Dover book that is not a reprint of a classic but is instead a new book.\n\nKrantz has had 9 Masters students and 20 Ph.D. students. Among the latter are Xiaojun Huang (holder of the Bergman Prize), Marco Peloso, Fausto Di Biase, Daowei Ma, and Siqi Fu.\n\nKrantz has organized conferences, including the Summer Workshop in Several Complex Variables held in Santa Cruz in 1989 and attended by 250 people. He was the principal lecturer at a CBMS conference at George Mason University in 1992. He organized and spoke at a conference on the corona problem held at the Fields Institute in Toronto, Canada in June 2012.\n\nIn 2012 he became a Fellow of the American Mathematical Society. Krantz has an Erdős number of 1.\n\nKrantz has taught at University of California, Los Angeles, Princeton University, Pennsylvania State University, and Washington University in St. Louis, where served as chair of the mathematics department. He has been a visiting faculty member at the Institute for Advanced Study, Princeton, the University of Paris, the Universidad Autónoma de Madrid, Pohang Institute of Science and Technology, the Mathematical Sciences Research Institute, the American Institute of Mathematics, Australian National University (as the Richardson Fellow), Texas A&M (as the Frontiers Lecturer), the University of Umeå, Uppsala University, the University of Oslo, Politecnico Torino, the University of Seoul, Université Paul Sabatier, and Beijing University.\n\nKrantz was editor-in-chief of the \"Notices of the American Mathematical Society\" for 2010 through 2015. Krantz is also editor-in-chief of the \"Journal of Mathematical Analysis and Applications\" and managing editor and founder of the \"Journal of Geometric Analysis\". He also edits for \"The American Mathematical Monthly\", \"Complex Variables and Elliptic Equations\", and \"The Bulletin of the American Mathematical Society\". Krantz is editor-in-chief of the\nnew Springer journal titled \"Complex Analysis and its Synergies\".\n\n\nKrantz has published more than 230 scholarly articles and 130 books.\n\n"}
{"id": "39783084", "url": "https://en.wikipedia.org/wiki?curid=39783084", "title": "T. R. Ramadas", "text": "T. R. Ramadas\n\nTrivandrum Ramakrishnan \"T. R.\" Ramadas (born 30 March 1955) is an Indian mathematician who specializes in algebraic and differential geometry, and mathematical physics. He was awarded the Shanti Swarup Bhatnagar Prize for Science and Technology in 1998, the highest science award in India, in the mathematical sciences category.\nHe studied engineering in IIT Kanpur then joined TIFR as a graduate student in physics finally changing to mathematics after his interactions with M S Narasimhan.\n\nHe is currently a professor at Chennai Mathematical Institute, Chennai, Tamil Nadu.\n\n"}
{"id": "18203720", "url": "https://en.wikipedia.org/wiki?curid=18203720", "title": "Topological degree theory", "text": "Topological degree theory\n\nIn mathematics, topological degree theory is a generalization of the winding number of a curve in the complex plane. It can be used to estimate the number of solutions of an equation, and is closely connected to fixed-point theory. When one solution of an equation is easily found, degree theory can often be used to prove existence of a second, nontrivial, solution. There are different types of degree for different types of maps: e.g. for maps between Banach spaces there is the Brouwer degree in R, the Leray-Schauder degree for compact mappings in normed spaces, the coincidence degree and various other types. There is also a degree for continuous maps between manifolds. \n\nTopological degree theory has applications in complementarity problems, differential equations, differential inclusions and dynamical systems.\n\n"}
{"id": "4232656", "url": "https://en.wikipedia.org/wiki?curid=4232656", "title": "Trakhtenbrot's theorem", "text": "Trakhtenbrot's theorem\n\nIn logic, finite model theory, and computability theory, Trakhtenbrot's theorem (due to Boris Trakhtenbrot) states that the problem of validity in first-order logic on the class of all finite models is undecidable. In fact, the class of valid sentences over finite models is not recursively enumerable (though it is co-recursively enumerable).\n\nTrakhtenbrot's theorem implies that Gödel's completeness theorem (that is fundamental to first-order logic) does not hold in the finite case. Also it seems counter-intuitive that being valid over all structures is 'easier' than over just the finite ones.\n\nThe theorem was first published in 1950: \"The Impossibility of an Algorithm for the Decidability Problem on Finite Classes\".\n\nWe follow the formulations as in \n\nLet σ be a relational vocabulary with one at least binary relation symbol. \n\nRemarks\n\n\nThis proof taken from Chapter 10, section 4, 5 of Mathematical Logic by H.-D. Ebbinghaus.\n\nAs in the most common proof of Gödel's First Incompleteness Theorem through using the undecidability of the halting problem, for each Turing machine formula_1 there is a corresponding arithmetical sentence formula_2, effectively derivable from formula_1, such that it is true if and only if formula_1 halts on the empty tape. Intuitively, formula_2 asserts \"there exists a natural number that is the Gödel code for the computation record of formula_1 on the empty tape that ends with halting\".\n\nIf the machine formula_1 does halt in finite steps, then the complete computation record is also finite, then there is a finite initial segment of the natural numbers such that the arithmetical sentence formula_2 is also true on this initial segment. Intuitively, this is because in this case, proving formula_2 requires the arithmetic properties of only finitely many numbers.\n\nIf the machine formula_1 does not halt in finite steps, then formula_2 is false in any finite model, since there's no finite computation record of formula_1 that ends with halting. \n\nThus, if formula_1 halts, formula_2 is true in some finite models. If formula_1 does not halt, formula_2 is false in all finite models. So, formula_1 does not halt if and only if formula_18 is true over all finite models.\n\nThe set of machines that does not halt is not recursively enumerable, so the set of valid sentences over finite models is not recursively enumerable.\n\nIn this section we exhibit a more rigorous proof from Libkin. Note in the above statement that the corollary also entails the theorem, and this is the direction we prove here.\n\nTheorem\n\nProof\n\nAccording to the previous lemma, we can in fact use finitely many binary relation symbols. The idea of the proof is similar to the proof of Fagin's theorem, and we encode Turing machines in first-order logic. What we want to prove is that for every Turing machine M we construct a sentence φ of vocabulary τ such that φ is finitely satisfiable if and only if M halts on the empty input, which is equivalent to the halting problem and therefore undecidable.\n\nLet M= ⟨Q, Σ, δ, q, Q, Q⟩ be a deterministic Turing machine with a single infinite tape.\n\n\nSince we are dealing with the problem of halting on an empty input we may assume w.l.o.g. that Δ={0,1} and that 0 represents a blank, while 1 represents some tape symbol. We define τ so that we can represent computations:\n\nWhere:\n\n\nThe sentence φ states that (i) <, min, T's and H's are interpreted as above and (ii) that the machine eventually halts. The halting condition is equivalent to saying that H(s, t) holds for some s, t and q∗ ∈ Q ∪ Q and after that state, the configuration of the machine does not change. Configurations of a halting machine (the nonhalting is not finite) can be represented as a τ (finite) sentence (more precisely, a finite τ-structure which satisfies the sentence). The sentence φ is: φ ≡ α ∧ β ∧ γ ∧ η ∧ ζ ∧ θ.\n\nWe break it down by components:\n\n\n\n\nWhere θ is:\n\nAnd:\n\nWhere θ is:\n\nformula_24\n\ns-1 and t+1 are first-order definable abbreviations for the predecessor and successor according to the ordering <. The sentence θ assures that the tape content in position s changes from 0 to 1, the state changes from q to q', the rest of the tape remains the same and that the head moves to s-1 (i. e. one position to the left), assuming s is not the first position in the tape. If it is, then all is handled by θ: everything is the same, except the head does not move to the left but stays put.\n\nIf φ has a finite model, then such a model that represents a computation of M (that starts with the empty tape (i.e. tape containing all zeros) and ends in a halting state). If M halts on the empty input, then the set of all configurations of the halting computations of M (coded with <, T's and H's) is a model of φ, which is finite, since the set of all configurations of halting computations is finite. It follows that M halts on the empty input iff φ has a finite model. Since halting on the empty input is undecidable, so is the question of whether φ has a finite model formula_25 (equivalently, whether φ is finitely satisfiable) is also undecidable (recursively enumerable, but not recursive). This concludes the proof.\n\nCorollary\n\nProof\n\nEnumerate all pairs formula_26 where formula_25 is finite and formula_28.\n\nCorollary\n\nProof\n\nFrom the previous lemma, the set of finitely satisfiable sentences is recursively enumerable. Assume that the set of all finitely valid sentences is recursively enumerable. Since ¬φ is finitely valid iff φ is not finitely satisfiable, we conclude that the set of sentences which are not finitely satisfiable is recursively enumerable. If both a set A and its complement are recursively enumerable, then A is recursive. It follows that the set of finitely satisfiable sentences is recursive, which contradicts Trakhtenbrot's theorem.\n\n"}
{"id": "314139", "url": "https://en.wikipedia.org/wiki?curid=314139", "title": "Triptych", "text": "Triptych\n\nA triptych ( ; from the Greek adjective \"τρίπτυχον\" \"triptukhon\" (\"three-fold\"), from \"tri\", i.e., \"three\" and \"ptysso\", i.e., \"to fold\" or \"ptyx\", i.e., \"fold\") is a work of art (usually a panel painting) that is divided into three sections, or three carved panels that are hinged together and can be folded shut or displayed open. It is therefore a type of polyptych, the term for all multi-panel works. The middle panel is typically the largest and it is flanked by two smaller related works, although there are triptychs of equal-sized panels. The form can also be used for pendant jewelry.\n\nDespite its connection to an art format, the term is sometimes used more generally to connote anything with three parts, particularly if they are integrated into a single unit.\n\nThe triptych form arises from early Christian art, and was a popular standard format for altar paintings from the Middle Ages onwards. Its geographical range was from the eastern Byzantine churches to the Celtic churches in the west. During the Byzantine period, tryptichs were often used for private devotional use, along with other relics such as icons.Renaissance painters such as Hans Memling and Hieronymus Bosch used the form. Sculptors also used it. Triptych forms also allow ease of transport.\n\nFrom the Gothic period onward, both in Europe and elsewhere, altarpieces in churches and cathedrals were often in triptych form. One such cathedral with an altarpiece triptych is Llandaff Cathedral. The Cathedral of Our Lady in Antwerp, Belgium, contains two examples by Rubens, and Notre Dame de Paris is another example of the use of triptych in architecture. One can also see the form echoed by the structure of many ecclesiastical stained glass windows. Although strongly identified as an altarpiece form, triptychs outside that context have been created, some of the best-known examples being works by Hieronymus Bosch, Max Beckmann, and Francis Bacon.\n\nThe then highest price ever paid for an artwork at auction was $142.4 million for a 1969 triptych, \"Three Studies of Lucian Freud\", by Francis Bacon in November 2012. The record was broken in May 2015 by $179.4 million for Pablo Picasso's 1955 painting \"Les Femmes d’Alger\".\n\nThe format has migrated and been used in other religions, including Islam and Buddhism. For example: the triptych \"Hilje-j-Sherif\" displayed at the National Museum of Oriental Art, Rome, Italy, and a page of the \"Qur'an\" at the Museum of Turkish and Islamic Arts in Istanbul, Turkey, exemplify Ottoman religious art adapting the motif. Likewise, Tibetan Buddhists have used it in traditional altars.\n\nA photographic triptych is a common style used in modern commercial artwork. The photographs are usually arranged with a plain border between them. The work may consist of separate images that are variants on a theme, or may be one larger image split into three.\n\n\n\n"}
{"id": "3507217", "url": "https://en.wikipedia.org/wiki?curid=3507217", "title": "Upper-convected time derivative", "text": "Upper-convected time derivative\n\nIn continuum mechanics, including fluid dynamics, an upper-convected time derivative or Oldroyd derivative, named after James G. Oldroyd, is the rate of change of some tensor property of a small parcel of fluid that is written in the coordinate system rotating and stretching with the fluid. \n\nThe operator is specified by the following formula:\nwhere:\n\nThe formula can be rewritten as:\n\nBy definition the upper-convected time derivative of the Finger tensor is always zero.\n\nIt can be shown that the upper-convected time derivative of a spacelike vector field is just its Lie derivative by the velocity field of the continuum.\n\nThe upper-convected derivative is widely use in polymer rheology for the description of behavior of a viscoelastic fluid under large deformations.\n\nFor the case of simple shear:\n\nThus,\n\nIn this case a material is stretched in the direction X and compresses in the directions Y and Z, so to keep volume constant.\nThe gradients of velocity are:\n\nThus,\n\n\n"}
{"id": "37987767", "url": "https://en.wikipedia.org/wiki?curid=37987767", "title": "Vertical electrical sounding", "text": "Vertical electrical sounding\n\nVertical electrical sounding (VES) is a geophysical method for investigation of a geological medium. The method is based on the estimation of the electrical conductivity or resistivity of the medium. The estimation is performed based on the measurement of voltage of electrical field induced by the distant grounded electrodes (current electrodes).\n\nFigures 1–4 show the possible configuration of the measurement setup. The electrodes \"A\" and \"B\" are current electrodes which are connected to a current source; \"N\" and \"M\" are potential electrodes which are used for the voltage measurements. As source, the direct current or low frequency alternating current is used.\n\nThe interpretation of the measurements can be performed based on the apparent resistivity values. The depth of investigation depends on the distance between the current electrodes. In order to obtain the apparent resistivity as the function of depth, the measurements for each position are performed with several different distances between current electrodes. The apparent resistivity is calculated as\n\nhere, \"k\" is a geometric factor, formula_2 — voltage between electrodes М and N, formula_3 — current in the line AB. The geometric factor is defined by\n\nhere \"r\" is the distance between electrodes.\n\nInterpretation of gathered data is performed based on the dependency ρ(AB/2).\n\nThe application of large electrode arrays allows for reconstructing complex 3D structure of geological media (see Electrical resistivity tomography). However, the interpretation of such measurement is rather difficult. \nIn this case, advanced interpretation techniques based on numerical methods can be applied.\n\n\n"}
{"id": "4908214", "url": "https://en.wikipedia.org/wiki?curid=4908214", "title": "Weyl's inequality", "text": "Weyl's inequality\n\nIn mathematics, there are at least two results known as Weyl's inequality.\n\nIn number theory, Weyl's inequality, named for Hermann Weyl, states that if \"M\", \"N\", \"a\" and \"q\" are integers, with \"a\" and \"q\" coprime, \"q\" > 0, and \"f\" is a real polynomial of degree \"k\" whose leading coefficient \"c\" satisfies\n\nfor some \"t\" greater than or equal to 1, then for any positive real number formula_2 one has\n\nThis inequality will only be useful when\n\nfor otherwise estimating the modulus of the exponential sum by means of the triangle inequality as formula_5 provides a better bound.\n\nIn linear algebra, Weyl's inequality is a theorem about the changes to eigenvalues of a Hermitian matrix that is perturbed. It is useful if we wish to know the eigenvalues of a Hermitian matrix but there is an uncertainty about its entries. We let \"H\" be the exact matrix and \"P\" be a perturbation matrix that represents the uncertainty. The matrix we 'measure' is formula_6.\n\nThe theorem says that if any two of \"M\", \"H\" and \"P\" are \"n\" by \"n\" Hermitian matrices, where \"M\" has eigenvalues\n\nand \"H\" has eigenvalues\n\nand \"P\" has eigenvalues\n\nthen the following inequalities hold for formula_10:\n\nMore generally, if formula_12, we have\n\nIf \"P\" is positive definite (that is, formula_14) then this implies\n\nNote that we can order the eigenvalues because the matrices are Hermitian and therefore the eigenvalues are real.\n\nLet formula_16 have singular values formula_17 and eigenvalues ordered so that formula_18. Then\n\nFor formula_20, with equality for formula_21.\n\nAssume that we have a bound on \"P\" in the sense that we know that its spectral norm (or, indeed, any consistent matrix norm) satisfies formula_22. Then it follows that all its eigenvalues are bounded in absolute value by formula_23. Applying Weyl's inequality, it follows that the spectra of \"M\" and \"H\" are close in the sense that\n\nThe singular values {\"σ\"} of a square matrix \"M\" are the square roots of eigenvalues of \"M*M\" (equivalently \"MM*\"). Since Hermitian matrices follow Weyl's inequality, if we take any matrix \"A\" then its singular values will be the square root of the eigenvalues of \"B=A*A\" which is a Hermitian matrix. Now since Weyl's inequality hold for B, therefore for the singular values of \"A\".\n\nThis result gives the bound for the perturbation in the singular values of a matrix \"A\" due to perturbation in \"A\".\n\n"}
