{"id": "2034838", "url": "https://en.wikipedia.org/wiki?curid=2034838", "title": "154 (number)", "text": "154 (number)\n\n154 (one hundred [and] fifty-four) is the natural number following 153 and preceding 155.\n\n154 is a nonagonal number. Its factorization makes 154 a sphenic number\n\nThere is no integer with exactly 154 coprimes below it, making 154 a noncototient, nor is there, in base 10, any integer that added up to its own digits yields 154, making 154 a self number\n\n154 is the sum of the first six factorials, if one starts with formula_1 and assumes that formula_2.\n\nWith just 17 cuts, a pancake can be cut up into 154 pieces (Lazy caterer's sequence).\n\nThe distinct prime factors of 154 add up to 20, and so do the ones of 153, hence the two form a Ruth-Aaron pair. 154! + 1 is a factorial prime.\n\n\n\n\n\n154 is also:\n\n\n\n"}
{"id": "106364", "url": "https://en.wikipedia.org/wiki?curid=106364", "title": "Algebraic structure", "text": "Algebraic structure\n\nIn mathematics, and more specifically in abstract algebra, an algebraic structure on a set \"A\" (called carrier set or underlying set) is a collection of finitary operations on \"A\"; the set \"A\" with this structure is also called an algebra.\n\nExamples of algebraic structures include groups, rings, fields, and lattices. More complex structures can be defined by introducing multiple operations, different underlying sets, or by altering the defining axioms. Examples of more complex algebraic structures include vector spaces, modules, and algebras.\n\nThe properties of specific algebraic structures are studied in abstract algebra. The general theory of algebraic structures has been formalized in universal algebra. The language of category theory is used to express and study relationships between different classes of algebraic and non-algebraic objects. This is because it is sometimes possible to find strong connections between some classes of objects, sometimes of different kinds. For example, Galois theory establishes a connection between certain fields and groups: two algebraic structures of different kinds.\n\nAddition and multiplication on numbers are the prototypical example of an operation that combines two elements of a set to produce a third. These operations obey several algebraic laws. For example, \"a\" + (\"b\" + \"c\") = (\"a\" + \"b\") + \"c\" and \"a\"(\"bc\") = (\"ab\")\"c\", both examples of the \"associative law\". Also \"a\" + \"b\" = \"b\" + \"a\", and \"ab\" = \"ba\", the \"commutative law.\" Many systems studied by mathematicians have operations that obey some, but not necessarily all, of the laws of ordinary arithmetic. For example, rotations of objects in three-dimensional space can be combined by performing the first rotation and then applying the second rotation to the object in its new orientation. This operation on rotations obeys the associative law, but can fail the commutative law.\n\nMathematicians give names to sets with one or more operations that obey a particular collection of laws, and study them in the abstract as algebraic structures. When a new problem can be shown to follow the laws of one of these algebraic structures, all the work that has been done on that category in the past can be applied to the new problem.\n\nIn full generality, algebraic structures may involve an arbitrary number of sets and operations that can combine more than two elements (higher arity), but this article focuses on binary operations on one or two sets. The examples here are by no means a complete list, but they are meant to be a representative list and include the most common structures. Longer lists of algebraic structures may be found in the external links and within \".\" Structures are listed in approximate order of increasing complexity.\n\nSimple structures: no binary operation:\n\n\nGroup-like structures: one binary operation. The binary operation can be indicated by any symbol, or with no symbol (juxtaposition) as is done for ordinary multiplication of real numbers.\n\n\n\nRing-like structures or Ringoids: two binary operations, often called addition and multiplication, with multiplication distributing over addition.\n\n\nLattice structures: two or more binary operations, including operations called meet and join, connected by the absorption law.\n\n\nArithmetics: two binary operations, addition and multiplication. \"S\" is an infinite set. Arithmetics are pointed unary systems, whose unary operation is injective successor, and with distinguished element 0.\n\n\nModule-like structures: composite systems involving two sets and employing at least two binary operations.\n\n\nAlgebra-like structures: composite system defined over two sets, a ring \"R\" and a \"R\" module \"M\" equipped with an operation called multiplication. This can be viewed as a system with five binary operations: two operations on \"R\", two on \"M\" and one involving both \"R\" and \"M\".\n\n\nFour or more binary operations:\n\n\nAlgebraic structures can also coexist with added structure of non-algebraic nature, such as partial order or a topology. The added structure must be compatible, in some sense, with the algebraic structure.\n\n\nAlgebraic structures are defined through different configurations of axioms. Universal algebra abstractly studies such objects. One major dichotomy is between structures that are axiomatized entirely by \"identities\" and structures that are not. If all axioms defining a class of algebras are identities, then the class of objects is a variety (not to be confused with algebraic variety in the sense of algebraic geometry).\n\nIdentities are equations formulated using only the operations the structure allows, and variables that are tacitly universally quantified over the relevant universe. Identities contain no connectives, existentially quantified variables, or relations of any kind other than the allowed operations. The study of varieties is an important part of universal algebra. An algebraic structure in a variety may be understood as the quotient algebra of term algebra (also called \"absolutely free algebra\") divided by the equivalence relations generated by a set of identities. So, a collection of functions with given signatures generate a free algebra, the term algebra \"T\". Given a set of equational identities (the axioms), one may consider their symmetric, transitive closure \"E\". The quotient algebra \"T\"/\"E\" is then the algebraic structure or variety. Thus, for example, groups have a signature containing two operators: the multiplication operator \"m\", taking two arguments, and the inverse operator \"i\", taking one argument, and the identity element \"e\", a constant, which may be considered an operator that takes zero arguments. Given a (countable) set of variables \"x\", \"y\", \"z\", etc. the term algebra is the collection of all possible terms involving \"m\", \"i\", \"e\" and the variables; so for example, \"m(i(x), m(x,m(y,e)))\" would be an element of the term algebra. One of the axioms defining a group is the identity \"m(x, i(x)) = e\"; another is \"m(x,e) = x\". The axioms can be represented as trees. These equations induce equivalence classes on the free algebra; the quotient algebra then has the algebraic structure of a group.\n\nSeveral non-variety structures fail to be varieties, because either:\n\n\nStructures whose axioms unavoidably include nonidentities are among the most important ones in mathematics, e.g., fields and division rings. Although structures with nonidentities retain an undoubted algebraic flavor, they suffer from defects varieties do not have. For example, the product of two fields is not a field.\n\nCategory theory is another tool for studying algebraic structures (see, for example, Mac Lane 1998). A category is a collection of \"objects\" with associated \"morphisms.\" Every algebraic structure has its own notion of homomorphism, namely any function compatible with the operation(s) defining the structure. In this way, every algebraic structure gives rise to a category. For example, the category of groups has all groups as objects and all group homomorphisms as morphisms. This concrete category may be seen as a category of sets with added category-theoretic structure. Likewise, the category of topological groups (whose morphisms are the continuous group homomorphisms) is a category of topological spaces with extra structure. A forgetful functor between categories of algebraic structures \"forgets\" a part of a structure.\n\nThere are various concepts in category theory that try to capture the algebraic character of a context, for instance\n\n\nIn a slight abuse of notation, the word \"structure\" can also refer to just the operations on a structure, instead of the underlying set itself. For example, the sentence, \"We have defined a ring \"structure\" on the set formula_1,\" means that we have defined ring \"operations\" on the set formula_1. For another example, the group formula_3 can be seen as a set formula_4 that is equipped with an \"algebraic structure,\" namely the \"operation\" formula_5.\n\n\n\n"}
{"id": "22577850", "url": "https://en.wikipedia.org/wiki?curid=22577850", "title": "Berge's lemma", "text": "Berge's lemma\n\nIn graph theory, Berge's lemma states that a matching \"M\" in a graph \"G\" is maximum (contains the largest possible number of edges) if and only if there is no augmenting path (a path that starts and ends on free (unmatched) vertices, and alternates between edges in and not in the matching) with \"M\".\n\nIt was proven by French mathematician Claude Berge in 1957 (though already observed by Petersen in 1891 and Kőnig in 1931).\n\nTo prove Berge's lemma, we first need another lemma. Take a graph \"G\" and let \"M\" and \"M′\" be two matchings in \"G\". Let \"G′\" be the resultant graph from taking the symmetric difference of \"M\" and \"M′\"; i.e. (\"M\" - \"M′\") ∪ (\"M′\" - \"M\"). \"G′\" will consist of connected components that are one of the following:\n\nThe lemma can be proven by observing that each vertex in \"G′\" can be incident to at most 2 edges: one from \"M\" and one from \"M′\". Graphs where every vertex has degree less than or equal to 2 must consist of either isolated vertices, cycles, and paths. Furthermore, each path and cycle in \"G′\" must alternate between \"M\" and \"M′\". In order for a cycle to do this, it must have an equal number of edges from \"M\" and \"M′\", and therefore be of even length.\n\nLet us now prove the contrapositive of Berge's lemma: \"G\" has a matching larger than \"M\" if and only if \"G\" has an augmenting path. Clearly, an augmenting path \"P\" of \"G\" can be used to produce a matching \"M′\" that is larger than \"M\" — just take \"M′\" to be the symmetric difference of \"P\" and \"M\" (\"M′\" contains exactly those edges of \"G\" that appear in exactly one of \"P\" and \"M\"). Hence, the reverse direction follows.\n\nFor the forward direction, let \"M′\" be a matching in \"G\" larger than \"M\". Consider \"D\", the symmetric difference of \"M\" and \"M′\". Observe that \"D\" consists of paths and even cycles (as observed by the earlier lemma). Since \"M′\" is larger than \"M\", \"D\" contains a component that has more edges from \"M′\" than \"M\". Such a component is a path in \"G\" that starts and ends with an edge from \"M′\", so it is an augmenting path.\n\nLet \"M\" be a maximum matching and consider an alternating chain such that the edges in the path alternates between being and not being in \"M\". If the alternating chain is a cycle or a path of even length, then a new maximum matching \"M′\" can be found by interchanging the edges found in \"M\" and not in \"M\". For example, if the alternating chain is (\"m\", \"n\", \"m\", \"n\", ...), where \"m\" ∈ \"M\" and \"n\" ∉ \"M\", interchanging them would make all \"n\" part of the new matching and make all \"m\" not part of the matching.\n\nAn edge is considered \"free\" if it belongs to a maximum matching but does not belong to all maximum matchings. An edge \"e\" is free if and only if, in an arbitrary maximum matching \"M\", edge \"e\" belongs to an even alternating path starting at an unmatched vertex or to an alternating cycle. By the first corollary, if edge \"e\" is part of such an alternating chain, then a new maximum matching, \"M′\", must exist and \"e\" would exist either in \"M\" or \"M′\", and therefore be free. Conversely, if edge \"e\" is free, then \"e\" is in some maximum matching \"M\" but not in \"M′\". Since \"e\" is not part of both \"M\" and \"M′\", it must still exist after taking the symmetric difference of \"M\" and \"M′\". The symmetric difference of \"M\" and \"M′\" results in a graph consisting of isolated vertices, even alternating cycles, and alternating paths. Suppose to the contrary that \"e\" belongs to some odd-length path component. But then one of \"M\" and \"M′\" must have one fewer edge than the other in this component, meaning that the component as a whole is an augmenting path with respect to that matching. By the original lemma, then, that matching (whether \"M\" or \"M′\") cannot be a maximum matching, which contradicts the assumption that both \"M\" and \"M′\" are maximum. So, since \"e\" cannot belong to any odd-length path component, it must either be in an alternating cycle or an even-length alternating path.\n\n"}
{"id": "27440323", "url": "https://en.wikipedia.org/wiki?curid=27440323", "title": "Cartan's lemma", "text": "Cartan's lemma\n\nIn mathematics, Cartan's lemma refers to a number of results named after either Élie Cartan or his son Henri Cartan:\n\n\n"}
{"id": "19983", "url": "https://en.wikipedia.org/wiki?curid=19983", "title": "Central moment", "text": "Central moment\n\nIn probability theory and statistics, a central moment is a moment of a probability distribution of a random variable about the random variable's mean; that is, it is the expected value of a specified integer power of the deviation of the random variable from the mean. The various moments form one set of values by which the properties of a probability distribution can be usefully characterised. Central moments are used in preference to ordinary moments, computed in terms of deviations from the mean instead of from zero, because the higher-order central moments relate only to the spread and shape of the distribution, rather than also to its location.\n\nSets of central moments can be defined for both univariate and multivariate distributions.\n\nThe \"n\"th moment about the mean (or \"n\"th central moment) of a real-valued random variable \"X\" is the quantity \"μ\" := E[(\"X\" − E[\"X\"])], where E is the expectation operator. For a continuous univariate probability distribution with probability density function \"f\"(\"x\"), the \"n\"th moment about the mean \"μ\" is\n\nFor random variables that have no mean, such as the Cauchy distribution, central moments are not defined.\n\nThe first few central moments have intuitive interpretations:\n\nThe \"n\"th central moment is translation-invariant, i.e. for any random variable \"X\" and any constant \"c\", we have\n\nFor all \"n\", the \"n\"th central moment is homogeneous of degree \"n\":\n\n\"Only\" for \"n\" such that n equals 1, 2, or 3 do we have an additivity property for random variables \"X\" and \"Y\" that are independent:\n\nA related functional that shares the translation-invariance and homogeneity properties with the \"n\"th central moment, but continues to have this additivity property even when \"n\" ≥ 4 is the \"n\"th cumulant κ(\"X\"). For \"n\" = 1, the \"n\"th cumulant is just the expected value; for \"n\" = either 2 or 3, the \"n\"th cumulant is just the \"n\"th central moment; for \"n\" ≥ 4, the \"n\"th cumulant is an \"n\"th-degree monic polynomial in the first \"n\" moments (about zero), and is also a (simpler) \"n\"th-degree polynomial in the first \"n\" central moments.\n\nSometimes it is convenient to convert moments about the origin to moments about the mean. The general equation for converting the \"n\"th-order moment about the origin to the moment about the mean is\n\nwhere \"μ\" is the mean of the distribution, and the moment about the origin is given by\n\nFor the cases \"n\" = 2, 3, 4 — which are of most interest because of the relations to variance, skewness, and kurtosis, respectively — this formula becomes (noting that formula_7 and formula_8):,\n\n... and so on, following Pascal's triangle, i.e.\n\nbecause formula_14\n\nThe following sum is a stochastic variable having a compound distribution\n\nwhere the formula_16 are mutually independent random variables sharing the same common distribution and formula_17 a random integer variable independent of the formula_18 with its own distribution. The moments of formula_19 are obtained as \n\nwhere formula_21 is defined as zero for formula_22.\n\nIn a symmetric distribution (one that is unaffected by being reflected about its mean), all odd central moments equal zero, because in the formula for the \"n\"th moment, each term involving a value of \"X\" less than the mean by a certain amount exactly cancels out the term involving a value of \"X\" greater than the mean by the same amount.\n\nFor a continuous bivariate probability distribution with probability density function \"f\"(\"x\",\"y\") the (\"j\",\"k\") moment about the mean \"μ\" = (\"μ\", \"μ\") is\n\n"}
{"id": "3509428", "url": "https://en.wikipedia.org/wiki?curid=3509428", "title": "Christos Papadimitriou", "text": "Christos Papadimitriou\n\nChristos Harilaos Papadimitriou (Greek: Χρήστος Χαρίλαος Παπαδημητρίου; born August 16, 1949) is a Greek theoretical computer scientist, and professor of Computer Science at University of California, Berkeley.\n\nPapadimitriou studied at the National Technical University of Athens, where in 1972 he received his Bachelor of Arts degree in Electrical Engineering. He continued to study at Princeton University, where he received his MS in Electrical Engineering in 1974 and his PhD in Electrical Engineering and Computer Science in 1976.\n\nPapadimitriou has taught at Harvard, MIT, the National Technical University of Athens, Stanford, UCSD, University of California, Berkeley and is currently the Donovan Family Professor of Computer Science at Columbia University.\n\nPapadimitriou co-authored a paper on pancake sorting with Bill Gates, then a Harvard undergraduate. Papadimitriou recalled \"Two years later, I called to tell him our paper had been accepted to a fine math journal. He sounded eminently disinterested. He had moved to Albuquerque, New Mexico to run a small company writing code for microprocessors, of all things. I remember thinking: 'Such a brilliant kid. What a waste.'\"\n\nIn 2001, Papadimitriou was inducted as a Fellow of the Association for Computing Machinery and in 2002 he was awarded the Knuth Prize. He became fellow of the U.S. National Academy of Engineering for contributions to complexity theory, database theory, and combinatorial optimization. In 2009 he was elected to the US National Academy of Sciences. During the 36th International Colloquium on Automata, Languages and Programming (ICALP 2009), there was a special event honoring Papadimitriou's contributions to computer science. In 2012, he, along with Elias Koutsoupias, was awarded the Gödel Prize for their joint work on the concept of the price of anarchy.\n\nPapadimitriou is the author of the textbook \"Computational Complexity\", one of the most widely used textbooks in the field of computational complexity theory. He has also co-authored the textbook \"Algorithms\" (2006) with Sanjoy Dasgupta and Umesh Vazirani, and the graphic novel \"Logicomix\" (2009) with Apostolos Doxiadis.\n\nHis name was listed in the 19th position on the CiteSeer search engine academic database and digital library.\n\nIn 1997, Papadimitriou received a doctorate \"honoris causa\" from the ETH Zurich.\n\nIn 2011, Papadimitriou received a doctorate \"honoris causa\" from the National Technical University of Athens.\n\nIn 2013, Papadimitriou received a doctorate \"honoris causa\" from the École polytechnique fédérale de Lausanne (EPFL).\n\nPapadimitriou was awarded the IEEE John von Neumann Medal in 2016, the EATCS Award in 2015, the Gödel Prize in 2012, the IEEE Computer Society Charles Babbage Award in 2004, and the Knuth Prize in 2002.\n\n\nAt UC Berkeley, in 2006, he joined a professor-and-graduate-student band called Lady X and The Positive Eigenvalues.\n"}
{"id": "1483049", "url": "https://en.wikipedia.org/wiki?curid=1483049", "title": "Circuit rank", "text": "Circuit rank\n\nIn graph theory, a branch of mathematics, the circuit rank, cyclomatic number, cycle rank, or nullity of an undirected graph is the minimum number of edges that must be removed from the graph to break all its cycles, making it into a tree or forest. Alternatively, it can be interpreted as the number of independent cycles in the graph. Unlike the corresponding feedback arc set problem for directed graphs, the circuit rank is easily computed using the formula\nwhere is the number of edges in the given graph, is the number of vertices, and is the number of connected components.\n\nThe circuit rank can be explained in terms of algebraic graph theory as the dimension of the cycle space of a graph, in terms of matroid theory as the corank of a graphic matroid, and in terms of topology as one of the Betti numbers of a topological space derived from the graph. It counts the ears in an ear decomposition of the graph, forms the basis of parameterized complexity on almost-trees, and has been applied in software metrics as part of the definition of cyclomatic complexity of a piece of code. Under the name of cyclomatic number, the concept was introduced by Gustav Kirchhoff.\n\nThe circuit rank of a graph may be described using matroid theory as the corank of the graphic matroid of . Using the greedy property of matroids, this means that one can find a minimum set of edges that breaks all cycles using a greedy algorithm that at each step chooses an edge that belongs to at least one cycle of the remaining graph.\n\nAlternatively, a minimum set of edges that breaks all cycles can be found by constructing a spanning forest of and choosing the complementary set of edges that do not belong to the spanning forest.\n\nIn algebraic graph theory, the circuit rank is also the dimension of the cycle space of formula_2. Intuitively, this can be explained as meaning that the circuit rank counts the number of independent cycles in the graph, where a collection of cycles is independent if it is not possible to form one of the cycles as the symmetric difference of some subset of the others.\n\nThis count of independent cycles can also be explained using homology theory, a branch of topology. Any graph may be viewed as an example of a 1-dimensional simplicial complex, a type of topological space formed by representing each graph edge by a line segment and gluing these line segments together at their endpoints.\nThe cyclomatic number is the rank of the first (integer) homology group of this complex,\nBecause of this topological connection, the cyclomatic number of a graph is also called the first Betti number of . More generally, the first Betti number of any topological space, defined in the same way, counts the number of independent cycles in the space.\n\nA variant of the circuit rank for planar graphs, normalized by dividing by the maximum possible circuit rank of any planar graph with the same vertex set, is called the meshedness coefficient. For a connected planar graph with edges and vertices, the meshedness coefficient can be computed by the formula\nHere, the numerator formula_5 of the formula is the circuit rank of the given graph, and the denominator formula_6 is the largest possible circuit rank of an -vertex planar graph. The meshedness coefficient ranges between 0 for trees and 1 for maximal planar graphs.\n\nThe circuit rank controls the number of ears in an ear decomposition of a graph, a partition of the edges of the graph into paths and cycles that is useful in many graph algorithms.\nIn particular, a graph is 2-vertex-connected if and only if it has an open ear decomposition. This is a sequence of subgraphs, where the first subgraph is a simple cycle, the remaining subgraphs are all simple paths, each path starts and ends on vertices that belong to previous subgraphs,\nand each internal vertex of a path appears for the first time in that path. In any biconnected graph with circuit rank formula_7, every open ear decomposition has exactly formula_7 ears.\n\nA graph with cyclomatic number formula_7 is also called a \"r\"-almost-tree, because only \"r\" edges need to be removed from the graph to make it into a tree or forest. A 1-almost-tree is a near-tree: a connected near-tree is a pseudotree, a cycle with a (possibly trivial) tree rooted at each vertex.\n\nSeveral authors have studied the parameterized complexity of graph algorithms on \"r\"-near-trees, parameterized by formula_7.\n\nThe cycle rank is an invariant of directed graphs that measures the level of nesting of cycles in the graph. It has a more complicated definition than circuit rank (closely related to the definition of tree-depth for undirected graphs) and is more difficult to compute. Another problem for directed graphs related to the circuit rank is the minimum feedback arc set, the smallest set of edges whose removal breaks all directed cycles. Both cycle rank and the minimum feedback arc set are NP-hard to compute.\n\nIt is also possible to compute a simpler invariant of directed graphs by ignoring the directions of the edges and computing the circuit rank of the underlying undirected graph. This principle forms the basis of the definition of cyclomatic complexity, a software metric for estimating how complicated a piece of computer code is.\n\nIn the fields of chemistry and cheminformatics, the circuit rank of a molecular graph is sometimes referred to as the Frèrejacque number and is the number of number of rings in the Smallest set of smallest rings (SSSR).\n\nOther numbers defined in terms of edge deletion from undirected graphs include the edge connectivity, the minimum number of edges to delete in order to disconnect the graph, and matching preclusion, the minimum number of edges to delete in order to prevent the existence of a perfect matching.\n"}
{"id": "211757", "url": "https://en.wikipedia.org/wiki?curid=211757", "title": "Disentanglement puzzle", "text": "Disentanglement puzzle\n\nDisentanglement puzzles (also called tanglement puzzles, tavern puzzles or topological puzzles) are a type of mechanical puzzle that involves disentangling one piece or set of pieces from another piece or set of pieces. The reverse problem of reassembling the puzzle can be as hard as—or even harder than—disentanglement. There are several different kinds of disentanglement puzzles, though a single puzzle may incorporate several of these features.\n\nWire-and-string puzzles usually consist of: \n\nOne can distinguish three subgroups of wire-and-string puzzles:\n\nOne particularly difficult puzzle was designed by R. Boomhower in 1966 and has been modified into different designs (but topologically similar). Different versions include a paddle-shaped design, a vertical beam on a wood support, and two vertical beams on a wood support. Variations also have the string passing through the slot once or two times. Names have included the Boomhower puzzle, T-Bar puzzle, Wit's End puzzle, and the Mini Rope Bridge puzzle. Some sources identify a topologically-equivalent puzzle called the Mystery Key issued by the Peter Pan company in the 1950s.\n\nWire puzzles consist of two or more entangled pieces of more or less stiff wire. \nThe pieces may or may not be closed loops. The closed pieces might be simple rings or have more complex shapes. Normally the puzzle must be solved by disentangling the two pieces without bending or cutting the wires. \n\nEarly wire puzzles were made from horseshoes and similar material.\n\nA plate-and-ring puzzle usually consists of three pieces:\n\n\nThe plate as well as the ring are usually made from metal. The ring has to be disentangled from the plate. \n\nSome puzzles have been created which may appear deceptively simple, but are actually impossible to solve. One such puzzle is the \"Notorious Figure Eight Puzzle\" (also called the \"Figure Eight Puzzle, or \"Possibly Impossible\"). It is sometimes sold with instructions giving hints as to its level of difficulty, and a \"solution\" is provided but is vague and impossible to follow, but the puzzle is actually impossible to solve.\n\nMost puzzle solvers try to solve such puzzles by mechanical manipulation, but some branches of mathematics can be used to create a model of disentanglement puzzles. Applying a configuration space with a topological framework is an analytical method to gain insight into the properties and solution of some disentanglement puzzles. However, some mathematicians have stated that capturing the important aspects of many such puzzles can often be difficult, and there is no universal algorithm that will provide the solution generally to such puzzles.\n\n"}
{"id": "10986849", "url": "https://en.wikipedia.org/wiki?curid=10986849", "title": "East Journal on Approximations", "text": "East Journal on Approximations\n\nThe East Journal on Approximations is a journal about approximation theory published in Sofia, Bulgaria.\n\n"}
{"id": "3951378", "url": "https://en.wikipedia.org/wiki?curid=3951378", "title": "El Nombre", "text": "El Nombre\n\nEl Nombre is an anthropomorphic Mexican gerbil character, originally from a series of educational sketches on \"Numbertime\", the BBC schools programme about mathematics. He was also the only character to appear in all \"Numbertime\" episodes. His voice was provided by Steve Steen, while the other characters' voices were provided by Sophie Aldred, Kate Robbins, and (from 1999) former \"Blue Peter\" host Janet Ellis. For the ninth (and final) series of \"Numbertime\" in 2001, Michael Fenton-Stevens also provided voices of certain other characters in the \"El Nombre\" sketches.\n\nThe character's name means \"The Name\" in Spanish, not \"The Number\", which would be \"El Número\".\n\n\"El Nombre\" is set in the fictional town of Santa Flamingo (originally known as Santo Flamingo), home of Little Juan, his Mama, Pedro Gonzales, Juanita Conchita, Maria Consuela Tequila Chiquita, Little Pepita Consuela Tequila Chiquita, Tanto the tarantula, Señor Gelato the ice-cream seller, Leonardo de Sombrero the pizza delivery boy, Señor Calculo the bank manager, Señor Manuel the greengrocer, Miss Constanza Bonanza the school teacher, Señora Fedora the balloon seller and mayor, Señor Loco the steam engine driver, Señor Chipito the carpenter and the local bandit Don Fandango (although it was not actually given a name until the fifth series of \"Numbertime\" premiered in January 1998); whenever he was needed, El Nombre swung into action to solve the townspeople's simple mathematical problems, usually talking in rhyme. His character was a parody of the fictional hero Zorro, wearing a similar black cowl mask and huge sombrero, appearing unexpectedly to save the townsfolk from injustice, and generally swinging around on his bullwhip - however, unlike Zorro, he was often quite inept (in fact, on one occasion, Tanto tipped a bucket of water onto him after he made him reenact the \"Incy Wincy Spider\" rhyme).\n\nWhen El Nombre first appeared on \"Numbertime\" in 1993, his purpose was merely to write numbers in the desert sand and demonstrate the correct ways to form them as his four-piece mariachi band played \"The Mexican Hat Dance\" (and said \"Again!\" once he had finished, as it gave them an excuse to play again); this was shot from an angle directly overhead leaving El Nombre almost completely eclipsed by his large sombrero. His appeal was instant and his success prompted rapid development of his role in the series (as from the second series in 1995, he was given \"two\" sketches per episode) - and since his basic beginning, El Nombre went on to appear in a total of 79 (89, if counting those from the \"revised\" version of the first series) sketches on \"Numbertime\" before gaining a series of his own, acquiring dramatic storylines and a full cast of characters, while continuing to demonstrate mathematical concepts, albeit in a dramatic and entertaining way. The stories moved away from solving simple mathematical equations to fighting petty crime, unrelated to the number-solving which made his name and for which he was created.\n\nAs well as being popular with schoolchildren, \"El Nombre\" also developed a cult following amongst students and parents, because of the many references to classic spaghetti Westerns; indeed, his popularity grew so much that in March 2004, the BBC released a 3-minute \"El Nombre\" theme song as a single.\n\n\nA gerbil named Pablo also appeared in the ninth series of \"Numbertime\" after Juan entered a competition on Radio Flamingo to win a holiday to the seaside resort of Costa Fortuna and won; Juan, Mama, Pedro, Juanita and Maria met him when they arrived at the resort's hotel (because he was their guide to it), and he went on to front a ring-toss stall when they visited its fairground the following week.\n\nAlthough none of the \"El Nombre\" sketches on \"Numbertime\" ever had a specific title, those of the first series were introduced by an announcer as \"Episodes 1-10\" (and they were slightly lengthened for the \"revised\" edition of that series, in September 1998; the third line of the opening song and his farewell catchphrase were also changed several times, to reflect the series' focus). All twenty-six episodes of the spin-off \"El Nombre\" series (thirteen in 2001 and a further thirteen in 2003), however, were titled - and their names are listed here.\n\nThe first six episodes of the first series were aired on BBC One as double bills in the CBBC strand on Fridays at 3:45 pm, while the next seven were aired individually on Wednesdays in the same timeslot; three episodes were later repeated on BBC Two as part of the CBBC Breakfast Show on 1 June, 19 July and 20 July 2001, but neither they, or the other ten episodes of the series, were repeated after that.\n\nThe second series was aired as double bills on the CBeebies Channel on Saturdays and Sundays at 3:30 pm; after the last episode aired on 29 November, the first one was immediately repeated again, and the series concluded its second consecutive run in the same timeslot on 4 January 2004. All thirteen episodes were later repeated on BBC Two in the CBeebies strand on Wednesdays from 7 January to 31 March 2004.\n\nIn October 2005, all twenty-six episodes were released on DVD by Maverick Entertainment; the first ten were previously released on a VHS entitled \"El Nombre to the Rescue\" by BBC Worldwide in 2001, which also featured an exclusive short (entitled \"Learn Your Numbers With Little Juan\", and edited together from the \"El Nombre\" sketches of the \"original\" first series of \"Numbertime\"). Some of the \"El Nombre\" (and cell-animated) sketches of the \"revised\" first, second and fifth, and fourth series of \"Numbertime\" were also released by BBC Active in 2009 on three DVDs entitled \"Fun with Numbers\" - which all came with accompanying books featuring the characters, and were subtitled \"Counting 1 to 10\", \"Shapes and Time\" (the featured sketches were mostly from the second series), and \"Adding and Taking Away\" respectively.\n\n\n"}
{"id": "21442284", "url": "https://en.wikipedia.org/wiki?curid=21442284", "title": "Emil Grosswald", "text": "Emil Grosswald\n\nEmil Grosswald (December 15, 1912 – April 11, 1989) was a mathematician who worked primarily in number theory.\n\nGrosswald was born on December 15, 1912 in Bucharest, Romania. He received a Master's degree in both mathematics and electrical engineering from the University of Bucharest in 1933, spent 6 months in Italy and then received a Diplôme from École supérieure d'électricité in Paris.\n\nGrosswald was Jewish. When war broke out, he fled from Paris in June, 1940 to the University of Montpellier, where he began doctoral studies in mathematics. He fled at the end of 1941, through Spain and Lisbon to Cuba. He moved to Puerto Rico in 1946 and then to the United States in 1948. He received his Ph.D. under Hans Rademacher from the University of Pennsylvania in 1950. He was Visiting Professor at the University of Paris in 1964–1965 and his book, \"The Theory of Numbers\" was written that year.\n\nHe met his wife Elisabeth (Lissy) Rosenthal in Cuba, probably in 1941 or 1942. They were married in 1950 in Saskatoon, Canada, where he had his first teaching position after receiving his Ph.D. They spent two years at the Institute for Advanced Studies in Princeton, New Jersey in 1951 and 1959. During their first stay, they met Albert Einstein, with whom Emil had a correspondence, later bequeathed to the University of Texas, and formed many friendships, among others with the physicist Freeman Dyson. Emil and Lissy had two daughters, Blanche, who became a professor of Social Work at Rutgers University but died in 2003 at the age of 50, and Vivian, a professor of law at the University of Pittsburgh. Vivian was decorated in 2007 by the Republic of Austria for her work as the United States appointee to the Austrian General Settlement Fund Committee for Nazi-era property compensation, and in 2013 by the government of France for her services in promotion of the French language and culture in the United States. Emil is the uncle of Pamela Ronald, whose father Robert Ronald (né Rosenthal) describes the family’s escape from the Nazis in his memoir, The son of Lissy’s second cousin (Ernest Beutler) is 2011 Nobel Laureate Bruce Beutler. Emil was also the nephew of the French musician Marcel Mihalovici, who arrived in Paris in the 1920s with George Enescu.\n\nGrosswald died April 11, 1989 in Narberth, Pennsylvania.\n\nGrosswald's first three scientific papers, written while he was in Cuba, were published under the pseudonym E. G. Garnea. He published articles in English, German, French, Spanish and Italian.\n\nAfter receiving his PhD in 1950, Grosswald taught at the University of Pennsylvania from 1952 to 1968 and then moved to Temple University and stayed until his retirement in 1980. He also held positions at the University of Saskatchewan (1950), Institute for Advanced Study (1951 and 1959), the Technion (1980–1981), Swarthmore College (1982), and the University of Pennsylvania (1984).\n\nGrosswald completed some works of his teacher Hans Rademacher, who died in 1969. Rademacher had prepared notes for an Earle Raymond Hedrick Lecture in Boulder, Colorado in 1963 on Dedekind sums, but fell ill, and Grosswald gave the lecture for him.\nAfter Rademacher's death, Grosswald edited and completed the notes and published them in the Carus Mathematical Monographs series as \"Dedekind Sums\".\nHe also edited for publication Rademacher's posthumous textbook \"Topics in Analytic Number Theory\".\n\nGrosswald was elected to the Board of Governors of the Mathematical Association of America for 1965–1968. Temple University's Mathematics Department annually sponsors the Emil Grosswald Memorial Lectures.\n\n\n\n"}
{"id": "857780", "url": "https://en.wikipedia.org/wiki?curid=857780", "title": "Entropic uncertainty", "text": "Entropic uncertainty\n\nIn quantum mechanics, information theory, and Fourier analysis, the entropic uncertainty or Hirschman uncertainty is defined as the sum of the temporal and spectral Shannon entropies. It turns out that Heisenberg's uncertainty principle can be expressed as a lower bound on the sum of these entropies. This is \"stronger\" than the usual statement of the uncertainty principle in terms of the product of standard deviations.\n\nIn 1957, Hirschman considered a function \"f\" and its Fourier transform \"g\" such that\nwhere the \"≈\" indicates convergence in , and normalized so that (by Plancherel's theorem),\n\nHe showed that for any such functions the sum of the Shannon entropies is non-negative,\n\nA tighter bound,\n\nwas conjectured by Hirschman and Everett, proven in 1975 by W. Beckner\nand in the same year interpreted by as a generalized quantum mechanical uncertainty principle by and Mycielski.\nThe equality holds in the case of Gaussian distributions.\n\nNote, however, that the above entropic uncertainty function is distinctly \"different\" from the quantum Von Neumann entropy represented in phase space.\n\nThe proof of this tight inequality depends on the so-called (\"q\", \"p\")-norm of the Fourier transformation. (Establishing this norm is the most difficult part of the proof.)\n\nFrom this norm, one is able to establish a lower bound on the sum of the (differential) Rényi entropies, , where , which generalize the Shannon entropies. For simplicity, we consider this inequality only in one dimension; the extension to multiple dimensions is straightforward and can be found in the literature cited.\n\nThe (\"q\", \"p\")-norm of the Fourier transform is defined to be\n\nIn 1961, Babenko found this norm for \"even\" integer values of \"q\". Finally, in 1975,\nusing Hermite functions as eigenfunctions of the Fourier transform, Beckner proved that the value of this norm (in one dimension) for all \"q\" ≥ 2 is\nThus we have the Babenko–Beckner inequality that\n\nFrom this inequality, an expression of the uncertainty principle in terms of the Rényi entropy can be derived.\n\nLetting formula_9, 2\"α\"=\"p\", and 2\"β\"=\"q\", so that and 1/2<\"α\"<1<\"β\", we have\nSquaring both sides and taking the logarithm, we get\n\nMultiplying both sides by \nreverses the sense of the inequality,\n\nRearranging terms, finally yields an inequality in terms of the sum of the Rényi entropies,\n\nNote that this inequality is symmetric with respect to and : One no longer need assume that ; only that they are positive and not both one, and that \"1/α + 1/β\" = 2. To see this symmetry, simply exchange the rôles of \"i\" and −\"i\" in the Fourier transform.\n\nTaking the limit of this last inequality as \"α, β\" → 1 yields the less general Shannon entropy inequality,\nvalid for any base of logarithm, as long as we choose an appropriate unit of information, bit, nat, etc.\n\nThe constant will be different, though, for a different normalization of the Fourier transform, (such as is usually used in physics, with normalizations chosen so that \"ħ\"=1 ), i.e.,\nIn this case, the dilation of the Fourier transform absolute squared by a factor of 2 simply adds log(2) to its entropy.\n\nThe Gaussian or normal probability distribution plays an important role in the relationship between variance and entropy: it is a problem of the calculus of variations to show that this distribution maximizes entropy for a given variance, and at the same time minimizes the variance for a given entropy. In fact, for any probability density function \"φ\" on the real line, Shannon's entropy inequality specifies:\nwhere \"H\" is the Shannon entropy and \"V\" is the variance, an inequality that is saturated only in the case of a normal distribution.\n\nMoreover, the Fourier transform of a Gaussian probability amplitude function is also Gaussian—and the absolute squares of both of these are Gaussian, too. This can then be used to derive the usual Robertson variance uncertainty inequality from the above entropic inequality, enabling \"the latter to be tighter than the former\". That is (for \"ħ\"=1), exponentiating the Hirschman inequality and using Shannon's expression above, \n\nHirschman explained that entropy—his version of entropy was the negative of Shannon's—is a \"measure of the concentration of [a probability distribution] in a set of small measure.\" Thus \"a low or large negative Shannon entropy means that a considerable mass of the probability distribution is confined to a set of small measure\".\n\nNote that this set of small measure need not be contiguous; a probability distribution can have several concentrations of mass in intervals of small measure, and the entropy may still be low no matter how widely scattered those intervals are. This is not the case with the variance: variance measures the concentration of mass about the mean of the distribution, and a low variance means that a considerable mass of the probability distribution is concentrated in a \"contiguous interval\" of small measure.\n\nTo formalize this distinction, we say that two probability density functions \"φ\" and \"φ\" are equimeasurable if\nwhere is the Lebesgue measure. Any two equimeasurable probability density functions have the same Shannon entropy, and in fact the same Rényi entropy, of any order. The same is not true of variance, however. Any probability density function has a radially decreasing equimeasurable \"rearrangement\" whose variance is less (up to translation) than any other rearrangement of the function; and there exist rearrangements of arbitrarily high variance, (all having the same entropy.)\n\n\n"}
{"id": "23265069", "url": "https://en.wikipedia.org/wiki?curid=23265069", "title": "Forward volatility", "text": "Forward volatility\n\nForward volatility is a measure of the implied volatility of a financial instrument over a period in the future, extracted from the term structure of volatility (which refers to how implied volatility differs for related financial instruments with different maturities).\n\nThe variance is the square of differences of measurements from the mean divided by the number of samples. The standard deviation is the square root of the variance. \nThe standard deviation of the continuously compounded returns of a financial instrument is called volatility.\n\nThe (yearly) volatility in a given asset price or rate over a term that starts from formula_1 corresponds to the spot volatility for that underlying, for the specific term. A collection of such volatilities forms a volatility term structure, similar to the yield curve. Just as forward rates can be derived from a yield curve, forward volatilities can be derived from a given term structure of volatility.\n\nGiven that the underlying random variables for non overlapping time intervals are independent, the variance is additive (see variance). So for yearly time slices we have the annualized volatility as\n\nformula_2\n\nwhere\n\nTo ease computation and get a non-recursive representation, we can also express the forward volatility directly in terms of spot volatilities:\n\nformula_9\n\nFollowing the same line of argumentation we get in the general case with formula_10 for the forward volatility seen at time formula_11:\n\nformula_12,\n\nwhich simplifies in the case of formula_1 to\n\nformula_14.\n\nThe volatilities in the market for 90 days are 18% and for 180 days 16.6%. In our notation we have formula_15 = 18% and formula_16 = 16.6% (treating a year as 360 days). \nWe want to find the forward volatility for the period starting with day 91 and ending with day 180. Using the above formular and setting formula_1 we get\n\nformula_18.\n"}
{"id": "51692308", "url": "https://en.wikipedia.org/wiki?curid=51692308", "title": "Geometric class field theory", "text": "Geometric class field theory\n\nIn mathematics, geometric class field theory is an extension of class field theory to higher-dimensional geometrical objects: much the same way as class field theory describes the abelianization of the Galois group of a local or global field, geometric class field theory describes the abelianized fundamental group of higher dimensional schemes in terms of data related to algebraic cycles.\n"}
{"id": "1980381", "url": "https://en.wikipedia.org/wiki?curid=1980381", "title": "Hatching", "text": "Hatching\n\nHatching (\"hachure\" in French) is an artistic technique used to create tonal or shading effects by drawing (or painting or scribing) closely spaced parallel lines. (It is also used in monochromatic heraldic representations to indicate what the tincture of a \"full-colour\" emblazon would be.) When lines are placed at an angle to one another, it is called cross-hatching.\n\nHatching is especially important in essentially linear media, such as drawing, and many forms of printmaking, such as engraving, etching and woodcut. In Western art, hatching originated in the Middle Ages, and developed further into cross-hatching, especially in the old master prints of the fifteenth century. Master ES and Martin Schongauer in engraving and Erhard Reuwich and Michael Wolgemut in woodcut were pioneers of both techniques, and Albrecht Dürer in particular perfected the technique of crosshatching in both media.\n\nArtists use the technique, varying the length, angle, closeness and other qualities of the lines, most commonly in drawing, linear painting and engraving.\n\nThe main concept is that the quantity, thickness and spacing of the lines will affect the brightness of the overall image, and emphasize forms creating the illusion of volume. Hatching lines should always follow (i.e. wrap around) the form. By increasing quantity, thickness and closeness, a darker area will result.\n\nAn area of shading next to another area which has lines going in another direction is often used to create contrast.\n\nLine work can be used to represent colours, typically by using the same type of hatch to represent particular tones. For example, red might be made up of lightly spaced lines, whereas green could be made of two layers of perpendicular dense lines, resulting in a realistic image.\n\n\n"}
{"id": "564719", "url": "https://en.wikipedia.org/wiki?curid=564719", "title": "Hybrid system", "text": "Hybrid system\n\nA hybrid system is a dynamical system that exhibits both continuous and discrete dynamic behavior – a system that can both \"flow\" (described by a differential equation) and \"jump\" (described by a state machine or automaton). Often, the term \"hybrid dynamical system\" is used, to distinguish over hybrid systems such as those that combine neural nets and fuzzy logic, or electrical and mechanical drivelines. A hybrid system has the benefit of encompassing a larger class of systems within its structure, allowing for more flexibility in modeling dynamic phenomena.\n\nIn general, the \"state\" of a hybrid system is defined by the values of the \"continuous variables\" and a discrete \"mode\". The state changes either continuously, according to a \"flow condition\", or discretely according to a \"control graph\". Continuous flow is permitted as long as so-called \"invariants\" hold, while discrete transitions can occur as soon as given \"jump conditions\" are satisfied. Discrete transitions may be associated with \"events\".\n\nHybrid systems have been used to model several cyber-physical systems, including physical systems with \"impact\", logic-dynamic controllers, and even Internet congestion.\n\nA canonical example of a hybrid system is the bouncing ball, a physical system with impact. Here, the ball (thought of as a point-mass) is dropped from an initial height and bounces off the ground, dissipating its energy with each bounce. The ball exhibits continuous dynamics between each bounce; however, as the ball impacts the ground, its velocity undergoes a discrete change modeled after an inelastic collision. A mathematical description of the bouncing ball follows. Let formula_1 be the height of the ball and formula_2 be the velocity of the ball. A hybrid system describing the ball is as follows:\n\nWhen formula_3, flow is governed by\nformula_4,\nwhere formula_5 is the acceleration due to gravity. These equations state that when the ball is above ground, it is being drawn to the ground by gravity.\n\nWhen formula_6, jumps are governed by\nformula_7,\nwhere formula_8 is a dissipation factor. This is saying that when the height of the ball is zero (it has impacted the ground), its velocity is reversed and decreased by a factor of formula_9. Effectively, this describes the nature of the inelastic collision.\n\nThe bouncing ball is an especially interesting hybrid system, as it exhibits Zeno behavior. Zeno behavior has a strict mathematical definition, but can be described informally as the system making an \"infinite\" number of jumps in a \"finite\" amount of time. In this example, each time the ball bounces it loses energy, making the subsequent jumps (impacts with the ground) closer and closer together in time.\n\nIt is noteworthy that the dynamical model is complete if and only if one adds the contact force between the ground and the ball. Indeed, without forces, one cannot properly define the bouncing ball and the model is, from a mechanical point of view, meaningless. The simplest contact model that represents the interactions between the ball and the ground, is the complementarity relation between the force and the distance (the gap) between the ball and the ground. This is written as\nformula_10\nSuch a contact model does not incorporate magnetic forces, nor gluing effects. When the complementarity relations are in, one can continue to integrate the system after the impacts have accumulated and vanished: the equilibrium of the system is well-defined as the static equilibrium of the ball on the ground, under the action of gravity compensated by the contact force formula_11. One also notices from basic convex analysis that the complementarity relation can equivalently be rewritten as the inclusion into a normal cone, so that the bouncing ball dynamics is a differential inclusion into a normal cone to a convex set. See Chapters 1, 2 and 3 in Acary-Brogliato's book cited below (Springer LNACM 35, 2008). See also the other references on non-smooth mechanics.\n\nThere are approaches to automatically proving properties of hybrid systems (e.g., some of the tools mentioned below). Common techniques for proving safety of hybrid systems are computation of reachable sets, abstraction refinement, and barrier certificates.\n\nMost verification tasks are undecidable, making general verification algorithms impossible. Instead, the tools are analyzed for their capabilities on benchmark problems. A possible theoretical characterization of this is algorithms that succeed with hybrid systems verification in all robust cases implying that many problems for hybrid systems, while undecidable, are at least quasi-decidable.\n\nTwo basic hybrid system modeling approaches can be classified, an implicit and an explicit one. The explicit approach is often represented by a hybrid automaton, a hybrid program or a hybrid Petri net. The implicit approach is often represented by guarded equations to result in systems of differential algebraic equations (DAEs) where the active equations may change, for example by means of a hybrid bond graph.\n\nAs a unified simulation approach for hybrid system analysis, there is a method based on DEVS formalism in which integrators for differential equations are quantized into atomic DEVS models. These methods generate traces of system behaviors in discrete event system manner which are different from discrete time systems. Detailed of this approach can be found in references [Kofman2004] [CF2006] [Nutaro2010] and the software tool PowerDEVS.\n\n\n\n"}
{"id": "38365576", "url": "https://en.wikipedia.org/wiki?curid=38365576", "title": "Induction-recursion", "text": "Induction-recursion\n\nIn intuitionistic type theory (ITT), a discipline within mathematical logic, induction-recursion is a feature for simultaneously declaring a type and function on that type. It allows the creation of larger types, such as universes, than inductive types. The types created still remain predicative inside ITT.\n\nAn inductive definition is given by rules for generating elements of a type. One can then define functions from that type by induction on the way the elements of the type are generated. Induction-recursion generalizes this situation since one can \"simultaneously\" define the type and the function, because the rules for generating elements of the type are allowed to refer to the function.\n\nInduction-recursion can be used to define large types including various universe constructions. It increases the proof-theoretic strength of type theory substantially. Nevertheless, inductive-recursive recursive definitions are still considered predicative.\n\nInduction-Recursion came out of investigations to the rules of Martin-Löf's intuitionistic type theory. The type theory has a number of \"type formers\" and four kinds of rules for each one. Martin-Löf had hinted that the rules for each type former followed a pattern, which preserved the properties of the type theory (e.g., strong normalization, predicativity). Researchers started looking for the most general description of the pattern, since that would tell what kinds of type formers could be added (or not added!) to extend the type theory.\n\nThe \"universe\" type former was the most interesting, because when the rules were written \"à la Tarski\", they simultaneously defined the \"universe type\" \"and \"a function that operated on it. This eventually lead Dybjer to Induction-Recursion.\n\nDybjer's initial papers called Induction-Recursion a \"schema\" for rules. It stated what type formers could be added to the type theory. Later, he and Setzer would write a new type former with rules that allowed new Induction-Recursion definitions to be made inside the type theory. This was added to the Half proof assistant (a variant of Alf).\n\nBefore covering Inductive-Recursive types, the simpler case is Inductive Types. Constructors for Inductive types can be self-referential, but in a limited way. The constructor's parameters must be \"positive\": \n\nWith Inductive types, a parameter's type can depend on earlier parameters, but they cannot refer to ones of the type being defined. Inductive-Recursive types go further and parameter's types \"can\" refer to earlier parameters that use the type being defined. These must be \"half-positive\":\n\nSo, if formula_1 is the type being defined and formula_2 is the function being (simultaneously) defined, these parameter declarations are positive:\n\nThis is half-positive:\n\nThese are not positive nor half-positive:\n\nA simple common example is the Universe à la Tarski type former. It creates a type formula_23 and a function formula_24. There is an element of formula_23 for every type in the type theory (except formula_23 itself!). The function formula_24 maps the elements of formula_23 to the associated type.\n\nThe type formula_23 has a constructor (or introduction rule) for each type former in the type theory. The one for dependent functions would be:\n\nformula_30\n\nThat is, it takes an element formula_31 of type formula_23 that will map to the type of the parameter and an element formula_33 that will map to the return type of the function (which is dependent on the value of the parameter). (The final formula_34 says that the result of the constructor is an element of type formula_23.)\n\nThe reduction (or computation rule) says that\n\nformula_36 becomes formula_37\n\nAfter reduction, the function formula_24 is operating on a smaller part of the input. If that holds when formula_24 is applied to any constructor, then formula_24 will always terminate. Without going into the details, Induction-Recursion states what kinds of definitions (or rules) can be added to the theory such that the function calls will always terminate.\n\nInduction-Recursion is implemented in Agda and Idris.\n\n\n"}
{"id": "25292663", "url": "https://en.wikipedia.org/wiki?curid=25292663", "title": "Integer points in convex polyhedra", "text": "Integer points in convex polyhedra\n\nThe study of integer points in convex polyhedra is motivated by questions such as \"how many nonnegative integer-valued solutions does a system of linear equations with nonnegative coefficients have\" or \"how many solutions does an integer linear program have\". Counting integer points in polyhedra or other questions about them arise in representation theory, commutative algebra, algebraic geometry, statistics, and computer science.\n\nThe set of integer points, or, more generally, the set of points of an affine lattice, in a polyhedron is called Z-polyhedron, from the mathematical notation formula_1 or Z for the set of integer numbers.\n\nFor a lattice Λ, Minkowski's theorem relates the number d(Λ) and the volume of a symmetric convex set \"S\" to the number of lattice points contained in \"S\".\n\nThe number of lattice points contained in a polytope all of whose vertices are elements of the lattice is described by the polytope's Ehrhart polynomial. Formulas for some of the coefficients of this polynomial involve d(Λ) as well.\n\nIn certain approaches to loop optimization, the set of the executions of the loop body is viewed as the set of integer points in a polyhedron defined by loop constraints.\n\n\n"}
{"id": "30961758", "url": "https://en.wikipedia.org/wiki?curid=30961758", "title": "Johannes Hjelmslev", "text": "Johannes Hjelmslev\n\nJohannes Trolle Hjelmslev (; 7 April 1873 – 16 February 1950) was a mathematician from Hørning, Denmark. Hjelmslev worked in geometry and history of geometry. He was the discoverer and eponym of the Hjelmslev transformation, a method for mapping an entire hyperbolic plane into a circle with a finite radius.\nHe was the father of Louis Hjelmslev.\n\n\n\n\n"}
{"id": "5815383", "url": "https://en.wikipedia.org/wiki?curid=5815383", "title": "Joseph Bernstein", "text": "Joseph Bernstein\n\nJoseph Bernstein (sometimes spelled I. N. Bernshtein; ; , \"Iosif Naumovič Bernštejn\"; born 18 April 1945) is an Soviet-born Israeli mathematician working at Tel Aviv University. He works in algebraic geometry, representation theory, and number theory.\n\nHe got first prize in 1962 International Mathematical Olympiad. Bernstein received his Ph.D. in 1972 under Israel Gelfand at Moscow State University, and moved to Harvard in 1983 due to growing anti-semitism in the Soviet Union. He was a visiting scholar at the Institute for Advanced Study in 1985-86 and again in 1997-98.\n\nBernstein was elected to the Israel Academy of Sciences and Humanities in 2002 and was elected to the United States National Academy of Sciences in 2004.\nIn 2004, Bernstein was awarded the Israel Prize for mathematics.\nIn 2012 he became a fellow of the American Mathematical Society.\n\n\n\n"}
{"id": "58612227", "url": "https://en.wikipedia.org/wiki?curid=58612227", "title": "Josephine D. Edwards", "text": "Josephine D. Edwards\n\nJosephine Dianne Edwards (18 August 1942 – 25 May 1985) was an Australian mathematician and mathematics educator who founded the Australian Mathematics Competition.\n\nShe was born in Oxford and was educated at the Ursuline School in Brentwood. She went on to study mathematics at the University of Oxford. In 1964, Edwards moved to Canberra. She taught mathematics at secondary schools in the Australian Capital Territory. In 1979, she joined the faculty at the College of Advanced Education in Canberra, later the University of Canberra. For eighteen years, she was a member of the Canberra Mathematical Association, also serving as its vice-president, president and secretary.\n\nShe helped establish and run the Australian Mathematics Competition, serving as chair of its founding committee, as a member of its board of governors from 1977 to 1985 and as editor for its publications from 1979. She was also an associate editor for the American publication \"The College Mathematics Journal\". Her articles on teaching mathematics appeared in journals in Australia, Canada and France.\n\nShe was married to John Pulley; the couple had three children as well as three children from Pulley's prior marriage.\n\nEdwards died in Canberra at the age of 42.\n\nIn 1996, she was awarded a BH Neumann Award.\n"}
{"id": "3543381", "url": "https://en.wikipedia.org/wiki?curid=3543381", "title": "Kato's conjecture", "text": "Kato's conjecture\n\nKato's conjecture is a mathematical problem named after mathematician Tosio Kato, of the University of California, Berkeley. Kato initially posed the problem in 1953.\n\nKato asked whether the square root of certain elliptic operators, defined via functional calculus, are analytic. The full statement of the problem as given by Auscher et. al. is: \"the domain of the square root of a uniformly complex elliptic operator L =-div (AV) with bounded measurable coefficients in R is the Sobolev space \"H\"(R) in any dimension with the estimate formula_1\". \n\nThe problem remained unresolved for nearly a half-century, until it was jointly solved in 2001 by Pascal Auscher, Steve Hofmann, Michael Lacey, Alan McIntosh, and Philippe Tchamitchian.\n"}
{"id": "2697574", "url": "https://en.wikipedia.org/wiki?curid=2697574", "title": "Leveler", "text": "Leveler\n\nA leveler performs an audio process similar to compression, which is used to reduce the dynamic range of a signal, so that the quietest portion of the signal is loud enough to hear and the loudest portion is not too loud. A leveler is different from a compressor in that the ratio and threshold are controlled with a single control. Levelers work especially well with vocals, as there are huge dynamic differences in the human voice and levelers work in such a way as to sound very natural, letting the character of the sound change with the different levels but still maintaining a predictable and usable dynamic range.\n\n"}
{"id": "1396888", "url": "https://en.wikipedia.org/wiki?curid=1396888", "title": "Levi graph", "text": "Levi graph\n\nIn combinatorial mathematics, a Levi graph or incidence graph is a bipartite graph associated with an incidence structure. From a collection of points and lines in an incidence geometry or a projective configuration, we form a graph with one vertex per point, one vertex per line, and an edge for every incidence between a point and a line. They are named for F. W. Levi, who wrote about them in 1942.\n\nThe Levi graph of a system of points and lines usually has girth at least six: Any 4-cycles would correspond to two lines through the same two points. Conversely any bipartite graph with girth at least six can be viewed as the Levi graph of an abstract incidence structure. Levi graphs of configurations are biregular, and every biregular graph with girth at least six can be viewed as the Levi graph of an abstract configuration.\n\nLevi graphs may also be defined for other types of incidence structure, such as the incidences between points and planes in Euclidean space. For every Levi graph, there is an equivalent hypergraph, and vice versa.\n\n"}
{"id": "1882363", "url": "https://en.wikipedia.org/wiki?curid=1882363", "title": "Littlewood's three principles of real analysis", "text": "Littlewood's three principles of real analysis\n\nLittlewood's three principles of real analysis are heuristics of J. E. Littlewood to help teach the essentials of measure theory in mathematical analysis.\n\nLittlewood stated the principles in his 1944 \"Lectures on the Theory of Functions\"\n\nas:\nThe first principle is based on the fact that the inner measure and outer measure are equal for measurable sets, the second is based on Lusin's theorem, and the third is based on Egorov's theorem.\n\nLittlewood's three principles are quoted in several real analysis texts, for example Royden,\nBressoud,\nand Stein & Shakarchi.\n\nRoyden gives the bounded convergence theorem as an application of the third principle. The theorem states that if a uniformly bounded sequence of functions converges pointwise, then their integrals on a set of finite measure converge to the integral of the limit function. If the convergence were uniform this would be a trivial result, and Littlewood's third principle tells us that the convergence is almost uniform, that is, uniform outside of a set of arbitrarily small measure. Because the sequence is bounded, the contribution to the integrals of the small set can be made arbitrarily small, and the integrals on the remainder converge because the functions are uniformly convergent there.\n"}
{"id": "26959564", "url": "https://en.wikipedia.org/wiki?curid=26959564", "title": "Luca Incurvati", "text": "Luca Incurvati\n\nLuca Incurvati is currently an Assistant Professor at the University of Amsterdam and formerly a lecturer at the University of Cambridge. He was awarded the Matthew Buncombe Prize for his masters thesis in 2005. He earned his PhD in philosophy at St John's College, Cambridge under the supervision of Michael Potter and Peter Smith. He has served as Director of Studies at Fitzwilliam, Gonville and Caius and Magdalene colleges at Cambridge.\n\n"}
{"id": "87027", "url": "https://en.wikipedia.org/wiki?curid=87027", "title": "Malleability (cryptography)", "text": "Malleability (cryptography)\n\nMalleability is a property of some cryptographic algorithms. An encryption algorithm is \"malleable\" if it is possible to transform a ciphertext into another ciphertext which decrypts to a related plaintext. That is, given an encryption of a plaintext formula_1, it is possible to generate another ciphertext which decrypts to formula_2, for a known function formula_3, without necessarily knowing or learning formula_1.\n\nMalleability is often an undesirable property in a general-purpose cryptosystem, since it allows an attacker to modify the contents of a message. For example, suppose that a bank uses a stream cipher to hide its financial information, and a user sends an encrypted message containing, say, \"TRANSFER $0000100.00 TO ACCOUNT #199.\" If an attacker can modify the message on the wire, and can guess the format of the unencrypted message, the attacker could be able to change the amount of the transaction, or the recipient of the funds, e.g. \"TRANSFER $0100000.00 TO ACCOUNT #227\". Malleability does not refer to the attacker's ability to read the encrypted message. Both before and after tampering, the attacker cannot read the encrypted message.\n\nOn the other hand, some cryptosystems are malleable by design. In other words, in some circumstances it may be viewed as a feature that anyone can transform an encryption of formula_1 into a valid encryption of formula_2 (for some restricted class of functions formula_3) without necessarily learning formula_1. Such schemes are known as homomorphic encryption schemes.\n\nA cryptosystem may be semantically secure against chosen plaintext attacks or even non-adaptive chosen ciphertext attacks (CCA1) while still being malleable. However, security against adaptive chosen ciphertext attacks (CCA2) is equivalent to non-malleability.\n\nIn a stream cipher, the ciphertext is produced by taking the exclusive or of the plaintext and a pseudorandom stream based on a secret key formula_9, as formula_10. An adversary can construct an encryption of formula_11 for any formula_12, as formula_13.\n\nIn the RSA cryptosystem, a plaintext formula_1 is encrypted as formula_15, where formula_16 is the public key. Given such a ciphertext, an adversary can construct an encryption of formula_17 for any formula_12, as formula_19. For this reason, RSA is commonly used together with padding methods such as OAEP or PKCS1.\n\nIn the ElGamal cryptosystem, a plaintext formula_1 is encrypted as formula_21, where formula_22 is the public key. Given such a ciphertext formula_23, an adversary can compute formula_24, which is a valid encryption of formula_25, for any formula_12.\nIn contrast, the Cramer-Shoup system (which is based on ElGamal) is not malleable.\n\nIn the Paillier, ElGamal, and RSA cryptosystems, it is also possible to combine \"several\" ciphertexts together in a useful way to produce a related ciphertext. In Paillier, given only the public key and an encryption of formula_27 and formula_28, one can compute a valid encryption of their sum formula_29. In ElGamal and in RSA, one can combine encryptions of formula_27 and formula_28 to obtain a valid encryption of their product formula_32.\n\nBlock ciphers in the cipher block chaining mode of operation, for example, are partly malleable: flipping a bit in a ciphertext block will completely mangle the plaintext it decrypts to, but will result in the same bit being flipped in the plaintext of the next block. This allows an attacker to 'sacrifice' one block of plaintext in order to change some data in the next one, possibly managing to maliciously alter the message. This is essentially the core idea of the padding oracle attack on CBC, which allows the attacker to decrypt almost an entire ciphertext without knowing the key. For this and many other reasons, using message authentication codes is needed to guard against any method of tampering.\n\nFischlin, in 2005, defined the notion of complete non-malleability as the ability of the system to remain non-malleable while giving the adversary additional power to choose a new public key which could be a function of the original public key. In other words, the adversary shouldn't be able to come up with a ciphertext whose underlying plaintext is related to the original message through a relation that also takes public keys into account.\n\n"}
{"id": "7409868", "url": "https://en.wikipedia.org/wiki?curid=7409868", "title": "Mary Celine Fasenmyer", "text": "Mary Celine Fasenmyer\n\nMary Celine Fasenmyer (October 4, 1906, Crown, Pennsylvania – December 27, 1996, Erie, Pennsylvania) was an American mathematician. She is most noted for her work on hypergeometric functions and linear algebra.\n\nFasenmyer grew up in Pennsylvania's oil country, and displayed mathematical talent in high school. For ten years after her graduation she taught and studied at Mercyhurst College in Erie, where she joined the Sisters of Mercy. The religious sister pursued her mathematical studies in Pittsburgh and the University of Michigan, obtaining her doctorate in 1946 under the direction of Earl Rainville, with a dissertation entitled \"Some Generalized Hypergeometric Polynomials\".\n\nAfter earning her Ph.D., Fasenmyer published two papers which expanded on her doctorate work. These would be further elaborated by Doron Zeilberger and Herbert Wilf into \"WZ theory\", which allowed computerized proof of many combinatorial identities. After this, she returned to Mercyhurst to teach and did not engage in further research.\n\nFasenmyer is most remembered for the method that bears her name, first described in her Ph.D. thesis concerning recurrence relations in hypergeometric series. The thesis demonstrated a purely algorithmic method to find recurrence relations satisfied by sums of terms of a hypergeometric polynomial, and requires only the series expansions of the polynomial. The beauty of her method is that it lends itself readily to computer automation. The work of Wilf and Zeilberger generalized the algorithm and established its correctness.\n\nThe hypergeometric polynomials she studied are called Sister Celine's polynomials.\n\n\n"}
{"id": "56069479", "url": "https://en.wikipedia.org/wiki?curid=56069479", "title": "Matroid parity problem", "text": "Matroid parity problem\n\nIn combinatorial optimization, the matroid parity problem is a problem of finding the largest independent set of paired elements in a matroid. The problem was formulated by as a common generalization of graph matching and matroid intersection. It is also known as polymatroid matching, or the matchoid problem.\n\nMatroid parity can be solved in polynomial time for linear matroids. However, it is NP-hard for certain compactly-represented matroids, and requires more than a polynomial number of steps in the matroid oracle model.\n\nApplications of matroid parity algorithms include finding large planar subgraphs and finding graph embeddings of maximum genus. These algorithms can also be used to find connected dominating sets and feedback vertex sets in graphs of maximum degree three.\n\nA matroid can be defined from a finite set of elements and from a notion of what it means for subsets of elements to be independent, subject to the following constraints:\n\nExamples of matroids include the linear matroids (in which the elements are vectors in a vector space, with linear independence), the graphic matroids (in which the elements are edges in an undirected graph, independent when they contain no cycle), and the partition matroids (in which the elements belong to a family of disjoint sets, and are independent when they contain at most one element in each set). Graphic matroids and partition matroids are special cases of linear matroids.\n\nIn the matroid parity problem, the input consists of a matroid together with a pairing on its elements, so that each element belongs to one pair. The goal is to find a subset of the pairs, as large as possible, so that the union of the pairs in the chosen subset is independent. Another seemingly more general variation, in which the allowable pairs form a graph rather than having only one pair per element, is equivalent: an element appearing in more than one pair could be replaced by multiple copies of the element, one per pair.\n\nThe matroid parity problem for linear matroids can be solved by a randomized algorithm in time formula_6, where formula_7 is the number of elements of the matroid, formula_8 is its rank (the size of the largest independent set), and formula_9 is the exponent in the time bounds for fast matrix multiplication.\nIn particular, using a matrix multiplication algorithm of Le Gall, it can be solved in time formula_10.\nWithout using fast matrix multiplication, the linear matroid parity problem can be solved in time formula_11.\n\nThese algorithms are based on a linear algebra formulation of the problem by . Suppose that an input to the problem consists of formula_12 pairs of formula_8-dimensional vectors (arranged as column vectors in a matrix formula_14 of size formula_15). Then the number of pairs in the optimal solution is\n\nwhere formula_2 is a block diagonal matrix whose blocks are formula_18 submatrices of the form\n\nfor a sequence of variables formula_20. The Schwartz–Zippel lemma can be used to test whether this matrix has full rank or not (that is, whether the solution has size formula_21 or not), by assigning random values to the variables formula_22 and testing whether the resulting matrix has determinant zero. By applying a greedy algorithm that removes pairs one at a time by setting their indeterminates to zero as long as the matrix remains of full rank (maintaining the inverse matrix using the Sherman–Morrison formula to check the rank after each removal), one can find a solution whenever this test shows that it exists. Additional methods extend this algorithm to the case that the optimal solution to the matroid parity problem has fewer than formula_21 pairs.\n\nFor graphic matroids, more efficient algorithms are known, with running time formula_24 on graphs with formula_12 vertices and formula_7 edges.\nFor simple graphs, formula_12 is formula_28, but for multigraphs, it may be larger, so it is also of interest to have algorithms with smaller or no dependence on formula_12 and worse dependence on formula_7. In these cases, it is also possible to solve the graphic matroid parity problem in randomized expected time formula_31, or in time formula_32 when each pair of edges forms a path.\n\nAlthough the matroid parity problem is NP-hard for arbitrary matroids, it can still be approximated efficiently. Simple local search algorithms provide a polynomial-time approximation scheme for this problem, and find solutions whose size, as a fraction of the optimal solution size, is arbitrarily close to one. The algorithm starts with the empty set as its solution, and repeatedly attempts to increase the solution size by one by removing at most a constant number formula_33 of pairs from the solution and replacing them by a different set with one more pair. When no more such moves are possible, the resulting solution is returned as the approximation to the optimal solution. To achieve an approximation ratio of formula_34, it suffices to choose formula_33 to be approximately formula_36.\n\nMany other optimization problems can be formulated as linear matroid parity problems, and solved in polynomial time using this formulation.\n\nThe clique problem, of finding a formula_37-vertex complete subgraph in a given formula_7-vertex graph formula_39, can be transformed into an instance of matroid parity as follows.\nConstruct a paving matroid on formula_40 elements, paired up so that there is one pair of elements per pair of vertices. Define a subset formula_1 of these elements to be independent if it satisfies any one of the following three conditions:\nThen there is a solution to the matroid parity problem for this matroid, of size formula_43, if and only if formula_39 has a clique of size formula_37. Since finding cliques of a given size is NP-complete, it follows that determining whether this type of matrix parity problem has a solution of size formula_43 is also NP-complete.\n\nThis problem transformation does not depend on the structure of the clique problem in any deep way, and would work for any other problem of finding size-formula_37 subsets of a larger set that satisfy a computable test. By applying it to a randomly-permuted graph that contains exactly one clique of size formula_37, one can show that any deterministic or randomized algorithm for matroid parity that accesses its matroid only by independence tests needs to make an exponential number of tests.\n"}
{"id": "4153112", "url": "https://en.wikipedia.org/wiki?curid=4153112", "title": "Micromagnetics", "text": "Micromagnetics\n\nMicromagnetics is a field of physics dealing with the prediction of magnetic behaviors at sub-micrometer length scales. The length scales considered are large enough for the atomic structure of the material to be ignored (the continuum approximation), yet small enough to resolve magnetic structures such as domain walls or vortices.\n\nMicromagnetics can deal with static equilibria, by minimizing the magnetic energy, and with dynamic behavior, by solving the time-dependent dynamical equation.\n\nMicromagnetics as a field (\"i.e.\", that deals specifically with the behaviour of (ferro)magnetic materials at sub-micrometer length scales) was introduced in 1963 when William Fuller Brown, Jr. published a paper on antiparallel domain wall structures. Until comparatively recently computational micromagnetics has been prohibitively expensive in terms of computational power, but smaller problems are now solvable on a modern desktop PC.\n\nThe purpose of static micromagnetics is to solve for the spatial distribution of the magnetization M at equilibrium. In most cases, as the temperature is much lower than the Curie temperature of the material considered, the modulus |M| of the magnetization is assumed to be everywhere equal to the saturation magnetization \"M\". The problem then consists in finding the spatial orientation of the magnetization, which is given by the \"magnetization direction vector\" m = M/\"M\", also called \"reduced magnetization\".\n\nThe static equilibria are found by minimizing the magnetic energy,\nsubject to the constraint |M|=\"M\" or |m|=1.\n\nThe contributions to this energy are the following:\n\nThe exchange energy is a phenomenological continuum description of the quantum-mechanical exchange interaction. It is written as:\n\nwhere \"A\" is the \"exchange constant\"; \"m\", \"m\" and \"m\" are the components of m;\nand the integral is performed over the volume of the sample.\n\nThe exchange energy tends to favor configurations where the magnetization varies only slowly across the sample. This energy is minimized when the magnetization is perfectly uniform.\n\nMagnetic anisotropy arises due to a combination of crystal structure and spin-orbit interaction. It can be generally written as:\n\nwhere \"F\", the anisotropy energy density, is a function of the orientation of the magnetization. Minimum-energy directions for \"F\" are called \"easy axes\".\n\nTime-reversal symmetry ensures that \"F\" is an even function of m. The simplest such function is\nwhere \"K\" is called the \"anisotropy constant\". In this approximation, called \"uniaxial anisotropy\", the easy axis is the \"z\" direction.\n\nThe anisotropy energy favors magnetic configurations where the magnetization is everywhere aligned along an easy axis.\n\nThe Zeeman energy is the interaction energy between the magnetization and any externally applied field. It's written as:\n\nwhere H is the applied field and µ is the vacuum permeability.\n\nThe Zeeman energy favors alignment of the magnetization parallel to the applied field.\n\nThe demagnetizing field is the magnetic field created by the magnetic sample upon itself. The associated energy is:\n\nwhere H is the demagnetizing field. This field depends on the magnetic configuration itself, and it can be found by solving:\n\nwhere −∇·M is sometimes called \"magnetic charge density\". The solution of these equations (c.f. magnetostatics) is:\n\nwhere r is the vector going from the current integration point to the point where H is being calculated.\n\nIt is worth noting that the magnetic charge density can be infinite at the edges of the sample, due to M changing discontinuously from a finite value inside to zero outside of the sample. This is usually dealt with by using suitable boundary conditions on the edge of the sample.\n\nThe energy of the demagnetizing field favors magnetic configurations that minimize magnetic charges. In particular, on the edges of the sample, the magnetization tends to run parallel to the surface. In most cases it is not possible to minimize this energy term at the same time as the others. The static equilibrium then is a compromise that minimizes the total magnetic energy, although it may not minimize individually any particular term.\n\nThe magnetoelastic energy describes the energy storage due to elastic lattice distortions. It may be neglected if magnetoelastic coupled effects are neglected.\nThere exists a preferred local distortion of the crystalline solid associated with the magnetization director m, . \nFor a simple model, one can assume this strain to be isochoric and fully\nisotropic in the lateral direction, yielding the deviatoric ansatz\n\nformula_10\n\nwhere the material parameter \"E > 0\" is the magnetostrictive\nconstant. Clearly, \"E\" is the strain induced by the magnetization in\nthe direction m. With this ansatz at hand, we consider the elastic\nenergy density to be a function of the elastic, stress-producing\nstrains formula_11. A quadratic form for the magnetoelastic energy is\n\nformula_12\n\nwhere formula_13\nis the fourth-order elasticity tensor. Here the elastic response is assumed to be isotropic (based on \nthe two Lamé constants λ and μ).\nTaking into account the constant length of m, we obtain the invariant-based representation\n\nformula_14\n\nThis energy term contributes to magnetostriction.\n\nThe purpose of dynamic micromagnetics is to predict the time evolution of the magnetic configuration of a sample subject to some non-steady conditions such as the application of a field pulse or an AC field. This is done by solving the Landau-Lifshitz-Gilbert equation, which is a partial differential equation describing the evolution of the magnetization in term of the local \"effective field\" acting on it.\n\nThe effective field is the local field \"felt\" by the magnetization. It can be described informally as the derivative of the magnetic energy density with respect to the orientation of the magnetization, as in:\n\nwhere d\"E\"/d\"V\" is the energy density. In variational terms, a change dm of the magnetization and the associated change d\"E\" of the magnetic energy are related by:\n\nIt should be noted that, since m is a unit vector, dm is always perpendicular to m. Then the above definition leaves unspecified the component of H that is parallel to m. This is usually not a problem, as this component has no effect on the magnetization dynamics.\n\nFrom the expression of the different contributions to the magnetic energy, the effective field can be found to be:\n\nThis is the equation of motion of the magnetization. It describes a Larmor precession of the magnetization around the effective field, with an additional damping term arising from the coupling of the magnetic system to the environment. The equation can be written in the so-called \"Gilbert form\" (or implicit form) as:\n\nwhere γ is the electron gyromagnetic ratio and α the Gilbert damping constant.\n\nIt can be shown that this is mathematically equivalent to the following \"Landau-Lifshitz\" (or explicit) form:\n\nThe interaction of micromagnetics with mechanics is also of interest in the context of industrial applications that deal with magneto-acoustic resonance such as in hypersound speakers, high frequency magnetostrictive transducers etc. \nFEM simulations taking into account the effect of magnetostriction into micromagnetics are of importance. Such simulations use models described above within a finite element framework.\n\nApart from conventional magnetic domains and domain-walls, the theory also treats the statics and dynamics of topological line and point configurations, e.g. magnetic vortex and antivortex states; or even 3d-Bloch points, where, for example, the magnetization leads radially into all directions from the origin, or into topologically equivalent configurations. Thus in space, and also in time, nano- (and even pico-)scales are used.\n\nThe corresponding topological quantum numbers are thought to be used as information carriers, to apply the most recent, and already studied, propositions in information technology.\n\n\n"}
{"id": "21068755", "url": "https://en.wikipedia.org/wiki?curid=21068755", "title": "Ménage problem", "text": "Ménage problem\n\nIn combinatorial mathematics, the ménage problem or problème des ménages asks for the number of different ways in which it is possible to seat a set of male-female couples at a dining table so that men and women alternate and nobody sits next to his or her partner. This problem was formulated in 1891 by Édouard Lucas and independently, a few years earlier, by Peter Guthrie Tait in connection with knot theory. For a number of couples equal to 3, 4, 5, ... the number of seating arrangements is\nMathematicians have developed formulas and recurrence equations for computing these numbers and related sequences of numbers. Along with their applications to etiquette and knot theory, these numbers also have a graph theoretic interpretation: they count the numbers of matchings and Hamiltonian cycles in certain families of graphs.\n\nLet \"M\" denote the number of seating arrangements for \"n\" couples. derived the formula\nMuch subsequent work has gone into alternative proofs for this formula and into generalized versions of the problem that count seating arrangements in which some couples are permitted to sit next to each other. A different formula for \"M\" involving Chebyshev polynomials was given by .\n\nUntil the work of , solutions to the ménage problem took the form of first finding all seating arrangements for the women and then counting, for each of these partial seating arrangements, the number of ways of completing it by seating the men away from their partners. However, as Bogart and Doyle showed, Touchard's formula may be derived directly by considering all seating arrangements at once rather than by factoring out the participation of the women.\n\nThere are 2×\"n\"! ways of seating the women: there are two sets of seats that can be arranged for the women, and there are \"n\"! ways of seating them at a particular set of seats. For each seating arrangement for the women, there are\nways of seating the men; this formula simply omits the 2×\"n\"! factor from Touchard's formula. The resulting smaller numbers (again, starting from \"n\" = 3),\nare called the ménage numbers. They satisfy the recurrence relation\nand the simpler four-term recurrence\nfrom which the ménage numbers themselves can easily be calculated.\n\nSolutions to the ménage problem may be interpreted in graph-theoretic terms, as directed Hamiltonian cycles in crown graphs. A crown graph is formed by removing a perfect matching from a complete bipartite graph \"K\"; it has 2\"n\" vertices of two colors, and each vertex of one color is connected to all but one of the vertices of the other color. In the case of the ménage problem, the vertices of the graph represent men and women, and the edges represent pairs of men and women who are allowed to sit next to each other. This graph is formed by removing the perfect matching formed by the male-female couples from a complete bipartite graph that connects every man to every woman. Any valid seating arrangement can be described by the sequence of people in order around the table, which forms a Hamiltonian cycle in the graph. However, two Hamiltonian cycles are considered to be equivalent if they connect the same vertices in the same cyclic order regardless of the starting vertex, while in the ménage problem the starting position is considered significant: if, as in Alice's tea party, all the guests shift their positions by one seat, it is considered a different seating arrangement even though it is described by the same cycle. Therefore, the number of oriented Hamiltonian cycles in a crown graph is smaller by a factor of 2\"n\" than the number of seating arrangements, but larger by a factor of (\"n\" − 1)! than the ménage numbers. The sequence of numbers of cycles in these graphs (as before, starting at \"n\" = 3) is\n\nA second graph-theoretic description of the problem is also possible. Once the women have been seated, the possible seating arrangements for the remaining men can be described as perfect matchings in a graph formed by removing a single Hamiltonian cycle from a complete bipartite graph; the graph has edges connecting open seats to men, and the removal of the cycle corresponds to forbidding the men to sit in either of the open seats adjacent to their wives. The problem of counting matchings in a bipartite graph, and therefore \"a fortiori\" the problem of computing ménage numbers, can be solved using the permanents of certain 0-1 matrices. In the case of the ménage problem, the matrices arising from this view of the problem are circulant matrices.\n\nTait's motivation for studying the ménage problem came from trying to find a complete listing of mathematical knots with a given number of crossings, say \"n\". In Dowker notation for knot diagrams, an early form of which was used by Tait, the 2\"n\" points where a knot crosses itself, in consecutive order along the knot, are labeled with the 2\"n\" numbers from 1 to 2\"n\". In a reduced diagram, the two labels at a crossing cannot be consecutive, so the set of pairs of labels at each crossing, used in Dowker notation to represent the knot, can be interpreted as a perfect matching in a graph that has a vertex for every number in the range from 1 to 2\"n\" and an edge between every pair of numbers that has different parity and are non-consecutive modulo 2\"n\". This graph is formed by removing a Hamiltonian cycle (connecting consecutive numbers) from a complete bipartite graph (connecting all pairs of numbers with different parity), and so it has a number of matchings equal to a ménage number. For alternating knots, this matching is enough to describe the knot diagram itself; for other knots, an additional positive or negative sign needs to be specified for each crossing pair to determine which of the two strands of the crossing lies above the other strand.\n\nHowever, the knot listing problem has some additional symmetries not present in the ménage problem: one obtains different Dowker notations for the same knot diagram if one begins the labeling at a different crossing point, and these different notations should all be counted as representing the same diagram. For this reason, two matchings that differ from each other by a cyclic permutation should be treated as equivalent and counted only once. solved this modified enumeration problem, showing that the number of different matchings is\n\n\n"}
{"id": "17193739", "url": "https://en.wikipedia.org/wiki?curid=17193739", "title": "Noisy text", "text": "Noisy text\n\nNoisy text is text with differences between the surface form of a coded representation of the text and the intended, correct, or original text. The noise may be due to typographic errors or colloquialisms always present in natural language and usually lowers the data quality in a way that makes the text less accessible to automated processing by computers, including natural language processing. The noise may also have been introduced through an extraction process (e.g., transcription or OCR) from media other than original electronic texts. \n\nLanguage usage over computer mediated discourses, like chats, emails and SMS texts, significantly differs from the standard form of the language. An urge towards shorter message length facilitating faster typing and the need for semantic clarity, shape the structure of this text used in such discourses.\n\nVarious business analysts estimate that unstructured data constitutes around 80% of the whole enterprise data. A great proportion of this data comprises chat transcripts, emails and other informal and semi-formal internal and external communications. Usually such text is meant for human consumption, but – given the amount of data – manual processing and evaluation of those resources is not practically feasible anymore. This raises the need for robust text mining methods.\n\nThe use of spell checkers and grammar checkers can reduce the amount of noise in typed text. Many word processors include this in the editing tool. Online, Google search includes a search term suggestion engine to guide users when they make mistakes with their queries.\n\n"}
{"id": "1505381", "url": "https://en.wikipedia.org/wiki?curid=1505381", "title": "Numerical weather prediction", "text": "Numerical weather prediction\n\nNumerical weather prediction (NWP) uses mathematical models of the atmosphere and oceans to predict the weather based on current weather conditions. Though first attempted in the 1920s, it was not until the advent of computer simulation in the 1950s that numerical weather predictions produced realistic results. A number of global and regional forecast models are run in different countries worldwide, using current weather observations relayed from radiosondes, weather satellites and other observing systems as inputs.\n\nMathematical models based on the same physical principles can be used to generate either short-term weather forecasts or longer-term climate predictions; the latter are widely applied for understanding and projecting climate change. The improvements made to regional models have allowed for significant improvements in tropical cyclone track and air quality forecasts; however, atmospheric models perform poorly at handling processes that occur in a relatively constricted area, such as wildfires.\n\nManipulating the vast datasets and performing the complex calculations necessary to modern numerical weather prediction requires some of the most powerful supercomputers in the world. Even with the increasing power of supercomputers, the forecast skill of numerical weather models extends to only about six days. Factors affecting the accuracy of numerical predictions include the density and quality of observations used as input to the forecasts, along with deficiencies in the numerical models themselves. Post-processing techniques such as model output statistics (MOS) have been developed to improve the handling of errors in numerical predictions.\n\nA more fundamental problem lies in the chaotic nature of the partial differential equations that govern the atmosphere. It is impossible to solve these equations exactly, and small errors grow with time (doubling about every five days). Present understanding is that this chaotic behavior limits accurate forecasts to about 14 days even with perfectly accurate input data and a flawless model. In addition, the partial differential equations used in the model need to be supplemented with parameterizations for solar radiation, moist processes (clouds and precipitation), heat exchange, soil, vegetation, surface water, and the effects of terrain. In an effort to quantify the large amount of inherent uncertainty remaining in numerical predictions, ensemble forecasts have been used since the 1990s to help gauge the confidence in the forecast, and to obtain useful results farther into the future than otherwise possible. This approach analyzes multiple forecasts created with an individual forecast model or multiple models.\n\nThe history of numerical weather prediction began in the 1920s through the efforts of Lewis Fry Richardson, who used procedures originally developed by Vilhelm Bjerknes to produce by hand a six-hour forecast for the state of the atmosphere over two points in central Europe, taking at least six weeks to do so. It was not until the advent of the computer and computer simulations that computation time was reduced to less than the forecast period itself. The ENIAC was used to create the first weather forecasts via computer in 1950, based on a highly simplified approximation to the atmospheric governing equations. In 1954, Carl-Gustav Rossby's group at the Swedish Meteorological and Hydrological Institute used the same model to produce the first operational forecast (i.e., a routine prediction for practical use). Operational numerical weather prediction in the United States began in 1955 under the Joint Numerical Weather Prediction Unit (JNWPU), a joint project by the U.S. Air Force, Navy and Weather Bureau. In 1956, Norman Phillips developed a mathematical model which could realistically depict monthly and seasonal patterns in the troposphere; this became the first successful climate model. Following Phillips' work, several groups began working to create general circulation models. The first general circulation climate model that combined both oceanic and atmospheric processes was developed in the late 1960s at the NOAA Geophysical Fluid Dynamics Laboratory.\n\nAs computers have become more powerful, the size of the initial data sets has increased and newer atmospheric models have been developed to take advantage of the added available computing power. These newer models include more physical processes in the simplifications of the equations of motion in numerical simulations of the atmosphere. In 1966, West Germany and the United States began producing operational forecasts based on primitive-equation models, followed by the United Kingdom in 1972 and Australia in 1977. The development of limited area (regional) models facilitated advances in forecasting the tracks of tropical cyclones as well as air quality in the 1970s and 1980s. By the early 1980s models began to include the interactions of soil and vegetation with the atmosphere, which led to more realistic forecasts.\n\nThe output of forecast models based on atmospheric dynamics is unable to resolve some details of the weather near the Earth's surface. As such, a statistical relationship between the output of a numerical weather model and the ensuing conditions at the ground was developed in the 1970s and 1980s, known as model output statistics (MOS). Starting in the 1990s, model ensemble forecasts have been used to help define the forecast uncertainty and to extend the window in which numerical weather forecasting is viable farther into the future than otherwise possible.\n\nThe atmosphere is a fluid. As such, the idea of numerical weather prediction is to sample the state of the fluid at a given time and use the equations of fluid dynamics and thermodynamics to estimate the state of the fluid at some time in the future. The process of entering observation data into the model to generate initial conditions is called \"initialization\". On land, terrain maps available at resolutions down to globally are used to help model atmospheric circulations within regions of rugged topography, in order to better depict features such as downslope winds, mountain waves and related cloudiness that affects incoming solar radiation. The main inputs from country-based weather services are observations from devices (called radiosondes) in weather balloons that measure various atmospheric parameters and transmits them to a fixed receiver, as well as from weather satellites. The World Meteorological Organization acts to standardize the instrumentation, observing practices and timing of these observations worldwide. Stations either report hourly in METAR reports, or every six hours in SYNOP reports. These observations are irregularly spaced, so they are processed by data assimilation and objective analysis methods, which perform quality control and obtain values at locations usable by the model's mathematical algorithms. The data are then used in the model as the starting point for a forecast.\nA variety of methods are used to gather observational data for use in numerical models. Sites launch radiosondes in weather balloons which rise through the troposphere and well into the stratosphere. Information from weather satellites is used where traditional data sources are not available. Commerce provides pilot reports along aircraft routes and ship reports along shipping routes. Research projects use reconnaissance aircraft to fly in and around weather systems of interest, such as tropical cyclones. Reconnaissance aircraft are also flown over the open oceans during the cold season into systems which cause significant uncertainty in forecast guidance, or are expected to be of high impact from three to seven days into the future over the downstream continent. Sea ice began to be initialized in forecast models in 1971. Efforts to involve sea surface temperature in model initialization began in 1972 due to its role in modulating weather in higher latitudes of the Pacific.\n\nAn atmospheric model is a computer program that produces meteorological information for future times at given locations and altitudes. Within any modern model is a set of equations, known as the primitive equations, used to predict the future state of the atmosphere. These equations—along with the ideal gas law—are used to evolve the density, pressure, and potential temperature scalar fields and the air velocity (wind) vector field of the atmosphere through time. Additional transport equations for pollutants and other aerosols are included in some primitive-equation high-resolution models as well. The equations used are nonlinear partial differential equations which are impossible to solve exactly through analytical methods, with the exception of a few idealized cases. Therefore, numerical methods obtain approximate solutions. Different models use different solution methods: some global models and almost all regional models use finite difference methods for all three spatial dimensions, while other global models and a few regional models use spectral methods for the horizontal dimensions and finite-difference methods in the vertical.\n\nThese equations are initialized from the analysis data and rates of change are determined. These rates of change predict the state of the atmosphere a short time into the future; the time increment for this prediction is called a \"time step\". This future atmospheric state is then used as the starting point for another application of the predictive equations to find new rates of change, and these new rates of change predict the atmosphere at a yet further time step into the future. This time stepping is repeated until the solution reaches the desired forecast time. The length of the time step chosen within the model is related to the distance between the points on the computational grid, and is chosen to maintain numerical stability. Time steps for global models are on the order of tens of minutes, while time steps for regional models are between one and four minutes. The global models are run at varying times into the future. The UKMET Unified Model is run six days into the future, while the European Centre for Medium-Range Weather Forecasts' Integrated Forecast System and Environment Canada's Global Environmental Multiscale Model both run out to ten days into the future, and the Global Forecast System model run by the Environmental Modeling Center is run sixteen days into the future. The visual output produced by a model solution is known as a prognostic chart, or \"prog\".\n\nSome meteorological processes are too small-scale or too complex to be explicitly included in numerical weather prediction models. \"Parameterization\" is a procedure for representing these processes by relating them to variables on the scales that the model resolves. For example, the gridboxes in weather and climate models have sides that are between and in length. A typical cumulus cloud has a scale of less than , and would require a grid even finer than this to be represented physically by the equations of fluid motion. Therefore, the processes that such clouds represent are parameterized, by processes of various sophistication. In the earliest models, if a column of air within a model gridbox was conditionally unstable (essentially, the bottom was warmer and moister than the top) and the water vapor content at any point within the column became saturated then it would be overturned (the warm, moist air would begin rising), and the air in that vertical column mixed. More sophisticated schemes recognize that only some portions of the box might convect and that entrainment and other processes occur. Weather models that have gridboxes with sizes between can explicitly represent convective clouds, although they need to parameterize cloud microphysics which occur at a smaller scale. The formation of large-scale (stratus-type) clouds is more physically based; they form when the relative humidity reaches some prescribed value. Sub-grid scale processes need to be taken into account. Rather than assuming that clouds form at 100% relative humidity, the cloud fraction can be related to a critical value of relative humidity less than 100%, reflecting the sub grid scale variation that occurs in the real world.\n\nThe amount of solar radiation reaching the ground, as well as the formation of cloud droplets occur on the molecular scale, and so they must be parameterized before they can be included in the model. Atmospheric drag produced by mountains must also be parameterized, as the limitations in the resolution of elevation contours produce significant underestimates of the drag. This method of parameterization is also done for the surface flux of energy between the ocean and the atmosphere, in order to determine realistic sea surface temperatures and type of sea ice found near the ocean's surface. Sun angle as well as the impact of multiple cloud layers is taken into account. Soil type, vegetation type, and soil moisture all determine how much radiation goes into warming and how much moisture is drawn up into the adjacent atmosphere, and thus it is important to parameterize their contribution to these processes. Within air quality models, parameterizations take into account atmospheric emissions from multiple relatively tiny sources (e.g. roads, fields, factories) within specific grid boxes.\n\nThe horizontal domain of a model is either \"global\", covering the entire Earth, or \"regional\", covering only part of the Earth. Regional models (also known as \"limited-area\" models, or LAMs) allow for the use of finer grid spacing than global models because the available computational resources are focused on a specific area instead of being spread over the globe. This allows regional models to resolve explicitly smaller-scale meteorological phenomena that cannot be represented on the coarser grid of a global model. Regional models use a global model to specify conditions at the edge of their domain (boundary conditions) in order to allow systems from outside the regional model domain to move into its area. Uncertainty and errors within regional models are introduced by the global model used for the boundary conditions of the edge of the regional model, as well as errors attributable to the regional model itself.\n\nHorizontal position may be expressed directly in geographic coordinates (latitude and longitude) for global models or in a map projection planar coordinates for regional models. The german weather service is using for its global ICON model (icosahedral non-hydrostatic global circulation model) a grid based on an regular Icosahedron. Basic cells in this grid are triangles instead of the four corner cells in a traditional latitude-longitude grid.\nThe advantage is that, different from a latitude-longitude cells are everywhere on the globe the same size. Disadvantage is that equations in this non rectangular grid are more complicated.\n\nThe vertical coordinate is handled in various ways. Lewis Fry Richardson's 1922 model used geometric height (formula_1) as the vertical coordinate. Later models substituted the geometric formula_1 coordinate with a pressure coordinate system, in which the geopotential heights of constant-pressure surfaces become dependent variables, greatly simplifying the primitive equations. This correlation between coordinate systems can be made since pressure decreases with height through the Earth's atmosphere. The first model used for operational forecasts, the single-layer barotropic model, used a single pressure coordinate at the 500-millibar (about ) level, and thus was essentially two-dimensional. High-resolution models—also called \"mesoscale models\"—such as the Weather Research and Forecasting model tend to use normalized pressure coordinates referred to as sigma coordinates. This coordinate system receives its name from the independent variable formula_3 used to scale atmospheric pressures with respect to the pressure at the surface, and in some cases also with the pressure at the top of the domain.\n\nBecause forecast models based upon the equations for atmospheric dynamics do not perfectly determine weather conditions, statistical methods have been developed to attempt to correct the forecasts. Statistical models were created based upon the three-dimensional fields produced by numerical weather models, surface observations and the climatological conditions for specific locations. These statistical models are collectively referred to as model output statistics (MOS), and were developed by the National Weather Service for their suite of weather forecasting models in the late 1960s.\n\nModel output statistics differ from the \"perfect prog\" technique, which assumes that the output of numerical weather prediction guidance is perfect. MOS can correct for local effects that cannot be resolved by the model due to insufficient grid resolution, as well as model biases. Because MOS is run after its respective global or regional model, its production is known as post-processing. Forecast parameters within MOS include maximum and minimum temperatures, percentage chance of rain within a several hour period, precipitation amount expected, chance that the precipitation will be frozen in nature, chance for thunderstorms, cloudiness, and surface winds.\n\nIn 1963, Edward Lorenz discovered the chaotic nature of the fluid dynamics equations involved in weather forecasting. Extremely small errors in temperature, winds, or other initial inputs given to numerical models will amplify and double every five days, making it impossible for long-range forecasts—those made more than two weeks in advance—to predict the state of the atmosphere with any degree of forecast skill. Furthermore, existing observation networks have poor coverage in some regions (for example, over large bodies of water such as the Pacific Ocean), which introduces uncertainty into the true initial state of the atmosphere. While a set of equations, known as the Liouville equations, exists to determine the initial uncertainty in the model initialization, the equations are too complex to run in real-time, even with the use of supercomputers. These uncertainties limit forecast model accuracy to about five or six days into the future.\n\nEdward Epstein recognized in 1969 that the atmosphere could not be completely described with a single forecast run due to inherent uncertainty, and proposed using an ensemble of stochastic Monte Carlo simulations to produce means and variances for the state of the atmosphere. Although this early example of an ensemble showed skill, in 1974 Cecil Leith showed that they produced adequate forecasts only when the ensemble probability distribution was a representative sample of the probability distribution in the atmosphere.\n\nSince the 1990s, \"ensemble forecasts\" have been used operationally (as routine forecasts) to account for the stochastic nature of weather processes – that is, to resolve their inherent uncertainty. This method involves analyzing multiple forecasts created with an individual forecast model by using different physical parametrizations or varying initial conditions. Starting in 1992 with ensemble forecasts prepared by the European Centre for Medium-Range Weather Forecasts (ECMWF) and the National Centers for Environmental Prediction, model ensemble forecasts have been used to help define the forecast uncertainty and to extend the window in which numerical weather forecasting is viable farther into the future than otherwise possible. The ECMWF model, the Ensemble Prediction System, uses singular vectors to simulate the initial probability density, while the NCEP ensemble, the Global Ensemble Forecasting System, uses a technique known as vector breeding. The UK Met Office runs global and regional ensemble forecasts where perturbations to initial conditions are produced using a Kalman filter. There are 24 ensemble members in the Met Office Global and Regional Ensemble Prediction System (MOGREPS).\n\nIn a single model-based approach, the ensemble forecast is usually evaluated in terms of an average of the individual forecasts concerning one forecast variable, as well as the degree of agreement between various forecasts within the ensemble system, as represented by their overall spread. Ensemble spread is diagnosed through tools such as spaghetti diagrams, which show the dispersion of one quantity on prognostic charts for specific time steps in the future. Another tool where ensemble spread is used is a meteogram, which shows the dispersion in the forecast of one quantity for one specific location. It is common for the ensemble spread to be too small to include the weather that actually occurs, which can lead to forecasters misdiagnosing model uncertainty; this problem becomes particularly severe for forecasts of the weather about ten days in advance. When ensemble spread is small and the forecast solutions are consistent within multiple model runs, forecasters perceive more confidence in the ensemble mean, and the forecast in general. Despite this perception, a \"spread-skill relationship\" is often weak or not found, as spread-error correlations are normally less than 0.6, and only under special circumstances range between 0.6–0.7. The relationship between ensemble spread and forecast skill varies substantially depending on such factors as the forecast model and the region for which the forecast is made.\n\nIn the same way that many forecasts from a single model can be used to form an ensemble, multiple models may also be combined to produce an ensemble forecast. This approach is called \"multi-model ensemble forecasting\", and it has been shown to improve forecasts when compared to a single model-based approach. Models within a multi-model ensemble can be adjusted for their various biases, which is a process known as \"superensemble forecasting\". This type of forecast significantly reduces errors in model output.\n\nAir quality forecasting attempts to predict when the concentrations of pollutants will attain levels that are hazardous to public health. The concentration of pollutants in the atmosphere is determined by their \"transport\", or mean velocity of movement through the atmosphere, their diffusion, chemical transformation, and ground deposition. In addition to pollutant source and terrain information, these models require data about the state of the fluid flow in the atmosphere to determine its transport and diffusion. Meteorological conditions such as thermal inversions can prevent surface air from rising, trapping pollutants near the surface, which makes accurate forecasts of such events crucial for air quality modeling. Urban air quality models require a very fine computational mesh, requiring the use of high-resolution mesoscale weather models; in spite of this, the quality of numerical weather guidance is the main uncertainty in air quality forecasts.\n\nA General Circulation Model (GCM) is a mathematical model that can be used in computer simulations of the global circulation of a planetary atmosphere or ocean. An atmospheric general circulation model (AGCM) is essentially the same as a global numerical weather prediction model, and some (such as the one used in the UK Unified Model) can be configured for both short-term weather forecasts and longer-term climate predictions. Along with sea ice and land-surface components, AGCMs and oceanic GCMs (OGCM) are key components of global climate models, and are widely applied for understanding the climate and projecting climate change. For aspects of climate change, a range of man-made chemical emission scenarios can be fed into the climate models to see how an enhanced greenhouse effect would modify the Earth's climate. Versions designed for climate applications with time scales of decades to centuries were originally created in 1969 by Syukuro Manabe and Kirk Bryan at the Geophysical Fluid Dynamics Laboratory in Princeton, New Jersey. When run for multiple decades, computational limitations mean that the models must use a coarse grid that leaves smaller-scale interactions unresolved.\n\nThe transfer of energy between the wind blowing over the surface of an ocean and the ocean's upper layer is an important element in wave dynamics. The spectral wave transport equation is used to describe the change in wave spectrum over changing topography. It simulates wave generation, wave movement (propagation within a fluid), wave shoaling, refraction, energy transfer between waves, and wave dissipation. Since surface winds are the primary forcing mechanism in the spectral wave transport equation, ocean wave models use information produced by numerical weather prediction models as inputs to determine how much energy is transferred from the atmosphere into the layer at the surface of the ocean. Along with dissipation of energy through whitecaps and resonance between waves, surface winds from numerical weather models allow for more accurate predictions of the state of the sea surface.\n\nTropical cyclone forecasting also relies on data provided by numerical weather models. Three main classes of tropical cyclone guidance models exist: Statistical models are based on an analysis of storm behavior using climatology, and correlate a storm's position and date to produce a forecast that is not based on the physics of the atmosphere at the time. Dynamical models are numerical models that solve the governing equations of fluid flow in the atmosphere; they are based on the same principles as other limited-area numerical weather prediction models but may include special computational techniques such as refined spatial domains that move along with the cyclone. Models that use elements of both approaches are called statistical-dynamical models.\n\nIn 1978, the first hurricane-tracking model based on atmospheric dynamics—the movable fine-mesh (MFM) model—began operating. Within the field of tropical cyclone track forecasting, despite the ever-improving dynamical model guidance which occurred with increased computational power, it was not until the 1980s when numerical weather prediction showed skill, and until the 1990s when it consistently outperformed statistical or simple dynamical models. Predictions of the intensity of a tropical cyclone based on numerical weather prediction continue to be a challenge, since statistical methods continue to show higher skill over dynamical guidance.\n\nOn a molecular scale, there are two main competing reaction processes involved in the degradation of cellulose, or wood fuels, in wildfires. When there is a low amount of moisture in a cellulose fiber, volatilization of the fuel occurs; this process will generate intermediate gaseous products that will ultimately be the source of combustion. When moisture is present—or when enough heat is being carried away from the fiber, charring occurs. The chemical kinetics of both reactions indicate that there is a point at which the level of moisture is low enough—and/or heating rates high enough—for combustion processes become self-sufficient. Consequently, changes in wind speed, direction, moisture, temperature, or lapse rate at different levels of the atmosphere can have a significant impact on the behavior and growth of a wildfire. Since the wildfire acts as a heat source to the atmospheric flow, the wildfire can modify local advection patterns, introducing a feedback loop between the fire and the atmosphere.\n\nA simplified two-dimensional model for the spread of wildfires that used convection to represent the effects of wind and terrain, as well as radiative heat transfer as the dominant method of heat transport led to reaction-diffusion systems of partial differential equations. More complex models join numerical weather models or computational fluid dynamics models with a wildfire component which allow the feedback effects between the fire and the atmosphere to be estimated. The additional complexity in the latter class of models translates to a corresponding increase in their computer power requirements. In fact, a full three-dimensional treatment of combustion via direct numerical simulation at scales relevant for atmospheric modeling is not currently practical because of the excessive computational cost such a simulation would require. Numerical weather models have limited forecast skill at spatial resolutions under , forcing complex wildfire models to parameterize the fire in order to calculate how the winds will be modified locally by the wildfire, and to use those modified winds to determine the rate at which the fire will spread locally. Although models such as Los Alamos' FIRETEC solve for the concentrations of fuel and oxygen, the computational grid cannot be fine enough to resolve the combustion reaction, so approximations must be made for the temperature distribution within each grid cell, as well as for the combustion reaction rates themselves.\n\n\n\n"}
{"id": "47943370", "url": "https://en.wikipedia.org/wiki?curid=47943370", "title": "Oscar H. Ibarra", "text": "Oscar H. Ibarra\n\nOscar H. Ibarra (born September 29, 1941 in Negros Occidental, Philippines) is a Filipino-American theoretical computer scientist, prominent for work in automata theory, formal languages, design and analysis of algorithms and computational complexity theory. He was a Professor of the Department of Computer Science at the University of California-Santa Barbara until his retirement in 2011. Previously, he was on the faculties of UC Berkeley (1967-1969) and the University of Minnesota (1969-1990). As of 2015, Ibarra was Professor Emeritus and Research Professor at UCSB.\n\nIbarra received a BS degree in Electrical Engineering from the University of the Philippines and MS and PhD degrees, also in Electrical Engineering, from the University of California, Berkeley in 1965 and 1967, respectively.\n\nIbarra was awarded a John Simon Guggenheim Memorial Foundation Fellowship in 1984. In 1993, he was elected a Fellow of the American Association for the Advancement of Science. He is a Fellow of the Institute of Electrical and Electronics Engineers and the Association for Computing Machinery. In 2001, he received the IEEE Computer Society's Harry H. Goode Memorial Award. He was elected member of the European Academy of Sciences (EAS) in 2003. He was awarded the Blaise Pascal Medal in Computer Science from EAS in 2007, and in 2008 he was elected a Foreign Member of Academia Europaea in the Informatics Section. In 2008, he was awarded a Distinguished Visiting Fellowship from the UK Royal Academy of Engineering. In July 2015, during the 40th anniversary celebration of the journal, Theoretical Computer Science, Ibarra was named the most prolific author in its 40-year history. He was listed in the Institute for Scientific Information (ISI) database of Highly Cited Researchers in Computer Science in 2003 and in the Computer Science Bibliography DBLP.\n\n"}
{"id": "15901488", "url": "https://en.wikipedia.org/wiki?curid=15901488", "title": "Prescribed scalar curvature problem", "text": "Prescribed scalar curvature problem\n\nIn Riemannian geometry, a branch of mathematics, the prescribed scalar curvature problem is as follows: given a closed, smooth manifold \"M\" and a smooth, real-valued function \"ƒ\" on \"M\", construct a Riemannian metric on \"M\" whose scalar curvature equals \"ƒ\". Due primarily to the work of J. Kazdan and F. Warner in the 1970s, this problem is well understood.\n\nIf the dimension of \"M\" is three or greater, then any smooth function \"ƒ\" which takes on a negative value somewhere is the scalar curvature of some Riemannian metric. The assumption that \"ƒ\" be negative somewhere is needed in general, since not all manifolds admit metrics which have strictly positive scalar curvature. (For example, the three-dimensional torus is such a manifold.) However, Kazdan and Warner proved that if \"M\" does admit some metric with strictly positive scalar curvature, then any smooth function \"ƒ\" is the scalar curvature of some Riemannian metric.\n\n\n"}
{"id": "6527939", "url": "https://en.wikipedia.org/wiki?curid=6527939", "title": "Proof of knowledge", "text": "Proof of knowledge\n\nIn cryptography, a proof of knowledge is an interactive proof in which the prover succeeds in 'convincing' a verifier that the prover knows something. What it means for a machine to 'know something' is defined in terms of computation. A machine 'knows something', if this something can be computed, given the machine as an input. As the program of the prover does not necessarily spit out the knowledge itself (as is the case for zero-knowledge proofs) a machine with a different program, called the knowledge extractor is introduced to capture this idea. We are mostly interested in what can be proven by polynomial time bounded machines. In this case the set of knowledge elements is limited to a set of witnesses of some language in NP.\n\nLet formula_1 be a statement of language formula_2 in NP, and formula_3 the set of witnesses for x that should be accepted in the proof. This allows us to define the following relation: formula_4.\n\nA proof of knowledge for relation formula_5 with knowledge error formula_6 is a two\nparty protocol with a prover formula_7 and a verifier formula_8 with the following two properties:\n\n\nThis is a more rigorous definition of Validity:\n\nLet formula_5 be a witness relation, formula_3 the set of all witnesses for public value formula_1, and formula_6 the knowledge error.\nA proof of knowledge is formula_6-valid if there exists a polynomial-time machine formula_14, given oracle access to formula_15, such that for every formula_15, it is the case that formula_25 and formula_26\n\nThe result formula_27 signifies that the Turing machine formula_14 did not come to a conclusion.\n\nThe knowledge error formula_29 denotes the probability that the verifier formula_8 might accept formula_1, even though the prover does in fact not know a witness formula_10. The knowledge extractor formula_14 is used to express what is meant by the knowledge of a Turing machine. If formula_14 can extract formula_10 from formula_15, we say that formula_15 knows the value of formula_10.\n\nThis definition of the validity property is a combination of the validity and strong validity properties in. For small knowledge errors formula_29, such as e.g. formula_40 or formula_41 it can be seen as being stronger than the soundness of ordinary interactive proofs.\n\nIn order to define a specific proof of knowledge, one need not only define the language, but also the witnesses the verifier should know. In some cases proving membership in a language may be easy, while computing a specific witness may be hard. This is best explained using an example:\n\nLet formula_42 be a cyclic group with generator formula_43 in which solving the discrete logarithm problem is believed to be hard. Deciding membership of the language formula_44 is trivial, as every formula_1 is in formula_42. However, finding the witness formula_10 such that formula_48 holds corresponds to solving the discrete logarithm problem.\n\nOne of the simplest and frequently used proofs of knowledge, the \"proof of knowledge of a discrete logarithm\", is due to Schnorr. The protocol is defined for a cyclic group formula_49 of order formula_50 with generator formula_43.\n\nIn order to prove knowledge of formula_52, the prover interacts with the verifier as follows:\n\n\nThe verifier accepts, if formula_58.\n\nProtocols which have the above three-move structure (commitment, challenge and response) are called \"sigma protocols\". The Greek letter formula_59 visualizes the flow of the protocol. Sigma protocols exist for proving various statements, such as those pertaining to discrete logarithms. Using these proofs, the prover can not only prove the knowledge of the discrete logarithm, but also that the discrete logarithm is of a specific form. For instance, it is possible to prove that two logarithms of formula_60 and formula_61 with respect to bases formula_62 and formula_63 are equal or fulfill some other linear relation. For \"a\" and \"b\" elements of formula_64, we say that the prover proves knowledge of formula_65 and formula_66 such that formula_67 and formula_68. Equality corresponds to the special case where \"a\" = 1 and \"b\" = 0. As formula_66 can be trivially computed from formula_65 this is equivalent to proving knowledge of an \"x\" such that formula_71.\n\nThis is the intuition behind the following notation, which is commonly used to express what exactly is proven by a proof of knowledge.\n\nstates that the prover knows an \"x\" that fulfills the relation above.\n\nProofs of knowledge are useful tool for the construction of identification protocols, and in their non-interactive variant, signature schemes. Such schemes are:\n\n\nThey are also used in the construction of group signature and anonymous digital credential systems.\n\n\n"}
{"id": "41593", "url": "https://en.wikipedia.org/wiki?curid=41593", "title": "Pseudorandom noise", "text": "Pseudorandom noise\n\nIn cryptography, pseudorandom noise (PRN ) is a signal similar to noise which satisfies one or more of the standard tests for statistical randomness. Although it seems to lack any definite pattern, pseudorandom noise consists of a deterministic sequence of pulses that will repeat itself after its period. \n\nIn cryptographic devices, the pseudorandom noise pattern is determined by a key and the repetition period can be very long, even millions of digits.\n\nPseudorandom noise is used in some electronic musical instruments, either by itself or as an input to subtractive synthesis, and in many white noise machines.\n\nIn spread-spectrum systems, the receiver correlates a locally generated signal with the received signal. Such spread-spectrum systems require a set of one or more \"codes\" or \"sequences\" such that\n\nIn a direct-sequence spread spectrum system, each bit in the pseudorandom binary sequence is known as a \"chip\" and the \"inverse\" of its period as \"chip rate\"; \"compare bit rate and symbol rate.\"\n\nIn a frequency-hopping spread spectrum sequence, each value in the pseudorandom sequence is known as a \"channel number\" and the \"inverse\" of its period as the \"hop rate\". FCC Part 15 mandates at least 50 different channels and at least a 2.5 Hz hop rate for narrow band frequency-hopping systems.\n\nGPS satellites broadcast data at a rate of 50 data bits per second – each satellite modulates its data with one PN bit stream at 1.023 million chips per second and the same data with another PN bit stream at 10.23 million chips per second.\nGPS receivers correlate the received PN bit stream with a local reference to measure distance. GPS is a receive-only system that uses relative timing measurements from several satellites (and the known positions of the satellites) to determine receiver position.\n\nOther range-finding applications involve two-way transmissions. A local station generates a pseudorandom bit sequence and transmits it to the remote location (using any modulation technique). Some object at the remote location echoes this PN signal back to the location station – either passively, as in some kinds of radar and sonar systems, or using an active transponder at the remote location, as in the Apollo Unified S-band system. By correlating a (delayed version of) the transmitted signal with the received signal, a precise round trip time to the remote location can be determined and thus the distance.\n\nA pseudo-noise code (PN code) or pseudo-random-noise code (PRN code) is one that has a spectrum similar to a random sequence of bits but is deterministically generated. The most commonly used sequences in direct-sequence spread spectrum systems are maximal length sequences, Gold codes, Kasami codes, and Barker codes.\n\n"}
{"id": "54511709", "url": "https://en.wikipedia.org/wiki?curid=54511709", "title": "Quantum knots", "text": "Quantum knots\n\nQuantum knots is a branch of quantum mechanics that connects quantum computing with Knot theory.\n\nWhile resisting the electron with Lorentz force it gets divided into three particles called quasiparticle along with an amount of energy from the electron. These quasiparticles tightly coupled by a knot like structure similar to the needle with a thread. If an electron from other orbit interact with this knot make other knot, by applying the knot position, the quantum states can be easily determined and it will solve the quantum superposition problems in quantum computing\n\n"}
{"id": "3975868", "url": "https://en.wikipedia.org/wiki?curid=3975868", "title": "Quantum metrology", "text": "Quantum metrology\n\nQuantum metrology is the study of making high-resolution and highly sensitive measurements of physical parameters using quantum theory to describe the physical systems, particularly exploiting quantum entanglement and quantum squeezing. This field promises to develop measurement techniques that give better precision than the same measurement performed in a classical framework.\n\nOne example of note is the use of the NOON state in a Mach-Zender interferometer to perform accurate phase measurements. A similar effect can be produced using less exotic states such as squeezed states. In atomic ensembles, spin squeezed states can be used for phase measurements.\n\nAn important application of particular note is the detection of gravitational radiation with projects such as LIGO. Here high precision distance measurements must be made of two widely separated masses. However, currently the measurements described by quantum metrology are usually not used as they are very difficult to implement and there are many other sources of noise which prohibit the detection of gravity waves which must be overcome first. Nevertheless, plans may call for the use of quantum metrology in LIGO.\n\nA central question of quantum metrology, how the precision, i.e., the variance of the parameter estimation, scales with the number of particles. Classical interferometers cannot overcome the shot-noise limit formula_1 where is formula_2 the number of particles. Quantum metrology can reach the Heisenberg limit given by formula_3\n\nHowever, if uncorrelated local noise is present, then for large particle numbers the scaling of the precision returns to shot-noise scaling formula_4\n\nThere are strong links between quantum metrology and quantum information science. It has been shown that quantum entanglement is needed to outperform classical interferometry in magnetrometry with a fully polarized ensemble of spins. It has been proved that a similar relation is generally valid for any linear interferometer, independent of the details of the scheme. Moreover, higher and higher levels of multipartite entanglement is needed to achieve a better and better accuracy in parameter estimation.\n"}
{"id": "475952", "url": "https://en.wikipedia.org/wiki?curid=475952", "title": "Race condition", "text": "Race condition\n\nA race condition or race hazard is the behavior of an electronics, software, or other system where the output is dependent on the sequence or timing of other uncontrollable events. It becomes a bug when events do not happen in the order the programmer intended.\n\nThe term race condition was already in use by 1954, for example in David A. Huffman's doctoral thesis \"The synthesis of sequential switching circuits\". \n\nRace conditions can occur especially in logic circuits, multithreaded or distributed software programs.\n\nA typical example of a race condition may occur when a logic gate combines signals that have traveled along different paths from the same source. The inputs to the gate can change at slightly different times in response to a change in the source signal. The output may, for a brief period, change to an unwanted state before settling back to the designed state. Certain systems can tolerate such glitches but if this output functions as a clock signal for further systems that contain memory, for example, the system can rapidly depart from its designed behaviour (in effect, the temporary glitch becomes a permanent glitch).\n\nConsider, for example, a two-input AND gate fed with a logic signal A on one input and its negation, NOT A, on another input. In theory the output (A AND NOT A) should never be true. If, however, changes in the value of A take longer to propagate to the second input than the first when A changes from false to true then a brief period will ensue during which both inputs are true, and so the gate's output will also be true.\n\nDesign techniques such as Karnaugh maps encourage designers to recognize and eliminate race conditions before they cause problems. Often logic redundancy can be added to eliminate some kinds of races.\n\nAs well as these problems, some logic elements can enter metastable states, which create further problems for circuit designers.\n\nA \"critical race condition\" occurs when the order in which internal variables are changed determines the eventual state that the state machine will end up in.\n\nA \"non-critical race condition\" occurs when the order in which internal variables are changed does not determine the eventual state that the state machine will end up in.\n\nA \"static race condition\" occurs when a signal and its complement are combined together.\n\nA \"dynamic race condition\" occurs when it results in multiple transitions when only one is intended. They are due to interaction between gates. It can be eliminated by using no more than two levels of gating.\n\nAn \"essential race condition\" occurs when an input has two transitions in less than the total feedback propagation time. Sometimes they are cured using inductive delay line elements to effectively increase the time duration of an input signal.\n\nRace conditions arise in software when an application depends on the sequence or timing of processes or threads for it to operate properly. As with electronics, there are critical race conditions that result in invalid execution and bugs. Critical race conditions often happen when the processes or threads depend on some shared state. Operations upon shared states are critical sections that must be mutually exclusive. Failure to obey this rule opens up the possibility of corrupting the shared state.\n\nThe memory model defined in the C11 and C++11 standards uses the term \"data race\" for a race condition caused by potentially concurrent operations on a shared memory location, of which at least one is a write. A C or C++ program containing a data race has undefined behavior.\n\nRace conditions have a reputation of being difficult to reproduce and debug, since the end result is nondeterministic and depends on the relative timing between interfering threads. Problems occurring in production systems can therefore disappear when running in debug mode, when additional logging is added, or when attaching a debugger, often referred to as a \"Heisenbug\". It is therefore better to avoid race conditions by careful software design rather than attempting to fix them afterwards.\n\nAs a simple example, let us assume that two threads want to increment the value of a global integer variable by one. Ideally, the following sequence of operations would take place:\n\nIn the case shown above, the final value is 2, as expected. However, if the two threads run simultaneously without locking or synchronization, the outcome of the operation could be wrong. The alternative sequence of operations below demonstrates this scenario:\n\nIn this case, the final value is 1 instead of the expected result of 2. This occurs because here the increment operations are not mutually exclusive. Mutually exclusive operations are those that cannot be interrupted while accessing some resource such as a memory location.\n\nMany software race conditions have associated computer security implications. A race condition allows an attacker with access to a shared resource to cause other actors that utilize that resource to malfunction, resulting in effects including denial of service and privilege escalation.\n\nA specific kind of race condition involves checking for a predicate (e.g. for authentication), then acting on the predicate, while the state can change between the \"time of check\" and the \"time of use\". When this kind of bug exists in security-sensitive code, a security vulnerability called a time-of-check-to-time-of-use (\"TOCTTOU\") bug is created.\n\nRace conditions are also intentionally used to create hardware random number generators and physically unclonable functions. PUFs can be created by designing circuit topologies with identical paths to a node and relying on manufacturing variations to randomly determine which paths will complete first. By measuring each manufactured circuit's specific set of race condition outcomes, a profile can be collected for each circuit and kept secret in order to later verify a circuit's identity.\n\nTwo or more programs may collide in their attempts to modify or access a file system, which can result in data corruption or privilege escalation. File locking provides a commonly used solution. A more cumbersome remedy involves organizing the system in such a way that one unique process (running a daemon or the like) has exclusive access to the file, and all other processes that need to access the data in that file do so only via interprocess communication with that one process. This requires synchronization at the process level.\n\nA different form of race condition exists in file systems where unrelated programs may affect each other by suddenly using up available resources such as disk space, memory space, or processor cycles. Software not carefully designed to anticipate and handle this race situation may then become unpredictable. Such a risk may be overlooked for a long time in a system that seems very reliable. But eventually enough data may accumulate or enough other software may be added to critically destabilize many parts of a system. An example of this occurred with the near loss of the Mars Rover \"Spirit\" not long after landing. A solution is for software to request and reserve all the resources it will need before beginning a task; if this request fails then the task is postponed, avoiding the many points where failure could have occurred. Alternatively, each of those points can be equipped with error handling, or the success of the entire task can be verified afterwards, before continuing. A more common approach is to simply verify that enough system resources are available before starting a task; however, this may not be adequate because in complex systems the actions of other running programs can be unpredictable.\n\nIn networking, consider a distributed chat network like IRC, where a user who starts a channel automatically acquires channel-operator privileges. If two users on different servers, on different ends of the same network, try to start the same-named channel at the same time, each user's respective server will grant channel-operator privileges to each user, since neither server will yet have received the other server's signal that it has allocated that channel. (This problem has been largely solved by various IRC server implementations.)\n\nIn this case of a race condition, the concept of the \"shared resource\" covers the state of the network (what channels exist, as well as what users started them and therefore have what privileges), which each server can freely change as long as it signals the other servers on the network about the changes so that they can update their conception of the state of the network. However, the latency across the network makes possible the kind of race condition described. In this case, heading off race conditions by imposing a form of control over access to the shared resource—say, appointing one server to control who holds what privileges—would mean turning the distributed network into a centralized one (at least for that one part of the network operation).\n\nRace conditions can also exist when a computer program is written with non-blocking sockets, in which case the performance of the program can be dependent on the speed of the network link.\n\nSoftware flaws in life-critical systems can be disastrous. Race conditions were among the flaws in the Therac-25 radiation therapy machine, which led to the death of at least three patients and injuries to several more.\n\nAnother example is the Energy Management System provided by GE Energy and used by Ohio-based FirstEnergy Corp (among other power facilities). A race condition existed in the alarm subsystem; when three sagging power lines were tripped simultaneously, the condition prevented alerts from being raised to the monitoring technicians, delaying their awareness of the problem. This software flaw eventually led to the North American Blackout of 2003. GE Energy later developed a software patch to correct the previously undiscovered error.\n\nNeuroscience is demonstrating that race conditions can occur in mammal (rat) brains as well.\n\nMany software tools exist to help detect race conditions in software. They can be largely categorized into two groups: static analysis tools and dynamic analysis tools.\n\nThread Safety Analysis is a static analysis tool for annotation-based intra-procedural static analysis, originally implemented as a branch of gcc, and now reimplemented in Clang, supporting PThreads.\n\nDynamic analysis tools include: Intel Inspector, a memory and thread checking and debugging tool to increase the reliability, security, and accuracy of C/C++ and Fortran applications; Intel Advisor, a sampling based, SIMD vectorization optimization and shared memory threading assistance tool for C, C++, C#, and Fortran software developers and architects; ThreadSanitizer, which uses binary (Valgrind-based) or source, LLVM-based instrumentation, and supports PThreads); and Helgrind, a Valgrind tool for detecting synchronisation errors in C, C++ and Fortran programs that use the POSIX pthreads threading primitives.\n\n\n"}
{"id": "55206702", "url": "https://en.wikipedia.org/wiki?curid=55206702", "title": "Seidel's algorithm", "text": "Seidel's algorithm\n\nSeidel's algorithm is an algorithm designed by Raimund Seidel in 1992 for the all-pairs-shortest-path problem for undirected, unweighted, connected graphs. It solves the problem in formula_1 expected time for a graph with formula_2 vertices, where formula_3 is the exponent in the complexity formula_4 of formula_5 matrix multiplication. If only the distances between each pair of vertices are sought, the same time bound can be achieved in the worst case. Note that even though the algorithm is designed for connected graphs, it can be applied individually to each connected component of a graph with the same running time overall. Note also that there is an exception to the expected running time given above for computing the paths: if formula_6 the expected running time becomes formula_7.\n\nThe core of the algorithm is a procedure that computes the length of the shortest-paths between any pair of vertices.\nThis can be done in formula_1 time in the worst case. Once the lengths are computed, the paths can be reconstructed using a Las Vegas algorithm whose expected running time is formula_1 for formula_10 and formula_7 for formula_6.\n\nThe python code below assumes the input graph is given as a formula_13 formula_14-formula_15 adjacency matrix formula_16 with zeros on the diagonal. It defines the function APD which returns a matrix with entries formula_17 such that formula_17 is the length of the shortest path between the vertices formula_19 and formula_20. The matrix class used can be any matrix class implementation supporting the multiplication, exponentiation, and indexing operators (for example numpy.matrix).\ndef APD ( A , n ):\nThe base case tests whether the input adjacency matrix describes a complete graph, in which case all shortest paths have length formula_15.\n\nAlgorithms for undirected and directed graphs with weights from a finite universe formula_22 also exist. The best known algorithm for the directed case is in time formula_23 by Zwick in 1998. This algorithm uses rectangular matrix multiplication instead of square matrix multiplication. Better upper bounds can be obtained if one uses the best rectangular matrix multiplication algorithm available instead of achieving rectangular multiplication via multiple square matrix multiplications. The best known algorithm for the undirected case is in time formula_24 by Shoshan and Zwick in 1999. The original implementation of this algorithm was erroneous and has been corrected by Eirinakis, Williamson, and Subramani in 2016.\n"}
{"id": "23992863", "url": "https://en.wikipedia.org/wiki?curid=23992863", "title": "Simultaneous game", "text": "Simultaneous game\n\nIn game theory, a simultaneous game is a game where each player chooses his action without knowledge of the actions chosen by other players. Simultaneous games contrast with sequential games, which are played by the players taking turns (moves alternate between players). Normal form representations are usually used for simultaneous games.\n\nRock-paper-scissors, a widely played hand game, is an example of a simultaneous game. Both players make a decision without knowledge of the opponent's decision, and reveal their hands at the same time. There are two players in this game and each of them has three different strategies to make their decision; the combination of strategy profiles forms a 3×3 table. We will display Player 1’s strategies as rows and Player 2’s strategies as columns. In the table, the numbers in red represent the payoff to Player 1, the numbers in blue represent the payoff to Player 2. Hence, the pay off for a 2 player game in rock-paper-scissors will look like this:\nThe prisoner's dilemma is also an example of a simultaneous game. Some variants of chess that belong to this class of games include Synchronous chess and Parity chess.\n\n\nBibliography\n"}
{"id": "28795896", "url": "https://en.wikipedia.org/wiki?curid=28795896", "title": "Structured derivations", "text": "Structured derivations\n\nStructured derivations (SD) is a logic-based format for presenting mathematical solutions and proofs created by Prof. Ralph-Johan Back and Joakim von Wright at Åbo Akademi University, Turku, Finland. The format was originally introduced as a way for presenting proofs in programming logic, but was later adapted to provide a practical approach to presenting proofs and derivations in mathematics education including exact formalisms. A structured derivation has a precise mathematical interpretation, and the syntax and the layout are precisely defined. The standardized syntax renders the format suitable for presenting and manipulating mathematics digitally.\n\nSD is a further development of the calculational proof format introduced by Edsger W. Dijkstra and others in the early 1990s. In essence, three main extensions have been made. First, a mechanism for decomposing proofs through the use of subderivations has been added. The calculational approach is limited to writing proof fragments, and longer derivations are commonly decomposed into several separate subproofs. Using SD with subderivations, on the other hand, the presentation of a complete proof or solution is kept together, as subproofs can be presented exactly where they are needed. In addition, SD makes it possible to handle assumptions and observations in proofs. As such, the format can be seen as combining the benefits of the calculational style with the decomposition facilities of natural deduction.\n\nThe following three examples will be used to illustrate the most central features of structured derivations.\n\nSolving a simple equation illustrates the basic structure of a structured derivation. The start of the solution is indicated by a bullet (formula_1) followed by the task we are to solve (in this case the equation formula_2).\nEach step in the solution consists of two terms, a relation and a justification that explains why the relationship between the two terms hold. The justifications are given equal amount of space as the mathematical terms in order to indicate the importance of explanations in mathematics.\n\nSpecifications of mathematical problems commonly contain information that can be used in the solution. When writing a proof or a solution as a structured derivation, all known information is listed in the beginning as \"assumptions\". These assumptions can be used to create new information that will be useful for solving the problem. This information can be added as \"observations\" that build on the assumptions. The following example uses two assumptions ((a)–(b)) and two observations ([1]–[2]). The introductory part of the solution (the task, assumptions and observations) is separated from the proof part by the formula_3-symbol, denoting logical provability.\n\n\"Sea water, where the mass-volume percentage of salt is 4.0%, is vaporized in a pool until its mass has decreased by 28%. What is the concentration of salt after the vaporization?\"\n\nWhen solving a mathematical problem or constructing a proof, there is often a need to solve smaller problems in order to solve the entire problem. These subsolutions or subproofs are commonly written as fragments on the paper. SD introduces a mechanism for handling this type of subsolutions in a way that keeps these together with the remaining solution in one single chain. These \"subderivations\" are indented and the return to the original level is indicated with an ellipsis (formula_4). The following example is the same as the one above; here, however, the information given as observations above is given in subderivations instead.\n\nStarting in 2001, SD has been empirically evaluated at different education levels with students aged 15–24. The most extensive study so far was a three-year-long quasi experiment conducted at a Finnish high school, where the test group was taught the compulsory mathematics courses using SD and the control group studied according to the traditional approach. The results indicate that the students in the test group performed better in all courses and the matriculation examination, even when potentially influencing factors have been taken into account. Other studies have indicated that students learn to justify their solutions during one single course and that students appreciate the new approach to writing mathematics.\n\n"}
{"id": "597584", "url": "https://en.wikipedia.org/wiki?curid=597584", "title": "Tree traversal", "text": "Tree traversal\n\nIn computer science, tree traversal (also known as tree search) is a form of graph traversal and refers to the process of visiting (checking and/or updating) each node in a tree data structure, exactly once. Such traversals are classified by the order in which the nodes are visited. The following algorithms are described for a binary tree, but they may be generalized to other trees as well.\n\nUnlike linked lists, one-dimensional arrays and other linear data structures, which are canonically traversed in linear order, trees may be traversed in multiple ways. They may be traversed in depth-first or breadth-first order. There are three common ways to traverse them in depth-first order: in-order, pre-order and post-order. Beyond these basic traversals, various more complex or hybrid schemes are possible, such as depth-limited searches like iterative deepening depth-first search.\n\nTraversing a tree involves iterating over all nodes in some manner. Because from a given node there is more than one possible next node (it is not a linear data structure), then, assuming sequential computation (not parallel), some nodes must be deferred—stored in some way for later visiting. This is often done via a stack (LIFO) or queue (FIFO). As a tree is a self-referential (recursively defined) data structure, traversal can be defined by recursion or, more subtly, corecursion, in a very natural and clear fashion; in these cases the deferred nodes are stored implicitly in the call stack.\n\nDepth-first search is easily implemented via a stack, including recursively (via the call stack), while breadth-first search is easily implemented via a queue, including corecursively.\n\nThese searches are referred to as \"depth-first search\" (DFS), as the search tree is deepened as much as possible on each child before going to the next sibling. For a binary tree, they are defined as display operations recursively at each node, starting with the root, whose algorithm is as follows:\nThe general recursive pattern for traversing a (non-empty) binary tree is this: At node N do the following:\n\n(L) Recursively traverse its left subtree. This step is finished at the node N again.\n\n(R) Recursively traverse its right subtree. This step is finished at the node N again.\n\n(N) Process N itself.\n\nThese steps can be done \"in any order\". If (L) is done before (R), the process is called left-to-right traversal, otherwise it is called right-to-left traversal. The following methods show left-to-right traversal:\n\n\nThe pre-order traversal is a topologically sorted one, because a parent node is processed before any of its child nodes is done.\n\n\nIn a binary search tree, in-order traversal retrieves data in sorted order.\n\n\nThe trace of a traversal is called a sequentialisation of the tree. The traversal trace is a list of each visited root. No one sequentialisation according to pre-, in- or post-order describes the underlying tree uniquely. Given a tree with distinct elements, either pre-order or post-order paired with in-order is sufficient to describe the tree uniquely. However, pre-order with post-order leaves some ambiguity in the tree structure.\nTo traverse any tree with depth-first search, perform the following operations recursively at each node:\nDepending on the problem at hand, the pre-order, in-order or post-order operations may be void, or you may only want to visit a specific child, so these operations are optional. Also, in practice more than one of pre-order, in-order and post-order operations may be required. For example, when inserting into a ternary tree, a pre-order operation is performed by comparing items. A post-order operation may be needed afterwards to re-balance the tree.\n\nTrees can also be traversed in \"level-order\", where we visit every node on a level before going to a lower level. This search is referred to as \"breadth-first search\" (BFS), as the search tree is broadened as much as possible on each depth before going to the next depth.\n\nThere are also tree traversal algorithms that classify as neither depth-first search nor breadth-first search. One such algorithm is Monte Carlo tree search, which concentrates on analyzing the most promising moves, basing the expansion of the search tree on random sampling of the search space.\n\nPre-order traversal while duplicating nodes and edges can make a complete duplicate of a binary tree. It can also be used to make a prefix expression (Polish notation) from expression trees: traverse the expression tree pre-orderly.\n\nIn-order traversal is very commonly used on binary search trees because it returns values from the underlying set in order, according to the comparator that set up the binary search tree (hence the name).\n\nPost-order traversal while deleting or freeing nodes and values can delete or free an entire binary tree. It can also generate a postfix representation of a binary tree.\n\nAll the above implementations require stack space proportional to the height of the tree which is a call stack for the recursive and a parent stack for the iterative ones. In a poorly balanced tree, this can be considerable. With the iterative implementations we can remove the stack requirement by maintaining parent pointers in each node, or by threading the tree (next section).\n\nA binary tree is threaded by making every left child pointer (that would otherwise be null) point to the in-order predecessor of the node (if it exists) and every right child pointer (that would otherwise be null) point to the in-order successor of the node (if it exists).\n\nAdvantages:\n\nDisadvantages:\n\nMorris traversal is an implementation of in-order traversal that uses threading:\n\nAlso, listed below is pseudocode for a simple queue based level-order traversal, and will require space proportional to the maximum number of nodes at a given depth. This can be as much as the total number of nodes / 2. A more space-efficient approach for this type of traversal can be implemented using an iterative deepening depth-first search.\n\nWhile traversal is usually done for trees with a finite number of nodes (and hence finite depth and finite branching factor) it can also be done for infinite trees. This is of particular interest in functional programming (particularly with lazy evaluation), as infinite data structures can often be easily defined and worked with, though they are not (strictly) evaluated, as this would take infinite time. Some finite trees are too large to represent explicitly, such as the game tree for chess or go, and so it is useful to analyze them as if they were infinite.\n\nA basic requirement for traversal is to visit every node eventually. For infinite trees, simple algorithms often fail this. For example, given a binary tree of infinite depth, a depth-first search will go down one side (by convention the left side) of the tree, never visiting the rest, and indeed a pre-order or post-order traversal will never visit \"any\" nodes, as it has not reached a leaf (and in fact never will). By contrast, a breadth-first (level-order) traversal will traverse a binary tree of infinite depth without problem, and indeed will traverse any tree with bounded branching factor.\n\nOn the other hand, given a tree of depth 2, where the root has infinitely many children, and each of these children has two children, a depth-first search will visit all nodes, as once it exhausts the grandchildren (children of children of one node), it will move on to the next (assuming it is not post-order, in which case it never reaches the root). By contrast, a breadth-first search will never reach the grandchildren, as it seeks to exhaust the children first.\n\nA more sophisticated analysis of running time can be given via infinite ordinal numbers; for example, the breadth-first search of the depth 2 tree above will take ω·2 steps: ω for the first level, and then another ω for the second level.\n\nThus, simple depth-first or breadth-first searches do not traverse every infinite tree, and are not efficient on very large trees. However, hybrid methods can traverse any (countably) infinite tree, essentially via a diagonal argument (\"diagonal\"—a combination of vertical and horizontal—corresponds to a combination of depth and breadth).\n\nConcretely, given the infinitely branching tree of infinite depth, label the root (), the children of the root (1), (2), …, the grandchildren (1, 1), (1, 2), …, (2, 1), (2, 2), …, and so on. The nodes are thus in a one-to-one correspondence with finite (possibly empty) sequences of positive numbers, which are countable and can be placed in order first by sum of entries, and then by lexicographic order within a given sum (only finitely many sequences sum to a given value, so all entries are reached—formally there are a finite number of compositions of a given natural number, specifically 2 compositions of ), which gives a traversal. Explicitly:\n\netc.\n\nThis can be interpreted as mapping the infinite depth binary tree onto this tree and then applying breadth-first search: replace the \"down\" edges connecting a parent node to its second and later children with \"right\" edges from the first child to the second child, from the second child to the third child, etc. Thus at each step one can either go down (append a (, 1) to the end) or go right (add one to the last number) (except the root, which is extra and can only go down), which shows the correspondence between the infinite binary tree and the above numbering; the sum of the entries (minus one) corresponds to the distance from the root, which agrees with the 2 nodes at depth in the infinite binary tree (2 corresponds to binary).\n\n\n"}
{"id": "19363223", "url": "https://en.wikipedia.org/wiki?curid=19363223", "title": "Trivial Graph Format", "text": "Trivial Graph Format\n\nTrivial Graph Format (TGF) is a simple text-based file format for describing graphs. It consists of a list of node definitions, which map node IDs to labels, followed by a list of edges, which specify node pairs and an optional edge label. Node IDs can be arbitrary identifiers, whereas labels for both nodes and edges are plain strings.\n\nThe graph may be interpreted as a directed or undirected graph. For directed graphs, to specify the concept of bi-directionality in an edge, one may either specify two edges (forward and back) or differentiate the edge by means of a label. For more powerful specification of graphs, see the other graph file formats below.\n\nA simple graph with 2 nodes and 1 edge might look like this:\nThe # sign marks the end of the node list and the start of the edge list.\n\n\n"}
{"id": "6263864", "url": "https://en.wikipedia.org/wiki?curid=6263864", "title": "Turing machine equivalents", "text": "Turing machine equivalents\n\nA Turing machine is a hypothetical computing device, first conceived by Alan Turing in 1936. Turing machines manipulate symbols on a potentially infinite strip of tape according to a finite table of rules, and they provide the theoretical underpinnings for the notion of a computer algorithm.\n\nWhile none of the following models have been shown to have more power than the single-tape, one-way infinite, multi-symbol Turing-machine model, their authors defined and used them to investigate questions and solve problems more easily than they could have if they had stayed with Turing's \"a\"-machine model.\n\nTuring equivalence\n\nMany machines that might be thought to have more computational capability than a simple universal Turing machine can be shown to have no more power. They might compute faster, perhaps, or use less memory, or their instruction set might be smaller, but they cannot compute more powerfully (i.e. more mathematical functions). (The Church–Turing thesis \"hypothesizes\" this to be true: that anything that can be “computed” can be computed by some Turing machine.)\n\nThe sequential-machine models\n\nAll of the following are called \"sequential machine models\" to distinguish them from \"parallel machine models\".\n\nTuring's \"a\"-machine model\n\nTuring's a-machine (as he called it) was left-ended, right-end-infinite. He provided symbols əə to mark the left end. Any of finite number of tape symbols were permitted. The instructions (if a universal machine), and the \"input\" and \"out\" were written only on \"F-squares\", and markers were to appear on \"E-squares\". In essence he divided his machine into two tapes that always moved together. The instructions appeared in a tabular form called \"5-tuples\" and were not executed sequentially.\n\nThe following models are single tape Turing machines but restricted with (i) restricted tape symbols { mark, blank }, and/or (ii) sequential, computer-like instructions, and/or (iii) machine-actions fully atomized.\n\nEmil Post in an independent description of a computational process, reduced the symbols allowed to the equivalent binary set of marks on the tape { \"mark\", \"blank\"=not_mark }. He changed the notion of \"tape\" from 1-way infinite to the right to an infinite set of rooms each with a sheet of paper in both directions. He atomized the Turing 5-tuples into 4-tuples—motion instructions separate from print/erase instructions. Although his 1936 model is ambiguous about this, Post's 1947 model did not require sequential instruction execution.\n\nHis extremely simple model can emulate any Turing machine, and although his 1936 \"Formulation 1\" does not use the word \"program\" or \"machine\", it is effectively a formulation of a very primitive programmable computer and associated programming language, with the boxes acting as an unbounded bitstring memory, and the set of instructions constituting a program.\n\nIn an influential paper, Hao Wang reduced Post's \"\" to machines that still use a two-way infinite binary tape, but whose instructions are simpler — being the \"atomic\" components of Post's instructions — and are by default executed sequentially (like a \"computer program\"). His stated principal purpose was to offer, as an alternative to Turing's theory, one that \"is more economical in the basic operations\". His results were \"program formulations\" of a variety of such machines, including the 5-instruction Wang W-machine with the instruction-set\n\nand his most-severely reduced 4-instruction Wang B-machine (\"B\" for \"basic\") with the instruction-set\n\nwhich has not even an ERASE-SQUARE instruction.\n\nMany authors later introduced variants of the machines discussed by Wang:\n\nMinsky evolved Wang's notion with his version of the (multi-tape) \"counter machine\" model that allowed SHIFT-LEFT and SHIFT-RIGHT motion of the separate heads but no printing at all. In this case the tapes would be left-ended, each end marked with a single \"mark\" to indicate the end. He was able to reduce this to a single tape, but at the expense of introducing multi-tape-square motion equivalent to multiplication and division rather than the much simpler { SHIFT-LEFT = DECREMENT, SHIFT-RIGHT = INCREMENT }.\n\nDavis, adding an explicit HALT instruction to one of the machines discussed by Wang, used a model with the instruction-set\n\nand also considered versions with tape-alphabets of size larger than 2.\n\nIn keeping with Wang's project to seek a Turing-equivalent theory \"economical in the basic operations\", and wishing to avoid unconditional jumps, a notable theoretical language is the 4-instruction language P\" introduced by Corrado Böhm in 1964 — the first \"GOTO-less\" imperative \"structured programming\" language to be proved Turing-complete.\n\nIn practical analysis, various types of multi-tape Turing machines are often used. Multi-tape machines are similar to single-tape machines, but there is some constant \"k\" number of independent tapes.\n\nIf the action table has at most one entry for each combination of symbol and state then the machine is a \"deterministic Turing machine\" (DTM). If the action table contains multiple entries for a combination of symbol and state then the machine is a \"non-deterministic Turing machine\" (NDTM). The two are computationally equivalent, that is, it is possible to turn any NDTM into a DTM (and \"vice versa\"). This can be proved via construction.\n\nAn oblivious Turing machine is a Turing machine where movement of the various heads are fixed functions of time, independent of the input. In other words, there is a predetermined sequence in which the various tapes are scanned, advanced, and written to. Pippenger and Fischer showed that any computation that can be performed by a multi-tape Turing machine in \"n\" steps can be performed by an oblivious two-tape Turing machine in steps.\n\nPeter van Emde Boas includes all machines of this type in one class, \"the register machine\". However, historically the literature has also called the most primitive member of this group i.e. \"the counter machine\" -- \"the register machine\". And the most primitive embodiment of a \"counter machine\" is sometimes called the \"Minsky machine\".\n\nThe primitive model register machine is, in effect, a multitape 2-symbol Post–Turing machine with its behavior restricted so its tapes act like simple \"counters\".\n\nBy the time of Melzak, Lambek, and Minsky the notion of a \"computer program\" produced a different type of simple machine with many left-ended tapes cut from a Post–Turing tape. In all cases the models permit only two tape symbols { mark, blank }.\n\nSome versions represent the positive integers as only a strings/stack of marks allowed in a \"register\" (i.e. left-ended tape), and a blank tape represented by the count \"0\". Minsky eliminated the PRINT instruction at the expense of providing his model with a mandatory single mark at the left-end of each tape.\n\nIn this model the single-ended tapes-as-registers are thought of as \"counters\", their instructions restricted to only two (or three if the TEST/DECREMENT instruction is atomized). Two common instruction sets are the following:\n\nAlthough his model is more complicated than this simple description, the Melzak \"pebble\" model extended this notion of \"counter\" to permit multi-\npebble adds and subtracts.\n\nMelzak recognized a couple serious defects in his register/counter-machine model: (i) Without a form of indirect addressing he would not be able to \"easily\" show the model is Turing equivalent, (ii) The program and registers were in different \"spaces\", so self-modifying programs would not be easy. When Melzak added indirect addressing to his model he created a random access machine model.\n\n(However, with Gödel numbering of the instructions Minsky offered a proof that with such numbering the general recursive functions were indeed possible; he offers proof that μ recursion is indeed possible).\n\nUnlike the RASP model, the RAM model does not allow the machine's actions to modify its instructions. Sometimes the model works only register-to-register with no accumulator, but most models seem to include an accumulator.\n\nvan Emde Boas divides the various RAM models into a number of sub-types:\n\nThe RASP is a RAM with the instructions stored together with their data in the same 'space' -- i.e. sequence of registers. The notion of a RASP was described at least as early as Kiphengst. His model had a \"mill\"—an accumulator, but now the instructions were in the registers with the data—the so-called von Neumann architecture. When the RASP has alternating even and odd registers—the even holding the \"operation code\" (instruction) and the odd holding its \"operand\" (parameter), then indirect addressing is achieved by simply modifying an instruction's operand.\n\nThe original RASP model of Elgot and Robinson had only three instructions in the fashion of the register-machine model, but they placed them in the register space together with their data. (Here COPY takes the place of CLEAR when one register e.g. \"z\" or \"0\" starts with and always contains 0. This trick is not unusual. The unit 1 in register \"unit\" or \"1\" is also useful.) \n\nThe RASP models allow indirect as well as direct-addressing; some allow \"immediate\" instructions too, e.g. \"Load accumulator with the constant 3\". The instructions may be of a highly restricted set such as the following 16 instructions of Hartmanis. This model uses an accumulator A. The mnemonics are those that the authors used (their CLA is \"load accumulator\" with constant or from register; STO is \"store accumulator\"). Their syntax is the following, excepting the jumps: \"n, <n>, «n»\" for \"immediate\", \"direct\" and \"indirect\"). Jumps are via two \"Transfer instructions\" TRA—unconditional jump by directly \"n\" or indirectly \"< n >\" jamming contents of register n into the instruction counter, TRZ (conditional jump if Accumulator is zero in the same manner as TRA): \n\nA relative late-comer is Schönhage's Storage Modification Machine or pointer machine. Another version is the Kolmogorov-Uspensii machine, and the Knuth \"linking automaton\" proposal. (For references see pointer machine). Like a state-machine diagram, a node emits at least two labelled \"edges\" (arrows) that point to another node or nodes which in turn point to other nodes, etc. The outside world points at the center node.\n\nAny of the above tape-based machines can be equipped with input and output tapes; any of the above register-based machines can be equipped with dedicated input and output registers. For example, the Schönhage pointer-machine model has two instructions called \"\"input\" \"λ\",\"λ\"\" and \"\"output\" \"β\"\".\n\nIt is difficult to study sublinear space complexity on multi-tape machines with the traditional model, because an input of size \"n\" already takes up space \"n\". Thus, to study small DSPACE classes, we must use a different model. In some sense, if we never \"write to\" the input tape, we don't want to charge ourself for this space. And if we never \"read from\" our output tape, we don't want to charge ourself for this space.\n\nWe solve this problem by introducing a \"k\"-string Turing machine with input and output. This is the same as an ordinary \"k\"-string Turing machine, except that the transition function is restricted so that the input tape can never be changed, and so that the output head can never move left. This model allows us to define deterministic space classes smaller than linear. Turing machines with input-and-output also have the same time complexity as other Turing machines; in the words of Papadimitriou 1994 Prop 2.2:\n\n\"k\"-string Turing machines with input and output can be used in the formal definition of the complexity resource DSPACE.\n\n"}
{"id": "15631055", "url": "https://en.wikipedia.org/wiki?curid=15631055", "title": "Undecidable problem", "text": "Undecidable problem\n\nIn computability theory and computational complexity theory, an undecidable problem is a decision problem for which it is proved to be impossible to construct an algorithm that always leads to a correct yes-or-no answer. The halting problem is an example: there is no algorithm that correctly determines whether arbitrary programs eventually halt when run. \n\nA decision problem is any arbitrary yes-or-no question on an infinite set of inputs. Because of this, it is traditional to define the decision problem equivalently as the set of inputs for which the problem returns \"yes\". These inputs can be natural numbers, but also other values of some other kind, such as strings of a formal language. Using some encoding, such as a Gödel numbering, the strings can be encoded as natural numbers. Thus, a decision problem informally phrased in terms of a formal language is also equivalent to a set of natural numbers. To keep the formal definition simple, it is phrased in terms of subsets of the natural numbers.\n\nFormally, a decision problem is a subset of the natural numbers. The corresponding informal problem is that of deciding whether a given number is in the set. A decision problem \"A\" is called decidable or effectively solvable if \"A\" is a recursive set. A problem is called partially decidable, semi-decidable, solvable, or provable if \"A\" is a recursively enumerable set. This means that there exists an algorithm that halts eventually when the answer is \"yes\" but may run for ever if the answer is \"no\". Partially decidable problems and any other problems that are not decidable are called undecidable.\n\nIn computability theory, the halting problem is a decision problem which can be stated as follows:\n\nAlan Turing proved in 1936 that a general algorithm running on a Turing machine that solves the halting problem for \"all\" possible program-input pairs necessarily cannot exist. Hence, the halting problem is \"undecidable\" for Turing machines.\n\nThe concepts raised by Gödel's incompleteness theorems are very similar to those raised by the halting problem, and the proofs are quite similar. In fact, a weaker form of the First Incompleteness Theorem is an easy consequence of the undecidability of the halting problem. This weaker form differs from the standard statement of the incompleteness theorem by asserting that a complete, consistent and sound axiomatization of all statements about natural numbers is unachievable. The \"sound\" part is the weakening: it means that we require the axiomatic system in question to prove only \"true\" statements about natural numbers. It is important to observe that the statement of the standard form of Gödel's First Incompleteness Theorem is completely unconcerned with the question of truth, but only concerns the issue of whether it can be proven.\n\nThe weaker form of the theorem can be proved from the undecidability of the halting problem as follows. Assume that we have a consistent and complete axiomatization of all true first-order logic statements about natural numbers. Then we can build an algorithm that enumerates all these statements. This means that there is an algorithm \"N\"(\"n\") that, given a natural number \"n\", computes a true first-order logic statement about natural numbers such that, for all the true statements, there is at least one \"n\" such that \"N\"(\"n\") yields that statement. Now suppose we want to decide if the algorithm with representation \"a\" halts on input \"i\". We know that this statement can be expressed with a first-order logic statement, say \"H\"(\"a\", \"i\"). Since the axiomatization is complete it follows that either there is an \"n\" such that \"N\"(\"n\") = \"H\"(\"a\", \"i\") or there is an \"n'\" such that \"N\"(\"n'\") = ¬ \"H\"(\"a\", \"i\"). So if we iterate over all \"n\" until we either find \"H\"(\"a\", \"i\") or its negation, we will always halt. This means that this gives us an algorithm to decide the halting problem. Since we know that there cannot be such an algorithm, it follows that the assumption that there is a consistent and complete axiomatization of all true first-order logic statements about natural numbers must be false.\n\nUndecidable problems can be related to different topics, such as logic, abstract machines or topology. Note that since there are uncountably many undecidable problems, any list, even one of infinite length, is necessarily incomplete.\n\nThere are two distinct senses of the word \"undecidable\" in contemporary use. The first of these is the sense used in relation to Gödel's theorems, that of a statement being neither provable nor refutable in a specified deductive system. The second sense is used in relation to computability theory and applies not to statements but to decision problems, which are countably infinite sets of questions each requiring a yes or no answer. Such a problem is said to be undecidable if there is no computable function that correctly answers every question in the problem set. The connection between these two is that if a decision problem is undecidable (in the recursion theoretical sense) then there is no consistent, effective formal system which proves for every question \"A\" in the problem either \"the answer to \"A\" is yes\" or \"the answer to \"A\" is no\".\n\nBecause of the two meanings of the word undecidable, the term independent is sometimes used instead of undecidable for the \"neither provable nor refutable\" sense. The usage of \"independent\" is also ambiguous, however. It can mean just \"not provable\", leaving open whether an independent statement might be refuted.\n\nUndecidability of a statement in a particular deductive system does not, in and of itself, address the question of whether the truth value of the statement is well-defined, or whether it can be determined by other means. Undecidability only implies that the particular deductive system being considered does not prove the truth or falsity of the statement. Whether there exist so-called \"absolutely undecidable\" statements, whose truth value can never be known or is ill-specified, is a controversial point among various philosophical schools.\n\nOne of the first problems suspected to be undecidable, in the second sense of the term, was the word problem for groups, first posed by Max Dehn in 1911, which asks if there is a finitely presented group for which no algorithm exists to determine whether two words are equivalent. This was shown to be the case in 1952.\n\nThe combined work of Gödel and Paul Cohen has given two concrete examples of undecidable statements (in the first sense of the term): The continuum hypothesis can neither be proved nor refuted in ZFC (the standard axiomatization of set theory), and the axiom of choice can neither be proved nor refuted in ZF (which is all the ZFC axioms \"except\" the axiom of choice). These results do not require the incompleteness theorem. Gödel proved in 1940 that neither of these statements could be disproved in ZF or ZFC set theory. In the 1960s, Cohen proved that neither is provable from ZF, and the continuum hypothesis cannot be proven from ZFC.\n\nIn 1970, Russian mathematician Yuri Matiyasevich showed that Hilbert's Tenth Problem, posed in 1900 as a challenge to the next century of mathematicians, cannot be solved. Hilbert's challenge sought an algorithm which finds all solutions of a Diophantine equation. A Diophantine equation is a more general case of Fermat's Last Theorem; we seek the integer roots of a polynomial in any number of variables with integer coefficients. Since we have only one equation but \"n\" variables, infinitely many solutions exist (and are easy to find) in the complex plane; however, the problem becomes impossible if solutions are constrained to integer values only. Matiyasevich showed this problem to be unsolvable by mapping a Diophantine equation to a recursively enumerable set and invoking Gödel's Incompleteness Theorem.\n\nIn 1936, Alan Turing proved that the halting problem—the question of whether or not a Turing machine halts on a given program—is undecidable, in the second sense of the term. This result was later generalized by Rice's theorem.\n\nIn 1973, Saharon Shelah showed the Whitehead problem in group theory is undecidable, in the first sense of the term, in standard set theory.\n\nIn 1977, Paris and Harrington proved that the Paris-Harrington principle, a version of the Ramsey theorem, is undecidable in the axiomatization of arithmetic given by the Peano axioms but can be proven to be true in the larger system of second-order arithmetic.\n\nKruskal's tree theorem, which has applications in computer science, is also undecidable from the Peano axioms but provable in set theory. In fact Kruskal's tree theorem (or its finite form) is undecidable in a much stronger system codifying the principles acceptable on basis of a philosophy of mathematics called predicativism.\n\nGoodstein's theorem is a statement about the Ramsey theory of the natural numbers that Kirby and Paris showed is undecidable in Peano arithmetic.\n\nGregory Chaitin produced undecidable statements in algorithmic information theory and proved another incompleteness theorem in that setting. Chaitin's theorem states that for any theory that can represent enough arithmetic, there is an upper bound \"c\" such that no specific number can be proven in that theory to have Kolmogorov complexity greater than \"c\". While Gödel's theorem is related to the liar paradox, Chaitin's result is related to Berry's paradox.\n\nIn 2007, researchers Kurtz and Simon, building on earlier work by J.H. Conway in the 1970s, proved that a natural generalization of the Collatz problem is undecidable.\n\n"}
{"id": "2231292", "url": "https://en.wikipedia.org/wiki?curid=2231292", "title": "Universally measurable set", "text": "Universally measurable set\n\nIn mathematics, a subset formula_1 of a Polish space formula_2 is universally measurable if it is measurable with respect to every complete probability measure on formula_2 that measures all Borel subsets of formula_2. In particular, a universally measurable set of reals is necessarily Lebesgue measurable (see #Finiteness condition below).\n\nEvery analytic set is universally measurable. It follows from projective determinacy, which in turn follows from sufficient large cardinals, that every projective set is universally measurable.\n\nThe condition that the measure be a probability measure; that is, that the measure of formula_2 itself be 1, is less restrictive than it may appear. For example, Lebesgue measure on the reals is not a probability measure, yet every universally measurable set is Lebesgue measurable. To see this, divide the real line into countably many intervals of length 1; say, \"N\"=<nowiki>[0,1)</nowiki>, \"N\"=<nowiki>[1,2)</nowiki>, \"N\"=<nowiki>[-1,0)</nowiki>, \"N\"=<nowiki>[2,3)</nowiki>, \"N\"=<nowiki>[-2,-1)</nowiki>, and so on. Now letting μ be Lebesgue measure, define a new measure ν by\nThen easily ν is a probability measure on the reals, and a set is ν-measurable if and only if it is Lebesgue measurable. More generally a universally measurable set must be measurable with respect to every sigma-finite measure that measures all Borel sets.\n\nSuppose formula_1 is a subset of Cantor space formula_8; that is, formula_1 is a set of infinite sequences of zeroes and ones. By putting a binary point before such a sequence, the sequence can be viewed as a real number between 0 and 1 (inclusive), with some unimportant ambiguity. Thus we can think of formula_1 as a subset of the interval <nowiki>[0,1]</nowiki>, and evaluate its Lebesgue measure, if that is defined. That value is sometimes called the coin-flipping measure of formula_1, because it is the probability of producing a sequence of heads and tails that is an element of formula_1 upon flipping a fair coin infinitely many times.\n\nNow it follows from the axiom of choice that there are some such formula_1 without a well-defined Lebesgue measure (or coin-flipping measure). That is, for such an formula_1, the probability that the sequence of flips of a fair coin will wind up in formula_1 is not well-defined. This is a pathological property of formula_1 that says that formula_1 is \"very complicated\" or \"ill-behaved\".\n\nFrom such a set formula_1, form a new set formula_19 by performing the following operation on each sequence in formula_1: Intersperse a 0 at every even position in the sequence, moving the other bits to make room. Although formula_19 is not intuitively any \"simpler\" or \"better-behaved\" than formula_1, the probability that the sequence of flips of a fair coin will be in formula_19 is well-defined. Indeed, to be in formula_19, the coin must come up tails on every even-numbered flip, which happens with probability zero.\n\nHowever formula_19 is \"not\" universally measurable. To see that, we can test it against a \"biased\" coin that always comes up tails on even-numbered flips, and is fair on odd-numbered flips. For a set of sequences to be \"universally\" measurable, an arbitrarily \"biased\" coin may be used (even one that can \"remember\" the sequence of flips that has gone before) and the probability that the sequence of its flips ends up in the set must be well-defined. However, when formula_19 is tested by the coin we mentioned (the one that always comes up tails on even-numbered flips, and is fair on odd-numbered flips), the propability to hit formula_19 is not well defined (for the same reason why formula_1 cannot be tested by the fair coin). Thus the formula_19 is \"not\" universally measurable.\n\n"}
{"id": "35623596", "url": "https://en.wikipedia.org/wiki?curid=35623596", "title": "Vincent's theorem", "text": "Vincent's theorem\n\nIn mathematics, Vincent's theorem—named after Alexandre Joseph Hidulphe Vincent—is a theorem that isolates the real roots of polynomials with rational coefficients.\n\nEven though Vincent's theorem is the basis of the fastest method for the isolation of the real roots of polynomials, it was almost totally forgotten, having been overshadowed by Sturm's theorem; consequently, it does not appear in any of the classical books on the theory of equations (of the 20th century), except for Uspensky's book. Two variants of this theorem are presented, along with several (continued fractions and bisection) real root isolation methods derived from them.\n\n\nTwo versions of this theorem are presented: the \"continued fractions\" version due to Vincent, and the \"bisection\" version due to Alesina and Galuzzi.\n\nThis statement of the \"continued fractions\" version can be found also in the Wikipedia article Budan's theorem.\n\nIf in a polynomial equation with rational coefficients and without multiple roots, one makes successive transformations of the form\n\nwhere formula_2 are any positive numbers greater than or equal to one, then after a number of such transformations, the resulting transformed equation either has zero sign variations or it has a single sign variation. In the first case there is no root, whereas in the second case there is a single positive real root. Furthermore, the corresponding root of the proposed equation is approximated by the finite continued fraction:\n\nMoreover, if infinitely many numbers formula_2 satisfying this property can be found, then the root is represented by the (infinite) corresponding continued fraction.\n\nThe above statement is an exact translation of the theorem found in Vincent's original papers; however, the following remarks are needed for a clearer understanding:\n\nLet \"p\"(\"x\") be a real polynomial of degree deg(\"p\") that has only simple roots. It is possible to determine a positive quantity δ so that for every pair of positive real numbers \"a\", \"b\" with formula_12, every transformed polynomial of the form\n\nhas exactly 0 or 1 sign variations. The second case is possible if and only if \"p\"(\"x\") has a single root within (\"a\", \"b\").\n\nFrom equation () the following criterion is obtained for determining whether a polynomial has any roots in the interval (\"a\", \"b\"):\n\nPerform on \"p\"(\"x\") the substitution\n\nand count the number of sign variations in the sequence of coefficients of the transformed polynomial; this number gives an \"upper bound\" on the number of real roots \"p\"(\"x\") has inside the open interval (\"a\", \"b\"). More precisely, the number \"ρ\"(\"p\") of real roots in the open interval (\"a\", \"b\")—multiplicities counted—of the polynomial \"p\"(\"x\") in R[\"x\"], of degree deg(\"p\"), is bounded above by the number of sign variations \"var\"(\"p\"), where\n\nAs in the case of Descartes' rule of signs if var(\"p\") = 0 it follows that ρ(\"p\") = 0 and if var(\"p\") = 1 it follows that ρ(\"p\") = 1.\n\nA special case of the Alesina–Galuzzi \"a_b roots test\" is Budan's \"0_1 roots test\".\n\nA detailed discussion of Vincent's theorem, its extension, the geometrical interpretation of the transformations involved and three different proofs can be found in the work by Alesina and Galuzzi. A fourth proof is due to Ostrowski who rediscovered a special case of a theorem stated by Obreschkoff, p. 81, in 1920–1923.\n\nTo prove (both versions of) Vincent's theorem Alesina and Galuzzi show that after a series of transformations mentioned in the theorem, a polynomial with one positive root eventually has one sign variation. To show this, they use the following corollary to the theorem by Obreschkoff of 1920–1923 mentioned earlier; that is, the following corollary gives the necessary conditions under which a polynomial with one positive root has exactly one sign variation in the sequence of its coefficients; see also the corresponding figure.\n\nConsider now the Möbius transformation \nand the three circles shown in the corresponding figure; assume that \n\n\n\nFrom the above it becomes obvious that if a polynomial has a single positive root inside the eight-shaped figure and all other roots are outside of it, it presents one sign variation in the sequence of its coefficients. This also guarantees the termination of the process.\n\nIn his fundamental papers, Vincent presented examples that show precisely how to use his theorem to isolate real roots of polynomials with continued fractions. However the resulting method had exponential computing time, a fact that mathematicians must have realized then, as was realized by Uspensky p. 136, a century later.\n\nThe exponential nature of Vincent's algorithm is due to the way the partial quotients \"a\" (in ) are computed. That is, to compute each partial quotient \"a\" (that is, to locate where the roots lie on the \"x\"-axis) Vincent uses Budan's theorem as a \"no roots test\"; in other words, to find the integer part of a root Vincent performs successive substitutions of the form \"x\" ← \"x\"+1 and stops only when the polynomials \"p\"(\"x\") and \"p\"(\"x\"+1) differ in the number of sign variations in the sequence of their coefficients (i.e. when the number of sign variations of \"p\"(\"x\"+1) is decreased).\n\nSee the corresponding diagram where the root lies in the interval (5, 6). It can be easily inferred that, if the root is far away from the origin, it takes a lot of time to find its integer part this way, hence the exponential nature of Vincent's method. Below there is an explanation of how this drawback is overcome.\n\nVincent was the last author in the 19th century to use his theorem for the isolation of the real roots of a polynomial.\n\nThe reason for that was the appearance of Sturm's theorem in 1827, which solved the real root isolation problem in polynomial time, by defining the precise number of real roots a polynomial has in a real open interval (\"a\", \"b\"). The resulting (Sturm's) method for computing the real roots of polynomials has been the only one widely known and used ever since—up to about 1980, when it was replaced (in almost all computer algebra systems) by methods derived from Vincent's theorem, the fastest one being the Vincent–Akritas–Strzeboński (VAS) method.\n\nSerret included in his Algebra, pp 363–368, along with its proof and directed all interested readers to Vincent's papers for examples on how it is used. Serret was the last author to mention in the 19th century.\n\nIn the 20th century cannot be found in any of the theory of equations books; the only exceptions are the books by Uspensky and Obreschkoff, where in the second there is just the statement of the theorem.\n\nIt was in Uspensky's book that Akritas found and made it the topic of his Ph.D. Thesis \"Vincent's Theorem in Algebraic Manipulation\", North Carolina State University, USA, 1978. A major achievement at the time was getting hold of Vincent's original paper of 1836, something that had eluded Uspensky—resulting thus in a great misunderstanding. Vincent's original paper of 1836 was made available to Akritas through the commendable efforts (interlibrary loan) of a librarian in the Library of the University of Wisconsin–Madison, USA.\n\nIsolation of the real roots of a polynomial is the process of finding open disjoint intervals such that each contains exactly one real root and every real root is contained in some interval. According to the French school of mathematics of the 19th century, this is the first step in computing the real roots, the second being their approximation to any degree of accuracy; moreover, the focus is on the positive roots, because to isolate the negative roots of the polynomial \"p\"(\"x\") replace \"x\" by −\"x\" (\"x\" ← −\"x\") and repeat the process.\n\nThe continued fractions version of can be used to isolate the positive roots of a given polynomial \"p\"(\"x\") of degree deg(\"p\"). To see this, represent by the Möbius transformation \nthe continued fraction that leads to a transformed polynomial with one sign variation in the sequence of its coefficients. Then, the single positive root of \"f\"(\"x\") (in the interval (0, ∞)) corresponds to \"that\" positive root of \"p\"(\"x\") that is in the open interval with endpoints formula_28 and formula_29. These endpoints are \"not\" ordered and correspond to \"M\"(0) and \"M\"(∞) respectively.\n\nTherefore, to isolate the positive roots of a polynomial, all that must be done is to compute—for \"each\" root—the variables \"a\", \"b\", \"c\", \"d\" of the corresponding Möbius transformation \nthat leads to a transformed polynomial as in equation (), with one sign variation in the sequence of its coefficients.\n\nCrucial Observation: The variables \"a\", \"b\", \"c\", \"d\" of a Möbius transformation \n(in ) leading to a transformed polynomial—as in equation ()—with one sign variation in the sequence of its coefficients can be computed:\n\nThe \"bisection part\" of this all important observation appeared as a special in the papers by Alesina and Galuzzi.\n\nAll methods described below (see the article on Budan's theorem for their historical background) need to compute (once) an upper bound, \"ub\", on the values of the positive roots of the polynomial under consideration. Exception is the VAS method where additionally lower bounds, \"lb\", must be computed at almost every cycle of the main loop. To compute the lower bound \"lb\" of the polynomial \"p\"(\"x\") compute the upper bound \"ub\" of the polynomial formula_32 and set formula_33.\n\nExcellent (upper and lower) bounds on the values of just the positive roots of polynomials have been developed by Akritas, Strzeboński and Vigklas based on previous work by Doru Stefanescu. They are described in P. S. Vigklas' Ph.D. Thesis and elsewhere. These bounds have already been implemented in the computer algebra systems Mathematica, SageMath, SymPy, Xcas etc.\n\nAll three methods described below follow the excellent presentation of François Boulier, p. 24.\n\nOnly one continued fractions method derives from . As stated above, it started in the 1830s when Vincent presented, in the papers several examples that show how to use his theorem to isolate the real roots of polynomials with continued fractions. However the resulting method had exponential computing time. Below is an explanation of how this method evolved.\n\nThis is the second method (after VCA) developed to handle the exponential behavior of Vincent's method.\n\nThe VAS continued fractions method is a \"direct\" implementation of Vincent's theorem. It was originally presented by Vincent from 1834 to 1938 in the papers in a exponential form; namely, Vincent computed each partial quotient \"a\" by a series of \"unit\" increments \"a\" ← \"a\" + 1, which are equivalent to substitutions of the form \"x\" ← \"x\" + 1.\n\nVincent's method was converted into its polynomial complexity form by Akritas, who in his 1978 Ph.D. Thesis (\"Vincent's theorem in algebraic manipulation\", North Carolina State University, USA) computed each partial quotient \"a\" as the lower bound, \"lb\", on the values of the positive roots of a polynomial. This is called the \"ideal\" positive lower root bound that computes the integer part of the smallest positive root (see the corresponding figure). To wit, now set \"a\" ← \"lb\" or, equivalently, perform the substitution \"x\" ← \"x\" + \"lb\", which takes about the same time as the substitution \"x\" ← \"x\" + 1.\n\nFinally, since the ideal positive lower root bound does not exist, Strzeboński introduced in 2005 the substitution formula_34, whenever formula_35; in general formula_36 and the value 16 was determined experimentally. Moreover, it has been shown that the VAS (continued fractions) method is faster than the fastest implementation of the VCA (bisection) method, a fact that was confirmed independently; more precisely, for the Mignotte polynomials of high degree VAS is about 50,000 times faster than the fastest implementation of VCA.\n\nIn 2007, Sharma removed the hypothesis of the ideal positive lower bound and proved that VAS is still polynomial in time.\n\nVAS is the default algorithm for root isolation in Mathematica, SageMath, SymPy, Xcas.\n\nFor a comparison between Sturm's method and VAS use the functions realroot(poly) and time(realroot(poly)) of Xcas. By default, to isolate the real roots of poly realroot uses the VAS method; to use Sturm's method write realroot(sturm, poly). See also the External links for an application by A. Berkakis for Android devices that does the same thing.\n\nHere is how VAS(\"p\", \"M\") works, where for simplicity Strzeboński's contribution is not included: \n\nBelow is a recursive presentation of VAS(\"p\", \"M\").\n\nVAS(p, M):<br>\n\nInput: A univariate, square-free polynomial formula_43, of degree deg(\"p\"), and the Möbius transformation \nOutput: A list of isolating intervals of the positive roots of \"p\"(\"x\").<br>\n\ncodice_1\n\nRemarks\n\nWe apply the VAS method to (note that: ).\n\ncodice_2\n\nList of isolation intervals: \n\nList of pairs to be processed: \nRemove the first and process it.\n\ncodice_3\n\nList of isolation intervals: \n\nList of pairs to be processed: \nRemove the first and process it.\n\ncodice_4\n\nList of isolation intervals: \n\nList of pairs to be processed: \nRemove the first and process it.\n\ncodice_5\n\nList of isolation intervals: \n\nList of pairs to be processed: \nRemove the first and process it.\n\ncodice_6\n\nList of isolation intervals: \n\nList of pairs to be processed: .\n\nFinished.\n\nTherefore, the two positive roots of the polynomial lie inside the isolation intervals and }. Each root can be approximated by (for example) bisecting the isolation interval it lies in until the difference of the endpoints is smaller than ; following this approach, the roots turn out to be and .\n\nThere are various bisection methods derived from ; they are all presented and compared elsewhere. Here the two most important of them are described, namely, the Vincent–Collins–Akritas (VCA) method and the Vincent–Alesina–Galuzzi (VAG) method.\n\nThe Vincent–Alesina–Galuzzi (VAG) method is the simplest of all methods derived from Vincent's theorem but has the most time consuming test (in line 1) to determine if a polynomial has roots in the interval of interest; this makes it the slowest of the methods presented in this article.\nBy contrast, the Vincent–Collins–Akritas (VCA) method is more complex but uses a simpler test (in line 1) than VAG. This along with certain improvements have made VCA the fastest bisection method.\n\nThis was the first method developed to overcome the exponential nature of Vincent's original approach, and has had quite an interesting history as far as its name is concerned. This method, which isolates the real roots, using Descartes' rule of signs and , had been originally called \"modified Uspensky's algorithm\" by its inventors Collins and Akritas. After going through names like \"Collins–Akritas method\" and \"Descartes' method\" (too confusing if ones considers Fourier's article), it was finally François Boulier, of Lille University, who gave it the name \"Vincent–Collins–Akritas\" (VCA) method, p. 24, based on the fact that \"Uspensky's method\" does not exist and neither does \"Descartes' method\". The best implementation of this method is due to Rouillier and Zimmerman, and to this date, it is the fastest bisection method. It has the same worst case complexity as Sturm's algorithm, but is almost always much faster. It has been implemented in Maple's RootFinding package.\n\nHere is how VCA(\"p\", (\"a\", \"b\")) works:\n\n\n\nBelow is a recursive presentation of the original algorithm VCA(\"p\", (\"a\", \"b\")).\n\nVCA(p, (a, b))<br>\n\nInput: A univariate, square-free polynomial \"p\"(\"ub\" * \"x\") ∈ Z[\"x\"], \"p\"(0) ≠ 0 of degree deg(\"p\"), and the open\ninterval (\"a\", \"b\") = (0, \"ub\"), where \"ub\" is an upper bound on the values of the positive\nroots of \"p\"(\"x\"). (The positive roots of \"p\"(\"ub\" * \"x\") are all in the open interval (0, 1)).<br>\nOutput: A list of isolating intervals of the positive roots of \"p\"(\"x\")<br>\n\ncodice_7\n\nRemark\n\nGiven the polynomial and considering as an upper bound on the values of the positive roots the arguments of the VCA method are: and .\n\ncodice_8\n\nList of isolation intervals: \n\nList of pairs to be processed: \n\nRemove the first and process it.\n\ncodice_9\n\nList of isolation intervals: \n\nList of pairs to be processed:\n\nRemove the first and process it.\n\ncodice_10\n\nList of isolation intervals: \n\nList of pairs to be processed: \n\nRemove the first and process it.\n\ncodice_11\n\nList of isolation intervals: \n\nList of pairs to be processed: \n\nRemove the first and process it.\n\ncodice_12\n\nList of isolation intervals: \n\nList of pairs to be processed: \n\nRemove the first and process it.\n\ncodice_13\n\nList of isolation intervals: \n\nList of pairs to be processed: \n\nRemove the first and process it.\n\ncodice_14\n\nList of isolation intervals: \n\nList of pairs to be processed: .\n\nFinished.\n\nTherefore, the two positive roots of the polynomial lie inside the isolation intervals and }. Each root can be approximated by (for example) bisecting the isolation interval it lies in until the difference of the endpoints is smaller than ; following this approach, the roots turn out to be and .\n\nThis was developed last and is the simplest real root isolation method derived from .\n\nHere is how VAG(\"p\", (\"a\", \"b\")) works: \n\nBelow is a recursive presentation of VAG(\"p\", (\"a\", \"b\")).\n\nVAG(p, (a, b))<br>\nInput: A univariate, square-free polynomial \"p\"(\"x\") ∈ Z[\"x\"], \"p\"(0) ≠ 0 of degree deg(\"p\") and the open interval (\"a\", \"b\") = (0, \"ub\"), where \"ub\" is an upper bound on the values of the positive roots of \"p\"(\"x\"). <br>\nOutput: A list of isolating intervals of the positive roots of \"p\"(\"x\").<br>\n\ncodice_15\n\nRemarks\n\nGiven the polynomial \"p\"(\"x\") = \"x\" − 7\"x\" + 7 and considering as an upper bound on the values of the positive roots \"ub\" = 4 the arguments of VAG are: \"p\"(\"x\") = \"x\" − 7\"x\" + 7 and (\"a\", \"b\") = (0, 4).\n\ncodice_16\n\nList of isolation intervals: {}.\n\nList of intervals to be processed: {(0, 2), (2, 4)}.\n\nRemove the first and process it.\n\ncodice_17\n\nList of isolation intervals: {}.\n\nList of intervals to be processed: {(0, 1), (1, 2), (2, 4)}.\n\nRemove the first and process it.\n\ncodice_18\n\nList of isolation intervals: {}.\n\nList of intervals to be processed: {(1, 2), (2, 4)}.\n\nRemove the first and process it.\n\ncodice_19\n\nList of isolation intervals: {}.\n\nList of intervals to be processed: {(1, ), (, 2), (2, 4)}.\n\nRemove the first and process it.\n\ncodice_20\n\nList of isolation intervals: {(1, )}.\n\nList of intervals to be processed: {(, 2), (2, 4)}.\n\nRemove the first and process it.\n\ncodice_21\n\nList of isolation intervals: {(1, ), (, 2)}.\n\nList of intervals to be processed: {(2, 4)}.\n\nRemove the first and process it.\n\ncodice_22\n\nList of isolation intervals: {(1, ), (, 2)}.\n\nList of intervals to be processed: ∅.\n\nFinished.\n\nTherefore, the two positive roots of the polynomial lie inside the isolation intervals and }. Each root can be approximated by (for example) bisecting the isolation interval it lies in until the difference of the endpoints is smaller than ; following this approach, the roots turn out to be and .\n\n\n"}
{"id": "46905726", "url": "https://en.wikipedia.org/wiki?curid=46905726", "title": "Yair Minsky", "text": "Yair Minsky\n\nYair Nathan Minsky (born in 1962) is an Israeli-American mathematician whose research concerns three-dimensional topology, differential geometry, group theory and holomorphic dynamics. He is a professor at Yale University. He is known for having proved Thurston's ending lamination conjecture and as a pioneer in the study of curve complex geometry.\n\nMinsky obtained his Ph.D. from Princeton University in 1989 under the supervision of William Paul Thurston, with the thesis \"Harmonic Maps and Hyperbolic Geometry\".\n\nHis Ph.D. students include Jason Behrstock, Erica Klarreich, Hossein Namazi and Kasra Rafi.\n\nHe received a Sloan Fellowship in 1995.\n\nHe was a speaker at the ICM (Madrid) 2006.\n\n\n\n\n\n"}
