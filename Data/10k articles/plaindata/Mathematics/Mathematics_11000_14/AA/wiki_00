{"id": "30533446", "url": "https://en.wikipedia.org/wiki?curid=30533446", "title": "Akhiezer's theorem", "text": "Akhiezer's theorem\n\nIn the mathematical field of complex analysis, Akhiezer's theorem is a result about entire functions proved by Naum Akhiezer.\n\nLet be an entire function of exponential type , with for real . Then the following are equivalent:\n\n\n\nwhere are the zeros of .\n\nIt is not hard to show that the Fejér–Riesz theorem is a special case.\n\n"}
{"id": "35261707", "url": "https://en.wikipedia.org/wiki?curid=35261707", "title": "Alberto González Domínguez", "text": "Alberto González Domínguez\n\nAlberto González Domínguez (11 April 1904 in Buenos Aires – 14 September 1982 in Buenos Aires) was an Argentine mathematician working on analysis, probability theory and quantum field theory.\n\nGonzález Domínguez received his Ph.D. from the University of Buenos Aires in 1939 under the direction of Julio Rey Pastor. That same year, González Domínguez received a Guggenheim Fellowship and worked for two years with Jacob Tamarkin at Brown University. González Domínguez spent most of his career as a professor at the University of Buenos Aires.\n\n"}
{"id": "2854628", "url": "https://en.wikipedia.org/wiki?curid=2854628", "title": "Ambient space", "text": "Ambient space\n\nAn ambient space or ambient configuration space is the space surrounding an object.\n\nIn mathematics, especially in geometry and topology, an \"ambient space\" is the space surrounding a mathematical object. For example, a line may be studied in isolation, or it may be studied as an object in two-dimensional space—in which case the ambient space is the plane, or as an object in three-dimensional space—in which case the ambient space is three-dimensional. To see why this makes a difference, consider the statement \"Lines that never meet are necessarily parallel.\" This is true if the ambient space is two-dimensional, but false if the ambient space is three-dimensional, because in the latter case the lines could be skew lines, rather than parallel.\n\n\n"}
{"id": "9579379", "url": "https://en.wikipedia.org/wiki?curid=9579379", "title": "Base (geometry)", "text": "Base (geometry)\n\nIn geometry, a base is a side of a polygon or a face of a polyhedron, particularly one oriented perpendicular to the direction in which height is measured, or on what is considered to be the \"bottom\" of the figure. This term is commonly applied to triangles, parallelograms, trapezoids, cylinders, cones, pyramids, parallelepipeds and frustums.\n\nBases are commonly used (together with heights) to calculate the areas and volumes of figures. In speaking about these processes, the measure (length or area) of a figure's base is often referred to as its \"base.\"\n\nBy this usage, the area of a parallelogram or the volume of a prism or cylinder can be calculated by multiplying its \"base\" by its height; likewise, the areas of triangles and the volumes of cones and pyramids are fractions of the products of their bases and heights. Some figures have two parallel bases (such as trapezoids and frustums), both of which are used to calculate the extent of the figures.\n\nThe extended base of a triangle (a particular case of an extended side) is the line that contains the base. The extended base is important in the context of obtuse triangles: the altitudes from the acute vertices are external to the triangle and perpendicularly intersect the extended opposite base (but not the base proper).\n"}
{"id": "51717116", "url": "https://en.wikipedia.org/wiki?curid=51717116", "title": "Bass–Quillen conjecture", "text": "Bass–Quillen conjecture\n\nIn mathematics, the Bass–Quillen conjecture relates vector bundles over a regular Noetherian ring \"A\" and over the polynomial ring formula_1. The conjecture is named for Hyman Bass and Daniel Quillen, who formulated the conjecture.\n\nThe conjecture is a statement about finitely generated projective modules. Such modules are also referred to as vector bundles. For a ring \"A\", the set of isomorphism classes of vector bundles over \"A\" of rank \"r\" is denoted by formula_2.\n\nThe conjecture asserts that for a regular Noetherian ring \"A\" the assignment\nyields a bijection\n\nIf \"A\" = \"k\" is a field, the Bass–Quillen conjecture asserts that any projective module over formula_5 is free. This question was raised by Jean-Pierre Serre and was later proved by Quillen and Suslin, see Quillen–Suslin theorem.\nMore generally, the conjecture was shown by Lindel in the case that \"A\" is a smooth algebra over a field \"k\".\nFurther known cases are reviewed in .\n\nThe set of isomorphism classes of vector bundles of rank \"r\" over \"A\" can also be identified with the nonabelian cohomology group\nPositive results about the homotopy invariance of \nof isotropic reductive groups \"G\" have been obtained by means of A homotopy theory.\n"}
{"id": "45554900", "url": "https://en.wikipedia.org/wiki?curid=45554900", "title": "Bhargava cube", "text": "Bhargava cube\n\nIn mathematics, in number theory, Bhargava cube (also called Bhargava's cube) is a configuration consisting of eight integers placed at the eight corners of a cube. This configuration was extensively used by Manjul Bhargava, an Indian-American Fields Medal winning mathematician, to study the composition laws of binary quadratic forms and other such forms. To each pair of opposite faces of a Bhargava cube one can associate an integer binary quadratic form thus getting three binary quadratic forms corresponding to the three pairs of opposite faces of the Bhargava cube. These three quadratic forms all have the same discriminant and Manjul Bhargava proved that their composition in the sense of Gauss is the identity element in the associated group of equivalence classes of primitive binary quadratic forms. Using this property as the starting point for a theory of composition of binary quadratic forms Manjul Bhargava went on to define fourteen different composition laws using a cube.\n\nAn expression of the form formula_1, where \"a\", \"b\" and \"c\" are fixed integers and \"x\" and \"y\" are variable integers, is called an integer binary quadratic form. The discriminant of the form is defined as \n\nThe form is said to be primitive if the coefficients \"a\", \"b\", \"c\" are relatively prime. Two forms\n\nare said to be equivalent if there exists a transformation\n\nwith integer coefficients satisfying formula_5 which transforms formula_6 to formula_7. This relation is indeed an equivalence relation in the set of integer binary quadratic forms and it preserves discriminants and primitivity.\n\nLet formula_6 and formula_7 be two primitive binary quadratic forms having the same discriminant and let the corresponding equivalence classes of forms be formula_10 and formula_11. One can find integers formula_12 such that\n\nThe class formula_17 is uniquely determined by the classes [\"Q\"(\"x\", \"y\")] and [\"Q\"(\"x\", \"y\")] and is called the composite of the classes formula_10 and formula_11. This is indicated by writing\n\nThe set of equivalence classes of primitive binary quadratic forms having a given discriminant \"D\" is a group under the composition law described above. The identity element of the group is the class determined by the following form:\nThe inverse of the class formula_22 is the class formula_23.\n\nLet (\"M\", \"N\") be the pair of 2 × 2 matrices associated with a pair of opposite sides of a Bhargava cube; the matrices are formed in such a way that their rows and columns correspond to the edges of the corresponding faces. The integer binary quadratic form associated with this pair of faces is defined as \nThe quadratic form is also defined as \nHowever, the former definition will be assumed in the sequel.\n\nLet the cube be formed by the integers \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\". The pairs of matrices associated with opposite edges are denoted by (\"M\", \"N\"), (\"M\", \"N\"), and (\"M\", \"N\"). The first rows of \"M\", \"M\" and \"M\" are respectively [\"a\" \"b\"], [\"a\" \"c\"] and [\"a\" \"e\"]. The opposite edges in the same face are the second rows. The corresponding edges in the opposite faces form the rows of the matrices \"N\", \"N\", \"N\" (see figure).\nThe quadratic form associated with the faces defined by the matrices \nformula_26 (see figure) is\nThe discriminant of a quadratic form \"Q\" is\n\nThe quadratic form associated with the faces defined by the matrices \nformula_29 (see figure) is\nThe discriminant of a quadratic form \"Q\" is\n\nThe quadratic form associated with the faces defined by the matrices \nformula_32 (see figure) is\nThe discriminant of a quadratic form \"Q\" is\n\nManjul Bhargava's surprising discovery can be summarised thus:\n\nThe three quadratic forms associated with the numerical Bhargava cube shown in the figure are computed as follows.\n\nThe composition formula_36 is the form formula_37 where formula_38 because of the following:\n\nAlso formula_42. Thus formula_43 is the identity element in the group defined by the Gauss composition.\n\nThe fact that the composition of the three binary quadratic forms associated with the Bhargava cube is the identity element in the group of such forms has been used by Manjul Bhargava to define a composition law for the cubes themselves.\n\nAn integer binary cubic in the form formula_44 can be represented by a triply symmetric Bhargava cube as in the figure. The law of composition of cubes can be used to define a law of composition for the binary cubic forms.\n\nThe pair of binary quadratic forms formula_45 can be represented by a doubly symmetric Bhargava cube as in the figure. The law of composition of cubes is now used to define a composition law on pairs of binary quadratic forms.\n"}
{"id": "2496210", "url": "https://en.wikipedia.org/wiki?curid=2496210", "title": "Bicoherence", "text": "Bicoherence\n\nIn mathematics, in the area of statistical analysis, bicoherence is a squared normalised version of the bispectrum. The bicoherence takes values bounded between 0 and 1, which make it a convenient measure for quantifying the extent of phase coupling in a signal. It is also known as bispectral coherency. The prefix \"bi-\" in \"bispectrum\" and \"bicoherence\" refers not to two time series \"x\", \"y\" but rather to two frequencies of a single signal.\n\nThe \"bispectrum\" is a statistic used to search for nonlinear interactions. The Fourier transform of the second-order cumulant, i.e., the autocorrelation function, is the traditional power spectrum. The Fourier transform of C(t,t) (third-order cumulant) is called bispectrum or bispectral density. They fall in the category of \"Higher Order Spectra\", or \"Polyspectra\" and provide supplementary information to the power spectrum. The third order polyspectrum (bispectrum) is the easiest to compute, and hence the most popular.\n\nThe difference with measuring \"coherence\" (coherence analysis is an extensively used method to study the correlations in frequency domain, between two simultaneously measured signals) is the need for both input and output measurements by estimating two auto-spectra and one cross spectrum. On the other hand, bicoherence is an auto-quantity, i.e. it can be computed from a single signal. The coherence function provides a quantification of deviations from linearity in the system which lies between the input and output measurement sensors. The bicoherence measures the proportion of the signal energy at any bifrequency that is quadratically phase coupled. It is usually normalized in the range similar to correlation coefficient and classical (second order) coherence. It was also used for depth of anasthesia assesement and widely in plasma physics (nonlinear energy transfer) and also for detection of gravitation waves.\n\nBispectrum and bicoherence may be applied to the case of non-linear interactions of a continuous spectrum of propagating waves in one dimension .\n\nBicoherence measurements have been carried out for EEG signals monitoring in sleep, wakefulness and seizures.\n\nThe bispectrum is defined as the triple product\nwhere formula_2 is the bispectrum evaluated at frequencies formula_3 and formula_4, formula_5 is the Fourier transform of the signal, and formula_6 denotes the complex conjugate. The Fourier transform is a complex quantity, and so is the bispectrum. From complex multiplication, the magnitude of the bispectrum is equal to the product of the magnitudes of each of the frequency components, and the phase of the bispectrum is the sum of the phases of each of the frequency components.\n\nSuppose that the three Fourier components formula_7, formula_8 and formula_9 were perfectly phase locked. Then if the Fourier transform was calculated several times from different parts of the time series, the bispectrum will always have the same value. If we add together all of the bispectra, they will sum without cancelling. On the other hand, suppose that the phases of each of these frequencies was random. Then, the bispectrum will have the same magnitude (assuming that the magnitude of the frequency components is the same) but the phase will be randomly oriented. Adding together all of the bispectra will result in cancellation, because of the random phase orientation, and so the sum of the bispectra will have a small magnitude. Detecting phase coupling requires summation over a number of independent samples- this is the first motivation for defining the bicoherence. Secondly, the bispectrum is not normalized, because it still depends on the magnitudes of each of the frequency components. The bicoherence includes a normalization factor that removes the magnitude dependence.\n\nThere is some inconsistency with the definition of the bicoherence normalization constant. Some of the definitions that have been used are\nwhich was provided in Sigl and Chamoun 1994, but does not appear to be correctly normalized. Alternatively, plasma physics typically uses\n\nThe numerator contains the magnitude of the bispectrum summed over all of the time series segments. This quantity is large if there is phase coupling, and approaches 0 in the limit of random phases. The denominator, which normalizes the bispectrum, is given by calculating the bispectrum after setting all of the phases to 0. This corresponds to the case where there is perfect phase coupling, because all of the samples have zero phase. Therefore, the bicoherence has a value between 0 (random phases) and 1 (total phase coupling).\n\n"}
{"id": "1964121", "url": "https://en.wikipedia.org/wiki?curid=1964121", "title": "Calculator input methods", "text": "Calculator input methods\n\nThere are various ways in which calculators interpret keystrokes. These can be categorized into two main types:\n\n\nThe immediate execution mode of operation (also known as single-step, algebraic entry system (AES) or chain calculation mode) is commonly employed on most general-purpose calculators. In most simple four-function calculators, such as the Windows calculator in Standard mode and those included with most early operating systems, each binary operation is executed as soon as the next operator is pressed, and therefore the order of operations in a mathematical expression is not taken into account. Scientific calculators, including the Scientific mode in the Windows calculator and most modern software calculators, have buttons for brackets and \"can\" take order of operation into account. Also, for unary operations, like √ or \"x\", the number is entered first, then the operator; this is largely because the display screens on these kinds of calculators are generally composed entirely of seven-segment characters and thus capable of displaying only numbers, not the functions associated with them. This mode of operation also make it impossible to change the expression being input without clearing the display entirely.\n\nThe first and third example have been given twice. The first version is for simple calculators, showing how it is necessary to rearrange operands in order to get the correct result. The second version is for scientific calculators, where operator precedence is observed.\n\nImmediate-execution calculators are based on a mixture of infix and postfix notation: binary operations are done as infix, but unary operations are postfix. Because operators are applied one-at-a-time, the user must work out which operator key to use at each stage, and this can lead to problems. When discussing these problems, Professor Harold Thimbleby has pointed out that button-operated calculators \"require numbers and operation signs to be punched in a certain order, and mistakes are easy to make and hard to spot\".\n\nProblems can occur because, for anything but the simplest calculation, in order to work out the value of a written formula, the user of a button-operated calculator is required to:\n\nMistakes can be hard to spot because:\n\nThe simplest example given by Professor Thimbleby of a possible problem when using an immediate-execution calculator is 4 × (−5). As a written formula the value of this is −20 because the minus sign is intended to indicate a negative number, rather than a subtraction, and this is the way that it would be interpreted by a formula calculator.\n\nOn an immediate-execution calculator, depending on which keys are used and the order in which they are pressed, the result for this calculation may be different. Also there are differences between calculators in the way a given sequence of button presses is interpreted. The result can be:\n\nThe effects of operator precedence, parentheses and non-commutative operators, on the sequence of button presses, are illustrated by:\n\nThese are only simple examples, but immediate-execution calculators can present even greater problems in more complex cases. In fact, Professor Thimbleby claims that users may have been conditioned to avoid them for all but the simplest calculations.\n\nThe potential problems with immediate-execution calculators stem from the fact that they are imperative. This means that the user must provide details of \"how\" the calculation has to be performed.\n\nProfessor Thimbleby has identified the need for a calculator that is more automatic and therefore easier to use, and he states that such a calculator should be more declarative. This means that the user should be able to specify only \"what\" has to be done, not how, and in which order, it has to be done.\n\nFormula calculators are more declarative because the typed-in formula specifies what is to be done, and the user does not have to provide any details of the step-by-step order in which the calculation has to be performed.\n\nDeclarative solutions are easier to understand than imperative solutions, and there has been a long-term trend from imperative to declarative methods. Formula calculators are part of this trend.\n\nMany software tools for the general user, such as spreadsheets, are declarative. Formula calculators are examples of such tools.\n\nSoftware calculators that simulate hand-held, immediate execution calculators do not use the full power of the computer: \"A computer is a far more powerful device than a hand-held calculator, and thus it is illogical and limiting to duplicate hand-held calculators on a computer.\" (Haxial Software Pty Ltd) Formula calculators use more of the computer's power because, besides calculating the value of a formula, they work out the order in which things should be done.\n\nInfix notation is a method where unary operations are input into the calculator in the same order as they are written on paper. Different forms of this input scheme exist. In the algebraic entry system with hierarchy (AESH), the precedence of basic mathematical operators is taken into account, whereas calculators with algebraic entry system with parentheses (AESP) support the entry of parentheses. An input scheme known as algebraic operating system (AOS) combines both.\n\nCalculators that use infix notation tend to incorporate a dot-matrix display to display the expression being entered, frequently accompanied by a seven-segment display for the result of the expression. Because the expression is not evaluated until it is fully entered, there is provision for editing the entered expression at any point prior to evaluation, as well as replaying entered expressions and their answers from memory.\n\nMost graphing calculators by Casio and Texas Instruments use this method. On its scientific calculators, Sharp calls this method Direct Algebraic Logic (D.A.L.), and Casio calls this method the Visually Perfect Algebraic Method (V.P.A.M.).\n\nIn reverse Polish notation, also known as postfix notation, all operations are entered after the operands on which the operation is performed. Reverse Polish notation is parenthesis-free, which usually leads to fewer button presses needed to perform an operation. By the use of a stack, one can enter formulas without the need to rearrange operands.\n\nHewlett-Packard's calculators are well-known examples among calculators which use RPN. Early models, such as the HP-35, used RPN entirely without any alternative methods. Later models, such as the HP 35s, also had infix notation and could conveniently allow users to switch between it and RPN.\n\nNote: The first example illustrates one of the few cases where reverse Polish notation does not use the fewest button presses – provided one does not rearrange operands. If one would do so then only six keystrokes would be needed.\n\nBASIC notation is a particular implementation of infix notation where functions require their parameters to be in brackets.\n\nThis method was used from the 1980s to the 1990s in BASIC programmable calculators and pocket computers. Texas Instruments would later implement the method in many of its graphing calculators, including the TI-83 and TI-84 Plus series. Most computer algebra systems also use this as the default input method.\n\nIn BASIC notation, the formula is entered as it would be entered in BASIC, using the codice_1 command – the codice_1 command itself being optional. On pressing \"ENTER\" or \"=\", the result would be displayed. As with standard infix notation, typing mistakes in the entered formula could be corrected using the same editor function as the one used when programming the calculator.\n\nFor the second example, two options are given depending on if the BASIC programmable pocket computers have dedicated trigonometric keys or not.\n\nThe ten-key notation input method first became popular with accountants' paper tape adding machines. It generally makes the assumption that entered numbers are being summed, although other operations are supported. Each number entered is followed by its sign (+/−), and a running total is kept. An assumption is made that the last operand can be implicitly used next, so by just entering another + (for example), one will reuse the most recent operand. Ten key input mode is available in printing calculators from companies such as Sharp, and in software calculators like Judy's TenKey used by accounting firms. Online tenkey training and certification tools are available as well, and some businesses use ten key typing speed as an employment criterion.\n\nModern computer algebra systems, as well as many scientific and graphing calculators, allow for \"pretty-printing\", that is, entry of equations such that fractions, surds and integrals, etc. are displayed in the way they would normally be written. Such calculators are generally similar in appearance to those using infix notation, but feature a full dot-matrix display and templates for entering expressions, which are navigated using arrow keys on the calculator. The templates contain spaces for values or expressions to be entered, and empty values would typically result in a syntax error, making it more cumbersome to navigate than standard infix notation; standard infix notation is often an option on such calculators as well.\n\nCasio used to call this feature \"Natural Display\" or \"Natural textbook display\", but now uses \"Natural-VPAM\". Sharp calls this \"WriteView\" on its scientific calculators and simply \"Equation Editor\" on its graphing calculators. HP calls this its \"Textbook\" display setting, which can be used in both RPN and Algebraic mode and in both the \"Stack\" and in the \"Equation Writer\" application. Mathematica calls this \"Semantic-Faithful Typesetting\". Mathcad calls this \"standard math notation\". Maple has a \"Math Equation Editor\", but does not have a special name for this input method. Texas Instruments calls it \"MathPrint,\" incorporating it in its high-end calculators, such as the TI-Nspire series, and in 2011 added the feature to its TI-84 series with the 2.55 OS update.\nFor the second example, two options are given, depending on whether the calculators will automatically insert needed parentheses or not. Machines equipped with an alphanumeric display will display SIN(30)×COS(30) before is pressed.\n\n"}
{"id": "3422372", "url": "https://en.wikipedia.org/wiki?curid=3422372", "title": "Carol number", "text": "Carol number\n\nA Carol number is an integer of the form formula_1. An equivalent formula is formula_2. The first few Carol numbers are: −1, 7, 47, 223, 959, 3967, 16127, 65023, 261119, 1046527 . \n\nCarol numbers were first studied by Cletus Emmanuel, who named them after a friend, Carol G. Kirnon.\n\nFor \"n\" > 2, the binary representation of the \"n\"-th Carol number is \"n\" − 2 consecutive ones, a single zero in the middle, and \"n\" + 1 more consecutive ones, or to put it algebraically,\n\nSo, for example, 47 is 101111 in binary, 223 is 11011111, etc. The difference between the 2\"n\"-th Mersenne number and the \"n\"-th Carol number is formula_4. This gives yet another equivalent expression for Carol numbers, formula_5. The difference between the \"n\"-th Kynea number and the \"n\"-th Carol number is the (\"n\" + 2)th power of two.\n\nStarting with 7, every third Carol number is a multiple of 7. Thus, for a Carol number to also be a prime number, its index \"n\" cannot be of the form 3\"x\" + 2 for \"x\" > 0. The first few Carol numbers that are also prime are 7, 47, 223, 3967, 16127 (these are listed in Sloane's ).\n\nThe 7th Carol number and 5th Carol prime, 16127, is also a prime when its digits are reversed, so it is the smallest Carol emirp. The 12th Carol number and 7th Carol prime, 16769023, is also a Carol emirp.\n\n, the largest known prime Carol number has index \"n\" = 695631, which has 418812 digits. It was found by Mark Rodenkirch in July 2016 using the programs CKSieve and PrimeFormGW. It is the 44th Carol prime.\n\nA generalized Carol number base \"b\" is defined to be a number of the form (\"b\"−1) − 2 with \"n\" ≥ 1, a generalized Carol number base \"b\" can be prime only if \"b\" is even, since if \"b\" is odd, then all generalized Carol numbers base \"b\" are even and thus not prime. A generalized Carol number to base \"b\" is also a generalized Carol number to base \"b\".\n\nLeast \"n\" ≥ 1 such that ((2\"b\")−1) − 2 is prime are\n\n, the largest known generalized Carol prime is (2−1) − 2.\n\n"}
{"id": "54981574", "url": "https://en.wikipedia.org/wiki?curid=54981574", "title": "Configuration space (mathematics)", "text": "Configuration space (mathematics)\n\nIn mathematics, a configuration space (also known as Fadell's configuration space) is a construction closely related to state spaces or phase spaces in physics. In physics, these are used to describe the state of a whole system as a single point in a high-dimensional space. In mathematics, they are used to describe assignments of a collection of points to positions in a topological space. More specifically, configuration spaces in mathematics are particular examples of configuration spaces in physics in the particular case of several non-colliding particles.\n\nFor a topological space formula_1, the \"n\" (ordered) configuration space of X is the set of \"n\"-tuples of pairwise distinct points in formula_1:\n\nThis space is generally endowed with the subspace topology from the inclusion of formula_4 into formula_5. It is also sometimes denoted formula_6, formula_7, or formula_8.\n\nThere is a natural action of the symmetric group formula_9 on the points in formula_4 given by\n\nThis action gives rise to the unordered configuration space of ,\n\nwhich is the orbit space of that action. The intuition is that this action \"forgets the names of the points\". The unordered configuration space is sometimes denoted formula_13. The collection of unordered configuration spaces over all formula_14 is the Ran space, and comes with a natural topology.\n\nFor a topological space formula_1 and a finite set formula_16, the configuration space of with particles labeled by is\n\nFor formula_18, define formula_19. Then the configuration space of \"X\" is formula_20, and is denoted simply formula_4.\n\n\nThe -strand braid group on a connected topological space is \nthe fundamental group of the unordered configuration space of . The -strand pure braid group on is\n\nThe first studied braid groups were the Artin braid groups formula_28. While the above definition is not the one that Emil Artin gave, Adolf Hurwitz implicitly defined the Artin braid groups as fundamental groups of configuration spaces of the complex plane considerably before Artin's definition (in 1891).\n\nIt follows from this definition and the fact that formula_29 and formula_30 are Eilenberg–MacLane spaces of type formula_31 that the unordered configuration space of the plane formula_30 is the classifying space of the Artin braid group, and formula_29 is the classifying space of the pure Artin braid group, when both are considered as discrete groups.\n\nIf the original space is a manifold, the configuration space of \"distinct,\" unordered points is also a manifold, while the configuration space of \"not necessarily distinct\" unordered points is instead an orbifold.\n\nA configuration space is a type of classifying space or (fine) moduli space. In particular, there is a universal bundle formula_34 which is a sub-bundle of the trivial bundle formula_35, and which has the property that the fiber over each point formula_36 is the \"n\" element subset of formula_37 classified by \"p\".\n\nThe homotopy type of configuration spaces is not homotopy invariant – for example, the spaces formula_38 are not homotopy equivalent for any two distinct values of formula_39. For instance, formula_40 is not connected, formula_41 is an Eilenberg–MacLane space of type formula_31, and formula_38 is simply connected for formula_44.\n\nIt used to be an open question whether there were examples of \"compact\" manifolds which were homotopy equivalent but had non-homotopy equivalent configuration spaces: such an example was found only in 2005 by Riccardo Longoni and Paolo Salvatore. Their example are two three-dimensional lens spaces, and the configuration spaces of at least two points in them. That these configuration spaces are not homotopy equivalent was detected by Massey products in their respective universal covers. Homotopy invariance for configuration spaces of simply connected closed manifolds remains open in general, and has been proved to hold over the base field formula_45.\n\nSome results are particular to configuration spaces of graphs. Robert Ghrist motivates these problems as related to robotics and motion planning: one can imagine placing several robots on tracks and trying to navigate them to different positions without collision. The tracks correspond to (the edges of) a graph, the robots correspond to particles, and successful navigation corresponds to a path in the configuration space of that graph.\n\nFor any graph formula_46, formula_47 is an Eilenberg–MacLane spaces of type formula_31 and strong deformation retracts to a subspace of dimension formula_49, where formula_49 is the number of vertices of degree at least 3. Moreover, formula_51 and formula_47 deformation retract to non-positively curved cubical complexes of dimension at most formula_53.\n\n"}
{"id": "39395816", "url": "https://en.wikipedia.org/wiki?curid=39395816", "title": "Connection (composite bundle)", "text": "Connection (composite bundle)\n\nComposite bundles formula_1 play a prominent role in gauge theory with symmetry breaking, e.g., gauge gravitation theory, non-autonomous mechanics where formula_2 is the time axis, e.g., mechanics with time-dependent parameters, and so on. There are the important relations between connections on fiber bundles formula_3, formula_4 and formula_5.\n\nIn differential geometry by a composite bundle is meant the composition\n\nof fiber bundles\n\nIt is provided with bundle coordinates formula_8, where formula_9 are bundle coordinates on a fiber bundle formula_5, i.e., transition functions of coordinates formula_11 are independent of coordinates formula_12.\n\nThe following fact provides the above mentioned physical applications of composite bundles. Given the composite bundle (1), let formula_13 be a global section\nof a fiber bundle formula_5, if any. Then the pullback bundle \nformula_15 over formula_16 is a subbundle of a fiber bundle formula_3.\n\nFor instance, let formula_18 be a principal bundle with a structure Lie group formula_19 which is reducible to its closed subgroup formula_20. There is a composite bundle formula_21 where formula_22 is a principal bundle with a structure group formula_20 and formula_24 is a fiber bundle associated with formula_18. Given a global section formula_13 of formula_24, the pullback bundle formula_28 is a reduced principal subbundle of formula_29 with a structure group formula_20. In gauge theory, sections of formula_24 are treated as classical Higgs fields.\n\nGiven the composite bundle formula_32 (1), let us consider the jet manifolds formula_33, formula_34, and formula_35 of the fiber bundles formula_5, formula_4, and formula_3, respectively. They are provided with the adapted coordinates formula_39, formula_40, and formula_41\n\nThere is the canonical map\n\nThis canonical map defines the relations between connections on fiber bundles formula_3, formula_44 and formula_5. These connections are given by the corresponding tangent-valued connection forms\n\nA connection formula_49 on a fiber bundle formula_44\nand a connection formula_51 on a fiber bundle formula_52 define a connection\n\non a composite bundle formula_3. It is called the composite connection. This is a unique connection such that the horizontal lift formula_55 onto formula_56 of a vector field formula_57 on formula_16 by means of the composite connection formula_59 coincides with the composition formula_60 of horizontal lifts of formula_57 onto formula_62 by means of a connection formula_51 and then onto formula_56 by means of a connection formula_49.\n\nGiven the composite bundle formula_56 (1), there is the following exact sequence of vector bundles over formula_56:\n\nwhere formula_69 and formula_70 are the vertical tangent bundle and the vertical cotangent bundle of formula_44. Every connection formula_49 on a fiber bundle formula_44 yields the splitting\n\nof the exact sequence (2). Using this splitting, one can construct a first order differential operator\n\non a composite bundle formula_3. It is called the vertical covariant differential.\nIt possesses the following important property.\n\nLet formula_13 be a section of a fiber bundle formula_5, and let formula_79 be the pullback bundle over formula_16. Every connection formula_49 induces the pullback connection\n\non formula_83. Then the restriction of a vertical covariant differential formula_84 to formula_85 coincides with the familiar covariant differential formula_86\non formula_83 relative to the pullback connection formula_88.\n\n\n\n"}
{"id": "6239", "url": "https://en.wikipedia.org/wiki?curid=6239", "title": "Contraction mapping", "text": "Contraction mapping\n\nIn mathematics, a contraction mapping, or contraction or contractor, on a metric space \"(M,d)\" is a function \"f\" from \"M\" to itself, with the property that there is some nonnegative real number formula_1 such that for all \"x\" and \"y\" in \"M\",\nThe smallest such value of \"k\" is called the Lipschitz constant of \"f\". Contractive maps are sometimes called Lipschitzian maps. If the above condition is instead satisfied for\n\"k\" ≤ 1, then the mapping is said to be a non-expansive map.\n\nMore generally, the idea of a contractive mapping can be defined for maps between metric spaces. Thus, if (\"M\",\"d\") and (\"N\",\"d\"') are two metric spaces, then formula_3 is a contractive mapping if there is a constant formula_4 such that\nfor all \"x\" and \"y\" in \"M\".\n\nEvery contraction mapping is Lipschitz continuous and hence uniformly continuous (for a Lipschitz continuous function, the constant \"k\" is no longer necessarily less than 1).\n\nA contraction mapping has at most one fixed point. Moreover, the Banach fixed-point theorem states that every contraction mapping on a nonempty complete metric space has a unique fixed point, and that for any \"x\" in \"M\" the iterated function sequence \"x\", \"f\" (\"x\"), \"f\" (\"f\" (\"x\")), \"f\" (\"f\" (\"f\" (\"x\"))), ... converges to the fixed point. This concept is very useful for iterated function systems where contraction mappings are often used. Banach's fixed-point theorem is also applied in proving the existence of solutions of ordinary differential equations, and is used in one proof of the inverse function theorem.\n\nContraction mapping plays an important role in dynamic programming problems.\n\nA non-expansive mapping with formula_6 can be strengthened to a firmly non-expansive mapping in a Hilbert space \"H\" if the following holds for all \"x\" and \"y\" in \"H\":\nwhere\n\nThis is a special case of formula_9 averaged nonexpansive operators with formula_10. A firmly non-expansive mapping is always non-expansive, via the Cauchy–Schwarz inequality.\n\nA subcontraction map or subcontractor is a map \"f\" on a metric space (\"M\",\"d\") such that\n\nIf the image of a subcontractor \"f\" is compact, then \"f\" has a fixed point.\n\nIn a locally convex space (\"E,P\") with topology given by a set \"P\" of seminorms, one can define for any \"p\" ∈ \"P\" a \"p\"-contraction as a map \"f\" such that there is some \"k\" < 1 such that \"p\"(\"f\"(\"x\") - \"f\"(\"y\")) ≤ \"k p\"(\"x - y\"). If \"f\" is a \"p\"-contraction for all \"p\" ∈ \"P\" and (\"E,P\") is sequentially complete, then \"f\" has a fixed point, given as limit of any sequence \"x = f\"(\"x\"), and if (\"E,P\") is Hausdorff, then the fixed point is unique.\n\n\n"}
{"id": "37709899", "url": "https://en.wikipedia.org/wiki?curid=37709899", "title": "Convergent matrix", "text": "Convergent matrix\n\nIn numerical linear algebra, a convergent matrix is a matrix that converges to the zero matrix under matrix exponentiation.\n\nWhen successive powers of a matrix T become small (that is, when all of the entries of T approach zero, upon raising T to successive powers), the matrix T converges to the zero matrix. A regular splitting of a non-singular matrix A results in a convergent matrix T. A semi-convergent splitting of a matrix A results in a semi-convergent matrix T. A general iterative method converges for every initial vector if T is convergent, and under certain conditions if T is semi-convergent.\n\nWe call an \"n\" × \"n\" matrix T a convergent matrix if\n\nfor each \"i\" = 1, 2, ..., \"n\" and \"j\" = 1, 2, ..., \"n\".\n\nLet\nComputing successive powers of T, we obtain\nand, in general,\nSince\nand\nT is a convergent matrix. Note that \"ρ\"(T) = , where \"ρ\"(T) represents the spectral radius of T, since is the only eigenvalue of T.\n\nLet T be an \"n\" × \"n\" matrix. The following properties are equivalent to T being a convergent matrix:\n\nA general iterative method involves a process that converts the system of linear equations\n\ninto an equivalent system of the form\n\nfor some matrix T and vector c. After the initial vector x is selected, the sequence of approximate solution vectors is generated by computing\n\nfor each \"k\" ≥ 0. For any initial vector x ∈ formula_11, the sequence formula_12 defined by (), for each \"k\" ≥ 0 and c ≠ 0, converges to the unique solution of () if and only if \"ρ\"(T) < 1, that is, T is a convergent matrix.\n\nA matrix splitting is an expression which represents a given matrix as a sum or difference of matrices. In the system of linear equations () above, with A non-singular, the matrix A can be split, that is, written as a difference\n\nso that () can be re-written as () above. The expression () is a regular splitting of A if and only if B ≥ 0 and C ≥ 0, that is, and C have only nonnegative entries. If the splitting () is a regular splitting of the matrix A and A ≥ 0, then \"ρ\"(T) < 1 and T is a convergent matrix. Hence the method () converges.\n\nWe call an \"n\" × \"n\" matrix T a semi-convergent matrix if the limit\n\nexists. If A is possibly singular but () is consistent, that is, b is in the range of A, then the sequence defined by () converges to a solution to () for every x ∈ formula_11 if and only if T is semi-convergent. In this case, the splitting () is called a semi-convergent splitting of A.\n\n\n"}
{"id": "6498435", "url": "https://en.wikipedia.org/wiki?curid=6498435", "title": "Correspondence problem", "text": "Correspondence problem\n\nThe correspondence problem refers to the problem of ascertaining which parts of one image correspond to which parts of another image, where differences are due to movement of the camera, the elapse of time, and/or movement of objects in the photos.\n\nGiven two or more images of the same 3D scene, taken from different points of view, the correspondence problem refers to the task of finding a set of points in one image which can be identified as the same points in another image. To do this, points or features in one image are matched with the corresponding points or features in another image. The images can be taken from a different point of view, at different times, or with objects in the scene in general motion relative to the camera(s).\n\nThe correspondence problem can occur in a stereo situation when two images of the same scene are used, or can be generalised to the N-view correspondence problem. In the latter case, the images may come from either N different cameras photographing at the same time or from one camera which is moving relative to the scene. The problem is made more difficult when the objects in the scene are in motion relative to the camera(s).\n\nA typical application of the correspondence problem occurs in panorama creation or image stitching — when two or more images which only have a small overlap are to be stitched into a larger composite image. In this case it is necessary to be able to identify a set of corresponding points in a pair of images in order to calculate the transformation of one image to stitch it onto the other image.\n\nThere are two basic ways to find the correspondences between two images.\n\nCorrelation-based – checking if one location in one image looks/seems like another in another image.\n\nFeature-based – finding features in the image and seeing if the layout of a subset of features is similar in the two images. To avoid the aperture problem a good feature should have local variation in two directions.\n\nIn computer vision the correspondence problem is studied for the case when a computer should solve it automatically with only images as input. Once the correspondence problem has been solved, resulting in a set of image points which are in correspondence, other methods can be applied to this set to reconstruct the position, motion and/or rotation of the corresponding 3D points in the scene.\n\nThe correspondence problem is also the basis of the particle image velocimetry measurement technique, which is nowadays widely used in the fluid mechanics field to quantitatively measure fluid motion.\n\nTo find the correspondence between set A [1,2,3,4,5] and set B [3,4,5,6,7] find where they overlap and how far off one set is from the other. Here we see that the last three numbers in set A correspond with the first three numbers in set B. This shows that B is offset 2 to the left of A.\n\nA simple method is to compare small patches between rectified images. This works best with images taken with roughly the same point of view and either at the same time or with little to no movement of the scene between image captures, such as stereo images.\n\nA small window is passed over a number of positions in one image. Each position is checked to see how well it compares with the same location in the other image. Several nearby locations are compared for objects in one image which may not be at exactly the same image-location in the other image. It is possible that there is no fit that is good enough. This may mean that the feature is not present in both images, it has moved farther than your search accounted for, it has changed too much, or is being hidden by other parts of the image.\n\n\n"}
{"id": "6166234", "url": "https://en.wikipedia.org/wiki?curid=6166234", "title": "Cryptomorphism", "text": "Cryptomorphism\n\nIn mathematics, two objects, especially systems of axioms or semantics for them, are called cryptomorphic if they are equivalent but not obviously equivalent. This word is a play on the many morphisms in mathematics, but \"cryptomorphism\" is only very distantly related to \"isomorphism\", \"homomorphism\", or \"morphisms\". The equivalence may possibly be in some informal sense, or may be formalized in terms of a bijection or equivalence of categories between the mathematical objects defined by the two cryptomorphic axiom systems.\n\nThe word was coined by Garrett Birkhoff before 1967, for use in the third edition of his book \"Lattice Theory\". Birkhoff did not give it a formal definition, though others working in the field have made some attempts since.\n\nIts informal sense was popularized (and greatly expanded in scope) by Gian-Carlo Rota in the context of matroid theory: there are dozens of equivalent axiomatic approaches to matroids, but two different systems of axioms often look very different. \n\nIn his 1997 book \"Indiscrete Thoughts\", Rota describes the situation as follows:\n\nThough there are many cryptomorphic concepts in mathematics outside of matroid theory and universal algebra, the word has not caught on among mathematicians generally. It is, however, in fairly wide use among researchers in matroid theory.\n\n\n"}
{"id": "50740226", "url": "https://en.wikipedia.org/wiki?curid=50740226", "title": "Cubane-type cluster", "text": "Cubane-type cluster\n\nA cubane-type cluster is an arrangement of atoms in a molecular structure that forms a cube. In the idealized case, the eight vertices are symmetry equivalent and the species has O symmetry. Such a structure is illustrated by the hydrocarbon cubane. With chemical formula CH, cubane has carbon atoms at the corners of a cube and covalent bonds forming the edges. Most cubanes have more complicated structures, usually with nonequivalent vertices. They may be simple covalent compounds or macromolecular or supramolecular cluster compounds. \n\nOther compounds having different elements in the corners, various atoms or groups bonded to the corners are all part of this class of structures. \n\nCubane clusters are common throughout bioinorganic chemistry. Ferredoxins containing [FeS] iron–sulfur clusters are pervasive in nature. The four iron atoms and four sulfur atoms form an alternating arrangement at the corners. The whole cluster is typically anchored by coordination of the iron atoms, usually with cysteine residues. In this way, each Fe center achieves tetrahedral coordination geometry. Some [FeS] clusters arise via dimerization of square-shaped [FeS] precursors. Many synthetic analogues are known including heterometallic derivatives.\n\nSeveral alkyllithium compounds exist as clusters in solution, typically tetramers, with the formula [RLi]. The individual RLi molecules are not observed. The four lithium atoms and the carbon from each alkyl group bonded to them occupy alternating vertices of the cube, with the additional atoms of the alkyl groups projecting off their respective corners.\n\nOctaazacubane is a hypothetical allotrope of nitrogen with formula N; the nitrogen atoms are the corners of the cube. Like the carbon-based cubane compounds, octaazacubane is predicted to be highly unstable due to angle strain at the corners, and it also does not enjoy the kinetic stability seen for its organic analogues.\n"}
{"id": "15718146", "url": "https://en.wikipedia.org/wiki?curid=15718146", "title": "Descriptional Complexity of Formal Systems", "text": "Descriptional Complexity of Formal Systems\n\nDCFS, the International Workshop on Descriptional Complexity of Formal Systems is an annual academic conference in the \nfield of computer science.\n\nBeginning with the 2011 edition, the proceedings of the workshop appear in the series Lecture Notes in Computer Science. Already since the very beginning, extended versions of selected papers are published as special issues of the International Journal of Foundations of Computer Science, the Journal of Automata, Languages and Combinatorics, of Theoretical Computer Science, and of Information and Computation In 2002 DCFS was the result of the merger of the workshops DCAGRS (Descriptional Complexity of Automata, Grammars and Related Structures) and FDSR (Formal Descriptions and Software Reliability). The workshop is often collocated with international conferences in related fields, such as ICALP, DLT and CIAA.\n\nTypical topics include:\nAs such, the topics of the conference overlap with those of the International Federation for Information Processing Working Group 1.2 on descriptional complexity.\n\nIn a survey on descriptional complexity, state that \"since more than a decade the Workshop on 'Descriptional Complexity of Formal Systems' (DCFS), [...] has contributed substantially to the development of [its] field of research.\" In a talk on the occasion of the 10th anniversary of the workshop, gave an overview about trends and directions in research papers presented at DCFS.\n\nSince 2006, the Chair of the Steering Committee of the DCFS workshop series is Giovanni Pighizzini.\n\nBasic information on each DCFS event, as well as on its precursors, DCAGRS and FSDR, is included in the following table.\n\n\n\n\n"}
{"id": "9256", "url": "https://en.wikipedia.org/wiki?curid=9256", "title": "Enigma machine", "text": "Enigma machine\n\nThe Enigma machines are a series of electro-mechanical rotor cipher machines mainly developed and used in the early- to mid-20th century to protect commercial, diplomatic and military communication. Enigma was invented by the German engineer Arthur Scherbius at the end of World War I. Early models were used commercially from the early 1920s, and adopted by military and government services of several countries, most notably Nazi Germany before and during World War II. Several different Enigma models were produced, but the German military models, having a plugboard, were the most complex. Japanese and Italian models were also in use.\n\nAround December 1932, Marian Rejewski, a Polish mathematician and cryptanalyst, while working at the Polish Cipher Bureau, used the theory of permutations and flaws in the German military message encipherment procedures to break the message keys of the plugboard Enigma machine. Rejewski achieved this result without knowledge of the wiring of the machine, so the result did not allow the Poles to decrypt actual messages. The French spy Hans-Thilo Schmidt obtained access to German cipher materials that included the daily keys used in September and October 1932. Those keys included the plugboard settings. The French passed the material to the Poles, and Rejewski used some of that material and the message traffic in September and October to solve for the unknown rotor wiring. Consequently, the Polish mathematicians were able to build their own Enigma machines, which were called Enigma doubles. Rejewski was aided by cryptanalysts Jerzy Różycki and Henryk Zygalski, both of whom had been recruited with Rejewski from Poznań University. The Polish Cipher Bureau developed techniques to defeat the plugboard and find all components of the daily key, which enabled the Cipher Bureau to read the German Enigma messages. Over time, the German cryptographic procedures improved, and the Cipher Bureau developed techniques and designed mechanical devices to continue reading the Enigma traffic. As part of that effort, the Poles exploited quirks of the rotors, compiled catalogues, built a cyclometer to help make a catalogue with 100,000 entries, made Zygalski sheets and built the electro-mechanical cryptologic bomb to search for rotor settings. In 1938, the Germans added complexity to the Enigma machines that finally became too expensive for the Poles to counter. The Poles had six \"bomby\", but when the Germans added two more rotors, ten times as many \"bomby\" were needed, and the Poles did not have the resources.\n\nOn 26 and 27 July 1939, in Pyry near Warsaw, the Poles initiated French and British military intelligence representatives into their Enigma-decryption techniques and equipment, including Zygalski sheets and the cryptologic bomb, and promised each delegation a Polish-reconstructed Enigma. The demonstration represented a vital basis for the later British continuation and effort. During the war, British cryptologists decrypted a vast number of messages enciphered on Enigma. The intelligence gleaned from this source, codenamed \"Ultra\" by the British, was a substantial aid to the Allied war effort.\n\nThough Enigma had some cryptographic weaknesses, in practice it was German procedural flaws, operator mistakes, failure to systematically introduce changes in encipherment procedures, and Allied capture of key tables and hardware that, during the war, enabled Allied cryptologists to succeed and \"turned the tide\" in the Allies' favour.\nThe German firm Scherbius & Ritter, co-founded by Arthur Scherbius, patented ideas for a cipher machine in 1918 and began marketing the finished product under the brand name \"Enigma\" in 1923, initially targeted at commercial markets. With its adoption (in slightly modified form) by the German Navy in 1926 and the German Army and Air Force soon after, the name \"Enigma\" became widely known in military circles.\n\nThe word \"enigma\" is a Latin word, derived from the Ancient Greek word enigma (αίνιγμα) used in English, but not native German.\n\nLike other rotor machines, the Enigma machine is a combination of mechanical and electrical subsystems. The mechanical subsystem consists of a keyboard; a set of rotating disks called \"rotors\" arranged adjacently along a spindle; one of various stepping components to turn at least one rotor with each key press, and a series of lamps, one for each letter.\n\nThe mechanical parts act in such a way as to form a varying electrical circuit. When a key is pressed, one or more rotors rotate on the spindle. On the sides of the rotors are a series of electrical contacts that, after rotation, line up with contacts on the other rotors or fixed wiring on either end of the spindle. When the rotors are properly aligned, each key on the keyboard is connected to a unique electrical pathway through the series of contacts and internal wiring. Current, typically from a battery, flows through the pressed key, into the newly configured set of circuits and back out again, ultimately lighting one display lamp, which shows the output letter. For example, when encrypting a message starting \"ANX...\", the operator would first press the \"A\" key, and the \"Z\" lamp might light, so \"Z\" would be the first letter of the ciphertext. The operator would next press \"N\", and then \"X\" in the same fashion, and so on.\n\nCurrent flowed from the battery (1) through a depressed bi-directional keyboard switch (2) to the plugboard (3). Next, it passed through the (unused in this instance, so shown closed) plug \"A\" (3) via the entry wheel (4), through the wiring of the three (Wehrmacht Enigma) or four (\"Kriegsmarine\" M4 and \"Abwehr\" variants) installed rotors (5), and entered the reflector (6). The reflector returned the current, via an entirely different path, back through the rotors (5) and entry wheel (4), proceeding through plug \"S\" (7) connected with a cable (8) to plug \"D\", and another bi-directional switch (9) to light the appropriate lamp.\n\nThe repeated changes of electrical path through an Enigma scrambler implemented a polyalphabetic substitution cipher that provided Enigma's security. The diagram on the right shows how the electrical pathway changed with each key depression, which caused rotation of at least the right-hand rotor. Current passed into the set of rotors, into and back out of the reflector, and out through the rotors again. The greyed-out lines are other possible paths within each rotor; these are hard-wired from one side of each rotor to the other. The letter \"A\" encrypts differently with consecutive key presses, first to \"G\", and then to \"C\". This is because the right-hand rotor has stepped, sending the signal on a completely different route. Eventually other rotors step with a key press.\n\nThe rotors (alternatively \"wheels\" or \"drums\", \"Walzen\" in German) formed the heart of an Enigma machine. Each rotor was a disc approximately in diameter made from hard rubber or bakelite with 26 brass, spring-loaded, electrical contact pins arranged in a circle on one face; the other side housing the corresponding number of circular plate electrical contacts. The pins and contacts represent the alphabet—typically the 26 letters A–Z (this will be assumed for the rest of this description). When the rotors were mounted side-by-side on the spindle, the pins of one rotor rested against the plate contacts of the neighbouring rotor, forming an electrical connection. Inside the body of the rotor, 26 wires connected each pin on one side to a contact on the other in a complex pattern. Most of the rotors were identified by Roman numerals, and each issued copy of rotor I was wired identically to all others. The same was true for the special thin beta and gamma rotors used in the M4 naval variant.\n\nBy itself, a rotor performs only a very simple type of encryption—a simple substitution cipher. For example, the pin corresponding to the letter \"E\" might be wired to the contact for letter \"T\" on the opposite face, and so on. Enigma's security came from using several rotors in series (usually three or four) and the regular stepping movement of the rotors, thus implementing a polyalphabetic substitution cipher.\n\nWhen placed in an Enigma, each rotor can be set to one of 26 possible positions. When inserted, it can be turned by hand using the grooved finger-wheel, which protrudes from the internal Enigma cover when closed. So that the operator can know the rotor's position, each had an \"alphabet tyre\" (or letter ring) attached to the outside of the rotor disc, with 26 characters (typically letters); one of these could be seen through the window, thus indicating the rotational position of the rotor. In early models, the alphabet ring was fixed to the rotor disc. A later improvement was the ability to adjust the alphabet ring relative to the rotor disc. The position of the ring was known as the \"Ringstellung\" (\"ring setting\"), and was a part of the initial setting prior to an operating session. In modern terms it was a part of the initialization vector.\nEach rotor contained a notch (or more than one) that controlled rotor stepping. In the military variants, the notches are located on the alphabet ring.\n\nThe Army and Air Force Enigmas were used with several rotors, initially three. On 15 December 1938, this changed to five, from which three were chosen for a given session. Rotors were marked with Roman numerals to distinguish them: I, II, III, IV and V, all with single notches located at different points on the alphabet ring. This variation was probably intended as a security measure, but ultimately allowed the Polish Clock Method and British Banburismus attacks.\n\nThe Naval version of the \"Wehrmacht\" Enigma had always been issued with more rotors than the other services: at first six, then seven, and finally eight. The additional rotors were marked VI, VII and VIII, all with different wiring, and had two notches, resulting in more frequent turnover. The four-rotor Naval Enigma (M4) machine accommodated an extra rotor in the same space as the three-rotor version. This was accomplished by replacing the original reflector with a thinner one and by adding a thin fourth rotor. That fourth rotor was one of two types, \"Beta\" or \"Gamma\", and never stepped, but could be manually set to any of 26 positions. One of the 26 made the machine perform identically to the three-rotor machine.\n\nTo avoid merely implementing a simple (and easily solvable) substitution cipher, every key press caused one or more rotors to step by one twenty-sixth of a full rotation, before the electrical connections were made. This changed the substitution alphabet used for encryption, ensuring that the cryptographic substitution was different at each new rotor position, producing a more formidable polyalphabetic substitution cipher. The stepping mechanism varied slightly from model to model. The right-hand rotor stepped once with each keystroke, and other rotors stepped less frequently.\n\nThe advancement of a rotor other than the left-hand one was called a \"turnover\" by the British. This was achieved by a ratchet and pawl mechanism. Each rotor had a ratchet with 26 teeth and every time a key was pressed, the set of spring-loaded pawls moved forward in unison, trying to engage with a ratchet. The alphabet ring of the rotor to the right normally prevented this. As this ring rotated with its rotor, a notch machined into it would eventually align itself with the pawl, allowing it to engage with the ratchet, and advance the rotor on its left. The right-hand pawl, having no rotor and ring to its right, stepped its rotor with every key depression. For a single-notch rotor in the right-hand position, the middle rotor stepped once for every 26 steps of the right-hand rotor. Similarly for rotors two and three. For a two-notch rotor, the rotor to its left would turn over twice for each rotation.\n\nThe first five rotors to be introduced (I–V) contained one notch each, while the additional naval rotors VI, VII and VIII each had two notches. The position of the notch on each rotor was determined by the letter ring which could be adjusted in relation to the core containing the interconnections. The points on the rings at which they caused the next wheel to move were as follows.\n\nThe design also included a feature known as \"double-stepping\". This occurred when each pawl aligned with both the ratchet of its rotor and the rotating notched ring of the neighbouring rotor. If a pawl engaged with a ratchet through alignment with a notch, as it moved forward it pushed against both the ratchet and the notch, advancing both rotors. In a three-rotor machine, double-stepping affected rotor two only. If in moving forward the ratchet of rotor three was engaged, rotor two would move again on the subsequent keystroke, resulting in two consecutive steps. Rotor two also pushes rotor one forward after 26 steps, but since rotor one moves forward with every keystroke anyway, there is no double-stepping. This double-stepping caused the rotors to deviate from odometer-style regular motion.\nWith three wheels and only single notches in the first and second wheels, the machine had a period of 26 × 25 × 26 = 16,900 (not 26 × 26 × 26, because of double-stepping). Historically, messages were limited to a few hundred letters, and so there was no chance of repeating any combined rotor position during a single session, denying cryptanalysts valuable clues.\n\nTo make room for the Naval fourth rotors, the reflector was made much thinner. The fourth rotor fitted into the space made available. No other changes were made, which eased the changeover. Since there were only three pawls, the fourth rotor never stepped, but could be manually set into one of 26 possible positions.\n\nA device that was designed, but not implemented before the war's end, was the \"Lückenfüllerwalze\" (gap-fill wheel) that implemented irregular stepping. It allowed field configuration of notches in all 26 positions. If the number of notches was a relative prime of 26 and the number of notches were different for each wheel, the stepping would be more unpredictable. Like the Umkehrwalze-D it also allowed the internal wiring to be reconfigured.\n\nThe current entry wheel (\"Eintrittswalze\" in German), or entry stator, connects the plugboard to the rotor assembly. If the plugboard is not present, the entry wheel instead connects the keyboard and lampboard to the rotor assembly. While the exact wiring used is of comparatively little importance to security, it proved an obstacle to Rejewski's progress during his study of the rotor wirings. The commercial Enigma connects the keys in the order of their sequence on a QWERTZ keyboard: \"Q\"→\"A\", \"W\"→\"B\", \"E\"→\"C\" and so on. The military Enigma connects them in straight alphabetical order: \"A\"→\"A\", \"B\"→\"B\", \"C\"→\"C\", and so on. It took inspired guesswork for Rejewski to penetrate the modification.\n\nWith the exception of models \"A\" and \"B\", the last rotor came before a 'reflector' (German: \"Umkehrwalze\", meaning 'reversal rotor'), a patented feature unique to Enigma among the period's various rotor machines. The reflector connected outputs of the last rotor in pairs, redirecting current back through the rotors by a different route. The reflector ensured that Enigma is self-reciprocal: conveniently, encryption was the same as decryption. The reflector also gave Enigma the property that no letter ever encrypted to itself. This was a severe conceptual flaw and a cryptological mistake subsequently exploited by codebreakers.\n\nIn Model 'C', the reflector could be inserted in one of two different positions. In Model 'D', the reflector could be set in 26 possible positions, although it did not move during encryption. In the \"Abwehr\" Enigma, the reflector stepped during encryption in a manner similar to the other wheels.\n\nIn the German Army and Air Force Enigma, the reflector was fixed and did not rotate; there were four versions. The original version was marked 'A', and was replaced by \"Umkehrwalze B\" on 1 November 1937. A third version, \"Umkehrwalze C\" was used briefly in 1940, possibly by mistake, and was solved by Hut 6. The fourth version, first observed on 2 January 1944, had a rewireable reflector, called \"Umkehrwalze D\", nick-named Uncle Dick by the British, allowing the Enigma operator to alter the connections as part of the key settings.\n\nThe plugboard (\"Steckerbrett\" in German) permitted variable wiring that could be reconfigured by the operator (visible on the front panel of Figure 1; some of the patch cords can be seen in the lid). It was introduced on German Army versions in 1930, and was soon adopted by the \"Reichsmarine\" (German Navy). The plugboard contributed more cryptographic strength than an extra rotor. Enigma without a plugboard (known as \"unsteckered Enigma\") can be solved relatively straightforwardly using hand methods; these techniques are generally defeated by the plugboard, driving Allied cryptanalysts to develop special machines to solve it.\n\nA cable placed onto the plugboard connected letters in pairs; for example, \"E\" and \"Q\" might be a steckered pair. The effect was to swap those letters before and after the main rotor scrambling unit. For example, when an operator presses \"E\", the signal was diverted to \"Q\" before entering the rotors. Up to 13 steckered pairs might be used at one time, although only 10 were normally used.\n\nCurrent flowed from the keyboard through the plugboard, and proceeded to the entry-rotor or \"Eintrittswalze\". Each letter on the plugboard had two jacks. Inserting a plug disconnected the upper jack (from the keyboard) and the lower jack (to the entry-rotor) of that letter. The plug at the other end of the crosswired cable was inserted into another letter's jacks, thus switching the connections of the two letters.\n\nOther features made various Enigma machines more secure or more convenient.\n\nSome M4 Enigmas used the \"Schreibmax\", a small printer that could print the 26 letters on a narrow paper ribbon. This eliminated the need for a second operator to read the lamps and transcribe the letters. The \"Schreibmax\" was placed on top of the Enigma machine and was connected to the lamp panel. To install the printer, the lamp cover and light bulbs had to be removed. It improved both convenience and operational security; the printer could be installed remotely such that the signal officer operating the machine no longer had to see the decrypted plaintext.\n\nAnother accessory was the remote lamp panel \"Fernlesegerät\". For machines equipped with the extra panel, the wooden case of the Enigma was wider and could store the extra panel. A lamp panel version could be connected afterwards, but that required, as with the \"Schreibmax\", that the lamp panel and lightbulbs be removed. The remote panel made it possible for a person to read the decrypted plaintext without the operator seeing it.\n\nIn 1944, the \"Luftwaffe\" introduced a plugboard switch, called the \"Uhr\" (clock), a small box containing a switch with 40 positions. It replaced the standard plugs. After connecting the plugs, as determined in the daily key sheet, the operator turned the switch into one of the 40 positions, each producing a different combination of plug wiring. Most of these plug connections were, unlike the default plugs, not pair-wise. In one switch position, the \"Uhr\" did not swap letters, but simply emulated the 13 stecker wires with plugs.\n\nThe Enigma transformation for each letter can be specified mathematically as a product of permutations. Assuming a three-rotor German Army/Air Force Enigma, let formula_1 denote the plugboard transformation, formula_2 denote that of the reflector, and formula_3 denote those of the left, middle and right rotors respectively. Then the encryption formula_4 can be expressed as\n\nAfter each key press, the rotors turn, changing the transformation. For example, if the right-hand rotor formula_6 is rotated formula_7 positions, the transformation becomes formula_8, where formula_9 is the cyclic permutation mapping \"A\" to \"B\", \"B\" to \"C\", and so forth. Similarly, the middle and left-hand rotors can be represented as formula_10 and formula_11 rotations of formula_12 and formula_13. The encryption transformation can then be described as\n\nCombining three rotors from a set of five, the rotor settings with 26 positions, and the plugboard with ten pairs of letters connected, the military Enigma has 158,962,555,217,826,360,000 (nearly 159 quintillion) different settings.\n\nA German Enigma operator would be given a plaintext message to encrypt. For each letter typed in, a lamp indicated a different letter according to a pseudo-random substitution, based upon the wiring of the machine. The letter indicated by the lamp would be recorded as the enciphered substitution. The action of pressing a key also moved the rotor so that the next key press used a different electrical pathway, and thus a different substitution would occur. For each key press there was rotation of at least the right hand rotor, giving a different substitution alphabet. This continued for each letter in the message until the message was completed and a series of substitutions, each different from the others, had occurred to create a cyphertext from the plaintext. The cyphertext would then be transmitted as normal to an operator of another Enigma machine. This operator would key in the cyphertext and—as long as all the settings of the deciphering machine were identical to those of the enciphering machine—for every key press the reverse substitution would occur and the plaintext message would emerge.\n\nIn use, the Enigma required a list of daily key settings and auxiliary documents. In German military practice, communications were divided into separate networks, each using different settings. These communication nets were termed \"keys\" at Bletchley Park, and were assigned code names, such as \"Red\", \"Chaffinch\", and \"Shark\". Each unit operating in a network was given the same settings list for its Enigma, valid for a period of time. The procedures for German Naval Enigma were more elaborate and more secure than those in other services and employed auxiliary codebooks. Navy codebooks were printed in red, water-soluble ink on pink paper so that they could easily be destroyed if they were endangered or if the vessel was sunk.\n\nAn Enigma machine's setting (its cryptographic key in modern terms; \"Schlüssel\" in German) specified each operator-adjustable aspect of the machine:\n\nFor a message to be correctly encrypted and decrypted, both sender and receiver had to configure their Enigma in the same way; rotor selection and order, ring positions, plugboard connections and starting rotor positions must be identical. Except for the starting positions, these settings were established beforehand, distributed in key lists and changed daily. For example, the settings for the 18th day of the month in the German Luftwaffe Enigma key list number 649 (see image) were as follows:\n\nEnigma was designed to be secure even if the rotor wiring was known to an opponent, although in practice considerable effort protected the wiring configuration. If the wiring is secret, the total number of possible configurations has been calculated to be around 3 x 10 (approximately 380 bits); with known wiring and other operational constraints, this is reduced to around 10 (76 bits). Users of Enigma were confident of its security because of the large number of possibilities; it was not then feasible for an adversary to even begin to try a brute force attack.\n\nMost of the key was kept constant for a set time period, typically a day. A different initial rotor position was used for each message, a concept similar to an initialisation vector in modern cryptography. The reason is that encrypting many messages with identical or near-identical settings (termed in cryptanalysis as being \"in depth\"), would enable an attack using a statistical procedure such as Friedman's Index of coincidence. The starting position for the rotors was transmitted just before the ciphertext, usually after having been enciphered. The exact method used was termed the \"indicator procedure\". Design weakness and operator sloppiness in these indicator procedures were two of the main weaknesses that made cracking Enigma possible.\n\nOne of the earliest \"indicator procedures\" for the Enigma was cryptographically flawed and allowed Polish cryptanalysts to make the initial breaks into the plugboard Enigma. The procedure had the operator set his machine in accordance with the secret settings that all operators on the net shared. The settings included an initial position for the rotors (the \"Grundstellung\"), say, \"AOH\". The operator turned his rotors until \"AOH\" was visible through the rotor windows. At that point, the operator chose his own arbitrary starting position for the message he would send. An operator might select \"EIN\", and that became the \"message setting\" for that encryption session. The operator then typed \"EIN\" into the machine twice. The results were the encrypted indicator. The \"EIN\" typed twice might encrypt into \"XHTLOA\", which would be transmitted along with the encrypted message. Finally, the operator then spun the rotors to his message settings, \"EIN\" in this example, and typed the plaintext of the message.\n\nAt the receiving end, the operator set the machine to the initial settings and typed in the first six letters of the message (\"XHTLOA\"). In this example, \"EINEIN\" emerged on the lamps, so the operator would learn the \"message setting\" that the sender used to encrypt this message. The receiving operator would set his rotors to \"EIN\", type in the rest of the ciphertext, and get the deciphered message.\n\nThis indicator scheme had two weaknesses. First, the use of a global initial position (\"Grundstellung\") meant all message keys used the same polyalphabetic substitution. In later indicator procedures, the operator selected his initial position for encrypting the indicator and sent that initial position in the clear. The second problem was the repetition of the indicator, which was a serious security flaw. The message setting was encoded twice, resulting in a relation between first and fourth, second and fifth, and third and sixth character. These security flaws enabled the Polish Cipher Bureau to break into the pre-war Enigma system as early as 1932. The early indicator procedure was subsequently described by German cryptanalysts as the \"faulty indicator technique\".\n\nDuring World War II, codebooks were only used each day to set up the rotors, their ring settings and the plugboard. For each message, the operator selected a random start position, let's say \"WZA\", and a random message key, perhaps \"SXT\". He moved the rotors to the \"WZA\" start position and encoded the message key \"SXT\". Assume the result was \"UHL\". He then set up the message key, \"SXT\", as the start position and encrypted the message. Next, he transmitted the start position, \"WZA\", the encoded message key, \"UHL\", and then the ciphertext. The receiver set up the start position according to the first trigram, \"WZA\", and decoded the second trigram, \"UHL\", to obtain the \"SXT\" message setting. Next, he used this \"SXT\" message setting as the start position to decrypt the message. This way, each ground setting was different and the new procedure avoided the security flaw of double encoded message settings.\n\nThis procedure was used by \"Wehrmacht\" and \"Luftwaffe\" only. The \"Kriegsmarine\" procedures on sending messages with the Enigma were far more complex and elaborate. Prior to encryption the message was encoded using the \"Kurzsignalheft\" code book. The \"Kurzsignalheft\" contained tables to convert sentences into four-letter groups. A great many choices were included, for example, logistic matters such as refuelling and rendezvous with supply ships, positions and grid lists, harbour names, countries, weapons, weather conditions, enemy positions and ships, date and time tables. Another codebook contained the \"Kenngruppen\" and \"Spruchschlüssel\": the key identification and message key.\n\nThe Army Enigma machine used only the 26 alphabet characters. Punctuation was replaced with rare character combinations. A space was omitted or replaced with an X. The X was generally used as full-stop.\n\nSome punctuation marks were different in other parts of the armed forces. The \"Wehrmacht\" replaced a comma with ZZ and the question mark with FRAGE or FRAQ.\n\nThe \"Kriegsmarine\" replaced the comma with Y and the question mark with UD. The combination CH, as in \"\"Acht\" (eight) or \"Richtung\"\" (direction), was replaced with Q (AQT, RIQTUNG). Two, three and four zeros were replaced with CENTA, MILLE and MYRIA.\n\nThe \"Wehrmacht\" and the \"Luftwaffe\" transmitted messages in groups of five characters.\n\nThe \"Kriegsmarine\", using the four rotor Enigma, had four-character groups. Frequently used names or words were varied as much as possible. Words like \"Minensuchboot\" (minesweeper) could be written as MINENSUCHBOOT, MINBOOT, MMMBOOT or MMM354. To make cryptanalysis harder, messages were limited to 250 characters. Longer messages were divided into several parts, each using a different message key.\n\nThe character substitutions by the Enigma machine as a whole can be expressed as a string of letters with each position occupied by the character that will replace the character at the corresponding position in the alphabet. For example a given machine configuration that encoded A to L, B to U, C to S, ..., and Z to J could be represented compactly as\n\nand the encoding of a particular character by that configuration could be represented by highlighting the encoded character as in\n\nSince the operation of an Enigma machine encoding a message is a series of such configurations, each associated with a single character being encoded, a sequence of such representations can be used to represent the operation of the machine as it encodes a message. For example, the process of encoding the first sentence of the main body of the famous \"Dönitz message\" to\n\ncan be represented as \n\nWhere the letters following each mapping are the letters that appear at the windows at that stage (the only state changes visible to the operator) and the numbers show the underlying physical position of each rotor.\n\nThe Enigma family included multiple designs. The earliest were commercial models dating from the early 1920s. Starting in the mid-1920s, the German military began to use Enigma, making a number of security-related changes. Various nations either adopted or adapted the design for their own cipher machines.\n\nAn estimated 100,000 Enigma machines were constructed. After the end of World War II, the Allies sold captured Enigma machines, still widely considered secure, to developing countries.\n\nOn 23 February 1918, Arthur Scherbius applied for a patent for a ciphering machine that used rotors. Scherbius and E. Richard Ritter founded the firm of Scherbius & Ritter. They approached the German Navy and Foreign Office with their design, but neither agency was interested. Scherbius & Ritter then assigned the patent rights to Gewerkschaft Securitas, who founded the \"Chiffriermaschinen Aktien-Gesellschaft\" (Cipher Machines Stock Corporation) on 9 July 1923; Scherbius and Ritter were on the board of directors.\n\nChiffriermaschinen AG began advertising a rotor machine—\"Enigma model A\"—which was exhibited at the Congress of the International Postal Union in 1924. The machine was heavy and bulky, incorporating a typewriter. It measured 65×45×38 cm and weighed about .\n\nIn 1924 Enigma \"model B\" was introduced, and was of a similar construction. While bearing the Enigma name, both models \"A\" and \"B\" were quite unlike later versions: they differed in physical size and shape, but also cryptographically, in that they lacked the reflector.\n\nThe reflector—suggested by Scherbius's colleague Willi Korn—was introduced in \"Enigma C\" (1926).\n\n\"Model C\" was smaller and more portable than its predecessors. It lacked a typewriter, relying on the operator; hence the informal name of \"glowlamp Enigma\" to distinguish it from models \"A\" and \"B\".\n\nThe \"Enigma C\" quickly gave way to \"Enigma D\" (1927). This version was widely used, with shipments to Sweden, the Netherlands, United Kingdom, Japan, Italy, Spain, United States and Poland. In 1927 Hugh Foss at the British Government Code and Cypher School was able to show that commercial Enigma machines could be broken provided that suitable cribs were available.\n\nOther countries used Enigma machines. The Italian Navy adopted the commercial Enigma as \"Navy Cipher D\". The Spanish also used commercial Enigma during their Civil War. British codebreakers succeeded in breaking these machines, which lacked a plugboard. Enigma were also used by diplomatic services.\n\nThere was also a large, eight-rotor printing model, the \"Enigma H\", called \"Enigma II\" by the \"Reichswehr\". In 1933 the Polish Cipher Bureau detected that it was in use for high-level military communications, but that it was soon withdrawn, as it was unreliable and jammed frequently.\n\nThe Swiss used a version of Enigma called \"model K\" or \"Swiss K\" for military and diplomatic use, which was very similar to commercial Enigma D. The machine was cracked by Poland, France, the United Kingdom and the United States (the latter codenamed it INDIGO). An \"Enigma T\" model (codenamed \"Tirpitz\") was used by Japan.\n\nOnce the British broke the Enigma, they fixed the problem with it and created their own, which the Germans believed to be unsolvable.\n\nThe Reichsmarine was the first military branch to adopt Enigma. This version, named \"Funkschlüssel C\" (\"Radio cipher C\"), had been put into production by 1925 and was introduced into service in 1926.\n\nThe keyboard and lampboard contained 29 letters—A-Z, Ä, Ö and Ü—which were arranged alphabetically, as opposed to the QWERTZUI ordering. The rotors had 28 contacts, with the letter \"X\" wired to bypass the rotors unencrypted.\nThree rotors were chosen from a set of five and the reflector could be inserted in one of four different positions, denoted α, β, γ and δ. The machine was revised slightly in July 1933.\n\nBy 15 July 1928, the German Army (\"Reichswehr\") had introduced their own exclusive version of the Enigma machine; the \"Enigma G\".\n\nThe \"Abwehr\" used the \"Enigma G\" (the \"Abwehr\" Enigma). This Enigma variant was a four-wheel unsteckered machine with multiple notches on the rotors. This model was equipped with a counter which incremented upon each key press, and so is also known as the \"counter machine\" or the \"Zählwerk\" Enigma.\n\nEnigma machine G was modified to the \"Enigma I\" by June 1930. Enigma I is also known as the \"Wehrmacht\", or \"Services\" Enigma, and was used extensively by German military services and other government organisations (such as the railways) before and during World War II.\n\nThe major difference between \"Enigma I\" (German Army version from 1930), and commercial Enigma models was the addition of a plugboard to swap pairs of letters, greatly increasing cryptographic strength.\n\nOther differences included the use of a fixed reflector and the relocation of the stepping notches from the rotor body to the movable letter rings. The machine measured and weighed around .\n\nIn August 1935, the Air Force introduced the Wehrmacht Enigma for their communications.\n\nBy 1930, the Reichswehr had suggested that the Navy adopt their machine, citing the benefits of increased security (with the plugboard) and easier interservice communications. The Reichsmarine eventually agreed and in 1934 brought into service the Navy version of the Army Enigma, designated \"Funkschlüssel\" ' or \"M3\". While the Army used only three rotors at that time, the Navy specified a choice of three from a possible five.\nIn December 1938, the Army issued two extra rotors so that the three rotors were chosen from a set of five. In 1938, the Navy added two more rotors, and then another in 1939 to allow a choice of three rotors from a set of eight.\n\nA four-rotor Enigma was introduced by the Navy for U-boat traffic on 1 February 1942, called \"M4\" (the network was known as \"Triton\", or \"Shark\" to the Allies). The extra rotor was fitted in the same space by splitting the reflector into a combination of a thin reflector and a thin fourth rotor.\n\nThe effort to break the Enigma was not disclosed until the 1970s. Since then, interest in the Enigma machine has grown. Enigmas are on public display in museums around the world, and several are in the hands of private collectors and computer history enthusiasts.\n\nThe \"Deutsches Museum\" in Munich has both the three- and four-rotor German military variants, as well as several civilian versions. Enigma machines are exhibited at the National Codes Centre in Bletchley Park, the Government Communications Headquarters, the Science Museum in London, the Polish Army Museum in Warsaw, the Swedish Army Museum (\"Armémuseum\") in Stockholm, the Military Museum of A Coruña in Spain, the Nordland Red Cross War Memorial Museum in Narvik, Norway, The Artillery, Engineers and Signals Museum in Hämeenlinna, Finland the Technical University of Denmark in Lyngby, Denmark, and at the Australian War Memorial and in the foyer of the Defence Signals Directorate, both in Canberra, Australia. The Jozef Pilsudski Institute in London exhibits a rare Polish Enigma double assembled in France in 1940.\nIn the United States, Enigma machines can be seen at the Computer History Museum in Mountain View, California, and at the National Security Agency's National Cryptologic Museum in Fort Meade, Maryland, where visitors can try their hand at enciphering and deciphering messages. Two machines that were acquired after the capture of during World War II are on display alongside the submarine at the Museum of Science and Industry in Chicago, Illinois. A four rotor device is on display in the ANZUS Corridor of the Pentagon on the second floor, A ring, between corridors 9 and 10. This machine is on loan from Australia. The United States Air Force Academy in Colorado Springs has a machine on display in the Computer Science Department. There is also a machine located at the National World War II Museum in New Orleans. The Museum of World War II in Boston has seven Enigma machines on display, including a U-Boat four-rotor model, one of three surviving examples of an Enigma machine with a printer, one of fewer than ten surviving ten-rotor code machines, an example blown up by a retreating German Army unit, and two three-rotor Enigmas that visitors can operate to encode and decode messages themselves.\nIn Canada, a Swiss Army issue Enigma-K, is in Calgary, Alberta. It is on permanent display at the Naval Museum of Alberta inside the Military Museums of Calgary. A 4-rotor Enigma machine is on display at the Military Communications and Electronics Museum at Canadian Forces Base (CFB) Kingston in Kingston, Ontario.\n\nOccasionally, Enigma machines are sold at auction; prices have in recent years ranged from US$40,000 to US$547,500 in 2017. Replicas are available in various forms, including an exact reconstructed copy of the Naval M4 model, an Enigma implemented in electronics (Enigma-E), various simulators and paper-and-scissors analogues.\n\nA rare \"Abwehr\" Enigma machine, designated G312, was stolen from the Bletchley Park museum on 1 April 2000. In September, a man identifying himself as \"The Master\" sent a note demanding £25,000 and threatening to destroy the machine if the ransom was not paid. In early October 2000, Bletchley Park officials announced that they would pay the ransom, but the stated deadline passed with no word from the blackmailer. Shortly afterward, the machine was sent anonymously to BBC journalist Jeremy Paxman, missing three rotors.\n\nIn November 2000, an antiques dealer named Dennis Yates was arrested after telephoning \"The Sunday Times\" to arrange the return of the missing parts. The Enigma machine was returned to Bletchley Park after the incident. In October 2001, Yates was sentenced to 10 months in prison and served three months.\n\nIn October 2008, the Spanish daily newspaper \"El País\" reported that 28 Enigma machines had been discovered by chance in an attic of Army headquarters in Madrid. These 4-rotor commercial machines had helped Franco's Nationalists win the Spanish Civil War because, though the British cryptologist Alfred Dilwyn Knox in 1937 broke the cipher generated by Franco's Enigma machines, this was not disclosed to the Republicans, who failed to break the cipher. The Nationalist government continued using its 50 Enigmas into the 1950s. Some machines have gone on display in Spanish military museums, including one at the National Museum of Science and Technology (MUNCYT) in La Coruña. Two have been given to Britain's GCHQ.\n\nThe Bulgarian military used Enigma machines with a Cyrillic keyboard; one is on display in the National Museum of Military History in Sofia.\n\nThe Enigma was influential in the field of cipher machine design, spinning off other rotor machines. The British Typex was originally derived from the Enigma patents; Typex even includes features from the patent descriptions that were omitted from the actual Enigma machine. The British paid no royalties for the use of the patents, to protect secrecy. The Typex implementation is not the same as that found in German or other Axis versions.\n\nA Japanese Enigma clone was codenamed GREEN by American cryptographers. Little used, it contained four rotors mounted vertically. In the U.S., cryptologist William Friedman designed the M-325, a machine logically similar, although not in construction.\n\nA unique rotor machine was constructed in 2002 by Netherlands-based Tatjana van Vark. This device makes use of 40-point rotors, allowing letters, numbers and some punctuation to be used; each rotor contains 509 parts.\n\nMachines like the SIGABA, NEMA, Typex and so forth, are deliberately not considered to be Enigma derivatives as their internal ciphering functions are not mathematically identical to the Enigma transform.\n\nSeveral software implementations exist, but not all exactly match Enigma behaviour. The most commonly used software derivative (that is not compliant with any hardware implementation of the Enigma) is at EnigmaCo.de. Many Java applet Enigmas only accept single letter entry, complicating use even if the applet is Enigma compliant. Technically, Enigma@home is the largest scale deployment of a software Enigma, but the decoding software does not implement encipherment making it a derivative (as all original machines could cipher and decipher).\n\nA user-friendly 3-rotor simulator, where users can select rotors, use the plugboard and define new settings for the rotors and reflectors is available. The output appears in separate windows which can be independently made \"invisible\" to hide decryption. Another includes an \"autotyping\" function which takes plaintext from a clipboard and converts it to cyphertext (or vice versa) at one of four speeds. The \"very fast\" option produces 26 characters in less than one second.\n\n\n\n\n\n"}
{"id": "53551948", "url": "https://en.wikipedia.org/wiki?curid=53551948", "title": "Erdős–Moser equation", "text": "Erdős–Moser equation\n\nIn number theory, the Erdős–Moser equation is\n\nwhere formula_2 and formula_3 are positive integers. The only known solution is 1 + 2 = 3, and Paul Erdős conjectured that no further solutions exist.\n\nLeo Moser in 1953 proved that 2 divides \"k\" and that there is no other solution with \"m\" < 10.\n\nIn 1966 it was shown that 6 ≤ \"k\" + 2 < \"m\" < 2\"k\".\n\nIn 1994 it was shown that lcm(1,2...,200) divides \"k\" and that any prime factor of \"m\" + 1 must be irregular and > 10000.\n\nMoser's method was extended in 1999 to show that \"m\" > 1.485 × 10.\n\nIn 2002 it was shown that all primes between 200 and 1000 must divide \"k\".\n\nIn 2009 it was shown that 2\"k\" / (2\"m\" – 1) must be a convergent of ln(2); large-scale computation of ln(2) was then used to show that \"m\" > 2.7139 × 10.\n\n \n"}
{"id": "57794", "url": "https://en.wikipedia.org/wiki?curid=57794", "title": "Fractal antenna", "text": "Fractal antenna\n\nA fractal antenna is an antenna that uses a fractal, self-similar design to maximize the effective length, or increase the perimeter (on inside sections or the outer structure), of material that can receive or transmit electromagnetic radiation within a given total surface area or volume.\n\nSuch fractal antennas are also referred to as multilevel and space filling curves, but the key aspect lies in their repetition of a motif over two or more scale sizes, or \"iterations\". For this reason, fractal antennas are very compact, multiband or wideband, and have useful applications in cellular telephone and microwave communications.\nA fractal antenna's response differs markedly from traditional antenna designs, in that it is capable of operating with good-to-excellent performance at many different frequencies simultaneously. Normally standard antennas have to be \"cut\" for the frequency for which they are to be used—and thus the standard antennas only work well at that frequency.\n\nThis makes the fractal antenna an excellent choice for wideband and multiband applications. In addition the fractal nature of the antenna shrinks its size, without the use of any components, such as inductors or capacitors.\n\nThe first fractal \"antennas\" were, in fact, fractal \"arrays\", with fractal arrangements of antenna elements, and not recognized initially as having self-similarity as their attribute. Log-periodic antennas are arrays, around since the 1950s (invented by Isbell and DuHamel), that are such fractal arrays. They are a common form used in TV antennas, and are arrow-head in shape.\n\nAntenna elements (as opposed to antenna arrays) made from self-similar shapes were first created by Nathan Cohen then a professor at Boston University, starting in 1988.\n\nCohen's efforts with a variety of fractal antenna designs were first published in 1995. Cohen's publication marked the inaugural scientific publication on fractal antennas. Most varieties of fractal antennas are so-called \"fractal element antennas\".\n\nMany fractal element antennas use the fractal structure as a virtual combination of capacitors and inductors. This makes the antenna so that it has many different resonances which can be chosen and adjusted by choosing the proper fractal design. This complexity arises because the current on the structure has a complex arrangement caused by the inductance and self capacitance. In general, although their effective electrical length is longer, the fractal element antennas are themselves physically smaller, again due to this reactive loading.\n\nThus fractal element antennas are shrunken compared to conventional designs, and do not need additional components, assuming the structure happens to have the desired resonant input impedance. In general the fractal dimension of a fractal antenna is a poor predictor of its performance and application. Not all fractal antennas work well for a given application or set of applications. Computer search methods and antenna simulations are commonly used to identify which fractal antenna designs best meet the need of the application.\n\nAlthough the first validation of the technology was published as early as 1995, recent independent studies show advantages of the fractal element technology in real-life applications, such as RFID and cell phones.\n\nOne researcher has stated to the contrary that fractals do not perform any better than \"meandering line\" (essentially, fractals with only one size scale, repeating in translation) antennas. Specifically quoting researcher Steven Best: \"Differing antenna geometries, fractal or otherwise, do not, in a manner different than other geometries, uniquely determine the EM behavior of the antenna.\" However, in the last few years, dozens of studies have shown superior performance with fractals, and the below reference of frequency invariance conclusively demonstrates that geometry is a key aspect in uniquely determining the EM behavior of frequency independent antennas.\n\nA different and also useful attribute of some fractal element antennas is their self-scaling aspect. In 1957, V.H. Rumsey presented results that angle-defined scaling was one of the underlying requirements to make antennas \"invariant\" (have same radiation properties) at a number, or range of, frequencies. Work by Y. Mushiake in Japan starting in 1948 demonstrated similar results of frequency independent antennas having self-complementarity.\n\nIt was believed that antennas had to be defined by angles for this to be true, but in 1999 it was discovered that \"self-similarity was one of the underlying requirements to make antennas frequency and bandwidth invariant\". In other words, the self-similar aspect was the underlying requirement, along with origin symmetry, for frequency 'independence'. Angle-defined antennas are self-similar, but other self-similar antennas are frequency independent although not angle-defined.\n\nThis analysis, based on Maxwell's equations, showed fractal antennas offer a closed-form and unique insight into a key aspect of electromagnetic phenomena. To wit: the invariance property of Maxwell's equations. This is now known as the HCR Principle. Mushiake's earlier work on self complementarity was shown to be limited to impedance smoothness, as expected from Babinet's Principle, but not frequency invariance.\n\nIn addition to their use as antennas, fractals have also found application in other antenna system components including loads, counterpoises, and ground planes. Confusion by those who claim \"grain of rice\"-sized fractal antennas arises, because such fractal structures serve the purpose of loads and counterpoises, rather than bona fide antennas.\n\nFractal inductors and fractal tuned circuits (fractal resonators) were also discovered and invented simultaneously with fractal element antennas. An emerging example of such is in metamaterials. A recent invention demonstrates using close-packed fractal resonators to make the first wideband metamaterial invisibility cloak at microwave frequencies (US patent 8,253,639). Peer reviewed publication may be found in the scholarly journal 'FRACTALS'.\n\nFractal filters (a type of tuned circuit) are another example where the superiority of the fractal approach for smaller size and better rejection has been proven.\n\nAs fractals can be used as counterpoises, loads, ground planes, and filters, all parts that can be integrated with antennas, they are considered parts of some antenna \"systems\" and thus are discussed in the context of fractal antennas.\n\nOwing to the fact that DNA is a self-similar, electrically conducting molecule, it is hypothesized that DNA acts as a fractal antenna in the presence an external electromagnetic field.\n\n\n"}
{"id": "1726089", "url": "https://en.wikipedia.org/wiki?curid=1726089", "title": "Galley division", "text": "Galley division\n\nIn arithmetic, the galley method, also known as the batello or the scratch method, was the most widely used method of division in use prior to 1600. The names galea and batello refer to a boat which the outline of the work was thought to resemble.\n\nAn earlier version of this method was used as early as 825 by Al-Khwarizmi. The galley method is thought to be of Arab origin and is most effective when used on a sand abacus. However, Lam Lay Yong's research pointed out that the galley method of division originated in the 1st century AD in ancient China.\n\nThe galley method writes fewer figures than long division, and results in interesting shapes and pictures as it expands both above and below the initial lines. It was the preferred method of division for seventeen centuries, far longer than long division's four centuries.\n\nSet up the problem by writing the dividend and then a bar. The quotient will be written after the bar. Steps:\nNow multiply each digit of the divisor by the new digit of the quotient and subtract that from the left-hand segment of the dividend. Where the subtrahend and the dividend segment differ, cross out the dividend digit and write if necessary the subtrahend digit and next vertical empty space. Cross out the divisor digit used.\n\nThe above is called the cross-out version and is the most common. An erasure version exists for situations where erasure is acceptable and there is not need to keep track of the intermediate steps. This is the method used with a sand abacus. Finally, there is a printers' method that uses neither erasure or crossouts. Only the top digit in each column of the dividend is active with a zero used to denote an entirely inactive column.\nGalley division was the favorite method of division with arithmeticians through the 18th century and it is thought that it fell out of use due to the lack of cancelled types in printing. It is still taught in the Moorish schools of North Africa and other parts of the Middle East.\n\nLam Lay Yong, mathematics professor of National University of Singapore, traced the origin of the galley method to the \"Sunzi Suanjing\" written about 400AD. The division described by Al-Khwarizmi in 825 was identical to the Sunzi algorithm for\ndivision.\n\n\n"}
{"id": "11422488", "url": "https://en.wikipedia.org/wiki?curid=11422488", "title": "Go and mathematics", "text": "Go and mathematics\n\nThe game of Go is one of the most popular games in the world. As a result of its elegant and simple rules, the game has long been an inspiration for mathematical research. Shen Kuo, a Chinese scholar in 11th century, estimated that the number of possible board positions is around 10 in Dream Pool Essays. In more recent years, research of the game by John H. Conway led to the invention of the surreal numbers and contributed to development of combinatorial game theory (with Go Infinitesimals being a specific example of its use in Go).\n\nGeneralized Go is played on \"n x n\" boards, and the computational complexity of determining the winner in a given position of generalized Go depends crucially on the ko rules.\n\nGo is “almost” in PSPACE, since in normal play, moves are not reversible, and it is only through capture that there is the possibility of the repeating patterns necessary for a harder complexity.\n\nWithout ko, Go is PSPACE-hard. This is proved by reducing True Quantified Boolean Formula, which is known to be PSPACE-complete, to generalized geography, to planar generalized geography, to planar generalized geography with maximum degree 3, finally to Go positions.\n\nGo with superko is not known to be in PSPACE. Though actual games seem never to last longer than formula_1 moves, in general it's not known if there were a polynomial bound on the length of Go games. If there were, Go would be PSPACE-complete. As it currently stands, it might be PSPACE-complete, EXPTIME-complete, or even EXPSPACE-complete.\n\nJapanese ko rules state that only the basic ko, that is, a move that reverts the board to the situation one move previously, is forbidden. Longer repetitive situations are allowed, thus potentially allowing a game to loop forever, such as the triple ko, where there are three kos at the same time, allowing a cycle of 12 moves.\n\nWith Japanese ko rules, Go is EXPTIME-complete.\n\nThe superko rule (also called the positional superko rule) states that a repetition of any board position that has previously occurred is forbidden. This is the ko rule used in most Chinese and US rulesets.\n\nIt is an open problem what the complexity class of Go is under superko rule. Though Go with Japanese ko rule is EXPTIME-complete, both the lower and the upper bounds of Robson’s EXPTIME-completeness proof break when the superko rule is added.\n\nIt is known that it is at least PSPACE-hard, since the proof in of the PSPACE-hardness of Go does not rely on the ko rule, or lack of the ko rule. It is also known that Go is in EXPSPACE.\n\nRobson showed that if the superko rule, that is, “no previous position may ever be recreated”, is added to certain two-player games that are EXPTIME-complete, then the new games would be EXPSPACE-complete. Intuitively, it's because it requires an exponential amount of space even to determine the legal moves from a position, because the game history leading up to a position could be exponentially long.\n\nAs a result, superko variants (moves that repeat a previous board position are not allowed) of generalized chess and checkers are EXPSPACE-complete, since generalized chess and checkers are EXPTIME-complete. However, this result does not apply to Go.\n\nA Go endgame begins when the board is divided into areas that are isolated from all other local areas by living stones, such that each local area has a polynomial size canonical game tree. In the language of combinatorial game theory, it happens when a Go game decomposes into a sum of subgames with polynomial size canonical game trees.\n\nWith that definition, Go endgames are PSPACE-hard.\n\nThis is proven by converting the Quantified Boolean Formula problem, which is PSPACE-complete, into a sum of small (with polynomial size canonical game trees) Go subgames. Note that the paper does not prove that Go endgames are in PSPACE, so they might not be PSPACE-complete.\n\nDetermining which side wins a ladder capturing race is PSPACE-complete, whether Japanese ko rule or superko rule is in place. This is proven by simulating QBF, known to be PSPACE-complete, with ladders that bounce around the board like light beams.\n\nSince each location on the board can be either empty, black, or white, there are a total of 3 possible board positions on a square board with length n; however only part of them are legal. Tromp and Farnebäck derived a recursive formula for legal positions formula_2 of a rectangle board with length m and n. The exact number of formula_3 is obtained in 2016. They also find an asymptotic formula formula_4, where formula_5, formula_6 and formula_7. It has been estimated that the observable universe contains around 10 atoms, far fewer than the number of possible legal positions of regular board size (m=n=19). As the board gets larger, the percentage of the positions that are legal decreases.\n\nThe computer scientist Victor Allis notes that typical games between experts last about 150 moves, with an average of about 250 choices per move, suggesting a game-tree complexity of 10. For the number of \"theoretically possible\" games, including games impossible to play in practice, Tromp and Farnebäck give lower and upper bounds of 10 and 10 respectively.\nThe most commonly quoted number for the number of possible games, 10 is derived from a simple permutation of 361 moves or 361! = 10. Another common derivation is to assume N intersections and L longest game for N^L total games. For example, 400 moves, as seen in some professional games, would be one out of 361 or 1 × 10 possible games.\n\nThe total number of possible games is a function both of the size of the board and the number of moves played. While most games last less than 400 or even 200 moves, many more are possible.\n\nThe total number of possible games can be estimated from the board size in a number of ways, some more rigorous than others. The simplest, a permutation of the board size, (N), fails to include illegal captures and positions. Taking N as the board size (19×19=361) and L as the longest game, N forms an upper limit. A more accurate limit is presented in the Tromp/Farnebäck paper.\n\n10 is thus an overestimate of the number of possible games that can be played in 200 moves and an underestimate of the number of games that can be played in 361 moves. Since there are about 31 million seconds in a year, it would take about 2¼ years, playing 16 hours a day at one move per second, to play 47 million moves.\n\n\n\n"}
{"id": "557931", "url": "https://en.wikipedia.org/wiki?curid=557931", "title": "Graph (abstract data type)", "text": "Graph (abstract data type)\n\nIn computer science, a graph is an abstract data type that is meant to implement the undirected graph and directed graph concepts from mathematics; specifically, the field of graph theory.\n\nA graph data structure consists of a finite (and possibly mutable) set of \"vertices\" or \"nodes\" or \"points\", together with a set of unordered pairs of these vertices for an undirected graph or a set of ordered pairs for a directed graph. These pairs are known as \"edges\", \"arcs\", or \"lines\" for an undirected graph and as \"arrows\", \"directed edges\", \"directed arcs\", or \"directed lines\" for a directed graph. The vertices may be part of the graph structure, or may be external entities represented by integer indices or references.\n\nA graph data structure may also associate to each edge some \"edge value\", such as a symbolic label or a numeric attribute (cost, capacity, length, etc.).\n\nThe basic operations provided by a graph data structure \"G\" usually include:\n\nStructures that associate values to the edges usually also provide:\n\nDifferent data structures for the representation of graphs are used in practice:\n\nThe following table gives the time complexity cost of performing various operations on graphs, for each of these representations, with |\"V\" | the number of vertices and |\"E\" | the number of edges. In the matrix representations, the entries encode the cost of following an edge. The cost of edges that are not present are assumed to be ∞.\n\nAdjacency lists are generally preferred because they efficiently represent sparse graphs. An adjacency matrix is preferred if the graph is dense, that is the number of edges |\"E\" | is close to the number of vertices squared, |\"V\" |, or if one must be able to quickly look up if there is an edge connecting two vertices.\n\n\n"}
{"id": "40338559", "url": "https://en.wikipedia.org/wiki?curid=40338559", "title": "Hybrid algorithm", "text": "Hybrid algorithm\n\nA hybrid algorithm is an algorithm that combines two or more other algorithms that solve the same problem, either choosing one (depending on the data), or switching between them over the course of the algorithm. This is generally done to combine desired features of each, so that the overall algorithm is better than the individual components.\n\n\"Hybrid algorithm\" does not refer to simply combining multiple algorithms to solve a different problem – many algorithms can be considered as combinations of simpler pieces – but only to combining algorithms that solve the same problem, but differ in other characteristics, notably performance.\n\nIn computer science, hybrid algorithms are very common in optimized real-world implementations of recursive algorithms, particularly implementations of \ndivide and conquer or decrease and conquer algorithms, where the size of the data decreases as one moves deeper in the recursion. In this case, one algorithm is used for the overall approach (on large data), but deep in the recursion, it switches to a different algorithm, which is more efficient on small data. A common example is in sorting algorithms, where the insertion sort, which is inefficient on large data, but very efficient on small data (say, five to ten elements), is used as the final step, after primarily applying another algorithm, such as merge sort or quicksort. Merge sort and quicksort are asymptotically optimal on large data, but the overhead becomes significant if applying them to small data, hence the use of a different algorithm at the end of the recursion. A highly optimized hybrid sorting algorithm is Timsort, which combines merge sort, insertion sort, together with additional logic (including binary search) in the merging logic.\n\nA general procedure for a simple hybrid recursive algorithm is \"short-circuiting the base case,\" also known as \"arm's-length recursion.\" In this case whether the next step will result in the base case is checked before the function call, avoiding an unnecessary function call. For example, in a tree, rather than recursing to a child node and then checking if it is null, checking null before recursing. This is useful for efficiency when the algorithm usually encounters the base case many times, as in many tree algorithms, but is otherwise considered poor style, particularly in academia, due to the added complexity.\n\nAnother example of hybrid algorithms for performance reasons are introsort and introselect, which combine one algorithm for fast average performance, falling back on another algorithm to ensure (asymptotically) optimal worst-case performance. Introsort begins with a quicksort, but switches to a heap sort if quicksort is not progressing well; analogously introselect begins with quickselect, but switches to median of medians if quickselect is not progressing well.\n\nCentralized distributed algorithms can often be considered as hybrid algorithms, consisting of an individual algorithm (run on each distributed processor), and a combining algorithm (run on a centralized distributor) – these correspond respectively to running the entire algorithm on one processor, or running the entire computation on the distributor, combining trivial results (a one-element data set from each processor). A basic example of these algorithms are distribution sorts, particularly used for external sorting, which divide the data into separate subsets, sort the subsets, and then combine the subsets into totally sorted data; examples include bucket sort and flashsort.\n\nHowever, in general distributed algorithms need not be hybrid algorithms, as individual algorithms or combining or communication algorithms may be solving different problems. For example, in models such as MapReduce, the Map and Reduce step solve different problems, and are combined to solve a different, third problem.\n\n"}
{"id": "40153832", "url": "https://en.wikipedia.org/wiki?curid=40153832", "title": "Inclusion (Boolean algebra)", "text": "Inclusion (Boolean algebra)\n\nIn Boolean algebra (structure), the inclusion relation formula_1 is defined as formula_2 and is the Boolean analogue to the subset relation in set theory. Inclusion is a partial order.\n\nThe inclusion relation <math>a can be expressed in many ways:\n\nThe inclusion relation has a natural interpretation in various Boolean algebras: in the subset algebra, the subset relation; in arithmetic Boolean algebra, divisibility; in the algebra of propositions, material implication; in the two-element algebra, the set { (0,0), (0,1), (1,1) }.\n\nSome useful properties of the inclusion relation are:\n\nThe inclusion relation may be used to define Boolean intervals such that formula_9 A Boolean algebra whose carrier set is restricted to the elements in an interval is itself a Boolean algebra.\n\n"}
{"id": "2053018", "url": "https://en.wikipedia.org/wiki?curid=2053018", "title": "Kaplan–Yorke map", "text": "Kaplan–Yorke map\n\nThe Kaplan–Yorke map is a discrete-time dynamical system. It is an example of a dynamical system that exhibits chaotic behavior. The Kaplan–Yorke map takes a point (\"x, y \") in the plane and maps it to a new point given by\n\nwhere \"mod\" is the modulo operator with real arguments. The map depends on only the one constant α.\n\nDue to roundoff error, successive applications of the modulo operator will yield zero after some ten or twenty iterations when implemented as a floating point operation on a computer. It is better to implement the following equivalent algorithm:\n\nwhere the formula_6 and formula_7 are computational integers. It is also best to choose formula_7 to be a large prime number in order to get many different values of formula_9.\n\nAnother way to avoid having the modulo operator yield zero after a short number of iterations is\n\nX = 2X (mod 0.99995)\n\nY = αY + cos(4πX)\n\nwhich, will still eventually return zero but takes many more iterations.\n\n"}
{"id": "37920642", "url": "https://en.wikipedia.org/wiki?curid=37920642", "title": "Lie-* algebra", "text": "Lie-* algebra\n\nIn mathematics, a Lie-* algebra is a D-module with a Lie* bracket. They were introduced by Alexander Beilinson and Vladimir Drinfeld (), and are similar to the conformal algebras discussed by and to vertex Lie algebras.\n\n"}
{"id": "21934376", "url": "https://en.wikipedia.org/wiki?curid=21934376", "title": "Local Fields", "text": "Local Fields\n\nCorps Locaux by Jean-Pierre Serre, originally published in 1962 and translated into English as Local Fields by Marvin Jay Greenberg in 1979, is a seminal graduate-level algebraic number theory text covering local fields, ramification, group cohomology, and local class field theory. The book's end goal is to present local class field theory from the cohomological point of view. This theory concerns extensions of \"local\" (i.e., complete for a discrete valuation) fields with finite residue field.\n\n"}
{"id": "2864495", "url": "https://en.wikipedia.org/wiki?curid=2864495", "title": "Majorization", "text": "Majorization\n\nIn mathematics, majorization is a preorder on vectors of real numbers. For a vector formula_1, we denote by formula_2 the vector with the same components, but sorted in descending order.\nGiven formula_3, we say that\nformula_4 weakly majorizes (or dominates) formula_5 from below written as formula_6 iff\n\nEquivalently, we say that formula_8 is weakly majorized (or dominated) by formula_9 from below, denoted as formula_10.\n\nSimilarly, we say that\nformula_4 weakly majorizes formula_5 from above written as formula_13 iff\n\nEquivalently, we say that formula_8 is weakly majorized by formula_9 from above, denoted as formula_17.\n\nIf formula_6 and in addition formula_19 we say that\nformula_4 majorizes (or dominates) formula_5 written as formula_22.\nEquivalently, we say that formula_8 is majorized (or dominated) by formula_9, denoted as formula_25.\n\nIt is easy to see that formula_22 if and only if formula_6 and formula_13.\n\nNote that the majorization order does not depend on the order of the components of the vectors formula_4 or formula_5. Majorization is not a partial order, since formula_22 and formula_32 do not imply formula_33, it only implies that the components of each vector are equal, but not necessarily in the same order.\n\nRegrettably, to confuse the matter, some literature sources use the reverse notation, e.g., formula_34 is replaced with formula_35, most notably, in Horn and Johnson, \"Matrix analysis\" (Cambridge Univ. Press, 1985), Definition 4.3.24, while the same authors switch to the traditional notation, introduced here, later in their \"Topics in Matrix Analysis\" (1994), and in the second edition of \"Matrix analysis\" (2013).\n\nA function formula_36 is said to be Schur convex when formula_37 implies formula_38. Similarly, formula_39 is Schur concave when formula_37 implies formula_41\n\nThe majorization partial order on finite sets, described here, can be generalized to the Lorenz ordering, a partial order on distribution functions.\n\nThe order of the entries does not affect the majorization, e.g., the statement formula_42 is simply \nequivalent to formula_43.\n\n(Strong) majorization: formula_44. For vectors with \"n\" components\n\n(Weak) majorization: formula_46. For vectors with \"n\" components:\n\nFor formula_48 we have\nformula_49 if and only if formula_50 is in the convex hull of all vectors obtained by permuting the coordinates of formula_51.\n\nFigure 1 displays the convex hull in 2D for the vector formula_52. Notice that the center of the convex hull, which is an interval in this case, is the vector formula_53. This is the \"smallest\" vector satisfying formula_49 for this given vector formula_51.\nFigure 2 shows the convex hull in 3D. The center of the convex hull, which is a 2D polygon in this case, is the \"smallest\" vector formula_50 satisfying formula_49 for this given vector formula_51.\n\nEach of the following statements is true if and only if formula_59:\n\n\n\n\nGiven formula_85, then formula_86 is said to \"majorize\" formula_87 if, for all formula_88, formula_89. If there is some formula_90 so that formula_89 for all formula_92, then formula_86 is said to \"dominate\" (or \"eventually dominate\") formula_87. Alternatively, the preceding terms are often defined requiring the strict inequality formula_95 instead of formula_89 in the foregoing definitions.\n\nVarious generalizations of majorization are discussed in chapters 14 and 15 of the reference work \"Inequalities: Theory of Majorization and Its Applications\". Albert W. Marshall, Ingram Olkin, Barry Arnold. Second edition. Springer Series in Statistics. Springer, New York, 2011. \n\n\n\n\n"}
{"id": "15978374", "url": "https://en.wikipedia.org/wiki?curid=15978374", "title": "Martin's maximum", "text": "Martin's maximum\n\nIn set theory, a branch of mathematical logic, Martin's maximum, introduced by , is a generalization of the proper forcing axiom, itself a generalization of Martin's axiom. It represents the broadest class of forcings for which a forcing axiom is consistent.\n\nMartin's maximum (MM) states that if \"D\" is a collection of formula_1 dense subsets of a notion of forcing that preserves stationary subsets of ω, then there is a \"D\"-generic filter. It is a well known fact that forcing with a ccc notion of forcing preserves stationary subsets of ω, thus MM extends MA(formula_1). If (\"P\",≤) is not a stationary set preserving notion of forcing, i.e., there is a stationary subset of ω, which becomes nonstationary when forcing with (\"P\",≤), then there is a collection \"D\" of formula_1 dense subsets of (\"P\",≤), such that there is no \"D\"-generic filter. This is why MM is called the maximal extension of Martin's axiom.\n\nThe existence of a supercompact cardinal implies the consistency of Martin's maximum. The proof uses Shelah's theories of semiproper forcing and iteration with revised countable supports.\n\nMM implies that the value of the continuum is formula_4 and that the ideal of nonstationary sets on ω is formula_4-saturated. It further implies stationary reflection, i.e., if \"S\" is a stationary subset of some regular cardinal κ≥ω and every element of \"S\" has countable cofinality, then there is an ordinal α<κ such that \"S\"∩α is stationary in α. In fact, \"S\" contains a closed subset of order type ω.\n\n\nTransfinite number\n"}
{"id": "1631409", "url": "https://en.wikipedia.org/wiki?curid=1631409", "title": "Matt Curtin", "text": "Matt Curtin\n\nMatt Curtin (born 1973) is a computer scientist and entrepreneur in Columbus, Ohio best known for his work in cryptography and firewall systems. He is the founder of Interhack Corporation, first faculty advisor of Open Source Club at The Ohio State University, and lecturer in the Department of Computer Science and Engineering at The Ohio State University, where he teaches a Common Lisp course. The author of two books, \"Developing Trust: Online Privacy and Security\" and \"\".\n\nCurtin's work includes helping to prove the weakness of the Data Encryption Standard and providing expert testimony in \"Blumofe v. Pharmatrak\", in which a key ruling was made by the U.S. Court of Appeals for the First Circuit, showing how the Electronic Communications Privacy Act (ECPA) applies to Web technology.\n\n"}
{"id": "26322871", "url": "https://en.wikipedia.org/wiki?curid=26322871", "title": "Microsoft Binary Format", "text": "Microsoft Binary Format\n\nIn computing, Microsoft Binary Format (MBF) was a format for floating-point numbers used in Microsoft's BASIC language products, including MBASIC, GW-BASIC and QuickBasic prior to version 4.00.\n\nIn 1975, Bill Gates and Paul Allen were working on Altair BASIC, which they were developing at Harvard University on a PDP-10 running their Altair emulator. One thing still missing was code to handle floating-point numbers, needed to support calculations with very big and very small numbers, which would be particularly useful for science and engineering. One of the proposed uses of the Altair was as a scientific calculator.\n\nAt a dinner at Currier House, an undergraduate residential house at Harvard, Gates and Allen complained to their dinner companions about having to write this code. One of them, Monte Davidoff, told them that he had written floating-point routines before and convinced Gates and Allen that he was capable of writing the Altair BASIC floating-point code. At the time there was no standard for floating-point numbers, so Davidoff had to come up with his own. He decided that 32 bits would allow enough range and precision. When Allen had to demonstrate it to MITS, it was the first time it ran on an actual Altair. But it worked, and when he entered ‘PRINT 2+2’, Davidoff's adding routine gave the correct answer.\n\nThe source code for Altair BASIC was thought to have been lost to history, but resurfaced in 2000. It had been sitting behind Gates's former tutor and dean Harry Lewis's file cabinet, who rediscovered it. A comment in the source credits Davidoff as the writer of Altair BASIC's math package.\n\nAltair BASIC took off, and soon most early home computers ran some form of Microsoft BASIC. The BASIC port for the 6502 CPU, such as used in the Commodore PET, took up more space due to the lower code density of the 6502. Because of this it would likely not fit in a single ROM chip together with the machine-specific input and output code. Since an extra chip was necessary, extra space was available, and this was used in part to extend the floating-point format from 32 to 40 bits. This extended format was not only provided by Commodore BASIC 1 & 2, but was also supported by AppleSoft BASIC I & II since version 1.1 (1977), KIM-1 BASIC since version 1.1a (1977), and MicroTAN BASIC since version 2b (1980). Not long afterwards the Z80 ports, such as Level II BASIC for the TRS-80 (1978), introduced the 64-bit, double-precision format as a separate data type from 32-bit, single-precision. Microsoft used the same floating-point formats in their implementation of Fortran and for their macro assembler MASM, although their spreadsheet Multiplan and their COBOL implementation used binary-coded decimal (BCD) floating point. Even so, for a while MBF became the de facto floating-point format on home computers, to the point where people still occasionally encounter legacy files and file formats using it.\n\nAs early as in 1976, Intel was starting the development of a floating-point coprocessor. Intel hoped to be able to sell a chip containing good implementations of all the operations found in the widely varying maths software libraries. John Palmer, who managed the project, contacted William Kahan, who suggested that Intel use the floating point of Digital Equipment Corporation's (DEC) VAX. The first VAX, the VAX-11/780 had just come out in late 1977, and its floating point was highly regarded. However, seeking to market their chip to the broadest possible market, Intel wanted the best floating point possible, and Kahan went on to draw up specifications. When rumours of Intel's new chip reached its competitors, they started a standardization effort, called IEEE 754, to prevent Intel from gaining too much ground. Kahan got Palmer's permission to participate; he was allowed to explain Intel's design decisions and their underlying reasoning, but not anything related to Intel's implementation architecture. VAX's floating-point formats differed from MBF only in that it had the sign in the most significant bit. It turned out that for double-precision numbers, an 8-bit exponent isn't wide enough for some wanted operations, e.g. to store the product of two 32-bit numbers.\n\nBoth Kahan's proposal and a counter-proposal by DEC therefore used 11 bits, like the time-tested 60-bit floating-point format of the CDC 6600 from 1965. Kahan's proposal also provided for infinities, which are useful when dealing with division-by-zero conditions; not-a-number values, which are useful when dealing with invalid operations; denormal numbers, which help mitigate problems caused by underflow; and a better balanced exponent bias, which can help avoid overflow and underflow when taking the reciprocal of a number. In 1980 the Intel 8087 chip was already released, but DEC remained opposed, to denormal numbers in particular, because of performance concerns and since it would give DEC a competitive advantage to standardise on DEC's format. The next year DEC had a study done in order to demonstrate that gradual underflow was a bad idea, but the study concluded the opposite, and DEC gave in. In 1985 the standard was ratified, but it had already become the de facto standard a year earlier, implemented by many manufacturers.\n\nBy the time QuickBASIC 4.00 was released, the IEEE 754 standard had become widely adopted—for example, it was incorporated into Intel's 387 coprocessor and every x86 processor from the 486 on. QuickBASIC versions 4.0 and 4.5 use IEEE 754 floating-point variables by default, but (at least in version 4.5) there is a command-line option for the IDE and the compiler that switches from IEEE to MBF floating-point numbers, to support earlier-written programs that rely on details of the MBF data formats. Visual Basic also uses the IEEE 754 format instead of MBF.\n\nMBF numbers consist of an 8-bit base-2 exponent with a bias of 128, so that exponents −127...−1 are represented by \"x\" = 1.'.127 (01h...7Fh), exponents 0...127 are represented by \"x\" = 128...255 (80h...FFh), with a special case for \"x\" = 0 (00h) representing the whole number being zero, a sign bit (positive mantissa: \"s\" = 0; negative mantissa: \"s\" = 1) and a 23-, 31- or 55-bit mantissa of the significand. There is always a 1-bit implied to the left of the explicit mantissa, and the radix point is located before this assumed bit. The MBF double-precision format provides less scale than the IEEE 754 format, and although the format itself provides almost one extra decimal digit of precision, in practice the stored values are less accurate because IEEE calculations use 80-bit intermediate results, and MBF doesn't. Unlike IEEE floating point, MBF doesn't support denormal numbers, infinities or NaNs.\n\nMBF single-precision format (32 bits, \"6-digit BASIC\"):\nMBF extended-precision format (40 bits, \"9?-digit BASIC\"):\nMBF double-precision format (64 bits):\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "455987", "url": "https://en.wikipedia.org/wiki?curid=455987", "title": "Operator algebra", "text": "Operator algebra\n\nIn functional analysis, an operator algebra is an algebra of continuous linear operators on a topological vector space with the multiplication given by the composition of mappings. \n\nThe results obtained in the study of operator algebras are phrased in algebraic terms, while the techniques used are highly analytic. Although the study of operator algebras is usually classified as a branch of functional analysis, it has direct applications to representation theory, differential geometry, quantum statistical mechanics, quantum information, and quantum field theory.\n\nOperator algebras can be used to study arbitrary sets of operators with little algebraic relation \"simultaneously\". From this point of view, operator algebras can be regarded as a generalization of spectral theory of a single operator. In general operator algebras are non-commutative rings.\n\nAn operator algebra is typically required to be closed in a specified operator topology inside the algebra of the whole continuous linear operators. In particular, it is a set of operators with both algebraic and topological closure properties. In some disciplines such properties are axiomized and algebras with certain topological structure become the subject of the research.\n\nThough algebras of operators are studied in various contexts (for example, algebras of pseudo-differential operators acting on spaces of distributions), the term \"operator algebra\" is usually used in reference to algebras of bounded operators on a Banach space or, even more specially in reference to algebras of operators on a separable Hilbert space, endowed with the operator norm topology.\n\nIn the case of operators on a Hilbert space, the Hermitian adjoint map on operators gives a natural involution which provides an additional algebraic structure which can be imposed on the algebra. In this context, the best studied examples are self-adjoint operator algebras, meaning that they are closed under taking adjoints. These include C*-algebras and von Neumann algebras. C*-algebras can be easily characterized abstractly by a condition relating the norm, involution and multiplication. Such abstractly defined C*-algebras can be identified to a certain closed subalgebra of the algebra of the continuous linear operators on a suitable Hilbert space. A similar result holds for von Neumann algebras.\n\nCommutative self-adjoint operator algebras can be regarded as the algebra of complex valued continuous functions on a locally compact space, or that of measurable functions on a standard measurable space. Thus, general operator algebras are often regarded as a noncommutative generalizations of these algebras, or the structure of the \"base space\" on which the functions are defined. This point of view is elaborated as the philosophy of noncommutative geometry, which tries to study various non-classical and/or pathological objects by noncommutative operator algebras.\n\nExamples of operator algebras which are not self-adjoint include:\n\n\n"}
{"id": "104790", "url": "https://en.wikipedia.org/wiki?curid=104790", "title": "Partition (number theory)", "text": "Partition (number theory)\n\nIn number theory and combinatorics, a partition of a positive integer \"n\", also called an integer partition, is a way of writing \"n\" as a sum of positive integers. Two sums that differ only in the order of their summands are considered the same partition. (If order matters, the sum becomes a composition.) For example, 4 can be partitioned in five distinct ways:\n\nThe order-dependent composition 1 + 3 is the same partition as 3 + 1, while the two distinct compositions 1 + 2 + 1 and 1 + 1 + 2 represent the same partition 2 + 1 + 1.\n\nA summand in a partition is also called a part. The number of partitions of \"n\" is given by the partition function \"p\"(\"n\"). So \"p\"(4) = 5. The notation \"λ\" ⊢ \"n\" means that \"λ\" is a partition of \"n\".\n\nPartitions can be graphically visualized with Young diagrams or Ferrers diagrams. They occur in a number of branches of mathematics and physics, including the study of symmetric polynomials, the symmetric group and in group representation theory in general.\n\nThe seven partitions of 5 are:\n\nIn some sources partitions are treated as the sequence of summands, rather than as an expression with plus signs. For example, the partition 2 + 2 + 1 might instead be written as the tuple or in the even more compact form where the superscript indicates the number of repetitions of a term.\n\nThere are two common diagrammatic methods to represent partitions: as Ferrers diagrams, named after Norman Macleod Ferrers, and as Young diagrams, named after the British mathematician Alfred Young. Both have several possible conventions; here, we use \"English notation\", with diagrams aligned in the upper-left corner.\n\nThe partition 6 + 4 + 3 + 1 of the positive number 14 can be represented\nby the following diagram:\nThe 14 circles are lined up in 4 rows, each having the size of a part of the partition. The diagrams for the 5 partitions of the number 4 are listed below:\n\nAn alternative visual representation of an integer partition is its \"Young diagram\". Rather than representing a partition with dots, as in the Ferrers diagram, the Young diagram uses boxes or squares. Thus, the Young diagram for the partition 5 + 4 + 1 is\nwhile the Ferrers diagram for the same partition is\n\nWhile this seemingly trivial variation doesn't appear worthy of separate mention, Young diagrams turn out to be extremely useful in the study of symmetric functions and group representation theory: in particular, filling the boxes of Young diagrams with numbers (or sometimes more complicated objects) obeying various rules leads to a family of objects called Young tableaux, and these tableaux have combinatorial and representation-theoretic significance. As a type of shape made by adjacent squares joined together, Young diagrams are a special kind of polyomino.\n\nIn number theory, the partition function \"p\"(\"n\") represents the number of possible partitions of a natural number \"n\", which is to say the number of distinct ways of representing \"n\" as a sum of natural numbers (with order irrelevant). By convention \"p\"(0) = 1, \"p\"(\"n\") = 0 for \"n\" negative.\n\nThe first few values of the partition function, starting with \"p\"(0) = 1, are:\n\nThe exact value of \"p\"(\"n\") for larger values of \"n\", is for example \"p\"(100) = 190,569,292, \"p\"(1000) is 24,061,467,864,032,622,473,692,149,727,991 or approximately 2.40615, and \"p\"(10000) is 36,167,251,325...,906,916,435,144 or approximately 3.61673.\n\n, the largest known prime number that counts a number of partitions is \"p\"(221444161), with 16569 decimal digits.\n\nThe generating function for \"p\"(\"n\") is given by\n\nExpanding each factor on the right-hand side as a geometric series, one can rewrite it as\n\nThe \"x\" term in this product counts the number of ways to write\n\nwhere each number \"i\" appears \"a\" times. This is precisely the definition of a partition of \"n\", so our product is the desired generating function. More generally, the generating function for the partitions of \"n\" into numbers from a set \"A\" can be found by taking only those terms in the product where \"k\" is an element of \"A\". This result is due to Euler.\n\nThe formulation of Euler's generating function is a special case of a \"q\"-Pochhammer symbol and is similar to the product formulation of many modular forms, and specifically the Dedekind eta function.\n\nThe denominator of the product is Euler's function and can be written, by the pentagonal number theorem, as\nwhere the exponents of \"x\" on the right hand side are the generalized pentagonal numbers; i.e., P = k(3k-1)/2 for \"k\" = 1, −1, 2, −2, 3, ... The signs in the summation alternate as formula_3 for a partition of \"k\" parts with largest part formula_4. This statistic (which is unrelated to the one described above) appears in the study of Ramanujan congruences.\n\nThere is a natural partial order on partitions given by inclusion of Young diagrams. This partially ordered set is known as \"Young's lattice\". The lattice was originally defined in the context of representation theory, where it is used to describe of the irreducible representations of symmetric groups \"S\" for all \"n\", together with their branching properties, in characteristic zero. It also has received significant study for its purely combinatorial properties; notably, it is the motivating example of a differential poset.\n\n\n"}
{"id": "421593", "url": "https://en.wikipedia.org/wiki?curid=421593", "title": "Permutable prime", "text": "Permutable prime\n\nA permutable prime, also known as anagrammatic prime, is a prime number which, in a given base, can have its digits' positions switched through any permutation and still be a prime number. H. E. Richert, who is supposedly the first to study these primes, called them permutable primes, but later they were also called absolute primes.\n\nIn base 10, all the permutable primes with fewer than 49,081 digits are known\n\nOf the above, there are 16 unique permutation sets, with smallest elements\n\nNote R = formula_1 is a repunit, a number consisting only of \"n\" ones (in base 10). Any repunit prime is a permutable prime with the above definition, but some definitions require at least two distinct digits.\n\nAll permutable primes of two or more digits are composed from the digits 1, 3, 7, 9, because no prime number except 2 is even, and no prime number besides 5 is divisible by 5. It is proven that no permutable prime exists which contains three different of the four digits 1, 3, 7, 9, as well as that there exists no permutable prime composed of two or more of each of two digits selected from 1, 3, 7, 9.\n\nThere is no \"n\"-digit permutable prime for 3 < \"n\" < 6·10 which is not a repunit. It is conjectured that there are no non-repunit permutable primes other than those listed above.\n\nIn base 2, only repunits can be permutable primes, because any 0 permuted to the ones place results in an even number. Therefore, the base 2 permutable primes are the Mersenne primes. The generalization can safely be made that for any positional number system, permutable primes with more than one digit can only have digits that are coprime with the radix of the number system. One-digit primes, meaning any prime below the radix, are always trivially permutable.\n\nIn base 12, the smallest elements of the unique permutation sets of the permutable primes with fewer than 9,739 digits are known (using inverted two and three for ten and eleven, respectively)\n\nThere is no \"n\"-digit permutable prime in base 12 for 4 < \"n\" < 12 which is not a repunit. It is conjectured that there are no non-repunit permutable primes in base 12 other than those listed above.\n\nIn base 10 and base 12, every permutable prime is a repunit or a near-repdigit, that is, it is a permutation of the integer \n\"P\"(\"b\", \"n\", \"x\", \"y\") = \"xxxx\"...\"xxxy\" (\"n\" digits, in base \"b\")\nwhere \"x\" and \"y\" are digits which is coprime to \"b\". Besides, \"x\" and \"y\" must be also coprime (since if there is a prime \"p\" divides both \"x\" and \"y\", then \"p\" also divides the number), so if \"x\" = \"y\", then \"x\" = \"y\" = 1. (This is not true in all bases, but exceptions are rare and could be finite in any given base; the only exceptions below 10 in bases up to 20 are: 139, 36A, 247, 78A, 29E (M. Fiorentini, 2015).)\n\nLet \"P\"(\"b\", \"n\", \"x\", \"y\") be a permutable prime in base \"b\" and let \"p\" be a prime such that \"n\" ≥ \"p\". If \"b\" is a primitive root of \"p\", and \"p\" does not divide \"x\" or |\"x\" - \"y\"|, then \"n\" is a multiple of \"p\" - 1. (Since \"b\" is a primitive root mod \"p\" and \"p\" does not divide |\"x\" − \"y\"|, the \"p\" numbers \"xxxx\"...\"xxxy\", \"xxxx\"...\"xxyx\", \"xxxx\"...\"xyxx\", ..., \"xxxx\"...\"xyxx\"...\"xxxx\" (only the \"b\" digit is \"y\", others are all \"x\"), \"xxxx\"...\"yxxx\"...\"xxxx\" (only the \"b\" digit is \"y\", others are all \"x\"), \"xxxx\"...\"xxxx\" (the repdigit with \"n\" \"x\"s) mod \"p\" are all different. That is, one is 0, another is 1, another is 2, ..., the other is \"p\" − 1. Thus, since the first \"p\" − 1 numbers are all primes, the last number (the repdigit with \"n\" \"x\"s) must be divisible by \"p\". Since \"p\" does not divide \"x\", so \"p\" must divide the repunit with \"n\" 1s. Since \"b\" is a primitive root mod \"p\", the multiplicative order of \"n\" mod \"p\" is \"p\" − 1. Thus, \"n\" must be divisible by \"p\" − 1)\n\nThus, if \"b\" = 10, the digits coprime to 10 are {1, 3, 7, 9}. Since 10 is a primitive root mod 7, so if \"n\" ≥ 7, then either 7 divides \"x\" (in this case, \"x\" = 7, since \"x\" ∈ {1, 3, 7, 9}) or |\"x\" − \"y\"| (in this case, \"x\" = \"y\" = 1, since \"x\", \"y\" ∈ {1, 3, 7, 9}. That is, the prime is a repunit) or \"n\" is a multiple of 7 − 1 = 6. Similarly, since 10 is a primitive root mod 17, so if \"n\" ≥ 17, then either 17 divides \"x\" (not possible, since \"x\" ∈ {1, 3, 7, 9}) or |\"x\" − \"y\"| (in this case, \"x\" = \"y\" = 1, since \"x\", \"y\" ∈ {1, 3, 7, 9}. That is, the prime is a repunit) or \"n\" is a multiple of 17 − 1 = 16. Besides, 10 is also a primitive root mod 19, 23, 29, 47, 59, 61, 97, 109, 113, 131, 149, 167, 179, 181, 193, ..., so \"n\" ≥ 17 is very impossible (since for this primes \"p\", if \"n\" ≥ \"p\", then \"n\" is divisible by \"p\" − 1), and if 7 ≤ \"n\" < 17, then \"x\" = 7, or \"n\" is divisible by 6 (the only possible \"n\" is 12). If \"b\" = 12, the digits coprime to 12 are {1, 5, 7, 11}. Since 12 is a primitive root mod 5, so if \"n\" ≥ 5, then either 5 divides \"x\" (in this case, \"x\" = 5, since \"x\" ∈ {1, 5, 7, 11}) or |\"x\" − \"y\"| (in this case, either \"x\" = \"y\" = 1 (That is, the prime is a repunit) or \"x\" = 1, \"y\" = 11 or \"x\" = 11, \"y\" = 1, since \"x\", \"y\" ∈ {1, 5, 7, 11}.) or \"n\" is a multiple of 5 − 1 = 4. Similarly, since 12 is a primitive root mod 7, so if \"n\" ≥ 7, then either 7 divides \"x\" (in this case, \"x\" = 7, since \"x\" ∈ {1, 5, 7, 11}) or |\"x\" − \"y\"| (in this case, \"x\" = \"y\" = 1, since \"x\", \"y\" ∈ {1, 5, 7, 11}. That is, the prime is a repunit) or \"n\" is a multiple of 7 − 1 = 6. Similarly, since 12 is a primitive root mod 17, so if \"n\" ≥ 17, then either 17 divides \"x\" (not possible, since \"x\" ∈ {1, 5, 7, 11}) or |\"x\" − \"y\"| (in this case, \"x\" = \"y\" = 1, since \"x\", \"y\" ∈ {1, 5, 7, 11}. That is, the prime is a repunit) or \"n\" is a multiple of 17 − 1 = 16. Besides, 12 is also a primitive root mod 31, 41, 43, 53, 67, 101, 103, 113, 127, 137, 139, 149, 151, 163, 173, 197, ..., so \"n\" ≥ 17 is very impossible (since for this primes \"p\", if \"n\" ≥ \"p\", then \"n\" is divisible by \"p\" − 1), and if 7 ≤ \"n\" < 17, then \"x\" = 7 (in this case, since 5 does not divide \"x\" or \"x\" − \"y\", so \"n\" must be divisible by 4) or \"n\" is divisible by 6 (the only possible \"n\" is 12).\n"}
{"id": "7151472", "url": "https://en.wikipedia.org/wiki?curid=7151472", "title": "Peter M. Neumann", "text": "Peter M. Neumann\n\nPeter Michael Neumann OBE (born 28 December 1940) is a British mathematician. He is a son of the mathematicians Bernhard Neumann and Hanna Neumann and, after gaining a B.A. from The Queen's College, Oxford in 1963, obtained his D.Phil from Oxford University in 1966.\n\nHe was a Tutorial Fellow at the Queen's College, Oxford and a lecturer at Oxford University. After retiring in 2008, he became an Emeritus Fellow at the Queen's College. His work has been in the field of group theory. He is also known for solving Alhazen's problem in 1997.\n\nIn 1987, he won the Lester R. Ford Award of the Mathematical Association of America for his review of Harold Edwards' book \"Galois Theory\".\nIn 2003, the London Mathematical Society awarded him the Senior Whitehead Prize. He was the first Chairman of the United Kingdom Mathematics Trust, from October 1996 to April 2004, succeeded by Bernard Silverman. He was appointed Officer of the Order of the British Empire (OBE) in the 2008 New Year Honours.\n"}
{"id": "28153829", "url": "https://en.wikipedia.org/wiki?curid=28153829", "title": "Rectilinear minimum spanning tree", "text": "Rectilinear minimum spanning tree\n\nIn graph theory, the rectilinear minimum spanning tree (RMST) of a set of \"n\" points in the plane (or more generally, in ℝ) is a minimum spanning tree of that set, where the weight of the edge between each pair of points is the rectilinear distance between those two points.\n\nBy explicitly constructing the complete graph on \"n\" vertices, which has \"n\"(\"n\"-1)/2 edges, a rectilinear minimum spanning tree can be found using existing algorithms for finding a minimum spanning tree. In particular, using Prim's algorithm with an adjacency matrix yields time complexity O(\"n\").\n\nIn the planar case, more efficient algorithms exist. They are based on the idea that connections may only happen with the nearest neighbour of a point in each octant - that is, each of the eight regions of the plane delimited by the coordinate axis from this point and their bisectors.\n\nThe resulting graph has only a linear number of edges and can be constructed in O(\"n\" log \"n\") using a divide and conquer algorithm or a sweep line algorithm.\n\nThe problem commonly arises in physical design of electronic circuits. In modern high-density integrated circuits wire routing is performed by wires which consist of segments running horizontally in one layer of metal and vertically in another metal layer. As a result, the wire length between two points is naturally measured with rectilinear distance. Although the routing of a whole net with multiple nodes is better represented by the rectilinear Steiner tree, the RMST provides a reasonable approximation and wire length estimate.\n"}
{"id": "4795751", "url": "https://en.wikipedia.org/wiki?curid=4795751", "title": "Riesz sequence", "text": "Riesz sequence\n\nIn mathematics, a sequence of vectors (\"x\") in a Hilbert space formula_1 is called a Riesz sequence if there exist constants formula_2 such that\n\nfor all sequences of scalars (\"a\") in the ℓ space ℓ. A Riesz sequence is called a Riesz basis if\n\nIf \"H\" is a finite-dimensional space, then every basis of \"H\" is a Riesz basis.\n\nLet formula_5 be in the \"L\" space \"L\"(R), let\n\nand let formula_7 denote the Fourier transform of formula_8. Define constants \"c\" and \"C\" with formula_2. Then the following are equivalent:\n\nThe first of the above conditions is the definition for (formula_12) to form a Riesz basis for the space it spans.\n\n"}
{"id": "7097464", "url": "https://en.wikipedia.org/wiki?curid=7097464", "title": "Ruth Aaronson Bari", "text": "Ruth Aaronson Bari\n\nRuth Aaronson Bari (November 17, 1917 – August 25, 2005) was an American mathematician known for her work in graph theory and algebraic homomorphisms. The daughter of Polish-Jewish immigrants to the United States, she was a professor at George Washington University beginning in 1966. She was the mother of environmental activist Judi Bari, science reporter Gina Kolata and art historian Martha Bari.\n\nBari grew up in Brooklyn and attended Brooklyn College, earning her bachelor's degree in mathematics in 1939. She earned her MA at Johns Hopkins University in 1943, but had originally enrolled in the doctoral program. When the university suggested that women in the graduate program should give up their fellowships so that men returning from World War II could study, Bari acceded. After marrying Arthur Bari she spent the next two decades devoted to family. She returned to Johns Hopkins, where she completed her dissertation on \"absolute reducibility of maps of at most 19 regions\" in 1966 at the age of 47.\n\nBari’s dissertation explored chromatic polynomials and the Birkhoff-Lewis conjecture. She determined that “Because of the fact that all other cubic maps with fewer than 20 regions contain at least one absolutely reducible configuration, it follows that the Birkhoff-Lewis conjecture holds for all maps with fewer than 20 regions.” Her Ph.D. advisor was Daniel Lewis, Jr. After she received her degree, mathematician William Tutte invited Bari to spend two weeks lecturing on her work in Canada at the University of Waterloo. Bari's work in the areas of graph theory and homomorphisms—and especially chromatic polynomials—has been recognized as influential. In 1976, two professors relied on computer work to solve the perennial problem of Bari’s dissertation, involving the four-color conjecture. When her daughter Martha asked her if she felt cheated by the technological solution, Bari replied, “I’m just grateful that it was solved within my lifetime and that I had the privilege to witness it.”\n\nDuring her teaching career, Bari participated in a class-action lawsuit against George Washington University which protested inequalities in promotion and pay for female faculty members. The protests were successful. Bari retired at the legally mandated age of 70 in 1988 with the distinction of professor emeritus.\n\nBari was active in the Washington, DC community. In the early 1970s, Bari used a grant from the National Science Foundation to start a master's degree program in teaching mathematics. She felt that math teachers in DC public schools were not as well prepared as they needed to be.\n\nHer daughters also became influential in their fields. Gina Kolata is a mathematics, health and science journalist for the \"New York Times\". Judi Bari (1949 – 1997) was a leading labor and environmental activist and feminist, who survived an assassination attempt. Dr. Martha Bari is an art historian at Hood College\n\nBari died on August 25, 2005. She was survived by her husband of 64 years of marriage, Arthur (1913-2006). Besides their three daughters, they had two grandchildren each by both Judi and Gina. She had lived in Silver Spring, Maryland since 1963 and was 87 years old at the time of her death.\n\n"}
{"id": "52796478", "url": "https://en.wikipedia.org/wiki?curid=52796478", "title": "Sage Manifolds", "text": "Sage Manifolds\n\nSageManifolds (following styling of SageMath) is an extension fully integrated into SageMath, to be used as a package for differential geometry and tensor calculus. The official page for the project is sagemanifolds.obspm.fr. It can be used on CoCalc.\n\nSageManifolds deals with \"differentiable manifolds\" of arbitrary dimension. The basic objects are \"tensor fields\" and not tensor components in a given vector frame or coordinate chart. In other words, various charts and frames can be introduced on the manifold and a given tensor field can have representations in each of them.\n\nAn important class of treated manifolds is that of \"pseudo-Riemannian manifolds\", among which \"Riemannian manifolds\" and \"Lorentzian manifolds\", with applications to General Relativity. In particular, SageManifolds implements the computation of the Riemann curvature tensor and associated objects (Ricci tensor, Weyl tensor). SageManifolds can also deal with \"generic affine connections\", not necessarily Levi-Civita ones.\n\nMore documentation is on doc.sagemath.org/html/en/reference/manifolds/.\n\nAs SageMath is, SageManifolds is a free and open source software based on the Python programming language. It is released under the GNU General Public License. To download and install SageManifolds, see here. It is more specifically GPL v2+ (meaning that a user may elect to use a licence higher than GPL version 2.)\n\nMuch of the source is on tickets at trac.sagemath.org.\n\nThere are Github repositories at github.com/sagemanifolds/SageManifolds.\n\nOther links are provided at sagemanifolds.obspm.fr/contact.html.\n"}
{"id": "360243", "url": "https://en.wikipedia.org/wiki?curid=360243", "title": "Several complex variables", "text": "Several complex variables\n\nThe theory of functions of several complex variables is the branch of mathematics dealing with complex valued functions\n\non the space of -tuples of complex numbers. As in complex analysis, which is the case but of a distinct character, these are not just any functions: they are supposed to be holomorphic or complex analytic, so that locally speaking they are power series in the variables .\n\nEquivalently, as it turns out, they are locally uniform limits of polynomials; or local solutions to the -dimensional Cauchy–Riemann equations.\n\nMany examples of such functions were familiar in nineteenth-century mathematics: abelian functions, theta functions, and some hypergeometric series. Naturally also any function of one variable that depends on some complex parameter is a candidate. The theory, however, for many years didn't become a full-fledged area in mathematical analysis, since its characteristic phenomena weren't uncovered. The Weierstrass preparation theorem would now be classed as commutative algebra; it did justify the local picture, ramification, that addresses the generalisation of the branch points of Riemann surface theory.\n\nWith work of Friedrich Hartogs, and of Kiyoshi Oka in the 1930s, a general theory began to emerge; others working in the area at the time were Heinrich Behnke, Peter Thullen and Karl Stein. Hartogs proved some basic results, such as every isolated singularity is removable, for any analytic function \nwhenever . Naturally the analogues of contour integrals will be harder to handle: when an integral surrounding a point should be over a three-dimensional manifold (since we are in four real dimensions), while iterating contour (line) integrals over two separate complex variables should come to a double integral over a two-dimensional surface. This means that the residue calculus will have to take a very different character.\n\nAfter 1945 important work in France, in the seminar of Henri Cartan, and Germany with Hans Grauert and Reinhold Remmert, quickly changed the picture of the theory. A number of issues were clarified, in particular that of analytic continuation. Here a major difference is evident from the one-variable theory: while for any open connected set in we can find a function that will nowhere continue analytically over the boundary, that cannot be said for . In fact the of that kind are rather special in nature (a condition called \"pseudoconvexity\"). The natural domains of definition of functions, continued to the limit, are called \"Stein manifolds\" and their nature was to make sheaf cohomology groups vanish. In fact it was the need to put (in particular) the work of Oka on a clearer basis that led quickly to the consistent use of sheaves for the formulation of the theory (with major repercussions for algebraic geometry, in particular from Grauert's work).\n\nFrom this point onwards there was a foundational theory, which could be applied to \"analytic geometry\" (a name adopted, confusingly, for the geometry of zeroes of analytic functions: this is not the analytic geometry learned at school), automorphic forms of several variables, and partial differential equations. The deformation theory of complex structures and complex manifolds was described in general terms by Kunihiko Kodaira and D. C. Spencer. The celebrated paper \"GAGA\" of Serre pinned down the crossover point from \"géometrie analytique\" to \"géometrie algébrique\".\n\nC. L. Siegel was heard to complain that the new \"theory of functions of several complex variables\" had few \"functions\" in it, meaning that the special function side of the theory was subordinated to sheaves. The interest for number theory, certainly, is in specific generalisations of modular forms. The classical candidates are the Hilbert modular forms and Siegel modular forms. These days these are associated to algebraic groups (respectively the Weil restriction from a totally real number field of , and the symplectic group), for which it happens that automorphic representations can be derived from analytic functions. In a sense this doesn't contradict Siegel; the modern theory has its own, different directions.\n\nSubsequent developments included the hyperfunction theory, and the edge-of-the-wedge theorem, both of which had some inspiration from quantum field theory. There are a number of other fields, such as Banach algebra theory, that draw on several complex variables.\n\nA function formula_3 defined on a domain formula_4 is called holomorphic if formula_3 satisfies one of the following two conditions.\n\nFor each index λ let\n\nand generalize the usual Cauchy–Riemann equation for one variable for each index λ, then we obtain\nLet\nthrough\nthe above equations (2) and (3) turn to be equivalent. \n\nTo show that above two conditions (i) and (ii) are equivalent, it is easy to prove (i) → (ii). To prove (ii) → (i) one uses Cauchy's integral formula on the n-multiple disc for several complex variables\nand then estimates the coefficients of the power series expansion formula_16 in (1). While in one variable case the Cauchy's integral formula is an integral over the circumference of a disc with some radius r, in several variables case over the surface of a multiple disc with radii formula_17 's as in (4).\n\nAs same as the one variable case, the identity theorem holds due to the properties of Laurent series that hold in several variable case.\n\nTherefore, Liouville's theorem for entire functions, and the maximal principle hold for several variables. Also, the inverse function theorem and implicit function theorem hold as in the one variable case.\n\nAs described in the previous there are similar results in several variables case as one variable case. However, there are very different aspects in several variable case. For example, Riemann mapping theorem, Mittag-Leffler's theorem, Weierstrass theorem, Runge's theorem and so on can not apply to the several variables case as it is in one variable case. The following example of analytic continuation in two variables shows these differences, which was one of motivations to complex analysis in several variables.\n\nIn several variables analytic continuation is defined in the same way as in one variable case. Namely, let formula_32 be open subsets in formula_33, formula_34 and formula_35. Assume that formula_36 and formula_37 is a connected component of formula_38. If formula_39 then formula_40 is defined as\nThe above formula_40 is called analytic continuation of formula_26 or formula_44. Note that formula_40 is uniquely determined by the identity theorem but may be multi-valued.\n\nIn one variable case, formula_46, for any open domain formula_47 there is a holomorphic function formula_26 on formula_8 that cannot be analytically continued beyond formula_8. This is because for any formula_51, formula_52 cannot be analytically continued beyond\nformula_53. However, in case of several variables formula_54, it can occur that there exists a strictly larger open domain formula_55 such that all formula_56 can be analytically continued to formula_57. This phenomenon is called Hartogs' phenomenon.\n\nThe simplest Stein manifold is the space (the complex -space), which consists of -tuples of complex numbers. It can be considered as an -dimensional vector space over complex numbers, which gives its dimension over . Hence, as a set, and as topological space, is identical to and its topological dimension is .\n\nIn coordinate-free language, any vector space over complex numbers may be thought of as a real vector space of twice dimensions, where a complex structure is specified with a linear operator (such that ) which defines the multiplication to the imaginary unit .\n\nAny such space, as a real space, is oriented. On the complex plane thought of as the Cartesian plane, multiplication to a complex number has the real matrix\n\na 2 × 2 real matrix that has the determinant\n\nLikewise, if one expresses any finite-dimensional complex linear operator as a real matrix (which will be composed from 2 × 2 blocks of the aforementioned form), then its determinant equals to the square of absolute value of the corresponding complex determinant. It is a non-negative number, which implies that the (real) orientation of the space is never reversed by a complex operator. The same applies to Jacobians of holomorphic functions from to .\n\n\n\n"}
{"id": "32234404", "url": "https://en.wikipedia.org/wiki?curid=32234404", "title": "Shuffle algebra", "text": "Shuffle algebra\n\nIn mathematics, a shuffle algebra is a Hopf algebra with a basis corresponding to words on some set, whose product is given by the shuffle product \"X\" ⧢ \"Y\" of two words \"X\", \"Y\": the sum of all ways of interlacing them. The interlacing is given by the riffle shuffle permutation.\n\nThe shuffle algebra on a finite set is the graded dual of the universal enveloping algebra of the free Lie algebra on the set.\n\nOver the rational numbers, the shuffle algebra is isomorphic to the polynomial algebra in the Lyndon words.\n\nThe shuffle product of words of lengths \"m\" and \"n\" is a sum over the ways of interleaving the two words, as shown in the following examples:\n\nIt may be defined inductively by\n\nThe shuffle product was introduced by . The name \"shuffle product\" refers to the fact that the product can be thought of as a sum over all ways of riffle shuffling two words together: this is the riffle shuffle permutation. The product is commutative and associative.\n\nThe shuffle product of two words in some alphabet is often denoted by the shuffle product symbol ⧢ (Unicode character U+2932 , derived from the Cyrillic letter sha).\n\nThe closely related infiltration product was introduced by . It is defined inductively on words over an alphabet \"A\" by\n\nFor example:\n\nThe infiltration product is also commutative and associative.\n\n\n\n"}
{"id": "2001331", "url": "https://en.wikipedia.org/wiki?curid=2001331", "title": "Simple precedence grammar", "text": "Simple precedence grammar\n\nA simple precedence grammar is a context-free formal grammar that can be parsed with a simple precedence parser. The concept was first developed by Niklaus Wirth and Helmut Weber from the ideas of Robert Floyd in their paper, \"EULER: a generalization of ALGOL, and its formal definition\", in the Communications of the ACM in 1966.\n\nG = (\"N\", Σ, \"P\", \"S\") is a simple precedence grammar if all the production rules in \"P\" comply with the following constraints:\n\n\nformula_2\n\nprecedence table:\n\n\n"}
{"id": "61220", "url": "https://en.wikipedia.org/wiki?curid=61220", "title": "Spintronics", "text": "Spintronics\n\nSpintronics (a portmanteau meaning spin transport electronics), also known as spin electronics, is the study of the intrinsic spin of the electron and its associated magnetic moment, in addition to its fundamental electronic charge, in solid-state devices.\n\nSpintronics fundamentally differs from traditional electronics in that, in addition to charge state, electron spins are exploited as a further degree of freedom, with implications in the efficiency of data storage and transfer. Spintronic systems are most often realised in dilute magnetic semiconductors (DMS) and Heusler alloys and are of particular interest in the field of quantum computing and neuromorphic computing.\n\nSpintronics emerged from discoveries in the 1980s concerning spin-dependent electron transport phenomena in solid-state devices. This includes the observation of spin-polarized electron injection from a ferromagnetic metal to a normal metal by Johnson and Silsbee (1985) and the discovery of giant magnetoresistance independently by Albert Fert et al. and Peter Grünberg et al. (1988). The origin of spintronics can be traced to the ferromagnet/superconductor tunneling experiments pioneered by Meservey and Tedrow and initial experiments on magnetic tunnel junctions by Julliere in the 1970s. The use of semiconductors for spintronics began with the theoretical proposal of a spin field-effect-transistor by Datta and Das in 1990 and of the electric dipole spin resonance by Rashba in 1960.\n\nThe spin of the electron is an intrinsic angular momentum that is separate from the angular momentum due to its orbital motion. The magnitude of the projection of the electron's spin along an arbitrary axis is formula_1, implying that the electron acts as a Fermion by the spin-statistics theorem. Like orbital angular momentum, the spin has an associated magnetic moment, the magnitude of which is expressed as\n\nIn a solid, the spins of many electrons can act together to affect the magnetic and electronic properties of a material, for example endowing it with a permanent magnetic moment as in a ferromagnet.\n\nIn many materials, electron spins are equally present in both the up and the down state, and no transport properties are dependent on spin. A spintronic device requires generation or manipulation of a spin-polarized population of electrons, resulting in an excess of spin up or spin down electrons. The polarization of any spin dependent property X can be written as\n\nA net spin polarization can be achieved either through creating an equilibrium energy split between spin up and spin down. Methods include putting a material in a large magnetic field (Zeeman effect), the exchange energy present in a ferromagnet or forcing the system out of equilibrium. The period of time that such a non-equilibrium population can be maintained is known as the spin lifetime, formula_4.\n\nIn a diffusive conductor, a spin diffusion length formula_5 can be defined as the distance over which a non-equilibrium spin population can propagate. Spin lifetimes of conduction electrons in metals are relatively short (typically less than 1 nanosecond). An important research area is devoted to extending this lifetime to technologically relevant timescales.\n\nThe mechanisms of decay for a spin polarized population can be broadly classified as spin-flip scattering and spin dephasing. Spin-flip scattering is a process inside a solid that does not conserve spin, and can therefore switch an incoming spin up state into an outgoing spin down state. Spin dephasing is the process wherein a population of electrons with a common spin state becomes less polarized over time due to different rates of electron spin precession. In confined structures, spin dephasing can be suppressed, leading to spin lifetimes of milliseconds in semiconductor quantum dots at low temperatures.\n\nSuperconductors can enhance central effects in spintronics such as magnetoresistance effects, spin lifetimes and dissipationless spin-currents.\n\nThe simplest method of generating a spin-polarised current in a metal is to pass the current through a ferromagnetic material. The most common applications of this effect involve giant magnetoresistance (GMR) devices. A typical GMR device consists of at least two layers of ferromagnetic materials separated by a spacer layer. When the two magnetization vectors of the ferromagnetic layers are aligned, the electrical resistance will be lower (so a higher current flows at constant voltage) than if the ferromagnetic layers are anti-aligned. This constitutes a magnetic field sensor.\n\nTwo variants of GMR have been applied in devices: (1) current-in-plane (CIP), where the electric current flows parallel to the layers and (2) current-perpendicular-to-plane (CPP), where the electric current flows in a direction perpendicular to the layers.\n\nOther metal-based spintronics devices:\n\nNon-volatile spin-logic devices to enable scaling are being extensively studied. Spin-transfer, torque-based logic devices that use spins and magnets for information processing have been proposed. These devices are part of the ITRS exploratory road map. Logic-in memory applications are already in the development stage. A 2017 review article can be found in \"Materials Today\".\n\nRead heads of magnetic hard drives are based on the GMR or TMR effect.\n\nMotorola developed a first-generation 256 kb magnetoresistive random-access memory (MRAM) based on a single magnetic tunnel junction and a single transistor that has a read/write cycle of under 50 nanoseconds. Everspin has since developed a 4 Mb version. Two second-generation MRAM techniques are in development: thermal-assisted switching (TAS) and spin-transfer torque (STT).\n\nAnother design, racetrack memory, encodes information in the direction of magnetization between domain walls of a ferromagnetic wire.\n\nMagnetic sensors can use the GMR effect.\n\nIn 2012, persistent spin helices of synchronized electrons were made to persist for more than a nanosecond, a 30-fold increase, longer than the duration of a modern processor clock cycle.\n\nDoped semiconductor materials display dilute ferromagnetism. In recent years, dilute magnetic oxides (DMOs) including ZnO based DMOs and TiO-based DMOs have been the subject of numerous experimental and computational investigations. Non-oxide ferromagnetic semiconductor sources (like manganese-doped gallium arsenide GaMnAs), increase the interface resistance with a tunnel barrier, or using hot-electron injection.\n\nSpin detection in semiconductors has been addressed with multiple techniques:\n\nThe latter technique was used to overcome the lack of spin-orbit interaction and materials issues to achieve spin transport in silicon.\n\nBecause external magnetic fields (and stray fields from magnetic contacts) can cause large Hall effects and magnetoresistance in semiconductors (which mimic spin-valve effects), the only conclusive evidence of spin transport in semiconductors is demonstration of spin precession and dephasing in a magnetic field non-collinear to the injected spin orientation, called the Hanle effect.\n\nApplications using spin-polarized electrical injection have shown threshold current reduction and controllable circularly polarized coherent light output. Examples include semiconductor lasers. Future applications may include a spin-based transistor having advantages over MOSFET devices such as steeper sub-threshold slope.\n\nMagnetic-tunnel transistor: The magnetic-tunnel transistor with a single base layer has the following terminals:\n\nThe magnetocurrent (MC) is given as:\n\nAnd the transfer ratio (TR) is\n\nMTT promises a highly spin-polarized electron source at room temperature.\n\nAntiferromagnetic storage media have been studied as an alternative to ferromagnetism, especially since with antiferromagnetic material the bits can be stored as well as with ferromagnetic material. Instead of the usual definition 0 -> 'magnetisation upwards', 1 -> 'magnetisation downwards', the states can be, e.g., 0 -> 'vertically-alternating spin configuration' and 1 -> 'horizontally-alternating spin configuration'.).\n\nThe main advantages of antiferromagnetic material are:\n\n\nResearch is being done into how to read and write information to antiferromagnetic spintronics as their net zero magnetization makes this difficult compared to conventional ferromagnetic spintronics. In modern MRAM, detection and manipulation of ferromagnetic order by magnetic fields has largely been done away with in favor of more efficient and scalable reading and writing by electrical current. Methods of reading and writing information by current rather than fields are also being investigated in antiferromagnets as fields are ineffective anyways. Writing methods currently being investigated in antiferromagnets are through spin-transfer torque and spin-orbit torque from the spin Hall effect and the Rashba effect. Reading information in antiferromagnets via magnetoresistance effects such as tunnel magnetoresistance is also being explored.\n\n\n\n"}
{"id": "51432", "url": "https://en.wikipedia.org/wiki?curid=51432", "title": "Surreal number", "text": "Surreal number\n\nIn mathematics, the surreal number system is a totally ordered proper class containing the real numbers as well as infinite and infinitesimal numbers, respectively larger or smaller in absolute value than any positive real number. The surreals share many properties with the reals, including the usual arithmetic operations (addition, subtraction, multiplication, and division); as such, they form an ordered field. If formulated in Von Neumann–Bernays–Gödel set theory, the surreal numbers are a universal ordered field in the sense that all other ordered fields, such as the rationals, the reals, the rational functions, the Levi-Civita field, the superreal numbers, and the hyperreal numbers, can be realized as subfields of the surreals. The surreals also contain all transfinite ordinal numbers; the arithmetic on them is given by the natural operations. It has also been shown (in Von Neumann–Bernays–Gödel set theory) that the maximal class hyperreal field is isomorphic to the maximal class surreal field; in theories without the axiom of global choice, this need not be the case, and in such theories it is not necessarily true that the surreals are a universal ordered field. \n\nIn 1907 Hans Hahn introduced Hahn series as a generalization of formal power series, and Hausdorff introduced certain ordered sets called η-sets for ordinals α and asked if it was possible to find a compatible ordered group or field structure. In 1962 Alling used a modified form of Hahn series to construct such ordered fields associated to certain ordinals α, and taking α to be the class of all ordinals in his construction gives a class that is an ordered field isomorphic to the surreal numbers. \nResearch on the go endgame by John Horton Conway led to another definition and construction of the surreal numbers. Conway's construction was introduced in Donald Knuth's 1974 book \"Surreal Numbers: How Two Ex-Students Turned on to Pure Mathematics and Found Total Happiness\". In his book, which takes the form of a dialogue, Knuth coined the term \"surreal numbers\" for what Conway had called simply \"numbers\". Conway later adopted Knuth's term, and used surreals for analyzing games in his 1976 book \"On Numbers and Games\".\n\nIn the Conway construction, the surreal numbers are constructed in stages, along with an ordering ≤ such that for any two surreal numbers \"a\" and \"b\", \"a\" ≤ \"b\" or \"b\" ≤ \"a\". (Both may hold, in which case \"a\" and \"b\" are equivalent and denote the same number.) Numbers are formed by pairing subsets of numbers already constructed: given subsets \"L\" and \"R\" of numbers such that all the members of \"L\" are strictly less than all the members of \"R\", then the pair { \"L\" | \"R\" } represents a number intermediate in value between all the members of \"L\" and all the members of \"R\".\n\nDifferent subsets may end up defining the same number: { \"L\" | \"R\" } and { \"L′\" | \"R′\" } may define the same number even if \"L\" ≠ \"L′\" and \"R\" ≠ \"R′\". (A similar phenomenon occurs when rational numbers are defined as quotients of integers: 1/2 and 2/4 are different representations of the same rational number.) So strictly speaking, the surreal numbers are equivalence classes of representations of form { \"L\" | \"R\" } that designate the same number.\n\nIn the first stage of construction, there are no previously existing numbers so the only representation must use the empty set: { | }. This representation, where \"L\" and \"R\" are both empty, is called 0. Subsequent stages yield forms like\n\nand\n\nThe integers are thus contained within the surreal numbers. (The above identities are definitions, in the sense that the right-hand side is a name for the left-hand side. That the names are actually appropriate will be evident when the arithmetic operations on surreal numbers are defined, as in the section below). Similarly, representations such as\n\narise, so that the dyadic rationals (rational numbers whose denominators are powers of 2) are contained within the surreal numbers.\n\nAfter an infinite number of stages, infinite subsets become available, so that any real number \"a\" can be represented by { \"L\" | \"R\" },\nwhere \"L\" is the set of all dyadic rationals less than \"a\" and \n\"R\" is the set of all dyadic rationals greater than \"a\" (reminiscent of a Dedekind cut). Thus the real numbers are also embedded within the surreals.\n\nThere are also representations like\n\nwhere ω is a transfinite number greater than all integers and ε is an infinitesimal greater than 0 but less than any positive real number. Moreover, the standard arithmetic operations (addition, subtraction, multiplication, and division) can be extended to these non-real numbers in a manner that turns the collection of surreal numbers into an ordered field, so that one can talk about 2ω or ω − 1 and so forth.\n\nSurreal numbers are constructed inductively as equivalence classes of pairs of sets of surreal numbers, restricted by the condition that each element of the first set is smaller than each element of the second set. The construction consists of three interdependent parts: the construction rule, the comparison rule and the equivalence rule.\n\nA \"form\" is a pair of sets of surreal numbers, called its \"left set\" and its \"right set\". A form with left set \"L\" and right set \"R\" is written { \"L\" | \"R\" }. When \"L\" and \"R\" are given as lists of elements, the braces around them are omitted.\n\nEither or both of the left and right set of a form may be the empty set. The form { { } | { } } with both left and right set empty is also written { | }.\n\nConstruction Rule\n\nThe numeric forms are placed in equivalence classes; each such equivalence class is a \"surreal number\". The elements of the left and right set of a form are drawn from the universe of the surreal numbers (not of \"forms\", but of their \"equivalence classes\").\n\nEquivalence Rule\n\nAn ordering relationship must be antisymmetric, i.e., it must have the property that \"x\" = \"y\" (i. e., \"x\" ≤ \"y\" and \"y\" ≤ \"x\" are both true) only when \"x\" and \"y\" are the same object. This is not the case for surreal number \"forms\", but is true by construction for surreal \"numbers\" (equivalence classes).\n\nThe equivalence class containing { | } is labeled 0; in other words, { | } is a form of the surreal number 0.\n\nThe recursive definition of surreal numbers is completed by defining comparison:\n\nGiven numeric forms \"x\" = { \"X\" | \"X\" } and \"y\" = { \"Y\" | \"Y\" }, \"x\" ≤ \"y\" if and only if:\nA comparison \"y\" ≤ \"c\" between a form \"y\" and a surreal number \"c\" is performed by choosing a form \"z\" from the equivalence class \"c\" and evaluating \"y\" ≤ \"z\"; and likewise for \"c\" ≤ \"x\" and for comparison \"b\" ≤ \"c\" between two surreal numbers.\n\nThis group of definitions is recursive, and requires some form of mathematical induction to define the universe of objects (forms and numbers) that occur in them. The only surreal numbers reachable via \"finite induction\" are the dyadic fractions; a wider universe is reachable given some form of transfinite induction.\n\nInduction Rule\n\nThe base case is actually a special case of the induction rule, with 0 taken as a label for the \"least ordinal\". Since there exists no \"S\" with \"i\" < 0, the expression formula_2 is the empty set; the only subset of the empty set is the empty set, and therefore \"S\" consists of a single surreal form { | } lying in a single equivalence class 0.\n\nFor every finite ordinal number \"n\", \"S\" is well-ordered by the ordering induced by the comparison rule on the surreal numbers.\n\nThe first iteration of the induction rule produces the three numeric forms { | 0 } < { | } < { 0 | } (the form { 0 | 0 } is non-numeric because 0≤0). The equivalence class containing { 0 | } is labeled 1 and the equivalence class containing { | 0 } is labeled −1. These three labels have a special significance in the axioms that define a ring; they are the additive identity (0), the multiplicative identity (1), and the additive inverse of 1 (−1). The arithmetic operations defined below are consistent with these labels.\n\nFor every \"i\" < \"n\", since every valid form in \"S\" is also a valid form in \"S\", all of the numbers in \"S\" also appear in \"S\" (as supersets of their representation in \"S\"). (The set union expression appears in our construction rule, rather than the simpler form \"S\", so that the definition also makes sense when \"n\" is a limit ordinal.) Numbers in \"S\" that are a superset of some number in \"S\" are said to have been \"inherited\" from generation \"i\". The smallest value of α for which a given surreal number appears in \"S\" is called its \"birthday\". For example, the birthday of 0 is 0, and the birthday of −1 is 1.\n\nA second iteration of the construction rule yields the following ordering of equivalence classes:\n\nComparison of these equivalence classes is consistent, irrespective of the choice of form. Three observations follow:\n\nThe informal interpretations of { 1 | } and { | −1 } are \"the number just after 1\" and \"the number just before −1\" respectively; their equivalence classes are labeled 2 and −2. The informal interpretations of { 0 | 1 } and { −1 | 0 } are \"the number halfway between 0 and 1\" and \"the number halfway between −1 and 0\" respectively; their equivalence classes are labeled / and −/. These labels will also be justified by the rules for surreal addition and multiplication below.\n\nThe equivalence classes at each stage \"n\" of induction may be characterized by their \"n\"-\"complete forms\" (each containing as many elements as possible of previous generations in its left and right sets). Either this complete form contains \"every\" number from previous generations in its left or right set, in which case this is the first generation in which this number occurs; or it contains all numbers from previous generations but one, in which case it is a new form of this one number. We retain the labels from the previous generation for these \"old\" numbers, and write the ordering above using the old and new labels:\n\nThe third observation extends to all surreal numbers with finite left and right sets. (For infinite left or right sets, this is valid in an altered form, since infinite sets might not contain a maximal or minimal element.) The number { 1, 2 | 5, 8 } is therefore equivalent to { 2 | 5 }; one can establish that these are forms of 3 by using the \"birthday property\", which is a consequence of the rules above.\n\nBirthday property\n\nThe addition, negation (additive inverse), and multiplication of surreal number \"forms\" \"x\" = { \"X\" | \"X\" } and \"y\" = { \"Y\" | \"Y\" } are defined by three recursive formulas.\n\nNegation of a given number \"x\" = { \"X\" | \"X\" } is defined by\nwhere the negation of a set \"S\" of numbers is given by the set of the negated elements of \"S\":\n\nThis formula involves the negation of the surreal \"numbers\" appearing in the left and right sets of \"x\", which is to be understood as the result of choosing a form of the number, evaluating the negation of this form, and taking the equivalence class of the resulting form. This only makes sense if the result is the same, irrespective of the choice of form of the operand. This can be proven inductively using the fact that the numbers occurring in \"X\" and \"X\" are drawn from generations earlier than that in which the form \"x\" first occurs, and observing the special case:\n\nThe definition of addition is also a recursive formula:\nwhere\n\nThis formula involves sums of one of the original operands and a surreal \"number\" drawn from the left or right set of the other. These are to be understood as the result of choosing a form of the numeric operand, performing the sum of the two forms, and taking the equivalence class of the resulting form. This only makes sense if the result is the same, irrespective of the choice of form of the numeric operand. This can also be proven inductively with the special cases:\n\nThe recursive formula for multiplication contains arithmetic expressions involving the operands and their left and right sets, such as the expression formula_7 that appears in the left set of the product of \"x\" and \"y\". This is to be understood as the set of surreal numbers resulting from choosing one number from each set that appears in the expression and evaluating the expression on these numbers. (In each individual evaluation of the expression, only one number is chosen from each set, and is substituted in each place where that set appears in the expression.)\n\nThis depends, in turn, on the ability to (a) multiply pairs of surreal \"numbers\" drawn from the left and right sets of \"x\" and \"y\" to get a surreal number, and negate the result; (b) multiply the surreal number \"form\" \"x\" or \"y\" and a surreal \"number\" drawn from the left or right set of the other operand to get a surreal number; and (c) add the resulting surreal numbers. This again involves special cases, this time containing 0 = { | }, the multiplicative identity 1 = { 0 | }, and its additive inverse -1 = { | 0 }.\n\nThe definition of division is done in terms of the reciprocal and multiplication:\n\nformula_9\n\nwhere\n\nformula_10\n\nfor positive formula_11. Only positive formula_12 are permitted in the formula, with any nonpositive terms being ignored (and formula_13 are always positive). This formula involves not only recursion in terms of being able to divide by numbers from the left and right sets of formula_11, but also recursion in that the members of the left and right sets of formula_15 itself. formula_16 is always a member of the left set of formula_15, and that can be used to find more terms in a recursive fashion. For example, if formula_18, then we know a left term of formula_19 will be formula_16. This in turn means formula_21 is a right term. This means formula_22 is a left term. This means formula_23 will be a right term. Continuing, this gives formula_24.\n\nFor negative formula_11, formula_15 is given by formula_27. If formula_28, then formula_29 is undefined.\n\nIt can be shown that the definitions of negation, addition and multiplication are consistent, in the sense that:\n\nWith these rules one can now verify that the numbers found in the first few generations were properly labeled. The construction rule is repeated to obtain more generations of surreals:\n\nFor each natural number (finite ordinal) \"n\", all numbers generated in \"S\" are dyadic fractions, i.e., can be written as an irreducible fraction formula_30\nwhere \"a\" and \"b\" are integers and 0 ≤ \"b\" < \"n\".\n\nThe set of all surreal numbers that are generated in some \"S\" for finite \"n\" may be denoted as \"S\" = formula_31. One may form the three classes \"S\" = { 0 }, \"S\" = formula_32, and \"S\" = formula_33, of which \"S\" is the union. No individual \"S\" is closed under addition and multiplication (except \"S\"), but \"S\" is; it is the subring of the rationals consisting of all dyadic fractions.\n\nThere are infinite ordinal numbers β for which the set of surreal numbers with birthday less than β is closed under the different arithmetic operations. For any ordinal α, the set of surreal numbers with birthday less than β = ω (using powers of ω) is closed under addition and forms a group; for birthday less than ω it is closed under multiplication and forms a ring; and for birthday less than an (ordinal) epsilon number ε it is closed under multiplicative inverse and forms a field. The latter sets are also closed under the exponential function as defined by Kruskal and Gonshor.\n\nHowever, it is always possible to construct a surreal number that is greater than any member of a set (by including the set on the left side of the constructor) and thus the collection of surreal numbers is a proper class. With their ordering and algebraic operations they constitute an ordered field, with the caveat that they do not form a set. In fact, it is a very special ordered field: the biggest one. Every other ordered field can be embedded in the surreals. The class of all surreal numbers is denoted by the symbol formula_34.\n\nDefine \"S\" as the set of all surreal numbers generated by the construction rule from subsets of \"S\". (This is the same inductive step as before, since the ordinal number ω is the smallest ordinal that is larger than all natural numbers; however, the set union appearing in the inductive step is now an infinite union of finite sets, and so this step can only be performed in a set theory that allows such a union.) A unique infinitely large positive number occurs in \"S\":\n\"S\" also contains objects that can be identified as the rational numbers. For example, the ω-complete form of the fraction / is given by:\nThe product of this form of / with any form of 3 is a form whose left set contains only numbers less than 1 and whose right set contains only numbers greater than 1; the birthday property implies that this product is a form of 1.\n\nNot only do all the rest of the rational numbers appear in \"S\"; the remaining finite real numbers do too. For example,\n\nThe only infinities in \"S\" are ω and −ω; but there are other non-real numbers in \"S\" among the reals. Consider the smallest positive number in \"S\":\nThis number is larger than zero but less than all positive dyadic fractions. It is therefore an infinitesimal number, often labeled ε. The ω-complete form of ε (resp. -ε) is the same as the ω-complete form of 0, except that 0 is included in the left (resp. right) set. The only \"pure\" infinitesimals in \"S\" are ε and its additive inverse -ε; adding them to any dyadic fraction \"y\" produces the numbers \"y\"±ε, which also lie in \"S\".\n\nOne can determine the relationship between ω and ε by multiplying particular forms of them to obtain:\nThis expression is only well-defined in a set theory which permits transfinite induction up to formula_39. In such a system, one can demonstrate that all the elements of the left set of ω · ε are positive infinitesimals and all the elements of the right set are positive infinities, and therefore ω · ε is the oldest positive finite number, i. e., 1. Consequently,\nSome authors systematically use ω in place of the symbol ε.\n\nGiven any \"x\" = { \"L\" | \"R\" } in \"S\", exactly one of the following is true:\n\n\"S\" is not an algebraic field, because it is not closed under arithmetic operations; consider ω+1, whose form formula_42 does not lie in any number in \"S\". The maximal subset of \"S\" that is closed under (finite series of) arithmetic operations is the field of real numbers, obtained by leaving out the infinities ±ω, the infinitesimals ±ε, and the infinitesimal neighbors \"y\"±ε of each nonzero dyadic fraction \"y\".\n\nThis construction of the real numbers differs from the Dedekind cuts of standard analysis in that it starts from dyadic fractions rather than general rationals and naturally identifies each dyadic fraction in \"S\" with its forms in previous generations. (The ω-complete forms of real elements of \"S\" are in one-to-one correspondence with the reals obtained by Dedekind cuts, under the proviso that Dedekind reals corresponding to rational numbers are represented by the form in which the cut point is omitted from both left and right sets.) The rationals are not an identifiable stage in the surreal construction; they are merely the subset \"Q\" of \"S\" containing all elements \"x\" such that \"x\" \"b\" = \"a\" for some \"a\" and some nonzero \"b\", both drawn from \"S\". By demonstrating that \"Q\" is closed under individual repetitions of the surreal arithmetic operations, one can show that it is a field; and by showing that every element of \"Q\" is reachable from \"S\" by a finite series (no longer than two, actually) of arithmetic operations \"including multiplicative inversion\", one can show that \"Q\" is strictly smaller than the subset of \"S\" identified with the reals.\n\nThe set \"S\" has the same cardinality as the real numbers \"R\". This can be demonstrated by exhibiting surjective mappings from \"S\" to the closed unit interval \"I\" of \"R\" and vice versa. Mapping \"S\" onto \"I\" is routine; map numbers less than or equal to ε (including -ω) to 0, numbers greater than or equal to 1-ε (including ω) to 1, and numbers between ε and 1-ε to their equivalent in \"I\" (mapping the infinitesimal neighbors \"y\"±ε of each dyadic fraction \"y\", along with \"y\" itself, to \"y\"). To map \"I\" onto \"S\", map the (open) central third (1/3, 2/3) of \"I\" onto { | } = 0; the central third (7/9, 8/9) of the upper third to { 0 | } = 1; and so forth. This maps a nonempty open interval of \"I\" onto each element of \"S\", monotonically. The residue of \"I\" consists of the Cantor set \"2\", each point of which is uniquely identified by a partition of the central-third intervals into left and right sets, corresponding precisely to a form { \"L\" | \"R\" } in \"S\". This places the Cantor set in one-to-one correspondence with the set of surreal numbers with birthday ω.\n\nContinuing to perform transfinite induction beyond \"S\" produces more ordinal numbers α, each represented as the largest surreal number having birthday α. (This is essentially a definition of the ordinal numbers resulting from transfinite induction.) The first such ordinal is ω+1 = { ω | }. There is another positive infinite number in generation ω+1:\nIt is important to observe that the surreal number ω−1 is not an ordinal; the ordinal ω is not the successor of any ordinal. This is a surreal number with birthday ω+1, which is labeled ω−1 on the basis that it coincides with the sum of ω = { 1, 2, 3, 4, ... | } and −1 = { | 0 }. Similarly, there are two new infinitesimal numbers in generation ω+1:\n\nAt a later stage of transfinite induction, there is a number larger than ω+\"k\" for all natural numbers \"k\":\nThis number may be labeled ω + ω both because its birthday is ω + ω (the first ordinal number not reachable from ω by the successor operation) and because it coincides with the surreal sum of ω and ω; it may also be labeled 2ω because it coincides with the product of ω = { 1, 2, 3, 4, ... | } and 2 = { 1 | }. It is the second limit ordinal; reaching it from ω via the construction step requires a transfinite induction on formula_43. This involves an infinite union of infinite sets, which is a \"stronger\" set theoretic operation than the previous transfinite induction required.\n\nNote that the \"conventional\" addition and multiplication of ordinals does not always coincide with these operations on their surreal representations. The sum of ordinals 1 + ω equals ω, but the surreal sum is commutative and produces 1 + ω = ω + 1 > ω. The addition and multiplication of the surreal numbers associated with ordinals coincides with the natural sum and natural product of ordinals.\n\nJust as 2ω is bigger than ω + \"n\" for any natural number \"n\", there is a surreal number ω/2 that is infinite but smaller than ω − \"n\" for any natural number \"n\". That is, ω/2 is defined by\nwhere on the right hand side the notation \"x\" − \"Y\" is used to mean { \"x\" − \"y\" : \"y\" in \"Y\" }. It can be identified as the product of ω and the form { 0 | 1 } of /. The birthday of / is the limit ordinal ω2.\n\nTo classify the \"orders\" of infinite and infinitesimal surreal numbers, also known as archimedean classes, Conway associated to each surreal number \"x\" the surreal number\nwhere \"r\" and \"s\" range over the positive real numbers. If \"x\" < \"y\" then ω is \"infinitely greater\" than ω, in that it is greater than \"r\" ω for all real numbers \"r\". Powers of ω also satisfy the conditions\nso they behave the way one would expect powers to behave.\n\nEach power of ω also has the redeeming feature of being the \"simplest\" surreal number in its archimedean class; conversely, every archimedean class within the surreal numbers contains a unique simplest member. Thus, for every positive surreal number \"x\" there will always exist some positive real number \"r\" and some surreal number \"y\" so that \"x\" − \"r\" ω is \"infinitely smaller\" than \"x\". The exponent \"y\" is the \"base ω logarithm\" of \"x\", defined on the positive surreals; it can be demonstrated that log maps the positive surreals onto the surreals and that log(\"xy\") = log(\"x\") + log(\"y\").\n\nThis gets extended by transfinite induction so that every surreal number \"x\" has a \"normal form\" analogous to the Cantor normal form for ordinal numbers. Every surreal number may be uniquely written as\nwhere every \"r\" is a nonzero real number and the \"y\"s form a strictly decreasing sequence of surreal numbers. This \"sum\", however, may have infinitely many terms, and in general has the length of an arbitrary ordinal number. (Zero corresponds of course to the case of an empty sequence, and is the only surreal number with no leading exponent.)\n\nLooked at in this manner, the surreal numbers resemble a power series field, except that the decreasing sequences of exponents must be bounded in length by an ordinal and are not allowed to be as long as the class of ordinals. This is the basis for the formulation of the surreal numbers as a Hahn series.\n\nIn contrast to the real numbers, a (proper) subset of the surreal numbers does not have a least upper (or lower) bound unless it has a maximal (minimal) element. Conway defines a gap as {\"L\" | \"R\"}, \"L\" < \"R\", \"L\" ∪ \"R\" = 𝐍𝐨; this is not a number because at least one of the sides is a proper class. Though similar, gaps are not quite the same as Dedekind sections, but we can still talk about a completion 𝐍𝐨 of the surreal numbers with the natural ordering which is a (proper class-sized) linear continuum.\n\nFor instance there is no least positive infinite surreal, but the gap ∞ = {\"x\": ∃ \"n\" ∈ ℕ: \"x\" < \"n\" | \"x\": ∀ \"n\" ∈ ℕ: \"x\" > \"n\"} is greater than all real numbers and less than all positive infinite surreals, and is thus the least upper bound of the reals in 𝐍𝐨. Similarly the gap 𝐎𝐧 = { 𝐍𝐨 | } is larger than all surreal numbers. (𝐎𝐧 is also the name of the class of ordinal numbers, and since 𝐎𝐧 is cofinal in 𝐍𝐨 and thus 𝐎𝐧 = { 𝐎𝐧 | } as well, this extends the equivalence of an ordinal α with the set of ordinals smaller than α).\n\nWith a bit of set-theoretic care 𝐍𝐨 can be equipped with a topology where the open sets are unions of open intervals (indexed by proper sets) and continuous functions can be defined. An equivalent of Cauchy sequences can be defined as well although they have to be indexed by the class of ordinals; these will always converge, but the limit may be either a number or a gap that can be expressed as ∑\"r\"ω with\n\"a\" decreasing and having no lower bound in 𝐍𝐨. (All such gaps can be understood as Cauchy sequences themselves but there are other types of gap that are not limits, such as ∞ and 𝐎𝐧).\n\nBased on unpublished work by Kruskal, a construction (by transfinite induction) that extends the real exponential function exp(\"x\") (with base \"e\") to the surreals was carried through by Gonshor.\n\nThe powers of ω function is also an exponential function, but does not have the properties desired for an extension of the function on the reals. It will, however, be needed in the development of the base-\"e\" exponential, and it is this function that is meant whenever the notation ω is used in the following.\n\nWhen \"y\" is a dyadic fraction, the power function , may be composed from multiplication, multiplicative inverse and square root, all of which can be defined inductively. Its values are completely determined by the basic relation and where defined it necessarily agrees with any other exponentiation that can exist.\n\nThe induction steps for the surreal exponential are based on the series expansion for the real exponential, more specifically those partial sums that can be shown by basic algebra to be positive but less than all later ones. For \"x\" positive these are denoted [\"x\"] and include all partial sums; for \"x\" negative but finite, [\"x\"] denotes the odd steps in the series starting from the first one with a positive real part (which always exists). For \"x\" negative infinite the odd-numbered partial sums are strictly decreasing and the [\"x\"] notation denotes the empty set, but it turns out that the corresponding elements are not needed in the induction.\n\nThe relations that hold for real\n\nare then\n\nand\n\nand this can be extended to the surreals with the definition \nUsing this definition, the following holds:\n\nThe surreal exponential is essentially given by its behaviour on positive powers of ω, i.e., the function \"g(a)\", combined with well-known behaviour on finite numbers. Only examples of the former will be given. In addition, holds for a large part of its range, for instance for any finite number with positive real part and any infinite number that is less than some iterated power of ω ( for some number of levels).\n\nA general exponentiation can be defined as , giving an interpretation to expressions like . Again it is essential to distinguish this definition from the \"powers of ω\" function, especially if ω may occur as the base.\n\nA surcomplex number is a number of the form , where \"a\" and \"b\" are surreal numbers. The surcomplex numbers form an algebraically closed field (except for being a proper class), isomorphic to the algebraic closure of the field generated by extending the rational numbers by a proper class of algebraically independent transcendental elements. Up to field isomorphism, this fact characterizes the field of surcomplex numbers within any fixed set theory.\n\nThe definition of surreal numbers contained one restriction: each element of L must be strictly less than each element of R. If this restriction is dropped we can generate a more general class known as \"games\". All games are constructed according to this rule:\n\nAddition, negation, and comparison are all defined the same way for both surreal numbers and games.\n\nEvery surreal number is a game, but not all games are surreal numbers, e.g. the game { 0 | 0 } is not a surreal number. The class of games is more general than the surreals, and has a simpler definition, but lacks some of the nicer properties of surreal numbers. The class of surreal numbers forms a field, but the class of games does not. The surreals have a total order: given any two surreals, they are either equal, or one is greater than the other. The games have only a partial order: there exist pairs of games that are neither equal, greater than, nor less than each other. Each surreal number is either positive, negative, or zero. Each game is either positive, negative, \"zero\", or \"fuzzy\" (incomparable with zero, such as {1|−1}).\n\nA move in a game involves the player whose move it is choosing a game from those available in L (for the left player) or R (for the right player) and then passing this chosen game to the other player. A player who cannot move because the choice is from the empty set has lost. A positive game represents a win for the left player, a negative game for the right player, a zero game for the second player to move, and a fuzzy game for the first player to move.\n\nIf \"x\", \"y\", and \"z\" are surreals, and \"x\"=\"y\", then \"x\" \"z\"=\"y\" \"z\". However, if \"x\", \"y\", and \"z\" are games, and \"x\"=\"y\", then it is not always true that \"x\" \"z\"=\"y\" \"z\". Note that \"=\" here means equality, not identity.\n\nThe surreal numbers were originally motivated by studies of the game Go, and there are numerous connections between popular games and the surreals. In this section, we will use a capitalized \"Game\" for the mathematical object {L|R}, and the lowercase \"game\" for recreational games like Chess or Go.\n\nWe consider games with these properties:\n\nFor most games, the initial board position gives no great advantage to either player. As the game progresses and one player starts to win, board positions will occur in which that player has a clear advantage. For analyzing games, it is useful to associate a Game with every board position. The value of a given position will be the Game {L|R}, where L is the set of values of all the positions that can be reached in a single move by Left. Similarly, R is the set of values of all the positions that can be reached in a single move by Right.\n\nThe zero Game (called 0) is the Game where L and R are both empty, so the player to move next (L or R) immediately loses. The sum of two Games G = { L1 | R1 } and H = { L2 | R2 } is defined as the Game G + H = { L1 + H, G + L2 | R1 + H, G + R2 } where the player to move chooses which of the Games to play in at each stage, and the loser is still the player who ends up with no legal move. One can imagine two chess boards between two players, with players making moves alternatively, but with complete freedom as to which board to play on. If G is the Game {L | R}, -G is the game {-R | -L}, i.e. with the role of the two players reversed. It is easy to show G - G = 0 for all Games G (where G - H is defined as G + (-H)).\n\nThis simple way to associate Games with games yields a very interesting result. Suppose two perfect players play a game starting with a given position whose associated Game is \"x\". We can classify all Games into four classes as follows:\n\n\nMore generally, we can define G > H as G - H > 0, and similarly for <, = and ||.\n\nThe notation G || H means that G and H are incomparable. G || H is equivalent to G−H || 0, i.e. that G > H, G < H and G = H are all false. Incomparable games are sometimes said to be \"confused\" with each other, because one or the other may be preferred by a player depending on what is added to it. A game confused with zero is said to be fuzzy, as opposed to positive, negative, or zero. An example of a fuzzy game is star (*).\n\nSometimes when a game nears the end, it will decompose into several smaller games that do not interact, except in that each player's turn allows moving in only one of them. For example, in Go, the board will slowly fill up with pieces until there are just a few small islands of empty space where a player can move. Each island is like a separate game of Go, played on a very small board. It would be useful if each subgame could be analyzed separately, and then the results combined to give an analysis of the entire game. This doesn't appear to be easy to do. For example, you might have two subgames where whoever moves first wins, but when they are combined into one big game, it's no longer the first player who wins. Fortunately, there is a way to do this analysis. Just use the following remarkable theorem:\n\nA game composed of smaller games is called the disjunctive sum of those smaller games, and the theorem states that the method of addition we defined is equivalent to taking the disjunctive sum of the addends.\n\nHistorically, Conway developed the theory of surreal numbers in the reverse order of how it has been presented here. He was analyzing Go endgames, and realized that it would be useful to have some way to combine the analyses of non-interacting subgames into an analysis of their disjunctive sum. From this he invented the concept of a Game and the addition operator for it. From there he moved on to developing a definition of negation and comparison. Then he noticed that a certain class of Games had interesting properties; this class became the surreal numbers. Finally, he developed the multiplication operator, and proved that the surreals are actually a field, and that it includes both the reals and ordinals.\n\nThe name surreal number was first used by Conway in 1972, but there are several alternative constructions developed both before and after that.\n\nIn what is now called the \"sign-expansion\" or \"sign-sequence\" of a surreal number, a surreal number is a function whose domain is an ordinal and whose codomain is { −1, +1 }. This is equivalent to Conway's L-R sequences.\n\nDefine the binary predicate \"simpler than\" on numbers by \"x\" is simpler than \"y\" if \"x\" is a proper subset of \"y\", \"i.e.\" if dom(\"x\") < dom(\"y\") and \"x\"(α) = \"y\"(α) for all α < dom(\"x\").\n\nFor surreal numbers define the binary relation < to be lexicographic order (with the convention that \"undefined values\" are greater than −1 and less than 1). So \"x\" < \"y\" if one of the following holds:\n\nEquivalently, let δ(\"x\",\"y\") = min({ dom(\"x\"), dom(\"y\")} ∪ { α :\nα < dom(\"x\") ∧ α < dom(\"y\") ∧ \"x\"(α) ≠ \"y\"(α) }),\nso that \"x\" = \"y\" if and only if δ(\"x\",\"y\") = dom(\"x\") = dom(\"y\"). Then, for numbers \"x\" and \"y\", \"x\" < \"y\" if and only if one of the following holds:\n\nFor numbers \"x\" and \"y\", \"x\" ≤ \"y\" if and only if \"x\" < \"y\" ∨ \"x\" = \"y\", and \"x\" > \"y\" if and only if \"y\" < \"x\". Also \"x\" ≥ \"y\" if and only if \"y\" ≤ \"x\".\n\nThe relation < is transitive, and for all numbers \"x\" and \"y\", exactly one of \"x\" < \"y\", \"x\" = \"y\", \"x\" > \"y\", holds (law of trichotomy). This means that < is a linear order (except that < is a proper class).\n\nFor sets of numbers, \"L\" and \"R\" such that ∀\"x\" ∈ \"L\" ∀\"y\" ∈ \"R\" (\"x\" < \"y\"), there exists a unique number \"z\" such that\n\nFurthermore, \"z\" is constructible from \"L\" and \"R\" by transfinite induction. \"z\" is the simplest number between \"L\" and \"R\". Let the unique number \"z\" be denoted by σ(\"L\",\"R\").\n\nFor a number \"x\", define its left set \"L\"(\"x\") and right set \"R\"(\"x\") by\n\nthen σ(\"L\"(\"x\"),\"R\"(\"x\")) = \"x\".\n\nOne advantage of this alternative realization is that equality is identity, not an inductively defined relation. Unlike Conway's realization of the surreal numbers, however, the sign-expansion requires a prior construction of the ordinals, while in Conway's realization, the ordinals are constructed as particular cases of surreals.\n\nHowever, similar definitions can be made that eliminate the need for prior construction of the ordinals. For instance, we could let the surreals be the (recursively-defined) class of functions whose domain is a subset of the surreals satisfying the transitivity rule ∀\"g\" ∈ dom \"f\" (∀\"h\" ∈ dom \"g\" (\"h\" ∈ dom \"f\" )) and whose range is { −, + }. \"Simpler than\" is very simply defined now—\"x\" is simpler than \"y\" if \"x\" ∈ dom \"y\". The total ordering is defined by considering \"x\" and \"y\" as sets of ordered pairs (as a function is normally defined): Either \"x\" = \"y\", or else the surreal number \"z\" = \"x\" ∩ \"y\" is in the domain of \"x\" or the domain of \"y\" (or both, but in this case the signs must disagree). We then have \"x\" < \"y\" if \"x\"(\"z\") = − or \"y\"(\"z\") = + (or both). Converting these functions into sign sequences is a straightforward task; arrange the elements of dom \"f\" in order of simplicity (i.e., inclusion), and then write down the signs that \"f\" assigns to each of these elements in order. The ordinals then occur naturally as those surreal numbers whose range is { + }.\n\nThe sum \"x\" + \"y\" of two numbers, \"x\" and \"y\", is defined by induction on dom(\"x\") and dom(\"y\") by \"x\" + \"y\" = σ(\"L\",\"R\"), where\n\nThe additive identity is given by the number 0 = { }, \"i.e.\" the number 0 is the unique function whose domain is the ordinal 0, and the additive inverse of the number \"x\" is the number − \"x\", given by dom(− \"x\") = dom(\"x\"), and, for α < dom(\"x\"), (− \"x\")(α) = − 1 if \"x\"(α) = + 1, and (− \"x\")(α) = + 1 if \"x\"(α) = − 1.\n\nIt follows that a number \"x\" is positive if and only if 0 < dom(\"x\") and \"x\"(0) = + 1, and \"x\" is negative if and only if 0 < dom(\"x\") and \"x\"(0) = − 1.\n\nThe product \"xy\" of two numbers, \"x\" and \"y\", is defined by induction on dom(\"x\") and dom(\"y\") by \"xy\" = σ(\"L\",\"R\"), where\n\nThe multiplicative identity is given by the number 1 = { (0,+ 1) }, \"i.e.\" the number 1 has domain equal to the ordinal 1, and 1(0) = + 1.\n\nThe map from Conway's realization to sign expansions is given by \"f\"({ \"L\" | \"R\" }) = σ(\"M\",\"S\"), where \"M\" = { \"f\"(\"x\") : \"x\" ∈ \"L\" } and \"S\" = { \"f\"(\"x\") : \"x\" ∈ \"R\" }.\n\nThe inverse map from the alternative realization to Conway's realization is given by \"g\"(\"x\") = { \"L\" | \"R\" }, where \"L\" = { \"g\"(\"y\") : \"y\" ∈ \"L\"(\"x\") } and \"R\" = { \"g\"(\"y\") : \"y\" ∈ \"R\"(\"x\") }.\n\nIn another approach to the surreals, given by Alling, explicit construction is bypassed altogether. Instead, a set of axioms is given that any particular approach to the surreals must satisfy. Much like the axiomatic approach to the reals, these axioms guarantee uniqueness up to isomorphism.\n\nA triple formula_44 is a surreal number system if and only if the following hold:\n\n\nBoth Conway's original construction and the sign-expansion construction of surreals satisfy these axioms.\n\nGiven these axioms, Alling derives Conway's original definition of ≤ and develops surreal arithmetic.\n\nA construction of the surreal numbers as a maximal binary pseudo-tree with simplicity (ancestor) and ordering relations is due to Philip Ehrlich, The difference from the usual definition of a tree is that the set of ancestors of a vertex is well-ordered, but may not have a maximal element (immediate predecessor); in other words the order type of that set is a general ordinal number, not just a natural number. This construction fulfills Alling's axioms as well and can easily be mapped to the sign-sequence representation.\n\nAlling also proves that the field of surreal numbers is isomorphic (as an ordered field) to the field of Hahn series with real coefficients on the value group of surreal numbers themselves (the series representation corresponding to the normal form of a surreal number, as defined above). This provides a connection between surreal numbers and more conventional mathematical approaches to ordered field theory.\n\nThis isomorphism makes the surreal numbers into a valued field where the valuation is the additive inverse of the exponent of the leading term in the Conway normal form, e.g., ν(ω) = -1. The valuation ring then consists of the finite surreal numbers (numbers with a real and/or an infinitesimal part). The reason for the sign inversion is that the exponents in the Conway normal form constitute a reverse well-ordered set, whereas Hahn series are formulated in terms of (non-reversed) well-ordered subsets of the value group.\n\nPhilip Ehrlich has constructed an isomorphism between Conway's maximal surreal number field and the maximal hyperreals in von Neumann–Bernays–Gödel set theory.\n\n\n\n"}
{"id": "1514392", "url": "https://en.wikipedia.org/wiki?curid=1514392", "title": "Training, validation, and test sets", "text": "Training, validation, and test sets\n\nIn machine learning, the study and construction of algorithms that can learn from and make predictions on data is a common task. Such algorithms work by making data-driven predictions or decisions, through building a mathematical model from input data.\n\nThe data used to build the final model usually comes from multiple datasets. In particular, three data sets are commonly used in different stages of the creation of the model.\n\nThe model is initially fit on a training dataset, that is a set of examples used to fit the parameters (e.g. weights of connections between neurons in artificial neural networks) of the model. The model (e.g. a neural net or a naive Bayes classifier) is trained on the training dataset using a supervised learning method (e.g. gradient descent or stochastic gradient descent). In practice, the training dataset often consist of pairs of an input vector (or scalar) and the corresponding output vector (or scalar), which is commonly denoted as the \"target\" (or \"label\"). The current model is run with the training dataset and produces a result, which is then compared with the \"target\", for each input vector in the training dataset. Based on the result of the comparison and the specific learning algorithm being used, the parameters of the model are adjusted. The model fitting can include both variable selection and parameter estimation.\n\nSuccessively, the fitted model is used to predict the responses for the observations in a second dataset called the validation dataset. The validation dataset provides an unbiased evaluation of a model fit on the training dataset while tuning the model's hyperparameters (e.g. the number of hidden units in a neural network). Validation datasets can be used for regularization by early stopping: stop training when the error on the validation dataset increases, as this is a sign of overfitting to the training dataset.\nThis simple procedure is complicated in practice by the fact that the validation dataset's error may fluctuate during training, producing multiple local minima. This complication has led to the creation of many ad-hoc rules for deciding when overfitting has truly begun.\n\nFinally, the test dataset is a dataset used to provide an unbiased evaluation of a \"final\" model fit on the training dataset.. When the data in the test dataset has never been used in training (for example in cross-validation), the test dataset is also called a holdout dataset.\n\nA training dataset is a dataset of examples used for learning, that is to fit the parameters (e.g., weights) of, for example, a classifier.\n\nMost approaches that search through training data for empirical relationships tend to overfit the data, meaning that they can identify apparent relationships in the training data that do not hold in general.\n\nA validation dataset is a dataset of examples used to tune the hyperparameters (i.e. the architecture) of a classifier. It is sometimes also called the development set or the \"dev set\". In artificial neural networks, a hyperparameter is, for example, the number of hidden units. It, as well as the testing set (as mentioned above), should follow the same probability distribution as the training dataset.\n\nIn order to avoid overfitting, when any classification parameter needs to be adjusted, it is necessary to have a validation dataset in addition to the training and test datasets. For example, if the most suitable classifier for the problem is sought, the training dataset is used to train the candidate algorithms, the validation dataset is used to compare their performances and decide which one to take and, finally, the test dataset is used to obtain the performance characteristics such as accuracy, sensitivity, specificity, F-measure, and so on. The validation dataset functions as a hybrid: it is training data used by testing, but neither as part of the low-level training nor as part of the final testing .\n\nThe basic process of using a validation dataset for model selection (as part of training dataset, validation dataset, and test dataset) is:\nAn application of this process is in early stopping, where the candidate models are successive iterations of the same network, and training stops when the error on the validation set grows, choosing the previous model (the one with minimum error).\n\nA test dataset is a dataset that is independent of the training dataset, but that follows the same probability distribution as the training dataset. If a model fit to the training dataset also fits the test dataset well, minimal overfitting has taken place (see figure below). A better fitting of the training dataset as opposed to the test dataset usually points to overfitting.\n\nA test set is therefore a set of examples used only to assess the performance (i.e. generalization) of a fully specified classifier.\n\nMost simply, part of the original dataset can be set aside and used as a test set: this is known as the holdout method.\n\nA dataset can be repeatedly split into a training dataset and a validation dataset: this is known as cross-validation. These repeated partitions can be done in various ways, such as dividing into 2 equal datasets and using them as training/validation, and then validation/training, or repeatedly selecting a random subset as a validation dataset. To validate the model performance, sometimes an additional test dataset that was held out from cross-validation is used.\n\nCross-validation doesn't work in situations where you can't shuffle your data, most notably in time-series.\n\nAnother example of parameter adjustment is hierarchical classification (sometimes referred to as instance space decomposition ), which splits a complete multi-class problem into a set of smaller classification problems. It serves for learning more accurate concepts due to simpler classification boundaries in subtasks and individual feature selection procedures for subtasks. When doing classification decomposition, the central choice is the order of combination of smaller classification steps, called the classification path. Depending on the application, it can be derived from the confusion matrix and, uncovering the reasons for typical errors and finding ways to prevent the system make those in the future. For example, on the validation set one can see which classes are most frequently mutually confused by the system and then the instance space decomposition is done as follows: firstly, the classification is done among well recognizable classes, and the difficult to separate classes are treated as a single joint class, and finally, as a second classification step the joint class is classified into the two initially mutually confused classes.\n\n\n"}
{"id": "11502667", "url": "https://en.wikipedia.org/wiki?curid=11502667", "title": "Xcas", "text": "Xcas\n\nXcas is a user interface to Giac, a free, basic Computer Algebra System (CAS) for Microsoft Windows, Apple macOS and Linux/Unix. Giac can be used directly inside software written in C++.\n\nGiac has a compatibility mode with Maple and MuPAD and Qcas and ExpressionsinBar software and TI-89, TI-92, Voyage 200 and TI-Nspire calculators. Users can use Giac/Xcas as well as a free software compatible with Maple to develop formal algorithms or use it in other software. Among other things Xcas can solve equations and draw graphs.\n\nCmathOOoCAS, an OpenOffice.org plugin which allows formal calculation in Calc spreadsheet and Writer word processing, uses Xcas to perform calculations.\n\nHere is a brief overview of what Xcas is able to do:\n\nXcas and Giac are open-source projects developed by Bernard Parisse's et al. at the Joseph Fourier University of Grenoble (Isère), France, since 2000. It is based on experiences gained with Parisse's former project Erable.\n\nPocket CAS and CAS Calc P11 utilize Giac. In 2013, it was also integrated into GeoGebra's CAS view.\n\nThe system was also chosen by Hewlett-Packard as the CAS for their HP Prime calculator, which utilizes the Giac/Xcas 1.4.9 engine under a dual-license scheme.\n\n\n"}
{"id": "46789502", "url": "https://en.wikipedia.org/wiki?curid=46789502", "title": "Yoshiharu Kohayakawa", "text": "Yoshiharu Kohayakawa\n\nYoshiharu Kohayakawa (Japanese: 小早川美晴; born 1963) is a Japanese-Brazilian mathematician working on discrete mathematics and probability theory. He is known for his work on Szemerédi's regularity lemma, which he extended to sparser graphs.\n\nKohayakawa was a student of Béla Bollobás on the University of Cambridge.\n\nAccording to Google Scholar, as of December 8, 2017, Kohayakawa's works have been cited over 2810 times, and his h-index is 31.\n\nHe is a titular member of the Brazilian Academy of Sciences.\n\nIn 2000, five American researchers received an USA NSF Research Grant in the value of $20,000 to go to Brazil to work in collaboration with him on mathematical problems.\n\nKohayakawa has an Erdős number of 1.\n\nHe was awarded the 2018 Fulkerson Prize.\n\n"}
{"id": "43613625", "url": "https://en.wikipedia.org/wiki?curid=43613625", "title": "Łukasiewicz–Moisil algebra", "text": "Łukasiewicz–Moisil algebra\n\nŁukasiewicz–Moisil algebras (LM algebras) were introduced in the 1940s by Grigore Moisil (initially under the name of Łukasiewicz algebras) in the hope of giving algebraic semantics for the \"n\"-valued Łukasiewicz logic. However, in 1956 Alan Rose discovered that for \"n\" ≥ 5, the Łukasiewicz–Moisil algebra does not model the Łukasiewicz logic. A faithful model for the ℵ-valued (infinitely-many-valued) Łukasiewicz–Tarski logic was provided by C. C. Chang's MV-algebra, introduced in 1958. For the axiomatically more complicated (finite) \"n\"-valued Łukasiewicz logics, suitable algebras were published in 1977 by Revaz Grigolia and called MV-algebras. MV-algebras are a subclass of LM-algebras, and the inclusion is strict for \"n\" ≥ 5. In 1982 Roberto Cignoli published some additional constraints that added to LM-algebras produce proper models for \"n\"-valued Łukasiewicz logic; Cignoli called his discovery proper Łukasiewicz algebras.\n\nMoisil however published in 1964 a logic to match his algebra (in the general \"n\" ≥ 5 case), now called Moisil logic. After coming in contact with Zadeh's fuzzy logic, in 1968 Moisil also introduced an infinitely-many-valued logic variant and its corresponding LM algebras. Although the Łukasiewicz implication cannot be defined in a LM algebra for \"n\" ≥ 5, the Heyting implication can be, i.e. LM algebras are Heyting algebras; as a result, Moisil logics can also be developed (from a purely logical standpoint) in the framework of Brower’s intuitionistic logic.\n\nA LM algebra is a De Morgan algebra (a notion also introduced by Moisil) with \"n\"-1 additional unary, \"modal\" operations: formula_1, i.e. an algebra of signature formula_2 where \"J\" = { 1, 2, ... \"n\"-1 }. (Some sources denote the additional operators as formula_3 to emphasize that they depend on the order \"n\" of the algebra.) The additional unary operators ∇ must satisfy the following axioms for all \"x\", \"y\" ∈ \"A\" and \"j\", \"k\" ∈ \"J\":\n\n\nThe duals of some of the above axioms follow as properties:\n\nAdditionally: formula_12 and formula_13. In other words, the unary \"modal\" operations formula_14 are lattice endomorphisms.\n\nLM algebras are the Boolean algebras. The canonical Łukasiewicz algebra formula_15 that Moisil had in mind were over the set \"L_n\" = } with negation formula_16 conjunction formula_17 and disjunction formula_18 and the unary \"modal\" operators:\n\nIf \"B\" is a Boolean algebra, then the algebra over the set \"B\" ≝ {(\"x\", \"y\") ∈ \"B\"×\"B\" | \"x\" ≤ \"y\"} with the lattice operations defined pointwise and with ¬(\"x\", \"y\") ≝ (¬\"y\", ¬\"x\"), and with the unary \"modal\" operators ∇(\"x\", \"y\") ≝ (\"y\", \"y\") and ∇(\"x\", \"y\") = ¬∇¬(\"x\", \"y\") = (\"x\", \"x\") [derived by axiom 4] is a three-valued Łukasiewicz algebra.\n\nMoisil proved that every LM algebra can be embedded in a direct product (of copies) of the canonical formula_15 algebra. As a corollary, every LM algebra is a subdirect product of subalgebras of formula_15.\n\nThe Heyting implication can be defined as:\n\nAntonio Monteiro showed that for every monadic Boolean algebra one can construct a trivalent Łukasiewicz algebra (by taking certain equivalence classes) and that any trivalent Łukasiewicz algebra is isomorphic to a Łukasiewicz algebra thus derived from a monadic Boolean algebra. Cignoli summarizes the importance of this result as: \"Since it was shown by Halmos that monadic Boolean algebras are the algebraic counterpart of classical first order monadic calculus, Monteiro considered that the representation of three-valued Łukasiewicz algebras into monadic Boolean algebras gives a proof of the consistency of Łukasiewicz three-valued logic relative to classical logic.\"\n\n"}
