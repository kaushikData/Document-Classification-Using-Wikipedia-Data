{"id": "29832799", "url": "https://en.wikipedia.org/wiki?curid=29832799", "title": "Advisory Committee on Mathematics Education", "text": "Advisory Committee on Mathematics Education\n\nThe Advisory Committee on Mathematics Education (ACME) is a British policy council for the Royal Society based in London, England. Founded in 2002 by the Royal Society and the Joint Mathematical Council, ACME analyzes mathematics education practices and provides advice on education policy. ACME is funded by the Gatsby Charitable Foundation (2002-2015) and the Department for Education.\n\nThe committee chair is appointed for a three year term. As of 2018, the membership is composed of:\n\n"}
{"id": "30979615", "url": "https://en.wikipedia.org/wiki?curid=30979615", "title": "Ahlfors finiteness theorem", "text": "Ahlfors finiteness theorem\n\nIn the mathematical theory of Kleinian groups, the Ahlfors finiteness theorem describes the quotient of the domain of discontinuity by a finitely generated Kleinian group. The theorem was proved by , apart from a gap that was filled by .\n\nThe Ahlfors finiteness theorem states that if Γ is a finitely-generated Kleinian group with region of discontinuity Ω, then\nΩ/Γ has a finite number of components, each of which is a compact Riemann surface with a finite number of points removed.\n\nThe Bers area inequality is a quantitative refinement of the Ahlfors finiteness theorem proved by . It states that if Γ is a non-elementary finitely-generated Kleinian group with \"N\" generators and with region of discontinuity Ω, then\nwith equality only for Schottky groups. (The area is given by the Poincaré metric in each component.)\nMoreover, if Ω is an invariant component then\nwith equality only for Fuchsian groups of the first kind (so in particular there can be at most two invariant components).\n\n"}
{"id": "6348217", "url": "https://en.wikipedia.org/wiki?curid=6348217", "title": "Albert Lautman", "text": "Albert Lautman\n\nAlbert Lautman (February 8, 1908 – August 1, 1944) was a French philosopher of mathematics, born in Paris. An escaped prisoner of war, was shot by the German authorities in Toulouse on 1 August 1944.\n\nHis father was a Jewish emigrant from Vienna who became a medical doctor after he was seriously wounded in the First World War.\n\n\n\n"}
{"id": "3107", "url": "https://en.wikipedia.org/wiki?curid=3107", "title": "Asymptote", "text": "Asymptote\n\nIn analytic geometry, an asymptote () of a curve is a line such that the distance between the curve and the line approaches zero as one or both of the \"x\" or \"y\" coordinates tends to infinity. Some sources include the requirement that the curve may not cross the line infinitely often, but this is unusual for modern authors. In projective geometry and related contexts, an asymptote of a curve is a line which is tangent to the curve at a point at infinity.\n\nThe word asymptote is derived from the Greek ἀσύμπτωτος (\"asumptōtos\") which means \"not falling together\", from ἀ priv. + σύν \"together\" + πτωτ-ός \"fallen\". The term was introduced by Apollonius of Perga in his work on conic sections, but in contrast to its modern meaning, he used it to mean any line that does not intersect the given curve.\n\nThere are three kinds of asymptotes: \"horizontal\", \"vertical\" and \"oblique\" asymptotes. For curves given by the graph of a function , horizontal asymptotes are horizontal lines that the graph of the function approaches as \"x\" tends to Vertical asymptotes are vertical lines near which the function grows without bound. An oblique asymptote has a slope that is non-zero but finite, such that the graph of the function approaches it as \"x\" tends to \n\nMore generally, one curve is a \"curvilinear asymptote\" of another (as opposed to a \"linear asymptote\") if the distance between the two curves tends to zero as they tend to infinity, although the term \"asymptote\" by itself is usually reserved for linear asymptotes.\n\nAsymptotes convey information about the behavior of curves \"in the large\", and determining the asymptotes of a function is an important step in sketching its graph. The study of asymptotes of functions, construed in a broad sense, forms a part of the subject of asymptotic analysis.\n\nThe idea that a curve may come arbitrarily close to a line without actually becoming the same may seem to counter everyday experience. The representations of a line and a curve as marks on a piece of paper or as pixels on a computer screen have a positive width. So if they were to be extended far enough they would seem to merge, at least as far as the eye could discern. But these are physical representations of the corresponding mathematical entities; the line and the curve are idealized concepts whose width is 0 (see Line). Therefore, the understanding of the idea of an asymptote requires an effort of reason rather than experience.\n\nConsider the graph of the function formula_1 shown to the right. The coordinates of the points on the curve are of the form formula_2 where x is a number other than 0. For example, the graph contains the points (1, 1), (2, 0.5), (5, 0.2), (10, 0.1), … As the values of formula_3 become larger and larger, say 100, 1,000, 10,000 …, putting them far to the right of the illustration, the corresponding values of formula_4, .01, .001, .0001, …, become infinitesimal relative to the scale shown. But no matter how large formula_3 becomes, its reciprocal formula_6 is never 0, so the curve never actually touches the \"x\"-axis. Similarly, as the values of formula_3 become smaller and smaller, say .01, .001, .0001, …, making them infinitesimal relative to the scale shown, the corresponding values of formula_4, 100, 1,000, 10,000 …, become larger and larger. So the curve extends farther and farther upward as it comes closer and closer to the \"y\"-axis. Thus, both the \"x\" and \"y\"-axes are asymptotes of the curve. These ideas are part of the basis of concept of a limit in mathematics, and this connection is explained more fully below.\n\nThe asymptotes most commonly encountered in the study of calculus are of curves of the form . These can be computed using limits and classified into \"horizontal\", \"vertical\" and \"oblique\" asymptotes depending on their orientation. Horizontal asymptotes are horizontal lines that the graph of the function approaches as \"x\" tends to +∞ or −∞. As the name indicates they are parallel to the \"x\"-axis. Vertical asymptotes are vertical lines (perpendicular to the \"x\"-axis) near which the function grows without bound. Oblique asymptotes are diagonal lines such that the difference between the curve and the line approaches 0 as \"x\" tends to +∞ or −∞. \n\nThe line \"x\" = \"a\" is a \"vertical asymptote\" of the graph of the function if at least one of the following statements is true:\n\n\nwhere formula_11 is the limit as \"x\" approaches the value \"a\" from the left (from lesser values), and formula_12 is the limit as \"x\" approaches \"a\" from the right.\n\nFor example, if ƒ(\"x\") = \"x\"/(\"x\"–1), the numerator approaches 1 and the denominator approaches 0 as \"x\" approaches 1. So\nand the curve has a vertical asymptote \"x\"=1. \n\nThe function \"ƒ\"(\"x\") may or may not be defined at \"a\", and its precise value at the point \"x\" = \"a\" does not affect the asymptote. For example, for the function\n\nhas a limit of +∞ as , \"ƒ\"(\"x\") has the vertical asymptote , even though \"ƒ\"(0) = 5. The graph of this function does intersect the vertical asymptote once, at (0,5). It is impossible for the graph of a function to intersect a vertical asymptote (or a vertical line in general) in more than one point. Moreover, if a function is continuous at each point where it is defined, it is impossible that its graph does intersect any vertical asymptote.\n\nA common example of a vertical asymptote is the case of a rational function at a point x such that the denominator is zero and the numerator is non-zero.\n\nIf a function has a vertical asymptote, then it isn't necessarily true that the derivative of the function has a vertical asymptote at the same place. An example is\n\nThis function has a vertical asymptote at formula_18 because \n\nand\n\nThe derivative of formula_21 is the function\n\nFor the sequence of points\n\nthat approaches formula_25 both from the left and from the right, the values formula_26 are constantly formula_27. Therefore, both one-sided limits of formula_28 at formula_27 can be neither formula_30 nor formula_31. Hence formula_32 doesn't have a vertical asymptote at formula_25.\n\n\"Horizontal asymptotes\" are horizontal lines that the graph of the function approaches as . The horizontal line \"y\" = \"c\" is a horizontal asymptote of the function \"y\" = \"ƒ\"(\"x\") if\nIn the first case, \"ƒ\"(\"x\") has \"y\" = \"c\" as asymptote when \"x\" tends to −∞, and in the second \"ƒ\"(\"x\") has \"y\" = \"c\" as an asymptote as \"x\" tends to +∞\n\nFor example, the arctangent function satisfies\n\nSo the line is a horizontal tangent for the arctangent when \"x\" tends to −∞, and is a horizontal tangent for the arctangent when \"x\" tends to +∞.\n\nFunctions may lack horizontal asymptotes on either or both sides, or may have one horizontal asymptote that is the same in both directions. For example, the function has a horizontal asymptote at \"y\" = 0 when \"x\" tends both to −∞ and +∞ because, respectively,\n\nWhen a linear asymptote is not parallel to the \"x\"- or \"y\"-axis, it is called an \"oblique asymptote\" or \"slant asymptote\". A function \"f\"(\"x\") is asymptotic to the straight line (\"m\" ≠ 0) if\n\nIn the first case the line is an oblique asymptote of \"ƒ\"(\"x\") when \"x\" tends to +∞, and in the second case the line is an oblique asymptote of \"ƒ(x)\" when \"x\" tends to −∞.\n\nAn example is ƒ(\"x\") = \"x\" + 1/\"x\", which has the oblique asymptote \"y\" = \"x\" (that is \"m\" = 1, \"n\" = 0) as seen in the limits\n\nThe asymptotes of many elementary functions can be found without the explicit use of limits (although the derivations of such methods typically use limits).\n\nThe oblique asymptote, for the function \"f\"(\"x\"), will be given by the equation \"y\"=\"mx\"+\"n\". The value for \"m\" is computed first and is given by\n\nwhere \"a\" is either formula_31 or formula_30 depending on the case being studied. It is good practice to treat the two cases separately. If this limit doesn't exist then there is no oblique asymptote in that direction.\n\nHaving \"m\" then the value for \"n\" can be computed by\n\nwhere \"a\" should be the same value used before. If this limit fails to exist then there is no oblique asymptote in that direction, even should the limit defining \"m\" exist. Otherwise is the oblique asymptote of \"ƒ\"(\"x\") as \"x\" tends to \"a\".\n\nFor example, the function has\n\nso that is the asymptote of \"ƒ\"(\"x\") when \"x\" tends to +∞.\n\nThe function has\n\nSo does not have an asymptote when \"x\" tends to +∞.\n\nA rational function has at most one horizontal asymptote or oblique (slant) asymptote, and possibly many vertical asymptotes.\n\nThe degree of the numerator and degree of the denominator determine whether or not there are any horizontal or oblique asymptotes. The cases are tabulated below, where deg(numerator) is the degree of the numerator, and deg(denominator) is the degree of the denominator.\n\nThe vertical asymptotes occur only when the denominator is zero (If both the numerator and denominator are zero, the multiplicities of the zero are compared). For example, the following function has vertical asymptotes at \"x\" = 0, and \"x\" = 1, but not at \"x\" = 2.\n\nWhen the numerator of a rational function has degree exactly one greater than the denominator, the function has an oblique (slant) asymptote. The asymptote is the polynomial term after dividing the numerator and denominator. This phenomenon occurs because when dividing the fraction, there will be a linear term, and a remainder. For example, consider the function\nshown to the right. As the value of \"x\" increases, \"f\" approaches the asymptote \"y\" = \"x\". This is because the other term, 1/(\"x\"+1), approaches 0.\n\nIf the degree of the numerator is more than 1 larger than the degree of the denominator, and the denominator does not divide the numerator, there will be a nonzero remainder that goes to zero as \"x\" increases, but the quotient will not be linear, and the function does not have an oblique asymptote.\n\nIf a known function has an asymptote (such as \"y\"=0 for \"f\"(x)=\"e\"), then the translations of it also have an asymptote.\n\nIf a known function has an asymptote, then the scaling of the function also have an asymptote.\n\nFor example, \"f\"(\"x\")=\"e\"+2 has horizontal asymptote \"y\"=0+2=2, and no vertical or oblique asymptotes.\n\nLet be a parametric plane curve, in coordinates \"A\"(\"t\") = (\"x\"(\"t\"),\"y\"(\"t\")). Suppose that the curve tends to infinity, that is:\nA line ℓ is an asymptote of \"A\" if the distance from the point \"A\"(\"t\") to ℓ tends to zero as \"t\" → \"b\". From the definition, only open curves that have some infinite branch can have an asymptote. No closed curve can have an asymptote.\n\nFor example, the upper right branch of the curve \"y\" = 1/\"x\" can be defined parametrically as \"x\" = \"t\", \"y\" = 1/\"t\" (where \"t\" > 0). First, \"x\" → ∞ as \"t\" → ∞ and the distance from the curve to the \"x\"-axis is 1/\"t\" which approaches 0 as \"t\" → ∞. Therefore, the \"x\"-axis is an asymptote of the curve. Also, \"y\" → ∞ as \"t\" → 0 from the right, and the distance between the curve and the \"y\"-axis is \"t\" which approaches 0 as \"t\" → 0. So the \"y\"-axis is also an asymptote. A similar argument shows that the lower left branch of the curve also has the same two lines as asymptotes.\n\nAlthough the definition here uses a parameterization of the curve, the notion of asymptote does not depend on the parameterization. In fact, if the equation of the line is formula_54 then the distance from the point \"A\"(\"t\") = (\"x\"(\"t\"),\"y\"(\"t\")) to the line is given by\nif γ(\"t\") is a change of parameterization then the distance becomes\nwhich tends to zero simultaneously as the previous expression.\n\nAn important case is when the curve is the graph of a real function (a function of one real variable and returning real values). The graph of the function \"y\" = \"ƒ\"(\"x\") is the set of points of the plane with coordinates (\"x\",\"ƒ\"(\"x\")). For this, a parameterization is\nThis parameterization is to be considered over the open intervals (\"a\",\"b\"), where \"a\" can be −∞ and \"b\" can be +∞.\n\nAn asymptote can be either vertical or non-vertical (oblique or horizontal). In the first case its equation is \"x\" = \"c\", for some real number \"c\". The non-vertical case has equation , where \"m\" and formula_58 are real numbers. All three types of asymptotes can be present at the same time in specific examples. Unlike asymptotes for curves that are graphs of functions, a general curve may have more than two non-vertical asymptotes, and may cross its vertical asymptotes more than once.\n\nLet be a parametric plane curve, in coordinates \"A\"(\"t\") = (\"x\"(\"t\"),\"y\"(\"t\")), and \"B\" be another (unparameterized) curve. Suppose, as before, that the curve \"A\" tends to infinity. The curve \"B\" is a curvilinear asymptote of \"A\" if the shortest distance from the point \"A\"(\"t\") to a point on \"B\" tends to zero as \"t\" → \"b\". Sometimes \"B\" is simply referred to as an asymptote of \"A\", when there is no risk of confusion with linear asymptotes.\n\nFor example, the function\nhas a curvilinear asymptote , which is known as a \"parabolic asymptote\" because it is a parabola rather than a straight line.\n\nAsymptotes are used in procedures of curve sketching. An asymptote serves as a guide line to show the behavior of the curve towards infinity. In order to get better approximations of the curve, curvilinear asymptotes have also been used although the term asymptotic curve seems to be preferred.\n\nThe asymptotes of an algebraic curve in the affine plane are the lines that are tangent to the projectivized curve through a point at infinity. For example, one may identify the asymptotes to the unit hyperbola in this manner. Asymptotes are often considered only for real curves, although they also make sense when defined in this way for curves over an arbitrary field.\n\nA plane curve of degree \"n\" intersects its asymptote at most at \"n\"−2 other points, by Bézout's theorem, as the intersection at infinity is of multiplicity at least two. For a conic, there are a pair of lines that do not intersect the conic at any complex point: these are the two asymptotes of the conic.\n\nA plane algebraic curve is defined by an equation of the form \"P\"(\"x\",\"y\") = 0 where \"P\" is a polynomial of degree \"n\"\nwhere \"P\" is homogeneous of degree \"k\". Vanishing of the linear factors of the highest degree term \"P\" defines the asymptotes of the curve: setting , if , then the line\nis an asymptote if formula_62 and formula_63 are not both zero. If formula_64 and formula_65, there is no asymptote, but the curve has a branch that looks like a branch of parabola. Such a branch is called a , even when it does not have any parabola that is a curvilinear asymptote. If formula_66 the curve has a singular point at infinity which may have several asymptotes or parabolic branches.\n\nOver the complex numbers, \"P\" splits into linear factors, each of which defines an asymptote (or several for multiple factors). 0ver the reals, \"P\" splits in factors that are linear or quadratic factors. Only the linear factors correspond to infinite (real) branches of the curve, but if a linear factor has multiplicity greater than one, the curve may have several asymptotes or parabolic branches. It may also occur that such a multiple linear factor corresponds to two complex conjugate branches, and does not corresponds to any infinite branch of the real curve. For example, the curve has no real points outside the square formula_67, but its highest order term gives the linear factor \"x\" with multiplicity 4, leading to the unique asymptote \"x\"=0.\n\nThe hyperbola\nhas the two asymptotes\nThe equation for the union of these two lines is\nSimilarly, the hyperboloid\nis said to have the asymptotic cone\n\nThe distance between the hyperboloid and cone approaches 0 as the distance from the origin approaches infinity.\nMore generally, let us consider a surface that has an implicit equation\nformula_73\nwhere the formula_74 are homogeneous polynomials of degree formula_75 and formula_76. Then the equation formula_77 defines a cone which is centered at the origin. It is called an asymptotic cone, because the distance to the cone of a point of the surface tends to zero when the point on the surface tends to infinity.\n\n\n\n"}
{"id": "1115798", "url": "https://en.wikipedia.org/wiki?curid=1115798", "title": "Augustus Edward Hough Love", "text": "Augustus Edward Hough Love\n\nAugustus Edward Hough Love FRS (17 April 1863, Weston-super-Mare – 5 June 1940, Oxford), often known as A. E. H. Love, was a mathematician famous for his work on the mathematical theory of elasticity. He also worked on wave propagation and his work on the structure of the Earth in \"Some Problems of Geodynamics\" won for him the Adams prize in 1911 when he developed a mathematical model of surface waves known as Love waves. \nLove also contributed to the theory of tidal locking and introduced the parameters known as Love numbers, which are widely used today. These numbers are also used in problems related to the tidal deformation of the Earth due to the gravitational attraction of the Moon and Sun.\n\nHe was educated at Wolverhampton Grammar School and in 1881 won a scholarship to St John's College, Cambridge, where he was at first undecided whether to study classics or mathematics. His successful progress (he was placed Second Wrangler) vindicated his choice of mathematics, and in 1886 he was elected Fellow of the college. In 1899 he was appointed Sedleian Professor of Natural Philosophy in the University of Oxford, a position which he retained until his death in 1940. He was also a Fellow of Queen's College.\n\nHe authored the two volume classic, \"\".\n\nHis other awards include the Royal Society Royal Medal in 1909 and Sylvester Medal in 1937, the London Mathematical Society De Morgan Medal in 1926. He was secretary to the London Mathematical Society between 1895 and 1910, and president for 1912–1913.\n\n"}
{"id": "14675761", "url": "https://en.wikipedia.org/wiki?curid=14675761", "title": "Birkhoff–Grothendieck theorem", "text": "Birkhoff–Grothendieck theorem\n\nIn mathematics, the Birkhoff–Grothendieck theorem classifies holomorphic vector bundles over the complex projective line. In particular every holomorphic vector bundle over formula_1 is a direct sum of holomorphic line bundles. The theorem was proved by , and is more or less equivalent to Birkhoff factorization introduced by .\n\nMore precisely, the statement of the theorem is as the following.\n\nEvery holomorphic vector bundle formula_2 on formula_1 is holomorphically isomorphic to a direct sum of line bundles:\n\nThe notation implies each summand is a Serre twist some number of times of the trivial bundle. The representation is unique up to permuting factors.\n\nThe same result holds in algebraic geometry for algebraic vector bundle over formula_5 for any field formula_6.\nIt also holds for formula_7 with one or two orbifold points, and for chains of projective lines meeting along nodes.\n\n\n"}
{"id": "8745236", "url": "https://en.wikipedia.org/wiki?curid=8745236", "title": "CIKS-1", "text": "CIKS-1\n\nIn cryptography, CIKS-1 is a block cipher designed in 2002 by A.A. Moldovyan and N.A. Moldovyan. Like its predecessor, Spectr-H64, it relies heavily on permutations of bits, so is better suited to implementation in hardware than in software.\n\nThe algorithm has a block size of 64 bits. It uses an 8 round structure in which half of the block determines the transformation of the other half in each round, similar to a Feistel cipher or RC5. In each round the key also undergoes a transformation dependent on the data. CIKS-1 uses four types of operations: data-dependent permutations, fixed permutations, XORs, and addition mod 4.\n\nThe designers of CIKS-1 didn't specify any key schedule for the cipher, but it uses a total key size of 256 bits. Kidney, Heys, and Norvell showed that round keys of low Hamming weight are relatively weak, so keys should be chosen carefully. The same researchers have also proposed a differential cryptanalysis of CIKS-1 which uses 2 chosen plaintexts.\n\n"}
{"id": "10459464", "url": "https://en.wikipedia.org/wiki?curid=10459464", "title": "Cerf theory", "text": "Cerf theory\n\nIn mathematics, at the junction of singularity theory and differential topology, Cerf theory is the study of families of smooth real-valued functions\n\non a smooth manifold formula_2, their generic singularities and the topology of the subspaces these singularities define, as subspaces of the function space. The theory is named after Jean Cerf, who initiated it in the late 1960s.\n\nMarston Morse proved that, provided formula_2 is compact, any smooth function\n\ncould be approximated by a Morse function. So for many purposes, one can replace arbitrary functions on formula_2 by Morse functions.\n\nAs a next step, one could ask, 'if you have a 1-parameter family of functions which start and end at Morse functions, can you assume the whole family is Morse?' In general the answer is no. Consider, for example, the family:\n\nas a 1-parameter family of functions on formula_7. \nAt time\n\nit has no critical points, but at time\n\nit is a Morse function with two critical points\n\nCerf showed that a 1-parameter family of functions between two Morse functions could be approximated by one that is Morse at all but finitely many degenerate times. The degeneracies involve a birth/death transition of critical points, as in the above example when formula_11 an index 0 and index 1 critical point are created (as formula_12 increases).\n\nLet's return to the general case that formula_2 is a compact manifold. Let formula_14 denote the space of Morse functions\n\nand formula_16 the space of smooth functions\n\nMorse proved that\n\nis an open and dense subset in the formula_19 topology.\n\nFor the purposes of intuition, here is an analogy. Think of the Morse functions as the top-dimensional open stratum in a stratification of formula_16 (we make no claim that such a stratification exists, but suppose one does). Notice that in stratified spaces, the co-dimension 0 open stratum is open and dense. For notational purposes, reverse the conventions for indexing the stratifications in a stratified space, and index the open strata not by their dimension, but by their co-dimension. This is convenient since formula_16 is infinite-dimensional if formula_2 is not a finite set. By assumption, the open co-dimension 0 stratum of formula_16 is formula_14, i.e.: formula_25. In a stratified space formula_26, frequently formula_27 is disconnected. The essential property of the co-dimension 1 stratum formula_28 is that any path in formula_26 which starts and ends in formula_27 can be approximated by a path that intersects formula_28 transversely in finitely many points, and does not intersect formula_32 for any formula_33.\n\nThus Cerf theory is the study of the positive co-dimensional strata of formula_16, i.e.: formula_35 for formula_36. In the case of\n\nonly for formula_11 is the function not Morse, and\n\nhas a cubic degenerate critical point corresponding to the birth/death transition.\n\nThe Morse Theorem asserts that if formula_17 is a Morse function, then near a critical point formula_41 it is conjugate to a function formula_42 of the form\n\nwhere formula_44.\n\nCerf's 1-parameter theorem asserts the essential property of the co-dimension one stratum.\n\nPrecisely, if formula_45 is a 1-parameter family of smooth functions on formula_2 with formula_47, and formula_48 Morse, then there exists a smooth 1-parameter family formula_49 such that formula_50, formula_51 is uniformly close to formula_52 in the formula_53-topology on functions formula_54. Moreover, formula_55 is Morse at all but finitely many times. At a non-Morse time the function has only one degenerate critical point formula_41, and near that point the family formula_55 is conjugate to the family\n\nwhere formula_59. If formula_60 this is a 1-parameter family of functions where two critical points are created (as formula_12 increases), and for formula_62 it is a 1-parameter family of functions where two critical points are destroyed.\n\nThe PL-Schoenflies problem for formula_63 was solved by J. W. Alexander in 1924. His proof was adapted to the smooth case by Morse and Emilio Baiada. The essential property was used by Cerf in order to prove that every orientation-preserving diffeomorphism of formula_64 is isotopic to the identity, seen as a 1-parameter extension of the Schoenflies theorem for formula_63. The corollary formula_66 at the time had wide implications in differential topology. The essential property was later used by Cerf to prove the pseudo-isotopy theorem for high-dimensional simply-connected manifolds. The proof is a 1-parameter extension of Smale's proof of the h-cobordism theorem (the rewriting of Smale's proof into the functional framework was done by Morse, also Milnor, and also by Cerf-Gramain-Morin following a suggestion of Thom).\n\nCerf's proof is built on the work of Thom and Mather. A useful modern summary of Thom and Mather's work from the period is the book of Golubitsky and Guillemin.\n\nBeside the above-mentioned applications, Robion Kirby used Cerf Theory as a key step in justifying the Kirby calculus.\n\nA stratification of the complement of an infinite co-dimension subspace of the space of smooth maps formula_67 was eventually developed by Sergeraert.\n\nDuring the seventies, the classification problem for pseudo-isotopies of non-simply connected manifolds was solved by Hatcher and Wagoner, discovering algebraic formula_68-obstructions on formula_69 (formula_70) and formula_71 (formula_72) and by Igusa, discovering obstructions of a similar nature on formula_69 (formula_74).\n"}
{"id": "4576710", "url": "https://en.wikipedia.org/wiki?curid=4576710", "title": "Classification theorem", "text": "Classification theorem\n\nIn mathematics, a classification theorem answers the classification problem \"What are the objects of a given type, up to some equivalence?\". It gives a non-redundant enumeration: each object is equivalent to exactly one class.\n\nA few related issues to classification are the following.\n\n\nThere exist many classification theorems in mathematics, as described below.\n\n\n\n\n"}
{"id": "47934379", "url": "https://en.wikipedia.org/wiki?curid=47934379", "title": "Daniel I. A. Cohen", "text": "Daniel I. A. Cohen\n\nDaniel Isaac Aryeh Cohen (born 1946) is an American mathematician and computer scientist who is now a professor emeritus at Hunter College.\n\nCohen earned a bachelor's degree in mathematics from Princeton University in 1967 and already as an undergraduate published a research paper about Sperner's lemma, which he learned about from Hans Rademacher. He completed his doctorate in 1975 from Harvard University under the joint supervision of Andrew M. Gleason and Gian-Carlo Rota. He was a mathematician at Hunter College in 1981 when the computer science department was founded, and became one of five initial computer science professors there.\n\nCohen is the author of the textbooks \"Basic Techniques of Combinatorial Theory\" (John Wiley & Sons, 1979) and \"Introduction to Computer Theory\" (John Wiley & Sons, 1986; 2nd ed., 1996).\n\nAn undergraduate award for a graduating senior at Hunter College, the Daniel I.A. Cohen Prize for Academic Excellence in Theoretical Computer Science, was named after Cohen.\n"}
{"id": "23558692", "url": "https://en.wikipedia.org/wiki?curid=23558692", "title": "Decimal32 floating-point format", "text": "Decimal32 floating-point format\n\nIn computing, decimal32 is a decimal floating-point computer numbering format that occupies 4 bytes (32 bits) in computer memory.\nIt is intended for applications where it is necessary to emulate decimal rounding exactly, such as financial and tax computations. Like the binary16 format, it is intended for memory saving storage.\n\nDecimal32 supports 7 decimal digits of significand and an exponent range of −95 to +96, i.e. to ±. (Equivalently, to .) Because the significand is not normalized (there is no implicit leading \"1\"), most values with less than 7 significant digits have multiple possible representations; , etc. Zero has 192 possible representations (384 when both signed zeros are included).\n\nDecimal32 floating point is a relatively new decimal floating-point format, formally introduced in the 2008 version of IEEE 754 as well as with .\n\nIEEE 754 allows two alternative representation methods for decimal32 values.\nThe standard does not specify how to signify which representation is used,\nfor instance in a situation where decimal32 values are communicated between systems.\n\nIn one representation method, based on binary integer decimal (BID),\nthe significand is represented as binary coded positive integer.\n\nThe other, alternative, representation method is based on\ndensely packed decimal (DPD) for most of the\nsignificand (except the most significant digit).\n\nBoth alternatives provide exactly the same range of representable numbers: 7 digits of significand and possible exponent values.\n\nIn both cases, the most significant 4 bits of the significand (which actually only have 10 possible values) are combined with the most significant 2 bits of the exponent (3 possible values) to use 30 of the 32 possible values of a 5-bit field called the combination field. The remaining combinations encode infinities and NaNs.\nNote: The sign bit of NaNs is ignored. The first bit of the remaining exponent determines whether the NaN is quiet or signaling.\n\nThis format uses a binary significand from 0 to 10−1 = = 98967F = . The encoding can represent binary significands up to 10×2−1 = = 9FFFFF = , but values larger than 10−1 are illegal (and the standard requires implementations to treat them as 0, if encountered on input).\n\nAs described above, the encoding varies depending on whether the most significant 4 bits of the significand are in the range 0 to 7 (0000 to 0111), or higher (1000 or 1001).\n\nIf the 2 bits after the sign bit are \"00\", \"01\", or \"10\", then the \nexponent field consists of the 8 bits following the sign bit, and the\nsignificand is the remaining 23 bits, with an implicit leading 0 bit:\n\nThis includes subnormal numbers where the leading significand digit is 0.\n\nIf the 2 bits after the sign bit are \"11\", then the 8-bit exponent field is shifted 2 bits to the right (after both the sign bit and the \"11\" bits thereafter), and the represented significand is in the remaining 21 bits. In this case there is an implicit (that is, not stored) leading 3-bit sequence \"100\" in the true significand.\n\nThe \"11\" 2-bit sequence after the sign bit indicates that there is an \"implicit\" \"100\" 3-bit\nprefix to the significand. Compare having an implicit 1 in the significand of normal\nvalues for the binary formats. Note also that the \"00\", \"01\", or \"10\" bits are part of the exponent field.\n\nNote that the leading bits of the significand field do \"not\" encode the most significant decimal digit; they are simply part of a larger pure-binary number. For example, a significand of is encoded as binary , with the leading 4 bits encoding 7; the first significand which requires a 24th bit is 2 = \n\nIn the above cases, the value represented is\n\nIf the four bits after the sign bit are \"1111\" then the value is an infinity or a NaN, as described above:\n\nIn this version, the significand is stored as a series of decimal digits. The leading digit is between 0 and 9 (3 or 4 binary bits), and the rest of the significand uses the densely packed decimal (DPD) encoding.\n\nUnlike the binary integer significand version, where the exponent changed position and came before the significand, this encoding combines the leading 2 bits of the exponent and the leading digit (3 or 4 bits) of the significand into the five bits that follow the sign bit.\n\nThese six bits after that are the exponent continuation field, providing the less-significant bits of the exponent.\n\nThe last 20 bits are the significand continuation field, consisting of two 10-bit \"declets\". Each declet encodes three decimal digits using the DPD encoding.\n\nIf the first two bits after the sign bit are \"00\", \"01\", or \"10\", then those are\nthe leading bits of the exponent, and the three bits after that are interpreted as\nthe leading decimal digit (0 to 7):\n\nIf the first two bits after the sign bit are \"11\", then the \nsecond two bits are the leading bits of the exponent, and the last bit is\nprefixed with \"100\" to form the leading decimal digit (8 or 9):\n\nThe remaining two combinations (11110 and 11111) of the 5-bit field\nare used to represent ±infinity and NaNs, respectively.\n\nThe DPD/3BCD transcoding for the declets is given by the following table.\nb9...b0 are the bits of the DPD, and d2...d0 are the three BCD digits.\n\nThe 8 decimal values whose digits are all 8s or 9s have four codings each.\nThe bits marked x in the table above are ignored on input, but will always be 0 in computed results.\n\nIn the above cases, with the \"true significand\" as the sequence of decimal digits decoded, the value represented is\n\n"}
{"id": "1374448", "url": "https://en.wikipedia.org/wiki?curid=1374448", "title": "Degree (graph theory)", "text": "Degree (graph theory)\n\nIn graph theory, the degree (or valency) of a vertex of a graph is the number of edges incident to the vertex, with loops counted twice. The degree of a vertex formula_1 is denoted formula_2 or formula_3. The maximum degree of a graph \"G\", denoted by Δ(\"G\"), and the minimum degree of a graph, denoted by δ(\"G\"), are the maximum and minimum degree of its vertices. In the graph on the right, the maximum degree is 5 and the minimum degree is 0. In a regular graph, all degrees are the same, and so we can speak of \"the\" degree of the graph.\n\nThe degree sum formula states that, given a graph formula_4,\n\nThe formula implies that in any undirected graph, the number of vertices with odd degree is even. This statement (as well as the degree sum formula) is known as the handshaking lemma. The latter name comes from a popular mathematical problem, to prove that in any group of people the number of people who have shaken hands with an odd number of other people from the group is even.\n\nThe degree sequence of an undirected graph is the non-increasing sequence of its vertex degrees; for the above graph it is (5, 3, 3, 2, 2, 1, 0). The degree sequence is a graph invariant so isomorphic graphs have the same degree sequence. However, the degree sequence does not, in general, uniquely identify a graph; in some cases, non-isomorphic graphs have the same degree sequence.\n\nThe degree sequence problem is the problem of finding some or all graphs with the degree sequence being a given non-increasing sequence of positive integers. (Trailing zeroes may be ignored since they are trivially realized by adding an appropriate number of isolated vertices to the graph.) A sequence which is the degree sequence of some graph, i.e. for which the degree sequence problem has a solution, is called a graphic or graphical sequence. As a consequence of the degree sum formula, any sequence with an odd sum, such as (3, 3, 1), cannot be realized as the degree sequence of a graph. The converse is also true: if a sequence has an even sum, it is the degree sequence of a multigraph. The construction of such a graph is straightforward: connect vertices with odd degrees in pairs by a matching, and fill out the remaining even degree counts by self-loops.\nThe question of whether a given degree sequence can be realized by a simple graph is more challenging. This problem is also called graph realization problem and can either be solved by the Erdős–Gallai theorem or the Havel–Hakimi algorithm. \nThe problem of finding or estimating the number of graphs with a given degree sequence is a problem from the field of graph enumeration.\n\n\n\n\n"}
{"id": "54093847", "url": "https://en.wikipedia.org/wiki?curid=54093847", "title": "Delsarte–Goethals code", "text": "Delsarte–Goethals code\n\nThe Delsarte–Goethals code is a type of error-correcting code.\n\nThe concept was introduced by mathematicians Ph. Delsarte and J.-M. Goethals in their published paper.\n\nA new proof of the properties of the Delsarte–Goethals code was published in 1970.\n\nThe Delsarte–Goethals code DG(\"m\",\"r\") for even \"m\" ≥ 4 and 0 ≤ \"r\" ≤ \"m\"/2 − 1 is a binary, non-linear code of length formula_1, size formula_2 and minimum distance formula_3\n\nThe code sits between the Kerdock code and the second-order Reed–Muller codes. More precisely, we have\n\nWhen \"r\" = 0, we have DG(\"m\",\"r\") = \"K\"(\"m\") and when \"r\" = \"m\"/2 − 1 we have DG(\"m\",\"r\") = RM(2,\"m\").\n\nFor \"r\" = \"m\"/2 − 1 the Delsarte–Goethals code has strength 7 and is therefore an orthogonal array OA(formula_5.\n"}
{"id": "3404894", "url": "https://en.wikipedia.org/wiki?curid=3404894", "title": "Difference in differences", "text": "Difference in differences\n\nDifference in differences (DID or DD) is a statistical technique used in econometrics and quantitative research in the social sciences that attempts to mimic an experimental research design using observational study data, by studying the differential effect of a treatment on a 'treatment group' versus a 'control group' in a natural experiment. It calculates the effect of a treatment (i.e., an explanatory variable or an independent variable) on an outcome (i.e., a response variable or dependent variable) by comparing the average change over time in the outcome variable for the treatment group, compared to the average change over time for the control group. Although it is intended to mitigate the effects of extraneous factors and selection bias, depending on how the treatment group is chosen, this method may still be subject to certain biases (e.g., mean regression, reverse causality and omitted variable bias).\n\nIn contrast to a time-series estimate of the treatment effect on subjects (which analyzes differences over time) or a cross-section estimate of the treatment effect (which measures the difference between treatment and control groups), difference in differences uses panel data to measure the differences, between the treatment and control group, of the changes in the outcome variable that occur over time.\n\nDifference in differences requires data measured from a treatment group and a control group at two or more different time periods, specifically at least one time period before \"treatment\" and at least one time period after \"treatment.\" In the example pictured, the outcome in the treatment group is represented by the line P and the outcome in the control group is represented by the line S. The outcome (dependent) variable in both groups is measured at time 1, before either group has received the treatment (i.e., the independent or explanatory variable), represented by the points \"P\" and \"S\". The treatment group then receives or experiences the treatment and both groups are again measured at time 2. Not all of the difference between the treatment and control groups at time 2 (that is, the difference between \"P\" and \"S\") can be explained as being an effect of the treatment, because the treatment group and control group did not start out at the same point at time 1. DID therefore calculates the \"normal\" difference in the outcome variable between the two groups (the difference that would still exist if neither group experienced the treatment), represented by the dotted line \"Q\". (Notice that the slope from \"P\" to \"Q\" is the same as the slope from \"S\" to \"S\".) The treatment effect is the difference between the observed outcome and the \"normal\" outcome (the difference between P and Q).\n\nConsider the model\n\nwhere formula_2 is the dependent variable for individual formula_3 and formula_4, formula_5 is the group to which formula_3 belongs, and formula_7 is short-hand for the dummy variable equal to 1 when the event described in formula_8 is true, and 0 otherwise. In the plot of time versus formula_9 by group, formula_10 is the vertical intercept for the graph for formula_11, and formula_12 is the time trend shared by both groups according to the parallel trend assumption (see Assumptions below). formula_13 is the treatment effect, and formula_14 is the residual term.\n\nLet's consider the average of the dependent variable and dummy indicators by group and time:\n\nand suppose for simplicity that formula_16 and formula_17. Note that formula_18 is not random; it just encodes how the groups and the periods are labeled. Then\n\nThe strict exogeneity assumption then implies that\n\nWithout loss of generality, assume that formula_21 is the treatment group, and formula_22 is the after period, then formula_23 and formula_24, giving the DID estimator\n\nwhich can be interpreted as the treatment effect of the treatment indicated by formula_18. Below it is shown how this estimator can be read as a coefficient in an ordinary least squares regression. The model described in this section is over-parametrized; to remedy that, one of the coefficients for the dummy variables can be set to 0, for example, we may set formula_27.\n\nAll the assumptions of the OLS model apply equally to DID. In addition, DID requires a parallel trend assumption. The parallel trend assumption says that formula_28 are the same in both formula_29 and formula_30. Given that the formal definition above accurately represents reality, this assumption automatically holds. However, a model with formula_31 may well be more realistic.\n\nAs illustrated to the right, the treatment effect is the difference between the observed value of \"y\" and what the value of \"y\" would have been with parallel trends, had there been no treatment. The Achilles' heel of DID is when something other than the treatment changes in one group but not the other at the same time as the treatment, implying a violation of the parallel trend assumption.\n\nTo guarantee the accuracy of the DID estimate, the composition of individuals of the two groups is assumed to remain unchanged over time. When using a DID model, various issues that may compromise the results, such as autocorrelation and Ashenfelter dips, must be considered and dealt with.\n\nThe DID method can be implemented according to the table below, where the lower right cell is the DID estimator.\n\nRunning a regression analysis gives the same result. Consider the OLS model\n\nwhere formula_33 is a dummy variable for the period, equal to formula_34 when formula_35, and formula_36 is a dummy variable for group membership, equal to formula_34 when formula_30. The composite variable formula_39 is a dummy variable indicating when formula_40. Although it is not shown rigorously here, this is a proper parametrization of the model formal definition, furthermore, it turns out that the group and period averages in that section relate to the model parameter estimates as follows\n\nwhere formula_42 stands for conditional averages computed on the sample, for example, formula_43 is the indicator for the after period, formula_44 is an indicator for the control group. To see the relation between this notation and the previous section, consider as above only one observation per time period for each group, then\n\nand so on for other values of formula_33 and formula_36, which is equivalent to\n\nBut this is the expression for the treatment effect that was given in the formal definition and in the above table.\n\nConsider one of the most famous DID studies, the Card and Krueger article on minimum wage in New Jersey, published in 1994. Card and Krueger compared employment in the fast food sector in New Jersey and in Pennsylvania, in February 1992 and in November 1992, after New Jersey's minimum wage rose from $4.25 to $5.05 in April 1992. Observing a change in employment in New Jersey only, before and after the treatment, would fail to control for omitted variables such as weather and macroeconomic conditions of the region. By including Pennsylvania as a control in a difference-in-differences model, any bias caused by variables common to New Jersey and Pennsylvania is implicitly controlled for, even when these variables are unobserved. Assuming that New Jersey and Pennsylvania have parallel trends over time, Pennsylvania's change in employment can be interpreted as the change New Jersey would have experienced, had they not increased the minimum wage, and vice versa. The evidence suggested that the increased minimum wage did not induce an increase in unemployment in New Jersey, as standard economic theory would suggest. The table below shows Card & Krueger's estimates of the treatment effect on employment, measured as FTEs (or full-time equivalents). Keeping in mind that the finding is controversial, Card and Krueger estimate that the $0.80 minimum wage increase in New Jersey led to a 2.75 FTE increase in employment.\n\n\n\n"}
{"id": "253251", "url": "https://en.wikipedia.org/wiki?curid=253251", "title": "Dimension of an algebraic variety", "text": "Dimension of an algebraic variety\n\nIn mathematics and specifically in algebraic geometry, the dimension of an algebraic variety may be defined in various equivalent ways.\n\nSome of these definitions are of geometric nature, while some other are purely algebraic and rely on commutative algebra. Some are restricted to algebraic varieties while others apply also to any algebraic set. Some are intrinsic, as independent of any embedding of the variety into an affine or projective space, while other are related to such an embedding.\n\nLet be a field, and be an algebraically closed extension. An affine algebraic set is the set of the common zeros in of the elements of an ideal in a polynomial ring formula_1 Let formula_2 be the algebra of the polynomial functions over . The dimension of is any of the following integers. It does not change if is enlarged, if is replaced by another algebraically closed extension of and if is replaced by another ideal having the same zeros (that is having the same radical). The dimension is also independent of the choice of coordinates; in other words it does not change if the are replaced by linearly independent linear combinations of them. The dimension of is\nThis definition generalizes a property of the dimension of a Euclidean space or a vector space. It is thus probably the definition that gives the easiest intuitive description of the notion. \nThis is the transcription of the preceding definition in the language of commutative algebra, the Krull dimension being the maximal length of the chains formula_5 of prime ideals of .\n\nThis definition shows that the dimension is a \"local property if formula_6is irreducible.\"\n\n\nThis rephrases the previous definition into a more geometric language.\nThis relates the dimension of a variety to that of a differentiable manifold. More precisely, if if defined over the reals, then the set of its real regular points, if it is not empty, is a differentiable manifold that has the same dimension as a variety and as a manifold.\nThis is the algebraic analogue to the fact that a connected manifold has a constant dimension.\nThis definition is not intrinsic as it apply only to algebraic sets that are explicitly embedded in an affine or projective space.\nThis the algebraic translation of the preceding definition.\nThis is the algebraic translation of the fact that the intersection of general hypersurfaces is an algebraic set of dimension . \nThis allows, through a Gröbner basis computation to compute the dimension of the algebraic set defined by a given system of polynomial equations.\nTaking initial ideals preserves Hilbert polynomial/series, and taking radicals preserves the dimension.\nThis allows to prove easily that the dimension is invariant under birational equivalence.\n\nLet \"V\" be a projective algebraic set defined as the set of the common zeros of a homogeneous ideal \"I\" in a polynomial ring formula_10 over a field \"K\", and let \"A\"=\"R\"/\"I\" be the graded algebra of the polynomials over \"V\".\n\nAll the definitions of the previous section apply, with the change that, when \"A\" or \"I\" appear explicitly in the definition, the value of the dimension must be reduced by one. For example, the dimension of \"V\" is one less than the Krull dimension of \"A\".\n\nGiven a system of polynomial equations over an algebraically closed field formula_11, it may be difficult to compute the dimension of the algebraic set that it defines.\n\nWithout further information on the system, there is only one practical method, which consists of computing a Gröbner basis and deducing the degree of the denominator of the Hilbert series of the ideal generated by the equations.\n\nThe second step, which is usually the fastest, may be accelerated in the following way: Firstly, the Gröbner basis is replaced by the list of its leading monomials (this is already done for the computation of the Hilbert series). Then each monomial like formula_12 is replaced by the product of the variables in it: formula_13 Then the dimension is the maximal size of a subset \"S\" of the variables, such that none of these products of variables depends only on the variables in \"S\".\n\nThis algorithm is implemented in several computer algebra systems. For example in Maple, this is the function \"Groebner[HilbertDimension],\" and in Macaulay2, this is the function \"dim\".\n\nThe \"real dimension\" of a set of real points, typically a semialgebraic set, is the dimension of its Zariski closure. For a semialgebraic set , the real dimension is one of the following equal integers:\n\nFor an algebraic set defined over the reals (that is defined by polynomials with real coefficients), it may occur that the real dimension of the set of its real points is smaller than its dimension as a semi algebraic set. For example, the algebraic surface of equation formula_23 is an algebraic variety of dimension two, which has only one real point (0, 0, 0), and thus has the real dimension zero.\n\nThe real dimension is more difficult to compute than the algebraic dimension.\nFor the case of a real hypersurface (that is the set of real solutions of a single polynomial equation), there exists a probabilistic algorithm to compute its real dimension.\n\nFor the case of an arbitrary system of polynomial equations and inequalities, computing a triangular decomposition of this system into so-called regular semi-algebraic systems yields the dimension of the solution set of this system.\nThe command \"RealTriangularize\" of the RegularChains library implements such decomposition.\n\n"}
{"id": "50521059", "url": "https://en.wikipedia.org/wiki?curid=50521059", "title": "Ehrhart's volume conjecture", "text": "Ehrhart's volume conjecture\n\nIn the geometry of numbers, Ehrhart's volume conjecture gives an upper bound on the volume of a convex body containing only one lattice point in its interior. It is a kind of converse to Minkowski's theorem, which guarantees that a centrally symmetric convex body \"K\" must contain a lattice point as soon as its volume exceeds formula_1. The conjecture states that a convex body \"K\" containing only one lattice point in its interior as it's barycenter cannot have volume greater than formula_2:\nEquality is achieved in this inequality when formula_4 is a copy of the standard simplex in Euclidean \"n\"-dimensional space, whose sides are scaled up by a factor of formula_5. Equivalently, formula_4 is congruent to the convex hull of the vectors formula_7, and formula_8. Presented in this manner, the origin is the only lattice point interior to the convex body \"K\".\n\nThe conjecture, furthermore, asserts that equality is achieved in the above inequality if and only if \"K\" is unimodularly equivalent to formula_9.\n"}
{"id": "316904", "url": "https://en.wikipedia.org/wiki?curid=316904", "title": "Ehrhart polynomial", "text": "Ehrhart polynomial\n\nIn mathematics, an integral polytope has an associated Ehrhart polynomial that encodes the relationship between the volume of a polytope and the number of integer points the polytope contains. The theory of Ehrhart polynomials can be seen as a higher-dimensional generalization of Pick's theorem in the Euclidean plane.\n\nThese polynomials are named after Eugène Ehrhart who studied them in the 1960s.\n\nInformally, if is a polytope, and is the polytope formed by expanding by a factor of in each dimension, then is the number of integer lattice points in .\n\nMore formally, consider a lattice formula_1 in Euclidean space and a -dimensional polytope in with the property that all vertices of the polytope are points of the lattice. (A common example is formula_2 and a polytope for which all vertices have integer coordinates.) For any positive integer , let be the -fold dilation of (the polytope formed by multiplying each vertex coordinate, in a basis for the lattice, by a factor of ), and let\n\nbe the number of lattice points contained in the polytope . Ehrhart showed in 1962 that is a rational polynomial of degree in , i.e. there exist rational numbers formula_4 such that:\n\nfor all positive integers .\n\nThe Ehrhart polynomial of the interior of a closed convex polytope can be computed as:\n\nwhere is the dimension of . This result is known as Ehrhart–Macdonald reciprocity.\n\nLet be a -dimensional unit hypercube whose vertices are the integer lattice points all of whose coordinates are 0 or 1. In terms of inequalities,\n\nThen the -fold dilation of is a cube with side length , containing integer points. That is, the Ehrhart polynomial of the hypercube is . Additionally, if we evaluate at negative integers, then\n\nas we would expect from Ehrhart–Macdonald reciprocity.\n\nMany other figurate numbers can be expressed as Ehrhart polynomials. For instance, the square pyramidal numbers are given by the Ehrhart polynomials of a square pyramid with an integer unit square as its base and with height one; the Ehrhart polynomial in this case is .\n\nLet be a rational polytope. In other words, suppose\n\nwhere formula_10 and formula_11. (Equivalently, is the convex hull of finitely many points in .) Then define\n\nIn this case, is a quasi-polynomial in . Just as with integral polytopes, Ehrhart–Macdonald reciprocity holds, that is,\n\nLet be a polygon with vertices (0,0), (0,2), (1,1) and (, 0). The number of integer points in will be counted by the quasi-polynomial \n\nIf is closed (i.e. the boundary faces belong to ), some of the coefficients of have an easy interpretation:\n\nBetke and Kneser established the following characterization of the Ehrhart coefficients. A functional formula_18 defined on integral polytopes is an SLformula_19 and translation invariant valuation if and only if there are formula_20 such that\n\nWe can define a generating function for the Ehrhart polynomial of an integral -dimensional polytope as\n\nThis series can be expressed as a rational function. Specifically, Ehrhart proved (1962) that there exist complex numbers , , such that the Ehrhart series of is\n\nAdditionally, Stanley's non-negativity theorem states that under the given hypotheses, will be non-negative integers, for .\n\nAnother result by Stanley shows that if is a lattice polytope contained in , then for all .\n\nThe -vector is in general not unimodal, but it is whenever it is symmetric, and the polytope has a\nregular unimodal triangulation.\n\nSimilar to the case of polytopes with integer vertices, one defines the Ehrhart series for a rational polytope. For a rational polytope , where is the smallest integer such that is an integer polytope ( is called the denominator of ), then one has\n\nwhere the are still non-negative integers.\n\nThe case and of these statements yields Pick's theorem. Formulas for the other coefficients are much harder to get; Todd classes of toric varieties, the Riemann–Roch theorem as well as Fourier analysis have been used for this purpose.\n\nIf is the toric variety corresponding to the normal fan of , then defines an ample line bundle on , and the Ehrhart polynomial of coincides with the Hilbert polynomial of this line bundle.\n\nEhrhart polynomials can be studied for their own sake. For instance, one could ask questions related to the roots of an Ehrhart polynomial. Furthermore, some authors have pursued the question of how these polynomials could be classified.\n\nIt is possible to study the number of integer points in a polytope if we dilate some facets of but not others. In other words, one would like to know the number of integer points in semi-dilated polytopes. It turns out that such a counting function will be what is called a multivariate quasi-polynomial. An Ehrhart-type reciprocity theorem will also hold for such a counting function.\n\nCounting the number of integer points in semi-dilations of polytopes has applications in enumerating the number of different dissections of regular polygons and the number of non-isomorphic unrestricted codes, a particular kind of code in the field of coding theory.\n\n\n"}
{"id": "10474", "url": "https://en.wikipedia.org/wiki?curid=10474", "title": "Eight queens puzzle", "text": "Eight queens puzzle\n\nThe eight queens puzzle is the problem of placing eight chess queens on an 8×8 chessboard so that no two queens threaten each other. Thus, a solution requires that no two queens share the same row, column, or diagonal. The eight queens puzzle is an example of the more general \"n\" queens problem of placing \"n\" non-attacking queens on an \"n\"×\"n\" chessboard, for which solutions exist for all natural numbers \"n\" with the exception of \"n\"=2 and \"n\"=3.\n\nChess composer Max Bezzel published the eight queens puzzle in 1848. Franz Nauck published the first solutions in 1850. Nauck also extended the puzzle to the \"n\" queens problem, with \"n\" queens on a chessboard of \"n\" × \"n\" squares.\n\nSince then, many mathematicians, including Carl Friedrich Gauss, have worked on both the eight queens puzzle and its generalized \"n\"-queens version. In 1874, S. Gunther proposed a method using determinants to find solutions. J.W.L. Glaisher refined Gunther's approach.\n\nIn 1972, Edsger Dijkstra used this problem to illustrate the power of what he called structured programming. He published a highly detailed description of a depth-first backtracking algorithm.\n\nThe problem of finding all solutions to the 8-queens problem can be quite computationally expensive, as there are 4,426,165,368 (\"i.e.\", C) possible arrangements of eight queens on an 8×8 board, but only 92 solutions. It is possible to use shortcuts that reduce computational requirements or rules of thumb that avoids brute-force computational techniques. For example, by applying a simple rule that constrains each queen to a single column (or row), though still considered brute force, it is possible to reduce the number of possibilities to 16,777,216 (that is, 8) possible combinations. Generating permutations further reduces the possibilities to just 40,320 (that is, 8!), which are then checked for diagonal attacks.\n\nMartin Richards published a program to count solutions to the n-queens problem using bitwise operations.. However, this solution has already been published by Zongyan Qiu.\n\nThe eight queens puzzle has 92 distinct solutions. If solutions that differ only by the symmetry operations of rotation and reflection of the board are counted as one, the puzzle has 12 solutions. These are called \"fundamental\" solutions; representatives of each are shown below.\n\nA fundamental solution usually has eight variants (including its original form) obtained by rotating 90, 180, or 270° and then reflecting each of the four rotational variants in a mirror in a fixed position. However, should a solution be equivalent to its own 90° rotation (as happens to one solution with five queens on a 5×5 board), that fundamental solution will have only two variants (itself and its reflection). Should a solution be equivalent to its own 180° rotation (but not to its 90° rotation), it will have four variants (itself and its reflection, its 90° rotation and the reflection of that). If \"n\" > 1, it is not possible for a solution to be equivalent to its own reflection because that would require two queens to be facing each other. Of the 12 fundamental solutions to the problem with eight queens on an 8×8 board, exactly one (solution 12 below) is equal to its own 180° rotation, and none is equal to its 90° rotation; thus, the number of distinct solutions is 11×8 + 1×4 = 92 (where the 8 is derived from four 90° rotational positions and their reflections, and the 4 is derived from two 180° rotational positions and their reflections).\n\nAll fundamental solutions are presented below:\n\nSolution 10 has the additional property that no three queens are in a straight line.\n\nThese brute-force algorithms to count the number of solutions are computationally manageable for , but would be intractable for problems of , as 20! = 2.433 × 10. If the goal is to find a single solution, one can show solutions exist for all \"n\" ≥ 4 with no search whatsoever.\nThese solutions exhibit stair-stepped patterns, as in the following examples for \"n\" = 8, 9 and 10:\n\nThe examples above can be obtained with the following formulas. Let (\"i\", \"j\") be the square in column \"i\" and row \"j\" on the \"n\" × \"n\" chessboard, \"k\" an integer.\n\n\nAnother approach is\n\n\nFor this results in fundamental solution 1 above. A few more examples follow.\n\n\nThe following table gives the number of solutions for placing \"n\" queens on an \"n\" × \"n\" board, both fundamental and all , for n=1–10, 24–27.\n\nThe six queens puzzle has fewer solutions than the five queens puzzle.\n\nThere is currently no known formula for the exact number of solutions, or even for its asymptotic behaviour. As of now, the 27x27 board is the highest-order board that has been completely enumerated. Finding a single solution for a bigger board is not difficult.\n\n\nFinding all solutions to the eight queens puzzle is a good example of a simple but nontrivial problem. For this reason, it is often used as an example problem for various programming techniques, including nontraditional approaches such as constraint programming, logic programming or genetic algorithms. Most often, it is used as an example of a problem that can be solved with a recursive algorithm, by phrasing the \"n\" queens problem inductively in terms of adding a single queen to any solution to the problem of placing \"n\"−1 queens on an n-by-n chessboard. The induction bottoms out with the solution to the 'problem' of placing 0 queens on the chessboard, which is the empty chessboard.\n\nThis technique is much more efficient than the naïve brute-force search algorithm, which considers all 64 = 2 = 281,474,976,710,656 possible blind placements of eight queens, and then filters these to remove all placements that place two queens either on the same square (leaving only 64!/56! = 178,462,987,637,760 possible placements) or in mutually attacking positions. This very poor algorithm will, among other things, produce the same results over and over again in all the different permutations of the assignments of the eight queens, as well as repeating the same computations over and over again for the different sub-sets of each solution. A better brute-force algorithm places a single queen on each row, leading to only 8 = 2 = 16,777,216 blind placements.\n\nIt is possible to do much better than this.\nOne algorithm solves the eight rooks puzzle by generating the permutations of the numbers 1 through 8 (of which there are 8! = 40,320), and uses the elements of each permutation as indices to place a queen on each row.\nThen it rejects those boards with diagonal attacking positions.\nThe backtracking depth-first search program, a slight improvement on the permutation method, constructs the search tree by considering one row of the board at a time, eliminating most nonsolution board positions at a very early stage in their construction.\nBecause it rejects rook and diagonal attacks even on incomplete boards, it examines only 15,720 possible queen placements.\nA further improvement, which examines only 5,508 possible queen\nplacements, is to combine the permutation based method with the early\npruning method: the permutations are generated depth-first, and\nthe search space is pruned if the partial permutation produces a\ndiagonal attack.\nConstraint programming can also be very effective on this problem.\nAn alternative to exhaustive search is an 'iterative repair' algorithm, which typically starts with all queens on the board, for example with one queen per column. It then counts the number of conflicts (attacks), and uses a heuristic to determine how to improve the placement of the queens. The 'minimum-conflicts' heuristic — moving the piece with the largest number of conflicts to the square in the same column where the number of conflicts is smallest — is particularly effective: it finds a solution to the 1,000,000 queen problem in less than 50 steps on average. This assumes that the initial configuration is 'reasonably good' — if a million queens all start in the same row, it will take at least 999,999 steps to fix it. A 'reasonably good' starting point can for instance be found by putting each queen in its own row and column so that it conflicts with the smallest number of queens already on the board.\n\nUnlike the backtracking search outlined above, iterative repair does not guarantee a solution: like all greedy procedures, it may get stuck on a local optimum. (In such a case, the algorithm may be restarted with a different initial configuration.) On the other hand, it can solve problem sizes that are several orders of magnitude beyond the scope of a depth-first search.\n\nThis animation illustrates backtracking to solve the problem. A queen is placed in a column that is known not to cause conflict. If a column is not found the program returns to the last good state and then tries a different column.\n\nThe following is a Pascal program by Niklaus Wirth in 1976. It finds one solution to the eight queens problem.\n\n\n\n"}
{"id": "35369474", "url": "https://en.wikipedia.org/wiki?curid=35369474", "title": "Emanuels Grīnbergs", "text": "Emanuels Grīnbergs\n\nEmanuels Donats Frīdrihs Jānis Grinbergs (1911–1982, westernized as Emanuel Grinberg) was a Latvian mathematician, known for Grinberg's theorem on the Hamiltonicity of planar graphs.\n\nGrinbergs was born on January 25, 1911 in St. Petersburg, the son of a Lutheran bishop from Latvia. Latvia became independent from Russia in 1917, and on the death of his father in 1923, Grinbergs' family returned to Riga, taking Grinbergs with them.\n\nIn 1927, he won a high school mathematics competition, the prize for which was to study in Lille, France. He then studied mathematics at the University of Latvia beginning in 1930. On graduating in 1934, he won a prize that again funded study in France; he did graduate studies in 1935 and 1936 at the École Normale Supérieure in Paris, during which he published his first paper, in geometry. He returned to the University of Latvia as a \"privatdozent\" in 1937, and joined the faculty as a dozent in 1940. His lectures at that time covered subjects including geometry, probability theory, and group theory. While there, he defended a thesis in geometry at the University of Latvia in 1943, entitled \"On Oscillations, Superoscillations and Characteristic Points\".\n\nIn the meantime, the Soviet Union had annexed Latvia in 1940, and the army of Nazi Germany had occupied it and incorporated it into the Reichskommissariat Ostland. Grinbergs was drafted into the Latvian Legion, part of the German military, in 1944. After the war, because of his service as a German soldier, he was held prisoner in a camp in Kutaisi, Georgia, until 1946; he lost his university position, and his doctorate (awarded during the German occupation) was annulled.\n\nGrinbergs returned to Latvia, where he became a factory worker in the Radiotehnika radio factory, while continuing to be interrogated regularly by the KGB. He developed mathematical models of electrical circuits, which he wrote up as a second thesis, \"Problems of analysis and synthesis of simple linear circuits\", his defense of which earned him a candidate degree.\n\nIn 1954, Grinbergs was allowed to return to the University of Latvia faculty. In 1956, he joined the Institute of Physics of the Latvian Academy of Sciences, and in 1960, he began working at the Computer Center of the University of Latvia, where he remained for the rest of his career, eventually becoming Chief Scientist there.\n\nGrinbergs' initial research interests were in geometry, and later shifted to graph theory. With professors Arins and Daube at the University of Latvia, Grinbergs was one of the first to work in applied mathematics and computer science in Latvia.\n\nGrinbergs and his collaborators wrote many papers on the design of electrical circuits and electronic filters, stemming from his radio work. He earned the State Prize of the Latvian SSR in 1980 for his research on nonlinear electronic circuit theory.\n\nAnother early line of research by Grinbergs at the Computer Center concerned the automated design of ship hulls, and the computations with spline curves and surfaces needed in this design. The goal of this research was to calculate patterns for cutting and then bending flat steel plates so that they could be welded together to form ship hulls without the need for additional machining after the bending step; the methods developed by Grinbergs were later used throughout the Soviet Union.\n\nIn graph theory, Grinbergs is best known for Grinberg's theorem, a necessary condition for a planar graph to have a Hamiltonian cycle that has been frequently used to find non-Hamiltonian planar graphs with other special properties. His researches in graph theory also concerned graph coloring, graph isomorphism, cycles in directed graphs, and a counterexample to a conjecture of András Ádám on the number of cycles in tournaments.\nOther topics in Grinbergs' research include Steiner triple systems, magnetohydrodynamics, operations research, and the mathematical modeling of hydrocarbon exploration.\n"}
{"id": "3934566", "url": "https://en.wikipedia.org/wiki?curid=3934566", "title": "Extension (predicate logic)", "text": "Extension (predicate logic)\n\nThe extension of a predicatea truth-valued functionis the set of tuples of values that, used as arguments, satisfy the predicate. Such a set of tuples is a relation.\n\nFor example, the statement \"\"d2\" is the weekday following \"d1\"\" can be seen as a truth function associating to each tuple (\"d2\", \"d1\") the value \"true\" or \"false\". The extension of this truth function is, by convention, the set of all such tuples associated with the value \"true\", i.e.\n\nBy examining this extension we can conclude that \"Tuesday is the weekday following Saturday\" (for example) is false.\n\nUsing set-builder notation, the extension of the \"n\"-ary predicate formula_1 can be written as\n\nIf the values 0 and 1 in the range of a characteristic function are identified with the values false and true, respectivelymaking the characteristic function a predicate, then for all relations \"R\" and predicates formula_1 the following two statements are equivalent:\n\n\n"}
{"id": "53398219", "url": "https://en.wikipedia.org/wiki?curid=53398219", "title": "Fluent (mathematics)", "text": "Fluent (mathematics)\n\nA fluent is a time-varying quantity or variable. The term was used by Isaac Newton in his early calculus to describe his form of a function. The concept was introduced by Newton in 1665 and detailed in his mathematical treatise, \"Method of Fluxions\". Newton described any variable that changed its value as a fluent – for example, the velocity of a ball thrown in the air. The derivative of a fluent is known as a fluxion, the main focus of Newton's calculus. A fluent can be found from its corresponding fluxion through integration.\n\n"}
{"id": "42442683", "url": "https://en.wikipedia.org/wiki?curid=42442683", "title": "Gale–Ryser theorem", "text": "Gale–Ryser theorem\n\nThe Gale–Ryser theorem is a result in graph theory and combinatorial matrix theory, two branches of combinatorics. It provides one of two known approaches to solving the bipartite realization problem, i.e. it gives a necessary and sufficient condition for two finite sequences of natural numbers to be the degree sequence of a labeled simple bipartite graph; a sequence obeying these conditions is called \"bigraphic\". It is an analog of the Erdős–Gallai theorem for simple graphs. The theorem was published in 1957 by H. J. Ryser and also by David Gale.\n\nA pair of sequences of nonnegative integers formula_1 and formula_2 with formula_3 is bigraphic if and only if formula_4 and the following inequality holds for k such that formula_5:\n\nSometimes this theorem is stated with the additional constraint formula_7. This condition is not necessary, because the labels of vertices of one partite set in a bipartite graph can be switched arbitrarily.\nIn 1962 Ford and Fulkerson gave a different but equivalent formulation for the theorem.\n\nThe theorem can also be stated in terms of zero-one matrices. The connection can be seen if one realizes that each bipartite graph has a biadjacency matrix where the column sums and row sums correspond to formula_1 and formula_2. Each sequence can also be considered as a partition of the same number formula_10. It turns out that partition formula_11 where formula_12 is the conjugate partition of formula_2. The conjugate partition can be determined by a Ferrers diagram. Moreover, there is a connection to the relation majorization. Consider sequences formula_1, formula_2 and formula_11 as formula_17-dimensional vectors formula_18, formula_19 and formula_20. Since formula_21, the theorem above states that a pair of nonnegative integer sequences a and b with nonincreasing a is bigraphic if and only if the conjugate partition formula_20 of formula_19 majorizes formula_18. \nA third formulation is in terms of degree sequences of simple directed graphs with at most one loop per vertex. In this case the matrix is interpreted as the adjacency matrix of such a directed graph. When are pairs of nonnegative integers formula_25 the indegree-outdegree pairs of a labeled directed graph with at most one loop per vertex? The theorem can easily be adapted to this formulation, because there does not exist a special order of b.\n\nThe proof is composed of two parts: the necessity of the condition and its sufficiency. We outline the proof of both parts in the language of matrices. To see that the condition in the theorem is necessary, consider the adjacency matrix of a bigraphic realization with row sums formula_2 and column sums formula_1, and shift all ones in the matrix to the left. The row sums remain, while the column sums are now formula_20. The operation of shifting all ones to the left increases a partition in majorization order, and so formula_20 majorizes formula_18.\n\nThe original proof of sufficiency of the condition was rather complicated. gave a simple algorithmic proof. The idea is to start with the Ferrers diagram of formula_19 and shift ones to the right until the column sums are formula_18. The algorithm runs in at most formula_17 steps, in each of which a single one entry is moved to the right.\n\nBerger proved that it suffices to consider those formula_34th inequalities such that formula_35 with formula_36 and the equality for formula_37.\n\nA pair of finite sequences of nonnegative integers formula_18 and formula_19 with nonincreasing formula_18 is bigraphic if and only if formula_4 and there exists a sequence formula_42 such that the pair formula_43 is bigraphic and formula_42 majorizes formula_18 . Moreover, in is also proved that pair formula_18 and formula_19 has more bigraphic realizations than pair formula_42 and formula_19. This yields to the result that regular sequences have for fixed numbers of vertices and edges the largest number of bigraphic realizations, if n divides m. They are the `contrary sequences' of threshold sequences with only one unique bigraphic realization, which is known as threshold graph. Minconvex sequences generalize this concept if n does not divide m. \n\nSimilar theorems describe the degree sequences of simple graphs and simple directed graphs. The first problem is characterized by the Erdős–Gallai theorem. The latter case is characterized by the Fulkerson–Chen–Anstee theorem.\n\n"}
{"id": "2336205", "url": "https://en.wikipedia.org/wiki?curid=2336205", "title": "Hilbert's fifteenth problem", "text": "Hilbert's fifteenth problem\n\nHilbert's fifteenth problem is one of the 23 Hilbert problems set out in a celebrated list compiled in 1900 by David Hilbert. The problem is to put Schubert's enumerative calculus on a rigorous foundation.\n\nSplitting the question, as now it would be understood, into Schubert calculus and enumerative geometry, the former is well-founded on the basis of the topology of Grassmannians, and intersection theory. The latter has status that is less clear, if clarified with respect to the position in 1900.\n\nWhile enumerative geometry made no connection with physics during the first century of its development, it has since emerged as a central element of string theory.\n\nThe entirety of the original problem statement is as follows:\n\nThe problem consists in this: To establish rigorously and with an exact determination of the limits of their validity those geometrical numbers which Schubert especially has determined on the basis of the so-called principle of special position, or conservation of number, by means of the enumerative calculus developed by him.\n\nAlthough the algebra of today guarantees, in principle, the possibility of carrying out the processes of elimination, yet for the proof of the theorems of enumerative geometry decidedly more is requisite, namely, the actual carrying out of the process of elimination in the case of equations of special form in such a way that the degree of the final equations and the multiplicity of their solutions may be foreseen.\n\nSchubert calculus is a branch of algebraic geometry introduced in the nineteenth century by Hermann Schubert, in order to solve various counting problems of projective geometry (part of enumerative geometry). It was a precursor of several more modern theories, for example characteristic classes, and in particular its algorithmic aspects are still of current interest.\n\nThe objects introduced by Schubert are the Schubert cells, which are locally closed sets in a Grassmannian defined by conditions of incidence of a linear subspace in projective space with a given flag. For details see Schubert variety.\n\n"}
{"id": "298428", "url": "https://en.wikipedia.org/wiki?curid=298428", "title": "Identity (mathematics)", "text": "Identity (mathematics)\n\nIn mathematics an identity is an equality relation \"A\" = \"B\", such that \"A\" and \"B\" contain some variables and \"A\" and \"B\" produce the same value as each other regardless of what values (usually numbers) are substituted for the variables. In other words, \"A\" = \"B\" is an identity if \"A\" and \"B\" define the same functions. This means that an \"identity\" is an \"equality\" between functions that are differently defined. For example, (\"a\" + \"b\")  =  \"a\" + 2\"ab\" + \"b\" and are identities. Identities are sometimes indicated by the triple bar symbol instead of , the equals sign.\n\nGeometrically, these are identities involving certain functions of one or more angles. They are distinct from triangle identities, which are identities involving both angles and side lengths of a triangle. Only the former are covered in this article.\n\nThese identities are useful whenever expressions involving trigonometric functions need to be simplified. An important application is the integration of non-trigonometric functions: a common technique involves first using the substitution rule with a trigonometric function, and then simplifying the resulting integral with a trigonometric identity.\n\nOne example is formula_1\nwhich is true for all complex values of formula_2 (since the complex numbers formula_3 are the domain of sin and cos), as opposed to\nwhich is true only for some values of formula_2, not all. For example, the latter equation is true when formula_6 false when formula_7.\n\nThe following identities hold for all integer exponents, provided that the base is non-zero:\n\nExponentiation is not commutative. This contrasts with addition and multiplication, which are. For example, and , but , whereas .\n\nExponentiation is not associative either. Addition and multiplication are. For example,\n\nSeveral important formulas, sometimes called \"logarithmic identities\" or \"log laws\", relate logarithms to one another.\n\nThe logarithm of a product is the sum of the logarithms of the numbers being multiplied; the logarithm of the ratio of two numbers is the difference of the logarithms. The logarithm of the power of a number is \"p\" times the logarithm of the number itself; the logarithm of a root is the logarithm of the number divided by \"p\". The following table lists these identities with examples. Each of the identities can be derived after substitution of the logarithm definitions x = b, and/or y = b, in the left hand sides.\n\nThe logarithm log(\"x\") can be computed from the logarithms of \"x\" and \"b\" with respect to an arbitrary base \"k\" using the following formula:\nTypical scientific calculators calculate the logarithms to bases 10 and \"e\". Logarithms with respect to any base \"b\" can be determined using either of these two logarithms by the previous formula:\nGiven a number \"x\" and its logarithm log(\"x\") to an unknown base \"b\", the base is given by:\n\nThe hyperbolic functions satisfy many identities, all of them similar in form to the trigonometric identities. In fact, Osborn's rule states that one can convert any trigonometric identity into a hyperbolic identity by expanding it completely in terms of integral powers of sines and cosines, changing sine to sinh and cosine to cosh, and switching the sign of every term which contains a product of 2, 6, 10, 14, ... sinhs.\n\nThe Gudermannian function gives a direct relationship between the circular functions and the hyperbolic ones that does not involve complex numbers.\n\n\n"}
{"id": "10797093", "url": "https://en.wikipedia.org/wiki?curid=10797093", "title": "Karamata's inequality", "text": "Karamata's inequality\n\nIn mathematics, Karamata's inequality, named after Jovan Karamata, also known as the majorization inequality, is a theorem in elementary algebra for convex and concave real-valued functions, defined on an interval of the real line. It generalizes the discrete form of Jensen's inequality.\n\nLet be an interval of the real line and let denote a real-valued, convex function defined on . If and are numbers in such that majorizes , then\n\nHere majorization means that and satisfies\n\nand we have the inequalities\n\nand the equality\n\nIf   is a strictly convex function, then the inequality () holds with equality if and only if we have for all }.\n\n\nThe finite form of Jensen's inequality is a special case of this result. Consider the real numbers and let \n\ndenote their arithmetic mean. Then majorizes the -tuple , since the arithmetic mean of the largest numbers of is at least as large as the arithmetic mean of all the numbers, for every }. By Karamata's inequality () for the convex function ,\n\nDividing by gives Jensen's inequality. The sign is reversed if   is concave.\n\nWe may assume that the numbers are in decreasing order as specified in ().\n\nIf for all }, then the inequality () holds with equality, hence we may assume in the following that for at least one .\n\nIf for an }, then the inequality () and the majorization properties () and () are not affected if we remove and . Hence we may assume that for all }.\n\nIt is a property of convex functions that for two numbers in the interval the slope\n\nof the secant line through the points and of the graph of   is a monotonically non-decreasing function in for fixed (and ). This implies that\n\nfor all }. Define and \n\nfor all }. By the majorization property (), for all } and by (), . Hence,\n\n_{=\\,x_i}{} - (\\underbrace{B_i - B_{i-1}}_{=\\,y_i})\\bigr)\\\\\n&=\\sum_{i=1}^n c_i (A_i - B_i) - \\sum_{i=1}^n c_i (A_{i-1} - B_{i-1})\\\\\n&=c_n (\\underbrace{A_n-B_n}_{=\\,0}) + \\sum_{i=1}^{n-1}(\\underbrace{c_i - c_{i + 1}}_{\\ge\\,0})(\\underbrace{A_i - B_i}_{\\ge\\,0}) - c_1(\\underbrace{A_0-B_0}_{=\\,0})\\\\\n&\\ge0,\n\nwhich proves Karamata's inequality ().\n\nTo discuss the case of equality in (), note that by () and our assumption for all }. Let be the smallest index such that , which exists due to (). Then . If   is strictly convex, then there is strict inequality in (), meaning that . Hence there is a strictly positive term in the sum on the right hand side of () and equality in () cannot hold.\n\nIf the convex function   is non-decreasing, then . The relaxed condition () means that , which is enough to conclude that in the last step of ().\n\nIf the function   is strictly convex and non-decreasing, then . It only remains to discuss the case . However, then there is a strictly positive term on the right hand side of () and equality in () cannot hold.\n\nAn explanation of Karamata's inequality and majorization theory can be found here.\n"}
{"id": "48235152", "url": "https://en.wikipedia.org/wiki?curid=48235152", "title": "Known-key distinguishing attack", "text": "Known-key distinguishing attack\n\nIn cryptography, a known-key distinguishing attack is an attack model against symmetric ciphers, whereby an attacker who knows the key can find a structural property in cipher, where the transformation from plaintext to ciphertext is not random. There is no common formal definition for what such a transformation may be. The chosen-key distinguishing attack is strongly related, where the attacker can choose a key to introduce such transformations.\n\nThese attacks do not directly compromise the confidentiality of ciphers, because in a classical scenario, the key is unknown to the attacker. Known-/chosen-key distinguishing attacks apply in the \"open key model\" instead. They are known to be applicable in some situations where block ciphers are converted to hash functions, leading to practical collision attacks against the hash.\n\nKnown-key distinguishing attacks were first introduced in 2007 by Lars Knudsen and Vincent Rijmen in a paper that proposed such an attack against 7 out of 10 rounds of the AES cipher and another attack against a generalized Feistel cipher. Their attack finds plaintext/ciphertext pairs for a cipher with a known key, where the input and output have \"s\" least significant bits set to zero, in less than 2 time (where \"s\" is fewer than half the block size).\n\nThese attacks have also been applied to reduced-round Threefish (Skein) and Phelix.\n\n"}
{"id": "21099324", "url": "https://en.wikipedia.org/wiki?curid=21099324", "title": "Krull–Schmidt category", "text": "Krull–Schmidt category\n\nIn category theory, a Krull–Schmidt category is a generalization of categories in which the Krull–Schmidt theorem holds. They arise, for example, in the study of finite-dimensional modules over an algebra.\n\nLet \"C\" be an additive category, or more generally an additive -linear category for a commutative ring . We call \"C\" a Krull–Schmidt category provided that every object decomposes into a finite direct sum of objects having local endomorphism rings. Equivalently, \"C\" has split idempotents and the endomorphism ring of every object is semiperfect.\n\nOne has the analogue of the Krull–Schmidt theorem in Krull–Schmidt categories:\n\nAn object is called \"indecomposable\" if it is not isomorphic to a direct sum of two nonzero objects. In a Krull–Schmidt category we have that\n\nOne can define the Auslander–Reiten quiver of a Krull–Schmidt category.\n\n\nThe category of finitely-generated projective modules over the integers has split idempotents, and every module is isomorphic to a finite direct sum of copies of the regular module, the number being given by the rank. Thus the category has unique decomposition into indecomposables, but is not Krull-Schmidt since the regular module does not have a local endomorphism ring.\n\n\n"}
{"id": "5258600", "url": "https://en.wikipedia.org/wiki?curid=5258600", "title": "Logic Lane", "text": "Logic Lane\n\nLogic Lane is a small historic cobbled lane that runs through University College in Oxford, England, so called because it was the location of a school of logicians. It links the High Street at the front of the college with Merton Street to the rear, which is also cobbled. Logic Lane covered bridge is a short covered bridge over the lane at the High Street end (see below).\n\nTo the west of the lane are the Radcliffe Quad and the Master's Lodgings. To the east are the 1903 Durham Buildings (on the High Street) and the Goodhart Quad. The lane is locked at night (usually at a time earlier than that advertised on the signs at either end of the lane), with gates at each end. It is mainly used by pedestrians, but vehicular access is possible.\n\nDuring July and August 1960, an archaeological excavation was undertaken to the east of Logic Lane before the construction of the Goodhart Building. Evidence of Bronze Age ditches were found, as well as Saxon remains.\n\nLogic Lane was formerly known as Horseman Lane in the 13th and 14th centuries. During the medieval period, a horse mill was located here. It was also known as Horsemull Lane. The name of Logic Lane was adopted by the 17th century, owing to the presence of a school of logicians at the northern end of the lane.\n\nA medieval street used to run across Logic Lane as an extension of the current Kybald Street to the west, but was closed in 1448.\n\nIn 1904, a covered bridge at the High Street end of the lane was built to link the older part of the college with the then new Durham Buildings. The lane was officially a public bridleway, and the city council opposed the scheme, but the court judgement was in favour of the college.\n\n\n"}
{"id": "11903542", "url": "https://en.wikipedia.org/wiki?curid=11903542", "title": "László Rédei", "text": "László Rédei\n\nLászló Rédei (Rákoskeresztúr, 15 November 1900—Budapest, 21 November 1980) was a Hungarian mathematician.\n\nHe graduated from the University of Budapest and initially worked as a schoolteacher. In 1940 he was appointed professor in the University of Szeged and in 1967 moved to the Mathematical Institute of the Hungarian Academy of Sciences in Budapest.\n\nHis mathematical work was in algebraic number theory and abstract algebra, especially group theory. He proved that every finite tournament contains an odd number of Hamiltonian paths. He gave several proofs of the theorem on quadratic reciprocity. He proved important results concerning the invariants of the class groups of quadratic number fields. In several cases, he determined if the ring of integers of the real quadratic field Q() is Euclidean or not. He successfully generalized Hajós's theorem. This led him to the investigations of lacunary polynomials over finite fields, which he eventually published in a book. This work on lacunary polynomials has had a big influence in the field of finite geometry where it plays an important role in the theory of blocking sets. He introduced a very general notion of skew product of groups, both the Schreier-extension and the Zappa–Szép product are special case of. He explicitly determined those finite noncommutative groups whose all proper subgroups were commutative (1947). This is one of the very early results which eventually led to the classification of all finite simple groups.\n\nHe was the president of the János Bolyai Mathematical Society (1947–1949). He was awarded the Kossuth Prize twice. He was elected corresponding member (1949), full member (1955) of the Hungarian Academy of Sciences.\n\n\n\n"}
{"id": "1707627", "url": "https://en.wikipedia.org/wiki?curid=1707627", "title": "Metric dimension (graph theory)", "text": "Metric dimension (graph theory)\n\nIn graph theory, the metric dimension of a graph \"G\" is the minimum cardinality of a subset \"S\" of vertices such that all other vertices are uniquely determined by their distances to the vertices in \"S\". Finding the metric dimension of a graph is an NP-hard problem; the decision version, determining whether the metric dimension is less than a given value, is NP-complete.\n\nFor an ordered subset formula_1 of vertices and a vertex \"v\" in a connected graph \"G\", the representation of \"v\" with respect to \"W\" is the ordered \"k\"-tuple formula_2, where \"d\"(\"x\",\"y\") represents the distance between the vertices \"x\" and \"y\". The set \"W\" is a resolving set (or locating set) for \"G\" if every two vertices of \"G\" have distinct representations. The metric dimension of \"G\" is the minimum cardinality of a resolving set for \"G\". A resolving set containing a minimum number of vertices is called a basis (or reference set) for \"G\". Resolving sets for graphs were introduced independently by and , while the concept of a resolving set and that of metric dimension were defined much earlier by Blumenthal in his monograph \"Theory and Applications of Distance Geometry\". Graphs are special examples of metric spaces with their intrinsic path metric.\n\n (see also and ) provides the following simple characterization of the metric dimension of a tree. If the tree is a path, its metric dimension is one. Otherwise, let \"L\" denote the set of degree-one vertices in the tree (usually called leaves, although Slater uses that word differently). Let \"K\" be the set of vertices that have degree greater than two, and that are connected by paths of degree-two vertices to one or more leaves. Then the metric dimension is |\"L\"| − |\"K\"|. A basis of this cardinality may be formed by removing from \"L\" one of the leaves associated with each vertex in \"K\". The same algorithm is valid for the line graph of the tree, as proved by (and thus any tree and its line graph have the same metric dimension).\n\nIn , it is proved that:\n\n prove the inequality formula_5 for any -vertex graph with diameter and metric dimension β. This bounds follows from the fact that each vertex that is not in the resolving set is uniquely determined by a distance vector of length β with each entry being an integer between 1 and (there are precisely formula_6 such vectors). However, the bound is only achieved for formula_7 or formula_8; the more precise bound formula_9 is proved by .\n\nFor specific graph classes, smaller bounds can hold. For example, proved that formula_10 for trees (the bound being tight for even values of ), and a bound of the form formula_11 for outerplanar graphs. The same authors proved that formula_12 for graphs with no complete graph of order as a minor and also gave bounds for chordal graphs and graphs of bounded treewidth. The authors \n\nDeciding whether the metric dimension of a graph is at most a given integer is NP-complete . It remains NP-complete for bounded-degree planar graphs , split graphs, bipartite graphs and their complements, line graphs of bipartite graphs , unit disk graphs , interval graphs of diameter 2 and permutation graphs of diameter 2 . \n\nFor any fixed constant \"k\", the graphs of metric dimension at most \"k\" can be recognized in polynomial time, by testing all possible \"k\"-tuples of vertices, but this algorithm is not fixed-parameter tractable (for the natural parameter \"k\", the solution size). Answering a question posed by , show that the metric dimension decision problem is complete for the parameterized complexity class W[2], implying that a time bound of the form \"n\" as achieved by this naive algorithm is likely optimal and that a fixed-parameter tractable algorithm (for the parameterization by \"k\") is unlikely to exist. Nevertheless, the problem becomes fixed-parameter tractable when restricted to interval graphs , and more generally to graphs of bounded tree-length , such as chordal graphs, permutation graphs or asteroidal-triple-free graphs.\n\nDeciding whether the metric dimension of a tree is at most a given integer can be done in linear time (; ). Other linear-time algorithms exist for cographs , chain graphs and cactus block graphs (a class including both cactus graphs and block graphs). The problem may be solved in polynomial time on outerplanar graphs . It may also be solved in polynomial time for graphs of bounded cyclomatic number , but this algorithm is again not fixed-parameter tractable (for the parameter \"cyclomatic number\") because the exponent in the polynomial depends on the cyclomatic number. There exist fixed-parameter tractable algorithms to solve the metric dimension problem for the parameters \"vertex cover\" , \"max leaf number\" and \"modular width\" . Graphs with bounded cyclomatic number, vertex cover number or max leaf number all have bounded treewidth, however it is an open problem to determine the complexity of the metric dimension problem even on graphs of treewidth 2, that is, series-parallel graphs .\n\nThe metric dimension of an arbitrary \"n\"-vertex graph may be approximated in polynomial time to within an approximation ratio of formula_15 by expressing it as a set cover problem, a problem of covering all of a given collection of elements by as few sets as possible in a given family of sets . In the set cover problem formed from a metric dimension problem, the elements to be covered are the formula_16 pairs of vertices to be distinguished, and the sets that can cover them are the sets of pairs that can be distinguished by a single chosen vertex. The approximation bound then follows by applying standard approximation algorithms for set cover. An alternative greedy algorithm that chooses vertices according to the difference in entropy between the equivalence classes of distance vectors before and after the choice achieves an even better approximation ratio, formula_17 . This approximation ratio is close to best possible, as under standard complexity-theoretic assumptions a ratio of formula_18 cannot be achieved in polynomial time for any formula_19 . The latter hardness of approximation still holds for instances restricted to subcubic graphs , and even to bipartite subcubic graphs as shown in Hartung's PhD thesis .\n\n"}
{"id": "3832714", "url": "https://en.wikipedia.org/wiki?curid=3832714", "title": "Modal operator", "text": "Modal operator\n\nA modal connective (or modal operator) is a logical connective for modal logic. It is an operator which forms propositions from propositions. In general, a modal operator has the \"formal\" property of being non-truth-functional, and is \"intuitively\" characterized by expressing a modal attitude (such as necessity, possibility, belief, or knowledge) about the proposition to which the operator is applied. \n\nThere are several ways to interpret modal operators in modal logic, including:\nalethic, deontic, axiological, epistemic, and doxastic.\n\nAlethic modal operators (M-operators) determine the fundamental conditions of possible worlds, especially causality, time-space parameters, and the action capacity of persons. They indicate the possibility, impossibility and necessity of actions, states of affairs, events, people, and qualities in the possible worlds. \n\nDeontic modal operators (P-operators) influence the construction of possible worlds as proscriptive or prescriptive norms, i.e. they indicate what is prohibited, obligatory, or permitted. \n\nAxiological modal operators (G-operators) transform the world's entities into values and disvalues as seen by a social group, a culture, or a historical period. Axiological modalities are highly subjective categories: what is good for one person may be considered as bad by another one. \n\nEpistemic modal operators (K-operators) reflect the level of knowledge, ignorance and belief in the possible world. \n\nDoxastic modal operators express belief in statements. \n"}
{"id": "34677780", "url": "https://en.wikipedia.org/wiki?curid=34677780", "title": "Murakami–Yano formula", "text": "Murakami–Yano formula\n\nIn geometry, the Murakami–Yano formula, introduced by , is a formula for the volume of a hyperbolic or spherical tetrahedron given in terms of its dihedral angles.\n\n"}
{"id": "945957", "url": "https://en.wikipedia.org/wiki?curid=945957", "title": "New Foundations", "text": "New Foundations\n\nIn mathematical logic, New Foundations (NF) is an axiomatic set theory, conceived by Willard Van Orman Quine as a simplification of the theory of types of \"Principia Mathematica\". Quine first proposed NF in a 1937 article titled \"New Foundations for Mathematical Logic\"; hence the name. Much of this entry discusses NFU, an important variant of NF due to Jensen (1969) and exposited in Holmes (1998). In 1940 and in a revision of 1951 Quine introduced an extension of NF sometimes called \"Mathematical Logic\" or \"ML\", that included proper classes as well as sets.\n\nNew Foundations has a universal set, so it is a non-well founded set theory. That is to say, it is an axiomatic set theory that allows infinite descending chains of membership such as\n… x ∈ x ∈ …x ∈ x ∈ x. It avoids Russell's paradox by permitting only stratifiable formulae to be defined using the axiom (schema) of comprehension. For instance x ∈ y is a stratifiable formula, but x ∈ x is not (for details of how this works see below).\n\nThe primitive predicates of Russellian unramified typed set theory (TST), a streamlined version of the theory of types, are equality (formula_1) and membership (formula_2). TST has a linear hierarchy of types: type 0 consists of individuals otherwise undescribed. For each (meta-) natural number \"n\", type \"n\"+1 objects are sets of type \"n\" objects; sets of type \"n\" have members of type \"n\"-1. Objects connected by identity must have the same type. The following two atomic formulas succinctly describe the typing rules: formula_3 and formula_4. (Quinean set theory seeks to eliminate the need for such superscripts.)\n\nThe axioms of TST are:\n\nThis type theory is much less complicated than the one first set out in the \"Principia Mathematica\", which included types for relations whose arguments were not necessarily all of the same type. In 1914, Norbert Wiener showed how to code the ordered pair as a set of sets, making it possible to eliminate relation types in favor of the linear hierarchy of sets described here.\n\nThe well-formed formulas of New Foundations (NF) are the same as the well-formed formulas of TST, but with the type annotations erased. The axioms of NF are:\n\n\nBy convention, NF's \"Comprehension\" schema is stated using the concept of stratified formula and making no direct reference to types. A formula formula_12 is said to be stratified if there exists a function \"f\" from pieces of syntax to the natural numbers, such that for any atomic subformula formula_13 of formula_12 we have \"f\"(\"y\") = \"f\"(\"x\") + 1, while for any atomic subformula formula_15 of formula_12, we have \"f\"(\"x\") = \"f\"(\"y\"). \"Comprehension\" then becomes:\nEven the indirect reference to types implicit in the notion of stratification can be eliminated. Theodore Hailperin showed in 1944 that \"Comprehension\" is equivalent to a finite conjunction of its instances, so that NF can be finitely axiomatized without any reference to the notion of type.\n\n\"Comprehension\" may seem to run afoul of problems similar to those in naive set theory, but this is not the case. For example, the existence of the impossible Russell class formula_19 is not an axiom of NF, because formula_20 cannot be stratified.\n\nRelations and functions are defined in TST (and in NF and NFU) as sets of ordered pairs in the usual way. The usual definition of the ordered pair, first proposed by Kuratowski in 1921, has a serious drawback for NF and related theories: the resulting ordered pair necessarily has a type two higher than the type of its arguments (its left and right projections). Hence for purposes of determining stratification, a function is three types higher than the members of its field.\n\nIf one can define a pair in such a way that its type is the same type as that of its arguments (resulting in a type-level ordered pair), then a relation or function is merely one type higher than the type of the members of its field. Hence NF and related theories usually employ Quine's set-theoretic definition of the \nordered pair, which yields a type-level ordered pair. Holmes (1998) takes the ordered pair and its left and right projections as primitive. Fortunately, whether the ordered pair is type-level by definition or by assumption (i.e., taken as primitive) usually does not matter.\n\nThe existence of a type-level ordered pair implies \"Infinity\", and NFU + \"Infinity\" interprets NFU + \"there is a type level ordered pair\" (they are not quite the same theory, but the differences are inessential). Conversely, NFU + \"Infinity\" + \"Choice\" proves the existence of a type-level ordered pair.\n\nNF (and NFU + \"Infinity\" + \"Choice\", described below and known consistent) allow the construction of two kinds of sets that ZFC and its proper extensions disallow because they are \"too large\" (some set theories admit these entities under the heading of proper classes):\n\nNew Foundations can be finitely axiomatized.\n\nThe category whose objects are the sets of NF and whose arrows are the functions between those sets is not Cartesian closed; Cartesian closure can be a useful property for a category of sets. Since NF lacks Cartesian closure, not every function curries as one might intuitively expect, and NF is not a topos.\n\nThe outstanding problem with NF is that it is not yet verified to be relatively consistent to any mainstream mathematical system. NF disproves \"Choice\", and so proves \"Infinity\" (Specker, 1953). But it is also known (Jensen, 1969) that allowing urelements (multiple distinct objects lacking members) yields NFU, a theory that is consistent relative to Peano arithmetic; if Infinity and Choice are added, the resulting theory has the same consistency strength as type theory with infinity or bounded Zermelo set theory. (NFU corresponds to a type theory TSTU, where type 0 has urelements, not just a single empty set.) There are other relatively consistent variants of NF.\n\nNFU is, roughly speaking, weaker than NF because in NF, the power set of the universe is the universe itself, while in NFU, the power set of the universe may be strictly smaller than the universe (the power set of the universe contains only sets, while the universe may contain urelements). In fact, this is necessarily the case in NFU+\"Choice\".\n\nSpecker has shown that NF is equiconsistent with TST + \"Amb\", where \"Amb\" is the axiom scheme of typical ambiguity which asserts formula_23 for any formula formula_12, formula_25 being the formula obtained by raising every type index in formula_12 by one. NF is also equiconsistent with the theory TST augmented with a \"type shifting automorphism\", an operation which raises type by one, mapping each type onto the next higher type, and preserves equality and membership relations (and which cannot be used in instances of \"Comprehension\": it is external to the theory). The same results hold for various fragments of TST in relation to the corresponding fragments of NF.\n\nIn the same year (1969) that Jensen proved NFU consistent, Grishin proved formula_27 consistent. formula_27 is the fragment of NF with full extensionality (no urelements) and those instances of \"Comprehension\" which can be stratified using just three types. This theory is a very awkward medium for mathematics (although there have been attempts to alleviate this awkwardness), largely because there is no obvious definition for an ordered pair. Despite this awkwardness, formula_27 is very interesting because \"every\" infinite model of TST restricted to three types satisfies \"Amb\". Hence for every such model there is a model of formula_27 with the same theory. This does not hold for four types: formula_31 is the same theory as NF, and we have no idea how to obtain a model of TST with four types in which \"Amb\" holds.\n\nIn 1983, Marcel Crabbé proved consistent a system he called NFI, whose axioms are unrestricted extensionality and those instances of \"Comprehension\" in which no variable is assigned a type higher than that of the set asserted to exist. This is a predicativity restriction, though NFI is not a predicative theory: it admits enough impredicativity to define the set of natural numbers (defined as the intersection of all inductive sets; note that the inductive sets quantified over are of the same type as the set of natural numbers being defined). Crabbé also discussed a subtheory of NFI, in which only parameters (free variables) are allowed to have the type of the set asserted to exist by an instance of \"Comprehension\". He called the result \"predicative NF\" (NFP); it is, of course, doubtful whether any theory with a self-membered universe is truly predicative. Holmes has shown that NFP has the same consistency strength as the predicative theory of types of \"Principia Mathematica\" without the Axiom of reducibility.\n\nSince 2015, several candidate proofs by Randall Holmes of the consistency of NF relative to ZF have been available both on arxiv and on the logician's home page. Holmes demonstrates the equiconsistency of a 'weird' variant of TST, namely TTT - 'tangled type theory with λ-types' - with NF. Holmes next shows that TTT is consistent relative to ZFA that is, ZF with atoms but without choice. Holmes demonstrates this by constructing in ZFA+C that is ZF with atoms and choice, a class model of ZFA which includes 'tangled webs of cardinals' . The candidate proofs are all rather long, but no irrecoverable faults have been identified by the NF community so far.\n\nNF steers clear of the three well-known paradoxes of set theory. That NFU, a consistent (relative to Peano arithmetic) theory, also avoids the paradoxes may increase one's confidence in this fact.\n\nThe \"Russell paradox\": An easy matter; formula_32 is not a stratified formula, so the existence of formula_19 is not asserted by any instance of \"Comprehension\". Quine said that he constructed NF with this paradox uppermost in mind.\n\n\"Cantor's paradox\" of the largest cardinal number exploits the application of Cantor's theorem to the universal set. Cantor's theorem says (given ZFC) that the power set formula_34 of any set formula_35 is larger than formula_35 (there can be no injection (one-to-one map) from formula_34 into formula_35). Now of course there is an injection from formula_39 into formula_40, if formula_40 is the universal set! The resolution requires that one observes that formula_42 makes no sense in the theory of types: the type of formula_34 is one higher than the type of formula_35. The correctly typed version (which is a theorem in the theory of types for essentially the same reasons that the original form of Cantor's theorem works in ZF) is formula_45, where formula_46 is the set of one-element subsets of formula_35. The specific instance of this theorem of interest is formula_48: there are fewer one-element sets than sets (and so fewer one-element sets than general objects, if we are in NFU). The \"obvious\" bijection formula_49 from the universe to the one-element sets is not a set; it is not a set because its definition is unstratified. Note that in all known models of NFU it is the case that formula_50; \"Choice\" allows one not only to prove that there are urelements but that there are many cardinals between formula_51 and formula_52.\n\nOne can now introduce some useful notions. A set formula_35 which satisfies the intuitively appealing formula_54 is said to be Cantorian: a Cantorian set satisfies the usual form of Cantor's theorem. A set formula_35 which satisfies the further condition that formula_56, the restriction of the singleton map to \"A\", is a set is not only Cantorian set but strongly Cantorian.\n\nThe \"Burali-Forti paradox\" of the largest ordinal number goes as follows. Define (following naive set theory) the ordinals as equivalence classes of well-orderings under isomorphism. There is an obvious natural well-ordering on the ordinals; since it is a well-ordering it belongs to an ordinal formula_57. It is straightforward to prove (by transfinite induction) that the order type of the natural order on the ordinals less than a given ordinal formula_58 is formula_58 itself. But this means that formula_57 is the order type of the ordinals formula_61 and so is strictly less than the order type of all the ordinals — but the latter is, by definition, formula_57 itself!\n\nThe solution to the paradox in NF(U) starts with the observation that the order type of the natural order on the ordinals less than formula_58 is of a higher type than formula_58. Hence a type level ordered pair is two types higher than the type of its arguments and the usual Kuratowski ordered pair four types higher. For any order type formula_58, we can define an order type formula_58 one type higher: if formula_67, then formula_68 is the order type of the order formula_69. The triviality of the T operation is only a seeming one; it is easy to show that T is a strictly monotone (order preserving) operation on the ordinals.\n\nNow the lemma on order types may be restated in a stratified manner: the order type of the natural order on the ordinals formula_70 is formula_71 or formula_72\ndepending on which pair is used (we assume the type level pair hereinafter). From this one may deduce that the order type on the ordinals formula_73 is formula_74, and thus formula_75. Hence the T operation is not a function; there cannot be a strictly monotone set map from ordinals to ordinals which sends an ordinal downward! Since T is monotone, we have formula_76, a \"descending sequence\" in the ordinals which cannot be a set.\n\nOne might assert that this result shows that no model of NF(U) is \"standard\", since the ordinals in any model of NFU are externally not well-ordered. One need not take a position on this, but can note that it is also a theorem of NFU that any set model of NFU has non-well-ordered \"ordinals\"; NFU does not conclude that the universe \"V\" is a model of NFU, despite \"V\" being a set, because the membership relation is not a set relation.\n\nFor a further development of mathematics in NFU, with a comparison to the development of the same in ZFC, see implementation of mathematics in set theory.\n\nML is an extension of NF that includes proper classes as well as sets.\nThe set theory of the 1940 first edition of Quine's \"Mathematical Logic\" married NF to the proper classes of NBG set theory, and included an axiom schema of unrestricted comprehension for proper classes. However proved that the system presented in \"Mathematical Logic\" was subject to the Burali-Forti paradox. This result does not apply to NF. showed how to amend Quine's axioms for ML so as to avoid this problem, and Quine included the resulting axiomatization in the 1951 second and final edition of \"Mathematical Logic\".\n\nWang proved that if NF is consistent then so is the revised ML, and also showed that the revised ML can prove the consistency of NF, that is that NF and the revised ML are equiconsistent.\n\nThere is a fairly simple method for producing models of NFU in bulk. Using well-known techniques of model theory, one can construct a nonstandard model of Zermelo set theory (nothing nearly as strong as full ZFC is needed for the basic technique) on which there is an external automorphism \"j\" (not a set of the model) which moves a rank formula_77 of the cumulative hierarchy of sets. We may suppose without loss of generality that formula_78. We talk about the automorphism moving the rank rather than the ordinal because we do not want to assume that every ordinal in the model is the index of a rank.\n\nThe domain of the model of NFU will be the nonstandard rank formula_77. The membership relation of the model of NFU will be\n\n\nIt may now be proved that this actually is a model of NFU. Let formula_12 be a stratified formula in the language of NFU. Choose an assignment of types to all variables in the formula which witnesses the fact that it is stratified. Choose a natural number \"N\" greater than all types assigned to variables by this stratification.\n\nExpand the formula formula_12 into a formula formula_83 in the language of the nonstandard model of Zermelo set theory with automorphism \"j\" using the definition of membership in the model of NFU. Application of any power of \"j\" to both sides of an equation or membership statement preserves its truth value because \"j\" is an automorphism. Make such an application to each atomic formula in formula_83 in such a way that each variable \"x\" assigned type \"i\" occurs with exactly formula_85 applications of \"j\". This is possible thanks to the form of the atomic membership statements derived from NFU membership statements, and to the formula being stratified. Each quantified sentence formula_86 can be converted to the form formula_87 (and similarly for existential quantifiers). Carry out this transformation everywhere and obtain a formula formula_88 in which \"j\" is never applied to a bound variable.\n\nChoose any free variable \"y\" in formula_12 assigned type \"i\". Apply formula_90 uniformly to the entire formula to obtain a formula formula_91 in which \"y\" appears without any application of \"j\". Now formula_92 exists (because \"j\" appears applied only to free variables and constants), belongs to formula_93, and contains exactly those \"y\" which satisfy the original formula\nformula_12 in the model of NFU. formula_95 has this extension in the model of NFU (the application of \"j\" corrects for the different definition of membership in the model of NFU). This establishes that \"Stratified Comprehension\" holds in the model of NFU.\n\nTo see that weak \"Extensionality\" holds is straightforward: each nonempty element of formula_96 inherits a unique extension from the nonstandard model, the empty set inherits its usual extension as well, and all other objects are urelements.\n\nThe basic idea is that the automorphism \"j\" codes the \"power set\" formula_93 of our \"universe\" formula_77 into its externally isomorphic copy formula_96 inside our \"universe.\" The remaining objects not coding subsets of the universe are treated as urelements.\n\nIf formula_58 is a natural number \"n\", one gets a model of NFU which claims that the universe is finite (it is externally infinite, of course). If formula_58 is infinite and the \"Choice\" holds in the nonstandard model of ZFC, one obtains a model of NFU + \"Infinity\" + \"Choice\".\n\nFor philosophical reasons, it is important to note that it is not necessary to work in ZFC or any related system to carry out this proof. A common argument against the use of NFU as a foundation for mathematics is that the reasons for relying on it have to do with the intuition that ZFC is correct. It is sufficient to accept TST (in fact TSTU). In outline: take the type theory TSTU (allowing urelements in each positive type) as a metatheory and consider the theory of set models of TSTU in TSTU (these models will be sequences of sets formula_102 (all of the same type in the metatheory) with embeddings of each formula_103 into formula_104 coding embeddings of the power set of formula_102 into formula_106 in a type-respecting manner). Given an embedding of formula_107 into formula_108 (identifying elements of the base \"type\" with subsets of the base type), embeddings may be defined from each \"type\" into its successor in a natural way. This can be generalized to transfinite sequences formula_109 with care.\n\nNote that the construction of such sequences of sets is limited by the size of the type in which they are being constructed; this prevents TSTU from proving its own consistency (TSTU + \"Infinity\" can prove the consistency of TSTU; to prove the consistency of TSTU+\"Infinity\" one needs a type containing a set of cardinality formula_110, which cannot be proved to exist in TSTU+\"Infinity\" without stronger assumptions). Now the same results of model theory can be used to build a model of NFU and verify that it is a model of NFU in much the same way, with the formula_109's being used in place of formula_77 in the usual construction. The final move is to observe that since NFU is consistent, we can drop the use of absolute types in our metatheory, bootstrapping the metatheory from TSTU to NFU.\n\nThe automorphism \"j\" of a model of this kind is closely related to certain natural operations in NFU. For example, if \"W\" is a well-ordering in the nonstandard model (we suppose here that we use Kuratowski pairs so that the coding of functions in the two theories will agree to some extent) which is also a well-ordering in NFU (all well-orderings of NFU are well-orderings in the nonstandard model of Zermelo set theory, but not vice versa, due to the formation of urelements in the construction of the model), and \"W\" has type α in NFU, then \"j\"(\"W\") will be a well-ordering of type \"T\"(α) in NFU.\n\nIn fact, \"j\" is coded by a function in the model of NFU. The function in the nonstandard model which sends the singleton of any element of formula_113 to its sole element, becomes in NFU a function which sends each singleton {\"x\"}, where \"x\" is any object in the universe, to \"j\"(\"x\"). Call this function \"Endo\" and let it have the following properties: \"Endo\" is an injection from the set of singletons into the set of sets, with the property that \"Endo\"( {\"x\"} ) = {\"Endo\"( {\"y\"} ) | \"y\"∈\"x\"} for each set \"x\". This function can define a type level \"membership\" relation on the universe, one reproducing the membership relation of the original nonstandard model.\n\nIn this section the effect is considered of adding various \"strong axioms of infinity\" to our usual base theory, NFU + \"Infinity\" + \"Choice\". This base theory, known consistent, has the same strength as TST + \"Infinity\", or Zermelo set theory with \"Separation\" restricted to bounded formulas (Mac Lane set theory).\n\nOne can add to this base theory strong axioms of infinity familiar from the ZFC context, such as \"there exists an inaccessible cardinal,\" but it is more natural to consider assertions about Cantorian and strongly Cantorian sets. Such assertions not only bring into being large cardinals of the usual sorts, but strengthen the theory on its own terms.\n\nThe weakest of the usual strong principles is:\n\n\nTo see how natural numbers are defined in NFU, see set-theoretic definition of natural numbers. The original form of this axiom given by Rosser was \"the set {\"m\"|1≤\"m\"≤\"n\"} has \"n\" members\", for each natural number \"n\". This intuitively obvious assertion is unstratified: what is provable in NFU is \"the set {\"m\"|1≤\"m\"≤\"n\"} has formula_114 members\" (where the \"T\" operation on cardinals is defined by formula_115; this raises the type of a cardinal by one). For any cardinal number (including natural numbers) to assert formula_116 is equivalent to asserting that the sets \"A\" of that cardinality are Cantorian (by a usual abuse of language, we refer to such cardinals as \"Cantorian cardinals\"). It is straightforward to show that the assertion that each natural number is Cantorian is equivalent to the assertion that the set of all natural numbers is strongly Cantorian.\n\n\"Counting\" is consistent with NFU, but increases its consistency strength noticeably; not, as one would expect, in the area of arithmetic, but in higher set theory. NFU + \"Infinity\" proves that each formula_117 exists, but not that formula_110 exists; NFU + \"Counting\" (easily) proves \"Infinity\", and further proves the existence of formula_119 for each n, but not the existence of formula_120. (See beth numbers).\n\n\"Counting\" implies immediately that one does not need to assign types to variables restricted to the set formula_121 of natural numbers for purposes of stratification; it is a theorem that the power set of a strongly Cantorian set is strongly Cantorian, so it is further not necessary to assign types to variables restricted to any iterated power set of the natural numbers, or to such familiar sets as the set of real numbers, the set of functions from reals to reals, and so forth. The set-theoretical strength of \"Counting\" is less important in practice than the convenience of not having to annotate variables known to have natural number values (or related kinds of values) with singleton brackets, or to apply the \"T\" operation in order to get stratified set definitions.\n\n\"Counting\" implies \"Infinity\"; each of the axioms below needs to be adjoined to NFU + \"Infinity\" to get the effect of strong variants of \"Infinity\"; Ali Enayat has investigated the strength of some of these axioms in models of NFU + \"the universe is finite\".\n\nA model of the kind constructed above satisfies \"Counting\" just in case the automorphism \"j\" fixes all natural numbers in the underlying nonstandard model of Zermelo set theory.\n\nThe next strong axiom we consider is the\n\n\nImmediate consequences include Mathematical Induction for unstratified conditions (which is not a consequence of \"Counting\"; many but not all unstratified instances of induction on the natural numbers follow from \"Counting\").\n\nThis axiom is surprisingly strong. Unpublished work of Robert Solovay shows that the consistency strength of the theory NFU* = NFU + \"Counting\" + \"Strongly Cantorian Separation\" is the same as that of Zermelo set theory + formula_123 \"Replacement\".\n\nThis axiom holds in a model of the kind constructed above (with \"Choice\") if the ordinals which are fixed by \"j\" and dominate only ordinals fixed by \"j\" in the underlying nonstandard model of Zermelo set theory are standard, and the power set of any such ordinal in the model is also standard. This condition is sufficient but not necessary.\n\nNext is\n\n\nThis very simple and appealing assertion is extremely strong. Solovay has shown the precise equivalence of the consistency strength of the theory NFUA = NFU + \"Infinity\" + \"Cantorian Sets\" with that of ZFC + a schema asserting the existence of an \"n\"-Mahlo cardinal for each concrete natural number \"n\". Ali Enayat has shown that the theory of Cantorian equivalence classes of well-founded extensional relations (which gives a natural picture of an initial segment of the cumulative hierarchy of ZFC) interprets the extension of ZFC with \"n\"-Mahlo cardinals directly. A permutation technique can be applied to a model of this theory to give a model in which the hereditarily strongly Cantorian sets with the usual membership relation model the strong extension of ZFC.\n\nThis axiom holds in a model of the kind constructed above (with \"Choice\") just in case the ordinals fixed by \"j\" in the underlying nonstandard model of ZFC are an initial (proper class) segment of the ordinals of the model.\n\nNext consider the\n\n\nThis combines the effect of the two preceding axioms and is actually even stronger (precisely how is not known). Unstratified mathematical induction enables proving that there are \"n\"-Mahlo cardinals for every \"n\", given \"Cantorian Sets\", which gives an extension of ZFC that is even stronger than the previous one, which only asserts that there are \"n\"-Mahlos for each concrete natural number (leaving open the possibility of nonstandard counterexamples).\n\nThis axiom will hold in a model of the kind described above if every ordinal fixed by \"j\" is standard, and every power set of an ordinal fixed by \"j\" is also standard in the underlying model of ZFC. Again, this condition is sufficient but not necessary.\n\nAn ordinal is said to be \"Cantorian\" if it is fixed by \"T\", and \"strongly Cantorian\" if it dominates only Cantorian ordinals (this implies that it is itself Cantorian). In models of the kind constructed above, Cantorian ordinals of NFU correspond to ordinals fixed by \"j\" (they are not the same objects because different definitions of ordinal numbers are used in the two theories).\n\nEqual in strength to \"Cantorian Sets\" is the\n\n\nRecall that formula_57 is the order type of the natural order on all ordinals. This only implies \"Cantorian Sets\" if we have \"Choice\" (but is at that level of consistency strength in any case). It is remarkable that one can even define formula_128: this is the \"n\"th term formula_129 of any finite sequence of ordinals \"s\" of length \"n\" such that formula_130, formula_131 for each appropriate \"i\". This definition is completely unstratified. The uniqueness of formula_128 can be proved (for those \"n\" for which it exists) and a certain amount of common-sense reasoning about this notion can be carried out, enough to show that \"Large Ordinals\" implies \"Cantorian Sets\" in the presence of \"Choice\". In spite of the knotty formal statement of this axiom, it is a very natural assumption, amounting to making the action of \"T\" on the ordinals as simple as possible.\n\nA model of the kind constructed above will satisfy \"Large Ordinals\", if the ordinals moved by \"j\" are exactly the ordinals which dominate some formula_133 in the underlying nonstandard model of ZFC.\n\n\nSolovay has shown the precise equivalence in consistency strength of NFUB = NFU + \"Infinity\" + \"Cantorian Sets\" + \"Small Ordinals\" with Morse–Kelley set theory plus the assertion that the proper class ordinal (the class of all ordinals) is a weakly compact cardinal. This is very strong indeed! Moreover, NFUB-, which is NFUB with \"Cantorian Sets\" omitted, is easily seen to have the same strength as NFUB.\n\nA model of the kind constructed above will satisfy this axiom if every collection of ordinals fixed by \"j\" is the intersection of some set of ordinals with the ordinals fixed by \"j\", in the underlying nonstandard model of ZFC.\n\nEven stronger is the theory NFUM = NFU + \"Infinity\" + \"Large Ordinals\" + \"Small Ordinals\". This is equivalent to Morse–Kelley set theory with a predicate on the classes which is a κ-complete nonprincipal ultrafilter on the proper class ordinal κ; in effect, this is Morse–Kelley set theory + \"the proper class ordinal is a measurable cardinal\"!\n\nThe technical details here are not the main point, which is that reasonable and natural (in the context of NFU) assertions turn out to be equivalent in power to very strong axioms of infinity in the ZFC context. This fact is related to the correlation between the existence of models of NFU, described above and satisfying these axioms, and the existence of models of ZFC with automorphisms having special properties.\n\n\n\n"}
{"id": "35076621", "url": "https://en.wikipedia.org/wiki?curid=35076621", "title": "Novikov–Shubin invariant", "text": "Novikov–Shubin invariant\n\nIn mathematics, a Novikov–Shubin invariant. introduced by , is an invariant of a compact Riemannian manifold related to the spectrum of the Laplace operator acting on square-integrable differential forms on its universal cover. It gives a measure of the density of eigenvalues around zero. It can be computed from a triangulation of the manifold, and it is an homotopy invariant. In particular it does not depend on the chosen Riemannian metric on the manifold. \n"}
{"id": "10416232", "url": "https://en.wikipedia.org/wiki?curid=10416232", "title": "Phase space method", "text": "Phase space method\n\nIn applied mathematics, the phase space method is a technique for constructing and analyzing solutions of dynamical systems, that is, solving time-dependent differential equations. \n\nThe method consists of first rewriting the equations as a system of differential equations that are first-order in time, by introducing additional variables. The original and the new variables form a vector in the phase space. The solution then becomes a curve in the phase space, parametrized by time. The curve is usually called a trajectory or an orbit. The differential equation is reformulated as a geometrical description of the curve, that is, as a differential equation in terms of the phase space variables only, without the original time parametrization. Finally, a solution in the phase space is transformed back into the original setting.\n\nThe phase space method is used widely in physics. It can be applied, for example, to find traveling wave solutions of reaction-diffusion systems.\n\n"}
{"id": "32052273", "url": "https://en.wikipedia.org/wiki?curid=32052273", "title": "Plethysm", "text": "Plethysm\n\nIn algebra, plethysm is an operation on symmetric functions introduced by Dudley E. Littlewood, who denoted it by {\"λ\"} ⊗ {\"μ\"}. The word \"plethysm\" for this operation (after the Greek word πληθυσμός meaning \"multiplication\") was introduced later by , who said that the name was suggested by M. L. Clark.\n\nIf symmetric functions are identified with operations in lambda rings, then plethysm corresponds to composition of operations.\n\nLet \"V\" be a vector space over complex numbers, considered as a representation of GL(\"V\"). Each Young diagram λ corresponds to a Schur functor L(-) on the category of GL(\"V\")-representations. Given two Young diagrams λ and μ, consider the decomposition of L(L(\"V\")) into a direct sum of irreducible representations of the group. By the representation theory of the general linear group we know that each summand is isomorphic to L(\"V\") for a Young diagram ν. So for some nonnegative multiplicities \"a\" there is an isomorphism\nThe problem of (outer) plethysm is to find an expression for the multiplicities \"a\".\n\nThis formulation is closely related to the classical question. The character of the GL(\"V\")-representation L(\"V\") is a symmetric function in dim(\"V\") variables, known as the Schur polynomial \"s\" corresponding to the Young diagram λ. Schur polynomials form a basis in the space of symmetric functions. Hence to understand the plethysm of two symmetric functions it would be enough to know their expressions in that basis and an expression for a plethysm of two arbitrary Schur polynomials {\"s\"}⊗{\"s\"} . The second piece of data is precisely the character of L(L(\"V\")).\n\n"}
{"id": "33006898", "url": "https://en.wikipedia.org/wiki?curid=33006898", "title": "Polyknight", "text": "Polyknight\n\nA polyknight is a plane geometric figure formed by selecting cells in a square lattice that could represent the path of a chess knight in which doubling back is allowed. It is a polyform with square cells which are not necessarily connected, comparable to the polyking. Alternatively, it can be interpreted as a connected subset of the vertices of a knight's graph, a graph formed by connecting pairs of lattice squares that are a knight's move apart.\n\nThree common ways of distinguishing polyominoes for enumeration can also be extended to polyknights:\n\nThe following table shows the numbers of polyknights of various types with \"n\" cells.\n"}
{"id": "2598499", "url": "https://en.wikipedia.org/wiki?curid=2598499", "title": "Psychologism", "text": "Psychologism\n\nPsychologism is a philosophical position, according to which psychology plays a central role in grounding or explaining some other, non-psychological type of fact or law.\n\nThe \"Oxford English Dictionary\" defines \"psychologism\" as: \"The view or doctrine that a theory of psychology or ideas forms the basis of an account of metaphysics, epistemology, or meaning; (sometimes) spec. the explanation or derivation of mathematical or logical laws in terms of psychological facts.\" Psychologism in epistemology, the idea that its problems \"can be solved satisfactorily by the psychological study of the development of mental processes\", was argued in John Locke's \"An Essay Concerning Human Understanding\" (1690).\n\nOther forms of psychologism are logical psychologism and mathematical psychologism. Logical psychologism is a position in logic (or the philosophy of logic) according to which logical laws and mathematical laws are grounded in, derived from, explained or exhausted by psychological facts or laws. Psychologism in the philosophy of mathematics is the position that mathematical concepts and/or truths are grounded in, derived from or explained by psychological facts or laws.\n\nThe word was coined by Johann Eduard Erdmann as \"Psychologismus\", being translated into English as \"psychologism\".\n\nJohn Stuart Mill was accused by Edmund Husserl of being an advocate of a type of logical psychologism, although this may not have been the case. So were many nineteenth-century German philosophers such as Christoph von Sigwart, Benno Erdmann, Theodor Lipps, Gerardus Heymans, Wilhelm Jerusalem, and , as well as a number of psychologists, past and present (e.g., Wilhelm Wundt and Gustave Le Bon).\n\nPsychologism was notably criticized by Gottlob Frege in his anti-psychologistic work \"The Foundations of Arithmetic\", and many of his works and essays, including his review of Husserl's \"Philosophy of Arithmetic\". Husserl, in the first volume of his \"Logical Investigations\", called \"The Prolegomena of Pure Logic\", criticized psychologism thoroughly and sought to distance himself from it. Frege's arguments were largely ignored, while Husserl's were widely discussed.\n\nIn \"Psychologism and Behaviorism\", Ned Block takes psychologism as the position that \"whether behavior is intelligent behavior depends on the character of the internal information processing that produces it.\" This is in contrast to a behavioral view which would state that intelligence can be ascribed to a being solely via observing its behavior. This latter type of behavioral view is strongly associated with the Turing test.\n\n\n"}
{"id": "1485502", "url": "https://en.wikipedia.org/wiki?curid=1485502", "title": "Ralph Henstock", "text": "Ralph Henstock\n\nRalph Henstock (2 June 1923 – 17 January 2007) was an English mathematician and author. As an Integration theorist, he is notable for Henstock–Kurzweil integral. Henstock brought the theory to a highly developed stage without ever having encountered Jaroslav Kurzweil's 1957 paper on the subject.\n\nHe was born in the coal-mining village of Newstead, Nottinghamshire, the only child of mineworker and former coalminer William Henstock and Mary Ellen Henstock (née Bancroft). On the Henstock side he was descended from 17th century Flemish immigrants called Hemstok.\n\nBecause of his early academic promise it was expected that Henstock would attend the University of Nottingham where his father and uncle had received technical education, but as it turned out he won scholarships which enabled him to study mathematics at St John's College, Cambridge from October 1941 until November 1943, when he was sent for war service to the Ministry of Supply’s department of Statistical Method and Quality Control in London. \n\nThis work did not satisfy him, so he enrolled at Birkbeck College, London where he joined the weekly seminar of Professor Paul Dienes which was then a focus for mathematical activity in London. Henstock wanted to study divergent series but Dienes prevailed upon him to get involved in the theory of integration, thereby setting him on course for his life’s work.\n\nA devoted Methodist, the lasting impression he made was one of gentle sincerity and amiability. Henstock married Marjorie Jardine in 1949. Their son John was born 10 July 1952. Ralph Henstock died on 17 January 2007 after a short illness.\n\nHe was awarded the Cambridge B.A. in 1944 and began research for the PhD in Birkbeck College, London, under the supervision of Paul Dienes. His PhD thesis, entitled \"Interval Functions and their Integrals\", was submitted in December 1948. His Ph.D. examiners were Burkill and H. Kestelman. In 1947 he returned briefly to Cambridge to complete the undergraduate mathematical studies which had been truncated by his Ministry of Supply work.\n\nMost of Henstock's work was concerned with integration. From initial studies of the Burkill and Ward integrals he formulated an integration process whereby the domain of integration is suitably partitioned for Riemann sums to approximate the integral of a function. His methods led to an integral on the real line that was very similar in construction and simplicity to the Riemann integral but which included the Lebesgue integral and, in addition, allowed non-absolute convergence.\n\nThese ideas were developed from the late 1950s. Independently, Jaroslav Kurzweil developed a similar Riemann-type integral on the real line. The resulting integral is now known as the Henstock-Kurzweil integral. On the real line it is equivalent to the Denjoy-Perron integral, but has a simpler definition.\n\nIn the following decades, Henstock developed extensively the distinctive features of his theory, inventing the concepts of division spaces or integration bases to demonstrate in general settings the properties and characteristics of mathematical integration. His theory provides a unified approach to non-absolute integral, as different kinds of Henstock integral, choosing an appropriate integration basis (division space, in Henstock's own terminology). It has been used in differential and integral equations, harmonic analysis, probability theory and Feynman integration. Numerous monographs and texts have appeared since 1980 and there have been several conferences devoted to the theory. It has been taught in standard courses in mathematical analysis.\n\nHenstock was author of 46 journal papers in the period 1946 to 2006. He published four books on analysis (Theory of Integration, 1963; Linear Analysis, 1967; Lectures on the Theory of Integration, 1988; and The General Theory of Integration, 1991). He wrote 171 reviews for MathSciNet. In 1994 he was awarded the Andy Prize of the XVIII Summer Symposium in Real Analysis. His academic career began as Assistant Lecturer, Bedford College for Women, 1947–48; then Assistant Lecturer at Birkbeck, 1948–51; Lecturer, Queen's University Belfast, 1951–56; Lecturer, Bristol University, 1956–60; Senior Lecturer and Reader, Queen’s University Belfast, 1960–64; Reader, Lancaster University, 1964–70; Chair of Pure Mathematics, New University of Ulster, 1970–88; and Leverhulme Fellow 1988-91.\n\nMuch of Henstock's earliest work was published by the Journal of the London Mathematical Society. These were \"On interval functions and their integrals\" I (21, 1946) and II (23, 1948); \"The efficiency of matrices for Taylor series\" (22, 1947); \"The efficiency of matrices for bounded sequences\" (25, 1950); \"The efficiency of convergence factors for functions of a continuous real variable\" (30, 1955); \"A new description of the Ward integral\" (35 1960); and \"The integrability of functions of interval functions\" (39 1964). \n\nHis works, published in \"Proceedings of the London Mathematical Society\", were \"Density integration\" (53, 1951); \"On the measure of sum sets (I) The theorems of Brunn, Minkowski, and Lusternik, (with A.M. McBeath)\" ([3] 3, 1953); \"Linear functions with domain a real countably infinite dimensional space\" ([3] 5, 1955); \"Linear and bilinear functions with domain contained in a real countably infinite dimensional space\" ([3] 6, 1956); \"The use of convergence factors in Ward integration\" ([3] 10, 1960); \"The equivalence of generalized forms of the Ward, variational, Denjoy-Stieltjes, and Perron-Stieltjes integrals\" ([3] 10, 1960); \"N-variation and N-variational integrals of set functions\" ([3] 11, 1961); \"Definitions of Riemann type of the variational integrals\" ([3] 11, 1961); \"Difference-sets and the Banach–Steinhaus theorem\" ([3] 13, 1963); \"Generalized integrals of vector-valued functions ([3] 19 1969)\n\nHis additional papers were\n\n\nThe journal Scientiae Mathematicae Japonicae published a special commemorative issue in his honor, January 2008. The above article is copied, with permission, from Real Analysis Exchange and from Scientiae Mathematicae Japonicae. The latter contains the following review of Henstock's work:\n\n1. Ralph Henstock, an obituary, by P. Bullen.\n\n2. Ralph Henstock: research summary, by E. Talvila.\n\n3. The integral à la Henstock, by Peng Yee Lee. \n\n4. The natural integral on the real line, by B. Thomson.\n\n5. Ralph Henstock's influence on integration theory, by W.F. Pfeffer.\n\n6. Henstock on random variation, by P. Muldowney.\n\n7. Henstock integral in harmonic analysis, by V.A. Skvortsov.\n\n8. Convergences on the Henstock-Kurzweil integral, by S. Nakanishi.\n\n\n"}
{"id": "15081922", "url": "https://en.wikipedia.org/wiki?curid=15081922", "title": "Rice–Shapiro theorem", "text": "Rice–Shapiro theorem\n\nIn computability theory, the Rice–Shapiro theorem is a generalization of Rice's theorem, and is named after Henry Gordon Rice and Norman Shapiro.\n\nLet \"A\" be a set of partial-recursive unary functions on the domain of natural numbers such that the set formula_1 is recursively enumerable, where formula_2 denotes the formula_3-th partial-recursive function in a Gödel numbering.\n\nThen for any unary partial-recursive function \"formula_4\", we have:\n\nIn the given statement, a finite function is a function with a finite domain formula_8 and formula_6 means that for every formula_10 it holds that formula_11 is defined and equal to formula_12.\n\nFor any finite unary function formula_13 on integers,\nlet formula_14 denote the 'frustum'\nof all partial-recursive functions that are defined, and agree with formula_13,\non formula_13's domain.\n\nEquip the set of all partial-recursive functions with the topology generated by these\nfrusta as base. Note that for every frustum formula_17, formula_18 is \nrecursively enumerable. More generally it holds for every set formula_19\nof partial-recursive functions:\n\nformula_20 is recursively enumerable iff \nformula_19 is a recursively enumerable union of frusta.\n\n"}
{"id": "14865628", "url": "https://en.wikipedia.org/wiki?curid=14865628", "title": "Rosetta-lang", "text": "Rosetta-lang\n\nThe Rosetta system-level specification language is a design language for complex, heterogeneous systems. Specific language design objectives include:\nRosetta was undergoing standardization at various times.\n\nThe Rosetta effort emerged from a meeting in of the Semiconductor Industry Council's System-Level Design Language committee in 1996. The objective of the meeting was to define requirements for a next-generation design language that would address perceived shortcomings in existing languages such as VHDL and Verilog. Specific concerns included inability to represent constraints, lack of a formal semantics, inability to represent heterogeneous systems, and heavy reliance on computer simulation for analysis. In response to these requirements, three major approaches were pursued:\n\nThe first approach ultimately resulted in SystemVerilog and extensions to VHDL while the second resulted in SystemC, all of which became Institute of Electrical and Electronics Engineers (IEEE) standards for the semiconductor industry.\nRosetta's original application domain was system on a chip semiconductor systems.\n\nRosetta resulted from the third approach with development beginning under the auspices of the Semiconductor Industry Council and the Air Force Research Laboratory. Originally developed by Perry Alexander and others at the University of Kansas, it was known simply as System-Level Design Language.\nStandardization was transferred to VHDL International by 2000 and renamed Rosetta (after the Rosetta Stone) to reflect the heterogeneous nature of its specifications. Eventually, VHDL International and the Open Verilog Initiative merged to form Accellera, an industry sponsored consortium for electronic design automation (EDA) standards. \nA draft standard of Rosetta was published in November 2003 through Accellera.\nStandardization was transferred to IEEE Design Automation Standards Committee (DASC) where it was developed by the Rosetta Working Group under IEEE project P1699 starting in March 2007.\nA draft of a language reference manual was published in April 2008, with editor Peter Ashenden of Australia.\nThe project was withdrawn in June 2013.\n\nRosetta is structured around three sub-languages that support defining various specification elements. The \"expression language\" defines basic functions and expressions used as terms and values in specifications. The expression language is a non-strict, purely functional, typed language in the spirit of Haskell. Functions are referentially transparent, encapsulated expressions and cannot have side effects. All Rosetta expressions have types that are determined statically. The type system is based on lifted sets where each type contains at least the bottom or undefined value. Any set can be used to define a type and any function that returns a set can be used to define a type former. Because type definitions can contain general Rosetta expressions, the type system is dependent.\n\nThe \"facet language\" defines individual specifications and specification composition to define systems. \"Facets\" and \"components\" define system models from one engineering perspective. Each facet is written by extending a \"domain\" that provides vocabulary and semantics for the model. Facets are defined by declaring items and defining properties for those items. As such, Rosetta is a declarative language and constructs cannot be guaranteed to evaluate to a value, although some facets can be evaluated. Using the design abstractions provided by its domain, a facet describes requirements, behavior, constraints, or function of a system. Facets are heterogeneous and may be defined using any of the predefined or user defined domains. The Rosetta semantics denotes each facet to a coalgebra that defines its model-of-computation. Because Rosetta is reflective, facets can be composed and transformed to define complex systems. A common specification technique is to write facets for different aspects of a system and then compose those aspects using product and sum operations to define the complete system.\n\nThe \"domain language\" defines specification domains otherwise known as specification types. The collection of domains forms a complete lattice ordered by homomorphism with the empty or \"null\" domain as its top element and the inconsistent or \"bottom\" domain as its bottom. There are three primary domain types. Units-of-semantics domains define basic units of specification. For example, the state_based domain defines the concept of state and next state without constraining the values or properties. Model-of-computation domains extend unit-of-semantics domains to define general computational models such as finite_state, continuous_time, discrete_time and frequency. Engineering domains extend model-of-computation domains to provide specification capabilities for specific engineering domains.\n\nSince its early days, Rosetta expanded to include design domains such as hydraulic and mechanical systems, networking systems, security and trust, and software defined radios.\n\nRosetta was influenced heavily by the non-strict, purely functional language Haskell, the Larch family of specification languages, and Prototype Verification System (PVS). A book devoted to the language was published in November 2006.\n\n"}
{"id": "324749", "url": "https://en.wikipedia.org/wiki?curid=324749", "title": "Sine wave", "text": "Sine wave\n\nA sine wave or sinusoid is a mathematical curve that describes a smooth periodic oscillation. A sine wave is a continuous wave. It is named after the function sine, of which it is the graph. It occurs often in pure and applied mathematics, as well as physics, engineering, signal processing and many other fields. Its most basic form as a function of time (\"t\") is:\n\nwhere:\n\nThe sine wave is important in physics because it retains its wave shape when added to another sine wave of the same frequency and arbitrary phase and magnitude. It is the only periodic waveform that has this property. This property leads to its importance in Fourier analysis and makes it acoustically unique.\n\nIn general, the function may also have:\n\n\nwhich is\n\nThe wavenumber is related to the angular frequency by:.\n\nwhere λ (Lambda) is the wavelength, \"f\" is the frequency, and \"v\" is the linear speed.\n\nThis equation gives a sine wave for a single dimension; thus the generalized equation given above gives the displacement of the wave at a position \"x\" at time \"t\" along a single line.\nThis could, for example, be considered the value of a wave along a wire.\n\nIn two or three spatial dimensions, the same equation describes a travelling plane wave if position \"x\" and wavenumber \"k\" are interpreted as vectors, and their product as a dot product. \nFor more complex waves such as the height of a water wave in a pond after a stone has been dropped in, more complex equations are needed.\n\nThis wave pattern occurs often in nature, including wind waves, sound waves, and light waves.\n\nA cosine wave is said to be \"sinusoidal\", because formula_8\nwhich is also a sine wave with a phase-shift of π/2 radians. Because of this \"head start\", it is often said that the cosine function \"leads\" the sine function or the sine \"lags\" the cosine.\n\nThe human ear can recognize single sine waves as sounding clear because sine waves are representations of a single frequency with no harmonics.\n\nTo the human ear, a sound that is made of more than one sine wave will have perceptible harmonics; addition of different sine waves results in a different waveform and thus changes the timbre of the sound. Presence of higher harmonics in addition to the fundamental causes variation in the timbre, which is the reason why the same musical note (the same frequency) played on different instruments sounds different. On the other hand, if the sound contains aperiodic waves along with sine waves (which are periodic), then the sound will be perceived \"noisy\" as noise is characterized as being aperiodic or having a non-repetitive pattern.\n\nIn 1822, French mathematician Joseph Fourier discovered that sinusoidal waves can be used as simple building blocks to describe and approximate any periodic waveform, including square waves. Fourier used it as an analytical tool in the study of waves and heat flow. It is frequently used in signal processing and the statistical analysis of time series.\n\nSince sine waves propagate without changing form in distributed linear systems, they are often used to analyze wave propagation. Sine waves traveling in two directions in space can be represented as\n\nWhen two waves having the same amplitude and frequency, and traveling in opposite directions, superpose each other, then a standing wave pattern is created. Note that, on a plucked string, the interfering waves are the waves reflected from the fixed end points of the string. Therefore, standing waves occur only at certain frequencies, which are referred to as resonant frequencies and are composed of a fundamental frequency and its higher harmonics. The resonant frequencies of a string are determined by the length between the fixed ends and the tension of the string.\n"}
{"id": "10415943", "url": "https://en.wikipedia.org/wiki?curid=10415943", "title": "Spherical mean", "text": "Spherical mean\n\nIn mathematics, the spherical mean of a function around a point is the average of all values of that function on a sphere of given radius centered at that point.\n\nConsider an open set \"U\" in the Euclidean space R and a continuous function \"u\" defined on \"U\" with real or complex values. Let \"x\" be a point in \"U\" and \"r\" > 0 be such that the closed ball \"B\"(\"x\", \"r\") of center \"x\" and radius \"r\" is contained in \"U\". The spherical mean over the sphere of radius \"r\" centered at \"x\" is defined as\n\nwhere ∂\"B\"(\"x\", \"r\") is the (\"n\"−1)-sphere forming the boundary of \"B\"(\"x\", \"r\"), d\"S\" denotes integration with respect to spherical measure and \"ω\"(\"r\") is the \"surface area\" of this (\"n\"−1)-sphere.\n\nEquivalently, the spherical mean is given by\n\nwhere \"ω\" is the area of the (\"n\"−1)-sphere of radius 1.\n\nThe spherical mean is often denoted as\n\nThe spherical mean is also defined for Riemannian manifolds in a natural manner.\n\n\n\n"}
{"id": "44076398", "url": "https://en.wikipedia.org/wiki?curid=44076398", "title": "Symplectic category", "text": "Symplectic category\n\nIn mathematics, Weinstein's symplectic category is (roughly) a category whose objects are symplectic manifolds and whose morphisms are canonical relations, inclusions of Lagrangian submanifolds \"L\" into formula_1, where the superscript minus means minus the given symplectic form (for example, the graph of a symplectomorphism; hence, minus). The notion was introduced by Alan Weinstein, according to whom \"Quantization problems suggest that the category of symplectic manifolds and symplectomorphisms be augmented by the inclusion of canonical relations as morphisms.\" The composition of canonical relations is given by a fiber product.\n\nStrictly speaking, the symplectic category is not a well-defined category (since the composition may not be well-defined) without some transversality conditions.\n\n\n"}
{"id": "8624486", "url": "https://en.wikipedia.org/wiki?curid=8624486", "title": "T-Square (software)", "text": "T-Square (software)\n\nT-Square is an early drafting program written by Peter Samson assisted by Alan Kotok and possibly Robert A. Saunders while they were students at the Massachusetts Institute of Technology and members of the Tech Model Railroad Club.\n\nT-Square was written for the PDP-1 computer and its Type 30 precision CRT that Digital Equipment Corporation donated to MIT in 1961. It is unlikely that many people have had the opportunity to use T-Square although Samson has said the group drew some schematics.\n\nStudents of Jack Dennis and John McCarthy discovered a stunning array of uses for the very expensive room-sized computers that were given to MIT. They were privileged to be enrolled when the school's first programming courses were taught.\n\nThey negotiated with their advisors and the operations manager John McKenzie for time and became single-users long before personal computers were available. About 1959 or 1960, some of this group of students became support staff and wrote software for about $1.75 USD per hour. They wrote the programming software which is used to build application software. Later Samson and Kotok became architects of DEC computers.\n\nDuring this period Samson created other \"firsts\" in application software for music, games and page layout so it is perhaps not surprising he wrote what may be the first drafting program. Based on this experience, later in life Samson worked on an electronic drafting program with 80,000 lines of code. He received a patent in virtual reality at Autodesk, a vendor of CAD and CAM software.\n\nTo move the cursor, T-Square used a Spacewar! game controller built by Kotok and Saunders in 1962. It is not known if Saunders was involved in repurposing it for T-Square. Kotok, who was about 20 years old, did participate. He was known for doing what needed to be done and for taking an interest in \"all things ingenious or intriguing.\"\n\nIvan Sutherland used a light pen in his programs as did Jack Gilmore and others before him. The pens allow fine detail but drawing on a vertical surface like a CRT tires the hand quickly. There is no evidence they studied ergonomics but T-Square used an input device more like a mouse in that it rested on a horizontal surface.\n\nThe Spacewar! control boxes were cobbled together with wood, Bakelite and toggle switches. Although they are often considered to be the first joysticks, Kotok did not accept credit for coinventing them with Saunders. He thought there were similar controllers in use at the time in games such as Tennis for Two and at NASA or another organization.\n\nT-Square is a small part of the reason people use today's computers for drafting, architecture, drawing and illustration and engineering. Prior to this revolution and in some places to this day, draftsmen and women used triangles, wood or metal T-squares, pencils and technical pens on film and paper. The beginning of this change can be seen in a video of Sutherland demonstrating Sketchpad.\n\nIn his 1963 MIT Ph.D. thesis, Sutherland explains he completed an early version that could draw parallel and perpendicular lines in November 1961. He goes on to say, \"Somewhat before my first effort was working, Welden Clark of Bolt, Beranek and Newman...\" showed him a \"similar program\" running on a PDP-1. T-Square and Sketchpad were developed in the same location a year or two apart but their influence on each other is unknown.\n\n"}
{"id": "13181245", "url": "https://en.wikipedia.org/wiki?curid=13181245", "title": "The Fifty-Nine Icosahedra", "text": "The Fifty-Nine Icosahedra\n\nThe Fifty-Nine Icosahedra is a book written and illustrated by H. S. M. Coxeter, P. Du Val, H. T. Flather and J. F. Petrie. It enumerates certain stellations of the regular convex or Platonic icosahedron, according to a set of rules put forward by J. C. P. Miller.\n\nFirst published by the University of Toronto in 1938, a Second Edition reprint by Springer-Verlag followed in 1982. Tarquin's 1999 Third Edition included new reference material and photographs by K. and D. Crennell.\n\nAlthough Miller did not contribute to the book directly, he was a close colleague of Coxeter and Petrie. His contribution is immortalised in his set of rules for defining which stellation forms should be considered \"properly significant and distinct\":\n\nRules (i) to (iii) are symmetry requirements for the face planes. Rule (iv) excludes buried holes, to ensure that no two stellations look outwardly identical. Rule (v) prevents any disconnected compound of simpler stellations.\n\nCoxeter was the main driving force behind the work. He carried out the original analysis based on Miller's rules, adopting a number of techniques such as combinatorics and abstract graph theory whose use in a geometrical context was then novel.\n\nHe observed that the stellation diagram comprised many line segments. He then developed procedures for manipulating combinations of the adjacent plane regions, to formally enumerate the combinations allowed under Miller's rules.\nHis graph, reproduced here, shows the connectivity of the various faces identified in the stellation diagram (see below). The Greek symbols represent sets of possible alternatives:\n\nDu Val devised a symbolic notation for identifying sets of congruent cells, based on the observation that they lie in \"shells\" around the original icosahedron. Based on this he tested all possible combinations against Miller's rules, confirming the result of Coxeter's more analytical approach.\n\nFlather's contribution was indirect: he made card models of all 59. When he first met Coxeter he had already made many stellations, including some \"non-Miller\" examples. He went on to complete the series of fifty-nine, which are preserved in the mathematics library of Cambridge University, England. The library also holds some non-Miller models, but it is not known whether these were made by Flather or by Miller's later students.\n\nJohn Flinders Petrie was a lifelong friend of Coxeter and had a remarkable ability to visualise four-dimensional geometry. He and Coxeter had worked together on many mathematical problems. His direct contribution to the fifty nine icosahedra was the exquisite set of three-dimensional drawings which provide much of the fascination of the published work.\n\nFor the Third Edition, Kate and David Crennell reset the text and redrew the diagrams. They also added a reference section containing tables, diagrams, and photographs of some of the Cambridge models (which at that time were all thought to be Flather's). Corrections to this edition have been published online.\n\nBefore Coxeter, only Brückner and Wheeler had recorded any significant sets of stellations, although a few such as the great icosahedron had been known for longer. Since publication of \"The 59\", Wenninger published instructions on making models of some; the numbering scheme used in his book has become widely referenced, although he only recorded a few stellations.\n\nIndex numbers are the Crennells' unless otherwise stated:\n\nCrennell\n\nCells\n\nFaces\n\nWenninger\n\nWheeler\n\nBrückner\n\nRemarks\n\n\n\n"}
{"id": "31998221", "url": "https://en.wikipedia.org/wiki?curid=31998221", "title": "Wahba's problem", "text": "Wahba's problem\n\nIn applied mathematics, Wahba's problem, first posed by Grace Wahba in 1965, seeks to find a rotation matrix (special orthogonal matrix) between two coordinate systems from a set of (weighted) vector observations. Solutions to Wahba's problem are often used in satellite attitude determination utilising sensors such as magnetometers and multi-antenna GPS receivers. The cost function that Wahba's problem seeks to minimise is as follows:\n\nwhere formula_3 is the \"k\"-th 3-vector measurement in the reference frame, formula_4 is the corresponding \"k\"-th 3-vector measurement in the body frame and formula_5 is a 3 by 3 rotation matrix between the coordinate frames.\nformula_6 is an optional set of weights for each observation.\n\nA number of solutions to the problem have appeared in literature, notably Davenport's q-method, QUEST and singular value decomposition-based methods. This is an alternative formulation of the Orthogonal Procrustes problem (consider all the vectors multiplied by the square-roots of the corresponding weights as columns of two matrices with \"N\" columns to obtain the alternative formulation).\n\nSeveral methods for solving Wahba's problem are discussed by Markley and Mortari.\n\nOne solution can be found using a singular value decomposition as reported by Markley\n\n1. Obtain a matrix formula_7 as follows:\n\nformula_8\n\n2. Find the singular value decomposition of formula_7\n\nformula_10\n\n3. The rotation matrix is simply:\n\nformula_11\n\nwhere formula_12\n\n\n"}
{"id": "4414192", "url": "https://en.wikipedia.org/wiki?curid=4414192", "title": "William Shanks", "text": "William Shanks\n\nWilliam Shanks (25 January 1812 – June 1882) was a British amateur mathematician.\n\nShanks is famous for his calculation of \"π\" to 707 places, accomplished in 1873, which, however, was only correct up to the first 527 places. This error was highlighted in 1944 by D. F. Ferguson (using a mechanical desk calculator).\n\nShanks earned his living by owning a boarding school at Houghton-le-Spring, which left him enough time to spend on his hobby of calculating mathematical constants. His routine was as follows: he would calculate new digits all morning; and then he would spend all afternoon checking his morning's work. To calculate \"π\", Shanks used Machin's formula:\n\nShanks's approximation was the longest expansion of \"π\" until the advent of the electronic digital computer about one century later.\n\nShanks also calculated \"e\" and the Euler–Mascheroni constant γ to many decimal places. He published a table of primes up to 60 000 and found the natural logarithms of 2, 3, 5 and 10 to 137 places.\n\nShanks died in Houghton-le-Spring, County Durham, England in June 1882, aged 70, and was buried at the local Hillside Cemetery on 17 June 1882.\n\n\n"}
