{"id": "30829236", "url": "https://en.wikipedia.org/wiki?curid=30829236", "title": "Arif Salimov", "text": "Arif Salimov\n\nArif Salimov (A.A. Salimov, born 1956, ) is an Azerbaijani/Soviet mathematician, known for his research in differential geometry. He obtained a B.Sc. degree from Baku State University, Azerbaijan in 1978, a PhD and Doctor of Sciences (Habilitation) degrees in geometry from Kazan State University, Russia in 1984 and 1998, respectively. His advisor was Vladimir Vishnevskii. Salimov is currently a full professor in the department of mathematics at Ataturk University, Turkey. He is an author and co-author of more than 100 articles. His primary areas of research are:\n\n"}
{"id": "27046146", "url": "https://en.wikipedia.org/wiki?curid=27046146", "title": "Arithmetic logic unit", "text": "Arithmetic logic unit\n\nAn arithmetic logic unit (ALU) is a combinational digital electronic circuit that performs arithmetic and bitwise operations on integer binary numbers. This is in contrast to a floating-point unit (FPU), which operates on floating point numbers. An ALU is a fundamental building block of many types of computing circuits, including the central processing unit (CPU) of computers, FPUs, and graphics processing units (GPUs). A single CPU, FPU or GPU may contain multiple ALUs.\n\nThe inputs to an ALU are the data to be operated on, called operands, and a code indicating the operation to be performed; the ALU's output is the result of the performed operation. In many designs, the ALU also has status inputs or outputs, or both, which convey information about a previous operation or the current operation, respectively, between the ALU and external status registers.\n\nAn ALU has a variety of input and output nets, which are the electrical conductors used to convey digital signals between the ALU and external circuitry. When an ALU is operating, external circuits apply signals to the ALU inputs and, in response, the ALU produces and conveys signals to external circuitry via its outputs.\n\nA basic ALU has three parallel data buses consisting of two input operands (\"A\" and \"B\") and a result output (\"Y\"). Each data bus is a group of signals that conveys one binary integer number. Typically, the A, B and Y bus widths (the number of signals comprising each bus) are identical and match the native word size of the external circuitry (e.g., the encapsulating CPU or other processor).\n\nThe \"opcode\" input is a parallel bus that conveys to the ALU an operation selection code, which is an enumerated value that specifies the desired arithmetic or logic operation to be performed by the ALU. The opcode size (its bus width) determines the maximum number of different operations the ALU can perform; for example, a four-bit opcode can specify up to sixteen different ALU operations. Generally, an ALU opcode is not the same as a machine language opcode, though in some cases it may be directly encoded as a bit field within a machine language opcode.\n\nThe status outputs are various individual signals that convey supplemental information about the result of the current ALU operation. General-purpose ALUs commonly have status signals such as:\n\nAt the end of each ALU operation, the status output signals are usually stored in external registers to make them available for future ALU operations (e.g., to implement multiple-precision arithmetic) or for controlling conditional branching. The collection of bit registers that store the status outputs are often treated as a single, multi-bit register, which is referred to as the \"status register\" or \"condition code register\".\n\nThe status inputs allow additional information to be made available to the ALU when performing an operation. Typically, this is a single \"carry-in\" bit that is the stored carry-out from a previous ALU operation.\n\nAn ALU is a combinational logic circuit, meaning that its outputs will change asynchronously in response to input changes. In normal operation, stable signals are applied to all of the ALU inputs and, when enough time (known as the \"propagation delay\") has passed for the signals to propagate through the ALU circuitry, the result of the ALU operation appears at the ALU outputs. The external circuitry connected to the ALU is responsible for ensuring the stability of ALU input signals throughout the operation, and for allowing sufficient time for the signals to propagate through the ALU before sampling the ALU result.\n\nIn general, external circuitry controls an ALU by applying signals to its inputs. Typically, the external circuitry employs sequential logic to control the ALU operation, which is paced by a clock signal of a sufficiently low frequency to ensure enough time for the ALU outputs to settle under worst-case conditions.\n\nFor example, a CPU begins an ALU addition operation by routing operands from their sources (which are usually registers) to the ALU's operand inputs, while the control unit simultaneously applies a value to the ALU's opcode input, configuring it to perform addition. At the same time, the CPU also routes the ALU result output to a destination register that will receive the sum. The ALU's input signals, which are held stable until the next clock, are allowed to propagate through the ALU and to the destination register while the CPU waits for the next clock. When the next clock arrives, the destination register stores the ALU result and, since the ALU operation has completed, the ALU inputs may be set up for the next ALU operation.\n\nA number of basic arithmetic and bitwise logic functions are commonly supported by ALUs. Basic, general purpose ALUs typically include these operations in their repertoires:\n\n\n\nALU shift operations cause operand A (or B) to shift left or right (depending on the opcode) and the shifted operand appears at Y. Simple ALUs typically can shift the operand by only one bit position, whereas more complex ALUs employ barrel shifters that allow them to shift the operand by an arbitrary number of bits in one operation. In all single-bit shift operations, the bit shifted out of the operand appears on carry-out; the value of the bit shifted into the operand depends on the type of shift.\n\nIn integer arithmetic computations, multiple-precision arithmetic is an algorithm that operates on integers which are larger than the ALU word size. To do this, the algorithm treats each operand as an ordered collection of ALU-size fragments, arranged from most-significant (MS) to least-significant (LS) or vice versa. For example, in the case of an 8-bit ALU, the 24-bit integer codice_1 would be treated as a collection of three 8-bit fragments: codice_2 (MS), codice_3, and codice_4 (LS). Since the size of a fragment exactly matches the ALU word size, the ALU can directly operate on this \"piece\" of operand.\n\nThe algorithm uses the ALU to directly operate on particular operand fragments and thus generate a corresponding fragment (a \"partial\") of the multi-precision result. Each partial, when generated, is written to an associated region of storage that has been designated for the multiple-precision result. This process is repeated for all operand fragments so as to generate a complete collection of partials, which is the result of the multiple-precision operation.\n\nIn arithmetic operations (e.g., addition, subtraction), the algorithm starts by invoking an ALU operation on the operands' LS fragments, thereby producing both a LS partial and a carry out bit. The algorithm writes the partial to designated storage, whereas the processor's state machine typically stores the carry out bit to an ALU status register. The algorithm then advances to the next fragment of each operand's collection and invokes an ALU operation on these fragments along with the stored carry bit from the previous ALU operation, thus producing another (more significant) partial and a carry out bit. As before, the carry bit is stored to the status register and the partial is written to designated storage. This process repeats until all operand fragments have been processed, resulting in a complete collection of partials in storage, which comprise the multi-precision arithmetic result.\n\nIn multiple-precision shift operations, the order of operand fragment processing depends on the shift direction. In left-shift operations, fragments are processed LS first because the LS bit of each partial—which is conveyed via the stored carry bit—must be obtained from the MS bit of the previously left-shifted, less-significant operand. Conversely, operands are processed MS first in right-shift operations because the MS bit of each partial must be obtained from the LS bit of the previously right-shifted, more-significant operand.\n\nIn bitwise logical operations (e.g., logical AND, logical OR), the operand fragments may be processed in any arbitrary order because each partial depends only on the corresponding operand fragments (the stored carry bit from the previous ALU operation is ignored).\n\nAlthough an ALU can be designed to perform complex functions, the resulting higher circuit complexity, cost, power consumption and larger size makes this impractical in many cases. Consequently, ALUs are often limited to simple functions that can be executed at very high speeds (i.e., very short propagation delays), and the external processor circuitry is responsible for performing complex functions by orchestrating a sequence of simpler ALU operations.\n\nFor example, computing the square root of a number might be implemented in various ways, depending on ALU complexity:\n\n\nThe implementations above transition from fastest and most expensive to slowest and least costly. The square root is calculated in all cases, but processors with simple ALUs will take longer to perform the calculation because multiple ALU operations must be performed.\n\nAn ALU is usually implemented either as a stand-alone integrated circuit (IC), such as the 74181, or as part of a more complex IC. In the latter case, an ALU is typically instantiated by synthesizing it from a description written in VHDL, Verilog or some other hardware description language. For example, the following VHDL code describes a very simple 8-bit ALU:\n\nMathematician John von Neumann proposed the ALU concept in 1945 in a report on the foundations for a new computer called the EDVAC.\n\nThe cost, size, and power consumption of electronic circuitry was relatively high throughout the infancy of the information age. Consequently, all serial computers and many early computers, such as the PDP-8, had a simple ALU that operated on one data bit at a time, although they often presented a wider word size to programmers. One of the earliest computers to have multiple discrete single-bit ALU circuits was the 1948 Whirlwind I, which employed sixteen of such \"math units\" to enable it to operate on 16-bit words.\n\nIn 1967, Fairchild introduced the first ALU implemented as an integrated circuit, the Fairchild 3800, consisting of an eight-bit ALU with accumulator. Other integrated-circuit ALUs soon emerged, including four-bit ALUs such as the Am2901 and 74181. These devices were typically \"bit slice\" capable, meaning they had \"carry look ahead\" signals that facilitated the use of multiple interconnected ALU chips to create an ALU with a wider word size. These devices quickly became popular and were widely used in bit-slice minicomputers.\n\nMicroprocessors began to appear in the early 1970s. Even though transistors had become smaller, there was often insufficient die space for a full-word-width ALU and, as a result, some early microprocessors employed a narrow ALU that required multiple cycles per machine language instruction. Examples of this includes the popular Zilog Z80, which performed eight-bit additions with a four-bit ALU. Over time, transistor geometries shrank further, following Moore's law, and it became feasible to build wider ALUs on microprocessors.\n\nModern integrated circuit (IC) transistors are orders of magnitude smaller than those of the early microprocessors, making it possible to fit highly complex ALUs on ICs. Today, many modern ALUs have wide word widths, and architectural enhancements such as barrel shifters and binary multipliers that allow them to perform, in a single clock cycle, operations that would have required multiple operations on earlier ALUs.\n\n\n\n"}
{"id": "55539504", "url": "https://en.wikipedia.org/wiki?curid=55539504", "title": "Bitcoin Gold", "text": "Bitcoin Gold\n\nBitcoin Gold is a distributed digital currency. It is a hard fork of Bitcoin, the open source cryptocurrency. The stated purpose of the hard fork is to restore the mining functionality with common Graphics Processing Units (GPU), in place of mining with specialized ASIC (customized chipsets), used to mine Bitcoin.\n\nASIC resistant GPU powered mining provides a solution, as this kind of hardware is ubiquitous, and anyone can start mining with a standard, off-the-shelf laptop computer.\n\nBitcoin Gold was hit by double-spending attack on May 18, 2018.\n\nThe hard fork occurred on October 24, 2017, at block height 491407. The Bitcoin Gold team used ‘post-mine’ - a mining of 100,000 coins after the fork had already occurred. The team did this via a rapid mining of approximately 8,000 blocks at 12.5 BTG per block. The bulk of premined coins have been placed into an ‘endowment’, and according to the developers will be used to grow and maintain the BTG ecosystem. However, of the 100K coins, some five percent were set aside as a bonus for the team, or about 833 coins for each of the six members.\n\nBitcoin Gold uses the memory hard equihash as its proof-of-work algorithm instead of SHA-256. Otherwise, the project follows the guidelines of the Bitcoin core project.\n\nIn 2018, Bitcoin Gold (and two other cryptocurrencies) were hit a by a successful 51% hashing attack by an unknown actor. The attackers successfully committed a double spend attack on Bitcoin Gold, a cryptocurrency forked from Bitcoin in 2017. Approximately $18.6 million USD worth of Bitcoin Gold was transferred to a cryptocurrency exchange (typically as part of a pair transaction in exchange of a fiat currency or another cryptocurrency) and then reverted in the public ledger maintained by consensus of Proof-of-Work by exercising a >51% mine power.\n"}
{"id": "780886", "url": "https://en.wikipedia.org/wiki?curid=780886", "title": "Characterization (mathematics)", "text": "Characterization (mathematics)\n\nIn mathematics, the statement that \"Property \"P\" characterizes object \"X\"\" means that not only does \"X\" have property \"P\", but that \"X\" is the \"only\" thing that has property \"P\". In other words, \"P\" is a defining property of \"X\". It is also common to find statements such as \"Property \"Q\" characterises \"Y\" up to isomorphism\". The first type of statement says in different words that the extension of \"P\" is a singleton set. The second says that the extension of \"Q\" is a single equivalence class (for isomorphism, in the given example — depending on how \"up to\" is being used, some other equivalence relation might be involved).\n\n\n"}
{"id": "3499637", "url": "https://en.wikipedia.org/wiki?curid=3499637", "title": "Colin Adams (mathematician)", "text": "Colin Adams (mathematician)\n\nColin Conrad Adams (born October 13, 1956) is a mathematician primarily working in the areas of hyperbolic 3-manifolds and knot theory. His book, \"The Knot Book\", has been praised for its accessible approach to advanced topics in knot theory. He is currently Francis Christopher Oakley Third Century Professor of Mathematics at Williams College, where he has been since 1985. He writes \"Mathematically Bent\", a column of math humor for the \"Mathematical Intelligencer\".\n\nAdams received a B.Sc. from MIT in 1978 and a Ph.D. in mathematics from the University of Wisconsin–Madison in 1983. His dissertation was entitled \"Hyperbolic Structures on Link Complements\" and supervised by James Cannon.\n\nIn 2012 he became a fellow of the American Mathematical Society.\n\nAmong his earliest contributions is his theorem that the Gieseking manifold is the unique cusped hyperbolic 3-manifold of smallest volume. The proof utilizes horoball-packing arguments. Adams is known for his clever use of such arguments utilizing horoball patterns and his work would be used in the later proof by Cao and Meyerhoff that the smallest cusped orientable hyperbolic 3-manifolds are precisely the figure-eight knot complement and its sibling manifold. \n\nAdams has investigated and defined a variety of geometric invariants of hyperbolic links and hyperbolic 3-manifolds in general. He developed techniques for working with volumes of special classes of hyperbolic links. He proved augmented alternating links, which he defined, were hyperbolic. In addition, he has defined almost alternating and toroidally alternating links. He has often collaborated and published this research with students from SMALL, an undergraduate summer research program at Williams.\n\n\n\n\n"}
{"id": "594682", "url": "https://en.wikipedia.org/wiki?curid=594682", "title": "Compactly generated group", "text": "Compactly generated group\n\nIn mathematics, a compactly generated (topological) group is a topological group \"G\" which is algebraically generated by one of its compact subsets. This should not be confused with the unrelated notion (widely used in algebraic topology) of a compactly generated space -- one whose topology is generated (in a suitable sense) by its compact subspaces.\n\nA topological group \"G\" is said to be compactly generated if there exists a compact subset \"K\" of \"G\" such that\n\nSo if \"K\" is symmetric, i.e. \"K\" = \"K\", then \n\nThis property is interesting in the case of locally compact topological groups, since locally compact compactly generated topological groups can be approximated by locally compact, separable metric factor groups of \"G\". More precisely, for a sequence \n\nof open identity neighborhoods, there exists a normal subgroup \"N\" contained in the intersection of that sequence, such that \n\nis locally compact metric separable (the Kakutani-Kodaira-Montgomery-Zippin theorem).\n"}
{"id": "27743220", "url": "https://en.wikipedia.org/wiki?curid=27743220", "title": "Computational Statistics (journal)", "text": "Computational Statistics (journal)\n\nComputational Statistics is a quarterly peer-reviewed scientific journal that publishes applications and research in the field of computational statistics, as well as reviews of hardware, software, and books. According to the \"Journal Citation Reports\", the journal has a 2012 impact factor of 0.482. It was established in 1986 as \"Computational Statistics Quarterly\" and obtained its current title in 1992. The journal is published by Springer Science+Business Media and the editor-in-chief is Yuichi Mori (Okayama University of Science).\n\n"}
{"id": "16273876", "url": "https://en.wikipedia.org/wiki?curid=16273876", "title": "Cox–Zucker machine", "text": "Cox–Zucker machine\n\nThe Cox–Zucker machine is an algorithm created by David A. Cox and Steven Zucker. This algorithm determines if a given set of sections provides a basis (up to torsion) for the Mordell–Weil group of an elliptic surface \"E\" → \"S\" where \"S\" is isomorphic to the projective line.\n\nThe algorithm was first published in the 1979 paper \"Intersection numbers of sections of elliptic surfaces\" by Cox and Zucker and it was later named the \"Cox–Zucker machine\" by Charles Schwartz in 1984.\n"}
{"id": "456410", "url": "https://en.wikipedia.org/wiki?curid=456410", "title": "Crystal system", "text": "Crystal system\n\nIn crystallography, the terms crystal system, crystal family, and lattice system each refer to one of several classes of space groups, lattices, point groups, or crystals. Informally, two crystals are in the same crystal system if they have similar symmetries, although there are many exceptions to this.\n\nCrystal systems, crystal families and lattice systems are similar but slightly different, and there is widespread confusion between them: in particular the trigonal crystal system is often confused with the rhombohedral lattice system, and the term \"crystal system\" is sometimes used to mean \"lattice system\" or \"crystal family\".\n\nSpace groups and crystals are divided into seven crystal systems according to their point groups, and into seven lattice systems according to their Bravais lattices. Five of the crystal systems are essentially the same as five of the lattice systems, but the hexagonal and trigonal crystal systems differ from the hexagonal and rhombohedral lattice systems. The six crystal families are formed by combining the hexagonal and trigonal crystal systems into one hexagonal family, in order to eliminate this confusion.\n\nA lattice system is a class of lattices with the same set of lattice point groups, which are subgroups of the arithmetic crystal classes. The 14 Bravais lattices are grouped into seven lattice systems: triclinic, monoclinic, orthorhombic, tetragonal, rhombohedral, hexagonal, and cubic.\n\nIn a crystal system, a set of point groups and their corresponding space groups are assigned to a lattice system. Of the 32 point groups that exist in three dimensions, most are assigned to only one lattice system, in which case both the crystal and lattice systems have the same name. However, five point groups are assigned to two lattice systems, rhombohedral and hexagonal, because both exhibit threefold rotational symmetry. These point groups are assigned to the trigonal crystal system. In total there are seven crystal systems: triclinic, monoclinic, orthorhombic, tetragonal, trigonal, hexagonal, and cubic.\n\nA crystal family is determined by lattices and point groups. It is formed by combining crystal systems which have space groups assigned to a common lattice system. In three dimensions, the crystal families and systems are identical, except the hexagonal and trigonal crystal systems, which are combined into one hexagonal crystal family. In total there are six crystal families: triclinic, monoclinic, orthorhombic, tetragonal, hexagonal, and cubic.\n\nSpaces with less than three dimensions have the same number of crystal systems, crystal families and lattice systems. In one-dimensional space, there is one crystal system. In 2D space, there are four crystal systems: oblique, rectangular, square, and hexagonal.\n\nThe relation between three-dimensional crystal families, crystal systems and lattice systems is shown in the following table: \n\nThe 7 crystal systems consist of 32 crystal classes (corresponding to the 32 crystallographic point groups) as shown in the following table:\n\nPoint symmetry can be thought of in the following fashion: consider the coordinates which make up the structure, and project them all through a single point, so that (\"x\",\"y\",\"z\") becomes (−\"x\",−\"y\",−\"z\"). This is the 'inverted structure'. If the original structure and inverted structure are identical, then the structure is \"centrosymmetric\". Otherwise it is \"non-centrosymmetric\". Still, even for non-centrosymmetric case, inverted structure in some cases can be rotated to align with the original structure. This is the case of non-centrosymmetric achiral structure. If the inverted structure cannot be rotated to align with the original structure, then the structure is chiral (enantiomorphic) and its symmetry group is \"enantiomorphic\".\n\nA direction (meaning a line without an arrow) is called \"polar\" if its two directional senses are geometrically or physically different. A polar symmetry direction of a crystal is called a polar axis. Groups containing a polar axis are called \"polar\". A polar crystal possess a \"unique\" axis (found in no other directions) such that some geometrical or physical property is different at the two ends of this axis. It may develop a dielectric polarization, e.g. in pyroelectric crystals. A polar axis can occur only in non-centrosymmetric structures. There should also not be a mirror plane or twofold axis perpendicular to the polar axis, because they will make both directions of the axis equivalent.\n\nThe crystal structures of chiral biological molecules (such as protein structures) can only occur in the 65 enantiomorphic space groups (biological molecules are usually chiral). \n\nThe distribution of the 14 Bravais lattices into lattice systems and crystal families is given in the following table.\n\nIn geometry and crystallography, a Bravais lattice is a category of symmetry groups for translational symmetry in three directions, or correspondingly, a category of translation lattices.\n\nSuch symmetry groups consist of translations by vectors of the form\n\nwhere \"n\", \"n\", and \"n\" are integers and a, a, and a are three non-coplanar vectors, called \"primitive vectors\".\n\nThese lattices are classified by space group of the translation lattice itself; there are 14 Bravais lattices in three dimensions; each can apply in one lattice system only. They represent the maximum symmetry a structure with the translational symmetry concerned can have.\n\nAll crystalline materials must, by definition fit in one of these arrangements (not including quasicrystals).\n\nFor convenience a Bravais lattice is depicted by a unit cell which is a factor 1, 2, 3 or 4 larger than the primitive cell. Depending on the symmetry of a crystal or other pattern, the fundamental domain is again smaller, up to a factor 48.\n\nThe Bravais lattices were studied by Moritz Ludwig Frankenheim in 1842, who found that there were 15 Bravais lattices. This was corrected to 14 by A. Bravais in 1848.\n\n‌The four-dimensional unit cell is defined by four edge lengths (\"a\", \"b\", \"c\", \"d\") and six interaxial angles (\"α\", \"β\", \"γ\", \"δ\", \"ε\", \"ζ\"). The following conditions for the lattice parameters define 23 crystal families\n\nThe names here are given according to Whittaker. They are almost the same as in Brown \"et al\", with exception for names of the crystal families 9, 13, and 22. The names for these three families according to Brown \"et al\" are given in parenthesis.\n\nThe relation between four-dimensional crystal families, crystal systems, and lattice systems is shown in the following table. Enantiomorphic systems are marked with an asterisk. The number of enantiomorphic pairs are given in parentheses. Here the term \"enantiomorphic\" has a different meaning than in the table for three-dimensional crystal classes. The latter means, that enantiomorphic point groups describe chiral (enantiomorphic) structures. In the current table, \"enantiomorphic\" means that a group itself (considered as a geometric object) is enantiomorphic, like enantiomorphic pairs of three-dimensional space groups P3 and P3, P422 and P422. Starting from four-dimensional space, point groups also can be enantiomorphic in this sense.\n\n\n"}
{"id": "36017481", "url": "https://en.wikipedia.org/wiki?curid=36017481", "title": "Free-form deformation", "text": "Free-form deformation\n\nIn computer graphics, free-form deformation (FFD) is a geometric technique used to model simple deformations of rigid objects. It is based on the idea of enclosing an object within a cube or another hull object, and transforming the object within the hull as the hull is deformed. Deformation of the hull is based on the concept of so-called \"hyper-patches\", which are three-dimensional analogs of parametric curves such as Bézier curves, B-splines, or NURBs. The technique was first described by Thomas W. Sederberg and Scott R. Parry in 1986, and is based on an earlier technique by Alan Barr. It was extended by Coquillart to a technique described as \"extended free-form deformation\", which refines the hull object by introducing additional geometry or by using different hull objects such as cylinders and prisms.\n\n"}
{"id": "22278053", "url": "https://en.wikipedia.org/wiki?curid=22278053", "title": "Froda's theorem", "text": "Froda's theorem\n\nIn mathematics, Darboux–Froda's theorem, named after Alexandru Froda, a Romanian mathematician, describes the set of discontinuities of a monotone real-valued function of a real variable. Usually, this theorem appears in literature without a name. It was written in A. Froda' thesis in 1929.. As it is acknowledged in the thesis, the theorem is in fact due to Jean Gaston Darboux.\n\n\nIf the function is continuous at formula_1 then the jump at formula_1 is zero. Moreover, if formula_11 is not continuous at formula_1, the jump can be zero at formula_1 if formula_14.\n\nLet \"f\" be a real-valued monotone function defined on an interval \"I\". Then the set of discontinuities of the first kind is at most countable.\n\nOne can prove that all points of discontinuity of a monotone real-valued function defined on an interval are jump discontinuities and hence, by our definition, of the first kind. With this remark Froda's theorem takes the stronger form:\n\nLet \"f\" be a monotone function defined on an interval formula_15. Then the set of discontinuities is at most countable.\n\nLet formula_16 be an interval and formula_11 defined on formula_15 an increasing function. We have\n\nfor any formula_20 and let formula_21 be formula_22 points inside formula_15 at which the jump of formula_11 is greater or equal to formula_25:\n\nWe have formula_27 or formula_28.\nThen\n\nand hence: formula_31.\n\nSince formula_32 we have that the number of points at which the jump is greater than formula_25 is finite or zero.\n\nWe define the following sets:\n\nWe have that each set formula_36 is finite or the empty set. The union\nformula_37 contains all points at which the jump is positive and hence contains all points of discontinuity. Since every formula_38 is at most countable, we have that formula_39 is at most countable.\n\nIf formula_11 is decreasing the proof is similar.\n\nIf the interval formula_15 is not closed and bounded (and hence by Heine–Borel theorem not compact) then the interval can be written as a countable union of closed and bounded intervals formula_42 with the property that any two consecutive intervals have an endpoint in common: formula_43\n\nIf formula_44 then formula_45 where formula_46 is a strictly decreasing sequence such that formula_47 In a similar way if formula_48 or if <math>I=(a,b)\\ -\\infty\\leq a.\n\nIn any interval formula_42 we have at most countable many points of discontinuity, and since a countable union of at most countable sets is at most countable, it follows that the set of all discontinuities is at most countable.\n\n\n"}
{"id": "46826337", "url": "https://en.wikipedia.org/wiki?curid=46826337", "title": "Gábor Korchmáros", "text": "Gábor Korchmáros\n\nGábor Korchmáros (born 1948) is a Hungarian mathematician, who works on finite geometry.\n\nKorchmáros received in 1972 from the University of Budapest a Ph.D. in mathematics. In 1973 on a postdoc grant, he studied at the Research Center of the Accademia dei Lincei in Rome. In 1976 he was awarded the Grunwald Prize of the János Bolyai Mathematical Society. In 1980 he received the Candidate of Sciences degree and in 2000 the Doctor of Sciences degree from the János Bolyai Mathematical Society. In 1987 he became a professor at the Università della Basilicata. He was a visiting professor at several universities, including the University of Sussex, the University of Delaware and the University of Szeged (Hungary).\n\nHis research involves the theory of ovals and their higher-dimensional generalizations over finite fields. One topic of his research is the collineation groups of ovals and embedding problems for arcs in ovals; these investigations have applications in coding theory and are related to the Hasse-Weil bound for elliptic curves. He also works on algebraic curves over finite fields and their automorphism groups, translation planes, finite Möbius planes, finite Minkowski planes, and elliptic curve cryptography. In the late 1970s he worked with Beniamino Segre.\n\nHe was awarded the Euler Medal of ICA in 2008, and in 2014 the Doctor Honoris Casusa degree at the University of Szeged.\n\n\n"}
{"id": "1157842", "url": "https://en.wikipedia.org/wiki?curid=1157842", "title": "Hasse–Weil zeta function", "text": "Hasse–Weil zeta function\n\nIn mathematics, the Hasse–Weil zeta function attached to an algebraic variety \"V\" defined over an algebraic number field \"K\" is one of the two most important types of L-function. Such \"L\"-functions are called 'global', in that they are defined as Euler products in terms of local zeta functions. They form one of the two major classes of global \"L\"-functions, the other being the \"L\"-functions associated to automorphic representations. Conjecturally there is just one essential type of global \"L\"-function, with two descriptions (coming from an algebraic variety, coming from an automorphic representation); this would be a vast generalisation of the Taniyama–Shimura conjecture, itself a very deep and recent result () in number theory.\n\nThe description of the Hasse–Weil zeta function \"up to finitely many factors of its Euler product\" is relatively simple. This follows the initial suggestions of Helmut Hasse and André Weil, motivated by the case in which \"V\" is a single point, and the Riemann zeta function results.\n\nTaking the case of \"K\" the rational number field Q, and \"V\" a non-singular projective variety, we can for almost all prime numbers \"p\" consider the reduction of \"V\" modulo \"p\", an algebraic variety \"V\" over the finite field F with \"p\" elements, just by reducing equations for \"V\". Again for almost all \"p\" it will be non-singular. We define\n\nto be the Dirichlet series of the complex variable \"s\", which is the infinite product of the local zeta functions\n\nThen \"Z\"(\"s\"), according to our definition, is well-defined only up to multiplication by rational functions in a finite number of formula_3. \n\nSince the indeterminacy is relatively harmless, and has meromorphic continuation everywhere, there is a sense in which the properties of \"Z(s)\" do not essentially depend on it. In particular, while the exact form of the functional equation for \"Z\"(\"s\"), reflecting in a vertical line in the complex plane, will definitely depend on the 'missing' factors, the existence of some such functional equation does not.\n\nA more refined definition became possible with the development of étale cohomology; this neatly explains what to do about the missing, 'bad reduction' factors. According to general principles visible in ramification theory, 'bad' primes carry good information (theory of the \"conductor\"). This manifests itself in the étale theory in the Ogg–Néron–Shafarevich criterion for good reduction; namely that there is good reduction, in a definite sense, at all primes \"p\" for which the Galois representation ρ on the étale cohomology groups of \"V\" is \"unramified\". For those, the definition of local zeta function can be recovered in terms of the characteristic polynomial of \n\nFrob(\"p\") being a Frobenius element for \"p\". What happens at the ramified \"p\" is that ρ is non-trivial on the inertia group \"I\"(\"p\") for \"p\". At those primes the definition must be 'corrected', taking the largest quotient of the representation ρ on which the inertia group acts by the trivial representation. With this refinement, the definition of \"Z\"(\"s\") can be upgraded successfully from 'almost all' \"p\" to \"all\" \"p\" participating in the Euler product. The consequences for the functional equation were worked out by Serre and Deligne in the later 1960s; the functional equation itself has not been proved in general.\n\nLet \"E\" be an elliptic curve over Q of conductor \"N\". Then, \"E\" has good reduction at all primes \"p\" not dividing \"N\", it has multiplicative reduction at the primes \"p\" that \"exactly\" divide \"N\" (i.e. such that \"p\" divides \"N\", but \"p\" does not; this is written \"p\" || \"N\"), and it has additive reduction elsewhere (i.e. at the primes where \"p\" divides \"N\"). The Hasse–Weil zeta function of \"E\" then takes the form\n\nHere, ζ(\"s\") is the usual Riemann zeta function and \"L\"(\"s\", \"E\") is called the \"L\"-function of \"E\"/Q, which takes the form\n\nwhere, for a given prime \"p\",\n\nwhere, in the case of good reduction \"a\" is \"p\" + 1 − (number of points of \"E\" mod \"p\"), and in the case of multiplicative reduction \"a\" is ±1 depending on whether \"E\" has split or non-split multiplicative reduction at \"p\".\n\nThe Hasse–Weil conjecture states that the Hasse–Weil zeta function should extend to a meromorphic function for all complex \"s\", and should satisfy a functional equation similar to that of the Riemann zeta function. For elliptic curves over the rational numbers, the Hasse–Weil conjecture follows from the modularity theorem.\n\n\n"}
{"id": "4044234", "url": "https://en.wikipedia.org/wiki?curid=4044234", "title": "Higgs sector", "text": "Higgs sector\n\nIn particle physics, the Higgs sector is the collection of quantum fields and/or particles that are responsible for the Higgs mechanism, i.e. for the spontaneous symmetry breaking of the Higgs field. The word \"sector\" refers to a subgroup of the total set of fields and particles.\n\n"}
{"id": "17006296", "url": "https://en.wikipedia.org/wiki?curid=17006296", "title": "Hilbert–Poincaré series", "text": "Hilbert–Poincaré series\n\nIn mathematics, and in particular in the field of algebra, a Hilbert–Poincaré series (also known under the name Hilbert series), named after David Hilbert and Henri Poincaré, is an adaptation of the notion of dimension to the context of graded algebraic structures (where the dimension of the entire structure is often infinite). It is a formal power series in one indeterminate, say \"t\", where the coefficient of \"t\" gives the dimension (or rank) of the sub-structure of elements homogeneous of degree \"n\". It is closely related to the Hilbert polynomial in cases when the latter exists; however, the Hilbert–Poincaré series describes the rank in every degree, while the Hilbert polynomial describes it only in all but finitely many degrees, and therefore provides less information. In particular the Hilbert–Poincaré series cannot be deduced from the Hilbert polynomial even if the latter exists. In good cases, the Hilbert–Poincaré series can be expressed as a rational function of its argument \"t\".\n\nLet \"K\" be a field, and let formula_1 be a N-graded vector space over \"K\", where each subspace \"V\" of vectors of degree \"n\" is finite-dimensional. Then the Hilbert–Poincaré series of \"V\" is the formal power series\nA similar definition can be given for an N-graded \"R\"-module over any commutative ring \"R\" in which each submodule of elements homogeneous of a fixed degree \"n\" is free of finite rank; it suffices to replace the dimension by the rank. Often the graded vector space or module of which the Hilbert–Poincaré series is considered has additional structure, for instance that of a ring, but the Hilbert–Poincaré series is independent of the multiplicative or other structure.\n\nExample: Since there are formula_3 monomials of degree \"k\" in variables formula_4 (by induction, say), it follows immediately that the Hilbert–Poincaré series of \"K\"[\"X\",\"X\",…,\"X\"] is formula_5\n\nSuppose \"M\" is a finitely generated graded module over formula_6 with an Artinian ring (e.g., a field) \"A\". Then the Poincaré series of \"M\" is a polynomial with integral coefficients divided by formula_7. The standard proof today is an induction on \"n\". Hilbert's original proof made a use of Hilbert's syzygy theorem (a projective resolution of \"M\"), which gives more homological information.\n\nHere is a proof by induction on the number \"n\" of indeterminates. If formula_8, then, since \"M\" has finite length, formula_9 if \"k\" is large enough. Next, suppose the theorem is true for formula_10 and consider the exact sequence of graded modules (exact degree-wise), with the notation formula_11,\nSince the length is additive, Poincaré series are also additive. Hence, we have:\nWe can write formula_14. Since \"K\" is killed by formula_15, we can regard it as a graded module over formula_16; the same is true for \"C\". The theorem thus now follows from the inductive hypothesis.\n\nAn example of graded vector space is associated to a chain complex, or cochain complex \"C\" of vector spaces; the latter takes the form\n\nThe Hilbert–Poincaré series (here often called the Poincaré polynomial) of the graded vector space formula_18 for this complex is\n\nThe Hilbert–Poincaré polynomial of the cohomology, with cohomology spaces \"H\" = \"H\"(\"C\"), is \n\nA famous relation between the two is that there is a polynomial formula_21 with non-negative coefficients, such that formula_22\n"}
{"id": "44770467", "url": "https://en.wikipedia.org/wiki?curid=44770467", "title": "Idris Assani", "text": "Idris Assani\n\nIdris Assani is a Beninese and African-American mathematician, who works as a professor of mathematics at the University of North Carolina at Chapel Hill.\n\nAlthough born in Niger, Assani is Beninese. He was educated in France, earning a bachelor's degree in commerce from Paris Dauphine University in 1981, a doctorate of the third cycle in mathematics from Pierre and Marie Curie University in 1981, and a doctor of science from Pierre and Marie Curie University in 1986, under the supervision of Antoine Brunel. He joined the UNC mathematics department in 1988 but, allegedly for racist reasons, was turned down for tenure. He appealed through the courts, won his case and gained tenure in 1995, and was promoted to full professor one year later. In doing so he became the first African-American tenured associate professor and the first African-American full professor at UNC, as well as the only mathematician there to be promoted from associate to full so quickly.\n\nAssani's research concerns ergodic theory. He is the author of the research monograph \"Wiener Wintner Ergodic Theorems\" (World Scientific, 2003), about mathematics related to the Wiener–Wintner theorem, and is also the editor of several volumes of collected papers.\n\nIn 2012, Assani was named as one of the inaugural fellows of the American Mathematical Society.\n\n"}
{"id": "31813104", "url": "https://en.wikipedia.org/wiki?curid=31813104", "title": "Internal category", "text": "Internal category\n\nIn mathematics, more specifically in category theory - internal categories are a generalisation of the notion of small category, and are defined with respect to a fixed ambient category. If the ambient category is taken to be the category of sets then one recovers the theory of small categories. In general, internal categories consist of a pair of objects in the ambient category - thought of as the 'object of objects' and 'object of morphisms', together with a collection of morphisms in the ambient category satisfying certain identities. Group objects, are common examples of internal categories.\n\nThere are notions of internal functors and natural transformations which make the collection of internal categories in a fixed category into a 2-category.\n\nLet formula_1 be a category with pullbacks. An internal category in formula_2 consists of the following data: two formula_2-objects formula_4 named \"object of objects\" and \"object of morphisms\" respectively and four formula_2-arrows formula_6 subject to coherence conditions expressing the axioms of category theory. See \n\n"}
{"id": "20442598", "url": "https://en.wikipedia.org/wiki?curid=20442598", "title": "JH (hash function)", "text": "JH (hash function)\n\nJH is a cryptographic hash function submitted to the NIST hash function competition by Hongjun Wu. Though chosen as one of the five finalists of the competition, JH ultimately lost to NIST hash candidate Keccak. JH has a 1024-bit state, and works on 512-bit input blocks. Processing an input block consists of three steps:\nThe resulting digest is the first 224, 256, 384 or 512 bits from the 1024-bit final value.\nIt is well suited to a bit slicing implementation using the SSE2 instruction set, giving speeds of 16.8 cycles per byte.\n\n"}
{"id": "40343127", "url": "https://en.wikipedia.org/wiki?curid=40343127", "title": "Joan Hutchinson", "text": "Joan Hutchinson\n\nJoan Prince Hutchinson (born 1945) is an American mathematician and Professor Emerita of Mathematics from Macalester College.\n\nJoan Hutchinson was born in Philadelphia, Pennsylvania; her father was a demographer and university professor, and her mother a high school mathematics teacher. She studied at Smith College in Northampton, Massachusetts, graduating in 1967 summa cum laude with an honors paper directed by Prof. Alice Dickinson.\nAfter graduation she worked as a computer programmer at the Woods Hole Oceanographic Institute and at the Harvard University Computing Center then studied mathematics (and English change ringing on tower bells) at the University of Warwick in Coventry England. Returning to the United States, Hutchinson did graduate work at the University of Pennsylvania earning a Ph.D. in mathematics in 1973 under the supervision of Herbert S. Wilf.\n\nShe was a John Wesley Young research instructor at Dartmouth College, 1973–1975. \nShe and her husband, fellow mathematician Stan Wagon, taught at Smith College, 1975-1990, and at Macalester College, 1990-2007. At both colleges they shared a full-time position in mathematics. She spent sabbaticals, taught, and held visiting positions at Tufts University, Carleton College, University of Colorado Boulder, University of Washington, University of Michigan, Mathematical Sciences Research Institute in Berkeley, California, and University of Colorado Denver.\n\nShe has served on committees of the American Mathematical Society, the Mathematical Association of America (MAA), SIAM Special Interest Group on Discrete Math (SIAM-DM), and the Association for Women in Mathematics, involved with the latter organization since a graduate student during its founding days in 1971. Mentoring women students and younger colleagues has been an important concern of her professional life. She served as the vice-chair of SIAM-DM, 2000-2002. She was a member of the editorial board of the \"American Mathematical Monthly\", 1986-1996, and continues on the board of the \"Journal of Graph Theory\" since 1993.\n\nHer research has focused on graph theory and discrete mathematics, specializing mainly in topological and chromatic graph theory and on visibility graphs;\nfor overviews of this work see and .\n\nShe has published over 75 research and expository papers in graph theory, many with the late Michael O. Albertson, formerly of Smith College.\nIn one of their most cited works, Albertson and Hutchinson completed work of Gabriel Andrew Dirac related to the Heawood conjecture by proving that, on any surface other than the sphere or Klein bottle, the only graphs meeting Heawood's bound on the chromatic number of surface-embedded graphs are the complete graphs.\nShe has also considered algorithmic aspects in these areas, for example, generalizing the planar separator theorem to surfaces.\nWith S. Wagon she has co-authored papers on algorithmic aspects of the four color theorem.\n\nAlbertson and Hutchinson also wrote together the textbook \"Discrete Mathematics with Algorithms\".\n\nIn 1994 she received the Carl B. Allendoerfer Award of the Mathematical Association of America for the expository article \"Coloring ordinary maps, maps of empires, and maps of the moon\" in \"Mathematics Magazine\".\nThe work of this paper was also included in an issue of \"What’s Happening in the Mathematical Sciences\" and in the Mathematical Recreations column of \"Scientific American\".\n\nIn 1998 she was a winner of the MAA North Central Section Teaching Award,\nand in 1999 she was a winner of the Deborah and Franklin Tepper Haimo Award for Excellence in College or University Teaching.\nOn the occasion of her 60th birthday, she was the honoree at the Graph Theory with Altitude conference at the University of Colorado Denver, organized by her former student Ellen Gethner, professor of computer science.\n"}
{"id": "25140222", "url": "https://en.wikipedia.org/wiki?curid=25140222", "title": "Joint spectral radius", "text": "Joint spectral radius\n\nIn mathematics, the joint spectral radius is a generalization of the classical notion of spectral radius of a matrix, to sets of matrices. In recent years this notion has found applications in a large number of engineering fields and is still a topic of active research.\n\nThe joint spectral radius of a set of matrices is the maximal asymptotic growth rate of products of matrices taken in that set. For a finite (or more generally compact) set of matrices formula_1 the joint spectral radius is defined as follows:\n\nIt can be proved that the limit exists and that the quantity actually does not depend on the chosen matrix norm (this is true for any norm but particularly easy to see if the norm is sub-multiplicative). The joint spectral radius was introduced in 1960 by Gian-Carlo Rota and Gilbert Strang, two mathematicians from MIT, but started attracting attention with the work of Ingrid Daubechies and Jeffrey Lagarias. They showed that the joint spectral radius can be used to describe smoothness properties of certain wavelet functions. A wide number of applications have been proposed since then. It is known that the joint spectral radius quantity is NP-hard to compute or to approximate, even when the set formula_3 consists of only two matrices with all nonzero entries of the two\nmatrices which are constrained to be equal. Moreover, the question \"formula_4\" is an undecidable problem. Nevertheless, in recent years much progress has been done on its understanding, and it appears that in practice the joint spectral radius can often be computed to satisfactory precision, and that it moreover can bring interesting insight in engineering and mathematical problems.\n\nIn spite of the negative theoretical results on the joint spectral radius computability, methods have been proposed that perform well in practice. Algorithms are even known, which can reach an arbitrary accuracy in an a priori computable amount of time. These algorithms can be seen as trying to approximate the unit ball of a particular vector norm, called the extremal norm. One generally distinguishes between two families of such algorithms: the first family, called polytope norm methods, construct the extremal norm by computing long trajectories of points. An advantage of these methods is that in the favorable cases it can find the exact value of the joint spectral radius and provide a certificate that this is the exact value.\n\nThe second methods approximate the extremal norm with modern optimization techniques, like ellipsoid norm approximation, semidefinite programming, Sum Of Squares, conic programming. The advantage of these methods is that they are easy to implement, and in practice, they provide in general the best bounds on the joint spectral radius.\n\nRelated to the computability of the joint spectral radius is the following conjecture:\n\n\"For any finite set of matrices formula_5 there is a product formula_6 of matrices in this set such that \nIn the above equation \"formula_8\" refers to the classical spectral radius of the matrix formula_9\n\nThis conjecture, proposed in 1995, has been proved to be false in 2003. The counterexample provided in that reference uses advanced measure-theoretical ideas. Subsequently, many other counterexamples have been provided, including an elementary counterexample that uses simple combinatorial properties matrices and a counterexample based on dynamical systems properties. Recently an explicit counterexample has been proposed in. Many questions related to this conjecture are still open, as for instance the question of knowing whether it holds for pairs of binary matrices.\n\nThe joint spectral radius was introduced for its interpretation as a stability condition for discrete-time switching dynamical systems. Indeed, the system defined by the equations\nis stable if and only if formula_11\n\nThe joint spectral radius became popular when Ingrid Daubechies and Jeffrey Lagarias showed that it rules the continuity of certain wavelet functions. Since then, it has found many applications, ranging from number theory to information theory, autonomous agents consensus, combinatorics on words...\n\nThe joint spectral radius is the generalization of the spectral radius of a matrix for a set of several matrices. However, much more quantities can be defined when considering a set of matrices: The joint spectral subradius characterizes the minimal rate of growth of products in the semigroup generated by formula_3. \nThe p-radius characterizes the rate of growth of the formula_13 average of the norms of the products in the semigroup.\nThe Lyapunov exponent of the set of matrices characterizes the rate of growth of the geometric average.\n"}
{"id": "32160213", "url": "https://en.wikipedia.org/wiki?curid=32160213", "title": "Korn–Kreer–Lenssen model", "text": "Korn–Kreer–Lenssen model\n\nThe Korn–Kreer–Lenssen model (KKL model) is a discrete trinomial model proposed in 1998 by Ralf Korn, Markus Kreer and Mark Lenssen to model illiquid securities and to value financial derivatives on these. It generalizes the binomial Cox-Ross-Rubinstein model in a natural way as the stock in a given time interval can either rise one unit up, fall one unit down or remain unchanged. In contrast to Black–Scholes or Cox-Ross-Rubinstein model the market consisting of stock and cash is not complete yet. To value and replicate a financial derivative an additional traded security related to the original security needs to be added. This might be a Low Exercise Price Option (or short LEPO). The mathematical proof of arbitrage free pricing is based on martingale representations for point processes pioneered in the 1980s and 1990 by Albert Shiryaev, Robert Liptser and Marc Yor.\n\nThe dynamics is based on continuous time linear birth-death processes and analytic formulae for option prices and Greeks can be stated. Later work looks at market completion with general calls or puts. A comprehensive introduction may be found in the attached MSc-thesis.\n\nThe model belongs to the class of trinomial models and the difference to the standard trinomial tree is the following: if formula_1 denotes the waiting time between two movements of the stock price then in the KKL-model formula_1 remains finite and exponentially distributed whereas in trinomial trees the time is discrete and the limit formula_3 is taken by numerical extrapolation afterwards.\n\n\n\n"}
{"id": "8267820", "url": "https://en.wikipedia.org/wiki?curid=8267820", "title": "Kurepa tree", "text": "Kurepa tree\n\nIn set theory, a Kurepa tree is a tree (\"T\", <) of height ω, each of whose levels is at most countable, and has at least ℵ many branches. This concept was introduced by . The existence of a Kurepa tree (known as the Kurepa hypothesis, though Kurepa originally conjectured that this was false) is consistent with the axioms of ZFC: Solovay showed in unpublished work that there are Kurepa trees in Gödel's constructible universe . More precisely, the existence of Kurepa trees follows from the diamond plus principle, which holds in the constructible universe. On the other hand, showed that if a strongly inaccessible cardinal is Lévy collapsed to ω then, in the resulting model, there are no Kurepa trees. The existence of an inaccessible cardinal is in fact equiconsistent with the failure of the Kurepa hypothesis, because if the Kurepa hypothesis is false then the cardinal ω is inaccessible in the constructible universe.\n\nA Kurepa tree with fewer than 2 branches is known as a Jech–Kunen tree.\n\nMore generally if κ is an infinite cardinal, then a κ-Kurepa tree is a tree of height κ with more than κ branches but at most |α| elements of each infinite level α<κ, and the Kurepa hypothesis for κ is the statement that there is a κ-Kurepa tree. Sometimes the tree is also assumed to be binary. The existence of a binary κ-Kurepa tree is equivalent to the existence of a Kurepa family: a set of more than κ subsets of κ such that their intersections with any infinite ordinal α<κ form a set of cardinality at most α. The Kurepa hypothesis is false if κ is an ineffable cardinal, and conversely Jensen showed that in the constructible universe for any uncountable regular cardinal κ there is a κ-Kurepa tree unless κ is ineffable.\n\nA Kurepa tree can be \"killed\" by forcing the existence of a function whose value on any non-root node is an ordinal less than the rank of the node, such that whenever three nodes, one of which is a lower bound for the other two, are mapped to the same ordinal, then the three nodes are comparable. This can be done without collapsing ℵ, and results in a tree with exactly ℵ branches.\n\n\n"}
{"id": "39491072", "url": "https://en.wikipedia.org/wiki?curid=39491072", "title": "List of things named after Felix Klein", "text": "List of things named after Felix Klein\n\nThese are things named after Felix Klein (1849 – 1925), a German mathematician.\n\n"}
{"id": "16004359", "url": "https://en.wikipedia.org/wiki?curid=16004359", "title": "Littlewood–Richardson rule", "text": "Littlewood–Richardson rule\n\nIn mathematics, the Littlewood–Richardson rule is a combinatorial description of the coefficients that arise when decomposing a product of two Schur functions as a linear combination of other Schur functions. These coefficients are natural numbers, which the Littlewood–Richardson rule describes as counting certain skew tableaux. They occur in many other mathematical contexts, for instance as multiplicity in the decomposition of tensor products of irreducible representations of general linear groups (or related groups like the special linear and special unitary groups), or in the decomposition of certain induced representations in the representation theory of the symmetric group, or in the area of algebraic combinatorics dealing with Young tableaux and symmetric polynomials.\n\nLittlewood–Richardson coefficients depend on three partitions, say formula_1, of which formula_2 and formula_3 describe the Schur functions being multiplied, and formula_4 gives the Schur function of which this is the coefficient in the linear combination; in other words they are the coefficients formula_5 such that\nThe Littlewood–Richardson rule states that formula_5 is equal to the number of Littlewood–Richardson tableaux of skew shape formula_8 and of weight formula_3.\n\nThe Littlewood–Richardson rule was first stated by but though they claimed it as a theorem they only proved it in some fairly simple special cases. \nThere are now several short proofs of the rule, such as , and using Bender-Knuth involutions.\n\nThe Littlewood–Richardson rule is notorious for the number of errors that appeared prior to its complete, published proof. Several published attempts to prove it are incomplete, and it is particularly difficult to avoid errors when doing hand calculations with it: even the original example in contains an error.\n\nA Littlewood–Richardson tableau is a skew semistandard tableau with the additional property that the sequence obtained by concatenating its reversed rows is a lattice word (or lattice permutation), which means that in every initial part of the sequence any number formula_10 occurs at least as often as the number formula_11. Another equivalent (though not quite obviously so) characterization is that the tableau itself, and any tableau obtained from it by removing some number of its leftmost columns, has a weakly decreasing weight. Many other combinatorial notions have been found that turn out to be in bijection with Littlewood–Richardson tableaux, and can therefore also be used to define the Littlewood–Richardson coefficients.\n\nConsider the case that formula_12, formula_13 and formula_14. Then the fact that formula_15 can be deduced from the fact that the two tableaux shown at the right are the only two Littlewood–Richardson tableaux of shape formula_8 and weight formula_3. Indeed, since the last box on the first nonempty line of the skew diagram can only contain an entry 1, the entire first line must be filled with entries 1 (this is true for any Littlewood–Richardson tableau); in the last box of the second row we can only place a 2 by column strictness and the fact that our lattice word cannot contain any larger entry before it contains a 2. For the first box of the second row we can now either use a 1 or a 2. Once that entry is chosen, the third row must contain the remaining entries to make the weight (3,2,1), in a weakly increasing order, so we have no choice left any more; in both case it turns out that we do find a Littlewood–Richardson tableau.\n\nThe condition that the sequence of entries read from the tableau in a somewhat peculiar order form a lattice word can be replaced by a more local and geometrical condition. Since in a semistandard tableau equal entries never occur in the same column, one can number the copies of any value from right to left, which is their order of occurrence in the sequence that should be a lattice word. Call the number so associated to each entry its index, and write an entry \"i\" with index \"j\" as \"i\"[\"j\"]. Now if some Littlewood–Richardson tableau contains an entry formula_18 with index \"j\", then that entry \"i\"[\"j\"] should occur in a row strictly below that of formula_19 (which certainly also occurs, since the entry \"i\" − 1 occurs as least as often as the entry \"i\" does). In fact the entry \"i\"[\"j\"] should also occur in a column no further to the right than that same entry formula_19 (which at first sight appears to be a stricter condition). If the weight of the Littlewood–Richardson tableau is fixed beforehand, then one can form a fixed collection of indexed entries, and if these are placed in a way respecting those geometric restrictions, in addition to those of semistandard tableaux and the condition that indexed copies of the same entries should respect right-to-left ordering of the indexes, then the resulting tableaux are guaranteed to be Littlewood–Richardson tableaux.\n\nThe Littlewood–Richardson as stated above gives a combinatorial expression for individual Littlewood–Richardson coefficients, but gives no indication of a practical method to enumerate the Littlewood–Richardson tableaux in order to find the values of these coefficients. Indeed, for given formula_1 there is no simple criterion to determine whether any Littlewood–Richardson tableaux of shape formula_8 and of weight formula_3 exist at all (although there are a number of necessary conditions, the simplest of which is formula_24); therefore it seems inevitable that in some cases one has to go through an elaborate search, only to find that no solutions exist.\n\nNevertheless, the rule leads to a quite efficient procedure to determine the full decomposition of a product of Schur functions, in other words to determine all coefficients formula_5 for fixed λ and μ, but varying ν. This fixes the weight of the Littlewood–Richardson tableaux to be constructed and the \"inner part\" λ of their shape, but leaves the \"outer part\" ν free. Since the weight is known, the set of indexed entries in the geometric description is fixed. Now for successive indexed entries, all possible positions allowed by the geometric restrictions can be tried in a backtracking search. The entries can be tried in increasing order, while among equal entries they can be tried by \"decreasing\" index. The latter point is the key to efficiency of the search procedure: the entry \"i\"[\"j\"] is then restricted to be in a column to the right of formula_26, but no further to the right than formula_27 (if such entries are present). This strongly restricts the set of possible positions, but \"always leaves at least one valid position for formula_28\"; thus every placement of an entry will give rise to at least one complete Littlewood–Richardson tableau, and the search tree contains no dead ends.\n\nA similar method can be used to find all coefficients formula_5 for fixed λ and ν, but varying μ.\n\nThe Littlewood–Richardson coefficients \"c\"   appear in the following interrelated ways:\n\n extended the Littlewood–Richardson rule to skew Schur functions as follows:\nwhere the sum is over all tableaux \"T\" on μ/ν such that for all \"j\", the sequence of integers λ+ω(\"T\") is non-increasing, and ω is the weight.\n\nPieri's formula, which is the special case of the Littlewood–Richardson rule in the case when one of the partitions has only one part, states that \nwhere \"S\" is the Schur function of a partition with one row and the sum is over all partitions λ obtained from μ by adding \"n\" elements to its Ferrers diagram, no two in the same column.\n\nIf both partitions are rectangular in shape, the sum is also multiplicity free . Fix \"a\", \"b\", \"p\", and \"q\" positive integers with \"p\" formula_36 \"q\". Denote by formula_37 the partition with \"p\" parts of length \"a\". The partitions indexing nontrivial components of formula_38 are those partitions formula_2 with length formula_40 such that \nFor example,\n\nThe examples of Littlewood-Richardson coefficients below are given in terms of products of Schur polynomials \"S\", indexed by partitions π, using the formula \n\nAll coefficients with ν at most 4 are given by:\n\nMost of the coefficients for small partitions are 0 or 1, which happens in particular whenever one of the factors is of the form \"S\" or \"S\", because of Pieri's formula and its transposed counterpart. The simplest example with a coefficient larger than 1 happens when neither of the factors has this form:\nFor larger partitions the coefficients become more complicated. For example,\n\nThe original example given by was (after correcting for 3 tableaux they found but forgot to include in the final sum) \nwith 26 terms coming from the following 34 tableaux:\n\nCalculating skew Schur functions is similar. \nFor example, the 15 Littlewood–Richardson tableaux for ν=5432 and λ=331 are \n\nso \"S\" = Σ\"c\"  \"S\" = \"S\" + \"S\" + \"S\" + \"S\" + 2\"S\" + 2\"S\" + 2\"S\" + 2\"S\" + 3\"S\" .\n\n\n"}
{"id": "46706590", "url": "https://en.wikipedia.org/wiki?curid=46706590", "title": "Loewner order", "text": "Loewner order\n\nIn mathematics, Loewner order is the partial order defined by the convex cone of positive semi-definite matrices. This order is usually employed to generalize the definitions of monotone and concave/convex scalar functions to monotone and concave/convex Hermitian valued functions. These functions arise naturally in matrix and operator theory and have applications in many areas of physics and engineering.\n\nLet \"A\" and \"B\" be two Hermitian matrices of order \"n\". We say that \"A ≥ B\" if \"A\" − \"B\" is positive semi-definite. Similarly, we say that \"A > B\" if \"A\" − \"B\" is positive definite.\n\nWhen \"A\" and \"B\" are real scalars (i.e. \"n\" = 1), the Loewner order reduces to the usual ordering of R. Although some familiar properties of the usual order of R are also valid when \"n\" ≥ 2, several properties are no longer valid. For instance, the comparability of two matrices may no longer be valid. In fact, if\nformula_1 and formula_2 then neither \"A\" ≥ \"B\" or \"B\" ≥ \"A\" holds true.\n\nMoreover, since \"A\" and \"B\" are Hermitian matrices, their eigenvalues are all real numbers.\nIf \"λ\"(\"B\") is the maximum eigenvalue of \"B\" and \"λ\"(\"A\") the minimum eigenvalue of \"A\", a sufficient criterion to have \"A\" ≥ \"B\" is that \"λ\"(\"A\") ≥ \"λ\"(\"B\"). If \"A\" or \"B\" is a multiple of the identity matrix, then this criterion is also necessary.\n\n\n"}
{"id": "748686", "url": "https://en.wikipedia.org/wiki?curid=748686", "title": "Lubell–Yamamoto–Meshalkin inequality", "text": "Lubell–Yamamoto–Meshalkin inequality\n\nIn combinatorial mathematics, the Lubell–Yamamoto–Meshalkin inequality, more commonly known as the LYM inequality, is an inequality on the sizes of sets in a Sperner family, proved by , , , and . It is named for the initials of three of its discoverers.\n\nThis inequality belongs to the field of combinatorics of sets, and has many applications in combinatorics. In particular, it can be used to prove Sperner's theorem. Its name is also used for similar inequalities.\n\nLet \"U\" be an \"n\"-element set, let \"A\" be a family of subsets of \"U\" such that no set in \"A\" is a subset of another set in \"A\", and let \"a\" denote the number of sets of size \"k\" in \"A\". Then\n\n proves the Lubell–Yamamoto–Meshalkin inequality by a double counting argument in which he counts the permutations of \"U\" in two different ways. First, by counting all permutations of \"U\" directly, one finds that there are \"n\"! of them. But secondly, one can generate a permutation (i.e., an order) of the elements of \"U\" by selecting a set \"S\" in \"A\" and concatenating a permutation of the elements of \"S\" with a permutation of the nonmembers (elements of \"U\\S\"). If |\"S\"| = \"k\", it will be associated in this way with \"k\"!(\"n\" − \"k\")! permutations, and in each of them the first \"k\" elements will be just the elements of \"S\". Each permutation can only be associated with a single set in \"A\", for if two prefixes of a permutation both formed sets in \"A\" then one would be a subset of the other. Therefore, the number of permutations that can be generated by this procedure is\nSince this number is at most the total number of all permutations,\nFinally dividing the above inequality by \"n\"! leads to the result.\n"}
{"id": "11564906", "url": "https://en.wikipedia.org/wiki?curid=11564906", "title": "Luopan", "text": "Luopan\n\nThe luopan or geomantic compass is a Chinese magnetic compass, also known as a Feng Shui compass. It is used by a Feng Shui practitioner to determine the precise direction of a structure or other item. Since the invention of the compass for use in Feng Shui, traditional feng shui has required its use.\n\nLike a conventional compass, a luopan is a direction finder. However, a luopan differs from a compass in several important ways. The most obvious difference is the Feng Shui formulas embedded in up to 40 concentric rings on the surface. This is a metal or wooden plate known as the \"heaven dial\". The circular metal or wooden plate typically sits on a wooden base known as the \"earth plate\". The heaven dial rotates freely on the earth plate.\n\nA red wire or thread that crosses the earth plate and heaven dial at 90-degree angles is the \"Heaven Center Cross Line\", or \"Red Cross Grid Line\". This line is used to find the direction and note position on the rings. \n\nA conventional compass has markings for four or eight directions, while a luopan typically contains markings for 24 directions. This translates to 15 degrees per direction. The Sun takes approximately 15.2 days to traverse a solar term, a series of 24 points on the ecliptic. Since there are 360 degrees on the luopan and approximately 365.25 days in a mean solar year, each degree on a luopan approximates a terrestrial day.\n\nUnlike a typical compass, a luopan does not point to the north magnetic pole of Earth. The needle of a luopan points to the south magnetic pole (it does not point to the geographic south pole). The Chinese word for \"compass\" translates to “south-pointing needle.”\n\nSince the Ming and Qing dynasties, three types of luopan have been popular. They have some formula rings in common, such as the 24 directions and the early and later heaven arrangements.\n\nThis luopan was said to have been used in the Tang dynasty. The San He contains three basic 24-direction rings. Each ring relates to a different method and formula. (The techniques grouped under the name \"Three Harmonies\" are San He methods.)\n\nThis luopan, also known as the \"jiang pan\" (after Jiang Da Hong) or the \"Yi Pan\" (because of the presence of Yijing hexagrams) incorporates many formulas used in San Yuan (Three Cycles). It contains one 24-direction ring, known as the Earth Plate Correct Needle, the ring for the 64 hexagrams, and others. (The techniques grouped under the name \"Flying Stars\" are an example of San Yuan methods.)\n\nThis luopan combines rings from the San He and San Yuan. It contains three 24-direction-rings and the 64 trigrams ring.\n\nEach Feng Shui master may design a luopan to suit preference and to offer students. Some designs incorporate the bagua (trigram) numbers, directions from the Eight Mansions () methods, and English equivalents.\n\nThe luopan is an image of the cosmos (a world model) based on tortoise plastrons used in divination. At its most basic level it serves as a means to assign proper positions in time and space, like the Ming Tang (Hall of Light). The markings are similar to those on a liubo board.\n\nThe oldest precursors of the luopan are the or , meaning \"astrolabe\" or \"diviner's board\"—also sometimes called \"liuren\" astrolabes—unearthed from tombs that date between 278 BCE and 209 BCE. These astrolabes consist of a lacquered, two-sided board with astronomical sightlines. Along with divination for Da Liu Ren the boards were commonly used to chart the motion of Taiyi through the nine palaces. The markings are virtually unchanged from the \"shi\" to the first magnetic compasses. The schematic of earth plate, heaven plate, and grid lines is part of the \"two cords and four hooks\" () geometrical diagram in use since at least the Warring States period.\nThe \"zhinan zhen\" or south-pointing needle, is the original magnetic compass, and was developed for Feng Shui. It featured the two cords and four hooks diagram, direction markers, and a magnetized spoon in the center.\n\n\n\n"}
{"id": "25567675", "url": "https://en.wikipedia.org/wiki?curid=25567675", "title": "MacMahon Master theorem", "text": "MacMahon Master theorem\n\nIn mathematics, the MacMahon Master theorem (MMT) is a result in enumerative combinatorics and linear algebra. It was discovered by Percy MacMahon and proved in his monograph \"Combinatory analysis\" (1916). It is often used to derive binomial identities, most notably Dixon's identity.\n\nIn the monograph, MacMahon found so many applications of his result, he called it \"a master theorem in the Theory of Permutations.\" He explained the title as follows: \"a Master Theorem from the masterly and rapid fashion in which it deals with various questions otherwise troublesome to solve.\"\n\nThe result was re-derived (with attribution) a number of times, most notably by I. J. Good who derived it from his multilinear generalization of the Lagrange inversion theorem. MMT was also popularized by Carlitz who found an exponential power series version. In 1962, Good found a short proof of Dixon's identity from MMT. In 1969, Cartier and Foata found a new proof of MMT by combining algebraic and bijective ideas (built on Foata's thesis) and further applications to combinatorics on words, introducing the concept of traces. Since then, MMT has become a standard tool in enumerative combinatorics.\n\nAlthough various \"q\"-Dixon identities have been known for decades, except for a Krattenthaler–Schlosser extension (1999), the proper q-analog of MMT remained elusive. After Garoufalidis–Lê–Zeilberger's quantum extension (2006), a number of noncommutative extensions were developed by Foata–Han, Konvalinka–Pak, and Etingof–Pak. Further connections to Koszul algebra and quasideterminants were also found by Hai–Lorentz, Hai–Kriegk–Lorenz, Konvalinka–Pak, and others.\n\nFinally, according to J. D. Louck, theoretical physicist Julian Schwinger re-discovered the MMT in the context of his generating function approach to the angular momentum theory of many-particle systems. Louck writes:\n\nLet formula_1 be a complex matrix, and let formula_2 be formal variables. Consider a coefficient\n(Here the notation formula_4 means \"the coefficient of monomial formula_5 in formula_6\".) Let formula_7 be another set of formal variables, and let formula_8 be a diagonal matrix. Then\nwhere the sum runs over all nonnegative integer vectors formula_10,\nand formula_11 denotes the identity matrix of size formula_12.\n\nConsider a matrix\nCompute the coefficients \"G\"(2\"n\", 2\"n\", 2\"n\") directly from the definition:\n\nwhere the last equality follows from the fact that on the right-hand side we have the product of the following coefficients:\nwhich are computed from the binomial theorem. On the other hand, we can compute the determinant explicitly:\nTherefore, by the MMT, we have a new formula for the same coefficients:\n\nwhere the last equality follows from the fact that we need to use an equal number of times all three terms in the power. Now equating the two formulas for coefficients \"G\"(2\"n\", 2\"n\", 2\"n\") we obtain an equivalent version of Dixon's identity:\n\n\n"}
{"id": "4291061", "url": "https://en.wikipedia.org/wiki?curid=4291061", "title": "Mario Szegedy", "text": "Mario Szegedy\n\nMario Szegedy (born October 23, 1960) is a Hungarian-American computer scientist, professor of computer science at Rutgers University. He received his Ph.D. in computer science in 1989 from the University of Chicago. He held a Lady Davis Postdoctoral Fellowship at the Hebrew University, Jerusalem (1989–90), a postdoc at the University of Chicago, 1991–92, and a postdoc at Bell Laboratories (1992).\n\nSzegedy's research areas include computational complexity theory and quantum computing.\n\nHe was awarded the Gödel Prize twice, in 2001 and 2005, for his work on probabilistically checkable proofs and on the space complexity of approximating the frequency moments in streamed data.\n\n"}
{"id": "35695532", "url": "https://en.wikipedia.org/wiki?curid=35695532", "title": "McMullen problem", "text": "McMullen problem\n\nThe McMullen problem is an open problem in discrete geometry named after Peter McMullen.\n\nIn 1972, McMullen has proposed the following problem:\n\nUsing the Gale transform, this problem can be reformulated as:\n\nThe number formula_5, formula_1 are connected by the relationships\n\nAlso, by simple geometric observation, it can be reformulated as:\n\nThe relation between formula_12 and formula_13 is\n\nThe equivalent projective dual statement to the McMullen problem is to determine the largest number formula_1 such that every set of formula_1 hyperplanes in general position in \"d\"-dimensional real projective space form an arrangement of hyperplanes in which one of the cells is bounded by all of the hyperplanes.\n\nThis problem is still open. However, the bounds of formula_1 are in the following results:\n\nThe conjecture of this problem is formula_21, and it is true for \"d\" = 2, 3, 4.\n"}
{"id": "28306582", "url": "https://en.wikipedia.org/wiki?curid=28306582", "title": "NAS Award in Mathematics", "text": "NAS Award in Mathematics\n\nThe NAS Award in Mathematics is awarded by the U.S. National Academy of Sciences \"for excellence of research in the mathematical sciences published within the past ten years.\" It has been awarded every four years since 1988.\n\nSource: NAS\n\n"}
{"id": "695026", "url": "https://en.wikipedia.org/wiki?curid=695026", "title": "Noncrossing partition", "text": "Noncrossing partition\n\nIn combinatorial mathematics, the topic of noncrossing partitions has assumed some importance because of (among other things) its application to the theory of free probability. The set of all noncrossing partitions is one of many sets enumerated by the Catalan numbers. The number of noncrossing partitions of an \"n\"-element set with \"k\" blocks is found in the Narayana number triangle.\n\nA partition of a set \"S\" is a pairwise disjoint set of non-empty subsets, called \"parts\" or \"blocks\", whose union is all of \"S\". Consider a finite set that is linearly ordered, or (equivalently, for purposes of this definition) arranged in a cyclic order like the vertices of a regular \"n\"-gon. No generality is lost by taking this set to be \"S\" = { 1, ..., \"n\" }. A noncrossing partition of \"S\" is a partition in which no two blocks \"cross\" each other, i.e., if \"a\" and \"b\" belong to one block and \"x\" and \"y\" to another, they are not arranged in the order \"a x b y\". If one draws an arch based at \"a\" and \"b\", and another arch based at \"x\" and \"y\", then the two arches cross each other if the order is \"a x b y\" but not if it is \"a x y b\" or \"a b x y\". In the latter two orders the partition { { \"a\", \"b\" }, { \"x\", \"y\" } } is noncrossing.\n\nEquivalently, if we label the vertices of a regular \"n\"-gon with the numbers 1 through \"n\", the convex hulls of different blocks of the partition are disjoint from each other, i.e., they also do not \"cross\" each other.\nThe set of all non-crossing partitions of \"S\" are denoted formula_1. There is an obvious order isomorphism between formula_2 and formula_3 for two finite sets formula_4 with the same size. That is, formula_1 depends essentially only on the size of formula_6 and we denote by formula_7 the non-crossing partitions on \"any\" set of size \"n\".\n\nLike the set of all partitions of the set { 1, ..., \"n\" }, the set of all noncrossing partitions is a lattice when partially ordered by saying that a finer partition is \"less than\" a coarser partition. However, although it is a subset of the lattice of all partitions, it is \"not\" a sublattice of the lattice of all partitions, because the join operations do not agree. In other words, the finest partition that is coarser than both of two noncrossing partitions is not always the finest \"noncrossing\" partition that is coarser than both of them.\n\nUnlike the lattice of all partitions of the set, the lattice of all noncrossing partitions of a set is self-dual, i.e., it is order-isomorphic to the lattice that results from inverting the partial order (\"turning it upside-down\"). This can be seen by observing that each noncrossing partition has a complement. Indeed, every interval within this lattice is self-dual.\n\nThe lattice of noncrossing partitions plays the same role in defining free cumulants in free probability theory that is played by the lattice of \"all\" partitions in defining joint cumulants in classical probability theory. To be more precise, let formula_8 be a non-commutative probability space (See free probability for terminology.), formula_9 a non-commutative random variable with free cumulants formula_10. Then\n\nwhere formula_12 denotes the number of blocks of length formula_13 in the non-crossing partition formula_14.\nThat is, the moments of a non-commutative random variable can be expressed as a sum of free cumulants over the sum non-crossing partitions. This is the free analogue of the moment-cumulant formula in classical probability.\nSee also Wigner semicircle distribution.\n\n"}
{"id": "11877586", "url": "https://en.wikipedia.org/wiki?curid=11877586", "title": "Number sentence", "text": "Number sentence\n\nIn mathematics education, a number sentence is typically an equation or inequality expressed using numbers and mathematical symbols. The term is used in primary level mathematics teaching in the US, Canada, UK, Australia, New Zealand and South Africa.\n\nThe term is used as means of asking students to write down equations using simple mathematical symbols (numerals, the four main basic mathematical operators, equality symbol). Sometimes boxes or shapes are used to indicate unknown values. As such, number sentences are used to introduce students to notions of structure and elementary algebra prior to a more formal treatment of these concepts.\n\nA number sentence without unknowns is equivalent to a logical proposition expressed using the notation of arithmetic.\n\n\n\nSome students will use a direct computational approach. They will carry out the addition 26 + 39 = 65, put 65 = 26 + formula_1, and then find that formula_1 = 39.\n"}
{"id": "1339640", "url": "https://en.wikipedia.org/wiki?curid=1339640", "title": "Oversampling", "text": "Oversampling\n\nIn signal processing, oversampling is the process of sampling a signal at a sampling frequency significantly higher than the Nyquist rate. Theoretically, a bandwidth-limited signal can be perfectly reconstructed if sampled at the Nyquist rate or above it. The Nyquist rate is defined as twice the highest frequency component in the signal. Oversampling is capable of improving resolution, reducing noise and can be helpful in avoiding aliasing and phase distortion by relaxing anti-aliasing filter performance requirements.\n\nA signal is said to be oversampled by a factor of \"N\" if it is sampled at \"N\" times the Nyquist rate.\n\nThere are three main reasons for performing oversampling:\n\nOversampling can make it easier to realize analog anti-aliasing filters. Without oversampling, it is very difficult to implement filters with the sharp cutoff necessary to maximize use of the available bandwidth without exceeding the Nyquist limit. By increasing the bandwidth of the sampling system, design constraints for the anti-aliasing filter may be relaxed. Once sampled, the signal can be digitally filtered and downsampled to the desired sampling frequency. In modern integrated circuit technology, the digital filter associated with this downsampling are easier to implement than a comparable analog filter required by a non-oversampled system.\n\nIn practice, oversampling is implemented in order to reduce cost and improve performance of an analog-to-digital converter (ADC) or digital-to-analog converter (DAC). When oversampling by a factor of N, the dynamic range also increases a factor of N because there are N times as many possible values for the sum. However, the signal-to-noise ratio (SNR) increases by formula_1, because summing up uncorrelated noise increases its amplitude by formula_1, while summing up a coherent signal increases its average by N. As a result, the SNR increases by formula_1. \n\nFor instance, to implement a 24-bit converter, it is sufficient to use a 20-bit converter that can run at 256 times the target sampling rate. Combining 256 consecutive 20-bit samples can increase the signal-to-noise ratio (SNR) by a factor of 16, effectively adding 4 bits to the resolution and producing a single sample with 24-bit resolution. While with N=256 there is an increase in dynamic range by 8 bits, and the level of coherent signal increases by a factor of N, the noise changes by a factor of formula_1=16, so the net SNR improves by a factor of 16, 4 bits or 24 dB.\n\nThe number of samples required to get formula_5 bits of additional data precision is\n\nTo get the mean sample scaled up to an integer with formula_5 additional bits, the sum of formula_8 samples is divided by formula_9:\n\nThis averaging is only effective if the signal contains sufficient uncorrelated noise to be recorded by the ADC. If not, in the case of a stationary input signal, all formula_9 samples would have the same value and the resulting average would be identical to this value; so in this case, oversampling would have made no improvement. In similar cases where the ADC records no noise and the input signal is changing over time, oversampling improves the result, but to an inconsistent and unpredictable extent. Adding some dithering noise to the input signal can actually improve the final result because the dither noise allows oversampling to work to improve resolution. In many practical applications, a small increase in noise is well worth a substantial increase in measurement resolution. In practice, the dithering noise can often be placed outside the frequency range of interest to the measurement, so that this noise can be subsequently filtered out in the digital domain—resulting in a final measurement, in the frequency range of interest, with both higher resolution and lower noise.\n\nIf multiple samples are taken of the same quantity with uncorrelated noise added to each sample, then averaging \"N\" samples reduces the noise power by a factor of 1/\"N\". If, for example, we oversample by a factor of 4, the signal-to-noise ratio in terms of power improves by factor of 4 which corresponds to a factor of 2 improvement in terms of voltage.\n\nCertain kinds of A/D converters known as delta-sigma converters produce disproportionately more quantization noise in the upper portion of their output spectrum. By running these converters at some multiple of the target sampling rate, and low-pass filtering the oversampled signal down to half the target sampling rate, a final result with \"less\" noise (over the entire band of the converter) can be obtained. Delta-sigma converters use a technique called noise shaping to move the quantization noise to the higher frequencies.\n\nConsider a signal with a bandwidth or highest frequency of \"B\" = 100 Hz. The sampling theorem states that sampling frequency would have to be greater than 200 Hz. Sampling at four times that rate requires a sampling frequency of 800 Hz. This gives the anti-aliasing filter a transition band of 300 Hz ((\"f\"/2) − \"B\" = (800 Hz/2) − 100 Hz = 300 Hz) instead of 0 Hz if the sampling frequency was 200 Hz.\n\nAchieving an anti-aliasing filter with 0 Hz transition band is unrealistic whereas an anti-aliasing filter with a transition band of 300 Hz is not difficult to create.\n\nThe term oversampling is also used to denote a process used in the reconstruction phase of digital-to-analog conversion, in which an intermediate high sampling rate is used between the digital input and the analogue output. Here, samples are interpolated in the digital domain to add additional samples in between, thereby converting the data to a higher sample rate, which is a form of upsampling. When the resulting higher-rate samples are converted to analog, a less complex/expensive analog low pass filter is required to remove the high-frequency content, which will consist of reflected images of the real signal created by the zero-order hold of the digital-to-analog converter. Essentially, this is a way to shift some of the complexity of the filtering into the digital domain and achieves the same benefit as oversampling in analog-to-digital conversion.\n\n"}
{"id": "152420", "url": "https://en.wikipedia.org/wiki?curid=152420", "title": "Passphrase", "text": "Passphrase\n\nA passphrase is a sequence of words or other text used to control access to a computer system, program or data. A passphrase is similar to a password in usage, but is generally longer for added security. Passphrases are often used to control both access to, and operation of, cryptographic programs and systems, especially those that derive an encryption key from a passphrase. The origin of the term is by analogy with \"password\". The modern concept of passphrases is believed to have been invented by Sigmund N. Porter in 1982.\n\nConsidering that the entropy of written English is less than 1.1 bits per character, passphrases can be relatively weak. NIST has estimated that the 23-character passphrase \"IamtheCapitanofthePina4\" contains a 45-bit strength. The equation employed here is:\n\nUsing this guideline, to achieve the 80-bit strength recommended for high security (non-military) by NIST, a passphrase would need to be 58 characters long, assuming a composition that includes uppercase and alphanumeric.\n\nThere is room for debate regarding the applicability of this equation, depending on the number of bits of entropy assigned. For example, the characters in five-letter words each contain 2.3 bits of entropy, which would mean only a 35-character passphrase is necessary to achieve 80 bit strength.\n\nIf the words or components of a passphrase may be found in a language dictionary—especially one available as electronic input to a software program—the passphrase is rendered more vulnerable to dictionary attack. This is a particular issue if the entire phrase can be found in a book of quotations or phrase compilations. However, the required effort (in time and cost) can be made impracticably high if there are enough words in the passphrase and how randomly they are chosen and ordered in the passphrase. The number of combinations which would have to be tested under sufficient conditions make a dictionary attack so difficult as to be infeasible. These are difficult conditions to meet, and selecting at least one word that cannot be found in \"any\" dictionary significantly increases passphrase strength.\n\nIf passphrases are chosen by humans they are usually biased by frequency of particular words in natural language. In the case of four word phrases, actual entropy rarely exceeds 30 bits. On the other hand, user-selected passwords tend to be much weaker than that and encouraging users to use even 2-word passphrases may be able to raise entropy from below 10 bits to over 20 bits.\n\nFor example, the widely used cryptography standard OpenPGP requires that a user make up a passphrase that must be entered whenever decrypting or signing messages. Internet services like Hushmail provide free encrypted e-mail or file sharing services, but the security present depends almost entirely on the quality of the chosen passphrase.\n\nPassphrases differ from passwords. A password is usually short—six to ten characters. Such passwords may be adequate for various applications (if frequently changed, if chosen using an appropriate policy, if not found in dictionaries, if sufficiently random, and/or if the system prevents online guessing, etc.) such as:\n\nBut passwords are typically not safe to use as keys for standalone security systems (e.g., encryption systems) that expose data to enable offline password guessing by an attacker. Passphrases are theoretically stronger, and so should make a better choice in these cases. First, they usually are (and always should be) much longer—20 to 30 characters or more is typical—making some kinds of brute force attacks entirely impractical. Second, if well chosen, they will not be found in any phrase or quote dictionary, so such dictionary attacks will be almost impossible. Third, they can be structured to be more easily memorable than passwords without being written down, reducing the risk of hardcopy theft. However, if a passphrase is not protected appropriately by the authenticator and the clear-text passphrase is revealed its use is no better than other passwords. For this reason it is recommended that passphrases not be reused across different or unique sites and services.\n\nIn 2012, two Cambridge University researchers analyzed passphrases from the Amazon PayPhrase system and found that a significant percentage are easy to guess due to common cultural references such as movie names and sports teams, losing much of the potential of using long passwords.\n\nWhen used in cryptography, commonly the password protects a long (machine generated) key, and the key protects the data. The key is so long a brute force attack (directly on the data) is impossible. A key derivation function is used, involving many thousands of iterations (salted & hashed), to slow down password cracking attacks.\n\nTypical advice about choosing a passphrase includes suggestions that it should be:\n\nOne method to create a strong passphrase is to use dice to select words at random from a long list, a technique often referred to as diceware. While such a collection of words might appear to violate the \"not from any dictionary\" rule, the security is based entirely on the large number of possible ways to choose from the list of words and not from any secrecy about the words themselves. For example, if there are 7776 words in the list and six words are chosen randomly, then there are \"7776 = 221073919720733357899776\" combinations, providing about 78 bits of entropy. (The number \"7776\" was chosen to allow words to be selected by throwing five dice. \"7776 = 6\") Random word sequences may then be memorized using techniques such as the memory palace.\n\nAnother is to choose two phrases, turn one into an acronym, and include it in the second, making the final passphrase. For instance, using two English language typing exercises, we have the following. \"The quick brown fox jumps over the lazy dog\", becomes \"tqbfjotld\". Including it in, \"Now is the time for all good men to come to the aid of their country\", might produce, \"Now is the time for all good tqbfjotld to come to the aid of their country\" as the passphrase.\n\nThere are several points to note here, all relating to why this example passphrase is not a good one.\n\nThe PGP Passphrase FAQ suggests a procedure that attempts a better balance between theoretical security and practicality than this example. All procedures for picking a passphrase involve a tradeoff between security and ease of use; security should be at least \"adequate\" while not \"too seriously\" annoying users. Both criteria should be evaluated to match particular situations.\n\nAnother supplementary approach to frustrating brute-force attacks is to derive the key from the passphrase using a deliberately slow hash function, such as PBKDF2 as described in RFC 2898.\n\nIf backward compatibility with Microsoft LAN Manager is not needed, in versions of Windows NT (including Windows 2000, Windows XP and later), a passphrase can be used as a substitute for a Windows password. If the passphrase is longer than 14 characters, this will also avoid the generation of a \"very\" weak LM hash.\n\nIn recent versions of Unix-like operating systems such as Linux, OpenBSD, NetBSD, Solaris and FreeBSD, up to 255-character passphrases can be used.\n\n\n"}
{"id": "4132316", "url": "https://en.wikipedia.org/wiki?curid=4132316", "title": "Peter B. Andrews", "text": "Peter B. Andrews\n\nPeter Bruce Andrews (born 1937) is an American mathematician and Professor of Mathematics, Emeritus at Carnegie Mellon University in Pittsburgh, Pennsylvania, and the creator of the mathematical logic Q. He received his Ph.D. from Princeton University in 1964 under the tutelage of Alonzo Church. He received the Herbrand Award in 2003. His research group designed the TPS automated theorem prover. A subsystem ETPS (Educational Theorem Proving System) of TPS is used to help students learn logic by interactively constructing natural deduction proofs.\n\n\n"}
{"id": "48473330", "url": "https://en.wikipedia.org/wiki?curid=48473330", "title": "Peter van Emde Boas", "text": "Peter van Emde Boas\n\nPeter van Emde Boas (born 3 April 1945, Amsterdam) is a Dutch computer scientist and professor at the University of Amsterdam. He gained his doctorate in 1974 under Adriaan van Wijngaarden.\n\nThe Van Emde Boas tree is named after him.\n\n"}
{"id": "17101042", "url": "https://en.wikipedia.org/wiki?curid=17101042", "title": "Polar sine", "text": "Polar sine\n\nIn geometry, the polar sine generalizes the sine function of angle to the vertex angle of a polytope. It is denoted by psin.\n\nLet v, ..., v, for \"n\" ≥ 2, be non-zero Euclidean vectors in \"n\"-dimensional space (ℝ) that are directed from a vertex of a parallelotope, forming the edges of the parallelotope. The polar sine of the vertex angle is:\n\nwhere the numerator is the determinant\n\nequal to the hyper volume of the parallelotope with vector edges\n\nand in the denominator the \"n\"-fold product\n\nof the magnitudes ||v|| of the vectors equals the hypervolume of the \"n\"-dimensional hyperrectangle, with edges equal to the magnitudes of the vectors ||v||, ||v||, ... ||v|| (not the vectors themselves). Also see Ericksson.\n\nThe parallelotope is like a \"squashed hyperrectangle\", so it has less hypervolume than the hyperrectangle, meaning (see image for the 3d case):\n\nand since this ratio can be negative, psin is always bounded between −1 and +1 by the inequalities:\n\nas for the ordinary sine, with either bound only being reached in case all vectors are mutually orthogonal.\n\nIn case \"n\" = 2, the polar sine is the ordinary sine of the angle between the two vectors.\n\nA non-negative version of the polar sine exists, which works in any -dimensional space for . In this case, the numerator in the definition is given as\nwhere the superscript T indicates matrix transposition. In the case that \"m\"=\"n\", the value of Ω for this non-negative definition of the polar sine is the absolute value of the Ω from the signed version of the polar sine given previously.\n\n\nIf the dimension of the space is more than \"n\" then the polar sine is non-negative and is unchanged whenever two of the vectors v and v are interchanged. Otherwise, it changes sign whenever two vectors are interchanged - due to the antisymmetry of row-exchanging in the determinant:\n\n\nThe polar sine does not change if all of the vectors v, ..., v are multiplied by positive constants \"c\", due to factorization:\n\nIf an odd number of these constants are instead negative, then the sign of the polar sine will change; however, its absolute value will remain unchanged.\n\n\nIf the vectors are not linearly independent, the polar sine will be zero. This will always be so in the degenerate case that the number of dimensions is strictly less than the number of vectors .\n\nPolar sines were investigated by Euler in the 18th century.\n\n"}
{"id": "3710507", "url": "https://en.wikipedia.org/wiki?curid=3710507", "title": "Proof of impossibility", "text": "Proof of impossibility\n\nA proof of impossibility, also known as negative proof, proof of an impossibility theorem, or negative result, is a proof demonstrating that a particular problem cannot be solved, or cannot be solved in general. Often proofs of impossibility have put to rest decades or centuries of work attempting to find a solution. To prove that something is impossible is usually much harder than the opposite task; it is necessary to develop a theory. Impossibility theorems are usually expressible as universal propositions in logic (see universal quantification).\n\nOne of the most famous proofs of impossibility was the 1882 proof of Ferdinand von Lindemann, showing that the ancient problem of squaring the circle cannot be solved, because the number is transcendental (non-algebraic) and only a subset of the algebraic numbers can be constructed by compass and straightedge. Two other classical problems—trisecting the general angle and doubling the cube—were also proved impossible in the nineteenth century.\n\nA problem arising in the sixteenth century was that of creating a general formula using radicals expressing the solution of any polynomial equation of fixed degree \"k\", where \"k\" ≥ 5. In the 1820s, the Abel–Ruffini theorem showed this to be impossible using concepts such as solvable groups from Galois theory, a new subfield of abstract algebra.\n\nAmong the most important proofs of impossibility of the 20th century, were those related to undecidability, which showed that there are problems that cannot be solved in general by any algorithm at all. The most famous is the halting problem.\n\nIn computational complexity theory, techniques like relativization (see oracle machine) provide \"weak\" proofs of impossibility excluding certain proof techniques. Other techniques like proofs of completeness for a complexity class provide evidence for the difficulty of problems by showing them to be just as hard to solve as other known problems that have proved intractable.\n\nOne widely used type of impossibility proof is proof by contradiction. In this type of proof it is shown that if something, such as a solution to a particular class of equations, were possible, then two mutually contradictory things would be true, such as a number being both even and odd. The contradiction implies that the original premise is impossible.\n\nOne type of proof by contradiction is proof by descent. Here it is postulated that something is possible, such as a solution to a class of equations, and that therefore there must be a smallest solution; then starting from the allegedly smallest solution, it is shown that a smaller solution can be found, contradicting the premise that the former solution was the smallest one possible. Thus the premise that a solution exists must be false.\n\nThis method of proof can also be interpreted slightly differently, as the method of \"infinite descent\". One postulates that a positive integer solution exists, whether or not it is the smallest one, and one shows that based on this solution a smaller solution must exist. But by mathematical induction it follows that a still smaller solution must exist, then a yet smaller one, and so on for an infinite number of steps. But this contradicts the fact that one cannot find smaller and smaller positive integers indefinitely; the contradiction implies that the premise that a solution exists is wrong.\n\nThere are two alternative methods of proving wrong a conjecture that something is impossible: by counterexample (constructive proof) and by logical contradiction (non-constructive proof).\n\nThe obvious way to disprove an impossibility conjecture by providing a single counterexample. For example, Euler proposed that at least \"n\" different \"n\" powers were necessary to sum to yet another \"n\" power. The conjecture was disproved in 1966 with a counterexample involving a count of only four different 5th powers summing to another fifth power:\nA proof by counterexample is a constructive proof.\n\nIn contrast, a non-constructive proof that something is \"not\" impossible proceeds by showing it is logically contradictory for \"all\" possible counterexamples to be invalid: At least \"one\" of the items on a list of possible counterexamples must actually be a valid counterexample to the impossibility conjecture. For example, a conjecture that it is impossible for an irrational power raised to an irrational power to be rational was disproved by showing that one of two possible counterexamples must be a valid counterexample, without showing which one it is.\n\nThe proof by Pythagoras (or more likely one of his students) about 500 BCE has had a profound effect on mathematics. It shows that the square root of 2 cannot be expressed as the ratio of two integers (counting numbers). The proof bifurcated \"the numbers\" into two non-overlapping collections—the rational numbers and the irrational numbers. This bifurcation was used by Cantor in his diagonal method, which in turn was used by Turing in his proof that the \"Entscheidungsproblem\" (the decision problem of Hilbert) is undecidable.\n\nProofs followed for various square roots of the primes up to 17.\n\nThere is a famous passage in Plato's \"Theaetetus\" in which it is stated that Teodorus (Plato's teacher) proved the irrationality of\ntaking all the separate cases up to the root of 17 square feet ... .\nA more general proof now exists that:\n\nThat is, it is impossible to express the \"m\"th root of an integer \"N\" as the ratio of two integers \"a\" and \"b\" that share no common prime factor except in cases in which \"b\" = 1.\n\nThree famous questions of Greek geometry were how:\n\n\nFor more than 2,000 years unsuccessful attempts were made to solve these problems; at last, in the 19th century it was proved that the desired constructions are logically impossible.\n\nA fourth problem of the ancient Greeks was to construct an equilateral polygon with a specified number \"n\" of sides, beyond the basic cases \"n\" = 3, 4, 5 that they knew how to construct.\n\nAll of these are problems in Euclidean construction, and Euclidean constructions can be done only if they involve only Euclidean numbers (by definition of the latter) (Hardy and Wright p. 159). Irrational numbers can be Euclidean. A good example is the irrational number the square root of 2. It is simply the length of the hypotenuse of a right triangle with legs both one unit in length, and it can be constructed with straightedge and compass. But it was proved centuries after Euclid that Euclidean numbers cannot involve any operations other than addition, subtraction, multiplication, division, and the extraction of square roots.\n\nBoth trisecting the general angle and doubling the cube require taking cube roots, which are not constructible numbers by compass and straightedge.\n\nformula_2 is not a Euclidean number ... and therefore it is impossible to construct, by Euclidean methods a length equal to the circumference of a circle of unit diameter\n\nA proof exists to demonstrate that any Euclidean number is an algebraic number—a number that is the solution to some polynomial equation. Therefore, because formula_2 was proved in 1882 to be a transcendental number and thus by definition not an algebraic number, it is not a Euclidean number. Hence the construction of a length formula_2 from a unit circle is impossible, and the circle cannot be squared.\n\nThe Gauss-Wantzel theorem showed in 1837 that constructing an equilateral \"n\"-gon is impossible for most values of \"n\".\n\nNagel and Newman consider the question raised by the parallel postulate to be \"...perhaps the most significant development in its long-range effects upon subsequent mathematical history\" (p. 9).\n\nThe question is: can the axiom that two parallel lines \"...will not meet even 'at infinity'\" (footnote, ibid) be derived from the other axioms of Euclid's geometry? It was not until work in the nineteenth century by \"... Gauss, Bolyai, Lobachevsky, and Riemann, that the impossibility of deducing the parallel axiom from the others was demonstrated. This outcome was of the greatest intellectual importance. ...a \"proof\" can be given of the \"impossibility of proving\" certain propositions [in this case, the parallel postlate] within a given system [in this case, Euclid's first four postulates]\". (p. 10)\n\nFermat's Last Theorem was conjectured by Pierre de Fermat in the 1600s, states the impossibility of finding solutions in positive integers for the equation formula_5 with formula_6. Fermat himself gave a proof for the \"n\" = 4 case using his technique of infinite descent, and other special cases were subsequently proved, but the general case was not proved until 1994 by Andrew Wiles.\n\nThis profound paradox presented by Jules Richard in 1905 informed the work of Kurt Gödel (cf Nagel and Newman p. 60ff) and Alan Turing. A succinct definition is found in \"Principia Mathematica\":\nKurt Gödel considered his proof to be “an analogy” of Richard's paradox, which he called “\"Richard's antinomy\"”. See more below about Gödel's proof.\n\nAlan Turing constructed this paradox with a machine and proved that this machine could not answer a simple question: will this machine be able to determine if any machine (including itself) will become trapped in an unproductive ‘infinite loop’ (i.e. it fails to continue its computation of the diagonal number).\n\nTo quote Nagel and Newman (p. 68), \"Gödel's paper is difficult. Forty-six preliminary definitions, together with several important preliminary theorems, must be mastered before the main results are reached\" (p. 68). In fact, Nagel and Newman required a 67-page introduction to their exposition of the proof. But if the reader feels strong enough to tackle the paper, Martin Davis observes that \"This remarkable paper is not only an intellectual landmark, but is written with a clarity and vigor that makes it a pleasure to read\" (Davis in Undecidable, p. 4). It is recommended that most readers see Nagel and Newman first.\n\nSo what did Gödel prove? In his own words:\n\nGödel compared his proof to \"Richard's antinomy\" (an \"antinomy\" is a contradiction or a paradox; for more see Richard's paradox):\n\n\nA number of similar undecidability proofs appeared soon before and after Turing's proof:\n\n\nFor an exposition suitable for non-specialists see Beltrami p. 108ff. Also see Franzen Chapter 8 pp. 137–148, and Davis pp. 263–266. Franzén's discussion is significantly more complicated than Beltrami's and delves into Ω—Gregory Chaitin's so-called \"halting probability\". Davis's older treatment approaches the question from a Turing machine viewpoint. Chaitin has written a number of books about his endeavors and the subsequent philosophic and mathematical fallout from them.\n\nA string is called \"(algorithmically) random\" if it cannot be produced from any shorter computer program. While most strings are random, no particular one can be proved so, except for finitely many short ones:\n\nBeltrami observes that \"Chaitin's proof is related to a paradox posed by Oxford librarian G. Berry early in the twentieth century that asks for 'the smallest positive integer that cannot be defined by an English sentence with fewer than 1000 characters.' Evidently, the shortest definition of this number must have at least 1000 characters. However, the sentence within quotation marks, which is itself a definition of the alleged number is less than 1000 characters in length!\" (Beltrami, p. 108)\n\nThe question \"Does any arbitrary \"Diophantine equation\" have an integer solution?\" is undecidable.That is, it is impossible to answer the question for all cases.\n\nFranzén introduces Hilbert's tenth problem and the MRDP theorem (Matiyasevich-Robinson-Davis-Putnam theorem) which states that \"no algorithm exists which can decide whether or not a Diophantine equation has \"any\" solution at all\". MRDP uses the undecidability proof of Turing: \"... the set of solvable Diophantine equations is an example of a computably enumerable but not decidable set, and the set of unsolvable Diophantine equations is not computably enumerable\" (p. 71).\n\nIn political science, Arrow's impossibility theorem states that it is impossible to devise a voting system that satisfies a set of five specific axioms. This theorem is proved by showing that four of the axioms together imply the opposite of the fifth.\n\nIn economics, Holmström's theorem is an impossibility theorem proving that no incentive system for a team of agents can satisfy all of three desirable criteria.\n\nIn natural science, impossibility assertions (like other assertions) come to be widely accepted as overwhelmingly probable rather than considered proved to the point of being unchallengeable. The basis for this strong acceptance is a combination of extensive evidence of something not occurring, combined with an underlying theory, very successful in making predictions, whose assumptions lead logically to the conclusion that something is impossible.\n\nTwo examples of widely accepted impossibilities in physics are perpetual motion machines, which violate the law of conservation of energy, and exceeding the speed of light, which violates the implications of special relativity. Another is the uncertainty principle of quantum mechanics, which asserts the impossibility of simultaneously knowing both the position and the momentum of a particle. Also Bell's theorem: no physical theory of local hidden variables can ever reproduce all of the predictions of quantum mechanics.\n\nWhile an impossibility assertion in science can never be absolutely proved, it could be refuted by the observation of a single counterexample. Such a counterexample would require that the assumptions underlying the theory that implied the impossibility be re-examined.\n\n\n"}
{"id": "518991", "url": "https://en.wikipedia.org/wiki?curid=518991", "title": "Quadratic integral", "text": "Quadratic integral\n\nIn mathematics, a quadratic integral is an integral of the form\n\nIt can be evaluated by completing the square in the denominator.\n\nAssume that the discriminant \"q\" = \"b\" − 4\"ac\" is positive. In that case, define \"u\" and \"A\" by\n\nand\n\nThe quadratic integral can now be written as\n\nThe partial fraction decomposition\n\nallows us to evaluate the integral:\n\nThe final result for the original integral, under the assumption that \"q\" > 0, is\n\nIn case the discriminant \"q\" = \"b\" − 4\"ac\" is negative, the second term in the denominator in\n\nis positive. Then the integral becomes\n\n"}
{"id": "55283391", "url": "https://en.wikipedia.org/wiki?curid=55283391", "title": "Seriation (statistics)", "text": "Seriation (statistics)\n\nIn combinatorial data analysis, seriation is the process of finding an arrangement of all objects in a set, in a linear order, given a loss function. The main goal is exploratory, to reveal structural information.\n"}
{"id": "30696913", "url": "https://en.wikipedia.org/wiki?curid=30696913", "title": "Sherry Gong", "text": "Sherry Gong\n\nSherry Gong is an American mathematician specializing in low-dimensional topology and known for her performance on mathematics competitions. She is a Hedrick Assistant Adjunct Professor at the University of California, Los Angeles.\n\nGong was born in New York City as the daughter of two mathematics professors, Guihua Gong and Liangqing Li, both affiliated with the University of Puerto Rico. She grew up in Toronto, Puerto Rico, and New Hampshire.\n\nShe received her AB in Mathematics from Harvard College, and \nbecame a graduate student in mathematics at MIT. She completed her Ph.D. there in 2018; her dissertation, \"Results on Spectral Sequences for Monopole and Singular Instanton Floer Homologies\", was supervised by Tomasz Mrowka.\n\nGong is the second U.S. woman (after Alison Miller won in 2004) to win a gold medal in the International Mathematical Olympiad, which Gong won in 2007, earning a tie for seventh place out of 536 participants (she scored a 32). She was the only woman on the U.S. team that year, and also one of only three women ever to make the U.S. team. She also tied for first place in the China Mathematical Olympiad for Girls in 2007. Gong attended a mathematics Olympiad for the first time when she was in the sixth grade — the 3rd Olympiada Matematica de Centroamerica y el Caribe, in Colombia. There she received a silver medal and also a special award for the most original solution. It was the first such award in the history of that Olympiad.\n\nGong participated in IMO five times, winning HM in 2002, bronze in 2003, silver in 2004 and 2005 and gold in 2007. In 2005 she was named the 2005 Clay Olympiad Scholar; the Clay Olympiad Scholar Award recognizes the most original solution to a problem on the US American Mathematics Olympiad (USAMO). In 2006 she earned a silver medal at the 2006 International Physics Olympiad. She was a winner (top twelve) at the United States of America Mathematical Olympiad in 2005, 2006, and 2007, including placing 2nd in 2007.\n\nAs a Harvard freshman, Gong scored over 100 in Harvard’s famous problem solving course, Math 55, which required perfect scores on all assignments, tests, bonus problems, and the final exam. In 2010 Gong helped coach the U.S. team that competed in the China Girls’ Mathematical Olympiad; five team members won gold medals. In 2011 she won the Alice T. Schafer Prize for Excellence in Mathematics by an Undergraduate Woman.\n"}
{"id": "318060", "url": "https://en.wikipedia.org/wiki?curid=318060", "title": "Special Interest Group", "text": "Special Interest Group\n\nA Special Interest Group (SIG) is a community within a larger organization with a shared interest in advancing a specific area of knowledge, learning or technology where members cooperate to affect or to produce solutions within their particular field, and may communicate, meet, and organize conferences. The term was used in 1961 by the Association for Computing Machinery (ACM), an academic and professional computer society. SIG was later popularized on CompuServe, an early online service provider, where SIGs were a section of the service devoted to particular interests.\n\nThe ACM includes many SIGs, some starting as smaller \"Special Interest Committees\" (SICs) and formed the first group in 1961. ACM supports further subdivision within SIGs for more impromptu informal discussion groups at conferences which are called Birds of a Feather (BoF).\nACM's Special Interest Groups (SIGs) represent major areas of computing, addressing the interests of technical communities that drive innovation. SIGs offer a wealth of conferences, publications and activities focused on specific computing sub-disciplines. They enable members to share expertise, discovery and best practices.\nThe Mathematical Association of America has 14 SIGs ranging from the Arts to the Web (for instruction).\n\nOrganizations that are not technical may also have Special Interest Groups, which are normally focused on a mutual interest or shared characteristic of a subset of members of the organization. An important example for this are trade unions. For identity-based advocacy groups, see identity politics. The Japan Association for Language Teaching (JALT) has several SIGs. Together they organize a Pan-SIG conference each year.\n\nPolitical Interest Groups\n\nThese interest groups represent interests that support and lobby for areas of special need. For example, the \"Sierra Club\" focuses on protecting the environment as well as the wild places on earth. The focus also on the education of people on preservation of the environment. Groups like this advocate for their special interest and form a base of support that will assist them in moving along their public issue. These political \"entrepreneurs\" are the classic view of the policy maker. An also much needed to these special interest groups is the patron. These patrons provide capital as well as support for the interest groups. The cause has to be one that many support and can get behind due to the quantity of other causes that lobby their patrons for support. Many of these dominant groups have sub-supporting groups that lobby for more specific issues, but assist in the overall cause.\n\n\n"}
{"id": "5386671", "url": "https://en.wikipedia.org/wiki?curid=5386671", "title": "Structure from motion", "text": "Structure from motion\n\nStructure from motion (SfM) is a photogrammetric range imaging technique for estimating three-dimensional structures from two-dimensional image sequences that may be coupled with local motion signals. It is studied in the fields of computer vision and visual perception. In biological vision, SfM refers to the phenomenon by which humans (and other living creatures) can recover 3D structure from the projected 2D (retinal) motion field of a moving object or scene.\n\nHumans perceive a lot of information about the three-dimensional structure in their environment by moving through it. When the observer moves and the objects around the observer move, information is obtained from images sensed over time.\n\nFinding structure from motion presents a similar problem to finding structure from stereo vision. In both instances, the correspondence between images and the reconstruction of 3D object needs to be found.\n\nTo find correspondence between images, features such as corner points (edges with gradients in multiple directions) are tracked from one image to the next. One of the most widely used feature detectors is the scale-invariant feature transform (SIFT). It uses the maxima from a difference-of-Gaussians (DOG) pyramid as features. The first step in SIFT is finding a dominant gradient direction. To make it rotation-invariant, the descriptor is rotated to fit this orientation. Another common feature detector is the SURF (\"speeded-up robust features\"). In SURF, the DOG is replaced with a Hessian matrix-based blob detector. Also, instead of evaluating the gradient histograms, SURF computes for the sums of gradient components and the sums of their absolute values. The features detected from all the images will then be matched. One of the matching algorithms that track features from one image to another is the Lukas–Kanade tracker.\n\nSometimes some of the matched features are incorrectly matched. This is why the matches should also be filtered. RANSAC (random sample consensus) is the algorithm that is usually used to remove the outlier correspondences. In the paper of Fischler and Bolles, RANSAC is used to solve the \"location determination problem\" (LDP), where the objective is to determine the points in space that project onto an image into a set of landmarks with known locations.\n\nThe feature trajectories over time are then used to reconstruct their 3D positions and the camera's motion.\nAn alternative is given by so-called direct approaches, where geometric information (3D structure and camera motion) is directly estimated from the images, without intermediate abstraction to features or corners.\n\nThere are several approaches to structure from motion. In incremental SFM, camera poses are solved for and added one by one to the collection. In global SFM , the poses of all cameras are solved for at the same time. A somewhat intermediate approach is out-of-core SFM, where several partial reconstructions are computed that are then integrated into a global solution.\n\nStructure from Motion photogrammetry with multi-view stereo provides hyperscale landform models using images acquired from a range of digital cameras and optionally a network of ground control points. The technique is not limited in temporal frequency and can provide point cloud data comparable in density and accuracy to those generated by terrestrial and airborne laser scanning at a fraction of the cost.. Structure from motion is also useful in remote or rugged environments where terrestrial laser scanning is limited by equipment portability and airborne laser scanning is limited by terrain roughness causing loss of data and image foreshortening.The technique has been applied in many settings such as rivers, badlands, sandy coastlines, fault zones, landslides, and coral reef settings. SfM has been also successfully applied for the characterization of rock masses through the determination of some properties a the orientation, persistence, etc. of discontinuities. A full range of digital cameras can be utilized, including digital SLR's, compact digital cameras and even smart phones. Generally though, higher accuracy data will be achieved with more expensive cameras, which include lenses of higher optical quality. The technique therefore offers exciting opportunities to characterize surface topography in unprecedented detail and, with multi-temporal data, to detect elevation, position and volumetric changes that are symptomatic of earth surface processes . Structure from Motion can be placed in the context of other digital surveying methods.\n\nCultural heritage is present everywhere. Its structural control, documentation and conservation is one of humanity's main duties (UNESCO). Under this point of view, SfM is used in order to properly estimate situations as well as planning and maintenance efforts and costs, control and restoration.\nBecause serious constraints often exist connected to the accessibility of the site and impossibility to install invasive surveying pillars that did not permit the use of traditional surveying routines (like total stations), SfM provides a non-invasive approach for the structure, without the direct interaction between the structure and any operator. The use is accurate as only qualitative considerations are needed. It is fast enough to respond to the monument’s immediate management needs.\nThe first operational phase is an accurate preparation of the photogrammetric surveying where is established the relation between best distance from the object, focal length, the ground sampling distance (GSD) and the sensor’s resolution. With this information the programmed photographic acquisitions must be made using vertical overlapping of at least 60% (figure 02).\n\n\n\nC++\n\nMatlab\n\nPython\n\n"}
{"id": "12413580", "url": "https://en.wikipedia.org/wiki?curid=12413580", "title": "Subjective logic", "text": "Subjective logic\n\nSubjective logic is a type of probabilistic logic that explicitly takes uncertainty and source trust into account. In general, subjective logic is suitable for modeling and analysing situations involving uncertainty and relatively unreliable sources. For example, it can be used for modeling and analysing trust networks and Bayesian networks.\n\nArguments in subjective logic are subjective opinions about state variables which can take values from a domain (aka state space), where a state value can be thought of as a proposition which can be true or false. A binomial opinion applies to a binary state variable, and can be represented as a Beta PDF (Probability Density Function). A multinomial opinion applies to a state variable of multiple possible values, and can be represented as a Dirichlet PDF (Probability Density Function). Through the correspondence between opinions and Beta/Dirichlet distributions, subjective logic provides an algebra for these functions. Opinions are also related to the belief representation in Dempster–Shafer belief theory.\n\nA fundamental aspect of the human condition is that nobody can ever determine with absolute certainty whether a proposition about the world is true or false. In addition, whenever the truth of a proposition is expressed, it is always done by an individual, and it can never be considered to represent a general and objective belief. These philosophical ideas are directly reflected in the mathematical formalism of subjective logic.\n\nSubjective opinions express subjective beliefs about the truth of state values/propositions with degrees of uncertainty, and can explicitly indicate the source of belief whenever required. An opinion is usually denoted as formula_1 where formula_2 is the source of the opinion, and formula_3 is the state variable to which the opinion applies. The variable formula_3 can take values from a domain (also called state space) e.g. denoted as formula_5. The values of a domain are assumed to be exhaustive and mutually disjoint, and sources are assumed to have a common semantic interpretation of a domain. The source and variable are attributes of an opinion. Indication of the source can be omitted whenever irrelevant.\n\nLet formula_6 be a value in a binary domain. A binomial opinion about the truth of value formula_6 is the ordered quadruple formula_8 where:\n\nThese components satisfy formula_9 and formula_10. The characteristics of various opinion classes are listed below.\n\nThe projected probability of a binomial opinion is defined as formula_11.\n\nBinomial opinions can be represented on an equilateral triangle as shown below. A point inside the triangle represents a formula_12 triple. The \"b\",\"d\",\"u\"-axes run from one edge to the opposite vertex indicated by the Belief, Disbelief or Uncertainty label. For example, a strong positive opinion is represented by a point towards the bottom right Belief vertex. The base rate, also called the prior probability, is shown as a red pointer along the base line, and the projected probability, formula_13, is formed by projecting the opinion onto the base, parallel to the base rate projector line. Opinions about three values/propositions X, Y and Z are visualized on the triangle to the left, and their equivalent Beta PDFs (Probability Density Functions) are visualized on the plots to the right. The numerical values and verbal qualitative descriptions of each opinion are also shown.\nThe Beta PDF is normally denoted as formula_14 where formula_15 and formula_16 are its two strength parameters. The Beta PDF of a binomial opinion formula_17 is the function\nformula_18\n\nLet formula_3 be a variable which can take values formula_20. A multinomial opinion over formula_3 is the composite\ntuple formula_22, where formula_23 is a belief mass distribution over the possible values of formula_3, formula_25 is the uncertainty mass, and formula_26 is a base rate distribution over the possible values of formula_3. These parameters satisfy formula_28 and formula_29 as well as formula_30.\n\nVisualising multinomial opinions can be challenging. Trinomial opinions can be simply visualised as points inside a tetrahedron. Opinions with dimensions larger than trinomial do not lend themselves to simple visualisation.\n\nDirichlet PDFs are normally denoted as formula_31 where formula_32 is a probability distribution over the values of formula_33, and formula_34 are the strength parameters. The Dirichlet PDF of a multinomial opinion formula_35 is the function\nformula_36 where the strength parameters are given by formula_37.\n\nMost operators in the table below are generalisations of binary logic and probability operators. For example \"addition\" is simply a generalisation of addition of probabilities. Some operators are only meaningful for combining binomial opinions, and some also apply to multinomial opinion. Most operators are binary, but \"complement\" is unary, and \"abduction\" is ternary. See the referenced puplications for mathematical details of each operator.\nY}=\\omega^{A}_{Y|X}\\;\\widetilde{\\circledcirc}\\;(a_{X},\\omega^{A}_{Y})\\,\\!</math>\n\nTransitive source combination can be denoted in a compact or expanded form. For example, the transitive trust path from analyst/source formula_2 via source formula_42 to the variable formula_3 can be denoted as formula_44 in compact form, or as formula_45 in expanded form. Here, formula_46 expresses that formula_47 has some trust/distrust in source formula_48, whereas formula_49 expresses that formula_48 has an opinion about the state of variable formula_33 which is given as an advice to formula_47. The expanded form is the most general, and corresponds directly to the way subjective logic expressions are formed with operators.\n\nIn case the argument opinions are equivalent to Boolean TRUE or FALSE, the result of any subjective logic operator is always equal to that of the corresponding propositional/binary logic operator. Similarly, when the argument opinions are equivalent to traditional probabilities, the result of any subjective logic operator is always equal to that of the corresponding probability operator (when it exists).\n\nIn case the argument opinions contain degrees of uncertainty, the operators involving multiplication and division (including deduction, abduction and Bayes' theorem) will produce derived opinions that always have correct projected probability but possibly with approximate variance when seen as Beta/Dirichlet PDFs.\nAll other operators produce opinions where the projected probabilities and the variance are always analytically correct.\n\nDifferent logic formulas that traditionally are equivalent in propositional logic do not necessarily have equal opinions. For example formula_53 in general although the distributivity of conjunction over disjunction, expressed as formula_54, holds in binary propositional logic. This is no surprise as the corresponding probability operators are also non-distributive. However, multiplication is distributive over addition, as expressed by formula_55. De Morgan's laws are also satisfied as e.g. expressed by formula_56.\n\nSubjective logic gives very efficient computation of mathematically complex models. This is possible by approximating the analytically correct functions whenever needed. While it is relatively simple to analytically multiply two Beta PDFs in the form of a joint Beta PDF, anything more complex than that quickly becomes intractable. When combining two Beta PDFs with some operator/connective, the analytical result is not always a Beta PDF and can involve hypergeometric series. In such cases, subjective logic always approximates the result as an opinion that is equivalent to a Beta PDF.\n\nSubjective logic is applicable when the situation to be analysed is characterised by considerable uncertainty and incomplete knowledge. In this way, subjective logic becomes a probabilistic logic for uncertain probabilities. The advantage is that uncertainty is preserved throughout the analysis and is made explicit in the results so that it is possible to distinguish between certain and uncertain conclusions.\n\nThe modelling of trust networks and Bayesian networks are typical applications of subjective logic.\n\nSubjective trust networks can be modelled with a combination of the transitivity and fusion operators. Let formula_46 express the referral trust edge from formula_2 to formula_42, and let formula_49 express the belief edge from formula_42 to formula_3. A subjective trust network can for example be expressed as formula_63 as illustrated in the figure below.\n\nThe indices 1, 2 and 3 indicate the chronological order in which the trust edges and advices are formed. Thus, given the set of trust edges with index 1, the origin trustor formula_2 receives advice from formula_42 and formula_66, and is thereby able to derive belief in variable formula_3. By expressing each trust edge and belief edge as an opinion, it is possible for formula_2 to derive belief in formula_3 expressed as formula_70.\n\nTrust networks can express the reliability of information sources, and can be used to determine subjective opinions about variables that the sources provide information about.\n\nIn the Bayesian network below, formula_3 and formula_72 are parent variables and formula_73 is the child variable. The analyst must learn the set of joint conditional opinions formula_74 in order to apply the deduction operator and derive the marginal opinion formula_75 on the variable formula_76. The conditional opinions express a conditional relationship between the parent variables and the child variable.\n\nThe deduced opinion is computed as formula_77. \nThe joint evidence opinion formula_78 can be computed as the product of independent evidence opinions on formula_3 and formula_72, or as the joint product of partially dependent evidence opinions.\n\nThe combination of a subjective trust network and a subjective Bayesian network is a subjective network. The subjective trust network can be used to obtain from various sources the opinions to be used as input opinions to the subjective Bayesian network, as illustrated in the figure below.\n\nTraditional Bayesian network typically do not take into account the reliability of the sources. In subjective networks, the trust in sources is explicitly taken into account.\n\n\n"}
{"id": "54016907", "url": "https://en.wikipedia.org/wiki?curid=54016907", "title": "Time translation symmetry", "text": "Time translation symmetry\n\nTime translation symmetry or temporal translation symmetry (TTS) is a mathematical transformation in physics that moves the times of events through a common interval. Time translation symmetry is the hypothesis that the laws of physics are unchanged, (i.e. invariant) under such a transformation. Time translation symmetry is a rigorous way to formulate the idea that the laws of physics are the same throughout history. Time translation symmetry is closely connected via the Noether theorem, to conservation of energy. In mathematics, the set of all time translations on a given system form a Lie group.\n\nThere are many symmetries in nature besides time translation, such as spatial translation or rotational symmetries. These symmetries can be broken and explain diverse phenomena such as crystals, superconductivity, and the Higgs mechanism. However, it was thought until very recently that time translation symmetry could not be broken. Time crystals, a state of matter first observed in 2017, break time translation symmetry.\n\nSymmetries are of prime importance in physics and are closely related to the hypothesis that certain physical quantities are only relative and unobservable. Symmetries apply to the equations that govern the physical laws (e.g. to a Hamiltonian or Lagrangian) rather than the initial conditions, values or magnitudes of the equations themselves and state that the laws remain unchanged under a transformation. If a symmetry is preserved under a transformation it is said to be \"invariant\". Symmetries in nature lead directly to conservation laws, something which is precisely formulated by the Noether theorem.\n\nTo formally describe time translation symmetry we say the equations, or laws, that describe a system at times formula_1 and formula_2 are the same for any value of formula_1 and formula_4.\n\nFor example, considering Newton's equation:\n\nOne finds for its solutions formula_6 the combination:\n\ndoes not depend on the variable formula_1. Of course, this quantity describes the total energy whose conservation is due to the time translation invariance of the equation of motion. By studying the composition of symmetry transformations, e.g. of geometric objects, one reaches the conclusion that they form a group and, more specifically, a Lie transformation group if one considers continuous, finite symmetry transformations. Different symmetries form different groups with different geometries. Time independent Hamiltonian systems form a group of time translations that is described by the non-compact, abelian, Lie group formula_9. TTS is therefore a dynamical or Hamiltonian dependent symmetry rather than a kinematical symmetry which would be the same for the entire set of Hamiltonians at issue. Other examples can be seen in the study of time evolution equations of classical and quantum physics.\n\nMany differential equations describing time evolution equations are expressions of invariants associated to some Lie group and the theory of these groups provides a unifying viewpoint for the study of all special functions and all their properties. In fact, Sophus Lie invented the theory of Lie groups when studying the symmetries of differential equations. The integration of a (partial) differential equation by the method of separation of variables or by Lie algebraic methods is intimately connected with the existence of symmetries. For example, the exact solubility of the Schrodinger equation in quantum mechanics can be traced back to the underlying invariances. In the latter case, the investigation of symmetries allows for an interpretation of the degeneracies, where different configurations to have the same energy, which generally occur in the energy spectrum of quantum systems. Continuous symmetries in physics are often formulated in terms of infinitesimal rather than finite transformations, i.e. one considers the Lie algebra rather than the Lie group of transformations\n\nThe invariance of a Hamiltonian formula_10 of an isolated system under time translation implies its energy does not change with the passage of time. Conservation of energy implies, according to the Heisenberg equations of motion, that formula_11.\n\nor:\n\nWhere formula_14 is the time translation operator which implies invariance of the Hamiltonian under the time translation operation and leads to the conservation of energy.\n\nIn many nonlinear field theories like general relativity or Yang-Mills theories, the basic field equations are highly nonlinear and exact solutions are only known for ‘sufficiently symmetric’ distributions of matter (e.g. rotationally or axially symmetric configurations). Time translation symmetry is guaranteed only in spacetimes where the metric is static: that is, where there is a coordinate system in which the metric coefficients contain no time variable. Many general relativity systems are not static in any frame of reference so no conserved energy can be defined.\n\nTime crystals, a state of matter first observed in 2017, break time translation symmetry.\n\n\n"}
{"id": "54423179", "url": "https://en.wikipedia.org/wiki?curid=54423179", "title": "Traité de mécanique céleste", "text": "Traité de mécanique céleste\n\nTraité de mécanique céleste () is a five-volume treatise on celestial mechanics written by Pierre-Simon Laplace and published from 1798 to 1825 with a second edition in 1829. In 1842, the government of Louis Philippe gave a grant of 40,000 francs for a 7-volume national edition of the \"Oeuvres de Laplace\" (1843–1847); the \"Traité de mécanique céleste\" with its four supplements occupies the first 5 volumes.\n\n\nThe famous American mathematician Nathaniel Bowditch translated the first four volumes of the \"Traité de mécanique céleste\" but not the fifth volume; however, Bowditch did make use of relevant portions of the fifth volume in his extensive commentaries for the first four volumes.\n\nTranslation by Nathaniel Bowditch\n"}
{"id": "46633", "url": "https://en.wikipedia.org/wiki?curid=46633", "title": "Turing tarpit", "text": "Turing tarpit\n\nA Turing tarpit (or Turing tar-pit) is any programming language or computer interface that allows for flexibility in function but is difficult to learn and use because it offers little or no support for common tasks. The phrase was coined in 1982 by Alan Perlis in the \"Epigrams on Programming\":\n\nIn any Turing complete language, it is possible to write any computer program, so in a very rigorous sense nearly all programming languages are equally capable. Showing that theoretical ability is not the same as usefulness in practice, Turing tarpits are characterized by having a simple abstract machine that requires the user to deal with many details in the solution of a problem. At the extreme opposite are interfaces that can perform very complex tasks with little human intervention but become obsolete if requirements change slightly.\n\nSome esoteric programming languages, such as Brainfuck, are specifically referred to as \"Turing tarpits\" because they deliberately implement the minimum functionality necessary to be classified as Turing complete languages. Using such languages is a form of mathematical recreation: programmers can work out how to achieve basic programming constructs in an extremely difficult but mathematically Turing-equivalent language.\n\n\n"}
{"id": "30260760", "url": "https://en.wikipedia.org/wiki?curid=30260760", "title": "Walk forward optimization", "text": "Walk forward optimization\n\nWalk forward optimization is a method used in finance for determining the best parameters to use in a trading strategy. The trading strategy is optimized with in-sample data for a time window in a data series. The remainder of the data are reserved for out of sample testing. A small portion of the reserved data following the in-sample data is tested with the results recorded. The in-sample time window is shifted forward by the period covered by the out of sample test, and the process repeated. At the end, all of the recorded results are used to assess the trading strategy.\n\nIt means to get the most suitable/stable parameters of the system and run the system with these parameters using another segment of data and these two segments of data do not overlap each other. It is the culmination of the following methods and helps in creation of robust systems.\n\nBacktesting is using past data to test a trading system. It's useful because, if a system was not profitable in the past, that's a strong sign it won't be profitable in the future. It refers to applying a trading system to historical data to verify how a system would have performed during the specified time period.\n\nForward testing is also known as Walk forward testing is the simulation of the real markets data on paper only. It means that though you are moving along the markets live, but you are not actually putting in real money, but doing virtual trading in the markets to understand the movements of markets better. Hence, it is also called Paper Trading. Forward performance testing is a simulation of actual trading and involves following the system's logic in a live market.\n\nOne of the biggest issues with system development is that many systems do not hold up into the future. There are several reasons for this. The first is that the system is not based on a valid premise. Another is that the testing is not sound for reasons such as:\n\nWalk Forward Analysis does optimization on a training set; test on a period after the set and then rolls it all forward and repeats the process. We have multiple out-of-sample periods and look at these results combined. Walk forward analysis was originally discussed by Robert E. Pardo. Walking forward can keep a trading model a step ahead. Walk forward is so called, as we have multiple walk training and testing periods is less likely to suffer from overfitting.\n\n\"Walk forward testing allows us to develop a trading system while maintaining a reasonable ‘degree of freedom’\". Walk-forward testing carries the idea of ‘out-of-sample’ testing to the next level. It is a specific application of a technique known as Cross-validation. It means to take a segment of your data to optimize a system, and another segment of data to validate. Hence, here you optimize a window of data say past 1000 bars, and then test it on next 200 bars. Then roll the whole thing forward 200 bars and repeat the process. This gives you a large out of sample period and allows you to see how stable the system is over time.\n\nSuppose you consider a strategy around a moving average. You take the first 3 months of data, and find that for that period a 20-minute moving average was optimal (using tick data). You then validate this rule by assessing its performance for the 4th month (i.e. profit, reward/risk or any other statistic of interest). Next, you repeat the optimization using data from month 2-4, and validate using month 5, and keep repeating this until you've reached the end of the data. The performance you get for the validation months (4-13) are your out-of-sample performance.\n\nBefore doing the backtesting or optimization, one needs to set up the data required which is the historical data of a specific time period. This historical data segment is divided into the following two types:\n\nThe process is to first develop a trading system using in-sample data and then apply the out-of-sample data to the system. The results of both cases can then be compared and tested.\n\nThe concept for walk-forward testing is similar to using ‘in-sample’ and ‘out-of-sample’ testing periods. Instead of optimizing on twenty years of data and using the last four years of data for testing, the optimization is done across ten years and the system is tested on the eleventh. Once this test is completed, move the whole time window forward one year and run the test-run on the next year. Find the optimum set of parameters for each of the 10-year windows and use that set of parameters to trade for the next year. Move the time window forward one year and run the test on the next year until all of the years in the data series have been tested.\n\nWhen the system performance is evaluated, all of the one-year windows are consolidated to compose the out-of-sample periods for each of the optimal windows. The out-of-sample performance is used to judge how good the system is.\n\nWalk-forward testing works like this. Let’s say that you have twelve years of data extending from 1998 to 2009 for the markets that you want to trade. Let’s also assume that your trading strategy needs a minimum of three years of data for testing and optimization.\n\nTo begin, start by developing and optimizing the system using only the first three years of data – in this example, 1998–2000. On these three years of data, try as many ideas as you like and optimize parameters in as many ways as you can think of. It is important not to look at any data after 2000! When you think you have found the ‘Holy Grail’ of trading systems, record the rules for the system with the optimum parameters. These rules and optimized parameters are to be used later for the final testing with new data starting with 2000.\n\nSlide the three-year time window of data forward a little – say one month. Now, the data that you are working with runs from the 2nd month of 1998 to the 2nd month of 2000. Repeat the analysis, including optimization and record the rules and optimized parameters. In the final pass, these parameters will be used for the 2nd month of 2000.\n\nContinue with ‘walking forward’ and optimizing the three-year data periods. Record the results for use in the first month following the three-year optimization period. When your data finally runs out in 2009, go back, and test the system for the entire period from 2000 to 2009. Switch the rules and parameters each month to use the ones that you found and recorded. In effect, you are performing a new out-of-sample test for each month. The system performance for these nine out-of-sample years (108 out-of-sample months) is a much better indication of how a system will perform in real time than the performance of any single time period used for optimization.\n\nThere is nothing magic about the assumed time-periods – three years for system development and one month for the walk-forward interval. Picking these two time parameters is a trade-off between optimization time and statistical validity of the results. In practice, I have found that using about 20% of the optimization period for the walk- forward window works fairly well. Which window sizes work best is also affected by the given system, for different systems the optimal training and out-of-sample window size will be different.\n\nIf the results for the ‘out-of-sample’ months look good, continue the walk-forward process in real time to find the parameters to use with real money. \"Another advantage\" to this method of system development and trading is that your system will better adapt to changes in market behavior over time. Markets do change with time – we have all seen systems that have made money for several years and then simply stopped working because the markets have changed how often these changes affect the system is related to the best size for the training and out-of-sample set. Manual in-sample and out-of-sample walk forward testing as described is useful, but automated walk forward testing with automated parameter selection is the best way to avoid curve fitting.\n\nFor a better understanding, please see the example here.\n\nIn order to evaluate any system, one should check out its performance when using the \"Out-of-Sample Data\" (test data) and not the \"In-Sample Data\" (data used for optimization of the system). Thus, walk forward test determines the optimized system performance as follows:\n\nHence, the out-of-sample data plays a crucial role in determining the validity and reliability of the system and is a realistic estimate of how a system should work in real markets.\n\n\n\n"}
