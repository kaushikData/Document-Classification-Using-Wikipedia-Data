{"id": "407339", "url": "https://en.wikipedia.org/wiki?curid=407339", "title": "92 (number)", "text": "92 (number)\n\n92 (ninety-two) is the natural number following 91 and preceding 93.\n\n92 is:\n\n\nThere are 92 Johnson solids. The snub dodecahedron has 92 faces, the most an Archimedean solid can have.\n\nFor \"n\" = 8, there are 92 solutions in the n-Queens Problem.\n\nThere are 92 \"atomic elements\" in the Look-and-say sequence, corresponding to the 92 non-transuranic elements in the chemist's periodic table.\n\n\nNinety-two is also:\n\nVehicles\n\n\nList of highways numbered 92\n"}
{"id": "42801450", "url": "https://en.wikipedia.org/wiki?curid=42801450", "title": "Adversarial queueing network", "text": "Adversarial queueing network\n\nIn queueing theory, an adversarial queueing network is a model where the traffic to the network is supplied by an opponent rather than as the result of a stochastic process. The model has seen use in describing the impact of packet injections on the performance of communication networks.\nThe model was first introduced in 1996.\n\nThe stability of an adversarial queueing network can be determined by considering a fluid limit.\n"}
{"id": "25137399", "url": "https://en.wikipedia.org/wiki?curid=25137399", "title": "Alphanumeric grid", "text": "Alphanumeric grid\n\nAn alphanumeric grid (also known as atlas grid) is a simple coordinate system on a grid in which each cell is identified by a combination of a letter and a number.\n\nAn advantage over numeric coordinates, which use two numbers instead of a number and a letter to refer to a grid cell, is that there can be no confusion over which coordinate refers to which direction. As an easy example, one could think about battleship; simply match the number at the top to the number on the bottom, then follow the two lines until they meet in a spot.\nAlgebraic chess notation uses an alphanumeric grid to refer to the squares of a chessboard.\n"}
{"id": "15845764", "url": "https://en.wikipedia.org/wiki?curid=15845764", "title": "A∞-operad", "text": "A∞-operad\n\nIn the theory of operads in algebra and algebraic topology, an A-operad is a parameter space for a multiplication map that is homotopy coherently associative. (An operad that describes a multiplication that is both homotopy coherently associative and homotopy coherently commutative is called an E-operad.)\n\nIn the (usual) setting of operads with an action of the symmetric group on topological spaces, an operad \"A\" is said to be an \"A\"-operad if all of its spaces \"A\"(\"n\") are Σ-equivariantly homotopy equivalent to the discrete spaces Σ (the symmetric group) with its multiplication action (where \"n\" ∈ N). In the setting of non-Σ operads (also termed nonsymmetric operads, operads without permutation), an operad \"A\" is \"A\"if all of its spaces \"A\"(\"n\") are contractible. In other categories than topological spaces, the notions of \"homotopy\" and \"contractibility\" have to be replaced by suitable analogs, such as homology equivalences in the category of chain complexes.\n\nThe letter \"A\" in the terminology stands for \"associative\", and the infinity symbols says that associativity is required up to \"all\" higher homotopies. More generally, there is a weaker notion of \"A\"-operad (\"n\" ∈ N), parametrizing multiplications that are associative only up to a certain level of homotopies. In particular,\n\n\nA space \"X\" is the loop space of some other space, denoted by \"BX\", if and only if \"X\" is an algebra over an \"A\"-operad and the monoid \"π\"(\"X\") of its connected components is a group. An algebra over an A-operad is referred to as an \"A\"-space. There are three consequences of this characterization of loop spaces. First, a loop space is an \"A\"-space. Second, a connected \"A\"-space \"X\" is a loop space. Third, the group completion of a possibly disconnected \"A\"-space is a loop space.\n\nThe importance of \"A\"-operads in homotopy theory stems from this relationship between algebras over \"A\"-operads and loop spaces.\n\nAn algebra over the \"A\" operad is called an \"A\"-algebra. Examples feature the Fukaya category of a symplectic manifold, when it can be defined (see also pseudoholomorphic curve).\n\nThe most obvious, if not particularly useful, example of an \"A\"-operad is the \"associative operad\" \"a\" given by \"a\"(\"n\") = Σ. This operad describes strictly associative multiplications. By definition, any other \"A\"-operad has a map to \"a\" which is a homotopy equivalence.\n\nA geometric example of an A-operad is given by the Stasheff polytopes or associahedra.\n\nA less combinatorial example is the operad of little intervals: The space \"A\"(\"n\") consists of all embeddings of \"n\" disjoint intervals into the unit interval.\n\n\n"}
{"id": "20890526", "url": "https://en.wikipedia.org/wiki?curid=20890526", "title": "Bootstrapping populations", "text": "Bootstrapping populations\n\nStarting with a sample formula_1 observed from a random variable \"X\" having a given distribution law with a set of non fixed parameters which we denote with a vector formula_2, a parametric inference problem consists of computing suitable values – call them estimates – of these parameters precisely on the basis of the sample. An estimate is suitable if replacing it with the unknown parameter does not cause major damage in next computations. In Algorithmic inference, suitability of an estimate reads in terms of compatibility with the observed sample.\n\nIn this framework, resampling methods are aimed at generating a set of candidate values to replace the unknown parameters that we read as compatible replicas of them. They represent a population of specifications of a random vector formula_3 compatible with an observed sample, where the compatibility of its values has the properties of a probability distribution. By plugging parameters into the expression of the questioned distribution law, we bootstrap entire populations of random variables compatible with the observed sample.\n\nThe rationale of the algorithms computing the replicas, which we denote \"population bootstrap\" procedures, is to identify a set of statistics formula_4 exhibiting specific properties, denoting a well behavior, w.r.t. the unknown parameters. The statistics are expressed as functions of the observed values formula_1, by definition. The formula_6 may be expressed as a function of the unknown parameters and a random seed specification formula_7 through the sampling mechanism formula_8, in turn. Then, by plugging the second expression in the former, we obtain formula_9 expressions as functions of seeds and parameters – the master equations – that we invert to find values of the latter as a function of: i) the statistics, whose values in turn are fixed at the observed ones; and ii) the seeds, which are random according to their own distribution. Hence from a set of seed samples we obtain a set of parameter replicas.\n\nGiven a formula_10 of a random variable \"X\" and a sampling mechanism formula_8 for \"X\", the realization x is given by formula_12, with formula_13. Focusing on well-behaved statistics,\n\nfor their parameters, the master equations read\n\nFor each sample seed formula_14 a vector of parameters formula_2 is obtained from the solution of the above system with formula_16 fixed to the observed values.\nHaving computed a huge set of compatible vectors, say \"N\", the empirical marginal distribution of formula_17 is obtained by:\n\nwhere formula_18 is the j-th component of the generic solution of (1) and where formula_19 is the indicator function of formula_18 in the interval formula_21\nSome indeterminacies remain if \"X\" is discrete and this we will be considered shortly.\nThe whole procedure may be summed up in the form of the following Algorithm, where the index formula_3 of formula_23 denotes the parameter vector from which the statistics vector is derived.\n\n You may easily see from a table of sufficient statistics that we obtain the curve in the picture on the left by computing the empirical distribution (2) on the population obtained through the above algorithm when: i) \"X\" is an Exponential random variable, ii) formula_24, and \nand the curve in the picture on the right when: i) \"X\" is a Uniform random variable in formula_26, ii) formula_27, and \n\nNote that the accuracy with which a parameter distribution law of\npopulations compatible with a sample is obtained is not a function of the sample size. Instead, it is a function of the number of seeds we draw. In turn, this number is purely a matter of computational time but does not require any extension of the observed data. With other bootstrapping methods focusing on a generation of sample replicas (like those proposed by ) the accuracy of the estimate distributions depends on the sample size.\n\nFor formula_29 expected to represent a Pareto distribution, whose specification requires values for the parameters formula_30 and \"k\", we have that the cumulative distribution function reads: \n\nA sampling mechanism formula_32 has formula_33 uniform seed \"U\" and explaining function formula_34 described by:\n\nA relevant statistic formula_36 is constituted by the pair of joint sufficient statistics for formula_37 and \"K\", respectively formula_38.\nThe master equations read\n\nwith formula_41.\n\nFigure on the right reports the three-dimensional plot of the empirical cumulative distribution function (2) of formula_42.\n\n"}
{"id": "8488020", "url": "https://en.wikipedia.org/wiki?curid=8488020", "title": "Bruce Kleiner", "text": "Bruce Kleiner\n\nBruce Alan Kleiner is an American mathematician, working in differential geometry and topology and geometric group theory.\n\nHe received his Ph.D. in 1990 from the University of California, Berkeley. His advisor was Wu-Yi Hsiang. He is now Professor of Mathematics at New York University.\n\nKleiner has written expository papers on the Ricci flow. Together with John Lott of the University of Michigan, he filled in details of Grigori Perelman's proof of the Geometrization conjecture (from which the Poincaré conjecture follows) in the years 2003–2006. Theirs was the first publication acknowledging Perelman's accomplishment (in May, 2006), which was shortly followed by similar papers by Huai-Dong Cao and Xi-Ping Zhu (in June) and John Morgan and Gang Tian (in July).\n\nKleiner found a relatively simple proof of Gromov's theorem on groups of polynomial growth.\n\n\n"}
{"id": "15333552", "url": "https://en.wikipedia.org/wiki?curid=15333552", "title": "C-minimal theory", "text": "C-minimal theory\n\nIn model theory, a branch of mathematical logic, a C-minimal theory is a theory that is \"minimal\" with respect to a ternary relation \"C\" with certain properties. Algebraically closed fields with a (Krull) valuation are perhaps the most important example.\n\nThis notion was defined in analogy to the o-minimal theories, which are \"minimal\" (in the same sense) with respect to a linear order.\n\nA \"C\"-relation is a ternary relation \"C\"(\"x\";\"yz\") that satisfies the following axioms.\nA C-minimal structure is a structure \"M\", in a signature containing the symbol \"C\", such that \"C\" satisfies the above axioms and every set of elements of \"M\" that is definable with parameters in \"M\" is a Boolean combination of instances of \"C\", i.e. of formulas of the form \"C\"(\"x\";\"bc\"), where \"b\" and \"c\" are elements of \"M\".\n\nA theory is called C-minimal if all of its models are C-minimal. A structure is called strongly C-minimal if its theory is C-minimal. One can construct C-minimal structures which are not strongly C-minimal.\n\nFor a prime number \"p\" and a \"p\"-adic number \"a\" let |\"a\"| denote its \"p\"-adic norm. Then the relation defined by formula_5 is a \"C\"-relation, and the theory of Q with addition and this relation is C-minimal. The theory of Q as a field, however, is not C-minimal.\n\n"}
{"id": "40520498", "url": "https://en.wikipedia.org/wiki?curid=40520498", "title": "COBYLA", "text": "COBYLA\n\nConstrained optimization by linear approximation (COBYLA) is a numerical optimization method for constrained problems where the derivative of the objective function is not known, invented by Michael J. D. Powell. That is, COBYLA can find the vector formula_1 with formula_2 that has the minimal (or maximal) formula_3 without knowing the gradient of formula_4. COBYLA is also the name of Powell's software implementation of the algorithm in Fortran.\n\nPowell invented COBYLA while working for Westland Helicopters.\n\nIt works by iteratively approximating the actual constrained optimization problem with linear programming problems. During an iteration, an approximating linear programming problem is solved to obtain a candidate for the optimal solution. The candidate solution is evaluated using the original objective and constraint functions, yielding a new data point in the optimization space. This information is used to improve the approximating linear programming problem used for the next iteration of the algorithm. When the solution cannot be improved anymore, the step size is reduced, refining the search. When the step size becomes sufficiently small, the algorithm finishes.\n\nThe COBYLA software is distributed under The GNU Lesser General Public License (LGPL).\n\n\n"}
{"id": "3539918", "url": "https://en.wikipedia.org/wiki?curid=3539918", "title": "Canadian Undergraduate Mathematics Conference", "text": "Canadian Undergraduate Mathematics Conference\n\nThe Canadian Undergraduate Mathematics Conference (CUMC) () is an annual academic conference of undergraduate mathematics students at a Canadian university. It is principally sponsored by the Canadian Mathematical Society, through its Student Committee. The host institution for a given year is determined by an election process during the previous year's gathering. CUMCs are usually held during the summer, for a period of approximately four days. CUMCs are intended to be bilingual (English and French).\n\nThe inaugural session was held at the McGill University campus in Montreal in 1994. CUMC meetings typically draw between 100 and 150 participants.\n\nStudent participants are encouraged to give talks (25 or 50 minutes) on their mathematical research or interests. Student abstracts are usually published online and in a conference kit distributed to participants.\n\nPast CUMCs have been held at the following locations:\nSome notable people invited to speak at the Canadian Undergraduate Mathematics Conference in the past:\n\n"}
{"id": "11843393", "url": "https://en.wikipedia.org/wiki?curid=11843393", "title": "Clock angle problem", "text": "Clock angle problem\n\nClock angle problems are a type of mathematical problem which involve finding the angles between the hands of an analog clock.\n\nClock angle problems relate two different measurements: angles and time. The angle is typically measured in degrees from the mark of number 12 clockwise. The time is usually based on 12-hour clock.\n\nA method to solve such problems is to consider the rate of change of the angle in degrees per minute. The hour hand of a normal 12-hour analogue clock turns 360° in 12 hours (720 minutes) or 0.5° per minute. The minute hand rotates through 360° in 60 minutes or 6° per minute.\n\nwhere:\n\nwhere:\n\nThe time is 5:24. The angle in degrees of the hour hand is:\n\nThe angle in degrees of the minute hand is:\n\nThe angle between the hands can be found using the following formula:\n\nwhere\nIf the angle is greater than 180 degrees then subtract it from 360 degrees.\n\nThe time is 2:20.\n\nThe time is 10:16.\n\nThe hour and minute hands are superimposed only when their angle is the same.\n\n10:54., and 12:00.\n\n\n"}
{"id": "7770047", "url": "https://en.wikipedia.org/wiki?curid=7770047", "title": "Conference matrix", "text": "Conference matrix\n\nIn mathematics, a conference matrix (also called a C-matrix) is a square matrix \"C\" with 0 on the diagonal and +1 and −1 off the diagonal, such that \"C\"\"C\" is a multiple of the identity matrix \"I\". Thus, if the matrix has order \"n\", \"C\"\"C\" = (\"n\"−1)\"I\". \nSome authors use a more general definition, which requires there to be a single 0 in each row and column but not necessarily on the diagonal.\n\nConference matrices first arose in connection with a problem in telephony. They were first described by Vitold Belevitch, who also gave them their name. Belevitch was interested in constructing ideal telephone conference networks from ideal transformers and discovered that such networks were represented by conference matrices, hence the name. Other applications are in statistics, and another is in elliptic geometry.\n\nFor \"n\" > 1, there are two kinds of conference matrix. Let us normalize \"C\" by, first (if the more general definition is used), rearranging the rows so that all the zeros are on the diagonal, and then negating any row or column whose first entry is negative. (These operations do not change whether a matrix is a conference matrix.) \nThus, a normalized conference matrix has all 1's in its first row and column, except for a 0 in the top left corner, and is 0 on the diagonal. Let \"S\" be the matrix that remains when the first row and column of \"C\" are removed. Then either \"n\" is evenly even (a multiple of 4), and \"S\" is antisymmetric (as is the normalized \"C\" if its first row is negated), or \"n\" is oddly even (congruent to 2 modulo 4) and \"S\" is symmetric (as is the normalized \"C\").\n\nIf \"C\" is a symmetric conference matrix of order \"n\" > 1, then not only must \"n\" be congruent to 2 (mod 4) but also \"n\" − 1 must be a sum of two square integers; there is a clever proof by elementary matrix theory in van Lint and Seidel. \"n\" will always be the sum of two squares if \"n\" − 1 is a prime power.\n\nGiven a symmetric conference matrix, the matrix \"S\" can be viewed as the Seidel adjacency matrix of a graph. The graph has \"n\" − 1 vertices, corresponding to the rows and columns of \"S\", and two vertices are adjacent if the corresponding entry in \"S\" is negative. This graph is strongly regular of the type called (after the matrix) a conference graph.\n\nThe existence of conference matrices of orders \"n\" allowed by the above restrictions is known only for some values of \"n\". For instance, if \"n\" = \"q\" + 1 where \"q\" is a prime power congruent to 1 (mod 4), then the Paley graphs provide examples of symmetric conference matrices of order \"n\", by taking \"S\" to be the Seidel matrix of the Paley graph. \nThe first few possible orders of a symmetric conference matrix are \"n\" = 2, 6, 10, 14, 18, (not 22, since 21 is not a sum of two squares), 26, 30, (not 34 since 33 is not a sum of two squares), 38, 42, 46, 50, 54, (not 58), 62 ; for every one of these, it is known that a symmetric conference matrix of that order exists. Order 66 seems to be an open problem.\n\nThe essentially unique conference matrix of order 6 is given by\nall other conference matrices of order 6 are obtained from this one by flipping the signs of some row and/or column (and by taking permutations of rows and/or columns, according to the definition in use).\n\nAntisymmetric matrices can also be produced by the Paley construction. Let \"q\" be a prime power with residue 3 (mod 4). Then there is a Paley digraph of order \"q\" which leads to an antisymmetric conference matrix of order \"n\" = \"q\" + 1. The matrix is obtained by taking for \"S\" the \"q\" × \"q\" matrix that has a +1 in position (\"i,j\") and −1 in position (\"j,i\") if there is an arc of the digraph from \"i\" to \"j\", and zero diagonal. Then \"C\" constructed as above from \"S\", but with the first row all negative, is an antisymmetric conference matrix.\n\nThis construction solves only a small part of the problem of deciding for which evenly even numbers \"n\" there exist antisymmetric conference matrices of order \"n\".\n\nSometimes a conference matrix of order \"n\" is just defined as a weighing matrix of the form \"W\"(\"n, n\"−1), where\n\"W\"(\"n,w\") is said to be of weight \"w\">0 and order \"n\" if it is a square matrix of size \"n\" with entries from {−1, 0, +1} satisfying \"W W\" = \"w I\". Using this definition, the zero element is no more required to be on the diagonal, but it is easy to see that still there must be exactly one zero element in each row and column. For example, the matrix\nwould satisfy this relaxed definition, but not the more strict one requiring the zero elements to be on the diagonal.\n\nA conference design is a generalization of conference matrices to non-rectangular matrices. A conference design C is an formula_3 matrix, with entries from {-1, 0, +1} satisfying formula_4, where\nformula_5 is the formula_6 identity matrix and at most one zero in each row.\nThe foldover designs of conference designs can be used as definitive screening designs .\n\nBelevitch obtained complete solutions for conference matrices for all values of \"n\" up to 38 and provided circuits for some of the smaller matrices. An \"ideal conference network\" is one where the loss of signal is entirely due to the signal being split between multiple conference subscriber ports. That is, there are no dissipation losses within the network. The network must contain ideal transformers only and no resistances. An \"n\"-port ideal conference network exists if and only if there exists a conference matrix of order \"n\". For instance, a 3-port conference network can be constructed with the well-known hybrid transformer circuit used for 2-wire to 4-wire conversion in telephone handsets and line repeaters. However, there is no order 3 conference matrix and this circuit does not produce an \"ideal\" conference network. A resistance is needed for matching which dissipates signal, or else signal is lost through mismatch.\n\nAs mentioned above, a necessary condition for a conference matrix to exist is that \"n\"−1 must be the sum of two squares. Where there is more than one possible sum of two squares for \"n\"−1 there will exist multiple essentially different solutions for the corresponding conference network. This situation occurs at \"n\" of 26 and 66. The networks are particularly simple when \"n\"−1 is a perfect square (\"n\" = 2, 10, 26, …).\n\n\n"}
{"id": "42304736", "url": "https://en.wikipedia.org/wiki?curid=42304736", "title": "Cryptographic High Value Product", "text": "Cryptographic High Value Product\n\nCryptographic High Value Product (CHVP) is a designation used within the information security community to identify assets that have high value, and which may be used to encrypt / decrypt secure communications, but which do not retain or store any classified information. When disconnected from the secure communication network, the CHVP equipment may be handled with a lower level of controls than required for COMSEC equipment.\n\n\n"}
{"id": "1250664", "url": "https://en.wikipedia.org/wiki?curid=1250664", "title": "Deep inference", "text": "Deep inference\n\nDeep inference names a general idea in structural proof theory that breaks with the classical sequent calculus by generalising the notion of structure to permit inference to occur in contexts of high structural complexity. The term \"deep inference\" is generally reserved for proof calculi where the structural complexity is unbounded; in this article we will use non-shallow inference to refer to calculi that have structural complexity greater than the sequent calculus, but not unboundedly so, although this is not at present established terminology.\n\nDeep inference is not important in logic outside of structural proof theory, since the phenomena that lead to the proposal of formal systems with deep inference are all related to the cut-elimination theorem. The first calculus of deep inference was proposed by Kurt Schütte, but the idea did not generate much interest at the time.\n\nNuel Belnap proposed display logic in an attempt to characterise the essence of structural proof theory. The calculus of structures was proposed in order to give a cut-free characterisation of noncommutative logic. Cirquent calculus was developed as a system of deep inference allowing to explicitly account for the possibility of subcomponent-sharing. \n\n"}
{"id": "35139397", "url": "https://en.wikipedia.org/wiki?curid=35139397", "title": "Faltings height", "text": "Faltings height\n\nIn mathematics, the Faltings height of an abelian variety defined over a number field is a measure of its arithmetic complexity. It was introduced by in his proof of the Mordell conjecture.\n\n\n"}
{"id": "2903138", "url": "https://en.wikipedia.org/wiki?curid=2903138", "title": "Formula game", "text": "Formula game\n\nA formula game is an artificial game represented by a fully quantified Boolean formula. Players' turns alternate and the space of possible moves is denoted by bound variables. If a variable is universally quantified, the formula following it has the same truth value as the formula beginning with the universal quantifier regardless of the move taken. If a variable is existentially quantified, the formula following it has the same truth value as the formula beginning with the existential quantifier for at least one move available at the turn. Turns alternate, and a player loses if he cannot move at his turn. In computational complexity theory, the language FORMULA-GAME is defined as all formulas formula_1 such that Player 1 has a winning strategy in the game represented by formula_1. FORMULA-GAME is PSPACE-complete.\n\n"}
{"id": "1803858", "url": "https://en.wikipedia.org/wiki?curid=1803858", "title": "Gaussian random field", "text": "Gaussian random field\n\nA Gaussian random field (GRF) is a random field involving Gaussian probability density functions of the variables. A one-dimensional GRF is also called a Gaussian process. An important special case of a GRF is the Gaussian free field.\n\nWith regard to applications of GRFs, the initial conditions of physical cosmology generated by quantum mechanical fluctuations during cosmic inflation are thought to be a GRF with a nearly scale invariant spectrum.\n\nOne way of constructing a GRF is by assuming that the field is the sum of a large number of plane, cylindrical or spherical waves with uniformly distributed random phase. Where applicable, the central limit theorem dictates that at any point, the sum of these individual plane-wave contributions will exhibit a Gaussian distribution. This type of GRF is completely described by its power spectral density, and hence, through the Wiener-Khinchin theorem, by its two-point autocorrelation function, which is related to the power spectral density through a Fourier transformation. \n\nSuppose \"f\"(\"x\") is the value of a GRF at a point \"x\" in some \"D\"-dimensional space. If we make a vector of the values of \"f\" at \"N\" points, \"x\", ..., \"x\", in the \"D\"-dimensional space, then the vector (\"f\"(\"x\"), ..., \"f\"(\"x\")) will always be distributed as a multivariate Gaussian.\n\n"}
{"id": "27290438", "url": "https://en.wikipedia.org/wiki?curid=27290438", "title": "Geomathematics", "text": "Geomathematics\n\nGeomathematics or Mathematical Geophysics is the application of mathematical intuition to solve problems in Geophysics. The most complicated problem in Geophysics is\nthe solution of the three dimensional inverse problem, where\nobservational constraints are used to infer physical properties.\nThe inverse procedure is much more sophisticated than the normal direct computation of what should be observed from a physical system.\nThe estimation procedure is often dubbed the inversion strategy (also called the inverse problem) as the procedure is intended to estimate from a set of observations the circumstances that produced them. The Inverse Process is thus the converse of the classical scientific method.\n\nAn important research area that utilises inverse methods is\nseismic tomography, a technique for imaging the subsurface of the Earth using seismic waves. Traditionally seismic waves produced by earthquakes or anthropogenic seismic sources (e.g., explosives, marine air guns) were used. \n\nCrystallography is one of the traditional areas of geology that use mathematics. Crystallographers make use of linear algebra by using the Metrical Matrix. The Metrical Matrix uses the basis vectors of the unit cell dimensions to find the volume of a unit cell, d-spacings, the angle between two planes, the angle between atoms, and the bond length. Miller’s Index is also helpful in the application of the Metrical Matrix. Brag’s equation is also useful when using an electron microscope to be able to show relationship between light diffraction angles, wavelength, and the d-spacings within a sample.\n\nGeophysics is one of the most math heavy disciplines of geology. There are many applications which include gravity, magnetic, seismic, electric, electromagnetic, resistivity, radioactivity, induced polarization, and well logging. Gravity and magnetic methods share similar characteristics because they’re measuring small changes in the gravitational field based on the density of the rocks in that area. While similar gravity fields tend to be more uniform and smooth compared to magnetic fields. Gravity is used often for oil exploration and seismic can also be used, but it is often significantly more expensive. Seismic is used more than most geophysics techniques because of its ability to penetrate, its resolution, and its accuracy.\n\nMany applications of mathematics in geomorphology are related to water. In the soil aspect things like Darcy’s law, Stoke’s law, and porosity are used.\n\nMathematics in Glaciology consists of theoretical, experimental, and modeling. It usually covers glaciers, sea ice, waterflow, and the land under the glacier.\n\nPolycrystalline ice deforms slower than single crystalline ice, due to the stress being on the basal planes that are already blocked by other ice crystals. It can be mathematically modeled with Hooke’s Law to show the elastic characteristics while using Lamé constants. Generally the ice has its linear elasticity constants averaged over one dimension of space to simplify the equations while still maintaining accuracy.\n\nViscoelastic polycrystalline ice is considered to have low amounts of stress usually below one bar. This type of ice system is where one would test for creep or vibrations from the tension on the ice. One of the more important equations to this area of study is called the relaxation function. Where it’s a stress-strain relationship independent of time. This area is usually applied to transportation or building onto floating ice.\n\nShallow-Ice approximation is useful for glaciers that have variable thickness, with a small amount of stress and variable velocity. One of the main goals of the mathematical work is to be able to predict the stress and velocity. Which can be affected by changes in the properties of the ice and temperature. This is an area in which the basal shear-stress formula can be used.\n\n\n"}
{"id": "6247207", "url": "https://en.wikipedia.org/wiki?curid=6247207", "title": "Heteroclinic network", "text": "Heteroclinic network\n\nIn mathematics, a heteroclinic network is an invariant set in the phase space of a dynamical system. It can be thought of loosely as the union of more than one heteroclinic cycle, although there are further constraints requiring the network to be transitive.\n"}
{"id": "46972511", "url": "https://en.wikipedia.org/wiki?curid=46972511", "title": "How to Bake Pi", "text": "How to Bake Pi\n\nHow to Bake Pi is a popular mathematics book by Dr. Eugenia Cheng, PhD, published in 2015. Each chapter of the book begins with a recipe for a dessert, to illustrate the methods and principles of mathematics and how they relate to one another. The book is an explanation of the foundations and architecture of Category theory. Category theory is a branch of mathematics that formalizes mathematical structure and its concepts. Dr. Eugenia Cheng received her PhD from the University of Cambridge in 2002 and is currently a senior lecturer of pure mathematics at the University of Sheffield and a senior lecturer in liberal arts at the School of the Art Institute of Chicago.\n\n"}
{"id": "54669874", "url": "https://en.wikipedia.org/wiki?curid=54669874", "title": "Hypercubane", "text": "Hypercubane\n\nHypercubane is a hypothetical polycyclic hydrocarbon with the chemical formula CH. It is a molecular analog of the four-dimensional hypercube or tesseract. Hypercubane possesses an unconventional geometry of the carbon framework. It has \"O\" symmetry like classic cubane CH and is characterized by double-shell architecture: C cubic core is located inside the high-symmetry carbon cage.\n\nHypercubane was first proposed in 2014 by Pichierri in the frame of density functional theory. The initial model of hypercubane was constructed from octamethylcubane by removing unnecessary hydrogen atoms and adding the ethylene bridges as well as intercarbon bonds between the sp and sp atoms. To facilitate the future hypercubane spectroscopic identification chemical shifts for both C and H NMR-active nuclei have been calculated by Pichierri. Two years later, in 2016, studying the pyrolysis of hypercubane by means of tight-binding molecular dynamics simulations, Maslov and Katin demonstrated that hypercubane possessed high thermal stability comparable with the classic cubane CH. It was shown that hypercubane lifetime at room temperature tended to infinity. Therefore, it can be assumed that hypercubane is a kinetically stable molecular system. Among the possible hypercubane decomposition products at high temperatures (more than 1000 K) one can observe polycyclic airscrew-like hydrocarbon CH based on three combined graphene fragments passivated by hydrogen atoms and three isolated acetylene molecules.\n\nTo date, there has been no method describing the synthesis of hypercubane.\n\n"}
{"id": "3149881", "url": "https://en.wikipedia.org/wiki?curid=3149881", "title": "Irving Kaplansky", "text": "Irving Kaplansky\n\nIrving Kaplansky (March 22, 1917 – June 25, 2006) was a mathematician, college professor, author, and musician.\n\nKaplansky or \"Kap\" as his friends and colleagues called him was born in Toronto, Ontario, Canada, to Polish-Jewish immigrants; his father worked as a tailor, and his mother ran a grocery and, eventually, a chain of bakeries. He went to Harbord Collegiate Institute receiving the Prince of Wales Scholarship as a teenager. He attended the University of Toronto as an undergraduate and finished first in his class for three consecutive years. In his senior year, he competed in the first William Lowell Putnam Mathematical Competition, becoming one of the first five recipients of the Putnam Fellowship, which paid for graduate studies at Harvard University. Administered by the Mathematical Association of America, the competition is widely considered to be the most difficult mathematics examination in the world and \"its difficulty is such that the median score is often zero or one (out of 120) despite being attempted by students specializing in mathematics.\" There have been approximately 150,000 participants since 1938 with only four recorded perfect scores. Kaplansky only got one question wrong ranking his performance amongst the highest recorded.\n\nAfter receiving his Ph.D. from Harvard in 1941 as Saunders Mac Lane's first student, he remained at Harvard as a Benjamin Peirce Instructor, and in 1944 moved with Mac Lane to Columbia University for one year to collaborate on work surrounding World War II working on \"miscellaneous studies in mathematics applied to warfare analysis with emphasis upon aerial gunnery, studies of fire control equipment, and rocketry and toss bombing\" with the Applied Mathematics Panel.\n\nHe was professor of mathematics at the University of Chicago from 1945 to 1984, and Chair of the department from 1962 to 1967. Kaplansky was the Director of the Mathematical Sciences Research Institute from 1984 to 1992, and the President of the American Mathematical Society from 1985 to 1986.\n\nKaplansky was also an accomplished amateur musician. He had perfect pitch, studied piano until the age of 15, earned money in high school as a dance band musician, taught Tom Lehrer, and played in Harvard's jazz band in graduate school. He also had a regular program on Harvard's student radio station. After moving to the University of Chicago, he stopped playing for two decades, but then returned to music as an accompanist for student-run Gilbert and Sullivan productions and as a calliope player in football game parades. He often composed music based on mathematical themes. One of those compositions, \"A Song About Pi\", is a melody based on assigning notes to the first 14 decimal places of pi, and has occasionally been performed by his daughter, singer-songwriter Lucy Kaplansky.\n\nKaplansky made major contributions to group theory, ring theory, the theory of operator algebras and field theory and created the Kaplansky density theorem, Kaplansky's game and Kaplansky conjecture. He published more than 150 articles and over 20 mathematical books.\nKaplansky was the doctoral supervisor of 55 students including notable mathematicians Hyman Bass, Susanna S. Epp, Günter Lumer, Eben Matlis, Donald Ornstein, Ed Posner, Alex F. T. W. Rosenberg, Judith D. Sally, and Harold Widom. He has over 800 academic descendants, including many through his academic grandchildren David J. Foulis (who studied with Kaplansky at the University of Chicago before completing his doctorate under the supervision of Kaplansky's student Fred Wright, Jr.) and Carl Pearcy (the student of H. Arlen Brown, who had been jointly supervised by Kaplansky and Paul Halmos).\n\nKaplansky was a member of the National Academy of Sciences and the American Academy of Arts and Sciences, Director of the Mathematical Sciences Research Institute, and President of the American Mathematical Society. He was the plenary speaker at the British Mathematical Colloquium in 1966. Won the William Lowell Putnam Mathematical Competition, the Guggenheim Fellowship, the Jeffery-Williams Prize, and the Leroy P. Steele Prize.\n\nwith I. N. Herstein: 2nd edn. 1978\n\n\n\n"}
{"id": "37956397", "url": "https://en.wikipedia.org/wiki?curid=37956397", "title": "Jacobson–Bourbaki theorem", "text": "Jacobson–Bourbaki theorem\n\nIn algebra, the Jacobson–Bourbaki theorem is a theorem used to extend Galois theory to field extensions that need not be separable. It was introduced by for commutative fields and extended to non-commutative fields by , and who credited the result to unpublished work by Nicolas Bourbaki. The extension of Galois theory to normal extensions is called the Jacobson–Bourbaki correspondence, which replaces the correspondence between some subfields of a field and some subgroups of a Galois group by a correspondence between some sub division rings of a division ring and some subalgebras of an algebra.\n\nThe Jacobson–Bourbaki theorem implies both the usual Galois correspondence for subfields of a Galois extension, and Jacobson's Galois correspondence for subfields of a purely inseparable extension of exponent at most 1.\n\nSuppose that \"L\" is a division ring.\nThe Jacobson–Bourbaki theorem states that there is a natural 1:1 correspondence between:\n\nThe sub division ring and the corresponding subalgebra are each other's commutants.\n\n"}
{"id": "44840839", "url": "https://en.wikipedia.org/wiki?curid=44840839", "title": "Judy L. Walker", "text": "Judy L. Walker\n\nJudy Leavitt Walker is an American mathematician. She is the Aaron Douglas Professor of Mathematics at the University of Nebraska–Lincoln, where she chaired the mathematics department from 2012 through 2016 and currently serves as the Interim Associate Vice Chancellor for Faculty Affairs. Her research is in the area of algebraic coding theory.\n\nIn 2012, Walker became one of the inaugural Fellows of the American Mathematical Society.\n\nThroughout her career, Walker has worked on issues related to women in mathematics. She is co-founder of the University of Nebraska's All Girls/All Math program for high school girls; she represented her department at the White House when they were recognized with a 1998 Presidential Award for Excellence in Science, Mathematics, and Engineering mentoring; and she is co-founder of the Nebraska Conference for Undergraduate Women in Mathematics.\n\nWalker graduated from the University of Michigan in 1990, and went on to graduate studies at the University of Illinois at Urbana–Champaign, earning her Ph.D. in 1996 under the supervision of Nigel Boston.\n\nShe joined the University of Nebraska–Lincoln faculty in 1996, was promoted to full professor in 2006, and took up the Aaron Douglas Professorship in 2012. She held a visiting professorship at École Polytechnique Fédérale de Lausanne (EPFL) during fall 2011.\n\nWalker's research is in coding theory, i.e., the study of error-correcting codes. Her primary contributions have been in algebraic geometry codes and low-density parity check codes.\n\n\n\n"}
{"id": "4322202", "url": "https://en.wikipedia.org/wiki?curid=4322202", "title": "Kulkarni–Nomizu product", "text": "Kulkarni–Nomizu product\n\nIn the mathematical field of differential geometry, the Kulkarni–Nomizu product (named for Ravindra Shripad Kulkarni and Katsumi Nomizu) is defined for two (0,2)-tensors and gives as a result a (0,4)-tensor. \n\nIf \"h\" and \"k\" are symmetric (0,2)-tensors, then the product is defined via:\n\nwhere the \"X\" are tangent vectors.\n\nNote that formula_2. The Kulkarni–Nomizu product is a special case of the product in the graded algebra\nwhere, on simple elements,\n(formula_5 denotes the symmetric product).\n\nThe Kulkarni–Nomizu product of a pair of symmetric tensors has the algebraic symmetries of the Riemann tensor. It is thus commonly used to express the contribution that the Ricci curvature (or rather, the Schouten tensor) and the Weyl tensor each makes to the curvature of a Riemannian manifold. This so-called Ricci decomposition is useful in differential geometry.\n\nWhen there is a metric tensor \"g\", the Kulkarni–Nomizu product of \"g\" with itself is the identity endomorphism of the space of 2-forms, Ω(\"M\"), under the identification (using the metric) of the endomorphism ring End(Ω(\"M\")) with the tensor product Ω(\"M\") ⊗ Ω(\"M\").\n\nA Riemannian manifold has constant sectional curvature \"k\" if and only if the Riemann tensor has the form\nwhere \"g\" is the metric tensor.\n\n"}
{"id": "5774599", "url": "https://en.wikipedia.org/wiki?curid=5774599", "title": "Lakes of Wada", "text": "Lakes of Wada\n\nIn mathematics, the are three disjoint connected open sets of the plane or open unit square with the counterintuitive property that they all have the same boundary. In other words, for any point selected on the boundary of \"one\" of the lakes, the other two lakes' boundaries also contain that point.\n\nMore than two sets with the same boundary are said to have the Wada property; examples include Wada basins in dynamical systems. This property is rare in real-world systems.\n\nThe lakes of Wada were introduced by , who credited the discovery to Takeo Wada. His construction is similar to the construction by of an indecomposable continuum, and in fact it is possible for the common boundary of the three sets to be an indecomposable continuum.\n\nThe Lakes of Wada are formed by starting with a closed unit square of dry land, and then digging 3 lakes according to the following rule:\n\nAfter an infinite number of days, the three lakes are still disjoint connected open sets, and the remaining dry land is the boundary of each of the 3 lakes.\n\nFor example, the first five days might be (see the image on the right):\n\nA variation of this construction can produce a countable infinite number of connected lakes with the same boundary: instead of extending the lakes in the order 1, 2, 0, 1, 2, 0, 1, 2, 0, ..., extend them in the order 0, 0, 1, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 3, 4, ...and so on.\n\nWada basins are certain special basins of attraction studied in the mathematics of non-linear systems. A basin having the property that every neighborhood of every point on the boundary of that basin intersects at least three basins is called a Wada basin, or said to have the Wada property. Unlike the Lakes of Wada, Wada basins are often disconnected.\n\nAn example of Wada basins is given by the Newton–Raphson method applied to a cubic polynomial with distinct roots, such as see the picture.\n\nA physical system that demonstrates Wada basins is the pattern of reflections between three spheres in contact—see chaotic scattering.\n\nIn chaos theory, Wada basins arise very frequently. Usually, the Wada property can be seen in the basin of attraction of dissipative dynamical systems. \nBut the exit basins of Hamiltonian system can also show the Wada property. In the context of the chaotic scattering of systems with multiple exit, basin of exit shows the Wada property. \nM. A. F. Sanjuán et al. had shown that in the Henon-Heiles system the exit basins have this Wada property.\n\n\n"}
{"id": "26520106", "url": "https://en.wikipedia.org/wiki?curid=26520106", "title": "Lie point symmetry", "text": "Lie point symmetry\n\nTowards the end of the nineteenth century, Sophus Lie introduced the notion of Lie group in order to study the solutions of ordinary differential equations (ODEs). He showed the following main property: the order of an ordinary differential equation can be reduced by one if it is invariant under one-parameter Lie group of point transformations. This observation unified and extended the available integration techniques. Lie devoted the remainder of his mathematical career to developing these continuous groups that have now an impact on many areas of mathematically based sciences. The applications of Lie groups to differential systems were mainly established by Lie and Emmy Noether, and then advocated by Élie Cartan.\n\nRoughly speaking, a Lie point symmetry of a system is a local group of transformations that maps every solution of the system to another solution of the same system. In other words, it maps the solution set of the system to itself. Elementary examples of Lie groups are translations, rotations and scalings.\n\nThe Lie symmetry theory is a well-known subject. In it are discussed continuous symmetries opposed to, for example, discrete symmetries. The literature for this theory can be found, among other places, in these notes.\n\nLie groups and hence their infinitesimal generators can be naturally \"extended\" to act on the space of independent variables, state variables (dependent variables) and derivatives of the state variables up to any finite order. There are many other kinds of symmetries. For example, contact transformations let coefficients of the transformations infinitesimal generator depend also on first derivatives of the coordinates. Lie-Bäcklund transformations let them involve derivatives up to an arbitrary order. The possibility of the existence of such symmetries was recognized by Noether. For Lie point symmetries, the coefficients of the infinitesimal generators depend only on coordinates, denoted by formula_1.\n\nLie symmetries were introduced by Lie in order to solve ordinary differential equations. Another application of symmetry methods is to reduce systems of differential equations, finding equivalent systems of differential equations of simpler form. This is called reduction. In the literature, one can find the classical reduction process, and the moving frame-based reduction process. Also symmetry groups can be used for classifying different symmetry classes of solutions.\n\nLie's fundamental theorems underline that Lie groups can be characterized by elements known as \"infinitesimal generators\". These mathematical objects form a Lie algebra of infinitesimal generators. Deduced \"infinitesimal symmetry conditions\" (defining equations of the symmetry group) can be explicitly solved in order to find the closed form of symmetry groups, and thus the associated infinitesimal generators.\n\nLet formula_2 be the set of coordinates on which a system is defined where formula_3 is the cardinal of formula_1. An infinitesimal generator formula_5 in the field formula_6 is a linear operator formula_7 that has formula_8 in its kernel and that satisfies the Leibniz rule:\nIn the canonical basis of elementary derivations formula_10, it is written as:\nwhere formula_12 is in formula_6 for all formula_14 in formula_15.\n\nLie algebras can be generated by a generating set of infinitesimal generators as defined above. To every Lie group, one can associate a Lie algebra. Roughly, a Lie algebra formula_16 is an algebra constituted by a vector space equipped with Lie bracket as additional operation. The base field of a Lie algebra depends on the concept of invariant. Here only finite-dimensional Lie algebras are considered.\n\nA dynamical system (or flow) is a one-parameter group action. Let us denote by formula_17 such a dynamical system, more precisely, a (left-)action of a group formula_18 on a manifold formula_19:\nsuch that for all point formula_1 in formula_19:\n\nA continuous dynamical system is defined on a group formula_25 that can be identified to formula_8 i.e. the group elements are continuous.\n\nAn invariant, roughly speaking, is an element that does not change under a transformation.\n\nIn this paragraph, we consider precisely \"expanded Lie point symmetries\" i.e. we work in an expanded space meaning that the distinction between independent variable, state variables and parameters are avoided as much as possible.\n\nA symmetry group of a system is a continuous dynamical system defined on a local Lie group formula_25 acting on a manifold formula_19. For the sake of clarity, we restrict ourselves to n-dimensional real manifolds formula_33 where formula_3 is the number of system coordinates.\n\nLet us define algebraic systems used in the forthcoming symmetry definition.\n\nLet formula_35 be a finite set of rational functions over the field formula_8 where formula_37 and formula_38 are polynomials in formula_39 i.e. in variables formula_2 with coefficients in formula_8. An \"algebraic system\" associated to formula_42 is defined by the following equalities and inequalities:\n\nAn algebraic system defined by formula_44 is \"regular\" (a.k.a. smooth) if the system formula_42 is of maximal rank formula_46, meaning that the Jacobian matrix formula_47 is of rank formula_46 at every solution formula_1 of the associated semi-algebraic variety.\n\nThe following theorem (see th. 2.8 in ch.2 of ) gives necessary and sufficient conditions so that a local Lie group formula_25 is a symmetry group of an algebraic system.\n\nTheorem. Let formula_25 be a connected local Lie group of a continuous dynamical system acting in the n-dimensional space formula_52. Let formula_53 with formula_54 define a regular system of algebraic equations:\nThen formula_25 is a symmetry group of this algebraic system if, and only if,\nfor every infinitesimal generator formula_5 in the Lie algebra formula_16 of formula_25.\n\nLet us consider the algebraic system defined on a space of 6 variables, namely formula_61 with:\n\nThe infinitesimal generator\nis associated to one of the one-parameter symmetry groups. It acts on 4 variables, namely formula_64 and formula_65. One can easily verify that formula_66 and formula_67. Thus the relations formula_68 are satisfied for any formula_1 in formula_70 that vanishes the algebraic system.\n\nLet us define systems of first-order ODEs used in the forthcoming symmetry definition.\n\nLet formula_71 be a derivation w.r.t. the continuous independent variable formula_72. We consider two sets formula_73 and formula_74. The associated coordinate set is defined by formula_75 and its cardinal is formula_76. With these notations, a \"system of first-order ODEs\" is a system where:\nand the set formula_44 specifies the evolution of state variables of ODEs w.r.t. the independent variable. The elements of the set formula_79 are called \"state variables\", these of formula_80 \"parameters\".\n\nOne can associate also a continuous dynamical system to a system of ODEs by resolving its equations.\n\nAn infinitesimal generator is a derivation that is closely related to systems of ODEs (more precisely to continuous dynamical systems). For the link between a system of ODEs, the associated vector field and the infinitesimal generator, see section 1.3 of. The infinitesimal generator formula_5 associated to a system of ODEs, described as above, is defined with the same notations as follows:\n\nHere is a geometrical definition of such symmetries. Let formula_17 be a continuous dynamical system and formula_84 its infinitesimal generator. A continuous dynamical system formula_85 is a Lie point symmetry of formula_17 if, and only if, formula_85 sends every orbit of formula_17 to an orbit. Hence, the infinitesimal generator formula_89 satisfies the following relation based on Lie bracket:\nwhere formula_91 is any constant of formula_84 and formula_89 i.e. formula_94. These generators are linearly independent.\n\nOne does not need the explicit formulas of formula_17 in order to compute the infinitesimal generators of its symmetries.\n\nConsider Pierre François Verhulst's logistic growth model with linear predation, where the state variable formula_96 represents a population. The parameter formula_97 is the difference between the growth and predation rate and the parameter formula_98 corresponds to the receptive capacity of the environment:\n\nThe continuous dynamical system associated to this system of ODEs is:\nThe independent variable formula_101 varies continuously; thus the associated group can be identified with formula_8.\n\nThe infinitesimal generator associated to this system of ODEs is:\n\nThe following infinitesimal generators belong to the 2-dimensional symmetry group of formula_17:\n\nThere exist many software packages in this area. For example, the package liesymm of Maple provides some Lie symmetry methods for PDEs. It manipulates integration of determining systems and also differential forms. Despite its success on small systems, its integration capabilities for solving determining systems automatically are limited by complexity issues. The DETools package uses the prolongation of vector fields for searching Lie symmetries of ODEs. Finding Lie symmetries for ODEs, in the general case, may be as complicated as solving the original system.\n"}
{"id": "44920908", "url": "https://en.wikipedia.org/wiki?curid=44920908", "title": "Linda Preiss Rothschild", "text": "Linda Preiss Rothschild\n\nLinda Preiss Rothschild (born February 28, 1945) is a professor emeritus of mathematics at the University of California, San Diego. Her thesis research concerned Lie groups, but subsequently her interests broadened to include also polynomial factorization, partial differential equations, harmonic analysis, and the theory of several complex variables.\n\nRothschild is the daughter of Philadelphia fur merchants; she was unable to attend the best high school of the city, as at the time it was restricted to boys. She graduated from the University of Pennsylvania in 1966. Rejected from graduate study at Princeton University because it was also male-only, she instead earned her Ph.D. in 1970 from the Massachusetts Institute of Technology under the supervision of Isadore Singer. She held temporary positions at MIT, Tufts University, Columbia University, the Institute for Advanced Study, and Princeton University before landing an associate professorship at the University of Wisconsin in 1976. She moved to San Diego in 1983, and retired in 2011.\n\nRothschild was president of the Association for Women in Mathematics from 1983 to 1985, and vice president of the American Mathematical Society from 1985 to 1987. She has been co-editor-in-chief of the journal \"Mathematical Research Letters\" since 1994.\n\nHer husband, M. Salah Baouendi, was also a distinguished professor of mathematics at UC San Diego; he died in 2011.\n\nRothschild was awarded a Sloan Fellowship in 1976.\nIn 1997, Rothschild gave the Noether Lecture of the Association for Women in Mathematics, on the subject \"How do Real Manifolds live in Complex Space?\", and she was an invited speaker at the International Congress of Mathematicians in 2006. She was elected as a fellow of the American Academy of Arts and Sciences in 2005, and in 2012 she became one of the inaugural fellows of the American Mathematical Society. A conference in her honor was held in 2008 at the University of Fribourg in Switzerland. In 2017, she was selected as a fellow of the Association for Women in Mathematics in the inaugural class.\n\nShe and her husband were jointly awarded the Stefan Bergman Prize of the American Mathematical Society in 2003.\n\n"}
{"id": "14337630", "url": "https://en.wikipedia.org/wiki?curid=14337630", "title": "May spectral sequence", "text": "May spectral sequence\n\nIn mathematics, the May spectral sequence is a spectral sequence, introduced by . It is used for calculating the initial term of the Adams spectral sequence, which is in turn used for calculating the stable homotopy groups of spheres. The May spectral sequence is described in detail in .\n\n"}
{"id": "3644015", "url": "https://en.wikipedia.org/wiki?curid=3644015", "title": "Michael Sipser", "text": "Michael Sipser\n\nMichael Fredric Sipser (born September 17, 1954) is a theoretical computer scientist who has made early contributions to computational complexity theory. He is a professor of Applied Mathematics and Dean of Science at the Massachusetts Institute of Technology.\n\nSipser was born and raised in Brooklyn, New York and moved to Oswego, New York when he was 12 years old. He earned his BA in mathematics from Cornell University in 1974 and his PhD in engineering from the University of California at Berkeley in 1980 under the direction of Manuel Blum.\n\nHe joined MIT's Laboratory for Computer Science as a research associate in 1979 and then was a Research Staff Member at IBM Research in San Jose. In 1980, he joined the MIT faculty. He spent the 1985-1986 academic year on the faculty of the University of California at Berkeley and then returned to MIT. From 2004 until 2014, he served as head of the MIT Mathematics department. He was appointed Dean of the MIT School of Science in 2014. He is a fellow of the American Academy of Arts and Sciences. In 2015 he was elected as a fellow of the American Mathematical Society \"for contributions to complexity theory and for leadership and service to the mathematical community.\"\nHe was elected as an ACM Fellow in 2017.\n\nSipser specializes in algorithms and complexity theory, specifically efficient error correcting codes, interactive proof systems, randomness, quantum computation, and establishing the inherent computational difficulty of problems. He introduced the method of probabilistic restriction for proving super-polynomial lower bounds on circuit complexity in a paper joint with Merrick Furst and James B. Saxe. Their result was later improved to be an exponential lower bound by Andrew Yao and Johan Håstad.\n\nIn an early derandomization theorem, Sipser showed that BPP is contained in the polynomial hierarchy, subsequently improved by Peter Gács and Clemens Lautemann to form what is now known as the Sipser-Gàcs-Lautemann theorem. Sipser also established a connection between expander graphs and derandomization. He and his PhD student Daniel Spielman introduced expander codes, an application of expander graphs. With fellow graduate student David Lichtenstein, Sipser proved that Go is PSPACE hard.\n\nIn quantum computation theory, he introduced the adiabatic algorithm jointly with Edward Farhi, Jeffrey Goldstone, and Samuel Gutmann.\n\nSipser has long been interested in the P versus NP problem. In 1975, he wagered an ounce of gold with Leonard Adleman that the problem would be solved with a proof that P≠NP by the end of the 20th century. Sipser sent Adleman an American gold eagle coin in 2000 because the problem remained (and remains) unsolved.\n\nSipser is the author of \"Introduction to the Theory of Computation\", a textbook for theoretical computer science.\n\nSipser lives in Cambridge, Massachusetts with his wife, Ina, and has two children: a daughter, Rachel, who graduated from New York University, and a younger son, Aaron, who is an undergraduate at MIT.\n\n"}
{"id": "50983273", "url": "https://en.wikipedia.org/wiki?curid=50983273", "title": "Monique Laurent", "text": "Monique Laurent\n\nMonique Laurent (born 1960) is a French computer scientist and mathematician who is an expert in mathematical optimization. She is a researcher at the Centrum Wiskunde & Informatica in Amsterdam where she is also a member of the Management Team. Laurent also holds a part-time position as a professor of econometrics and operations research at Tilburg University.\n\nLaurent earned a doctorate from Paris Diderot University in 1986, under the supervision of Michel Deza.\nShe worked at CNRS from 1988 to 1997, when she moved to CWI. She took a second position at Tilburg in 2009.\n\nWith Deza, Laurent is the author of the book \"Geometry of Cuts and Metrics\" (Algorithms and Combinatorics 15, Springer, 1997).\n\nShe was an invited speaker at the International Congress of Mathematicians in 2014.\nShe was elected as a fellow of the Society for Industrial and Applied Mathematics in 2017, \"for contributions to discrete and polynomial optimization and revealing interactions between them\".\n\n"}
{"id": "18062676", "url": "https://en.wikipedia.org/wiki?curid=18062676", "title": "Monogenic field", "text": "Monogenic field\n\nIn mathematics, a monogenic field is an algebraic number field \"K\" for which there exists an element \"a\" such that the ring of integers \"O\" is the subring Z[\"a\"] of \"K\" generated by \"a\". Then \"O\" is a quotient of the polynomial ring Z[\"X\"] and the powers of \"a\" constitute a power integral basis.\n\nIn a monogenic field \"K\", the field discriminant of \"K\" is equal to the discriminant of the minimal polynomial of α.\n\nExamples of monogenic fields include:\n\nWhile all quadratic fields are monogenic, already among cubic fields there are many that are not monogenic. The first example of a non-monogenic number field that was found is the cubic field generated by a root of the polynomial formula_11, due to Richard Dedekind.\n\n"}
{"id": "2598825", "url": "https://en.wikipedia.org/wiki?curid=2598825", "title": "Nathan Jacobson", "text": "Nathan Jacobson\n\nNathan Jacobson (October 5, 1910 – December 5, 1999) was an American mathematician.\n\nBorn Nachman Arbiser in Warsaw, Jacobson emigrated to America with his family in 1918. Recognized as one of the leading algebraists of his generation, he wrote more than a dozen standard textbooks. He graduated from the University of Alabama in 1930 and was awarded a doctorate in mathematics from Princeton University in 1934. While working on his thesis, \"Non-commutative polynomials and cyclic algebras\", he was advised by Joseph Wedderburn.\n\nJacobson taught and researched at Bryn Mawr College (1935–1936), the University of Chicago (1936–1937), the University of North Carolina at Chapel Hill (1937–1943), and Johns Hopkins University (1943–1947) before joining Yale University in 1947. He remained at Yale until his retirement.\n\nHe was a member of the National Academy of Sciences and the American Academy of Arts and Sciences. He served as president of the American Mathematical Society from 1971 to 1973, and was awarded their highest honour, the Leroy P. Steele prize for lifetime achievement, in 1998. He was also vice-president of the International Mathematical Union from 1972 to 1974.\n\n\n\n\n"}
{"id": "3003553", "url": "https://en.wikipedia.org/wiki?curid=3003553", "title": "Noether normalization lemma", "text": "Noether normalization lemma\n\nIn mathematics, the Noether normalization lemma is a result of commutative algebra, introduced by Emmy Noether in 1926. It states that for any field \"k\", and any finitely generated commutative \"k\"-algebra \"A\", there exists a nonnegative integer \"d\" and algebraically independent elements \"y\", \"y\", ..., \"y\" in \"A\" \nsuch that \"A\" is a finitely generated module over the polynomial ring \"S\":=\"k\"[\"y\", \"y\", ..., \"y\"].\n\nThe integer \"d\" is uniquely determined; it is the Krull dimension of the ring \"A\". When \"A\" is an integral domain, \"d\" is also the transcendence degree of the field of fractions of \"A\" over \"k\".\n\nThe theorem has a geometric interpretation. Suppose \"A\" is integral. Let \"S\" be the coordinate ring of the \"d\"-dimensional affine space formula_1, and \"A\" as the coordinate ring of some other \"d\"-dimensional affine variety \"X\". Then the inclusion map \"S\" → \"A\" induces a surjective finite morphism of affine varieties formula_2. The conclusion is that any affine variety is a branched covering of affine space.\nWhen \"k\" is infinite, such a branched covering map can be constructed by taking a general projection from an affine space containing \"X\" to a \"d\"-dimensional subspace.\n\nMore generally, in the language of schemes, the theorem can equivalently be stated as follows: every affine \"k\"-scheme (of finite type) \"X\" is finite over an affine \"n\"-dimensional space. The theorem can be refined to include a chain of ideals of \"R\" (equivalently, closed subsets of \"X\") that are finite over the affine coordinate subspaces of the appropriate dimensions.\n\nThe form of the Noether normalization lemma stated above can be used as an important step in proving Hilbert's Nullstellensatz. This gives it further geometric importance, at least formally, as the Nullstellensatz underlies the development of much of classical algebraic geometry. The theorem is also an important tool in establishing the notions of Krull dimension for \"k\"-algebras.\n\nThe following proof is due to Nagata and is taken from Mumford's red book. A proof in the geometric flavor is also given in the page 127 of the red book and this mathoverflow thread.\n\nThe ring \"A\" in the lemma is generated as a \"k\"-algebra by elements, say, formula_3. We shall induct on \"m\". If formula_4, then the assertion is trivial. Assume now formula_5. It is enough to show that there is a subring \"S\" of \"A\" that is generated by formula_6 elements, such that \"A\" is finite over \"S.\" Indeed, by the inductive hypothesis, we can find algebraically independent elements formula_7 of \"S\" such that \"S\" is finite over formula_8. \n\nSince otherwise there would be nothing to prove, we can also assume that there is a nonzero polynomial \"f\" in \"m\" variables over \"k\" such that\nGiven an integer \"r\" which is determined later, set\nThen the preceding reads:\nNow, the highest term in formula_12 of formula_13 looks\nThus, if \"r\" is larger than any exponent formula_15 appearing in \"f\", then the highest term of formula_16 in formula_12 also has the form as above. In other words, formula_12 is integral over formula_19. Since formula_20 are also integral over that ring, \"A\" is integral over \"S\". It follows \"A\" is finite over \"S,\" and since \"S\" is generated by \"m-1\" elements, by the inductive hypothesis we are done.\n\nIf \"A\" is an integral domain, then \"d\" is the transcendence degree of its field of fractions. Indeed, \"A\" and formula_21 have the same transcendence degree (i.e., the degree of the field of fractions) since the field of fractions of \"A\" is algebraic over that of \"S\" (as \"A\" is integral over \"S\") and \"S\" obviously has transcendence degree \"d\". Thus, it remains to show the Krull dimension of the polynomial ring \"S\" is \"d\". (this is also a consequence of dimension theory.) We induct on \"d\", formula_22 being trivial. Since formula_23 is a chain of prime ideals, the dimension is at least \"d\". To get the reverse estimate, let formula_24 be a chain of prime ideals. Let formula_25. We apply the noether normalization and get formula_26 (in the normalization process, we're free to choose the first variable) such that \"S\" is integral over \"T\". By inductive hypothesis, formula_27 has dimension \"d\" - 1. By incomparability, formula_28 is a chain of length formula_29 and then, in formula_30, it becomes a chain of length formula_6. Since formula_32, we have formula_33. Hence, formula_34.\n\nThe following refinement appears in Eisenbud's book, which builds on Nagata's idea:\n\nGeometrically speaking, the last part of the theorem says that for formula_35 any general linear projection formula_36 induces a finite morphism formula_37 (cf. the lede); besides Eisenbud, see also .\n\nThe proof of generic freeness (the statement later) illustrates a typical yet nontrivial application of the normalization lemma. The generic freeness says: let formula_38 be rings such that formula_39 is a Noetherian integral domain and suppose there is a ring homomorphism formula_40 that exhibits formula_41 as a finitely generated algebra over formula_39. Then there is some formula_43 such that formula_44 is a free formula_45-module.\n\nLet formula_46 be the fraction field of formula_39. We argue by induction on the Krull dimension of formula_48. The basic case is when the Krull dimension is formula_49; i.e., formula_50. This is to say there is some formula_43 such that formula_52 and so formula_44 is free as an formula_45-module. For the inductive step, note formula_48 is a finitely generated formula_46-algebra. Hence, by the Noether normalization lemma, formula_48 contains algebraically independent elements formula_58 such that formula_48 is finite over the polynomial ring formula_60. Multiplying each formula_61 by elements of formula_39, we can assume formula_61 are in formula_41. We now consider:\nIt need not be the case that formula_41 is finite over formula_39. But that will be the case after invertible one element, as follows. If formula_68 is an element of formula_41, then, as an element of formula_48, it is integral over formula_60; i.e., formula_72 for some formula_73 in formula_60. Thus, some formula_43 kills all the denominators of the coefficients of formula_73 and so formula_68 is integral over formula_78. Choosing some finitely many generators of formula_41 as an formula_80-algebra and applying this observation to each generator, we find some formula_43 such that formula_44 is integral (thus finite) over formula_78. Replace formula_84 by formula_85 and then we can assume formula_41 is finite over formula_87.\nTo finish, consider a finite filtration formula_88 by formula_80-submodules such that formula_90 for prime ideals formula_91 (such a filtration exists by the theory of associated primes). For each \"i\", if formula_92, by inductive hypothesis, we can choose some formula_93 in formula_39 such that formula_95 is free as an formula_96-module, while formula_80 is a polynomial ring and thus free. Hence, with formula_98, formula_44 is a free module over formula_45. formula_101\n\n"}
{"id": "30875811", "url": "https://en.wikipedia.org/wiki?curid=30875811", "title": "Non-standard model", "text": "Non-standard model\n\nIn model theory, a discipline within mathematical logic, a non-standard model is a model of a theory that is not isomorphic to the intended model (or standard model).\n\nIf the intended model is infinite and the language is first-order, then the Löwenheim–Skolem theorems guarantee the existence of non-standard models. The non-standard models can be chosen as elementary extensions or elementary substructures of the intended model.\n\nNon-standard models are studied in set theory, non-standard analysis and non-standard models of arithmetic.\n\n"}
{"id": "17160278", "url": "https://en.wikipedia.org/wiki?curid=17160278", "title": "Overlap–save method", "text": "Overlap–save method\n\nOverlap–save is the traditional name for an efficient way to evaluate the discrete convolution between a very long signal formula_1 and a finite impulse response (FIR) filter formula_2:\n\nwhere h[m]=0 for m outside the region [1, \"M\"].\nThe concept is to compute short segments of \"y\"[\"n\"] of an arbitrary length \"L\", and concatenate the segments together. Consider a segment that begins at \"n\" = \"kL\" + \"M\", for any integer \"k\", and define:\n\nThen, for \"kL\" + \"M\"  ≤  \"n\"  ≤  \"kL\" + \"L\" + \"M\" − 1, and equivalently \"M\"  ≤  \"n\" − \"kL\"  ≤  \"L\" + \"M\" − 1, we can write:\n\nThe task is thereby reduced to computing \"y\"[\"n\"], for \"M\"  ≤  \"n\"  ≤  \"L\" +\" M\" − 1. The process described above is illustrated in the accompanying figure.\n\nNow note that if we periodically extend \"x\"[\"n\"] with period \"N\"  ≥  \"L\" + \"M\" − 1, according to:\n\nthe convolutions  formula_7  and  formula_8  are equivalent in the region \"M\"  ≤  \"n\"  ≤  \"L\" + \"M\" − 1. So it is sufficient to compute the N-point circular (or cyclic) convolution of formula_9 with formula_10  in the region [1, \"N\"].  The subregion [\"M\", \"L\" + \"M\" − 1] is appended to the output stream, and the other values are discarded.\n\nThe advantage is that the circular convolution can be computed very efficiently as follows, according to the circular convolution theorem:\n\nwhere:\n\ncodice_1\n\nWhen the DFT and its inverse is implemented by the FFT algorithm, the pseudocode above requires about N log(N) + N complex multiplications for the FFT, product of arrays, and IFFT. Each iteration produces N-M+1 output samples, so the number of complex multiplications per output sample is about:\n\nFor example, when M=201 and N=1024, equals 13.67, whereas direct evaluation of would require up to 201 complex multiplications per output sample, the worst case being when both x and h are complex-valued. Also note that for any given M, has a minimum with respect to N. It diverges for both small and large block sizes.\n\n\"Overlap–discard\" and \"Overlap–scrap\" are less commonly used labels for the same method described here. However, these labels are actually better (than \"overlap–save\") to distinguish from overlap–add, because both methods \"save\", but only one discards. \"Save\" merely refers to the fact that \"M\" − 1 input (or output) samples from segment \"k\" are needed to process segment \"k\" + 1.\n\nThe overlap–save algorithm may be extended to include other common operations of a system:\n\n\n\n\n"}
{"id": "34789305", "url": "https://en.wikipedia.org/wiki?curid=34789305", "title": "Paratingent cone", "text": "Paratingent cone\n\nIn mathematics, the paratingent cone and contingent cone were introduced by , and are closely related to tangent cones.\n"}
{"id": "4050532", "url": "https://en.wikipedia.org/wiki?curid=4050532", "title": "Pi-system", "text": "Pi-system\n\nIn mathematics, a -system (or pi-system) on a set Ω is a collection \"P\" of certain subsets of Ω, such that\n\n\nThat is, \"P\" is a non-empty family of subsets of Ω that is closed under finite intersections.\nThe importance of -systems arises from the fact that if two probability measures agree on a -system, then they agree on the \"σ\"-algebra generated by that -system. Moreover, if other properties, such as equality of integrals, hold for the -system, then they hold for the generated \"σ\"-algebra as well. This is the case whenever the collection of subsets for which the property holds is a \"λ\"-system. -systems are also useful for checking independence of random variables.\n\nThis is desirable because in practice, -systems are often simpler to work with than \"σ\"-algebras. For example, it may be awkward to work with \"σ\"-algebras generated by infinitely many sets formula_1. So instead we may examine the union of all \"σ\"-algebras generated by finitely many sets formula_2. This forms a -system that generates the desired \"σ\"-algebra. Another example is the collection of all interval subsets of the real line, along with the empty set, which is a -system that generates the very important Borel \"σ\"-algebra of subsets of the real line.\n\n\nA \"λ\"-system on Ω is a set \"D\" of subsets of Ω, satisfying\n\nWhilst it is true that any \"σ\"-algebra satisfies the properties of being both a -system and a \"λ\"-system, it is not true that any -system is a \"λ\"-system, and moreover it is not true that any -system is a \"σ\"-algebra. However, a useful classification is that any set system which is both a \"λ\"-system and a -system is a \"σ\"-algebra. This is used as a step in proving the -\"λ\" theorem.\n\nLet formula_17 be a \"λ\"-system, and let formula_18 be a -system contained in formula_17. The -\"λ\" Theorem states that the \"σ\"-algebra formula_20 generated by formula_21 is contained in formula_17: formula_23.\n\nThe -\"λ\" theorem can be used to prove many elementary measure theoretic results. For instance, it is used in proving the uniqueness claim of the Carathéodory extension theorem for \"σ\"-finite measures.\n\nThe -\"λ\" theorem is closely related to the monotone class theorem, which provides a similar relationship between monotone classes and algebras, and can be used to derive many of the same results. Since -systems are simpler classes than algebras, it can be easier to identify the sets that are in them while, on the other hand, checking whether the property under consideration determines a \"λ\"-system is often relatively easy. Despite the difference between the two theorems, the -\"λ\" theorem is sometimes referred to as the monotone class theorem.\n\nLet \"μ\", \"μ\" : \"F\" → \"R\" be two measures on the \"σ\"-algebra \"F\", and suppose that \"F\" = \"σ\"(\"I\") is generated by a -system \"I\". If\nthen \"μ\" = \"μ\".\nThis is the uniqueness statement of the Carathéodory extension theorem for finite measures. If this result does not seem very remarkable, consider the fact that it usually is very difficult or even impossible to fully describe every set in the \"σ\"-algebra, and so the problem of equating measures would be completely hopeless without such a tool.\n\nIdea of proof\nDefine the collection of sets\nBy the first assumption, \"μ\" and \"μ\" agree on \"I\" and thus \"I\" ⊆ \"D\". By the second assumption, Ω ∈ \"D\", and it can further be shown that \"D\" is a \"λ\"-system. It follows from the -\"λ\" theorem that \"σ\"(\"I\") ⊆ \"D\" ⊆ \"σ\"(\"I\"), and so \"D\" = \"σ\"(\"I\"). That is to say, the measures agree on \"σ\"(\"I\").\n\n-systems are more commonly used in the study of probability theory than in the general field of measure theory. This is primarily due to probabilistic notions such as independence, though it may also be a consequence of the fact that the -\"λ\" theorem was proven by the probabilist Eugene Dynkin. Standard measure theory texts typically prove the same results via monotone classes, rather than -systems.\n\nThe -\"λ\" theorem motivates the common definition of the probability distribution of a random variable formula_25 in terms of its cumulative distribution function. Recall that the cumulative distribution of a random variable is defined as\nwhereas the seemingly more general \"law\" of the variable is the probability measure \nwhere formula_28 is the Borel \"σ\"-algebra. We say that the random variables formula_29, and formula_30 (on two possibly different probability spaces) are equal in distribution (or \"law\"), formula_31, if they have the same cumulative distribution functions, \"F\" = \"F\". The motivation for the definition stems from the observation that if \"F\" = \"F\", then that is exactly to say that formula_32 and formula_33 agree on the -system formula_34 which generates formula_28, and so by the example above: formula_36.\n\nA similar result holds for the joint distribution of a random vector. For example, suppose \"X\" and \"Y\" are two random variables defined on the same probability space formula_37, with respectively generated -systems formula_38 and formula_39. The joint cumulative distribution function of (\"X\",\"Y\") is\nHowever, formula_41 and formula_42. Since \nis a -system generated by the random pair (\"X\",\"Y\"), the -\"λ\" theorem is used to show that the joint cumulative distribution function suffices to determine the joint law of (\"X\",\"Y\"). In other words, (\"X\",\"Y\") and (\"W\",\"Z\") have the same distribution if and only if they have the same joint cumulative distribution function.\n\nIn the theory of stochastic processes, two processes formula_44 are known to be equal in distribution if and only if they agree on all finite-dimensional distributions. i.e. for all formula_45.\nThe proof of this is another application of the -\"λ\" theorem.\n\nThe theory of -system plays an important role in the probabilistic notion of independence. If \"X\" and \"Y\" are two random variables defined on the same probability space formula_37 then the random variables are independent if and only if their -systems formula_48 satisfy\nwhich is to say that formula_50 are independent. This actually is a special case of the use of -systems for determining the distribution of (\"X\",\"Y\").\n\nLet formula_51, where formula_52 are iid standard normal random variables. Define the radius and argument (arctan) variables\nThen formula_54 and formula_55 are independent random variables.\n\nTo prove this, it is sufficient to show that the -systems formula_56 are independent: i.e.\nConfirming that this is the case is an exercise in changing variables. Fix formula_58, then the probability can be expressed as an integral of the probability density function of formula_59.\n\n\n"}
{"id": "44640159", "url": "https://en.wikipedia.org/wiki?curid=44640159", "title": "Scale co-occurrence matrix", "text": "Scale co-occurrence matrix\n\nScale co-occurrence matrix (SCM) is a method for image feature extraction within scale space after wavelet transformation, proposed by Wu Jun and Zhao Zhongming (Institute of Remote Sensing Application, China). In practice, we first do discrete wavelet transformation for one gray image and get sub images with different scales. Then we construct a series of scale based concurrent matrixes, every matrix describing the gray level variation between two adjacent scales. Last we use selected functions (such as Harris statistical approach) to calculate measurements with SCM and do feature extraction and classification. \nOne basis of the method is the fact: way texture information changes from one scale to another can represent that texture in some extent thus it can be used as a criterion for feature extraction. The matrix captures the relation of features between different scales rather than the features within a single scale space, which can represent the scale property of texture better. Also, there are several experiments showing that it can get more accurate results for texture classification than the traditional texture classification.\n\nTexture can be regarded as a similarity grouping in an image. Traditional texture analysis can be divided into four major issues: feature extraction, texture discrimination, texture classification and shape from texture(to reconstruct 3D surface geometry from texture information). For tradition feature extraction, approaches are usually categorized into structural, statistical, model based and transform. \nWavelet transformation is a popular method in numerical analysis and functional analysis, which captures both frequency and location information. Gray level co-occurrence matrix provides an important basis for SCM construction. \nSCM based on discrete wavelet frame transformation make use of both correlations and feature information so that it combines structural and statistical benefits.\n\nIn order to do SCM we have to use discrete wavelet frame (DWF) transformation first to get a series of sub images. The discrete wavelet frames is nearly identical to the standard wavelet transform, except that one upsamples the filters, rather than downsamples the image. Given an image, the DWF decomposes its channel using the same method as the wavelet transform, but without the subsampling process. This results in four filtered images with the same size as the input image. The decomposition is then continued in the LL channels only as in the wavelet transform, but since the image is not subsampled, the filter has to be upsampled by inserting zeros in between its coefficients. The number of channels, hence the number of features for DWF is given by 3 × l − 1.\nOne dimension discrete wavelet frame decompose the image in this way:\n\nIf there are two sub images \"X\" and \"X\" from the parent image \"X\" (in practice \"X\" = \"X\"), \"X\" = [1 1;1 2], \"X\" = [1 1;1 4],the grayscale is 4 so that we can get \"k\" = 1, \"G\" = 4.\n\"X\"(1,1), (1,2) and (2,1) are 1, while \"X\"(1,1), (1,2) and (2,1) are 1, thus Φ(1,1) = 3; Similarly, Φ(2,4) = 1.\nThe SCM is as following:\n\n"}
{"id": "11597641", "url": "https://en.wikipedia.org/wiki?curid=11597641", "title": "Schur functor", "text": "Schur functor\n\nIn mathematics, especially in the field of representation theory, Schur functors are certain functors from the category of modules over a fixed commutative ring to itself. They generalize the constructions of exterior powers and symmetric powers of a vector space. Schur functors are indexed by Young diagrams in such a way that the horizontal diagram with \"n\" cells corresponds to the \"n\"th exterior power functor, and the vertical diagram with \"n\" cells corresponds to the \"n\"th symmetric power functor. If a vector space \"V\" is a representation of a group \"G\", then formula_1 also has a natural action of \"G\" for any Schur functor formula_2.\n\nSchur functors are indexed by partitions and are described as follows. Let \"R\" be a commutative ring, \"E\" an \"R\"-module\nand λ a partition of a positive integer \"n\". Let \"T\" be a Young tableau of shape λ, thus indexing the factors of the \"n\"-fold direct product, \"E\" × \"E\" × ... × \"E\", with the boxes of \"T\". Consider those maps of \"R\"-modules formula_3 satisfying the following conditions\n\n(1) formula_4 is multilinear,\n\n(2) formula_4 is alternating in the entries indexed by each column of \"T\",\n\n(3) formula_4 satisfies an exchange condition stating that if formula_7 are numbers from column \"i\" of \"T\" then\n\nwhere the sum is over \"n\"-tuples \"x' \" obtained from \"x\" by exchanging the elements indexed by \"I\" with any formula_9 elements indexed by the numbers in column formula_10 (in order).\n\nThe universal \"R\"-module formula_11 that extends formula_4 to a mapping of \"R\"-modules formula_13 is the image of \"E\" under the Schur functor indexed by λ.\n\nFor an example of the condition (3) placed on formula_4\nsuppose that λ is the partition formula_15 and the tableau\n\"T\" is numbered such that its entries are 1, 2, 3, 4, 5 when read\ntop-to-bottom (left-to-right). Taking formula_16 (i.e.,\nthe numbers in the second column of \"T\") we have\n\nwhile if formula_18 then\n\nFix a vector space \"V\" over a field of characteristic zero. We identify partitions and the corresponding Young diagrams. The following descriptions hold:\n\nLet \"V\" be a complex vector space of dimension \"k\". It's a tautological representation of its automorphism group GL(\"V\"). If λ is a diagram where each row has no more than \"k\" cells, then S(\"V\") is an irreducible GL(\"V\")-representation of highest weight λ. In fact, any rational representation of GL(\"V\") is isomorphic to a direct sum of representations of the form S(\"V\") ⊗ det(\"V\"), where λ is a Young diagram with each row strictly shorter than \"k\", and \"m\" is any (possibly negative) integer.\n\nIn this context Schur-Weyl duality states that as a formula_21-module\n\nwhere formula_23 is the number of standard young tableaux of shape λ. More generally, we have the decomposition of the tensor product as formula_24-bimodule\n\nwhere formula_26 is the Specht module indexed by λ. Schur functors can also be used to describe the coordinate ring of certain flag varieties.\n\nFor two Young diagrams λ and μ consider the composition of the corresponding Schur functors S(S(-)). This composition is called a plethysm of λ and μ. From the general theory it's known that, at least for vector spaces over a characteristic zero field, the plethysm is isomorphic to a direct sum of Schur functors. The problem of determining which Young diagrams occur in that description and how to calculate their multiplicities is open, aside from some special cases like Sym(Sym(\"V\")).\n\n\n\n"}
{"id": "2789076", "url": "https://en.wikipedia.org/wiki?curid=2789076", "title": "Schwartz–Zippel lemma", "text": "Schwartz–Zippel lemma\n\nIn mathematics, the Schwartz–Zippel lemma (also called the DeMillo-Lipton-Schwartz–Zippel lemma) is a tool commonly used in probabilistic polynomial identity testing, i.e. in the problem of determining whether a given multivariate polynomial is the\n0-polynomial (or identically equal to 0). It was discovered independently by Jack Schwartz, Richard Zippel, and Richard DeMillo and Richard J. Lipton, although DeMillo and Lipton's slightly weaker version was shown a year prior to Schwartz and Zippel's result. The finite field version of this bound was proved by Øystein Ore in 1922.\n\nThe input to the problem is an \"n\"-variable polynomial over a field F. It can occur in the following forms:\n\nFor example, is\n\nTo solve this, we can multiply it out and check that all the coefficients are 0. However, this takes exponential time. In general, a polynomial can be algebraically represented by an arithmetic formula or circuit.\n\nLet\n\nbe the determinant of the polynomial matrix.\n\nCurrently, there is no known sub-exponential time algorithm that can solve this problem deterministically. However, there are randomized polynomial algorithms for testing polynomial identities. Their analysis usually requires a bound on the probability that a non-zero polynomial will have roots at randomly selected test points. The Schwartz–Zippel lemma provides this as follows:\n\nTheorem 1 (Schwartz, Zippel). \"Let\"\n\n\"be a non-zero polynomial of total degree\" \"over a field F. Let S be a finite subset of F and let\" \"be selected at random independently and uniformly from S. Then\"\n\nIn the single variable case, this follows directly from the fact that a polynomial of degree \"d\" can have no more than \"d\" roots. It seems logical, then, to think that a similar statement would hold for multivariable polynomials. This is, in fact, the case.\n\n\"Proof.\" The proof is by mathematical induction on \"n\". For , as was mentioned before, \"P\" can have at most \"d\" roots. This gives us the base case.\nNow, assume that the theorem holds for all polynomials in variables. We can then consider \"P\" to be a polynomial in \"x\" by writing it as\n\nSince is not identically 0, there is some such that formula_6 is not identically 0. Take the largest such . Then formula_7, since the degree of formula_8 is at most d.\n\nNow we randomly pick formula_9 from . By the induction hypothesis, formula_10\n\nIf formula_11, then formula_12 is of degree (and thus not identically zero) so\n\nIf we denote the event formula_14 by , the event formula_15 by , and the complement of by formula_16, we have\n\nThe importance of the Schwartz–Zippel Theorem and Testing Polynomial Identities follows\nfrom algorithms which are obtained to problems that can be reduced to the problem\nof polynomial identity testing.\n\n\"Given a pair of polynomials formula_18 and formula_19, is\"\n\nThis problem can be solved by reducing it to the problem of polynomial identity testing. It is equivalent to checking if\n\nHence if we can determine that\nwhere\n\nthen we can determine whether the two polynomials are equivalent.\n\nComparison of polynomials has applications for branching programs (also called binary decision diagrams). A read-once branching program can be represented by a multilinear polynomial which computes (over any field) on {0,1}-inputs the same Boolean function as the branching program, and two branching programs compute the same function if and only if the corresponding polynomials are equal. Thus, identity of Boolean functions computed by read-once branching programs can be reduced to polynomial identity testing.\n\nComparison of two polynomials (and therefore testing polynomial identities) also has\napplications in 2D-compression, where the problem of finding the equality of two\n2D-texts \"A\" and \"B\" is reduced to the problem\nof comparing equality of two polynomials formula_24 and formula_25.\n\n\"Given formula_26, is formula_27 a prime number?\"\n\nA simple randomized algorithm developed by Manindra Agrawal and Somenath Biswas can determine probabilistically\nwhether formula_27 is prime and uses polynomial identity testing to do so.\n\nThey propose that all prime numbers \"n\" (and only prime numbers) satisfy the following\npolynomial identity:\n\nThis is a consequence of the Frobenius endomorphism.\n\nLet\n\nThen formula_31 \"iff n is prime\". The proof can be found in [4]. However, \nsince this polynomial has degree formula_27, and since formula_27 may or may not be a prime, \nthe Schwartz–Zippel method would not work. Agrawal and Biswas use a more sophisticated technique, which divides \nformula_34 by a random monic polynomial of small degree.\n\nPrime numbers are used in a number of applications such as hash table sizing, pseudorandom number\ngenerators and in key generation for cryptography. Therefore, finding very large prime numbers\n(on the order of (at least) formula_35) becomes very important and efficient primality testing algorithms\nare required.\n\n\"Let formula_36 be a graph of vertices where is even. Does contain a perfect matching?\"\n\nTheorem 2 : \"A Tutte matrix determinant is not a -polynomial if and only if there exists a perfect matching.\"\n\nA subset of is called a matching if each vertex in is incident with at most one edge in . A matching is perfect if each vertex in has exactly one edge that is incident to it in . Create a \"Tutte matrix\" in the following way:\n\nwhere\n\nThe Tutte matrix determinant (in the variables \"x\", ) is then defined as the determinant of this skew-symmetric matrix which coincides with the square of the pfaffian of the matrix \"A\" and is non-zero (as polynomial) if and only if a perfect matching exists.\nOne can then use polynomial identity testing to find whether contains a perfect matching. There exists a deterministic black-box algorithm for graphs with polynomially bounded permanents (Grigoriev & Karpinski 1987).\n\nIn the special case of a balanced bipartite graph on formula_39 vertices this matrix takes the form of a block matrix \nif the first \"m\" rows (resp. columns) are indexed with the first subset of the bipartition and the last \"m\" rows with the complementary subset. In this case the pfaffian coincides with the usual determinant of the \"m\" × \"m\" matrix \"X\" (up to sign). Here \"X\" is the Edmonds matrix.\n\n\n"}
{"id": "343400", "url": "https://en.wikipedia.org/wiki?curid=343400", "title": "Simple theorems in the algebra of sets", "text": "Simple theorems in the algebra of sets\n\nThe simple theorems in the algebra of sets are some of the elementary properties of the algebra of union (infix ∪), intersection (infix ∩), and set complement (postfix ') of sets.\n\nThese properties assume the existence of at least two sets: a given universal set, denoted U, and the empty set, denoted {}. The algebra of sets describes the properties of all possible subsets of U, called the power set of U and denoted \"P\"(U). \"P\"(U) is assumed closed under union, intersection, and set complement. The algebra of sets is an interpretation or model of Boolean algebra, with union, intersection, set complement, U, and {} interpreting Boolean sum, product, complement, 1, and 0, respectively.\n\nThe properties below are stated without proof, but can be derived from a small number of properties taken as axioms. A \"*\" follows the algebra of sets interpretation of Huntington's (1904) classic postulate set for Boolean algebra. These properties can be visualized with Venn diagrams. They also follow from the fact that \"P\"(U) is a Boolean lattice. The properties followed by \"L\" interpret the lattice axioms.\n\nElementary discrete mathematics courses sometimes leave students with the impression that the subject matter of set theory is no more than these properties. For more about elementary set theory, see set, set theory, algebra of sets, and naive set theory. For an introduction to set theory at a higher level, see also axiomatic set theory, cardinal number, ordinal number, Cantor–Bernstein–Schroeder theorem, Cantor's diagonal argument, Cantor's first uncountability proof, Cantor's theorem, well-ordering theorem, axiom of choice, and Zorn's lemma.\n\nThe properties below include a defined binary operation, relative complement, denoted by infix \"\\\". The \"relative complement of \"A\" in \"B\",\" denoted \"B\" \\\"A\", is defined as (\"A\" ∪\"B\"′)′ and as \"A\"′ ∩\"B\".\nPROPOSITION 1. For any U and any subset \"A\" of U:\nPROPOSITION 2. For any sets \"A\", \"B\", and \"C\":\nThe distributive laws:\nPROPOSITION 3. Some properties of ⊆:\n\n"}
{"id": "35652136", "url": "https://en.wikipedia.org/wiki?curid=35652136", "title": "Snell envelope", "text": "Snell envelope\n\nThe Snell envelope, used in stochastics and mathematical finance, is the smallest supermartingale dominating a stochastic process. The Snell envelope is named after James Laurie Snell.\n\nGiven a filtered probability space formula_1 and an absolutely continuous probability measure formula_2 then an adapted process formula_3 is the Snell envelope with respect to formula_4 of the process formula_5 if \n\nGiven a (discrete) filtered probability space formula_18 and an absolutely continuous probability measure formula_2 then the Snell envelope formula_20 with respect to formula_4 of the process formula_22 is given by the recursive scheme\nwhere formula_26 is the join.\n\n"}
{"id": "15869259", "url": "https://en.wikipedia.org/wiki?curid=15869259", "title": "Standard array", "text": "Standard array\n\nIn coding theory, a standard array (or Slepian array) is a formula_1 by formula_2 array that lists all elements of a particular formula_3 vector space. Standard arrays are used to decode linear codes; i.e. to find the corresponding codeword for any received vector.\n\nA standard array for an [\"n\",\"k\"]-code is a formula_1 by formula_2 array where:\n\n\nFor example, the [\"5\",\"2\"]-code formula_6 = {0, 01101, 10110, 11011} has a standard array as follows:\n\nNote that the above is only one possibility for the standard array; had 00011 been chosen as the first coset leader of weight two, another standard array representing the code would have been constructed.\n\nNote that the first row contains the 0 vector and the codewords of formula_6 (0 itself being a codeword). Also, the leftmost column contains the vectors of minimum weight enumerating vectors of weight 1 first and then using vectors of weight 2. Note also that each possible vector in the vector space appears exactly once.\n\nBecause each possible vector can appear only once in a standard array some care must be taken during construction. A standard array can be created as follows:\n\n\nNote that adding vectors is done mod q. For example, binary codes are added mod 2 (which equivalent to bit-wise XOR addition). For example, in formula_9, 11000 + 11011 = 00011.\n\nNote also that selecting different coset leaders will create a slightly different but equivalent standard array, and will not affect results when decoding. \n\nLet formula_8 be the binary [4,2]-code. i.e. C = {0000, 1011, 0101, 1110}. To construct the standard array, we first list the codewords in a row.\n\nWe then select a vector of minimum weight (in this case, weight 1) that has not been used. This vector becomes the coset leader for the second row.\n\nFollowing step 3, we complete the row by adding the coset leader to each codeword.\n\nWe then repeat steps 2 and 3 until we have completed all rows. We stop when we have reached formula_11 rows.\n\nNote that in this example we could not have chosen the vector 0001 as the coset leader of the final row, even though it meets the critedia of having minimal weight (1), because the vector was already present in the array. We could, however, have chosen it as the first coset leader and constructed a different standard array.\n\nTo decode a vector using a standard array, subtract the error vector - or coset leader - from the vector received. The result will be one of the codewords in formula_8. For example, say we are using the code C = {0000, 1011, 0101, 1110}, and have constructed the corresponding standard array, as shown from the example above. If we receive the vector 0110 as a message, we find that vector in the standard array. We then subtract the vector's coset leader, namely 1000, to get the result 1110. We have received the codeword 1110.\n\nDecoding via a standard array is a form of nearest neighbour decoding. In practice, decoding via a standard array requires large amounts of storage - a code with 32 codewords requires a standard array with formula_13 entries. Other forms of decoding, such as syndrome decoding, are more efficient.\n\nNote that decoding via standard array does not guarantee that all vectors are decoded correctly. If we receive the vector 1010, using the standard array above would decode the message as 1110, a codeword distance 1 away. However, 1010 is also distance 1 away from the codeword 1011. In such a case some implementations might ask for the message to be resent, or the ambiguous bit may be marked as an erasure and a following outer code may correct it. This ambiguity is another reason that different decoding methods are sometimes used.\n\n"}
{"id": "22231261", "url": "https://en.wikipedia.org/wiki?curid=22231261", "title": "Sum of radicals", "text": "Sum of radicals\n\nIn computational complexity theory, there is an open problem of whether some information about a sum of radicals may be computed in polynomial time depending on the input size, i.e., in the number of bits necessary to represent this sum. It is of importance for many problems in computational geometry, since the computation of the Euclidean distance between two points in the general case involves the computation of a square root, and therefore the perimeter of a polygon or the length of a polygonal chain takes the form of a sum of radicals. \n\nThe sum of radicals is defined as a finite linear combination of radicals:\n\nwhere formula_2 are natural numbers and formula_3 are real numbers.\n\nMost theoretical research in computational geometry of combinatorial character assumes the computational model of infinite precision real RAM, i.e., an abstract computer in which real numbers and operations on them are performed with infinite precision and the input size of a real number and the cost of an elementary operation are constants. However, there is research in computational complexity, especially in computer algebra, where the input size of a number is the number of bits necessary for its representation. \n\nIn particular, of interest in computational geometry is the problem of determining the sign of the sum of radicals. For instance, the length of a polygonal path in which all vertices have integer coordinates may be expressed using the Pythagorean theorem as a sum of integer square roots, so in order to determine whether one path is longer or shorter than another in a Euclidean shortest path problem, it is necessary to determine the sign of an expression in which the first path's length is subtracted from the second; this expression is a sum of radicals.\n\nIn a similar way, the sum of radicals problem is inherent in the problem of minimum-weight triangulation in the Euclidean metric. \n\nIn 1991, Blömer proposed a polynomial time Monte Carlo algorithm for determining whether a sum of radicals is zero, or more generally whether it represents a rational number. \nWhile Blömer's result does not resolve the computational complexity of finding the sign of the sum of radicals, it does imply that if the latter problem is in class NP, then it is also in co-NP.\n\n"}
{"id": "90465", "url": "https://en.wikipedia.org/wiki?curid=90465", "title": "Super-Poulet number", "text": "Super-Poulet number\n\nA super-Poulet number is a Poulet number, or pseudoprime to base 2, whose every divisor \"d\" divides\n\nFor example, 341 is a super-Poulet number: it has positive divisors {1, 11, 31, 341} and we have:\n\nWhen formula_1 is not prime, then it and every divisor of it are a pseudoprime to base 2, and a super-Poulet number.\nThe super-Poulet numbers below 10,000 are :\n\nIt is relatively easy to get super-Poulet numbers with 3 distinct prime divisors. If you find three Poulet numbers with three common prime factors, you get a super-Poulet number, as you built the product of the three prime factors.\n\nExample:\n2701 = 37 * 73 is a Poulet number, \n4033 = 37 * 109 is a Poulet number, \n7957 = 73 * 109 is a Poulet number;\n\nso 294409 = 37 * 73 * 109 is a Poulet number too.\n\nSuper-Poulet numbers with up to 7 distinct prime factors you can get with the following numbers:\n\n\nFor example, 1118863200025063181061994266818401 = 6421 * 12841 * 51361 * 57781 * 115561 * 192601 * 205441 is a super-Poulet number with 7 distinct prime factors and 120 Poulet numbers.\n\n"}
{"id": "51714", "url": "https://en.wikipedia.org/wiki?curid=51714", "title": "Taylor's theorem", "text": "Taylor's theorem\n\nIn calculus, Taylor's theorem gives an approximation of a \"k\"-times differentiable function around a given point by a \"k\"-th order Taylor polynomial. For analytic functions the Taylor polynomials at a given point are finite-order truncations of its Taylor series, which completely determines the function in some neighborhood of the point. The exact content of \"Taylor's theorem\" is not universally agreed upon. Indeed, there are several versions of it applicable in different situations, and some of them contain explicit estimates on the approximation error of the function by its Taylor polynomial.\n\nTaylor's theorem is named after the mathematician Brook Taylor, who stated a version of it in 1712. Yet an explicit expression of the error was not provided until much later on by Joseph-Louis Lagrange. An earlier version of the result was already mentioned in 1671 by James Gregory.\n\nTaylor's theorem is taught in introductory-level calculus courses and is one of the central elementary tools in mathematical analysis. Within pure mathematics it is the starting point of more advanced asymptotic analysis and is commonly used in more applied fields of numerics, as well as in mathematical physics. Taylor's theorem also generalizes to multivariate and vector valued functions formula_1 on any dimensions \"n\" and \"m\". This generalization of Taylor's theorem is the basis for the definition of so-called jets, which appear in differential geometry and partial differential equations.\n\nIf a real-valued function \"f\" is differentiable at the point \"a\" then it has a linear approximation at the point \"a\". This means that there exists a function \"h\" such that\n\nHere\n\nis the linear approximation of \"f\" at the point \"a\". The graph of is the tangent line to the graph of \"f\" at . The error in the approximation is\nNote that this goes to zero a little bit faster than as \"x\" tends to \"a\", given the limiting behavior of \"h\".\nIf we wanted a better approximation to \"f\", we might instead try a quadratic polynomial instead of a linear function. Instead of just matching one derivative of \"f\" at \"a\", we can match two derivatives, thus producing a polynomial that has the same slope and concavity as \"f\" at \"a\". The quadratic polynomial in question is\n\nTaylor's theorem ensures that the quadratic approximation is, in a sufficiently small neighborhood of the point \"a\", a better approximation than the linear approximation. Specifically,\n\nHere the error in the approximation is\n\nwhich, given the limiting behavior of formula_8, goes to zero faster than formula_9 as \"x\" tends to \"a\".\n\nIn general, the error in approximating a function by a polynomial of degree \"k\" will go to zero a little bit faster than as \"x\" tends to \"a\". But this might not always be the case: it is also possible that increasing the degree of the approximating polynomial does not increase the quality of approximation at all even if the function \"f\" to be approximated is infinitely many times differentiable. An example of this behavior is given below, and it is related to the fact that unlike analytic functions, more general functions are not (locally) determined by the values of their derivatives at a single point.\n\nTaylor's theorem is of asymptotic nature: it only tells us that the error \"R\" in an approximation by a \"k\"-th order Taylor polynomial \"P\" tends to zero faster than any nonzero \"k\"-th degree polynomial as \"x\" → \"a\". It does not tell us how large the error is in any concrete neighborhood of the center of expansion, but for this purpose there are explicit formulae for the remainder term (given below) which are valid under some additional regularity assumptions on \"f\". These enhanced versions of Taylor's theorem typically lead to uniform estimates for the approximation error in a small neighborhood of the center of expansion, but the estimates do not necessarily hold for neighborhoods which are too large, even if the function \"f\" is analytic. In that situation one may have to select several Taylor polynomials with different centers of expansion to have reliable Taylor-approximations of the original function (see animation on the right.)\n\nThere are several things we might do with the remainder term:\n\nThe precise statement of the most basic version of Taylor's theorem is as follows:\n\nThe polynomial appearing in Taylor's theorem is the \"k\"-th order Taylor polynomial\n\nof the function \"f\" at the point \"a\". The Taylor polynomial is the unique \"asymptotic best fit\" polynomial in the sense that if there exists a function and a \"k\"-th order polynomial \"p\" such that\n\nthen \"p\" = \"P\". Taylor's theorem describes the asymptotic behavior of the remainder term\n\nwhich is the approximation error when approximating \"f\" with its Taylor polynomial. Using the little-o notation, the statement in Taylor's theorem reads as\n\nUnder stronger regularity assumptions on \"f\" there are several precise formulae for the remainder term \"R\" of the Taylor polynomial, the most common ones being the following.\n\nThese refinements of Taylor's theorem are usually proved using the mean value theorem, whence the name. Also other similar expressions can be found. For example, if \"G\"(\"t\") is continuous on the closed interval and differentiable with a non-vanishing derivative on the open interval between \"a\" and \"x\", then\n\nfor some number \"ξ\" between \"a\" and \"x\". This version covers the Lagrange and Cauchy forms of the remainder as special cases, and is proved below using Cauchy's mean value theorem.\n\nThe statement for the integral form of the remainder is more advanced than the previous ones, and requires understanding of Lebesgue integration theory for the full generality. However, it holds also in the sense of Riemann integral provided the (\"k\" + 1)th derivative of \"f\" is continuous on the closed interval [\"a\",\"x\"].\n\nDue to absolute continuity of \"f\" on the closed interval between \"a\" and \"x\" its derivative \"f\" exists as an \"L\"-function, and the result can be proven by a formal calculation using fundamental theorem of calculus and integration by parts.\n\nIt is often useful in practice to be able to estimate the remainder term appearing in the Taylor approximation, rather than having an exact formula for it. Suppose that \"f\" is -times continuously differentiable in an interval \"I\" containing \"a\". Suppose that there are real constants \"q\" and \"Q\" such that\nthroughout \"I\". Then the remainder term satisfies the inequality\n\nif , and a similar estimate if . This is a simple consequence of the Lagrange form of the remainder. In particular, if\non an interval with some formula_18 , then\n\nFor example, the third-order Taylor polynomial of a smooth function \"f\": R → R is, denoting x − a = v,\n\nLet\n\nwhere, as in the statement of Taylor's theorem,\n\nIt is sufficient to show that\n\nThe proof here is based on repeated application of L'Hôpital's rule. Note that, for each , formula_24. Hence each of the first \"k\"−1 derivatives of the numerator in formula_25 vanishes at formula_26, and the same is true of the denominator. Also, since the condition that the function \"f\" be \"k\" times differentiable at a point requires differentiability up to order \"k\"−1 in a neighborhood of said point (this is true, because differentiability requires a function to be defined in a whole neighborhood of a point), the numerator and its \"k\" − 2 derivatives are differentiable in a neighborhood of \"a\". Clearly, the denominator also satisfies said condition, and additionally, doesn't vanish unless \"x\"=\"a\", therefore all conditions necessary for L'Hopital's rule are fulfilled, and its use is justified. So\n\nwhere the second to last equality follows by the definition of the derivative at \"x\" = \"a\".\n\nLet \"G\" be any real-valued function, continuous on the closed interval between \"a\" and \"x\" and differentiable with a non-vanishing derivative on the open interval between \"a\" and \"x\", and define\n\nThen, by Cauchy's mean value theorem,\n\nfor some ξ on the open interval between \"a\" and \"x\". Note that here the numerator is exactly the remainder of the Taylor polynomial for \"f\"(\"x\"). Compute\n\nplug it into (*) and rearrange terms to find that\n\nThis is the form of the remainder term mentioned after the actual statement of Taylor's theorem with remainder in the mean value form.\nThe Lagrange form of the remainder is found by choosing formula_32 and the Cauchy form by choosing formula_33.\n\nRemark. Using this method one can also recover the integral form of the remainder by choosing\n\nbut the requirements for \"f\" needed for the use of mean value theorem are too strong, if one aims to prove the claim in the case that \"f\" is only absolutely continuous. However, if one uses Riemann integral instead of Lebesgue integral, the assumptions cannot be weakened.\n\nDue to absolute continuity of \"f\" on the closed interval between \"a\" and \"x\" its derivative \"f\" exists as an \"L\"-function, and we can use fundamental theorem of calculus and integration by parts. This same proof applies for the Riemann integral assuming that \"f\" is continuous on the closed interval and differentiable on the open interval between \"a\" and \"x\", and this leads to the same result than using the mean value theorem.\n\nThe fundamental theorem of calculus states that\n\nNow we can integrate by parts and use the fundamental theorem of calculus again to see that\n\nwhich is exactly Taylor's theorem with remainder in the integral form in the case \"k=1\".\nThe general statement is proved using induction. Suppose that\n\nIntegrating the remainder term by parts we arrive at\n\nSubstituting this into the formula shows that if it holds for the value \"k\", it must also hold for the value \"k\" + 1.\nTherefore, since it holds for \"k\" = 1, it must hold for every positive integer \"k\".\n\nWe prove the special case, where \"f\" : R → R has continuous partial derivatives up to the order \"k\"+1 in some closed ball \"B\" with center a. The strategy of the proof is to apply the one-variable case of Taylor's theorem to the restriction of \"f\" to the line segment adjoining x and a. Parametrize the line segment between a and x by u(\"t\") = We apply the one-variable version of Taylor's theorem to the function :\n\nApplying the chain rule for several variables gives\n\nwhere formula_41 is the multinomial coefficient. Since formula_42, we get\n\n\n\n"}
{"id": "54637700", "url": "https://en.wikipedia.org/wiki?curid=54637700", "title": "Teknomo–Fernandez algorithm", "text": "Teknomo–Fernandez algorithm\n\nThe Teknomo–Fernandez algorithm (TF algorithm), is an efficient algorithm for generating the background image of a given video sequence.\n\nBy assuming that the background image is shown in the majority of the video, the algorithm is able to generate a good background image of a video in formula_1-time using only a small number of binary operations and Boolean Bit operations, which require a small amount of memory and has built-in operators found in many programming languages such as C, C++, and Java.\n\nPeople tracking from videos usually involves some form of background subtraction to segment foreground from background. Once foreground images are extracted, then desired algorithms (such as those for motion tracking, object tracking, and facial recognition) may be executed using these images.\n\nHowever, background subtraction requires that the background image is already available and unfortunately, this is not always the case. Traditionally, the background image is searched for manually or automatically from the video images when there are no objects. More recently, automatic background generation through object detection, medial filtering, medoid filtering, approximated median filtering, linear predictive filter, non-parametric model, Kalman filter, and adaptive smoothening have been suggested; however, most of these methods have high computational complexity and are resource-intensive.\n\nThe Teknomo–Fernandez algorithm is also an automatic background generation algorithm. Its advantage, however, is its computational speed of only formula_1-time, depending on the resolution formula_3 of an image and its accuracy gained within a manageable number of frames. Only at least three frames from a video is needed to produce the background image assuming that for every pixel position, the background occurs in the majority of the videos. Furthermore, it can be performed for both grayscale and colored videos.\n\n\nGenerally, however, the algorithm will certainly work whenever the following single important assumption holds: For each pixel position, the majority of the pixel values in the entire video contain the pixel value of the actual background image (at that position).As long as each part of the background is shown in the majority of the video, the entire background image needs not to appear in any of its frames. The algorithm is expected to work accurately.\n\n\nAt the first level, three frames are selected at random from the image sequence to produce a background image by combining them using the first equation. This yields a better background image at the second level. The procedure is repeated until desired level formula_13.\n\nAt level formula_14, the probability formula_15 that the modal bit predicted is the actual modal bit is represented by the equation formula_16.\nThe table below gives the computed probability values across several levels using some specific initial probabilities. It can be observed that even if the modal bit at the considered position is at a low 60% of the frames, the probability of accurate modal bit determination is already more than 99% at 6 levels.\nThe space requirement of the Teknomo–Fernandez algorithm is given by the function formula_17, depending on the resolution formula_3 of the image, the number formula_19 of frames in the video, and the desired number formula_13 of levels. However, the fact that formula_13 will probably not exceed 6 reduces the space complexity to formula_22.\n\nThe entire algorithm runs in formula_1-time, only depending on the resolution of the image. Computing the modal bit for each bit can be done in formula_24-time while the computation of the resulting image from the three given images can be done in formula_1-time. The number of the images to be processed in formula_13 levels is formula_27. However, since formula_28, then this is actually formula_24, thus the algorithm runs in formula_1.\n\nA variant of the Teknomo–Fernandez algorithm that incorporates the Monte-Carlo method named CRF has been developed. Two different configurations of CRF were implemented: CRF9,2 and CRF81,1. Experiments on some colored video sequences showed that the CRF configurations outperform the TF algorithm in terms of accuracy. However, the TF algorithm remains more efficient in terms of processing time.\n\n\n\n"}
{"id": "3666492", "url": "https://en.wikipedia.org/wiki?curid=3666492", "title": "True north", "text": "True north\n\nTrue north (also called geodetic north) is the direction along Earth's surface towards the geographic North Pole.\n\nGeodetic north differs from \"magnetic\" north (the direction a compass points toward the Magnetic North Pole), and from grid north (the direction northwards along the grid lines of a map projection). Geodetic true north also differs very slightly from \"astronomical\" true north (typically by a few arcseconds) because the local gravity may not point at the exact rotational axis of Earth.\n\nThe direction of astronomical true north is marked in the skies by the north celestial pole. This is within about 1° of the position of Polaris, so that the star would appear to trace a tiny circle in the sky each sidereal day. Due to the axial precession of Earth, true north rotates in an arc with respect to the stars that takes approximately 25,000 years to complete. Around 2100–02, Polaris will make its closest approach to the celestial north pole (extrapolated from recent Earth precession). The visible star nearest the north celestial pole 5,000 years ago was Thuban.\n\nOn maps published by the United States Geological Survey (USGS) and the United States Armed Forces, true north is marked with a line terminating in a five-pointed star. The east and west edges of the USGS topographic quadrangle maps of the United States are meridians of longitude, thus indicating true north (so they are not exactly parallel). Maps issued by the United Kingdom Ordnance Survey contain a diagram showing the difference between true north, grid north, and magnetic north at a point on the sheet; the edges of the map are likely to follow grid directions rather than true, and the map will thus be truly rectangular/square.\n\n"}
{"id": "33068494", "url": "https://en.wikipedia.org/wiki?curid=33068494", "title": "Walther recursion", "text": "Walther recursion\n\nIn computer programming, Walther recursion (named after Christoph Walther) is a method of analysing recursive functions that can determine if the function is definitely terminating, given finite inputs. It allows a more natural style of expressing computation than simply using primitive recursive functions.\n\nSince the halting problem cannot be solved in general, there need to be still programs that will terminate, but which Walther recursion cannot prove to terminate. Walther recursion may be used in total functional languages in order to allow a more liberal style of showing primitive recursion.\n\n\n"}
{"id": "3352292", "url": "https://en.wikipedia.org/wiki?curid=3352292", "title": "Wavelet transform", "text": "Wavelet transform\n\nIn mathematics, a wavelet series is a representation of a square-integrable (real- or complex-valued) function by a certain orthonormal series generated by a wavelet. This article provides a formal, mathematical definition of an orthonormal wavelet and of the integral wavelet transform. \n\nA function formula_1 is called an orthonormal wavelet if it can be used to define a Hilbert basis, that is a complete orthonormal system, for the Hilbert space formula_2 of square integrable functions.\n\nThe Hilbert basis is constructed as the family of functions formula_3 by means of dyadic translations and dilations of formula_4,\n\nfor integers formula_6.\n\nIf under the standard inner product on formula_2,\n\nthis family is orthonormal, it is an orthonormal system:\n\nwhere formula_10 is the Kronecker delta.\n\nCompleteness is satisfied if every function formula_11 may be expanded in the basis as\n\nwith convergence of the series understood to be convergence in norm. Such a representation of \"f\" is known as a wavelet series. This implies that an orthonormal wavelet is self-dual.\n\nThe integral wavelet transform is the integral transform defined as\n\nThe wavelet coefficients formula_14 are then given by\n\nHere, formula_16 is called the binary dilation or dyadic dilation, and formula_17 is the binary or dyadic position.\n\nThe fundamental idea of wavelet transforms is that the transformation should allow only changes in time extension, but not shape. This is affected by choosing suitable basis functions that allow for this. Changes in the time extension are expected to conform to the corresponding analysis frequency of the basis function. Based on the uncertainty principle of signal processing,\n\nwhere t represents time and ω angular frequency (ω = 2πf, where f is temporal frequency).\n\nThe higher the required resolution in time, the lower the resolution in frequency has to be. The larger the extension of the analysis windows is chosen, the larger is the value of formula_19.\n\nWhen Δt is large,\n\nWhen Δt is small\n\nIn other words, the basis function Ψ can be regarded as an impulse response of a system with which the function x(t) has been filtered. The transformed signal provides information about the time and the frequency. Therefore, wavelet-transformation contains information similar to the short-time-Fourier-transformation, but with additional special properties of the wavelets, which show up at the resolution in time at higher analysis frequencies of the basis function. The difference in time resolution at ascending frequencies for the Fourier transform and the wavelet transform is shown below.\n\nThis shows that wavelet transformation is good in time resolution of high frequencies, while for slowly varying functions, the frequency resolution is remarkable.\n\nAnother example: The analysis of three superposed sinusoidal signals formula_20 with STFT and wavelet-transformation.\n\nWavelet compression is a form of data compression well suited for image compression (sometimes also video compression and audio compression). Notable implementations are JPEG 2000, DjVu and ECW for still images, CineForm, and the BBC's Dirac. The goal is to store image data in as little space as possible in a file. Wavelet compression can be either lossless or lossy.\n\nUsing a wavelet transform, the wavelet compression methods are adequate for representing transients, such as percussion sounds in audio, or high-frequency components in two-dimensional images, for example an image of stars on a night sky. This means that the transient elements of a data signal can be represented by a smaller amount of information than would be the case if some other transform, such as the more widespread discrete cosine transform, had been used.\n\nDiscrete wavelet transform has been successfully applied for the compression of electrocardiograph (ECG) signals In this work, the high correlation between the corresponding wavelet coefficients of signals of successive cardiac cycles is utilized employing linear prediction.\nWavelet compression is not good for all kinds of data: transient signal characteristics mean good wavelet compression, while smooth, periodic signals are better compressed by other methods, particularly traditional harmonic compression (frequency domain, as by Fourier transforms and related).\n\nSee Diary Of An x264 Developer: The problems with wavelets (2010) for discussion of practical issues of current methods using wavelets for video compression.\n\nFirst a wavelet transform is applied. This produces as many coefficients as there are pixels in the image (i.e., there is no compression yet since it is only a transform). These coefficients can then be compressed more easily because the information is statistically concentrated in just a few coefficients. This principle is called transform coding. After that, the coefficients are quantized and the quantized values are entropy encoded and/or run length encoded.\n\nA few 1D and 2D applications of wavelet compression use a technique called \"wavelet footprints\".\n\nWavelets have some slight benefits over Fourier transforms in reducing computations when examining specific frequencies. However, they are rarely more sensitive, and indeed, the common Morlet wavelet is mathematically identical to a short-time Fourier transform using a Gaussian window function. The exception is when searching for signals of a known, non-sinusoidal shape (e.g., heartbeats); in that case, using matched wavelets can outperform standard STFT/Morlet analyses.\n\nThe wavelet transform can provide us with the frequency of the signals and the time associated to those frequencies, making it very convenient for its application in numerous fields. For instance, signal processing of accelerations for gait analysis, for fault detection, for design of low power pacemakers and also in ultra-wideband (UWB) wireless communications.\n\n\\cdot\\Psi\\left[\\frac{k - m c_0^n}{c_0^n}T\\right] = \\frac{1}{\\sqrt{c_0^n}}\\cdot\\Psi\\left[\\left(\\frac{k}{c_0^n} - m\\right)T\\right]</math>\n\nSuch discrete wavelets can be used for the transformation:\n\nAs apparent from wavelet-transformation representation (shown below) \n\nwhere c is scaling factor, τ represents time shift factor\n\nand as already mentioned in this context, the wavelet-transformation corresponds to a convolution of a function y(t) and a wavelet-function. A convolution can be implemented as a multiplication in the frequency domain. With this the following approach of implementation results into:\n\nThere are many different types of wavelet transforms for specific purposes. See also a full list of wavelet-related transforms but the common ones are listed below: Mexican hat wavelet, Haar Wavelet, Daubechies wavelet, triangular wavelet.\n\n\n\n"}
