{"id": "1752068", "url": "https://en.wikipedia.org/wiki?curid=1752068", "title": "143 (number)", "text": "143 (number)\n\n143 (one hundred [and] forty-three) is the natural number following 142 and preceding 144.\n\n143 is the sum of seven consecutive primes (11 + 13 + 17 + 19 + 23 + 29 + 31). But this number is never the sum of an integer and its base 10 digits, making it a self number.\n\nEvery positive integer is the sum of at most 143 seventh powers (see Waring's problem).\n\n143 is the difference in the first exception to the pattern shown below:\n\n\n\n\n\n143 is also:\n\n\n"}
{"id": "5113977", "url": "https://en.wikipedia.org/wiki?curid=5113977", "title": "242 (number)", "text": "242 (number)\n\n242 (two hundred [and] forty-two) is the natural number following 241 and preceding 243.\n\n242 is the smallest integer to start a run of four consecutive integers with the same number of divisors.\n\n242 is a nontotient since there is no integer with 242 coprimes below it.\n\n242 is a palindrome.\n\n242 is also:\n"}
{"id": "305608", "url": "https://en.wikipedia.org/wiki?curid=305608", "title": "78 (number)", "text": "78 (number)\n\n78 (seventy-eight) is the natural number following 77 and followed by 79.\n\n78 is:\n\n\n77 and 78 form a Ruth-Aaron pair.\n\n\n78 is also:\n"}
{"id": "16688292", "url": "https://en.wikipedia.org/wiki?curid=16688292", "title": "Alfred Cardew Dixon", "text": "Alfred Cardew Dixon\n\nSir Alfred Cardew Dixon, 1st Baronet Warford FRS (22 May 1865 – 4 May 1936) was an English mathematician.\n\nDixon was born on 22 May 1865 in Northallerton, Yorkshire, England. He studied at the University of London and graduated with an MA. He entered Trinity College, Cambridge, in 1883 and graduated as Senior Wrangler in the Mathematical Tripos in 1886. In 1888, Dixon was awarded the second Smith's Prize, and also appointed a Fellow of Trinity College, Cambridge. He took the degree of Sc.D. at Cambridge University in 1897. He was Professor of Mathematics at Queen's College, Galway, from 1893 to 1901. In 1901 he was appointed to the chair at Queen's University Belfast, which he held till 1930, receiving the title of Emeritus Professor on retirement.\n\nDixon was elected to the Royal Society in 1904 and after he retired from Queen's University Belfast, he served as president of the London Mathematical Society from 1931 until 1933. Queen's University Belfast conferred on him the honorary degree of D.Sc. in 1932.\n\nDixon was well known for his work in differential equations. He did early work on Fredholm integrals independently of Fredholm. He worked both on ordinary differential equations and on partial differential equations studying Abelian integrals, automorphic functions, and functional equations.\n\nIn 1894 Dixon wrote \"The Elementary Properties of the Elliptic Functions\". Certain elliptic functions (meromorphic doubly periodic functions) denoted cm and sm satisfying the identity cm(\"z\") + sm(\"z\") = 1 are known as Dixon's elliptic functions.\n\nDixon's identity is any of several closely related identities involving binomial coefficients and hypergeometric functions.\n"}
{"id": "44453564", "url": "https://en.wikipedia.org/wiki?curid=44453564", "title": "Attack tolerance", "text": "Attack tolerance\n\nIn the context of complex networks, attack tolerance is the network’s robustness meaning the ability to maintain the overall connectivity and diameter of the network as nodes are removed.\n\nIf an attack was to be mounted on a network, it would not be through random nodes but ones that were the most significant to the network. Different methods of ranking are utilized to determine the nodes priority in the network.\n\nThis form of attack prioritizes the most connected nodes as the most important ones. This takes into account the network (represented by graph formula_1) changing over time, by analyzing the network as a series of snapshots (indexed by formula_2); we denote the snapshot at time formula_3 by formula_4. The average of the degree of a node, labeled formula_5, within a given snapshot formula_6, throughout a time interval (a sequence of formula_7 snapshots) formula_8, is given by:\n\nformula_9\n\nThis form of attack prioritizes nodes that occur most frequently over a period of time. The equation below calculates the frequency that a node (i) occurs in a time interval formula_8. When the node is present during the snapshot then equation is equal to 1, but if the node is not present then it is equal to 0.\n\nformula_11\n\nWhere\n\nformula_12\n\nThis form of attack prioritizes nodes by the summation of temporal distances from one node to all other nodes over a period of time. The equation below calculates the temporal distance of a node (i) by averaging the sum of all the temporal distances for the interval [t,t].\n\nformula_13\n\nNot all networks are the same, so it is no surprise that an attack on different networks would have different results. The common method for measuring change in the network is through the average of the size of all the isolated clusters, <nowiki></nowiki>, and the fraction of the nodes contained in the largest cluster, S. When no nodes have been attacked, both S and <nowiki></nowiki> equal 1.\n\nIn the ER model, the network generated is homogeneous, meaning each node has the same number of links. This is considered to be an exponential network. When comparing the connectivity of the ER model when it undergoes random failures vs directed attacks, we are shown that the exponential network reacts the same way to a random failure as it does to a directed attack. This is due to the homogeneity of the network, making it so that it does not matter whether a random node is selected or one is specifically targeted. All the nodes on average are the same in degree therefore attacking one shouldn't cause anymore damage than attacking another. As the number of attacks go up and more nodes are removed, we observe that S decreases non-linearly and acts as if a threshold exists when a fraction of the nodes (f) has been removed, (f≈.28). At this point, S goes to zero. The average size of the isolated clusters behaves opposite, increasing exponentially to <nowiki></nowiki>= 2, also approaching the threshold line f≈.28, except decreases back to 1 after. \nThis model was tested for a large range of nodes and proven to maintain the same pattern.\n\nIn the scale-free model, the network is defined by its degree distribution following the power law, which means that each node has no set number of links, unlike the exponential network. This makes the scale-free model more vulnerable because there are nodes that are more important than others, and if these nodes were to be deliberately attacked the network would break down. However this inhomogeneous network has its strengths when it comes to random failures. Due to the power law there are many more nodes in the system that have very few links, and probability estimates that these are the nodes that will be targeted (because there are more of them). Severing these smaller nodes will not affect the network as a whole and therefore allows the structure of the network to stay approximately the same.\nWhen the scale-free model undergoes random failures, \"S\" slowly decreases with no threshold-like behavior and <nowiki></nowiki> remains approximately 1. This indicates that the network is being broken apart one by one and not by large clusters. However, when the scale-free model undergoes deliberate attack the system behaves similarly to an exponential system, except it breaks down much quicker. As the number of attacks increases, \"S\" decreases with a threshold close to f=0.05, and <nowiki></nowiki> increases to the same threshold and then decreases back to one. The speed at which this type of network breaks down shows the vulnerability of common networks that are used everyday, such as the Internet.\n\n"}
{"id": "57917884", "url": "https://en.wikipedia.org/wiki?curid=57917884", "title": "Bashar ibn Shu'aib", "text": "Bashar ibn Shu'aib\n\nBashar (or Bishr) ben Phinehas ibn Shu'aib was a tenth century Jewish mathematician.\n\nAccording to Hottinger (\"Promptuarium\", p. 96), the Arabic works of Ibn Shu'aib are often quoted by Arabic writers. In 997, Abu 'Ali 'Isa ibn Zara'ah addressed to Ibn Shu'aib a pamphlet against Judaism which seemed to be an answer to a pro-Jewish work by Ibn Shu'aib (see Ibn Abi Uṣaibi'a, \"Uyun al-Anba\", ii. 236).\n\n"}
{"id": "6980594", "url": "https://en.wikipedia.org/wiki?curid=6980594", "title": "Basic skills", "text": "Basic skills\n\nBasic skills can be compared to higher order thinking skills. Facts and methods are highly valued under the back-to-basics approach to education.\n\n\nTeaching methods that emphasize \"basic skills\" tend to be compatible with traditional education rather than student-centered standards based education reform. Materials that are primarily marketed to homeschoolers such as Saxon math and Modern Curriculum Press are based on emphasis on basic skills. Such curricula typically require much less teacher training, less expensive and smaller books, and do not require purchasing expensive expendable materials such as scissors, paste, paint, beads as is required by reform mathematics curricula such as Investigations in Number, Data, and Space.\n\nMost local, state and federal education agencies are committed to standards based education reform, which is based on beliefs which conflict with the outcomes of traditional education. The goal is that all students will succeed at one high world-class level of what students know and are able to do, rather than different students learning different amounts on different tracks, producing some failures and some successes. Higher order thinking skills are emphasized by the new standards. A widely cited paper by Constance Kamii even suggests that teaching of basic arithmetic methods is harmful to learning, and guided the thinking behind many of today's commonly used mathematics teaching curricula.\n\nIn the United Kingdom, basic skills education is literacy and numeracy education for adults who for some reason did not acquire these skills or a level sufficient for everyday adult life when they were at school. It is therefore often referred to as \"adult basic skills\". Skills for life is a basic skills programme running in further education colleges, taken by young people over 16 and by older adults. Students on vocational courses and apprentices are often required to take \"key skills\" units in communication, application of number and information and communication technology (ICT)...\n\nCommon name for the previous standard literacy and numeracy testing, undertaken in Years 3, 5, 7 & 9. The Literacy and Numeracy test has been replaced by a standard Australia wide test called the NAPLAN test.\n\n"}
{"id": "9898864", "url": "https://en.wikipedia.org/wiki?curid=9898864", "title": "Berger code", "text": "Berger code\n\nIn telecommunication, a Berger code is a unidirectional error detecting code, named after its inventor, J. M. Berger. Berger codes can detect all unidirectional errors. Unidirectional errors are errors that only flip ones into zeroes or only zeroes into ones, such as in asymmetric channels. The check bits of Berger codes are computed by summing all the zeroes in the information word, and expressing that sum in natural binary. If the information word consists of formula_1 bits, then the Berger code needs formula_2 \"check bits\", giving a Berger code of length k+n. (In other words, the formula_3 check bits are enough to check up to formula_4 information bits).\nBerger codes can detect any number of one-to-zero bit-flip errors, as long as no zero-to-one errors occurred in the same code word.\nSimilarly, Berger codes can detect any number of zero-to-one bit-flip errors, as long as no one-to-zero bit-flip errors occur in the same code word.\nBerger codes cannot correct any error.\n\nLike all unidirectional error detecting codes,\nBerger codes can also be used in delay-insensitive circuits.\n\nAs stated above, Berger codes detect \"any\" number of unidirectional errors. For a \"given code word\", if the only errors that have occurred are that some (or all) bits with value 1 have changed to value 0, then this transformation will be detected by the Berger code implementation. To understand why, consider that there are three such cases:\n\nFor case 1, the number of 0-valued bits in the information section will, by definition of the error, increase. Therefore, our Berger check code will be lower than the actual 0-bit-count for the data, and so the check will fail.\n\nFor case 2, the number of 0-valued bits in the information section have stayed the same, but the value of the check data has changed. Since we know some 1s turned into 0s, but no 0s have turned into 1s (that's how we defined the error model in this case), the encoded binary value of the check data will go down (e.g., from binary 1011 to 1010, or to 1001, or 0011). Since the information data has stayed the same, it has the same number of zeros it did before, and that will no longer match the mutated check value.\n\nFor case 3, where bits have changed in both the information and the check sections, notice that the number of zeros in the information section has \"gone up\", as described for case 1, and the binary value stored in the check portion has \"gone down\", as described for case 2. Therefore, there is no chance that the two will end up mutating in such a way as to become a different valid code word.\n\nA similar analysis can be performed, and is perfectly valid, in the case where the only errors that occur are that some 0-valued bits change to 1. Therefore, if all the errors that occur on a specific codeword all occur in the same direction, these errors will be detected. For the next code word being transmitted (for instance), the errors can go in the opposite direction, and they will still be detected, as long as they all go in the same direction as each other.\n\nUnidirectional errors are common in certain situations. For instance, in flash memory, bits can more easily be programmed to a 0 than can be reset to a 1.\n\n"}
{"id": "28293188", "url": "https://en.wikipedia.org/wiki?curid=28293188", "title": "Bid–ask matrix", "text": "Bid–ask matrix\n\nThe bid–ask matrix is a matrix with elements corresponding with exchange rates between the assets. These rates are in \"physical units\" (e.g. number of stocks) and not with respect to any \"numeraire\". The formula_1 element of the matrix is the number of units of asset formula_2 which can be exchanged for 1 unit of asset formula_3.\n\nA formula_4 matrix formula_5 is a \"bid-ask matrix\", if\n\nAssume a market with 2 assets (A and B), such that formula_12 units of A can be exchanged for 1 unit of B, and formula_13 units of B can be exchanged for 1 unit of A. Then the \"bid–ask matrix\" formula_14 is:\n\nIf given a bid–ask matrix formula_14 for formula_17 assets such that formula_18 and formula_19 is the number of assets which with any non-negative quantity of them can be \"discarded\" (traditionally formula_20). Then the solvency cone formula_21 is the convex cone spanned by the unit vectors formula_22 and the vectors formula_23.\n\nSimilarly given a (constant) solvency cone it is possible to extract the bid–ask matrix from the bounding vectors.\n\n"}
{"id": "14988352", "url": "https://en.wikipedia.org/wiki?curid=14988352", "title": "Blum–Shub–Smale machine", "text": "Blum–Shub–Smale machine\n\nIn computation theory, the Blum–Shub–Smale machine, or BSS machine, is a model of computation introduced by Lenore Blum, Michael Shub and Stephen Smale, intended to describe computations over the real numbers. Essentially, a BSS machine is a Random Access Machine with registers that can store arbitrary real numbers and that can compute rational functions over reals at unit cost. It is often referred to as Real RAM model.\n\nA BSS machine M is given by the set formula_1 of formula_2 instructions, indexed formula_3. A configuration of M is a tuple formula_4, where k is the number of the instruction currently executed, r and w are copy registers and x stores the content of all registers of M. The computation begins with configuration formula_5 and ends whenever formula_6, the final content of x is said to be the output of the machine.\n\nThe instructions of M can be of the following types:\n\n\n"}
{"id": "650086", "url": "https://en.wikipedia.org/wiki?curid=650086", "title": "Contour line", "text": "Contour line\n\nA contour line (also isoline, isopleth, or isarithm) of a function of two variables is a curve along which the function has a constant value, so that the curve joins points of equal value. It is a plane section of the three-dimensional graph of the function \"f\"(\"x\", \"y\") parallel to the \"x\", \"y\" plane. In cartography, a contour line (often just called a \"contour\") joins points of equal elevation (height) above a given level, such as mean sea level. A contour map is a map illustrated with contour lines, for example a topographic map, which thus shows valleys and hills, and the steepness or gentleness of slopes. The contour interval of a contour map is the difference in elevation between successive contour lines.\n\nMore generally, a contour line for a function of two variables is a curve connecting points where the function has the same particular value.\n\nThe gradient of the function is always perpendicular to the contour lines. When the lines are close together the magnitude of the gradient is large: the variation is steep. A level set is a generalization of a contour line for functions of any number of variables.\n\nContour lines are curved, straight or a mixture of both lines on a map describing the intersection of a real or hypothetical surface with one or more horizontal planes. The configuration of these contours allows map readers to infer relative gradient of a parameter and estimate that parameter at specific places. Contour lines may be either traced on a visible three-dimensional model of the surface, as when a photogrammetrist viewing a stereo-model plots elevation contours, or interpolated from estimated surface elevations, as when a computer program threads contours through a network of observation points of area centroids. In the latter case, the method of interpolation affects the reliability of individual isolines and their portrayal of slope, pits and peaks.\n\nContour lines are often given specific names beginning \"iso-\" () according to the nature of the variable being mapped, although in many usages the phrase \"contour line\" is most commonly used. Specific names are most common in meteorology, where multiple maps with different variables may be viewed simultaneously. The prefix \"iso-\" can be replaced with \"isallo-\" to specify a contour line connecting points where a variable changes at the same \"rate\" during a given time period.\n\nThe words \"isoline\" and \"isarithm\" ( ' \"number\") are general terms covering all types of contour line. The word \"isogram\" ( ' \"writing or drawing\") was proposed by Francis Galton in 1889 as a convenient generic designation for lines indicating equality of some physical condition or quantity; but it commonly refers to a word without a repeated letter.\n\nAn isogon (from or \"gonia\", meaning 'angle') is a contour line for a variable which measures direction. In meteorology and in geomagnetics, the term \"isogon\" has specific meanings which are described below. An isocline (from or \"klinein\", meaning 'to lean or slope') is a line joining points with equal slope. In population dynamics and in geomagnetics, the terms \"isocline\" and \"isoclinic line\" have specific meanings which are described below.\n\nA curve of equidistant points is a set of points all at the same distance from a given point, line, or polyline. In this case the function whose value is being held constant along a contour line is a distance function.\n\nIn geography, the word \"isopleth\" (from or \"plethos\", meaning 'quantity') is used for contour lines that depict a variable which cannot be measured at a point, but which instead must be calculated from data collected over an area. An example is population density, which can be calculated by dividing the population of a census district by the surface area of that district. Each calculated value is presumed to be the value of the variable at the centre of the area, and isopleths can then be drawn by a process of interpolation. The idea of an isopleth map can be compared with that of a choropleth map.\n\nIn meteorology, the word \"isopleth\" is used for any type of contour line.\n\nMeteorological contour lines are based on interpolation of the point data received from weather stations and weather satellites. Weather stations are seldom exactly positioned at a contour line (when they are, this indicates a measurement precisely equal to the value of the contour). Instead, lines are drawn to best approximate the locations of exact values, based on the scattered information points available.\n\nMeteorological contour maps may present collected data such as actual air pressure at a given time, or generalized data such as average pressure over a period of time, or forecast data such as predicted air pressure at some point in the future\n\nThermodynamic diagrams use multiple overlapping contour sets (including isobars and isotherms) to present a picture of the major thermodynamic factors in a weather system.\n\nAn isobar (from or \"baros\", meaning 'weight') is a line of equal or constant pressure on a graph, plot, or map; an isopleth or contour line of pressure.\nMore accurately, isobars are lines drawn on a map joining places of equal average atmospheric pressure reduced to sea level for a specified period of time. In meteorology, the barometric pressures shown are reduced to sea level, not the surface pressures at the map locations. The distribution of isobars is closely related to the magnitude and direction of the wind field, and can be used to predict future weather patterns. Isobars are commonly used in television weather reporting.\n\nIsallobars are lines joining points of equal pressure change during a specific time interval. These can be divided into \"anallobars\", lines joining points of equal pressure increase during a specific time interval, and \"katallobars\", lines joining points of equal pressure decrease. In general, weather systems move along an axis joining high and low isallobaric centers. Isallobaric gradients are important components of the wind as they increase or decrease the geostrophic wind.\n\nAn isopycnal is a line of constant density. An \"isoheight\" or \"isohypse\" is a line of constant geopotential height on a constant pressure surface chart. Isohypse and isoheight are simply known as lines showing equal pressure on a map.\n\nAn isotherm (from or \"thermē\", meaning 'heat') is a line that connects points on a map that have the same temperature. Therefore, all points through which an isotherm passes have the same or equal temperatures at the time indicated. An isotherm at 0 °C is called the freezing level. The term was coined by the Prussian geographer and naturalist Alexander von Humboldt, who as part of his research into the geographical distribution of plants published the first map of isotherms in Paris, in 1817.\n\nAn isogeotherm is a line of equal mean annual temperature. An isocheim is a line of equal mean winter temperature, and an isothere is a line of equal mean summer temperature.\n\nAn isohel (from or \"helios\", meaning 'Sun') is a line of equal or constant solar radiation.\n\nAn isohyet or isohyetal line (from or \"huetos\", meaning 'rain') is a line joining points of equal rainfall on a map in a given period . A map with isohyets is called an isohyetal map.\n\nAn isohume is a line of constant relative humidity, while a isodrosotherm (from or \"drosos\", meaning 'dew', and or \"therme\", meaning 'heat') is a line of equal or constant dew point.\n\nAn isoneph is a line indicating equal cloud cover.\n\nAn isochalaz is a line of constant frequency of hail storms, and an isobront is a line drawn through geographical points at which a given phase of thunderstorm activity occurred simultaneously.\n\nSnow cover is frequently shown as a contour-line map.\n\nAn isotach (from or \"tachus\", meaning 'fast') is a line joining points with constant wind speed.\nIn meteorology, the term isogon refers to a line of constant wind direction.\n\nAn isopectic line denotes equal dates of ice formation each winter, and an isotac denotes equal dates of thawing.\n\nContours are one of several common methods used to denote elevation or altitude and depth on maps. From these contours, a sense of the general terrain can be determined. They are used at a variety of scales, from large-scale engineering drawings and architectural plans, through topographic maps and bathymetric charts, up to continental-scale maps.\n\n\"Contour line\" is the most common usage in cartography, but isobath for underwater depths on bathymetric maps and isohypse for elevations are also used.\n\nIn cartography, the contour interval is the elevation difference between adjacent contour lines. The contour interval should be the same over a single map. When calculated as a ratio against the map scale, a sense of the hilliness of the terrain can be derived.\n\nThere are several rules to note when interpreting terrain contour lines:\n\nOf course, to determine differences in elevation between two points, the contour interval, or distance in altitude between two adjacent contour lines, must be known, and this is normally stated in the map key. Usually contour intervals are consistent throughout a map, but there are exceptions. Sometimes intermediate contours are present in flatter areas; these can be dashed or dotted lines at half the noted contour interval. When contours are used with hypsometric tints on a small-scale map that includes mountains and flatter low-lying areas, it is common to have smaller intervals at lower elevations so that detail is shown in all areas. Conversely, for an island which consists of a plateau surrounded by steep cliffs, it is possible to use smaller intervals as the height increases.\n\nAn isopotential map is a measure of electrostatic potential in space, often depicted in two dimensions with the electostatic charges inducing that electric potential. The term equipotential line or isopotential line refers to a curve of constant electric potential. Whether crossing an equipotential line represents ascending or descending the potential is inferred from the labels on the charges. In three dimensions, equipotential surfaces may be depicted with a two dimensional cross-section, showing equipotential lines at the intersection of the surfaces and the cross-section.\n\nThe general mathematical term level set is often used to describe the full collection of points having a particular potential, especially in higher dimensional space.\n\nIn the study of the Earth's magnetic field, the term isogon or isogonic line refers to a line of constant magnetic declination, the variation of magnetic north from geographic north. An agonic line is drawn through points of zero magnetic declination. An isoporic line refers to a line of constant annual variation of magnetic declination\n\nAn isoclinic line connects points of equal magnetic dip, and an aclinic line is the isoclinic line of magnetic dip zero.\n\nAn isodynamic line (from or \"dynamis\" meaning 'power') connects points with the same intensity of magnetic force.\n\nBesides ocean depth, oceanographers use contour to describe diffuse variable phenomena much as meteorologists do with atmospheric phenomena. In particular, isobathytherms are lines showing depths of water with equal temperature, isohalines show lines of equal ocean salinity, and Isopycnals are surfaces of equal water density.\n\nVarious geological data are rendered as contour maps in structural geology, sedimentology, stratigraphy and economic geology. Contour maps are used to show the below ground surface of geologic strata, fault surfaces (especially low angle thrust faults) and unconformities. Isopach maps use isopachs (lines of equal thickness) to illustrate variations in thickness of geologic units.\n\nIn discussing pollution, density maps can be very useful in indicating sources and areas of greatest contamination. Contour maps are especially useful for diffuse forms or scales of pollution. Acid precipitation is indicated on maps with isoplats. Some of the most widespread applications of environmental science contour maps involve mapping of environmental noise (where lines of equal sound pressure level are denoted isobels), air pollution, soil contamination, thermal pollution and groundwater contamination. By contour planting and contour ploughing, the rate of water runoff and thus soil erosion can be substantially reduced; this is especially important in riparian zones.\n\nAn isoflor is an isopleth contour connecting areas of comparable biological diversity. Usually, the variable is the number of species of a given genus or family that occurs in a region. Isoflor maps are thus used to show distribution patterns and trends such as centres of diversity.\n\nIn economics, contour lines can be used to describe features which vary quantitatively over space. An isochrone shows lines of equivalent drive time or travel time to a given location and is used in the generation of isochrone maps. An isotim shows equivalent transport costs from the source of a raw material, and an isodapane shows equivalent cost of travel time.\n\nContour lines are also used to display non-geographic information in economics. Indifference curves (as shown at left) are used to show bundles of goods to which a person would assign equal utility. An isoquant (in the image at right) is a curve of equal production quantity for alternative combinations of input usages, and an isocost curve (also in the image at right) shows alternative usages having equal production costs.\n\nIn political science an analogous method is used in understanding coalitions (for example the diagram in Laver and Shepsle's work).\n\nIn population dynamics, an isocline shows the set of population sizes at which the rate of change, or partial derivative, for one population in a pair of interacting populations is zero.\n\nIn statistics, isodensity lines or isodensanes are lines that join points with the same value of a probability density. Isodensanes are used to display bivariate distributions. For example, for a bivariate elliptical distribution the isodensity lines are ellipses.\n\nVarious types of graphs in thermodynamics, engineering, and other sciences use isobars (constant pressure), isotherms (constant temperature), isochors (constant specific volume), or other types of isolines, even though these graphs are usually not related to maps. Such isolines are useful for representing more than two dimensions (or quantities) on two-dimensional graphs. Common examples in thermodynamics are some types of phase diagrams.\n\nIsoclines are used to solve ordinary differential equations.\n\nIn interpreting radar images, an isodop is a line of equal Doppler velocity, and an isoecho is a line of equal radar reflectivity.\n\n\n\nThe idea of lines that join points of equal value was rediscovered several times. The oldest known isobath (contour line of constant depth) is found on a map dated 1584 of the river Spaarne, near Haarlem, by Dutchman Pieter Bruinsz. In 1701, Edmond Halley used such lines (isogons) on a chart of magnetic variation. The Dutch engineer Nicholas Cruquius drew the bed of the river Merwede with lines of equal depth (isobaths) at intervals of 1 fathom in 1727, and Philippe Buache used them at 10-fathom intervals on a chart of the English Channel that was prepared in 1737 and published in 1752. Such lines were used to describe a land surface (contour lines) in a map of the Duchy of Modena and Reggio by Domenico Vandelli in 1746, and they were studied theoretically by Ducarla in 1771, and Charles Hutton used them in the Schiehallion experiment. In 1791, a map of France by J. L. Dupain-Triel used contour lines at 20-metre intervals, hachures, spot-heights and a vertical section. In 1801, the chief of the Corps of Engineers, Haxo, used contour lines at the larger scale of 1:500 on a plan of his projects for Rocca d'Aufo.\n\nBy around 1843, when the Ordnance Survey started to regularly record contour lines in Great Britain and Ireland, they were already in general use in European countries. Isobaths were not routinely used on nautical charts until those of Russia from 1834, and those of Britain from 1838.\n\nWhen maps with contour lines became common, the idea spread to other applications. Perhaps the latest to develop are air quality and noise pollution contour maps, which first appeared in the United States in approximately 1970, largely as a result of national legislation requiring spatial delineation of these parameters. In 2007, Pictometry International was the first to allow users to dynamically generate elevation contour lines to be laid over oblique images.\n\nTo maximize readability of contour maps, there are several design choices available to the map creator, principally line weight, line color, line type and method of numerical marking.\n\nLine weight is simply the darkness or thickness of the line used. This choice is made based upon the least intrusive form of contours that enable the reader to decipher the background information in the map itself. If there is little or no content on the base map, the contour lines may be drawn with relatively heavy thickness. Also, for many forms of contours such as topographic maps, it is common to vary the line weight and/or color, so that a different line characteristic occurs for certain numerical values. For example, in the topographic map above, the even hundred foot elevations are shown in a different weight from the twenty foot intervals.\n\nLine color is the choice of any number of pigments that suit the display. Sometimes a sheen or gloss is used as well as color to set the contour lines apart from the base map. Line colour can be varied to show other information.\n\nLine type refers to whether the basic contour line is solid, dashed, dotted or broken in some other pattern to create the desired effect. Dotted or dashed lines are often used when the underlying base map conveys very important (or difficult to read) information. Broken line types are used when the location of the contour line is inferred.\n\nNumerical marking is the manner of denoting the arithmetical values of contour lines. This can be done by placing numbers along some of the contour lines, typically using interpolation for intervening lines. Alternatively a map key can be produced associating the contours with their values.\n\nIf the contour lines are not numerically labeled and adjacent lines have the same style (with the same weight, color and type), then the direction of the gradient cannot be determined from the contour lines alone. However, if the contour lines cycle through three or more styles, then the direction of the gradient can be determined from the lines. The orientation of the numerical text labels is often used to indicate the direction of the slope.\n\nMost commonly contour lines are drawn in plan view, or as an observer in space would view the Earth's surface: ordinary map form. However, some parameters can often be displayed in profile view showing a vertical profile of the parameter mapped. Some of the most common parameters mapped in profile are air pollutant concentrations and sound levels. In each of those cases it may be important to analyze (air pollutant concentrations or sound levels) at varying heights so as to determine the air quality or noise health effects on people at different elevations, for example, living on different floor levels of an urban apartment. In actuality, both plan and profile view contour maps are used in air pollution and noise pollution studies.\n\nLabels are a critical component of elevation maps. A properly labeled contour map helps the reader to quickly interpret the shape of the terrain. If numbers are placed close to each other, it means that the terrain is steep. Labels should be placed along a slightly curved line \"pointing\" to the summit or nadir, from several directions if possible, making the visual identification of the summit or nadir easy. Contour labels can be oriented so a reader is facing uphill when reading the label.\n\nManual labeling of contour maps is a time-consuming process, however, there are a few software systems that can do the job automatically and in accordance with cartographic conventions, called automatic label placement.\n\n"}
{"id": "22079394", "url": "https://en.wikipedia.org/wiki?curid=22079394", "title": "DNSS point", "text": "DNSS point\n\nDNSS points, also known as Skiba points, arise in optimal control problems that exhibit multiple optimal solutions. A DNSS pointformula_1named alphabetically after Deckert and Nishimura, Sethi, and Skibaformula_1is an indifference point in an optimal control problem such that starting from such a point, the problem has more than one different optimal solutions. A good discussion of such points can be found in Grass et al.\n\nOf particular interest here are discounted infinite horizon optimal control problems that are autonomous. These problems can be formulated as\n\ns.t.\n\nwhere formula_5 is the discount rate, formula_6 and formula_7 are the state and control variables, respectively, at time formula_8, functions formula_9 and formula_10 are assumed to be continuously differentiable with respect to their arguments and they do not depend explicitly on time formula_8, and formula_12 is the set of feasible controls and it also is explicitly independent of time formula_8. Furthermore, it is assumed that the integral converges for any admissible solution formula_14. In such a problem with one-dimensional state variable formula_15, the initial state formula_16 is called a \"DNSS point\" if the system starting from it exhibits multiple optimal solutions or equilibria. Thus, at least in the neighborhood of formula_17, the system moves to one equilibrium for formula_18 and to another for formula_19. In this sense, formula_17 is an indifference point from which the system could move to either of the two equilibria.\n\nFor two-dimensional optimal control problems, Grass et al. and Zeiler et al. present examples that exhibit DNSS curves. \n\nSome references on the application of DNSS points are Caulkins et al. and Zeiler et al.\n\nSuresh P. Sethi identified such indifference points for the first time in 1977. Further, Skiba, Sethi, and Deckert and Nishimura explored these indifference points in economic models. The term DNSS (Deckert, Nishimura, Sethi, Skiba) points, introduced by Grass et al., recognizes (alphabetically) the contributions of these authors.\n\nThese indifference points have been referred to earlier as \"Skiba points\" or \"DNS points\" in the literature.\n\nA simple problem exhibiting this behavior is given by formula_21 formula_22 and formula_23. It is shown in Grass et al. that formula_24 is a DNSS point for this problem because the optimal path formula_6 can be either formula_26 or formula_27. Note that for formula_28, the optimal path is formula_29 and for formula_30, the optimal path is formula_31.\n\nFor further details and extensions, the reader is referred to Grass et al.\n"}
{"id": "50551441", "url": "https://en.wikipedia.org/wiki?curid=50551441", "title": "DeepArt", "text": "DeepArt\n\nDeepArt or DeepArt.io is a website that allows users to create unique artistic images by using an algorithm to redraw one image using the stylistic elements of another image. This uses \"A Neural Algorithm of Artistic Style\" that was developed by several of its creators to separate style elements from a piece of art. The tool allows users to create imitation works of art using the style of famous artists. The neural algorithm is used by the Deep Art website to create a representation of an image provided by the user by using the 'style' of another image provided by the user. A similar program, Prisma, is an iOS and Android app that was based on the open source programming that underlies DeepArt.\n\n\n"}
{"id": "56061", "url": "https://en.wikipedia.org/wiki?curid=56061", "title": "Discrete space", "text": "Discrete space\n\nIn topology, a discrete space is a particularly simple example of a topological space or similar structure, one in which the points form a \"discontinuous sequence\", meaning they are \"isolated\" from each other in a certain sense. The discrete topology is the finest topology that can be given on a set, i.e., it defines all subsets as open sets. In particular, each singleton is an open set in the discrete topology.\n\nGiven a set \"X\":\nfor any formula_3. In this case formula_4 is called a discrete metric space or a space of isolated points.\n\nA metric space formula_16 is said to be \"uniformly discrete\" if there exists a \"packing radius\" formula_17 such that, for any formula_18, one has either formula_19 or formula_20. The topology underlying a metric space can be discrete, without the metric being uniformly discrete: for example the usual metric on the set {1, 1/2, 1/4, 1/8, ...} of real numbers.\nLet X = {1, 1/2, 1/4, 1/8, ...}, consider this set using the usual metric on the real numbers. Then, X is a discrete space, since for each point 1/2, we can surround it with the interval (1/2 - ɛ, 1/2 + ɛ), where ɛ = 1/2(1/2 - 1/2) = 1/2. The intersection (1/2 - ɛ, 1/2 + ɛ) ∩ {1/2} is just the singleton {1/2}. Since the intersection of two open sets is open, and singletons are open, it follows that X is a discrete space.\n\nHowever, X cannot be uniformly discrete. To see why, suppose there exists an r>0 such that d(x,y)>r whenever x≠y. It suffices to show that there are at least two points x and y in X that are closer to each other than r. Since the distance between adjacent points 1/2 and 1/2 is 1/2, we need to find an n that satisfies this inequality:\n\nformula_21\n\nformula_22\n\nformula_23\n\nformula_24\n\nformula_25\n\nformula_26\n\nSince there is always an n bigger than any given real number, it follows that there will always be at least two points in X that are closer to each other than any positive r, therefore X is not uniformly discrete.\n\nThe underlying uniformity on a discrete metric space is the discrete uniformity, and the underlying topology on a discrete uniform space is the discrete topology.\nThus, the different notions of discrete space are compatible with one another.\nOn the other hand, the underlying topology of a non-discrete uniform or metric space can be discrete; an example is the metric space \"X\" := {1/\"n\" : \"n\" = 1,2,3...} (with metric inherited from the real line and given by d(\"x\",\"y\") = |\"x\" − \"y\"|).\nObviously, this is not the discrete metric; also, this space is not complete and hence not discrete as a uniform space.\nNevertheless, it is discrete as a topological space.\nWe say that \"X\" is \"topologically discrete\" but not \"uniformly discrete\" or \"metrically discrete\".\n\nAdditionally:\n\nAny function from a discrete topological space to another topological space is continuous, and any function from a discrete uniform space to another uniform space is uniformly continuous. That is, the discrete space \"X\" is free on the set \"X\" in the category of topological spaces and continuous maps or in the category of uniform spaces and uniformly continuous maps. These facts are examples of a much broader phenomenon, in which discrete structures are usually free on sets.\n\nWith metric spaces, things are more complicated, because there are several categories of metric spaces, depending on what is chosen for the morphisms. Certainly the discrete metric space is free when the morphisms are all uniformly continuous maps or all continuous maps, but this says nothing interesting about the metric structure, only the uniform or topological structure. Categories more relevant to the metric structure can be found by limiting the morphisms to Lipschitz continuous maps or to short maps; however, these categories don't have free objects (on more than one element). However, the discrete metric space is free in the category of bounded metric spaces and Lipschitz continuous maps, and it is free in the category of metric spaces bounded by 1 and short maps. That is, any function from a discrete metric space to another bounded metric space is Lipschitz continuous, and any function from a discrete metric space to another metric space bounded by 1 is short.\n\nGoing the other direction, a function \"f\" from a topological space \"Y\" to a discrete space \"X\" is continuous if and only if it is \"locally constant\" in the sense that every point in \"Y\" has a neighborhood on which \"f\" is constant.\n\nA discrete structure is often used as the \"default structure\" on a set that doesn't carry any other natural topology, uniformity, or metric; discrete structures can often be used as \"extreme\" examples to test particular suppositions. For example, any group can be considered as a topological group by giving it the discrete topology, implying that theorems about topological groups apply to all groups. Indeed, analysts may refer to the ordinary, non-topological groups studied by algebraists as \"discrete groups\" . In some cases, this can be usefully applied, for example in combination with Pontryagin duality. A 0-dimensional manifold (or differentiable or analytical manifold) is nothing but a discrete topological space. We can therefore view any discrete group as a 0-dimensional Lie group.\n\nA product of countably infinite copies of the discrete space of natural numbers is homeomorphic to the space of irrational numbers, with the homeomorphism given by the continued fraction expansion. A product of countably infinite copies of the discrete space {0,1} is homeomorphic to the Cantor set; and in fact uniformly homeomorphic to the Cantor set if we use the product uniformity on the product. Such a homeomorphism is given by using ternary notation of numbers. (See Cantor space.)\n\nIn the foundations of mathematics, the study of compactness properties of products of {0,1} is central to the topological approach to the ultrafilter principle, which is a weak form of choice.\n\nIn some ways, the opposite of the discrete topology is the trivial topology (also called the \"indiscrete topology\"), which has the fewest possible open sets (just the empty set and the space itself). Where the discrete topology is initial or free, the indiscrete topology is final or cofree: every function \"from\" a topological space \"to\" an indiscrete space is continuous, etc.\n\n\n"}
{"id": "52235725", "url": "https://en.wikipedia.org/wiki?curid=52235725", "title": "Double Diamond (design process model)", "text": "Double Diamond (design process model)\n\nDouble Diamond is the name of a design process model developed by the British Design Council in 2005. It suggests that the design process should have four phases:\n\n"}
{"id": "2360715", "url": "https://en.wikipedia.org/wiki?curid=2360715", "title": "Enterprise risk management", "text": "Enterprise risk management\n\nEnterprise risk management (ERM) in business includes the methods and processes used by organizations to manage risks and seize opportunities related to the achievement of their objectives. ERM provides a framework for risk management, which typically involves identifying particular events or circumstances relevant to the organization's objectives (risks and opportunities), assessing them in terms of likelihood and magnitude of impact, determining a response strategy, and monitoring process. By identifying and proactively addressing risks and opportunities, business enterprises protect and create value for their stakeholders, including owners, employees, customers, regulators, and society overall.\n\nERM can also be described as a risk-based approach to managing an enterprise, integrating concepts of internal control, the Sarbanes–Oxley Act, data protection and strategic planning. ERM is evolving to address the needs of various stakeholders, who want to understand the broad spectrum of risks facing complex organizations to ensure they are appropriately managed. Regulators and debt rating agencies have increased their scrutiny on the risk management processes of companies.\n\nAccording to Thomas Stanton of Johns Hopkins University, the point of enterprise risk management is not to create more bureaucracy, but to facilitate discussion on what the really big risks are.\n\nThere are various important ERM frameworks, each of which describes an approach for identifying, analyzing, responding to, and monitoring risks and opportunities, within the internal and external environment facing the enterprise. Management selects a \"risk response strategy\" for specific risks identified and analyzed, which may include:\n\n\nMonitoring is typically performed by management as part of its internal control activities, such as review of analytical reports or management committee meetings with relevant experts, to understand how the risk response strategy is working and whether the objectives are being achieved.\n\nIn 2003, the Casualty Actuarial Society (CAS) defined ERM as the discipline by which an organization in any industry assesses, controls, exploits, finances, and monitors risks from all sources for the purpose of increasing the organization's short- and long-term value to its stakeholders.\" The CAS conceptualized ERM as proceeding across the two dimensions of \"risk type\" and \"risk management processes.\" The risk types and examples include:\n\nThe risk management process involves:\n\nThe COSO \"Enterprise Risk Management-Integrated Framework\" published in 2004 (New edition COSO ERM 2017 is not Mentioned and the 2004 version is outdated) defines ERM as a \"…process, effected by an entity's board of directors, management, and other personnel, applied in strategy setting and across the enterprise, designed to identify potential events that may affect the entity, and manage risk to be within its risk appetite, to provide reasonable assurance regarding the achievement of entity objectives.\"\n\nThe COSO ERM Framework has eight Components and four objectives categories. It is an expansion of the COSO Internal Control-Integrated Framework published in 1992 and amended in 1994. The eight components - additional components highlighted - are:\n\n\nThe four objectives categories - additional components highlighted - are:\n\nThe RIMS Risk Maturity Model (RMM) for Enterprise Risk Management, published in 2006, is an umbrella framework of content and methodology that detail the requirements for sustainable and effective enterprise risk management. The RMM model consists of twenty-five competency drivers for seven attributes that create ERM’s value and utility in an organization. The 7 attributes are:\n\nThe model was developed by Steven Minsky, CEO of LogicManager, and published by the Risk and Insurance Management Society in collaboration with the RIMS ERM Committee. The Risk Maturity Model is based on the Capability Maturity Model, a methodology founded by the Carnegie Mellon University Software Engineering Institute (SEI) in the 1980s.\n\nOrganizations by nature manage risks and have a variety of existing departments or functions (\"risk functions\") that identify and manage particular risks. However, each risk function varies in capability and how it coordinates with other risk functions. A central goal and challenge of ERM is improving this capability and coordination, while integrating the output to provide a unified picture of risk for stakeholders and improving the organization's ability to manage the risks effectively.\n\nThe primary risk functions in large corporations that may participate in an ERM program typically include:\n\n\nVarious consulting firms offer suggestions for how to implement an ERM program. Common topics and challenges include:\n\n\nIn addition to information technology audit, internal auditors play an important role in evaluating the risk-management processes of an organization and advocating their continued improvement. However, to preserve its organizational independence and objective judgment, Internal Audit professional standards indicate the function should not take any direct responsibility for making risk management decisions for the enterprise or managing the risk-management function.\n\nInternal auditors typically perform an annual risk assessment of the enterprise, to develop a plan of audit engagements for the upcoming year. This plan is updated at various frequencies in practice. This typically involves review of the various risk assessments performed by the enterprise (e.g., strategic plans, competitive benchmarking, and SOX 404 top-down risk assessment), consideration of prior audits, and interviews with a variety of senior management. It is designed for identifying audit projects, not to identify, prioritize, and manage risks directly for the enterprise.\n\nThe risk management processes of corporations worldwide are under increasing regulatory and private scrutiny. Risk is an essential part of any business. Properly managed, it drives growth and opportunity. Executives struggle with business pressures that may be partly or completely beyond their immediate control, such as distressed financial markets; mergers, acquisitions and restructurings; disruptive technology change; geopolitical instabilities; and the rising price of energy.\n\nSection 404 of the Sarbanes-Oxley Act of 2002 required U.S. publicly traded corporations to utilize a control framework in their internal control assessments. Many opted for the COSO Internal Control Framework, which includes a risk assessment element. In addition, new guidance issued by the Securities and Exchange Commission (SEC) and PCAOB in 2007 placed increasing scrutiny on top-down risk assessment and included a specific requirement to perform a fraud risk assessment. Fraud risk assessments typically involve identifying scenarios of potential (or experienced) fraud, related exposure to the organization, related controls, and any action taken as a result.\n\nThe New York Stock Exchange requires the Audit Committees of its listed companies to \"discuss policies with respect to risk assessment and risk management.\" The related commentary continues: \"While it is the job of the CEO and senior management to assess and manage the company’s exposure to risk, the audit committee must discuss guidelines and policies to govern the process by which this is handled. The audit committee should discuss the company’s major financial risk exposures and the steps management has taken to monitor and control such exposures. The audit committee is not required to be the sole body responsible for risk assessment and management, but, as stated above, the committee must discuss guidelines and policies to govern the process by which risk assessment and management is undertaken. Many companies, particularly financial companies, manage and assess their risk through mechanisms other than the audit committee. The processes these companies have in place should be reviewed in a general manner by the audit committee, but they need not be replaced by the audit committee.\"\n\nStandard & Poor's (S&P), the debt rating agency, plans to include a series of questions about risk management in its company evaluation process. This will rollout to financial companies in 2007. The results of this inquiry is one of the many factors considered in debt rating, which has a corresponding impact on the interest rates lenders charge companies for loans or bonds. On May 7, 2008, S&P also announced that it would begin including an ERM assessment in its ratings for non-financial companies starting in 2009, with initial comments in its reports during Q4 2008.\n\nISO 31000 is an International Standard for Risk Management which was published on 13 November 2009. An accompanying standard, ISO 31010 - Risk Assessment Techniques, soon followed publication (December 1, 2009) together with the updated Risk Management vocabulary ISO Guide 73.\n\nIFC Performance Standard focuses on the management of Health, Safety, Environmental and Social risks. The third edition was published on January 1, 2012 after a two-year negotiation process with the private sector, governments and civil society organisations. It has been adopted by the Equator Banks, a consortium of over 90 commercial banks in 37 countries.\n\nData privacy rules, such as the European Union's General Data Protection Regulation, increasingly foresee significant penalties for failure to maintain adequate protection of individuals' personal data such as names, e-mail addresses and personal financial information, or alert affected individuals when data privacy is breached. The EU regulation requires any organization--including organizations located outside the EU--to appoint a Data Protection Officer reporting to the highest management level if they handle the personal data of anyone living in the EU.\n\nIn 2003, the Enterprise Risk Management Committee of the Casualty Actuarial Society (CAS) issued its overview of ERM. This paper laid out the evolution, rationale, definitions, and frameworks for ERM from the casualty actuarial perspective, and also included a vocabulary, conceptual and technical foundations, actual practice and applications, and case studies.\n\nThe CAS has specific stated ERM goals, including being \"a leading supplier internationally of educational materials relating to Enterprise Risk Management (ERM) in the property casualty insurance arena,\" and has sponsored research, development, and training of casualty actuaries in that regard. The CAS has refrained from issuing its own credential; instead, in 2007, the CAS Board decided that the CAS should participate in the initiative to develop a global ERM designation, and make a final decision at some later date.\n\nIn 2007, the Society of Actuaries developed the Chartered Enterprise Risk Analyst (CERA) credential in response to the growing field of enterprise risk management. This is the first new professional credential to be introduced by the SOA since 1949. A CERA studies to focus on how various risks, including operational, investment, strategic, and reputational combine to affect organizations. CERAs work in environments beyond insurance, reinsurance and the consulting markets, including broader financial services, energy, transportation, media, technology, manufacturing and healthcare.\n\nIt takes approximately three to four years to complete the CERA curriculum which combines basic actuarial science, ERM principles and a course on professionalism. To earn the CERA credential, candidates must take five exams, fulfill an educational experience requirement, complete one online course, and attend one in-person course on professionalism.\n\nInitially all CERAs were members of the Society of Actuaries but in 2009 the CERA designation became a global specialized professional credential, awarded and regulated by multiple actuarial bodies.\n\nThe Institute and Faculty of Actuaries (the merged body formed in 2010 from the Institute of Actuaries and the Faculty of Actuaries) is the professional body representing actuaries in the United Kingdom. In March 2008, Enterprise Risk Management was adopted as one of the six actuarial practice areas, reflecting the increased involvement of actuaries in the ERM field.\n\nA regular newsletter communicates the ongoing work that the profession performs in respect of ERM.\n\nSome of the key areas that the profession works on are summarised below (together with some of the recent outcomes in each area):\n\n\nFrom April 2010 actuaries were able to study ERM as one of the Specialist Technical Stage exams (ST9 course information), which (with other exam passes) gives candidates the Chartered Enterprise Risk Actuary (CERA) qualification. In July 2010 the first nine actuaries to obtain the CERA qualification were announced. The CERA qualification is offered by 13 participating actuarial associations, with further information available at a global or UK level.\n\nVarious events (e.g. networking evenings and webinars) are available to actuaries and other interested parties.\nThe main event is the Risk and Investment Conference, which is often held during the summer months. There is also some regularly reviewed material available from the profession which may be of use in developing knowledge of ERM.\n\n\nA committee has been established to consider research and thought leadership in the ERM field (including what the “elevator speech” on ERM issues might be, definition of the scope of ERM and demonstration of the value of ERM).\n\nSome areas in which work has been completed include:\n\n- ERM - A guide to Implementation \n- A survey on actuaries in risk management \n- A suggested common risk classification system for the actuarial profession\n\nResearch topics will be categorised and subject to a number of tests before proceeding with the research.\n\n- enterprise-wide test (not just topic-specific / silo-based)\n- risk management test (management = taking actions, not just modelling)\n- director test (important enough for the Board, not just line managers)\n\n\nActuaries continue to look to demonstrate and promote the value of actuaries and the CERA qualification in the field of ERM - including through publication of articles in the Actuary\n\nThe Actuarial Profession also liaises with other professions where appropriate- e.g. the Institution of Civil Engineers on considering ERM in the context of Risk Analysis and Management for Projects (RAMP).\n\nIt is clear that companies recognize ERM as a critical management issue. This is demonstrated through the prominence assigned to ERM within organizations and the resources devoted to building ERM capabilities. In a 2008 survey by Towers Perrin, at most life insurance companies, responsibility for ERM resides within the C-suite. Most often, the chief risk officer (CRO) or the chief financial officer (CFO) is in charge of ERM, and these individuals typically report directly to the chief executive officer. From their vantage point, the CRO and CFO are able to look across the organization and develop a perspective on the risk profile of the firm and how that profile matches its risk appetite. They act as drivers to improve skills, tools and processes for evaluating risks and to weigh various actions to manage those exposures. Companies are also actively enhancing their ERM tools and capabilities. Three quarters of responding companies said they have tools for specifically monitoring and managing enterprise-wide risk. These tools are used primarily for identifying and measuring risk and for management decision making. Respondents also reported that they have made good progress in building their ERM capabilities in certain areas.\n\nIn this study, more than 80% of respondents reported that they currently have adequate or better controls in place for most major risks. In addition, about 60% currently have a coordinated process for risk governance and include risk management\nin decision making to optimize risk adjusted returns.\n\nIn another survey conducted in May and June 2008, against the backdrop of the developing financial crisis, six major findings came to light regarding risk and capital management among insurers worldwide:\n\nNedbank in South Africa approaches ERM as a strategy to help them \"optimise risk versus return on a sustainable basis, and risk management is therefore approached across three integrated core dimensions: Managing risk as a THREAT...as an UNCERTAINTY and as an OPPORTUNITY\".\n\nThe Reserve Bank of Australia - The Bank has established a risk appetite statement regarding its key risks, including risk appetite statements, a supporting risk management framework, and implementation guidelines.\n\n"}
{"id": "2912899", "url": "https://en.wikipedia.org/wiki?curid=2912899", "title": "Ergodic sequence", "text": "Ergodic sequence\n\nIn mathematics, an ergodic sequence is a certain type of integer sequence, having certain equidistribution properties.\n\nLet formula_1 be an infinite, strictly increasing sequence of positive integers. Then, given an integer \"q\", this sequence is said to be ergodic mod \"q\" if, for all integers formula_2, one has\n\nwhere\n\nand card is the count (the number of elements) of a set, so that formula_5 is the number of elements in the sequence \"A\" that are less than or equal to \"t\", and\n\nso formula_7 is the number of elements in the sequence \"A\", less than \"t\", that are equivalent to \"k\" modulo \"q\". That is, a sequence is an ergodic sequence if it becomes uniformly distributed mod \"q\" as the sequence is taken to infinity.\n\nAn equivalent definition is that the sum\n\nvanish for every integer \"k\" with formula_9. \n\nIf a sequence is ergodic for all \"q\", then it is sometimes said to be ergodic for periodic systems.\n\nThe sequence of positive integers is ergodic for all \"q\".\n\nAlmost all Bernoulli sequences, that is, sequences associated with a Bernoulli process, are ergodic for all \"q\". That is, let formula_10 be a probability space of random variables over two letters formula_11. Then, given formula_12, the random variable formula_13 is 1 with some probability \"p\" and is zero with some probability 1-\"p\"; this is the definition of a Bernoulli process. Associated with each formula_14 is the sequence of integers\n\nThen almost every sequence formula_16 is ergodic.\n\n"}
{"id": "13547826", "url": "https://en.wikipedia.org/wiki?curid=13547826", "title": "Forbidden graph characterization", "text": "Forbidden graph characterization\n\nIn graph theory, a branch of mathematics, many important families of graphs can be described by a finite set of individual graphs\nthat do not belong to the family and further exclude all graphs from the family which contain any of these forbidden graphs as (induced) subgraph or minor.\nA prototypical example of this phenomenon is Kuratowski's theorem, which states that a graph is planar (can be drawn without crossings in the plane) if and only if it does not contain either of two forbidden graphs, the complete graph \"K\" and the complete bipartite graph \"K\". For Kuratowski's theorem, the notion of containment is that of graph homeomorphism, in which a subdivision of one graph appears as a subgraph of the other. Thus, every graph either has a planar drawing (in which case it belongs to the family of planar graphs) or it has a subdivision of one of these two graphs as a subgraph (in which case it does not belong to the planar graphs).\n\nMore generally, a forbidden graph characterization is a method of specifying a family of graph, or hypergraph, structures, by specifying substructures that are forbidden from existing within any graph in the family. Different families vary in the nature of what is \"forbidden\". In general, a structure \"G\" is a member of a family formula_1 if and only if a forbidden substructure is not contained in \"G\". The forbidden substructure might be one of:\nThe set of structures that are forbidden from belonging to a given graph family can also be called an obstruction set for that family.\n\nForbidden graph characterizations may be used in algorithms for testing whether a graph belongs to a given family. In many cases, it is possible to test in polynomial time whether a given graph contains any of the members of the obstruction set, and therefore whether it belongs to the family defined by that obstruction set.\n\nIn order for a family to have a forbidden graph characterization, with a particular type of substructure, the family must be closed under substructures.\nThat is, every substructure (of a given type) of a graph in the family must be another graph in the family. Equivalently, if a graph is not part of the family, all larger graphs containing it as a substructure must also be excluded from the family. When this is true, there always exists an obstruction set (the set of graphs that are not in the family but whose smaller substructures all belong to the family). However, for some notions of what a substructure is, this obstruction set could be infinite. The Robertson–Seymour theorem proves that, for the particular case of graph minors, a family that is closed under minors always has a finite obstruction set.\n\n"}
{"id": "40023277", "url": "https://en.wikipedia.org/wiki?curid=40023277", "title": "Geometrically regular ring", "text": "Geometrically regular ring\n\nIn algebraic geometry, a geometrically regular ring is a Noetherian ring over a field that remains a regular ring after any finite extension of the base field. Geometrically regular schemes are defined in a similar way. In older terminology, points with regular local rings were called simple points, and points with geometrically regular local rings were called absolutely simple points. Over fields that are of characteristic 0, or algebraically closed, or more generally perfect, geometrically regular rings are the same as regular rings. Geometric regularity originated when Chevalley and Weil pointed out to that, over non-perfect fields, the Jacobian criterion for a simple point of an algebraic variety is not equivalent to the condition that the local ring is regular.\n\nA Noetherian local ring containing a field \"k\" is geometrically regular over \"k\" if and only if it is formally smooth over \"k\".\n\n gave the following two examples of local rings that are regular but not geometrically regular. \n\n\n\n"}
{"id": "58619302", "url": "https://en.wikipedia.org/wiki?curid=58619302", "title": "Hanoi graph", "text": "Hanoi graph\n\nIn graph theory and recreational mathematics, the Hanoi graphs are undirected graphs whose vertices represent the possible states of the Tower of Hanoi puzzle, and whose edges represent allowable moves between pairs of states.\n\nThe puzzle consists of a set of disks of different sizes, placed in increasing order of size on a fixed set of towers.\nThe Hanoi graph for a puzzle with formula_1 disks on formula_2 towers is denoted formula_3. Each state of the puzzle is determined by the choice of one tower for each disk, so the graph has formula_4 vertices.\n\nIn the moves of the puzzle, the smallest disk on one tower is moved either to an unoccupied tower or to a tower whose smallest disk is larger. If there are formula_5 unoccupied towers, the number of allowable moves is\nwhich ranges from a maximum of formula_7\nto formula_10 (when all disks are on one tower and formula_5 is formula_10). Therefore, the degrees of the vertices in the Hanoi graph range from a maximum of formula_7 to a minimum of formula_10.\nThe total number of edges is\n\nFor formula_16 (no disks) there is only one state of the puzzle and one vertex of the graph.\nFor formula_17, the Hanoi graph formula_3 can be decomposed into formula_2 copies of the smaller Hanoi graph formula_20, one for each placement of the largest disk. These copies are connected to each other only at states where the largest disk is free to move: it is the only disk in its tower, and some other tower is unoccupied.\n\nEvery Hanoi graph contains a Hamiltonian cycle.\n\nThe Hanoi graph formula_21 is a complete graph on formula_2 vertices. Because they contain complete graphs, all larger Hanoi graphs formula_3 require at least formula_2 colors in any graph coloring. They may be colored with exactly formula_2 colors by summing the indexes of the towers containing each disk, and using the sum modulo formula_2 as the color.\n\nA particular case of the Hanoi graphs that has been well studied since the work of is the case of the three-tower Hanoi graphs, formula_27. These graphs are penny graphs (the contact graphs of non-overlapping unit disks in the plane), with an arrangement of disks that resembles the Sierpinski triangle. One way of constructing this arrangement is to arrange the numbers of Pascal's triangle on the points of a hexagonal lattice, with unit spacing, and place a unit disk on each point whose number is odd.\nThe diameter of these graphs, and the length of the solution to the standard form of the Tower of Hanoi puzzle (in which the disks all start on one tower and must all move to one other tower) is formula_28.\n\nFor formula_29, the structure of the Hanoi graphs is not as well understood, and the diameter of these graphs is unknown.\nWhen formula_30 or when formula_31 and formula_32, these graphs are nonplanar.\n"}
{"id": "2522311", "url": "https://en.wikipedia.org/wiki?curid=2522311", "title": "Horn function", "text": "Horn function\n\nIn the theory of special functions in mathematics, the Horn functions (named for Jakob Horn) are the 34 distinct convergent hypergeometric series of order two (i.e. having two independent variables), enumerated by (corrected by ). They are listed in . B. C. Carlson revealed a problem with the Horn function classification scheme.\n\n"}
{"id": "193748", "url": "https://en.wikipedia.org/wiki?curid=193748", "title": "Integration by substitution", "text": "Integration by substitution\n\nIn calculus, integration by substitution, also known as \"u\"-substitution, is a method for finding integrals. Using the fundamental theorem of calculus often requires finding an antiderivative. For this and other reasons, integration by substitution is an important tool in mathematics. It is the counterpart to the chain rule for differentiation.\n\nLet be an interval and be a differentiable function with integrable derivative. Suppose that is a continuous function. Then\n\nIn Leibniz notation, the substitution yields \nWorking heuristically with infinitesimals yields the equation\nwhich suggests the substitution formula above. (This equation may be put on a rigorous foundation by interpreting it as a statement about differential forms.) One may view the method of integration by substitution as a partial justification of Leibniz's notation for integrals and derivatives.\n\nThe formula is used to transform one integral into another integral that is easier to compute. Thus, the formula can be used from left to right or from right to left in order to simplify a given integral. When used in the latter manner, it is sometimes known as u\"-substitution or w\"-substitution.\n\nIntegration by substitution can be derived from the fundamental theorem of calculus as follows. Let and be two functions satisfying the above hypothesis that is continuous on and is integrable on the closed interval . Then the function is also integrable on . Hence the integrals\n\nand\n\nin fact exist, and it remains to show that they are equal.\n\nSince is continuous, it has an antiderivative . The composite function is then defined. Since is differentiable, combining the chain rule and the definition of an antiderivative gives\n\nApplying the fundamental theorem of calculus twice gives\n\nwhich is the substitution rule.\n\nConsider the integral\n\nIf we apply the formula from right to left and make the substitution , we obtain and hence . Therefore\n\nSince the lower limit was replaced with , and the upper limit replaced with , a transformation back into terms of was unnecessary.\n\nFor the integral\nthe formula needs to be used from left to right. The substitution , is useful because formula_11:\n\nThe resulting integral can be computed using integration by parts or a double angle formula followed by one more substitution. One can also note that the function being integrated is the upper right quarter of a circle with a radius of one, and hence integrating the upper right quarter from zero to one is the geometric equivalent to the area of one quarter of the unit circle, or .\n\nSubstitution can be used to determine antiderivatives. One chooses a relation between and , determines the corresponding relation between and by differentiating, and performs the substitutions. An antiderivative for the substituted function can hopefully be determined; the original substitution between and is then undone.\n\nSimilar to our first example above, we can determine the following antiderivative with this method:\n\nwhere \"C\" is an arbitrary constant of integration.\n\nNote that there were no integral boundaries to transform, but in the last step we had to revert the original substitution .\n\nOne may also use substitution when integrating functions of several variables. \nHere the substitution function needs to be injective and continuously differentiable, and the differentials transform as\n\nwhere denotes the determinant of the Jacobian matrix of partial derivatives of \"φ\" at the point . This formula expresses the fact that the absolute value of the determinant of a matrix equals the volume of the parallelotope spanned by its columns or rows.\n\nMore precisely, the \"change of variables\" formula is stated in the next theorem:\n\nTheorem. Let be an open set in and an injective differentiable function with continuous partial derivatives, the Jacobian of which is nonzero for every in . Then for any real-valued, compactly supported, continuous function , with support contained in ,\n\nThe conditions on the theorem can be weakened in various ways. First, the requirement that be continuously differentiable can be replaced by the weaker assumption that be merely differentiable and have a continuous inverse . This is guaranteed to hold if is continuously differentiable by the inverse function theorem. Alternatively, the requirement that can be eliminated by applying Sard's theorem .\n\nFor Lebesgue measurable functions, the theorem can be stated in the following form :\n\nTheorem. Let be a measurable subset of and an injective function, and suppose for every in there exists in such that as (here is little-\"o\" notation). Then is measurable, and for any real-valued function defined on ,\nin the sense that if either integral exists (including the possibility of being properly infinite), then so does the other one, and they have the same value.\n\nAnother very general version in measure theory is the following :\n\nTheorem. Let be a locally compact Hausdorff space equipped with a finite Radon measure , and let be a σ-compact Hausdorff space with a σ-finite Radon measure . Let be a continuous and absolutely continuous function (where the latter means that whenever ). Then there exists a real-valued Borel measurable function on such that for every Lebesgue integrable function , the function is Lebesgue integrable on , and\nFurthermore, it is possible to write\nfor some Borel measurable function on .\n\nIn geometric measure theory, integration by substitution is used with Lipschitz functions. A bi-Lipschitz function is a Lipschitz function which is injective and whose inverse function is also Lipschitz. By Rademacher's theorem a bi-Lipschitz mapping is differentiable almost everywhere. In particular, the Jacobian determinant of a bi-Lipschitz mapping is well-defined almost everywhere. The following result then holds:\n\nTheorem. Let be an open subset of and be a bi-Lipschitz mapping. Let be measurable. Then\nin the sense that if either integral exists (or is properly infinite), then so does the other one, and they have the same value.\n\nThe above theorem was first proposed by Euler when he developed the notion of double integrals in 1769. Although generalized to triple integrals by Lagrange in 1773, and used by Legendre, Laplace, Gauss, and first generalized to variables by Mikhail Ostrogradski in 1836, it resisted a fully rigorous formal proof for a surprisingly long time, and was first satisfactorily resolved 125 years later, by Élie Cartan in a series of papers beginning in the mid-1890s (; ).\n\nSubstitution can be used to answer the following important question in probability: given a random variable formula_20 with probability density formula_21 and another random variable formula_22 related to formula_20 by the equation formula_24, what is the probability density for formula_22?\n\nIt is easiest to answer this question by first answering a slightly different question: what is the probability that formula_22 takes a value in some particular subset formula_27? Denote this probability formula_28. Of course, if formula_22 has probability density formula_30 then the answer is\n\nbut this isn't really useful because we don't know formula_30; it's what we're trying to find. We can make progress by considering the problem in the variable formula_20. formula_22 takes a value in formula_27 whenever formula_20 takes a value in formula_37, so\n\nChanging from variable formula_39 to formula_40 gives\n\nCombining this with our first equation gives\n\nso\n\nIn the case where formula_20 and formula_22 depend on several uncorrelated variables, i.e. formula_46 and formula_24, formula_30 can be found by substitution in several variables discussed above. The result is\n\n\n\n"}
{"id": "42530503", "url": "https://en.wikipedia.org/wiki?curid=42530503", "title": "Journal of Pure and Applied Algebra", "text": "Journal of Pure and Applied Algebra\n\nThe Journal of Pure and Applied Algebra is a monthly peer-reviewed scientific journal covering that part of algebra likely to be of general mathematical interest: algebraic results with immediate applications, and the development of algebraic theories of sufficiently general relevance to allow for future applications. Its founding editors-in-chief were P. Freyd (University of Pennsylvania) and A. Heller (City University of New York). The current managing editors are Eric Friedlander (University of Southern California), Chuck Weibel (Rutgers University), and Srikanth Iyengar (University of Utah).\n\nThe journal is abstracted and indexed in Current Contents/Physics, Chemical, & Earth Sciences, Mathematical Reviews, PASCAL, Science Citation Index, Zentralblatt MATH, and Scopus. According to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 0.652.\n"}
{"id": "24350828", "url": "https://en.wikipedia.org/wiki?curid=24350828", "title": "L-moment", "text": "L-moment\n\nIn statistics, L-moments are a sequence of statistics used to summarize the shape of a probability distribution. They are linear combinations of order statistics (L-statistics) analogous to conventional moments, and can be used to calculate quantities analogous to standard deviation, skewness and kurtosis, termed the L-scale, L-skewness and L-kurtosis respectively (the L-mean is identical to the conventional mean). Standardised L-moments are called L-moment ratios and are\nanalogous to standardized moments. Just as for conventional moments, a theoretical distribution has a set of population L-moments. Sample L-moments can be defined for a sample from the population, and can be used as estimators of the population L-moments.\n\nFor a random variable \"X\", the \"r\"th population L-moment is\n\nwhere \"X\" denotes the \"k\" order statistic (\"k\" smallest value) in an independent sample of size \"n\" from the distribution of \"X\" and formula_2 denotes expected value. In particular, the first four population L-moments are\n\nNote that the coefficients of the \"k\"-th L-moment are the same as in the \"k\"-th term of the binomial transform, as used in the \"k\"-order finite difference (finite analog to the derivative).\n\nThe first two of these L-moments have conventional names:\nThe L-scale is equal to half the mean difference.\n\nThe sample L-moments can be computed as the population L-moments of the sample, summing over \"r\"-element subsets of the sample formula_9 hence averaging by dividing by the binomial coefficient:\n\nGrouping these by order statistic counts the number of ways an element of an \"n\"-element sample can be the \"j\"th element of an \"r\"-element subset, and yields formulas of the form below. Direct estimators for the first four L-moments in a finite sample of \"n\" observations are:\nwhere is the th order statistic and formula_15 is a binomial coefficient. Sample L-moments can also be defined indirectly in terms of probability weighted moments, which leads to a more efficient algorithm for their computation.\n\nA set of \"L-moment ratios\", or scaled L-moments, is defined by\nThe most useful of these are formula_17, called the \"L-skewness\", and formula_18, the \"L-kurtosis\".\n\nL-moment ratios lie within the interval (–1, 1). Tighter bounds can be found for some specific L-moment ratios; in particular, the L-kurtosis formula_18 lies in [-¼,1), and\n\nA quantity analogous to the coefficient of variation, but based on L-moments, can also be defined:\nformula_21\nwhich is called the \"coefficient of L-variation\", or \"L-CV\". For a non-negative random variable, this lies in the interval (0,1) and is identical to the Gini coefficient.\n\nL-moments are statistical quantities that are derived from probability weighted moments (PWM) which were defined earlier (1979). PWM are used to efficiently estimate the parameters of distributions expressable in inverse form such as the Gumbel, the Tukey, and the Wakeby distributions.\n\nThere are two common ways that L-moments are used, in both cases analogously to the conventional moments:\n\nIn addition to doing these with standard moments, the latter (estimation) is more commonly done using maximum likelihood methods; however using L-moments provides a number of advantages. Specifically, L-moments are more robust than conventional moments, and existence of higher L-moments only requires that the random variable have finite mean. One disadvantage of L-moment ratios for estimation is their typically smaller sensitivity. For instance, the Laplace distribution has a kurtosis of 6 and weak exponential tails, but a larger 4th L-moment ratio than e.g. the student-t distribution with d.f.=3, which has an infinite kurtosis and much heavier tails.\n\nAs an example consider a dataset with a few data points and one outlying data value. If the ordinary standard deviation of this data set is taken it will be highly influenced by this one point: however, if the L-scale is taken it will be far less sensitive to this data value. Consequently, L-moments are far more meaningful when dealing with outliers in data than conventional moments. However, there are also other better suited methods to achieve an even higher robustness than just replacing moments by L-moments. One example of this is using L-moments as summary statistics in extreme value theory (EVT). This application shows the limited robustness of L-moments, i.e. L-statistics are not resistant statistics, as a single extreme value can throw them off, but because they are only linear (not higher-order statistics), they are less affected by extreme values than conventional moments.\n\nAnother advantage L-moments have over conventional moments is that their existence only requires the random variable to have finite mean, so the L-moments exist even if the higher conventional moments do not exist (for example, for Student's t distribution with low degrees of freedom). A finite variance is required in addition in order for the standard errors of estimates of the L-moments to be finite.\n\nSome appearances of L-moments in the statistical literature include the book by David & Nagaraja (2003, Section 9.9) and a number of papers. A number of favourable comparisons of L-moments with ordinary moments have been reported.\n\nThe table below gives expressions for the first two L-moments and numerical values of the first two L-moment ratios of some common continuous probability distributions with constant L-moment ratios.\nMore complex expressions have been derived for some further distributions for which the L-moment ratios vary with one or more of the distributional parameters, including the log-normal, Gamma, generalized Pareto, generalized extreme value, and generalized logistic distributions.\nThe notation for the parameters of each distribution is the same as that used in the linked article. In the expression for the mean of the Gumbel distribution, \"γ\" is the Euler–Mascheroni constant 0.57721… .\n\n\"Trimmed L-moments\" are generalizations of L-moments that give zero weight to extreme observations. They are therefore more robust to the presence of outliers, and unlike L-moments they may be well-defined for distributions for which the mean does not exist, such as the Cauchy distribution.\n\n\n"}
{"id": "12795419", "url": "https://en.wikipedia.org/wiki?curid=12795419", "title": "Laplace principle (large deviations theory)", "text": "Laplace principle (large deviations theory)\n\nIn mathematics, Laplace's principle is a basic theorem in large deviations theory which is similar to Varadhan's lemma. It gives an asymptotic expression for the Lebesgue integral of exp(−\"θφ\"(\"x\")) over a fixed set \"A\" as \"θ\" becomes large. Such expressions can be used, for example, in statistical mechanics to determining the limiting behaviour of a system as the temperature tends to absolute zero.\n\nLet \"A\" be a Lebesgue-measurable subset of \"d\"-dimensional Euclidean space R and let \"φ\" : R → R be a measurable function with\n\nThen\n\nwhere ess inf denotes the essential infimum. Heuristically, this may be read as saying that for large \"θ\",\n\nThe Laplace principle can be applied to the family of probability measures P given by\n\nto give an asymptotic expression for the probability of some event \"A\" as \"θ\" becomes large. For example, if \"X\" is a standard normally distributed random variable on R, then\n\nfor every measurable set \"A\".\n\n"}
{"id": "44454257", "url": "https://en.wikipedia.org/wiki?curid=44454257", "title": "Louvain Modularity", "text": "Louvain Modularity\n\nThe Louvain Method for community detection is a method to extract communities from large networks created by Blondel \"et al\". from the University of Louvain (affiliation of authors has given the method its name). The method is a greedy optimization method that appears to run in time formula_1.\n\nThe inspiration for this method of community detection is the optimization of Modularity as the algorithm progresses. Modularity is a scale value between -1 and 1 that measures the density of edges inside communities to edges outside communities. Optimizing this value theoretically results in the best possible grouping of the nodes of a given network, however going through all possible iterations of the nodes into groups is impractical so heuristic algorithms are used.\nIn the Louvain Method of community detection, first small communities are found by optimizing modularity locally on all nodes, then each small community is grouped into one node and the first step is repeated. The method is similar to the earlier method by Clauset, Newman and Moore that connects communities whose amalgamation produces the largest increase in modularity.\n\nThe value to be optimized is modularity, defined as a value between -1 and 1 that measures the density of links inside communities compared to links between communities. For a weighted graph, modularity is defined as:\n\nformula_2\n\nwhere\n\nIn order to maximize this value efficiently, the Louvain Method has two phases that are repeated iteratively.\n\nFirst, each node in the network is assigned to its own community. Then for each node formula_4, the change in modularity is calculated for removing formula_4 from its own community and moving it into the community of each neighbor formula_5 of formula_4. This value is easily calculated by two steps: (1) removing formula_4 from its original community, and (2) inserting formula_4 to the community of formula_5. The two equations are quite similar, and the equation for step (2) is: \n\nformula_21\n\nWhere formula_22 is sum of all the weights of the links inside the community formula_4 is moving into, formula_24 is the sum of all the weights of the links to nodes in the community formula_4 is moving into, formula_6 is the weighted degree of formula_4, formula_28 is the sum of the weights of the links between formula_4 and other nodes in the community that formula_4 is moving into, and formula_31 is the sum of the weights of all links in the network. Then, once this value is calculated for all communities formula_4 is connected to, formula_4 is placed into the community that resulted in the greatest modularity increase. If no increase is possible, formula_4 remains in its original community. This process is applied repeatedly and sequentially to all nodes until no modularity increase can occur. Once this local maximum of modularity is hit, the first phase has ended.\n\nIn the second phase of the algorithm, it groups all of the nodes in the same community and builds a new network where nodes are the communities from the previous phase. Any links between nodes of the same community are now represented by self loops on the new community node and links from multiple nodes in the same community to a node in a different community are represented by weighted edges between communities. Once the new network is created, the second phase has ended and the first phase can be re-applied to the new network.\n\n\nWhen comparing modularity optimization methods, the two measures of importance are the speed and the resulting modularity value. A higher speed is better as it shows a method is more efficient than others and a higher modularity value is desirable as it points to having better defined communities.\nThe compared methods are, the algorithm of Clauset, Newman, and Moore, Pons and Latapy, and Watika and Tsurumi.\n-/- in the table refers to a method that took over 24hrs to run. This table (from ) shows that the Louvain method outperforms many similar modularity optimization methods in both the modularity and the time categories.\n\n\n"}
{"id": "6935703", "url": "https://en.wikipedia.org/wiki?curid=6935703", "title": "Miklós Ajtai", "text": "Miklós Ajtai\n\nMiklós Ajtai (born 2 July 1946) is a computer scientist at the IBM Almaden Research Center, United States. In 2003, he received the Knuth Prize for his numerous contributions to the field, including a classic sorting network algorithm (developed jointly with J. Komlós and Endre Szemerédi), exponential lower bounds, superlinear time-space tradeoffs for branching programs, and other \"unique and spectacular\" results.\n\nOne of Ajtai's results states that the length of proofs in propositional logic of the pigeonhole principle for \"n\" items grows faster than any polynomial in \"n\". He also proved that the statement \"any two countable structures that are second-order equivalent are also isomorphic\" is both consistent with and independent of ZFC. Ajtai and Szemerédi proved the corners theorem, an important step toward higher-dimensional generalizations of the Szemerédi theorem. With Komlós and Szemerédi he proved the \"ct\"/log \"t\" upper bound for the Ramsey number \"R\"(3,\"t\"). The corresponding lower bound was proved by Kim only in 1995, a result that earned him a Fulkerson Prize. With Chvátal, Newborn, and Szemerédi, Ajtai proved the crossing number inequality, that any drawing of a graph with \"n\" vertices and \"m\" edges, where , has at least crossings. Ajtai and Dwork devised in 1997 a lattice-based public-key cryptosystem; Ajtai has done extensive work on lattice problems. For his numerous contributions in Theoretical Computer Science he received the Knuth Prize.\n\nAjtai received his Candidate of Sciences degree in 1976 from the Hungarian Academy of Sciences. Since 1995 he has been an external member of the Hungarian Academy of Sciences.\n\n"}
{"id": "39378034", "url": "https://en.wikipedia.org/wiki?curid=39378034", "title": "Minimum rank of a graph", "text": "Minimum rank of a graph\n\nIn mathematics, the minimum rank is a graph parameter formula_1 for any graph \"G\". It was motivated by the Colin de Verdière's invariant.\n\nThe adjacency matrix of a given undirected graph is a symmetric matrix whose rows and columns both correspond to the vertices of the graph. Its coefficients are all 0 or 1, and the coefficient in row \"i\" and column \"j\" is nonzero whenever vertex \"i\" is adjacent to vertex \"j\" in the graph. More generally, one can define a \"generalized adjacency matrix\" to be any matrix of real numbers with the same pattern of nonzeros. The minimum rank of the graph formula_2 is denoted by formula_3 and is defined as the smallest rank of any generalized adjacency matrix of the graph.\n\n\nSeveral families of graphs may be characterized in terms of their minimum ranks.\n"}
{"id": "16293393", "url": "https://en.wikipedia.org/wiki?curid=16293393", "title": "Multipartite entanglement", "text": "Multipartite entanglement\n\nIn the case of systems composed of formula_1 subsystems the classification of entangled states is richer than in the bipartite case. Indeed, in multipartite entanglement apart from fully separable states and fully entangled states, there also exists the notion of partially separable states.\n\nThe definitions of fully separable and fully entangled multipartite states naturally generalizes that of separable and entangled states in the bipartite case, as follows.\n\nDefinition [Full formula_2-partite separability (formula_2-separability) of formula_2 systems]: The state formula_5 of formula_2 subsystems formula_7 with Hilbert space formula_8 is fully separable if and only if it can be written in the form\nCorrespondingly, the state formula_5 is fully entangled if it cannot be written in the above form.\n\nAs in the bipartite case, the set of formula_2-separable states is \"convex\" and \"closed\" with respect to trace norm, and separability is preserved under formula_2-separable operations formula_13 which are a straightforward generalization of the bipartite ones:\n\nAs mentioned above, though, in the multipartite setting we also have different notions of partial separability.\n\nDefinition [separability with respect to partitions]: The state formula_5 of formula_2 subsystems formula_7 is separable with respect to a given partition formula_18, where formula_19 are disjoint subsets of the indices formula_20, if and only if it can be written\n\nDefinition [semiseparability]: The state formula_5 is semiseparable if and only if it is separable under all formula_23-formula_24 partitions, formula_25.\n\nDefinition [s-particle entanglement]: An formula_2-particle system can have at most formula_27-particle entanglement if it is a mixture of all states such that each of them is separable with respect to some partition formula_28, where all sets of indices formula_29 have cardinality formula_30.\n\nAn equivalent definition to Full m-partite separability is given as follows: The pure state formula_31 of formula_2 subsystems formula_7 is fully formula_2-partite separable if and only if it can be written\n\nIn order to check this, it is enough to compute reduced density matrices of elementary subsystems and see whether they are pure. However, this cannot be done so easily in the multipartite case, as only rarely multipartite pure states admit the \"generalized Schmidt Decomposition\" formula_36. A multipartite state admits generalized Schmidt decomposition if, tracing out any subsystem, the rest is in a fully separable state. \nThus, in general the entanglement of a pure state is described by the spectra of the reduced density matrices of all bipartite partitions: the state is genuinely formula_2-partite entangled if and only if all bipartite partitions produce mixed reduced density matrices.\n\nIn the multipartite case there is no simple necessary and sufficient condition for separability like the one given by the PPT criterion for the formula_38 and formula_39 cases. However, many separability criteria used in the bipartite setting can be generalized to the multipartite case.\n\nThe characterization of separability in terms of positive but not completely positive maps can be naturally generalized from the bipartite case, as follows.\n\nAny positive but not completely positive (PnCP) map formula_40 provides a nontrivial necessary separability criterion in the form:\nwhere formula_42 is the identity acting on the first subsystem formula_43.\nThe state formula_5 is \"separable\" if and only if the above condition is satisfied for all PnCP maps formula_40.\n\nThe definition of entanglement witnesses and the Choi-Jamiolkowski isomorphism that links PnCP maps to entanglement witnesses in the bipartite case can also be generalized to the multipartite setting.\nWe therefore get a separability condition from entanglement witnesses for multipartite states: the state formula_5 is separable if it has non-negative mean value formula_47 for all entanglement witnesses formula_48. Correspondingly, the entanglement of formula_5 is detected by the witness formula_50 if and only if formula_51.\n\nThe above description provides a full characterization of formula_2-separability of formula_2-partite systems.\n\nThe \"range criterion\" can also be immediately generalized from the bipartite to the multipartite case. In the latter case the range of formula_5 must be spanned by the vectors formula_55, while the range of formula_56 partially transposed with respect to the subset formula_57 must be spanned by the products of these vectors where those with indices formula_58 are complex conjugated. If the state formula_5 is \"separable\", then all such partial transposes must lead to matrices with non-negative spectrum, i.e. all the matrices formula_56 should be states themselves.\n\nThe \"realignment criteria\" from the bipartite case are generalized to permutational criteria in the multipartite setting: if the state formula_61 is separable, then the matrix formula_62, obtained from the original state via permutation formula_63 of matrix indices in product basis, satisfies formula_64.\n\nFinally, the contraction criterion generalizes immediately from the bipartite to the multipartite case.\n\nMany of the axiomatic entanglement measures for bipartite states, such as relative entropy of entanglement, robustness of entanglement and squashed entanglement can be generalized to the multipartite setting.\nThe relative entropy of entanglement, for example, can be generalized to the multipartite case by taking a suitable set in place of the set of bipartite separable states. One can take the set of fully separable states, even though with this choice the measure will not distinguish between truly multipartite entanglement and several instances of bipartite entanglement, such as formula_65. In order to analyze truly multipartite entanglement one has to consider the set of states containing no more than formula_66-particle entanglement.\n\nIn the case of squashed entanglement, its multipartite version can be obtained by simply replacing the mutual information of the bipartite system with its generalization for multipartite systems, i.e. \nformula_67.\n\nHowever, in the multipartite setting many more parameters are needed to describe the entanglement of the states, and therefore many new entanglement measures have been constructed, especially for pure multipartite states.\n\nIn the multipartite setting there are entanglement measures that simply are functions of sums of bipartite entanglement measures, as, for instance, the global entanglement, which is given by the sum of concurrences between one qubit and all others. For these multipartite entanglement measures the 'monotonicity under LOCC is simply inherited from the bipartite measures. But there are also entanglement measures that were constructed specifically for multipartite states, as the following:\n\nThe first multipartite entanglement measure that is neither a direct generalization nor an easy combination of bipartite measures was introduced by Coffman \"et al.\" and called tangle.\n\nDefinition [tangle]:\nwhere the formula_69-tangles on the right-hand-side are the squares of \"concurrence\".\n\nThe tangle measure is permutationally invariant; it vanishes on all states that are separable under any cut; it is nonzero, for example, on the GHZ-state; it can be thought to be zero for states that are 3-entangled (i.e. that are not product with respect to any cut) as, for instance, the W-state. Moreover, there might be the possibility to obtain a good generalization of the \"tangle\" for multiqubit systems by means of hyperdeterminant.\n\nThis was one of the first entanglement measures constructed specifically for multipartite states.\n\nDefinition [Schmidt measure]: The minimum of formula_70, where formula_71 is the number of terms in an expansion of the state in product basis.\n\nThis measure is zero if and only if the state is fully product; therefore, it cannot distinguish between truly multipartite entanglement and bipartite entanglement, but it may nevertheless be useful in many contexts.\n\nThis is an interesting class of multipartite entanglement measures obtained in the context of classification of states. Namely, one considers any homogeneous function of the state: if it is invariant under SLOCC (stochastic LOCC) operations with determinant equal to 1, then it is an \"entanglement monotone in the strong sense\", i.e. it satisfies the condition of strong monotonicity.\n\nIt was proved by Miyake that hyperdeterminants are entanglement monotones and they describe truly multipartite entanglement in the sense that states such as products of formula_72's have zero entanglement. In particular concurrence and tangle are special cases of hyperdeterminant. Indeed for two qubits concurrence is simply the modulus of the determinant, which is the hyperdeterminant of first order; whereas the tangle is the hyperdeterminant of second order, i.e. a function of tensors with three indices.\n\nDefinition [geometric entanglement]: \nwhere formula_74, with formula_75 the set of formula_76-separable states. This measure belongs to a family of entanglement measures defined by Barnum and Linden, and it is the multipartite generalization of the Shimony measure.\n\nThe entanglement can be quantified using a geometric measure of entanglement.\n\nThis entanglement measure is a generalization of the entanglement of assistance and was constructed in the context of spin chains. Namely, one chooses two spins and performs LOCC operations that aim at obtaining the largest possible bipartite entanglement between them (measured according to a chosen entanglement measure for two bipartite states).\n\n"}
{"id": "3513327", "url": "https://en.wikipedia.org/wiki?curid=3513327", "title": "NTRUSign", "text": "NTRUSign\n\nNTRUSign, also known as the NTRU Signature Algorithm, is a public-key cryptography digital signature algorithm based on the GGH signature scheme. The original version of NTRUSign was Polynomial Authentication and Signature Scheme (PASS), and was published at CrypTEC'99 . The improved version of PASS was named as NTRUSign, and was presented at the rump session of Asiacrypt 2001 and published in peer-reviewed form at the RSA Conference 2003 . The 2003 publication included parameter recommendations for 80-bit security. A subsequent 2005 publication revised the parameter recommendations for 80-bit security, presented parameters that gave claimed security levels of 112, 128, 160, 192 and 256 bits, and described an algorithm to derive parameter sets at any desired security level. NTRU Cryptosystems, Inc. have applied for a patent on the algorithm.\n\nNTRUSign involves mapping a message to a random point in 2\"N\"-dimensional space, where \"N\" is one of the NTRUSign parameters, and solving the closest vector problem in a lattice closely related to the NTRUEncrypt lattice. NTRUSign is claimed to be faster than those algorithms at low security levels, and considerably faster at high security levels. However, analysis had shown that original scheme is insecure and would leak knowledge of private key.\n\nA redesigned pqNTRUSign had been submitted to NIST's Post-Quantum Cryptography Standardization competition. It is based on \"hash-and-sign\" (contrasting Fiat–Shamir transformation) methodology, and claims to achieve smaller signature size. \n\nNTRUSign is under consideration for standardization by the IEEE P1363 working group.\n\nIt was demonstrated in 2000 by Wu, Bao, Ye and Deng that the signature of PASS, the original version of NTURSign, can be forged easily without knowing the private key . NTRUSign is not a zero-knowledge signature scheme and a transcript of signatures leaks information about the private key, as first observed by Gentry and Szydlo. Nguyen and Regev demonstrated in 2006 that for the original unperturbed NTRUSign parameter sets an attacker can recover the private key with as few as 400 signatures.\n\nThe current proposals use \"perturbations\" to increase the transcript length required to recover the private key: the signer displaces the point representing the message by a small secret amount before the signature itself is calculated. NTRU claimed that at least 2 signatures are needed, and probably considerably more, before a transcript of perturbed signatures enabled any useful attack. In 2012 an attack on the scheme with perturbations was presented that required a few thousand signatures for standard security parameters.\n\nThe pqNTRUSign claims a 128-bit classical and quantum security for their given parameter set. \n\n"}
{"id": "48753670", "url": "https://en.wikipedia.org/wiki?curid=48753670", "title": "Newton–Krylov method", "text": "Newton–Krylov method\n\nNewton–Krylov methods are numerical methods for solving non-linear problems using Krylov subspace linear solvers.\n"}
{"id": "43043289", "url": "https://en.wikipedia.org/wiki?curid=43043289", "title": "Polygon covering", "text": "Polygon covering\n\nA covering of a polygon is a set of primitive units (e.g. squares) whose union equals the polygon. A polygon covering problem is a problem of finding a covering with a smallest number of units for a given polygon. This is an important class of problems in computational geometry. There are many different polygon covering problems, depending on the type of polygon being covered and on the types of units allowed in the covering. An example polygon covering problem is: given a rectilinear polygon, find a smallest set of squares whose union equals the polygon.\n\nIn some scenarios, it is not required to cover the entire polygon but only its edges (this is called \"polygon edge covering\") or its vertices (this is called \"polygon vertex covering\").\n\nIn a covering problem, the units in the covering are allowed to overlap, as long as their union is exactly equal to the target polygon. This is in contrast to a packing problem, in which the units must be disjoint and their union may be smaller than the target polygon, and to a polygon partition problem, in which the units must be disjoint \"and\" their union must be equal to the target polygon.\n\nA polygon covering problem is a special case of the set cover problem. In general, the problem of finding a smallest set covering is NP-complete, but for special classes of polygons, a smallest polygon covering can be found in polynomial time.\n\nA unit \"u\" contained in a polygon \"P\" is called \"maximal\" if it is not contained in any other unit in \"P\". When looking for a polygon covering, it is sufficient to consider maximal units, since every unit which is not maximal can be replaced with a maximal unit containing it without affecting the size of the covering.\n\nA \"covering\" of a polygon \"P\" is a collection of maximal units, possibly overlapping, whose union equals \"P\".\n\nA \"minimal covering\" is a covering that does not contain any other covering (i.e. it is a local minimum).\n\nA \"minimum covering\" is a covering with a smallest number of units (i.e. a global minimum). Every minimum covering is minimal, but not vice versa.\n\nA rectilinear polygon can always be covered with a finite number of squares.\n\nFor hole-free polygons, a minimum covering by squares can be found in time O(\"n\"), where \"n\" is the number of vertices of the polygon. The algorithm uses a local optimization approach: it builds the covering by iteratively selecting maximal squares that are essential to the cover (- contain uncovered points not covered by other maximal squares) and then deleting from the polygon the points that become unnecessary (- unneeded to support future squares). Here is a simplified pseudo-code of the algorithm:\n\nFor polygons which may contain holes, finding a minimum such covering is NP-hard. This sharp difference between hole-free and general polygons can be intuitively explained based on the following analogy between maximal squares in a rectilinear polygon and nodes in an undirected graph:\n\n\nIn a hole-free rectilinear polygon, all maximal squares are either continuators or separators; thus, such a polygon is analogous to a tree graph. A general polygon is analogous to a general graph. Just like the Vertex cover problem is polynomial for tree graphs but NP-hard for general graphs, the square covering problem is linear for hole-free polygons but NP-hard for general polygons.\n\nIt is possible to use the linear algorithm to get a 2-approximation – i.e., a covering with at most 2⋅OPT squares, where OPT is the number of squares in a minimum covering:\nThe number of squares in the resulting covering is at most OPT+HOLES, where HOLES is the number of holes. It is possible to prove that OPT≥HOLES. Hence the number of squares in the covering is at most 2⋅OPT.\n\nFor general rectilinear polygons, the problem of finding a minimum rectangle covering is NP-hard, even when the target polygon is hole-free. Several partial solutions have been suggested to this problem:\n\n1. In orthogonally convex polygons, the number of rectangles in a minimum covering is equal to the number of blocks in an anti rectangle, and this fact can be used to build a polynomial time algorithm for finding a minimum covering by rectangles.\n\n2. Even when the target polygon is only half-orthogonally convex (i.e. only in the \"y\" direction), a minimum covering by rectangles can be found in time O(\"n\"), where \"n\" is the number of vertices of the polygon.\n\n3. An approximation algorithm which gives good empirical results on real-life data is presented by.\n\n4. For rectilinear polygons which may contain holes, there is an O() factor approximation algorithm.\n\nFor a rectilinear polygon which is half-orthogonally convex (i.e. only in the \"x\" direction), a minimum covering by orthogonally convex polygons can be found in time O(\"n\"^2), where \"n\" is the number of vertices of the polygon. The same is true for a covering by rectilinear star polygons.\n\nThe number of orthogonally-convex components in a minimum covering can, in some cases, be found without finding the covering itself, in time O(\"n\").\n\nA rectilinear star polygon is a polygon \"P\" containing a point \"p\", such that for every point \"q\" in \"P\", there is an orthogonally convex polygon containing \"p\" and \"q\".\n\nThe problem of covering a polygon with star polygons is a variant of the art gallery problem.\n\nThe visibility graph for the problem of minimally covering hole-free rectilinear polygons with star polygons is a perfect graph. This perfectness property implies a polynomial algorithm for finding a minimum covering of any rectilinear polygon with rectilinear star polygons.\n\nThe most general class of polygons for which coverings by squares or rectangles can be found is the class of polygons without acute interior angles. This is because an acute angle cannot be covered by a finite number of rectangles. This problem is NP-hard, but several approximation algorithms exist.\n\nIn some cases, a polygon has to be covered not with arbitrary rectangles but with rectangles from a finite family.\n\nFinding the smallest set of triangles covering a given polygon is NP-hard. It is also hard to approximate - every polynomial-time algorithm might find a covering with size (1+1/19151) times the minimal covering.\n\nIf the polygon is in general position (i.e. no two edges are collinear), then every triangle can cover at most 3 polygon edges. Hence every Polygon triangulation is a 3-approximation.\n\nIf the covering is restricted to triangles whose vertices are vertices of the polygon (i.e. Steiner points are not allowed), then the problem is NP-complete.\n\nIf Steiner points are not allowed \"and\" the polygon is in general position (i.e. no two edges are collinear), then every minimal covering without Steiner points is also a minimal partitioning of the polygon to triangles (i.e., the triangles in the minimal covering to not overlap). Hence, the minimum covering problem is identical to the Polygon triangulation problem, which can be solved in time O(\"n\"log\"n\"). Note that if we drop the general position assumption, there are polygons in which the triangles in the optimal covering overlap. Think of the Star of David for example.\n\nThe problem of covering only the boundary of a polygon with triangles is NP-complete, but there is an efficient 2-approximation.\n\nCovering a polygon (which may contain holes) with convex polygons is NP-hard. There is an O(log\"n\") approximation algorithm.\n\nCovering a polygon with convex polygons is NP-hard even when the target polygon is hole-free. It is also APX-hard. The problem is NP-complete when the covering must not introduce new vertices (i.e. Steiner points are not allowed).\n\nCovering a polygon (which may contain holes) with star polygons is NP-hard.\n\nCovering a general (non-rectilinear) polygon with star polygons is NP-hard even when the target polygon is hole-free.\n\nCovering a polygon (which may contain holes) with spirals is NP-hard.\n\nCovering a polygon with Pseudotriangles has also been studied.\n\nAdditional information can be found in.\n\n"}
{"id": "2134047", "url": "https://en.wikipedia.org/wiki?curid=2134047", "title": "Principles of Mathematical Logic", "text": "Principles of Mathematical Logic\n\nPrinciples of Mathematical Logic is the 1950 American translation of the 1938 second edition of David Hilbert's and Wilhelm Ackermann's classic text \"Grundzüge der theoretischen Logik\", on elementary mathematical logic. The 1928 first edition thereof is considered the first elementary text clearly grounded in the formalism now known as first-order logic (FOL). Hilbert and Ackermann also formalized FOL in a way that subsequently achieved canonical status. FOL is now a core formalism of mathematical logic, and is presupposed by contemporary treatments of Peano arithmetic and nearly all treatments of axiomatic set theory.\n\nThe 1928 edition included a clear statement of the Entscheidungsproblem (decision problem) for FOL, and also asked whether that logic was complete (i.e., whether all semantic truths of FOL were theorems derivable from the FOL axioms and rules). The former problem was answered in the negative first by Alonzo Church and independently by Alan Turing in 1936. The latter was answered affirmatively by Kurt Gödel in 1929.\n\nThe text also touched on set theory and relational algebra as ways of going beyond FOL. Contemporary notation for logic owes more to this text than it does to the notation of \"Principia Mathematica\", long popular in the English speaking world.\n\n"}
{"id": "16089583", "url": "https://en.wikipedia.org/wiki?curid=16089583", "title": "Privilege Management Infrastructure", "text": "Privilege Management Infrastructure\n\nPrivilege Management is the process of managing user authorisations based on the ITU-T Recommendation X.509. The 2001 edition of X.509 specifies most (but not all) of the components of a Privilege Management Infrastructure (PMI), based on X.509 attribute certificates (ACs). Later editions of X.509 (2005 and 2009) have added further components to the PMI, including a delegation service (in 2005 ) and interdomain authorisation (in the 2009 edition ).\n\nPrivilege management infrastructures (PMIs) are to authorisation what public key infrastructures (PKIs) are to authentication. PMIs use attribute certificates (ACs) to hold user privileges, in the form of attributes, instead of public key certificates (PKCs) to hold public keys. PMIs have Sources of Authority (SoAs) and Attribute Authorities (AAs) that issue ACs to users, instead of certification authorities (CAs) that issue PKCs to users. Usually PMIs rely on an underlying PKI, since ACs have to be digitally signed by the issuing AA, and the PKI is used to validate the AA's signature.\n\nAn X.509 AC is a generalisation of the well known X.509 public key certificate (PKC), in which the public key of the PKC has been replaced by any set of attributes of the certificate holder (or subject). Therefore, one could in theory use X.509 ACs to hold a user's public key as well as any other attribute of the user. (In a similar vein, X.509 PKCs can also be used to hold privilege attributes of the subject, by adding them to the subject directory attributes extension of an X.509 PKC). However, the life cycle of public keys and user privileges are usually very different, and therefore it isn't usually a good idea to combine both of them in the same certificate. Similarly, the authority that assigns a privilege to someone is usually different from the authority that certifies someone's public key. Therefore, it isn't usually a good idea to combine the functions of the SoA/AA and the CA in the same trusted authority. PMIs allow privileges and authorisations to be managed separately from keys and authentication.\n\nThe first open source implementation of an X.509 PMI was built with funding under the EC PERMIS project, and the software is available from here. A description of the implementation can be found in.\n\nX.509 ACs and PMIs are used today in Grids (see Grid computing), to assign privileges to users, and to carry the privileges around the Grid. In the most popular Grid privilege management system today, called VOMS, user privileges, in the shape of VO memberships and roles, are placed inside an X.509 AC by the VOMS server, signed by the VOMS server, and then embedded in the user's X.509 proxy certificate for carrying around the Grid.\n\nBecause of the rise in popularity of XML SOAP based services, SAML attribute assertions are now more popular than X.509 ACs for transporting user attributes. However, they both have similar functionality, which is to strongly bind a set of privilege attributes to a user.\n"}
{"id": "32538447", "url": "https://en.wikipedia.org/wiki?curid=32538447", "title": "Pyrrho's lemma", "text": "Pyrrho's lemma\n\nIn statistics, Pyrrho's lemma is the result that if one adds just one extra variable as a regressor from a suitable set to a linear regression model, one can get any desired outcome in terms of the coefficients (signs and sizes), as well as predictions, the R-squared, the t-statistics, prediction- and confidence-intervals. The argument for the coefficients was advanced by Herman Wold and Lars Juréen but named, extended to include the other statistics and explained more fully by Theo Dijkstra. Dijkstra named it after the sceptic philosopher Pyrrho and concludes his article by noting that this lemma provides \"some ground for a wide-spread scepticism concerning products of extensive datamining\". One can only prove that a model 'works' by testing it on data different from the data that gave it birth. \n\nThe result has been discussed in the context of econometrics.\n"}
{"id": "20894986", "url": "https://en.wikipedia.org/wiki?curid=20894986", "title": "Radon–Riesz property", "text": "Radon–Riesz property\n\nThe Radon–Riesz property is a mathematical property for normed spaces that helps ensure convergence in norm. Given two assumptions (essentially weak convergence and continuity of norm), we would like to ensure convergence in the norm topology.\n\nSuppose that (\"X\", ||·||) is a normed space. We say that \"X\" has the \"Radon–Riesz property\" (or that \"X\" is a \"Radon–Riesz space\") if whenever formula_1 is a sequence in the space and formula_2 is a member of \"X\" such that formula_1 converges weakly to formula_2 and formula_5, then formula_1 converges to formula_2 in norm; that is, formula_8.\n\nAlthough it would appear that Johann Radon was one of the first to make significant use of this property in 1913, M. I. Kadets and V. L. Klee also used versions of the Radon–Riesz property to make advancements in Banach space theory in the late 1920s. It is common for the Radon–Riesz property to also be referred to as the Kadets–Klee property or property (H). According to Robert Megginson, the letter H does not stand for anything. It was simply referred to as property (H) in a list of properties for normed spaces that starts with (A) and ends with (H). This list was given by K. Fan and I. Glicksberg (Observe that the definition of (H) given by Fan and Glicksberg includes additionally the rotundity of the norm, so it does not coincide with the Radon-Riesz property itself). The \"Riesz\" part of the name refers to Frigyes Riesz. He also made some use of this property in the 1920s.\n\nIt is important to know that the name \"Kadets-Klee property\" is used sometimes to speak about the coincidence of the weak topologies and norm topologies in the unit sphere of the normed space.\n\n1. Every real Hilbert space is a Radon–Riesz space. Indeed, suppose that \"H\" is a real Hilbert space and that formula_9 is a sequence in \"H\" converging weakly to a member formula_10 of \"H\". Using the two assumptions on the sequence and the fact that\nand letting \"n\" tend to infinity, we see that \nThus \"H\" is a Radon–Riesz space.\n\n2. Every uniformly convex Banach space is a Radon-Riesz space. See Section 3.7 of Haim Brezis' Functional analysis.\n\n"}
{"id": "24474524", "url": "https://en.wikipedia.org/wiki?curid=24474524", "title": "Scale-free ideal gas", "text": "Scale-free ideal gas\n\nThe scale-free ideal gas (SFIG) is a physical model assuming a collection of non-interacting elements with an stochastic proportional growth. It is the scale-invariant version of an ideal gas. Some cases of city-population, electoral results and cites to scientific journals can be approximately considered scale-free ideal gases.\n\nIn a one-dimensional discrete model with size-parameter \"k\", where \"k\" and \"k\" are the minimum and maximum allowed sizes respectively, and \"v\" = \"dk\"/\"dt\" is the growth, the bulk probability density function \"F\"(\"k\", \"v\") of a scale-free ideal gas follows\n\nwhere \"N\" is the total number of elements, Ω = ln \"k\"/\"k\" is the logaritmic \"volume\" of the system, formula_2 is the mean relative growth and formula_3 is the standard deviation of the relative growth. The entropy equation of state is\n\nwhere formula_5 is a constant that accounts for dimensionality and formula_6 is the elementary volume in phase space, with formula_7 the elementary time and \"M\" the total number of allowed discrete sizes. This expression has the same form as the one-dimensional ideal gas, changing the thermodynamical variables (\"N\", \"V\", \"T\") by (\"N\", Ω,\"σ\").\n\nZipf's law may emerge in the external limits of the density since it is a special regime of scale-free ideal gases.\n"}
{"id": "3542454", "url": "https://en.wikipedia.org/wiki?curid=3542454", "title": "Second-order arithmetic", "text": "Second-order arithmetic\n\nIn mathematical logic, second-order arithmetic is a collection of axiomatic systems that formalize the natural numbers and their subsets. It is an alternative to axiomatic set theory as a foundation for much, but not all, of mathematics. A precursor to second-order arithmetic, involving third-order parameters, was introduced by David Hilbert and Paul Bernays in their book Grundlagen der Mathematik. The standard axiomatization of second-order arithmetic is denoted Z.\n\nSecond-order arithmetic includes, but is significantly stronger than, its first-order counterpart Peano arithmetic. Unlike Peano arithmetic, second-order arithmetic allows quantification over sets of natural numbers as well as numbers themselves. Because real numbers can be represented as (infinite) sets of natural numbers in well-known ways, and because second order arithmetic allows quantification over such sets, it is possible to formalize the real numbers in second-order arithmetic. For this reason, second-order arithmetic is sometimes called “analysis” (Sieg 2013, p. 291).\n\nSecond-order arithmetic can also be seen as a weak version of set theory in which every element is either a natural number or a set of natural numbers. Although it is much weaker than Zermelo–Fraenkel set theory, second-order arithmetic can prove essentially all of the results of classical mathematics expressible in its language.\n\nA subsystem of second-order arithmetic is a theory in the language of second-order arithmetic each axiom of which is a theorem of full second-order arithmetic (Z). Such subsystems are essential to reverse mathematics, a research program investigating how much of classical mathematics can be derived in certain weak subsystems of varying strength. Much of core mathematics can be formalized in these weak subsystems, some of which are defined below. Reverse mathematics also clarifies the extent and manner in which classical mathematics is nonconstructive.\n\nThe language of second-order arithmetic is two-sorted. The first sort of terms and in particular variables, usually denoted by lower case letters, consists of individuals, whose intended interpretation is as natural numbers. The other sort of variables, variously called “set variables,” “class variables,” or even “predicates” are usually denoted by upper-case letters. They refer to classes/predicates/properties of individuals, and so can be thought of as sets of natural numbers. Both individuals and set variables can be quantified universally or existentially. A formula with no bound set variables (that is, no quantifiers over set variables) is called arithmetical. An arithmetical formula may have free set variables and bound individual variables.\n\nIndividual terms are formed from the constant 0, the unary function \"S\" (the \"successor function\"), and the binary operations + and formula_1 (addition and multiplication). The successor function adds 1 to its input. The relations = (equality) and < (comparison of natural numbers) relate two individuals, whereas the relation ∈ (membership) relates an individual and a set (or class). Thus in notation the language of second-order arithmetic is given by the signature formula_2.\n\nFor example, formula_3, is a well-formed formula of second-order arithmetic that is arithmetical, has one free set variable \"X\" and one bound individual variable \"n\" (but no bound set variables, as is required of an arithmetical formula)—whereas formula_4 is a well-formed formula that is not arithmetical, having one bound set variable \"X\" and one bound individual variable \"n\".\n\nSeveral different interpretations of the quantifiers are possible. If second-order arithmetic is studied using the full semantics of second-order logic then the set quantifiers range over all subsets of the range of the number variables. If second-order arithmetic is formalized using the semantics of first-order logic (Henkin semantics) then any model includes a domain for the set variables to range over, and this domain may be a proper subset of the full powerset of the domain of number variables (Shapiro 1991, pp. 74–75).\n\nThe following axioms are known as the \"basic axioms\", or sometimes the \"Robinson axioms.\" The resulting first-order theory, known as Robinson arithmetic, is essentially Peano arithmetic without induction. The domain of discourse for the quantified variables is the natural numbers, collectively denoted by N, and including the distinguished member formula_5, called \"zero.\"\n\nThe primitive functions are the unary successor function, denoted by prefix formula_6, and two binary operations, addition and multiplication, denoted by infix \"+\" and \"formula_7\", respectively. There is also a primitive binary relation called order, denoted by infix \"<\".\n\nAxioms governing the successor function and zero:\n\nAddition defined recursively:\n\nMultiplication defined recursively:\n\nAxioms governing the order relation \"<\":\n\nThese axioms are all first order statements. That is, all variables range over the natural numbers and not sets thereof, a fact even stronger than their being arithmetical. Moreover, there is but one existential quantifier, in axiom 3. Axioms 1 and 2, together with an axiom schema of induction make up the usual Peano-Dedekind definition of N. Adding to these axioms any sort of axiom schema of induction makes redundant the axioms 3, 10, and 11.\n\nIf φ(\"n\") is a formula of second-order arithmetic with a free number variable \"n\" and possible other free number or set variables (written \"m\" and \"X\"), the induction axiom for φ is the axiom:\nThe (full) second-order induction scheme consists of all instances of this axiom, over all second-order formulas.\n\nOne particularly important instance of the induction scheme is when φ is the formula “formula_20” expressing the fact that \"n\" is a member of \"X\" (\"X\" being a free set variable): in this case, the induction axiom for φ is\nThis sentence is called the second-order induction axiom.\n\nIf φ(\"n\") is a formula with a free variable \"n\" and possibly other free variables, but not the variable \"Z\", the comprehension axiom for φ is the formula\n\nThis axiom makes it possible to form the set formula_23 of natural numbers satisfying φ(\"n\"). There is a technical restriction that the formula φ may not contain the variable \"Z\", for otherwise the formula formula_24 would lead to the comprehension axiom\nwhich is inconsistent. This convention is assumed in the remainder of this article.\n\nThe formal theory of second-order arithmetic (in the language of second-order arithmetic) consists of the basic axioms, the comprehension axiom for every formula φ (arithmetic or otherwise), and the second-order induction axiom. This theory is sometimes called \"full second-order arithmetic\" to distinguish it from its subsystems, defined below. Because full second-order semantics imply that every possible set exists, the comprehension axioms may be taken to be part of the deductive system when these semantics are employed (Shapiro 1991, p. 66).\n\nThis section describes second-order arithmetic with first-order semantics. Thus a model formula_26 of the language of second-order arithmetic consists of a set \"M\" (which forms the range of individual variables) together with a constant 0 (an element of \"M\"), a function \"S\" from \"M\" to \"M\", two binary operations + and · on \"M\", a binary relation < on \"M\", and a collection \"D\" of subsets of \"M\", which is the range of the set variables. Omitting \"D\" produces a model of the language of first order arithmetic.\n\nWhen \"D\" is the full powerset of \"M\", the model formula_26 is called a full model. The use of full second-order semantics is equivalent to limiting the models of second-order arithmetic to the full models. In fact, the axioms of second-order arithmetic have only one full model. This follows from the fact that the axioms of Peano arithmetic with the second-order induction axiom have only one model under second-order semantics.\n\nWhen \"M\" is the usual set of natural numbers with its usual operations, formula_26 is called an ω-model. In this case, the model may be identified with \"D\", its collection of sets of naturals, because this set is enough to completely determine an ω-model.\n\nThe unique full formula_29-model, which is the usual set of natural numbers with its usual structure and all its subsets, is called the intended or standard model of second-order arithmetic.\n\nThe first-order functions that are provably total in second-order arithmetic are precisely the same as those representable in system F (Girard and Taylor 1987, pp. 122–123). Almost equivalently, system F is the theory of functionals corresponding to second-order arithmetic in a manner parallel to how Gödel's system T corresponds to first-order arithmetic in the Dialectica interpretation.\n\nThere are many named subsystems of second-order arithmetic.\n\nA subscript 0 in the name of a subsystem indicates that it includes only\na restricted portion of the full second-order induction scheme (Friedman 1976). Such a restriction lowers the proof-theoretic strength of the system significantly. For example, the system ACA described below is equiconsistent with Peano arithmetic. The corresponding theory ACA, consisting of ACA plus the full second-order induction scheme, is stronger than Peano arithmetic.\n\nMany of the well-studied subsystems are related to closure properties of models. For example, it can be shown that every ω-model of full second-order arithmetic is closed under Turing jump, but not every ω-model closed under Turing jump is a model of full second-order arithmetic. The subsystem ACA includes just enough axioms to capture the notion of closure under Turing jump.\n\nACA is defined as the theory consisting of the basic axioms, the arithmetical comprehension axiom scheme (in other words the comprehension axiom for every \"arithmetical\" formula φ) and the ordinary second-order induction axiom. It would be equivalent to include the entire arithmetical induction axiom scheme, in other words to include the induction axiom for every arithmetical formula φ.\n\nIt can be shown that a collection \"S\" of subsets of ω determines an ω-model of ACA if and only if \"S\" is closed under Turing jump, Turing reducibility, and Turing join (Simpson 2009, pp. 311–313).\n\nThe subscript 0 in ACA indicates that not every instance of the induction axiom scheme is included this subsystem. This makes no difference for ω-models, which automatically satisfy every instance of the induction axiom. It is of importance, however, in the study of non-ω-models. The system consisting of ACA plus induction for all formulas is sometimes called ACA with no subscript.\n\nThe system ACA is a conservative extension of first-order arithmetic (or first-order Peano axioms), defined as the basic axioms, plus the first order induction axiom scheme (for all formulas φ involving no class variables at all, bound or otherwise), in the language of first order arithmetic (which does not permit class variables at all). In particular it has the same proof-theoretic ordinal ε as first-order arithmetic, owing to the limited induction schema.\n\nA formula is called \"bounded arithmetical\", or Δ, when all its quantifiers are of the form ∀\"n\"<\"t\" or ∃\"n\"<\"t\" (where \"n\" is the individual variable being quantified and \"t\" is an individual term), where\nstands for\nand\nstands for\n\nA formula is called Σ (or sometimes Σ), respectively Π (or sometimes Π) when it of the form ∃\"m\"(φ), respectively ∀\"m\"(φ) where φ is a bounded arithmetical formula and \"m\" is an individual variable (that is free in φ). More generally, a formula is called Σ, respectively Π when it is obtained by adding existential, respectively universal, individual quantifiers to a Π, respectively Σ formula (and Σ and Π are all equivalent to Δ). By construction, all these formulas are arithmetical (no class variables are ever bound) and, in fact, by putting the formula in Skolem prenex form one can see that every arithmetical formula is equivalent to a Σ or Π formula for all large enough \"n\".\n\nThe subsystem RCA is a weaker system than ACA and is often used as the base system in reverse mathematics. It consists of: the basic axioms, the Σ induction scheme, and the Δ comprehension scheme. The former term is clear: the Σ induction scheme is the induction axiom for every Σ formula φ. The term “Δ comprehension” is more complex, because there is no such thing as a Δ formula. The Δ comprehension scheme instead asserts the comprehension axiom for every Σ formula which is equivalent to a Π formula. This scheme includes, for every Σ formula φ and every Π formula ψ, the axiom:\n\nThe set of first-order consequences of RCA is the same as those of the subsystem IΣ of Peano arithmetic in which induction is restricted to Σ formulas. In turn, IΣ is conservative over primitive recursive arithmetic (PRA) for formula_35 sentences. Moreover, the proof-theoretic ordinal of formula_36 is ω, the same as that of PRA.\n\nIt can be seen that a collection \"S\" of subsets of ω determines an ω-model of RCA if and only if \"S \"is closed under Turing reducibility and Turing join. In particular, the collection of all computable subsets of ω gives an ω-model of RCA. This is the motivation behind the name of this system—if a set can be proved to exist using RCA, then the set is recursive (i.e. computable).\n\nSometimes an even weaker system than RCA is desired. One such system is defined as follows: one must first augment the language of arithmetic with an exponential function (in stronger systems the exponential can be defined in terms of addition and multiplication by the usual trick, but when the system becomes too weak this is no longer possible) and the basic axioms by the obvious axioms defining exponentiation inductively from multiplication; then the system consists of the (enriched) basic axioms, plus Δ comprehension, plus Δ induction.\n\nOver ACA, each formula of second-order arithmetic is equivalent to a Σ or Π formula for all large enough \"n\". The system Π-comprehension is the system consisting of the basic axioms, plus the ordinary second-order induction axiom and the comprehension axiom for every Π formula φ. This is equivalent to Σ-comprehension (on the other hand, Δ-comprehension, defined analogously to Δ-comprehension, is weaker).\n\nProjective determinacy is the assertion that every two-player perfect information game with moves being integers, game length ω and projective payoff set is determined, that is one of the players has a winning strategy. (The first player wins the game if the play belongs to the payoff set; otherwise, the second player wins.) A set is projective iff (as a predicate) it is expressible by a formula in the language of second order arithmetic, allowing real numbers as parameters, so projective determinacy is expressible as a schema in the language of Z.\n\nMany natural propositions expressible in the language of second order arithmetic are independent of Z and even ZFC but are provable from projective determinacy. Examples include coanalytic perfect subset property, measurability and the property of Baire for formula_37 sets, formula_38 uniformization, etc. Over a weak base theory (such as RCA), projective determinacy implies comprehension and provides an essentially complete theory of second order arithmetic — natural statements in the language of Z that are independent of Z with projective determinacy are hard to find (Woodin 2001).\n\nZFC + {there are \"n\" Woodin cardinals: \"n\" is a natural number} is conservative over Z with projective determinacy, that is a statement in the language of second order arithmetic is provable in Z with projective determinacy iff its translation into the language of set theory is provable in ZFC + {there are \"n\" Woodin cardinals: \"n\"∈N}.\n\nSecond-order arithmetic directly formalizes natural numbers and sets of natural numbers. However, it is able to formalize other mathematical objects indirectly via coding techniques, a fact which was first noticed by Weyl (Simpson 2009, p. 16). The integers, rational numbers, and real numbers can all be formalized in the subsystem RCA, along with complete separable metric spaces and continuous functions between them (Simpson 2009, Chapter II).\n\nThe research program of Reverse Mathematics uses these formalizations of mathematics in second-order arithmetic to study the set-existence axioms required to prove mathematical theorems (Simpson 2009, p. 32). For example, the intermediate value theorem for functions from the reals to the reals is provable in RCA (Simpson 2009, p. 87), while the Bolzano/Weierstrass theorem is equivalent to ACA over RCA (Simpson 2009, p. 34).\n\n\n"}
{"id": "31306009", "url": "https://en.wikipedia.org/wiki?curid=31306009", "title": "Selberg's zeta function conjecture", "text": "Selberg's zeta function conjecture\n\nIn mathematics, the Selberg conjecture, named after Atle Selberg, is a theorem about the density of zeros of the Riemann zeta function ζ(1/2 + \"it\"). It is known that the function has infinitely many zeroes on this line in the complex plane: the point at issue is how densely they are clustered. Results on this can be formulated in terms of \"N\"(\"T\"), the function counting zeroes on the line for which the value of \"t\" satisfies 0 ≤ \"t\" ≤ \"T\".\n\nIn 1942 Atle Selberg investigated the problem of the Hardy–Littlewood conjecture 2; and he proved that for any\n\nthere exist\n\nand\n\nsuch that for\n\nand\n\nthe inequality\n\nholds true.\n\nIn his turn, Selberg stated a conjecture relating to shorter intervals, namely that it is possible to decrease the value of the exponent \"a\" = 0.5 in\n\nIn 1984 Anatolii Karatsuba proved that for a fixed formula_8 satisfying the condition\n\na sufficiently large \"T\" and\n\nthe interval in the ordinate \"t\" (\"T\", \"T\" + \"H\") contains at least \"cH\" ln \"T\" real zeros of the Riemann zeta function\n\nand thereby confirmed the Selberg conjecture. The estimates of Selberg and Karatsuba cannot be improved in respect of the order of growth as \"T\" → +∞.\n\nIn 1992 Karatsuba proved that an analog of the Selberg conjecture holds for \"almost all\" intervals (\"T\", \"T\" + \"H\"], \"H\" = \"T\", where ε is an arbitrarily small fixed positive number. The Karatsuba method permits one to investigate zeroes of the Riemann zeta-function on \"supershort\" intervals of the critical line, that is, on the intervals (\"T\", \"T\" + \"H\"], the length \"H\" of which grows slower than any, even arbitrarily small degree \"T\".\n\nIn particular, he proved that for any given numbers ε, ε satisfying the conditions 0 < ε, ε< 1 almost all intervals (\"T\", \"T\" + \"H\"] for \"H\" ≥ exp[(ln \"T\")] contain at least \"H\" (ln \"T\") zeros of the function ζ(1/2 + \"it\"). This estimate is quite close to the conditional result that follows from the Riemann hypothesis.\n"}
{"id": "9436252", "url": "https://en.wikipedia.org/wiki?curid=9436252", "title": "Serial binary adder", "text": "Serial binary adder\n\nThe serial binary adder or bit-serial adder is a digital circuit that performs binary addition bit by bit. The serial full adder has three single-bit inputs for the numbers to be added and the carry in. There are two single-bit outputs for the sum and carry out. The carry-in signal is the previously calculated carry-out signal. The addition is performed by adding each bit, lowest to highest, one per clock cycle.\n\nSerial binary addition is done by a flip-flop and a full adder. The flip-flop takes the carry-out signal on each clock cycle and provides its value as the carry-in signal on the next clock cycle. After all of the bits of the input operands have arrived, all of the bits of the sum have come out of the sum output.\n\nThe serial binary subtracter operates the same as the serial binary adder, except the subtracted number is converted to its two's complement before being added. Alternatively, the number to be subtracted is converted to its ones' complement, by inverting its bits, and the carry flip-flop is initialized to a 1 instead of to 0 as in addition. The ones' complement plus the 1 is the two's complement.\n\n\n\n\"*addition starts from lowest\"\n\n\n\n"}
{"id": "545288", "url": "https://en.wikipedia.org/wiki?curid=545288", "title": "Spectral leakage", "text": "Spectral leakage\n\nThe Fourier transform of a function of time, s(t), is a complex-valued function of frequency, S(f), often referred to as a frequency spectrum. Any linear time-invariant operation on s(t) produces a new spectrum of the form H(f)•S(f), which changes the relative magnitudes and/or angles (phase) of the non-zero values of S(f). Any other type of operation creates new frequency components that may be referred to as spectral leakage in the broadest sense. Sampling, for instance, produces leakage, which we call aliases of the original spectral component. For Fourier transform purposes, sampling is modeled as a product between s(t) and a Dirac comb function. The spectrum of a product is the convolution between S(f) and another function, which inevitably creates the new frequency components. But the term 'leakage' usually refers to the effect of \"windowing\", which is the product of s(t) with a different kind of function, the window function. Window functions happen to have finite duration, but that is not necessary to create leakage. Multiplication by a time-variant function is sufficient.\n\nLeakage caused by a window function is most easily characterized by its effect on a sinusoidal s(t) function, whose unwindowed Fourier transform is zero for all but one frequency. The customary frequency of choice is 0 Hz, because the windowed Fourier transform is simply the Fourier transform of the window function itself:\n\nWhen both sampling and windowing are applied to s(t), in either order, the leakage caused by windowing is a relatively localized spreading of frequency components, with often a blurring effect, whereas the aliasing caused by sampling is a periodic repetition of the entire blurred spectrum.\n\nThe total leakage of a window function is measured by a metric called \"equivalent noise bandwidth\" (ENBW) or \"noise equivalent bandwidth\" (NEB). The best window in that regard is the simplest, called \"rectangular\" because of its flat top and vertical sides. Its spreading effect occurs mostly a factor of 10 to 100 below the amplitude of the original component. Unfortunately the spreading is very wide, which may mask important spectrum details at even lower levels. That prevents the rectangular window from being a popular choice. Non-rectangular window functions actually increase the total leakage, but they can also redistribute it to places where it does the least harm, depending on the application. Specifically, to different degrees they reduce the level of the spreading by increasing the high-level leakage in the near vicinity of the original component. In general, they control the trade-off between resolving comparable strength signals with similar frequencies or resolving disparate strength signals with dissimilar frequencies: one speaks of \"high resolution\" versus \"high dynamic range\" windows. And leakage near the original component is actually beneficial for a metric known as \"scalloping loss\".\n\nWe customarily think of leakage as a spreading out of (say) a sinusoid in one \"bin\" of a DFT into the other bins at levels that generally decrease with distance. What that actually means is that when the actual sinusoid frequency lies in bin \"k\", its presence is sensed/recorded at different levels in the other bins; i.e. the correlations they measure are non-zero. The value measured in bin k+10 and plotted on the spectrum graph is the response of that measurement to the imperfect (i.e. windowed) sinusoid 10 bins away. And when the input is just white noise (energy at all frequencies), the value measured in bin k is the sum of its responses to a continuum of frequencies. One could say that leakage is actually a \"leaking in\" process, rather than leaking out. That perspective might help to interpret the different noise-floor levels between the two graphs in the figure on the right. Both spectra were made from the same data set with the same noise power. But the bins in the bottom graph each responded more strongly than the bins in the top graph. The exact amount of the difference is given by the ENBW difference of the two window functions.\n\n"}
{"id": "4058119", "url": "https://en.wikipedia.org/wiki?curid=4058119", "title": "Two Generals' Problem", "text": "Two Generals' Problem\n\nIn computing, the Two Generals Problem is a thought experiment meant to illustrate the pitfalls and design challenges of attempting to coordinate an action by communicating over an unreliable link. It is related to the more general Byzantine Generals Problem and appears often in introductory classes about computer networking (particularly with regard to the Transmission Control Protocol, where it shows that TCP can't guarantee state consistency between endpoints and why), though it applies to any type of two-party communication where failures of communication are possible. A key concept in epistemic logic, this problem highlights the importance of common knowledge. Some authors also refer to this as the Two Generals Paradox, the Two Armies Problem, or the Coordinated Attack Problem. The Two Generals Problem was the first computer communication problem to be proved to be unsolvable. An important consequence of this proof is that generalizations like the Byzantine Generals problem are also unsolvable in the face of arbitrary communication failures, thus providing a base of realistic expectations for any distributed consistency protocols.\n\nTwo armies, each led by a different general, are preparing to attack a fortified city. The armies are encamped near the city, each in its own valley. A third valley separates the two hills, and the only way for the two generals to communicate is by sending messengers through the valley. Unfortunately, the valley is occupied by the city's defenders and there's a chance that any given messenger sent through the valley will be captured.\n\nWhile the two generals have agreed that they will attack, they haven't agreed upon a time for attack. It is required that the two generals have their armies attack the city at the same time in order to succeed, else the lone attacker army will die trying. They must thus communicate with each other to decide on a time to attack and to agree to attack at that time, and each general must know that the other general knows that they have agreed to the attack plan. Because acknowledgement of message receipt can be lost as easily as the original message, a potentially infinite series of messages is required to come to consensus.\n\nThe thought experiment involves considering how they might go about coming to consensus. In its simplest form one general is known to be the leader, decides on the time of attack, and must communicate this time to the other general. The problem is to come up with algorithms that the generals can use, including sending messages and processing received messages, that can allow them to correctly conclude:\n\nAllowing that it is quite simple for the generals to come to an agreement on the time to attack (i.e. one successful message with a successful acknowledgement), the subtlety of the Two Generals' Problem is in the impossibility of designing algorithms for the generals to use to safely agree to the above statement.\n\nThe first general may start by sending a message \"Attack at 0900 on August 4.\" However, once dispatched, the first general has no idea whether or not the messenger got through. This uncertainty may lead the first general to hesitate to attack due to the risk of being the sole attacker.\n\nTo be sure, the second general may send a confirmation back to the first: \"I received your message and will attack at 0900 on August 4.\" However, the messenger carrying the confirmation could face capture and the second general may hesitate, knowing that the first might hold back without the confirmation.\n\nFurther confirmations may seem like a solution—let the second general send a second confirmation: \"I received your confirmation of the planned attack at 0900 on August 4.\" However, this new messenger from the second general is liable to be captured, too. Thus it quickly becomes evident that no matter how many rounds of confirmation are made, there is no way to guarantee the second requirement that each general be sure the other has agreed to the attack plan. Both generals will always be left wondering whether their last messenger got through.\n\nBecause this protocol is deterministic, suppose there is a sequence of a fixed number of messages, one or more successfully delivered and one or more not. The assumption is that there should be a \"shared certainty for both generals to attack\".\n\nConsider the last such message that was successfully delivered. If that last message had not been successfully delivered, then one general at least (presumably the receiver) would decide not to attack. From the viewpoint of the sender of that last message, however, the sequence of messages sent and delivered is exactly the same as it would have been, had that message been delivered.\n\nSince the protocol is deterministic, the general sending that last message will still decide to attack. \nWe've now created a situation where the suggested protocol leads one general to attack and the other not to attack—contradicting the assumption that the protocol was a solution to the problem.\n\nA nondeterministic protocol with a variable message count can be compared to a finite tree, where each leaf or branch (node) in the tree represents an explored example up to a specified point.\n\nThe roots of this tree are labeled with the possible starting messages, and the branch nodes stemming from these roots are labeled with the possible next messages. Leaf nodes represent examples which end after sending the last message. A protocol that terminates before sending any messages is represented by a null tree.\n\nSuppose there exists a nondeterministic protocol which solves the problem. Then, by a similar argument to the deterministic example in the previous section, where a deterministic protocol can be obtained from the non-deterministic one by removing all leaf nodes, the deterministic protocol must then also solve the problem.\n\nSince the nondeterministic protocol is finite, it then follows that the protocol represented by the empty tree would solve the problem. Clearly this is not possible. Therefore a nondeterministic protocol which solves the problem cannot exist.\n\nA pragmatic approach to dealing with the Two Generals' Problem is to use schemes that accept the uncertainty of the communications channel and not attempt to eliminate it, but rather mitigate it to an acceptable degree. For example, the first general could send 100 messengers, anticipating that the probability of all being captured is low. With this approach the first general will attack no matter what, and the second general will attack if any message is received. Alternatively the first general could send a stream of messages and the second general could send acknowledgments to each, with each general feeling more comfortable with every message received. As seen in the proof, however, neither can be certain that the attack will be coordinated. There's no algorithm that they can use (e.g. attack if more than four messages are received) which will be certain to prevent one from attacking without the other. Also, the first general can send a marking on each message saying it is message 1, 2, 3 ... of n. This method will allow the second general to know how reliable the channel is and send an appropriate number of messages back to ensure a high probability of at least one message being received. If the channel can be made to be reliable, then one message will suffice and additional messages do not help. The last is as likely to get lost as the first.\n\nAssuming that the generals must sacrifice lives every time a messenger is sent and intercepted, an algorithm can be designed to minimize the number of messengers required to achieve the maximum amount of confidence the attack is coordinated. To save them from sacrificing hundreds of lives to achieve a very high confidence in coordination, the generals could agree to use the absence of messengers as an indication that the general who began the transaction has received at least one confirmation, and has promised to attack. Suppose it takes a messenger 1 minute to cross the danger zone, allowing 200 minutes of silence to occur after confirmations have been received will allow us to achieve extremely high confidence while not sacrificing messenger lives. In this case messengers are used only in the case where a party has not received the attack time. At the end of 200 minutes, each general can reason: \"I have not received an additional message for 200 minutes; either 200 messengers failed to cross the danger zone, or it means the other general has confirmed and committed to the attack and has confidence I will too\".\n\nThe Two Generals Problem and its impossibility proof was first published by E. A. Akkoyunlu, K. Ekanadham, and R. V. Huber in 1975 in \"Some Constraints and Trade-offs in the Design of Network Communications\", where it is described starting on page 73 in the context of communication between two groups of gangsters.\n\nThis problem was given the name the \"Two Generals Paradox\" by Jim Gray in 1978 in \"Notes on Data Base Operating Systems\" starting on page 465. This reference is widely given as a source for the definition of the problem and the impossibility proof, though both were published previously as above.\n"}
{"id": "29759120", "url": "https://en.wikipedia.org/wiki?curid=29759120", "title": "Uniqueness theorem", "text": "Uniqueness theorem\n\nIn mathematics, a uniqueness theorem is a theorem proving that certain conditions determine a unique solution. Examples of uniqueness theorems include:\n\nA theorem, also called a unicity theorem, stating the uniqueness of a mathematical object, which usually means that there is only one object fulfilling given properties, or that all objects of a given class are equivalent (i.e., they can be represented by the same model). This is often expressed by saying that the object is uniquely determined by a certain set of data. The word unique is sometimes replaced by essentially unique, whenever one wants to stress that the uniqueness is only referred to the underlying structure, whereas the form may vary in all ways that do not affect the mathematical content.\n\nA uniqueness theorem / proof is, at least within mathematics of differential equations, often combined with an existence theorem / proof to a combined existence and uniqueness theorem.\n\n"}
{"id": "3978418", "url": "https://en.wikipedia.org/wiki?curid=3978418", "title": "Victor Shoup", "text": "Victor Shoup\n\nVictor Shoup is a computer scientist and mathematician. He obtained a PhD in computer science from the University of Wisconsin–Madison in 1989 , and he did his undergraduate work at the University of Wisconsin-Eau Claire. He is a professor at the Courant Institute of Mathematical Sciences at New York University, focusing on algorithm and cryptography courses. He has held positions at AT&T Bell Labs, the University of Toronto, Saarland University, and the IBM Zurich Research Laboratory.\n\nShoup's main research interests and contributions are computer algorithms relating to number theory, algebra, and cryptography. His contributions to these fields include:\n\n\n"}
{"id": "21796270", "url": "https://en.wikipedia.org/wiki?curid=21796270", "title": "Vienna Series in Theoretical Biology", "text": "Vienna Series in Theoretical Biology\n\nThe Vienna Series in Theoretical Biology is a book series published by MIT Press and devoted to advances in theoretical biology at large. By promoting the formulation and discussion of new theoretical concepts, the series intends to help fill the gaps in our understanding of some of the major open questions of biology, such as the origin and organization of organismal form, the relationship between development and evolution, and the biological bases of cognition and mind.\n\nThe Vienna Series grew out of the Altenberg Workshops in Theoretical Biology organized by the Konrad Lorenz Institute for Evolution and Cognition Research (KLI), an international center for advanced study in Altenberg, near Vienna, Austria. The KLI fosters research projects, workshops, archives, book projects, and the journal Biological Theory, all devoted to aspects of theoretical biology, with an emphasis on integrating the developmental, evolutionary, and cognitive sciences.\n\nGerd B. Müller, Günter Wagner, Werner Callebaut\n\n\n"}
{"id": "244097", "url": "https://en.wikipedia.org/wiki?curid=244097", "title": "Window function", "text": "Window function\n\nIn signal processing and statistics, a window function (also known as an apodization function or tapering function) is a mathematical function that is zero-valued outside of some chosen interval, normally symmetric around the middle of the interval, usually near a maximum in the middle, and usually tapering away from the middle. Mathematically, when another function or waveform/data-sequence is \"multiplied\" by a window function, the product is also zero-valued outside the interval: all that is left is the part where they overlap, the \"view through the window\". Equivalently, and in actual practice, the segment of data within the window is first isolated, and then only that data is multiplied by the window function values. Thus, tapering, not segmentation, is the main purpose of window functions.\n\nThe reasons for examining segments of a longer function include detection of transient events and time-averaging of frequency spectra. The duration of the segments is determined in each application by requirements like time and frequency resolution. But that method also changes the frequency content of the signal by an effect called spectral leakage. Window functions allow us to distribute the leakage spectrally in different ways, according to the needs of the particular application. There are many choices detailed in this article, but many of the differences are so subtle as to be insignificant in practice.\n\nIn typical applications, the window functions used are non-negative, smooth, \"bell-shaped\" curves. Rectangle, triangle, and other functions can also be used. A rectangular window does not modify the data segment at all. It's only for modelling purposes that we say it multiplies by 1 inside the window and by 0 outside. A more general definition of window functions does not require them to be identically zero outside an interval, as long as the product of the window multiplied by its argument is square integrable, and, more specifically, that the function goes sufficiently rapidly toward zero.\n\nWindow functions are used in spectral analysis/modification/resynthesis, the design of finite impulse response filters, as well as beamforming and antenna design.\n\nThe Fourier transform of the function cos ω\"t\" is zero, except at frequency ±ω. However, many other functions and waveforms do not have convenient closed-form transforms. Alternatively, one might be interested in their spectral content only during a certain time period.\n\nIn either case, the Fourier transform (or a similar transform) can be applied on one or more finite intervals of the waveform. In general, the transform is applied to the product of the waveform and a window function. Any window (including rectangular) affects the spectral estimate computed by this method.\n\nWindowing of a simple waveform like cos ω\"t\" causes its Fourier transform to develop non-zero values (commonly called spectral leakage) at frequencies other than ω. The leakage tends to be worst (highest) near ω and least at frequencies farthest from ω.\n\nIf the waveform under analysis comprises two sinusoids of different frequencies, leakage can interfere with the ability to distinguish them spectrally. If their frequencies are dissimilar and one component is weaker, then leakage from the stronger component can obscure the weaker one's presence. But if the frequencies are similar, leakage can render them \"unresolvable\" even when the sinusoids are of equal strength. The rectangular window has excellent resolution characteristics for sinusoids of comparable strength, but it is a poor choice for sinusoids of disparate amplitudes. This characteristic is sometimes described as \"low dynamic range\".\n\nAt the other extreme of dynamic range are the windows with the poorest resolution and sensitivity, which is the ability to reveal relatively weak sinusoids in the presence of additive random noise. That is because the noise produces a stronger response with high-dynamic-range windows than with high-resolution windows. Therefore, high-dynamic-range windows are most often justified in \"wideband applications\", where the spectrum being analyzed is expected to contain many different components of various amplitudes.\n\nIn between the extremes are moderate windows, such as Hamming and Hann. They are commonly used in \"narrowband applications\", such as the spectrum of a telephone channel. In summary, spectral analysis involves a trade-off between resolving comparable strength components with similar frequencies and resolving disparate strength components with dissimilar frequencies. That trade-off occurs when the window function is chosen.\n\nWhen the input waveform is time-sampled, instead of continuous, the analysis is usually done by applying a window function and then a discrete Fourier transform (DFT). But the DFT provides only a sparse sampling of the actual discrete-time Fourier transform (DTFT) spectrum. Figure 2, row 3 shows a DTFT for a rectangularly-windowed sinusoid. The actual frequency of the sinusoid is indicated as \"13\" on the horizontal axis. Everything else is leakage, exaggerated by the use of a logarithmic presentation. The unit of frequency is \"DFT bins\"; that is, the integer values on the frequency axis correspond to the frequencies sampled by the DFT. So the figure depicts a case where the actual frequency of the sinusoid coincides with a DFT sample, and the maximum value of the spectrum is accurately measured by that sample. In row 4, it misses the maximum value by ½ bin, and the resultant measurement error is referred to as scalloping loss (inspired by the shape of the peak). For a known frequency, such as a musical note or a sinusoidal test signal, matching the frequency to a DFT bin can be prearranged by choices of a sampling rate and a window length that results in an integer number of cycles within the window.\n\nThe concepts of resolution and dynamic range tend to be somewhat subjective, depending on what the user is actually trying to do. But they also tend to be highly correlated with the total leakage, which is quantifiable. It is usually expressed as an equivalent bandwidth, B. It can be thought of as redistributing the DTFT into a rectangular shape with height equal to the spectral maximum and width B. The separable forms of all other window functions have corners that depend on the choice of the coordinate axes. The isotropy/anisotropy of a two-dimensional window function is shared by its two-dimensional Fourier transform. The difference between the separable and radial forms is akin to the result of diffraction from rectangular vs. circular appertures, which can be visualized in terms of the product of two sinc functions vs. an Airy function, respectively.\n\n\n\n"}
{"id": "2291002", "url": "https://en.wikipedia.org/wiki?curid=2291002", "title": "Yiannis N. Moschovakis", "text": "Yiannis N. Moschovakis\n\nYiannis Nicholas Moschovakis (; born January 18, 1938) is a set theorist, descriptive set theorist, and recursion (computability) theorist, at UCLA.\n\nHis book \"Descriptive Set Theory\" (North-Holland) is the primary reference for the subject. He is especially associated with the development of the effective, or lightface, version of descriptive set theory, and he is known for the Moschovakis coding lemma that is named after him.\n\nMoschovakis earned his Ph.D. from University of Wisconsin–Madison in 1963 under the direction of Stephen Kleene, with a dissertation entitled \"Recursive Analysis\". In 2015 he was elected as a fellow of the American Mathematical Society \"for contributions to mathematical logic, especially set theory and computability theory, and for exposition\".\n\nFor many years he has split his time between UCLA and University of Athens (he retired from the latter in July 2005).\n\nMoschovakis is married to Joan Moschovakis, with whom he gave the 2014 Lindström Lectures at the University of Gothenburg.\n\n\n"}
{"id": "1256751", "url": "https://en.wikipedia.org/wiki?curid=1256751", "title": "Zeno machine", "text": "Zeno machine\n\nIn mathematics and computer science, Zeno machines (abbreviated ZM, and also called accelerated Turing machine, ATM) are a hypothetical computational model related to Turing machines that allows a countably infinite number of algorithmic steps to be performed in finite time. These machines are ruled out in most models of computation.\n\nMore formally, a Zeno machine is a Turing machine that takes 2 units of time to perform its \"n\"-th step; thus, the first step takes 0.5 units of time, the second takes 0.25, the third 0.125 and so on, so that after one unit of time, a countably infinite (i.e. ℵ) number of steps will have been performed.\n\nThe idea of Zeno machines was first discussed by Hermann Weyl in 1927; the name refers to Zeno's paradoxes, attributed to the ancient Greek philosopher Zeno of Elea. Zeno machines play a crucial role in some theories. The theory of the Omega Point devised by physicist Frank J. Tipler, for instance, can only be valid if Zeno machines are possible.\n\nZeno machines would allow some functions to be computed that are not Turing-computable. For example, the halting problem for Turing machines can be solved by a Zeno machine (using the following pseudocode algorithm):\n\nComputing of this kind that goes beyond the Turing Limit is called hypercomputation, in this case hypercomputation through a supertask - see there for further discussion and literature.\n\n"}
