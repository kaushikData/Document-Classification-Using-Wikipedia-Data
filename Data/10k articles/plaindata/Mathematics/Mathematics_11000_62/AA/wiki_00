{"id": "21156145", "url": "https://en.wikipedia.org/wiki?curid=21156145", "title": "41 (number)", "text": "41 (number)\n\n41 (forty-one) is the natural number following 40 and preceding 42.\n\n\n\n\n\n\n\n"}
{"id": "2304796", "url": "https://en.wikipedia.org/wiki?curid=2304796", "title": "Alethic modality", "text": "Alethic modality\n\nAlethic modality (from Greek ἀλήθεια = truth) is a linguistic modality that indicates modalities of truth, in particular the modalities of logical necessity, possibility or impossibility.\n\nAlethic modality is often associated with epistemic modality in research, and it has been questioned whether this modality should be considered distinct from epistemic modality which denotes the speaker's evaluation or judgment of the truth. The criticism states that there is no real difference between \"the truth in the world\" (alethic) and \"the truth in an individual's mind\" (epistemic). An investigation has not found a single language in which alethic and epistemic modalities would be formally distinguished, for example by the means of a grammatical mood. In such a language, \"A circle can't be square\", \"can't be\" would be expressed by an alethic mood, whereas for \"He can't be that wealthy\", \"can't be\" would be expressed by an epistemic mood. As we can see, this is not a distinction drawn in English grammar.\n\n\"You can't give these plants too much water.\" is a well-known play on the distinction between perhaps alethic and hortatory or injunctive modalities. The dilemma is fairly easily resolved when listening through paralinguistic cues and particularly suprasegmental cues (intonation). So while there may not be a morphologically based alethic mood, this does not seem to preclude the usefulness of distinguishing between these two types of modes. Alethic modality might then concern what are considered to be apodictic statements.\n"}
{"id": "14091966", "url": "https://en.wikipedia.org/wiki?curid=14091966", "title": "Archibald Smith", "text": "Archibald Smith\n\nArchibald Smith of Jordanhill (10 August 1813, in Greenhead, North Lanarkshire – 26 December 1872, in London) was a Scots-born barrister and amateur mathematician.\n\nHe was the only son of James Smith FRSE (1782-1867), a wealthy merchant and antiquary and owner of the Jordanhill estate in Glasgow, and his wife Mary Wilson, granddaughter of Alexander Wilson, professor of astronomy in Glasgow University (and brother of Patrick Wilson). He was educated at the Redland School near Bristol from 1826 to 1828.\n\nArchibald studied Law at Glasgow University from 1828, and then at Trinity College, Cambridge, where he was Senior Wrangler, said to be the first Scot to achieve this position, and first Smith's prizeman in 1836, elected a fellow of Trinity College. He was one of the founders of the \"Cambridge Mathematical Journal\". He graduated BA in 1836 and MA in 1839.\n\nHe entered Lincoln's Inn, and was called to the bar as a barrister in 1841. He then practised as an equity draughtsman and property lawyer in London.\n\nHis scientific work was mainly in the field of applications of magnetism and the Earth's magnetic field. He obtained practical formulae for the correction of magnetic compass observations made on board ship, which General Sir Edward Sabine published in the \"Transactions\" of the Royal Society: Smith later made convenient tables. In 1859 he edited William Scoresby's \"Journal of a Voyage to Australia for Magnetical Research\" and gave an exact formula for the effect of the iron of a ship on the compass. In 1862, in conjunction with the hydrographer Sir Frederick John Owen Evans FRS (1815-1885), then superintendent of the compass department of the navy, he published an \"Admiralty Manual for ascertaining and applying the Deviations of the Compass caused by the Iron in a Ship\".\n\nHe was elected a Fellow of the Royal Society of Edinburgh in 1837 his proposer being James David Forbes. Elected a Fellow of the Royal Society in June 1856, he was awarded its Royal Medal in 1865 \"for his papers in the Philosophical Transactions and elsewhere, on the magnetism of ships\". In 1866 Emperor Alexander II of Russia presented him with a gold compass, set in diamonds, and emblazoned with the Imperial Arms.\n\nHe died in London on 26 December 1872.\n\nIn 1853, Smith married Susan Emma Parker, daughter of Sir James Parker of Rothley Temple, Leicestershire, and Mary Babington. They had six sons and two daughters:\n\n\n"}
{"id": "13100627", "url": "https://en.wikipedia.org/wiki?curid=13100627", "title": "Bruce Harding", "text": "Bruce Harding\n\nBruce Harding is a gemstone cutter and mechanical design engineer and mathematician. He received the Lapidary Award of the Eastern Federation of the Mineralogical and Lapidary Society in 1975. That same year, in an article on \"Faceting Limits\" in \"Gems and Gemology\", the magazine of the Gemological Institute of America, he identified the effect of an observer's head blocking rays of illumination for the main facets of a number of gem materials, including diamonds. In 1986 Harding developed one of the earliest software programs to perform ray path analysis. Harding was a speaker at the International Diamond Cut Conference (IDCC) in Moscow in 2004.\n\nA translator of \"Angewandte Getribelehre\" to \"Applied Kinematics\", McGraw-Hill 1967.\n\nTranslator of Russian to \"Optimizing Faceting for Beauty\", British 'Journal of Gemmology', Jan.2004.\n\nU.S. valve patents 4,326,754 & 4,632,140A.\n\n\n"}
{"id": "8050713", "url": "https://en.wikipedia.org/wiki?curid=8050713", "title": "CRESTA", "text": "CRESTA\n\nCRESTA (Catastrophe Risk Evaluation and Standardizing Target Accumulations) was founded as a joint project of Swiss Reinsurance Company, Gerling-Konzern Globale Reinsurance Company, and Munich Reinsurance Company. CRESTA has set itself the aim of establishing a globally uniform system for the accumulation risk control of natural hazards - particularly earthquakes, storms and floods. Those risk zones are essentially based on the observed and expected seismic activity, as well as on other natural disasters, such as droughts, floods and storms. CRESTA zones regard the distribution of insured values within a region or country for easier assessment of risks. CRESTA Zones are the essential basis for reinsurance negotiation and portfolio analysis. Nowadays, CRESTA sets widely accepted standards which apply throughout the international insurance industry. CRESTA zone information is used by most insurers for assessing the insurance catastrophe premiums they will charge.\n\nWhile the acronym CRESTA stands for Catastrophe Risk Evaluation and Standardizing Target Accumulations, the name was derived from the name of the hotel (Cresta Hotel) where the founding meeting occurred, and the subsequent creation of a suitable acronym to correspond to the name.\n\nCRESTA has set itself the aim of establishing a globally uniform system for the accumulation risk control of natural hazards - particularly earthquakes, storms and floods. Nowadays, widely accepted standards apply throughout the international insurance industry. CRESTA's main tasks are:\n\n\nIn addition, CRESTA also undertakes the following activities:\n\nAs of 18.8.2008 http://www.cresta.org/)\n\nThere is basically no risk that cannot be insured against, however it is difficult to assess how risky it is to give out insurance contracts to customers and to assess for insurance providers how high their rates should be. One main indicator for every insurer is the premiums that are paid for their reinsurance. When applying for reinsurance the forms will often be based on the models provided by CRESTA. CRESTA (Catastrophe Risk Evaluating and Standardising Target Accumulations) is an independent group led by Munich Re and Swiss Re which promotes the accurate and efficient mapping and evaluation of catastrophe perils. CRESTA furthermore and most importantly established a worldwide zone classification used within the insurance industry, the CRESTA Zones. The CRESTA Zones are defined for much of the world down to postcode level. CRESTA Zones can form the basis of reinsurance negotiation and portfolio analysis.\n\nIn order for an insurer to calculate the risk distribution in their portfolio for each natural disaster type, accumulations of insured property in individual CRESTA Zones will be taken into account.\n\nIn seismic risk analysis, for example, a successful loss estimation of insured and reinsured values depends on the seismic hazard analysis, on the vulnerability of facilities and on the ability to calculate the earthquake risk premium, also known as average annual loss (AAL). CRESTA is simply one of the biggest, if not the most comprehensive and most actual information resource for natural risks and hazards.\n\nOn the downside, CRESTA specific information is not available freely. CRESTA zone information can either be bought directly from CRESTA or bundled with map systems.\n\nInformation about CRESTA:\n\nCRESTA Organisation, http://www.cresta.org\n\nWhat are CRESTA Zones? https://www.europa.uk.com/what-are-cresta-zones/\n\nNew 2013 CRESTA zones improve natural hazard risk management, https://www.europa.uk.com/article-2013-cresta-zones/\n\nGfK GeoMarketing, http://www.gfk-geomarketing.com/cresta_zones\n\nEuropa Technologies, https://www.europa.uk.com/global-map-data/global-cresta-plus/\n"}
{"id": "341442", "url": "https://en.wikipedia.org/wiki?curid=341442", "title": "Cantor's theorem", "text": "Cantor's theorem\n\nIn elementary set theory, Cantor's theorem is a fundamental result that states that, for any set formula_1, the set of all subsets of formula_1 (the power set of formula_1, denoted by formula_4) has a strictly greater cardinality than formula_1 itself. For finite sets, Cantor's theorem can be seen to be true by simple enumeration of the number of subsets. Counting the empty subset, a set with formula_6 members has formula_7 subsets, so that if formula_8 then formula_9, and the theorem holds because formula_10 is true for all non-negative integers.\n\nMuch more significant is Cantor's discovery of an argument that is applicable to any set, which showed that the theorem holds for infinite sets, countable or uncountable, as well as finite ones. As a particularly important consequence, the power set of the set of natural numbers, a countably infinite set with cardinality ℵ = card(ℕ), is uncountably infinite and has the same size as the set of real numbers, a cardinality often referred to as the cardinality of the continuum: 𝔠 = card(ℝ) = card(𝒫(ℕ)). The relationship between these cardinal numbers is often expressed symbolically by the equality formula_11.\n\nThe theorem is named for German mathematician Georg Cantor, who first stated and proved it at the end of the 19th century. Cantor's theorem had immediate and important consequences for the philosophy of mathematics. For instance, by iteratively taking the power set of an infinite set and applying Cantor's theorem, we obtain an endless hierarchy of infinite cardinals, each strictly larger than the one before it. Consequently, the theorem implies that there is no largest cardinal number (colloquially, \"there's no largest infinity\").\n\nCantor's argument is elegant and remarkably simple. The complete proof is presented below, with detailed explanations to follow.\n\nBy definition of cardinality, we have card(\"X\") < card(\"Y\") for any two sets \"X\" and \"Y\" if and only if there is an injective function but no bijective function from \"X\" to \"Y\". It suffices to show that there is no surjection from \"X\" to \"Y\". This is the heart of Cantor's theorem: there is no surjective function from any set \"A\" to its power set. To establish this, it is enough to show that no function \"f\" that maps elements in \"A\" to subsets of \"A\" can reach every possible subset, i.e., we just need to demonstrate the existence of a subset of \"A\" that is not equal to \"f\"(\"x\") for any \"x\" ∈ \"A\". (Recall that each \"f\"(\"x\") is a subset of \"A\".) Such a subset is given by the following construction, sometimes called the \"Cantor diagonal set\" of \"f\":\n\nThis means, by definition, that for all \"x\" in \"A\", \"x\" ∈ \"B\" if and only if \"x\" ∉ \"f\"(\"x\"). For all \"x\" the sets \"B\" and \"f\"(\"x\") cannot be the same because \"B\" was constructed from elements of \"A\" whose images (under \"f\") did not include themselves. More specifically, consider any \"x\" ∈ \"A\", then either \"x\" ∈ \"f\"(\"x\") or \"x\" ∉ \"f\"(\"x\"). In the former case, \"f\"(\"x\") cannot equal \"B\" because \"x\" ∈ \"f\"(\"x\") by assumption and \"x\" ∉ \"B\" by the construction of \"B\". In the latter case, \"f\"(\"x\") cannot equal \"B\" because \"x\" ∉ \"f\"(\"x\") by assumption and \"x\" ∈ \"B\" by the construction of \"B\".\n\nEquivalently, and slightly more formally, we just proved that the existence of ξ ∈ \"A\" such that \"f\"(ξ) = \"B\" implies the following contradiction:\n\nTherefore, by reductio ad absurdum, the assumption must be false. Thus there is no ξ ∈ \"A\" such that \"f\"(ξ) = \"B\"; in other words, \"B\" is not in the image of \"f\" and \"f\" does not map to every element of the power set of \"A\", i.e., \"f\" is not surjective (onto).\n\nFinally, to complete the proof, we need to exhibit an injective function from \"A\" to its power set. Finding such a function is trivial: just map \"x\" to the singleton set {\"x\"}. The argument is now complete, and we have established the strict inequality for any set \"A\" that card(\"A\") < card(𝒫(\"A\")).\n\nAnother way to think of the proof is that \"B\", empty or non-empty, is always in the power set of \"A\". For \"f\" to be onto, some element of \"A\" must map to \"B\". But that leads to a contradiction: no element of \"B\" can map to \"B\" because that would contradict the criterion of membership in \"B\", thus the element mapping to \"B\" must not be an element of \"B\" meaning that it satisfies the criterion for membership in \"B\", another contradiction. So the assumption that an element of \"A\" maps to \"B\" must be false; and \"f\" cannot be onto.\n\nBecause of the double occurrence of \"x\" in the expression \"\"x\" ∉ \"f\"(\"x\")\", this is a diagonal argument. For a countable (or finite) set, the argument of the proof given above can be illustrated by constructing a table in which each row is labelled by a unique \"x\" from \"A\" = {\"x\", \"x\", …}, in this order. \"A\" is assumed to admit a linear order so that such table can be constructed. Each column of the table is labelled by a unique \"y\" from the power set of \"A\"; the columns are ordered by the argument to \"f\", i.e. the column labels are \"f\"(\"x\"), \"f\"(\"x\"), …, in this order. The intersection of each row \"x\" and column \"y\" records a true/false bit whether \"x\" ∈ \"y\". Given the order chosen for the row and column labels, the main diagonal \"D\" of this table thus records whether \"x\" ∈ \"f\"(\"x\") for all \"x\" in \"A\". The set \"B\" constructed in the previous paragraphs coincides with the row labels for the subset of entries on this main diagonal \"D\" where the table records that \"x\" ∈ \"f\"(\"x\") is false. Each column records the values of the indicator function of the set corresponding to the column. The indicator function of \"B\" coincides with the logically negated (true ↔ false) entries of the main diagonal. Thus the indicator function of \"B\" does not agree with any column in at least one entry. Consequently, no column represents \"B\".\n\nFor a finite set, the proof can also be illustrated using a more prosaic presentation known as the barber paradox.\n\nDespite the simplicity of the above proof, it is rather difficult for an automated theorem prover to produce it. The main difficulty lies in an automated discovery of the Cantor diagonal set. Lawrence Paulson noted in 1992 that Otter could not do it, whereas Isabelle could, albeit with a certain amount of direction in terms of tactics that might perhaps be considered cheating.\n\nTo understand the proof, let's examine it for the specific case when \"X\" is countably infinite. Without loss of generality, we may take \"X\" = ℕ = {1, 2, 3...}, the set of natural numbers.\n\nSuppose that ℕ is equinumerous with its power set 𝒫(ℕ). Let us see a sample of what 𝒫(ℕ) looks like:\n\n𝒫(ℕ) contains infinite subsets of ℕ, e.g. the set of all even numbers {2, 4, 6...}, as well as the empty set.\n\nNow that we have an idea of what the elements of 𝒫(ℕ) look like, let us attempt to pair off each element of ℕ with each element of 𝒫(ℕ) to show that these infinite sets are equinumerous. In other words, we will attempt to pair off each element of ℕ with an element from the infinite set 𝒫(ℕ), so that no element from either infinite set remains unpaired. Such an attempt to pair elements would look like this:\n\nGiven such a pairing, some natural numbers are paired with subsets that contain the very same number. For instance, in our example the number 2 is paired with the subset {1, 2, 3}, which contains 2 as a member. Let us call such numbers \"selfish\". Other natural numbers are paired with subsets that do not contain them. For instance, in our example the number 1 is paired with the subset {4, 5}, which does not contain the number 1. Call these numbers \"non-selfish\". Likewise, 3 and 4 are non-selfish.\n\nUsing this idea, let us build a special set of natural numbers. This set will provide the contradiction we seek. Let \"D\" be the set of \"all\" non-selfish natural numbers. By definition, the power set 𝒫(ℕ) contains all sets of natural numbers, and so it contains this set \"D\" as an element. If the mapping is bijective, \"D\" must be paired off with some natural number, say \"d\". However, this causes a problem. If \"d\" is in \"D\", then \"d\" is selfish because it is in the corresponding set, contradicting the definition of \"D\". If \"d\" is not in \"D\", then it is non-selfish and should instead be a member of \"D\". Therefore, no such element \"d\" which maps to \"D\" can exist.\n\nSince there is no natural number which can be paired with \"D\", we have contradicted our original supposition, that there is a bijection between ℕ and 𝒫(ℕ).\n\nNote that the set \"D\" may be empty. This would mean that every natural number \"x\" maps to a set of natural numbers that contains \"x\". Then, every number maps to a nonempty set and no number maps to the empty set. But the empty set is a member of 𝒫(ℕ), so the mapping still does not cover 𝒫(ℕ).\n\nThrough this proof by contradiction we have proven that the cardinality of ℕ and 𝒫(ℕ) cannot be equal. We also know that the cardinality of 𝒫(ℕ) cannot be less than the cardinality of ℕ because 𝒫(ℕ) contains all singletons, by definition, and these singletons form a \"copy\" of ℕ inside of 𝒫(ℕ). Therefore, only one possibility remains, and that is that the cardinality of 𝒫(ℕ) is strictly greater than the cardinality of ℕ, proving Cantor's theorem.\n\nCantor's theorem and its proof are closely related to two paradoxes of set theory.\n\nCantor's paradox is the name given to a contradiction following from Cantor's theorem together with the assumption that there is a set containing all sets, the universal set V. In order to distinguish this paradox from the next one discussed below, it is important to note what this contradiction is. By Cantor's theorem |𝒫(\"X\")| > |\"X\"| for any set \"X\". On the other hand, all elements of 𝒫(V) are sets, and thus contained in V, therefore |𝒫(V)| ≤ |V|.\n\nAnother paradox can be derived from the proof of Cantor's theorem by instantiating the function \"f\" with the identity function; this turns Cantor's diagonal set into what is sometimes called the \"Russell set\" of a given set \"A\":\n\nThe proof of Cantor's theorem is straightforwardly adapted to show that assuming a set of all sets \"U\" exists, then considering its Russell set \"R\" leads to the contradiction:\n\nThis argument is known as Russell's paradox. As a point of subtlety, the version of Russell's paradox we have presented here is actually a theorem of Zermelo; we can conclude from the contradiction obtained that we must reject the hypothesis that \"R\"∈\"U\", thus disproving the existence of a set containing all sets. This was possible because we have used restricted comprehension (as featured in ZFC) in the definition of \"R\" above, which in turn entailed that\n\nHad we used unrestricted comprehension (as in Frege's system for instance) by defining the Russell set simply as formula_20, then the axiom system itself would have entailed the contradiction, with no further hypotheses needed.\n\nDespite the syntactical similarities between the Russell set (in either variant) and the Cantor diagonal set, Alonzo Church emphasized that Russell's paradox is independent of considerations of cardinality and its underlying notions like one-to-one correspondence.\n\nCantor gave essentially this proof in a paper published in 1891 \"Über eine elementare Frage der Mannigfaltigkeitslehre\", where the diagonal argument for the uncountability of the reals also first appears (he had earlier proved the uncountability of the reals by other methods). The version of this argument he gave in that paper was phrased in terms of indicator functions on a set rather than subsets of a set. He showed that if \"f\" is a function defined on \"X\" whose values are 2-valued functions on \"X\", then the 2-valued function \"G\"(\"x\") = 1 − \"f\"(\"x\")(\"x\") is not in the range of \"f\".\n\nBertrand Russell has a very similar proof in \"Principles of Mathematics\" (1903, section 348), where he shows that there are more propositional functions than objects. \"For suppose a correlation of all objects and some propositional functions to have been affected, and let phi-\"x\" be the correlate of \"x\". Then \"not-phi-\"x\"(\"x\"),\" i.e. \"phi-\"x\" does not hold of \"x\"\" is a propositional function not contained in this correlation; for it is true or false of \"x\" according as phi-\"x\" is false or true of \"x\", and therefore it differs from phi-\"x\" for every value of \"x\".\" He attributes the idea behind the proof to Cantor.\n\nErnst Zermelo has a theorem (which he calls \"Cantor's Theorem\") that is identical to the form above in the paper that became the foundation of modern set theory (\"Untersuchungen über die Grundlagen der Mengenlehre I\"), published in 1908. See Zermelo set theory.\n\nCantor's theorem has been generalized to any category with products.\n\nPatrick Grim has applied Cantor's theorem to argue that the set of all truths does not exist.\n\n\n\n"}
{"id": "55872661", "url": "https://en.wikipedia.org/wiki?curid=55872661", "title": "Category of representations", "text": "Category of representations\n\nIn representation theory, the category of representations of some algebraic structure has the representations of as objects and equivariant maps as morphisms between them. One of the basic thrusts of representation theory is to understand the conditions under which this category is semisimple; i.e., whether an object decomposes into simple objects (see Maschke's theorem for the case of finite groups).\n\nThe Tannakian formalism gives conditions under which a group \"G\" may be recovered from the category of representations of it together with the forgetful functor to the category of vector spaces.\n\nThe Grothendieck ring of the category of finite-dimensional representations of a group \"G\" is called the representation ring of \"G\".\n\nDepending on the types of the representations one wants to consider, it is typical to use slightly different definitions.\n\nFor a finite group and a field , the category of representations of over has \n\nThe category is denoted by formula_1 or formula_2.\n\nFor a Lie group, one typically requires the representations to be smooth or admissible. For the case of a Lie algebra, see Lie algebra representation. See also: category O.\n\nThere is an isomorphism of categories between the category of representations of a group over a field (described above) and the category of modules over the group ring [], denoted []-Mod.\n\nEvery group can be viewed as a category with a single object, where morphisms in this category are the elements of and composition is given by the group operation. Given an arbitrary category ', a \"representation\" of in ' is a functor from to '. Such a functor selects an object of ' and a subgroup of the automorphism group of that object. For example, a -set is equivalent to a functor from to Set, the category of sets, and a linear representation is equivalent to a functor to Vect, the category of vector spaces over a field.\n\nIn this setting, the category of linear representations of over is the functor category → Vect, which has natural transformations as its morphisms.\n\nThe category of linear representations of a group has a monoidal structure given by the tensor product of representations, which is an important ingredient in Tannaka-Krein duality (see below).\n\nMaschke's theorem states that when the characteristic of doesn't divide the order of , the category of representations of over is semisimple.\n\nGiven a group with a subgroup , there are two fundamental functors between the categories of representations of and (over a fixed field): one is a forgetful functor called the restriction functor\nand the other, the induction functor\n\nWhen and are finite groups, they are adjoint to each other\na theorem called Frobenius reciprocity.\n\nThe basic question is whether the decomposition into irreducible representations (simple objects of the category) behaves under restriction or induction. The question may be attacked for instance by the Mackey theory.\n\nTannaka–Krein duality concerns the interaction of a compact topological group and its category of linear representations. Tannaka's theorem describes the converse passage from the category of finite dimensional representations of a group back to the group , allowing one to recover the group from its category of representations. Krein's theorem in effect completely characterizes all categories that can arise from a group in this fashion. These concepts can be applied to representations of several different structures, see the main article for details.\n\n"}
{"id": "863791", "url": "https://en.wikipedia.org/wiki?curid=863791", "title": "Cauchy product", "text": "Cauchy product\n\nIn mathematics, more specifically in mathematical analysis, the Cauchy product is the discrete convolution of two infinite series. It is named after the French mathematician Augustin Louis Cauchy.\n\nThe Cauchy product may apply to infinite series or power series. When people apply it to finite sequences or finite series, it is by abuse of language: they actually refer to discrete convolution.\n\nConvergence issues are discussed in the next section.\n\nLet formula_1 and formula_2 be two infinite series with complex terms. The Cauchy product of these two infinite series is defined by a discrete convolution as follows:\n\nConsider the following two power series\n\nwith complex coefficients formula_7 and formula_8. The Cauchy product of these two power series is defined by a discrete convolution as follows:\n\nLet and be real or complex sequences. It was proved by Franz Mertens that, if the series formula_11 converges to and formula_12 converges to , and at least one of them converges absolutely, then their Cauchy product converges to .\n\nIt is not sufficient for both series to be convergent; if both sequences are conditionally convergent, the Cauchy product does not have to converge towards the product of the two series, as the following example shows:\n\nConsider the two alternating series with\n\nwhich are only conditionally convergent (the divergence of the series of the absolute values follows from the direct comparison test and the divergence of the harmonic series). The terms of their Cauchy product are given by\n\nfor every integer . Since for every we have the inequalities and , it follows for the square root in the denominator that , hence, because there are summands,\n\nfor every integer . Therefore, does not converge to zero as , hence the series of the diverges by the term test.\n\nAssume without loss of generality that the series formula_11 converges absolutely.\nDefine the partial sums\n\nwith\n\nThen\n\nby rearrangement, hence\n\nFix . Since formula_20 by absolute convergence, and since converges to as , there exists an integer such that, for all integers ,\n\n(this is the only place where the absolute convergence is used). Since the series of the converges, the individual must converge to 0 by the term test. Hence there exists an integer such that, for all integers ,\n\nAlso, since converges to as , there exists an integer such that, for all integers ,\n\nThen, for all integers , use the representation () for , split the sum in two parts, use the triangle inequality for the absolute value, and finally use the three estimates (), () and () to show that\n\nBy the definition of convergence of a series, as required.\n\nIn cases where the two sequences are convergent but not absolutely convergent, the Cauchy product is still Cesàro summable. Specifically:\n\nIf formula_22, formula_23 are real sequences with formula_24 and formula_25 then\n\nThis can be generalised to the case where the two sequences are not convergent but just Cesàro summable:\n\nFor formula_27 and formula_28, suppose the sequence formula_22 is formula_30 summable with sum \"A\" and formula_23 is formula_32 summable with sum \"B\". Then their Cauchy product is formula_33 summable with sum \"AB\".\n\n\nby definition and the binomial formula. Since, formally, formula_38 and formula_39, we have shown that formula_40. Since the limit of the Cauchy product of two absolutely convergent series is equal to the product of the limits of those series, we have proven the formula formula_41 for all formula_34.\n\n\nAll of the foregoing applies to sequences in formula_48 (complex numbers). The Cauchy product can be defined for series in the formula_49 spaces (Euclidean spaces) where multiplication is the inner product. In this case, we have the result that if two series converge absolutely then their Cauchy product converges absolutely to the inner product of the limits.\n\nLet formula_50 such that formula_51 (actually the following is also true for formula_52 but the statement becomes trivial in that case) and let formula_53 be infinite series with complex coefficients, from which all except the formula_54th one converge absolutely, and the formula_54th one converges. Then the series\nconverges and we have:\n\nThis statement can be proven by induction over formula_54: The case for formula_59 is identical to the claim about the Cauchy product. This is our induction base.\n\nThe induction step goes as follows: Let the claim be true for an formula_50 such that formula_51, and let formula_62 be infinite series with complex coefficients, from which all except the formula_63th one converge absolutely, and the formula_63th one converges. We first apply the induction hypothesis to the series formula_65. We obtain that the series\nconverges, and hence, by the triangle inequality and the sandwich criterion, the series\nconverges, and hence the series\nconverges absolutely. Therefore, by the induction hypothesis, by what Mertens proved, and by renaming of variables, we have:\nTherefore, the formula also holds for formula_63.\n\nA finite sequence can be viewed as an infinite sequence with only finitely many nonzero terms. A finite sequence can be viewed as a function formula_71 with finite support. For any complex-valued functions \"f\", \"g\" on formula_72 with finite support, one can take their convolution:\nThen formula_74 is the same thing as the Cauchy product of formula_75 and formula_76.\n\nMore generally, given a unital semigroup \"S\", one can form the semigroup algebra formula_77 of \"S\", with, as usual, the multiplication given by convolution. If one takes, for example, formula_78, then the multiplication on formula_77 is a sort of a generalization of the Cauchy product to higher dimension.\n"}
{"id": "6810363", "url": "https://en.wikipedia.org/wiki?curid=6810363", "title": "Classical Wiener space", "text": "Classical Wiener space\n\nIn mathematics, classical Wiener space is the collection of all continuous functions on a given domain (usually a sub-interval of the real line), taking values in a metric space (usually \"n\"-dimensional Euclidean space). Classical Wiener space is useful in the study of stochastic processes whose sample paths are continuous functions. It is named after the American mathematician Norbert Wiener.\n\nConsider \"E\" ⊆ R and a metric space (\"M\", \"d\"). The classical Wiener space \"C\"(\"E\"; \"M\") is the space of all continuous functions \"f\" : \"E\" → \"M\". I.e. for every fixed \"t\" in \"E\",\n\nIn almost all applications, one takes \"E\" = [0, \"T\"] or [0, +∞) and \"M\" = R for some \"n\" in N. For brevity, write \"C\" for \"C\"([0, \"T\"]; R); this is a vector space. Write \"C\" for the linear subspace consisting only of those functions that take the value zero at the infimum of the set \"E\". Many authors refer to \"C\" as \"classical Wiener space\".\n\nThe vector space \"C\" can be equipped with the uniform norm\n\nturning it into a normed vector space (in fact a Banach space). This norm induces a metric on \"C\" in the usual way: formula_4. The topology generated by the open sets in this metric is the topology of uniform convergence on [0, \"T\"], or the uniform topology.\n\nThinking of the domain [0, \"T\"] as \"time\" and the range R as \"space\", an intuitive view of the uniform topology is that two functions are \"close\" if we can \"wiggle space a bit\" and get the graph of \"f\" to lie on top of the graph of \"g\", while leaving time fixed. Contrast this with the Skorokhod topology, which allows us to \"wiggle\" both space and time.\n\nWith respect to the uniform metric, \"C\" is both a separable and a complete space:\n\nSince it is both separable and complete, \"C\" is a Polish space.\n\nRecall that the modulus of continuity for a function \"f\" : [0, \"T\"] → R is defined by\n\nThis definition makes sense even if \"f\" is not continuous, and it can be shown that \"f\" is continuous if and only if its modulus of continuity tends to zero as δ → 0:\n\nBy an application of the Arzelà-Ascoli theorem, one can show that a sequence formula_7 of probability measures on classical Wiener space \"C\" is tight if and only if both the following conditions are met:\n\nThere is a \"standard\" measure on \"C\", known as classical Wiener measure (or simply Wiener measure). Wiener measure has (at least) two equivalent characterizations:\n\nIf one defines Brownian motion to be a Markov stochastic process \"B\" : [0, \"T\"] × Ω → R, starting at the origin, with almost surely continuous paths and independent increments\n\nthen classical Wiener measure γ is the law of the process \"B\".\n\nAlternatively, one may use the abstract Wiener space construction, in which classical Wiener measure γ is the radonification of the canonical Gaussian cylinder set measure on the Cameron-Martin Hilbert space corresponding to \"C\".\n\nClassical Wiener measure is a Gaussian measure: in particular, it is a strictly positive probability measure.\n\nGiven classical Wiener measure γ on \"C\", the product measure γ × γ is a probability measure on \"C\", where γ denotes the standard Gaussian measure on R.\n\n"}
{"id": "7193", "url": "https://en.wikipedia.org/wiki?curid=7193", "title": "Commutator", "text": "Commutator\n\nIn mathematics, the commutator gives an indication of the extent to which a certain binary operation fails to be commutative. There are different definitions used in group theory and ring theory.\n\nThe commutator of two elements, and , of a group , is the element\n\nIt is equal to the group's identity if and only if and commute (i.e., if and only if ). The subgroup of \"G\" generated by all commutators is called the \"derived group\" or the \"commutator subgroup\" of \"G\". Note that one must consider the subgroup generated by the set of commutators because in general the set of commutators is not closed under the group operation. Commutators are used to define nilpotent and solvable groups.\n\nThe above definition of the commutator is used by some group theorists, as well as throughout this article. However, many other group theorists define the commutator as\n\nCommutator identities are an important tool in group theory. The expression denotes the conjugate of by , defined as .\n\n\nThe third relation is called anticommutativity, while the fourth is the Jacobi identity.\n\n\nAn additional identity may be found for this last expression, in the form:\n\n[[Hadamard's lemma]], applied on nested commutators holds, and underlies the [[Baker–Campbell–Hausdorff formula#An important lemma|Baker–Campbell–Hausdorff expansion]] of log(exp(\"A\") exp(\"B\")):\n\nThis formula is valid in any ring or algebra in which the [[exponential function]] can be meaningfully defined, for example, in a [[Banach algebra]] or in a ring of [[formal power series]].\n\nUse of the same expansion expresses the above Lie group commutator in terms of a series of nested Lie bracket (algebra) commutators,\n\nThese identities can be written more generally using the subscript convention to include the anticommutator defined above.\nFor example, \n\nWhen dealing with graded algebras, the commutator is usually replaced by the graded commutator, defined in homogeneous components as \n\nEspecially if one deals with multiple commutators, another notation turns out to be useful, the adjoint representation:\n\nThen is a linear derivation: \n\nand, crucially, it is a Lie algebra homomorphism:\n\nBy contrast, it is not always an algebra homomorphism; it does not hold in general:\n\n\nThe general Leibniz rule, expanding repeated derivatives of a product, can be written abstractly using the adjoint representation:\n\nReplacing \"x\" by the differentiation operator formula_34, and \"y\" by the multiplication operator formula_35, we get formula_36, and applying both sides to a function \"g\", the identity becomes the general Leibniz rule for formula_37.\n\n\n"}
{"id": "3755377", "url": "https://en.wikipedia.org/wiki?curid=3755377", "title": "Confluence (abstract rewriting)", "text": "Confluence (abstract rewriting)\n\nIn computer science, confluence is a property of rewriting systems, describing which terms in such a system can be rewritten in more than one way, to yield the same result. This article describes the properties in the most abstract setting of an abstract rewriting system.\n\nThe usual rules of elementary arithmetic form an abstract rewriting system.\nFor example, the expression (11 + 9) × (2 + 4) can be evaluated starting either at the left or at the right parentheses;\nhowever, in both cases the same result is obtained eventually.\nThis suggests that the arithmetic rewriting system is a confluent one.\n\nA second, more abstract example is obtained from the following proof of each group element equalling the inverse of its inverse:\n\nThis proof starts from the given group axioms A1-A3, and establishes five propositions R4, R6, R10, R11, and R12, each of them using some earlier ones, and R12 being the main theorem. Some of the proofs require non-obvious, if not creative, steps, like applying axiom A2 in reverse, thereby rewriting \"1\" to \"\"a\" ⋅ a\" in the first step of R6's proof. One of the historical motivations to develop the \"theory of term rewriting\" was to avoid the need for such steps, which are difficult to find by an unexperienced human, let alone by a computer program. \n\nIf a term rewriting system is confluent and \"terminating\", a straightforward method exists to prove equality between two expressions (a.k.a. \"terms\") \"s\" and \"t\":\nStarting with \"s\", apply equalities from left to right as long as possible, eventually obtaining a term \"s’\".\nObtain from \"t\" a term \"t’\" in a similar way.\nIf both terms \"s’\" and \"t’\" literally agree, then \"s\" and \"t\" are (not surprisingly) proven equal.\nMore important, if they disagree, \"s\" and \"t\" cannot be equal.\nThat is, any two terms \"s\" and \"t\" that can be proven equal at all, can be so by that method.\n\nThe success of that method doesn't depend on a certain sophisticated order in which to apply rewrite rules, as confluence ensures that any sequence of rule applications will eventually lead to the same result (while the \"termination\" property ensures that any sequence will eventually reach an end at all). Therefore, if a confluent and terminating term rewriting system can be provided for some equational theory, \nnot a tinge of creativity is required to perform proofs of term equality; that task hence becomes amenable to computer programs. Modern approaches handle more general \"abstract rewriting systems\" rather than \"term\" rewriting systems; the latter are a special case of the former.\n\nA rewriting system can be expressed as a directed graph in which nodes represent expressions and edges represent rewrites. So, for example, if the expression \"a\" can be rewritten into \"b\", then we say that \"b\" is a \"reduct\" of \"a\" (alternatively, \"a\" \"reduces to\" \"b\", or \"a\" is an \"expansion\" of \"b\"). This is represented using arrow notation; \"a\" → \"b\" indicates that \"a\" reduces to \"b\". Intuitively, this means that the corresponding graph has a directed edge from \"a\" to \"b\".\n\nIf there is a path between two graph nodes \"c\" and \"d\", then it forms a \"reduction sequence\". So, for instance, if \"c\" → \"c\"’ → \"c\"’’ → ... → \"d\"’ → \"d\", then we can write \"c\" \"d\", indicating the existence of a reduction sequence from \"c\" to \"d\". Formally, is the reflexive-transitive closure of →. Using the example from the previous paragraph, we have (11+9)×(2+4) → 20×(2+4) and 20×(2+4) → 20×6, so (11+9)×(2+4) 20×6.\n\nWith this established, confluence can be defined as follows. \"a\" ∈ \"S\" is deemed confluent if for all pairs \"b\", \"c\" ∈ \"S\" such that \"a\" \"b\" and \"a\" \"c\", there exists a \"d\" ∈ \"S\" with \"b\" \"d\" and \"c\" \"d\". If every \"a\" ∈ \"S\" is confluent, we say that → is confluent, or has the \"Church-Rosser property\". This property is also sometimes called the \"diamond property\", after the shape of the diagram shown on the right. Some authors reserve the term \"diamond property\" for a variant of the diagram with single reductions everywhere; that is, whenever \"a\" → \"b\" and \"a\" → \"c\", there must exist a \"d\" such that \"b\" → \"d\" and \"c\" → \"d\". The single-reduction variant is strictly stronger than the multi-reduction one. \n\nAn element \"a\" ∈ \"S\" is said to be locally (or weakly) confluent if for all \"b\", \"c\" ∈ \"S\" with \"a\" → \"b\" and \"a\" → \"c\" there exists \"d\" ∈ \"S\" with \"b\" \"d\" and \"c\" \"d\". If every \"a\" ∈ \"S\" is locally confluent, then → is called locally (or weakly) confluent, or having the \"weak Church-Rosser property\". This is different from confluence in that \"b\" and \"c\" must be reduced from \"a\" in one step. In analogy with this, confluence is sometimes referred to as \"global confluence\". \n\nThe relation , introduced as a notation for reduction sequences, may be viewed as a rewriting system in its own right, whose relation is the reflexive-transitive closure of \"→\". Since a sequence of reduction sequences is again a reduction sequence (or, equivalently, since forming the reflexive-transitive closure is idempotent), = . It follows that → is confluent if and only if is locally confluent.\n\nA rewriting system may be locally confluent without being (globally) confluent. Examples are shown in picture 3 and 4. However, Newman's lemma states that if a locally confluent rewriting system has no infinite reduction sequences (in which case it is said to be \"terminating\" or \"strongly normalizing\"), then it is globally confluent.\n\nThe definition of local confluence differs from that of global confluence in that only elements reached from a given element in a single rewriting step are considered. By considering one element reached in a single step and another element reached by an arbitrary sequence, we arrive at the intermediate concept of semi-confluence: \"a\" ∈ \"S\" is said to be semi-confluent if for all \"b\", \"c\" ∈ \"S\" with \"a\" → \"b\" and \"a\" \"c\" there exists \"d\" ∈ \"S\" with \"b\" \"d\" and \"c\" \"d\"; if every \"a\" ∈ \"S\" is semi-confluent, we say that → is semi-confluent. \n\nA semi-confluent element need not be confluent, but a semi-confluent rewriting system is necessarily confluent, and a confluent system is trivially semi-confluent.\n\nStrong confluence is another variation on local confluence that allows us to conclude that a rewriting system is globally confluent. An element \"a\" ∈ \"S\" is said to be strongly confluent if for all \"b\", \"c\" ∈ \"S\" with \"a\" → \"b\" and \"a\" → \"c\" there exists \"d\" ∈ \"S\" with \"b\" \"d\" and either \"c\" → \"d\" or \"c\" = \"d\"; if every \"a\" ∈ \"S\" is strongly confluent, we say that → is strongly confluent. \n\nA confluent element need not be strongly confluent, but a strongly confluent rewriting system is necessarily confluent.\n\n\n"}
{"id": "22721042", "url": "https://en.wikipedia.org/wiki?curid=22721042", "title": "Convex volume approximation", "text": "Convex volume approximation\n\nIn the analysis of algorithms, several authors have studied the computation of the volume of high-dimensional convex bodies, a problem that can also be used to model many other problems in combinatorial enumeration.\nOften these works use a black box model of computation in which the input is given by a subroutine for testing whether a point is inside or outside of the convex body, rather than by an explicit listing of the vertices or faces of a convex polytope.\nIt is known that, in this model, no deterministic algorithm can achieve an accurate approximation, and even for an explicit listing of faces or vertices the problem is #P-hard.\nHowever, a joint work by Martin Dyer, Alan M. Frieze and Ravindran Kannan provided a randomized polynomial time approximation scheme for the problem,\nproviding a sharp contrast between the capabilities of randomized and deterministic algorithms.\n\nThe main result of the paper is a randomized algorithm for finding an formula_1 approximation to the volume of a convex body formula_2 in formula_3-dimensional Euclidean space by assuming the existence of a membership oracle. The algorithm takes time bounded by a polynomial in formula_3, the dimension of formula_2 and formula_6.\nThe algorithm combines two ideas:\nThe given convex body can be approximated by a sequence of nested bodies, eventually reaching one of known volume (a hypersphere), with this approach used to estimate the factor by which the volume changes at each step of this sequence. Multiplying these factors gives the approximate volume of the original body.\n\nThis work earned its authors the 1991 Fulkerson Prize.\nAlthough the time for this algorithm is polynomial, it has a high exponent.\nSubsequent authors improved the running time of this method by providing more quickly mixing Markov chains for the same problem.\n"}
{"id": "9276503", "url": "https://en.wikipedia.org/wiki?curid=9276503", "title": "Dan Segal", "text": "Dan Segal\n\nDaniel Segal (born 1947) is a British mathematician and a Professor of Mathematics at the University of Oxford. He specialises in algebra and group theory.\n\nHe studied at Peterhouse, Cambridge, before taking a PhD at Queen Mary College, University of London, in 1972, supervised by Bertram Wehrfritz, with a dissertation on group theory entitled \"Groups of Automorphisms of Infinite Soluble Groups\". He is a Fellow of All Souls College at Oxford, where he is sub-warden.\n\nHis postgraduate students have included Marcus du Sautoy and Geoff Smith. He is the son of psychoanalyst Hanna Segal and brother of philosopher Gabriel Segal. \n\n"}
{"id": "9635691", "url": "https://en.wikipedia.org/wiki?curid=9635691", "title": "Davies attack", "text": "Davies attack\n\nIn cryptography, the Davies attack is a dedicated statistical cryptanalysis method for attacking the Data Encryption Standard (DES). The attack was originally created in 1987 by Donald Davies. In 1994, Eli Biham and Alex Biryukov made significant improvements to the technique. It is a known-plaintext attack based on the non-uniform distribution of the outputs of pairs of adjacent S-boxes. It works by collecting many known plaintext/ciphertext pairs and calculating the empirical distribution of certain characteristics. Bits of the key can be deduced given sufficiently many known plaintexts, leaving the remaining bits to be found through brute force. There are tradeoffs between the number of required plaintexts, the number of key bits found, and the probability of success; the attack can find 24 bits of the key with 2 known plaintexts and 53% success rate.\n\nThe Davies attack can be adapted to other Feistel ciphers besides DES. In 1998, Pornin developed techniques for analyzing and maximizing a cipher's resistance to this kind of cryptanalysis.\n\n"}
{"id": "679351", "url": "https://en.wikipedia.org/wiki?curid=679351", "title": "Deformation theory", "text": "Deformation theory\n\nIn mathematics, deformation theory is the study of \"infinitesimal conditions\" associated with varying a solution \"P\" of a problem to slightly different solutions \"P\", where ε is a small number, or vector of small quantities. The infinitesimal conditions are therefore the result of applying the approach of differential calculus to solving a problem with constraints. One might think, in analogy, of a structure that is not completely rigid, and that deforms slightly to accommodate forces applied from the outside; this explains the name.\n\nSome characteristic phenomena are: the derivation of first-order equations by treating the ε quantities as having negligible squares; the possibility of \"isolated solutions\", in that varying a solution may not be possible, \"or\" does not bring anything new; and the question of whether the infinitesimal constraints actually 'integrate', so that their solution does provide small variations. In some form these considerations have a history of centuries in mathematics, but also in physics and engineering. For example, in the geometry of numbers a class of results called \"isolation theorems\" was recognised, with the topological interpretation of an \"open orbit\" (of a group action) around a given solution. Perturbation theory also looks at deformations, in general of operators.\n\nThe most salient deformation theory in mathematics has been that of complex manifolds and algebraic varieties. This was put on a firm basis by foundational work of Kunihiko Kodaira and Donald C. Spencer, after deformation techniques had received a great deal of more tentative application in the Italian school of algebraic geometry. One expects, intuitively, that deformation theory of the first order should equate the Zariski tangent space with a moduli space. The phenomena turn out to be rather subtle, though, in the general case.\n\nIn the case of Riemann surfaces, one can explain that the complex structure on the Riemann sphere is isolated (no moduli). For genus 1, an elliptic curve has a one-parameter family of complex structures, as shown in elliptic function theory. The general Kodaira–Spencer theory identifies as the key to the deformation theory the sheaf cohomology group\n\nwhere Θ is (the sheaf of germs of sections of) the holomorphic tangent bundle. There is an obstruction in the \"H\" of the same sheaf; which is always zero in case of a curve, for general reasons of dimension. In the case of genus 0 the \"H\" vanishes, also. For genus 1 the dimension is the Hodge number \"h\" which is therefore 1. It is known that all curves of genus one have equations of form \"y\" = \"x\" + \"ax\" + \"b\". These obviously depend on two parameters, a and b, whereas the isomorphism classes of such curves have only one parameter. Hence there must be an equation relating those a and b which describe isomorphic elliptic curves. It turns out that curves for which \"b\"\"a\" has the same value, describe isomorphic curves. I.e. varying a and b is one way to deform the structure of the curve \"y\" = \"x\" + \"ax\" + \"b\", but not all variations of \"a,b\" actually change the isomorphism class of the curve.\n\nOne can go further with the case of genus \"g\" > 1, using Serre duality to relate the \"H\" to\n\nwhere Ω is the holomorphic cotangent bundle and the notation Ω means the \"tensor square\" (\"not\" the second exterior power). In other words, deformations are regulated by holomorphic quadratic differentials on a Riemann surface, again something known classically. The dimension of the moduli space, called Teichmüller space in this case, is computed as 3\"g\" − 3, by the Riemann–Roch theorem.\n\nThese examples are the beginning of a theory applying to holomorphic families of complex manifolds, of any dimension. Further developments included: the extension by Spencer of the techniques to other structures of differential geometry; the assimilation of the Kodaira–Spencer theory into the abstract algebraic geometry of Grothendieck, with a consequent substantive clarification of earlier work; and deformation theory of other structures, such as algebras.\n\nAnother method for formalizing deformation theory is using functors on the category of local Artin algebras over a field. A pre-deformation functor is defined as a functor\nsuch that formula_4 is a point. The intuition is that we want to study the infinitesimal structure of some moduli space around a point where lying above that point is the space of interest. It is typically the case that it is easier to described the functor for a moduli problem instead of finding an actual space. For example, if we want to consider the moduli-space of hypersurfaces of degree formula_5 in formula_6, then we could consider the functor\nwhere\n\nAlthough in general, it is more convenient/required to work with functor of groupoids instead of sets. This is true for moduli of curves.\n\nInfinitesimals have long been in use by mathematicians for non-rigorous arguments in calculus. The idea is that if we consider polynomials formula_9 with an infinitesimal formula_10, then only the first order terms really matter; that is, we can consider\nA simple application of this is that we can find the derivatives of monomials using infinitesimals:\nthe formula_10 term contains the derivative of the monomial, demonstrating its use in calculus. We could also interpret this equation as the first two terms of the Taylor expansion of the monomial. Infinitesimals can be made rigorous using nilpotent elements in local artin algebras. In the ring formula_14 we see that arguments with infinitesimals can work. This motivates the notation formula_15. \n\nMoreover, if we want to consider higher-order terms of a taylor approximation then we could consider the artin algebras formula_16. For our monomial, suppose we want to write out the second order expansion, then\nRecall that a Taylor expansion (at zero) can be written out as\nhence the previous two equations show that the second derivative of formula_19 is formula_20.\n\nIn general, since we want to consider arbitrary order Taylor expansions in any number of variables, we will consider the category of all local artin algebras over a field.\n\nTo motivative the definition of a pre-deformation functor, consider the projective hypersurface over a field\nIf we want to consider an infinitesimal deformation of this space, then we could write down a Cartesian square\nwhere formula_23. Then, the space on the right hand corner is one example of an infinitesimal deformation: the extra scheme theoretic structure of the nilpotent elements in formula_24 (which is topologically a point) allows us to organize this infinitesimal data. Since we want to consider all possible expansions, we will let our predeformation functor be defined on objects as\nwhere formula_26 is a local Artin formula_27-algebra.\n\nA pre-deformation functor is called smooth if for any surjection formula_28 such that the square of any element in the kernel is zero, there is a surjection\nThis is motivated by the following question: given a deformation\ndoes there exist an extension of this cartesian diagram to the cartesian diagrams\nthe name smooth comes from the lifting criterion of a smooth morphism of schemes.\n\nRecall that the tangent space of a scheme formula_32 can be described as the formula_33-set\nSince we are considering the tangent space of a point of some moduli space, we can define the tangent space of our (pre)-deformation functor as\n\nDeformation theory was famously applied in birational geometry by Shigefumi Mori to study the existence of rational curves on varieties. For a Fano variety of positive dimension Mori showed that there is a rational curve passing through every point. The method of the proof later became known as Mori's bend-and-break. The rough idea is to start with some curve \"C\" through a chosen point and keep deforming it until it breaks into several components. Replacing \"C\" by one of the components has the effect of decreasing either the genus or the degree of \"C\". So after several repetitions of the procedure, eventually we'll obtain a curve of genus 0, i.e. a rational curve. The existence and the properties of deformations of \"C\" require arguments from deformation theory and a reduction to positive characteristic.\n\nOne of the major applications of deformation theory is in arithmetic. It can be used to answer the following question: if we have a variety formula_36, what are the possible extensions formula_37? If our variety is a curve, then the vanishing formula_38 implies that every deformation induces a variety over formula_39; that is, if we have a smooth curve\nand a deformation\nthen we can always extend it to a diagram of the form\nThis implies that we can construct a formal scheme formula_43 giving a curve over formula_39.\n\nThe Serre–Tate theorem asserts, roughly speaking, that the deformations of abelian scheme \"A\" is controlled by deformations of the \"p\"-divisible group formula_45 consisting of its \"p\"-power torsion points.\n\nAnother application of deformation theory is with Galois deformations. It allows us to answer the question: If we have a Galois representation\nhow can we extend it to a representation\n\nThe so-called Deligne conjecture arising in the context of algebras (and Hochschild cohomology) stimulated much interest in deformation theory in relation to string theory (roughly speaking, to formalise the idea that a string theory can be regarded as a deformation of a point-particle theory). This is now accepted as proved, after some hitches with early announcements. Maxim Kontsevich is among those who have offered a generally accepted proof of this.\n\n\n\n\n\n"}
{"id": "7805214", "url": "https://en.wikipedia.org/wiki?curid=7805214", "title": "Description number", "text": "Description number\n\nDescription numbers are numbers that arise in the theory of Turing machines. They are very similar to Gödel numbers, and are also occasionally called \"Gödel numbers\" in the literature. Given some universal Turing machine, every Turing machine can, given its encoding on that machine, be assigned a number. This is the machine's description number. These numbers play a key role in Alan Turing's proof of the undecidability of the halting problem, and are very useful in reasoning about Turing machines as well.\n\nSay we had a Turing machine \"M\" with states q, ... q, with a tape alphabet with symbols s, ... s, with the blank denoted by s, and transitions giving the current state, current symbol, and actions performed (which might be to overwrite the current tape symbol and move the tape head left or right, or maybe not move it at all), and the next state. Under the original universal machine described by Alan Turing, this machine would be encoded as input to it as follows:\n\n\nThe UTM's input thus consists of the transitions separated by semicolons, so its input alphabet consists of the seven symbols, 'D', 'A', 'C', 'L', 'R', 'N', and ';'. For example, for a very simple Turing machine that alternates printing 0 and 1 on its tape forever:\n\n\nLetting the blank be s, '0' be s and '1' be s, the machine would be encoded by the UTM as:\n\nDADDCCRDAA;DAADDCRDA;\n\nBut then, if we replaced each of the seven symbols 'A' by 1, 'C' by 2, 'D' by 3, 'L' by 4, 'R' by 5, 'N' by 6, and ';' by 7, we would have an encoding of the Turing machine as a natural number: this is the description number of that Turing machine under Turing's universal machine. The simple Turing machine described above would thus have the description number 313322531173113325317. There is an analogous process for every other type of universal Turing machine. It is usually not necessary to actually compute a description number in this way: the point is that every natural number may be interpreted as the code for at most one Turing machine, though many natural numbers may not be the code for any Turing machine (or to put it another way, they represent Turing machines that have no states). The fact that such a number always exists for any Turing machine is generally the important thing.\n\nDescription numbers play a key role in many undecidability proofs, such as the proof that the halting problem is undecidable. In the first place, the existence of this direct correspondence between natural numbers and Turing machines shows that the set of all Turing machines is denumerable, and since the set of all partial functions is uncountably infinite, there must certainly be many functions that cannot be computed by Turing machines.\n\nBy making use of a technique similar to Cantor's diagonal argument, it is possible exhibit such an uncomputable function, for example, that the halting problem in particular is undecidable. First, let us denote by U(e, x) the action of the universal Turing machine given a description number e and input x, returning 0 if e is not the description number of a valid Turing machine. Now, supposing that there were some algorithm capable of settling the halting problem, i.e. a Turing machine TEST(e) which given the description number of some Turing machine would return 1 if the Turing machine halts on every input, or 0 if there are some inputs that would cause it to run forever. By combining the outputs of these machines, it should be possible to construct another machine δ(k) that returns U(k, k) + 1 if TEST(k) is 1 and 0 if TEST(k) is 0. From this definition δ is defined for every input and must naturally be total recursive. Since δ is built up from what we have assumed are Turing machines as well then it too must have a description number, call it e. So, we can feed the description number e to the UTM again, and by definition, δ(k) = U(e, k), so δ(e) = U(e, e). But since TEST(e) is 1, by our other definition, δ(e) = U(e, e) + 1, leading to a contradiction. Thus, TEST(e) cannot exist, and in this way we have settled the halting problem as undecidable.\n\n\n"}
{"id": "58899", "url": "https://en.wikipedia.org/wiki?curid=58899", "title": "Direct sum of modules", "text": "Direct sum of modules\n\nIn abstract algebra, the direct sum is a construction which combines several modules into a new, larger module. The direct sum of modules is the smallest module which contains the given modules as submodules with no \"unnecessary\" constraints, making it an example of a coproduct. Contrast with the direct product, which is the dual notion.\n\nThe most familiar examples of this construction occur when considering vector spaces (modules over a field) and abelian groups (modules over the ring Z of integers). The construction may also be extended to cover Banach spaces and Hilbert spaces.\n\nWe give the construction first in these two cases, under the assumption that we have only two objects. Then we generalise to an arbitrary family of arbitrary modules. The key elements of the general construction are more clearly identified by considering these two cases in depth.\n\nSuppose \"V\" and \"W\" are vector spaces over the field \"K\". The cartesian product \"V\" × \"W\" can be given the structure of a vector space over \"K\" by defining the operations componentwise:\n\n\nfor \"v\", \"v\", \"v\" ∈ \"V\", \"w\", \"w\", \"w\" ∈ \"W\", and α ∈ \"K\".\n\nThe resulting vector space is called the \"direct sum\" of \"V\" and \"W\" and is usually denoted by a plus symbol inside a circle:\n\nIt is customary to write the elements of an ordered sum not as ordered pairs (\"v\", \"w\"), but as a sum \"v\" + \"w\".\n\nThe subspace \"V\" × {0} of \"V\" ⊕ \"W\" is isomorphic to \"V\" and is often identified with \"V\"; similarly for {0} × \"W\" and \"W\". (See \"internal direct sum\" below.) With this identification, every element of \"V\" ⊕ \"W\" can be written in one and only one way as the sum of an element of \"V\" and an element of \"W\". The dimension of \"V\" ⊕ \"W\" is equal to the sum of the dimensions of \"V\" and \"W\". One elementary use is the reconstruction\nof a finite vector space from any subspace \"W\" and its orthogonal complement: \n\nThis construction readily generalises to any finite number of vector spaces.\n\nFor abelian groups \"G\" and \"H\" which are written additively, the direct product of \"G\" and \"H\" is also called a direct sum . Thus the cartesian product \"G\" × \"H\" is equipped with the structure of an abelian group by defining the operations componentwise:\n\n\nfor \"g\", \"g\" in \"G\", and \"h\", \"h\" in \"H\".\n\nIntegral multiples are similarly defined componentwise by\n\n\nfor \"g\" in \"G\", \"h\" in \"H\", and \"n\" an integer. This parallels the extension of the scalar product of vector spaces to the direct sum above.\n\nThe resulting abelian group is called the \"direct sum\" of \"G\" and \"H\" and is usually denoted by a plus symbol inside a circle:\n\nIt is customary to write the elements of an ordered sum not as ordered pairs (\"g\", \"h\"), but as a sum \"g\" + \"h\".\n\nThe subgroup \"G\" × {0} of \"G\" ⊕ \"H\" is isomorphic to \"G\" and is often identified with \"G\"; similarly for {0} × \"H\" and \"H\". (See \"internal direct sum\" below.) With this identification, it is true that every element of \"G\" ⊕ \"H\" can be written in one and only one way as the sum of an element of \"G\" and an element of \"H\". The rank of \"G\" ⊕ \"H\" is equal to the sum of the ranks of \"G\" and \"H\".\n\nThis construction readily generalises to any finite number of abelian groups.\n\nOne should notice a clear similarity between the definitions of the direct sum of two vector spaces and of two abelian groups. In fact, each is a special case of the construction of the direct sum of two modules. Additionally, by modifying the definition one can accommodate the direct sum of an infinite family of modules. The precise definition is as follows .\n\nLet \"R\" be a ring, and {\"M\" : \"i\" ∈ \"I\"} a family of left \"R\"-modules indexed by the set \"I\". The \"direct sum\" of {\"M\"} is then defined to be the set of all sequences formula_4 where formula_5 and formula_6 for cofinitely many indices \"i\". (The direct product is analogous but the indices do not need to cofinitely vanish.)\n\nIt can also be defined as functions α from \"I\" to the disjoint union of the modules \"M\" such that α(\"i\") ∈ \"M\" for all \"i\" ∈ \"I\" and α(\"i\") = 0 for cofinitely many indices \"i\". These functions can equivalently be regarded as finitely supported sections of the fiber bundle over the index set \"I\", with the fiber over formula_7 being formula_8.\n\nThis set inherits the module structure via component-wise addition and scalar multiplication. Explicitly, two such sequences (or functions) α and β can be added by writing formula_9 for all \"i\" (note that this is again zero for all but finitely many indices), and such a function can be multiplied with an element \"r\" from \"R\" by defining formula_10 for all \"i\". In this way, the direct sum becomes a left \"R\"-module, and it is denoted\n\nIt is customary to write the sequence formula_4 as a sum formula_13. Sometimes a primed summation formula_14 is used to indicate that cofinitely many of the terms are zero.\n\n\nSuppose \"M\" is some \"R\"-module, and \"M\" is a submodule of \"M\" for every \"i\" in \"I\". If every \"x\" in \"M\" can be written in one and only one way as a sum of finitely many elements of the \"M\", then we say that \"M\" is the internal direct sum of the submodules \"M\" . In this case, \"M\" is naturally isomorphic to the (external) direct sum of the \"M\" as defined above .\n\nA submodule \"N\" of \"M\" is a direct summand of \"M\" if there exists some other submodule \"N′\" of \"M\" such that \"M\" is the \"internal\" direct sum of \"N\" and \"N′\". In this case, \"N\" and \"N′\" are complementary submodules.\n\nIn the language of category theory, the direct sum is a coproduct and hence a colimit in the category of left \"R\"-modules, which means that it is characterized by the following universal property. For every \"i\" in \"I\", consider the \"natural embedding\"\n\nwhich sends the elements of \"M\" to those functions which are zero for all arguments but \"i\". If \"f\" : \"M\" → \"M\" are arbitrary \"R\"-linear maps for every \"i\", then there exists precisely one \"R\"-linear map\n\nsuch that \"f\" o \"j\" = \"f\" for all \"i\".\n\nDually, the direct product is the product.\n\nThe direct sum gives a collection of objects the structure of a commutative monoid, in that the addition of objects is defined, but not subtraction. In fact, subtraction can be defined, and every commutative monoid can be extended to an abelian group. This extension is known as the Grothendieck group. The extension is done by defining equivalence classes of pairs of objects, which allows certain pairs to be treated as inverses. The construction, detailed in the article on the Grothendieck group, is \"universal\", in that it has the universal property of being unique, and homomorphic to any other embedding of an abelian monoid in an abelian group.\n\nIf the modules we are considering carry some additional structure (e.g. a norm or an inner product), then the direct sum of the modules can often be made to carry this additional structure, as well. In this case, we obtain the coproduct in the appropriate category of all objects carrying the additional structure. Two prominent examples occur for Banach spaces and Hilbert spaces.\n\nIn some classical texts, the notion of direct sum of algebras over a field is also introduced. This construction, however, does not provide a coproduct in the category of algebras, but a direct product (\"see note below\" and the remark on direct sums of rings).\n\nA direct sum of algebras \"X\" and \"Y\" is the direct sum as vector spaces, with product \nConsider these classical examples:\nJoseph Wedderburn exploited the concept of a direct sum of algebras in his classification of hypercomplex numbers. See his \"Lectures on Matrices\" (1934), page 151.\nWedderburn makes clear the distinction between a direct sum and a direct product of algebras: For the direct sum the field of scalars acts jointly on both parts: formula_27 while for the direct product a scalar factor may be collected alternately with the parts, but not both:formula_28.\nIan R. Porteous uses the three direct sums above, denoting them formula_29, as rings of scalars in his analysis of \"Clifford Algebras and the Classical Groups\" (1995).\n\nThe construction described above, as well as Wedderburn's use of the terms \"direct sum\" and \"direct product\" follow a different convention from the one in category theory. In categorical terms, Wedderburn's \"direct sum\" is a categorical product, whilst Wedderburn's \"direct product\" is a coproduct (or categorical sum), which (for commutative algebras) actually corresponds to the tensor product of algebras.\n\nA composition algebra (\"A\", *, n) is an algebra over a field \"A\", an involution * and a \"norm\" n(\"x\") = \"x x\"*. Any field \"K\" gives rise to a series of composition algebras beginning with \"K\", and the trivial involution, so that n(\"x\") = \"x\". The inductive step in the series involves forming the direct sum A ⊕ A and using the new involution formula_30\n\nLeonard Dickson developed this construction doubling quaternions for Cayley numbers, and the doubling method involving the direct sum A ⊕ A is called the Cayley–Dickson construction. In the instance beginning with \"K\" = ℝ, the series generates complex numbers, quaternions, octonions, and sedenions. Beginning with \"K\" = ℂ and the norm n(\"z\") = \"z\", the series continues with bicomplex numbers, biquaternions, and bioctonions.\n\nMax Zorn realized that the classical Cayley–Dickson construction missed constructing some composition algebras that arise as real subalgebras in the (ℂ, \"z\") series, in particular the split-octonions. A modified Cayley–Dickson construction, still based on use of the direct sum A ⊕ A of a base algebra A, has since been used to exhibit the series ℝ, split-complex numbers, split-quaternions, and split-octonions.\n\nThe direct sum of two Banach spaces \"X\" and \"Y\" is the direct sum of \"X\" and \"Y\" considered as vector spaces, with the norm ||(\"x\",\"y\")|| = ||\"x\"|| + ||\"y\"|| for all \"x\" in \"X\" and \"y\" in \"Y\".\n\nGenerally, if \"X\" is a collection of Banach spaces, where \"i\" traverses the index set \"I\", then the direct sum ⨁ \"X\" is a module consisting of all functions \"x\" defined over \"I\" such that \"x\"(\"i\") ∈ \"X\" for all \"i\" ∈ \"I\" and\n\nThe norm is given by the sum above. The direct sum with this norm is again a Banach space.\n\nFor example, if we take the index set \"I\" = N and \"X\" = R, then the direct sum ⨁\"X\" is the space \"l\", which consists of all the sequences (\"a\") of reals with finite norm ||\"a\"|| = ∑ |\"a\"|.\n\nA closed subspace \"A\" of a Banach space \"X\" is complemented if there is another closed subspace \"B\" of \"X\" such that \"X\" is equal to the internal direct sum formula_32. Note that not every closed subspace is complemented, e.g. \"c\" is not complemented in formula_33.\n\nLet {(\"M\",\"b\")  : \"i\" ∈ \"I\"} be a family indexed by \"I\" of modules equipped with bilinear forms. The orthogonal direct sum is the module direct sum with bilinear form \"B\" defined by\n\nin which the summation makes sense even for infinite index sets \"I\" because only finitely many of the terms are non-zero.\n\nIf finitely many Hilbert spaces \"H\"...,\"H\" are given, one can construct their orthogonal direct sum as above (since they are vector spaces), defining the inner product as:\n\nThe resulting direct sum is a Hilbert space which contains the given Hilbert spaces as mutually orthogonal subspaces.\n\nIf infinitely many Hilbert spaces \"H\" for \"i\" in \"I\" are given, we can carry out the same construction; notice that when defining the inner product, only finitely many summands will be non-zero. However, the result will only be an inner product space and it will not necessarily be complete. We then define the direct sum of the Hilbert spaces \"H\" to be the completion of this inner product space. \n\nAlternatively and equivalently, one can define the direct sum of the Hilbert spaces \"H\" as the space of all functions α with domain \"I\", such that α(\"i\") is an element of \"H\" for every \"i\" in \"I\" and:\n\nThe inner product of two such function α and β is then defined as:\n\nThis space is complete and we get a Hilbert space.\n\nFor example, if we take the index set \"I\" = N and \"X\" = R, then the direct sum ⨁ \"X\" is the space \"l\", which consists of all the sequences (\"a\") of reals with finite norm formula_38. Comparing this with the example for Banach spaces, we see that the Banach space direct sum and the Hilbert space direct sum are not necessarily the same. But if there are only finitely many summands, then the Banach space direct sum is isomorphic to the Hilbert space direct sum, although the norm will be different.\n\nEvery Hilbert space is isomorphic to a direct sum of sufficiently many copies of the base field (either R or C). This is equivalent to the assertion that every Hilbert space has an orthonormal basis. More generally, every closed subspace of a Hilbert space is complemented: it admits an orthogonal complement. Conversely, the Lindenstrauss–Tzafriri theorem asserts that if every closed subspace of a Banach space is complemented, then the Banach space is isomorphic (topologically) to a Hilbert space.\n\n\n"}
{"id": "35063240", "url": "https://en.wikipedia.org/wiki?curid=35063240", "title": "Eta invariant", "text": "Eta invariant\n\nIn mathematics, the eta invariant of a self-adjoint elliptic differential operator on a compact manifold is formally the number of positive eigenvalues minus the number of negative eigenvalues. In practice both numbers are often infinite so are defined using zeta function regularization. It was introduced by who used it to extend the Hirzebruch signature theorem to manifolds with boundary. The name comes from the fact that it is a generalization of the Dirichlet eta function.\n\nThey also later used the eta invariant of a self-adjoint operator to define the eta invariant of a compact odd-dimensional smooth manifold.\ndefined the signature defect of the boundary of a manifold as the eta invariant, and used this to show that Hirzebruch's signature defect of a cusp of a Hilbert modular surface can be expressed in terms of the value at \"s\"=0 or 1 of a Shimizu L-function.\n\nThe eta invariant of self-adjoint operator \"A\" is given by \"η\"(0), where \"η\" is the analytic continuation of\n\nand the sum is over the nonzero eigenvalues λ of \"A\".\n\n"}
{"id": "14498167", "url": "https://en.wikipedia.org/wiki?curid=14498167", "title": "Goldbach–Euler theorem", "text": "Goldbach–Euler theorem\n\nIn mathematics, the Goldbach–Euler theorem (also known as Goldbach's theorem), states that the sum of 1/(\"p\" − 1) over the set of perfect powers \"p\", excluding 1 and omitting repetitions, converges to 1:\n\nThis result was first published in Euler's 1737 paper \"Variæ observationes circa series infinitas\". Euler attributed the result to a letter (now lost) from Goldbach.\n\nGoldbach's original proof to Euler involved assigning a constant to the harmonic series:\nformula_2, which is divergent. Such a proof is not considered rigorous by modern standards. There is a strong resemblance between the method of sieving out powers employed in his proof and the method of factorization used to derive Euler's product formula for the Riemann zeta function.\n\nLet x be given by\n\nSince the sum of the reciprocal of every power of two is formula_4, subtracting the terms with powers of two from x gives\n\nRepeat the process with the terms with the powers of three: formula_6\n\nAbsent from the above sum are now all terms with powers of two and three. Continue by removing terms with powers of 5, 6 and so on until the right side is exhausted to the value of 1. Eventually, we obtain the equation\n\nwhich we rearrange into\n\nwhere the denominators consist of all positive integers that are the non-powers minus one. By subtracting the previous equation from the definition of x given above, we obtain\n\nwhere the denominators now consist only of perfect powers minus one.\n\nWhile lacking mathematical rigor, Goldbach's proof provides a reasonably intuitive visualization of the problem. Rigorous proofs require proper and more careful treatment of the divergent terms of the harmonic series. Other proofs make use of the fact that the sum of 1/\"p\" over the set of perfect powers \"p\", excluding 1 but including repetitions, converges to 1 by demonstrating the equivalence:\n\n\n"}
{"id": "2712653", "url": "https://en.wikipedia.org/wiki?curid=2712653", "title": "Guess value", "text": "Guess value\n\nIn mathematical modeling, a guess value is more commonly called a starting value or initial value. These are necessary for most optimization problems which use search algorithms, because those algorithms are mainly deterministic and iterative, and they need to start somewhere. One common type of application is nonlinear regression.\n\nThe quality of the initial values can have a considerable impact on the success or lack of such of the search algorithm. This is because the fitness function or objective function (in many cases a sum of squared errors (SSE)) can have difficult shapes. In some parts of the search region, the function may increase exponentially, in others quadratically, and there may be regions where the function asymptotes to a plateau. Starting values that fall in an exponential region can lead to algorithm failure because of arithmetic overflow. Starting values that fall in the asymptotic plateau region can lead to algorithm failure because of \"dithering\". Deterministic search algorithms may use a slope function to go to a minimum. If the slope is very small, then underflow errors can cause the algorithm to wander, seemingly aimlessly; this is dithering.\n\nGuess values can be determined a number of ways. Guessing is one of them. If one is familiar with the type of problem, then this is an educated guess or guesstimate. Other techniques include linearization, solving simultaneous equations, reducing dimensions, treating the problem as a time series, converting the problem to a (hopefully) linear differential equation, and using mean values. Further methods for determining starting values and optimal values in their own right come from stochastic methods, the most commonly known of these being evolutionary algorithms and particularly genetic algorithms.\n"}
{"id": "896120", "url": "https://en.wikipedia.org/wiki?curid=896120", "title": "History of programming languages", "text": "History of programming languages\n\nThe first high-level programming language was Plankalkül, created by Konrad Zuse between 1942 and 1945. The first high-level language to have an associated compiler, was created by Corrado Böhm in 1951, for his PhD thesis. The first commercially available language was FORTRAN (FORmula TRANslation); developed in 1956 (first manual appeared in 1956, but first developed in 1954) by a team led by John Backus at IBM.\n\nWhen FORTRAN was first introduced it was treated with suspicion because of the belief that programs compiled from high-level language would be less efficient than those written directly in machine code. FORTRAN became popular because it provided a means of porting existing code to new computers, in a hardware market that was rapidly evolving; the language eventually became known for its efficiency.\n\nDuring 1842–1843 Ada Lovelace translated the memoir of Italian mathematician Francis Maneclang about Charles Babbage's newest proposed machine, the analytical engine; she supplemented the memoir with notes that specified in detail a method for calculating Bernoulli numbers with the engine, recognized by some historians as the world's first computer program.\n\nThe first computer codes were specialized for their applications: e.g., Alonzo Church was able to express the lambda calculus in a formulaic way and the Turing machine was an abstraction of the operation of a tape-marking machine.\n\nTo some people, some degree of expressive power and human-readability is required before the status of \"programming language\" is granted. Jacquard Looms and Charles Babbage's Difference Engine both had simple, extremely limited languages for describing the actions that these machines should perform.\n\nIn the 1940s, the first recognizably modern electrically powered computers were created. The limited speed and memory capacity forced programmers to write hand tuned assembly language programs. It was eventually realized that programming in assembly language required a great deal of intellectual effort.\n\nThe first programming languages designed to communicate instructions to a computer were written in the 1950s. An early high-level programming language to be designed for a computer was Plankalkül, developed by the Germans for Z1 by Konrad Zuse between 1943 and 1945. However, it was not implemented until 1998 and 2000.\n\nJohn Mauchly's Short Code, proposed in 1949, was one of the first high-level languages ever developed for an electronic computer. Unlike machine code, Short Code statements represented mathematical expressions in understandable form. However, the program had to be translated into machine code every time it ran, making the process much slower than running the equivalent machine code.\nAt the University of Manchester, Alick Glennie developed Autocode in the early 1950s, with the second iteration developed for the Mark 1 by R. A. Brooker in 1954, known as the \"Mark 1 Autocode\". Brooker also developed an autocode for the Ferranti Mercury in the 1950s in conjunction with the University of Manchester. The version for the EDSAC 2 was devised by D. F. Hartley of University of Cambridge Mathematical Laboratory in 1961. Known as EDSAC 2 Autocode, it was a straight development from Mercury Autocode adapted for local circumstances, and was noted for its object code optimisation and source-language diagnostics which were advanced for the time. A contemporary but separate thread of development, Atlas Autocode was developed for the University of Manchester Atlas 1 machine.\n\nIn 1954, language FORTRAN was invented at IBM by a team led by John Backus; it was the first widely used high level general purpose programming language to have a functional implementation, as opposed to just a design on paper. It is still a popular language for high-performance computing and is used for programs that benchmark and rank the world's fastest supercomputers.\n\nAnother early programming language was devised by Grace Hopper in the US, called FLOW-MATIC. It was developed for the UNIVAC I at Remington Rand during the period from 1955 until 1959. Hopper found that business data processing customers were uncomfortable with mathematical notation, and in early 1955, she and her team wrote a specification for an English programming language and implemented a prototype. The FLOW-MATIC compiler became publicly available in early 1958 and was substantially complete in 1959. Flow-Matic was a major influence in the design of COBOL, since only it and its direct descendent AIMACO were in actual use at the time.\n\nOther languages still in use today include LISP (1958), invented by John McCarthy and COBOL (1959), created by the Short Range Committee. Another milestone in the late 1950s was the publication, by a committee of American and European computer scientists, of \"a new language for algorithms\"; the \"ALGOL 60 Report\" (the \"ALGOrithmic Language\"). This report consolidated many ideas circulating at the time and featured three key language innovations:\n\nAnother innovation, related to this, was in how the language was described:\n\nAlgol 60 was particularly influential in the design of later languages, some of which soon became more popular. The Burroughs large systems were designed to be programmed in an extended subset of Algol.\n\nAlgol's key ideas were continued, producing ALGOL 68:\n\nAlgol 68's many little-used language features (for example, concurrent and parallel blocks) and its complex system of syntactic shortcuts and automatic type coercions made it unpopular with implementers and gained it a reputation of being \"difficult\". Niklaus Wirth actually walked out of the design committee to create the simpler Pascal language.\n\nSome notable languages that were developed in this period include:\n\nThe period from the late 1960s to the late 1970s brought a major flowering of programming languages. Most of the major language paradigms now in use were invented in this period:\n\nEach of these languages spawned an entire family of descendants, and most modern languages count at least one of them in their ancestry.\n\nThe 1960s and 1970s also saw considerable debate over the merits of \"structured programming\", which essentially meant programming without the use of \"goto\". A significant fraction of programmers believed that, even in languages that provide \"goto\", it is bad programming style to use it except in rare circumstances. This debate was closely related to language design: some languages did not include a \"goto\" at all, which forced structured programming on the programmer.\n\nTo provide even faster compile times, some languages were structured for \"one-pass compilers\" which expect subordinate routines to be defined first, as with Pascal, where the main routine, or driver function, is the final section of the program listing.\n\nSome notable languages that were developed in this period include:\n\nThe 1980s were years of relative consolidation in imperative languages. Rather than inventing new paradigms, all of these movements elaborated upon the ideas invented in the previous decade. C++ combined object-oriented and systems programming. The United States government standardized Ada, a systems programming language intended for use by defense contractors. In Japan and elsewhere, vast sums were spent investigating so-called fifth-generation programming languages that incorporated logic programming constructs. The functional languages community moved to standardize ML and Lisp. Research in Miranda, a functional language with lazy evaluation, began to take hold in this decade.\n\nOne important new trend in language design was an increased focus on programming for large-scale systems through the use of \"modules\", or large-scale organizational units of code. Modula, Ada, and ML all developed notable module systems in the 1980s. Module systems were often wedded to generic programming constructs---generics being, in essence, parametrized modules (see also polymorphism in object-oriented programming).\n\nAlthough major new paradigms for imperative programming languages did not appear, many researchers expanded on the ideas of prior languages and adapted them to new contexts. For example, the languages of the Argus and Emerald systems adapted object-oriented programming to distributed systems.\n\nThe 1980s also brought advances in programming language implementation. The RISC movement in computer architecture postulated that hardware should be designed for compilers rather than for human assembly programmers. Aided by processor speed improvements that enabled increasingly aggressive compilation techniques, the RISC movement sparked greater interest in compilation technology for high-level languages.\n\nLanguage technology continued along these lines well into the 1990s. \n\nSome notable languages that were developed in this period include:\n\nThe rapid growth of the Internet in the mid-1990s was the next major historic event in programming languages. By opening up a radically new platform for computer systems, the Internet created an opportunity for new languages to be adopted. In particular, the JavaScript programming language rose to popularity because of its early integration with the Netscape Navigator web browser. Various other scripting languages achieved widespread use in developing customized applications for web servers such as PHP. The 1990s saw no fundamental novelty in imperative languages, but much recombination and maturation of old ideas. This era began the spread of functional languages. A big driving philosophy was programmer productivity. Many \"rapid application development\" (RAD) languages emerged, which usually came with an IDE, garbage collection, and were descendants of older languages. All such languages were object-oriented. These included Object Pascal, Visual Basic, and Java. Java in particular received much attention.\n\nMore radical and innovative than the RAD languages were the new scripting languages. These did not directly descend from other languages and featured new syntaxes and more liberal incorporation of features. Many consider these scripting languages to be more productive than even the RAD languages, but often because of choices that make small programs simpler but large programs more difficult to write and maintain. Nevertheless, scripting languages came to be the most prominent ones used in connection with the Web.\n\nSome notable languages that were developed in this period include:\n\nProgramming language evolution continues, in both industry and research. Some of the recent trends have included:\n\n\nSome notable languages developed during this period include:\n\nSome key people who helped develop programming languages:\n\n\n\n"}
{"id": "40393020", "url": "https://en.wikipedia.org/wiki?curid=40393020", "title": "Homography (computer vision)", "text": "Homography (computer vision)\n\nIn the field of computer vision, any two images of the same planar surface in space are related by a homography (assuming a pinhole camera model). This has many practical applications, such as image rectification, image registration, or computation of camera motion—rotation and translation—between two images. Once camera rotation and translation have been extracted from an estimated homography matrix, this information may be used for navigation, or to insert models of 3D objects into an image or video, so that they are rendered with the correct perspective and appear to have been part of the original scene (see Augmented reality).\n\nWe have two cameras \"a\" and \"b\", looking at points formula_1 in a plane.\nPassing from the projection formula_2 of formula_1 in \"b\" to the projection formula_4 of formula_1 in \"a\":\n\n"}
{"id": "5365395", "url": "https://en.wikipedia.org/wiki?curid=5365395", "title": "Incentive compatibility", "text": "Incentive compatibility\n\nA mechanism is called incentive-compatible (IC) if every participant can achieve the best outcome to himself just by acting according to his true preferences. \n\nThere are several different degrees of incentive-compatibility: \n\nEvery DSIC mechanism is also BNIC, but a BNIC mechanism may exist even if no DSIC mechanism exists.\n\nTypical examples of DSIC mechanisms are majority voting between two alternatives, and second-price auction.\n\nTypical examples of a mechanisms that are not DSIC are plurality voting between three or more alternatives and first-price auction.\n\nA randomized mechanism is a probability-distribution on deterministic mechanisms. There are two ways to define incentive-compatibility of randomized mechanisms:\n\nThe famous Revelation principle comes in two variants corresponding to the two flavors of incentive-compatibility:\n"}
{"id": "7210758", "url": "https://en.wikipedia.org/wiki?curid=7210758", "title": "Integral cryptanalysis", "text": "Integral cryptanalysis\n\nIn cryptography, integral cryptanalysis is a cryptanalytic attack that is particularly applicable to block ciphers based on substitution-permutation networks. It was originally designed by Lars Knudsen as a dedicated attack against Square, so it is commonly known as the Square attack. It was also extended to a few other ciphers related to Square: CRYPTON, Rijndael, and SHARK. Stefan Lucks generalized the attack to what he called a \"saturation attack\" and used it to attack Twofish, which is not at all similar to Square, having a radically different Feistel network structure. Forms of integral cryptanalysis have since been applied to a variety of ciphers, including Hierocrypt, IDEA, Camellia, Skipjack, MISTY1, MISTY2, SAFER++, KHAZAD, and \"FOX\" (now called IDEA NXT).\n\nUnlike differential cryptanalysis, which uses pairs of chosen plaintexts with a fixed XOR difference, integral cryptanalysis uses sets or even multisets of chosen plaintexts of which part is held constant and another part varies through all possibilities. For example, an attack might use 256 chosen plaintexts that have all but 8 of their bits the same, but all differ in those 8 bits. Such a set necessarily has an XOR sum of 0, and the XOR sums of the corresponding sets of ciphertexts provide information about the cipher's operation. This contrast between the differences of pairs of texts and the sums of larger sets of texts inspired the name \"integral cryptanalysis\", borrowing the terminology of calculus.\n\n"}
{"id": "6018290", "url": "https://en.wikipedia.org/wiki?curid=6018290", "title": "José Ádem", "text": "José Ádem\n\nJosé Ádem (born in Tuxpan, Veracruz, October 27, 1921; died February 14, 1991) was a Mexican mathematician who worked in algebraic topology, and proved the Ádem relations between Steenrod squares.\n\nÁdem showed an interest in mathematics from an early age, and moved to Mexico City in 1941 to pursue a degree in engineering and mathematics. He obtained his B.S. in mathematics from the National Autonomous University of Mexico (UNAM) in 1949. During this time met Solomon Lefschetz, a famous algebraic topologist who was spending prolonged periods of time in Mexico. Lefschetz recognized Ádem's mathematical talent, and sent him as a doctoral student to Princeton University where he graduated in 1952. His dissertation, \"Iterations of the squaring operations in algebraic topology\", was written under the supervision of Norman Steenrod and introduced what are now called the Ádem relations.\n\nHis brother Julián Adem was also a distinguished Mexican mathematician, who obtained a Ph.D. in applied mathematics from Brown University in 1953. Julián's son is topologist Alejandro Adem.\n\nÁdem became a researcher at the Mathematics Institute of UNAM (1954–1961), and then head of the Mathematics Department at the Instituto Politécnico Nacional (1961–1973).\nHe was elected to El Colegio Nacional on 4 April 1960.\n\nIn 1951 he was awarded a Guggenheim Fellowship. He started in 1956 the second series of the Boletín de la Sociedad Matemática Mexicana.\n\n"}
{"id": "27311945", "url": "https://en.wikipedia.org/wiki?curid=27311945", "title": "Journal of Logic and Computation", "text": "Journal of Logic and Computation\n\nThe Journal of Logic and Computation is a peer-reviewed academic journal focused on logic and computing. It was established in 1990 and is published by Oxford University Press under licence from Professor Dov Gabbay as owner of the journal.\n"}
{"id": "23994987", "url": "https://en.wikipedia.org/wiki?curid=23994987", "title": "Kostant partition function", "text": "Kostant partition function\n\nIn representation theory, a branch of mathematics, the Kostant partition function, introduced by , of a root system formula_1 is the number of ways one can represent a vector (weight) as a non-negative integer linear combination of the positive roots formula_2. Kostant used it to rewrite the Weyl character formula as a formula (the Kostant multiplicity formula) for the multiplicity of a weight of an irreducible representation of a semisimple Lie algebra. An alternative formula, that is more computationally efficient in some cases, is Freudenthal's forumula.\n\nThe Kostant partition function can also be defined for Kac–Moody algebras and has similar properties.\n\nConsider the A2 root systems, with positive roots formula_3, formula_4, and formula_5. If an element formula_6 can be expressed as a non-negative integer linear combination of formula_3, formula_4, and formula_9, then since formula_10, it can also be expressed as a non-negative integer linear combination of formula_3 and formula_4:\nwith formula_14 and formula_15 being non-negative integers. This expression gives \"one\" way to write formula_6 as a non-negative integer combination of positive roots; other expressions can be obtained by replacing formula_17 with formula_9 some number of times. We can do the replacement formula_19 times, where formula_20. Thus, if the Kostant partition function is denoted by formula_21, we obtain the formula\nThis result is shown graphically in the image at right. If an element formula_6 is not of the form formula_13, then formula_25.\n\nFor each root formula_26 and each formula_27, we can \"formally\" apply the formula for the sum of a geometric series to obtain\nwhere we do not worry about convergence—that is, the equality is understood at the level of formal power series. Using Weyl's denominator formula\n\nwe obtain a formal expression for the reciprocal of the Weyl denominator:\n\nHere, the first equality is by taking a product over the positive roots of the geometric series formula and the second equality is by counting all the ways a given exponential formula_31 can occur in the product.\n\nThis argument shows that we can convert the Weyl character formula for the irreducible representation with highest weight formula_32:\n\nfrom a quotient to a product:\n\nUsing the preceding rewriting of the character formula, it is relatively easy to write the character as a sum of exponentials. The coefficients of these exponentials are the multiplicities of the corresponding weights. We thus obtain a formula for the multiplicity of a given weight formula_6 in the irreducible representation with highest weight formula_32:\nThis result is the Kostant multiplicity formula. \n\nThe dominant term in this formula is the term formula_38; the contribution of this term is formula_39, which is just the multiplicity of formula_6 in the Verma module with highest weight formula_32. If formula_32 is sufficiently far inside the fundamental Weyl chamber and formula_6 is sufficiently close to formula_32, it may happen that all other terms in the formula are zero. Specifically, unless formula_45 is higher than formula_46, the value of the Kostant partition function on formula_47 will be zero. Thus, although the sum is nominally over the whole Weyl group, in most cases, the number of nonzero terms is smaller than the order of the Weyl group.\n\n"}
{"id": "1065304", "url": "https://en.wikipedia.org/wiki?curid=1065304", "title": "Link encryption", "text": "Link encryption\n\nLink encryption is an approach to communications security that encrypts and decrypts all traffic at each network routing point (e.g. network switch, or node through which it passes) until arrival at its final destination. This repeated decryption and encryption is necessary to allow the routing information contained in each transmission to be read and employed further to direct the transmission toward its destination, before which it is re-encrypted. This contrasts with end-to-end encryption where internal information, but not the header/routing information, are encrypted by the sender at the point of origin and only decrypted by the intended receiver.\n\nLink encryption offers a couple of advantages:\n\nOn the other hand, end-to-end encryption ensures only the recipient sees the plaintext.\n\nLink encryption can be used with end-to-end systems by superencrypting the messages.\n\nBulk encryption refers to encrypting a large number of circuits at once, after they have been multiplexed.\n"}
{"id": "7664719", "url": "https://en.wikipedia.org/wiki?curid=7664719", "title": "Movable singularity", "text": "Movable singularity\n\nIn the theory of ordinary differential equations, a movable singularity is a point where the solution of the equation behaves badly and which is \"movable\" in the sense that its location depends on the initial conditions of the differential equation.\nSuppose we have an ordinary differential equation in the complex domain. Any given solution \"y\"(\"x\") of this equation may well have singularities at various points (i.e. points at which it is not a regular holomorphic function, such as branch points, essential singularities or poles). A singular point is said to be movable if its location depends on the particular solution we have chosen, rather than being fixed by the equation itself.\n\nFor example the equation\n\nhas solution formula_2 for any constant \"c\". This solution has a branchpoint at formula_3, and so the equation has a movable branchpoint (since it depends on the choice of the solution, i.e. the choice of the constant \"c\").\n\nIt is a basic feature of linear ordinary differential equations that singularities of solutions occur only at singularities of the equation, and so linear equations do not have movable singularities.\n\nWhen attempting to look for 'good' nonlinear differential equations it is this property of linear equations that one would like to see: asking for no movable singularities is often too stringent, instead one often asks for the so-called Painlevé property: 'any movable singularity should be a pole', first used by Sofia Kovalevskaya.\n\n"}
{"id": "742477", "url": "https://en.wikipedia.org/wiki?curid=742477", "title": "Newton polygon", "text": "Newton polygon\n\nIn mathematics, the Newton polygon is a tool for understanding the behaviour of polynomials over local fields.\n\nIn the original case, the local field of interest was the field of formal Laurent series in the indeterminate \"X\", i.e. the field of fractions of the formal power series ring\n\nover \"K\", where \"K\" was the real number or complex number field. This is still of considerable utility with respect to Puiseux expansions. The Newton polygon is an effective device for understanding the leading terms\n\nof the power series expansion solutions to equations\n\nwhere \"P\" is a polynomial with coefficients in \"K\"[\"X\"], the polynomial ring; that is, implicitly defined algebraic functions. The exponents \"r\" here are certain rational numbers, depending on the branch chosen; and the solutions themselves are power series in\n\nwith \"Y\" = \"X\" for a denominator \"d\" corresponding to the branch. The Newton polygon gives an effective, algorithmic approach to calculating \"d\".\n\nAfter the introduction of the p-adic numbers, it was shown that the Newton polygon is just as useful in questions of ramification for local fields, and hence in algebraic number theory. Newton polygons have also been useful in the study of elliptic curves.\n\nA priori, given a polynomial over a field, the behaviour of the roots (assuming it has roots) will be unknown. Newton polygons provide one technique for the study of the behaviour of the roots.\n\nLet formula_1 be a local field with discrete valuation formula_2 and let\n\nwith formula_4. Then the Newton polygon of formula_5 is defined to be the lower convex hull of the set of points\n\nignoring the points with formula_7.\nRestated geometrically, plot all of these points \"P\" on the \"xy\"-plane. Let's assume that the points indices increase from left to right (\"P\" is the leftmost point, \"P\" is the rightmost point). Then, starting at \"P\", draw a ray straight down parallel with the \"y\"-axis, and rotate this ray counter-clockwise until it hits the point \"P\" (not necessarily \"P\"). Break the ray here. Now draw a second ray from \"P\" straight down parallel with the \"y\"-axis, and rotate this ray counter-clockwise until it hits the point \"P\". Continue until the process reaches the point \"P\"; the resulting polygon (containing the points \"P\", \"P\", \"P\", ..., \"P\", \"P\") is the Newton polygon.\n\nAnother, perhaps more intuitive way to view this process is this : consider a rubber band surrounding all the points \"P\", ..., \"P\". Stretch the band upwards, such that the band is stuck on its lower side by some of the points (the points act like nails, partially hammered into the xy plane). The vertices of the Newton polygon are exactly those points.\n\nFor a neat diagram of this see Ch6 §3 of \"Local Fields\" by JWS Cassels, LMS Student Texts 3, CUP 1986. It is on p99 of the 1986 paperback edition.\n\nNewton polygons are named after Isaac Newton, who first described them and some of their uses in correspondence from the year 1676 addressed to Henry Oldenburg.\n\nA Newton Polygon is sometimes a special case of a , and can be used to construct asymptotic solutions of two-variable polynomial equations like \nformula_8\n\nAnother application of the Newton polygon comes from the following result:\n\nLet\n\nbe the slopes of the line segments of the Newton polygon of formula_10 (as defined above) arranged in increasing order, and let\n\nbe the corresponding lengths of the line segments projected onto the x-axis (i.e. if we have a line segment stretching between the points formula_12 and formula_13 then the length is formula_14). Then for each integer formula_15, formula_10 has exactly formula_17 roots with valuation formula_18.\n\nIn the context of a valuation, we are given certain information in the form of the valuations of elementary symmetric functions of the roots of a polynomial, and require information on the valuations of the actual roots, in an algebraic closure. This has aspects both of ramification theory and singularity theory. The valid inferences possible are to the valuations of power sums, by means of Newton's identities.\n\n\n\n"}
{"id": "35093912", "url": "https://en.wikipedia.org/wiki?curid=35093912", "title": "Non-Archimedean geometry", "text": "Non-Archimedean geometry\n\nIn mathematics, non-Archimedean geometry is any of a number of forms of geometry in which the axiom of Archimedes is negated. An example of such a geometry is the Dehn plane. Non-Archimedean geometries may, as the example indicates, have properties significantly different from Euclidean geometry.\n\nThere are two senses in which the term may be used, referring to geometries over fields which violate one of the two senses of the Archimedean property (i.e. with respect to order or magnitude).\n\nThe first sense of the term is the geometry over a non-Archimedean ordered field, or a subset thereof. The aforementioned Dehn plane takes the self-product of the finite portion of a certain non-Archimedean ordered field based on the field of rational functions. In this geometry, there are significant differences from Euclidean geometry; in particular, there are infinitely many parallels to a straight line through a point—so the parallel postulate fails—but the sum of the angles of a triangle is still a straight angle.\n\nIntuitively, in such a space, the points on a line cannot be described by the real numbers or a subset thereof, and there exist segments of \"infinite\" or \"infinitesimal\" length.\n\nThe second sense of the term is the metric geometry over a non-Archimedean valued field, or ultrametric space. In such a space, even more contradictions to Euclidean geometry result. For example, all triangles are isosceles, and overlapping balls nest. An example of such a space is the p-adic numbers.\n\nIntuitively, in such a space, distances fail to \"add up\" or \"accumulate\".\n"}
{"id": "1519351", "url": "https://en.wikipedia.org/wiki?curid=1519351", "title": "Nonmetricity tensor", "text": "Nonmetricity tensor\n\nIn mathematics, the nonmetricity tensor in differential geometry is the covariant derivative of the metric tensor. It is therefore a tensor field of order three. It vanishes for the case of Riemannian geometry.\n"}
{"id": "538132", "url": "https://en.wikipedia.org/wiki?curid=538132", "title": "Octagonal number", "text": "Octagonal number\n\nAn octagonal number is a figurate number that represents an octagon. The octagonal number for \"n\" is given by the formula 3\"n\" - 2\"n\", with \"n\" > 0. The first few octagonal numbers are:\n\n1, 8, 21, 40, 65, 96, 133, 176, 225, 280, 341, 408, 481, 560, 645, 736, 833, 936 \n\nOctagonal numbers can be formed by placing triangular numbers on the four sides of a square. To put it algebraically, the \"n\"-th octagonal number is\n\nThe octagonal number for \"n\" can also be calculated by adding the square of \"n\" to twice the (\"n - 1\")th pronic number.\n\nOctagonal numbers consistently alternate parity.\n\nOctagonal numbers are occasionally referred to as \"star numbers,\" though that term is more commonly used to refer to centered dodecagonal numbers.\n\nSolving the formula for the \"n\"-th octagonal number, formula_2 for \"n\" gives\nAn arbitrary number \"x\" can be checked for octagonality by putting it in this equation. If \"n\" is an integer, then \"x\" is the \"n\"-th octagonal number. If \"n\" is not an integer, then \"x\" is not octagonal.\n\n"}
{"id": "35870410", "url": "https://en.wikipedia.org/wiki?curid=35870410", "title": "Octic equation", "text": "Octic equation\n\nIn algebra, an octic equation is an equation of the form\n\nwhere .\n\nAn octic function is a function of the form\n\nwhere . In other words, it is a polynomial of degree eight. If , then \"f\" is a septic function (), sextic function (), etc.\n\nThe equation may be obtained from the function by setting .\n\nThe \"coefficients\" may be either integers, rational numbers, real numbers, complex numbers or, more generally, members of any field.\n\nSince an octic function is defined by a polynomial with an even degree, it has the same infinite limit when the argument goes to positive or negative infinity. If the leading coefficient is positive, then the function increases to positive infinity at both sides; and thus the function has a global minimum. Likewise, if is negative, the octic function decreases to negative infinity and has a global maximum. The derivative of an octic function is a septic function.\n\nBy the Abel–Ruffini theorem, there is no general algebraic formula for a solution of an octic equation in terms of its parameters. However, some sub-classes of octics do have such formulas.\n\nTrivially, octics of the form\n\nwith positive \"a\" have the solutions \n\nwhere formula_5 is the \"k\"-th eighth root of 1 in the complex plane.\n\nOctics of the form \ncan be solved through factorisation or application of the quadratic formula in the variable . \n\nOctics of the form\n\ncan be solved using the quartic formula in the variable .\n\nIn some cases some of the quadrisections (partitions into four regions of equal area) of a triangle by perpendicular lines are solutions of an octic equation.\n\n"}
{"id": "47548749", "url": "https://en.wikipedia.org/wiki?curid=47548749", "title": "Paul Balmer", "text": "Paul Balmer\n\nPaul Balmer (born 1970) is a Swiss mathematician, working in algebra. He is a professor of mathematics at UCLA.\n\nBalmer received his Ph.D. from the University of Lausanne in 1998, under the supervision of Manuel Ojanguren, with a thesis entitled \"Groupes de Witt dérivés des Schémas\" (in French).\n\nHis research centers around triangulated categories. More specifically, he is a proponent of tensor-triangular geometry, an umbrella topic which covers geometric aspects of algebraic geometry, modular representation theory, stable homotopy theory, and other areas, by means of relevant tensor-triangulated categories.\n\nBalmer was an Invited Speaker at the International Congress of Mathematicians in Hyderabad in 2010, with a talk on \"Tensor Triangular Geometry\".\nIn 2012, he became a fellow of the American Mathematical Society.\nHe was awarded a Humboldt Prize in 2015.\n"}
{"id": "24497161", "url": "https://en.wikipedia.org/wiki?curid=24497161", "title": "Periodic matrix set", "text": "Periodic matrix set\n\nIn mathematics, a periodic matrix set is a set of square matrices in which each square matrix is of a different size, and such that each cell within each matrix within a set contains data associated with some type of periodic distribution. \n\nA set may be specified to contain a fixed number of matrices and is identified by a set number (\"S\"), where \"S\" is the set identification number and \"M\" is the number of matrices included in the set. There is no limit to the number of matrices which may be members of a periodic set.\n\nEach matrix within a set has an identification number (a) and must contain a \"root cell\". A root cell must be located at any corner of a matrix. All root cells must be located at the same corner of each matrix within a single set. A diagonal line drawn from a root cell to the opposite corner of the same matrix is a \"root diagonal\".\n\nThe periodicity is defined by \"partial square rings\" (rings) of cells adjoining a root cell on two sides. All cells within the same ring, (even if they are located in a different matrix) have a similar \"period\". If a matrix contains (n+1) cells then the outermost ring contains \"2n+1\" cells which are all included in the same period. A ring identification number (n) identifies each period. The root cell is also the smallest ring and is identified as; n = 0. Each subsequent ring (1, 2, 3, etc.) has 2n+1 cells (3, 5, 7, etc.).\n\nIndividual cells contained within a ring are identified by their deviation from the root diagonal. Each cell within a ring is assigned a deviation number (D). All cells intersected by the root diagonal have; D = 0. All cell locations in a column deviation have positive values of D. All cell locations in a row deviation have negative values of D.\n\nAny cell within a set will require three numbers for the identification of its location;a is the matrix numbern is the ring numberD is the deviation number\n\nThe cell could also have its location identified as;a is the matrix numberx is the column number (root cell = 0)y is the row number (root cell = 0)\n\nThe two locational systems are analogous to Radial (anD) and Cartesian (axy) systems. Generally this article will use the \"anD\" locational method.\n\nThe contents of any cell must contain data that is periodic in some manner.\n\nCombinations of sets are possible; however each set must be conformable for combination. A resultant set (R) is the combination of N sets each having M matrices.\n\nTwo sets (of compatible construction) may combine so that the root cells on similar sized matrices are adjacent. This is a \"set pair\" and is identified by a \"pair number\" (P). The resultant matrices are not square but are 2n x n rectangular.\n\nFour sets may also combine so that all root cells on similar sized matrices are adjacent. This gives a resultant set of square matrices having an even number of cells on each side. All root cells will form a central 2x2 \"core\" within each resultant matrix. The resultant set is actually two pairs. Each pair forms half of the resultant set. The identifiers (P,S) will tag each quadrant of the resultant set, which is all of the original sets. P = +½ represents the upper pairP = -½ represents the lower pairS = +½ represents the right set of each pair.S = -½ represents the left set of each pair.\n\nFive identifiers are required to locate any cell in R;P is the pair numberS is the set numbera is the matrix numbern is the ring numberD is the deviation number\n\nPeriodic matrix sets have an application to chemistry (for example, in the periodic table) and particle physics (for example, with sub atomic particles). The resultant set R is of special interest.\n\nThe periodic rings may be associated with quantum harmonic oscillation. A quantum harmonic oscillator has energy (E) defined as; E = (n + ½)ћω. Where; ћ = h/2π and h is Planck's constant, and ω is frequency. The number of cells in each period may be written as; 2E/ћω.\n\nThe rings may also be associated with atomic orbitals. If the ring number (n) is equal to the quantum number for orbital angular momentum (the azimuthal number l ), then the rings (0, 1, 2, 3) correspond to the orbitals (s, p, d, f). The ring number is NOT equal to the principal quantum number (n ). The number of cells per ring is half the number of electrons per orbital due to spin duality of the electrons.\n\nThe quantum numbers are; n is the principal quantum numberl is the quantum number for orbital angular momentum (the azimuthal number)ml is the orbital magnetic momentms is the spin magnetic moment\n\nThe spin quantum number (s) is not normally used in chemistry applications as all electrons are; s = ½.\nThe atomic number (Z) may be expressed as a function of energies which in turn are functions of the quantum numbers.\n\nIf a resultant set is R then the locational numbers correspond to the quantum numbers as follows.\nS = ms\nn = l\nD = ml\n\nThe Madelung rule gives the \"P\" and \"a\" relationships. This rule may be generalized as follows; 2a - P = n + l + s\n(2n + 2l + 2s - 4a) = 1\n\nThis generalization may also be obtained from J coupling.\n\nIf; P = -½\nThen; a = ½(n + l)\n\nIf; P = +½\nThen; a = ½(n + l) + ½\n\nThe sub-atomic particles may be grouped as an R combination.\n\nA set is considered to be \"locationally compliant\" if the data contained in each cell is also a function of the location of the cell. Let an R resultant set be populated with atomic numbers. Each cell contains one atomic number (Z). The number in each cell should be a function of the locators of the cell. If a term is associated with each locator then the atomic number will be the sum of all terms and a constant.\n\nZ = Z + Z + Z + Z + Z - ½\n\nThe five locator terms are as follows.\n\nZ = -2a(P+½)\n\nZ = -2(n+½)(S+½)\n\nZ = 4a(a+1)(a+½)/3\n\nZ = -2n(n+½)\n\nZ = (D+½)\n\nThis distribution of atomic numbers in R is a locationally compliant matrix set of the Periodic Table. The following tables show the resultant matrices populated with the atomic numbers.\n\nR showing combined matrices 1 to 4 populated with atomic number (Z)\n\na = 1\na = 2\n\na = 3\n\na = 4\n\n\n"}
{"id": "157935", "url": "https://en.wikipedia.org/wiki?curid=157935", "title": "Plaintext", "text": "Plaintext\n\nIn cryptography, plaintext or cleartext is unencrypted information, as opposed to information encrypted for storage or transmission. \"Plaintext\" usually means unencrypted information pending input into cryptographic algorithms, usually encryption algorithms. \"Cleartext\" usually refers to data that is transmitted or stored unencrypted ('in the clear').\n\nWith the advent of computing, the term \"plaintext\" expanded beyond human-readable documents to mean any data, including binary files, in a form that can be viewed or used without requiring a key or other decryption device. Information—a message, document, file, etc.—if to be communicated or stored in encrypted form is referred to as plaintext.\n\nPlaintext is used as input to an encryption algorithm; the output is usually termed ciphertext, particularly when the algorithm is a cipher. Codetext is less often used, and almost always only when the algorithm involved is actually a code. Some systems use multiple layers of encryption, with the output of one encryption algorithm becoming \"plaintext\" input for the next.\n\nInsecure handling of plaintext can introduce weaknesses into a cryptosystem by letting an attacker bypass the cryptography altogether. Plaintext is vulnerable in use and in storage, whether in electronic or paper format. \"Physical security\" means the securing of information and its storage media from physical, attack—for instance by someone entering a building to access papers, storage media, or computers. Discarded material, if not disposed of securely, may be a security risk. Even shredded documents and erased magnetic media might be reconstructed with sufficient effort.\n\nIf plaintext is stored in a computer file, the storage media, the computer and its components, and all backups must be secure. Sensitive data is sometimes processed on computers whose mass storage is removable, in which case physical security of the removed disk is vital. In the case of securing a computer, useful (as opposed to handwaving) security must be physical (e.g., against burglary, brazen removal under cover of supposed repair, installation of covert monitoring devices, etc.), as well as virtual (e.g., operating system modification, illicit network access, Trojan programs). Wide availability of keydrives, which can plug into most modern computers and store large quantities of data, poses another severe security headache. A spy (perhaps posing as a cleaning person) could easily conceal one, and even swallow it if necessary.\n\nDiscarded computers, disk drives and media are also a potential source of plaintexts. Most operating systems do not actually erase anything—they simply mark the disk space occupied by a deleted file as 'available for use', and remove its entry from the file system directory. The information in a file deleted in this way remains fully present until overwritten at some later time when the operating system reuses the disk space. With even low-end computers commonly sold with many gigabytes of disk space and rising monthly, this 'later time' may be months later, or never. Even overwriting the portion of a disk surface occupied by a deleted file is insufficient in many cases. Peter Gutmann of the University of Auckland wrote a celebrated 1996 paper on the recovery of overwritten information from magnetic disks; areal storage densities have gotten much higher since then, so this sort of recovery is likely to be more difficult than it was when Gutmann wrote. \n\nModern hard drives automatically remap failing sectors, moving data to good sectors. This process makes information on those failing, excluded sectors invisible to the file system and normal applications. Special software, however, can still extract information from them. \n\nSome government agencies (e.g., US NSA) require that personnel physically pulverize discarded disk drives and, in some cases, treat them with chemical corrosives. This practice is not widespread outside government, however. Garfinkel and Shelat (2003) analyzed 158 second-hand hard drives they acquired at garage sales and the like, and found that less than 10% had been sufficiently sanitized. The others contained a wide variety of readable personal and confidential information. See data remanence. \n\nPhysical loss is a serious problem. The US State Department, Department of Defense, and the British Secret Service have all had laptops with secret information, including in plaintext, lost or stolen. Appropriate disk encryption techniques can safeguard data on misappropriated computers or media.\n\nOn occasion, even when data on host systems is encrypted, media that personnel use to transfer data between systems is plaintext because of poorly designed data policy. For example, in October 2007, the HM Revenue and Customs lost CDs that contained the unencryped records of 25 million child benefit recipients in the United Kingdom.\n\nModern cryptographic systems resist known plaintext or even chosen plaintext attacks, and so may not be entirely compromised when plaintext is lost or stolen. Older systems resisted the effects of plaintext data loss on security with less effective techniques—such as padding and Russian copulation to obscure information in plaintext that could be easily guessed.\n\n\n"}
{"id": "555745", "url": "https://en.wikipedia.org/wiki?curid=555745", "title": "Schwarzian derivative", "text": "Schwarzian derivative\n\nIn mathematics, the Schwarzian derivative, named after the German mathematician Hermann Schwarz, is a certain operator that is invariant under all linear fractional transformations. Thus, it occurs in the theory of the complex projective line, and in particular, in the theory of modular forms and hypergeometric functions. It plays an important role in the theory of univalent functions, conformal mapping and Teichmüller spaces.\n\nThe Schwarzian derivative of a holomorphic function of one complex variable is defined by\n\nThe same formula also defines the Schwarzian derivative of a \"C\" function of one real variable.\nThe alternative notation\n\nis frequently used.\n\nThe Schwarzian derivative of any fractional linear transformation\n\nis zero. Conversely, the fractional linear transformations are the only functions with this property. Thus, the Schwarzian derivative precisely measures the degree to which a function fails to be a fractional linear transformation.\n\nIf \"g\" is a fractional linear transformation, then the composition \"g\"  \"f\" has the same Schwarzian derivative as \"f\"; and on the other hand, the Schwarzian derivative of \"f\" \"g\" is given by the chain rule\n\nMore generally, for any sufficiently differentiable functions \"f\" and \"g\"\n\nThis makes the Schwarzian derivative an important tool in one-dimensional dynamics since it implies that all iterates of a function with negative Schwarzian will also have negative Schwarzian.\n\nIntroducing the function of two complex variables\n\nits second mixed partial derivative is given by\n\nand the Schwarzian derivative is given by the formula:\n\nThe Schwarzian derivative has a simple inversion formula, exchanging the dependent and the independent variables. One has\n\nwhich follows from the inverse function theorem, namely that formula_10\n\nThe Schwarzian derivative has a fundamental relation with a second-order linear ordinary differential equation in the complex plane. Let formula_11 and formula_12 be two linearly independent holomorphic solutions of\n\nThen the ratio formula_14 satisfies\n\nover the domain on which formula_11 and formula_12 are defined, and formula_18 The converse is also true: if such a \"g\" exists, and it is holomorphic on a simply connected domain, then two solutions formula_19 and formula_20 can be found, and furthermore, these are unique up to a common scale factor.\n\nWhen a linear second-order ordinary differential equation can be brought into the above form, the resulting \"Q\" is sometimes called the Q-value of the equation.\n\nNote that the Gaussian hypergeometric differential equation can be brought into the above form, and thus pairs of solutions to the hypergeometric equation are related in this way.\n\nIf \"f\" is a holomorphic function on the unit disc, D, then W. Kraus (1932) and Nehari (1949) proved that a \"necessary condition\" for \"f\" to be univalent is\n\nConversely if \"f\"(\"z\") is a holomorphic function on D satisfying\n\nthen Nehari proved that \"f\" is univalent.\n\nIn particular a \"sufficient condition\" for univalence is\n\nThe Schwarzian derivative and associated second order ordinary differential equation can be used to determine the Riemann mapping between the upper half-plane or unit circle and any bounded polygon in the complex plane, the edges of which are circular arcs or straight lines. For polygons with straight edges, this reduces to the Schwarz–Christoffel mapping, which can be derived directly without using the Schwarzian derivative. The \"accessory parameters\" that arise as constants of integration are related to the eigenvalues of the second order differential equation. Already in 1890 Felix Klein had studied the case of quadrilaterals in terms of the Lamé differential equation.\n\nLet Δ be a circular arc polygon with angles α, ..., α in clockwise order. Let \"f\" : H → Δ be a holomorphic map extending continuously to a map between the boundaries. Let the vertices correspond to points \"a\", ..., \"a\" on the real axis. Then \"p\"(\"x\") = \"S\"(\"f\")(\"x\") is real-valued for \"x\" real and not one of the points. By the Schwarz reflection principle \"p\"(\"x\") extends to a rational function on the complex plane with a double pole at \"a\":\n\nThe real numbers β are called \"accessory parameters\". They are subject to \"3\" linear constraints:\n\nwhich correspond to the vanishing of the coefficients of formula_28 and formula_29 in the expansion of \"p\"(\"z\") around \"z\" = ∞. The mapping \"f\"(\"z\") can then be written as\n\nwhere formula_31 and formula_32 are linearly independent holomorphic solutions of the linear second order ordinary differential equation\n\nThere are \"n\"−3 linearly independent accessory parameters, which can be difficult to determine in practise.\n\nFor a triangle, when \"n\" = 3, there are no accessory parameters. The ordinary differential equation is equivalent to the hypergeometric differential equation and \"f\"(\"z\") can be written in terms of hypergeometric functions.\n\nFor a quadrilateral the accessory parameters depend on one independent variable λ. Writing \"U\"(\"z\") = \"q\"(\"z\")\"u\"(\"z\") for a suitable choice of \"q\"(\"z\"), the ordinary differential equation takes the form\n\nThus formula_35 are eigenfunctions of a Sturm-Liouville equation on the interval formula_36. By the Sturm separation theorem, the non-vanishing of formula_32 forces λ to be the lowest eigenvalue.\n\nUniversal Teichmüller space is defined to be the space of real analytic quasiconformal mappings of the unit disc D, or equivalently the upper half-plane H, onto itself, with two mappings considered to be equivalent if on the boundary one is obtained from the other by composition with a Möbius transformation. Identifying D with the lower hemisphere of the Riemann sphere, any quasiconformal self-map \"f\" of the lower hemisphere corresponds naturally to a conformal mapping of the upper hemisphere formula_38 onto itself. In fact formula_38 is determined as the restriction to the upper hemisphere of the solution of the Beltrami differential equation\n\nwhere μ is the bounded measurable function defined by\n\non the lower hemisphere, extended to 0 on the upper hemisphere.\n\nIdentifying the upper hemisphere with D, Lipman Bers used the Schwarzian derivative to define a mapping\n\nwhich embeds universal Teichmüller space into an open subset \"U\" of the space of bounded holomorphic functions \"g\" on D with the uniform norm. Frederick Gehring showed in 1977 that \"U\" is the interior of the closed subset of Schwarzian derivatives of univalent functions.\n\nFor a compact Riemann surface \"S\" of genus greater than 1, its universal covering space is the unit disc D on which its fundamental group Γ acts by Möbius transformations. The Teichmüller space of \"S\" can be identified with the subspace of the universal Teichmüller space invariant under Γ. The holomorphic functions \"g\" have the property that\n\nis invariant under Γ, so determine quadratic differentials on \"S\". In this way, the Teichmüller space of \"S\" is realized as an open subspace of the finite-dimensional complex vector space of quadratic differentials on \"S\".\n\nThe transformation property\n\nallows the Schwarzian derivative to be interpreted as a continuous 1-cocycle or crossed homomorphism of the diffeomorphism group of the circle with coefficients in the module of densities of degree 2 on the circle.\nLet \"F\"(S) be the space of tensor densities of degree λ on S. The group of orientation-preserving diffeomorphisms of S, Diff(S), acts on \"F\"(S) via pushforwards. If \"f\" is an element of Diff(S) then consider the mapping\n\nIn the language of group cohomology the chain-like rule above says that this mapping is a 1-cocycle on Diff(S) with coefficients in \"F\"(S). In fact\n\nand the 1-cocycle generating the cohomology is \"f\" → \"S\"(\"f\"). The computation of 1-cohomology is a particular case of the more general result\n\nNote that if \"G\" is a group and \"M\" a \"G\"-module, then the identity defining a crossed homomorphism \"c\" of \"G\" into \"M\" can be expressed in terms of standard homomorphisms of groups: it is encoded in a homomorphism of \"G\" into the semidirect product \nformula_48 such that the composition of with the projection formula_48 onto \"G\" is the identity map; the correspondence is by the map \"C\"(\"g\") = (\"c\"(\"g\"), \"g\"). The crossed homomorphisms form a vector space and containing as a subspace the coboundary crossed homomorphisms \"b\"(\"g\") = \"g\" ⋅ \"m\" − \"m\" for \"m\" in \"M\". A simple averaging argument shows that, if \"K\" is a compact group and \"V\" a topological vector space on which \"K\" acts continuously, then the higher cohomology groups vanish \"H\"(\"K\", \"V\") = (0) for \"m\" > 0. n particular for 1-cocycles χ with\n\naveraging over \"y\", using left invariant of the Haar measure on \"K\" gives\n\nwith\n\nThus by averaging it may be assumed that \"c\" satisfies the normalisation condition \"c\"(\"x\") = 0 for \"x\" in Rot(S). Note that if any element \"x\" in \"G\" satisifes \"c\"(\"x\") = 0 then \"C\"(\"x\") = (0,\"x\"). But then, since \"C\" is a homomorphism,\n\"C\"(\"xgx\") = \"C\"(\"x\")\"C\"(\"g\")\"C\"(\"x\"), so that \"c\" satisfies the equivariance condition \"c\"(\"xgx\")=\"x\" ⋅ \"c\"(\"g\"). Thus it may be assumed that the cocycle satisifies these normalisation conditions for Rot(S). The Schwarzian derivative in fact vanishes whenever \"x\" is a Möbius transformation corresponding to SU(1,1). The other two 1-cycles discussed below vanish only on Rot(S) (λ = 0, 1). \n\nThere is an infinitesimal version of this result giving a 1-cocycle for Vect(S), the Lie algebra of smooth vector fields, and hence for the Witt algebra, the subalgebra of trigonometric polynomial vector fields. Indeed, when \"G\" is a Lie group and the action of \"G\" on \"M\" is smooth, there is a Lie algebraic version of crossed homomorphism obtained by taking the corresponding homomorphisms of the Lie algebras (the derivatives of the homomotphisms at the identity). This also makes sense for\nDiff(S) and leads to the 1-cocycle \n\nwhich satisfies the identity\n\nIn the Lie algebra case, the coboundary maps have the form \"b\"(\"X\") = \"X\" ⋅ \"m\" for \"m\" in \"M\". In both cases the 1-cohomology is defined as the space of crossed homomorphisms modulo coboundaries. The natural correspondence between group homomorphisms and Lie algebra homomorphisms leads to the \"van Est inclusion map\"\n\nIn this way the calculation can be reduced to that of Lie algebra cohomology. By continuity this reduces to the computation of crossed homomorphisms of the Witt algebra into \"F\"(S). The normalisations conditions on the group crossed homomorphism imply the following additional conditions for :\n\nfor \"x\" in Rot(S). \n\nFollowing the conventions of , a basis of the Witt algebra is given by\n\nso that [\"d\",\"d\"] = (\"m\" – \"n\") \"d\". A basis for the complexification of \"F\"(S) is given by \n\nso that\n\nfor \"g\" in Rot(S) = T. This forces for suitable coefficients . The crossed homomorphism condition\n\nThe condition (\"d\"/\"d\"θ) = 0, implies that \"a\" = 0. From this condition and the recurrence relation, it follows that up to scalar multiples, this has a unique non-zero solution when λ equals 0, 1 or 2 and only the zero solution otherwise. The solution for corresponds to the group 1-cocycle formula_61. The solution for corresponds to the group 1-cocycle (\"f\") = log \"f' \". The corresponding Lie algebra 1-cocycles for λ = 0, 1, 2 are given up to a scalar multiple by \n\nThe crossed homomorphisms in turn give rise to the central extension of Diff(S) and of its Lie algebra Vect(S), the so-called Virasoro algebra.\n\nThe group Diff(S) and its central extension also appear naturally in the context of Teichmüller theory and string theory. In fact the homeomorphisms of S induced by quasiconformal self-maps of D are precisely the quasisymmetric homeomorphisms of S; these are exactly homeomorphisms which do not send four points with cross ratio 1/2 to points with cross ratio near 1 or 0. Taking boundary values, universal Teichmüller can be identified with the quotient of the group of quasisymmetric homeomorphisms QS(S) by the subgroup of Möbius transformations Moeb(S). (It can also be realized naturally as the space of quasicircles in C.) Since\n\nthe homogeneous space Diff(S)/Moeb(S) is naturally a subspace of universal Teichmüller space. It is also naturally a complex manifold and this and other natural geometric structures are compatible with those on Teichmüller space. The dual of the Lie algebra of Diff(S) can be identified with the space of Hill's operators on S\n\nand the coadjoint action of Diff(S) invokes the Schwarzian derivative. The inverse of the diffeomorphism \"f\" sends the Hill's operator to\n\nThe Schwarzian derivative and the other 1-cocycle defined on Diff(S) can be extended to biholomorphic between open sets in the complex plane. In this case the local description leads to the theory of analytic pseudogroups, formalizing the theory of infinite-dimensional groups and Lie algebras first studied by Élie Cartan in the 1910s. This is related to affine and projective structures on Riemann surfaces as well as the theory of Schwarzian or projective connections, discussed by Gunning, Schiffer and Hawley.\n\nA holomorphic pseudogroup Γ on C consists of a collection of biholomorphisms between open sets \"U\" and \"V\" in C which contains the identity maps for each open \"U\", which is closed under restricting to opens, which is closed under composition (when possible), which is closed under taking inverses and such that if a biholomorphisms is locally in Γ, then it too is in Γ. The pseudogroup is said to be \"transitive\" if, given and in C, there is a biholomorphism in Γ such that . A particular case of transitive pseudogroups are those which are \"flat\", i.e. contain all complex translations . Let \"G\" be the group, under composition, of formal power series transformations with . A holomorphic pseudogroup Γ defines a subgroup \"A\" of \"G\", namely the subgroup defined by the Taylor series expansion about 0 (or \"jet\") of elements of Γ with . Conversely if Γ is flat it is uniquely determined by \"A\": a biholomorphism on \"U\" is contained in Γ in if and only if the power series of lies in \"A\" for every in \"U\": in other words the formal power series for at is given by an element of \"A\" with replaced by ; or more briefly all the jets of \"f\" lie in \"A\". \n\nThe group \"G\" has a natural homomorphisms onto the group \"G\" of \"k\"-jets obtained by taking the truncated power series taken up to the term \"z\". This group acts faithfully on the space of polynomials of degree \"k\" (truncating terms of order higher than \"k\"). Truncations similarly define homomorphisms of \"G\" onto \"G\"; the kernel consists of maps \"f\" with \"f\"(\"z\") = \"z\" + \"bz\", so is Abelian. Thus the group \"G\" is solvable, a fact also clear from the fact that it is in triangular form for the basis of monomials. \n\nA flat pseudogroup Γ is said to be \"defined by differential equations\" if there is a finite integer \"k\" such that homomorphism of \"A\" into \"G\" is faithful and the image is a closed subgroup. The smallest such \"k\" is said to be the \"order\" of Γ.\nThere is a complete classification of all subgroups \"A\" that arise in this way which satisfy the additional assumptions that the image of \"A\" in \"G\" is a complex subgroup and that \"G\" equals C*: this implies that the pseudogroup also contains the scaling transformations \"S\"(\"z\") = \"az\" for \"a\" ≠ 0, i.e. contains \"A\" contains every polynomial \"az\" with \"a\" ≠ 0.\n\nThe only possibilities in this case are that \"k\" = 1 and \"A\" = {\"az\": \"a\" ≠ 0}; or that \"k\" = 2 and \"A\" = {\"az\"/(1−\"bz\") : \"a\" ≠ 0}. The former is the pseudogroup defined by affine subgroup of the complex Möbius group (the \"az\" + \"b\" transformations fixing ∞); the latter is the pseudogroup defined by the whole complex Möbius group.\n\nThis classification can easily be reduced to a Lie algebraic problem since the formal Lie algebra formula_66 of \"G\" consists of formal vector fields \"F\"(\"z\") \"d\"/\"dz\" with \"F\" a formal power series. It contains the polynomial vectors fields with basis \"d\" = \"z\" \"d\"/\"dz\" (\"n\" ≥ 0), which is a subalgebra of the Witt algebra. The Lie brackets are given by [\"d\",\"d\"] = (\"n\" − \"m\")\"d\". Again these act on the space of polynomials of degree ≤ \"k\" by differentiation—it can be identified with C<nowiki></nowiki>\"z\"<nowiki></nowiki>/(\"z\")—and the images of \"d\", ..., \"d\" give a basis of the Lie algebra of \"G\". Note that . Let formula_67 denote the Lie algebra of \"A\": it is isomorphic to a subalgebra of the Lie algebra of \"G\". It contains \"d\" and is invariant under Ad(\"S\"). Since formula_67 is a Lie subalgebra of the Witt algebra, the only possibility is that it has basis \"d\" or basis \"d\", \"d\" for some \"n\" ≥ 1. There are corresponding group elements of the form \"f\"(\"z\")= \"z\" + \"bz\" + ... Composing this with translations yields \"T\" ∘ \"f\" ∘ \"T\"(\"z\") = \"cz\" + \"dz\" + ... with \"c\", \"d\" ≠ 0. Unless \"n\" = 2, this contradicts the form of subgroup \"A\"; so \"n\" = 2.\n\nThe Schwarzian derivative is related to the pseudogroup for the complex Möbius group. In fact if \"f\" is a biholomorphism defined on \"V\" then (\"f\") = \"S\"(\"f\") is a quadratic differential on \"V\". If \"g\" is a bihomolorphism defined on \"U\" and \"g\"(\"V\") ⊆ \"U\", \"S\"(\"f\" ∘ \"g\") and \"S\"(\"g\") are quadratic differentials on \"U\"; moreover \"S\"(\"f\") is a quadratic differential on \"V\", so that \"g\"\"S\"(f) is also a quadratic differential on \"U\". The identity\n\nis thus the analogue of a 1-cocycle for the pseudogroup of biholomorphisms with coefficients in holomorphic quadratic differentials. Similarly formula_70 and formula_71 are 1-cocycles for the same pseudogroup with values in holomorphic functions and holomorphic differentials. In general 1-cocycle can be defined for holomorphic differentials of any order so that\n\nApplying the above identity to inclusion maps \"j\", it follows that (\"j\") = 0 ;and hence that if \"f\" is the restriction of \"f\", so that \"f\" ∘ \"j\" = \"f\", then (\"f\") = (\"f\"). On the other hand taking the local holomororphic flow defined by holomorphic vector fields,—the exponential of the vector fields—the holomorphic pseudogroup of local biholomorphisms is generated by holomorphic vector fields. If the 1-cocycle satisfies suitable continuity or analyticity conditions, it induces a 1-cocycle of holomorphic vector fields, also compatible with restriction. Accordingly it defines a 1-cocycle on holomorphic vector fields on C:\n\nRestricting to the Lie algebra of polynomial vector fields with basis \"d\" = \"z\" \"d\"/\"dz\" (\"n\" ≥ -1), these can be determined using the same methods of Lie algebra cohomology (as in the previous section on crossed homomorphisms). There the calculation was for the whole Witt algebra acting on densities of order \"k\", whereas here it is just for a subalgebra acting on holomorphic (or polynomial) differentials of order \"k\". Again, assuming that vanishes on rotations of C, there are non-zero 1-cocycles, unique up to scalar multiples. only for differentials of degree 0, 1 and 2 given by the same derivative formula\n\nwhere \"p\"(\"z\") is a polynomial.\n\nThe 1-cocycles define the three pseudogroups by (\"f\") = 0: this gives the scaling group (\"k\" = 0); the affine group (\"k\" = 1); and the whole complex Möbius group (\"k\" = 2). So these 1-cocycles are the special ordinary differential equations defining the pseudogroup. More significantly they can be used to define corresponding affine or projective structures and connections on Riemann surfaces. If Γ is a pseudogroup of smooth mappings on R, a topological space \"M\" is said to have a Γ-structure if it has a collection of charts \"f\" that are homeomorphisms from open sets \"V\" in \"M\" to open sets \"U\" in R such that, for every non-empty intersection, the natural map from to lies in Γ. This defines the structure of a smooth \"n\"-manifold if Γ consists of local diffeomorphims and a Riemann surface if \"n\" = 2—so that R ≡ C—and Γ consists of biholomorphisms. If Γ is the affine pseudogroup, \"M\" is said to have an affine structure; and if Γ is the Möbius pseudogroup, \"M\" is said to have a projective structure. Thus a genus one surface given as C/Λ for some lattice Λ ⊂ C has an affine structure; and a genus \"p\" > 1 surface given as the quotient of the upper half plane or unit disk by a Fuchsian group has a projective structure.\n\n"}
{"id": "5621883", "url": "https://en.wikipedia.org/wiki?curid=5621883", "title": "Seymour Ginsburg", "text": "Seymour Ginsburg\n\nSeymour Ginsburg (December 12, 1927 – December 5, 2004) was an American pioneer of automata theory, formal language theory, and\ndatabase theory, in particular; and computer science, in general. His work was influential in distinguishing theoretical Computer Science from the disciplines of Mathematics and Electrical Engineering.\n\nDuring his career, Ginsburg published over 100 papers and three books on various topics in theoretical Computer Science.\n\nSeymour Ginsburg received his B.S. from City College of New York in 1948, where along with fellow student Martin Davis he attended an honors mathematics class taught by Emil Post. He earned a Ph.D. in Mathematics from the University of Michigan in 1952, studying under Ben Dushnik.\n\nGinsburg's professional career began in 1951 when he accepted a position as Assistant Professor of Mathematics at the University of Miami in Florida. He turned his attention wholly towards Computer Science in 1955, when he moved to California to work for the Northrop Corporation. He followed this with positions at the National Cash Register Corporation, Hughes Aircraft, and System Development Corporation.\n\nAt SDC, Ginsburg first concentrated on the theory of abstract machines. He subsequently formed and led a research project dedicated to formal language theory and the foundations of Computer Science. Members of the research group included: Sheila Greibach, Michael A. Harrison, Gene Rose, Ed Spanier, and Joe Ullian. The work that came out of this group distinguished Computer Science theory from other fields, putting Ginsburg at the center of what became the theoretical Computer Science community.\n\nIt was during the SDC years that a young Jeff Ullman spent one summer working for Ginsburg, learning both formal language theory and a broad approach to research in Computer Science theory. Al Aho credited Ullman's summer with Ginsburg as being highly influential on Aho's career in Computer Science. In an interview, Aho recalled that there was little Computer Science at Princeton while he was studying for his PhD. However, after Ullman returned from his summer with Ginsburg, he stated that Ullman \"essentially taught Hopcroft, and me, formal language theory\".\n\nGinsburg joined the faculty of University of Southern California in 1966 where he helped to establish the Computer Science department in 1968. He was awarded a Guggenheim Fellowship in 1974 and spent the year touring the world, lecturing on the areas of theoretical Computer Science which he had helped to create. Ginsburg was named the first Fletcher Jones Professor of Computer Science at USC in 1978, a chair he held until his retirement in 1999. He continued his work on formal language theory and automata through the 1970s.\n\nAt USC in the 1980s, Ginsburg created a research group dedicated to Database theory. He organized the first PODS (Symposium on Principles of Database Systems) in Marina del Rey in 1982 and was a moving force at the conference into the 1990s. He was honored with a surprise session at the 1992 PODS on the occasion of his 64th birthday. A festschrift edited by Jeff Ullman was created in his honor for the occasion.\n\nGinsburg's career ended suddenly in 1999 when he was diagnosed with the onset of Alzheimer's disease. He retired from active teaching and became Professor Emeritus of Computer Science at USC. He spent his last years in declining health until dying on December 5, 2004.\n\nGinsburg was remembered fondly in a memorial published in the ACM SIGMOD Record in 2005. Beyond his contributions to Computer Science theory, he was remembered for the clarity of focus he brought to research and the seriousness with which he took his role as an advisor to PhD students. He was also remembered for his generous support of younger researchers. Those who benefitted from Ginsburg's mentorship, who were not also his PhD students, included: Jonathan Goldstine, Sheila Greibach, Michael A. Harrison, Richard Hull, and Jeff Ullman.\n\nGinsburg's early work concentrated on automata theory. In 1958, he proved that \"don't-care\" circuit minimization does not necessarily yield a minimal result. His work in automata theory led the switching theory community into a more theoretical direction. This work culminated in the publication of a book on the mathematics of machines in 1962.\n\nGinsburg turned his attention to formal language theory in the 1960s. He studied context-free grammars and published a well-known comprehensive overview of context-free languages in 1966. Ginsburg was the first to observe the connection between context-free languages and \"ALGOL-like\" languages. This brought the field of formal language theory to bear on programming language research. Ginsburg's results on context-free grammars and push-down acceptors are considered to be some of the deepest and most beautiful in the area. They remain standard tools for many computer scientists working in the areas of formal languages and automata. Many of his papers at this time were co-authored with other prominent formal language researchers, including Sheila Greibach, and Michael A. Harrison.\n\nThe unification of different views of formal systems was a constant theme in Ginsburg's work. In formal language theory his papers examined the relationships between grammar-based systems, acceptor-based systems, and algebraic characterizations of families of languages. The culmination of this work was the creation of one of the deepest branches of Computer Science, Abstract Families of Languages, in collaboration with Sheila Greibach in 1967.\n\nIn 1974, Ginsburg, along with Armin B. Cremers, developed the theory of Grammar Forms.\n\nIn the 1980s, Ginsburg became an early pioneer in the field of Database Theory. He continued to work in this field until his retirement. His professional contributions spanned subjects as diverse as Functional dependency, object histories, spreadsheet histories, Datalog, and data restructuring.\n\n\n"}
{"id": "45254486", "url": "https://en.wikipedia.org/wiki?curid=45254486", "title": "Simultaneous algebraic reconstruction technique", "text": "Simultaneous algebraic reconstruction technique\n\nThe SART algorithm (simultaneous algebraic reconstruction technique), proposed by Anders Andersen and Avinash Kak in 1984, has had a major impact in computerized tomography (CT) imaging applications where the projection data is limited. It generates a good reconstruction in just one iteration and it is superior to standard algebraic reconstruction technique (ART).\n\nAs a measure of its popularity, researchers have proposed various extensions to \nSART: OS-SART, FA-SART, VW-OS-SART, SARTF, etc. Researchers have also studied how SART can best be implemented on different parallel processing architectures. SART and its proposed extensions are used in emission CT in nuclear medicine, dynamic CT, and holographic tomography, and other reconstruction applications. Convergence\nof the SART algorithm was theoretically established in 2004 by Jiang and Wang. Further convergence analysis was done by Yan.\n\nAn application of SART to ionosphere was presented by Hobiger et al. Their method does not use matrix algebra and therefore it can be implemented in a low-level programming language. Its convergence speed is significantly higher than that of classical SART. A discrete version of SART called DART was developed by Batenburg and Sijbers.\n"}
{"id": "896746", "url": "https://en.wikipedia.org/wiki?curid=896746", "title": "Sylver coinage", "text": "Sylver coinage\n\nSylver coinage is a mathematical game for two players, invented by John H. Conway. It is discussed in chapter 18 of\n\"Winning Ways for Your Mathematical Plays\". This article summarizes that chapter. \n\nThe two players take turns naming positive integers that are not the sum of nonnegative multiples of previously named integers. \nAfter 1 is named, all positive integers can be expressed in this way:\n1 = 1, 2 = 1 + 1, 3 = 1 + 1 + 1, etc., ending the game. The player who named 1 loses.\n\nSylver coinage is named after\nJames Joseph Sylvester, who proved that if \"a\" and \"b\"\nare relatively prime positive integers, then (\"a\" − 1)(\"b\"  − 1) − 1 is the largest number that is not a sum of nonnegative multiples of \"a\" and \"b\". Thus, if \"a\" and \"b\" are the first two moves in a game of sylver coinage, this formula gives the largest number that can still be played. More generally, if\nthe greatest common divisor of the moves played so far is \"g\", then only finitely many multiples of \"g\" can remain to be played, and after they are all played then \"g\" must decrease on the next move. Therefore, every game of sylver coinage must eventually end. \nWhen a sylver coinage game has only a finite number of remaining moves, the largest number that can still be played is called the Frobenius number, and finding this number is called the coin problem.\n\nA sample game between A and B:\n\nEach of A's moves was to a winning position.\n\nUnlike many similar mathematical games, sylver coinage has not been completely solved, mainly because many positions have infinitely many possible moves. Furthermore, the main theorem that identifies a class of winning positions, due to R. L. Hutchings, guarantees that such a position has a winning strategy but does not identify the strategy. Hutchings's Theorem states that any of the prime numbers 5, 7, 11, 13, …, wins as a first move, but very little is known about the subsequent winning moves: these are the only winning openings known.\n\nWhen the greatest common divisor of the moves that have been made so far is 1, the remaining set of numbers that can be played will be a finite set, and can be described mathematically as the set of gaps of a numerical semigroup. Some of these finite positions, including all of the positions after the second player has responded to one of Hutchings' winning moves, allow a special move that Sicherman calls an \"ender\".\nAn ender is a number that may only be played immediately: playing any other number would rule it out. If an ender exists, it is always the largest number that can still be played. For instance, after the moves (4,5), the largest number that can still be played is 11. Playing 11 cannot rule out any smaller numbers, but playing any of the smaller available numbers (1, 2, 3, 6, or 7) would rule out playing 11, so 11 is an ender. When an ender exists, the next player can win by following a strategy-stealing argument. If one of the non-ender moves can win, the next player takes that winning move. And if none of the non-ender moves wins, then the next player can win by playing the ender and forcing the other player to make one of the other non-winning moves. However, although this argument proves that the next player can win, it does not identify a winning strategy for the player. After playing a prime number that is 5 or larger as a first move, the first player in a game of sylver coinage can always win by following this (non-constructive) ender strategy on their next turn.\nIf there are any other winning openings, they must be 3-smooth numbers (numbers of the form for non-negative integers and ).\nFor, if any number that is not of this form and is not prime is played, then the second player can win by choosing a large prime factor of .\nThe first few 3-smooth numbers, 1, 2, 3, 4, 6, 8, 9, and 12, are all losing openings, for which complete strategies are known by which the second player can win.\nBy Dickson's lemma (applied to the pairs of exponents of these numbers), only finitely many 3-smooth numbers can be winning openings, but it is not known whether any of them are.\n\n\n"}
{"id": "50818263", "url": "https://en.wikipedia.org/wiki?curid=50818263", "title": "Tree stack automaton", "text": "Tree stack automaton\n\nA tree stack automaton (plural: tree stack automata) is a formalism considered in automata theory. It is a finite state automaton with the additional ability to manipulate a tree-shaped stack. It is an automaton with storage whose storage roughly resembles the configurations of a thread automaton. A restricted class of tree stack automata recognises exactly the languages generated by multiple context-free grammars (or linear context-free rewriting systems).\n\nFor a finite and non-empty set , a \"tree stack over \" is a tuple where\nThe set of all tree stacks over is denoted by .\n\nThe set of \"predicates\" on , denoted by , contains the following unary predicates:\nfor every .\n\nThe set of \"instructions\" on , denoted by , contains the following partial functions:\nfor every positive integer and every .\n\nA \"tree stack automaton\" is a 6-tuple where\n\nA \"configuration of \" is a tuple where\n\nA transition is \"applicable\" to a configuration if\nThe \"transition relation of \" is the binary relation on configurations of that is the union of all the relations for a transition where, whenever is applicable to , we have and is obtained from by removing the prefix .\n\nThe \"language of \" is the set of all words for which there is some state and some tree stack such that where\n\nTree stack automata are equivalent to Turing machines.\n\nA tree stack automaton is called \"-restricted\" for some positive natural number if, during any run of the automaton, any position of the tree stack is accessed at most times from below.\n\n1-restricted tree stack automata are equivalent to pushdown automata and therefore also to context-free grammars.\n-restricted tree stack automata are equivalent to linear context-free rewriting systems and multiple context-free grammars of fan-out at most (for every positive integer ).\n"}
{"id": "39503349", "url": "https://en.wikipedia.org/wiki?curid=39503349", "title": "Tutte embedding", "text": "Tutte embedding\n\nIn graph drawing and geometric graph theory, a Tutte embedding or barycentric embedding of a simple 3-vertex-connected planar graph is a crossing-free straight-line embedding with the properties that the outer face is a convex polygon and that each interior vertex is at the average (or barycenter) of its neighbors' positions. If the outer polygon is fixed, this condition on the interior vertices determines their position uniquely as the solution to a system of linear equations. Solving the equations geometrically produces a planar embedding. Tutte's spring theorem, proven by , states that this unique solution is always crossing-free, and more strongly that every face of the resulting planar embedding is convex. It is called the spring theorem because such an embedding can be found as the equilibrium position for a system of springs representing the edges of the graph.\n\nLet \"G\" be the graph of a cube, and (selecting one of its quadrilateral faces as the outer face) fix the four vertices of the outer face at the four corners of a unit square, the points whose \"x\" and \"y\" coordinates are all four combinations of zero and one.\nThen, if the remaining four vertices are placed at the four points whose \"x\" and \"y\" coordinates are combinations of 1/3 and 2/3, as in the figure, the result will be a Tutte embedding. For, at each interior vertex \"v\" of the embedding, and for each of the two coordinates, the three neighbors of \"v\" have coordinate values that are equal to \"v\", smaller by 1/3, and larger by 1/3; the average of these values is the same as the coordinate value of \"v\" itself.\n\nThe condition that a vertex \"v\" be at the average of its neighbors' positions may be expressed as two linear equations, one for the \"x\" coordinate of \"v\" and another for the \"y\" coordinate of \"v\". For a graph with \"n\" vertices, \"h\" of which are fixed in position on the outer face, there are two equations for each interior vertex and also two unknowns (the coordinates) for each interior vertex. Therefore, this gives a system of linear equations with 2(\"n\" − \"h\") equations in 2(\"n\" − \"h\") unknowns, the solution to which is a Tutte embedding. As showed, for 3-vertex-connected planar graphs, this system is non-degenerate. Therefore, it has a unique solution, and (with the outer face fixed) the graph has a unique Tutte embedding. This embedding can be found in polynomial time by solving the system of equations, for instance by using Gaussian elimination.\n\nBy Steinitz's theorem, the 3-connected planar graphs to which Tutte's spring theorem applies coincide with the polyhedral graphs, the graphs formed by the vertices and edges of a convex polyhedron. According to the Maxwell–Cremona correspondence, a two-dimensional embedding of a planar graph forms the vertical projection of a three-dimensional convex polyhedron if and only if the embedding has an \"equilibrium stress\", an assignment of forces to each edge (affecting both endpoints in equal and opposite directions parallel to the edge) such that the forces cancel at every vertex. For a Tutte embedding, assigning to each edge an attractive force proportional to its length (like a spring) makes the forces cancel at all interior vertices, but this is not necessarily an equilibrium stress at the vertices of the outer polygon. However, when the outer polygon is a triangle, it is possible to assign repulsive forces to its three edges to make the forces cancel there, too. In this way, Tutte embeddings can be used to find Schlegel diagrams of every convex polyhedron. For every 3-connected planar graph \"G\", either \"G\" itself or the dual graph of \"G\" has a triangle, so either this gives a polyhedral representation of \"G\" or of its dual; in the case that the dual graph is the one with the triangle, polarization gives a polyhedral representation of \"G\" itself.\n\nIn geometry processing, Tutte's embedding is used for 2D \"uv\"-parametrization formula_1of 3D surfaces formula_2 most commonly for the cases where the topology of the surface remains the same across formula_2 and formula_1(disk topology). Tutte's method minimizes the total distortion energy of the parametrized space by considering each transformed vertex as a point mass, and edges across the corresponding vertices as springs. The tightness of each spring is determined by the length of the edges in the original 3D surface to preserve the shape. Since it is reasonable to have smaller parametrized edge lengths for the smaller edges of formula_2, and larger parametrized edge lengths for the larger edges of formula_2, the spring constants formula_7are usually taken as the inverse of the absolute distance between the vertices formula_8 in the 3D space.\n\nformula_9\n\nwhere formula_10 represents the set of edges in the original 3D surface. When the weights formula_7 are positive (as is the case above), it is guaranteed that the mapping is bijective without any inversions. But when no further constraints are applied, the solution that minimizes the distortion energy trivially collapses to a single point in the parametrized space. \n\nTherefore, one must provide boundary conditions where a set of known vertices of the 3D surface are mapped to known points in the 2D parametrized space. One common way of choosing such boundary conditions is to consider the vertices on the largest boundary loop of the original 3D surface which can be then constrained to be mapped to the outer ring of a unit disk in the 2D parametrized space. Note that if the 3D surface is a manifold, the boundary edges can be detected by verifying that they belong to only one face of the mesh.\n\nApplications of parametrization in graphics and animation include texture mapping, among many others.\n\n generalized Tutte's spring theorem to graphs on higher-genus surfaces with non-positive curvature, and proved analogous results for graphs embedded on a torus.\n\nLet each remaining vertex of the polytope be free to move in space, and replace each edge of the polytope by a spring. Then, find the minimum-energy configuration of the system of springs. As they show, the system of equations obtained in this way is again non-degenerate, but it is unclear under what conditions this method will find an embedding that realizes all facets of the polytope as convex polyhedra.\n\nThe result that every simple planar graph can be drawn with straight line edges is called Fáry's theorem. The Tutte spring theorem proves this for 3-connected planar graphs, but the result is true more generally for planar graphs regardless of connectivity. Using the Tutte spring system for a graph that is not 3-connected may result in degeneracies, in which subgraphs of the given graph collapse onto a point or a line segment; however, an arbitrary planar graph may be drawn using the Tutte embedding by adding extra edges to make it 3-connected, drawing the resulting 3-connected graph, and then removing the extra edges.\n\nA graph is \"k\"-vertex-connected, but not necessarily planar, if and only if it has an embedding into (\"k\" −1)-dimensional space in which an arbitrary \"k\"-tuple of vertices are placed at the vertices of a simplex and, for each remaining vertex \"v\", the convex hull of the neighbors of \"v\" is full-dimensional with \"v\" in its interior. If such an embedding exists, one can be found by fixing the locations of the chosen \"k\" vertices and solving a system of equations that places each vertex at the average of its neighbors, just as in Tutte's planar embedding.\n\nIn finite element mesh generation, Laplacian smoothing is a common method for postprocessing a generated mesh to improve the quality of its elements; it is particularly popular for quadrilateral meshes, for which other methods such as Lloyd's algorithm for triangular mesh smoothing are less applicable. In this method, each vertex is moved to or towards the average of its neighbors' positions, but this motion is only performed for a small number of iterations, to avoid large distortions of element sizes or (in the case of non-convex mesh domains) tangled non-planar meshes.\n\nForce-directed graph drawing systems continue to be a popular method for visualizing graphs, but these systems typically use more complicated systems of forces that combine attractive forces on graph edges (as in Tutte's embedding) with repulsive forces between arbitrary pairs of vertices. These additional forces may cause the system to have many locally stable configurations rather than, as in Tutte's embedding, a single global solution.\n"}
{"id": "7190735", "url": "https://en.wikipedia.org/wiki?curid=7190735", "title": "Verdier duality", "text": "Verdier duality\n\nIn mathematics, Verdier duality is a duality in sheaf theory that generalizes Poincaré duality for manifolds. Verdier duality was introduced by as an analog for locally compact spaces of the coherent duality for schemes due to Grothendieck. It is commonly encountered when studying constructible or perverse sheaves.\n\nVerdier duality states that certain image functors for sheaves are actually adjoint functors. There are two versions.\n\nGlobal Verdier duality states that for a continuous map formula_1, the derived functor of the direct image with proper supports \"Rf\" has a right adjoint \"f\" in the derived category of sheaves, in other words, for a sheaf formula_2 on X and formula_3 on Y we have \nThe exclamation mark is often pronounced \"shriek\" (slang for exclamation mark), and the maps called \"\"f\" shriek\" or \"\"f\" lower shriek\" and \"\"f\" upper shriek\" – see also shriek map.\n\nLocal Verdier duality states that\nin the derived category of sheaves of \"k\" modules over \"Y\". \nIt is important to note that the distinction between the global and local versions is that the former relates maps between sheaves, whereas the latter relates (complexes of) sheaves directly and so can be evaluated locally. Taking global sections of both sides in the local statement gives global Verdier duality.\n\nThe dualizing complex \"D\" on \"X\" is defined to be\nwhere \"p\" is the map from \"X\" to a point. Part of what makes Verdier duality interesting in the singular setting is that when \"X\" is not a manifold (a graph or singular algebraic variety for example) then the dualizing complex is not quasi-isomorphic to a sheaf concentrated in a single degree. From this perspective the derived category is necessary in the study of singular spaces.\n\nIf \"X\" is a finite-dimensional locally compact space, and \"D\"(\"X\") the bounded derived category of sheaves of abelian groups over \"X\", then the Verdier dual is a contravariant functor\n\ndefined by\n\nIt has the following properties:\n\nPoincaré duality can be derived as a special case of Verdier duality. Here one explicitly calculates cohomology of a space using the machinery of sheaf cohomology.\n\nSuppose \"X\" is a compact orientable \"n\"-dimensional manifold, \"k\" is a field and \"k\" is the constant sheaf on \"X\" with coefficients in \"k\". Let \"f=p\" be the constant map. Global Verdier duality then states\nTo understand how Poincaré duality is obtained from this statement, it is perhaps easiest to understand both sides piece by piece. Let \nbe an injective resolution of the constant sheaf. Then by standard facts on right derived functors \nis a complex whose cohomology is the compactly supported cohomology of \"X\". Since morphisms between complexes of sheaves (or vector spaces) themselves form a complex we find that\nwhere the last non-zero term is in degree 0 and the ones to the left are in negative degree. Morphisms in the derived category are obtained from the homotopy category of chain complexes of sheaves by taking the zeroth cohomology of the complex, i.e.\n\nFor the other side of the Verdier duality statement above, we have to take for granted the fact that when \"X\" is a compact orientable \"n\"-dimensional manifold \nwhich is the dualizing complex for a manifold. Now we can re-express the right hand side as\nWe finally have obtained the statement that\nBy repeating this argument with the sheaf \"k\" replaced with the same sheaf placed in degree \"i\" we get the classical Poincaré duality\n\n\n"}
{"id": "177668", "url": "https://en.wikipedia.org/wiki?curid=177668", "title": "Voronoi diagram", "text": "Voronoi diagram\n\nIn mathematics, a Voronoi diagram is a partitioning of a plane into regions based on distance to points in a specific subset of the plane. That set of points (called seeds, sites, or generators) is specified beforehand, and for each seed there is a corresponding region consisting of all points closer to that seed than to any other. These regions are called Voronoi cells. The Voronoi diagram of a set of points is dual to its Delaunay triangulation.\n\nIt is named after Georgy Voronoi, and is also called a Voronoi tessellation, a Voronoi decomposition, a Voronoi partition, or a Dirichlet tessellation (after Peter Gustav Lejeune Dirichlet). Voronoi diagrams have practical and theoretical applications in a large number of fields, mainly in science and technology, but also in visual art.\nThey are also known as Thiessen polygons.\n\nIn the simplest case, shown in the first picture, we are given a finite set of points {\"p\", …, \"p\"} in the Euclidean plane. In this case each site \"p\" is simply a point, and its corresponding Voronoi cell \"R\" consists of every point in the Euclidean plane whose distance to \"p\" is less than or equal to its distance to any other \"p\". Each such cell is obtained from the intersection of half-spaces, and hence it is a convex polygon. The line segments of the Voronoi diagram are all the points in the plane that are equidistant to the two nearest sites. The Voronoi vertices (nodes) are the points equidistant to three (or more) sites.\n\nLet formula_1 be a metric space with distance function formula_2. Let formula_3 be a set of indices and let formula_4 be a tuple (ordered collection) of nonempty subsets (the sites) in the space formula_5. The Voronoi cell, or Voronoi region, formula_6, associated with the site formula_7 is the set of all points in formula_5 whose distance to formula_7 is not greater than their distance to the other sites formula_10, where formula_11 is any index different from formula_12. In other words, if formula_13 denotes the distance between the point formula_14 and the subset formula_15, then\n\nThe Voronoi diagram is simply the tuple of cells formula_17. In principle, some of the sites can intersect and even coincide (an application is described below for sites representing shops), but usually they are assumed to be disjoint. In addition, infinitely many sites are allowed in the definition (this setting has applications in geometry of numbers and crystallography), but again, in many cases only finitely many sites are considered.\n\nIn the particular case where the space is a finite-dimensional Euclidean space, each site is a point, there are finitely many points and all of them are different, then the Voronoi cells are convex polytopes and they can be represented in a combinatorial way using their vertices, sides, 2-dimensional faces, etc. Sometimes the induced combinatorial structure is referred to as the Voronoi diagram. However, in general the Voronoi cells may not be convex or even connected.\n\nIn the usual Euclidean space, we can rewrite the formal definition in usual terms. Each Voronoi polygon formula_6 is associated with a generator point formula_7.\nLet formula_5 be the set of all points in the Euclidean space. Let formula_21 be a point that generates its Voronoi region formula_22, formula_23 that generates formula_24, and formula_25 that generates formula_26, and so on. Then, as expressed by Tran \"et al\" \"all locations in the Voronoi polygon are closer to the generator point of that polygon than any other generator point in the Voronoi diagram in Euclidean plane\".\n\nAs a simple illustration, consider a group of shops in a city. Suppose we want to estimate the number of customers of a given shop. With all else being equal (price, products, quality of service, etc.), it is reasonable to assume that customers choose their preferred shop simply by distance considerations: they will go to the shop located nearest to them. In this case the Voronoi cell formula_6 of a given shop formula_7 can be used for giving a rough estimate on the number of potential customers going to this shop (which is modeled by a point in our city).\n\nFor most cities, the distance between points can be measured using the familiar\nEuclidean distance: formula_29 or the Manhattan distance:formula_30. The corresponding Voronoi diagrams look different for different distance metrics.\n\n\nInformal use of Voronoi diagrams can be traced back to Descartes in 1644. Peter Gustav Lejeune Dirichlet used 2-dimensional and 3-dimensional Voronoi diagrams in his study of quadratic forms in 1850.\nBritish physician John Snow used a Voronoi diagram in 1854 to illustrate how the majority of people who died in the Broad Street cholera outbreak lived closer to the infected Broad Street pump than to any other water pump.\n\nVoronoi diagrams are named after Russian mathematician Georgy Fedosievych Voronoy who defined and studied the general \"n\"-dimensional case in 1908. Voronoi diagrams that are used in geophysics and meteorology to analyse spatially distributed data (such as rainfall measurements) are called Thiessen polygons after American meteorologist Alfred H. Thiessen. In condensed matter physics, such tessellations are also known as Wigner–Seitz unit cells. Voronoi tessellations of the reciprocal lattice of momenta are called Brillouin zones. For general lattices in Lie groups, the cells are simply called fundamental domains. In the case of general metric spaces, the cells are often called metric fundamental polygons.\nOther equivalent names for this concept (or particular important cases of it): Voronoi polyhedra, Voronoi polygons, domain(s) of influence, Voronoi decomposition, Voronoi tessellation(s), Dirichlet tessellation(s).\n\nVoronoi tessellations of regular lattices of points in two or three dimensions give rise to many familiar tessellations.\n\nFor the set of points (\"x\", \"y\") with \"x\" in a discrete set \"X\" and \"y\" in a discrete set \"Y\", we get rectangular tiles with the points not necessarily at their centers.\n\nAlthough a normal Voronoi cell is defined as the set of points closest to a single point in \"S\", an \"n\"th-order Voronoi cell is defined as the set of points having a particular set of \"n\" points in \"S\" as its \"n\" nearest neighbors. Higher-order Voronoi diagrams also subdivide space.\n\nHigher-order Voronoi diagrams can be generated recursively. To generate the \"n\"-order Voronoi diagram from set \"S\", start with the (\"n\" − 1)-order diagram and replace each cell generated by \"X\" = {\"x\", \"x\", ..., \"x\"} with a Voronoi diagram generated on the set \"S\" − \"X\".\n\nFor a set of \"n\" points the (\"n\" − 1)-order Voronoi diagram is called a farthest-point Voronoi diagram.\n\nFor a given set of points \"S\" = {\"p\", \"p\", ..., \"p\"} the farthest-point Voronoi diagram divides the plane into cells in which the same point of \"P\" is the farthest point. A point of \"P\" has a cell in the farthest-point Voronoi diagram if and only if it is a vertex of the convex hull of \"P\". Let \"H\" = {\"h\", \"h\", ..., \"h\"} be the convex hull of \"P\"; then the farthest-point Voronoi diagram is a subdivision of the plane into \"k\" cells, one for each point in \"H\", with the property that a point \"q\" lies in the cell corresponding to a site \"h\" if and only if d(\"q\", \"h\") > d(\"q\", \"p\") for each \"p\" ∈ \"S\" with \"h\" ≠ \"p\", where d(\"p\", \"q\") is the Euclidean distance between two points \"p\" and \"q\".\n\nThe boundaries of the cells in the farthest-point Voronoi diagram have the structure of a topological tree, with infinite rays as its leaves. Every finite tree is isomorphic to the tree formed in this way from a farthest-point Voronoi diagram.\n\nAs implied by the definition, Voronoi cells can be defined for metrics other than Euclidean, such as the Mahalanobis distance or Manhattan distance. However, in these cases the boundaries of the Voronoi cells may be more complicated than in the Euclidean case, since the equidistant locus for two points may fail to be subspace of codimension 1, even in the 2-dimensional case.\nA weighted Voronoi diagram is the one in which the function of a pair of points to define a Voronoi cell is a distance function modified by multiplicative or additive weights assigned to generator points. In contrast to the case of Voronoi cells defined using a distance which is a metric, in this case some of the Voronoi cells may be empty. A power diagram is a type of Voronoi diagram defined from a set of circles using the power distance; it can also be thought of as a weighted Voronoi diagram in which a weight defined from the radius of each circle is added to the squared distance from the circle's center.\n\nThe Voronoi diagram of \"n\" points in \"d\"-dimensional space requires formula_31 storage space. Therefore, Voronoi diagrams are often not feasible for \"d\" > 2. An alternative is to use approximate Voronoi diagrams, where the Voronoi cells have a fuzzy boundary, which can be approximated.\n\nVoronoi diagrams are also related to other geometric structures such as the medial axis (which has found applications in image segmentation, optical character recognition, and other computational applications), straight skeleton, and zone diagrams. Besides points, such diagrams use lines and polygons as seeds. By augmenting the diagram with line segments that connect to nearest points on the seeds, a planar subdivision of the environment is obtained. This structure can be used as a navigation mesh for path-finding through large spaces. The navigation mesh has been generalized to support 3D multi-layered environments, such as an airport or a multi-storey building.\n\n\n\n\n\n\n\nDirect algorithms:\n\nStarting with a Delaunay triangulation (obtain the dual):\n\n\n\n\n"}
{"id": "32084687", "url": "https://en.wikipedia.org/wiki?curid=32084687", "title": "Weyl module", "text": "Weyl module\n\nIn algebra, a Weyl module is a representation of a reductive algebraic group, introduced by and named after Hermann Weyl. In characteristic 0 these representations are irreducible, but in positive characteristic they can be reducible, and their decomposition into irreducible components can be hard to determine.\n\n\n"}
{"id": "31567349", "url": "https://en.wikipedia.org/wiki?curid=31567349", "title": "Widest path problem", "text": "Widest path problem\n\nIn graph algorithms, the widest path problem is the problem of finding a path between two designated vertices in a weighted graph, maximizing the weight of the minimum-weight edge in the path. The widest path problem is also known as the bottleneck shortest path problem or the maximum capacity path problem. It is possible to adapt most shortest path algorithms to compute widest paths, by modifying them to use the bottleneck distance instead of path length. However, in many cases even faster algorithms are possible.\n\nFor instance, in a graph that represents connections between routers in the Internet, where the weight of an edge represents the bandwidth of a connection between two routers, the widest path problem is the problem of finding an end-to-end path between two Internet nodes that has the maximum possible bandwidth. The smallest edge weight on this path is known as the capacity or bandwidth of the path. As well as its applications in network routing, the widest path problem is also an important component of the Schulze method for deciding the winner of a multiway election, and has been applied to digital compositing, metabolic pathway analysis, and the computation of maximum flows.\n\nA closely related problem, the minimax path problem, asks for the path that minimizes the maximum weight of any of its edges. It has applications that include transportation planning. Any algorithm for the widest path problem can be transformed into an algorithm for the minimax path problem, or vice versa, by reversing the sense of all the weight comparisons performed by the algorithm, or equivalently by replacing every edge weight by its negation.\n\nIn an undirected graph, a widest path may be found as the path between the two vertices in the maximum spanning tree of the graph, and a minimax path may be found as the path between the two vertices in the minimum spanning tree.\n\nIn any graph, directed or undirected, there is a straightforward algorithm for finding a widest path once the weight of its minimum-weight edge is known: simply delete all smaller edges and search for any path among the remaining edges using breadth first search or depth first search. Based on this test, there also exists a linear time algorithm for finding a widest path in an undirected graph, that does not use the maximum spanning tree. The main idea of the algorithm is to apply the linear-time path-finding algorithm to the median edge weight in the graph, and then either to delete all smaller edges or contract all larger edges according to whether a path does or does not exist, and recurse in the resulting smaller graph.\n\nA solution to the minimax path problem between the two opposite corners of a grid graph can be used to find the weak Fréchet distance between two polygonal chains. Here, each grid graph vertex represents a pair of line segments, one from each chain, and the weight of an edge represents the Fréchet distance needed to pass from one pair of segments to another.\n\nIf all edge weights of an undirected graph are positive, then the minimax distances between pairs of points (the maximum edge weights of minimax paths) form an ultrametric; conversely every finite ultrametric space comes from minimax distances in this way. A data structure constructed from the minimum spanning tree allows the minimax distance between any pair of vertices to be queried in constant time per query, using lowest common ancestor queries in a Cartesian tree. The root of the Cartesian tree represents the heaviest minimum spanning tree edge, and the children of the root are Cartesian trees recursively constructed from the subtrees of the minimum spanning tree formed by removing the heaviest edge. The leaves of the Cartesian tree represent the vertices of the input graph, and the minimax distance between two vertices equals the weight of the Cartesian tree node that is their lowest common ancestor. Once the minimum spanning tree edges have been sorted, this Cartesian tree can be constructed in linear time.\n\nIn directed graphs, the maximum spanning tree solution cannot be used. Instead, several different algorithms are known; the choice of which algorithm to use depends on whether a start or destination vertex for the path is fixed, or whether paths for many start or destination vertices must be found simultaneously.\n\nThe all-pairs widest path problem has applications in the Schulze method for choosing a winner in multiway elections in which voters rank the candidates in preference order. The Schulze method constructs a complete directed graph in which the vertices represent the candidates and every two vertices are connected by an edge. Each edge is directed from the winner to the loser of a pairwise contest between the two candidates it connects, and is labeled with the margin of victory of that contest. Then the method computes widest paths between all pairs of vertices, and the winner is the candidate whose vertex has wider paths to each opponent than vice versa. The results of an election using this method are consistent with the Condorcet method – a candidate who wins all pairwise contests automatically wins the whole election – but it generally allows a winner to be selected, even in situations where the Concorcet method itself fails. The Schulze method has been used by several organizations including the Wikimedia Foundation.\n\nTo compute the widest path widths for all pairs of nodes in a dense directed graph, such as the ones that arise in the voting application, the asymptotically fastest known approach takes time where ω is the exponent for fast matrix multiplication. Using the best known algorithms for matrix multiplication, this time bound becomes . Instead, the reference implementation for the Schulze method uses a modified version of the simpler Floyd–Warshall algorithm, which takes time. For sparse graphs, it may be more efficient to repeatedly apply a single-source widest path algorithm.\n\nIf the edges are sorted by their weights, then a modified version of Dijkstra's algorithm can compute the bottlenecks between a designated start vertex and every other vertex in the graph, in linear time. The key idea behind the speedup over a conventional version of Dijkstra's algorithm is that the sequence of bottleneck distances to each vertex, in the order that the vertices are considered by this algorithm, is a monotonic subsequence of the sorted sequence of edge weights; therefore, the priority queue of Dijkstra's algorithm can be implemented as a bucket queue: an array indexed by the numbers from 1 to (the number of edges in the graph), where array cell contains the vertices whose bottleneck distance is the weight of the edge with position in the sorted order. This method allows the widest path problem to be solved as quickly as sorting; for instance, if the edge weights are represented as integers, then the time bounds for integer sorting a list of integers would apply also to this problem.\n\n suggest that service vehicles and emergency vehicles should use minimax paths when returning from a service call to their base. In this application, the time to return is less important than the response time if another service call occurs while the vehicle is in the process of returning. By using a minimax path, where the weight of an edge is the maximum travel time from a point on the edge to the farthest possible service call, one can plan a route that minimizes the maximum possible delay between receipt of a service call and arrival of a responding vehicle. use maximin paths to model the dominant reaction chains in metabolic networks; in their model, the weight of an edge is the free energy of the metabolic reaction represented by the edge.\n\nAnother application of widest paths arises in the Ford–Fulkerson algorithm for the maximum flow problem. Repeatedly augmenting a flow along a maximum capacity path in the residual network of the flow leads to a small bound, , on the number of augmentations needed to find a maximum flow; here, the edge capacities are assumed to be integers that are at most . However, this analysis does not depend on finding a path that has the exact maximum of capacity; any path whose capacity is within a constant factor of the maximum suffices. Combining this approximation idea with the shortest path augmentation method of the Edmonds–Karp algorithm leads to a maximum flow algorithm with running time .\n\nIt is possible to find maximum-capacity paths and minimax paths with a single source and single destination very efficiently even in models of computation that allow only comparisons of the input graph's edge weights and not arithmetic on them. The algorithm maintains a set of edges that are known to contain the bottleneck edge of the optimal path; initially, is just the set of all edges of the graph. At each iteration of the algorithm, it splits into an ordered sequence of subsets of approximately equal size; the number of subsets in this partition is chosen in such a way that all of the split points between subsets can be found by repeated median-finding in time . The algorithm then reweights each edge of the graph by the index of the subset containing the edge, and uses the modified Dijkstra algorithm on the reweighted graph; based on the results of this computation, it can determine in linear time which of the subsets contains the bottleneck edge weight. It then replaces by the subset that it has determined to contain the bottleneck weight, and starts the next iteration with this new set . The number of subsets into which can be split increases exponentially with each step, so the number of iterations is proportional to the iterated logarithm function, , and the total time is . In a model of computation where each edge weight is a machine integer, the use of repeated bisection in this algorithm can be replaced by a list-splitting technique of , allowing to be split into smaller sets in a single step and leading to a linear overall time bound.\n\nA variant of the minimax path problem has also been considered for sets of points in the Euclidean plane. As in the undirected graph problem, this Euclidean minimax path problem can be solved efficiently by finding a Euclidean minimum spanning tree: every path in the tree is a minimax path. However, the problem becomes more complicated when a path is desired that not only minimizes the hop length but also, among paths with the same hop length, minimizes or approximately minimizes the total length of the path. The solution can be approximated using geometric spanners.\n\nIn number theory, the unsolved Gaussian moat problem asks whether or not minimax paths in the Gaussian prime numbers have bounded or unbounded minimax length. That is, does there exist a constant such that, for every pair of points and in the infinite Euclidean point set defined by the Gaussian primes, the minimax path in the Gaussian primes between and has minimax edge length at most ?\n"}
