{"id": "4603366", "url": "https://en.wikipedia.org/wiki?curid=4603366", "title": "188 (number)", "text": "188 (number)\n\n188 (one hundred [and] eighty-eight) is the natural number following 187 and preceding 189.\n\n\n\n\n 188 is also:\n\n\n"}
{"id": "4739452", "url": "https://en.wikipedia.org/wiki?curid=4739452", "title": "189 (number)", "text": "189 (number)\n\n189 (one hundred [and] eighty-nine) is the natural number following 188 and preceding 190.\n\n189 is:\n\nAlso:\n\n\n\n\n\n\n 189 is also:\n\n\n"}
{"id": "3118117", "url": "https://en.wikipedia.org/wiki?curid=3118117", "title": "Adjunction formula", "text": "Adjunction formula\n\nIn mathematics, especially in algebraic geometry and the theory of complex manifolds, the adjunction formula relates the canonical bundle of a variety and a hypersurface inside that variety. It is often used to deduce facts about varieties embedded in well-behaved spaces such as projective space or to prove theorems by induction.\n\nLet \"X\" be a smooth algebraic variety or smooth complex manifold and \"Y\" be a smooth subvariety of \"X\". Denote the inclusion map by \"i\" and the ideal sheaf of \"Y\" in \"X\" by formula_1. The conormal exact sequence for \"i\" is\nwhere Ω denotes a cotangent bundle. The determinant of this exact sequence is a natural isomorphism\nwhere formula_4 denotes the dual of a line bundle.\n\nSuppose that \"D\" is a smooth divisor on \"X\". Its normal bundle extends to a line bundle formula_5 on \"X\", and the ideal sheaf of \"D\" corresponds to its dual formula_6. The conormal bundle formula_7 is formula_8, which, combined with the formula above, gives\nIn terms of canonical classes, this says that\nBoth of these two formulas are called the adjunction formula.\n\nThe restriction map formula_11 is called the Poincaré residue. Suppose that \"X\" is a complex manifold. Then on sections, the Poincaré residue can be expressed as follows. Fix an open set \"U\" on which \"D\" is given by the vanishing of a function \"f\". Any section over \"U\" of formula_5 can be written as \"s\"/\"f\", where \"s\" is a holomorphic function on \"U\". Let η be a section over \"U\" of ω. The Poincaré residue is the map\nthat is, it is formed by applying the vector field ∂/∂\"f\" to the volume form η, then multiplying by the holomorphic function \"s\". If \"U\" admits local coordinates \"z\", ..., \"z\" such that for some \"i\", ∂\"f\"/∂\"z\" ≠ 0, then this can also be expressed as\n\nAnother way of viewing Poincaré residue first reinterprets the adjunction formula as an isomorphism\nOn an open set \"U\" as before, a section of formula_8 is the product of a holomorphic function \"s\" with the form . The Poincaré residue is the map that takes the wedge product of a section of ω and a section of formula_8.\n\nThe adjunction formula is false when the conormal exact sequence is not a short exact sequence. However, it is possible to use this failure to relate the singularities of \"X\" with the singularities of \"D\". Theorems of this type are called inversion of adjunction. They are an important tool in modern birational geometry.\n\nThe genus-degree formula for plane curves can be deduced from the adjunction formula. Let \"C\" ⊂ P be a smooth plane curve of degree \"d\" and genus \"g\". Let \"H\" be the class of a hypersurface in P, that is, the class of a line. The canonical class of P is −3\"H\". Consequently, the adjunction formula says that the restriction of to \"C\" equals the canonical class of \"C\". This restriction is the same as the intersection product restricted to \"C\", and so the degree of the canonical class of \"C\" is . By the Riemann–Roch theorem, \"g\" − 1 = (\"d\" − 3)\"d\" − \"g\" + 1, which implies the formula\n\nSimilarly, if \"C\" is a smooth curve on the quadric surface P×P with bidegree (\"d\",\"d\") (meaning \"d\",\"d\" are its intersection degrees with a fiber of each projection to P), since the canonical class of P×P has bidegree (−2,−2), the adjunction formula shows that the canonical class of \"C\" is the intersection product of divisors of bidegrees (\"d\",\"d\") and (\"d\"−2,\"d\"−2). The intersection form on P×P is formula_19 by definition of the bidegree and by bilinearity, so applying Riemann–Roch gives formula_20 or\nThis gives a simple proof of the existence of curves of any genus as the graph of a function of degree formula_22.\n\nThe genus of a curve \"C\" which is the complete intersection of two surfaces \"D\" and \"E\" in P can also be computed using the adjunction formula. Suppose that \"d\" and \"e\" are the degrees of \"D\" and \"E\", respectively. Applying the adjunction formula to \"D\" shows that its canonical divisor is , which is the intersection product of and \"D\". Doing this again with \"E\", which is possible because \"C\" is a complete intersection, shows that the canonical divisor \"C\" is the product , that is, it has degree . By the Riemann–Roch theorem, this implies that the genus of \"C\" is\nMore generally, if \"C\" is the complete intersection of hypersurfaces of degrees in P, then an inductive computation shows that the canonical class of \"C\" is formula_24. The Riemann–Roch theorem implies that the genus of this curve is\n\n\n"}
{"id": "31698015", "url": "https://en.wikipedia.org/wiki?curid=31698015", "title": "Berlekamp–Welch algorithm", "text": "Berlekamp–Welch algorithm\n\nThe Berlekamp–Welch algorithm, also known as the Welch–Berlekamp algorithm, is named for Elwyn R. Berlekamp and Lloyd R. Welch. This is a decoder algorithm that efficiently corrects errors in Reed–Solomon codes for an RS(\"n\", \"k\"), code based on the Reed Solomon original view where a message formula_1 is used as coefficients of a polynomial formula_2 or used with Lagrange interpolation to generate the polynomial formula_2 of degree < \"k\" for inputs formula_4 and then formula_2 is applied to formula_6 to create an encoded codeword formula_7.\n\nThe goal of the decoder is to recover the original encoding polynomial formula_2, using the known inputs formula_9 and received codeword formula_10 with possible errors. It also computes an error polynomial formula_11 where formula_12 corresponding to errors in the received codeword.\n\nDefining \"e\" = number of errors, the key set of \"n\" equations is\n\nWhere E(\"a\") = 0 for the \"e\" cases when b ≠ F(a), and E(a) ≠ 0 for the \"n\" - \"e\" non error cases where \"b\" = F(\"a\") . These equations can't be solved directly, but by defining Q() as the product of E() and F():\n\nand adding the constraint that the most significant coefficient of E(a) = \"e\" = 1, the result will lead to a set of equations that can be solved with linear algebra.\n\nwhere \"q\" = \"n\" - \"e\" - 1. Since \"e\" is constrained to be 1, the equations become:\n\nresulting in a set of equations which can be solved using linear algebra, with time complexity O(n^3).\n\nThe algorithm begins assuming the maximum number of errors \"e\" = ⌊ (\"n\"-\"k\")/2 ⌋. If the equations can not be solved (due to redundancy), \"e\" is reduced by 1 and the process repeated, until the equations can be solved or \"e\" is reduced to 0, indicating no errors. If Q()/E() has remainder = 0, then F() = Q()/E() and the code word values F(\"a\") are calculated for the locations where E(\"a\") = 0 to recover the original code word. If the remainder ≠ 0, then an uncorrectable error has been detected.\n\nConsider RS(7,3) (\"n\" = 7, \"k\" = 3) defined in with \"α\" = 3 and input values: \"a\" = i-1 : {0,1,2,3,4,5,6}. The message to be systematically encoded is {1,6,3}. Using Lagrange interpolation, \"F(a)\" = 3 x + 2 x + 1, and applying \"F(a)\" for \"a\" = 3 to \"a\" = 6, results in the code word {1,6,3,6,1,2,2}. Assume errors occur at \"c\" and \"c\" resulting in the received code word {1,5,3,6,3,2,2}. Start off with \"e\" = 2 and solve the linear equations:\n\n<br>\n<br>\n\nStarting from the bottom of the right matrix, and the constraint \"e\" = 1:\n\nformula_22\n\nformula_23\n\nformula_24 with remainder = 0.\n\nE(\"a\") = 0 at \"a\" = 1 and \"a\" = 4\nCalculate F(\"a\" = 1) = 6 and F(\"a\" = 4) = 1 to produce corrected code word {1,6,3,6,1,2,2}.\n\n\n"}
{"id": "326483", "url": "https://en.wikipedia.org/wiki?curid=326483", "title": "Brahmagupta–Fibonacci identity", "text": "Brahmagupta–Fibonacci identity\n\nIn algebra, the Brahmagupta–Fibonacci identity expresses the product of two sums of two squares as a sum of two squares in two different ways. Hence the set of all sums of two squares is closed under multiplication. Specifically, the identity says\nFor example,\n\nThe identity is also known as the Diophantus identity, as it was first proved by Diophantus of Alexandria. It is a special case of Euler's four-square identity, and also of Lagrange's identity. \n\nBrahmagupta proved and used a more general identity (the Brahmagupta identity), equivalent to\nThis shows that, for any fixed \"A\", the set of all numbers of the form \"x\" + \"A\" \"y\" is closed under multiplication.\n\nThe identity holds in the ring of integers, the ring of rational numbers and, more generally, any commutative ring. All four forms of the identity can be verified by expanding each side of the equation. Also, (2) can be obtained from (1), or (1) from (2), by changing \"b\" to −\"b\", and likewise with (3) and (4).\n\nThe identity is actually first found in Diophantus' \"Arithmetica\" (III, 19), of the third century A.D.\nIt was rediscovered by Brahmagupta (598–668), an Indian mathematician and astronomer, who generalized it (to the Brahmagupta identity) and used it in his study of what is now called Pell's equation. His \"Brahmasphutasiddhanta\" was translated from Sanskrit into Arabic by Mohammad al-Fazari, and was subsequently translated into Latin in 1126. The identity later appeared in Fibonacci's \"Book of Squares\" in 1225.\n\nAnalogous identities are Euler's four-square related to quaternions, and Degen's eight-square derived from the octonions which has connections to Bott periodicity. There is also Pfister's sixteen-square identity, though it is no longer bilinear.\n\nIf \"a\", \"b\", \"c\", and \"d\" are real numbers, the Brahmagupta–Fibonacci identity is equivalent to the multiplicativity property for absolute values of complex numbers:\n\nThis can be seen as follows: expanding the right side and squaring both sides, the multiplication property is equivalent to\n\nand by the definition of absolute value this is in turn equivalent to\n\nAn equivalent calculation in the case that the variables \"a\", \"b\", \"c\", and \"d\" are rational numbers shows the identity may be interpreted as the statement that the norm in the field Q(\"i\") is multiplicative: the norm is given by \nand the multiplicativity calculation is the same as the preceding one.\n\nIn its original context, Brahmagupta applied his discovery of this identity to the solution of Pell's equation \"x\" − \"Ay\" = 1. Using the identity in the more general form\n\nhe was able to \"compose\" triples (\"x\", \"y\", \"k\") and (\"x\", \"y\", \"k\") that were solutions of \"x\" − \"Ay\" = \"k\", to generate the new triple\n\nNot only did this give a way to generate infinitely many solutions to \"x\" − \"Ay\" = 1 starting with one solution, but also, by dividing such a composition by \"k\"\"k\", integer or \"nearly integer\" solutions could often be obtained. The general method for solving the Pell equation given by Bhaskara II in 1150, namely the chakravala (cyclic) method, was also based on this identity.\n\nWhen used in conjunction with one of Fermat's theorems, the Brahmagupta–Fibonacci identity proves that the product of a square and any number of primes of the form 4\"n\" + 1 is a sum of two squares.\n\n\n"}
{"id": "6172", "url": "https://en.wikipedia.org/wiki?curid=6172", "title": "Cantor set", "text": "Cantor set\n\nIn mathematics, the Cantor set is a set of points lying on a single line segment that has a number of remarkable and deep properties. It was discovered in 1874 by Henry John Stephen Smith and introduced by German mathematician Georg Cantor in 1883.\n\nThrough consideration of this set, Cantor and others helped lay the foundations of modern point-set topology. Although Cantor himself defined the set in a general, abstract way, the most common modern construction is the Cantor ternary set, built by removing the middle thirds of a line segment. Cantor himself mentioned the ternary construction only in passing, as an example of a more general idea, that of a perfect set that is nowhere dense.\n\nThe Cantor ternary set formula_1 is created by iteratively deleting the open middle third from a set of line segments. One starts by deleting the open middle third (, ) from the interval [0, 1], leaving two line segments: [0, ] ∪ [, 1]. Next, the open middle third of each of these remaining segments is deleted, leaving four line segments: [0, ] ∪ [, ] ∪ [, ] ∪ [, 1]. This process is continued ad infinitum, where the \"n\"th set is\n\nThe Cantor ternary set contains all points in the interval [0, 1] that are not deleted at any step in this infinite process:\n\nThe first six steps of this process are illustrated below.\n\nUsing the idea of self-similar transformations, formula_4 and formula_5 the explicit closed formulas for the Cantor set are\nwhere every middle third is removed as the open interval formula_7 from the closed interval formula_8 surrounding it, or\nwhere the middle third formula_10 of the foregoing closed interval formula_11 is removed by intersecting with formula_12\n\nThis process of removing middle thirds is a simple example of a finite subdivision rule. The Cantor ternary set is an example of a fractal string.\n\nIn arithmetical terms, the Cantor set consists of all real numbers on the unit interval that are expressible as a ternary (base 3) fraction using only the digits 0 and 2. As the above diagram illustrates, each point in the Cantor set is uniquely located by a path through an infinitely deep binary tree, where the path turns left or right at each level according to which side of a deleted segment the point lies on. Representing each left turn with 0 and each right turn with 2 yields the ternary fraction for a point. Replacing the \"2\" digits in these fractions with \"1\" digits produces a bijective mapping between the Cantor set and the set of infinite binary sequences.\n\nSince the Cantor set is defined as the set of points not excluded, the proportion (i.e., measure) of the unit interval remaining can be found by total length removed. This total is the geometric progression\n\nSo that the proportion left is 1 – 1 = 0.\n\nThis calculation suggests that the Cantor set cannot contain any interval of non-zero length. It may seem surprising that there should be anything left—after all, the sum of the lengths of the removed intervals is equal to the length of the original interval. However, a closer look at the process reveals that there must be something left, since removing the \"middle third\" of each interval involved removing open sets (sets that do not include their endpoints). So removing the line segment (⁄, ⁄) from the original interval [0, 1] leaves behind the points ⁄ and ⁄. Subsequent steps do not remove these (or other) endpoints, since the intervals removed are always internal to the intervals remaining. So the Cantor set is not empty, and in fact contains an uncountably infinite number of points (as follows from the above description in terms of paths in an infinite binary tree).\n\nIt may appear that \"only\" the endpoints of the construction segments are left, but that is not the case either. The number ⁄, for example, has the unique ternary form 0.020202… = . It is in the bottom third, and the top ninth of that third, and the bottom twenty-seventh of that ninth, and so on. Since it is never in one of the middle segments, it is never removed. Yet it is also not an endpoint of any middle segment, because it is not a multiple of any power of 1/3\n— the set of \"endpoints of segments\" being contained in the set\nwhich is a countably infinite set.\nAs to cardinality, \"most\" elements of the Cantor set are not endpoints of intervals,\nand the whole Cantor set is not countable.\n\nIt can be shown that there are as many points left behind in this process as there were to begin with, and that therefore, the Cantor set is uncountable. To see this, we show that there is a function \"f\" from the Cantor set formula_1 to the closed interval [0,1] that is surjective (i.e. \"f\" maps from formula_1 onto [0,1]) so that the cardinality of formula_1 is no less than that of [0,1]. Since formula_1 is a subset of [0,1], its cardinality is also no greater, so the two cardinalities must in fact be equal, by the Cantor–Bernstein–Schroeder theorem.\n\nTo construct this function, consider the points in the [0, 1] interval in terms of base 3 (or ternary) notation. \nRecall that the proper ternary fractions, more precisely: the elements of formula_19, admit more than one representation in this notation, as for example ⁄, that can be written as 0.1 = , but also as 0.0222... = , and ⁄, that can be written as 0.2 = but also as 0.1222... = .\nWhen we remove the middle third, this contains the numbers with ternary numerals of the form 0.1xxxxx... where xxxxx... is strictly between 00000... and 22222... So the numbers remaining after the first step consist of\nThis can be summarized by saying that those numbers that admit a ternary representation such that the first digit after the radix point is not 1 are the ones remaining after the first step.\n\nThe second step removes numbers of the form 0.01xxxx... and 0.21xxxx..., and (with appropriate care for the endpoints) it can be concluded that the remaining numbers are those with a ternary numeral where neither of the first \"two\" digits is 1.\n\nContinuing in this way, for a number not to be excluded at step \"n\", it must have a ternary representation whose \"n\"th digit is not 1. For a number to be in the Cantor set, it must not be excluded at any step, it must admit a numeral representation consisting entirely of 0s and 2s.\n\nIt is worth emphasising that numbers like 1, ⁄ = 0.1 and ⁄ = 0.21 are in the Cantor set, as they have ternary numerals consisting entirely of 0s and 2s: 1 = 0.222... = , ⁄ = 0.0222... = and ⁄ = 0.20222... = .\nAll the latter numbers are “endpoints”, here: right limit points of formula_1. The same is true for the left limit points of formula_1, e. g. ⁄ = 0.1222... = = and ⁄ = 0.21222... = = . All these endpoints are \"proper ternary\" fractions (elements of formula_22) of the form ⁄, where when irreducible the denominator \"q\" is a power of 3. The ternary representation of these fractions terminates (i. e. is finite) or — what is the same — is infinite and “ends” in either infinitely many recurring 0s or infinitely many recurring 2s. And such a fraction is a left limit point of formula_1 if its ternary representation avoiding any 1 “ends” in infinitely many recurring 0s and a right limit point of formula_1 if its 1 avoiding ternary numeral “ends” in infinitely many recurring 2s.\n\nThis set of endpoints is dense in formula_1 (but not dense in [0, 1]) and makes up a countably infinite set. The numbers in formula_1 which are \"not\" endpoints also have only 0s and 2s in their ternary representation, but there is \"no\" digit position from which on there are \"only\" digits 0 resp. \"only\" digits 2, i. e. there is no trailing infinite repetition of the digit 0 nor of the digit 2.\n\nThe function from formula_1 to [0,1] is defined by taking the ternary numeral that does consist entirely of 0s and 2s, replacing all the 2s by 1s, and interpreting the sequence as a binary representation of a real number. In a formula,\n\nFor any number \"y\" in [0,1], its binary representation can be translated into a ternary representation of a number \"x\" in formula_1 by replacing all the 1s by 2s. With this, \"f\"(\"x\") = \"y\" so that \"y\" is in the range of \"f\". For instance if \"y\" = ⁄ = 0.100110011001... = , we write \"x\" = = 0.200220022002... = ⁄. Consequently, \"f\" is surjective. However, \"f\" is \"not\" injective — the values for which \"f\"(\"x\") coincides are those at opposing ends of one of the \"middle thirds\" removed. For instance, take\nso\nThus there are as many points in the Cantor set as there are in the interval [0, 1] (which has the uncountable cardinality However, the set of endpoints of the removed intervals is countable, so there must be uncountably many numbers in the Cantor set which are not interval endpoints. As noted above, one example of such a number is ⁄, which can be written as 0.020202... = in ternary notation. In fact, given any formula_34, there exist formula_35 such that formula_36. This was first demonstrated by Steinhaus in 1917, who proved, via a geometric argument, the equivalent assertion that formula_37 for every formula_34. Since this construction provides an injection from formula_39 to formula_40, we have formula_41 as an immediate corollary. Assuming that formula_42 for any infinite set formula_43 (a statement shown to be equivalent to the axiom of choice by Tarski), this provides another demonstration that formula_44.\n\nThe Cantor set contains as many points as the interval from which it is taken, yet itself contains no interval of nonzero length. The irrational numbers have the same property, but the Cantor set has the additional property of being closed, so it is not even dense in any interval, unlike the irrational numbers which are dense in every interval.\n\nIt has been conjectured that all algebraic irrational numbers are normal. Since members of the Cantor set are not normal, this would imply that all members of the Cantor set are either rational or transcendental.\n\nThe Cantor set is the prototype of a fractal. It is self-similar, because it is equal to two copies of itself, if each copy is shrunk by a factor of 3 and translated. More precisely, the Cantor set is equal to the union of two functions, the left and right self-similarity transformations of itself, formula_45 and formula_46, which leave the Cantor set invariant up to homeomorphism: formula_47 \n\nRepeated iteration of formula_48 and formula_49 can be visualized as an infinite binary tree. That is, at each node of the tree, one may consider the subtree to the left or to the right. Taking the set formula_50 together with function composition forms a monoid, the dyadic monoid.\n\nThe automorphisms of the binary tree are its hyperbolic rotations, and are given by the modular group. Thus, the Cantor set is a homogeneous space in the sense that for any two points formula_51 and formula_52 in the Cantor set formula_1, there exists a homeomorphism formula_54 with formula_55. An explicit construction of formula_56 can be described more easily if we see the Cantor set as a product space of countably many copies of the discrete space formula_57. Then the map formula_58 defined by formula_59 is an involutive homeomorphism exchanging formula_51 and formula_52.\n\nIt has been found that some form of conservation law is always responsible behind scaling and self-similarity. In the case of Cantor set it can be seen that the formula_62th moment (where formula_63 is the fractal dimension) of all the surviving intervals at any stage of the construction process is equal to constant which is equal to one in the case of Cantor set \n. We know that there are formula_64 intervals of size formula_65\npresent in the system at the formula_66th step of its construction. Then if we level the surviving intervals\nas formula_67 then the formula_62th moment is formula_69 since formula_70.\n\nThe Hausdorff dimension of the Cantor set is equal to ln(2)/ln(3) ≈ 0.631.\n\nAn essential property of Cantor sets is giving sufficiency of fractals for any given Hausdorff dimension formula_71:\n\nTheorem. For any given formula_72 there are uncountable fractals with Hausdorff dimension formula_71 in n-dimensional Euclidean space formula_74 \n\nAlthough \"the\" Cantor set typically refers to the original, middle-thirds Cantor described above, topologists often talk about \"a\" Cantor set, which means any topological space that is homeomorphic (topologically equivalent) to it.\n\nAs the above summation argument shows, the Cantor set is uncountable but has Lebesgue measure 0. Since the Cantor set is the complement of a union of open sets, it itself is a closed subset of the reals, and therefore a complete metric space. Since it is also totally bounded, the Heine–Borel theorem says that it must be compact.\n\nFor any point in the Cantor set and any arbitrarily small neighborhood of the point, there is some other number with a ternary numeral of only 0s and 2s, as well as numbers whose ternary numerals contain 1s. Hence, every point in the Cantor set is an accumulation point (also called a cluster point or limit point) of the Cantor set, but none is an interior point. A closed set in which every point is an accumulation point is also called a perfect set in topology, while a closed subset of the interval with no interior points is nowhere dense in the interval.\n\nEvery point of the Cantor set is also an accumulation point of the complement of the Cantor set.\n\nFor any two points in the Cantor set, there will be some ternary digit where they differ — one will have 0 and the other 2. By splitting the Cantor set into \"halves\" depending on the value of this digit, one obtains a partition of the Cantor set into two closed sets that separate the original two points. In the relative topology on the Cantor set, the points have been separated by a clopen set. Consequently, the Cantor set is totally disconnected. As a compact totally disconnected Hausdorff space, the Cantor set is an example of a Stone space.\n\nAs a topological space, the Cantor set is naturally homeomorphic to the product of countably many copies of the space formula_75, where each copy carries the discrete topology. This is the space of all sequences in two digits \n\nwhich can also be identified with the set of 2-adic integers. The basis for the open sets of the product topology are cylinder sets; the homeomorphism maps these to the subspace topology that the Cantor set inherits from the natural topology on the real number line. This characterization of the Cantor space as a product of compact spaces gives a second proof that Cantor space is compact, via Tychonoff's theorem.\n\nFrom the above characterization, the Cantor set is homeomorphic to the p-adic integers, and, if one point is removed from it, to the p-adic numbers.\n\nThe Cantor set is a subset of the reals, which are a metric space with respect to the ordinary distance metric; therefore the Cantor set itself is a metric space, by using that same metric. Alternatively, one can use the p-adic metric on formula_77: given two sequences formula_78, the distance between them is formula_79, where formula_80 is the smallest index such that formula_81; if there is no such index, then the two sequences are the same, and one defines the distance to be zero. These two metrics generate the same topology on the Cantor set.\n\nWe have seen above that the Cantor set is a totally disconnected perfect compact metric space. Indeed, in a sense it is the only one: every nonempty totally disconnected perfect compact metric space is homeomorphic to the Cantor set. See Cantor space for more on spaces homeomorphic to the Cantor set.\n\nThe Cantor set is sometimes regarded as \"universal\" in the category of compact metric spaces, since any compact metric space is a continuous image of the Cantor set; however this construction is not unique and so the Cantor set is not universal in the precise categorical sense. The \"universal\" property has important applications in functional analysis, where it is sometimes known as the \"representation theorem for compact metric spaces\".\n\nFor any integer \"q\" ≥ 2, the topology on the group G=Z (the countable direct sum) is discrete. Although the Pontrjagin dual Γ is also Z, the topology of Γ is compact. One can see that Γ is totally disconnected and perfect - thus it is homeomorphic to the Cantor set. It is easiest to write out the homeomorphism explicitly in the case \"q\"=2. (See Rudin 1962 p 40.)\n\nThe geometric mean of the Cantor set is approximately 0.274974.\n\nThe Cantor set can be seen as the compact group of binary sequences, and as such, it is endowed with a natural Haar measure. When normalized so that the measure of the set is 1, it is a model of an infinite sequence of coin tosses. Furthermore, one can show that the usual Lebesgue measure on the interval is an image of the Haar measure on the Cantor set, while the natural injection into the ternary set is a canonical example of a singular measure. It can also be shown that the Haar measure is an image of any probability, making the Cantor set a universal probability space in some ways.\n\nIn Lebesgue measure theory, the Cantor set is an example of a set which is uncountable and has zero measure.\n\nIf we define a Cantor number as a member of the Cantor set, then\n\nInstead of repeatedly removing the middle third of every piece as in the Cantor set, we could also keep removing any other fixed percentage (other than 0% and 100%) from the middle. In the case where the middle ⁄ of the interval is removed, we get a remarkably accessible case — the set consists of all numbers in [0,1] that can be written as a decimal consisting entirely of 0s and 9s.\n\nBy removing progressively smaller percentages of the remaining pieces in every step, one can also construct sets homeomorphic to the Cantor set that have positive Lebesgue measure, while still being nowhere dense. See Smith–Volterra–Cantor set for an example.\n\nOne can modify the construction of the Cantor set by dividing randomly instead of equally. Besides, to incorporate time we can divide only one of the available intervals at each step instead of dividing all the available intervals. In the case of stochastic triadic Cantor set the resulting process can be described by the following rate equation\n\nand for the stochastic dyadic Cantor set\n\nwhere formula_84 is the number of intervals of size between formula_51 and formula_86. In the case of triadic Cantor set the fractal dimension is formula_87 which is \nless than its deterministic counterpart formula_88. In the case of stochastic dyadic Cantor set\nthe fractal dimension is formula_89 which is again less than that of its deterministic counterpart formula_90. In the case of stochastic dyadic Cantor set the solution for formula_91 exhibits dynamic scaling as its solution in the long-time limit is formula_92 where the fractal dimension of the stochastic dyadic Cantor set formula_93. In either case, like triadic Cantor set, the formula_62th moment (formula_95) of stochastic triadic and dyadic Cantor set too are conserved quantities.\n\nCantor dust is a multi-dimensional version of the Cantor set. It can be formed by taking a finite Cartesian product of the Cantor set with itself, making it a Cantor space. Like the Cantor set, Cantor dust has zero measure.\n\nA different 2D analogue of the Cantor set is the Sierpinski carpet, where a square is divided up into nine smaller squares, and the middle one removed. The remaining squares are then further divided into nine each and the middle removed, and so on ad infinitum. The 3D analogue of this is the Menger sponge.\n\nCantor himself defined the set in a general, abstract way, and mentioned the ternary construction only in passing, as an example of a more general idea, that of a perfect set that is nowhere dense. The original paper provides several different constructions of the abstract concept.\n\nThis set would have been considered abstract at the time when Cantor devised it. Cantor himself was led to it by practical concerns about the set of points where a trigonometric series might fail to converge. The discovery did much to set him on the course for developing an abstract, general theory of infinite sets.\n\nA column capital from the Ancient Egyptian site of the island of Philae carries a pattern which resembles the Cantor set. Cantor may have seen the image, as his cousin was an Egyptologist.\n\n\n"}
{"id": "2888207", "url": "https://en.wikipedia.org/wiki?curid=2888207", "title": "Cecil C. Rousseau", "text": "Cecil C. Rousseau\n\nCecil Clyde Rousseau (born January 13, 1938 in Philadelphia) is a mathematician and author who specializes in graph theory and combinatorics. He is a professor emeritus at The University of Memphis and former chair of the USAMO.\n\nRousseau received his Ph.D. in 1968 from Texas A&M University.\n\nHe has an Erdős number of 1, and is Erdős' 5th most common co-author, with 35 joint papers.\n\nIn 2012, Rousseau received the Paul Erdős Award from the World Federation of National Mathematics Competitions.\n\nTo his students and colleagues, he's known affectionately as C²R.\n\n"}
{"id": "5816420", "url": "https://en.wikipedia.org/wiki?curid=5816420", "title": "Center of curvature", "text": "Center of curvature\n\nIn geometry, the center of curvature of a curve is found at a point that is at a distance from the curve equal to the radius of curvature lying on the normal vector. It is the point at infinity if the curvature is zero. The osculating circle to the curve is centered at the centre of curvature. Cauchy defined the center of curvature \"C\" as the intersection point of two infinitely close normal lines to the curve. The locus of centers of curvature for each point on the curve comprise the evolute of the curve. This term is generally used in Physics regarding to study of the lenses.\n\nIt can also be defined as the spherical distance between the point at which all the rays, falling on the lens either seems to converge to it (in case of Convex Lens) or diverge from it (in case of Concave Lens) and the lens itself.\n\n"}
{"id": "17040082", "url": "https://en.wikipedia.org/wiki?curid=17040082", "title": "Centerpoint (geometry)", "text": "Centerpoint (geometry)\n\nIn statistics and computational geometry, the notion of centerpoint is a generalization of the median to data in higher-dimensional Euclidean space. Given a set of points in \"d\"-dimensional space, a centerpoint of the set is a point such that any hyperplane that goes through that point divides the set of points in two roughly equal subsets: the smaller part should have at least a 1/(\"d\" + 1) fraction of the points. Like the median, a centerpoint need not be one of the data points. Every non-empty set of points (with no duplicates) has at least one centerpoint.\n\nClosely related concepts are the Tukey depth of a point (the minimum number of sample points on one side of a hyperplane through the point) and a Tukey median of a point set (a point maximizing the Tukey depth). A centerpoint is a point of depth at least \"n\"/(\"d\" + 1), and a Tukey median must be a centerpoint, but not every centerpoint is a Tukey median. Both terms are named after John Tukey.\n\nFor a different generalization of the median to higher dimensions, see geometric median.\n\nA simple proof of the existence of a centerpoint may be obtained using Helly's theorem. Suppose there are \"n\" points, and consider the family of closed half-spaces that contain more than \"dn\"/(\"d\" + 1) of the points. Fewer than \"n\"/(\"d\" + 1) points are excluded from any one of these halfspaces, so the intersection of any subset of \"d\" + 1 of these halfspaces must be nonempty. By Helly's theorem, it follows that the intersection of all of these halfspaces must also be nonempty. Any point in this intersection is necessarily a centerpoint.\n\nFor points in the Euclidean plane, a centerpoint may be constructed in linear time. In any dimension \"d\", a Tukey median (and therefore also a centerpoint) may be constructed in time O(\"n\" + \"n\" log \"n\").\n\nA randomized algorithm that repeatedly replaces sets of \"d\" + 2 points by their Radon point can be used to compute an approximation to a centerpoint of any point set, in the sense that its Tukey depth is linear in the sample set size, in an amount of time that is polynomial in both the number of points and the dimension.\n\n"}
{"id": "1384568", "url": "https://en.wikipedia.org/wiki?curid=1384568", "title": "Character group", "text": "Character group\n\nIn mathematics, a character group is the group of representations of a group by complex-valued functions. These functions can be thought of as one-dimensional matrix representations and so are special cases of the group characters that arise in the related context of character theory. Whenever a group is represented by matrices, the function defined by the trace of the matrices is called a character; however, these traces \"do not\" in general form a group. Some important properties of these one-dimensional characters apply to characters in general:\nThe primary importance of the character group for finite abelian groups is in number theory, where it is used to construct Dirichlet characters. The character group of the cyclic group also appears in the theory of the discrete Fourier transform. For locally compact abelian groups, the character group (with an assumption of \ncontinuity) is central to Fourier analysis.\n\nLet \"G\" be an abelian group. A function formula_1 mapping the group to the non-zero complex numbers is called a character of \"G\" if it is a group homomorphism from formula_2 to formula_3—that is, if formula_4.\n\nIf \"f\" is a character of a finite group \"G\", then each function value \"f\"(\"g\") is a root of unity (since formula_5 such that formula_6, formula_7).\n\nEach character \"f\" is a constant on conjugacy classes of \"G\", that is, \"f\"(\"h\" \"g\" \"h\") = \"f\"(\"g\"). For this reason, the character is sometimes called the class function.\n\nA finite abelian group of order \"n\" has exactly \"n\" distinct characters. These are denoted by \"f\", ..., \"f\". The function \"f\" is the trivial representation; that is, formula_8. It is called the principal character of G; the others are called the non-principal characters. The non-principal characters have the property that formula_9 for some formula_10.\n\nIf \"G\" is an abelian group of order \"n\", then the set of characters \"f\" forms an abelian group under multiplication formula_11 for each element formula_10. This group is the character group of G and is sometimes denoted as formula_13. It is of order \"n\". The identity element of formula_13 is the principal character \"f\". The inverse of \"f\" is the reciprocal 1/\"f\". Note that since formula_15, the inverse is equal to the complex conjugate.\n\nConsider the formula_16 matrix \"A\"=\"A\"(\"G\") whose matrix elements are formula_17 where formula_18 is the \"k\"th element of \"G\". \n\nThe sum of the entries in the \"j\"th row of \"A\" is given by\n\nThe sum of the entries in the \"k\"th column of \"A\" is given by\n\nLet formula_25 denote the conjugate transpose of \"A\". Then \nThis implies the desired orthogonality relationship for the characters: i.e.,\n\nwhere formula_28 is the Kronecker delta and formula_29 is the complex conjugate of formula_30.\n\n\n"}
{"id": "16648043", "url": "https://en.wikipedia.org/wiki?curid=16648043", "title": "Circle packing theorem", "text": "Circle packing theorem\n\nThe circle packing theorem (also known as the Koebe–Andreev–Thurston theorem) describes the possible tangency relations between circles in the plane whose interiors are disjoint. A circle packing is a connected collection of circles (in general, on any Riemann surface) whose interiors are disjoint. The intersection graph of a circle packing is the graph having a vertex for each circle, and an edge for every pair of circles that are tangent. If the circle packing is on the plane, or, equivalently, on the sphere, then its intersection graph is called a coin graph; more generally, intersection graphs of interior-disjoint geometric objects are called tangency graphs or contact graphs. Coin graphs are always connected, simple, and planar. The circle packing theorem states that these are the only requirements for a graph to be a coin graph:\n\nCircle packing theorem: For every connected simple planar graph \"G\" there is a circle packing in the plane whose intersection graph is (isomorphic to) \"G\".\n\nA maximal planar graph \"G\" is a finite simple planar graph to which no more edges can be added while preserving planarity. Such a graph always has a unique planar embedding, in which every face of the embedding (including the outer face) is a triangle. In other words, every maximal planar graph \"G\" is the 1-skeleton of a simplicial complex which is homeomorphic to the sphere. The circle packing theorem guarantees the existence of a circle packing with finitely many circles whose intersection graph is isomorphic to \"G\". As the following theorem states more formally, every maximal planar graph can have at most one packing.\n\nKoebe–Andreev–Thurston theorem: If \"G\" is a finite maximal planar graph, then the circle packing whose tangency graph is isomorphic to \"G\" is unique, up to Möbius transformations and reflections in lines.\n\nThurston observes that this uniqueness is a consequence of the Mostow rigidity theorem. To see this, let \"G\" be represented by a circle packing. Then the plane in which the circles are packed may be viewed as the boundary of a halfspace model for three-dimensional hyperbolic space; with this view, each circle is the boundary of a plane within the hyperbolic space. One can define a set of disjoint planes in this way from the circles of the packing, and a second set of disjoint planes defined by the circles that circumscribe each triangular gap between three of the circles in the packing. These two sets of planes meet at right angles, and form the generators of a reflection group whose fundamental domain can be viewed as a hyperbolic manifold. By Mostow rigidity, the hyperbolic structure of this domain is uniquely determined, up to isometry of the hyperbolic space; these isometries, when viewed in terms of their actions on the Euclidean plane on the boundary of the half-plane model, translate to Möbius transformations.\n\nThere is also a more elementary proof of the same uniqueness property, based on the maximum principle and on the observation that, in the triangle connecting the centers of three mutually tangent circles, the\nangle formed at the center of one of the circles is monotone decreasing in its radius and monotone increasing in the two other radii. Given two packings for the same graph \"G\", one may apply reflections and Möbius transformations to make the outer circles in these two packings correspond to each other and have the same radii. Then, let \"v\" be an interior vertex of \"G\" for which the circles in the two packings have sizes that are as far apart as possible: that is, choose \"v\" to maximize the ratio \"r\"/\"r\" of the radii of its circles in the two packings. For each triangular face of \"G\" containing \"v\", it follows that the angle at the center of the circle for \"v\" in the first packing is less than or equal to the angle in the second packing, with equality possible only when the other two circles forming the triangle have the same ratio \"r\"/\"r\" of radii in the two packings. But the sum of the angles of all of these triangles surrounding the center of the triangle must be 2π in both packings, so all neighboring vertices to \"v\" must have the same ratio as \"v\" itself. By applying the same argument to these other circles in turn, it follows that all circles in both packings have the same ratio. But the outer circles have been transformed to have ratio 1, so \"r\"/\"r\" = 1 and the two packings have identical radii for all circles.\n\nA conformal map between two open sets in the plane or in a higher-dimensional space is a continuous function from one set to the other that preserves the angles between any two curves. The Riemann mapping theorem, formulated by Bernhard Riemann in 1851, states that, for any two open topological disks in the plane, there is a conformal map from one disk to the other. Conformal mappings have applications in mesh generation, map projection, and other areas. However, it is not always easy to construct a conformal mapping between two given domains in an explicit way.\n\nAt the Bieberbach conference in 1985, William Thurston conjectured that circle packings could be used to approximate conformal mappings. More precisely, Thurston used circle packings to find a conformal mapping from an arbitrary open disk \"A\" to the interior of a circle; the mapping from one topological disk \"A\" to another disk \"B\" could then be found by composing the map from \"A\" to a circle with the inverse of the map from \"B\" to a circle.\n\nThurston's idea was to pack circles of some small radius \"r\" in a hexagonal tessellation of the plane, within region \"A\", leaving a narrow region near the boundary of \"A\", of width \"r\", where no more circles of this radius can fit. He then constructs a maximal planar graph \"G\" from the intersection graph of the circles, together with one additional vertex adjacent to all the circles on the boundary of the packing. By the circle packing theorem, this planar graph can be represented by a circle packing \"C\" in which all the edges (including the ones incident to the boundary vertex) are represented by tangencies of circles. The circles from the packing of \"A\" correspond one-for-one with the circles from \"C\", except for the boundary circle of \"C\" which corresponds to the boundary of \"A\". This correspondence of circles can be used to construct a continuous function from \"A\" to \"C\" in which each circle and each gap between three circles is mapped from one packing to the other by a Möbius transformation. Thurston conjectured that, in the limit as the radius \"r\" approaches zero, the functions from \"A\" to \"C\" constructed in this way would approach the conformal function given by the Riemann mapping theorem.\n\nThurston's conjecture was proven by . More precisely, they showed that, as \"n\" goes to infinity, the function \"f\" determined using Thurston's method from hexagonal packings of radius-1/\"n\" circles converges uniformly on compact subsets of \"A\" to a conformal map from \"A\" to \"C\".\n\nDespite the success of Thurston's conjecture, practical applications of this method have been hindered by the difficulty of computing circle packings and by its relatively slow convergence rate. However it has some advantages when applied to non-simply-connected domains and in selecting initial approximations for numerical techniques that compute Schwarz–Christoffel mappings, a different technique for conformal mapping of polygonal domains.\n\nThere are many known proofs of the circle packing theorem. Paul Koebe's original proof is\nbased on his conformal uniformization theorem saying that a finitely connected planar domain\nis conformally equivalent to a circle domain. There are several different topological proofs\nthat are known. Thurston's proof is based on Brouwer's fixed point theorem. \nThere is also a proof using a discrete variant of Perron's method of constructing solutions to the\nDirichlet problem. Yves Colin de Verdière proved\nthe existence of the circle packing as a minimizer of a convex function on a certain configuration\nspace.\n\nThe circle packing theorem is a useful tool to study various problems in planar\ngeometry, conformal mappings and planar graphs. An elegant proof of the planar separator theorem, \noriginally due to Lipton and Tarjan, has been obtained in this way.\nAnother application of the circle packing theorem is that unbiased limits of\nbounded-degree planar graphs are almost surely recurrent.\nOther applications include implications for the cover time.\nand estimates for the largest eigenvalue of bounded-genus graphs.\n\nIn graph drawing, circle packing has been used to find drawings of planar graphs with bounded angular resolution and with bounded slope number.\nFáry's theorem, that every graph that can be drawn without crossings in the plane using curved edges can also be drawn without crossings using straight line segment edges, follows as a simple corollary of the circle packing theorem: by placing vertices at the centers of the circles and drawing straight edges between them, a straight-line planar embedding is obtained.\nA stronger form of the circle packing theorem asserts that any polyhedral graph and its dual graph can be represented by two circle packings, such that the two tangent circles representing a primal graph edge and the two tangent circles representing the dual of the same edge always have their tangencies at right angles to each other at the same point of the plane. A packing of this type can be used to construct a convex polyhedron that represents the given graph and that has a midsphere, a sphere tangent to all of the edges of the polyhedron. Conversely, if a polyhedron has a midsphere, then the circles formed by the intersections of the sphere with the polyhedron faces and the circles formed by the horizons on the sphere as viewed from each polyhedron vertex form a dual packing of this type.\n\n describe a numerical relaxation algorithm for finding circle packings, based on ideas of William Thurston. The version of the circle packing problem that they solve takes as input a planar graph, in which all the internal faces are triangles and for which the external vertices have been labeled by positive numbers. It produces as output a circle packing whose tangencies represent the given graph, and for which the circles representing the external vertices have the radii specified in the input. As they suggest, the key to the problem is to first calculate the radii of the circles in the packing; once the radii are known, the geometric positions of the circles are not difficult to calculate. They begin with a set of tentative radii that do not correspond to a valid packing, and then repeatedly perform the following steps:\nEach of these steps may be performed with simple trigonometric calculations, and as Collins and Stephenson argue, the system of radii converges rapidly to a unique fixed point for which all covering angles are exactly 2π. Once the system has converged, the circles may be placed one at a time, at each step using the positions and radii of two neighboring circles to determine the center of each successive circle.\n\nThe circle packing theorem generalizes to graphs that are not planar.\nIf \"G\" is a graph that can be embedded on a surface \"S\",\nthen there is a constant curvature Riemannian metric \"d\" on \"S\" and a circle packing on (\"S\", \"d\") whose contacts graph is isomorphic to \"G\". If \"S\" is closed (compact and without boundary)\nand \"G\" is a triangulation of \"S\", then (\"S\", \"d\") and the packing are unique up to conformal equivalence. If \"S\" is the sphere, then this equivalence is up to Möbius transformations; if it is a torus, then the equivalence is up to scaling by a constant and isometries, while if \"S\" has genus at least 2, then the equivalence is up to isometries.\n\nAnother generalization of the circle packing theorem involves replacing the condition of tangency with a specified intersection angle between circles corresponding to neighboring vertices. A particularly elegant version is as follows. Suppose that \"G\" is a finite 3-connected planar graph (that is, a polyhedral graph), then there is a pair of circle packings, one whose intersection graph is isomorphic to \"G\", another whose intersection graph is isomorphic to the planar dual of \"G\",\nand for every vertex in \"G\" and face adjacent to it, the circle in the first packing corresponding to the vertex\nintersects orthogonally with the circle in the second packing corresponding to the face. For instance, applying this result to the graph of the tetrahedron gives, for any four mutuall tangent circles, a second set of four mutually tangent circles each of which is orthogonal to three of the first four. A further generalization, replacing intersection angle with inversive distance, allows the specification of packings in which some circles are required to be disjoint from each other rather than crossing or being tangent.\n\nYet another variety of generalizations allow shapes that are not circles.\nSuppose that \"G\" = (\"V\", \"E\") is a finite planar graph, and to each vertex \"v\" of \"G\"\ncorresponds a shape formula_1, which is homeomorphic\nto the closed unit disk and whose boundary is smooth.\nThen there is a packing formula_2 in the plane\nsuch that formula_3 if and only if formula_4\nand for each formula_5 the set formula_6 is obtained from formula_7 by translating\nand scaling. (Note that in the original circle packing theorem, there are three real parameters per vertex,\ntwo of which describe the center of the corresponding circle and one of which describe the radius, and there is one equation per edge. This also holds in this generalization.)\nOne proof of this generalization can be obtained by applying Koebe's original proof and the theorem\nof Brandt and Harrington stating that any finitely connected domain is conformally equivalent to\na planar domain whose boundary components have specified shapes, up to translations and scaling.\n\nThe circle packing theorem was first proved by Paul Koebe.\nWilliam Thurston \nrediscovered the circle packing theorem, and\nnoted that it followed from the work of E. M. Andreev. Thurston also proposed a scheme for using the circle packing theorem to obtain a homeomorphism of a simply connected proper subset of the plane onto the interior of the unit disk. The \"Thurston Conjecture for Circle Packings\" is his conjecture that the homeomorphism will converge to the Riemann mapping as the radii of the circles tend to zero. The Thurston Conjecture was later proved\nby Burton Rodin and Dennis Sullivan.\nThis led to a flurry of research on extensions of the circle packing theorem, relations to\nconformal mappings, and applications.\n\n\n\n"}
{"id": "650751", "url": "https://en.wikipedia.org/wiki?curid=650751", "title": "Complete Heyting algebra", "text": "Complete Heyting algebra\n\nIn mathematics, especially in order theory, a complete Heyting algebra is a Heyting algebra that is complete as a lattice. Complete Heyting algebras are the objects of three different categories; the category CHey, the category Loc of locales, and its opposite, the category Frm of frames. Although these three categories contain the same objects, they differ in their morphisms, and thus get distinct names. Only the morphisms of CHey are homomorphisms of complete Heyting algebras.\n\nLocales and frames form the foundation of pointless topology, which, instead of building on point-set topology, recasts the ideas of general topology in categorical terms, as statements on frames and locales.\n\nConsider a partially ordered set (\"P\", ≤) that is a complete lattice. Then \"P\" is a \"complete Heyting algebra\" if any of the following equivalent conditions hold:\n\nThe system of all open sets of a given topological space ordered by inclusion is a complete Heyting algebra.\n\nThe objects of the category CHey, the category Frm of frames and the category Loc of locales are the complete lattices satisfying the infinite distributive law. These categories differ in what constitutes a morphism.\n\nThe morphisms of Frm are (necessarily monotone) functions that preserve finite meets and arbitrary joins. Such functions are not homomorphisms of complete Heyting algebras. The definition of Heyting algebras crucially involves the existence of right adjoints to the binary meet operation, which together define an additional implication operation ⇒. Thus, a \"homomorphism of complete Heyting algebras\" is a morphism of frames that in addition preserves implication. The morphisms of Loc are opposite to those of Frm, and they are usually called maps (of locales).\n\nThe relation of locales and their maps to topological spaces and continuous functions may be seen as follows. Let\nbe any map. The power sets \"P\"(\"X\") and \"P\"(\"Y\") are complete Boolean algebras, and the map\nis a homomorphism of complete Boolean algebras. Suppose the spaces \"X\" and \"Y\" are topological spaces, endowed with the topology \"O\"(\"X\") and \"O\"(\"Y\") of open sets on \"X\" and \"Y\". Note that \"O\"(\"X\") and \"O\"(\"Y\") are subframes of \"P\"(\"X\") and \"P\"(\"Y\"). If \"ƒ\" is a continuous function, then\npreserves finite meets and arbitrary joins of these subframes. This shows that \"O\" is a functor from the category Top of topological spaces to the category Loc of locales, taking any continuous map\nto the map\nin Loc that is defined in Frm to be the inverse image frame homomorphism\nIt is common, given a map of locales\nin Loc, to write\nfor the frame homomorphism that defines it in Frm. Hence, using this notation, \"O\"(\"ƒ\") is defined by the equation \n\nConversely, any locale \"A\" has a topological space \"S\"(\"A\") that best approximates the locale, called its \"spectrum\". In addition, any map of locales\ndetermines a continuous map\nand this assignment is functorial: letting \"P\"(1) denote the locale that is obtained as the powerset of the terminal set the points of \"S\"(\"A\") are the maps\nin Loc, i.e., the frame homomorphisms\nFor each we define the set that consists of the points such that It is easy to verify that this defines a frame homomorphism whose image is therefore a topology on \"S\"(\"A\"). Then, if\nto each point we assign the point \"S\"(\"ƒ\")(\"q\") defined by letting \"S\"(\"ƒ\")(p)* be the composition of \"p\"* with \"ƒ\"*, hence obtaining a continuous map\n\nThis defines a functor formula_17 from Loc to Top, which is right adjoint to \"O\".\n\nAny locale that is isomorphic to the topology of its spectrum is called \"spatial\", and any topological space that is homeomorphic to the spectrum of its locale of open sets is called \"sober\". The adjunction between topological spaces and locales restricts to an equivalence of categories between sober spaces and spatial locales.\n\nAny function that preserves all joins (and hence any frame homomorphism) has a right adjoint, and, conversely, any function that preserves all meets has a left adjoint. Hence, the category Loc is isomorphic to the category whose objects are the frames and whose morphisms are the meet preserving functions whose left adjoints preserve finite meets. This is often regarded as a representation of Loc, but it should not be confused with Loc itself, whose morphisms are formally the same as frame homomorphisms in the opposite direction.\n\n\n\n\n"}
{"id": "39739", "url": "https://en.wikipedia.org/wiki?curid=39739", "title": "Confocal", "text": "Confocal\n\nIn geometry, confocal means having the same foci: confocal conic sections.\n\n\n"}
{"id": "32673841", "url": "https://en.wikipedia.org/wiki?curid=32673841", "title": "Continuous dual q-Hahn polynomials", "text": "Continuous dual q-Hahn polynomials\n\nIn mathematics, the continuous dual \"q\"-Hahn polynomials are a family of basic hypergeometric orthogonal polynomials in the basic Askey scheme. give a detailed list of their properties.\n\nThe polynomials are given in terms of basic hypergeometric functions and the Pochhammer symbol by \n\nIn which formula_2\n\n"}
{"id": "15398838", "url": "https://en.wikipedia.org/wiki?curid=15398838", "title": "Contour set", "text": "Contour set\n\nIn mathematics, contour sets generalize and formalize the everyday notions of\n\nGiven a relation on pairs of elements of set formula_1\nand an element formula_3 of formula_1\n\nThe upper contour set of formula_3 is the set of all formula_7 that are related to formula_3:\n\nThe lower contour set of formula_3 is the set of all formula_7 such that formula_3 is related to them:\n\nThe strict upper contour set of formula_3 is the set of all formula_7 that are related to formula_3 without formula_3 being \"in this way\" related to any of them:\n\nThe strict lower contour set of formula_3 is the set of all formula_7 such that formula_3 is related to them without any of them being \"in this way\" related to formula_3:\n\nThe formal expressions of the last two may be simplified if we have defined\nso that formula_25 is related to formula_26 but formula_26 is \"not\" related to formula_25, in which case the strict upper contour set of formula_3 is\n\nand the strict lower contour set of formula_3 is\n\nIn the case of a function formula_33 considered in terms of relation formula_34, reference to the contour sets of the function is implicitly to the contour sets of the implied relation\n\nConsider a real number formula_3, and the relation formula_37. Then\n\nConsider, more generally, the relation\nThen\n\nIt would be \"technically\" possible to define contour sets in terms of the relation\nthough such definitions would tend to confound ready understanding.\n\nIn the case of a real-valued function formula_33 (whose arguments might or might not be themselves real numbers), reference to the contour sets of the function is implicitly to the contour sets of the relation\nNote that the arguments to formula_33 might be vectors, and that the notation used might instead be\n\nIn economics, the set formula_1 could be interpreted as a set of goods and services or of possible outcomes, the relation formula_65 as \"strict preference\", and the relationship formula_66 as \"weak preference\". Then\n\nSuch preferences might be captured by a utility function formula_75, in which case\n\nOn the assumption that formula_66 is a total ordering of formula_1, the complement of the upper contour set is the strict lower contour set.\n\nand the complement of the strict upper contour set is the lower contour set.\n\n\n"}
{"id": "1743431", "url": "https://en.wikipedia.org/wiki?curid=1743431", "title": "Convergent series", "text": "Convergent series\n\nIn mathematics, a series is the sum of the terms of an infinite sequence of numbers.\n\nGiven an infinite sequence formula_1, the \"n\"th partial sum formula_2 is the sum of the first \"n\" terms of the sequence, that is,\n\nA series is convergent if the sequence of its partial sums formula_4 tends to a limit; that means that the partial sums become closer and closer to a given number when the number of their terms increases. More precisely, a series converges, if there exists a number formula_5 such that for any arbitrarily small positive number formula_6, there is a (sufficiently large) integer formula_7 such that for all formula_8,\n\nIf the series is convergent, the number formula_5 (necessarily unique) is called the sum of the series.\n\nAny series that is not convergent is said to be divergent.\n\n\nThere are a number of methods of determining whether a series converges or diverges.\n\nComparison test. The terms of the sequence formula_22 are compared to those of another sequence formula_23. If,\n\nfor all \"n\", formula_24, and formula_25 converges, then so does formula_26\n\nHowever, if,\n\nfor all \"n\", formula_27, and formula_25 diverges, then so does formula_26\n\nRatio test. Assume that for all \"n\", formula_30. Suppose that there exists formula_31 such that\n\nIf \"r\" < 1, then the series converges. If then the series diverges. If the ratio test is inconclusive, and the series may converge or diverge.\n\nRoot test or \"n\"th root test. Suppose that the terms of the sequence in question are non-negative. Define \"r\" as follows:\n\nIf \"r\" < 1, then the series converges. If then the series diverges. If the root test is inconclusive, and the series may converge or diverge.\n\nThe ratio test and the root test are both based on comparison with a geometric series, and as such they work in similar situations. In fact, if the ratio test works (meaning that the limit exists and is not equal to 1) then so does the root test; the converse, however, is not true. The root test is therefore more generally applicable, but as a practical matter the limit is often difficult to compute for commonly seen types of series.\n\nIntegral test. The series can be compared to an integral to establish convergence or divergence. Let formula_34 be a positive and monotonically decreasing function. If\n\nthen the series converges. But if the integral diverges, then the series does so as well.\n\nLimit comparison test. If formula_36, and the limit formula_37 exists and is not zero, then formula_38 converges if and only if formula_25 converges.\n\nAlternating series test. Also known as the \"Leibniz criterion\", the alternating series test states that for an alternating series of the form formula_40, if formula_22 is monotonically decreasing, and has a limit of 0 at infinity, then the series converges.\n\nCauchy condensation test. If formula_22 is a positive monotone decreasing sequence, then\nformula_43 converges if and only if formula_44 converges.\n\nDirichlet's test\n\nAbel's test\n\nRaabe's test\n\nFor any sequence formula_45, formula_46 for all n. Therefore,\n\nThis means that if formula_48 converges, then formula_38 also converges (but not vice versa).\n\nIf the series formula_48 converges, then the series formula_38 is absolutely convergent. An absolutely convergent sequence is one in which the length of the line created by joining together all of the increments to the partial sum is finitely long. The power series of the exponential function is absolutely convergent everywhere.\n\nIf the series formula_38 converges but the series formula_48 diverges, then the series formula_38 is conditionally convergent. The path formed by connecting the partial sums of a conditionally convergent series is infinitely long. The power series of the logarithm is conditionally convergent.\n\nThe Riemann series theorem states that if a series converges conditionally, it is possible to rearrange the terms of the series in such a way that the series converges to any value, or even diverges.\n\nLet formula_55 be a sequence of functions. \nThe series formula_56 is said to converge uniformly to \"f\"\nif the sequence formula_57 of partial sums defined by\n\nconverges uniformly to \"f\".\n\nThere is an analogue of the comparison test for infinite series of functions called the Weierstrass M-test.\n\nThe Cauchy convergence criterion states that a series\nconverges if and only if the sequence of partial sums is a Cauchy sequence.\nThis means that for every formula_60 there is a positive integer formula_7 such that for all formula_62 we have\nwhich is equivalent to \n\n\n"}
{"id": "5745790", "url": "https://en.wikipedia.org/wiki?curid=5745790", "title": "Conway puzzle", "text": "Conway puzzle\n\nConway's puzzle, or Blocks-in-a-Box, is a packing problem using rectangular blocks, named after its inventor, mathematician John Conway. It calls for packing thirteen 1 × 2 × 4 blocks, one 2 × 2 × 2 block, one 1 × 2 × 2 block, and three 1 × 1 × 3 blocks into a 5 × 5 × 5 box.\n\nThe solution of the Conway puzzle is straightforward once one realizes, based on parity considerations, that the three 1 × 1 × 3 blocks need to be placed so that precisely one of them appears in each 5 × 5 × 1 slice of the cube. This is analogous to similar insight that facilitates the solution of the simpler Slothouber–Graatsma puzzle.\n\n\n"}
{"id": "275768", "url": "https://en.wikipedia.org/wiki?curid=275768", "title": "Counting", "text": "Counting\n\nCounting is the process of determining the number of elements of a finite set of objects. The traditional way of counting consists of continually increasing a (mental or spoken) counter by a unit for every element of the set, in some order, while marking (or displacing) those elements to avoid visiting the same element more than once, until no unmarked elements are left; if the counter was set to one after the first object, the value after visiting the final object gives the desired number of elements. The related term \"enumeration\" refers to uniquely identifying the elements of a finite (combinatorial) set or infinite set by assigning a number to each element.\nCounting sometimes involves numbers other than one; for example, when counting money, counting out change, \"counting by twos\" (2, 4, 6, 8, 10, 12, ...), or \"counting by fives\" (5, 10, 15, 20, 25, ...).\n\nThere is archaeological evidence suggesting that humans have been counting for at least 50,000 years. Counting was primarily used by ancient cultures to keep track of social and economic data such as number of group members, prey animals, property, or debts (i.e., accountancy). Notched bones were also found in the Border Caves in South Africa that may suggest that the concept of counting was known to humans as far back as 44,000 BCE. The development of counting led to the development of mathematical notation, numeral systems, and writing.\n\nCounting can occur in a variety of forms.\n\nCounting can be verbal; that is, speaking every number out loud (or mentally) to keep track of progress. This is often used to count objects that are present already, instead of counting a variety of things over time.\n\nCounting can also be in the form of tally marks, making a mark for each number and then counting all of the marks when done tallying. This is useful when counting objects over time, such as the number of times something occurs during the course of a day. Tallying is base 1 counting; normal counting is done in base 10. Computers use base 2 counting (0's and 1's).\n\nCounting can also be in the form of finger counting, especially when counting small numbers. This is often used by children to facilitate counting and simple mathematical operations. Finger-counting uses unary notation (one finger = one unit), and is thus limited to counting 10 (unless you start in with your toes). Older finger counting used the four fingers and the three bones in each finger (phalanges) to count to the number twelve. Other hand-gesture systems are also in use, for example the Chinese system by which one can count to 10 using only gestures of one hand. By using finger binary (base 2 counting), it is possible to keep a finger count up to .\n\nVarious devices can also be used to facilitate counting, such as hand tally counters and abacuses.\n\nInclusive counting is usually encountered when dealing with time in the Romance languages. In exclusive counting languages such as English, when counting \"8\" days from Sunday, Monday will be \"day 1\", Tuesday \"day 2\", and the following Monday will be the \"eighth day\". When counting \"inclusively,\" the Sunday (the start day) will be \"day 1\" and therefore the following Sunday will be the \"eighth day\". For example, the French phrase for \"fortnight\" is \"quinzaine\" (15 [days]), and similar words are present in Greek (δεκαπενθήμερο, \"dekapenthímero\"), Spanish (\"quincena\") and Portuguese (\"quinzena\"). In contrast, the English word \"fortnight\" itself derives from \"a fourteen-night\", as the archaic \"sennight\" does from \"a seven-night\"; the English words are not examples of inclusive counting.\n\nNames based on inclusive counting appear in other calendars as well: in the Roman calendar the \"nones\" (meaning \"nine\") is 8 days before the \"ides\"; and in the Christian calendar Quinquagesima (meaning 50) is 49 days before Easter Sunday.\n\nMusical terminology also uses inclusive counting of intervals between notes of the standard scale: going up one note is a second interval, going up two notes is a third interval, etc., and going up seven notes is an \"octave\".\n\nLearning to count is an important educational/developmental milestone in most cultures of the world. Learning to count is a child's very first step into mathematics, and constitutes the most fundamental idea of that discipline. However, some cultures in Amazonia and the Australian Outback do not count, and their languages do not have number words.\n\nMany children at just 2 years of age have some skill in reciting the count list (i.e., saying \"one, two, three, ...\"). They can also answer questions of ordinality for small numbers, e.g., \"What comes after \"three\"?\". They can even be skilled at pointing to each object in a set and reciting the words one after another. This leads many parents and educators to the conclusion that the child knows how to use counting to determine the size of a set. Research suggests that it takes about a year after learning these skills for a child to understand what they mean and why the procedures are performed. In the mean time, children learn how to name cardinalities that they can subitize.\n\nIn mathematics, the essence of counting a set and finding a result \"n\", is that it establishes a one-to-one correspondence (or bijection) of the set with the set of numbers {1, 2, ..., \"n\"}. A fundamental fact, which can be proved by mathematical induction, is that no bijection can exist between {1, 2, ..., \"n\"} and {1, 2, ..., \"m\"} unless ; this fact (together with the fact that two bijections can be composed to give another bijection) ensures that counting the same set in different ways can never result in different numbers (unless an error is made). This is the fundamental mathematical theorem that gives counting its purpose; however you count a (finite) set, the answer is the same. In a broader context, the theorem is an example of a theorem in the mathematical field of (finite) combinatorics—hence (finite) combinatorics is sometimes referred to as \"the mathematics of counting.\"\n\nMany sets that arise in mathematics do not allow a bijection to be established with {1, 2, ..., \"n\"} for \"any\" natural number \"n\"; these are called infinite sets, while those sets for which such a bijection does exist (for some \"n\") are called finite sets. Infinite sets cannot be counted in the usual sense; for one thing, the mathematical theorems which underlie this usual sense for finite sets are false for infinite sets. Furthermore, different definitions of the concepts in terms of which these theorems are stated, while equivalent for finite sets, are inequivalent in the context of infinite sets.\n\nThe notion of counting may be extended to them in the sense of establishing (the existence of) a bijection with some well-understood set. For instance, if a set can be brought into bijection with the set of all natural numbers, then it is called \"countably infinite.\" This kind of counting differs in a fundamental way from counting of finite sets, in that adding new elements to a set does not necessarily increase its size, because the possibility of a bijection with the original set is not excluded. For instance, the set of all integers (including negative numbers) can be brought into bijection with the set of natural numbers, and even seemingly much larger sets like that of all finite sequences of rational numbers are still (only) countably infinite. Nevertheless, there are sets, such as the set of real numbers, that can be shown to be \"too large\" to admit a bijection with the natural numbers, and these sets are called \"uncountable.\" Sets for which there exists a bijection between them are said to have the same cardinality, and in the most general sense counting a set can be taken to mean determining its cardinality. Beyond the cardinalities given by each of the natural numbers, there is an infinite hierarchy of infinite cardinalities, although only very few such cardinalities occur in ordinary mathematics (that is, outside set theory that explicitly studies possible cardinalities).\n\nCounting, mostly of finite sets, has various applications in mathematics. One important principle is that if two sets \"X\" and \"Y\" have the same finite number of elements, and a function is known to be injective, then it is also surjective, and vice versa. A related fact is known as the pigeonhole principle, which states that if two sets \"X\" and \"Y\" have finite numbers of elements \"n\" and \"m\" with , then any map is \"not\" injective (so there exist two distinct elements of \"X\" that \"f\" sends to the same element of \"Y\"); this follows from the former principle, since if \"f\" were injective, then so would its restriction to a strict subset \"S\" of \"X\" with \"m\" elements, which restriction would then be surjective, contradicting the fact that for \"x\" in \"X\" outside \"S\", \"f\"(\"x\") cannot be in the image of the restriction. Similar counting arguments can prove the existence of certain objects without explicitly providing an example. In the case of infinite sets this can even apply in situations where it is impossible to give an example.\n\nThe domain of enumerative combinatorics deals with computing the number of elements of finite sets, without actually counting them; the latter usually being impossible because infinite families of finite sets are considered at once, such as the set of permutations of {1, 2, ..., \"n\"} for any natural number \"n\".\n\n"}
{"id": "54380446", "url": "https://en.wikipedia.org/wiki?curid=54380446", "title": "Euler–Boole summation", "text": "Euler–Boole summation\n\nEuler–Boole summation is a summation method for alternating series based on Euler's polynomials, which are defined by\n\nThe concept is named after Leonhard Euler and George Boole.\n\nThe periodic Euler functions are\n\nThe Euler–Boole formula to sum alternating series is\n\nwhere formula_4, formula_5, formula_6 and formula_7 is the \"k\"th derivative.\n\n"}
{"id": "5863022", "url": "https://en.wikipedia.org/wiki?curid=5863022", "title": "François Budan de Boislaurent", "text": "François Budan de Boislaurent\n\nFerdinand François Désiré Budan de Boislaurent (28 September 1761 – 6 October 1840) was a French amateur mathematician, best known for a tract, \"Nouvelle méthode pour la résolution des équations numériques\",\nfirst published in Paris in 1807, but based on work from 1803.\n\nBudan was born in Limonade, Cap-Français, Saint-Domingue (now Haiti) on 28 September 1761. His early education was at Juilly, France. He then proceeded to Paris where he studied medicine, receiving a doctorate for a thesis entitled \"Essai sur cette question d'économie médicale : Convient-il qu'un malade soit instruit de sa situation?\" Budan died in Paris on 6 October 1840.\n\nBudan explains in his book how, given a monic polynomial p(x), the coefficients of p(x+1) can be obtained by developing a \"Pascal-like triangle\" with first row the coefficients of p(x), rather than by expanding successive powers of x+1, as in Pascal's triangle proper, and then summing; thus, the method has the flavour of lattice path combinatorics. Taken together with Descartes' \"Rule of Signs\", this leads to an upper bound on the number of the real roots a polynomial has inside an open interval. Although Budan's Theorem, as this result was known, was taken up by, among others, (1779-1854), in his celebrated algebra textbook, it tended to be eclipsed by an equivalent result due to Joseph Fourier, as the consequence of a priority dispute. Interest in Budan's theorem has been revived because some further computational results are more easily deducible from it than from Fourier's version of the theorem.\n\nBudan's book was read across the English Channel; for example, Peter Barlow includes mention of it in his entry on \"Approximation\" in his \"Dictionary\" (1814), although grouping it with the method of Joseph-Louis Lagrange as being accurate, but of more theoretical interest than practical use. Budan's work on approximation was studied by Horner in preparing his celebrated article in the Philosophical Transactions of the Royal Society of London in 1819 that gave rise to the term Horner's method; Horner comments there and elsewhere on Budan's results, at first being sceptical of the value of Budan's work, but later warming to it. Thus, these writers in English have a different appreciation of Budan's work to a French writer, such as Bourdon; indeed, Horner was praised over Budan for being able to go directly from p(x) to p(x+a) for any a, rather than taking this in steps after the manner of Budan. Barlow and Horner show some awareness of the work of another writer in French, (1773-1849), who also looked at how to obtain the coefficients of p(x+a) from those of p(x) along the lines of Budan and Horner about the same time as Horner first published his work. But Budan's name and theorem only appear in late editions of Francoeur's book.\n\nBudan, in common with other writers in French of the period working on root extraction, does not mention Paolo Ruffini, notwithstanding Ruffini had been in correspondence with Lagrange; this was not just an English failing. Ruffini's work on the topic dates, in the first instance, from 1804, but, as with Budan and then Horner, several subsequent reworkings.\n\n\n"}
{"id": "36084772", "url": "https://en.wikipedia.org/wiki?curid=36084772", "title": "Friedrich Karl Schmidt", "text": "Friedrich Karl Schmidt\n\nFriedrich Karl Schmidt (22 September 1901 – 25 January 1977) was a German mathematician, who made notable contributions to algebra and number theory.\n\nSchmidt studied from 1920 to 1925 in Freiburg and Marburg. In 1925 he completed his doctorate at the Albert-Ludwigs-Universität Freiburg under the direction of Alfred Loewy. In 1927 he became a \"Privatdozent\" (lecturer) at the University of Erlangen, where he received his habilitation and in 1933 became a professor extraordinarius. In 1933/34 he was a \"Dozent\" at the University of Göttingen, where he worked with Helmut Hasse. Schmidt was then a professor ordinarius at the University of Jena from 1934 to 1945. During WW II, he was at the \"Deutsche Versuchsanstalt für Segelflug\" (German Research Station for Gliding) in Reichenhall. He was a professor from 1946 to 1952 at Westfälischen Wilhelms-Universität in Münster and from 1952 to 1966 at the University of Heidelberg, where he retired as professor emeritus.\nIn the mid-1930s Schmidt was on the editorial staff of .\n\nSchmidt was elected in 1954 a member of the Heidelberger Akademie der Wissenschaften and was made in 1968 an honorary doctor of the Free University of Berlin.\n\nSchmidt is known for his contributions to the theory of algebraic function fields and in particular for his definitiohn of a zeta function for algebraic function fields and his proof of the generalized Riemann–Roch theorem for algebraic function fields (where the base field can be an arbitrary perfect field). He also made contributions to class field theory and valuation theory.\n"}
{"id": "2213247", "url": "https://en.wikipedia.org/wiki?curid=2213247", "title": "Generic filter", "text": "Generic filter\n\nIn the mathematical field of set theory, a generic filter is a kind of object used in the theory of forcing, a technique used for many purposes, but especially to establish the independence of certain propositions from certain formal theories, such as ZFC. For example, Paul Cohen used the method to establish that ZFC, if consistent, cannot prove the continuum hypothesis, which states that there are exactly aleph-one real numbers. In the contemporary re-interpretation of Cohen's proof, it proceeds by constructing a generic filter that codes more than formula_1 reals, without changing the value of formula_1.\n\nFormally, let \"P\" be a partially ordered set, and let \"F\" be a filter on \"P\"; that is, \"F\" is a subset of \"P\" such that:\n\nNow if \"D\" is a collection of dense open subsets of \"P\", in the topology whose basic open sets are all sets of the form {\"q\"|\"q\"≤\"p\"} for particular \"p\" in \"P\", then \"F\" is said to be \"D\"-generic if \"F\" meets all sets in \"D\"; that is,\n\nSimilarly, if \"M\" is a transitive model of ZFC (or some sufficient fragment thereof), with \"P\" an element of \"M\", then \"F\" is said to be M\"-generic, or sometimes generic over \"M, if \"F\" meets all dense open subsets of \"P\" that are elements of \"M\".\n"}
{"id": "9550030", "url": "https://en.wikipedia.org/wiki?curid=9550030", "title": "History of algebra", "text": "History of algebra\n\nAs a branch of mathematics, algebra emerged at the end of the 16th century in Europe, with the work of François Viète. Algebra can essentially be considered as doing computations similar to those of arithmetic but with non-numerical mathematical objects. However, until the 19th century, algebra consisted essentially of the theory of equations. For example, the fundamental theorem of algebra belongs to the theory of equations and is not, nowadays, considered as belonging to algebra (in fact, every proof must use the completeness of the real numbers, which is not an algebraic property).\n\nThis article describes the history of the theory of equations, called here \"algebra\", from the origins to the emergence of algebra as a separate area of mathematics.\n\nThe word \"algebra\" is derived from the Arabic word الجبر \"al-jabr\", and this comes from the treatise written in the year 830 by the medieval Persian mathematician, Muhammad ibn Mūsā al-Khwārizmī, whose Arabic title, \"Kitāb al-muḫtaṣar fī ḥisāb al-ğabr wa-l-muqābala\", can be translated as \"The Compendious Book on Calculation by Completion and Balancing\". The treatise provided for the systematic solution of linear and quadratic equations. According to one history, \"[i]t is not certain just what the terms \"al-jabr\" and \"muqabalah\" mean, but the usual interpretation is similar to that implied in the previous translation. The word 'al-jabr' presumably meant something like 'restoration' or 'completion' and seems to refer to the transposition of subtracted terms to the other side of an equation; the word 'muqabalah' is said to refer to 'reduction' or 'balancing'—that is, the cancellation of like terms on opposite sides of the equation. Arabic influence in Spain long after the time of al-Khwarizmi is found in \"Don Quixote\", where the word 'algebrista' is used for a bone-setter, that is, a 'restorer'.\" The term is used by al-Khwarizmi to describe the operations that he introduced, \"reduction\" and \"balancing\", referring to the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation.\n\nAlgebra did not always make use of the symbolism that is now ubiquitous in mathematics; instead, it went through three distinct stages. The stages in the development of symbolic algebra are approximately as follows:\n\nEqually important as the use or lack of symbolism in algebra was the degree of the equations that were addressed. Quadratic equations played an important role in early algebra; and throughout most of history, until the early modern period, all quadratic equations were classified as belonging to one of three categories.\nwhere p and q are positive.\nThis trichotomy comes about because quadratic equations of the form formula_4, with p and q positive, have no positive roots.\n\nIn between the rhetorical and syncopated stages of symbolic algebra, a geometric constructive algebra was developed by classical Greek and Vedic Indian mathematicians in which algebraic equations were solved through geometry. For instance, an equation of the form formula_5 was solved by finding the side of a square of area \"A\".\n\nIn addition to the three stages of expressing algebraic ideas, some authors recognized four conceptual stages in the development of algebra that occurred alongside the changes in expression. These four stages were as follows:\n\n\nThe origins of algebra can be traced to the ancient Babylonians, who developed a positional number system that greatly aided them in solving their rhetorical algebraic equations. The Babylonians were not interested in exact solutions but approximations, and so they would commonly use linear interpolation to approximate intermediate values. One of the most famous tablets is the Plimpton 322 tablet, created around 1900–1600 BC, which gives a table of Pythagorean triples and represents some of the most advanced mathematics prior to Greek mathematics.\n\nBabylonian algebra was much more advanced than the Egyptian algebra of the time; whereas the Egyptians were mainly concerned with linear equations the Babylonians were more concerned with quadratic and cubic equations. The Babylonians had developed flexible algebraic operations with which they were able to add equals to equals and multiply both sides of an equation by like quantities so as to eliminate fractions and factors. They were familiar with many simple forms of factoring, three-term quadratic equations with positive roots, and many cubic equations although it is not known if they were able to reduce the general cubic equation.\n\nAncient Egyptian algebra dealt mainly with linear equations while the Babylonians found these equations too elementary and developed mathematics to a higher level than the Egyptians.\n\nThe Rhind Papyrus, also known as the Ahmes Papyrus, is an ancient Egyptian papyrus written c. 1650 BC by Ahmes, who transcribed it from an earlier work that he dated to between 2000 and 1800 BC. It is the most extensive ancient Egyptian mathematical document known to historians. The Rhind Papyrus contains problems where linear equations of the form formula_6 and formula_7 are solved, where \"a\", \"b\", and \"c\" are known and \"x\", which is referred to as \"aha\" or heap, is the unknown. The solutions were possibly, but not likely, arrived at by using the \"method of false position\", or \"regula falsi\", where first a specific value is substituted into the left hand side of the equation, then the required arithmetic calculations are done, thirdly the result is compared to the right hand side of the equation, and finally the correct answer is found through the use of proportions. In some of the problems the author \"checks\" his solution, thereby writing one of the earliest known simple proofs.\n\nIt is sometimes alleged that the Greeks had no algebra, but this is inaccurate. By the time of Plato, Greek mathematics had undergone a drastic change. The Greeks created a geometric algebra where terms were represented by sides of geometric objects, usually lines, that had letters associated with them, and with this new form of algebra they were able to find solutions to equations by using a process that they invented, known as \"the application of areas\". \"The application of areas\" is only a part of geometric algebra and it is thoroughly covered in Euclid's \"Elements\".\n\nAn example of geometric algebra would be solving the linear equation ax = bc. The ancient Greeks would solve this equation by looking at it as an equality of areas rather than as an equality between the ratios a:b and c:x. The Greeks would construct a rectangle with sides of length b and c, then extend a side of the rectangle to length a, and finally they would complete the extended rectangle so as to find the side of the rectangle that is the solution.\n\nIamblichus in \"Introductio arithmatica\" tells us that Thymaridas (c. 400 BCE – c. 350 BCE) worked with simultaneous linear equations. In particular, he created the then famous rule that was known as the \"bloom of Thymaridas\" or as the \"flower of Thymaridas\", which states that:\nIf the sum of \"n\" quantities be given, and also the sum of every pair containing a particular quantity, then this particular quantity is equal to 1/ (n - 2) of the difference between the sums of these pairs and the first given sum.\n\nor using modern notion, the solution of the following system of \"n\" linear equations in \"n\" unknowns,\n\nis,\nformula_8\n\nIamblichus goes on to describe how some systems of linear equations that are not in this form can be placed into this form.\n\nEuclid (Greek: ) was a Greek mathematician who flourished in Alexandria, Egypt, almost certainly during the reign of Ptolemy I (323–283 BCE). Neither the year nor place of his birth have been established, nor the circumstances of his death.\n\nEuclid is regarded as the \"father of geometry\". His \"Elements\" is the most successful textbook in the history of mathematics. Although he is one of the most famous mathematicians in history there are no new discoveries attributed to him, rather he is remembered for his great explanatory skills. The \"Elements\" is not, as is sometimes thought, a collection of all Greek mathematical knowledge to its date, rather, it is an elementary introduction to it.\n\nThe geometric work of the Greeks, typified in Euclid's \"Elements\", provided the framework for generalizing formulae beyond the solution of particular problems into more general systems of stating and solving equations.\n\nBook II of the \"Elements\" contains fourteen propositions, which in Euclid's time were extremely significant for doing geometric algebra. These propositions and their results are the geometric equivalents of our modern symbolic algebra and trigonometry. Today, using modern symbolic algebra, we let symbols represent known and unknown magnitudes (i.e. numbers) and then apply algebraic operations on them. While in Euclid's time magnitudes were viewed as line segments and then results were deduced using the axioms or theorems of geometry.\n\nMany basic laws of addition and multiplication are included or proved geometrically in the \"Elements\". For instance, proposition 1 of Book II states:\n\nBut this is nothing more than the geometric version of the (left) distributive law, formula_9; and in Books V and VII of the \"Elements\" the commutative and associative laws for multiplication are demonstrated.\n\nMany basic equations were also proved geometrically. For instance, proposition 5 in Book II proves that formula_10, and proposition 4 in Book II proves that formula_11.\n\nFurthermore, there are also geometric solutions given to many equations. For instance, proposition 6 of Book II gives the solution to the quadratic equation , and proposition 11 of Book II gives a solution to .\n\n\"Data\" is a work written by Euclid for use at the schools of Alexandria and it was meant to be used as a companion volume to the first six books of the \"Elements\". The book contains some fifteen definitions and ninety-five statements, of which there are about two dozen statements that serve as algebraic rules or formulas. Some of these statements are geometric equivalents to solutions of quadratic equations. For instance, \"Data\" contains the solutions to the equations and the familiar Babylonian equation , .\n\nA conic section is a curve that results from the intersection of a cone with a plane. There are three primary types of conic sections: ellipses (including circles), parabolas, and hyperbolas. The conic sections are reputed to have been discovered by Menaechmus (c. 380 BC – c. 320 BC) and since dealing with conic sections is equivalent to dealing with their respective equations, they played geometric roles equivalent to cubic equations and other higher order equations.\n\nMenaechmus knew that in a parabola, the equation y = \"l\"x holds, where \"l\" is a constant called the latus rectum, although he was not aware of the fact that any equation in two unknowns determines a curve. He apparently derived these properties of conic sections and others as well. Using this information it was now possible to find a solution to the problem of the duplication of the cube by solving for the points at which two parabolas intersect, a solution equivalent to solving a cubic equation.\n\nWe are informed by Eutocius that the method he used to solve the cubic equation was due to Dionysodorus (250 BC – 190 BC). Dionysodorus solved the cubic by means of the intersection of a rectangular hyperbola and a parabola. This was related to a problem in Archimedes' \"On the Sphere and Cylinder\". Conic sections would be studied and used for thousands of years by Greek, and later Islamic and European, mathematicians. In particular Apollonius of Perga's famous \"Conics\" deals with conic sections, among other topics.\n\nChinese Mathematics dates to at least 300 BC with the \"Zhoubi Suanjing\", generally considered to be one of the oldest Chinese mathematical documents.\n\n\"Chiu-chang suan-shu\" or \"The Nine Chapters on the Mathematical Art\", written around 250 BC, is one of the most influential of all Chinese math books and it is composed of some 246 problems. Chapter eight deals with solving determinate and indeterminate simultaneous linear equations using positive and negative numbers, with one problem dealing with solving four equations in five unknowns.\n\n\"Ts'e-yuan hai-ching\", or \"Sea-Mirror of the Circle Measurements\", is a collection of some 170 problems written by Li Zhi (or Li Ye) (1192 – 1279 CE). He used \"fan fa\", or Horner's method, to solve equations of degree as high as six, although he did not describe his method of solving equations.\n\n\"Shu-shu chiu-chang\", or \"Mathematical Treatise in Nine Sections\", was written by the wealthy governor and minister Ch'in Chiu-shao (c. 1202 – c. 1261) and with the invention of a method of solving simultaneous congruences, now called Chinese remainder theorem, it marks the high point in Chinese indeterminate analysis.\n\nThe earliest known magic squares appeared in China. In \"Nine Chapters\" the author solves a system of simultaneous linear equations by placing the coefficients and constant terms of the linear equations into a magic square (i.e. a matrix) and performing column reducing operations on the magic square. The earliest known magic squares of order greater than three are attributed to Yang Hui (fl. c. 1261 – 1275), who worked with magic squares of order as high as ten.\n\n\"Ssy-yüan yü-chien\"《四元玉鑒》, or \"Precious Mirror of the Four Elements\", was written by Chu Shih-chieh in 1303 and it marks the peak in the development of Chinese algebra. The four elements, called heaven, earth, man and matter, represented the four unknown quantities in his algebraic equations. The \"Ssy-yüan yü-chien\" deals with simultaneous equations and with equations of degrees as high as fourteen. The author uses the method of \"fan fa\", today called Horner's method, to solve these equations.\n\nThe \"Precious Mirror\" opens with a diagram of the arithmetic triangle (Pascal's triangle) using a round zero symbol, but Chu Shih-chieh denies credit for it. A similar triangle appears in Yang Hui's work, but without the zero symbol.\n\nThere are many summation series equations given without proof in the \"Precious mirror\". A few of the summation series are:\n\nDiophantus was a Hellenistic mathematician who lived c. 250 CE, but the uncertainty of this date is so great that it may be off by more than a century. He is known for having written \"Arithmetica\", a treatise that was originally thirteen books but of which only the first six have survived. \"Arithmetica\" has very little in common with traditional Greek mathematics since it is divorced from geometric methods, and it is different from Babylonian mathematics in that Diophantus is concerned primarily with exact solutions, both determinate and indeterminate, instead of simple approximations.\n\nIt is usually rather difficult to tell whether a given Diophantine equation is solvable. There is no evidence that suggests Diophantus even realized that there could be two solutions to a quadratic equation. He also considered simultaneous quadratic equations. Also, no general method may be abstracted from all Diophantus' solutions.\n\nIn \"Arithmetica\", Diophantus is the first to use symbols for unknown numbers as well as abbreviations for powers of numbers, relationships, and operations; thus he used what is now known as \"syncopated\" algebra. The main difference between Diophantine syncopated algebra and modern algebraic notation is that the former lacked special symbols for operations, relations, and exponentials. So, for example, what we would write as\nDiophantus would have written this as\nwhere the symbols represent the following:\nNote that the coefficients come after the variables and that addition is represented by the juxtaposition of terms. A literal symbol-for-symbol translation of Diophantus's syncopated equation into a modern symbolic equation would be the following:\n\nand, to clarify, if the modern parentheses and plus are used then the above equation can be rewritten as:\n\n\"Arithmetica\" is a collection of some 150 solved problems with specific numbers and there is no postulational development nor is a general method explicitly explained, although generality of method may have been intended and there is no attempt to find all of the solutions to the equations. \"Arithmetica\" does contain solved problems involving several unknown quantities, which are solved, if possible, by expressing the unknown quantities in terms of only one of them. \"Arithmetica\" also makes use of the identities:\n\nThe Indian mathematicians were active in studying about number systems. The earliest known Indian mathematical documents are dated to around the middle of the first millennium BC (around the 6th century BC).\n\nThe recurring themes in Indian mathematics are, among others, determinate and indeterminate linear and quadratic equations, simple mensuration, and Pythagorean triples.\n\nAryabhata (476–550) was an Indian mathematician who authored \"Aryabhatiya\". In it he gave the rules,\nand\n\nBrahmagupta (fl. 628) was an Indian mathematician who authored \"Brahma Sphuta Siddhanta\". In his work Brahmagupta solves the general quadratic equation for both positive and negative roots. In indeterminate analysis Brahmagupta gives the Pythagorean triads formula_19, formula_20, formula_21, but this is a modified form of an old Babylonian rule that Brahmagupta may have been familiar with. He was the first to give a general solution to the linear Diophantine equation ax + by = c, where a, b, and c are integers. Unlike Diophantus who only gave one solution to an indeterminate equation, Brahmagupta gave \"all\" integer solutions; but that Brahmagupta used some of the same examples as Diophantus has led some historians to consider the possibility of a Greek influence on Brahmagupta's work, or at least a common Babylonian source.\n\nLike the algebra of Diophantus, the algebra of Brahmagupta was syncopated. Addition was indicated by placing the numbers side by side, subtraction by placing a dot over the subtrahend, and division by placing the divisor below the dividend, similar to our notation but without the bar. Multiplication, evolution, and unknown quantities were represented by abbreviations of appropriate terms. The extent of Greek influence on this syncopation, if any, is not known and it is possible that both Greek and Indian syncopation may be derived from a common Babylonian source.\n\nBhāskara II (1114 – c. 1185) was the leading mathematician of the 12th century. In Algebra, he gave the general solution of Pell's equation. He is the author of \"Lilavati\" and \"Vija-Ganita\", which contain problems dealing with determinate and indeterminate linear and quadratic equations, and Pythagorean triples and he fails to distinguish between exact and approximate statements. Many of the problems in \"Lilavati\" and \"Vija-Ganita\" are derived from other Hindu sources, and so Bhaskara is at his best in dealing with indeterminate analysis.\n\nBhaskara uses the initial symbols of the names for colors as the symbols of unknown variables. So, for example, what we would write today as\n\nBhaskara would have written as\n\nwhere \"ya\" indicates the first syllable of the word for \"black\", and \"ru\" is taken from the word \"species\". The dots over the numbers indicate subtraction.\n\nThe first century of the Islamic Arab Empire saw almost no scientific or mathematical achievements since the Arabs, with their newly conquered empire, had not yet gained any intellectual drive and research in other parts of the world had faded. In the second half of the 8th century, Islam had a cultural awakening, and research in mathematics and the sciences increased. The Muslim Abbasid caliph al-Mamun (809–833) is said to have had a dream where Aristotle appeared to him, and as a consequence al-Mamun ordered that Arabic translation be made of as many Greek works as possible, including Ptolemy's \"Almagest\" and Euclid's \"Elements\". Greek works would be given to the Muslims by the Byzantine Empire in exchange for treaties, as the two empires held an uneasy peace. Many of these Greek works were translated by Thabit ibn Qurra (826–901), who translated books written by Euclid, Archimedes, Apollonius, Ptolemy, and Eutocius.\n\nThere are three theories about the origins of Arabic Algebra. The first emphasizes Hindu influence, the second emphasizes Mesopotamian or Persian-Syriac influence and the third emphasizes Greek influence. Many scholars believe that it is the result of a combination of all three sources.\n\nThroughout their time in power, before the fall of Islamic civilization, the Arabs used a fully rhetorical algebra, where often even the numbers were spelled out in words. The Arabs would eventually replace spelled out numbers (e.g. twenty-two) with Arabic numerals (e.g. 22), but the Arabs did not adopt or develop a syncopated or symbolic algebra until the work of Ibn al-Banna in the 13th century and Abū al-Hasan ibn Alī al-Qalasādī in the 15th century.\n\nThe Muslim Persian mathematician Muhammad ibn Mūsā al-Khwārizmī was a faculty member of the \"House of Wisdom\" (\"Bait al-Hikma\") in Baghdad, which was established by Al-Mamun. Al-Khwarizmi, who died around 850 CE, wrote more than half a dozen mathematical and astronomical works, some of which were based on the Indian \"Sindhind\". One of al-Khwarizmi's most famous books is entitled \"Al-jabr wa'l muqabalah\" or \"The Compendious Book on Calculation by Completion and Balancing\", and it gives an exhaustive account of solving polynomials up to the second degree. The book also introduced the fundamental concept of \"reduction\" and \"balancing\", referring to the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation. This is the operation which Al-Khwarizmi originally described as \"al-jabr\".\n\nR. Rashed and Angela Armstrong write:\n\n\"Al-Jabr\" is divided into six chapters, each of which deals with a different type of formula. The first chapter of \"Al-Jabr\" deals with equations whose squares equal its roots (ax = bx), the second chapter deals with squares equal to number (ax = c), the third chapter deals with roots equal to a number (bx = c), the fourth chapter deals with squares and roots equal a number (ax + bx = c), the fifth chapter deals with squares and number equal roots (ax + c = bx), and the sixth and final chapter deals with roots and number equal to squares (bx + c = ax).\n\nIn \"Al-Jabr\", al-Khwarizmi uses geometric proofs, he does not recognize the root x = 0, and he only deals with positive roots. He also recognizes that the discriminant must be positive and described the method of completing the square, though he does not justify the procedure. The Greek influence is shown by \"Al-Jabr\"'s geometric foundations and by one problem taken from Heron. He makes use of lettered diagrams but all of the coefficients in all of his equations are specific numbers since he had no way of expressing with parameters what he could express geometrically; although generality of method is intended.\n\nAl-Khwarizmi most likely did not know of Diophantus's \"Arithmetica\", which became known to the Arabs sometime before the 10th century. And even though al-Khwarizmi most likely knew of Brahmagupta's work, \"Al-Jabr\" is fully rhetorical with the numbers even being spelled out in words. So, for example, what we would write as\nDiophantus would have written as\nAnd al-Khwarizmi would have written as\n\n'Abd al-Hamīd ibn Turk authored a manuscript entitled \"Logical Necessities in Mixed Equations\", which is very similar to al-Khwarzimi's \"Al-Jabr\" and was published at around the same time as, or even possibly earlier than, \"Al-Jabr\". The manuscript gives exactly the same geometric demonstration as is found in \"Al-Jabr\", and in one case the same example as found in \"Al-Jabr\", and even goes beyond \"Al-Jabr\" by giving a geometric proof that if the discriminant is negative then the quadratic equation has no solution. The similarity between these two works has led some historians to conclude that Arabic algebra may have been well developed by the time of al-Khwarizmi and 'Abd al-Hamid.\n\nArabic mathematicians treated irrational numbers as algebraic objects. The Egyptian mathematician Abū Kāmil Shujā ibn Aslam (c. 850–930) was the first to accept irrational numbers (often in the form of a square root, cube root or fourth root) as solutions to quadratic equations or as coefficients in an equation. He was also the first to solve three non-linear simultaneous equations with three unknown variables.\n\nAl-Karkhi (953–1029), also known as Al-Karaji, was the successor of Abū al-Wafā' al-Būzjānī (940–998) and he discovered the first numerical solution to equations of the form ax + bx = c. Al-Karkhi only considered positive roots. Al-Karkhi is also regarded as the first person to free algebra from geometrical operations and replace them with the type of arithmetic operations which are at the core of algebra today. His work on algebra and polynomials, gave the rules for arithmetic operations to manipulate polynomials. The historian of mathematics F. Woepcke, in \"Extrait du Fakhri, traité d'Algèbre par Abou Bekr Mohammed Ben Alhacan Alkarkhi\" (Paris, 1853), praised Al-Karaji for being \"the first who introduced the theory of algebraic calculus\". Stemming from this, Al-Karaji investigated binomial coefficients and Pascal's triangle.\n\nOmar Khayyám (c. 1050 – 1123) wrote a book on Algebra that went beyond \"Al-Jabr\" to include equations of the third degree. Omar Khayyám provided both arithmetic and geometric solutions for quadratic equations, but he only gave geometric solutions for general cubic equations since he mistakenly believed that arithmetic solutions were impossible. His method of solving cubic equations by using intersecting conics had been used by Menaechmus, Archimedes, and Ibn al-Haytham (Alhazen), but Omar Khayyám generalized the method to cover all cubic equations with positive roots. He only considered positive roots and he did not go past the third degree. He also saw a strong relationship between Geometry and Algebra.\n\nIn the 12th century, Sharaf al-Dīn al-Tūsī (1135–1213) wrote the \"Al-Mu'adalat\" (\"Treatise on Equations\"), which dealt with eight types of cubic equations with positive solutions and five types of cubic equations which may not have positive solutions. He used what would later be known as the \"Ruffini-Horner method\" to numerically approximate the root of a cubic equation. He also developed the concepts of the maxima and minima of curves in order to solve cubic equations which may not have positive solutions. He understood the importance of the discriminant of the cubic equation and used an early version of Cardano's formula to find algebraic solutions to certain types of cubic equations. Some scholars, such as Roshdi Rashed, argue that Sharaf al-Din discovered the derivative of cubic polynomials and realized its significance, while other scholars connect his solution to the ideas of Euclid and Archimedes.\n\nSharaf al-Din also developed the concept of a function. In his analysis of\nthe equation formula_24 for example, he begins by changing the equation's form to formula_25. He then states that the question of whether the equation has a solution depends on whether or not the “function” on the left side reaches the value formula_26. To determine this, he finds a maximum value for the function. He proves that the maximum value occurs when formula_27, which gives the functional value formula_28. Sharaf al-Din then states that if this value is less than formula_26, there are no positive solutions; if it is equal to formula_26, then there is one solution at formula_27; and if it is greater than formula_26, then there are two solutions, one between formula_33 and formula_34 and one between formula_34 and formula_36.\n\nIn the early 15th century, Jamshīd al-Kāshī developed an early form of Newton's method to numerically solve the equation formula_37 to find roots of formula_38. Al-Kāshī also developed decimal fractions and claimed to have discovered it himself. However, J. Lennart Berggrenn notes that he was mistaken, as decimal fractions were first used five centuries before him by the Baghdadi mathematician Abu'l-Hasan al-Uqlidisi as early as the 10th century.\n\nAl-Hassār, a mathematician from Morocco specializing in Islamic inheritance jurisprudence during the 12th century, developed the modern symbolic mathematical notation for fractions, where the numerator and denominator are separated by a horizontal bar. This same fractional notation appeared soon after in the work of Fibonacci in the 13th century.\n\nAbū al-Hasan ibn Alī al-Qalasādī (1412–1486) was the last major medieval Arab algebraist, who made the first attempt at creating an algebraic notation since Ibn al-Banna two centuries earlier, who was himself the first to make such an attempt since Diophantus and Brahmagupta in ancient times. The syncopated notations of his predecessors, however, lacked symbols for mathematical operations. Al-Qalasadi \"took the first steps toward the introduction of algebraic symbolism by using letters in place of numbers\" and by \"using short Arabic words, or just their initial letters, as mathematical symbols.\"\n\nJust as the death of Hypatia signals the close of the Library of Alexandria as a mathematical center, so does the death of Boethius signal the end of mathematics in the Western Roman Empire. Although there was some work being done at Athens, it came to a close when in 529 the Byzantine emperor Justinian closed the pagan philosophical schools. The year 529 is now taken to be the beginning of the medieval period. Scholars fled the West towards the more hospitable East, particularly towards Persia, where they found haven under King Chosroes and established what might be termed an \"Athenian Academy in Exile\". Under a treaty with Justinian, Chosroes would eventually return the scholars to the Eastern Empire. During the Dark Ages, European mathematics was at its nadir with mathematical research consisting mainly of commentaries on ancient treatises; and most of this research was centered in the Byzantine Empire. The end of the medieval period is set as the fall of Constantinople to the Turks in 1453.\n\nThe 12th century saw a flood of translations from Arabic into Latin and by the 13th century, European mathematics was beginning to rival the mathematics of other lands. In the 13th century, the solution of a cubic equation by Fibonacci is representative of the beginning of a revival in European algebra.\n\nAs the Islamic world was declining after the 15th century, the European world was ascending. And it is here that Algebra was further developed.\n\nModern notation for arithmetic operations was introduced between the end of the 15th century and the beginning of the 16th century by Johannes Widmann and Michael Stifel. At the end of 16th century, François Viète introduced symbols, presently called variables, for representing indeterminate or unknown numbers. This created a new algebra consisting of computing with symbolic expressions as if they were numbers.\n\nAnother key event in the further development of algebra was the general algebraic solution of the cubic and quartic equations, developed in the mid-16th century. The idea of a determinant was developed by Japanese mathematician Kowa Seki in the 17th century, followed by Gottfried Leibniz ten years later, for the purpose of solving systems of simultaneous linear equations using matrices. Gabriel Cramer also did some work on matrices and determinants in the 18th century.\n\nBy tradition, the first unknown variable in an algebraic problem is nowadays represented by the symbol formula_39; if there is a second or a third unknown, these are labeled formula_40 and formula_41 respectively. Algebraic \"x\" is conventionally printed in italic type to distinguish it from the sign of multiplication.\n\nMathematical historians generally agree that the use of \"x\" in algebra was introduced by René Descartes and was first published in his treatise \"La Géométrie\" (1637). In that work, he used letters from the beginning of the alphabet (\"a\", \"b\", \"c\"...) for known quantities, and letters from the end of the alphabet (\"z\", \"y\", \"x\"...) for unknowns. It has been suggested that he later settled on \"x\" (in place of \"z\") for the first unknown because of its relatively greater abundance in the French and Latin typographical fonts of the time.\n\nThree alternative theories of the origin of algebraic \"x\" were suggested in the 19th century: (1) a symbol used by German algebraists and thought to be derived from a cursive letter \"r\", mistaken for \"x\"; (2) the numeral \"1\" with oblique strikethrough; and (3) an Arabic/Spanish source (see below). But the Swiss-American historian of mathematics Florian Cajori examined these and found all three lacking in concrete evidence; Cajori credited Descartes as the originator, and described his \"x\", \"y\", and \"z\" as \"free from tradition[,] and their choice purely arbitrary.\"\n\nNevertheless, the Hispano-Arabic hypothesis continues to have a presence in popular culture today. It is the claim that algebraic \"x\" is the abbreviation of a supposed loanword from Arabic in Old Spanish. The theory originated in 1884 with the German \"orientalist\" Paul de Lagarde, shortly after he published his edition of a 1505 Spanish/Arabic bilingual glossary in which Spanish \"cosa\" (\"thing\") was paired with its Arabic equivalent, شىء (\"shay\"), transcribed as \"xei\". (The \"sh\" sound in Old Spanish was routinely spelled \"x\".) Evidently Lagarde was aware that Arab mathematicians, in the \"rhetorical\" stage of algebra's development, often used that word to represent the unknown quantity. He surmised that \"nothing could be more natural\" (Nichts war also natürlicher...) than for the initial of the Arabic word—romanized as the Old Spanish \"x\"—to be adopted for use in algebra. A later reader reinterpreted Lagarde's conjecture as having \"proven\" the point. Lagarde was unaware that early Spanish mathematicians used, not a \"transcription\" of the Arabic word, but rather its \"translation\" in their own language, \"cosa\". There is no instance of \"xei\" or similar forms in several compiled historical vocabularies of Spanish.\n\nAlthough the mathematical notion of function was implicit in trigonometric and logarithmic tables, which existed in his day, Gottfried Leibniz was the first, in 1692 and 1694, to employ it explicitly, to denote any of several geometric concepts derived from a curve, such as abscissa, ordinate, tangent, chord, and the perpendicular. In the 18th century, \"function\" lost these geometrical associations.\n\nLeibniz realized that the coefficients of a system of linear equations could be arranged into an array, now called a matrix, which can be manipulated to find the solution of the system, if any. This method was later called Gaussian elimination. Leibniz also discovered Boolean algebra and symbolic logic, also relevant to algebra.\n\nThe ability to do algebra is a skill cultivated in mathematics education. As explained by Andrew Warwick, Cambridge University students in the early 19th century practiced \"mixed mathematics\", doing exercises based on physical variables such as space, time, and weight. Over time the association of variables with physical quantities faded away as mathematical technique grew. Eventually mathematics was concerned completely with abstract polynomials, complex numbers, hypercomplex numbers and other concepts. Application to physical situations was then called applied mathematics or mathematical physics, and the field of mathematics expanded to include abstract algebra. For instance, the issue of constructible numbers showed some mathematical limitations, and the field of Galois theory was developed.\n\nThe Hellenistic mathematician Diophantus has traditionally been known as \"the father of algebra\" but debate now exists as to whether or not Al-Khwarizmi deserves this title instead. Those who support Diophantus point to the fact that the algebra found in \"Al-Jabr\" is more elementary than the algebra found in \"Arithmetica\" and that \"Arithmetica\" is syncopated while \"Al-Jabr\" is fully rhetorical.\n\nThose who support Al-Khwarizmi point to the fact that he gave an exhaustive explanation for the algebraic solution of quadratic equations with positive roots, and was the first to teach algebra in an elementary form and for its own sake, whereas Diophantus was primarily concerned with the theory of numbers. Al-Khwarizmi also introduced the fundamental concept of \"reduction\" and \"balancing\" (which he originally used the term \"al-jabr\" to refer to), referring to the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation. Other supporters of Al-Khwarizmi point to his algebra no longer being concerned \"with a series of problems to be resolved, but an exposition which starts with primitive terms in which the combinations must give all possible prototypes for equations, which henceforward explicitly constitute the true object of study.\" They also point to his treatment of an equation for its own sake and \"in a generic manner, insofar as it does not simply emerge in the course of solving a problem, but is specifically called on to define an infinite class of problems.\"\n\n\n"}
{"id": "1938461", "url": "https://en.wikipedia.org/wiki?curid=1938461", "title": "Hyperstructure", "text": "Hyperstructure\n\nHyperstructures are algebraic structures equipped with at least one multi-valued operation, called a \"hyperoperation\". The largest classes of the hyperstructures are the ones called \"Hv\" – structures.\n\nA hyperoperation (*) on a non-empty set \"H\" is a mapping from \"H\" × \"H\" to power set \"P\"*(\"H\") (the set of all non-empty subsets of \"H\"), i.e.\n\n(*): \"H\" × \"H\" → \"P\"*(\"H\"): (\"x\", \"y\") → \"x\"*\"y\" ⊆ \"H\".\n\nIf \"Α\", \"Β\" ⊆ \"Η\" then we define\n\n(\"Η\",*) is a \"semihypergroup\" if (*) is an associative hyperoperation, i.e. \"x\"*(\"y\"*\"z\") = (\"x\"*\"y\")*\"z\", for all \"x\",\"y\",\"z\" of \"H\".\nFurthermore, a \"hypergroup\" is a semihypergroup (\"H\", *), where the reproduction axiom is valid, i.e. \"a\"*\"H\" = \"H\"*\"a\" = \"H\", for all \"a\" of \"H\".\n\n"}
{"id": "54227932", "url": "https://en.wikipedia.org/wiki?curid=54227932", "title": "Jacques Herbrand Prize", "text": "Jacques Herbrand Prize\n\nThe Jacques Herbrand Prize (French: Prix Jacques Herbrand) is an award given by the French Academy of Sciences to young researchers (up to 35 years) in the field of mathematics, physics, and their non-military applications.\nIt was created in 1996, and first awarded in 1998.\nIn 2001, it was renamed to Grand Prix Jacques Herbrand .\nUntil 2002, the prize was given each year in both fields; since 2003, it is given alternatingly.\nIt is endowed with 15000, later with 20000 euros, and named in honor of the French logician Jacques Herbrand (1908-1931).\n\n\n"}
{"id": "4244250", "url": "https://en.wikipedia.org/wiki?curid=4244250", "title": "Joel Spencer", "text": "Joel Spencer\n\nJoel Spencer (born April 20, 1946) is an American mathematician. He is a combinatorialist who has worked on probabilistic methods in combinatorics and on Ramsey theory. He received his doctorate from Harvard University in 1970, under the supervision of Andrew Gleason. He is currently () a professor at the Courant Institute of Mathematical Sciences of New York University.\n\nIn 1984 Spencer received a Lester R. Ford Award. He was an Erdős Lecturer at Hebrew University of Jerusalem in 2001. In 2012 he became a fellow of the American Mathematical Society.\nHe was elected as a fellow of the Society for Industrial and Applied Mathematics in 2017, \"for contributions to discrete mathematics and theory of computing, particularly random graphs and networks, Ramsey theory, logic, and randomized algorithms\".\n\n\n\n"}
{"id": "3130497", "url": "https://en.wikipedia.org/wiki?curid=3130497", "title": "Jovan Karamata", "text": "Jovan Karamata\n\nJovan Karamata (; February 1, 1902 – August 14, 1967) was Serbian mathematician. He is remembered for contributions to analysis, in particular, the Tauberian theory and the theory of slowly varying functions. Considered to be among the most influential Serbian mathematicians of the 20th century, Karamata was one of the founders of the Mathematical Institute of the Serbian Academy of Sciences and Arts, established in 1946.\n\nJovan Karamata was born in Zagreb on February 1, 1902 into a family descended from merchants based in the city of Zemun, which was then in Austria-Hungary, and now in Serbia. Being of Aromanian origin, the family traced its roots back to Pyrgoi, Eordaia, West Macedonia \"(his father Ioannis Karamatas was the president of the \"Greek Community of Zemun\")\". Its business affairs on the borders of the Austro-Hungarian and Ottoman empires were very well known. In 1914, he finished most of his primary school in Zemun but because of constant warfare on the borderlands, Karamata's father sent him, together with his brothers and his sister, to Switzerland for their own safety. In Lausanne, 1920, he finished primary school oriented towards mathematics and sciences. In the same year he enrolled at the Engineering faculty of Belgrade University and, after several years moved to the Philosophy and Mathematicians sector, where he graduated in 1925.\n\nHe spent the years 1927–1928 in Paris, as a fellow of the Rockefeller Foundation, and in 1928 he became Assistant for Mathematics at the Faculty of Philosophy of Belgrade University. In 1930 he became Assistant Professor, in 1937 Associate Professor and, after the end of World War II, in 1950 he became Full Professor. In 1951 he was elected Full Professor at the University of Geneva. In 1933 he became a member of Yugoslav Academy of Sciences and Arts, Czech Royal Society in 1936, and Serbian Royal Academy in 1939 as well as a fellow of Serbian Academy of Sciences in 1948. He was one of the founders of the \"Mathematical Institute\" of the Serbian Academy of Sciences and Arts in 1946.\n\nKaramata was member of the Swiss, French and German mathematical societies, the French Association for the Development of Science, and the primary editor of the journal \"L’Enseignement Mathématique\" in Geneva. He also taught at the University of Novi Sad.\n\nIn 1931 he married Emilija Nikolajevic, who gave birth to their two sons and a daughter. His wife died in 1959. After a long illness, Karamata died on August 14, 1967 in Geneva. His ashes rest in his native town of Zemun.\n\nKaramata published 122 scientific papers, 15 monographs and text-books as well as 7 professional-pedagogical papers.\n\nKaramata is best known for his work on mathematical analysis. He introduced the notion of regularly varying function, and discovered a new class of theorems of Tauberian type, today known as Karamata's tauberian theorems. He also worked on Mercer's theorems, Froullani’s integrals, and other topics in analysis. In 1935 he introduced the brackets and braces notation for Stirling numbers (analogous to the binomial coefficients notation), which is now known as Karamata notation. He is also cited for Karamata's inequality.\n\nIn Serbia, Karamata founded the \"Karamata's (Yugoslav) school of mathematics”. Today, Karamata is the most frequently cited Serbian mathematician. He is the developer and co-developer of dozens of mathematical theorems and has had a lasting influence in 20th-century mathematics.\n\n\n"}
{"id": "24659626", "url": "https://en.wikipedia.org/wiki?curid=24659626", "title": "Karin Erdmann", "text": "Karin Erdmann\n\nKarin Erdmann (born 1948) is a German mathematician specializing in the areas of algebra known as representation theory (especially modular representation theory) and homological algebra (especially Hochschild cohomology). She is notable for her work in modular representation theory which has been cited over 500 times according to the Mathematical Reviews. Her nephew Martin Erdmann is professor for experimental particle physics at the RWTH Aachen University.\n\nShe attended the Justus-Liebig-Universität Gießen and wrote her Ph.D. thesis on \"2-Hauptblöcke von Gruppen mit Dieder-Gruppen als 2-Sylow-Gruppen\" (Principal 2-blocks of groups with dihedral Sylow 2-subgroups) in 1976 under the direction of Gerhard O. Michler. She has contributed to the understanding of the representation theory of the symmetric group.\n\nShe is a university lecturer at the Mathematical Institute at the University of Oxford where she has had 21 doctoral students and 29 descendants. She has published over 75 papers and her work has been cited over 500 times.\n\nErdmann was the inaugural Emmy Noether Lecturer of the German Mathematical Society in 2008.\n\n\n"}
{"id": "7276069", "url": "https://en.wikipedia.org/wiki?curid=7276069", "title": "Kendall tau distance", "text": "Kendall tau distance\n\nThe Kendall tau rank distance is a metric that counts the number of pairwise disagreements between two ranking lists. The larger the distance, the more dissimilar the two lists are. Kendall tau distance is also called bubble-sort distance since it is equivalent to the number of swaps that the bubble sort algorithm would take to place one list in the same order as the other list. The Kendall tau distance was created by Maurice Kendall.\n\nThe Kendall tau ranking distance between two lists formula_1 and formula_2 is\n\nwhere \n\nformula_9 will be equal to 0 if the two lists are identical and formula_10 (where formula_11 is the list size) if one list is the reverse of the other. Often Kendall tau distance is normalized by dividing by formula_10 so a value of 1 indicates maximum disagreement. The normalized Kendall tau distance therefore lies in the interval [0,1].\n\nKendall tau distance may also be defined as\n\nwhere\n\nKendall tau distance can also be defined as the total number of discordant pairs.\n\nKendall tau distance in Rankings: A permutation (or ranking) is an array of N integers where each of the integers between 0 and N-1 appears exactly once.\nThe Kendall tau distance between two rankings is the number of pairs that are in different order in the two rankings. For example, the Kendall tau distance between 0 3 1 6 2 5 4 and 1 0 3 6 4 2 5 is four because the pairs 0-1, 3-1, 2-4, 5-4 are in different order in the two rankings, but all other pairs are in the same order.\n\nIf Kendall tau function is performed as formula_22 instead of formula_9 (where formula_1 and formula_2 are the rankings of formula_26 and formula_27 elements respectively), then triangular inequality is not guaranteed. The triangular inequality fails in cases where there are repetitions in the lists. So then we are not any more dealing with a metric.\n\nSuppose we rank a group of five people by height and by weight:\n\nHere person A is tallest and third-heaviest, and so on.\n\nIn order to calculate the Kendall tau distance, pair each person with every other person and count the number of times the values in list 1 are in the opposite order of the values in list 2.\n\nSince there are 4 pairs whose values are in opposite order, the Kendall tau distance is 4. The normalized Kendall tau distance is\n\nA value of 0.4 indicates that 40% of pairs differ in ordering between the two lists.\n\nGiven two rankings formula_29, it is possible to rename the items such that formula_30. Then, the problem of computing the Kendall tau distance reduces to computing the number of \"inversions\" in formula_2 --- the number of index pairs formula_32 such that formula_33 while formula_34. There are several algorithms for calculating this number.\n\n\n\n"}
{"id": "42563034", "url": "https://en.wikipedia.org/wiki?curid=42563034", "title": "KiSAO", "text": "KiSAO\n\nThe Kinetic Simulation Algorithm Ontology (KiSAO) supplies information about existing algorithms available for the simulation of systems biology models, their characterization and interrelationships. KiSAO is part of the BioModels.net project and of the COMBINE initiative.\n\nKiSAO consists of three main branches:\nThe elements of each algorithm branch are linked to characteristic and parameter branches using \"has characteristic\" and \"has parameter\" relationships accordingly.The algorithm branch itself is hierarchically structured using \"subClassOf\" relationships, which denote that the descendant algorithms were derived from, or specify, more general ancestors.\n\n"}
{"id": "17140644", "url": "https://en.wikipedia.org/wiki?curid=17140644", "title": "Laplace invariant", "text": "Laplace invariant\n\nIn differential equations, the Laplace invariant of any of certain differential operators is a certain function of the coefficients and their derivatives. Consider a bivariate hyperbolic differential operator of the second order\n\nwhose coefficients\n\nare smooth functions of two variables. Its Laplace invariants have the form\n\nTheir importance is due to the classical theorem:\n\nTheorem: \"Two operators of the form are equivalent under gauge transformations if and only if their Laplace invariants coincide pairwise.\"\n\nHere the operators\n\nare called \"equivalent\" if there is a gauge transformation that takes one to the other:\n\nLaplace invariants can be regarded as factorization \"remainders\" for the initial operator \"A\":\n\nIf at least one of Laplace invariants is not equal to zero, i.e.\n\nthen this representation is a first step of the Laplace–Darboux transformations used for solving\n\"non-factorizable\" bivariate linear partial differential equations (LPDEs).\n\nIf both Laplace invariants are equal to zero, i.e.\n\nthen the differential operator \"A\" is factorizable and corresponding linear partial differential equation of second order is solvable.\n\nLaplace invariants have been introduced for a bivariate linear partial differential operator (LPDO) of order 2 and of hyperbolic type. They are a particular case of \"generalized invariants\" which can be constructed for a bivariate LPDO of arbitrary order and arbitrary type; see Invariant factorization of LPDOs.\n\n\n"}
{"id": "2915428", "url": "https://en.wikipedia.org/wiki?curid=2915428", "title": "List of Wenninger polyhedron models", "text": "List of Wenninger polyhedron models\n\nThis is an indexed list of the uniform and stellated polyhedra from the book \"Polyhedron Models\", by Magnus Wenninger.\n\nThe book was written as a guide book to building polyhedra as physical models. It includes templates of face elements for construction and helpful hints in building, and also brief descriptions on the theory behind these shapes. It contains the 75 nonprismatic uniform polyhedra, as well as 44 stellated forms of the convex regular and quasiregular polyhedra.\n\nModels listed here can be cited as \"Wenninger Model Number \"N\"\", or \"W\" for brevity.\n\nThe polyhedra are grouped in 5 tables: Regular (1–5), Semiregular (6–18), regular star polyhedra (20–22,41), Stellations and compounds (19–66), and uniform star polyhedra (67–119). \"The four regular star polyhedra are listed twice because they belong to both the uniform polyhedra and stellation groupings.\"\n\n\n\n"}
{"id": "901613", "url": "https://en.wikipedia.org/wiki?curid=901613", "title": "Logical framework", "text": "Logical framework\n\nIn logic, a logical framework provides a means to define (or present) a logic as a signature in a higher-order type theory in such a way that provability of a formula in the original logic reduces to a type inhabitation problem in the framework type theory. This approach has been used successfully for (interactive) automated theorem proving. The first logical framework was Automath; however, the name of the idea comes from the more widely known Edinburgh Logical Framework, LF. Several more recent proof tools like Isabelle are based on this idea. Unlike a direct embedding, the logical framework approach allows many logics to be embedded in the same type system.\n\nA logical framework is based on a general treatment of syntax, rules and proofs by means of a dependently typed lambda calculus. Syntax is treated in a style similar to, but more general than Per Martin-Löf's system of arities.\n\nTo describe a logical framework, one must provide the following:\n\n\nThis is summarized by:\n\nIn the case of the LF logical framework, the meta-language is the λΠ-calculus. This is a system of first-order dependent function types which are related by the propositions as types principle to first-order minimal logic. The key features of the λΠ-calculus are that it consists of entities of three levels: objects, types and kinds (or type classes, or families of types). It is predicative, all well-typed terms are strongly normalizing and Church-Rosser and the property of being well-typed is decidable. However, type inference is undecidable.\n\nA logic is represented in the LF logical framework by the judgements-as-types representation mechanism. This is inspired by Per Martin-Löf's development of Kant's notion of judgement, in the 1983 Siena Lectures. The two higher-order judgements, the hypothetical formula_1 and the general, formula_2, correspond to the ordinary and dependent function space, respectively. The methodology of judgements-as-types is that judgements are represented as the types of their proofs. A logical system formula_3 is represented by its signature which assigns kinds and types to a finite set of constants that represents its syntax, its judgements and its rule schemes. An object-logic's rules and proofs are seen as primitive proofs of hypothetico-general judgements formula_4.\n\nAn implementation of the LF logical framework is provided by the Twelf system at Carnegie Mellon University. Twelf includes\n\n\n\n"}
{"id": "298420", "url": "https://en.wikipedia.org/wiki?curid=298420", "title": "Maxima and minima", "text": "Maxima and minima\n\nIn mathematical analysis, the maxima and minima (the respective plurals of maximum and minimum) of a function, known collectively as extrema (the plural of extremum), are the largest and smallest value of the function, either within a given range (the local or relative extrema) or on the entire domain of a function (the global or absolute extrema). Pierre de Fermat was one of the first mathematicians to propose a general technique, adequality, for finding the maxima and minima of functions.\n\nAs defined in set theory, the maximum and minimum of a set are the greatest and least elements in the set, respectively. Unbounded infinite sets, such as the set of real numbers, have no minimum or maximum.\n\nA real-valued function \"f\" defined on a domain \"X\" has a global (or absolute) maximum point at \"x\" if \"f\"(\"x\") ≥ \"f\"(\"x\") for all \"x\" in \"X\". Similarly, the function has a global (or absolute) minimum point at \"x\" if \"f\"(\"x\") ≤ \"f\"(\"x\") for all \"x\" in \"X\". The value of the function at a maximum point is called the maximum value of the function and the value of the function at a minimum point is called the minimum value of the function.\n\nIf the domain \"X\" is a metric space then \"f\" is said to have a local (or relative) maximum point at the point \"x\" if there exists some \"ε\" > 0 such that \"f\"(\"x\") ≥ \"f\"(\"x\") for all \"x\" in \"X\" within distance \"ε\" of \"x\". Similarly, the function has a local minimum point at \"x\" if \"f\"(\"x\") ≤ \"f\"(\"x\") for all \"x\" in \"X\" within distance \"ε\" of \"x\". A similar definition can be used when \"X\" is a topological space, since the definition just given can be rephrased in terms of neighbourhoods.\n\nIn both the global and local cases, the concept of a strict extremum can be defined. For example, \"x\" is a strict global maximum point if, for all \"x\" in \"X\" with \"x\" ≠ \"x\", we have \"f\"(\"x\") > \"f\"(\"x\"), and \"x\" is a strict local maximum point if there exists some \"ε\" > 0 such that, for all \"x\" in \"X\" within distance \"ε\" of \"x\" with \"x\" ≠ \"x\", we have \"f\"(\"x\") > \"f\"(\"x\"). Note that a point is a strict global maximum point if and only if it is the unique global maximum point, and similarly for minimum points.\n\nA continuous real-valued function with a compact domain always has a maximum point and a minimum point. An important example is a function whose domain is a closed (and bounded) interval of real numbers (see the graph above).\n\nFinding global maxima and minima is the goal of mathematical optimization. If a function is continuous on a closed interval, then by the extreme value theorem global maxima and minima exist. Furthermore, a global maximum (or minimum) either must be a local maximum (or minimum) in the interior of the domain, or must lie on the boundary of the domain. So a method of finding a global maximum (or minimum) is to look at all the local maxima (or minima) in the interior, and also look at the maxima (or minima) of the points on the boundary, and take the largest (or smallest) one.\n\nLocal extrema of differentiable functions can be found by Fermat's theorem, which states that they must occur at critical points. One can distinguish whether a critical point is a local maximum or local minimum by using the first derivative test, second derivative test, or higher-order derivative test, given sufficient differentiability.\n\nFor any function that is defined piecewise, one finds a maximum (or minimum) by finding the maximum (or minimum) of each piece separately, and then seeing which one is largest (or smallest).\n\n\nFor functions of more than one variable, similar conditions apply. For example, in the (enlargeable) figure at the right, the necessary conditions for a \"local\" maximum are similar to those of a function with only one variable. The first partial derivatives as to \"z\" (the variable to be maximized) are zero at the maximum (the glowing dot on top in the figure). The second partial derivatives are negative. These are only necessary, not sufficient, conditions for a local maximum because of the possibility of a saddle point. For use of these conditions to solve for a maximum, the function \"z\" must also be differentiable throughout. The second partial derivative test can help classify the point as a relative maximum or relative minimum.\nIn contrast, there are substantial differences between functions of one variable and functions of more than one variable in the identification of global extrema. For example, if a bounded differentiable function \"f\" defined on a closed interval in the real line has a single critical point, which is a local minimum, then it is also a global minimum (use the intermediate value theorem and Rolle's theorem to prove this by reductio ad absurdum). In two and more dimensions, this argument fails, as the function\nshows. Its only critical point is at (0,0), which is a local minimum with ƒ(0,0) = 0. However, it cannot be a global one, because ƒ(2,3) = −5.\nIf the domain of a function for which an extremum is to be found consists itself of functions, i.e. if an extremum is to be found of a functional, the extremum is found using the calculus of variations.\n\nMaxima and minima can also be defined for sets. In general, if an ordered set \"S\" has a greatest element \"m\", \"m\" is a maximal element. Furthermore, if \"S\" is a subset of an ordered set \"T\" and \"m\" is the greatest element of \"S\" with respect to order induced by \"T\", \"m\" is a least upper bound of \"S\" in \"T\". The similar result holds for least element, minimal element and greatest lower bound.\n\nIn the case of a general partial order, the least element (smaller than all other) should not be confused with a minimal element (nothing is smaller). Likewise, a greatest element of a partially ordered set (poset) is an upper bound of the set which is contained within the set, whereas a maximal element \"m\" of a poset \"A\" is an element of \"A\" such that if \"m\" ≤ \"b\" (for any \"b\" in \"A\") then \"m\" = \"b\". Any least element or greatest element of a poset is unique, but a poset can have several minimal or maximal elements. If a poset has more than one maximal element, then these elements will not be mutually comparable.\n\nIn a totally ordered set, or \"chain\", all elements are mutually comparable, so such a set can have at most one minimal element and at most one maximal element. Then, due to mutual comparability, the minimal element will also be the least element and the maximal element will also be the greatest element. Thus in a totally ordered set we can simply use the terms minimum and maximum. If a chain is finite then it will always have a maximum and a minimum. If a chain is infinite then it need not have a maximum or a minimum. For example, the set of natural numbers has no maximum, though it has a minimum. If an infinite chain \"S\" is bounded, then the closure \"Cl(S)\" of the set occasionally has a minimum and a maximum, in such case they are called the greatest lower bound and the least upper bound of the set \"S\", respectively.\n\n\n"}
{"id": "597998", "url": "https://en.wikipedia.org/wiki?curid=597998", "title": "Multinomial theorem", "text": "Multinomial theorem\n\nIn mathematics, the multinomial theorem describes how to expand a power of a sum in terms of powers of the terms in that sum. It is the generalization of the binomial theorem to polynomials.\n\nFor any positive integer \"m\" and any nonnegative integer \"n\", the multinomial formula tells us how a sum with \"m\" terms expands when raised to an arbitrary power \"n\":\n\nwhere\nis a multinomial coefficient. The sum is taken over all combinations of nonnegative integer indices \"k\" through \"k\" such that the sum of all \"k\" is \"n\". That is, for each term in the expansion, the exponents of the \"x\" must add up to \"n\". Also, as with the binomial theorem, quantities of the form \"x\" that appear are taken to equal 1 (even when \"x\" equals zero).\n\nIn the case \"m\" = 2, this statement reduces to that of the binomial theorem.\n\nThe third power of the trinomial \"a\" + \"b\" + \"c\" is given by\n\nThis can be computed by hand using the distributive property of multiplication over addition, but it can also be done (perhaps more easily) with the multinomial theorem, which gives us a simple formula for any coefficient we might want. It is possible to \"read off\" the multinomial coefficients from the terms by using the multinomial coefficient formula. For example:\n\nThe statement of the theorem can be written concisely using multiindices:\n\nwhere \n\nand\n\nThis proof of the multinomial theorem uses the binomial theorem and induction on \"m\".\n\nFirst, for \"m\" = 1, both sides equal \"x\" since there is only one term \"k\" = \"n\" in the sum. For the induction step, suppose the multinomial theorem holds for \"m\". Then\nby the induction hypothesis. Applying the binomial theorem to the last factor,\nwhich completes the induction. The last step follows because\nas can easily be seen by writing the three coefficients using factorials as follows:\n\nThe numbers\nappearing in the theorem are the multinomial coefficients. They can be expressed in numerous ways, including as a product of binomial coefficients or of factorials:\n\nThe substitution of \"x\" = 1 for all \"i\" into the multinomial theorem\ngives immediately that \n\nThe number of terms in a multinomial sum, #, is equal to the number of monomials of degree \"n\" on the variables \"x\", …, \"x\":\n\nThe count can be performed easily using the method of stars and bars.\n\nThe largest power of a prime formula_22 that divides a multinomial coefficient may be computed using a generalization of Kummer's theorem.\n\nThe multinomial coefficients have a direct combinatorial interpretation, as the number of ways of depositing \"n\" distinct objects into \"m\" distinct bins, with \"k\" objects in the first bin, \"k\" objects in the second bin, and so on.\n\nIn statistical mechanics and combinatorics if one has a number distribution of labels then the multinomial coefficients naturally arise from the binomial coefficients. Given a number distribution {\"n\"} on a set of \"N\" total items, \"n\" represents the number of items to be given the label \"i\". (In statistical mechanics \"i\" is the label of the energy state.)\n\nThe number of arrangements is found by \n\nMultiplying the number of choices at each step results in:\n\nUpon cancellation, we arrive at the formula given in the introduction.\n\nThe multinomial coefficient is also the number of distinct ways to permute a multiset of \"n\" elements, and \"k\" are the multiplicities of each of the distinct elements. For example, the number of distinct permutations of the letters of the word MISSISSIPPI, which has 1 M, 4 Is, 4 Ss, and 2 Ps is\n\nOne can use the multinomial theorem to generalize Pascal's triangle or Pascal's pyramid to Pascal's simplex. This provides a quick way to generate a lookup table for multinomial coefficients.\n\n"}
{"id": "1884117", "url": "https://en.wikipedia.org/wiki?curid=1884117", "title": "Nyāya Sūtras", "text": "Nyāya Sūtras\n\nThe Nyāya Sūtras is an ancient Indian Sanskrit text composed by , and the foundational text of the Nyaya school of Hindu philosophy. The date when the text was composed, and the biography of its author is unknown, but variously estimated between 6th-century BCE and 2nd-century CE. The text may have been composed by more than one author, over a period of time. The text consists of five books, with two chapters in each book, with a cumulative total of 528 aphoristic sutras, about rules of reason, logic, epistemology and metaphysics.\n\nThe Nyāya Sūtras is a Hindu text, notable for focusing on knowledge and logic, and making no mention of Vedic rituals. The first book is structured as a general introduction and table of contents of sixteen categories of knowledge. Book two is about \"pramana\" (epistemology), book three is about \"prameya\" or the objects of knowledge, and the text discusses the nature of knowledge in remaining books. It set the foundation for Nyaya tradition of the empirical theory of validity and truth, opposing uncritical appeals to intuition or scriptural authority.\n\nThe Nyaya sutras cover a wide range of topics, including Tarka-Vidyā, the science of debate or Vāda-Vidyā, the science of discussion. The \"Nyāya Sutras\" are related to but extend the epistemological and metaphysical system. Later commentaries expanded, expounded and discussed Nyaya sutras, the earlier surviving commentaries being by Vātsyāyana (c.450–500 CE), followed by the \"Nyāyavārttika\" of Uddyotakāra (c. 6th–7th centuries), Vācaspati Miśra's \"Tātparyatīkā\" (9th century), Udayana's \"Tātparyapariśuddhi\" (10th century), and Jayanta's \"Nyāyamañjarī\" (10th century).\n\nThe \"Nyaya-sutras\" is attributed to Gautama, who was at least the principal author. According to Karl Potter, this name has been a very common Indian name, and the author is also reverentially referred to as Gotama, Dirghatapas and Aksapada Gautama. Little is known about Gautama, or which century he lived in. Scholarly estimates, based on textual analysis, vary from the 6th century BCE, making him a contemporary of Buddha and Mahavira, to as late as the 2nd century CE. Some scholars favor the theory that the cryptic text Nyaya-sutras was expanded over time by multiple authors, with the earliest layer from about mid-first millennium BCE that was composed by Gautama. The earliest layer is likely to be Book 1 and 5 of the text, while Book 3 and 4 may have been added last, but this is not certain.\n\nIt is likely, states Jeaneane Fowler, that Nyaya and the science of reason stretch back into the Vedic era; it developed in the ancient Indian tradition that involved \"dialectical tournaments, in the halls of kings and schools of Vedic philosophers\", and Gautama was the one who distilled and systematized this pre-existing knowledge into sutras, or aphoristic compilations called \"nyayasutras\".\n\nThe Nyaya school of Hinduism influenced all other schools of Hindu philosophy, as well as Buddhism. Despite their differences, these scholars studied with each other and debated ideas, with Tibetan records suggesting that Buddhist scholars spent years residing with Hindu Nyaya scholars to master the art of reasoning and logic. This cooperation has enabled scholars to place the currently surviving version of the \"Nyayasutras\", to a terminus ante quem (completed before) date of about the 2nd century CE, because one of the most famous and established Buddhist scholars of that era, Nagarjuna, explicitly states, \"sutra 4.2.25 is addressed against the Madhyamika system\" of Buddhism. Other ancient Buddhist texts confirm that Nyayasutras existed before them, and the text is considered the primary text of old Nyaya school of Hinduism.\n\nThe text is written in \"sutra\" genre. A \"sutra\" is a Sanskrit word that means \"string, thread\", and represents a condensed manual of knowledge of a specific field or school. Each \"sutra\" is any short rule, like a theorem distilled into few words or syllables, around which \"teachings of ritual, philosophy, grammar or any field of knowledge\" can be woven. Sutras were compiled to be remembered, used as reference and to help teach and transmit ideas from one generation to the next.\n\nThe Nyayasutra is divided into five books, each book subdivided into two chapters each. The structure of the text is, states Potter, a layout of \"ahnikas\" or lessons served into daily portions, with portion consisting of a number of \"sutras\" or aphorisms. The architecture of the text is also split and collated into \"prakaranas\" or topics, which later commentators such as Vatsyayana and Vacaspati Misra utilized to compose their bhasya, ancient texts that have survived into the modern era. There are several surviving manuscripts of the Nyayasutras, with a slight difference in number of sutras, of which the \"Chowkhamba\" edition is often studied.\n\nThe first sutra 1.1.1 of the text asserts its scope and the following sixteen categories of knowledge as a means to gain competence in any field of interest:\nThese sixteen categories cover many sections of the text. The verse 1.1.2 of the \"Nyāya Sūtra\" declares the text's goal is to study and describe the attainment of liberation of soul from wrong knowledge, faults and sorrow, through the application of above sixteen categories of perfecting knowledge.\n\nThe Nyaya-sutras assert the premise that \"all knowledge is not intrinsically valid\", that \"most knowledge is not valid unless proven\" and \"truth exists whether we human beings know it or not\". However, states Fowler, the text accepts the foundation that \"some knowledge is self evident\" and axiomatic in every field of knowledge, which can neither be proven nor needs proof, such as \"I am conscious\", \"I think\" and \"soul exists\". Furthermore, the text presents its thesis that knowledge is not self-revealing, man must make effort to gain knowledge and this is a systematic process that empowers one to learn correct knowledge, and abandon incorrect knowledge.\n\nThe Nyāya sutras asserts and then discusses four reliable means of obtaining knowledge (\"pramāṇa\"), viz., Perception, Inference, Comparison and Reliable Testimony.\n\nThe Nyayasutras assert that perception is the primary proper means of gaining true knowledge. All other epistemic methods are directly or indirectly based on perception, according to the text, and anything that is claimed to be \"true knowledge\" must be confirmed or confirmable by perception. This it terms as the doctrine of convergence, and this doctrine includes direct or implied perception. Gautama defines perception as the knowledge that arises by the contact of one or more senses with an object or phenomenon. Gautama dedicates many sutras to discuss both the object and subject in the process of perception, and when senses may be unreliable. Erratic eyesight or other senses (\"Avyabhicara\") can be a source of doubt or false knowledge, as can prejudgmental or prejudicial state of mind, states the Nyayasutras.\n\nThe text asserts \"Pratyaksa\" leads to \"Laukika\" or ordinary knowledge, where the five senses directly and clearly apprehend a reality, and this is true definite knowledge according to the text. It defines indefinite knowledge as one where there is doubt, and the text gives an example of seeing a distant stationary object in the evening and wondering whether it is a post or a man standing in the distance. In some of these cases, states Nyayasutras, correct knowledge is formulated by the principle of cumulative evidence. \"Manas\" (mind) is considered an internal sense, in the text, and it can either lead to correct or incorrect knowledge depending on how it includes, excludes or integrates information. These ideas are compiled, in later chapters of the text, into its treatise on \"Aprama\" (Theory on Errors).\n\nThe epistemic rationale for inference as a reliable source of knowledge, and Nyaya's theory has been a major contribution to the diverse schools of Hinduism, and other schools looked up to Nyaya scholars for insights on correct knowledge and incorrect knowledge through inference. The sections in Nyayasutras on inference blossomed into a treatise on syllogism over time.\n\nNyayasutras defines inference as the knowledge that follows or derives from other knowledge. It always follows perception, states the text, and is a universal relation or essential principle. One form of inference is a \"Purvavat\", or as Fowler translates, \"from cause to effect or a priori\". Thus, if a path or road is wet or river is swollen, states the text, then \"it has rained\" is a valid knowledge. The sutras assert that the \"universal relationship\" between the two is necessary for correct, reliable knowledge, that is \"if in all cases of A, B is true, then one may correctly infer B whenever A is perceived\". Further, there is a causal relation between the two, whether one knows or not of that cause, but inferred knowledge does not require one to know the cause for it to be valid knowledge, states Nyayasutra. The text states one must not confuse coexistence as a universal relation, and that while deduction and induction both are useful and valid means for gaining true knowledge, it lists rules when this method can lead to false knowledge.\n\nThe word \"upamana\", states Fowler, is a compound of \"upa\" (similarity) and \"mana\" (knowledge). It is a means of gaining knowledge based on \"similarity, comparison, analogy\", and considered reliable in Nyaya and many schools of Hinduism (but not in Vaisheshika and Charvaka, or Buddhism).\n\nThe Nyayasutras define \"upamana\" as the knowledge of a thing based on \"its likeness to another thing which is familiar\". It differs from \"Anumana\" (inference) in lacking a direct or immediate causal relation. It differs from \"Pratyaksha\" (perception), states the text, in using a linguistic referent and the foundation of pre-existing knowledge within the individual and what he learnt from his teachers, friends, family and past knowledge inherited from the wise, through a process of social cooperation. The \"Upamana\" method is secondary, it relies on perception, combined with linguistic referent and context. Comparison is not isolated \"pramana\" means, and sometimes works together with the \"Anumana\" and \"Sabda\" epistemic methods. Comparison is, in Nyayasutras, the process of permeating or infusing hypothesis, examples and tests, thus leading to objectivity and correct knowledge about something new and what one already presumes to know.\n\n\"Śabda\" (Sanskrit: शब्द, Word), in \"Nyayasutras\", means relying on word, testimony of a reliable source. \"Sabda-pramana\" has been an accepted and reliable method to knowledge by all orthodox schools of Hinduism including Nyaya, asserting that a human being needs to know numerous facts, and with the limited time and energy available, he can learn only a fraction of those facts and truths directly. He must rely on others, his parents, family, friends, teachers, ancestors and kindred members of society to rapidly acquire and share knowledge and thereby enrich each other's lives. This means of gaining correct knowledge is either spoken or written, but it is through \"Sabda\" (words). In addition to words, state the Nyayasutras, \"Shabda\" as a means of true knowledge depends on an agreed convention on what words mean, the structure of sentences, establishing context and their import. The source must be reliable and comprehensible, and the receiver of knowledge must be able to understand the knowledge therefrom.\n\nThe reliability of the source is important, and legitimate knowledge can only come from the \"Sabda\" of reliable sources. The schools of Hindu philosophy have debated if, how and when reliability of source can be objectively established. Gautama, in the Nyayasutras, offers a description for a reliable source. Some schools, such as Charvaka, state that this is never possible, and therefore \"Sabda\" in the Vedas or anyone else, can never be a proper \"pramana\". Other schools debate means to establish reliability.\n\nThe text, in sutras 1.1.32 and 1.1.39 presents its theory of proper Argument, stating that it must include five members:\n\nThe text defines and aphoristically discusses each of these.\n\nThe Nyayasutras define and discuss \"Samsaya\" (Sanskrit: संशय, doubt) in sutras 1.1.23, 2.1.1 to 2.1.7, 3.2.1, 4.2.4 among others. This discussion is similar to those found in other schools of Hindu philosophy, expands on the theory of doubt presented by Kanada in the Vaisheshika school, but disagrees with the Charvaka school's theory of doubt and consequent \"there is no empirical knowledge ever\".\n\nThe theory of doubt, according to the Nyayasutras, starts with the premise that doubt is part of the human learning process and occurs when conflicting possibilities exist with regard to a cognized object. Doubt is neither error nor absence of knowledge, but a form of uncertainty and human struggle with probability when it faces incomplete or inconsistent information. It is a knowledge that is possibly partially valid and partially invalid, but doubt is a form of knowledge that has positive value. Doubt is an invitation to \"proceed to further investigation\", asserts the text. All four means of knowledge discovery (perception, inference, comparison and testimony) may be useful in this investigation, but doubt is both a psychological state and a means to knowledge, not in itself a valid knowledge, according to the sutras.\n\nThe Nyayasutra defines error as knowledge, an opinion or a conclusion about something that is different from what it really is. Gautama states in the text that the error is always in the process of cognition itself, or the \"subjective self\", and not in the object. It is the duty of the knowledge-seeker to \"test the validity of his knowledge\", both in assumptions or through practice (experience), but neither the object of knowledge nor the knowledge itself is responsible for errors; only the knowledge-seeker and his process of cognition is. The Nyaya theory shares ideas on the theory of errors with Advaita Vedanta, Buddhism and Mimamsa schools of Indian philosophies, states Rao, and these schools likely influenced each other.\n\nThe text identifies and cautions against five kinds of fallacious reasoning (\"hetvabhasa\") in sutra 1.2.4, discussing each in the sutras that follow, stating that these lead to false knowledge, in contrast to proper reasoning (\"hetu\"), which leads to true knowledge. The five fallacies or errors, according to Nyayasutras, are to be avoided, in addition to watching for debating tricks (\"chala\") used by those whose aim isn't true knowledge. The five forms of bogus reasoning identified by the text, states Ganeri, are:\n\nThe Nyayasutras dedicate many sections on causality and causal relations (\"Karana\", Sanskrit: कारण), particularly Book 4. Causes, in Nyaya view states Fowler, are \"antecedents of their effects invariably and unconditionally\". A specific effect is produced by a specific cause (plurality in causes accepted), and in Nyayasutras view a specific cause produces a specific effect and no other (plurality in effect, or contradictory effect is not accepted). The sutras assert that there cannot be reciprocity to a cause, either we misunderstand the cause or misapprehend the effect. The text rejects remote or supernatural causes, and rejects that qualities are causes. The text asserts that causes are immediately antecedent, causes exist before an effect in time, and to know something is to understand the effect and the specific cause(s).\n\nThe text identifies three types of causes – inherent or material cause (\"Samavayi-karana\"), non-inherent cause (\"Asamavayi-karana\"), and efficient cause (\"Nimitta-karana\"). These, it states, arise from \"Dravya\" (substance), \"Guna\" (quality) and \"Karma\" (action).\n\nThe text seeds the theory of negative entities, where both being and non-being, presence and absence of something is considered correct and useful knowledge. Absence of book on a table or absence of particular color in a painting has a place in its epistemic process, in addition to positively verifiable characteristics of the table or a painting.\n\nEarly Nyaya school scholars considered the hypothesis of Ishvara as a creator God with the power to grant blessings, boons and fruits. However, the Nyayasutras and early Nyaya scholars rejected this hypothesis, and were non-theistic or atheists.\n\nIn Nyayasutra's Book 4, Chapter 1 examines what causes production and destruction of entities (life, matter) in universe. It considers many hypotheses, including \"Ishvara\". Verses 19–21, postulates Ishvara exists and is the cause, states a consequence of postulate, then presents contrary evidence, and from contradiction concludes that the postulate must be invalid.\n\nLater scholars of Nyaya school reconsidered this question and offered counter arguments for what is God (Ishvara) and various arguments to prove the existence of \"Ishvara\". The 5th century CE Nyaya school scholar Prastapada, for example, revisited the premise of God. He was followed by Udayana, who in his text \"Nyayakusumanjali\", interpreted \"it\" in verse 4.1.21 of Nyaya Sutra above, as \"human action\" and \"him\" as \"Ishvara\", then he developed counter arguments to prove the existence of Ishvara, a reasoning that fueled the debate and disagreements on God in Neo-Nyaya and other Hindu traditions of 2nd millennium CE.\n\nA large part of the third book of the Nyayasutras is dedicated to the premise and the nature of a Self (soul, atman) and its relation to knowledge, liberation from sorrow and inner freedom (moksha).\n\nThe sutras 4.2.42 to 4.2.48 of Nyayasutras, states Stephen Phillips, state that \"philosophy is a form of yoga\".\n\nThe text recommends yogic meditation in quiet places such as a forest, cave or sandy beach in sutra 4.2.42, that the knowledge seeker should purify one's soul by Yamas, Niyamas and spiritualism of yoga in sutra 4.2.46. Meditation is a treasured and recommended practice in the text, and extensively discussed by Nyaya scholars that followed Aksyapada Gautama. Vatsyayana wrote in his commentary on Nyayasutras, for example, that meditation is that which enables the mind to contact one's soul, which is accompanied by a conscious eagerness to get at the truth, and such meditation is an essential practice to gain true knowledge.\n\nThe Nyayasutras state that one must study the means of correct knowledge and hold discussions with the learned, sincere and unenvious fellow seekers of knowledge state sutras 4.2.47 and 4.2.48. One must, translates Phillips, take into account \"consideration of personal character as well as the nature of beliefs held by the opponent\", in deciding the nature of one's discussions, according to Nyayasutras. In some cases, asserts the text, it is better to avoid arguing with hostile opponents and use methods of knowledge like \"a fence is used to safeguard the growth of seeds\".\n\nThe earliest surviving complete bhasya (review and commentary) on Nyaya Sutras is by Vatsyayana. This commentary itself inspired many secondary and tertiary \"bhasya\". Vatsyayana's commentary has been variously dated to be from the 5th century CE, or much earlier around 2nd century BCE. Another often studied surviving commentary on the text is credited to Vacaspati Mishra from about 9th century CE.\nOther historical Indian commentaries and works inspired by Nyayasutras and which have survived into the modern era, include \"Nyaya-varttika\" by 6th-century Uddyotakara, \"Nyaya-bhasyatika\" by 6th-century Bhavivikta, another \"Nyaya-bhasyatika\" by 7th-century Aviddhakarna, \"Nyaya-bhusana\" by 9th-century Bhasarvajana, \"Nyaya-manjari\" by 9th-century Kashmir scholar Jayanta Bhatta, \"Nyaya-prakirnaka\" by 10th-century Karnata scholar Trilocana, and \"Nyaya-kandali\" by 10th-century Bengal scholar Sridhara.\n\nNumerous other commentaries are referenced in other Indian historical texts, but these manuscripts are either lost or yet to be found. Starting around 11th- to 12th century CE, Udayana wrote a primary work, that built upon and expanded the theories on reason found in Nyayasutras. Udayana's work created the foundation for Navya-Nyaya (new Nyaya) school. The Hindu scholar Gangesa of 13th- or 14th-century, integrated the Gautama's Nyayasutras and Udayana's Navya-Nyaya work, to create the influential \"Tattvacintāmaṇi\" text considered a masterpiece by scholars.\n\nThe Nyaya-sutras have been one of the foundations for the historic debate between Hinduism's premise that ultimate reality and atman (soul) exists, and Buddhism's premise that there is voidness and anatta (no-soul). In Nyaya-sutra, the Buddhist premises and arguments to refute those premise are found in many chapters, such as sutras of chapters 3.2, 4.1 and 4.2. The text has been influential in this debate, with the 2nd-century Buddhist scholar Nagarjuna states that the Nyaya school and Buddhism differ on their conception of Self (Atman) and their views on the Vedas, and the sutra 4.2.25 of Nyayasutra is addressed against the Madhyamika system of Buddhism.\n\nNagarjuna's \"Madhyamika-karika\" targets Nyaya-sutra, among other Hindu texts, for his critique and in order to establish his doctrine of no self and voidness. In this text, and Vigrahavya-vartani, he presents his proof of voidness by challenging the \"Pramanas\" at the foundation of Nyaya-sutras. In his work \"Pramana-vihetana\", Nagarjuna, takes up each of the sixteen categories of knowledge in Gautama's Nyaya-sutras at the foundation of Nyaya's discussion of \"soul exists and the nature of soul in liberation process\", and critiques them using the argument that these categories are relational and therefore unreal. The Nagarjuna's texts, along with Gautama's Nyaya-sutras states Sanjit Sadhukhan, influenced Vatsyayana's work who called Nagarjuna's doctrine of voidness as flawed, and presented his arguments refuting Nagarjuna's theory on \"objects of knowledge are unreal, like a dream or a form of jugglery and a mirage\", but by first presenting his demonstration that the theory of reason and knowledge in the Nyaya-sutras are valid.\n\nThe Buddhist thesis that all things are negative in nature (inasmuch as a thing's nature is constituted by its differences from others) is rejected, as is the view that all things are eternal or that all things are noneternal. Both these latter views are untrue to experience.\n\nThe Nyayasutras were influential to the Vedanta schools of Hindu philosophy, and provided the epistemological foundations. The terms \"Nyaya\" and \"Mimamsa\" were synonymous, states Hajime Nakamura, in the earliest Dharmasutras of 1st millennium BCE. Over time, Nyaya, Mimamsa and Vedanta became three distinct and related schools.\n\n\n\n\n"}
{"id": "145865", "url": "https://en.wikipedia.org/wiki?curid=145865", "title": "Parts-per notation", "text": "Parts-per notation\n\nIn science and engineering, the parts-per notation is a set of pseudo-units to describe small values of miscellaneous dimensionless quantities, e.g. mole fraction or mass fraction. Since these fractions are quantity-per-quantity measures, they are pure numbers with no associated units of measurement. Commonly used are ppm (parts-per-million, ), ppb (parts-per-billion, ), ppt (parts-per-trillion, ) and ppq (parts-per-quadrillion, ). This notation is not part of the SI system and its meaning is ambiguous.\n\nParts-per notation is often used describing dilute solutions in chemistry, for instance, the relative abundance of dissolved minerals or pollutants in water. The quantity “1 ppm” can be used for a mass fraction if a water-borne pollutant is present at one-millionth of a gram per gram of sample solution. When working with aqueous solutions, it is common to assume that the density of water is 1.00 g/mL. Therefore, it is common to equate 1 kilogram of water with 1 L of water. Consequently, 1 ppm corresponds to 1 mg/L and 1 ppb corresponds to 1 μg/L.\n\nSimilarly, parts-per notation is used also in physics and engineering to express the value of various proportional phenomena. For instance, a special metal alloy might expand 1.2 micrometers per meter of length for every degree Celsius and this would be expressed as “\"α\" = 1.2 ppm/°C.” Parts-per notation is also employed to denote the change, stability, or uncertainty in measurements. For instance, the accuracy of land-survey distance measurements when using a laser rangefinder might be 1 millimeter per kilometer of distance; this could be expressed as “Accuracy = 1 ppm.”\n\nParts-per notations are all dimensionless quantities: in mathematical expressions, the units of measurement always cancel. In fractions like “2 nanometers per meter” (2 nm/m = 2 nano = 2 × 10 = 2 ppb = 2 × 0.000000001) so the quotients are pure-number coefficients with positive values less than 1. When parts-per notations, including the percent symbol (%), are used in regular prose (as opposed to mathematical expressions), they are still pure-number dimensionless quantities. However, they generally take the literal “parts per” meaning of a comparative ratio (e.g., “2 ppb” would generally be interpreted as “two parts in a billion parts”).\n\nParts-per notations may be expressed in terms of any unit of the same measure. For instance, the coefficient of thermal expansion of a certain brass alloy, \"α\" = 18.7 ppm/°C, may be expressed as 18.7 (µm/m)/°C, or as 18.7 (µin/in)/°C; the numeric value representing a relative proportion does not change with the adoption of a different unit of measure. Similarly, a metering pump that injects a trace chemical into the main process line at the proportional flow rate \"Q\" = 125 ppm, is doing so at a rate that may be expressed in a variety of volumetric units, including 125 µL/L, 125 µgal/gal, 125 cm/m, etc.\n\nIn nuclear magnetic resonance spectroscopy (NMR), chemical shift is usually expressed in ppm. It represents the difference of a measured frequency in parts per million from the reference frequency. The reference frequency depends on the instrument's magnetic field and the element being measured. It is usually expressed in MHz. Typical chemical shifts are rarely more than a few hundred Hz from the reference frequency, so chemical shifts are conveniently expressed in ppm (Hz/MHz). Parts-per notation gives a dimensionless quantity that does not depend on the instrument's field strength.\n\n\n\n\n\n\nAlthough the International Bureau of Weights and Measures (an international standards organization known also by its French-language initials BIPM) recognizes the use of parts-per notation, it is not formally part of the International System of Units (SI). Note that although “percent” (%) is not formally part of the SI, both the BIPM and the ISO take the position that \"“in mathematical expressions, the internationally recognized symbol % (percent) may be used with the SI to represent the number 0.01”\" for dimensionless quantities. According to IUPAP, \"“a continued source of annoyance to unit purists has been the continued use of percent, ppm, ppb, and ppt.”\" Although SI-compliant expressions should be used as an alternative, the parts-per notation remains nevertheless widely used in technical disciplines. The main problems with the parts-per notation are the following:\n\nBecause the named numbers starting with a “billion” have different values in different countries, the BIPM suggests avoiding the use of “ppb” and “ppt” to prevent misunderstanding. The U.S. National Institute of Standards and Technology (NIST) takes the stringent position, stating that \"“the language-dependent terms [ . . . ] are not acceptable for use with the SI to express the values of quantities.”\"\n\nAlthough \"ppt\" usually means \"parts per trillion\", it occasionally means \"parts per thousand\". Unless the meaning of \"ppt\" is defined explicitly, it has to be determined from the context.\n\nAnother problem of the parts-per notation is that it may refer to mass fraction, mole fraction or volume fraction. Since it is usually not stated which quantity is used, it is better to write the unit as kg/kg, mol/mol or m/m (even though they are all dimensionless). The difference is quite significant when dealing with gases and it is very important to specify which quantity is being used. For example, the conversion factor between a mass fraction of 1 ppb and a mole fraction of 1 ppb is about 4.7 for the greenhouse gas CFC-11 in air. For volume fraction, the suffix \"V\" or \"v\" is sometimes appended to the parts-per notation (e.g., ppmV, ppbv, pptv). Unfortunately, ppbv and pptv are also often used for mole fractions (which is identical to volume fraction only for ideal gases).\n\nTo distinguish the mass fraction from volume fraction or mole fraction, the letter w (standing for weight) is sometimes added to the abbreviation (e.g., ppmw, ppbw).\n\nThe usage of the parts-per notation is generally quite fixed inside most specific branches of science, leading some researchers to draw the conclusion that their own usage (mass/mass, mol/mol, volume/volume, or others) is the only correct one. This, in turn, leads them to not specify their usage in their publications, and others may therefore misinterpret their results. For example, electrochemists often use volume/volume, while chemical engineers may use mass/mass as well as volume/volume. Many academic papers of otherwise excellent level fail to specify their usage of the parts-per notation.\n\nSI-compliant units that can be used as alternatives are shown in the chart below. Expressions that the BIPM explicitly does not recognize as being suitable for denoting dimensionless quantities with the SI are shown in red text.\n\nNote that the notations in the “SI units” column above are all dimensionless quantities; that is, the units of measurement factor out in expressions like “1 nm/m” (1 nm/m = 1 nano = 1 × 10) so the quotients are pure-number coefficients with values less than 1.\n\nBecause of the cumbersome nature of expressing certain dimensionless quantities per SI guidelines, the International Union of Pure and Applied Physics (IUPAP) in 1999 proposed the adoption of the special name \"uno\" (symbol: U) to represent the number 1 in dimensionless quantities. This symbol is not to be confused with the always-italicized symbol for the variable \"uncertainty\" (symbol: \"U\"). This unit name \"uno\" and its symbol could be used in combination with the SI prefixes to express the values of dimensionless quantities that are much less—or even \"greater\"—than one.\n\nCommon parts-per notations in terms of the uno are given in the table below.\n\nIn 2004, a report to the International Committee for Weights and Measures (known also by its French-language initials CIPM) stated that response to the proposal of the uno \"had been almost entirely negative\" and the principal proponent \"recommended dropping the idea\". To date, the uno has not been adopted by any standards organization and it appears unlikely it will ever become an officially sanctioned way to express low-value (high-ratio) dimensionless quantities. The proposal was instructive, however, as to the perceived shortcomings of the current options for denoting dimensionless quantities.\n\nParts-per notation may properly be used only to express true dimensionless quantities; that is, the units of measurement \"must\" cancel in expressions like \"1 mg/kg\" so that the quotients are pure numbers with values less than 1. Mixed-unit quantities such as \"a radon concentration of 15 pCi/L\" are not dimensionless quantities and may not be expressed using any form of parts-per notation, such as \"15 ppt\". Other examples of measures that are not dimensionless quantities are as follows:\n\n\nNote however, that it is not uncommon to express aqueous concentrations—particularly in drinking-water reports intended for the general public—using parts-per notation (2.1 ppm, 0.8 ppb, etc.) and further, for those reports to state that the notations denote milligrams per liter or micrograms per liter. Although \"2.1 mg/L\" is not a dimensionless quantity, it is assumed in scientific circles that \"2.1 mg/kg\" (2.1 ppm) is the true measure because one liter of water has a mass of about one kilogram. The goal in all technical writing (including drinking-water reports for the general public) is to clearly communicate to the intended audience with minimal confusion. Drinking water is intuitively a volumetric quantity in the public’s mind so measures of contamination expressed on a per-liter basis are considered to be easier to grasp. Still, it is technically possible, for example, to \"dissolve\" more than one liter of a very hydrophilic chemical in 1 liter of water; parts-per notation would be confusing when describing its solubility in water (greater than a million parts per million), so one would simply state the volume (or mass) that will dissolve into a liter, instead.\n\nWhen reporting air-borne rather than water-borne densities, a slightly different convention is used since air is approximately 1000 times less dense than water. In water, 1 µg/m is roughly equivalent to parts-per-trillion whereas in air, it is roughly equivalent to parts-per-billion. Note also, that in the case of air, this convention is much less accurate. Whereas one liter of water is almost exactly 1 kg, one cubic meter of air is often taken as 1.143 kg—much less accurate, but still close enough for many practical uses.\n\n\n"}
{"id": "12377419", "url": "https://en.wikipedia.org/wiki?curid=12377419", "title": "Path-based strong component algorithm", "text": "Path-based strong component algorithm\n\nIn graph theory, the strongly connected components of a directed graph may be found using an algorithm that uses depth-first search in combination with two stacks, one to keep track of the vertices in the current component and the second to keep track of the current search path. Versions of this algorithm have been proposed by , , , , and ; of these, Dijkstra's version was the first to achieve linear time.\n\nThe algorithm performs a depth-first search of the given graph \"G\", maintaining as it does two stacks \"S\" and \"P\" (in addition to the normal call stack for a recursive function).\nStack \"S\" contains all the vertices that have not yet been assigned to a strongly connected component, in the order in which the depth-first search reaches the vertices.\nStack \"P\" contains vertices that have not yet been determined to belong to different strongly connected components from each other. It also uses a counter \"C\" of the number of vertices reached so far, which it uses to compute the preorder numbers of the vertices.\n\nWhen the depth-first search reaches a vertex \"v\", the algorithm performs the following steps:\n\nThe overall algorithm consists of a loop through the vertices of the graph, calling this recursive search on each vertex that does not yet have a preorder number assigned to it.\n\nLike this algorithm, Tarjan's strongly connected components algorithm also uses depth first search together with a stack to keep track of vertices that have not yet been assigned to a component, and moves these vertices into a new component when it finishes expanding the final vertex of its component. However, in place of the stack \"P\", Tarjan's algorithm uses a vertex-indexed array of preorder numbers, assigned in the order that vertices are first visited in the depth-first search. The preorder array is used to keep track of when to form a new component.\n\n"}
{"id": "2715150", "url": "https://en.wikipedia.org/wiki?curid=2715150", "title": "Prime signature", "text": "Prime signature\n\nThe prime signature of a number is the multiset of exponents of its prime factorization. The prime signature of a number having prime factorization formula_1 is formula_2.\n\nFor example, all prime numbers have a prime signature of {1}, the squares of primes have a prime signature of {2}, the products of 2 distinct primes have a prime signature of {1,1} and the products of a square of a prime and a different prime (e.g. 12,18,20... ) have a prime signature of {2,1}.\n\nThe divisor function τ(\"n\"), the Möbius function \"μ\"(\"n\"), the number of distinct prime divisors ω(\"n\") of \"n\", the number of prime divisors Ω(\"n\") of \"n\", the indicator function of the squarefree integers, and many other important functions in number theory, are functions of the prime signature of \"n\".\n\nIn particular, τ(\"n\") equals the product of the incremented by 1 exponents from the prime signature of \"n\". \nFor example, 20 has prime signature {2,1} and so the number of divisors is (2+1) × (1+1) = 6. Indeed, there are six divisors: 1, 2, 4, 5, 10 and 20.\n\nThe smallest number of each prime signature is a product of primorials. The first few are:\n\nGiven a number with prime signature \"S\", it is\n\n\n"}
{"id": "358069", "url": "https://en.wikipedia.org/wiki?curid=358069", "title": "Proof by infinite descent", "text": "Proof by infinite descent\n\nIn mathematics, a proof by infinite descent is a particular kind of proof by contradiction that relies on the least integer principle. One typical application is to show that a given equation has no solutions.\n\nTypically, one shows that if a solution to a problem existed, which in some sense was related to one or more natural numbers, it would necessarily imply that a second solution existed, which was related to one or more 'smaller' natural numbers. This in turn would imply a third solution related to smaller natural numbers, implying a fourth solution, therefore a fifth solution, and so on. However, there cannot be an infinity of ever-smaller natural numbers, and therefore by mathematical induction (repeating the same step) the original premise—that any solution exists— is incorrect: its correctness produces a contradiction.\n\nAn alternative way to express this is to assume one or more solutions or examples exists. Then there must be a smallest solution or example—a minimal counterexample. We then prove that if a smallest solution exists, it must imply the existence of a smaller solution (in some sense)—which again proves that the existence of any solution would lead to a contradiction.\n\nThe earliest uses of the method of infinite descent appear in Euclid's \"Elements\". A typical example is Proposition 31 of Book 7, in which Euclid proves that every composite integer is divided (in Euclid's terminology \"measured\") by some prime number. \n\nThe method was much later developed by Fermat, who coined the term and often used it for Diophantine equations. Two typical examples are showing the non-solvability of the Diophantine equation \"r\" + \"s\" = \"t\" and proving Fermat's theorem on sums of two squares, which states that an odd prime \"p\" can be expressed as a sum of two squares only when \"p\" ≡ 1 (mod 4) (see proof). In some cases, to the modern eye, his \"method of infinite descent\" is an exploitation of the inversion of the doubling function for rational points on an elliptic curve \"E\". The context is of a hypothetical non-trivial rational point on \"E\". Doubling a point on \"E\" roughly doubles the length of the numbers required to write it (as number of digits), so that a \"halving\" a point gives a rational with smaller terms. Since the terms are positive, they cannot decrease forever. In this way Fermat was able to show the non-existence of solutions in many cases of Diophantine equations of classical interest (for example, the problem of four perfect squares in arithmetic progression).\n\nIn the number theory of the twentieth century, the infinite descent method was taken up again, and pushed to a point where it connected with the main thrust of algebraic number theory and the study of L-functions. The structural result of Mordell, that the rational points on an elliptic curve \"E\" form a finitely-generated abelian group, used an infinite descent argument based on \"E\"/2\"E\" in Fermat's style.\n\nTo extend this to the case of an abelian variety \"A\", André Weil had to make more explicit the way of quantifying the size of a solution, by means of a height function – a concept that became foundational. To show that \"A\"(\"Q\")/2\"A\"(\"Q\") is finite, which is certainly a necessary condition for the finite generation of the group \"A\"(\"Q\") of rational points of \"A\", one must do calculations in what later was recognised as Galois cohomology. In this way, abstractly-defined cohomology groups in the theory become identified with \"descents\" in the tradition of Fermat. The Mordell–Weil theorem was at the start of what later became a very extensive theory.\n\nThe proof that the square root of 2 () is irrational (i.e. cannot be expressed as a fraction of two whole numbers) was discovered by the ancient Greeks, and is perhaps the earliest known example of a proof by infinite descent. Pythagoreans discovered that the diagonal of a square is incommensurable with its side, or in modern language, that the square root of two is irrational. Little is known with certainty about the time or circumstances of this discovery, but the name of Hippasus of Metapontum is often mentioned. For a while, the Pythagoreans treated as an official secret the discovery that the square root of two is irrational, and, according to legend, Hippasus was murdered for divulging it. The square root of two is occasionally called \"Pythagoras' number\" or \"Pythagoras' Constant\", for example .\n\nThe ancient Greeks, not having algebra, worked out a geometric proof by infinite descent (John Horton Conway presented another geometric proof (no. 8 ' ' ' ) by infinite descent that may be more accessible). The following is an algebraic proof along similar lines:\n\nSuppose that were rational. Then it could be written as\n\nfor two natural numbers, and . Then squaring would give\n\nso 2 must divide \"p\". Because 2 is a prime number, it must also divide \"p\", by Euclid's lemma. So \"p\" = 2\"r\", for some integer \"r\".\n\nBut then\n\nwhich shows that 2 must divide \"q\" as well. So \"q\" = 2\"s\" for some integer \"s\".\n\nThis gives\n\nTherefore, if could be written as a rational number, it could always be written as a rational number with smaller parts, which itself could be written with yet-smaller parts, \"ad infinitum\". But this is impossible in the set of natural numbers. Since is a real number, which can be either rational or irrational, the only option left is for to be irrational.\n\n(Alternatively, this proves that if were rational, no \"smallest\" representation as a fraction could exist, as any attempt to find a \"smallest\" representation \"p\"/\"q\" would imply a smaller one existed, which is a similar contradiction).\n\nFor positive integer \"k\", suppose that is not an integer, but is rational and can be expressed as ⁄ for natural numbers \"m\" and \"n\", and let \"q\" be the largest integer no greater than . Then\n\nThe numerator and denominator were each multiplied by the expression ( − \"q\")—which is positive but less than 1—and then simplified independently. So two resulting products, say \"m' \" and \"n' \", are themselves integers, which are less than \"m\" and \"n\" respectively. Therefore, no matter what natural numbers \"m\" and \"n\" are used to express , there exist smaller natural numbers \"m' \" < \"m\" and \"n' \" < \"n\" that have the same ratio. But infinite descent on the natural numbers is impossible, so this disproves the original assumption that could be expressed as a ratio of natural numbers.\n\nThe non-solvability of formula_8 in integers is sufficient to show the non-solvability of formula_9 in integers, which is a special case of Fermat's Last Theorem, and the historical proofs of the latter proceeded by more broadly proving the former using infinite descent. The following more recent proof demonstrates both of these impossibilities by proving still more broadly that a Pythagorean triangle cannot have any two of its sides each either a square or twice a square, since there is no smallest such triangle:\n\nSuppose there exists such a Pythagorean triangle. Then it can be scaled down to give a primitive (i.e., with no common factors other than 1) Pythagorean triangle with the same property. Primitive Pythagorean triangles' sides can be written as formula_10 formula_11 formula_12, with \"a\" and \"b\" relatively prime and with \"a+b\" odd and hence \"y\" and \"z\" both odd. The property that \"y\" and \"z\" are each odd means that neither \"y\" nor \"z\" can be twice a square. Furthermore, if \"x\" is a square or twice a square, then each of \"a\" and \"b\" is a square or twice a square. There are three cases, depending on which two sides are postulated to each be a square or twice a square:\n\n\nIn any of these cases, one Pythagorean triangle with two sides each of which is a square or twice a square has led to a smaller one, which in turn would lead to a smaller one, etc.; since such a sequence cannot go on infinitely, the original premise that such a triangle exists must be wrong.\n\nThis implies that the equations\ncannot have non-trivial solutions, since non-trivial solutions would give Pythagorean triangles with two sides being squares.\n\nFor other similar proofs by infinite descent for the \"n\" = 4 case of Fermat's Theorem, see the articles by Grant and Perella and Barbara.\n\n\n"}
{"id": "25717", "url": "https://en.wikipedia.org/wiki?curid=25717", "title": "Regular expression", "text": "Regular expression\n\nA regular expression, regex or regexp (sometimes called a rational expression) is a sequence of characters that define a \"search pattern\". Usually this pattern is used by string searching algorithms for \"find\" or \"find and replace\" operations on strings, or for input validation. It is a technique that developed in theoretical computer science and formal language theory.\n\nThe concept arose in the 1950s when the American mathematician Stephen Cole Kleene formalized the description of a \"regular language\". The concept came into common use with Unix text-processing utilities. Since the 1980s, different syntaxes for writing regular expressions exist, one being the POSIX standard and another, widely used, being the Perl syntax.\n\nRegular expressions are used in search engines, search and replace dialogs of word processors and text editors, in text processing utilities such as sed and AWK and in lexical analysis. Many programming languages provide regex capabilities, built-in or via libraries.\n\nThe phrase \"regular expressions\", and consequently, \"regexes\", is often used to mean the specific, standard textual syntax (distinct from the mathematical notation described below) for representing patterns for matching text. Each character in a regular expression (that is, each character in the string describing its pattern) is either a metacharacter, having a special meaning, or a regular character that has a literal meaning. For example, in the regex codice_1, \"a\" is a literal character which matches just 'a', while '.' is a meta character that matches every character except a newline. Therefore, this regex matches, for example, 'a ', or 'ax', or 'a0'. Together, metacharacters and literal characters can be used to identify text of a given pattern, or process a number of instances of it. Pattern matches may vary from a precise equality to a very general similarity, as controlled by the metacharacters. For example, codice_2 is a very general pattern, codice_3 (match all lower case letters from 'a' to 'z') is less general and codice_4 is a precise pattern (matches just 'a'). The metacharacter syntax is designed specifically to represent prescribed targets in a concise and flexible way to direct the automation of text processing of a variety of input data, in a form easy to type using a standard ASCII keyboard.\n\nA very simple case of a regular expression in this syntax is to locate a word spelled two different ways in a text editor, the regular expression codice_5 matches both \"serialise\" and \"serialize\". Wildcards also achieve this, but are more limited in what they can pattern, as they have fewer metacharacters and a simple language-base.\n\nThe usual context of wildcard characters is in globbing similar names in a list of files, whereas regexes are usually employed in applications that pattern-match text strings in general. For example, the regex matches excess whitespace at the beginning or end of a line. An advanced regular expression that matches any numeral is .\nA regex processor translates a regular expression in the above syntax into an internal representation which can be executed and matched against a string representing the text being searched in. One possible approach is the Thompson's construction algorithm to construct a nondeterministic finite automaton (NFA), which is then made deterministic and the resulting deterministic finite automaton (DFA) is run on the target text string to recognize substrings that match the regular expression.\nThe picture shows the NFA scheme codice_6 obtained from the regular expression codice_7, where \"s\" denotes a simpler regular expression in turn, which has already been recursively translated to the NFA \"N\"(\"s\").\n\nRegular expressions originated in 1951, when mathematician Stephen Cole Kleene described regular languages using his mathematical notation called \"regular sets\". These arose in theoretical computer science, in the subfields of automata theory (models of computation) and the description and classification of formal languages. Other early implementations of pattern matching include the SNOBOL language, which did not use regular expressions, but instead its own pattern matching constructs.\n\nRegular expressions entered popular use from 1968 in two uses: pattern matching in a text editor and lexical analysis in a compiler. Among the first appearances of regular expressions in program form was when Ken Thompson built Kleene's notation into the editor QED as a means to match patterns in text files. For speed, Thompson implemented regular expression matching by just-in-time compilation (JIT) to IBM 7094 code on the Compatible Time-Sharing System, an important early example of JIT compilation. He later added this capability to the Unix editor ed, which eventually led to the popular search tool grep's use of regular expressions (\"grep\" is a word derived from the command for regular expression searching in the ed editor: codice_8 meaning \"Global search for Regular Expression and Print matching lines\"). Around the same time when Thompson developed QED, a group of researchers including Douglas T. Ross implemented a tool based on regular expressions that is used for lexical analysis in compiler design.\n\nMany variations of these original forms of regular expressions were used in Unix programs at Bell Labs in the 1970s, including vi, lex, sed, AWK, and expr, and in other programs such as Emacs. Regexes were subsequently adopted by a wide range of programs, with these early forms standardized in the POSIX.2 standard in 1992.\n\nIn the 1980s the more complicated regexes arose in Perl, which originally derived from a regex library written by Henry Spencer (1986), who later wrote an implementation of \"Advanced Regular Expressions\" for Tcl. The Tcl library is a hybrid NFA/DFA implementation with improved performance characteristics. Software projects that have adopted Spencer's Tcl regular expression implementation include PostgreSQL. Perl later expanded on Spencer's original library to add many new features, but has not yet caught up with Spencer's Advanced Regular Expressions implementation in terms of performance or Unicode handling. Part of the effort in the design of Perl 6 is to improve Perl's regex integration, and to increase their scope and capabilities to allow the definition of parsing expression grammars. The result is a mini-language called Perl 6 rules, which are used to define Perl 6 grammar as well as provide a tool to programmers in the language. These rules maintain existing features of Perl 5.x regexes, but also allow BNF-style definition of a recursive descent parser via sub-rules.\n\nThe use of regexes in structured information standards for document and database modeling started in the 1960s and expanded in the 1980s when industry standards like ISO SGML (precursored by ANSI \"GCA 101-1983\") consolidated. The kernel of the structure specification language standards consists of regexes. Its use is evident in the DTD element group syntax.\n\nStarting in 1997, Philip Hazel developed PCRE (Perl Compatible Regular Expressions), which attempts to closely mimic Perl's regex functionality and is used by many modern tools including PHP and Apache HTTP Server.\n\nToday regexes are widely supported in programming languages, text processing programs (particularly lexers), advanced text editors, and some other programs. Regex support is part of the standard library of many programming languages, including Java and Python, and is built into the syntax of others, including Perl and ECMAScript. Implementations of regex functionality is often called a regex engine, and a number of libraries are available for reuse.\n\nA regular expression, often called a pattern, is an expression used to specify a set of strings required for a particular purpose. A simple way to specify a finite set of strings is to list its elements or members. However, there are often more concise ways to specify the desired set of strings. For example, the set containing the three strings \"Handel\", \"Händel\", and \"Haendel\" can be specified by the pattern codice_9; we say that this pattern matches each of the three strings. In most formalisms, if there exists at least one regular expression that matches a particular set then there exists an infinite number of other regular expression that also match it—the specification is not unique. Most formalisms provide the following operations to construct regular expressions.\n\nThe wildcard codice_2 matches any character. For example, codice_15 matches any string that contains an \"a\", then any other character and then a \"b\", codice_16 matches any string that contains an \"a\" and a \"b\" at some later point.\n\nThese constructions can be combined to form arbitrarily complex expressions, much like one can construct arithmetical expressions from numbers and the operations +, −, ×, and ÷. For example, codice_17 and are both valid patterns which match the same strings as the earlier example, codice_9.\n\nThe precise syntax for regular expressions varies among tools and with context; more detail is given in the \"Syntax\" section.\n\nRegular expressions describe regular languages in formal language theory. They have the same expressive power as regular grammars.\n\nRegular expressions consist of constants, which denote sets of strings, and operator symbols, which denote operations over these sets. The following definition is standard, and found as such in most textbooks on formal language theory. Given a finite alphabet Σ, the following constants are defined\nas regular expressions:\n\nGiven regular expressions R and S, the following operations over them are defined\nto produce regular expressions:\n\nTo avoid parentheses it is assumed that the Kleene star has the highest priority, then concatenation and then alternation. If there is no ambiguity then parentheses may be omitted. For example, codice_20 can be written as codice_21, and codice_22 can be written as codice_23.\nMany textbooks use the symbols ∪, +, or ∨ for alternation instead of the vertical bar.\n\nExamples:\n\nThe formal definition of regular expressions is purposely parsimonious and avoids defining the redundant quantifiers codice_11 and codice_13, which can be expressed as follows: codice_30 = codice_31, and codice_32 = codice_33. Sometimes the complement operator is added, to give a \"generalized regular expression\"; here \"R\" matches all strings over Σ* that do not match \"R\". In principle, the complement operator is redundant, as it can always be circumscribed by using the other operators. However, the process for computing such a representation is complex, and the result may require expressions of a size that is double exponentially larger.\n\nRegular expressions in this sense can express the regular languages, exactly the class of languages accepted by deterministic finite automata. There is, however, a significant difference in compactness. Some classes of regular languages can only be described by deterministic finite automata whose size grows exponentially in the size of the shortest equivalent regular expressions. The standard example here is the languages\n\"L\" consisting of all strings over the alphabet {\"a\",\"b\"} whose \"k\"-from-last letter equals \"a\". On one hand, a regular expression describing \"L\" is given by\nformula_1.\n\nGeneralizing this pattern to \"L\" gives the expression:\nformula_2\n\nOn the other hand, it is known that every deterministic finite automaton accepting the language \"L\" must have at least 2 states. Luckily, there is a simple mapping from regular expressions to the more general nondeterministic finite automata (NFAs) that does not lead to such a blowup in size; for this reason NFAs are often used as alternative representations of regular languages. NFAs are a simple variation of the type-3 grammars of the Chomsky hierarchy.\n\nIn the opposite direction, there are many languages easily described by a DFA that are not easily described a regular expression. For instance, determining the validity of a given ISBN number requires computing the modulus of the integer base 11, and can be easily implemented with an 11-state DFA. However, a regular expression to answer the same problem of divisibility by 11 is at least multiple megabytes in length.\n\nGiven a regular expression, Thompson's construction algorithm computes an equivalent nondeterministic finite automaton. A conversion in the opposite direction is achieved by Kleene's algorithm.\n\nFinally, it is worth noting that many real-world \"regular expression\" engines implement features that cannot be described by the regular expressions in the sense of formal language theory; rather, they implement \"regexes\". See below for more on this.\n\nAs seen in many of the examples above, there is more than one way to construct a regular expression to achieve the same results.\n\nIt is possible to write an algorithm that, for two given regular expressions, decides whether the described languages are equal; the algorithm reduces each expression to a minimal deterministic finite state machine, and determines whether they are isomorphic (equivalent).\n\nAlgebraic laws for regular expressions can be obtained using a method by Gischer which is best explained along an example: In order to check whether (\"X\"+\"Y\") and (\"X\" \"Y\") denote the same regular language, for all regular expressions \"X\", \"Y\", it is necessary and sufficient to check whether the particular regular expressions (\"a\"+\"b\") and (\"a\" \"b\") denote the same language over the alphabet Σ={\"a\",\"b\"}. More generally, an equation \"E\"=\"F\" between regular-expression terms with variables holds if, and only if, its instantiation with different variables replaced by different symbol constants holds.\n\nThe redundancy can be eliminated by using Kleene star and set union to find an interesting subset of regular expressions that is still fully expressive, but perhaps their use can be restricted. This is a surprisingly difficult problem. As simple as the regular expressions are, there is no method to systematically rewrite them to some normal form. The lack of axiom in the past led to the star height problem. In 1991, Dexter Kozen axiomatized regular expressions as a Kleene algebra, using equational and Horn clause axioms.\nAlready in 1964, Redko had proved that no finite set of purely equational axioms can characterize the algebra of regular languages.\n\nA regex \"pattern\" matches a target \"string\". The pattern is composed of a sequence of \"atoms\". An atom is a single point within the regex pattern which it tries to match to the target string. The simplest atom is a literal, but grouping parts of the pattern to match an atom will require using codice_34 as metacharacters. Metacharacters help form: \"atoms\"; \"quantifiers\" telling how many atoms (and whether it is a \"greedy\" quantifier or not); a logical OR character, which offers a set of alternatives, and a logical NOT character, which negates an atom's existence; and backreferences to refer to previous atoms of a completing pattern of atoms. A match is made, not when all the atoms of the string are matched, but rather when all the pattern atoms in the regex have matched. The idea is to make a small pattern of characters stand for a large number of possible strings, rather than compiling a large list of all the literal possibilities.\n\nDepending on the regex processor there are about fourteen metacharacters, characters that may or may not have their literal character meaning, depending on context, or whether they are \"escaped\", i.e. preceded by an escape sequence, in this case, the backslash codice_35. Modern and POSIX extended regexes use metacharacters more often than their literal meaning, so to avoid \"backslash-osis\" or leaning toothpick syndrome it makes sense to have a metacharacter escape to a literal mode; but starting out, it makes more sense to have the four bracketing metacharacters codice_34 and codice_37 be primarily literal, and \"escape\" this usual meaning to become metacharacters. Common standards implement both. The usual metacharacters are codice_38 and codice_35. The usual characters that become metacharacters when escaped are codice_40 and codice_41.\n\nWhen entering a regex in a programming language, they may be represented as a usual string literal, hence usually quoted; this is common in C, Java, and Python for instance, where the regex codice_42 is entered as codice_43. However, they are often written with slashes as delimiters, as in codice_44 for the regex codice_42. This originates in ed, where codice_46 is the editor command for searching, and an expression codice_44 can be used to specify a range of lines (matching the pattern), which can be combined with other commands on either side, most famously codice_48 as in grep (\"global regex print\"), which is included in most Unix-based operating systems, such as Linux distributions. A similar convention is used in sed, where search and replace is given by codice_49 and patterns can be joined with a comma to specify a range of lines as in codice_50. This notation is particularly well-known due to its use in Perl, where it forms part of the syntax distinct from normal string literals. In some cases, such as sed and Perl, alternative delimiters can be used to avoid collision with contents, and to avoid having to escape occurrences of the delimiter character in the contents. For example, in sed the command codice_51 will replace a codice_46 with an codice_53, using commas as delimiters.\n\nThe IEEE POSIX standard has three sets of compliance: BRE (Basic Regular Expressions), ERE (Extended Regular Expressions), and SRE (Simple Regular Expressions). SRE is deprecated, in favor of BRE, as both provide backward compatibility. The subsection below covering the \"character classes\" applies to both BRE and ERE.\n\nBRE and ERE work together. ERE adds codice_11, codice_13, and codice_56, and it removes the need to escape the metacharacters codice_34 and codice_37, which are \"required\" in BRE. Furthermore, as long as the POSIX standard syntax for regexes is adhered to, there can be, and often is, additional syntax to serve specific (yet POSIX compliant) applications. Although POSIX.2 leaves some implementation specifics undefined, BRE and ERE provide a \"standard\" which has since been adopted as the default syntax of many tools, where the choice of BRE or ERE modes is usually a supported option. For example, GNU grep has the following options: \"grep -E\" for ERE, and \"grep -G\" for BRE (the default), and \"grep -P\" for Perl regexes.\n\nPerl regexes have become a de facto standard, having a rich and powerful set of atomic expressions. Perl has no \"basic\" or \"extended\" levels. As in POSIX EREs, codice_34 and codice_37 are treated as metacharacters unless escaped; other metacharacters are known to be literal or symbolic based on context alone. Additional functionality includes lazy matching, backtracking, named capture groups, and recursive patterns.\n\nIn the POSIX standard, Basic Regular Syntax (BRE) requires that the metacharacters codice_34 and codice_37 be designated codice_63 and codice_64, whereas Extended Regular Syntax (ERE) does not.\n\nExamples:\n\nThe meaning of metacharacters escaped with a backslash is reversed for some characters in the POSIX Extended Regular Expression (ERE) syntax. With this syntax, a backslash causes the metacharacter to be treated as a literal character. So, for example, codice_75 is now codice_34 and codice_77 is now codice_37. Additionally, support is removed for codice_79 backreferences and the following metacharacters are added:\n\nExamples:\n\nPOSIX Extended Regular Expressions can often be used with modern Unix utilities by including the command line flag -E.\n\nThe character class is the most basic regex concept after a literal match. It makes one small sequence of characters match a larger set of characters. For example, could stand for the uppercase alphabet, and could mean any digit. Character classes apply to both POSIX levels.\n\nWhen specifying a range of characters, such as (i.e. lowercase \"\" to uppercase \"\"), the computer's locale settings determine the contents by the numeric ordering of the character encoding. They could store digits in that sequence, or the ordering could be \"abc…zABC…Z\", or \"aAbBcC…zZ\". So the POSIX standard defines a character class, which will be known by the regex processor installed. Those definitions are in the following table:\n\nPOSIX character classes can only be used within bracket expressions. For example, matches the uppercase letters and lowercase \"a\" and \"b\".\n\nAn additional non-POSIX class understood by some tools is , which is usually defined as plus underscore. This reflects the fact that in many programming languages these are the characters that may be used in identifiers. The editor [[Vim (text editor)|Vim]] further distinguishes \"word\" and \"word-head\" classes (using the notation and ) since in many programming languages the characters that can begin an identifier are not the same as those that can occur in other positions.\n\nNote that what the POSIX regex standards call \"character classes\" are commonly referred to as \"POSIX character classes\" in other regex flavors which support them. With most other regex flavors, the term \"character class\" is used to describe what POSIX calls \"bracket expressions\".\n\nBecause of its expressive power and (relative) ease of reading, many other utilities and programming languages have adopted syntax similar to Perl's—for example, [[Java (programming language)|Java]], [[JavaScript]], [[Python (programming language)|Python]], [[Ruby (programming language)|Ruby]], [[Qt (software)|Qt]], [[Microsoft]]'s [[.NET Framework]], and [[XML Schema (W3C)|XML Schema]]. Some languages and tools such as [[Boost C++ Libraries|Boost]] and [[PHP]] support multiple regex flavors. Perl-derivative regex implementations are not identical and usually implement a subset of features found in Perl 5.0, released in 1994. Perl sometimes does incorporate features initially found in other languages, for example, Perl 5.10 implements syntactic extensions originally developed in [[PCRE]] and Python.\n\nIn Python and some other implementations (e.g. Java), the three common quantifiers (codice_12, codice_13 and codice_11) are [[greedy algorithm|greedy]] by default because they match as many characters as possible. The regex codice_87 applied to the string\n\nmatches the entire line instead of matching only the first character, \". The aforementioned quantifiers may, however, be made \"lazy\" or \"minimal\" or \"reluctant\", matching as few characters as possible, by appending a question mark: codice_88 matches only codice_89.\n\nHowever, this does not ensure that not the whole sentence is matched in some contexts. The question-mark operator does not change the meaning of the dot operator, so this still can match the quotes in the input. A pattern like codice_90 will still match the whole input if this is the string\n\nTo ensure that the quotes cannot be part of the match, the dot has to be replaced, e. g. like this: codice_91 This will match a quoted text part without additional quotes in it.\n\nIn Java, quantifiers may be made \"possessive\" by appending a plus sign, which disables backing off, even if doing so would allow the overall match to succeed: While the regex codice_92 applied to the string\n\nmatches the entire line, the regex codice_93 does , because codice_94 consumes the entire input, including the final codice_95. Thus, possessive quantifiers are most useful with negated character classes, e.g. codice_96, which matches codice_89 when applied to the same string.\n\nPossessive quantifiers are easier to implement than greedy and lazy quantifiers, and are typically more efficient at runtime.\n\nMany features found in virtually all modern regular expression libraries provide an expressive power that far exceeds the [[regular language]]s. For example, many implementations allow grouping subexpressions with parentheses and recalling the value they match in the same expression (\"\"). This means that, among other things, a pattern can match strings of repeated words like \"papa\" or \"WikiWiki\", called \"squares\" in formal language theory. The pattern for these strings is codice_98.\n\nThe language of squares is not regular, nor is it [[context-free language|context-free]], due to the [[Pumping lemma for context-free languages|pumping lemma]]. However, [[pattern matching]] with an unbounded number of backreferences, as supported by numerous modern tools, is still [[context-sensitive language|context sensitive]].\n\nHowever, many tools, libraries, and engines that provide such constructions still use the term \"regular expression\" for their patterns. This has led to a nomenclature where the term regular expression has different meanings in [[formal language|formal language theory]] and pattern matching. For this reason, some people have taken to using the term \"regex\", \"regexp\", or simply \"pattern\" to describe the latter. [[Larry Wall]], author of the Perl programming language, writes in an essay about the design of Perl 6:\n\nThere are at least three different [[algorithm]]s that decide whether and how a given regex matches a string.\n\nThe oldest and fastest relies on a result in formal language theory that allows every [[nondeterministic finite automaton]] (NFA) to be transformed into a [[deterministic finite automaton]] (DFA). The DFA can be constructed explicitly and then run on the resulting input string one symbol at a time. Constructing the DFA for a regular expression of size \"m\" has the time and memory cost of [[Big O notation|\"O\"]](2), but it can be run on a string of size \"n\" in time \"O\"(\"n\").\n\nAn alternative approach is to simulate the NFA directly, essentially building each DFA state on demand and then discarding it at the next step. This keeps the DFA implicit and avoids the exponential construction cost, but running cost rises to \"O\"(\"mn\"). The explicit approach is called the DFA algorithm and the implicit approach the NFA algorithm. Adding caching to the NFA algorithm is often called the \"lazy DFA\" algorithm, or just the DFA algorithm without making a distinction. These algorithms are fast, but using them for recalling grouped subexpressions, lazy quantification, and similar features is tricky.\n\nThe third algorithm is to match the pattern against the input string by [[backtracking]]. This algorithm is commonly called NFA, but this terminology can be confusing. Its running time can be exponential, which simple implementations exhibit when matching against expressions like that contain both alternation and unbounded quantification and force the algorithm to consider an exponentially increasing number of sub-cases. This behavior can cause a security problem called [[ReDoS|Regular expression Denial of Service]].\n\nAlthough backtracking implementations only give an exponential guarantee in the worst case, they provide much greater flexibility and expressive power. For example, any implementation which allows the use of backreferences, or implements the various extensions introduced by Perl, must include some kind of backtracking. Some implementations try to provide the best of both algorithms by first running a fast DFA algorithm, and revert to a potentially slower backtracking algorithm only when a backreference is encountered during the match.\n\nIn theoretical terms, any token set can be matched by regular expressions as long as it is pre-defined. In terms of historical implementations, regexes were originally written to use [[American Standard Code for Information Interchange|ASCII]] characters as their token set though regex libraries have supported numerous other [[character set]]s. Many modern regex engines offer at least some support for [[Unicode]]. In most respects it makes no difference what the character set is, but some issues do arise when extending regexes to support Unicode.\n\n\nRegexes are useful in a wide variety of [[text processing]] tasks, and more generally [[string processing]], where the data need not be textual. Common applications include [[data validation]], [[data scraping]] (especially [[web scraping]]), [[data wrangling]], simple [[parsing]], the production of [[syntax highlighting]] systems, and many other tasks.\n\nWhile regexes would be useful on Internet [[Search engine (computing)|search engine]]s, processing them across the entire database could consume excessive computer resources depending on the complexity and design of the regex. Although in many cases system administrators can run regex-based queries internally, most search engines do not offer regex support to the public. Notable exceptions: [[Google Code Search]], [[Exalead]]. Google Code Search has been shut down as of January 2012.\nIt used a trigram index to speed queries.\n\nThe specific syntax rules vary depending on the specific implementation, [[programming language]], or [[Library (computing)|library]] in use. Additionally, the functionality of regex implementations can vary between [[Software versioning|version]]s.\n\nBecause regexes can be difficult to both explain and understand without examples, interactive web sites for testing regexes are a useful resource for learning regexes by experimentation.\nThis section provides a basic description of some of the properties of regexes by way of illustration.\n\nThe following conventions are used in the examples.\n\nAlso worth noting is that these regexes are all Perl-like syntax. Standard [[#POSIX Basic Regular Expressions|POSIX]] regular expressions are different.\n\nUnless otherwise indicated, the following examples conform to the [[Perl]] programming language, release 5.8.8, January 31, 2006. This means that other implementations may lack support for some parts of the syntax shown here (e.g. basic vs. extended regex, codice_118 vs. codice_119, or lack of codice_120 instead of [[POSIX]] codice_121).\n\nThe syntax and conventions used in these examples coincide with that of other programming environments as well.\n\nRegular expressions can often be created (\"induced\" or \"learned\") based on a set of example strings. This is known as the [[induction of regular languages]], and is part of the general problem of [[grammar induction]] in [[computational learning theory]]. Formally, given examples of strings in a regular language, and perhaps also given examples of strings \"not\" in that regular language, it is possible to induce a grammar for the language, i.e., a regular expression that generates that language. Not all regular languages can be induced in this way (see [[language identification in the limit]]), but many can. For example, the set of examples {1, 10, 100}, and negative set (of counterexamples) {11, 1001, 101, 0} can be used to induce the regular expression 1⋅0* (1 followed by zero or more 0s).\n\n\n\n\n[[Category:Automata (computation)]]\n[[Category:Formal languages]]\n[[Category:Pattern matching]]\n[[Category:Programming constructs]]\n[[Category:Regular expressions| ]]\n[[Category:Articles with example code]]\n[[Category:1951 introductions]]"}
{"id": "3155895", "url": "https://en.wikipedia.org/wiki?curid=3155895", "title": "Robin Gandy", "text": "Robin Gandy\n\nRobin Oliver Gandy (22 September 1919 – 20 November 1995) was a British mathematician and logician.\nHe was a friend, student, and associate of Alan Turing, having been supervised by Turing during his PhD at the University of Cambridge, where they worked together.\n\nRobin Gandy was born in the village of Rotherfield Peppard, Oxfordshire, England. He was the son of Thomas Hall Gandy (1876–1948) and Ida Caroline née Hony (1885–1977) and great-great-grandson of the architect and artist Joseph Gandy (1771–1843).\n\nEducated at Abbotsholme School, Gandy took two years of the Mathematical Tripos, at King's College, Cambridge, before enlisting for military service in 1940. During World War II he worked on radio intercept equipment at Hanslope Park, where Alan Turing was working on a speech encipherment project, and he became one of Turing's lifelong friends and associates. In 1946, he completed Part III of the Mathematical Tripos, then began studying for a PhD under Turing's supervision. He completed his thesis, \"On axiomatic systems in mathematics and theories in Physics\", in 1952. He was a member of the Cambridge Apostles.\n\nGandy held positions at the University of Leicester, the University of Leeds, and the University of Manchester. Gandy was a visiting associate professor at Stanford University from 1966 to 1967, and held a similar position at University of California, Los Angeles in 1968. In 1969, he moved to Wolfson College, Oxford, where he became Reader in Mathematical Logic. One of the residential buildings of the college is now named in his honour.\n\nHe is best known for his work in recursion theory. His contributions include the Spector–Gandy theorem, the Gandy Stage Comparison theorem, and the Gandy Selection Theorem. He also made a significant contribution to the , and his generalisation of the Turing machine is called a Gandy machine.\n\nGandy died in Oxford, England.\n"}
{"id": "976305", "url": "https://en.wikipedia.org/wiki?curid=976305", "title": "Simon Donaldson", "text": "Simon Donaldson\n\nSir Simon Kirwan Donaldson FRS (born 20 August 1957), is an English mathematician known for his work on the topology of smooth (differentiable) four-dimensional manifolds and Donaldson–Thomas theory. He is currently a permanent member of the Simons Center for Geometry and Physics at Stony Brook University and a Professor in Pure Mathematics at Imperial College London.\n\nDonaldson's father was an electrical engineer in the physiology department at the University of Cambridge, and his mother earned a science degree there. Donaldson gained a BA degree in mathematics from Pembroke College, Cambridge in 1979, and in 1980 began postgraduate work at Worcester College, Oxford, at first under Nigel Hitchin and later under Michael Atiyah's supervision. Still a postgraduate student, Donaldson proved in 1982 a result that would establish his fame. He published the result in a paper \"Self-dual connections and the topology of smooth 4-manifolds\" which appeared in 1983. In the words of Atiyah, the paper \"stunned the mathematical world\" (Atiyah 1986).\n\nWhereas Michael Freedman classified topological four-manifolds, Donaldson's work focused on four-manifolds admitting a differentiable structure, using instantons, a particular solution to the equations of Yang–Mills gauge theory which has its origin in quantum field theory. One of Donaldson's first results gave severe restrictions on the intersection form of a smooth four-manifold. As a consequence, a large class of the topological four-manifolds do not admit any smooth structure at all. Donaldson also derived polynomial invariants from gauge theory. These were new topological invariants sensitive to the underlying smooth structure of the four-manifold. They made it possible to deduce the existence of \"exotic\" smooth structures—certain topological four-manifolds could carry an infinite family of different smooth structures.\n\nAfter gaining his DPhil degree from Oxford University in 1983, Donaldson was appointed a Junior Research Fellow at All Souls College, Oxford, he spent the academic year 1983–84 at the Institute for Advanced Study in Princeton, and returned to Oxford as Wallis Professor of Mathematics in 1985. After spending one year visiting Stanford University, he moved to Imperial College London in 1998.\n\nIn 2014, he joined the Simons Center for Geometry and Physics at Stony Brook University in New York, United States.\n\nDonaldson received the Junior Whitehead Prize from the London Mathematical Society in 1985 and in the following year he was elected a Fellow of the Royal Society and, also in 1986, he received a Fields Medal. He was awarded the 1994 Crafoord Prize.\n\nIn February 2006, Donaldson was awarded the King Faisal International Prize for science for his work in pure mathematical theories linked to physics, which have helped in forming an understanding of the laws of matter at a subnuclear level.\n\nIn April 2008, he was awarded the Nemmers Prize in Mathematics, a mathematics prize awarded by Northwestern University.\n\nIn 2009 he was awarded the Shaw Prize in Mathematics (jointly with Clifford Taubes) for their contributions to geometry in 3 and 4 dimensions.\n\nIn 2010, he was elected a foreign member of the Royal Swedish Academy of Sciences.\n\nDonaldson was knighted in the 2012 New Year Honours for services to mathematics.\n\nIn 2012 he became a fellow of the American Mathematical Society.\n\nIn March 2014, he was awarded the degree \"Docteur Honoris Causa\" by Université Joseph Fourier, Grenoble.\n\nIn 2014 he was awarded the Breakthrough Prize in Mathematics \"for the new revolutionary invariants of 4-dimensional manifolds and for the study of the relation between stability in algebraic geometry and in global differential geometry, both for bundles and for Fano varieties.\"\n\nIn January 2017, he was awarded the degree \"DOCTOR HONORIS CAUSA\" by the Universidad Complutense de Madrid, Spain.\n\nIn January 2019, he will be awarded the Oswald Veblen Prize in Geometry (jointly with Xiuxiong Chen and Song Sun).\n\nDonaldson's work is on the application of mathematical analysis (especially the analysis of elliptic partial differential equations) to problems in geometry. The problems mainly concern 4-manifolds, complex differential geometry and symplectic geometry. The following theorems have been mentioned:\n\n\nDonaldson's recent work centers on a problem in complex differential geometry concerning a conjectural relationship between algebro-geometric \"stability\" conditions for smooth projective varieties and the existence of \"extremal\" Kähler metrics, typically those with constant scalar curvature (see for example cscK metric). Donaldson obtained results in the toric case of the problem (see for example ). He then solved the Kähler-Einstein case of the problem in 2012, in collaboration with Chen and Sun. This latest spectacular achievement involved a number of difficult and technical papers. The first of these was the paper of on Gromov-Hausdorff limits. The summary of the existence proof for Kähler-Einstein metrics appears in . Full details of the proofs appear in .\n\nSee also Donaldson theory.\n\n\n\n"}
{"id": "12791220", "url": "https://en.wikipedia.org/wiki?curid=12791220", "title": "Singular integral", "text": "Singular integral\n\nIn mathematics, singular integrals are central to harmonic analysis and are intimately connected with the study of partial differential equations. Broadly speaking a singular integral is an integral operator\n\nwhose kernel function \"K\" : R×R → R is singular along the diagonal \"x\" = \"y\". Specifically, the singularity is such that |\"K\"(\"x\", \"y\")| is of size |\"x\" − \"y\"| asymptotically as |\"x\" − \"y\"| → 0. Since such integrals may not in general be absolutely integrable, a rigorous definition must define them as the limit of the integral over |\"y\" − \"x\"| > ε as ε → 0, but in practice this is a technicality. Usually further assumptions are required to obtain results such as their boundedness on \"L\"(R).\n\nThe archetypal singular integral operator is the Hilbert transform \"H\". It is given by convolution against the kernel \"K\"(\"x\") = 1/(π\"x\") for \"x\" in R. More precisely,\n\nThen it can be shown that \"T\" is bounded on \"L\"(R) and satisfies a weak-type (1, 1) estimate.\n\nProperty 1. is needed to ensure that convolution () with the tempered distribution p.v. \"K\" given by the principal value integral\nis a well-defined Fourier multiplier on \"L\". Neither of the properties 1. or 2. is necessarily easy to verify, and a variety of sufficient conditions exist. Typically in applications, one also has a \"cancellation\" condition\n\nwhich is quite easy to check. It is automatic, for instance, if \"K\" is an odd function. If, in addition, one assumes 2. and the following size condition\n\nthen it can be shown that 1. follows.\n\nThe smoothness condition 2. is also often difficult to check in principle, the following sufficient condition of a kernel \"K\" can be used:\nObserve that these conditions are satisfied for the Hilbert and Riesz transforms, so this result is an extension of those result.\n\nThese are even more general operators. However, since our assumptions are so weak, it is not necessarily the case that these operators are bounded on \"L\".\n\nA function \"K\" : R×R → R is said to be a \"Calderón–Zygmund kernel\" if it satisfies the following conditions for some constants \"C\" > 0 and δ > 0.\n\n\"T\" is said to be a \"singular integral operator of non-convolution type\" associated to the Calderón–Zygmund kernel \"K\" if\n\nwhenever \"f\" and \"g\" are smooth and have disjoint support. Such operators need not be bounded on \"L\"\n\nA singular integral of non-convolution type \"T\" associated to a Calderón–Zygmund kernel \"K\" is called a \"Calderón–Zygmund operator\" when it is bounded on \"L\", that is, there is a \"C\" > 0 such that\n\nfor all smooth compactly supported ƒ.\n\nIt can be proved that such operators are, in fact, also bounded on all \"L\" with 1 < \"p\" < ∞.\n\nThe \"T\"(\"b\") theorem provides sufficient conditions for a singular integral operator to be a Calderón–Zygmund operator, that is for a singular integral operator associated to a Calderón–Zygmund kernel to be bounded on \"L\". In order to state the result we must first define some terms.\n\nA \"normalised bump\" is a smooth function φ on R supported in a ball of radius 10 and centred at the origin such that |∂ φ(\"x\")| ≤ 1, for all multi-indices |α| ≤ \"n\" + 2. Denote by τ(φ)(\"y\") = φ(\"y\" − \"x\") and φ(\"x\") = \"r\"φ(\"x\"/\"r\") for all \"x\" in R and \"r\" > 0. An operator is said to be \"weakly bounded\" if there is a constant \"C\" such that\n\nfor all normalised bumps φ and ψ. A function is said to be \"accretive\" if there is a constant \"c\" > 0 such that Re(\"b\")(\"x\") ≥ \"c\" for all \"x\" in R. Denote by \"M\" the operator given by multiplication by a function \"b\".\n\nThe \"T\"(\"b\") theorem states that a singular integral operator \"T\" associated to a Calderón–Zygmund kernel is bounded on \"L\" if it satisfies all of the following three conditions for some bounded accretive functions \"b\" and \"b\":\n\n(a) formula_14 is weakly bounded;\n\n(b) formula_15 is in BMO;\n\n(c) formula_16 is in BMO, where \"T\" is the transpose operator of \"T\".\n\n\n"}
{"id": "16073214", "url": "https://en.wikipedia.org/wiki?curid=16073214", "title": "Tarski's exponential function problem", "text": "Tarski's exponential function problem\n\nIn model theory, Tarski's exponential function problem asks whether the theory of the real numbers together with the exponential function is decidable. Tarski had previously shown that the theory of the real numbers (without the exponential function) is decidable.\n\nThe ordered real field R is a structure over the language of ordered rings \"L\" = (+,·,−,<,0,1), with the usual interpretation given to each symbol. It was proved by Tarski that the theory of the real field, Th(R), is decidable. That is, given any \"L\"-sentence \"φ\" there is an effective procedure for determining whether\n\nHe then asked whether this was still the case if one added a unary function exp to the language that was interpreted as the exponential function on R, to get the structure R.\n\nThe problem can be reduced to finding an effective procedure for determining whether any given exponential polynomial in \"n\" variables and with coefficients in Z has a solution in R. showed that Schanuel's conjecture implies such a procedure exists, and hence gave a conditional solution to Tarski's problem. Schanuel's conjecture deals with all complex numbers so would be expected to be a stronger result than the decidability of R, and indeed, Macintyre and Wilkie proved that only a real version of Schanuel's conjecture is required to imply the decidability of this theory.\n\nEven the real version of Schanuel's conjecture is not a necessary condition for the decidability of the theory. In their paper, Macintyre and Wilkie showed that an equivalent result to the decidability of Th(R) is what they dubbed the Weak Schanuel's Conjecture. This conjecture states that there is an effective procedure that, given \"n\" ≥ 1 and exponential polynomials in \"n\" variables with integer coefficients \"f\"..., \"f\", \"g\", produces an integer \"η\" ≥ 1 that depends on \"n\", \"f\"..., \"f\", \"g\", and such that if \"α\" ∈ R is a non-singular solution of the system\n\nthen either \"g\"(\"α\") = 0 or |\"g\"(\"α\")| > \"η\".\n\nRecently there are attempts at handling the theory of the real numbers with functions such as exp, sin by relaxing decidability to the weaker notion of quasi-decidability. A theory is quasi-decidable if there is a procedure that decides satisfiability but that may run forever for inputs that are not robust in a certain, well-defined sense. Such a procedure exists for systems of n equations in n variables ().\n\n\n"}
{"id": "35508439", "url": "https://en.wikipedia.org/wiki?curid=35508439", "title": "Transmon", "text": "Transmon\n\nIn quantum computing, and more specifically in superconducting quantum computing, a transmon is a type of superconducting charge qubit that was designed to have reduced sensitivity to charge noise. The transmon was developed by Robert J. Schoelkopf, Michel Devoret, Steven M. Girvin and their colleagues at Yale University in 2007. Its name is an abbreviation of the term \"transmission line shunted plasma oscillation qubit\"; one which consists of a Cooper-pair box \"where the two superconductors are also capacitatively shunted in order to decrease the sensitivity to charge noise, while maintaining a sufficient anharmonicity for selective qubit control\".\nThe transmon achieves its reduced sensitivity to charge noise by significantly increasing the ratio of the Josephson energy to the charging energy. This is accomplished through the use of a large shunting capacitor. The result is energy level spacings that are approximately independent of offset charge. Planar on-chip transmon qubits have T coherence times ~ 30 μs to 40 μs. By replacing the superconducting transmission line cavity with a three-dimensional superconducting cavity, recent work on transmon qubits has shown significantly improved T times, as long as 95 μs. These results demonstrate that previous T times were not limited by Josephson junction losses. Understanding the fundamental limits on the coherence time in superconducting qubits such as the transmon is an active area of research.\n\nThe transmon design is similar to the charge qubit, both are described by the same Hamiltonian, with the only difference being the increase in the formula_1 ratio, achieved by shunting the Josephson junction with an additional large capacitor. Here formula_2 is the Josephson energy of the junction, and formula_3 is the charging energy inversely proportional to the total capacitance of the qubit circuit. The benefit of increasing the formula_1 ratio is insensitivity to charge noise - the energy levels become independent of electrical charge across the junction, thus the coherence times of the qubit are prolonged. The disadvantage is decrease in the anharmonicity formula_5, where formula_6 is the energy of the state formula_7. Reduced anharmonicity complicates the device operation as a two level system, e.g. exciting the device from the ground state to the first excited state by a resonant pulse also populates the second excited state. This complication is overcome by complex microwave pulse design, that takes into account the higher energy levels, and prohibits their excitation by destructive interference.\n\nMeasurement, control and coupling of the transmons is performed by means of microwave resonators with techniques of circuit quantum electrodynamics, also applicable to other superconducting qubits. The coupling to the resonators is done by a putting a capacitor between the qubit and the resonator, at a point where the resonator electromagnetic field is biggest. For example, in IBM Quantum Experience devices, the resonators are implemented with \"quarter wave\" coplanar waveguide with maximal field at the signal-ground short at the waveguide end, thus every IBM transmon qubit has a long resonator \"tail\". The initial proposal included similar transmission line resonators coupled to every transmon, becoming a part of the name. However, charge qubits operated at a similar formula_1 regime, coupled to different kinds of microwave cavities are referred as transmons as well.\n\n"}
{"id": "41104045", "url": "https://en.wikipedia.org/wiki?curid=41104045", "title": "Urs Schreiber", "text": "Urs Schreiber\n\nUrs Schreiber (born 1974) is a mathematician specializing in the connection between mathematics and theoretical physics (especially string theory) and currently working as a researcher at the Czech Academy of Sciences, Institute of Mathematics, Department for Algebra, Geometry and Mathematical Physics. \n\nSchreiber obtained his doctorate from the University of Duisburg-Essen in 2005 with a thesis supervised by Robert Graham and titled \"From Loop Space Mechanics to Nonabelian Strings\".\n\nSchreiber's research fields include the mathematical foundation of quantum field theory. \n\nSchreiber is a co-creator of the \"n\"Lab, a wiki for research mathematicians and physicists working in higher category theory.\n\n\n\n"}
{"id": "1593924", "url": "https://en.wikipedia.org/wiki?curid=1593924", "title": "Wess–Zumino–Witten model", "text": "Wess–Zumino–Witten model\n\nIn theoretical physics and mathematics, a Wess–Zumino–Witten (WZW) model, also called a Wess–Zumino–Novikov–Witten model, is a type of two-dimensional conformal field theory named after Julius Wess, Bruno Zumino, Sergei Novikov and Edward Witten.\nThe symmetry algebra of a WZW model is an affine Lie algebra.\n\nLet \"G\" denote a compact simply-connected Lie group and \"g\" its simple Lie algebra. Suppose that \"γ\" is a \"G\"-valued field on the complex plane. More precisely, we want \"γ\" to be defined on the Riemann sphere S ², which amounts to the complex plane compactified by adding a point at infinity.\n\nThe WZW model is then a nonlinear sigma model defined by \"γ\" with an action given by\n\nHere, is the partial derivative and the usual summation convention over indices is used, with a Euclidean metric. Here, formula_2 is the Killing form on \"g\", and thus the first term is the standard kinetic term of quantum field theory.\n\nThe term \"S\" is called the \"Wess–Zumino term\" and can be written as\n\nwhere [,] is the commutator, is the completely anti-symmetric tensor, and the integration coordinates \"y\" for \"i\"=1,2,3 range over the unit ball B ³. In this integral, the field γ has been extended so that it is defined on the interior of the unit ball. This extension can always be done because the homotopy group π (\"G\") always vanishes for any compact, simply-connected Lie group, and we originally defined \"γ\" on the 2-sphere S ² = ∂B ³.\n\nNote that if \"e\" are the basis vectors for the Lie algebra, then formula_4 are the structure constants of the Lie algebra. Note also that the structure constants are completely anti-symmetric, and thus they define a 3-form on the group manifold of \"G\". Thus, the integrand above is just the pullback of the harmonic 3-form to the ball B ³. Denoting the harmonic 3-form by \"c\" and the pullback by \"γ\", one then has \nThis form leads directly to a topological analysis of the WZ term.\n\nGeometrically, this term describes the torsion of the respective manifold. The presence of this torsion compels teleparallelism of the manifold, and thus trivialization of the torsionful curvature tensor; and hence arrest of the renormalization flow, an infrared fixed point of the renormalization group, a phenomenon termed geometrostasis.\n\nThe extension of the field to the interior of the ball is not unique; the need that the physics be independent of the extension imposes a quantization condition on the coupling parameter \"k\", the \"level\". Consider two different extensions of γ to the interior of the ball. They are maps from flat 3-space into the Lie group \"G\". Consider now glueing these two balls together at their boundary S². The result of the gluing is a topological 3-sphere; each ball B³ is a hemisphere of S³. The two different extensions of \"γ\" on each ball now becomes a map S³ → \"G\". However, the homotopy group π(\"G\") = ℤ for any compact, connected simple Lie group \"G\".\n\nThus, one has\nwhere \"γ\" and \"γ' \" denote the two different extensions onto the ball, and \"n\", an integer, is the winding number of the glued-together map.\n\nThe physics that this model leads to will stay the same if\n\nThus, topological considerations lead one to conclude that the level \"k\" must be an integer when \"G\" is a connected, compact, simple Lie group. For a semisimple or disconnected compact Lie group, the level consists of an integer for each connected, simple component.\n\nThis topological obstruction can also be seen in the representation theory of the affine Lie algebra symmetry of the theory. When each level is a positive integer the affine Lie algebra has unitary highest weight representations with highest weights that are dominant integral. Such representations are easier to work with as they decompose into finite-dimensional subalgebras with respect to the subalgebras spanned by each simple root, the corresponding negative root and their commutator, which is a Cartan generator.\n\nOften one is interested in a WZW model with a noncompact simple Lie group \"G\", such as SL(2,ℝ) which has been used by Juan Maldacena and Hirosi Ooguri to describe string theory on a three-dimensional anti-de Sitter space, which is the universal cover of the group SL(2,ℝ). In this case, as π(SL(2, ℝ)) = 0, there is no topological obstruction and the level need not be integral. Correspondingly, the representation theory of such noncompact Lie groups is much richer than that of their compact counterparts.\n\nAlthough in the above, the WZW model is defined on the Riemann sphere, it can be generalized so that the field γ lives on a compact Riemann surface.\n\nThe current algebra of the WZW model is a Kac–Moody algebra. The stress energy tensor is given by the Sugawara construction.\n\nTaking the quotient of two WZW models gives a new conformal field theory whose central charge is the difference of the two original ones.\n"}
