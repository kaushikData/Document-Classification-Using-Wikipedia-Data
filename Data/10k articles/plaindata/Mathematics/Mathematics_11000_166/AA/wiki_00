{"id": "391875", "url": "https://en.wikipedia.org/wiki?curid=391875", "title": "82 (number)", "text": "82 (number)\n\n82 (eighty-two) is the natural number following 81 and preceding 83.\n\n82 is:\n\n\nEighty-two is also:\n\n\n"}
{"id": "57031208", "url": "https://en.wikipedia.org/wiki?curid=57031208", "title": "ACM Doctoral Dissertation Award", "text": "ACM Doctoral Dissertation Award\n\nThe ACM Doctoral Dissertation Award is awarded annually by the Association for Computing Machinery to the authors of the best doctoral dissertations in computer science and computer engineering. The award is accompanied by a prize of US $20,000 and winning dissertations are published in the ACM Digital Library. Honorable mentions are awarded $10,000. Financial support is provided by Google. The number of awarded dissertations may vary year-to-year.\n\nACM also awards the ACM India Doctoral Dissertation Award. Several Special Interest Groups (SIGs) award a Doctoral Dissertation Award.\n\n"}
{"id": "1138222", "url": "https://en.wikipedia.org/wiki?curid=1138222", "title": "Abelian variety of CM-type", "text": "Abelian variety of CM-type\n\nIn mathematics, an abelian variety \"A\" defined over a field \"K\" is said to have CM-type if it has a large enough commutative subring in its endomorphism ring End(\"A\"). The terminology here is from complex multiplication theory, which was developed for elliptic curves in the nineteenth century. One of the major achievements in algebraic number theory and algebraic geometry of the twentieth century was to find the correct formulations of the corresponding theory for abelian varieties of dimension \"d\" > 1. The problem is at a deeper level of abstraction, because it is much harder to manipulate analytic functions of several complex variables.\n\nThe formal definition is that \n\nthe tensor product of End(\"A\") with the rational number field Q, should contain a commutative subring of dimension 2\"d\" over Q. When \"d\" = 1 this can only be a quadratic field, and one recovers the cases where End(\"A\") is an order in an imaginary quadratic field. For \"d\" > 1 there are comparable cases for CM-fields, the complex quadratic extensions of totally real fields. There are other cases that reflect that \"A\" may not be a simple abelian variety (it might be a cartesian product of elliptic curves, for example). Another name for abelian varieties of CM-type is abelian varieties with sufficiently many complex multiplications.\n\nIt is known that if \"K\" is the complex numbers, then any such \"A\" has a field of definition which is in fact a number field. The possible types of endomorphism ring have been classified, as rings with involution (the Rosati involution), leading to a classification of CM-type abelian varieties. To construct such varieties in the same style as for elliptic curves, starting with a lattice Λ in C, one must take into account the Riemann relations of abelian variety theory.\n\nThe CM-type is a description of the action of a (maximal) commutative subring \"L\" of End(\"A\") on the holomorphic tangent space of \"A\" at the identity element. Spectral theory of a simple kind applies, to show that \"L\" acts via a basis of eigenvectors; in other words \"L\" has an action that is via diagonal matrices on the holomorphic vector fields on \"A\". In the simple case, where \"L\" is itself a number field rather than a product of some number of fields, the CM-type is then a list of complex embeddings of \"L\". There are 2\"d\" of those, occurring in complex conjugate pairs; the CM-type is a choice of one out of each pair. It is known that all such possible CM-types can be realised.\n\nBasic results of Goro Shimura and Yutaka Taniyama compute the Hasse–Weil L-function of \"A\", in terms of the CM-type and a Hecke L-function with Hecke character, having infinity-type derived from it. These generalise the results of Max Deuring for the elliptic curve case.\n"}
{"id": "3568037", "url": "https://en.wikipedia.org/wiki?curid=3568037", "title": "Adaptive simulated annealing", "text": "Adaptive simulated annealing\n\nAdaptive simulated annealing (ASA) is a variant of simulated annealing (SA) algorithm in which the algorithm parameters that control temperature schedule and random step selection are automatically adjusted according to algorithm progress. This makes the algorithm more efficient and less sensitive to user defined parameters than canonical SA. These are in the standard variant often selected on the basis of experience and experimentation (since optimal values are problem dependent), which represents a significant deficiency in practice.\n\nThe algorithm works by representing the parameters of the function to be optimized as continuous numbers, and as dimensions of a hypercube (N dimensional space). Some SA algorithms apply Gaussian moves to the state, while others have distributions permitting faster temperature schedules. Imagine the state as a point in a box and the moves as a rugby-ball shaped cloud around it. The temperature and the step size are adjusted so that all of the search space is sampled to a coarse resolution in the early stages, whilst the state is directed to favorable areas in the late stages. Another ASA variant, thermodynamic simulated annealing, automatically adjusts the temperature at each step based on the energy difference between the two states, according to the laws of thermodynamics.\n\n\n\n"}
{"id": "21127927", "url": "https://en.wikipedia.org/wiki?curid=21127927", "title": "Age stratification", "text": "Age stratification\n\nIn sociology, age stratification refers to the hierarchical ranking of people into age groups within a society. Age stratification could also be defined as a system of inequalities linked to age. In Western societies, for example, both the old and the young are perceived and treated as relatively incompetent and excluded from much social life. Age stratification based on an ascribed status is a major source inequality, and thus may lead to ageism. Ageism is a social inequality resulting from age stratification. This is a sociological concept that comes with studying aging population. Age stratification within a population can have major implications, affecting things such as workforce trends, social norms, family structures, government policies, and even health outcomes.\n\nAge stratification is not a fixed phenomena, but rather varies with the passage of time and between cultures and populations. Shifting age structure of a population changes the age stratification. As life expectancy has increased dramatically in the last two centuries, the age strata by which people are characterized has changed. With people living longer lives than ever before in more developed areas of the world, there is now a category of \"old-old\" people which refers to persons ages 85+. Changes in the age structure of populations affects the way in which they distribute resources, along with a shift in expectations from different age strata. For example, as Japan's population has dramatically aged - with individuals aged 65+ accounting for approximately 25% of the population - the country has found itself with an unfavorable dependency ratio. In an effort to avoid economic downfall, the expectations of young-old and middle-old people have changed. Elderly citizens are encouraged to put off retirement, and the elderly tech market is booming.\n\nAge is a major component of entry and exit for many parts of life - school, starting a family, retirement, etc. Shifting social status with age can lead to ageism. Discrimination by a person's age can have profound impacts on the way a society operates - including behavioral expectations, the distribution of resources, and even policies and laws.\n\nIn the United States, discrimination on the basis of one's age is prohibited in the workplace by the Age Discrimination in Employment Act of 1967. Enforced by the Equal Employment Opportunity Commission, the act is meant to keep employers unbiased in regards to age when dealing with hiring, promotions, terms, etc. The law also makes it illegal for employees to be harassed due to their age. Emergence of new occupations can lead to a polarization of age cohorts by workforce. As a result, a quick shift of the occupational distribution increases occupational age discrimination.\n\nThe unequal distribution of resources and social support between age strata can lead to health disparities in the population. In the U.S., evidence indicates older adults face higher risk of experiencing depression and other mental health issues.\n\n\n\n"}
{"id": "288288", "url": "https://en.wikipedia.org/wiki?curid=288288", "title": "André Sainte-Laguë", "text": "André Sainte-Laguë\n\nAndré Sainte-Laguë (, 20 April 1882 – 18 January 1950) was a French mathematician who was a pioneer in the area of graph theory.\nHis research on seat allocation methods (published in 1910) led to one being named after him, the Sainte-Laguë method. Also named after him is the Sainte-Laguë Index for measuring the proportionality of an electoral outcome.\n\nHe is also notable for his informal calculation that supposedly demonstrated that a bumblebee could not fly, referred to in the introduction of 'Le Vol des Insectes' (Hermann and Cle, Paris, 1934) by the entomologist Antoine Magnan. This casual calculation was based on a comparison between an aeroplane and a bee, making the wrong assumption that bees' wings were smooth and flat. He, and others, soon corrected this assumption, but the story of the scientist who demonstrated that bee flight was impossible persists to this day.\n\nHe published several popular math texts, including \"From the known to the unknown\" (foreword by biologist Jean Rostand) which has been translated into several languages.\n\nBorn in Casteljaloux (Lot-et-Garonne) in 1882, Sainte-Lague was admitted at once, at the age of 20 years at the Ecole Polytechnique and Ecole Normale Superieure. He chose the latter and became a professor in the provinces, then in Paris. During World War I, having been wounded three times, he was attached to the Department of Inventions of the Normal School from 1917 to 1919, studied long-range artillery shells, and thereafter, the flight of birds and matters relating to aviation (theory test fish).\n\nAfter the First World War, as a professor in the schools of Paris, he became a lecturer in mathematics at the Conservatoire National des Arts et Metiers. Then he received in 1938, the Chair of Applied Mathematics. He trained generations of engineers and technicians. He was the organizer and host of the Mathematics Section of the Palace of Discovery, where his encyclopedic mind is still present.\n\nBesides his academic career, he led a life of an activist, especially the , of which he was president in 1929. From the earliest days of the occupation, he took an important part in the resistance and was even imprisoned for a while. He resumed his duties at the Conservatoire National des Arts et Metiers after the Liberation and had a growing number of students. At his death he was teaching three courses totaling two thousand five hundred students.\n\nOfficer of the Legion of Honour, Croix de Guerre and Medal of the Resistance, a professor at the School of Special Public Works, chairman of the International Confederation Intellectual of Workers, Vice-President of the Confederation of the Middle Class, former president of the Society of Fellows, former vice-president of National Economic Council, former member of the General Council of the Banque de France, former Deputy Provisional Consultative Assembly .(...)\n\nHis sudden death came at the very moment he had just accepted the chairmanship of the Committee of the League of Friends of the Psychic Institute, where he was vice president in 1949 and member since 1934. \"(R. Warcollier, Vice- President of IMI, January–February–March 1950)\n"}
{"id": "30975689", "url": "https://en.wikipedia.org/wiki?curid=30975689", "title": "Bloch wave – MoM method", "text": "Bloch wave – MoM method\n\nBloch wave – MoM is a first principles technique for determining the photonic band structure of triply-periodic electromagnetic media such as photonic crystals. It is based on the 3-dimensional spectral domain method (Kastner [1987]), specialized to triply-periodic media. This technique uses the method of moments (MoM) in combination with a Bloch wave expansion of the electromagnetic field to yield a matrix eigenvalue equation for the propagation bands. The eigenvalue is the frequency (for a given propagation constant) and the eigenvector is the set of current amplitudes on the surface of the scatterers. Bloch wave - MoM is similar in principle to the Plane wave expansion method, but since it additionally employs the method of moments to produce a surface integral equation, it is significantly more efficient both in terms of the number of unknowns and the number of plane waves needed for good convergence. \n\nBloch wave - MoM is the extension to 3 dimensions of the spectral domain MoM method commonly used for analyzing 2D periodic structures such as frequency selective surfaces (FSS). In both cases, the field is expanded as a set of eigenfunction modes (either a Bloch wave in 3D or a discrete plane wave - aka Floquet mode - spectrum in 2D), and an integral equation is enforced on the surface of the scatterers in each unit cell. In the FSS case, the unit cell is 2-dimensional and in the photonic crystal case, the unit cell is 3-dimensional.\n\nThe Bloch wave - MoM approach will be illustrated here for the case of perfectly electrically conducting (PEC) structures admitting only electric current sources, J. However, It can also be readily expanded to dielectric structures, using the well-known interior and exterior equivalent problems commonly employed in ordinary spatial domain method of moments formulations (Harrington [1961]). In dielectric problems, there are twice as many unknowns - J & M - and also twice as many equations to enforce - continuity of tangential E & H - at the dielectric interfaces (Scott [1998]).\nFor PEC structures, the electric field E is related to the vector magnetic potential A via the well-known relation:\n\nand the vector magnetic potential is in turn related to the source currents via:\n\nwhere\n\nTo solve equations (1.1) and (1.2) within the infinite periodic volume, we may assume a Bloch wave expansion for all currents, fields and potentials:\n\nwhere for simplicity, we assume an orthogonal lattice in which α only depends on \"m\", β only depends on \"n\" and γ only depends on \"p\". With this assumption,\n\nand,\n\nwhere \"l\", \"l\", \"l\" are the unit cell dimensions in the \"x\",\"y\",\"z\" directions respectively, λ is the effective wavelength in the crystal and θ, φ are the directions of propagation in spherical coordinates. \nThe quantity \"k\" in equations (1.1) and (1.2) comes originally from the time derivative in Maxwell's equations and is the \"free space\" propagation constant (actually, the propagation constant of whatever dielectric medium the metallic scatterers are embedded in), proportional to frequency as in equation (1.3). On the other hand, \"k\" in the equations above comes from the \"assumed Bloch wave solution\" given by equations (2.1) & (2.2). As a result, it represents the propagation constant inside the periodic medium, inversely proportional to wavelength. These two \"k's\", i.e. the free space propagation constant (proportional to frequency) and the propagation constant of the Bloch wave (inversely proportional to wavelength), are in general different thereby allowing for dispersion in the solution. The band diagram is essentially a plot of \"k\" as a function of \"k\".\nThe Bloch wave expansions in equations (2.1) are nothing more than exponential Fourier series multiplied by the cell-to-cell propagation factor: formula_11 The Bloch wave expansions are chosen since any field solution within an infinite periodic volume must have the same periodicity as the medium itself, or stated another way, the fields in neighboring cells must be identical up to a (real or complex) propagation factor. In passbands the propagation factor is an exponential function with purely imaginary argument and in the stop bands (or band gaps) it is a decaying exponential function whose argument has a real component.\nThe wave numbers α, β and γ satisfy the relations: formula_12 and outside of these ranges, the bands are periodic. \n\nThe Bloch waves are periodic functions of space, with periods \"l\", \"l\", \"l\" and the bands are periodic functions of wavenumber, with periods: formula_13, formula_14 and formula_15\n\nSubstituting equations (2.1) into (1.1) and (1.2) yields the spectral domain Greens function relating the radiated electric field to its source currents:\n\nwhere,\n\nis the tensor Green's function in the spectral domain. Note that the spatial domain convolution has been transformed into a simple multiplication in the spectral domain, consistent with the convolution theorem for Fourier transforms.\nWith this equation for the electric field, the electric field boundary condition (requiring that the total tangential electric field be zero on the surface of PEC scatterer) becomes:\n\nSince we are seeking characteristic modes (eigenmodes) of the structure, there is no impressed E-field on the RHS of this electric field integral equation (EFIE). Equation (3.3) is not strictly correct however, since it is only the tangential components of electric field that are actually zero on the surface of the PEC scatterer. This inexactness will be resolved presently, when we test this equation with the electric current basis functions - defined as residing on the surface of the scatterer.\n\nAs is usual in the method of moments, the source currents are now expanded in terms of a sum over some known set of basis functions with unknown weighting coefficients \"J\" :\n\nDifferent structures will have different sets of basis functions for representing the currents on the elements and as in the ordinary spatial domain method of moments, the solution (in this case, the band diagram) \"is a function of the set of basis functions used\".\n\nSubstituting (4.1) into (3.3) and then testing the resulting equation with the \"i\"-th current basis function (i.e., dotting from the left and integrating over the domain of the \"i\"-th current basis function, thereby completing the quadratic form) produces the \"i\"-th row of the matrix eigenvalue equation for a 3-dimensional array of PEC scatterers as:\nAs in all MoM formulations, the reaction concept in electromagnetics (Rumsey [1954], Harrington [1961]) was used in obtaining this equation. The electric field boundary/continuity conditions are \"tested\" (or enforced) by being integrated against electric current basis functions (for dielectric structures, the magnetic field continuity conditions are additionally tested by being integrated against magnetic current basis functions), and this is how the electric (and magnetic) field boundary conditions are converted into a matrix equation via the method of moments. This process is wholly analogous to that used to decompose a periodic function into its Fourier sine & cosine components, the only difference being that in this case the basis functions are not necessarily orthogonal, merely linearly independent. \nThis matrix equation is easy to implement and requires only that the 3D Fourier transform (FT) of the basis functions be computed, preferably in closed form (see Scott [1998], available on researchgate.net). In fact, computing bands of a 3D photonic crystal with this method is no more difficult than computing reflection and transmission from a 2D periodic surface using the spectral domain method . This is because equation (4.2) is identical to the basic EFIE for a freestanding PEC FSS (Scott [1989] or Frequency selective surface eq. (4.2)), the only difference being the stronger singularity in 3D which significantly accelerates convergence of the triple sums, and of course the fact that the vectors are now 3-dimensional. As a result, an ordinary PC is sufficient to compute bands of many types of photonic crystals.\nIt's evident from (4.2) that the EFIE could become singular whenever the free space wavenumber is exactly equal to one of the wave numbers in any of the 3 periodic coordinate directions. This can happen for example when the free space wavelength exactly equals the lattice spacing. This is a statistically rare occurrence in computational practice and corresponds to a propagation anomaly similar to a Wood's reflection anomaly for gratings.\n\nTo compute bands of the crystal (i.e. \"k\"-\"k\" diagrams), successive values of frequency (\"k\") are tried - in conjunction with pre-selected values of propagation constant (\"k\") and propagation direction (θ & φ) - until a combination is found which drives the determinant of the matrix to zero. Equation (4.2) has been used to compute bands in various types of doped and undoped photonic crystals (Scott [1998], Scott [2002], both available on researchgate.net). Not surprisingly, doping photonic crystals with defects provides a means for designing photonic passbands, in just the same way that doping semiconductors with chemical impurities provides a means for designing electronic passbands.\nFor many subsectional basis functions, such as those having a half-sine or triangular shape along a round wire, the FT of the basis function for negative wave numbers -α, -β, -γ is the complex conjugate of the basis function FT for positive wave numbers. As a result, the matrix in eqn. (4.2) is Hermitian. And as a result of that, only half the matrix needs to be computed. And a second result is that the determinant is a purely real function of the real-valued wavenumber \"k\". Zeroes generally occur at zero-crossings (inflection points, where curvature is zero), so a simple root-finding algorithm such as Newton's method is usually sufficient to find the roots to a very high degree of accuracy. If may still be useful however to plot the determinant as a function of \"k\", to observe its behavior near the zeros.\nAs a matter of computational convenience, whenever the matrix is larger than 2x2 it's vastly more efficient to compute the determinant either by reducing the matrix to upper triangular form using QR decomposition or to compute the determinant by reducing to echelon form using Gaussian elimination, rather than trying to actually compute the determinant of the matrix directly.\n\n\n"}
{"id": "53316953", "url": "https://en.wikipedia.org/wiki?curid=53316953", "title": "Classical Lie algebras", "text": "Classical Lie algebras\n\nThe classical Lie algebras are finite-dimensional Lie algebras that can be classified into four types: formula_1 and formula_2. These types are defined as follows:\n\nwhere formula_11 is the general Lie algebra of matrices formula_12 by formula_12 with coefficients in formula_14 or formula_15, formula_16 is the identity matrix of dimension formula_17, formula_18 denotes transposition and formula_19.\nExcept for the low-dimensional cases formula_20 and formula_21, the classical Lie algebras are simple.\n\nThe Moyal algebra is an infinite-dimensional Lie algebra that contains all classical Lie algebras as subalgebras.\n"}
{"id": "4576873", "url": "https://en.wikipedia.org/wiki?curid=4576873", "title": "Clique graph", "text": "Clique graph\n\nIn graph theory, a clique graph of an undirected graph \"G\" is another graph \"K\"(\"G\") that represents the structure of cliques in \"G\".\n\nClique graphs were discussed at least as early as 1968, and a characterization of clique graphs was given in 1971.\n\nA clique of a graph \"G\" is a set \"X\" of vertices of \"G\" with the property that every pair of distinct vertices in \"X\" are adjacent in \"G\".\nA maximal clique of a graph \"G\" is a clique \"X\" of vertices of \"G\", such that there is no clique \"Y\" of vertices of \"G\" that contains all of \"X\" and at least one other vertex.\n\nGiven a graph \"G\", its clique graph \"K\"(\"G\") is a graph such that\n\nThe clique graph \"K\"(\"G\") can also be characterized as the intersection graph of the maximal cliques of \"G\".\n\nA graph \"H\" is the clique graph \"K\"(\"G\") of another graph if and only if there exists a collection \"C\" of cliques in \"H\" whose union covers all the edges of \"H\", such that \"C\" forms a Helly family. This means that, if \"S\" is a subset of \"C\" with the property that every two members of \"S\" have a non-empty intersection, then \"S\" itself should also have a non-empty intersection. However, the cliques in \"C\" do not necessarily have to be maximal cliques.\n\nWhen \"H\" =\"K\"(\"G\"), a family \"C\" of this type may be constructed in which each clique in \"C\" corresponds to a vertex \"v\" in \"G\", and consists of the cliques in \"G\" that contain \"v\". These cliques all have \"v\" in their intersection, so they form a clique in \"H\". The family \"C\" constructed in this way has the Helly property, because any subfamily of \"C\" with pairwise nonempty intersections must correspond to a clique in \"G\", which can be extended to a maximal clique that belongs to the intersection of the subfamily.\n\nConversely, when \"H\" has a Helly family \"C\" of its cliques, covering all edges of \"H\", then it is the clique graph \"K\"(\"G\") for a graph \"G\" whose vertices are the disjoint union of the vertices of \"H\" and the elements of \"C\". This graph \"G\" has an edge for each pair of cliques in \"C\" with nonempty intersection, and for each pair of a vertex of \"H\" and a clique in \"C\" that contains it. However, it does not contain any edges connecting pairs of vertices in \"H\". The maximal cliques in this graph \"G\" each consist of one vertex of \"H\" together with all the cliques in \"C\" that contain it, and their intersection graph is isomorphic to \"H\".\n\nHowever, this characterization does not lead to efficient algorithms: the problem of recognizing whether a given graph is a clique graph is NP-complete.\n\n"}
{"id": "23252771", "url": "https://en.wikipedia.org/wiki?curid=23252771", "title": "Completeness of atomic initial sequents", "text": "Completeness of atomic initial sequents\n\nIn sequent calculus, the completeness of atomic initial sequents states that initial sequents (where is an arbitrary formula) can be derived from only atomic initial sequents (where is an atomic formula). This theorem plays a role analogous to eta expansion in lambda calculus, and dual to cut-elimination and beta reduction. Typically it can be established by induction on the structure of , much more easily than cut-elimination.\n\n"}
{"id": "51361722", "url": "https://en.wikipedia.org/wiki?curid=51361722", "title": "Cop-win graph", "text": "Cop-win graph\n\nIn graph theory, a cop-win graph is an undirected graph on which the pursuer (cop) can always win a pursuit-evasion game in which he chases a robber, the players alternately\nmoving along an edge of a graph or staying put, until the cop lands on the robber's vertex. Finite cop-win graphs are also called dismantlable graphs or constructible graphs, because they can be dismantled by repeatedly removing a dominated vertex (one whose closed neighborhood is a subset of another vertex's neighborhood) or constructed by repeatedly adding such a vertex. The cop-win graphs can be recognized in polynomial time by a greedy algorithm that constructs a dismantling order. They include the chordal graphs, and the graphs that contain a universal vertex.\n\nCop-win graphs (and the complementary class of graphs, robber-win graphs) were introduced by in the context of the following pursuit-evasion game, whose invention they credit to G. Gabor and A. Quilliot. Two players, a cop and a robber, are positioned at different initial vertices of a given graph. They play in turns; on each player's turn, the player may either move to an adjacent vertex or stay put. The game ends, and the cop wins, if the cop can end his turn on the same vertex as the robber. The robber wins by indefinitely evading the cop. A cop-win graph is a graph with the property that, no matter where the two players start, the cop can always force a win.\n\nThe closed neighborhood of a vertex in a given graph is the set of vertices consisting of itself and all other vertices adjacent to . The vertex is said to be \"dominated\" by another vertex when . That is, and are adjacent, and every other neighbor of is also a neighbor of . call a vertex that is dominated by another vertex an \"irreducible vertex\".\n\nA \"dismantling order\" or \"domination elimination ordering\" of a given graph is an ordering of the vertices such that, if the vertices are removed one-by-one in this order, each vertex (except the last) is dominated. A graph is dismantlable if and only if it has a dismantling order.\n\nThe family of cop-win graphs is closed under strong products of graphs. The cop can win in a strong product of cop-win graphs by playing to win one of the factors of the product, and after doing so playing in the remaining factors in the same way while continuing to stay on the same vertex as the robber in the already-won factor. For instance, the king's graph, a strong product of two path graphs, is cop-win.\n\nIt is not true that every induced subgraph of a cop-win graph is cop-win. However, certain special induced subgraphs do remain cop-win.\n\nEvery dismantlable graph is cop-win. A winning strategy for the cop is to find a dominated vertex , and to follow (by induction) an optimal strategy on the graph formed by removing , pretending that the robber is on the vertex that dominates whenever he is actually on . This will result either in an actual win of the game, or in a position where the robber is on and the cop is on the dominating vertex, from which the cop can win in one more move. This strategy can be used as the basis for a proof by induction of the fact that, in an -vertex graph, the cop can force a win in at most moves.\n\nConversely, every cop-win graph has a dominated vertex. For, if the robber plays optimally to make the game last as long as possible and the last position of the game before the cop wins has the cop at vertex \"c\" and the robber at \"r\", then \"r\" must be dominated by \"c\", else the robber could prolong the game for at least one move. The function that maps \"r\" onto \"c\" and leaves every other vertex in place is a retraction, so the graph formed by removing the dominated vertex must remain cop-win. By induction, it follows that every cop-win graph is dismantlable. The same argument shows that, in a graph with no dominated vertices, the robber can never lose: there is always a move to a vertex that is not adjacent to the cop. In an arbitrary graph that is not cop-win, the robber can win by removing all dominated vertices and playing within the remaining subgraph, which must be non-empty else the graph would be dismantlable.\n\nIf a vertex in a cop-win graph is dominated, then (as other dominated vertices are removed) it remains dominated until it is either removed itself or it becomes the final vertex of a dismantling order. Therefore, the collection of valid dismantling orders has the structure of an antimatroid, and a dismantling order can be found by a simple greedy algorithm that repeatedly finds and removes any dominated vertex. The process succeeds if and only if the graph is cop-win, so as well as providing an algorithm for finding dismantling orders this method provides an algorithm for testing whether a given graph is cop-win.\nOne way to find each successive vertex to remove is to perform the following steps:\nIn a graph with vertices, edges, and degeneracy , this process can be carried out in time\n\nAn alternative and more complicated algorithm by involves maintaining a number called the \"deficit\" for each adjacent pair of vertices , which counts the number of neighbors of that are not neighbors of . It constructs and maintains the actual \"deficit set\" (neighbors of that are not neighbors of ) only when the deficit is small. To speed up its computations, it uses decision trees that classify vertices according to their adjacencies with small blocks of vertices. It performs the following steps:\nThe time for this procedure is .\n\nEvery finite chordal graph is a dismantlable graph, and every elimination ordering of a chordal graph (an ordering of the vertices in which the later neighbors of each vertex form a clique) is a valid dismantling order. However, there exist infinite chordal graphs that are not cop-win.\n\nEvery graph that has a universal vertex is dismantlable, for every other vertex is dominated by the universal vertex, and any vertex ordering that places the universal vertex last is a valid dismantling order. Conversely, almost all dismantlable graphs have a universal vertex, in the sense that, among all -vertex dismantlable graphs, the fraction of these graphs that have a universal vertex goes to one in the limit as goes to infinity.\nThe hereditarily cop-win graphs are the graphs in which every isometric subgraph is cop-win. This is not true for all cop-win graphs; for instance, the five-vertex wheel graph is cop-win but contains an isometric 4-cycle, which is not cop-win, so this wheel graph is not hereditarily cop-win. The hereditarily cop-win graphs are the same as the bridged graphs, graphs in which every cycle of length four or more has a shortcut, a pair of vertices closer in the graph than they are in the cycle. A cop-win graph is hereditarily cop-win if and only if it has neither the 4-cycle nor 5-cycle as induced cycles. For a hereditarily cop-win graph, the reversal of any breadth-first traversal is a valid dismantling order, from which it follows that any vertex can be chosen as the last vertex of a dismantling order.\n\nA similar game with larger numbers of cops can be used to define the cop number of a graph, the smallest number of cops needed to win the game. The cop-win graphs are exactly the graphs of cop number equal to one.\n\n"}
{"id": "22728378", "url": "https://en.wikipedia.org/wiki?curid=22728378", "title": "De Bruijn–Erdős theorem (graph theory)", "text": "De Bruijn–Erdős theorem (graph theory)\n\nIn graph theory, the De Bruijn–Erdős theorem relates graph coloring of an infinite graph to the same problem on its finite subgraphs. It states that, when all finite subgraphs can be colored with formula_1 colors, the same is true for the whole graph. The theorem was proved by , after whom it is named.\n\nThe De Bruijn–Erdős theorem has several different proofs, all depending in some way on the axiom of choice. Its applications include extending the four-color theorem and Dilworth's theorem from finite graphs and partially ordered sets to infinite ones, and reducing the Hadwiger–Nelson problem on the chromatic number of the plane to a problem about finite graphs. It may be generalized from finite numbers of colors to sets of colors whose cardinality is a strongly compact cardinal.\n\nAn undirected graph is a mathematical object consisting of a set of vertices, and another set of pairs of vertices, called edges. The two vertices associated with each edge are called its endpoints. The graph is finite when its vertices and edges form finite sets, and infinite otherwise. A graph coloring associates each vertex with a color drawn from a set of colors, in such a way that every edge has two different colors at its endpoints. A frequent goal in graph coloring is to minimize the total number of colors that are used; the chromatic number of a graph is this minimum number of colors. The four-color theorem states that every finite graph that can be drawn without crossings in the Euclidean plane needs at most four colors, but some other graphs require more than four colors. It is a consequence of the axiom of choice that the chromatic number is well-defined for infinite graphs, but for these graphs the chromatic number might itself be infinite.\n\nA subgraph of a graph is another graph obtained from a subset of its vertices and a subset of its edges. If the larger graph is colored, the same coloring can be used for the subgraph. Therefore, the chromatic number of a subgraph cannot be larger than the chromatic number of the whole graph. The De Bruijn–Erdős theorem concerns the chromatic numbers of infinite graphs, and shows that (again, assuming the axiom of choice) they can be calculated from the chromatic numbers of their finite subgraphs. It states that, if the chromatic numbers of the finite subgraphs of a graph formula_2 have a finite maximum value formula_1, then the chromatic number of formula_2 itself is exactly formula_1. On the other hand, if there is no finite upper bound on the chromatic numbers of the finite subgraphs of formula_2, then the chromatic number of formula_2 itself must be infinite.\n\nOne application of the De Bruijn–Erdős theorem is to the Hadwiger–Nelson problem, which asks how many colors are needed to color the points of the Euclidean plane so that every two points that are a unit distance apart have different colors. This is a graph coloring problem for an infinite graph that has a vertex for every point of the plane and an edge for every two points whose Euclidean distance is exactly one. The subgraphs of this graph are called unit distance graphs. A seven-vertex unit distance graph, the Moser spindle, requires four colors; in 2018, much larger unit distance graphs were found that require five colors. The whole infinite graph has a known coloring with seven colors based on a hexagonal tiling of the plane. Therefore, the chromatic number of the plane must belong to the set {5,6,7}, but it is not known which of these three numbers is the correct value. The De Bruijn–Erdős theorem shows that, for this problem, there exists a finite unit distance graph with the same chromatic number as the whole plane, so if the chromatic number is greater than five then this fact can be proved by a finite calculation.\n\nThe De Bruijn–Erdős theorem may also be used to extend Dilworth's theorem from finite to infinite partially ordered sets. Dilworth's theorem states that the width of a partial order (the maximum number of elements in a set of mutually incomparable elements) equals the minimum number of chains (totally ordered subsets) into which the partial order may be partitioned. A partition into chains may be interpreted as a coloring of the incomparability graph of the partial order. This is a graph with a vertex for each element of the order and an edge for each pair of incomparable elements. Using this coloring interpretation, together with a separate proof of Dilworth's theorem for finite partially ordered sets, it is possible to prove that an infinite partially ordered set has finite width if and only if it has a partition into chains.\n\nIn the same way, the De Bruijn–Erdős theorem extends the four-color theorem from finite planar graphs to infinite planar graphs. Every finite planar graph can be colored with four colors, by the four-color theorem. The De Bruijn–Erdős theorem then shows that\nevery graph that can be drawn without crossings in the plane, finite or infinite, can be colored with four colors. More generally every infinite graph for which all finite subgraphs are planar can again be four-colored.\n\nThe De Bruijn–Erdős theorem may also be used to answer a question of Fred Galvin concerning an intermediate value theorem for graph chromatic numbers: for every two finite numbers , and every graph with chromatic number , there is a subgraph of with chromatic number . To see this, find a finite subgraph of with the same chromatic number as itself, and then delete vertices one by one until the target chromatic number is reached. However, the situation for infinite chromatic numbers is more complicated: it is consistent with set theory that there exists a graph with vertices, and with chromatic number , but with no subgraph of chromatic number . Here, and are the first and second uncountable Aleph numbers.\n\nThe original proof of the De Bruijn–Erdős theorem, by De Bruijn, used transfinite induction.\n\nA different proof using Zorn's lemma was given by Lajos Pósa, and also in the 1951 Ph.D. thesis of Gabriel Andrew Dirac. If is an infinite graph in which every finite subgraph is -colorable, then by Zorn's lemma it is a subgraph of a maximal graph with the same property (one to which no more edges may be added without causing some finite subgraph to require more than  colors). The binary relation of nonadjacency in is an equivalence relation and its equivalence classes provide a -coloring of . However, this proof is more difficult to generalize than the compactness proof.\n\nThe theorem can also be proved using ultrafilters or non-standard analysis. gives a proof for graphs with a countable number of vertices based on Kőnig's infinity lemma.\n\nAll proofs of the De Bruijn–Erdős theorem use some form of the axiom of choice. Some form of this assumption is necessary, as there exist models of mathematics in which both the axiom of choice and the De Bruijn–Erdős theorem are false. More precisely, showed that the theorem is a consequence of the Boolean prime ideal theorem, a property that is implied by the axiom of choice but weaker than the full axiom of choice, and showed that the De Bruijn–Erdős theorem and the Boolean prime ideal theorem are equivalent in axiomatic power.\n\nFor a counterexample to the theorem in models of set theory without choice, let be an infinite graph in which the vertices represent all possible real numbers. In , connect each two real numbers and by an edge whenever the value is a rational number. Equivalently, in this graph, edges exist between all real numbers and all real numbers of the form , where is any rational number. Each path in this graph, starting from any real number ,\nalternates between numbers that differ from by a rational number plus an even multiple of \nand numbers that differ from by a rational number plus an odd multiple of .\nThis alternation prevents from containing any cycles of odd length, so each of its finite subgraphs\nrequires only two colors. However, in the Solovay model in which every set of real numbers is Lebesgue measurable, requires infinitely many colors, since in this case each color class must be a measurable set and it can be shown that every measurable set of real numbers with no edges in must have measure zero. Therefore, in the Solovay model, the (infinite) chromatic number of all of is much larger than the chromatic number of its finite subgraphs (at most two).\n\nThe De Bruijn–Erdős theorem for countable graphs can be shown to be equivalent in axiomatic power, within a theory of second-order arithmetic, to Kőnig's infinity lemma.\n\n proves the following theorem, which may be seen as a generalization of the De Bruijn–Erdős theorem. Let be an infinite set, for instance the set of vertices in an infinite graph. For each element of , let be a finite set of colors. Additionally, for every finite subset of , choose some particular coloring of , in which the color of each element of belongs to . Then there exists a global coloring of all of with the property that every finite set has a finite superset on which and agree. In particular, if we choose a -coloring for every finite subgraph of an infinite graph , then there is a -coloring of in which each finite graph has a larger supergraph whose coloring agrees with the coloring of the whole graph.\n\nIf a graph does not have finite chromatic number, then the De Bruijn–Erdős theorem implies that it must contain finite subgraphs of every possible chromatic number. Researchers have also investigated other conditions on the subgraphs that are forced to occur in this case; for instance, unboundedly chromatic graphs must also contain every possible finite bipartite graph as a subgraph. However, they may have arbitrarily large odd girth, and therefore they may avoid any finite set of non-bipartite subgraphs.\n\nThe De Bruijn–Erdős theorem also applies directly to hypergraph coloring problems, where one requires that each hyperedge have vertices of more than one color. As for graphs, a hypergraph has a -coloring if and only if each of its finite sub-hypergraphs has a -coloring. It is a special case of the compactness theorem of Kurt Gödel stating that a set of first-order sentences has a model if and only if every finite subset of it has a model. More specifically, the De Bruijn–Erdős theorem can be interpreted as the compactness of the first-order structures whose non-logical values are any finite set of colors and whose only predicate on these values is inequality.\n\nThe theorem may also be generalized to situations in which the number of colors is an infinite cardinal number. If is a strongly compact cardinal, then for every graph and cardinal number , has chromatic number at most if and only if each of its subgraphs of cardinality less than has chromatic number at most . The original De Bruijn–Erdős theorem is the case of this generalization, since a set is finite if and only if its cardinality is less than . However, some assumption such as the one of being a strongly compact cardinal is necessary: if the generalized continuum hypothesis is true, then for every infinite cardinal , there exists a graph of cardinality such that the chromatic number of is greater than , but such that every subgraph of whose vertex set has smaller power than has chromatic number at most . characterizes the infinite graphs that obey a generalization of the De Bruijn–Erdős theorem, in that their chromatic number is equal to the maximum chromatic number of their strictly smaller subgraphs.\n\n"}
{"id": "13725281", "url": "https://en.wikipedia.org/wiki?curid=13725281", "title": "Differential inclusion", "text": "Differential inclusion\n\nIn mathematics, differential inclusions are a generalization of the concept of ordinary differential equation of the form\n\nwhere \"F\" is a multivalued map, i.e. \"F\"(\"t\", \"x\") is a \"set\" rather than a single point in formula_2. Differential inclusions arise in many situations including differential variational inequalities, projected dynamical systems, dynamic Coulomb friction problems and fuzzy set arithmetic.\n\nFor example, the basic rule for Coulomb friction is that the friction force has magnitude \"μN\" in the direction opposite to the direction of slip, where \"N\" is the normal force and \"μ\" is a constant (the friction coefficient). However, if the slip is zero, the friction force can be \"any\" force in the correct plane with magnitude smaller than or equal to \"μN\" Thus, writing the friction force as a function of position and velocity leads to a set-valued function.\n\nExistence theory usually assumes that \"F\"(\"t\", \"x\") is an upper hemicontinuous function of \"x\", measurable in \"t\", and that \"F\"(\"t\", \"x\") is a closed, convex set for all \"t\" and \"x\". \nExistence of solutions for the initial value problem\n\nfor a sufficiently small time interval [\"t\", \"t\" + \"ε\"), \"ε\" > 0 then follows.\nGlobal existence can be shown provided \"F\" does not allow \"blow-up\" (formula_4 as formula_5 for a finite formula_6).\n\nExistence theory for differential inclusions with non-convex \"F\"(\"t\", \"x\") is an active area of research.\n\nUniqueness of solutions usually requires other conditions. \nFor example, suppose formula_7 satisfies a one-sided Lipschitz condition:\n\nfor some \"C\" for all \"x\" and \"x\". Then the initial value problem\n\nhas a unique solution.\n\nThis is closely related to the theory of maximal monotone operators, as developed by Minty and Haïm Brezis.\n\nFilippov's theory only allows for discontinuities in the derivative formula_10, but allows no discontinuities in the state, i.e. formula_11 need be continuous. Schatzman and later Moreau (who gave it the currently accepted name) extended the notion to \"measure differential inclusion\" (MDI) in which the inclusion is evaluated by taking the limit from above for formula_11.\n\nDifferential inclusions can be used to understand and suitably interpret discontinuous ordinary differential equations, such as arise for Coulomb friction in mechanical systems and ideal switches in power electronics. An important contribution has been made by A. F. Filippov, who studied regularizations of discontinuous equations. Further the technique of regularization was used by N.N. Krasovskii in the theory of differential games.\n\nDifferential inclusions are also found at the foundation of non-smooth dynamical systems (NSDS) analysis, which is used in the \"analog\" study of switching electrical circuits using idealized component equations (for example using idealized, straight vertical lines for the ) and in the study of certain non-smooth mechanical system such as stick-slip oscillations in systems with dry friction or the dynamics of impact phenomena. Software that solves NSDS systems exists, such as INRIA's Siconos.\n\n\n"}
{"id": "11444450", "url": "https://en.wikipedia.org/wiki?curid=11444450", "title": "Eleanor Robson", "text": "Eleanor Robson\n\nEleanor Robson is a Professor of Ancient Near Eastern History at the Department of History, University College London, chair of the British Institute for the Study of Iraq and a Quondam Fellow of All Souls College, Oxford.\n\nRobson is the author or co-author of several books on Mesopotamian culture and the history of mathematics. In 2003, she won the Lester R. Ford Award of the Mathematical Association of America for her work on Plimpton 322, a clay tablet of Babylonian mathematics; contrary to previous theories according to which this tablet represented a table of Pythagorean triples, Robson showed that it could have been a collection of school exercises in solving quadratic equations. She has also been widely quoted for her criticism of the U.S. Government's failure to prevent looting at the National Museum of Iraq during the Iraq War in 2003.\n\n\n"}
{"id": "5393997", "url": "https://en.wikipedia.org/wiki?curid=5393997", "title": "Elementary amenable group", "text": "Elementary amenable group\n\nIn mathematics, a group is called elementary amenable if it can be built up from finite groups and abelian groups by a sequence of simple operations that result in amenable groups when applied to amenable groups. Since finite groups and abelian groups are amenable, every elementary amenable group is amenable - however, the converse is not true.\n\nFormally, the class of elementary amenable groups is the smallest subclass of the class of all groups that satisfies the following conditions:\n\nThe Tits alternative implies that any amenable linear group is locally virtually solvable; hence, for linear groups, amenability and elementary amenability coincide.\n"}
{"id": "10377", "url": "https://en.wikipedia.org/wiki?curid=10377", "title": "Euclidean algorithm", "text": "Euclidean algorithm\n\nIn mathematics, the Euclidean algorithm, or Euclid's algorithm, is an efficient method for computing the greatest common divisor (GCD) of two numbers, the largest number that divides both of them without leaving a remainder. It is named after the ancient Greek mathematician Euclid, who first described it in his \"Elements\" (c. 300 BC).\nIt is an example of an \"algorithm\", a step-by-step procedure for performing a calculation according to well-defined rules,\nand is one of the oldest algorithms in common use. It can be used to reduce fractions to their simplest form, and is a part of many other number-theoretic and cryptographic calculations.\n\nThe Euclidean algorithm is based on the principle that the greatest common divisor of two numbers does not change if the larger number is replaced by its difference with the smaller number. For example, 21 is the GCD of 252 and 105 (as 252 = 21 × 12 and 105 = 21 × 5), and the same number 21 is also the GCD of 105 and 252 − 105 = 147. Since this replacement reduces the larger of the two numbers, repeating this process gives successively smaller pairs of numbers until the two numbers become equal. When that occurs, they are the GCD of the original two numbers. By reversing the steps, the GCD can be expressed as a sum of the two original numbers each multiplied by a positive or negative integer, e.g., 21 = 5 × 105 + (−2) × 252. The fact that the GCD can always be expressed in this way is known as Bézout's identity.\n\nThe version of the Euclidean algorithm described above (and by Euclid) can take many subtraction steps to find the GCD when one of the given numbers is much bigger than the other. A more efficient version of the algorithm shortcuts these steps, instead replacing the larger of the two numbers by its remainder when divided by the smaller of the two (with this version, the algorithm stops when reaching a zero remainder). With this improvement, the algorithm never requires more steps than five times the number of digits (base 10) of the smaller integer. This was proven by Gabriel Lamé in 1844, and marks the beginning of computational complexity theory. Additional methods for improving the algorithm's efficiency were developed in the 20th century.\n\nThe Euclidean algorithm has many theoretical and practical applications. It is used for reducing fractions to their simplest form and for performing division in modular arithmetic. Computations using this algorithm form part of the cryptographic protocols that are used to secure internet communications, and in methods for breaking these cryptosystems by factoring large composite numbers. The Euclidean algorithm may be used to solve Diophantine equations, such as finding numbers that satisfy multiple congruences according to the Chinese remainder theorem, to construct continued fractions, and to find accurate rational approximations to real numbers. Finally, it can be used as a basic tool for proving theorems in number theory such as Lagrange's four-square theorem and the uniqueness of prime factorizations. The original algorithm was described only for natural numbers and geometric lengths (real numbers), but the algorithm was generalized in the 19th century to other types of numbers, such as Gaussian integers and polynomials of one variable. This led to modern abstract algebraic notions such as Euclidean domains.\n\nThe Euclidean algorithm calculates the greatest common divisor (GCD) of two natural numbers \"a\" and \"b\". The greatest common divisor \"g\" is the largest natural number that divides both \"a\" and \"b\" without leaving a remainder. Synonyms for the GCD include the \"greatest common factor\" (GCF), the \"highest common factor\" (HCF), the \"highest common divisor\" (HCD), and the \"greatest common measure\" (GCM). The greatest common divisor is often written as gcd(\"a\", \"b\") or, more simply, as (\"a\", \"b\"), although the latter notation is ambiguous, also used for concepts such as an ideal in the ring of integers, which is closely related to GCD.\n\nIf gcd(\"a\", \"b\") = 1, then \"a\" and \"b\" are said to be coprime (or relatively prime). This property does not imply that \"a\" or \"b\" are themselves prime numbers. For example, neither 6 nor 35 is a prime number, since they both have two prime factors: 6 = 2 × 3 and 35 = 5 × 7. Nevertheless, 6 and 35 are coprime. No natural number other than 1 divides both 6 and 35, since they have no prime factors in common.\n\nLet \"g\" = gcd(\"a\", \"b\"). Since \"a\" and \"b\" are both multiples of \"g\", they can be written \"a\" = \"mg\" and \"b\" = \"ng\", and there is no larger number \"G\" > \"g\" for which this is true. The natural numbers \"m\" and \"n\" must be coprime, since any common factor could be factored out of \"m\" and \"n\" to make \"g\" greater. Thus, any other number \"c\" that divides both \"a\" and \"b\" must also divide \"g\". The greatest common divisor \"g\" of \"a\" and \"b\" is the unique (positive) common divisor of \"a\" and \"b\" that is divisible by any other common divisor \"c\".\n\nThe GCD can be visualized as follows. Consider a rectangular area \"a\" by \"b\", and any common divisor \"c\" that divides both \"a\" and \"b\" exactly. The sides of the rectangle can be divided into segments of length \"c\", which divides the rectangle into a grid of squares of side length \"c\". The greatest common divisor \"g\" is the largest value of \"c\" for which this is possible. For illustration, a 24-by-60 rectangular area can be divided into a grid of: 1-by-1 squares, 2-by-2 squares, 3-by-3 squares, 4-by-4 squares, 6-by-6 squares or 12-by-12 squares. Therefore, 12 is the greatest common divisor of 24 and 60. A 24-by-60 rectangular area can be divided into a grid of 12-by-12 squares, with two squares along one edge (24/12 = 2) and five squares along the other (60/12 = 5).\n\nThe GCD of two numbers \"a\" and \"b\" is the product of the prime factors shared by the two numbers, where a same prime factor can be used multiple times, but only as long as the product of these factors divides both \"a\" and \"b\". For example, since 1386 can be factored into 2 × 3 × 3 × 7 × 11, and 3213 can be factored into 3 × 3 × 3 × 7 × 17, the greatest common divisor of 1386 and 3213 equals 63 = 3 × 3 × 7, the product of their shared prime factors. If two numbers have no prime factors in common, their greatest common divisor is 1 (obtained here as an instance of the empty product), in other words they are coprime. A key advantage of the Euclidean algorithm is that it can find the GCD efficiently without having to compute the prime factors. Factorization of large integers is believed to be a computationally very difficult problem, and the security of many widely used cryptographic protocols is based upon its infeasibility.\n\nAnother definition of the GCD is helpful in advanced mathematics, particularly ring theory. The greatest common divisor \"g\"  of two nonzero numbers \"a\" and \"b\" is also their smallest positive integral linear combination, that is, the smallest positive number of the form \"ua\" + \"vb\" where \"u\" and \"v\" are integers. The set of all integral linear combinations of \"a\" and \"b\" is actually the same as the set of all multiples of \"g\" (\"mg\", where \"m\" is an integer). In modern mathematical language, the ideal generated by \"a\" and \"b\" is the ideal generated by \"g\" alone (an ideal generated by a single element is called a principal ideal, and all ideals of the integers are principal ideals). Some properties of the GCD are in fact easier to see with this description, for instance the fact that any common divisor of \"a\" and \"b\" also divides the GCD (it divides both terms of \"ua\" + \"vb\"). The equivalence of this GCD definition with the other definitions is described below.\n\nThe GCD of three or more numbers equals the product of the prime factors common to all the numbers, but it can also be calculated by repeatedly taking the GCDs of pairs of numbers. For example,\n\nThus, Euclid's algorithm, which computes the GCD of two integers, suffices to calculate the GCD of arbitrarily many integers.\n\nThe Euclidean algorithm proceeds in a series of steps such that the output of each step is used as an input for the next one. Let \"k\" be an integer that counts the steps of the algorithm, starting with zero. Thus, the initial step corresponds to \"k\" = 0, the next step corresponds to \"k\" = 1, and so on.\n\nEach step begins with two nonnegative remainders \"r\" and \"r\". Since the algorithm ensures that the remainders decrease steadily with every step, \"r\" is less than its predecessor \"r\". The goal of the \"k\"th step is to find a quotient \"q\" and remainder \"r\" that satisfy the equation\n\nand that have \"r\" < \"r\". In other words, multiples of the smaller number \"r\" are subtracted from the larger number \"r\" until the remainder \"r\" is smaller than \"r\".\n\nIn the initial step (\"k\" = 0), the remainders \"r\" and \"r\" equal \"a\" and \"b\", the numbers for which the GCD is sought. In the next step (\"k\" = 1), the remainders equal \"b\" and the remainder \"r\" of the initial step, and so on. Thus, the algorithm can be written as a sequence of equations\n\nIf \"a\" is smaller than \"b\", the first step of the algorithm swaps the numbers. For example, if \"a\" < \"b\", the initial quotient \"q\" equals zero, and the remainder \"r\" is \"a\". Thus, \"r\" is smaller than its predecessor \"r\" for all \"k\" ≥ 0.\n\nSince the remainders decrease with every step but can never be negative, a remainder \"r\" must eventually equal zero, at which point the algorithm stops. The final nonzero remainder \"r\" is the greatest common divisor of \"a\" and \"b\". The number \"N\" cannot be infinite because there are only a finite number of nonnegative integers between the initial remainder \"r\" and zero.\n\nThe validity of the Euclidean algorithm can be proven by a two-step argument. In the first step, the final nonzero remainder \"r\" is shown to divide both \"a\" and \"b\". Since it is a common divisor, it must be less than or equal to the greatest common divisor \"g\". In the second step, it is shown that any common divisor of \"a\" and \"b\", including \"g\", must divide \"r\"; therefore, \"g\" must be less than or equal to \"r\". These two conclusions are inconsistent unless \"r\" = \"g\".\n\nTo demonstrate that \"r\" divides both \"a\" and \"b\" (the first step), \"r\" divides its predecessor \"r\"\n\nsince the final remainder \"r\" is zero. \"r\" also divides its next predecessor \"r\"\n\nbecause it divides both terms on the right-hand side of the equation. Iterating the same argument, \"r\" divides all the preceding remainders, including \"a\" and \"b\". None of the preceding remainders \"r\", \"r\", etc. divide \"a\" and \"b\", since they leave a remainder. Since \"r\" is a common divisor of \"a\" and \"b\", \"r\" ≤ \"g\".\n\nIn the second step, any natural number \"c\" that divides both \"a\" and \"b\" (in other words, any common divisor of \"a\" and \"b\") divides the remainders \"r\". By definition, \"a\" and \"b\" can be written as multiples of \"c\" : \"a\" = \"mc\" and \"b\" = \"nc\", where \"m\" and \"n\" are natural numbers. Therefore, \"c\" divides the initial remainder \"r\", since \"r\" = \"a\" − \"q\"\"b\" = \"mc\" − \"q\"\"nc\" = (\"m\" − \"q\"\"n\")\"c\". An analogous argument shows that \"c\" also divides the subsequent remainders \"r\", \"r\", etc. Therefore, the greatest common divisor \"g\" must divide \"r\", which implies that \"g\" ≤ \"r\". Since the first part of the argument showed the reverse (\"r\" ≤ \"g\"), it follows that \"g\" = \"r\". Thus, \"g\" is the greatest common divisor of all the succeeding pairs:\n\nFor illustration, the Euclidean algorithm can be used to find the greatest common divisor of \"a\" = 1071 and \"b\" = 462. To begin, multiples of 462 are subtracted from 1071 until the remainder is less than 462. Two such multiples can be subtracted (\"q\" = 2), leaving a remainder of 147:\n\nThen multiples of 147 are subtracted from 462 until the remainder is less than 147. Three multiples can be subtracted (\"q\" = 3), leaving a remainder of 21:\n\nThen multiples of 21 are subtracted from 147 until the remainder is less than 21. Seven multiples can be subtracted (\"q\" = 7), leaving no remainder:\n\nSince the last remainder is zero, the algorithm ends with 21 as the greatest common divisor of 1071 and 462. This agrees with the gcd(1071, 462) found by prime factorization above. In tabular form, the steps are\n\nThe Euclidean algorithm can be visualized in terms of the tiling analogy given above for the greatest common divisor. Assume that we wish to cover an \"a\"-by-\"b\" rectangle with square tiles exactly, where \"a\" is the larger of the two numbers. We first attempt to tile the rectangle using \"b\"-by-\"b\" square tiles; however, this leaves an \"r\"-by-\"b\" residual rectangle untiled, where \"r\" < \"b\". We then attempt to tile the residual rectangle with \"r\"-by-\"r\" square tiles. This leaves a second residual rectangle \"r\"-by-\"r\", which we attempt to tile using \"r\"-by-\"r\" square tiles, and so on. The sequence ends when there is no residual rectangle, i.e., when the square tiles cover the previous residual rectangle exactly. The length of the sides of the smallest square tile is the GCD of the dimensions of the original rectangle. For example, the smallest square tile in the adjacent figure is 21-by-21 (shown in red), and 21 is the GCD of 1071 and 462, the dimensions of the original rectangle (shown in green).\n\nAt every step \"k\", the Euclidean algorithm computes a quotient \"q\" and remainder \"r\" from two numbers \"r\" and \"r\"\n\nwhere the magnitude of \"r\" is strictly less than that of \"r\". The theorem which underlies the definition of the Euclidean division ensures that such a quotient and remainder always exist and are unique.\n\nIn Euclid's original version of the algorithm, the quotient and remainder are found by repeated subtraction; that is, \"r\" is subtracted from \"r\" repeatedly until the remainder \"r\" is smaller than \"r\". After that \"r\" and \"r\" are exchanged and the process is iterated. Euclidean division reduces all the steps between two exchanges into a single step, which is thus more efficient. Moreover, the quotients are not needed, thus one may replace Euclidean division by the modulo operation, which gives only the remainder. Thus the iteration of the Euclidean algorithm becomes simply\n\nImplementations of the algorithm may be expressed in pseudocode. For example, the division-based version may be programmed as\n\nAt the beginning of the \"k\"th iteration, the variable \"b\" holds the latest remainder \"r\", whereas the variable \"a\" holds its predecessor, \"r\". The step \"b\" := \"a\" mod \"b\" is equivalent to the above recursion formula \"r\" ≡ \"r\" mod \"r\". The temporary variable \"t\" holds the value of \"r\" while the next remainder \"r\" is being calculated. At the end of the loop iteration, the variable \"b\" holds the remainder \"r\", whereas the variable \"a\" holds its predecessor, \"r\".\n\nIn the subtraction-based version which was Euclid's original version, the remainder calculation (\"b\" = \"a\" mod \"b\") is replaced by repeated subtraction. Contrary to the division-based version, which works with arbitrary integers as input, the subtraction-based version supposes that the input consists of positive integers and stops when \"a\" = \"b\":\n\nThe variables \"a\" and \"b\" alternate holding the previous remainders \"r\" and \"r\". Assume that \"a\" is larger than \"b\" at the beginning of an iteration; then \"a\" equals \"r\", since \"r\" > \"r\". During the loop iteration, \"a\" is reduced by multiples of the previous remainder \"b\" until \"a\" is smaller than \"b\". Then \"a\" is the next remainder \"r\". Then \"b\" is reduced by multiples of \"a\" until it is again smaller than \"a\", giving the next remainder \"r\", and so on.\n\nThe recursive version is based on the equality of the GCDs of successive remainders and the stopping condition gcd(\"r\", 0) = \"r\".\n\nFor illustration, the gcd(1071, 462) is calculated from the equivalent gcd(462, 1071 mod 462) = gcd(462, 147). The latter GCD is calculated from the gcd(147, 462 mod 147) = gcd(147, 21), which in turn is calculated from the gcd(21, 147 mod 21) = gcd(21, 0) = 21.\n\nIn another version of Euclid's algorithm, the quotient at each step is increased by one if the resulting negative remainder is smaller in magnitude than the typical positive remainder. Previously, the equation\n\nassumed that . However, an alternative negative remainder can be computed:\nif or\nif .\n\nIf is replaced by when , then one gets a variant of Euclidean algorithm such that\nat each step.\n\nLeopold Kronecker has shown that this version requires the least number of steps of any version of Euclid's algorithm. More generally, it has been proven that, for every input numbers \"a\" and \"b\", the number of steps is minimal if and only if is chosen in order that formula_3 where formula_4 is the golden ratio.\n\nThe Euclidean algorithm is one of the oldest algorithms in common use. It appears in Euclid's \"Elements\" (c. 300 BC), specifically in Book 7 (Propositions 1–2) and Book 10 (Propositions 2–3). In Book 7, the algorithm is formulated for integers, whereas in Book 10, it is formulated for lengths of line segments. (In modern usage, one would say it was formulated there for real numbers. But lengths, areas, and volumes, represented as real numbers in modern usage, are not measured in the same units and there is no natural unit of length, area, or volume; the concept of real numbers was unknown at that time.) The latter algorithm is geometrical. The GCD of two lengths \"a\" and \"b\" corresponds to the greatest length \"g\" that measures \"a\" and \"b\" evenly; in other words, the lengths \"a\" and \"b\" are both integer multiples of the length \"g\".\n\nThe algorithm was probably not discovered by Euclid, who compiled results from earlier mathematicians in his \"Elements\". The mathematician and historian B. L. van der Waerden suggests that Book VII derives from a textbook on number theory written by mathematicians in the school of Pythagoras. The algorithm was probably known by Eudoxus of Cnidus (about 375 BC). The algorithm may even pre-date Eudoxus, judging from the use of the technical term ἀνθυφαίρεσις (\"anthyphairesis\", reciprocal subtraction) in works by Euclid and Aristotle.\n\nCenturies later, Euclid's algorithm was discovered independently both in India and in China, primarily to solve Diophantine equations that arose in astronomy and making accurate calendars. In the late 5th century, the Indian mathematician and astronomer Aryabhata described the algorithm as the \"pulverizer\", perhaps because of its effectiveness in solving Diophantine equations. Although a special case of the Chinese remainder theorem had already been described in the Chinese book \"Sunzi Suanjing\", the general solution was published by Qin Jiushao in his 1247 book \"Shushu Jiuzhang\" (數書九章 \"Mathematical Treatise in Nine Sections\"). The Euclidean algorithm was first described in Europe in the second edition of Bachet's \"Problèmes plaisants et délectables\" (\"Pleasant and enjoyable problems\", 1624). In Europe, it was likewise used to solve Diophantine equations and in developing continued fractions. The extended Euclidean algorithm was published by the English mathematician Nicholas Saunderson, who attributed it to Roger Cotes as a method for computing continued fractions efficiently.\n\nIn the 19th century, the Euclidean algorithm led to the development of new number systems, such as Gaussian integers and Eisenstein integers. In 1815, Carl Gauss used the Euclidean algorithm to demonstrate unique factorization of Gaussian integers, although his work was first published in 1832. Gauss mentioned the algorithm in his \"Disquisitiones Arithmeticae\" (published 1801), but only as a method for continued fractions. Peter Gustav Lejeune Dirichlet seems to have been the first to describe the Euclidean algorithm as the basis for much of number theory. Lejeune Dirichlet noted that many results of number theory, such as unique factorization, would hold true for any other system of numbers to which the Euclidean algorithm could be applied. Lejeune Dirichlet's lectures on number theory were edited and extended by Richard Dedekind, who used Euclid's algorithm to study algebraic integers, a new general type of number. For example, Dedekind was the first to prove Fermat's two-square theorem using the unique factorization of Gaussian integers. Dedekind also defined the concept of a Euclidean domain, a number system in which a generalized version of the Euclidean algorithm can be defined (as described below). In the closing decades of the 19th century, the Euclidean algorithm gradually became eclipsed by Dedekind's more general theory of ideals.\n\nOther applications of Euclid's algorithm were developed in the 19th century. In 1829, Charles Sturm showed that the algorithm was useful in the Sturm chain method for counting the real roots of polynomials in any given interval.\n\nThe Euclidean algorithm was the first integer relation algorithm, which is a method for finding integer relations between commensurate real numbers. Several novel integer relation algorithms have been developed, such as the algorithm of Helaman Ferguson and R.W. Forcade (1979) and the LLL algorithm.\n\nIn 1969, Cole and Davie developed a two-player game based on the Euclidean algorithm, called \"The Game of Euclid\", which has an optimal strategy. The players begin with two piles of \"a\" and \"b\" stones. The players take turns removing \"m\" multiples of the smaller pile from the larger. Thus, if the two piles consist of \"x\" and \"y\" stones, where \"x\" is larger than \"y\", the next player can reduce the larger pile from \"x\" stones to \"x\" − \"my\" stones, as long as the latter is a nonnegative integer. The winner is the first player to reduce one pile to zero stones.\n\nBézout's identity states that the greatest common divisor \"g\" of two integers \"a\" and \"b\" can be represented as a linear sum of the original two numbers \"a\" and \"b\". In other words, it is always possible to find integers \"s\" and \"t\" such that \"g\" = \"sa\" + \"tb\".\n\nThe integers \"s\" and \"t\" can be calculated from the quotients \"q\", \"q\", etc. by reversing the order of equations in Euclid's algorithm. Beginning with the next-to-last equation, \"g\" can be expressed in terms of the quotient \"q\" and the two preceding remainders, \"r\" and \"r\":\n\nThose two remainders can be likewise expressed in terms of their quotients and preceding remainders,\n\nSubstituting these formulae for \"r\" and \"r\" into the first equation yields \"g\" as a linear sum of the remainders \"r\" and \"r\". The process of substituting remainders by formulae involving their predecessors can be continued until the original numbers \"a\" and \"b\" are reached:\n\nAfter all the remainders \"r\", \"r\", etc. have been substituted, the final equation expresses \"g\" as a linear sum of \"a\" and \"b\": \"g\" = \"sa\" + \"tb\". Bézout's identity, and therefore the previous algorithm, can both be generalized to the context of Euclidean domains.\n\nBézout's identity provides yet another definition of the greatest common divisor \"g\" of two numbers \"a\" and \"b\". Consider the set of all numbers \"ua\" + \"vb\", where \"u\" and \"v\" are any two integers. Since \"a\" and \"b\" are both divisible by \"g\", every number in the set is divisible by \"g\". In other words, every number of the set is an integer multiple of \"g\". This is true for every common divisor of \"a\" and \"b\". However, unlike other common divisors, the greatest common divisor is a member of the set; by Bézout's identity, choosing \"u\" = \"s\" and \"v\" = \"t\" gives \"g\". A smaller common divisor cannot be a member of the set, since every member of the set must be divisible by \"g\". Conversely, any multiple \"m\" of \"g\" can be obtained by choosing \"u\" = \"ms\" and \"v\" = \"mt\", where \"s\" and \"t\" are the integers of Bézout's identity. This may be seen by multiplying Bézout's identity by \"m\",\n\nTherefore, the set of all numbers \"ua\" + \"vb\" is equivalent to the set of multiples \"m\" of \"g\". In other words, the set of all possible sums of integer multiples of two numbers (\"a\" and \"b\") is equivalent to the set of multiples of gcd(\"a\", \"b\"). The GCD is said to be the generator of the ideal of \"a\" and \"b\". This GCD definition led to the modern abstract algebraic concepts of a principal ideal (an ideal generated by a single element) and a principal ideal domain (a domain in which every ideal is a principal ideal).\n\nCertain problems can be solved using this result. For example, consider two measuring cups of volume \"a\" and \"b\". By adding/subtracting \"u\" multiples of the first cup and \"v\" multiples of the second cup, any volume \"ua\" + \"vb\" can be measured out. These volumes are all multiples of \"g\" = gcd(\"a\", \"b\").\n\nThe integers \"s\" and \"t\" of Bézout's identity can be computed efficiently using the extended Euclidean algorithm. This extension adds two recursive equations to Euclid's algorithm\n\nwith the starting values\n\nUsing this recursion, Bézout's integers \"s\" and \"t\" are given by \"s\" = \"s\" and \"t\" = \"t\", where \"N+1\" is the step on which the algorithm terminates with \"r\" = 0.\n\nThe validity of this approach can be shown by induction. Assume that the recursion formula is correct up to step \"k\" − 1 of the algorithm; in other words, assume that\n\nfor all \"j\" less than \"k\". The \"k\"th step of the algorithm gives the equation\n\nSince the recursion formula has been assumed to be correct for \"r\" and \"r\", they may be expressed in terms of the corresponding \"s\" and \"t\" variables\n\nRearranging this equation yields the recursion formula for step \"k\", as required\n\nThe integers \"s\" and \"t\" can also be found using an equivalent matrix method. The sequence of equations of Euclid's algorithm\n\ncan be written as a product of 2-by-2 quotient matrices multiplying a two-dimensional remainder vector\n\nLet M represent the product of all the quotient matrices\n\nThis simplifies the Euclidean algorithm to the form\n\nTo express \"g\" as a linear sum of \"a\" and \"b\", both sides of this equation can be multiplied by the inverse of the matrix M. The determinant of M equals (−1), since it equals the product of the determinants of the quotient matrices, each of which is negative one. Since the determinant of M is never zero, the vector of the final remainders can be solved using the inverse of M\n\nSince the top equation gives\n\nthe two integers of Bézout's identity are \"s\" = (−1)\"m\" and \"t\" = (−1)\"m\". The matrix method is as efficient as the equivalent recursion, with two multiplications and two additions per step of the Euclidean algorithm.\n\nBézout's identity is essential to many applications of Euclid's algorithm, such as demonstrating the unique factorization of numbers into prime factors. To illustrate this, suppose that a number \"L\" can be written as a product of two factors \"u\" and \"v\", that is, \"L\" = \"uv\". If another number \"w\" also divides \"L\" but is coprime with \"u\", then \"w\" must divide \"v\", by the following argument: If the greatest common divisor of \"u\" and \"w\" is 1, then integers \"s\" and \"t\" can be found such that\n\nby Bézout's identity. Multiplying both sides by \"v\" gives the relation\n\nSince \"w\" divides both terms on the right-hand side, it must also divide the left-hand side, \"v\". This result is known as Euclid's lemma. Specifically, if a prime number divides \"L\", then it must divide at least one factor of \"L\". Conversely, if a number \"w\" is coprime to each of a series of numbers \"a\", \"a\", …, \"a\", then \"w\" is also coprime to their product, \"a\" × \"a\" × … × \"a\".\n\nEuclid's lemma suffices to prove that every number has a unique factorization into prime numbers. To see this, assume the contrary, that there are two independent factorizations of \"L\" into \"m\" and \"n\" prime factors, respectively\n\nSince each prime \"p\" divides \"L\" by assumption, it must also divide one of the \"q\" factors; since each \"q\" is prime as well, it must be that \"p\" = \"q\". Iteratively dividing by the \"p\" factors shows that each \"p\" has an equal counterpart \"q\"; the two prime factorizations are identical except for their order. The unique factorization of numbers into primes has many applications in mathematical proofs, as shown below.\n\nDiophantine equations are equations in which the solutions are restricted to integers; they are named after the 3rd-century Alexandrian mathematician Diophantus. A typical \"linear\" Diophantine equation seeks integers \"x\" and \"y\" such that\n\nwhere \"a\", \"b\" and \"c\" are given integers. This can be written as an equation for \"x\" in modular arithmetic:\n\nLet \"g\" be the greatest common divisor of \"a\" and \"b\". Both terms in \"ax\" + \"by\" are divisible by \"g\"; therefore, \"c\" must also be divisible by \"g\", or the equation has no solutions. By dividing both sides by \"c\"/\"g\", the equation can be reduced to Bezout's identity\n\nwhere \"s\" and \"t\" can be found by the extended Euclidean algorithm. This provides one solution to the Diophantine equation, \"x\" = \"s\" (\"c\"/\"g\") and \"y\" = \"t\" (\"c\"/\"g\").\n\nIn general, a linear Diophantine equation has no solutions, or an infinite number of solutions. To find the latter, consider two solutions, (\"x\", \"y\") and (\"x\", \"y\"), where\n\nor equivalently\n\nTherefore, the smallest difference between two \"x\" solutions is \"b\"/\"g\", whereas the smallest difference between two \"y\" solutions is \"a\"/\"g\". Thus, the solutions may be expressed as\n\nBy allowing \"u\" to vary over all possible integers, an infinite family of solutions can be generated from a single solution (\"x\", \"y\"). If the solutions are required to be \"positive\" integers (\"x\" > 0, \"y\" > 0), only a finite number of solutions may be possible. This restriction on the acceptable solutions allows some systems of Diophantine equations with more unknowns than equations to have a finite number of solutions; this is impossible for a system of linear equations when the solutions can be any real number (see Underdetermined system).\n\nA finite field is a set of numbers with four generalized operations. The operations are called addition, subtraction, multiplication and division and have their usual properties, such as commutativity, associativity and distributivity. An example of a finite field is the set of 13 numbers {0, 1, 2, …, 12} using modular arithmetic. In this field, the results of any mathematical operation (addition, subtraction, multiplication, or division) is reduced modulo 13; that is, multiples of 13 are added or subtracted until the result is brought within the range 0–12. For example, the result of 5 × 7 = 35 mod 13 = 9. Such finite fields can be defined for any prime \"p\"; using more sophisticated definitions, they can also be defined for any power \"m\" of a prime \"p\". Finite fields are often called Galois fields, and are abbreviated as GF(\"p\") or GF(\"p\").\n\nIn such a field with \"m\" numbers, every nonzero element \"a\" has a unique modular multiplicative inverse, \"a\" such that This inverse can be found by solving the congruence equation \"ax\" ≡ 1 mod \"m\", or the equivalent linear Diophantine equation\n\nThis equation can be solved by the Euclidean algorithm, as described above. Finding multiplicative inverses is an essential step in the RSA algorithm, which is widely used in electronic commerce; specifically, the equation determines the integer used to decrypt the message. Note that although the RSA algorithm uses rings rather than fields, the Euclidean algorithm can still be used to find a multiplicative inverse where one exists. The Euclidean algorithm also has other applications in error-correcting codes; for example, it can be used as an alternative to the Berlekamp–Massey algorithm for decoding BCH and Reed–Solomon codes, which are based on Galois fields.\n\nEuclid's algorithm can also be used to solve multiple linear Diophantine equations. Such equations arise in the Chinese remainder theorem, which describes a novel method to represent an integer \"x\". Instead of representing an integer by its digits, it may be represented by its remainders \"x\" modulo a set of \"N\" coprime numbers \"m\":\n\nThe goal is to determine \"x\" from its \"N\" remainders \"x\". The solution is to combine the multiple equations into a single linear Diophantine equation with a much larger modulus \"M\" that is the product of all the individual moduli \"m\", and define \"M\" as\n\nThus, each \"M\" is the product of all the moduli \"except\" \"m\". The solution depends on finding \"N\" new numbers \"h\" such that\n\nWith these numbers \"h\", any integer \"x\" can be reconstructed from its remainders \"x\" by the equation\n\nSince these numbers \"h\" are the multiplicative inverses of the \"M\", they may be found using Euclid's algorithm as described in the previous subsection.\n\nThe Euclidean algorithm can be used to arrange the set of all positive rational numbers into an infinite binary search tree, called the Stern–Brocot tree.\nThe number 1 (expressed as a fraction 1/1) is placed at the root of the tree, and the location of any other number \"a\"/\"b\" can be found by computing gcd(\"a\",\"b\") using the original form of the Euclidean algorithm, in which each step replaces the larger of the two given numbers by its difference with the smaller number (not its remainder), stopping when two equal numbers are reached. A step of the Euclidean algorithm that replaces the first of the two numbers corresponds to a step in the tree from a node to its right child, and a step that replaces the second of the two numbers corresponds to a step in the tree from a node to its left child. The sequence of steps constructed in this way does not depend on whether \"a\"/\"b\" is given in lowest terms, and forms a path from the root to a node containing the number \"a\"/\"b\". This fact can be used to prove that each positive rational number appears exactly once in this tree.\n\nFor example, 3/4 can be found by starting at the root, going to the left once, then to the right twice:\n\nThe Euclidean algorithm has almost the same relationship to another binary tree on the rational numbers called the Calkin–Wilf tree. The difference is that the path is reversed: instead of producing a path from the root of the tree to a target, it produces a path from the target to the root.\n\nThe Euclidean algorithm has a close relationship with continued fractions. The sequence of equations can be written in the form\n\nThe last term on the right-hand side always equals the inverse of the left-hand side of the next equation. Thus, the first two equations may be combined to form\n\nThe third equation may be used to substitute the denominator term \"r\"/\"r\", yielding\n\nThe final ratio of remainders \"r\"/\"r\" can always be replaced using the next equation in the series, up to the final equation. The result is a continued fraction\n\nIn the worked example above, the gcd(1071, 462) was calculated, and the quotients \"q\" were 2, 3 and 7, respectively. Therefore, the fraction 1071/462 may be written\n\nas can be confirmed by calculation.\n\nCalculating a greatest common divisor is an essential step in several integer factorization algorithms, such as Pollard's rho algorithm, Shor's algorithm, Dixon's factorization method and the Lenstra elliptic curve factorization. The Euclidean algorithm may be used to find this GCD efficiently. Continued fraction factorization uses continued fractions, which are determined using Euclid's algorithm.\n\nThe computational efficiency of Euclid's algorithm has been studied thoroughly. This efficiency can be described by the number of division steps the algorithm requires, multiplied by the computational expense of each step. The first known analysis of Euclid's algorithm is due to A. A. L. Reynaud in 1811, who showed that the number of division steps on input (\"u\", \"v\") is bounded by \"v\"; later he improved this to \"v\"/2  + 2. Later, in 1841, P. J. E. Finck showed that the number of division steps is at most 2 log \"v\" + 1, and hence Euclid's algorithm runs in time polynomial in the size of the input. Émile Léger, in 1837, studied the worst case, which is when the inputs are consecutive Fibonacci numbers. Finck's analysis was refined by Gabriel Lamé in 1844, who showed that the number of steps required for completion is never more than five times the number \"h\" of base-10 digits of the smaller number \"b\".\n\nIn the uniform cost model (suitable for analyzing the complexity of gcd calculation on numbers that fit into a single machine word), each step of the algorithm takes constant time, and Lamé's analysis implies that the total running time is also \"O\"(\"h\"). However, in a model of computation suitable for computation with larger numbers, the computational expense of a single remainder computation in the algorithm can be as large as \"O\"(\"h\"). In this case the total time for all of the steps of the algorithm can be analyzed using a telescoping series, showing that it is also \"O\"(\"h\"). Modern algorithmic techniques based on the Schönhage–Strassen algorithm for fast integer multiplication can be used to speed this up, leading to quasilinear algorithms for the GCD.\n\nThe number of steps to calculate the GCD of two natural numbers, \"a\" and \"b\", may be denoted by \"T\"(\"a\", \"b\"). If \"g\" is the GCD of \"a\" and \"b\", then \"a\" = \"mg\" and \"b\" = \"ng\" for two coprime numbers \"m\" and \"n\". Then\n\nas may be seen by dividing all the steps in the Euclidean algorithm by \"g\". By the same argument, the number of steps remains the same if \"a\" and \"b\" are multiplied by a common factor \"w\": \"T\"(\"a\", \"b\") = \"T\"(\"wa\", \"wb\"). Therefore, the number of steps \"T\" may vary dramatically between neighboring pairs of numbers, such as T(\"a\", \"b\") and T(\"a\", \"b\" + 1), depending on the size of the two GCDs.\n\nThe recursive nature of the Euclidean algorithm gives another equation\n\nwhere \"T\"(\"x\", 0) = 0 by assumption.\n\nIf the Euclidean algorithm requires \"N\" steps for a pair of natural numbers \"a\" > \"b\" > 0, the smallest values of \"a\" and \"b\" for which this is true are the Fibonacci numbers \"F\" and \"F\", respectively. More precisely, if the Euclidean algorithm requires \"N\" steps for the pair \"a\" > \"b\", then one has \"a\" ≥ \"F\" and \"b\" ≥ \"F\". This can be shown by induction. If \"N\" = 1, \"b\" divides \"a\" with no remainder; the smallest natural numbers for which this is true is \"b\" = 1 and \"a\" = 2, which are \"F\" and \"F\", respectively. Now assume that the result holds for all values of \"N\" up to \"M\" − 1. The first step of the \"M\"-step algorithm is \"a\" = \"q\"\"b\" + \"r\", and the Euclidean algorithm requires \"M\" − 1 steps for the pair \"b\" > \"r\". By induction hypothesis, one has \"b\" ≥ \"F\" and \"r\" ≥ \"F\". Therefore, \"a\" = \"q\"\"b\" + \"r\" ≥ \"b\" + \"r\" ≥ \"F\" + \"F\" = \"F\",\nwhich is the desired inequality.\nThis proof, published by Gabriel Lamé in 1844, represents the beginning of computational complexity theory, and also the first practical application of the Fibonacci numbers.\n\nThis result suffices to show that the number of steps in Euclid's algorithm can never be more than five times the number of its digits (base 10). For if the algorithm requires \"N\" steps, then \"b\" is greater than or equal to \"F\" which in turn is greater than or equal to \"φ\", where \"φ\" is the golden ratio. Since \"b\" ≥ \"φ\", then \"N\" − 1 ≤ log\"b\". Since log\"φ\" > 1/5, (\"N\" − 1)/5 < log\"φ\" log\"b\" = log\"b\". Thus, \"N\" ≤ 5 log\"b\". Thus, the Euclidean algorithm always needs less than \"O\"(\"h\") divisions, where \"h\" is the number of digits in the smaller number \"b\".\n\nThe average number of steps taken by the Euclidean algorithm has been defined in three different ways. The first definition is the average time \"T\"(\"a\") required to calculate the GCD of a given number \"a\" and a smaller natural number \"b\" chosen with equal probability from the integers 0 to \"a\" − 1\n\nwith the residual error being of order \"a\", where \"ε\" is infinitesimal. The constant \"C\" (\"Porter's Constant\") in this formula equals\n\nwhere \"γ\" is the Euler–Mascheroni constant and ζ' is the derivative of the Riemann zeta function. The leading coefficient (12/π) ln 2 was determined by two independent methods.\n\nSince the first average can be calculated from the tau average by summing over the divisors \"d\" of \"a\"\n\nit can be approximated by the formula\n\nwhere Λ(\"d\") is the Mangoldt function.\n\nA third average \"Y\"(\"n\") is defined as the mean number of steps required when both \"a\" and \"b\" are chosen randomly (with uniform distribution) from 1 to \"n\"\n\nSubstituting the approximate formula for \"T\"(\"a\") into this equation yields an estimate for \"Y\"(\"n\")\n\nIn each step \"k\" of the Euclidean algorithm, the quotient \"q\" and remainder \"r\" are computed for a given pair of integers \"r\" and \"r\"\n\nThe computational expense per step is associated chiefly with finding \"q\", since the remainder \"r\" can be calculated quickly from \"r\", \"r\", and \"q\"\n\nThe computational expense of dividing \"h\"-bit numbers scales as \"O\"(\"h\"(\"ℓ\"+1)), where \"ℓ\" is the length of the quotient.\n\nFor comparison, Euclid's original subtraction-based algorithm can be much slower. A single integer division is equivalent to the quotient \"q\" number of subtractions. If the ratio of \"a\" and \"b\" is very large, the quotient is large and many subtractions will be required. On the other hand, it has been shown that the quotients are very likely to be small integers. The probability of a given quotient \"q\" is approximately ln|\"u\"/(\"u\" − 1)| where \"u\" = (\"q\" + 1). For illustration, the probability of a quotient of 1, 2, 3, or 4 is roughly 41.5%, 17.0%, 9.3%, and 5.9%, respectively. Since the operation of subtraction is faster than division, particularly for large numbers, the subtraction-based Euclid's algorithm is competitive with the division-based version. This is exploited in the binary version of Euclid's algorithm.\n\nCombining the estimated number of steps with the estimated computational expense per step shows that the Euclid's algorithm grows quadratically (\"h\") with the average number of digits \"h\" in the initial two numbers \"a\" and \"b\". Let \"h\", \"h\", …, \"h\" represent the number of digits in the successive remainders \"r\", \"r\", …, \"r\". Since the number of steps \"N\" grows linearly with \"h\", the running time is bounded by\n\nEuclid's algorithm is widely used in practice, especially for small numbers, due to its simplicity. For comparison, the efficiency of alternatives to Euclid's algorithm may be determined.\n\nOne inefficient approach to finding the GCD of two natural numbers \"a\" and \"b\" is to calculate all their common divisors; the GCD is then the largest common divisor. The common divisors can be found by dividing both numbers by successive integers from 2 to the smaller number \"b\". The number of steps of this approach grows linearly with \"b\", or exponentially in the number of digits. Another inefficient approach is to find the prime factors of one or both numbers. As noted above, the GCD equals the product of the prime factors shared by the two numbers \"a\" and \"b\". Present methods for prime factorization are also inefficient; many modern cryptography systems even rely on that inefficiency.\n\nThe binary GCD algorithm is an efficient alternative that substitutes division with faster operations by exploiting the binary representation used by computers. However, this alternative also scales like \"O\"(\"h\"²). It is generally faster than the Euclidean algorithm on real computers, even though it scales in the same way. Additional efficiency can be gleaned by examining only the leading digits of the two numbers \"a\" and \"b\". The binary algorithm can be extended to other bases (\"k\"-ary algorithms), with up to fivefold increases in speed. Lehmer's GCD algorithm uses the same general principle as the binary algorithm to speed up GCD computations in arbitrary bases.\n\nA recursive approach for very large integers (with more than 25,000 digits) leads to quasilinear integer GCD algorithms, such as those of Schönhage, and Stehlé and Zimmermann. These algorithms exploit the 2×2 matrix form of the Euclidean algorithm given above. These quasilinear methods generally scale as \n\nAlthough the Euclidean algorithm is used to find the greatest common divisor of two natural numbers (positive integers), it may be generalized to the real numbers, and to other mathematical objects, such as polynomials, quadratic integers and Hurwitz quaternions. In the latter cases, the Euclidean algorithm is used to demonstrate the crucial property of unique factorization, i.e., that such numbers can be factored uniquely into irreducible elements, the counterparts of prime numbers. Unique factorization is essential to many proofs of number theory.\n\nEuclid's algorithm can be applied to real numbers, as described by Euclid in Book 10 of his \"Elements\". The goal of the algorithm is to identify a real number such that two given real numbers, and , are integer multiples of it: and , where and are integers. This identification is equivalent to finding an integer relation among the real numbers and ; that is, it determines integers and such that . Euclid uses this algorithm to treat the question of incommensurable lengths.\n\nThe real-number Euclidean algorithm differs from its integer counterpart in two respects. First, the remainders are real numbers, although the quotients are integers as before. Second, the algorithm is not guaranteed to end in a finite number of steps. If it does, the fraction is a rational number, i.e., the ratio of two integers\n\nand can be written as a finite continued fraction . If the algorithm does not stop, the fraction is an irrational number and can be described by an infinite continued fraction . Examples of infinite continued fractions are the golden ratio and the square root of two, . The algorithm is unlikely to stop, since almost all ratios of two real numbers are irrational.\n\nAn infinite continued fraction may be truncated at a step to yield an approximation to that improves as is increased. The approximation is described by convergents ; the numerator and denominators are coprime and obey the recurrence relation\n\nwhere and are the initial values of the recursion. The convergent is the best rational number approximation to with denominator :\n\nPolynomials in a single variable \"x\" can be added, multiplied and factored into irreducible polynomials, which are the analogs of the prime numbers for integers. The greatest common divisor polynomial of two polynomials and is defined as the product of their shared irreducible polynomials, which can be identified using the Euclidean algorithm. The basic procedure is similar to that for integers. At each step , a quotient polynomial and a remainder polynomial are identified to satisfy the recursive equation\n\nwhere and . The quotient polynomial is chosen so that the leading term of equals the leading term of ; this ensures that the degree of each remainder is smaller than the degree of its predecessor: . Since the degree is a nonnegative integer, and since it decreases with every step, the Euclidean algorithm concludes in a finite number of steps. The final nonzero remainder is the greatest common divisor of the original two polynomials, and .\n\nFor example, consider the following two quartic polynomials, which each factor into two quadratic polynomials\n\nDividing by yields a remainder . In the next step, is divided by yielding a remainder . Finally, dividing by yields a zero remainder, indicating that is the greatest common divisor polynomial of and , consistent with their factorization.\n\nMany of the applications described above for integers carry over to polynomials. The Euclidean algorithm can be used to solve linear Diophantine equations and Chinese remainder problems for polynomials; continued fractions of polynomials can also be defined.\n\nThe polynomial Euclidean algorithm has other applications, such as Sturm chains, a method for counting the zeros of a polynomial that lie inside a given real interval. This in turn has applications in several areas, such as the Routh–Hurwitz stability criterion in control theory.\n\nFinally, the coefficients of the polynomials need not be drawn from integers, real numbers or even the complex numbers. For example, the coefficients may be drawn from a general field, such as the finite fields described above. The corresponding conclusions about the Euclidean algorithm and its applications hold even for such polynomials.\n\nThe Gaussian integers are complex numbers of the form , where and are ordinary integers and is the square root of negative one. By defining an analog of the Euclidean algorithm, Gaussian integers can be shown to be uniquely factorizable, by the argument above. This unique factorization is helpful in many applications, such as deriving all Pythagorean triples or proving Fermat's theorem on sums of two squares. In general, the Euclidean algorithm is convenient in such applications, but not essential; for example, the theorems can often be proven by other arguments.\n\nThe Euclidean algorithm developed for two Gaussian integers and is nearly the same as that for ordinary integers, but differs in two respects. As before, the task at each step is to identify a quotient and a remainder such that\n\nwhere , where , and where every remainder is strictly smaller than its predecessor: . The first difference is that the quotients and remainders are themselves Gaussian integers, and thus are complex numbers. The quotients are generally found by rounding the real and complex parts of the exact ratio (such as the complex number ) to the nearest integers. The second difference lies in the necessity of defining how one complex remainder can be \"smaller\" than another. To do this, a norm function is defined, which converts every Gaussian integer into a ordinary integer. After each step of the Euclidean algorithm, the norm of the remainder is smaller than the norm of the preceding remainder, . Since the norm is a nonnegative integer and decreases with every step, the Euclidean algorithm for Gaussian integers ends in a finite number of steps. The final nonzero remainder is , the Gaussian integer of largest norm that divides both and ; it is unique up to multiplication by a unit, or .\n\nMany of the other applications of the Euclidean algorithm carry over to Gaussian integers. For example, it can be used to solve linear Diophantine equations and Chinese remainder problems for Gaussian integers; continued fractions of Gaussian integers can also be defined.\n\nA set of elements under two binary operations, denoted as addition and multiplication, is called a Euclidean domain if it forms a commutative ring and, roughly speaking, if a generalized Euclidean algorithm can be performed on them. The two operations of such a ring need not be the addition and multiplication of ordinary arithmetic; rather, they can be more general, such as the operations of a mathematical group or monoid. Nevertheless, these general operations should respect many of the laws governing ordinary arithmetic, such as commutativity, associativity and distributivity.\n\nThe generalized Euclidean algorithm requires a \"Euclidean function\", i.e., a mapping from into the set of nonnegative integers such that, for any two nonzero elements and in , there exist and in such that and . An example of this mapping is the norm function used to order the Gaussian integers above. The function can be the magnitude of the number or the degree of a polynomial. The basic principle is that each step of the algorithm reduces \"f\" inexorably; hence, if can be reduced only a finite number of times, the algorithm must stop in a finite number of steps. This principle relies on the natural well-ordering of the non-negative integers; roughly speaking, this requires that every non-empty set of non-negative integers has a smallest member.\n\nThe fundamental theorem of arithmetic applies to any Euclidean domain: Any number from a Euclidean domain can be factored uniquely into irreducible elements. Any Euclidean domain is a unique factorization domain (UFD), although the converse is not true. The Euclidean domains and the UFD's are subclasses of the GCD domains, domains in which a greatest common divisor of two numbers always exists. In other words, a greatest common divisor may exist (for all pairs of elements in a domain), although it may not be possible to find it using a Euclidean algorithm. A Euclidean domain is always a principal ideal domain (PID), an integral domain in which every ideal is a principal ideal. Again, the converse is not true: not every PID is a Euclidean domain.\n\nThe unique factorization of Euclidean domains is useful in many applications. For example, the unique factorization of the Gaussian integers is convenient in deriving formulae for all Pythagorean triples and in proving Fermat's theorem on sums of two squares. Unique factorization was also a key element in an attempted proof of Fermat's Last Theorem published in 1847 by Gabriel Lamé, the same mathematician who analyzed the efficiency of Euclid's algorithm, based on a suggestion of Joseph Liouville. Lamé's approach required the unique factorization of numbers of the form , where and are integers, and {2} .</math>\n\nIf the function corresponds to a norm function, such as that used to order the Gaussian integers above, then the domain is known as \"norm-Euclidean\". The norm-Euclidean rings of quadratic integers are exactly those where is one of the values −11, −7, −3, −2, −1, 2, 3, 5, 6, 7, 11, 13, 17, 19, 21, 29, 33, 37, 41, 57, or 73. The cases and yield the Gaussian integers and Eisenstein integers, respectively.\n\nIf is allowed to be any Euclidean function, then the list of possible values of for which the domain is Euclidean is not yet known. The first example of a Euclidean domain that was not norm-Euclidean (with ) was published in 1994. In 1973, Weinberger proved that a quadratic integer ring with is Euclidean if, and only if, it is a principal ideal domain, provided that the generalized Riemann hypothesis holds.\n\nThe Euclidean algorithm may be applied to noncommutative rings such as the set of Hurwitz quaternions. Let and represent two elements from such a ring. They have a common right divisor if and for some choice of and in the ring. Similarly, they have a common left divisor if and for some choice of and in the ring. Since multiplication is not commutative, there are two versions of the Euclidean algorithm, one for right divisors and one for left divisors. Choosing the right divisors, the first step in finding the by the Euclidean algorithm can be written\n\nwhere represents the quotient and the remainder. This equation shows that any common right divisor of and is likewise a common divisor of the remainder . The analogous equation for the left divisors would be\n\nWith either choice, the process is repeated as above until the greatest common right or left divisor is identified. As in the Euclidean domain, the \"size\" of the remainder must be strictly smaller than , and there must be only a finite number of possible sizes for , so that the algorithm is guaranteed to terminate.\n\nMost of the results for the GCD carry over to noncommutative numbers. For example, Bézout's identity states that the right can be expressed as a linear combination of and . In other words, there are numbers and such that\n\nThe analogous identity for the left GCD is nearly the same:\n\nBézout's identity can be used to solve Diophantine equations. For instance, one of the standard proofs of Lagrange's four-square theorem, that every positive integer can be represented as a sum of four squares, is based on quaternion GCDs in this way.\n\n\n\n"}
{"id": "40221453", "url": "https://en.wikipedia.org/wiki?curid=40221453", "title": "Foias constant", "text": "Foias constant\n\nIn mathematical analysis, the Foias constant is a real number named after Ciprian Foias.\n\nIt is defined in the following way: for any real number \"x\" > 0, there is a sequence defined by the recurrence relation\n\nfor \"n\" = 1, 2, 3, ... The Foias constant is the unique choice \"α\" such that if \"x\" = \"α\" then the sequence diverges to infinity. Numerically, it is\n\nNo closed form for the constant is known.\n\nWhen \"x\" = \"α\" then we have the limit:\n\nwhere \"log\" denotes the natural logarithm. Consequently, one has by the prime number theorem that in this case\n\nwhere is the prime-counting function.\n\n\n"}
{"id": "57374397", "url": "https://en.wikipedia.org/wiki?curid=57374397", "title": "Fritsch graph", "text": "Fritsch graph\n\nIn the mathematical field of graph theory, the Fritsch graph is a planar graph with 9 vertices and 21 edges.\nIt was obtained by Fritsch as a minimal sized counterexample to the Alfred Kempe's attempt to prove the four-color theorem.\n"}
{"id": "7992717", "url": "https://en.wikipedia.org/wiki?curid=7992717", "title": "Fréchet surface", "text": "Fréchet surface\n\nIn mathematics, a Fréchet surface is an equivalence class of parametrized surfaces in a metric space. In other words, a Fréchet surface is a way of thinking about surfaces independently of how they are \"written down\" (parametrized). The concept is named after the French mathematician Maurice Fréchet.\n\nLet \"M\" be a compact 2-dimensional manifold, either closed or with boundary, and let (\"X\", \"d\") be a metric space. A parametrized surface in \"X\" is a map\n\nthat is continuous with respect to the topology on \"M\" and the metric topology on \"X\". Let\n\nwhere the infimum is taken over all homeomorphisms \"σ\" of \"M\" to itself. Call two parametrized surfaces \"f\" and \"g\" in \"X\" equivalent if and only if\n\nAn equivalence class [\"f\"] of parametrized surfaces under this notion of equivalence is called a Fréchet surface; each of the parametrized surfaces in this equivalence class is called a parametrization of the Fréchet surface [\"f\"].\n\nMany properties of parametrized surfaces are actually properties of the Fréchet surface, i.e. of the whole equivalence class, and not of any particular parametrization.\n\nFor example, given two Fréchet surfaces, the value of \"ρ\"(\"f\", \"g\") is independent of the choice of the parametrizations \"f\" and \"g\", and is called the Fréchet distance between the Fréchet surfaces.\n\n\n"}
{"id": "1668732", "url": "https://en.wikipedia.org/wiki?curid=1668732", "title": "Fundamental polygon", "text": "Fundamental polygon\n\nIn mathematics, a fundamental polygon can be defined for every compact Riemann surface of genus greater than 0. It encodes not only the topology of the surface through its fundamental group but also determines the Riemann surface up to conformal equivalence. By the uniformization theorem, every compact Riemann surface has simply connected universal covering surface given by exactly one of the following:\n\n\nIn the first case of genus zero, the surface is conformally equivalent to the Riemann sphere.\n\nIn the second case of genus one, the surface is conformally equivalent to a torus C/Λ for some lattice Λ in C. The fundamental polygon of Λ is either a period parallelogram or a centrally symmetric polygon, a result first proved by Fedorov in 1891.\n\nIn the last case of genus \"g\" > 1, the Riemann surface is conformally equivalent to \"H\"/Γ where Γ is a Fuchsian group of Möbius transformations. A fundamental domain for Γ is given by a convex polygon for the hyperbolic metric on \"H\". These can be defined by Dirichlet polygons and have an even number of sides. The structure of the fundamental group Γ can be read off from such a polygon. Using the theory of quasiconformal mappings and the Beltrami equation, it can be shown there is a canonical convex Dirichlet polygon with 4\"g\" sides, first defined by Fricke, which corresponds to the standard presentation of Γ as the group with 2\"g\" generators \"a\", \"b\", \"a\", \"b\", ..., \"a\", \"b\" and the single relation [\"a\",\"b\"][\"a\",\"b\"] ⋅⋅⋅ [\"a\",\"b\"] = 1, where [\"a\",\"b\"] = \"a\" \"b\" \"a\"\"b\".\n\nAny Riemannian metric on an oriented closed 2-manifold \"M\" defines a complex structure on \"M\", making \"M\" a compact Riemann surface. Through the use of fundamental polygons, it follows that two oriented closed 2-manifolds are classified by their genus, that is half the rank of the Abelian group Γ/[Γ,Γ], where Γ = (\"M\"). Moreover, it also follows from the theory of quasiconformal mappings that two compact Riemann surfaces are diffeomorphic if and only if they are homeomorphic. Consequently, two closed oriented 2-manifolds are \nhomeomorphic if and only if they are diffeomorphic. Such a result can also be proved using the methods of differential topology.\n\nIn the case of genus one, a fundamental convex polygon is sought for the action by translation of Λ = Z a ⊕ Z b on R = C where a and b are linearly independent over R. (After performing a real linear transformation on R, it can be assumed if necessary that Λ = Z = Z + Z \"i\"; for a genus one Riemann surface it can be taken to have the form Λ = Z = Z + Z ω, with Im ω > 0.) A fundamental domain is given by the parallelogram for where and are generators of Λ.\n\nIf \"C\" is the interior of a fundamental convex polygon, then the translates + x cover R as x runs over Λ. It follows that the boundary points of \"C\" are formed of intersections ∩ ( + x). These are compact convex sets in ∂\"C\" and thus either vertices of \"C\" or sides of \"C\". It follows that every closed side of \"C\" can be written this way. Translating by −x it follows that ∩ ( − x) is also a side of C. Thus sides of \"C\" occur in parallel pairs of equal length. The end points of two such parallel segments of equal length can be joined so that they intersect and the intersection occurs at the midpoints of the line segments joining the endpoints. It follows that the intersections of al such segments occur at the same point. Translating that point to the origin, it follows that the polygon is centrally symmetric; that is, if a point \"z\" is in the polygon, so too is −\"z\".\n\nIt is easy to see translates of a centrally symmetric convex hexagon tessellate the plane. If \"A\" is a point of the hexagon, then the lattice is generated by the displacement vectors \"AB\" and \"AC\" where \"B\" and \"C\" are the two vertices which are not neighbours of \"A\" and not opposite \"A\". Indeed, the second picture shows how the hexagon is equivalent to the parallelogram obtained by displacing the two triangles chopped off by the segments \"AB\" and \"AC\". Equally well the first picture shows another way of matching a tiling by parallelograms with the hexagonal tiling. If the centre of the hexagon is 0 and the vertices in order are a, b, c, −a, −b and −c, then Λ is the Abelian group with generators and .\n\nFedorov's theorem, established by the Russian crystallographer Evgraf Fedorov in 1891, asserts that parallelograms and centrally symmetric hexagons are the only convex polygons that are fundamental domains. There are several proofs of this, some of the more recent ones related to results in convexity theory, the geometry of numbers and circle packing, such as the Brunn–Minkowski inequality.\nTwo elementary proofs due to H. S. M. Coxeter and Voronoi will be presented here.\n\nCoxeter's proof proceeds by assuming that there is a centrally symmetric convex polygon \"C\" with 2\"m\" sides. Then a large closed parallelogram formed from \"N\" fundamental parallelograms is tiled by translations of \"C\" which go beyond the edges of the large parallelogram. This induces a tiling on the torus C/\"N\"Λ. Let \"v\", \"e\" and \"f\" be the number of vertices, edges and faces in this tiling (taking into account identifications in the quotient space). Then, because the Euler–Poincaré characteristic of a torus is zero,\n\nOn the other hand, since each vertex is on at least 3 different edges and every edge is between two vertices,\n\nMoreover, since every edge is on exactly two faces,\n\nHence\n\nso that\n\nas required.\n\nVoronoi's proof starts with the observation that every edge of \"C\" corresponds to an element x of Λ. In fact the edge is the orthogonal bisector of the radius from 0 to x. Hence the foot of the perpendicular from 0 to each edge lies in the interior of each edge. If y is any lattice point, then 1/2 y cannot lie in \"C\"; for if so, –1/2 y would also lie in \"C\", contradicting \"C\" being a fundamental domain for Λ. Let ±x, ..., ±x be the 2\"m\" distinct points of Λ corresponding to sides of \"C\". Fix generators a and b of Λ. Thus x = α a + β b, where α and β are integers. It is not possible for both α and β to be even, since otherwise ± 1/2 x would be a point of Λ on a side, which contradicts \"C\" being a fundamental domain. So there are three possibilities for the pair of integers (α, β) modulo 2: (0,1), (1,0) and (1,1). Consequently, if \"m\" > 3, there would be x and x with \"i\" ≠ \"j\" with both coordinates of x − x even, i.e. 1/2 (x + x) lies in Λ. But this is the midpoint of the line segment joining two interior points of edges and hence lies in \"C\", the interior of the polygon. This again contradicts the fact that \"C\" is a fundamental domain. So \"reductio ad absurdum\" \"m\" ≤ 3, as claimed.\n\nFor a lattice Λ in C = R, a fundamental domain can be defined canonically using the conformal structure of C. Note that the group of conformal transformations of C is given by complex affine transformations with . These transformations preserve Euclidean metric up to a factor, as well as preserving the orientation. It is the subgroup of the Möbius group fixing the point at ∞. The metric structure can be used to define a canonical fundamental domain by (It is obvious from the definition that it is a fundamental domain.) This is an example of a Dirichlet domain or Voronoi diagram: since complex translations form an Abelian group, so commute with the action of Λ, these concepts coincide. The canonical fundamental domain for with is either a symmetric convex parallelogram or hexagon with centre 0. By conformal equivalence, the period \"ω\" can be further restricted to satisfy and . As Dirichlet showed (\"Dirichlet's hexagon theorem\", 1850), for almost all \"ω\" the fundamental domain is a hexagon. For , the midpoints of sides are given by ±1/2, ±\"ω\"/2 and ; the sides bisect the corresponding radii from 0 orthogonally, which determines the vertices completely. In fact the first vertex must have the form and with \"x\" and \"y\" real; so if , then and . Hence and . The six vertices are therefore and .\nEvery compact Riemann surface \"X\" has a universal covering surface which is a simply connected Riemann surface . The fundamental group of \"X\" acts as deck transformations of and can be identified with a subgroup Γ of the group of biholomorphisms of . The group Γ thus acts freely on with compact quotient space /Γ, which can be identified with \"X\". Thus the classification of compact Riemann surfaces can be reduced to the study of possible groups Γ. By the uniformization theorem is either the Riemann sphere, the complex plane or the unit disk/upper halfplane. The first important invariant of a compact Riemann surface is its \"genus\", a topological invariant given by half the rank of the Abelian group (which can be identified with the homology group ). The genus is zero if the covering space is the Riemann sphere; one if it is the complex plane; and greater than one if it is the unit disk or upper halfplane.\n\nBihomolomorphisms of the Riemann sphere are just complex Möbius transformations and every non-identity transformation has at least one fixed point, since the corresponding complex matrix always has at least one non-zero eigenvector. Thus if is the Riemann sphere, then \"X\" must be simply connected and biholomorphic to the Riemann sphere, the \"genus zero\" Riemann surface. When is the complex plane, the group of biholomorphisms is the affine group, the complex Möbius transformations fixing ∞, so the transformations with . The non-identity transformations without fixed points are just those with and , i.e. the non-zero translations. The group Γ can thus be identified with a lattice Λ in C and \"X\" with a quotient C/Λ, as described in the section on fundamental polygons in genus one. In the third case when is the unit disk or upper half plane, the group of biholomorphisms consists of the complex Möbius transformations fixing the unit circle or the real axis. In the former case, the transformations correspond to elements of the group in the latter case they correspond to real Möbius transformations, so elements of \n\nThe study and classification of possible groups Γ that act freely on the unit disk or upper halfplane with compact quotient—the Fuchsian groups of the first kind—can be accomplished by studying their fundamental polygons, as described below. As Poincaré observed, each such polygon has special properties, namely it is convex and has a natural pairing between its sides. These not only allow the group to be recovered but provide an explicit presentation of the group by generators and relations. Conversely Poincaré proved that any such polygon gives rise to a compact Riemann surface; in fact, Poincaré's polygon theorem applied to more general polygons, where the polygon was allowed to have ideal vertices, but his proof was complete only in the compact case, without such vertices. Without assumptions on the convexity of the polygon, complete proofs have been given by Maskit and de Rham, based on an idea of Siegel, and can be found in , and . Carathéodory gave an elementary treatment of the existence of tessellations by Schwarz triangles, i.e. tilings by geodesic triangles with angles /\"a\", /\"b\", /\"c\" with sum less than where \"a\", \"b\", \"c\" are integers. When all the angles equal /2\"g\", this establishes the tiling by regular \"4g\"-sided hyperbolic polygons and hence the existence of a particular compact Riemann surface of genus \"g\" as a quotient space. This special example, which has a cyclic group Z of bihomolomorphic symmetries, is used in the development below.\n\nThe classification up to homeomorphism and diffeomorphism of compact Riemann surfaces implies the classification of closed orientable 2-manifolds up to homeomorphism and diffeomorphism: any two 2-manifolds with the same genus are diffeomorphic. In fact using a partition of unity, every closed orientable 2-manifold admits a Riemannian metric. For a compact Riemann surface a conformal metric can also be introduced with is conformal, so that in holomorphic coordinates the metric takes the form \"ρ\"(\"z\") . Once this metric has been chosen, locally biholomorphic mappings are precisely orientation-preserving diffeomorphisms that are conformal, i.e. scale the metric by a smooth function. The existence of isothermal coordinates—which can be proved using either local existence theorems for the Laplacian or the Beltrami equation—shows that every closed oriented Riemannian 2-manifold can be given a complex structure compatible with its metric, and hence has the structure of a compact Riemann surface. This construction shows that the classification of closed orientable 2-manifolds up to diffeomorphism or homeomorphism can be reduced to the case of compact Riemann surfaces.\n\nThe classification up to homeomorphism and diffeomorphism of compact Riemann surfaces can be accomplished using the fundamental polygon. Indeed as Poincaré observed convex fundamental polygons for compact Riemann surfaces \"H\"/Γ can be constructed by adapting the method Dirichlet from the Euclidean space to hyperbolic space. Then following Nevanlinna and Jost, the fundamental domain can be modified in steps to yield a non-convex polygon with vertices lying in a single orbit of Γ and piecewise geodesic sides. The pairing relation on the sides is also modified in each of these steps. Each step involves cutting the polygon by a diagonal geodesic segment in the interior of the polygon and reassembling the polygon using one of the Möbius transformations involved in the pairing. No two paired sides can have a common vertex in the final pairing relation, which satisfies similar properties to the original relation. This polygon can in turn be successively modified by reassembling the polygon after cutting it by a diagonal piecewise geodesic segment in its interior. The final polygon has 4\"g\" equivalent vertices, with sides that are piecewise geodesic. The sides are labelled by the group elements which give the Möbius transformation to the paired side. In order the labelling is\n\nso that Γ is generated by the \"a\" and \"b\" subject to the single relation\n\nUsing the theory of intersection numbers, it follows that the shape obtained by joining vertices by geodesics is also a proper polygon, not necessarily convex, and is also a fundamental domain with the same group elements giving the pairing. This yields a fundamental polygon with edges given by geodesic segments and with the standard labelling. The abelianisation of Γ, the quotient group , is a free Abelian group with 2\"g\" generators. Thus the genus \"g\" is a topological invariant. It is easy to see that two Riemann surfaces with the same genus are homeomorphic since as topological space since they are obtained by identifying sides of a 4\"g\"-sided polygon—a Euclidean polygon in the Klein model—by diffeomorphisms between paired sides. Applying this construction to the regular 4\"g\"-sided polygon allows the Riemann surface to be viewed topologically as a doughnut with \"g\" holes, the standard description of oriented surfaces in introductory texts on topology.\n\nThere are several further results:\n\n\nThese results are tied up with the interrelation between homeomorphisms and the fundamental group: this reflects the fact that the mapping class group of a Riemann surface—the group of quasiconformal self-homomorphisms of a Riemann surface \"H\"/Γ modulo those homotopic to the identity—can be identified with the outer automorphism group of Γ (the Dehn–Nielsen–Baer theorem). To see this connection, note that if \"f\" is a quasiconformal homeomorphism of \"X\" = \"H\"/Γ onto \"X\" = \"H\"/Γ, then \"f\" lifts to a quasiconformal homeomorphism of \"H\" onto itself. This lift is unique up to pre-composition with elements of Γ and post-composition with elements of Γ. If is the projection of \"H\" onto \"X\", then and Γ is just the group of homeomorphisms \"g\" of \"H\" such that ∘ \"g\" = . If follows that for \"g\" in Γ where \"θ\" is a group isomorphism of Γ onto Γ. A different choice of changes \"θ\" by composition with an inner automorphism: such isomorphisms are said to be \"equivalent\".\n\nTwo isomorphisms \"θ\" and \"θ\"′ are equivalent if and only if the corresponding homeomorphisms \"f\" and \"f\" are homotopic. In fact it suffices to show that a quasiconformal self-homeomorphism \"f\" of a surface induces an inner automorphism of the fundamental group if and only if it is homotopic to the identity map: in other words the homomorphism of the quasiconformal self-homeomorphism group of \"H\"/Γ into Out Γ passes to the mapping class group on which it is injective. Indeed suppose first that \"F\"(\"t\") is a continuous path of self-homeomorphisms with \"F\"(0) = id and . Then there is a continuous lift (\"t\") with (0) = id. Moreover for each \"g\" in Γ, is a continuously varying element of Γ equal to \"g\" for ; so discreteness of Γ forces this element to be constant and hence equal to \"g\" so that (\"t\") commutes with Γ, so \"F\"(1) induces the trivial automorphism. If on the other hand \"F\" is a quasiconformal lift of \"f\" inducing an inner automorphism of Γ, after composition with an element Γ if necessary it can be assumed that \"F\" commutes with Γ. Since \"F\" is quasiconformal, it extends to a quasisymmetric homeomorphism of the circle which also commutes with Γ. Each in Γ is hyperbolic so has two fixed points on the circle \"a\" such that for all other points \"z\", \"g\"(\"z\") tends to \"a\" as \"n\" tends to infinity. Hence \"F\" must fix these points; since these points are dense in the circle as \"g\" varies, it follows that \"F\" fixes the unit circle. Let \"μ\" = \"F\" / \"F\", so that \"μ\" is a Γ-invariant Beltrami differential. Let \"F\"(\"t\") be the solution of the Beltrami equation \"tμ\" normalised to fix three points on the unit circle. Then \"F\"(\"t\") commutes with Γ and so, as for , is the identity on the unit circle. By construction \"F\"(\"t\") is an isotopy between the identity and \"F\". This proves injectivity.\n\nThe proof of surjectivity relies on comparing the hyperbolic metric on \"D\" with a word-length metric on Γ. Assuming with out loss of generality that 0 lies in the interior of a convex fundamental polygon \"C\" and \"g\" is an element of Γ, the ray from 0 to \"g\"(0)—the hyperbolic geodesic—passes through a succession of translates of \"C\". Each of these is obtained from the previous one by applying a generator of Γ or a fixed product of generators (if successive translates meet in a vertex). It follows that the hyperbolic distance between 0 and \"g\"(0) is less than 4\"g\" times the word length of \"g\" plus twice diameter of the fundamental polygon. Thus the metric on Γ defined by the word length \"L\"(\"g\") satisfies\n\nfor positive constants \"a\" and \"b\". Conversely there are positive constants \"c\" and \"d\" such that\n\nGiven a point formula_10 in the upper half-plane H, and a discrete subgroup Γ of that acts freely discontinuously on the upper half-plane, then one can define the Dirichlet polygon as the set of points\n\nHere, \"d\" is a hyperbolic metric on the upper half-plane. The metric fundamental polygon is more usually called the Dirichlet polygon.\n\nIn this section, starting from an arbitrary Dirichlet polygon, a description will be given of the method of , elaborated in , for modifying the polygon to a non-convex polygon with 4g equivalent vertices and a canonical pairing on the sides. This treatment is an analytic counterpart of the classical topological classification of orientable 2-dimensional polyhedra presented in .\n\nGiven a Riemann surface of genus \"g\" greater than one, Fricke described another fundamental polygon, the Fricke canonical polygon, which is a very special example of a Dirichlet polygon. The polygon is related to the standard presentation of the fundamental group of the surface. Fricke's original construction is complicated and described in . Using the theory of quasiconformal mappings of Ahlfors and Bers, gave a new, shorter and more precise version of Fricke's construction. The Fricke canonical polygon has the following properties:\n\n\nThe above construction is sufficient to guarantee that each side of the polygon is a closed (non-trivial) loop in the Riemann surface H/Γ. As such, each side can thus an element of the fundamental group formula_17. In particular, the fundamental group formula_18 has 2\"g\" generators formula_19, with exactly one defining constraint,\n\nThe genus of the Riemann surface H/Γ is \"g\".\n\nThe area of the standard fundamental polygon is formula_21 where \"g\" is the genus of the Riemann surface (equivalently, where 4\"g\" is the number of the sides of the polygon). Since the standard polygon is a representative of H/Γ, the total area of the Riemann surface is equal to the area of the standard polygon. The area formula follows from the Gauss–Bonnet theorem and is in a certain sense generalized through the Riemann–Hurwitz formula.\n\nExplicit expressions can be given for the regular standard 4\"g\"-sided polygon, with rotational symmetry. In this case, that of a genus formula_22 Riemann surface with \"g\"-fold rotational symmetry, the group may be given by formula_23 generators formula_24. These generators are given by the following fractional linear transforms acting on the upper half-plane:\n\nfor formula_26. The parameters are given by\n\nand\n\nand\n\nIt may be verified that these generators obey the constraint\n\nwhich gives the totality of the group presentation.\n\n\n"}
{"id": "11958", "url": "https://en.wikipedia.org/wiki?curid=11958", "title": "George Berkeley", "text": "George Berkeley\n\nGeorge Berkeley (; 12 March 168514 January 1753) – known as Bishop Berkeley (Bishop of Cloyne) – was an Irish philosopher whose primary achievement was the advancement of a theory he called \"immaterialism\" (later referred to as \"subjective idealism\" by others). This theory denies the existence of material substance and instead contends that familiar objects like tables and chairs are only ideas in the minds of perceivers and, as a result, cannot exist without being perceived. Berkeley is also known for his critique of abstraction, an important premise in his argument for immaterialism.\n\nBerkeley was the namesake of the city of Berkeley, California, which is perhaps most famous as the home of the prestigious university.\n\nIn 1709, Berkeley published his first major work, \"\", in which he discussed the limitations of human vision and advanced the theory that the proper objects of sight are not material objects, but light and colour. This foreshadowed his chief philosophical work, \"A Treatise Concerning the Principles of Human Knowledge\", in 1710, which, after its poor reception, he rewrote in dialogue form and published under the title \"Three Dialogues between Hylas and Philonous\" in 1713.\n\nIn this book, Berkeley's views were represented by Philonous (Greek: \"lover of mind\"), while Hylas (Greek: \"matter\") embodies the Irish thinker's opponents, in particular John Locke.\nBerkeley argued against Isaac Newton's doctrine of absolute space, time and motion in \"De Motu\" (On Motion), published 1721. His arguments were a precursor to the views of Mach and Einstein. In 1732, he published \"Alciphron\", a Christian apologetic against the free-thinkers, and in 1734, he published \"The Analyst\", a critique of the foundations of calculus, which was influential in the development of mathematics.\n\nHis last major philosophical work, \"Siris\" (1744), begins by advocating the medicinal use of tar water and then continues to discuss a wide range of topics, including science, philosophy, and theology. Interest in Berkeley's work increased after World War II because he tackled many of the issues of paramount interest to philosophy in the 20th century, such as the problems of perception, the difference between primary and secondary qualities, and the importance of language.\n\nBerkeley was born at his family home, Dysart Castle, near Thomastown, County Kilkenny, Ireland, the eldest son of William Berkeley, a cadet of the noble family of Berkeley. Little is known of his mother. He was educated at Kilkenny College and attended Trinity College, Dublin, where he was elected a Scholar in 1702, earning a bachelor's degree in 1704 and completing a master's degree in 1707. He remained at Trinity College after completion of his degree as a tutor and Greek lecturer.\n\nHis earliest publication was on mathematics, but the first that brought him notice was his \"\", first published in 1709. In the essay, Berkeley examines visual distance, magnitude, position and problems of sight and touch. While this work raised much controversy at the time, its conclusions are now accepted as an established part of the theory of optics.\n\nThe next publication to appear was the \"Treatise Concerning the Principles of Human Knowledge\" in 1710, which had great success and gave him a lasting reputation, though few accepted his theory that nothing exists outside the mind. This was followed in 1713 by \"Three Dialogues between Hylas and Philonous\", in which he propounded his system of philosophy, the leading principle of which is that the world, as represented by our senses, depends for its existence on being perceived.\n\nFor this theory, the \"Principles\" gives the exposition and the \"Dialogues\" the defence. One of his main objectives was to combat the prevailing materialism of his time. The theory was largely received with ridicule, while even those such as Samuel Clarke and William Whiston, who did acknowledge his \"extraordinary genius,\" were nevertheless convinced that his first principles were false.\n\nShortly afterwards, Berkeley visited England and was received into the circle of Addison, Pope and Steele. In the period between 1714 and 1720, he interspersed his academic endeavours with periods of extensive travel in Europe, including one of the most extensive Grand Tours of the length and breadth of Italy ever undertaken. In 1721, he took Holy Orders in the Church of Ireland, earning his doctorate in divinity, and once again chose to remain at Trinity College Dublin, lecturing this time in Divinity and in Hebrew. In 1721/2 he was made Dean of Dromore and, in 1724, Dean of Derry.\n\nIn 1723, following her violent quarrel with Jonathan Swift, who had been her intimate friend for many years, Esther Vanhomrigh (for whom Swift had created the nickname \"Vanessa\") named Berkeley her co-heir along with the barrister Robert Marshall; her choice of legatees caused a good deal of surprise since she did not know either of them well, although Berkeley as a very young man had known her father. Swift said generously that he did not grudge Berkeley his inheritance, much of which vanished in a lawsuit in any event. A story that Berkeley and Marshall disregarded a condition of the inheritance that they must publish the correspondence between Swift and Vanessa is probably untrue.\n\nIn 1725, he began the project of founding a college in Bermuda for training ministers and missionaries in the colony, in pursuit of which he gave up his deanery with its income of £1100.\n\nIn 1728, he married Anne Forster, daughter of John Forster, Chief Justice of the Irish Common Pleas, and his first wife Rebecca Monck. He then went to America on a salary of £100 per annum. He landed near Newport, Rhode Island, where he bought a plantation at Middletownthe famous \"Whitehall\". It has been claimed that \"he introduced Palladianism into America by borrowing a design from [William] Kent's \"Designs of Inigo Jones\" for the door-case of his house in Rhode Island, Whitehall.\" He also brought to New England John Smibert, the British artist he \"discovered\" in Italy, who is generally regarded as the founding father of American portrait painting. Meanwhile, he drew up plans for the ideal city he planned to build on Bermuda. He lived at the plantation while he waited for funds for his college to arrive. The funds, however, were not forthcoming, and in 1732 he left America and returned to London. He and Anne had four children who survived infancy: Henry, George, William and Julia, and at least two other children who died in infancy. William's death in 1751 was a great cause of grief to his father.\n\nWhile living in London's Saville Street, he took part in efforts to create a home for the city's abandoned children. The Foundling Hospital was founded by Royal Charter in 1739, and Berkeley is listed as one of its original governors. In 1734, he was appointed Bishop of Cloyne in Ireland, a position he was to hold until his death. Soon afterwards, he published \"Alciphron, or The Minute Philosopher\", directed against both Shaftesbury and Bernard de Mandeville; and in 1735–37 \"The Querist\".\n\nHis last two publications were \"Siris: A Chain of Philosophical Reflexions and Inquiries Concerning the Virtues of Tarwater, And divers other Subjects connected together and arising one from another\" (1744) and \"Further Thoughts on Tar-water\" (1752). Pine tar is an effective antiseptic and disinfectant when applied to cuts on the skin, but Berkeley argued for the use of pine tar as a broad panacea for diseases. His 1744 work on tar-water sold more copies than any of his other books during Berkeley's lifetime.\n\nHe remained at Cloyne until 1752, when he retired. With his wife and daughter Julia he went to Oxford to live with his son George and supervise his education. He died soon afterward and was buried in Christ Church Cathedral, Oxford. His affectionate disposition and genial manners made him much loved and held in warm regard by many of his contemporaries. Anne outlived her husband by many years, and died in 1786.\n\nThe use of the concepts of \"spirit\" and \"idea\" is central in Berkeley's philosophy. As used by him, these concepts are difficult to translate into modern terminology. His concept of \"spirit\" is close to the concept of \"conscious subject\" or of \"mind\", and the concept of \"idea\" is close to the concept of \"sensation\" or \"state of mind\" or \"conscious experience\".\n\nThus Berkeley denied the existence of matter as a metaphysical substance, but did not deny the existence of physical objects such as apples or mountains. (\"I do not argue against the existence of any one thing that we can apprehend, either by sense or reflection. That the things I see with mine eyes and touch with my hands do exist, really exist, I make not the least question. The only thing whose existence we deny, is that which philosophers call matter or corporeal substance. And in doing of this, there is no damage done to the rest of mankind, who, I dare say, will never miss it.\", \"Principles \"#35) This basic claim of Berkeley's thought, his \"idealism\", is sometimes and somewhat derisively called \"immaterialism\" or, occasionally, subjective idealism. In \"Principles #3,\" he wrote, using a combination of Latin and English, \"esse is percipi\" (to be is to be perceived), most often if slightly inaccurately attributed to Berkeley as the pure Latin phrase \"esse est percipi.\" The phrase appears associated with him in authoritative philosophical sources, e.g., \"Berkeley holds that there are no such mind-independent things, that, in the famous phrase, esse est percipi (aut percipere) – to be is to be perceived (or to perceive).\"\n\nHence, human knowledge is reduced to two elements: that of spirits and of ideas (\"Principles\" #86). In contrast to ideas, a spirit cannot be perceived. A person's spirit, which perceives ideas, is to be comprehended intuitively by inward feeling or reflection (\"Principles\" #89). For Berkeley, we have no direct 'idea' of spirits, albeit we have good reason to believe in the existence of other spirits, for their existence explains the purposeful regularities we find in experience. (\"It is plain that we cannot know the existence of other spirits otherwise than by their operations, or the ideas by them excited in us\", Dialogues #145). This is the solution that Berkeley offers to the problem of other minds. Finally, the order and purposefulness of the whole of our experience of the world and especially of nature overwhelms us into believing in the existence of an extremely powerful and intelligent spirit that causes that order. According to Berkeley, reflection on the attributes of that external spirit leads us to identify it with God. Thus a material thing such as an apple consists of a collection of ideas (shape, color, taste, physical properties, etc.) which are caused in the spirits of humans by the spirit of God.\n\nA convinced adherent of Christianity, Berkeley believed God to be present as an immediate cause of all our experiences.\n\nHere is Berkeley's proof of the existence of God:\n\nAs T.I. Oizerman explained:\n\nBerkeley believed that God is not the distant engineer of Newtonian machinery that in the fullness of time led to the growth of a tree in the university quadrangle. Rather, the perception of the tree is an idea that God's mind has produced in the mind, and the tree continues to exist in the quadrangle when \"nobody\" is there, simply because God is an infinite mind that perceives all.\n\nThe philosophy of David Hume concerning causality and objectivity is an elaboration of another aspect of Berkeley's philosophy. A.A. Luce, the most eminent Berkeley scholar of the 20th century, constantly stressed the continuity of Berkeley's philosophy. The fact that Berkeley returned to his major works throughout his life, issuing revised editions with only minor changes, also counts against any theory that attributes to him a significant volte-face.\n\nJohn Locke (Berkeley's predecessor) states that we define an object by its primary and secondary qualities. He takes heat as an example of a secondary quality. If you put one hand in a bucket of cold water, and the other hand in a bucket of warm water, then put both hands in a bucket of lukewarm water, one of your hands is going to tell you that the water is cold and the other that the water is hot. Locke says that since two different objects (both your hands) perceive the water to be hot \"and\" cold, then the heat is not a quality of the water.\n\nWhile Locke used this argument to distinguish primary from secondary qualities, Berkeley extends it to cover primary qualities in the same way. For example, he says that size is not a quality of an object because the size of the object depends on the distance between the observer and the object, or the size of the observer. Since an object is a different size to different observers, then size is not a quality of the object. Berkeley rejects shape with a similar argument and then asks: if neither primary qualities nor secondary qualities are of the object, then how can we say that there is anything more than the qualities we observe?\n\nIn his \"Essay Towards a New Theory of Vision\", Berkeley frequently criticised the views of the Optic Writers, a title that seems to include Molyneux, Wallis, Malebranche and Descartes. In sections 1–51, Berkeley argued against the classical scholars of optics by holding that: \"spatial depth, as the distance that separates the perceiver from the perceived object is itself invisible\". That is, we do not see space directly or deduce its form logically using the laws of optics. Space for Berkeley is no more than a contingent expectation that visual and tactile sensations will follow one another in regular sequences that we come to expect through habit.\n\nBerkeley goes on to argue that visual cues, such as the perceived extension or 'confusion' of an object, can only be used to indirectly judge distance, because the viewer learns to associate visual cues with tactile sensations. Berkeley gives the following analogy regarding indirect distance perception: one perceives distance indirectly just as one perceives a person's embarrassment indirectly. When looking at an embarrassed person, we infer indirectly that the person is embarrassed by observing the red color on the person's face. We know through experience that a red face tends to signal embarrassment, as we've learned to associate the two.\n\nThe question concerning the visibility of space was central to the Renaissance perspective tradition and its reliance on classical optics in the development of pictorial representations of spatial depth. This matter was debated by scholars since the 11th-century Arab polymath and mathematician Alhazen (al-Hasan Ibn al-Haytham) affirmed in experimental contexts the visibility of space. This issue, which was raised in Berkeley's theory of vision, was treated at length in the \"Phenomenology of Perception\" of Maurice Merleau-Ponty, in the context of confirming the visual perception of spatial depth (\"la profondeur\"), and by way of refuting Berkeley's thesis.\n\nBerkeley wrote about the perception of size in addition to that of distance. He is frequently misquoted as believing in size–distance invariance – a view held by the Optic Writers. This idea is that we scale the image size according to distance in a geometrical manner. The error may have become commonplace because the eminent historian and psychologist E. G. Boring perpetuated it. In fact, Berkeley argued that the same cues that evoke distance also evoke size, and that we do not first see size and then calculate distance. It is worth quoting Berkeley's words on this issue (Section 53):\nWhat inclines men to this mistake (beside the humour of making one see by geometry) is, that the same perceptions or ideas which suggest distance, do also suggest magnitude... I say they do not first suggest distance, and then leave it to the judgement to use that as a medium, whereby to collect the magnitude; but they have as close and immediate a connexion with the magnitude as with the distance; and suggest magnitude as independently of distance, as they do distance independently of magnitude.\n\n\"Berkeley's works display his keen interest in natural philosophy [...] from his earliest writings (\"Arithmetica\", 1707) to his latest (\"Siris\", 1744). Moreover, much of his philosophy is shaped fundamentally by his engagement with the science of his time.\" The profundity of this interest can be judged from numerous entries in Berkeley's \"Philosophical Commentaries\" (1707–1708), e.g. \"Mem. to Examine & accurately discuss the scholium of the 8th Definition of Mr Newton's Principia.\" (#316)\n\nBerkeley argued that forces and gravity, as defined by Newton, constituted \"occult qualities\" that \"expressed nothing distinctly\". He held that those who posited \"something unknown in a body of which they have no idea and which they call the principle of motion, are in fact simply stating that the principle of motion is unknown.\" Therefore, those who \"affirm that active force, action, and the principle of motion are really in bodies are adopting an opinion not based on experience.\" Forces and gravity existed nowhere in the phenomenal world. On the other hand, if they resided in the category of \"soul\" or \"incorporeal thing\", they \"do not properly belong to physics\" as a matter. Berkeley thus concluded that forces lay beyond any kind of empirical observation and could not be a part of proper science. He proposed his theory of signs as a means to explain motion and matter without reference to the \"occult qualities\" of force and gravity.\n\nBerkeley's razor is a rule of reasoning proposed by the philosopher Karl Popper in his study of Berkeley's key scientific work \"De Motu\". Berkeley's razor is considered by Popper to be similar to Ockham's razor but \"more powerful\". It represents an extreme, empiricist view of scientific observation that states that the scientific method provides us with no true insight into the nature of the world. Rather, the scientific method gives us a variety of partial explanations about regularities that hold in the world and that are gained through experiment. The nature of the world, according to Berkeley, is only approached through properly metaphysical speculation and reasoning. Popper summarises Berkeley's razor as such:\nA general practical result – which I propose to call \"Berkeley's razor\" – of [Berkeley's] analysis of physics allows us \"a priori\" to eliminate from physical science all essentialist explanations. If they have a mathematical and predictive content they may be admitted \"qua\" mathematical hypotheses (while their essentialist interpretation is eliminated). If not they may be ruled out altogether. This razor is sharper than Ockham's: ALL entities are ruled out except those which are perceived.\n\nIn addition to his contributions to philosophy, Berkeley was also very influential in the development of mathematics, although in a rather indirect sense. \"Berkeley was concerned with mathematics and its philosophical interpretation from the earliest stages of his intellectual life.\"\nBerkeley's \"Philosophical Commentaries\" (1707–1708) witness to his interest in mathematics:\n\nAxiom. No reasoning about things whereof we have no idea. Therefore no reasoning about Infinitesimals. (#354)\n\nTake away the signs from Arithmetic & Algebra, & pray what remains? (#767)\nThese are sciences purely Verbal, & entirely useless but for Practise in Societys of Men. No speculative knowledge, no comparison of Ideas in them. (#768)\nIn 1707, Berkeley published two treatises on mathematics. In 1734, he published \"The Analyst\", subtitled \"A DISCOURSE Addressed to an Infidel Mathematician\", a critique of calculus. Florian Cajori called this treatise \"the most spectacular event of the century in the history of British mathematics.\" However, a recent study suggests that Berkeley misunderstood Leibnizian calculus. The mathematician in question is believed to have been either Edmond Halley, or Isaac Newton himself—though if to the latter, then the discourse was posthumously addressed, as Newton died in 1727. \"The Analyst\" represented a direct attack on the foundations and principles of calculus and, in particular, the notion of fluxion or infinitesimal change, which Newton and Leibniz used to develop the calculus. In his critique, Berkeley coined the phrase \"ghosts of departed quantities\", familiar to students of calculus. Ian Stewart's book \"From Here to Infinity\" captures the gist of his criticism.\n\nBerkeley regarded his criticism of calculus as part of his broader campaign against the religious implications of Newtonian mechanicsas a defence of traditional Christianity against deism, which tends to distance God from His worshipers. Specifically, he observed that both Newtonian and Leibnizian calculus employed infinitesimals sometimes as positive, nonzero quantities and other times as a number explicitly equal to zero. Berkeley's key point in \"The Analyst\" was that Newton's calculus (and the laws of motion based in calculus) lacked rigorous theoretical foundations. He claimed that\n\nIn every other Science Men prove their Conclusions by their Principles, and not their Principles by the Conclusions. But if in yours you should allow your selves this unnatural way of proceeding, the Consequence would be that you must take up with Induction, and bid adieu to Demonstration. And if you submit to this, your Authority will no longer lead the way in Points of Reason and Science.\nBerkeley did not doubt that calculus produced real world truth; simple physics experiments could verify that Newton's method did what it claimed to do. \"The cause of Fluxions cannot be defended by reason\", but the results could be defended by empirical observation, Berkeley's preferred method of acquiring knowledge at any rate. Berkeley, however, found it paradoxical that \"Mathematicians should deduce true Propositions from false Principles, be right in Conclusion, and yet err in the Premises.\" In \"The Analyst\" he endeavoured to show \"how Error may bring forth Truth, though it cannot bring forth Science\". Newton's science, therefore, could not on purely scientific grounds justify its conclusions, and the mechanical, deistic model of the universe could not be rationally justified.\n\nThe difficulties raised by Berkeley were still present in the work of Cauchy whose approach to calculus was a combination of infinitesimals and a notion of limit, and were eventually sidestepped by Weierstrass by means of his (ε, δ) approach, which eliminated infinitesimals altogether. More recently, Abraham Robinson restored infinitesimal methods in his 1966 book \"Non-standard analysis\" by showing that they can be used rigorously.\n\nThe tract \"A Discourse on Passive Obedience\" (1712) is considered Berkeley's major contribution to moral and political philosophy.\n\nIn \"A Discourse on Passive Obedience\", Berkeley defends the thesis that people have \"a moral duty to observe the negative precepts (prohibitions) of the law, including the duty not to resist the execution of punishment.\" However, Berkeley does make exceptions to this sweeping moral statement, stating that we need not observe precepts of \"usurpers or even madmen\" and that people can obey different supreme authorities if there are more than one claims to the highest authority.\n\nBerkeley defends this thesis with a deductive proof stemming from the laws of nature. First, he establishes that because God is perfectly good, the end to which he commands humans must also be good, and that end must not benefit just one person, but the entire human race. Because these commands—or laws—if practiced, would lead to the general fitness of humankind, it follows that they can be discovered by the right reason—for example, the law to never resist supreme power can be derived from reason because this law is \"the only thing that stands between us and total disorder\". Thus, these laws can be called the laws of nature, because they are derived from God—the creator of nature himself. \"These laws of nature include duties never to resist the supreme power, lie under oath...or do evil so that good may come of it.\"\n\nOne may view Berkeley’s doctrine on Passive Obedience as a kind of 'Theological Utilitarianism', insofar as it states that we have a duty to uphold a moral code which presumably is working towards the ends of promoting the good of humankind. However, the concept of 'ordinary' Utilitarianism is fundamentally different in that it \"makes utility the one and only \"ground\" of obligation\"—that is, Utilitarianism is concerned with whether particular actions are morally permissible in specific situations, while Berkeley’s doctrine is concerned with whether or not we should follow moral rules in any and all circumstances. Whereas Utilitarianism might, for example, justify a morally impermissible act in light of the specific situation, Berkeley’s doctrine of Passive Obedience holds that it is never morally permissible to not follow a moral rule, even when it seems like breaking that moral rule might achieve the happiest ends. Berkeley holds that even though sometimes, the consequences of an action in a specific situation might be bad, the general tendencies of that action benefits humanity.\n\nOther important sources for Berkeley's views on morality are \"Alciphron\" (1732), especially dialogues I–III, and the \"Discourse to Magistrates\" (1738).\" \"Passive Obedience\" is notable partly for containing one of the earliest statements of rule utilitarianism.\n\nGeorge Berkeley’s theory that matter does not exist comes from the belief that \"sensible things are those only which are immediately perceived by sense.\" Berkeley says in his book called The Principles of Human Knowledge that “the ideas of sense are stronger, livelier, and clearer than those of the imagination; and they are also steady, orderly and coherent” (Berkeley 18). From this we can tell that the things that we are perceiving are truly real rather than it just being a dream.\n\nAll knowledge comes from perception; what we perceive are ideas, not things in themselves; a thing in itself must be outside experience; so the world only consists of ideas and minds that perceive those ideas; a thing only exists so far as it perceives or is perceived. Through this we can see that consciousness is considered something that exists to Berkeley due to its ability to perceive. \"'To be,' said of the object, means to be perceived; 'to be,' said of the subject, means to perceive or \"esse est percipi.\" \" \nBerkeley’s ideas were somewhat controversial due to his argument's refuting Descartes' worldview, which was expanded upon by Locke, and the rejection of Berkeley’s form of empiricism by several philosophers of the seventeenth and eighteenth centuries. In Locke’s worldview, \"the world causes the perceptual ideas we have of it by the way it interacts with our senses.\" This contradicts with Berkeley's worldview because not only does it suggest the existence of physical causes in the world, but in fact there is no physical world beyond our ideas. The only causes that exist in Berkeley’s worldview are those that are a result of the use of the will.\n\nBerkeley’s theory relies heavily on his form of empiricism, which in turn relies heavily on the senses. His empiricism can be defined by five propositions: all significant words stand for ideas; all knowledge about our ideas; all ideas come from without or from within; if from without it must be by the senses, and they are called sensations, if from within they are the operations of the mind, and are called thoughts. Berkeley clarifies his distinction between ideas by saying they \"are imprinted on the senses,\" \"perceived by attending to the passions and operations of the mind,\" or \"are formed by help of memory and imagination.\" One refutation of his idea was: if someone leaves a room and stops perceiving that room does that room no longer exist? Berkeley answers this by claiming that it is still being perceived and the consciousness that is doing the perceiving is God. This claim is the only thing holding up his argument which is \"depending for our knowledge of the world, and of the existence of other minds, upon a God that would never deceive us.\" Berkeley’s argument hinges upon an omniscient, omnipresent deity.\n\nBerkeley's \"Treatise Concerning the Principles of Human Knowledge\" was published three years before the publication of Arthur Collier's \"Clavis Universalis\", which made assertions similar to those of Berkeley's. However, there seemed to have been no influence or communication between the two writers.\n\nGerman philosopher Arthur Schopenhauer once wrote of him: \"Berkeley was, therefore, the first to treat the subjective starting-point really seriously and to demonstrate irrefutably its absolute necessity. He is the father of idealism...\".\n\nGeorge Berkeley has gone down in the handbooks as a great spokesman of British empiricism.\n\nToday, every student of the history of philosophy is familiar with the view that there was a sort of linear development involving three great \"British Empiricists\", leading from Locke through Berkeley to Hume.\n\nBerkeley influenced many modern philosophers, especially David Hume. Thomas Reid admitted that he put forward a drastic criticism of Berkeleianism after he had been an admirer of Berkeley's philosophical system for a long time. Berkeley's \"thought made possible the work of Hume and thus Kant, notes Alfred North Whitehead.\" Some authors draw a parallel between Berkeley and Edmund Husserl.\n\nWhen Berkeley visited America, the American educator Samuel Johnson visited him, and the two later corresponded. Johnson convinced Berkeley to establish a scholarship program at Yale, and to donate a large number of books as well as his plantation to the college when the philosopher returned to England. It was one of Yale's largest and most important donations; it doubled its library holdings, improved the college's financial position and brought Anglican religious ideas and English culture into New England. Johnson also took Berkeley's philosophy and used parts of it as a framework for his own American Practical Idealism school of philosophy. As Johnson's philosophy was taught to about half the graduates of American colleges between 1743 and 1776, and over half of the contributors to the \"Declaration of Independence \"were connected to it, Berkeley's ideas were indirectly a foundation of the American Mind.\n\nOutside of America, during Berkeley's lifetime his philosophical ideas were comparatively uninfluential. But interest in his doctrine grew from the 1870s when Alexander Campbell Fraser, \"the leading Berkeley scholar of the nineteenth century\", published \"The Works of George Berkeley.\" A powerful impulse to serious studies in Berkeley's philosophy was given by A. A. Luce and Thomas Edmund Jessop, \"two of the twentieth century's foremost Berkeley scholars,\" thanks to whom Berkeley scholarship was raised to the rank of a special area of historico-philosophical science.\n\nThe proportion of Berkeley scholarship, in literature on the history of philosophy, is increasing. This can be judged from the most comprehensive bibliographies on George Berkeley. During the period of 1709–1932, about 300 writings on Berkeley were published. That amounted to 1.5 publication per annum. During the course of 1932–79, over one thousand works were brought out, i.e., 20 works per annum. Since then, the number of publications has reached 30 per annum. In 1977 publication began in Ireland of a special journal on Berkeley's life and thought (\"Berkeley Studies\").\n\nOther than philosophy, Berkeley also influenced modern psychology with his work on John Locke's theory of association and how it could be used to explain how humans gain knowledge in the physical world. He also used the theory to explain perception, stating that all qualities where, as Locke would call them, \"secondary qualities\" therefore perception laid entirely in the perceiver and not in the object. These are both topics today studied in modern psychology.\n\nThe University of California, Berkeley, was named after him, although the pronunciation has evolved to suit American English: ( ). The naming was suggested in 1866 by Frederick Billings, a trustee of the then College of California. Billings was inspired by Berkeley's \"Verses on the Prospect of Planting Arts and Learning in America\", particularly the final stanza: \"Westward the course of empire takes its way; The first four Acts already past, A fifth shall close the Drama with the day; Time's noblest offspring is the last.\"\n\nOn 18 April 1735, The Town of Berkley, currently the least populated town in Bristol County, Massachusetts, was founded and named after the renowned philosopher. Located 40 miles south of Boston and 25 miles north of Middletown (Rhode Island), where Berkeley lived at his farmhouse, Whitehall Museum House remains today to commemorate the farmhouse modified by Dean George Berkeley when he lived in the northern section of Newport, Rhode Island (that comprises present-day Middletown, Rhode Island) in 1729–31, while working to open his planned St Paul's College on the island of Bermuda. It is also known as Berkeley House (or Bishop George Berkeley House), and was listed on the National Register of Historic Places in 1970.[1]\n\nA residential college and an Episcopal seminary at Yale University also bear Berkeley's name, as does the Berkeley Library at Trinity College, Dublin.\n\nAlso named for him is Berkeley Preparatory School in Tampa, Florida. This leading private school is affiliated with the Episcopal Church, has almost 1300 students from pre-kindergarten through twelfth grade, and was founded in 1960.\n\nAn Ulster History Circle blue plaque commemorating him is located in Bishop Street Within, city of Derry.\n\n\"Bishop Berkeley's Gold Medals\" are two awards given annually at Trinity College Dublin, \"provided outstanding merit is shown\", to candidates answering a special examination in Greek. The awards were founded in 1752 by Berkeley who was a Fellow in 1707–24.\n\nBerkeley is honoured together with Joseph Butler with a feast day on the liturgical calendar of the Episcopal Church (USA) on 16 June.\n\n\n\n\n\n\n\"The Works of George Berkeley\". Ed. by Alexander Campbell Fraser. In 4 Volumes. Oxford: Clarendon Press, 1901.\nEwald, William B., ed., 1996. \"From Kant to Hilbert: A Source Book in the Foundations of Mathematics\", 2 vols. Oxford Uni. Press.\n\n\n\n"}
{"id": "158571", "url": "https://en.wikipedia.org/wiki?curid=158571", "title": "Harry Nyquist", "text": "Harry Nyquist\n\nHarry Nyquist (born Harry Theodor Nyqvist , ; February 7, 1889 – April 4, 1976) was a Swedish-born American electronic engineer who made important contributions to communication theory.\n\nNyquist was born in the village Nilsby of the parish Stora Kil \n, Värmland, Sweden. He was the son of Lars Jonsson Nyqvist (b. 1847) and Katrina Eriksdotter (b. 1857). His parents had eight children: Elin Teresia, Astrid, Selma, Harry Theodor, Amelie, Olga Maria, Axel Martin and Herta Alfrida. He emigrated to the USA in 1907.\n\nHe entered the University of North Dakota in 1912 and received B.S. and M.S. degrees in electrical engineering in 1914 and 1915, respectively. He received a Ph.D. in physics at Yale University in 1917.\n\nHe worked at AT&T's Department of Development and Research from 1917 to 1934, and continued when it became Bell Telephone Laboratories that year, until his retirement in 1954.\n\nNyquist received the IRE Medal of Honor in 1960 for \"fundamental contributions to a quantitative understanding of thermal noise, data transmission and negative feedback.\" \nIn October 1960 he was awarded the Stuart Ballantine Medal of the Franklin Institute \"for his theoretical analyses and practical inventions in the field of communications systems during the past forty years including, particularly, his original work in the theories of telegraph transmission, thermal noise in electric conductors, and in the history of feedback systems.\" \nIn 1969 he was awarded the National Academy of Engineering's fourth Founder's Medal \"in recognition of his many fundamental contributions to engineering.\"\nIn 1975 Nyquist received together with Hendrik Bode the Rufus Oldenburger Medal from the American Society of Mechanical Engineers.\n\nNyquist lived in Pharr, Texas after his retirement, and died in Harlingen, Texas on April 4, 1976.\n\nAs an engineer at Bell Laboratories, Nyquist did important work on thermal noise (\"Johnson–Nyquist noise\"), the stability of feedback amplifiers, telegraphy, facsimile, television, and other important communications problems. With Herbert E. Ives, he helped to develop AT&T's first facsimile machines that were made public in 1924. In 1932, he published a classic paper on stability of feedback amplifiers. The Nyquist stability criterion can now be found in all textbooks on feedback control theory.\n\nHis early theoretical work on determining the bandwidth requirements for transmitting information laid the foundations for later advances by Claude Shannon, which led to the development of information theory. In particular, Nyquist determined that the number of independent pulses that could be put through a telegraph channel per unit time is limited to twice the bandwidth of the channel, and published his results in the papers \"Certain factors affecting telegraph speed\" (1924) and \"Certain topics in Telegraph Transmission Theory\" (1928). This rule is essentially a dual of what is now known as the Nyquist–Shannon sampling theorem.\n\n\n"}
{"id": "2876834", "url": "https://en.wikipedia.org/wiki?curid=2876834", "title": "Heath–Jarrow–Morton framework", "text": "Heath–Jarrow–Morton framework\n\nThe Heath–Jarrow–Morton (HJM) framework is a general framework to model the evolution of interest rate curves – instantaneous forward rate curves in particular (as opposed to simple forward rates). When the volatility and drift of the instantaneous forward rate are assumed to be deterministic, this is known as the Gaussian Heath–Jarrow–Morton (HJM) model of forward rates. For direct modeling of simple forward rates the Brace–Gatarek–Musiela model represents an example.\n\nThe HJM framework originates from the work of David Heath, Robert A. Jarrow, and Andrew Morton in the late 1980s, especially \"Bond pricing and the term structure of interest rates: a new methodology\" (1987) – working paper, Cornell University, and \"Bond pricing and the term structure of interest rates: a new methodology\" (1989) – working paper (revised ed.), Cornell University. It has its critics, however, with Paul Wilmott describing it as \"...actually just a big rug for [mistakes] to be swept under\".\n\nThe key to these techniques is the recognition that the drifts of the no-arbitrage evolution of certain variables can be expressed as functions of their volatilities and the correlations among themselves. In other words, no drift estimation is needed.\n\nModels developed according to the HJM framework are different from the so-called short-rate models in the sense that HJM-type models capture the full dynamics of the entire forward rate curve, while the short-rate models only capture the dynamics of a point on the curve (the short rate).\n\nHowever, models developed according to the general HJM framework are often non-Markovian and can even have infinite dimensions. A number of researchers have made great contributions to tackle this problem. They show that if the volatility structure of the forward rates satisfy certain conditions, then an HJM model can be expressed entirely by a finite state Markovian system, making it computationally feasible. Examples include a one-factor, two state model (O. Cheyette, \"Term Structure Dynamics and Mortgage Valuation\", \"Journal of Fixed Income,\" 1, 1992; P. Ritchken and L. Sankarasubramanian in \"Volatility Structures of Forward Rates and the Dynamics of Term Structure\", \"Mathematical Finance\", 5, No. 1, Jan 1995), and later multi-factor versions.\n\nThe class of models developed by Heath, Jarrow and Morton (1992) is based on modelling the forward rates, yet it does not capture all of the complexities of an evolving term structure.\n\nThe model begins by introducing the instantaneous forward rate formula_1, formula_2, which is defined as the continuous compounding rate available at time formula_3 as seen from time formula_4. The relation between bond prices and the forward rate is also provided in the following way:\n\nHere formula_6 is the price at time formula_4 of a zero-coupon bond paying $1 at maturity formula_8. The risk-free money market account is also defined as\n\nThis last equation lets us define formula_10, the risk free short rate. The HJM framework assumes that the dynamics of formula_11 under a risk-neutral pricing measure formula_12 are the following:\n\nWhere formula_14 is a formula_15-dimensional Wiener process and formula_16, formula_17 are formula_18 adapted processes. Now based on these dynamics for formula_19, we'll attempt to find the dynamics for formula_20 and find the conditions that need to be satisfied under risk-neutral pricing rules. Let's define the following process:\n\nThe dynamics of formula_22 can be obtained through Leibniz's rule:\n\nIf we define formula_24, formula_25 and assume that the conditions for Fubini's Theorem are satisfied in the formula for the dynamics of formula_22, we get:\n\nBy Itō's lemma, the dynamics of formula_6 are then:\n\nBut formula_30 must be a martingale under the pricing measure formula_12, so we require that formula_32. Differentiating this with respect to formula_33. we get:\n\nWhich finally tells us that the dynamics of formula_19 must be of the following form:\n\nWhich allows us to price bonds and interest rate derivatives based on our choice of formula_37.\n\n\n\n"}
{"id": "1156776", "url": "https://en.wikipedia.org/wiki?curid=1156776", "title": "Implicant", "text": "Implicant\n\nIn Boolean logic, the term implicant has either a generic or a particular meaning. In the generic use, it refers to the hypothesis of an implication (wiktionary:implicant). In the particular use, it refers to specific instance of this generic meaning, which occurs relative to a formula that is either a sum of products or a product of sums, as explained further bolow..\n\nIt its particular use, an implicant is a \"covering\" (sum term or product term) of one or more minterms in a sum of products (or maxterms in product of sums) of a Boolean function. Formally, a product term \"P\" in a sum of products is an implicant of the Boolean function \"F\" if \"P\" implies \"F\". More precisely:\n\nwhere\n\nThis means that \"P\" formula_1 \"F\" with respect to the natural ordering of the Boolean space. For instance, the function\n\nis implied by formula_3, by formula_4, by formula_5, by formula_6 and many others; these are the implicants of formula_7.\n\nA prime implicant of a function is an implicant that cannot be covered by a more general, (more reduced - meaning with fewer literals) implicant. W. V. Quine defined a \"prime implicant\" of \"F\" to be an implicant that is minimal - that is, the removal of any literal from \"P\" results in a non-implicant for \"F\". Essential prime implicants (aka core prime implicants) are prime implicants that cover an output of the function that no combination of other prime implicants is able to cover.\n\nUsing the example above, one can easily see that while formula_3 (and others) is a prime implicant, formula_4 and formula_5 are not. From the latter, multiple literals can be removed to make it prime:\n\n\nThe process of removing literals from a Boolean term is called expanding the term. Expanding by one literal doubles the number of input combinations for which the term is true (in binary Boolean algebra). Using the example function above, we may expand formula_4 to formula_3 or to formula_20 without changing the cover of formula_7.\n\nThe sum of all prime implicants of a Boolean function is called its complete sum, minimal covering sum, or Blake canonical form.\n\n\n"}
{"id": "5957084", "url": "https://en.wikipedia.org/wiki?curid=5957084", "title": "Indeterminate system", "text": "Indeterminate system\n\nAn indeterminate system is a system of simultaneous equations (especially linear equations) which has more than one solution. The system may be said to be underspecified. If the system is linear, then the presence of more than one solution implies that there are an infinite number of solutions, but that property does not extend to nonlinear systems.\n\nAn indeterminate system is consistent, the latter implying that there exists at least one solution. For a system of linear equations, the number of equations in an indeterminate system could be the same as the number of unknowns, less than the number of unknowns (an underdetermined system), or greater than the number of unknowns (an overdetermined system). Conversely, any of those three cases may or may not be indeterminate.\n\nThe following examples of indeterminate systems have respectively fewer, the same, and more equations than unknowns:\n\nIn linear systems, indeterminacy occurs if and only if the number of independent equations (the rank of the augmented matrix of the system) is less than the number of unknowns and is the same as the rank of the coefficient matrix. For if there are at least as many independent equations as unknowns that will eliminate any stretches of overlap of the equations' surfaces in the geometric space of the unknowns (aside from possibly a single point), so there cannot be more than one solution; and if the rank of the augmented matrix exceeds (necessarily by one if at all) the rank of the coefficient matrix then the equations jointly contradict each other, so there are no solutions.\n\nLet the system of equations be written in matrix form as \n\nwhere \"A\" is the \"m\"×\"n\" coefficient matrix, \"x\" is the \"n\"×1 vector of unknowns, and \"b\" is an \"m\"×1 vector of constants. Then if the system is indeterminate, the infinite solution set is the set of all \"x\" vectors generated by\n\nwhere formula_6 is the Moore-Penrose pseudoinverse of \"A\" and \"w\" is any \"n\"×1 vector.\n\n\n"}
{"id": "261320", "url": "https://en.wikipedia.org/wiki?curid=261320", "title": "Initialized fractional calculus", "text": "Initialized fractional calculus\n\nIn mathematical analysis, initialization of the differintegrals is a topic in fractional calculus.\n\nA certain counterintuitive property of the differintegral operator should be pointed out, namely the composition law. Although\n\nwherein \"D\" is the left inverse of \"D\", the converse is not necessarily true:\n\nIt is instructive to consider elementary integer-order calculus to see what's happening. First, integrate then differentiate, using the example function 3\"x\" + 1:\n\non exchanging the order of composition:\n\nin which the constant of integration is \"c\". Even if it wasn't obvious, the initialization terms \"ƒ\"<nowiki>'</nowiki>(0) = \"c\", \"ƒ\"<nowiki>\"</nowiki>(0) = \"d\", etc. could be used. If we neglected those initialization terms, the last equation would show the composition of integration then differentiation (and vice versa) would not hold.\n\nThis is the problem that with the differintegral. If the differintegral is initialized properly, then the hoped-for composition law holds. The problem is that in differentiation, we lose information, as we lost the \"c\" in the first equation.\n\nIn fractional calculus, however, since the operator has been fractionalized and is thus continuous, an entire complementary function is needed, not just a constant or set of constants. We call this complementary function formula_5.\n\nWorking with a properly initialized differintegral is the subject of initialized fractional calculus.\n\n\n"}
{"id": "53451378", "url": "https://en.wikipedia.org/wiki?curid=53451378", "title": "International Symposium on Mathematical Foundations of Computer Science", "text": "International Symposium on Mathematical Foundations of Computer Science\n\nMFCS, the International Symposium on Mathematical Foundations of Computer Science is an academic conference organized annually since 1972. The topics of the conference cover the entire field of theoretical computer science. Up to 2012, the conference was held in different locations in Poland, Czech Republic and Slovakia, but since MFCS 2013, it travels around Europe. All contributions are strongly peer-reviewed. In 1974–2015, the articles appeared in proceedings published by Springer in the Lecture Notes in Computer Science series. Beginning in 2016, the proceedings are instead published by the Leibniz International Proceedings in Informatics.\n\nSince 2012, the Steering Committee of the MFCS symposia series is chaired by Antonín Kučera.\n"}
{"id": "29259913", "url": "https://en.wikipedia.org/wiki?curid=29259913", "title": "Iterated conditional modes", "text": "Iterated conditional modes\n\nIn statistics, iterated conditional modes is a deterministic algorithm for obtaining a configuration of a local maximum of the joint probability of a Markov random field. It does this by iteratively maximizing the probability of each variable conditioned on the rest.\n\n"}
{"id": "24771038", "url": "https://en.wikipedia.org/wiki?curid=24771038", "title": "Joseph Dauben", "text": "Joseph Dauben\n\nJoseph Warren Dauben (born 29 December 1944, Santa Monica) is a Herbert H. Lehman Distinguished Professor of History at the Graduate Center of the City University of New York. He obtained his Ph.D. from Harvard University.\n\nHis fields of expertise are history of science, history of mathematics, the scientific revolution, sociology of science, intellectual history, 17-18th centuries, history of Chinese science, and the history of botany.\n\nHis book \"Abraham Robinson\" was reviewed positively by Moshé Machover, but the review noted that it avoids discussing any of Robinson's negative aspects, and \"in this respect [the book] borders on the hagiographic, painting a portrait without warts.\"\n\nDauben is a 1980 Guggenheim Fellow.\n\nHe is a Fellow of the American Association for the Advancement of Science, and a Fellow of the New York Academy of Sciences (since 1982).\n\nDauben is an elected member (1991) of the International Academy of the History of Science. and an elected foreign member (2001) of German Academy of Sciences Leopoldina.\n\nDauben delivered an invited lecture at the 1998 International Congress of Mathematicians in Berlin on Karl Marx's mathematical work.\n\nIn 2002 Dauben became an honorary member of the Institute for History of Natural Science of the Chinese Academy of Sciences.\n\nIn 1985–1994 Dauben served as the Chair of the Executive Committee of the International Commission on the History of Mathematics.\n\n\n"}
{"id": "871687", "url": "https://en.wikipedia.org/wiki?curid=871687", "title": "Killing spinor", "text": "Killing spinor\n\nKilling spinor is a term used in mathematics and physics. By the more narrow definition, commonly used in mathematics, the term Killing spinor indicates those twistor\nspinors which are also eigenspinors of the Dirac operator. The term is named after Wilhelm Killing.\n\nAnother equivalent definition is that Killing spinors are the solutions to the Killing equation for a so-called Killing number.\n\nMore formally:\n\nIn physics, Killing spinors are used in supergravity and superstring theory, in particular for finding solutions which preserve some supersymmetry. They are a special kind of spinor field related to Killing vector fields and Killing tensors.\n\n\n"}
{"id": "313735", "url": "https://en.wikipedia.org/wiki?curid=313735", "title": "Knot polynomial", "text": "Knot polynomial\n\nIn the mathematical field of knot theory, a knot polynomial is a knot invariant in the form of a polynomial whose coefficients encode some of the properties of a given knot.\n\nThe first knot polynomial, the Alexander polynomial, was introduced by James Waddell Alexander II in 1923, but other knot polynomials were not found until almost 60 years later.\n\nIn the 1960s, John Conway came up with a skein relation for a version of the Alexander polynomial, usually referred to as the Alexander–Conway polynomial. The significance of this skein relation was not realized until the early 1980s, when Vaughan Jones discovered the Jones polynomial. This led to the discovery of more knot polynomials, such as the so-called HOMFLY polynomial.\n\nSoon after Jones' discovery, Louis Kauffman noticed the Jones polynomial could be computed by means of a state-sum model, which involved the bracket polynomial, an invariant of framed knots. This opened up avenues of research linking knot theory and statistical mechanics.\n\nIn the late 1980s, two related breakthroughs were made. Edward Witten demonstrated that the Jones polynomial, and similar Jones-type invariants, had an interpretation in Chern–Simons theory. Viktor Vassiliev and Mikhail Goussarov started the theory of finite type invariants of knots. The coefficients of the previously named polynomials are known to be of finite type (after perhaps a suitable \"change of variables\").\n\nIn recent years, the Alexander polynomial has been shown to be related to Floer homology. The graded Euler characteristic of the knot Floer homology of Ozsváth and Szabó is the Alexander polynomial.\n\nAlexander–Briggs notation is a notation that simply organizes knots by their crossing number.\nThe order of Alexander–Briggs notation of prime knot is usually sured. (See List of prime knots.)\n\nNotice that Alexander polynomial and Conway polynomial can \"not\" recognize the difference of left-trefoil knot and right-trefoil knot.\n\nSo the same situation as granny knot and square knot,since the addition of knots in formula_1 is the product of knots in knot polynomials.\n\n\n\n"}
{"id": "54141066", "url": "https://en.wikipedia.org/wiki?curid=54141066", "title": "Krivine machine", "text": "Krivine machine\n\nIn theoretical computer science, the Krivine machine is an \"abstract machine\" (sometimes called \"virtual machine\"). As an abstract machine, it shares features with Turing machines and the SECD machine. The Krivine machine explains how to compute a recursive function. More specifically it aims to define rigorously head normal form reduction of a lambda term using call-by-name reduction. Thanks to its formalism, it tells in details how a kind of reduction works and sets the theoretical foundation of the operational semantics of functional programming languages. On the other hand, Krivine machine implements call-by-name because it evaluates the body of a β-redex before it evaluates its parameter. In other words, in an expression (\"λ\" \"x\". \"t\") \"u\" it evaluates first \"λ\" \"x\". \"t\" before evaluating \"u\". In functional programming, this would mean that in order to evaluate a function applied to a parameter, it evaluates first the function before evaluating the parameter.\n\nThe Krivine machine has been designed by the French logician at the beginning of the 1980s.\n\nThe Krivine machine is based on two concepts related to lambda calculus, namely head reduction and call by name.\n\nA redex (one says also β-redex) is a term of the lambda calculus of the form (\"λ\" \"x\". \"t\") \"u\". If a term has the shape (\"λ\" \"x\". \"t\") \"u\" ... \"u\" it is said to be a \"head redex\". A \"head normal form\" is a term of the lambda calculus which is not a head redex A \"head reduction\" is a (non empty) sequence of contractions of a term which contracts head redexes. A head reduction of a term \"t\" (which is supposed not to be in head normal form) is a head reduction which starts from a term \"t\" and ends on a head normal form. From an abstract point of view, head reduction is the way a program computes when it evaluates a recursive sub-program. To understand how such a reduction can be implemented is important. One of the aims of the Krivine machine is to propose a process to reduct a term in head normal form and to describe formally this process. Like Turing used an abstract machine to describe formally the notion of algorithm, used an abstract machine to describe formally the notion of head normal form reduction.\n\nThe term ((\"λ\" 0) (\"λ\" 0)) (\"λ\" 0) (which corresponds, if one uses explicit variables, to the term (\"λx\".\"x\") (\"λy\".\"y\") (\"λz\".\"z\")) is not in head normal form because (\"λ\" 0) (\"λ\" 0) contracts in (\"λ\" 0) yielding the head redex (\"λ\" 0) (\"λ\" 0) which contracts in (\"λ\" 0) and which is therefore the head normal form of ((\"λ\" 0) (\"λ\" 0)) (\"λ\" 0). Said otherwise the head normal form contraction is:\nwhich corresponds to :\nWe will see further how the Krivine machine reduces the term ((\"λ\" 0) (\"λ\" 0)) (\"λ\" 0).\n\nTo implement the head reduction of a term \"u v\" which is an application, but which is not a redex, one must reduce the body \"u\" to exhibit an abstraction and therefore create a redex with \"v\". When a redex appears, one reduces it. To reduce always the body of an application first is called \"call by name\". The Krivine machine implements \"call by name\".\n\nThe presentation of the Krivine machine given here is based on notations of lambda terms that use de Bruijn indices and assumes that the terms of which it computes the head normal forms are closed. It modifies the current state until it cannot do it anymore, in which case it obtains a head normal form. This head normal form represents the result of the computation or yields an error, meaning that the term it started from is not correct. However, it can enter an infinite sequence of transitions, which means that the term it attempts reducing has no head normal form and corresponds to a non terminating computation.\n\nIt has been proved that the Krivine machine implements correctly the call by name head normal form reduction in the lambda-calculus. Moreover, the Krivine machine is deterministic, since each pattern of the state corresponds to at most one machine transition.\n\nThe state has three components\n\nThe term is a λ-term with de Bruijn indices. The stack and the environment belong to the same recursive data structure. More precisely, the environment and the stack are lists of pairs \"<term, environment>\", that are called \"closures\". In what follows, the insertion as the head of a list ℓ (stack or environment) of an element \"a\" is written \"a:ℓ\", whereas the empty list is written □. The stack is the location where the machine stores the closures that must be evaluated furthermore, whereas the environment is the association between the indices and the closures at a given time during the evaluation. The first element of the environment is the closure associated with the index \"0\", the second element corresponds to the closure associated with index \"1\" etc. If the machine has to evaluate an index, it fetches there the pair \"<term, environment>\" the closure that yields the term to be evaluated and the environment in which this term must be evaluated. This intuitive explanations allow understanding the operating rules of the machine. If one writes \"t\" for term, p for stack, and e for environment, the states associated with these three entities will be written \"t\", p, e. The rules explain how the machine transforms a state into another state, after identifying the patterns among the states.\n\nThe \"initial state\" aims to evaluate a term \"t\", it is the state \"t\",□,□, in which the term is \"t\" and the stack and the environment are empty. The \"final state\" (in absence of error) is of the form \"λ t\", □, e, in other words, the resulting terms is an abstraction together with its environment and an empty stack.\n\nThe Krivine machine has four transitions : App, Abs, Zero, Succ.\nThe transition App removes the parameter of an application and put it on the stack for further evaluation. The transition Abs removes the λ of the term and pop up the closure from the top of the stack and put it on the top of the environment. This closure corresponds to the de Bruijn index \"0\" in the new environment. The transition Zero takes the first closure of the environment. The term of this closure becomes the current term and the environment of this closure becomes the current environment. The transition Succ removes the first closure of the environment list and decreases the value of the index.\n\nLet us evaluate the term (\"λ\" 0 0) (\"λ\" 0) which corresponds to the term (\"λ\" \"x\". \"x\" \"x\") (\"λ\" \"x\". \"x\"). Let us start with the state (\"λ\" 0 0) (\"λ\" 0), □, □.\n\nThe conclusion is that the head normal form of the term (\"λ\" 0 0) (\"λ\" 0) is \"λ\" 0. This translates with variables in: the head normal form of the term (\"λ\" \"x\". \"x\" \"x\") (\"λ\" \"x\". \"x\") is \"λ\" \"x\". \"x\".\n\nLet us evaluate the term ((\"λ\" 0) (\"λ\" 0)) (\"λ\" 0)\nThis confirms the above fact that the normal form of the term ((\"λ\" 0) (\"λ\" 0)) (\"λ\" 0) is (\"λ\" 0).\n\n\nContent in this edit is translated from the existing French Wikipedia article at ; see its history for attribution.\n\n"}
{"id": "29426820", "url": "https://en.wikipedia.org/wiki?curid=29426820", "title": "Lantern relation", "text": "Lantern relation\n\nIn geometric topology, a branch of mathematics, the lantern relation is a relation that appears between certain Dehn twists in the mapping class group of a surface. The most general version of the relation involves seven Dehn twists. The relation was discovered by Dennis Johnson in 1979.\nThe general form of the lantern relation involves seven Dehn twists in the mapping class group of a disk with three holes, as shown in the figure on the right. According to the relation,\nwhere , , and are the right-handed Dehn twists around the blue curves , , and , and , , , are the right-handed Dehn twists around the four red curves.\n\nNote that the Dehn twists , , , on the right-hand side all commute (since the curves are disjoint, so the order in which they appear does not matter. However, the cyclic order of the three Dehn twists on the left does matter:\nAlso, note that the equalities written above are actually equality up to homotopy or isotopy, as is usual in the mapping class group.\n\nThough we have stated the lantern relation for a disk with three holes, the relation appears in the mapping class group of any surface in which such a disk can be embedded in a nontrivial way. Depending on the setting, some of the Dehn twists appearing in the lantern relation may be homotopic to the identity function, in which case the relation involves fewer than seven Dehn twists.\n\nThe lantern relation is used in several different presentations for the mapping class groups of surfaces.\n\n"}
{"id": "20914512", "url": "https://en.wikipedia.org/wiki?curid=20914512", "title": "Lateral computing", "text": "Lateral computing\n\nLateral computing is a lateral thinking approach to solving computing problems.\nLateral thinking has been made popular by Edward de Bono. This thinking technique is applied to generate creative ideas and solve problems. Similarly, by applying lateral-computing techniques to a problem, it can become much easier to arrive at a computationally inexpensive, easy to implement, efficient, innovative or unconventional solution.\n\nThe traditional or conventional approach to solving computing problems is to either build mathematical models or have an IF- THEN -ELSE structure. For example, a brute-force search is used in many chess engines, but this approach is computationally expensive and sometimes may arrive at poor solutions. It is for problems like this that lateral computing can be useful to form a better solution.\n\nA simple problem of truck backup can be used for illustrating lateral-computing. This is one of the difficult tasks for traditional computing techniques, and has been efficiently solved by the use of fuzzy logic (which is a lateral computing technique). Lateral-computing sometimes arrives at a novel solution for particular computing problem by using the model of how living beings, such as how humans, ants, and honeybees, solve a problem; how pure crystals are formed by annealing, or evolution of living beings or quantum mechanics etc.\n\nChess position analysis can be used to illustrate the logical thinking. The following board position describes a chess problem which has to be solved with two moves.\nThe white has several options to make a move and checkmate the black. The move Rd5 × Rd7 or Rf7 × Rd7 will immediately provide material advantage to white. There are similar moves which capture pieces and provide immediate material advantages to the white. But a knight move Nc6 which does not provide any material advantage, provides a solution for checkmate for black in two moves.\nThis is an example which illustrates the use of logical thinking. The logical thinking in chess progresses by evaluating the immediate material gain in each move. This will result in a solution which will require more number of moves or failure to checkmate. However, the not so obvious move of knight results in a very powerful checkmate. Even though this move does not look logical, it is the solution to two-move checkmate problem. A computer programmed to play chess might miss out some good opportunities if it does a material-based search to find moves. Several attempts have been made to build the powerful chess computers in history. But these chess computers have been defeated by Grandmaster human chess players.\n\nThe attempts to use logic programming such as prolog to represent knowledge and build artificial intelligent systems has not provided the anticipated thrust to solving interesting problems. The lack of generalization and learning capability of these systems and exponential growth of the IF-THEN ELSE rules has made this approach unpopular. An example to illustrate the failure of the rule-based system is the following flawed proof:\n\nThis would imply that 4 = 5, which a wrong result. While taking the square roots, the step of considering the signs has been missed. This has resulted in an absurd outcome. A rule-based system, even if it missed a simple rule in its database may yield such an unacceptable output.\n\nAnother interesting mathematical proof gone wrong is as follows:\n\nIf a = 1, then we get an absurd result of 1 = 2\n\nLateral thinking is technique for creative thinking for solving problems. The brain as center of thinking has a self-organizing information system. It tends to create patterns and traditional thinking process uses them to solve problems. The lateral thinking technique proposes to escape from this patterning to arrive at better solutions through new ideas. Provocative use of information processing is the basic underlying principle of lateral thinking,\n\nThe provocative operator (PO) is something which characterizes lateral thinking. Its function is to generate new ideas by provocation and providing escape route from old ideas. It creates a provisional arrangement of information.\n\nWater logic is contrast to traditional or rock logic. Water logic has boundaries which depends on circumstances and conditions while rock logic has hard boundaries. Water logic, in someways, resembles fuzzy logic.\n\nLateral computing does a provocative use of information processing similar to lateral-thinking. This is explained with the use of evolutionary computing which is a very useful lateral-computing technique. The evolution proceeds by change and selection. While random mutation provides change, the selection is through survival of the fittest. The random mutation works as a provocative information processing and provides a new avenue for generating better solutions for the computing problem.\n\nLateral computing takes the analogies from real-world examples such as:\n\n\nDifferentiating factors of \"lateral computing\":\n\n\nIt is very hard to draw a clear boundary between conventional and lateral computing. Over a period of time, some unconventional computing techniques become integral part of mainstream computing. So there will always be an overlap between conventional and lateral computing. It will be tough task classifying a computing technique as a conventional or lateral computing technique as shown in the figure. The boundaries are fuzzy and one may approach with fuzzy sets.\n\nLateral computing is a fuzzy set of all computing techniques which use unconventional computing approach. Hence Lateral computing includes those techniques which use semi-conventional or hybrid computing. The degree of membership for lateral computing techniques is greater than 0 in the fuzzy set of unconventional computing techniques.\n\nThe following brings out some important differentiators for lateral computing.\n\n\n\nParallel computing focuses on improving the performance of the computers/algorithms through the use of several computing elements (such as processing elements). The computing speed is improved by using several computing elements. Parallel computing is an extension of conventional sequential computing. However, in lateral computing, the problem is solved using unconventional information processing whether using a sequential or parallel computing.\n\nThere are several computing techniques which fit the Lateral computing paradigm. Here is a brief description of some of the Lateral Computing techniques:\n\nSwarm intelligence (SI) is the property of a system whereby the collective behaviors of (unsophisticated) agents, interacting locally with their environment, cause coherent functional global patterns to emerge. SI provides a basis with which it is possible to explore collective (or distributed) problem solving without centralized control or the provision of a global model.\n\nOne interesting swarm intelligent technique is the Ant Colony algorithm:\n\nAgents are encapsulated computer systems that are situated in some environment and are capable of flexible, autonomous action in that environment in order to meet their design objectives. Agents are considered to be autonomous (independent, not-controllable), reactive (responding to events), pro-active (initiating actions of their own volition), and social (communicative). Agents vary in their abilities: they can be static or mobile, or may or may not be intelligent. Each agent may have its own task and/or role. Agents, and multi-agent systems, are used as a metaphor to model complex distributed processes. Such agents invariably need to interact with one another in order to manage their inter-dependencies. These interactions involve agents cooperating, negotiating and coordinating with one another.\n\nAgent-based systems are computer programs that try to simulate various complex phenomena via virtual \"agents\" that represent the components of a business system. The behaviors of these agents are programmed with rules that realistically depict how business is conducted. As widely varied individual agents interact in the model, the simulation shows how their collective behaviors govern the performance of the entire system - for instance, the emergence of a successful product or an optimal schedule. These simulations are powerful strategic tools for \"what-if\" scenario analysis: as managers change agent characteristics or \"rules,\" the impact of the change can be easily seen in the model output\n\nBy analogy, a computational grid is a hardware and software infrastructure that provides dependable, consistent, pervasive, and inexpensive access to high-end computational capabilities. The applications of grid computing are in:\n\nThe autonomic nervous system governs our heart rate and body temperature, thus freeing our conscious brain from the burden of dealing with these and many other low-level, yet vital, functions. The essence of autonomic computing is self-management, the intent of which is to free system administrators from the details of system operation and maintenance.\n\nFour aspects of autonomic computing are:\n\nThis is a grand challenge promoted by IBM.\n\nOptical computing is to use photons rather than conventional electrons for computing. There are quite a few instances of optical computers and successful use of them. The conventional logic gates use semiconductors, which use electrons for transporting the signals. In case of optical computers, the photons in a light beam are used to do computation.\n\nThere are numerous advantages of using optical devices for computing such as immunity to electromagnetic interference, large bandwidth, etc.\n\nDNA computing uses strands of DNA to encode the instance of the problem and to manipulate them using techniques commonly available in any molecular biology laboratory in order to simulate operations that select the solution of the problem if it exists.\n\nSince the DNA molecule is also a code, but is instead made up of a sequence of four bases that pair up in a predictable manner, many scientists have thought about the possibility of creating a molecular computer. These computers rely on the much faster reactions of DNA nucleotides binding with their complements, a brute force method that holds enormous potential for creating a new generation of computers that would be 100 billion times faster than today's fastest PC. DNA computing has been heralded as the \"first example of true nanotechnology\", and even the \"start of a new era\", which forges an unprecedented link between computer science and life science.\n\nExample applications of DNA computing are in solution for the Hamiltonian path problem which is a known NP complete one. The number of required lab operations using DNA grows linearly with the number of vertices of the graph. Molecular algorithms have been reported that solves the cryptographic problem in a polynomial number of steps. As known, factoring large numbers is a relevant problem in many cryptographic applications.\n\nIn a quantum computer, the fundamental unit of information (called a quantum bit or qubit), is not binary but rather more quaternary in nature. This qubit property arises as a direct consequence of its adherence to the laws of quantum mechanics, which differ radically from the laws of classical physics. A qubit can exist not only in a state corresponding to the logical state 0 or 1 as in a classical bit, but also in states corresponding to a blend or quantum superposition of these classical states. In other words, a qubit can exist as a zero, a one, or simultaneously as both 0 and 1, with a numerical coefficient representing the probability for each state. A quantum computer manipulates qubits by executing a series of quantum gates, each a unitary transformation acting on a single qubit or pair of qubits. In applying these gates in succession, a quantum computer can perform a complicated unitary transformation to a set of qubits in some initial state.\n\nField-programmable gate arrays (FPGA) are making it possible to build truly reconfigurable computers. The computer architecture is transformed by on the fly reconfiguration of the FPGA circuitry. The optimal matching between architecture and algorithm improves the performance of the reconfigurable computer. The key feature is hardware performance and software flexibility.\n\nFor several applications such as fingerprint matching, DNA sequence comparison, etc., reconfigurable computers have been shown to perform several orders of magnitude better than conventional computers.\n\nThe Simulated annealing algorithm is designed by looking at how the pure crystals form from a heated gaseous state while the system is cooled slowly. The computing problem is redesigned as a simulated annealing exercise and the solutions are arrived at. The working principle of simulated annealing is borrowed from metallurgy: a piece of metal is heated (the atoms are given thermal agitation), and then the metal is left to cool slowly. The slow and regular cooling of the metal allows the atoms to slide progressively their most stable (\"minimal energy\") positions. (Rapid cooling would have \"frozen\" them in whatever position they happened to be at that time.) The resulting structure of the metal is stronger and more stable. By simulating the process of annealing inside a computer program, it is possible to find answers to difficult and very complex problems. Instead of minimizing the energy of a block of metal or maximizing its strength, the program minimizes or maximizes some objective relevant to the problem at hand.\n\nOne of the main components of \"Lateral-computing\" is soft computing which approaches problems with human information processing model. The Soft Computing technique comprises Fuzzy logic, neuro-computing, evolutionary-computing, machine learning and probabilistic-chaotic computing.\n\nInstead of solving a problem by creating a non-linear equation model of it, the biological neural network analogy is used for solving the problem. The neural network is trained like a human brain to solve a given problem. This approach has become highly successful in solving some of the pattern recognition problems.\n\nThe genetic algorithm (GA) resembles the natural evolution to provide a universal optimization. Genetic algorithms start with a population of chromosomes which represent the various solutions. The solutions are evaluated using a fitness function and a selection process determines which solutions are to be used for competition process. These algorithms are highly successful in solving search and optimization problems. The new solutions are created using evolutionary principles such as mutation and crossover.\n\nFuzzy logic is based on the fuzzy sets concepts proposed by Lotfi Zadeh. The degree of membership concept is central to fuzzy sets. The fuzzy sets differ from crisp sets since they allow an element to belong to a set to a degree (degree of membership). This approach finds good applications for control problems. The Fuzzy logic has found enormous applications and has already found a big market presence in consumer electronics such as washing machines, microwaves, mobile phones, Televisions, Camcoders etc.\n\nProbabilistic computing engines, e.g. use of probabilistic graphical model such as Bayesian network. Such computational techniques are referred to as randomization, yielding probabilistic algorithms. When interpreted as a physical phenomenon through classical statistical thermodynamics, such techniques lead to energy savings that are proportional to the probability p with which each primitive computational step is guaranteed to be correct (or equivalently to the probability of error, (1–p). Chaotic Computing is based on the chaos theory.\n\nFractal Computing are objects displaying self-similarity at different scales. Fractals generation involves small iterative algorithms. The fractals have dimensions greater than their topological dimensions. The length of the fractal is infinite and size of it cannot be measured. It is described by an iterative algorithm unlike a Euclidean shape which is given by a simple formula. There are several types of fractals and Mandelbrot sets are very popular.\n\nFractals have found applications in image processing, image compression music generation, computer games etc. Mandelbrot set is a fractal named after its creator. Unlike the other fractals, even though the Mandelbrot set is self-similar at magnified scales, the small scale details are not identical to the whole. I.e., the Mandelbrot set is infinitely complex. But the process of generating it is based on an extremely simple equation. The Mandelbrot set \"M\" is a collection of complex numbers. The numbers \"Z\" which belong to \"M\" are computed by iteratively testing the Mandelbrot equation. \"C\" is a constant. If the equation converges for chosen \"Z\", then \"Z\" belongs to \"M\".\nMandelbrot equation:\n\nA Randomized algorithm makes arbitrary choices during its execution. This allows a savings in execution time at the beginning of a program. The disadvantage of this method is the possibility that an incorrect solution will occur. A well-designed randomized algorithm will have a very high probability of returning a correct answer. The two categories of randomized algorithms are:\n\nConsider an algorithm to find the \"k\" element of an array. A deterministic approach would be to choose a pivot element near the median of the list and partition the list around that element. The randomized approach to this problem would be to choose a pivot at random, thus saving time at the beginning of the process. Like approximation algorithms, they can be used to more quickly solve tough NP-complete problems. An advantage over the approximation algorithms, however, is that a randomized algorithm will eventually yield an exact answer if executed enough times\n\nHuman beings/animals learn new skills, languages/concepts. Similarly, machine learning algorithms provide capability to generalize from training data. There are two classes of Machine Learning (ML):\n\n\nOne of the well known machine learning technique is Back Propagation Algorithm. This mimics how humans learn from examples. The training patterns are repeatedly presented to the network. The error is back propagated and the network weights are adjusted using gradient descent. The network converges through several hundreds of iterative computations.\n\nThis is another class of highly successful machine learning techniques successfully applied to tasks such as text classification, speaker recognition, image recognition etc.\n\nThere are several successful applications of lateral-computing techniques. Here is a small set of applications that illustrates lateral computing:\n\n\nAbove is a review of lateral-computing techniques. Lateral-computing is based on the lateral-thinking approach and applies unconventional techniques to solve computing problems. While, most of the problems are solved in conventional techniques, there are problems which require lateral-computing. Lateral-computing provides advantage of computational efficiency, low cost of implementation, better solutions when compared to conventional computing for several problems. The lateral-computing successfully tackles a class of problems by exploiting tolerance for imprecision, uncertainty and partial truth to achieve tractability, robustness and low solution cost. Lateral-computing techniques which use the human like information processing models have been classified as \"Soft Computing\" in literature.\n\nLateral-computing is valuable while solving numerous computing problems whose mathematical models are unavailable. They provide a way of developing innovative solutions resulting in smart systems with Very High Machine IQ (VHMIQ). This article has traced the transition from lateral-thinking to lateral-computing. Then several lateral-computing techniques have been described followed by their applications. Lateral-computing is for building new generation artificial intelligence based on unconventional processing.\n\n\n\n"}
{"id": "21353343", "url": "https://en.wikipedia.org/wiki?curid=21353343", "title": "Lindeberg's condition", "text": "Lindeberg's condition\n\nIn probability theory, Lindeberg's condition is a sufficient condition (and under certain conditions also a necessary condition) for the central limit theorem (CLT) to hold for a sequence of independent random variables. Unlike the classical CLT, which requires that the random variables in question have finite variance and be both independent and identically distributed, Lindeberg's CLT only requires that they have finite variance, satisfy Lindeberg's condition, and be independent. It is named after the Finnish mathematician Jarl Waldemar Lindeberg.\n\nLet formula_1 be a probability space, and formula_2, be \"independent\" random variables defined on that space. Assume the expected values formula_3 and variances formula_4 exist and are finite. Also let formula_5\n\nIf this sequence of independent random variables formula_6 satisfies Lindeberg's condition:\n\nfor all formula_8, where 1 is the indicator function, then the central limit theorem holds, i.e. the random variables\n\nconverge in distribution to a standard normal random variable as formula_10\n\nLindeberg's condition is sufficient, but not in general necessary (i.e. the inverse implication does not hold in general).\nHowever, if the sequence of independent random variables in question satisfies\n\nthen Lindeberg's condition is both sufficient and necessary, i.e. it holds if and only if the result of central limit theorem holds.\n\nBecause the Lindeberg condition implies formula_12 as formula_13, it guarantees that the contribution of any individual random variable formula_6 (formula_15) to the variance formula_16 is arbitrarily small, for sufficiently large values of formula_17.\n\n"}
{"id": "24562032", "url": "https://en.wikipedia.org/wiki?curid=24562032", "title": "List of important publications in cryptography", "text": "List of important publications in cryptography\n\nThis is a list of important publications in cryptography, organized by field.\n\nSome reasons why a particular publication might be regarded as important:\n\nDescription: Presented the index of coincidence method for codebreaking.\n\nDescription: The breaking of the Enigma.\n\nDescription: Almost nothing had been published in cryptography in several decades and very few non-government researchers were thinking about it. \"The Codebreakers\", a popular and non academic book, made many more people aware and contains a lot of technical information, although it requires careful reading to extract it. Its 1967 appearance was followed by the appearance of many papers over the next few years.\n\nDescription: The method of differential cryptanalysis.\n\nDescription: The method of linear cryptanalysis.\n\nDescription: Information theory based analysis of cryptography. The original form of this paper was a confidential Bell Labs report from 1945, not the one published.\n\nDescription: The paper provides a rigorous basis to encryption (e.g., partial information) and shows that it possible to equate the slightest cryptanalysis to solve a pure math problem. \nSecond, it introduces the notion of computational indistinguishability.\n\nDescription: This paper explains how to construct a zero-knowledge proof system for any language in NP.\n\nDescription: Feistel ciphers are a form of cipher of which DES is the most important. It would be hard to overestimate the importance of either Feistel or DES. Feistel pushed a transition from stream ciphers to block ciphers. Although most ciphers operate on streams, most of the important ciphers today are block ciphers at their core.\n\n\nDescription: DES is not only one of the most widely deployed ciphers in the world but has had a profound impact on the development of cryptography. Roughly a generation of cryptographers devoted much of their time to attacking and improving DES.\n\nDescription: This paper suggested public key cryptography and presented Diffie–Hellman key exchange. For more information about this work see: W.Diffie, M.E.Hellman, \"Privacy and Authentication: An Introduction to Cryptography\", in Proc. IEEE, Vol 67(3) Mar 1979, pp 397–427.\n\nDescription: In this paper (along with Loren M. Kohnfelder,\"Using Certificates for Key Distribution in a Public-Key Cryptosystem\", MIT Technical report 19 May 1978), Kohnfelder introduced certificates (signed messages containing public keys) which are the heart of all modern key management systems.\n\nDescription: This paper introduced a branch of public key cryptography, known as public key distribution systems. Merkle's work predated \"New directions in cryptography\" though it was published after it. The Diffie–Hellman key exchange is an implementation of such a Merkle system. Hellman himself has argued that a more correct name would be Diffie–Hellman–Merkle key exchange.\n\nDescription: The RSA encryption method. The first public-key encryption method.\n\nDescription: A safe method for sharing a secret.\n\nDescription: Introduced the adversarial model against which almost all cryptographic protocols are judged.\n\nDescription: This paper introduced the basic ideas of cryptographic protocols and showed how both secret-key and public-key encryption could be used to achieve authentication.\n\n\nDescription: The Kerberos authentication protocol, which allows individuals communicating over an insecure network to prove their identity to one another in a secure and practical manner.\n\nDescription: Network software in distributed systems.\n\n"}
{"id": "1508442", "url": "https://en.wikipedia.org/wiki?curid=1508442", "title": "Logarithmically concave function", "text": "Logarithmically concave function\n\nIn convex analysis, a non-negative function is logarithmically concave (or log-concave for short) if its domain is a convex set, and if it satisfies the inequality\nfor all and . If is strictly positive, this is equivalent to saying that the logarithm of the function, , is concave; that is,\nfor all and .\n\nExamples of log-concave functions are the 0-1 indicator functions of convex sets (which requires the more flexible definition), and the Gaussian function.\n\nSimilarly, a function is log-convex if it satisfies the reverse inequality\nfor all and .\n\n\n\n\n\n\nLog-concave distributions are necessary for a number of algorithms, e.g. adaptive rejection sampling. Every distribution with log-concave density is a maximum entropy probability distribution with specified mean \"μ\" and Deviation risk measure \"D\". \nAs it happens, many common probability distributions are log-concave. Some examples:\n\nNote that all of the parameter restrictions have the same basic source: The exponent of non-negative quantity must be non-negative in order for the function to be log-concave.\n\nThe following distributions are non-log-concave for all parameters:\n\nNote that the cumulative distribution function (CDF) of all log-concave distributions is also log-concave. However, some non-log-concave distributions also have log-concave CDF's:\n\nThe following are among the properties of log-concave distributions:\n\n\n"}
{"id": "43763081", "url": "https://en.wikipedia.org/wiki?curid=43763081", "title": "Logic for Programming, Artificial Intelligence and Reasoning", "text": "Logic for Programming, Artificial Intelligence and Reasoning\n\nThe International Conference on Logic for Programming, Artificial Intelligence and Reasoning (LPAR) is an academic conference aiming at discussing cutting-edge results in the fields of automated reasoning, computational logic, programming languages and their applications.\n\nIt grew out of the Russian Conferences on Logic Programming 1990 and 1991; the idea to organize the conference was largely due to Robert Kowalski who proposed to create the Russian Association for Logic Programming. The conference was renamed in 1992 to \"Logic Programming \"and Automated Reasoning\"\" (LPAR) to reflect its extended scope, due to considerable interest in automated reasoning in the Former Soviet Union. After a break from 1995 to 1998, LPAR continued in 1999 under the name \"Logic \"for\" Programming and Automated Reasoning\", to indicate an extension of its logic part beyond logic programming. In 2001, the name changed to \"Logic for Programming, Artificial \"Intelligence and\" Reasoning\".\n\nThe LPAR steering committee consists of Matthias Baaz, Chris Fermüller, Geoff Sutcliffe, and Andrei Voronkov (chair).\n\n"}
{"id": "56679890", "url": "https://en.wikipedia.org/wiki?curid=56679890", "title": "Lucia Caporaso", "text": "Lucia Caporaso\n\nLucia Caporaso is an Italian mathematician, holding a professorship in mathematics at Roma Tre University. Her research includes work in algebraic geometry, arithmetic geometry, tropical geometry and enumerative geometry.\n\nCaporaso earned a laurea from Sapienza University of Rome in 1989. She completed her Ph.D. at Harvard University in 1993. Her dissertation, \"On a Compactification of the Universal Picard Variety over the Moduli Space of Stable Curves\", was supervised by Joe Harris.\n\nShe became a Benjamin Pierce Assistant Professor of Mathematics at Harvard, a researcher at the University of Rome Tor Vergata, an assistant professor at the Massachusetts Institute of Technology, and an associate professor at the University of Sannio, before moving to Roma Tre as a professor in 2001. Since 2013, she has headed the Department of Mathematics and Physics at Roma Tre.\n\nCaporaso was the 1997 winner of the Bartolozzi Prize.\n\nShe is an invited speaker at the 2018 International Congress of Mathematicians, speaking in the section on algebraic and complex geometry.\n\n"}
{"id": "45284801", "url": "https://en.wikipedia.org/wiki?curid=45284801", "title": "Maria-Carme Calderer", "text": "Maria-Carme Calderer\n\nMaria-Carme Calderer is a professor of mathematics at University of Minnesota. Her research concerns applied mathematics.\n\nCalderer received her Ph.D. from Heriot-Watt University in 1980. She was a postdoctoral researcher at the Institute for Mathematics and its Applications from 1984 to 1987, first as a postdoctoral researcher, and then as a visiting professor. She worked at Penn State from 1989 until 2002, when she joined the faculty of University of Minnesota.\n\nIn 2000, Calderer received the Teresa Cohen Service Award from Penn State University.\n\nIn 2012, Calderer became a fellow of the American Mathematical Society.\n\nCalderer was raised in Barcelona, Spain. She is married to Douglas Arnold, professor of mathematics at University of Minnesota.\n\n"}
{"id": "8494021", "url": "https://en.wikipedia.org/wiki?curid=8494021", "title": "Mike Byster", "text": "Mike Byster\n\nMichael Byster (born March 5, 1959) is an American mathematician, mental calculator, and math educator. He worked as a commodity trader until he quit his job to devote himself to teaching children his methods. He has spoken at over 10,000 schools for free and continues to mentor kids. Mike is able to do many arithmetic problems in his head at very fast speeds. During a study done years ago, Byster was claimed to have one of the fastest mathematical minds in the world.\n\nByster was raised along with his older sister in the Chicago suburb of Skokie, Illinois. His parents Gloria and Dave encouraged his math shortcuts at a young age. He went to Niles North High School and then attended University of Illinois at Urbana–Champaign, graduating with a bachelor's degree in finance in 1981.\n\nByster used to work at the mercantile exchange, but after his cousin, a math teacher in a Chicago area high school, invited him to show the class his shortcuts for doing base 10 arithmetic, Byster quit his job to devote himself to teaching children his methods. After that, he continued to do shows for free to schools across the United States. In December 2003, he released the website Mike's Math, but this was discontinued in 2007.\nIn 2008, Byster produced the Brainetics math and memory system. Byster claims that Brainetics uses both sides of the brain to process and store information, allowing anyone to recall the information at a fast pace.\n\nMike appeared on ABC's 20/20 in 2007. From 2007 to 2010, he appeared on television shows multiple times. On January 21, 2010, Mike appeared on Oprah Radio's \"Gayle King\" show. Mike has appeared on The Shopping Channel in Canada and QVC in the United States multiple times. He appeared on Fox News June 8, 2011. He has also appeared on WGN in July 2011, Good Day New York in August 2011 and then Fox News Boston on October 27, 2011. He was on WBEZ's Afternoon Shift on September 27, 2013. In November 2016 Mike gave a TEDx talk entitled 'How to Think Like a Genius'. On June 26, 2017, Mike competed on the Fox TV competition series Superhuman. Byster won that week's competition and with that received $50,000 and the title of Superhuman. He is a monthly guest on Maryland radio talk show The Rude Awakening Show.\n\nByster got married in the early 1990s and has a son, Josh, to whom he has taught his methods. He currently resides in the northern suburbs of Chicago. Byster is Jewish.\n"}
{"id": "28150097", "url": "https://en.wikipedia.org/wiki?curid=28150097", "title": "Petersen family", "text": "Petersen family\n\nIn graph theory, the Petersen family is a set of seven undirected graphs that includes the Petersen graph and the complete graph \"K\". The Petersen family is named after Danish mathematician Julius Petersen, the namesake of the Petersen graph.\n\nAny of the graphs in the Petersen family can be transformed into any other graph in the family by Δ-Y or Y-Δ transforms, operations in which a triangle is replaced by a degree-three vertex or vice versa. These seven graphs form the forbidden minors for linklessly embeddable graphs, graphs that can be embedded into three-dimensional space in such a way that no two cycles in the graph are linked. They are also among the forbidden minors for the YΔY-reducible graphs.\n\nThe form of Δ-Y and Y-Δ transforms used to define the Petersen family is as follows:\nThese transformations are so called because of the Δ shape of a triangle in a graph and the Y shape of a degree-three vertex. Although these operations can in principle lead to multigraphs, that does not happen within the Petersen family. Because these operations preserve the number of edges in a graph, there are only finitely many graphs or multigraphs that can be formed from a single starting graph \"G\" by combinations of Δ-Y and Y-Δ transforms.\n\nThe Petersen family then consists of every graph that can be reached from the Petersen graph by a combination of Δ-Y and Y-Δ transforms. There are seven graphs in the family, including the complete graph \"K\" on six vertices, the eight-vertex graph formed by removing a single edge from the complete bipartite graph \"K\", and the seven-vertex complete tripartite graph \"K\".\n\nA minor of a graph \"G\" is another graph formed from \"G\" by contracting and removing edges. As the Robertson–Seymour theorem shows, many important families of graphs can be characterized by a finite set of forbidden minors: for instance, according to Wagner's theorem, the planar graphs are exactly the graphs that have neither the complete graph \"K\" nor the complete bipartite graph \"K\" as minors.\n\nNeil Robertson, Paul Seymour, and Robin Thomas used the Petersen family as part of a similar characterization of linkless embeddings of graphs, embeddings of a given graph into Euclidean space in such a way that every cycle in the graph is the boundary of a disk that is not crossed by any other part of the graph. Horst Sachs had previously studied such embeddings, shown that the seven graphs of the Petersen family do not have such embeddings, and posed the question of characterizing the linklessly embeddable graphs by forbidden subgraphs. Robertson et al. solved Sachs' question by showing that the linkless embeddable graphs are exactly the graphs that do not have a member of the Petersen family as a minor.\n\nThe Petersen family also form some of the forbidden minors for another family of graphs, the YΔY-reducible graphs. A connected graph is YΔY-reducible if it can be reduced to a single vertex by a sequence of steps, each of which is a Δ-Y or Y-Δ transform, the removal of a self-loop or multiple adjacency, the removal of a vertex with one neighbor, and the replacement of a vertex of degree two and its two neighboring edges by a single edge. For instance, the complete graph \"K\" can be reduced to a single vertex by a Y-Δ transform that turns it into a triangle with doubled edges, removal of the three doubled edges, a Δ-Y transform that turns it into the claw \"K\", and removal of the three degree-one vertices of the claw. Each of the Petersen family graphs forms a minimal forbidden minor for the family of YΔY-reducible graphs. However, Neil Robertson provided an example of an apex graph (a linkless embeddable graph formed by adding one vertex to a planar graph) that is not YΔY-reducible, showing that the YΔY-reducible graphs form a proper subclass of the linkless embeddable graphs and have additional forbidden minors. In fact, as Yaming Yu showed, there are at least 68,897,913,652 forbidden minors for the YΔY-reducible graphs beyond the seven of the Petersen family.\n"}
{"id": "22737733", "url": "https://en.wikipedia.org/wiki?curid=22737733", "title": "Polyadic algebra", "text": "Polyadic algebra\n\nPolyadic algebras (more recently called Halmos algebras) are algebraic structures introduced by Paul Halmos. They are related to first-order logic in a way analogous to the relationship between Boolean algebras and propositional logic (see Lindenbaum–Tarski algebra).\n\nThere are other ways to relate first-order logic to algebra, including Tarski's cylindric algebras (when equality is part of the logic) and Lawvere's functorial semantics (a categorical approach).\n\n"}
{"id": "757531", "url": "https://en.wikipedia.org/wiki?curid=757531", "title": "Prime (order theory)", "text": "Prime (order theory)\n\nIn mathematics, an element \"p\" of a partial order (P, ≤) is a meet prime element when \"p\" is the principal element of a principal prime ideal. Equivalently, if \"P\" is a lattice, \"p\" ≠ \"top\", and for all \"a\", \"b\" in \"P\", \n\n"}
{"id": "19281610", "url": "https://en.wikipedia.org/wiki?curid=19281610", "title": "Q-analysis", "text": "Q-analysis\n\nQ-analysis is a mathematical framework to describe and analyze set systems, or equivalently simplicial complexes. This idea was first introduced by Ronald Atkin in the early 1970s. Atkin was a British mathematician teaching at the University of Essex. Crediting the inspiration of his idea to Clifford Dowker’s paper (Homology Groups of Relations, Annals of Mathematics, 1952), he became interested in the algebra of relations in social structures. He tried to explain his idea in both mathematical and also accessible forms to both technical and general audience. His main ideas are reflected in \"The Mathematical Structure of Human Affairs\" (1974). That book covers the key ideas in q-analysis and its application to a wide range of examples, like analyzing game of chess, urban structures, politics at university, people and complexes, works of abstract art, and to physics. He contended that q-analysis can be considered as a powerful generalized method wherever we are dealing with relationships among sets.\n\nQ-analysis\n\nA simplex of vertices can be represented as a polyhedron in dimensions, so that for example a triangle of three vertices can be drawn on a plane of two dimensions and is accordingly called a 2-simplex. When simplices share vertices, the intersections of their vertex sets are themselves simplices of equal or lower dimension. For example, two triangles with two vertices in common share not only the two 0-simplex vertices but the 1-simplex line between them. The triangles are said to be both 1- and 0-connected because they share 1- and 0-dimensional faces.\n\nQ-analysis of a simplicial complex consists in stepping through all up to the dimension of the largest simplex and constructing for each a graph of the simplices that are -connected at each level, and in particular, determining how many connected components are present for each .\n\nQ-analysis can thus provide a rich summary of (literally) multi-faceted relationships between entities.\n\n\n\n"}
{"id": "25220", "url": "https://en.wikipedia.org/wiki?curid=25220", "title": "Quantum computing", "text": "Quantum computing\n\nQuantum computing is computing using quantum-mechanical phenomena, such as superposition and entanglement. A quantum computer is a device that performs quantum computing. Such a computer is different from binary digital electronic computers based on transistors. Whereas common digital computing requires that the data be encoded into binary digits (bits), each of which is always in one of two definite states (0 or 1), quantum computation uses quantum bits or qubits, which can be in superpositions of states. A quantum Turing machine is a theoretical model of such a computer and is also known as the universal quantum computer. The field of quantum computing was initiated by the work of Paul Benioff and Yuri Manin in 1980, Richard Feynman in 1982, and David Deutsch in 1985. \n\nLarge-scale quantum computers would theoretically be able to solve certain problems much more quickly than any classical computers that use even the best currently known algorithms, like integer factorization using Shor's algorithm (which is a quantum algorithm) and the simulation of quantum many-body systems. There exist quantum algorithms, such as Simon's algorithm, that run faster than any possible probabilistic classical algorithm.\nA classical computer could in principle (with exponential resources) simulate a quantum algorithm, as quantum computation does not violate the Church–Turing thesis. On the other hand, quantum computers may be able to efficiently solve problems which are not \"practically\" feasible on classical computers.\n\nA classical computer has a memory made up of bits, where each bit is represented by either a one or a zero. A quantum computer, on the other hand, maintains a sequence of qubits, which can represent a one, a zero, or any quantum superposition of those two qubit states; a pair of qubits can be in any quantum superposition of 4 states, and three qubits in any superposition of 8 states. In general, a quantum computer with formula_1 qubits can be in an arbitrary superposition of up to formula_2 different states simultaneously. (This compares to a normal computer that can only be in \"one\" of these formula_2 states at any one time).\n\nA quantum computer operates on its qubits using quantum gates and measurement (which also alters the observed state). An algorithm is composed of a fixed sequence of quantum logic gates and a problem is encoded by setting the initial values of the qubits, similar to how a classical computer works. The calculation usually ends with a measurement, collapsing the system of qubits into one of the formula_2 eigenstates, where each qubit is zero or one, decomposing into a classical state. The outcome can, therefore, be at most formula_1 classical bits of information (or, if the algorithm did not end with a measurement, the result is an unobserved quantum state).\n\nQuantum algorithms are often probabilistic, in that they provide the correct solution only with a certain known probability. Note that the term non-deterministic computing must not be used in that case to mean probabilistic (computing) because the term non-deterministic has a different meaning in computer science.\n\nAn example of an implementation of qubits of a quantum computer could start with the use of particles with two spin states: \"down\" and \"up\" (typically written formula_6 and formula_7, or formula_8 and formula_9). This is true because any such system can be mapped onto an effective spin-1/2 system.\n\nA quantum computer with a given number of qubits is fundamentally different from a classical computer composed of the same number of classical bits. For example, representing the state of an \"n\"-qubit system on a classical computer requires the storage of 2 complex coefficients, while to characterize the state of a classical \"n\"-bit system it is sufficient to provide the values of the \"n\" bits, that is, only \"n\" numbers. Although this fact may seem to indicate that qubits can hold exponentially more information than their classical counterparts, care must be taken not to overlook the fact that the qubits are only in a probabilistic superposition of all of their states. This means that when the final state of the qubits is measured, they will only be found in one of the possible configurations they were in before the measurement. It is generally incorrect to think of a system of qubits as being in one particular state before the measurement. Since the fact that they were in a superposition of states before the measurement was made directly affects the possible outcomes of the computation.\n\nTo better understand this point, consider a classical computer that operates on a three-bit register. If the exact state of the register at a given time is not known, it can be described as a probability distribution over the formula_10 different three-bit strings 000, 001, 010, 011, 100, 101, 110, and 111. If there is no uncertainty over its state, then it is in exactly one of these states with probability 1. However, if it is a probabilistic computer, then there is a possibility of it being in any \"one\" of a number of different states.\n\nThe state of a three-qubit quantum computer is similarly described by an eight-dimensional vector formula_11 (or a one dimensional vector with\neach vector node holding the amplitude and the state as the bit string of qubits). Here, however, the coefficients formula_12 are complex numbers, and it is the sum of the \"squares\" of the coefficients' absolute values, formula_13, that must equal 1. For each formula_14, the absolute value squared formula_15 gives the probability of the system being found in the formula_14-th state after a measurement. However, because a complex number encodes not just a magnitude but also a direction in the complex plane, the phase difference between any two coefficients (states) represents a meaningful parameter. This is a fundamental difference between quantum computing and probabilistic classical computing.\n\nIf you measure the three qubits, you will observe a three-bit string. The probability of measuring a given string is the squared magnitude of that string's coefficient (i.e., the probability of measuring 000 = formula_17, the probability of measuring 001 = formula_18, etc.). Thus, measuring a quantum state described by complex coefficients formula_11 gives the classical probability distribution formula_20 and we say that the quantum state \"collapses\" to a classical state as a result of making the measurement.\n\nAn eight-dimensional vector can be specified in many different ways depending on what basis is chosen for the space. The basis of bit strings (e.g., 000, 001, …, 111) is known as the computational basis. Other possible bases are unit-length, orthogonal vectors and the eigenvectors of the Pauli-x operator. Ket notation is often used to make the choice of basis explicit. For example, the state formula_11 in the computational basis can be written as:\n\nThe computational basis for a single qubit (two dimensions) is formula_24 and formula_25.\n\nUsing the eigenvectors of the Pauli-x operator, a single qubit is formula_26 and formula_27.\n\nWhile a classical 3-bit state and a quantum 3-qubit state are each eight-dimensional vectors, they are manipulated quite differently for classical or quantum computation. For computing in either case, the system must be initialized, for example into the all-zeros string, formula_28, corresponding to the vector (1,0,0,0,0,0,0,0). In classical randomized computation, the system evolves according to the application of stochastic matrices, which preserve that the probabilities add up to one (i.e., preserve the L1 norm). In quantum computation, on the other hand, allowed operations are unitary matrices, which are effectively rotations (they preserve that the sum of the squares add up to one, the Euclidean or L2 norm). (Exactly what unitaries can be applied depend on the physics of the quantum device.) Consequently, since rotations can be undone by rotating backward, quantum computations are reversible. (Technically, quantum operations can be probabilistic combinations of unitaries, so quantum computation really does generalize classical computation. See quantum circuit for a more precise formulation.)\n\nFinally, upon termination of the algorithm, the result needs to be read off. In the case of a classical computer, we \"sample\" from the probability distribution on the three-bit register to obtain one definite three-bit string, say 000. Quantum mechanically, one \"measures\" the three-qubit state, which is equivalent to collapsing the quantum state down to a classical distribution (with the coefficients in the classical state being the squared magnitudes of the coefficients for the quantum state, as described above), followed by sampling from that distribution. This destroys the original quantum state. Many algorithms will only give the correct answer with a certain probability. However, by repeatedly initializing, running and measuring the quantum computer's results, the probability of getting the correct answer can be increased. In contrast, counterfactual quantum computation allows the correct answer to be inferred when the quantum computer is not actually running in a technical sense, though earlier initialization and frequent measurements are part of the counterfactual computation protocol.\n\nFor more details on the sequences of operations used for various quantum algorithms, see universal quantum computer, Shor's algorithm, Grover's algorithm, Deutsch–Jozsa algorithm, amplitude amplification, quantum Fourier transform, quantum gate, quantum adiabatic algorithm and quantum error correction.\n\nInteger factorization, which underpins the security of public key cryptographic systems, is believed to be computationally infeasible with an ordinary computer for large integers if they are the product of few prime numbers (e.g., products of two 300-digit primes). By comparison, a quantum computer could efficiently solve this problem using Shor's algorithm to find its factors. This ability would allow a quantum computer to break many of the cryptographic systems in use today, in the sense that there would be a polynomial time (in the number of digits of the integer) algorithm for solving the problem. In particular, most of the popular public key ciphers are based on the difficulty of factoring integers or the discrete logarithm problem, both of which can be solved by Shor's algorithm. In particular, the RSA, Diffie–Hellman, and elliptic curve Diffie–Hellman algorithms could be broken. These are used to protect secure Web pages, encrypted email, and many other types of data. Breaking these would have significant ramifications for electronic privacy and security.\n\nHowever, other cryptographic algorithms do not appear to be broken by those algorithms. Some public-key algorithms are based on problems other than the integer factorization and discrete logarithm problems to which Shor's algorithm applies, like the McEliece cryptosystem based on a problem in coding theory. Lattice-based cryptosystems are also not known to be broken by quantum computers, and finding a polynomial time algorithm for solving the dihedral hidden subgroup problem, which would break many lattice based cryptosystems, is a well-studied open problem. It has been proven that applying Grover's algorithm to break a symmetric (secret key) algorithm by brute force requires time equal to roughly 2 invocations of the underlying cryptographic algorithm, compared with roughly 2 in the classical case, meaning that symmetric key lengths are effectively halved: AES-256 would have the same security against an attack using Grover's algorithm that AES-128 has against classical brute-force search (see Key size). Quantum cryptography could potentially fulfill some of the functions of public key cryptography. Quantum-based cryptographic systems could ,therefore, be more secure than traditional systems against quantum hacking.\n\nBesides factorization and discrete logarithms, quantum algorithms offering a more than polynomial speedup over the best known classical algorithm have been found for several problems, including the simulation of quantum physical processes from chemistry and solid state physics, the approximation of Jones polynomials, and solving Pell's equation. No mathematical proof has been found that shows that an equally fast classical algorithm cannot be discovered, although this is considered unlikely. For some problems, quantum computers offer a polynomial speedup. The most well-known example of this is \"quantum database search\", which can be solved by Grover's algorithm using quadratically fewer queries to the database than that are required by classical algorithms. In this case, the advantage is not only provable but also optimal, it has been shown that Grover's algorithm gives the maximal possible probability of finding the desired element for any number of oracle lookups. Several other examples of provable quantum speedups for query problems have subsequently been discovered, such as for finding collisions in two-to-one functions and evaluating NAND trees.\n\nProblems that can be addressed with Grover's algorithm have the following properties:\n\nFor problems with all these properties, the running time of Grover's algorithm on a quantum computer will scale as the square root of the number of inputs (or elements in the database), as opposed to the linear scaling of classical algorithms. A general class of problems to which Grover's algorithm can be applied is Boolean satisfiability problem. In this instance, the \"database\" through which the algorithm is iterating is that of all possible answers. An example (and possible) application of this is a password cracker that attempts to guess the password or secret key for an encrypted file or system. Symmetric ciphers such as Triple DES and AES are particularly vulnerable to this kind of attack. This application of quantum computing is a major interest of government agencies.\n\nSince chemistry and nanotechnology rely on understanding quantum systems, and such systems are impossible to simulate in an efficient manner classically, many believe quantum simulation will be one of the most important applications of quantum computing. Quantum simulation could also be used to simulate the behavior of atoms and particles at unusual conditions such as the reactions inside a collider.\n\nAdiabatic quantum computation relies on the adiabatic theorem to undertake calculations. A system is placed in the ground state for a simple Hamiltonian, which is slowly evolved to a more complicated Hamiltonian whose ground state represents the solution to the problem in question. The adiabatic theorem states that if the evolution is slow enough the system will stay in its ground state at all times through the process.\n\nThe Quantum algorithm for linear systems of equations or \"HHL Algorithm\", named after its discoverers Harrow, Hassidim, and Lloyd, is expected to provide speedup over classical counterparts.\n\nJohn Preskill has introduced the term \"quantum supremacy\" to refer to the hypothetical speedup advantage that a quantum computer would have over a classical computer in a certain field. Google announced in 2017 that it expected to achieve quantum supremacy by the end of the year, and IBM says that the best classical computers will be beaten on some task within about five years. Quantum supremacy has not been achieved yet, and skeptics like Gil Kalai doubt that it will ever be. Bill Unruh doubted the practicality of quantum computers in a paper published back in 1994. Paul Davies pointed out that a 400-qubit computer would even come into conflict with the cosmological information bound implied by the holographic principle. Those such as Roger Schlafly have pointed out that the claimed theoretical benefits of quantum computing go beyond the proven theory of quantum mechanics and imply non-standard interpretations, such as the many-worlds interpretation and negative probabilities. Schlafly maintains that the Born rule is just \"metaphysical fluff\" and that quantum mechanics does not rely on probability any more than other branches of science but simply calculates the expected values of observables. He also points out that arguments about Turing complexity cannot be run backwards. Those who prefer Bayesian interpretations of quantum mechanics have questioned the physical nature of the mathematical abstractions employed.\n\nThere are a number of technical challenges in building a large-scale quantum computer, and thus far quantum computers have yet to solve a problem faster than a classical computer. David DiVincenzo, of IBM, listed the following requirements for a practical quantum computer:\n\nOne of the greatest challenges is controlling or removing quantum decoherence. This usually means isolating the system from its environment as interactions with the external world cause the system to decohere. However, other sources of decoherence also exist. Examples include the quantum gates, and the lattice vibrations and background thermonuclear spin of the physical system used to implement the qubits. Decoherence is irreversible, as it is effectively non-unitary, and is usually something that should be highly controlled, if not avoided. Decoherence times for candidate systems in particular, the transverse relaxation time \"T\" (for NMR and MRI technology, also called the \"dephasing time\"), typically range between nanoseconds and seconds at low temperature. Currently, some quantum computers require their qubits to be cooled to 20 millikelvins in order to prevent significant decoherence.\n\nAs a result, time-consuming tasks may render some quantum algorithms inoperable, as maintaining the state of qubits for a long enough duration will eventually corrupt the superpositions.\n\nThese issues are more difficult for optical approaches as the timescales are orders of magnitude shorter and an often-cited approach to overcoming them is optical pulse shaping. Error rates are typically proportional to the ratio of operating time to decoherence time, hence any operation must be completed much more quickly than the decoherence time.\n\nAs described in the Quantum threshold theorem, if the error rate is small enough, it is thought to be possible to use quantum error correction to suppress errors and decoherence. This allows the total calculation time to be longer than the decoherence time if the error correction scheme can correct errors faster than decoherence introduces them. An often cited figure for the required error rate in each gate for fault-tolerant computation is 10, assuming the noise is depolarizing.\n\nMeeting this scalability condition is possible for a wide range of systems. However, the use of error correction brings with it the cost of a greatly increased number of required qubits. The number required to factor integers using Shor's algorithm is still polynomial, and thought to be between \"L\" and \"L\", where \"L\" is the number of qubits in the number to be factored; error correction algorithms would inflate this figure by an additional factor of \"L\". For a 1000-bit number, this implies a need for about 10 bits without error correction. With error correction, the figure would rise to about 10 bits. Computation time is about \"L\" or about 10 steps and at 1 MHz, about 10 seconds.\n\nA very different approach to the stability-decoherence problem is to create a topological quantum computer with anyons, quasi-particles used as threads and relying on braid theory to form stable logic gates.\n\nThere are a number of quantum computing models, distinguished by the basic elements in which the computation is decomposed. The four main models of practical importance are:\nThe quantum Turing machine is theoretically important but the direct implementation of this model is not pursued. All four models of computation have been shown to be equivalent; each can simulate the other with no more than polynomial overhead.\n\nFor physically implementing a quantum computer, many different candidates are being pursued, among them (distinguished by the physical system used to realize the qubits):\n\n\nA large number of candidates demonstrates that the topic, in spite of rapid progress, is still in its infancy. There is also a vast amount of flexibility.\n\nIn 1959 Richard Feynman in his lecture \"There's Plenty of Room at the Bottom\" states the possibility of using quantum effects for computation.\n\nIn 1980 Paul Benioff described quantum mechanical Hamiltonian models of computers and the Russian mathematician Yuri Manin motivated the development of quantum computers.\n\nIn 1981, at a conference co-organized by MIT and IBM, physicist Richard Feynman urged the world to build a quantum computer. He said, \"Nature isn't classical, dammit, and if you want to make a simulation of nature, you'd better make it quantum mechanical, and by golly, it's a wonderful problem because it doesn't look so easy.\"\n\nIn 1984, BB84 is published, the world's first quantum cryptography protocol by IBM scientists Charles Bennett and Gilles Brassard.\n\nIn 1993, an international group of six scientists, including Charles Bennett, showed that perfect quantum teleportation is possible in principle, but only if the original is destroyed.\n\nIn 1994 Peter Shor, at AT&T's Bell Labs discovered an important quantum algorithm, which allows a quantum computer to factor large integers exponentially much faster than the best known classical algorithm. Shor's algorithm can theoretically break many of the public-key cryptosystems in use today. Its invention sparked a tremendous interest in quantum computers.\n\nIn 1996, The DiVincenzo's criteria are published which is a list of conditions that are necessary for constructing a quantum computer proposed by the theoretical physicist David P. DiVincenzo in his 2000 paper \"The Physical Implementation of Quantum Computation\".\n\nIn 2001, researchers demonstrated Shor's algorithm to factor 15 using a 7-qubit NMR computer.\n\nIn 2005, researchers at the University of Michigan built a semiconductor chip ion trap. Such devices from standard lithography may point the way to scalable quantum computing.\n\nIn 2009, researchers at Yale University created the first solid-state quantum processor. The two-qubit superconducting chip had artificial atom qubits made of a billion aluminum atoms that acted like a single atom that could occupy two states.\n\nA team at the University of Bristol also created a silicon chip based on quantum optics, able to run Shor's algorithm.\nFurther developments were made in 2010.\nSpringer publishes a journal (\"Quantum Information Processing\") devoted to the subject.\n\nIn February 2010, Digital Combinational Circuits like an adder, subtractor etc. are designed with the help of Symmetric Functions organized from different quantum gates.\n\nIn April 2011, a team of scientists from Australia and Japan made a breakthrough in quantum teleportation. They successfully transferred a complex set of quantum data with full transmission integrity, without affecting the qubits' superpositions.\n\nIn 2011, D-Wave Systems announced the first commercial quantum annealer, the D-Wave One, claiming a 128 qubit processor. On May 25, 2011, Lockheed Martin agreed to purchase a D-Wave One system. Lockheed and the University of Southern California (USC) will house the D-Wave One at the newly formed USC Lockheed Martin Quantum Computing Center. D-Wave's engineers designed the chips with an empirical approach, focusing on solving particular problems. Investors liked this more than academics, who said D-Wave had not demonstrated they really had a quantum computer. Criticism softened after a D-Wave paper in \"Nature\", that proved the chips have some quantum properties. Two published papers have suggested that the D-Wave machine's operation can be explained classically, rather than requiring quantum models. Later work showed that classical models are insufficient when all available data is considered. Experts remain divided on the ultimate classification of the D-Wave systems though their quantum behavior was established concretely with a demonstration of entanglement.\n\nDuring the same year, researchers at the University of Bristol created an all-bulk optics system that ran a version of Shor's algorithm to successfully factor 21.\n\nIn September 2011 researchers proved quantum computers can be made with a Von Neumann architecture (separation of RAM).\n\nIn November 2011 researchers factorized 143 using 4 qubits.\n\nIn February 2012 IBM scientists said that they had made several breakthroughs in quantum computing with superconducting integrated circuits.\n\nIn April 2012 a multinational team of researchers from the University of Southern California, Delft University of Technology, the Iowa State University of Science and Technology, and the University of California, Santa Barbara, constructed a two-qubit quantum computer on a doped diamond crystal that can easily be scaled up and is functional at room temperature. Two logical qubit directions of electron spin and nitrogen kernels spin were used, with microwave impulses. This computer ran Grover's algorithm generating the right answer from the first try in 95% of cases.\n\nIn September 2012, Australian researchers at the University of New South Wales said the world's first quantum computer was just 5 to 10 years away, after announcing a global breakthrough enabling the manufacture of its memory building blocks. A research team led by Australian engineers created the first working qubit based on a single atom in silicon, invoking the same technological platform that forms the building blocks of modern-day computers.\n\nIn October 2012, Nobel Prizes were presented to David J. Wineland and Serge Haroche for their basic work on understanding the quantum world, which may help make quantum computing possible.\n\nIn November 2012, the first quantum teleportation from one macroscopic object to another was reported by scientists at the University of Science and Technology of China in Hefei.\n\nIn December 2012, the first dedicated quantum computing software company, 1QBit was founded in Vancouver, BC. 1QBit is the first company to focus exclusively on commercializing software applications for commercially available quantum computers, including the D-Wave Two. 1QBit's research demonstrated the ability of superconducting quantum annealing processors to solve real-world problems.\n\nIn February 2013, a new technique, boson sampling, was reported by two groups using photons in an optical lattice that is not a universal quantum computer but may be good enough for practical problems. \"Science\" Feb 15, 2013\n\nIn May 2013, Google announced that it was launching the Quantum Artificial Intelligence Lab, hosted by NASA Ames Research Center, with a 512-qubit D-Wave quantum computer. The USRA (Universities Space Research Association) will invite researchers to share time on it with the goal of studying quantum computing for machine learning. Google added that they had \"already developed some quantum machine learning algorithms\" and had \"learned some useful principles\", such as that \"best results\" come from \"mixing quantum and classical computing\".\n\nIn early 2014 it was reported, based on documents provided by former NSA contractor Edward Snowden, that the U.S. National Security Agency (NSA) is running a $79.7 million research program (titled \"Penetrating Hard Targets\") to develop a quantum computer capable of breaking vulnerable encryption.\n\nIn 2014, a group of researchers from ETH Zürich, USC, Google ,and Microsoft reported a definition of quantum speedup, and were not able to measure quantum speedup with the D-Wave Two device, but did not explicitly rule it out.\n\nIn 2014, researchers at University of New South Wales used silicon as a protectant shell around qubits, making them more accurate, increasing the length of time they will hold information, and possibly making quantum computers easier to build.\n\nIn April 2015 IBM scientists claimed two critical advances towards the realization of a practical quantum computer. They claimed the ability to detect and measure both kinds of quantum errors simultaneously, as well as a new, square quantum bit circuit design that could scale to larger dimensions.\n\nIn October 2015, QuTech successfully conducts the Loophole-free Bell inequality violation test using electron spins separated by 1.3 kilometres.\n\nIn October 2015 researchers at the University of New South Wales built a quantum logic gate in silicon for the first time.\n\nIn December 2015 NASA publicly displayed the world's first fully operational $15-million quantum computer made by the Canadian company D-Wave at the Quantum Artificial Intelligence Laboratory at its Ames Research Center in California's Moffett Field. The device was purchased in 2013 via a partnership with Google and Universities Space Research Association. The presence and use of quantum effects in the D-Wave quantum processing unit is more widely accepted. In some tests, it can be shown that the D-Wave quantum annealing processor outperforms Selby’s algorithm. Only two of this computer have been made so far.\n\nIn May 2016, IBM Research announced that for the first time ever it is making quantum computing available to members of the public via the cloud, who can access and run experiments on IBM’s quantum processor. The service is called the IBM Quantum Experience. The quantum processor is composed of five superconducting qubits and is housed at the IBM T. J. Watson Research Center in New York.\n\nIn August 2016, scientists at the University of Maryland successfully built the first reprogrammable quantum computer.\n\nIn October 2016 Basel University described a variant of the electron-hole based quantum computer, which instead of manipulating\nelectron spins uses electron holes in a semiconductor at low (mK) temperatures which are a lot less vulnerable to decoherence. \nThis has been dubbed the \"positronic\" quantum computer as the quasi-particle behaves like it has a positive electrical charge.\n\nIn March 2017, IBM announced an industry-first initiative to build commercially available universal quantum computing systems called IBM Q. The company also released a new API (Application Program Interface) for the IBM Quantum Experience that enables developers and programmers to begin building interfaces between its existing five quantum bit (qubit) cloud-based quantum computer and classical computers, without needing a deep background in quantum physics.\n\nIn May 2017, IBM announced that it has successfully built and tested its most powerful universal quantum computing processors. The first is a 16 qubit processor that will allow for more complex experimentation than the previously available 5 qubit processor. The second is IBM's first prototype commercial processor with 17 qubits and leverages significant materials, device, and architecture improvements to make it the most powerful quantum processor created to date by IBM.\n\nIn July 2017, a group of U.S. researchers announced a quantum simulator with 51 qubits. The announcement was made by Mikhail Lukin of Harvard University at the International Conference on Quantum Technologies in Moscow. A quantum simulator differs from a computer. Lukin’s simulator was designed to solve one equation. Solving a different equation would require building a new system. A computer can solve many different equations.\n\nIn September 2017, IBM Research scientists use a 7 qubit device to model the largest molecule, Beryllium hydride, ever by a quantum computer. The results were published as the cover story in the peer-reviewed journal Nature.\n\nIn October 2017, IBM Research scientists successfully \"broke the 49-qubit simulation barrier\" and simulated 49- and 56-qubit short-depth circuits, using the Lawrence Livermore National Laboratory's Vulcan supercomputer, and the University of Illinois' Cyclops Tensor Framework (originally developed at the University of California). The results were published in arxiv.\n\nIn November 2017, the University of Sydney research team in Australia successfully made a microwave circulator, an important quantum computer part, 1000 times smaller than a conventional circulator by using topological insulators to slow down the speed of light in a material.\n\nIn December 2017, IBM announced its first IBM Q Network clients. The companies, universities, and labs to explore practical quantum applications, using IBM Q 20 qubit commercial systems, for business and science include: JPMorgan Chase, Daimler AG, Samsung, JSR Corporation, Barclays, Hitachi Metals, Honda, Nagase, Keio University, Oak Ridge National Lab, Oxford University and University of Melbourne.\n\nIn December 2017, Microsoft released a preview version of a \"Quantum Development Kit\". It includes a programming language, Q#, which can be used to write programs that are run on an emulated quantum computer.\n\nIn 2017 D-Wave reported to start selling a 2000 qubit quantum computer.\n\nIn late 2017 and early 2018 IBM, Intel, and Google each reported testing quantum processors containing 50, 49, and 72 qubits, respectively, all realized using superconducting circuits. By number of qubits, these circuits are approaching the range in which simulating their quantum dynamics is expected to become prohibitive on classical computers, although it has been argued that further improvements in error rates are needed to put classical simulation out of reach.\n\nIn February 2018, scientists reported, for the first time, the discovery of a new form of light, which may involve polaritons, that could be useful in the development of quantum computers.\n\nIn February 2018, QuTech reported successfully testing a silicon-based two-spin-qubits quantum processor.\n\nIn June 2018, Intel begins testing silicon-based spin-qubit processor, manufactured in the company's D1D Fab in Oregon.\n\nIn July 2018, a team led by the University of Sydney has achieved the world's first multi-qubit demonstration of a quantum chemistry calculation performed on a system of trapped ions, one of the leading hardware platforms in the race to develop a universal quantum computer.\n\nThe class of problems that can be efficiently solved by quantum computers is called BQP, for \"bounded error, quantum, polynomial time\". Quantum computers only run probabilistic algorithms, so BQP on quantum computers is the counterpart of BPP (\"bounded error, probabilistic, polynomial time\") on classical computers. It is defined as the set of problems solvable with a polynomial-time algorithm, whose probability of error is bounded away from one half. A quantum computer is said to \"solve\" a problem if, for every instance, its answer will be right with high probability. If that solution runs in polynomial time, then that problem is in BQP.\n\nBQP is contained in the complexity class \"#P\" (or more precisely in the associated class of decision problems \"P\"), which is a subclass of PSPACE.\n\nBQP is suspected to be disjoint from NP-complete and a strict superset of P, but that is not known. Both integer factorization and discrete log are in BQP. Both of these problems are NP problems suspected to be outside BPP, and hence outside P. Both are suspected to not be NP-complete. There is a common misconception that quantum computers can solve NP-complete problems in polynomial time. That is not known to be true, and is generally suspected to be false.\n\nThe capacity of a quantum computer to accelerate classical algorithms has rigid limits—upper bounds of quantum computation's complexity. The overwhelming part of classical calculations cannot be accelerated on a quantum computer. A similar fact takes place for particular computational tasks, like the search problem, for which Grover's algorithm is optimal.\n\nBohmian Mechanics is a non-local hidden variable interpretation of quantum mechanics. It has been shown that a non-local hidden variable quantum computer could implement a search of an N-item database at most in formula_29 steps. This is slightly faster than the formula_30 steps taken by Grover's algorithm. Neither search method will allow quantum computers to solve NP-Complete problems in polynomial time.\n\nAlthough quantum computers may be faster than classical computers for some problem types, those described above cannot solve any problem that classical computers cannot already solve. A Turing machine can simulate these quantum computers, so such a quantum computer could never solve an undecidable problem like the halting problem. The existence of \"standard\" quantum computers does not disprove the Church–Turing thesis. It has been speculated that theories of quantum gravity, such as M-theory or loop quantum gravity, may allow even faster computers to be built. Currently, \"defining\" computation in such theories is an open problem due to the \"problem of time\", i.e., there currently exists no obvious way to describe what it means for an observer to submit input to a computer and later receive output.\n\n\n \n"}
{"id": "46781339", "url": "https://en.wikipedia.org/wiki?curid=46781339", "title": "Quillen spectral sequence", "text": "Quillen spectral sequence\n\nIn the area of mathematics known as K-theory, the Quillen spectral sequence, also called the Brown–Gersten–Quillen or BGQ spectral sequence (named after Kenneth Brown, \nStephen Gersten, and Daniel Quillen), is a spectral sequence converging to the sheaf cohomology of a type of topological space that occurs in algebraic geometry. It is used in calculating the homotopy properties of a simplicial group.\n\n\n"}
{"id": "1239220", "url": "https://en.wikipedia.org/wiki?curid=1239220", "title": "Sauwastika", "text": "Sauwastika\n\nThe term sauwastika (or \"sauvastika\") (as a character: 卍) is sometimes used to distinguish the left-facing from the right-facing swastika symbol, a meaning which developed in 19th century scholarship.\n\nThe left-facing variant is favoured in Bön and Gurung Dharma; it is called \"yungdrung\" in Bon and \"Gurung Yantra\" in Gurung Dharma. Both the right-facing and left-facing variants are employed in Hinduism and Buddhism; however, the \"left-facing\" is more commonly used in Buddhism than Hinduism and the right-facing is more commonly used in Hinduism than Buddhism.\n\nIn Buddhism the left-facing sauwastika is imprinted on the chest, feet, palms of Buddha and also the first of the 65 auspicious symbols on the footprint of the Buddha. In Hinduism the left-facing sauwastika is associated with esoteric tantric practices and often stands for Goddess Kali.\n\nSanskrit \"sauvastika\" is the adjective derived from \"svastika\" through the standard process of vṛddhi of the vowel \"u\" in the initial syllable. (\"Svastika\" is the transformation of \"su asti\" + \"ka\" through sandhi. Vrddhi applies to the original form before sandhi.) It is attested as an adjective meaning \"benedictive, salutatory\". The connection to a \"reversed\" \"svastika\" is probably first made by Eugène Burnouf in 1852, and taken up by Schliemann in \"Ilios\" (1880), based on a letter from Max Müller, who is in turn quoting Burnouf. The term \"sauwastika\" is used in the sense of \"backwards swastika\" by Eugène Goblet d'Alviella (1894): \"In India it [the \"gammadion\"] bears the name of \"swastika\", when its arms are bent towards the right, and \"sauwastika\" when they are turned in the other direction.\"\n\nThe term has been misspelled as \"suavastika\", a term attributed to Max Müller by Wilson (1896).\nWilson finds that \"The 'suavastika' which Max Müller names and believes was applied to the swastika sign, with the ends bent to the left ... seems not to be reported with that meaning by any other author except Burnouf.\"\n\nEugene Burnouf, the first Western expert on Buddhism, stated in his book \"Lotus de la bonne loi\" (1852) that the \"sauvastika\" was a Buddhist variant of the \"svastika\".\n\nWhen Heinrich Schliemann discovered swastika motifs in Troy, he wrote to the Indologist Max Müller, who, quoting Burnouf, confirmed this distinction, adding that \"the \"svastika\" was originally a symbol of the sun, perhaps of the vernal sun as opposed to the autumnal sun, the \"sauvastika\", and, therefore, a natural symbol of light, life, health, peace and wealth.\" The letter was published in Schliemann's book \"Ilios\" (1880):\nThe term \"sauvastika\" thus cannot be confirmed as authentic and is probably due to Burnouf (1852). Notions that sauwastikas are considered \"evil\" or inauspicious versions of the auspicious swastika in Indian religions have even less substance, since even Burnouf counts the svastika and the sauvastika equally among the \"sixty-five auspicious signs\".\n\nD'Alviella (1894) voices doubts about the distinction:\nAlthough the more common form is the right-facing swastika, the symbol is used in both orientations for the sake of balance in Hinduism. Buddhists almost always use the left-facing swastika.\n\nThe sauwastika was a favorite symbol of the last Russian Empress Alexandra Feodorovna. She put this sign everywhere for good luck. In particular, she drew it with a pencil on the wall, at the window opening, and on the wallpaper above the bed of her son Tsarevich Alexei Nikolaevich in Ipatiev House, where the murdered Emperor's family spent the last days of their lives.\n\n\n"}
