{"id": "407354", "url": "https://en.wikipedia.org/wiki?curid=407354", "title": "101 (number)", "text": "101 (number)\n\n101 (one hundred [and] one) is the natural number following 100 and preceding 102.\n\nIt is variously pronounced \"one hundred and one\" / \"a hundred and one\", \"one hundred one\" / \"a hundred one\", and \"one oh one\". As an ordinal number, 101st (one hundred [and] first), rather than 101th, is the correct form.\n\n101 is:\n\nGiven 101, the Mertens function returns 0. It is the second prime having this property.\n\nFor a 3-digit number in base 10, this number has a relatively simple divisibility test. The candidate number is split into groups of four, starting with the rightmost four, and added up to produce a 4-digit number. If this 4-digit number is of the form 1000\"a\" + 100\"b\" + 10\"a\" + \"b\" (where \"a\" and \"b\" are integers from 0 to 9), such as 3232 or 9797, or of the form 100\"b\" + \"b\", such as 707 and 808, then the number is divisible by 101.\n\nOn the seven-segment display of a calculator, 101 is both a strobogrammatic prime and a dihedral prime.\n\n\nAccording to Books in Print, more books are now published with a title that begins with '101' than '100'. They usually describe or discuss a list of items, such as \"101 Ways to...\" or \"101 Questions and Answers About...\" . This marketing tool is used to imply that the customer is given a little extra information beyond books that include only 100 items. Some books have taken this marketing scheme even further with titles that begin with '102', '103', or '1001'. The number is used in this context as a slang term when referring to \"a 101 document\" what is usually referred to as a statistical survey or overview of some topic.\n\nRoom 101 is a torture chamber in the novel \"Nineteen Eighty-Four\" by George Orwell.\n\nCreative Writing 101 by Raymond Carver, \"A writer's values and craft. This was what the man (John Gardner) taught and what he stood for, and this is what I've kept by me in the years since that brief but all important time.\"\n\nIn American university course numbering systems, the number 101 is often used for an introductory course at a beginner's level in a department's subject area. This common numbering system was designed to make transfer between colleges easier. In theory, any numbered course in one academic institution should bring a student to the same standard as a similarly numbered course at other institutions. The term was first introduced by the University of Buffalo in 1929.\n\nBased on this usage, the term \"101\" has been extended to mean an introductory level of learning or a collection of introductory materials to a topic.\n\n\n"}
{"id": "891481", "url": "https://en.wikipedia.org/wiki?curid=891481", "title": "123 (number)", "text": "123 (number)\n\n123 (one hundred [and] twenty-three) is the natural number following 122 and preceding 124.\n\n\nThe Book of Numbers says that Aaron died at the age of 123.\n\n\n123 is also:\n\n"}
{"id": "1040671", "url": "https://en.wikipedia.org/wiki?curid=1040671", "title": "129 (number)", "text": "129 (number)\n\n129 (one hundred [and] twenty-nine) is the natural number following 128 and preceding 130.\n\n129 is the sum of the first ten prime numbers. It is the smallest number that can be expressed as a sum of three squares in four different ways: formula_1, formula_2, formula_3, and formula_4.\n\n129 is the product of only two primes, 3 and 43, making 129 a semiprime. Since 3 and 43 are both Gaussian primes, this means that 129 is a Blum integer.\n\n129 is a repdigit in base 6 (333).\n\n129 is a happy number.\n\n\n\n129 is also:\n\n"}
{"id": "13714607", "url": "https://en.wikipedia.org/wiki?curid=13714607", "title": "Analysis of molecular variance", "text": "Analysis of molecular variance\n\nAnalysis of molecular variance (AMOVA), is a statistical model for the molecular variation in a single species, typically biological. The name and model are inspired by ANOVA. The method was developed by Laurent Excoffier, Peter Smouse and Joseph Quattro at Rutgers University in 1992.\n\nSince developing AMOVA, Excoffier has written a program for running such analyses. This program, which runs on Windows is called Arlequin, and is freely available on Excoffier's website. There is also an implementation by Sandrine Pavoine in R language in the ade4 package available on CRAN (Comprehensive R Archive Network). Another implementation is in Info-Gen, which also runs on Windows. The student version is free and fully functional. Native language of the application is Spanish but an English version is also available.\n\nAn additional free statistical package, GenAlEx, is geared toward teaching as well as research and allows for complex genetic analyses to be employed and compared within the commonly used Microsoft Excel interface. This software allows for calculation of analyses such as AMOVA, as well as comparisons with other types of closely related statistics including F-statistics and Shannon's index, and more.\n\n\n<br>\n"}
{"id": "3227937", "url": "https://en.wikipedia.org/wiki?curid=3227937", "title": "Anne-Marie Imafidon", "text": "Anne-Marie Imafidon\n\nAnne-Marie Osawemwenze Ore-Ofe Imafidon (born 1990) is a British computing, mathematics and language child prodigy. She is one of the youngest to pass two GCSEs in two different subjects while in primary school. She passed two GCSE Examinations (in Mathematics and Information technology) at the age of 11. Imafidon founded and became CEO of Stemettes in 2013, a social enterprise promoting women in STEM careers, and was honoured with an MBE in 2016.\n\nImafidon was born in England in 1990. Her father, Chris Imafidon, is an ophthalmologist who emigrated to London, and her mother is Ann Imafidon. She and her 3 younger siblings, Christina and twins Peter and Paula, are child prodigies, breaking age records in educational attainments.\n\nImafidon began school at St Saviour Church of England Primary School in Walthamstow, London, and could speak six languages by the age of 10.\n\nAt 13, in 2003, she received a British scholarship to study mathematics at Johns Hopkins University. At 15, in 2005, she was admitted a degree program by the University of Oxford. At 17, she started a master's degree at Oxford University and, at 19 in June 2010, she became the youngest ever graduate with a master's degree.\n\nImafidon worked briefly for Goldman Sachs, Hewlett Packard, and Deutschebank before launching and becoming CEO of Stemettes in 2013, championing the work of women in STEM. Stemettes runs panel sessions and hackathons supporting girls and young women who are considering a STEM career. In April 2014, Imafidon was the keynote speaker at the BCSWomen Lovelace Colloquium\n\n"}
{"id": "518199", "url": "https://en.wikipedia.org/wiki?curid=518199", "title": "Axiomatic semantics", "text": "Axiomatic semantics\n\nAxiomatic semantics is an approach based on mathematical logic for proving the correctness of computer programs. It is closely related to Hoare logic.\n\nAxiomatic semantics define the meaning of a command in a program by describing its effect on assertions about the program state. The assertions are logical statements—predicates with variables, where the variables define the state of the program.\n\n"}
{"id": "26812527", "url": "https://en.wikipedia.org/wiki?curid=26812527", "title": "Barrett reduction", "text": "Barrett reduction\n\nIn modular arithmetic, Barrett reduction is a reduction algorithm introduced in 1986 by P.D. Barrett. A naive way of computing\n\nwould be to use a fast division algorithm. Barrett reduction is an algorithm designed to optimize this operation assuming formula_2 is constant, and formula_3, replacing divisions by multiplications.\n\nLet formula_4 be the inverse of formula_2 as a floating point number. Then\n\nwhere formula_7 denotes the floor function. The result is exact, as long as formula_8 is computed with sufficient accuracy.\n\nBarrett initially considered an integer version of the above algorithm when the values fit into machine words.\n\nWhen calculating formula_9 using the method above, but with integers, the obvious analogue would be to use division by formula_2:\nfunc reduce(a uint) uint {\n\nHowever, division can be expensive and, in cryptographic settings, may not be a constant-time instruction on some CPUs. Thus Barrett reduction approximates formula_11 with a value formula_12 because division by formula_13 is just a right-shift and so is cheap.\n\nIn order to calculate the best value for formula_14 given formula_13 consider:\n\nIn order for formula_14 to be an integer, we need to round formula_18 somehow. Rounding to the nearest integer will give the best approximation but can result in formula_12 being larger than formula_11, which can cause underflows. Thus formula_21 is generally used.\n\nThus we can approximate the function above with:\nfunc reduce(a uint) uint {\n\nHowever, since formula_22, the value of codice_1 in that function can end up being one too small, and thus codice_2 is only guaranteed to be within formula_23 rather than formula_24 as is generally required. A conditional subtraction will correct this:\nfunc reduce(a uint) uint {\n\nSince formula_12 is only an approximation, the valid range of formula_26 needs to be considered. The error of the approximation of formula_11 is:\n\nThus the error in the value of codice_1 is formula_29. As long as formula_30 then the reduction is valid thus formula_31. The reduction function might not immediately give the wrong answer when formula_32 but the bounds on formula_26 must be respected to ensure the correct answer in the general case.\n\nBy choosing larger values of formula_34, the range of values of formula_26 for which the reduction is valid can be increased, but larger values of formula_34 may cause overflow problems elsewhere.\n\nConsider the case of formula_37 when operating with 16-bit integers.\n\nThe smallest value of formula_34 that makes sense is formula_39 because if formula_40 then the reduction will only be valid for values that are already minimal! For a value of seven, formula_41. For a value of eight formula_42. Thus formula_43 provides no advantage because the approximation of formula_44 in that case (formula_45) is exactly the same as formula_46. For formula_47, we get formula_48, which is a better approximation.\n\nNow we consider the valid input ranges for formula_39 and formula_47. In the first case, formula_51 so formula_31 implies formula_53. Since formula_26 is an integer, effectively the maximum value is 478. (In practice the function happens to work for values up to 504.)\n\nIf we take formula_47 then formula_56 and so the maximum value of formula_26 is 7387. (In practice the function happens to work until 7473.)\n\nThe next value of formula_34 that results in a better approximation is 13, giving formula_59. However, note that the intermediate value formula_60 in the calculation will then overflow an unsigned 16-bit value when formula_61, thus formula_39 works better in this situation.\n\nLet formula_63 be the smallest integer such that formula_64. Take formula_65 as a reasonable value for formula_34 in the above equations. As in the code snippets above, let\n\n\nBecause of the floor function, formula_69 is an integer and formula_70. Also, if formula_71 then formula_72. In that case, writing the snippets above as an expression:\n\nThe proof that formula_72 follows:\n\nIf formula_71, then\n\nSince formula_77 regardless of formula_26, it follows that\n\nBarrett's primary motivation for considering reduction was the implementation of RSA, where the values in question will almost certainly exceed the size of a machine word. In this situation, Barrett provided an algorithm that approximates the single-word version above but for multi-word values. For details see section 14.3.3 of the Handbook of Applied Cryptography.\n\nMontgomery reduction is another similar algorithm.\n\n"}
{"id": "15375152", "url": "https://en.wikipedia.org/wiki?curid=15375152", "title": "Brian Conrey", "text": "Brian Conrey\n\nJohn Brian Conrey is an American mathematician and the executive director of the American Institute of Mathematics. His research interests are in number theory, specifically analysis of L-functions and the Riemann zeta function. He received his B.S. from Santa Clara University in 1976 and received his Ph.D. at the University of Michigan in 1980.\n\nHe is the founding executive director of the American Institute of Mathematics and on the editing board of the Journal of Number Theory. The American Mathematical Society jointly awarded him the eighth annual Levi L. Conant Prize for expository writing in 2008 for \"The Riemann Hypothesis\". In 2015 he was elected as a fellow of the American Mathematical Society.\n\nSince 2005 he has been part-time professor at the University of Bristol, England.\n"}
{"id": "373165", "url": "https://en.wikipedia.org/wiki?curid=373165", "title": "Cardinal assignment", "text": "Cardinal assignment\n\nIn set theory, the concept of cardinality is significantly developable without recourse to actually defining cardinal numbers as objects in the theory itself (this is in fact a viewpoint taken by Frege; Frege cardinals are basically equivalence classes on the entire universe of sets, by equinumerosity). The concepts are developed by defining equinumerosity in terms of functions and the concepts of one-to-one and onto (injectivity and surjectivity); this gives us a pseudo-ordering relation\n\non the whole universe by size. It is not a true partial ordering because antisymmetry need not hold: if both formula_2 and formula_3, it is true by the Cantor–Bernstein–Schroeder theorem that formula_4 i.e. \"A\" and \"B\" are equinumerous, but they do not have to be literally equal (see isomorphism). That at least one of formula_2 and formula_3 holds turns out to be equivalent to the axiom of choice.\n\nNevertheless, most of the \"interesting\" results on cardinality and its arithmetic can be expressed merely with =.\n\nThe goal of a cardinal assignment is to assign to every set \"A\" a specific, unique set that is only dependent on the cardinality of \"A\". This is in accordance with Cantor's original vision of a cardinals: to take a set and abstract its elements into canonical \"units\" and collect these units into another set, such that the only thing special about this set is its size. These would be totally ordered by the relation formula_7 and = would be true equality. As Y. N. Moschovakis says, however, this is mostly an exercise in mathematical elegance, and you don't gain much unless you are \"allergic to subscripts.\" However, there are various valuable applications of \"real\" cardinal numbers in various models of set theory.\n\nIn modern set theory, we usually use the Von Neumann cardinal assignment, which uses the theory of ordinal numbers and the full power of the axioms of choice and replacement. Cardinal assignments do need the full axiom of choice, if we want a decent cardinal arithmetic and an assignment for \"all\" sets.\n\nFormally, assuming the axiom of choice, the cardinality of a set \"X\" is the least ordinal α such that there is a bijection between \"X\" and α. This definition is known as the von Neumann cardinal assignment. If the axiom of choice is not assumed we need to do something different. The oldest definition of the cardinality of a set \"X\" (implicit in Cantor and explicit in Frege and Principia Mathematica) is as the set of all sets that are equinumerous with \"X\": this does not work in ZFC or other related systems of axiomatic set theory because this collection is too large to be a set, but it does work in type theory and in New Foundations and related systems. However, if we restrict from this class to those equinumerous with \"X\" that have the least rank, then it will work (this is a trick due to Dana Scott: it works because the collection of objects with any given rank is a set).\n\n"}
{"id": "31837559", "url": "https://en.wikipedia.org/wiki?curid=31837559", "title": "Chang number", "text": "Chang number\n\nIn mathematics, the Chang number of an irreducible representation of a simple complex Lie algebra is its dimension modulo 1 + \"h\", where \"h\" is the Coxeter number. Chang numbers are named after , who rediscovered an element of order \"h\" + 1 found by .\n\nIn particular, for the exceptional compact Lie groups \"G\", F4, E6, E7, and E8 the number \"h\" + 1 = 7, 13, 13, 19, 31 is always prime, so the Chang number of an irreducible representation is always +1, 0, or −1.\n\nFor example, the first few irreducible representations of G2 (with Coxeter number \"h\" = 6) have dimensions 1, 7, 14, 27, 64, 77, 182, 189, 273, 286...\nThese are congruent to 1, 0, 0, −1, 1, 0, 0, 0, 0, −1... mod 7 = \"h\" + 1.\n\n"}
{"id": "4504051", "url": "https://en.wikipedia.org/wiki?curid=4504051", "title": "Channel state information", "text": "Channel state information\n\nIn wireless communications, channel state information (CSI) refers to known channel properties of a communication link. This information describes how a signal propagates from the transmitter to the receiver and represents the combined effect of, for example, scattering, fading, and power decay with distance. The method is called Channel estimation. The CSI makes it possible to adapt transmissions to current channel conditions, which is crucial for achieving reliable communication with high data rates in multiantenna systems.\n\nCSI needs to be estimated at the receiver and usually quantized and fed back to the transmitter (although reverse-link estimation is possible in TDD systems). Therefore, the transmitter and receiver can have different CSI. The CSI at the transmitter and the CSI at the receiver are sometimes referred to as CSIT and CSIR, respectively.\n\nThere are basically two levels of CSI, namely instantaneous CSI and statistical CSI.\n\nInstantaneous CSI (or short-term CSI) means that the current channel conditions are known, which can be viewed as knowing the impulse response of a digital filter. This gives an opportunity to adapt the transmitted signal to the impulse response and thereby optimize the received signal for spatial multiplexing or to achieve low bit error rates.\n\nStatistical CSI (or long-term CSI) means that a statistical characterization of the channel is known. This description can include, for example, the type of fading distribution, the average channel gain, the line-of-sight component, and the spatial correlation. As with instantaneous CSI, this information can be used for transmission optimization.\n\nThe CSI acquisition is practically limited by how fast the channel conditions are changing. In fast fading systems where channel conditions vary rapidly under the transmission of a single information symbol, only statistical CSI is reasonable. On the other hand, in slow fading systems instantaneous CSI can be estimated with reasonable accuracy and used for transmission adaptation for some time before being outdated.\n\nIn practical systems, the available CSI often lies in between these two levels; instantaneous CSI with some estimation/quantization error is combined with statistical information.\n\nIn a narrowband flat-fading channel with multiple transmit and receive antennas (MIMO), the system is modeled as\nwhere formula_2 and formula_3 are the receive and transmit vectors, respectively, and formula_4 and formula_5 are the channel matrix and the noise vector, respectively. The noise is often modeled as circular symmetric complex normal with\nwhere the mean value is zero and the noise covariance matrix formula_7 is known.\n\nIdeally, the channel matrix formula_4 is known perfectly. Due to channel estimation errors, the channel information can be represented as\nwhere formula_10 is the channel estimate and formula_11 is the estimation error covariance matrix. The vectorization formula_12 was used to achieve the column stacking of formula_4, as multivariate random variables are usually defined as vectors.\n\nIn this case, the statistics of formula_4 are known. In a Rayleigh fading channel, this corresponds to knowing that\nfor some known channel covariance matrix formula_16.\n\nSince the channel conditions vary, instantaneous CSI needs to be estimated on a short-term basis. A popular approach is so-called training sequence (or pilot sequence), where a known signal is transmitted and the channel matrix formula_4 is estimated using the combined knowledge of the transmitted and received signal.\n\nLet the training sequence be denoted formula_18, where the vector formula_19 is transmitted over the channel as\nBy combining the received training signals formula_21 for formula_22, the total training signalling becomes\nwith the training matrix formula_24 and the noise matrix formula_25.\n\nWith this notation, channel estimation means that formula_26 should be recovered from the knowledge of formula_27 and formula_28.\n\nIf the channel and noise distributions are unknown, then the least-square estimator (also known as the minimum-variance unbiased estimator) is\nwhere formula_30 denotes the conjugate transpose. The estimation Mean Square Error (MSE) is proportional to\nwhere formula_32 denotes the trace. The error is minimized when formula_33 is a scaled identity matrix. This can only be achieved when formula_34 is equal to (or larger than) the number of transmit antennas. The simplest example of an optimal training matrix is to select formula_35 as a (scaled) identity matrix of the same size that the number of transmit antennas.\n\nIf the channel and noise distributions are known, then this a priori information can be exploited to decrease the estimation error. This approach is known as Bayesian estimation and for Rayleigh fading channels it exploits that \n\nThe MMSE estimator is the Bayesian counterpart to the least-square estimator and becomes\nwhere formula_38 denotes the Kronecker product and the identity matrix formula_39 has the dimension of the number of receive antennas. The estimation Mean Square Error (MSE) is\nand is minimized by a training matrix formula_28 that in general can only be derived through numerical optimization. But there exist heuristic solutions with good performance based on waterfilling. As opposed to least-square estimation, the estimation error for spatially correlated channels can be minimized even if formula_34 is smaller than the number of transmit antennas. Thus, MMSE estimation can both decrease the estimation error and shorten the required training sequence. It needs however additionally the knowledge of the channel correlation matrix formula_16 and noise correlation matrix formula_7. In absence of an accurate knowledge of these correlation matrices, robust choices need to be made to avoid MSE degradation.\n\nIn a data-aided approach, the channel estimation is based on some known data, which is known both at the transmitter and at the receiver, such as training sequences or pilot data. In a blind approach, the estimation is based only on the received data, without any known transmitted sequence. The tradeoff is the accuracy versus the overhead. A data-aided approach requires more bandwidth or it has a higher overhead than a blind approach, but it can achieve a better channel estimation accuracy than a blind estimator.\n"}
{"id": "36996546", "url": "https://en.wikipedia.org/wiki?curid=36996546", "title": "Chemical reaction network theory", "text": "Chemical reaction network theory\n\nChemical reaction network theory is an area of applied mathematics that attempts to model the behaviour of real world chemical systems. Since its foundation in the 1960s, it has attracted a growing research community, mainly due to its applications in biochemistry and theoretical chemistry. It has also attracted interest from pure mathematicians due to the interesting problems that arise from the mathematical structures involved.\n\nDynamical properties of reaction networks were studied in chemistry and physics after invention of the law of mass action. The essential steps in this study were introduction of detailed balance for the complex chemical reactions by Rudolf Wegscheider (1901), development of the quantitative theory of chemical chain reactions by Nikolay Semyonov (1934), development of kinetics of catalytic reactions by Cyril Norman Hinshelwood, and many other results.\n\nThree eras of chemical dynamics can be revealed in the flux of research and publications. These eras may be associated with leaders: the first is the van 't Hoff era, the second may be called the Semenov–Hinshelwood era and the third is definitely the Aris era. \nThe \"eras\" may be distinguished based on the main focuses of the scientific leaders:\n\nThe mathematical discipline \"chemical reaction network theory\" was originated by Rutherford Aris, a famous expert in chemical engineering, with support of Clifford Truesdell, the founder and editor-in-chief of the journal \"Archive for Rational Mechanics and Analysis\". The paper of R. Aris in this journal was communicated to the journal by C. Truesdell. It opened the series of papers of other authors (which were communicated already by R. Aris). The well known papers of this series are the works of Frederick J. Krambeck, Roy Jackson, Friedrich Josef Maria Horn, Martin Feinberg and others, published in the 1970s. In his second \"prolegomena\" paper, R. Aris mentioned the work of N.Z. Shapiro, L.S. Shapley (1965), where an important part of his scientific program was realized.\n\nSince then, the chemical reaction network theory has been further developed by a large number of researchers internationally.\n\nA chemical reaction network (often abbreviated to CRN) comprises a set of reactants, a set of products (often intersecting the set of reactants), and a set of reactions. For example, the pair of combustion reactions\n\n"}
{"id": "48640355", "url": "https://en.wikipedia.org/wiki?curid=48640355", "title": "Computational Geometry (journal)", "text": "Computational Geometry (journal)\n\nComputational Geometry, also known as Computational Geometry: Theory and Applications, is a peer-reviewed mathematics journal for research in theoretical and applied computational geometry, its applications, techniques, and design and analysis of geometric algorithms. All aspects of computational geometry are covered, including the numerical, graph theoretical and combinatorial aspects, as well as fundamental problems in various areas of application of computational geometry: in computer graphics, pattern recognition, image processing, robotics, electronic design automation, CAD/CAM, and geographical information systems.\n\nThe journal was founded in 1991 by Jörg-Rüdiger Sack and Jorge Urrutia.\nIt is indexed by \"Mathematical Reviews\", Zentralblatt MATH, Science Citation Index, and Current Contents/Engineering, Computing and Technology.\n"}
{"id": "2846373", "url": "https://en.wikipedia.org/wiki?curid=2846373", "title": "Concurrent validity", "text": "Concurrent validity\n\nConcurrent validity is a type of evidence that can be gathered to defend the use of a test for predicting other outcomes. It is a parameter used in sociology, psychology, and other psychometric or behavioral sciences. Concurrent validity is demonstrated when a test correlates well with a measure that has previously been validated. The two measures may be for the same construct, but more often used for different, but presumably related, constructs.\n\nThe two measures in the study are taken at the same time. This is in contrast to predictive validity, where one measure occurs earlier and is meant to predict some later measure. In both cases, the (concurrent) predictive power of the test is analyzed using a simple correlation or linear regression.\n\nConcurrent validity and predictive validity are two types of criterion-related validity. The difference between concurrent validity and predictive validity rests solely on the time at which the two measures are administered. Concurrent validity applies to validation studies in which the two measures are administered at approximately the same time. For example, an employment test may be administered to a group of workers and then the test scores can be correlated with the ratings of the workers' supervisors taken on the same day or in the same week. The resulting correlation would be a concurrent validity coefficient. This type of evidence might be used to support the use of the employment test for future selection of employees.\n\nConcurrent validity may be used as a practical substitute for predictive validity. In the example above, predictive validity would be the best choice for validating an employment test, because using the employment test on existing employees may not be a strong analog for using the tests for selection. Reduced motivation and restriction of range are just two possible biasing effects for concurrent validity studies.\n\nConcurrent validity differs from convergent validity in that it focuses on the power of the focal test to \"predict\" outcomes on another test or some outcome variable. Convergent validity refers to the observation of strong correlations between two tests that are assumed to measure the same construct. It is the interpretation of the focal test as a \"predictor\" that differentiates this type of evidence from convergent validity, though both methods rely on simple correlations in the statistical analysis.\n\n"}
{"id": "439526", "url": "https://en.wikipedia.org/wiki?curid=439526", "title": "Cryptographic hash function", "text": "Cryptographic hash function\n\nA cryptographic hash function is a special class of hash function that has certain properties which make it suitable for use in cryptography. It is a mathematical algorithm that maps data of arbitrary size to a bit string of a fixed size (a hash) and is designed to be a one-way function, that is, a function which is infeasible to invert. The only way to recreate the input data from an ideal cryptographic hash function's output is to attempt a brute-force search of possible inputs to see if they produce a match, or use a rainbow table of matched hashes. Bruce Schneier has called one-way hash functions \"the workhorses of modern cryptography\".\nThe input data is often called the \"message\", and the output (the \"hash value\" or \"hash\") is often called the \"message digest\" or simply the \"digest\".\n\nThe ideal cryptographic hash function has five main properties:\n\nCryptographic hash functions have many information-security applications, notably in digital signatures, message authentication codes (MACs), and other forms of authentication. They can also be used as ordinary hash functions, to index data in hash tables, for fingerprinting, to detect duplicate data or uniquely identify files, and as checksums to detect accidental data corruption. Indeed, in information-security contexts, cryptographic hash values are sometimes called (\"digital\") \"fingerprints\", \"checksums\", or just \"hash values\", even though all these terms stand for more general functions with rather different properties and purposes.\n\nMost cryptographic hash functions are designed to take a string of any length as input and produce a fixed-length hash value.\n\nA cryptographic hash function must be able to withstand all known types of cryptanalytic attack. In theoretical cryptography, the security level of a cryptographic hash function has been defined using the following properties:\n\n\nCollision resistance implies second pre-image resistance, but does not imply pre-image resistance. The weaker assumption is always preferred in theoretical cryptography, but in practice, a hash-function which is only second pre-image resistant is considered insecure and is therefore not recommended for real applications.\n\nInformally, these properties mean that a malicious adversary cannot replace or modify the input data without changing its digest. Thus, if two strings have the same digest, one can be very confident that they are identical. Second pre-image resistance prevents an attacker from crafting a document with the same hash as a document the attacker cannot control. Collision resistance prevents an attacker from creating two distinct documents with the same hash.\n\nA function meeting these criteria may still have undesirable properties. Currently popular cryptographic hash functions are vulnerable to \"length-extension\" attacks: given and but not \"m\", by choosing a suitable \"m\" an attacker can calculate where || denotes concatenation. This property can be used to break naive authentication schemes based on hash functions. The HMAC construction works around these problems.\n\nIn practice, collision resistance is insufficient for many practical uses.\nIn addition to collision resistance, it should be impossible for an adversary to find two messages with substantially similar digests; or to infer any useful information about the data, given only its digest. In particular, a hash function should behave as much as possible like a random function (often called a random oracle in proofs of security) while still being deterministic and efficiently computable. This rules out functions like the SWIFFT function, which can be rigorously proven to be collision resistant assuming that certain problems on ideal lattices are computationally difficult, but as a linear function, does not satisfy these additional properties.\n\nChecksum algorithms, such as CRC32 and other cyclic redundancy checks, are designed to meet much weaker requirements, and are generally unsuitable as cryptographic hash functions. For example, a CRC was used for message integrity in the WEP encryption standard, but an attack was readily discovered which exploited the linearity of the checksum.\n\nIn cryptographic practice, \"difficult\" generally means \"almost certainly beyond the reach of any adversary who must be prevented from breaking the system for as long as the security of the system is deemed important\". The meaning of the term is therefore somewhat dependent on the application since the effort that a malicious agent may put into the task is usually proportional to his expected gain. However, since the needed effort usually multiplies with the digest length, even a thousand-fold advantage in processing power can be neutralized by adding a few dozen bits to the latter.\n\nFor messages selected from a limited set of messages, for example passwords or other short messages, it can be feasible to invert a hash by trying all possible messages in the set. Because cryptographic hash functions are typically designed to be computed quickly, special key derivation functions that require greater computing resources have been developed that make such brute force attacks more difficult.\n\nIn some theoretical analyses \"difficult\" has a specific mathematical meaning, such as \"not solvable in asymptotic polynomial time\". Such interpretations of \"difficulty\" are important in the study of provably secure cryptographic hash functions but do not usually have a strong connection to practical security. For example, an exponential time algorithm can sometimes still be fast enough to make a feasible attack. Conversely, a polynomial time algorithm (e.g., one that requires \"n\" steps for \"n\"-digit keys) may be too slow for any practical use.\n\nAn illustration of the potential use of a cryptographic hash is as follows: Alice poses a tough math problem to Bob and claims she has solved it. Bob would like to try it himself, but would yet like to be sure that Alice is not bluffing. Therefore, Alice writes down her solution, computes its hash and tells Bob the hash value (whilst keeping the solution secret). Then, when Bob comes up with the solution himself a few days later, Alice can prove that she had the solution earlier by revealing it and having Bob hash it and check that it matches the hash value given to him before. (This is an example of a simple commitment scheme; in actual practice, Alice and Bob will often be computer programs, and the secret would be something less easily spoofed than a claimed puzzle solution).\n\nAn important application of secure hashes is verification of message integrity. Comparing message digests (hash digests over the message) calculated before, and after, transmission can determine whether any changes have been made to the message or file.\n\nMD5, SHA1, or SHA2 hash digests are sometimes published on websites or forums to allow verification of integrity for downloaded files, including files retrieved using file sharing such as mirroring. This practice establishes a chain of trust so long as the hashes are posted on a site authenticated by HTTPS. Using a cryptographic hash and a chain of trust prevents malicious changes to the file to go undetected. Other error detecting codes such as cyclic redundancy checks only prevent against non-malicious alterations of the file.\n\nAlmost all digital signature schemes require a cryptographic hash to be calculated over the message. This allows the signature calculation to be performed on the relatively small, statically sized hash digest. The message is considered authentic if the signature verification succeeds given the signature and recalculated hash digest over the message. So the message integrity property of the cryptographic hash is used to create secure and efficient digital signature schemes.\n\nPassword verification commonly relies on cryptographic hashes. Storing all user passwords as cleartext can result in a massive security breach if the password file is compromised. One way to reduce this danger is to only store the hash digest of each password. To authenticate a user, the password presented by the user is hashed and compared with the stored hash. A password reset method is required when password hashing is performed; original passwords cannot be recalculated from the stored hash value.\n\nStandard cryptographic hash functions are designed to be computed quickly, and, as a result, it is possible to try guessed passwords at high rates. Common graphics processing units can try billions of possible passwords each second. Password hash functions that perform Key stretching - such as PBKDF2, scrypt or Argon2 - commonly use repeated invocations of a cryptographic hash to increase the time (and in some cases computer memory) required to perform brute force attacks on stored password hash digests. A password hash requires the use of a large random, non-secret salt value which can be stored with the password hash. The salt randomizes the output of the password hash, making it impossible for an adversary to store tables of passwords and precomputed hash values to which the password hash digest can be compared.\n\nThe output of a password hash function can also be used as a cryptographic key. Password hashes are therefore also known as Password Based Key Derivation Functions (PBKDFs).\n\nA proof-of-work system (or protocol, or function) is an economic measure to deter denial-of-service attacks and other service abuses such as spam on a network by requiring some work from the service requester, usually meaning processing time by a computer. A key feature of these schemes is their asymmetry: the work must be moderately hard (but feasible) on the requester side but easy to check for the service provider. One popular system – used in Bitcoin mining and Hashcash – uses partial hash inversions to prove that work was done, to unlock a mining reward in Bitcoin and as a good-will token to send an e-mail in Hashcash. The sender is required to find a message whose hash value begins with a number of zero bits. The average work that sender needs to perform in order to find a valid message is exponential in the number of zero bits required in the hash value, while the recipient can verify the validity of the message by executing a single hash function. For instance, in Hashcash, a sender is asked to generate a header whose 160 bit SHA-1 hash value has the first 20 bits as zeros. The sender will on average have to try 2 times to find a valid header.\n\nA message digest can also serve as a means of reliably identifying a file; several source code management systems, including Git, Mercurial and Monotone, use the sha1sum of various types of content (file content, directory trees, ancestry information, etc.) to uniquely identify them. Hashes are used to identify files on peer-to-peer filesharing networks. For example, in an ed2k link, an MD4-variant hash is combined with the file size, providing sufficient information for locating file sources, downloading the file and verifying its contents. Magnet links are another example. Such file hashes are often the top hash of a hash list or a hash tree which allows for additional benefits.\n\nOne of the main applications of a hash function is to allow the fast look-up of a data in a hash table. Being hash functions of a particular kind, cryptographic hash functions lend themselves well to this application too.\n\nHowever, compared with standard hash functions, cryptographic hash functions tend to be much more expensive computationally. For this reason, they tend to be used in contexts where it is necessary for users to protect themselves against the possibility of forgery (the creation of data with the same digest as the expected data) by potentially malicious participants.\n\nThere are several methods to use a block cipher to build a cryptographic hash function, specifically a one-way compression function.\n\nThe methods resemble the block cipher modes of operation usually used for encryption. Many well-known hash functions, including MD4, MD5, SHA-1 and SHA-2 are built from block-cipher-like components designed for the purpose, with feedback to ensure that the resulting function is not invertible. SHA-3 finalists included functions with block-cipher-like components (e.g., Skein, BLAKE) though the function finally selected, Keccak, was built on a cryptographic sponge instead.\n\nA standard block cipher such as AES can be used in place of these custom block ciphers; that might be useful when an embedded system needs to implement both encryption and hashing with minimal code size or hardware area. However, that approach can have costs in efficiency and security. The ciphers in hash functions are built for hashing: they use large keys and blocks, can efficiently change keys every block, and have been designed and vetted for resistance to related-key attacks. General-purpose ciphers tend to have different design goals. In particular, AES has key and block sizes that make it nontrivial to use to generate long hash values; AES encryption becomes less efficient when the key changes each block; and related-key attacks make it potentially less secure for use in a hash function than for encryption.\n\nA hash function must be able to process an arbitrary-length message into a fixed-length output. This can be achieved by breaking the input up into a series of equal-sized blocks, and operating on them in sequence using a one-way compression function. The compression function can either be specially designed for hashing or be built from a block cipher. A hash function built with the Merkle–Damgård construction is as resistant to collisions as is its compression function; any collision for the full hash function can be traced back to a collision in the compression function.\n\nThe last block processed should also be unambiguously length padded; this is crucial to the security of this construction. This construction is called the Merkle–Damgård construction. Most common classical hash functions, including SHA-1 and MD5, take this form.\n\nA straightforward application of the Merkle–Damgård construction, where the size of hash output is equal to the internal state size (between each compression step), results in a narrow-pipe hash design. This design causes many inherent flaws, including length-extension, multicollisions, long message attacks, generate-and-paste attacks, and also cannot be parallelized. As a result, modern hash functions are built on wide-pipe constructions that have a larger internal state size — which range from tweaks of the Merkle–Damgård construction to new constructions such as the sponge construction and HAIFA construction. None of the entrants in the NIST hash function competition use a classical Merkle–Damgård construction.\n\nMeanwhile, truncating the output of a longer hash, such as used in SHA-512/256, also defeats many of these attacks.\n\nHash functions can be used to build other cryptographic primitives. For these other primitives to be cryptographically secure, care must be taken to build them correctly.\n\nMessage authentication codes (MACs) (also called keyed hash functions) are often built from hash functions. HMAC is such a MAC.\n\nJust as block ciphers can be used to build hash functions, hash functions can be used to build block ciphers. Luby-Rackoff constructions using hash functions can be provably secure if the underlying hash function is secure. Also, many hash functions (including SHA-1 and SHA-2) are built by using a special-purpose block cipher in a Davies–Meyer or other construction. That cipher can also be used in a conventional mode of operation, without the same security guarantees. See SHACAL, BEAR and LION.\n\nPseudorandom number generators (PRNGs) can be built using hash functions. This is done by combining a (secret) random seed with a counter and hashing it.\n\nSome hash functions, such as Skein, Keccak, and RadioGatún output an arbitrarily long stream and can be used as a stream cipher, and stream ciphers can also be built from fixed-length digest hash functions. Often this is done by first building a cryptographically secure pseudorandom number generator and then using its stream of random bytes as keystream. SEAL is a stream cipher that uses SHA-1 to generate internal tables, which are then used in a keystream generator more or less unrelated to the hash algorithm. SEAL is not guaranteed to be as strong (or weak) as SHA-1. Similarly, the key expansion of the HC-128 and HC-256 stream ciphers makes heavy use of the SHA-256 hash function.\n\nConcatenating outputs from multiple hash functions provides collision resistance as good as the strongest of the algorithms included in the concatenated result. For example, older versions of Transport Layer Security (TLS) and Secure Sockets Layer (SSL) use concatenated MD5 and SHA-1 sums.\nThis ensures that a method to find collisions in one of the hash functions does not defeat data protected by both hash functions.\n\nFor Merkle–Damgård construction hash functions, the concatenated function is as collision-resistant as its strongest component, but not more collision-resistant. Antoine Joux observed that 2-collisions lead to n-collisions: if it is feasible for an attacker to find two messages with the same MD5 hash, the attacker can find as many messages as the attacker desires with identical MD5 hashes with no greater difficulty. Among the n messages with the same MD5 hash, there is likely to be a collision in SHA-1. The additional work needed to find the SHA-1 collision (beyond the exponential birthday search) requires only polynomial time.\n\nThere are many cryptographic hash algorithms; this section lists a few algorithms that are referenced relatively often. A more extensive list can be found on the page containing a comparison of cryptographic hash functions.\n\nMD5 was designed by Ronald Rivest in 1991 to replace an earlier hash function MD4, and was specified in 1992 as RFC 1321. Collisions against MD5 can be calculated within seconds which makes the algorithm unsuitable for most use cases where a cryptographic hash is required. MD5 produces a digest of 128 bits (16 bytes).\n\nSHA-1 was developed as part of the U.S. Government's Capstone project. The original specification - now commonly called SHA-0 - of the algorithm was published in 1993 under the title Secure Hash Standard, FIPS PUB 180, by U.S. government standards agency NIST (National Institute of Standards and Technology). It was withdrawn by the NSA shortly after publication and was superseded by the revised version, published in 1995 in FIPS PUB 180-1 and commonly designated SHA-1. Collisions against the full SHA-1 algorithm can be produced using the shattered attack and the hash function should be considered broken. SHA-1 produces a hash digest of 160 bits (20 bytes).\n\nDocuments may refer to SHA-1 as just \"SHA\", even though this may conflict with the other Standard Hash Algorithms such as SHA-0, SHA-2 and SHA-3.\n\nRIPEMD (RACE Integrity Primitives Evaluation Message Digest) is a family of cryptographic hash functions developed in Leuven, Belgium, by Hans Dobbertin, Antoon Bosselaers and Bart Preneel at the COSIC research group at the Katholieke Universiteit Leuven, and first published in 1996. RIPEMD was based upon the design principles used in MD4, and is similar in performance to the more popular SHA-1. RIPEMD-160 has however not been broken. As the name implies, RIPEMD-160 produces a hash digest of 160 bits (20 bytes).\n\nIn computer science and cryptography, Whirlpool is a cryptographic hash function. It was designed by Vincent Rijmen and Paulo S. L. M. Barreto, who first described it in 2000. Whirlpool is based on a substantially modified version of the Advanced Encryption Standard (AES). Whirlpool produces a hash digest of 512 bits (64 bytes).\n\nSHA-2 (Secure Hash Algorithm 2) is a set of cryptographic hash functions designed by the United States National Security Agency (NSA), first published in 2001.[3] They are built using the Merkle–Damgård structure, from a one-way compression function itself built using the Davies–Meyer structure from a (classified) specialized block cipher.\n\nSHA-2 basically consists of two hash algorithms: SHA-256 and SHA-512. SHA-224 is a variant of SHA-256 with different starting values and truncated output. SHA-384 and the lesser known SHA-512/224 and SHA-512/256 are all variants of SHA-512. SHA-512 is more secure than SHA-256 and is commonly faster than SHA-256 on 64 bit machines such as AMD64.\n\nThe output size in bits is given by the extension to the \"SHA\" name, so SHA-224 has an output size of 224 bits (28 bytes), SHA-256 produces 32 bytes, SHA-384 produces 48 bytes and finally SHA-512 produces 64 bytes.\n\nSHA-3 (Secure Hash Algorithm 3) was released by NIST on August 5, 2015. SHA-3 is a subset of the broader cryptographic primitive family Keccak. The Keccak algorithm is the work of Guido Bertoni, Joan Daemen, Michael Peeters, and Gilles Van Assche. Keccak is based on a sponge construction which can also be used to build other cryptographic primitives such as a stream cipher. SHA-3 provides the same output sizes as SHA-2: 224, 256, 384 and 512 bits.\n\nConfigurable output sizes can also be obtained using the SHAKE-128 and SHAKE-256 functions. Here the -128 and -256 extensions to the name imply the security strength of the function rather than the output size in bits.\n\nAn improved version of BLAKE called BLAKE2 was announced in December 21, 2012. It was created by Jean-Philippe Aumasson, Samuel Neves, Zooko Wilcox-O'Hearn, and Christian Winnerlein with the goal to replace widely used, but broken MD5 and SHA-1 algorithms. When run on 64-bit x64 and ARM architectures, BLAKE2b is faster than SHA-3, SHA-2, SHA-1, and MD5. Although BLAKE nor BLAKE2 have not been standardized as SHA-3 it has been used in many protocols including the Argon2 password hash for the high efficiency that it offers on modern CPUs. As BLAKE was a candidate for SHA-3, BLAKE and BLAKE2 both offer the same output sizes as SHA-3 - including a configurable output size.\n\nThere is a long list of cryptographic hash functions but many have been found to be vulnerable and should not be used. For instance, NIST selected 51 hash functions as candidates for round 1 of the SHA-3 hash competition, of which 10 were considered broken and 16 showed significant weaknesses and therefore didn't make it to the next round; more information can be found on the main article about the NIST hash function competitions.\n\nEven if a hash function has never been broken, a successful attack against a weakened variant may undermine the experts' confidence. For instance, in August 2004 collisions were found in several then-popular hash functions, including MD5. These weaknesses called into question the security of stronger algorithms derived from the weak hash functions—in particular, SHA-1 (a strengthened version of SHA-0), RIPEMD-128, and RIPEMD-160 (both strengthened versions of RIPEMD).\n\nOn 12 August 2004, Joux, Carribault, Lemuet, and Jalby announced a collision for the full SHA-0 algorithm. Joux et al. accomplished this using a generalization of the Chabaud and Joux attack. They found that the collision had complexity 2 and took about 80,000 CPU hours on a supercomputer with 256 Itanium 2 processors—equivalent to 13 days of full-time use of the supercomputer.\n\nIn February 2005, an attack on SHA-1 was reported that would find collision in about 2 hashing operations, rather than the 2 expected for a 160-bit hash function. In August 2005, another attack on SHA-1 was reported that would find collisions in 2 operations. Other theoretical weaknesses of SHA-1 have been known: and in February 2017 Google announced a collision in SHA-1. Security researchers recommend that new applications can avoid these problems by using later members of the SHA family, such as SHA-2, or using techniques such as randomized hashing that do not require collision resistance.\n\nA successful, practical attack broke MD5 used within certificates for Transport Layer Security in 2008.\n\nMany cryptographic hashes are based on the Merkle–Damgård construction. All cryptographic hashes that directly use the full output of a Merkle–Damgård construction are vulnerable against length extension attacks. This makes the MD5, SHA-1, RIPEMD-160, Whirlpool and the SHA-256 / SHA-512 hash algorithms all vulnerable against this specific attack. SHA-3, BLAKE2 and the truncated SHA-2 variants are not vulnerable against this type of attack.\n\n\n\n"}
{"id": "3711980", "url": "https://en.wikipedia.org/wiki?curid=3711980", "title": "David Eugene Smith", "text": "David Eugene Smith\n\nDavid Eugene Smith (January 21, 1860 – July 29, 1944) was an American mathematician, educator, and editor.\n\nDavid Eugene Smith is considered one of the founders of the field of mathematics education. Smith was born in Cortland, New York, to Abram P. Smith, attorney and surrogate judge, and Mary Elizabeth Bronson, who taught her young son Latin and Greek. He attended Syracuse University, graduating in 1881 (Ph. D., 1887; LL.D., 1905). He studied to be a lawyer concentrating in arts and humanities, but accepted an instructorship in mathematics at the Cortland Normal School in 1884 where he attended as a young man. While at the Cortland Normal School Smith became a member of the Young Men's Debating Club (today the Delphic Fraternity.) He became a professor at the Michigan State Normal College in 1891 (later Eastern Michigan University), the principal at the State Normal School in Brockport, New York (1898), and a professor of mathematics at Teachers College, Columbia University (1901) where he remained until his retirement in 1926.\n\nSmith became president of the Mathematical Association of America in 1920 and served as the president of the History of Science Society in 1927. He also wrote a large number of publications of various types. He was editor of the \"Bulletin\" of the American Mathematical Society; contributed to other mathematical journals; published a series of textbooks; translated Felix Klein's \"Famous Problems of Geometry\", Fink's \"History of Mathematics\", and the Treviso Arithmetic. He edited Augustus De Morgan's \"A Budget of Paradoxes\" (1915) and wrote many books on Mathematics which are listed below.\n\n\n"}
{"id": "7991", "url": "https://en.wikipedia.org/wiki?curid=7991", "title": "Disperser", "text": "Disperser\n\nA disperser is a one-sided extractor. Where an extractor requires that every event gets the same probability under the uniform distribution and the extracted distribution, only the latter is required for a disperser. So for a disperser, an event formula_1 we have:\nformula_2\n\nDefinition (Disperser): \"A\" formula_3\"-disperser is a function\"\n\nformula_4\n\n\"such that for every distribution\" formula_5 \"on\" formula_6 \"with\" formula_7 \"the support of the distribution\" formula_8 \"is of size at least\" formula_9.\n\nAn (\"N\", \"M\", \"D\", \"K\", \"e\")-disperser is a bipartite graph with \"N\" vertices on the left side, each with degree \"D\", and \"M\" vertices on the right side, such that every subset of \"K\" vertices on the left side is connected to more than (1 − \"e\")\"M\" vertices on the right.\n\nAn extractor is a related type of graph that guarantees an even stronger property; every (\"N\", \"M\", \"D\", \"K\", \"e\")-extractor is also an (\"N\", \"M\", \"D\", \"K\", \"e\")-disperser.\n\nA disperser is a high-speed mixing device used to disperse or dissolve pigments and other solids into a liquid.\n\n"}
{"id": "39456961", "url": "https://en.wikipedia.org/wiki?curid=39456961", "title": "Dunham expansion", "text": "Dunham expansion\n\nIn quantum chemistry, the Dunham expansion is an expression for the rotational-vibrational energy levels of a diatomic molecule:\n\nwhere \"v\" and \"J\" are the vibrational and rotational quantum numbers.\nThe constant coefficients formula_2 are called Dunham parameters with formula_3 representing the electronic energy. The expression derives from a semiclassical treatment of a perturbational approach to deriving the energy levels. The Dunham parameters are typically calculated by a least-squares fitting procedure of energy levels with the quantum numbers.\n\nThis table adapts the sign conventions from the book of Huber and Herzberg.\n"}
{"id": "6997477", "url": "https://en.wikipedia.org/wiki?curid=6997477", "title": "Engineering technician", "text": "Engineering technician\n\nAn engineering technician is primarily trained in the skills and techniques related to a specific branch of engineering, with a practical understanding of the relevant engineering concepts. Engineering technicians often assist engineers and technologists in projects relating to research and development, or focus on post-development activities like implementation or operation. An engineering technician is between a skilled craft worker and a technologist.\n\nThe Dublin Accord was signed in 2002 as an international agreement for the recognition of engineering technician qualifications. The Dublin Accord is analogous to the Washington Accord for engineers and the Sydney Accord for engineering technologists.\n\nEngineering technicians help solve technical problems in many ways. They build or set up equipment, conduct experiments, and collect data and calculate results. They might also help to make a model of new equipment. Some technicians work in quality control, where they check products, do tests, and collect data. In manufacturing, they help to design and develop products. They also find ways to produce things efficiently.There are multiple fields in this job such as; software design, repair, etc.\nThey may also be people who produce technical drawings or engineering drawings.\n\nEngineering technician diplomas and 2-year degrees are generally offered by technical schools and non-university higher education institutions like colleges of further education, industrial schools, and community colleges. Many 4-year colleges and universities offer bachelor's degrees in engineering technology, but engineering technologists are somewhat different from technicians. In some countries, there were vocational education schools that may have also conferred the title without awarding any degrees.\n\nBoth Portugal and Spain use the title of \"engenharia técnica\"/\"ingeniería técnica\", literally \"technical engineering\", for the professionals who were awarded a short-cycle 3- to 4-year undergraduate degree (associate degree or bachelor's degree) in a technical engineering field by colleges or technical engineering institutes in the case of Portugal, and universities in Spain. Spanish \"technical engineers\" have full competency in their respective professional field of engineering, being the difference that the three or four year Engineers have competence only in their speciality (Mechanical, Electrical, Chemical, etc.)and the \"Engineering Superior School\" Engineers have wider competences.\n\nIn the United States, the Technology Accreditation Commission of the Accreditation Board for Engineering and Technology (ABET) grants 2-year associate degree programs to students that meet a set of specified standards. These programs include at least a college algebra and trigonometry course and, if needed, one or two basic science courses at any accredited school. The number of math and science prerequisite courses depends on the branch of engineering that the student chooses.\n\nEngineering technicians apply scientific and engineering skills usually gained in post secondary programs below the bachelor's degree level or through short-cycle bachelor's degrees. However, some university institutions award undergraduate degrees in the field of engineering which may confer the title of Engineering technician to the student, who is, however, eligible to become a fully chartered engineer after further studies at the master's degree level. Engineering technicians are called professional engineers in the UK only.\n\nIn Canada, the term certified engineering technician is protected by legislation and can only be used by technicians certified by provincial member organizations of the Canadian Council of Technicians and Technologists, such as the Ontario Association of Certified Engineering Technicians and Technologists and the Association of Science and Engineering Technology Professionals of Alberta.\n\nIn the United Kingdom, the term Engineering Technician and post-nominals EngTech are protected in civil law, and can only be used by technicians registered with the Engineering Council UK.\n\nThe term engineer is not protected in the UK, however those holding EngTech accreditation can be employed as Engineers, providing they have the additional experience/qualifications. In the UK the term \"Engineer\" is used by thousands of unskilled, semi-skilled and skilled trades.\n\nIn the eyes of professional engineers (but not the public) It is more acceptable to reserve the title 'Engineer' until you can fulfil the criteria required by the Engineering Council UK for Incorporated Engineer (IEng) or Chartered Engineer (CEng) status - A Bachelors / Masters Engineer Degree and additional professional practice experience (minimum 4 years), as well as a nominated sponsor who can verify your claims is the most common method for IEng/CEng accreditation.\n\nThis procedure helps reduce the saturation of the term 'Engineer', a concern to many who have completed years of undergraduate and postgraduate study and often work as 'Junior Support Engineers' for a number of years after graduation before being able to confidently state their profession.\n\n\n"}
{"id": "8181", "url": "https://en.wikipedia.org/wiki?curid=8181", "title": "Examples of differential equations", "text": "Examples of differential equations\n\nDifferential equations arise in many problems in physics, engineering, and other sciences. The following examples show how to solve differential equations in a few simple cases when an exact solution exists.\n\nEquations in the form formula_1 are called separable and solved by formula_2 and thus \nformula_3. Prior to dividing by formula_4, one needs to check if there are stationary (also called equilibrium)\nsolutions formula_5 satisfying formula_6.\n\nA separable \"linear\" ordinary differential equation of the first order \nmust be homogeneous and has the general form\n\nwhere formula_8 is some known function. We may solve this by separation of variables (moving the \"y\" terms to one side and the \"t\" terms to the other side),\n\nSince the separation of variables in this case involves dividing by \"y\", we must check if the constant function \"y=0\" is a solution of the original equation. Trivially, if \"y=0\" then \"y'=0\", so \"y=0\" is actually a solution of the original equation. We note that \"y=0\" is not allowed in the transformed equation.\n\nWe solve the transformed equation with the variables already separated by Integrating,\n\nwhere \"C\" is an arbitrary constant. Then, by exponentiation, we obtain\n\nHere, formula_12, so formula_13. But we have independently checked that \"y=0\" is also a solution of the original equation, thus \nwith an arbitrary constant \"A\", which covers all the cases. It is easy to confirm that this is a solution by plugging it into the original differential equation:\n\nSome elaboration is needed because \"ƒ\"(\"t\") might not even be integrable. One must also assume something about the domains of the functions involved before the equation is fully defined. The solution above assumes the real case.\n\nIf formula_16 is a constant, the solution is particularly simple, formula_17 and describes, e.g., if formula_18, the exponential decay of radioactive material at the macroscopic level. If the value of formula_19 is not known a priori, it can be determined from two measurements of the solution. For example,\n\ngives formula_21 and formula_22.\n\nFirst-order linear non-homogeneous ODEs (ordinary differential equations) are not separable. They can be solved by the following approach, known as an \"integrating factor\" method. Consider first-order linear ODEs of the general form:\n\nThe method for solving this equation relies on a special integrating factor, \"μ\":\n\nWe choose this integrating factor because it has the special property that its derivative is itself times the function we are integrating, that is:\n\nMultiply both sides of the original differential equation by \"μ\" to get:\n\nBecause of the special \"μ\" we picked, we may substitute \"dμ\"/\"dx\" for \"μ\" \"p\"(\"x\"), simplifying the equation to:\n\nUsing the product rule in reverse, we get:\n\nIntegrating both sides:\n\nFinally, to solve for \"y\" we divide both sides by formula_30:\n\nSince \"μ\" is a function of \"x\", we cannot simplify any further directly.\n\nSuppose a mass is attached to a spring which exerts an attractive force on the mass proportional to the extension/compression of the spring. For now, we may ignore any other forces (gravity, friction, etc.). We shall write the extension of the spring at a time \"t\" as \"x\"(\"t\"). Now, using Newton's second law we can write (using convenient units):\n\nwhere \"m\" is the mass and \"k\" is the spring constant that represents a measure of spring stiffness. For simplicity's sake, let us take \"m=k\" as an example.\n\nIf we look for solutions that have the form formula_33, where \"C\" is a constant, we discover the relationship formula_34, and thus formula_35 must be one of the complex numbers formula_36 or formula_37. Thus, using Euler's formula we can say that the solution must be of the form:\n\nSee a solution by WolframAlpha.\n\nTo determine the unknown constants \"A\" and \"B\", we need \"initial conditions\", i.e. equalities that specify the state of the system at a given time (usually \"t\" = 0).\n\nFor example, if we suppose at \"t\" = 0 the extension is a unit distance (\"x\" = 1), and the particle is not moving (\"dx\"/\"dt\" = 0). We have\n\nand so \"A\" = 1.\n\nand so \"B\" = 0.\n\nTherefore \"x\"(\"t\") = cos \"t\". This is an example of simple harmonic motion.\n\nSee a solution by Wolfram Alpha.\n\nThe above model of an oscillating mass on a spring is plausible but not very realistic: in practice, friction will tend to decelerate the mass and have magnitude proportional to its velocity (i.e. \"dx\"/\"dt\"). Our new differential equation, expressing the balancing of the acceleration and the forces, is\n\nwhere formula_42 is the damping coefficient representing friction. Again looking for solutions of the form formula_33, we find that\n\nThis is a quadratic equation which we can solve. If formula_45 there are two complex conjugate roots \"a\" ± \"ib\", and the solution (with the above boundary conditions) will look like this:\n\nLet us for simplicity take formula_47, then formula_48 and formula_49.\n\nThe equation can be also solved in MATLAB symbolic toolbox as\n\nalthough the solution looks rather ugly,\nThis is a model of a damped oscillator. The plot of displacement against time would look like this:\n\nwhich resembles how one would expect a vibrating spring to behave as friction removes energy from the system.\n\nThe following example of a first order linear systems of ODEs\n\ncan be easily solved symbolically using numerical analysis software.\n\n\n\n"}
{"id": "48870697", "url": "https://en.wikipedia.org/wiki?curid=48870697", "title": "Gamma-object", "text": "Gamma-object\n\nIn mathematics, a Γ-object of a pointed category \"C\" is a contravariant functor from Γ to \"C\".\n\nThe basic example is Segal's so-called Γ-space, which may be thought of as a generalization of simplicial abelian group (or simplicial abelian monoid). More precisely, one can define a Gamma space as an O-monoid object in an infinity-category. The notion plays a role in the generalization of algebraic K-theory that replaces an abelian group by something higher.\n"}
{"id": "697531", "url": "https://en.wikipedia.org/wiki?curid=697531", "title": "Geometric–harmonic mean", "text": "Geometric–harmonic mean\n\nIn mathematics, the geometric–harmonic mean M(\"x\", \"y\") of two positive real numbers \"x\" and \"y\" is defined as follows: we form the geometric mean of \"g\" = \"x\" and \"h\" = \"y\" and call it \"g\", i.e. \"g\" is the square root of \"xy\". We also form the harmonic mean of \"x\" and \"y\" and call it \"h\", i.e. \"h\" is the reciprocal of the arithmetic mean of the reciprocals of \"x\" and \"y\". These may be done sequentially (in any order) or simultaneously.\n\nNow we can iterate this operation with \"g\" taking the place of \"x\" and \"h\" taking the place of \"y\". In this way, two sequences (\"g\") and (\"h\") are defined:\n\nand \n\nBoth of these sequences converge to the same number, which we call the geometric–harmonic mean M(\"x\", \"y\") of \"x\" and \"y\". The geometric–harmonic mean is also designated as the harmonic–geometric mean. (cf. Wolfram MathWorld below.)\n\nThe existence of the limit can be proved by the means of Bolzano–Weierstrass theorem in a manner almost identical to the proof of existence of arithmetic–geometric mean.\n\nM(\"x\", \"y\") is a number between the geometric and harmonic mean of \"x\" and \"y\"; in particular it is between \"x\" and \"y\". M(\"x\", \"y\") is also homogeneous, i.e. if \"r\" > 0, then M(\"rx\", \"ry\") = \"r\" M(\"x\", \"y\").\n\nIf AG(\"x\", \"y\") is the arithmetic–geometric mean, then we also have\n\nWe have the following inequality involving the Pythagorean means {\"H\", \"G\", \"A\"} and iterated Pythagorean means {\"HG\", \"HA\", \"GA\"}:\n\nwhere the iterated Pythagorean means have been identified with their parts {\"H\", \"G\", \"A\"} in progressing order:\n\n\n"}
{"id": "12274200", "url": "https://en.wikipedia.org/wiki?curid=12274200", "title": "Herbrand structure", "text": "Herbrand structure\n\nIn first-order logic, a Herbrand structure S is a structure over a vocabulary σ, that is defined solely by the syntactical properties of σ. The idea is to take the symbols of terms as their values, e.g. the denotation of a constant symbol c is just 'c' (the symbol).\n\nHerbrand structures play an important role in the foundations of logic programming.\n\n\"Herbrand universe\" will serve as the universe in \"Herbrand structure\".\n\n(1) The \"Herbrand universe of a first-order language\" L, is the set of all ground terms of L. If the language has no constants, then the language is extended by adding an arbitrary new constant. \n\n\n(2) The \"Herbrand universe of a closed formula in Skolem normal form\" F, is the set of all terms without variables, that can be constructed using the function symbols and constants of F. If F has no constants, then F is extended by adding an arbitrary new constant.\n\n\nLet L be a first-order language with the vocabulary \nthen the Herbrand universe of L (or σ) is {c, f(c), g(c), f(f(c)), f(g(c)), g(f(c)), g(g(c)), ...}.\n\nNotice that the relation symbols are not relevant for a Herbrand universe.\n\nA \"Herbrand structure\" interprets terms on top of a \"Herbrand universe\".\n\nLet S be a structure, with vocabulary σ and universe \"U\". Let \"T\" be the set of all terms over σ and \"T\" be the subset of all variable-free terms. S is said to be a \"Herbrand structure\" iff\n\nFor a constant symbol c and a 1-ary function symbol f(.) we have the following interpretation:\n\nIn addition to the universe, defined in \"Herbrand universe\", and the term denotations, defined in \"Herbrand structure\", the \"Herbrand base\" completes the interpretation by denoting the relation symbols.\n\nA \"Herbrand base\" is the set of all ground atoms of whose argument terms are the Herbrand universe.\n\nFor a 2-ary relation symbol R, we get with the terms from above:\n\n\n"}
{"id": "44417413", "url": "https://en.wikipedia.org/wiki?curid=44417413", "title": "Ind-scheme", "text": "Ind-scheme\n\nIn algebraic geometry, an ind-scheme is a set-valued functor that can be written (represented) as a direct limit (i.e., inductive limit) of closed embedding of schemes.\n\n\n\n"}
{"id": "24339475", "url": "https://en.wikipedia.org/wiki?curid=24339475", "title": "Johansen test", "text": "Johansen test\n\nIn statistics, the Johansen test, named after Søren Johansen, is a procedure for testing cointegration of several, say \"k\", I(1) time series. For the presence of I(2) variables see Ch. 9 of his 1995 textbook. This test permits more than one cointegrating relationship so is more generally applicable than the Engle–Granger test which is based on the Dickey–Fuller (or the augmented) test for unit roots in the residuals from a single (estimated) cointegrating relationship.\n\nThere are two types of Johansen test, either with trace or with eigenvalue, and the inferences might be a little bit different. The null hypothesis for the trace test is that the number of cointegration vectors is \"r\"=\"r\"*<\"k\", vs. the alternative that \"r\"=\"k\". Testing proceeds sequentially for \"r\"*=1,2,etc. and the first non-rejection of the null is taken as an estimate of \"r\". The null hypothesis for the \"maximum eigenvalue\" test is as for the trace test but the alternative is \"r\"=\"r\"*+1 and, again, testing proceeds sequentially for \"r\"*=1,2,etc., with the first non-rejection used as an estimator for \"r\".\n\nJust like a unit root test, there can be a constant term, a trend term, both, or neither in the model. For a general VAR(\"p\") model:\n\nThere are two possible specifications for error correction: that is, two VECM (vector error correction models):\n\n1. The longrun VECM:\n\n2. The transitory VECM:\n\nBe aware that the two are the same. In both VECM (Vector Error Correction Model),\n\nInferences are drawn on Π, and they will be the same, so is the explanatory power.\n\n"}
{"id": "41274264", "url": "https://en.wikipedia.org/wiki?curid=41274264", "title": "K-theory spectrum", "text": "K-theory spectrum\n\nIn mathematics, given a ring \"R\", the K-theory spectrum of \"R\" is an Ω-spectrum formula_1 whose \"n\"-th term is given by, writing formula_2 for the suspension of \"R\",\nwhere \"+\" means the Quillen's + construction. By definition, formula_4.\n"}
{"id": "2811119", "url": "https://en.wikipedia.org/wiki?curid=2811119", "title": "L-notation", "text": "L-notation\n\nL-notation is an asymptotic notation analogous to big-O notation, denoted as formula_1 for a bound variable formula_2 tending to infinity. Like big-O notation, it is usually used to roughly convey the computational complexity of a particular algorithm.\n\nIt is defined as\n\nwhere \"c\" is a positive constant, and formula_4 is a constant formula_5.\n\nL-notation is used mostly in computational number theory, to express the complexity of algorithms for difficult number theory problems, e.g. sieves for integer factorization and methods for solving discrete logarithms. The benefit of this notation is that it simplifies the analysis of these algorithms. The formula_6 expresses the dominant term, and the formula_7 takes care of everything smaller.\nWhen formula_4 is 0, then\n\nis a polynomial function of ln \"n\"; when formula_4 is 1 then \n\nis a fully exponential function of ln \"n\" (and thereby polynomial in \"n\").\n\nIf formula_4 is between 0 and 1, the function is subexponential of ln \"n\" (and superpolynomial).\n\nMany general-purpose integer factorization algorithms have subexponential time complexities. The best is the general number field sieve, which has an expected running time of\n\nfor formula_14. The best such algorithm prior to the number field sieve was the quadratic sieve which has running time\n\nFor the elliptic curve discrete logarithm problem, the fastest general purpose algorithm is the baby-step giant-step algorithm, which has a running time on the order of the square-root of the group order \"n\". In \"L\"-notation this would be \n\nThe existence of the AKS primality test, which runs in polynomial time, means that the time complexity for primality testing is known to be at most\n\nwhere \"c\" has been proven to be at most 6.\n\nL-notation has been defined in various forms throughout the literature. The first use of it came from Carl Pomerance in his paper \"Analysis and comparison of some integer factoring algorithms\". This form had only the formula_18 parameter: the formula_4 in the formula was formula_20 for the algorithms he was analyzing. Pomerance had been using the letter formula_21 (or lower case formula_22) in this and previous papers for formulae that involved many logarithms.\n\nThe formula above involving two parameters was introduced by Arjen Lenstra and Hendrik Lenstra in their article on \"Algorithms in Number Theory\". It was introduced in their analysis of a discrete logarithm algorithm of Coppersmith. This is the most commonly used form in the literature today.\n\nThe Handbook of Applied Cryptography defines the L-notation with a big formula_23 around the formula presented in this article. This is not the standard definition. The big formula_23 would suggest that the running time is an upper bound. However, for the integer factoring and discrete logarithm algorithms that L-notation is commonly used for, the running time is not an upper bound, so this definition is not preferred.\n"}
{"id": "312383", "url": "https://en.wikipedia.org/wiki?curid=312383", "title": "Law of total probability", "text": "Law of total probability\n\nIn probability theory, the law (or formula) of total probability is a fundamental rule relating marginal probabilities to conditional probabilities. It expresses the total probability of an outcome which can be realized via several distinct events—hence the name.\n\nThe law of total probability is the proposition that if <math>\\left\\\n"}
{"id": "25172838", "url": "https://en.wikipedia.org/wiki?curid=25172838", "title": "List of things named after Joseph-Louis Lagrange", "text": "List of things named after Joseph-Louis Lagrange\n\nSeveral concepts from mathematics and physics are named after the mathematician and astronomer Joseph-Louis Lagrange, as are a crater on the moon and .\n\n\n\n\n"}
{"id": "18720", "url": "https://en.wikipedia.org/wiki?curid=18720", "title": "Lodovico Ferrari", "text": "Lodovico Ferrari\n\nLodovico de Ferrari (2 February 1522 – 5 October 1565) was an Italian mathematician.\n\nBorn in Bologna, Italy, Lodovico's grandfather, Bartholomew de Ferrari, was forced out of Milan to Bologna. Lodovico settled in Bologna, Italy and he began his career as the servant of Gerolamo Cardano. He was extremely bright, so Cardano started teaching him mathematics. Ferrari aided Cardano on his solutions for quadratic equations and cubic equations, and was mainly responsible for the solution of quartic equations that Cardano published. While still in his teens, Ferrari was able to obtain a prestigious teaching post in Rome after Cardano resigned from it and recommended him. Ferrari retired when young at 42 years old, and wealthy. He then moved back to his home town of Bologna where he lived with his widowed sister Maddalena to take up a professorship of mathematics at the University of Bologna in 1565. Shortly thereafter, he died of white arsenic poisoning, according to a legend - because of his sisters.\n"}
{"id": "4367424", "url": "https://en.wikipedia.org/wiki?curid=4367424", "title": "Lovász conjecture", "text": "Lovász conjecture\n\nIn graph theory, the Lovász conjecture (1969) is a classical problem on Hamiltonian paths in graphs. It says:\nOriginally László Lovász stated the problem in the opposite way, but\nthis version became standard. In 1996 László Babai published a conjecture sharply contradicting this conjecture, but both conjectures remain widely open. It is not even known if a single counterexample would necessarily lead to a series of counterexamples. \n\nThe problem of finding Hamiltonian paths in highly symmetric graphs is quite old. As Donald Knuth describes it in volume 4 of \"The Art of Computer Programming\", the problem originated in British campanology (bell-ringing). Such Hamiltonian paths and cycles are also closely connected to Gray codes. In each case the constructions are explicit.\n\nAnother version of Lovász conjecture states that\n\nThere are 5 known examples of vertex-transitive graphs with no Hamiltonian cycles (but with Hamiltonian paths): the complete graph formula_1, the Petersen graph, the Coxeter graph and two graphs derived from the Petersen and Coxeter graphs by replacing each vertex with a triangle.\n\nNone of the 5 vertex-transitive graphs with no Hamiltonian cycles is a Cayley graph. This observation leads to a weaker version of the conjecture:\n\nThe advantage of the Cayley graph formulation is that such graphs correspond to a finite group formula_2 and a\ngenerating set formula_3. Thus one can ask for which formula_2 and formula_3 the conjecture holds rather than attack it in full generality.\n\nFor directed Cayley graphs (digraphs) the Lovász conjecture is false. Various counterexamples were obtained by Robert Alexander Rankin. Still, many of the below results hold in this restrictive setting.\n\nEvery directed Cayley graph of an abelian group has a Hamiltonian path; however, every cyclic group whose order is not a prime power has a directed Cayley graph that does not have a Hamiltonian cycle.\nIn 1986, D. Witte proved that the Lovász conjecture holds for the Cayley graphs of p-groups. It is open even for dihedral groups, although for special sets of generators some progress has been made.\n\nWhen group formula_6 is a symmetric group, there are many attractive generating sets. For example, the Lovász conjecture holds in the following cases of generating sets:\n\nStong has shown that the conjecture holds for the Cayley graph of the wreath product Z wr Z with the natural minimal generating set when \"m\" is either even or three. In particular this holds for the cube-connected cycles, which can be generated as the Cayley graph of the wreath product Z wr Z.\n\nFor general finite groups, only a few results are known:\n\nFinally, it is known that for every finite group formula_2 there exists a generating set of size at most formula_17 such that the corresponding Cayley graph is Hamiltonian (Pak-Radoičić). This result is based on classification of finite simple groups.\n\nThe Lovász conjecture was also established for random generating sets of size formula_18.\n"}
{"id": "2211835", "url": "https://en.wikipedia.org/wiki?curid=2211835", "title": "Lustre (programming language)", "text": "Lustre (programming language)\n\nLustre is a formally defined, declarative, and synchronous dataflow programming language for programming reactive systems. It began as a research project in the early 1980s. A formal presentation of the language can be found in the 1991 Proceedings of the IEEE. In 1993 it progressed to practical, industrial use in a commercial product as the core language of the industrial environment SCADE, developed by Esterel Technologies. It is now used for critical control software in aircraft, helicopters, and nuclear power plants.\n\nA Lustre program is a series of \"node\" definitions, written as:\n\nWhere codice_1 is the name of the node, codice_2 is the name of the single input of this node and codice_3 is the name of the single output.\nIn this example the node codice_1 returns the negation of its input codice_2, which is the expected result.\n\nAdditional internal variables can be declared as follow: \nNote: The equations order doesn't matter, the order of lines codice_6 and codice_7 doesn't change the result.\n\n node Edge (X : bool) returns (E : bool);\n\n\n"}
{"id": "9569479", "url": "https://en.wikipedia.org/wiki?curid=9569479", "title": "Maclaurin's inequality", "text": "Maclaurin's inequality\n\nIn mathematics, Maclaurin's inequality, named after Colin Maclaurin, is a refinement of the inequality of arithmetic and geometric means. \n\nLet \"a\", \"a\", ..., \"a\" be positive real numbers, and for \"k\" = 1, 2, ..., \"n\" define the averages \"S\" as follows:\n\nThe numerator of this fraction is the elementary symmetric polynomial of degree \"k\" in the \"n\" variables \"a\", \"a\", ..., \"a\", that is, the sum of all products of \"k\" of the numbers \"a\", \"a\", ..., \"a\" with the indices in increasing order. The denominator is the number of terms in the numerator, the binomial coefficient formula_2\n\nMaclaurin's inequality is the following chain of inequalities:\n\nwith equality if and only if all the \"a\" are equal.\n\nFor \"n\" = 2, this gives the usual inequality of arithmetic and geometric means of two numbers. Maclaurin's inequality is well illustrated by the case \"n\" = 4:\n\nMaclaurin's inequality can be proved using the Newton's inequalities.\n\n"}
{"id": "44035209", "url": "https://en.wikipedia.org/wiki?curid=44035209", "title": "Musselman's theorem", "text": "Musselman's theorem\n\nIn Euclidean geometry, Musselman's theorem is a property of certain circles defined by an arbitrary triangle.\n\nSpecifically, let formula_1 be a triangle, and formula_2, formula_3, and formula_4 its vertices. Let formula_5, formula_6, and formula_7 be the vertices of the reflection triangle formula_8, obtained by mirroring each vertex of formula_1 across the opposite side. Let formula_10 be the circumcenter of formula_1. Consider the three circles formula_12, formula_13, and formula_14 defined by the points formula_15, formula_16, and formula_17, respectively. The theorem says that these three Musselman circles meet in a point formula_18, that is the inverse with respect to the circumcenter of formula_1 of the isogonal conjugate or the nine-point center of formula_1.\n\nThe common point formula_18 is the Gilbert point of formula_1, which is point formula_23 in Clark Kimberling's list of triangle centers.\n\nThe theorem was proposed as an advanced problem by John Rogers Musselman and René Goormaghtigh in 1939, and a proof was presented by them in 1941. A generalization of this result was stated and proved by Goormaghtigh.\n\nThe generalization of Musselman's theorem by Goormaghtigh does not mention the circles explicitly.\n\nAs before, let formula_2, formula_3, and formula_4 be the vertices of a triangle formula_1, and formula_10 its circumcenter. Let formula_29 be the orthocenter of formula_1, that is, the intersection of its three altitude lines. Let formula_31, formula_32, and formula_33 be three points on the segments formula_34, formula_35, and formula_36, such that formula_37. Consider the three lines formula_38, formula_39, and formula_40, perpendicular to formula_34, formula_35, and formula_36 though the points formula_31, formula_32, and formula_33, respectively. Let formula_47, formula_48, and formula_49 be the intersections of these perpendicular with the lines formula_50, formula_51, and formula_52, respectively.\n\nIt had been observed by Joseph Neuberg, in 1884, that the three points formula_47, formula_48, and formula_49 lie on a common line formula_56. Let formula_57 be the projection of the circumcenter formula_10 on the line formula_56, and formula_60 the point on formula_61 such that formula_62. Goormaghtigh proved that formula_60 is the inverse with respect to the circumcircle of formula_1 of the isogonal conjugate of the point formula_65 on the Euler line formula_66, such that formula_67.\n"}
{"id": "14240296", "url": "https://en.wikipedia.org/wiki?curid=14240296", "title": "One-way quantum computer", "text": "One-way quantum computer\n\nThe one-way or measurement based quantum computer (MBQC) is a method of quantum computing that first prepares an entangled \"resource state\", usually a cluster state or graph state, then performs single qubit measurements on it. It is \"one-way\" because the resource state is destroyed by the measurements.\n\nThe outcome of each individual measurement is random, but they are related in such a way that the computation always succeeds. In general the choices of basis for later measurements need to depend on the results of earlier measurements, and hence the measurements cannot all be performed at the same time.\n\nAny one-way computation can be made into a quantum circuit by using quantum gates to prepare the resource state. For cluster and graph resource states, this requires only one two-qubit gate per bond, so is efficient.\n\nConversely, any quantum circuit can be simulated by a one-way computer using a two-dimensional cluster state as the resource state, by laying out the circuit diagram on the cluster; Z measurements (formula_1 basis) remove physical qubits from the cluster, while measurements in the X-Y plane (formula_2 basis) teleport the logical qubits along the \"wires\" and perform the required quantum gates. This is also polynomially efficient, as the required size of cluster scales as the size of the circuit (qubits x timesteps), while the number of measurement timesteps scales as the number of circuit timesteps.\n\nMeasurement-based computation on a periodic 3D lattice cluster state can be used to implement topological quantum error correction. Topological cluster state computation is closely related to Kitaev's toric code, as the 3D topological cluster state can be constructed and measured over time by a repeated sequence of gates on a 2D array.\n\nOne-way quantum computation has been demonstrated by running the 2 qubit Grover's algorithm on a 2x2 cluster state of photons. A linear optics quantum computer based on one-way computation has been proposed.\n\nCluster states have also been created in optical lattices, but were not used for computation as the atom qubits were too close together to measure individually.\n\nIt has been shown that the (spin formula_3) AKLT state on a 2D Honeycomb lattice can be used as a resource for MBQC.\nMore recently it has been shown that a spin-mixture AKLT state can be used as a resource.\n\n"}
{"id": "5744042", "url": "https://en.wikipedia.org/wiki?curid=5744042", "title": "Partial geometry", "text": "Partial geometry\n\nAn incidence structure formula_1 consists of points formula_2, lines formula_3, and flags formula_4 where a point formula_5 is said to be incident with a line formula_6 if formula_7. It is a () partial geometry if there are integers formula_8 such that:\n\n\nA partial geometry with these parameters is denoted by formula_21.\n\n\n\nA partial linear space formula_32 of order formula_33 is called a semipartial geometry if there are integers formula_34 such that:\n\n\nA semipartial geometry is a partial geometry if and only if formula_45.\n\nIt can be easily shown that the collinearity graph of such a geometry is strongly regular with parameters \nformula_46.\n\nA nice example of such a geometry is obtained by taking the affine points of formula_47 and only those lines that intersect the plane at infinity in a point of a fixed Baer subplane; it has parameters formula_48.\n\n\n"}
{"id": "2604194", "url": "https://en.wikipedia.org/wiki?curid=2604194", "title": "Pierre Samuel", "text": "Pierre Samuel\n\nPierre Samuel (12 September 1921 – 23 August 2009) was a French mathematician, known for his work in commutative algebra and its applications to algebraic geometry. The two-volume work \"Commutative Algebra\" that he wrote with Oscar Zariski is a classic. Other books of his covered projective geometry and algebraic number theory. \n\nSamuel ran a Paris seminar during the 1960s, and became Professeur émérite at the Université Paris-Sud (Orsay). His lectures on unique factorization domains published by the Tata Institute of Fundamental Research played a significant role in computing the Picard group of a Zariski surface via the work of Jeffrey Lang and collaborators. The method was inspired by earlier work of Nathan Jacobson and Pierre Cartier another outstanding member of the Bourbaki group. Nicholas Katz related this to the concept of \"p\"-curvature of a connection introduced by Alexander Grothendieck.\n\nHe was a member of the Bourbaki group, and filmed some of their meetings. A French television documentary on Bourbaki broadcast some of this footage in 2000.\n\nSamuel was also active in issues of social justice, including concerns about environmental degradation (where he was influenced by Grothendieck), and arms control. He died in Paris in August 2009.\n\nHis doctoral students include Lucien Szpiro and Daniel Lazard.\n\nIn 1958 he was an invited speaker (\"Relations d'équivalence en géométrie algébrique\") at the ICM in Edinburgh. In 1969 he won the Lester R. Ford Award.\n\n\n\n"}
{"id": "366136", "url": "https://en.wikipedia.org/wiki?curid=366136", "title": "Pontryagin duality", "text": "Pontryagin duality\n\nIn mathematics, specifically in harmonic analysis and the theory of topological groups, Pontryagin duality explains the general properties of the Fourier transform on locally compact abelian groups, such as formula_1, the circle, or finite cyclic groups. The Pontryagin duality theorem itself states that locally compact abelian groups identify naturally with their bidual.\n\nThe subject is named after Lev Semenovich Pontryagin who laid down the foundations for the theory of locally compact abelian groups and their duality during his early mathematical works in 1934. Pontryagin's treatment relied on the group being second-countable and either compact or discrete. This was improved to cover the general locally compact abelian groups by Egbert van Kampen in 1935 and André Weil in 1940.\n\nPontryagin duality places in a unified context a number of observations about functions on the real line or on finite abelian groups:\n\n\nThe theory, introduced by Lev Pontryagin and combined with Haar measure introduced by John von Neumann, André Weil and others depends on the theory of the dual group of a locally compact abelian group.\n\nIt is analogous to the dual vector space of a vector space: a finite-dimensional vector space \"V\" and its dual vector space \"V*\" are not naturally isomorphic, but their endomorphism algebras (matrix algebras) are: formula_2 via the transpose. Similarly, a group \"G\" and its dual group formula_3 are not in general isomorphic, but their group algebras are: formula_4 via the Fourier transform, though one must carefully define these algebras analytically. More categorically, this is not just an isomorphism of endomorphism algebras, but an isomorphism of categories – see categorical considerations.\n\nA topological group is called \"locally compact\" if the underlying topological space is locally compact and Hausdorff. It's called \"abelian\" if the underlying group is abelian.\n\nExamples of locally compact abelian groups are:\n\n\nIf formula_13 is a locally compact \"abelian\" group, a character of formula_13 is a continuous group homomorphism from formula_13 with values in the circle group formula_9. The set of all characters on formula_13 can be made into a locally compact abelian group, called the \"dual group\" of formula_13 and denoted formula_19. The group operation on the dual group is given by pointwise multiplication of characters, the inverse of a character is its complex conjugate and the topology on the space of characters is that of uniform convergence on compact sets (i.e., the compact-open topology, viewing formula_3 as a subset of the space of all continuous functions from formula_13 to formula_9.). This topology in general is not metrizable. However, if the group formula_13 is a separable locally compact abelian group, then the dual group is metrizable.\n\nThis is analogous to the dual space in linear algebra: just as for a vector space formula_24 over a field formula_25, the dual space is formula_26, so too is the dual group formula_27. More abstractly, these are both examples of representable functors, being represented respectively by formula_25 and formula_9.\n\nA group that is isomorphic (as topological groups) to its dual group is called \"self-dual\". While the reals and finite cyclic groups are self-dual, the group and the dual group are not \"naturally\" isomorphic, and should be thought of as two different groups.\n\nThe dual of formula_8 is isomorphic to the circle group formula_9. A character on the infinite cyclic group of integers formula_8 under addition is determined by its value at the generator 1. Thus for any character formula_33 on formula_8, formula_35. Moreover, this formula defines a character for any choice of formula_36 in formula_9. The topology of uniform convergence on compact sets is in this case the topology of pointwise convergence. This is the topology of the circle group inherited from the complex numbers.\n\nThe dual of formula_9 is canonically isomorphic with formula_8. Indeed, a character on formula_9 is of the form formula_41 for formula_42 an integer. Since formula_9 is compact, the topology on the dual group is that of uniform convergence, which turns out to be the discrete topology.\n\nThe group of real numbers formula_1, is isomorphic to its own dual; the characters on formula_1 are of the form formula_46 for formula_47 a real number. With these dualities, the version of the Fourier transform to be introduced next coincides with the classical Fourier transform on formula_1.\n\nAnalogously, the group of formula_49-adic numbers formula_12 is isomorphic to its dual. (In fact, any finite extension of formula_12 is also self-dual.) It follows that the adeles are self-dual.\n\nCanonical means that there is a naturally defined map formula_54 ; more importantly, the map should be functorial in formula_13. The canonical isomorphism is defined on formula_56 as follows:\n\nIn other words, each group element formula_58 is identified to the evaluation character on the dual. This is strongly analogous to the canonical isomorphism between a finite-dimensional vector space and its double dual, formula_59. If formula_13 is a finite abelian group, then formula_61 but this isomorphism is not canonical. Making this statement precise (in general) requires thinking about dualizing not only on groups, but also on maps between the groups, in order to treat dualization as a functor and prove the identity functor and the dualization functor are not naturally equivalent. Also it should be noted that the duality theorem implies that for any group (not necessarily finite) the dualization functor is an exact functor.\n\nOne of the most remarkable facts about a locally compact group \"G\" is that it carries an essentially unique natural measure, the Haar measure, which allows one to consistently measure the \"size\" of sufficiently regular subsets of \"G\". \"Sufficiently regular subset\" here means a Borel set; that is, an element of the σ-algebra generated by the compact sets. More precisely, a right Haar measure on a locally compact group \"G\" is a countably additive measure μ defined on the Borel sets of \"G\" which is \"right invariant\" in the sense that μ(\"Ax\") = μ(\"A\") for \"x\" an element of \"G\" and \"A\" a Borel subset of \"G\" and also satisfies some regularity conditions (spelled out in detail in the article on Haar measure). Except for positive scaling factors, a Haar measure on \"G\" is unique.\n\nThe Haar measure on \"G\" allows us to define the notion of integral for (complex-valued) Borel functions defined on the group. In particular, one may consider various \"L\" spaces associated to the Haar measure μ. Specifically,\n\nNote that, since any two Haar measures on \"G\" are equal up to a scaling factor, this \"L\"-space is independent of the choice of Haar measure and thus perhaps could be written as \"L(G)\". However, the \"L\"-norm on this space depends on the choice of Haar measure, so if one wants to talk about isometries it is important to keep track of the Haar measure being used.\n\nThe dual group of a locally compact abelian group is used as the underlying space for an abstract version of the Fourier transform. If formula_63, then the Fourier transform is the function formula_64 on formula_3 defined by\n\nwhere the integral is relative to Haar measure formula_67 on formula_13. This is also denoted formula_69. Note the Fourier transform depends on the choice of Haar measure. It is not too difficult to show that the Fourier transform of an formula_70 function on formula_13 is a bounded continuous function on formula_3 which vanishes at infinity.\n\nThe \"inverse Fourier transform\" of an integrable function on formula_3 is given by\n\nwhere the integral is relative to the Haar measure formula_76 on the dual group formula_3. The measure formula_76 on formula_3 that appears in the Fourier inversion formula is called the dual measure to formula_67 and may be denoted formula_90.\n\nThe various Fourier transforms can be classified in terms of their domain and transform domain (the group and dual group) as follows:\n\nAs an example, suppose formula_91, so we can think about formula_3 as formula_5 by the pairing formula_94 If formula_67 is the Lebesgue measure on Euclidean space, we obtain the ordinary Fourier transform on formula_5 and the dual measure needed for the Fourier inversion formula is formula_97. If we want to get a Fourier inversion formula with the same measure on both sides (that is, since we can think about formula_5 as its own dual space we can ask for formula_90 to equal formula_67) then we need to use\n\nHowever, if we change the way we identify formula_5 with its dual group, by using the pairing\n\nthen Lebesgue measure on formula_5 is equal to its own dual measure. This convention minimizes the number of factors of formula_105 that show up in various places when computing Fourier transforms or inverse Fourier transforms on Euclidean space. (In effect it limits the formula_105 only to the exponent rather than as some messy factor outside the integral sign.) Note that the choice of how to identify formula_5 with its dual group affects the meaning of the term \"self-dual function\", which is a function on formula_5 equal to its own Fourier transform: using the classical pairing formula_109 the function formula_110 is self-dual, but using the (cleaner) pairing formula_111 makes formula_112 self-dual instead.\n\nThe space of integrable functions on a locally compact abelian group \"G\" is an algebra, where multiplication is convolution: the convolution of two integrable functions \"f\" and \"g\" is defined as\n\nThis algebra is referred to as the \"Group Algebra\" of \"G\". By the Fubini–Tonelli theorem, the convolution is submultiplicative with respect to the formula_70 norm, making formula_114 a Banach algebra. The Banach algebra formula_114 has a multiplicative identity element if and only if \"G\" is a discrete group, namely the function that is 1 at the identity and zero elsewhere. In general, however, it has an approximate identity which is a net (or generalized sequence) formula_118 indexed on a directed set formula_119 such that formula_120\n\nThe Fourier transform takes convolution to multiplication, i.e. it is a homomorphism of abelian Banach algebras formula_121 (of norm ≤ 1):\n\nIn particular, to every group character on \"G\" corresponds a unique \"multiplicative linear functional\" on the group algebra defined by\n\nIt is an important property of the group algebra that these exhaust the set of non-trivial (that is, not identically zero) multiplicative linear functionals on the group algebra; see section 34 of . This means the Fourier transform is a special case of the Gelfand transform.\n\nAs we have stated, the dual group of a locally compact abelian group is a locally compact abelian group in its own right and thus has a Haar measure, or more precisely a whole family of scale-related Haar measures.\n\nSince the complex-valued continuous functions of compact support on \"G\" are formula_124-dense, there is a unique extension of the Fourier transform from that space to a unitary operator\n\nand we have the formula\n\nNote that for non-compact locally compact groups \"G\" the space formula_114 does not contain formula_142, so the Fourier transform of general formula_124-functions on \"G\" is \"not\" given by any kind of integration formula (or really any explicit formula). To define the formula_124 Fourier transform one has to resort to some technical trick such as starting on a dense subspace like the continuous functions with compact support and then extending the isometry by continuity to the whole space. This unitary extension of the Fourier transform is what we mean by the Fourier transform on the space of square integrable functions.\n\nThe dual group also has an inverse Fourier transform in its own right; it can be characterized as the inverse (or adjoint, since it is unitary) of the formula_124 Fourier transform. This is the content of the formula_124 Fourier inversion formula which follows.\n\nIn the case formula_150 the dual group formula_3 is naturally isomorphic to the group of integers formula_8 and the Fourier transform specializes to the computation of coefficients of Fourier series of periodic functions.\n\nIf \"G\" is a finite group, we recover the discrete Fourier transform. Note that this case is very easy to prove directly.\n\nOne important application of Pontryagin duality is the following characterization of compact abelian topological groups:\n\nThat \"G\" being compact implies formula_3 is discrete or that \"G\" being discrete implies that formula_3 is compact is an elementary consequence of the definition of the compact-open topology on formula_3 and does not need Pontryagin duality. One uses Pontryagin duality to prove the converses.\n\nThe Bohr compactification is defined for any topological group \"G\", regardless of whether \"G\" is locally compact or abelian. One use made of Pontryagin duality between compact abelian groups and discrete abelian groups is to characterize the Bohr compactification of an arbitrary abelian \"locally compact\" topological group. The \"Bohr compactification\" \"B(G)\" of \"G\" is formula_158, where \"H\" has the group structure formula_3, but given the discrete topology. Since the inclusion map\n\nis continuous and a homomorphism, the dual morphism\n\nis a morphism into a compact group which is easily shown to satisfy the requisite universal property.\n\nSee also almost periodic function.\n\nIt is useful to regard the dual group functorially. In what follows, LCA is the category of locally compact abelian groups and continuous group homomorphisms. The dual group construction of formula_3 is a contravariant functor LCA → LCA, represented (in the sense of representable functors) by the circle group formula_9 as formula_164 In particular, the double dual functor formula_165 is \"covariant\".\n\nThis isomorphism is analogous to the double dual of finite-dimensional vector spaces (a special case, for real and complex vector spaces).\n\nThe duality interchanges the subcategories of discrete groups and compact groups. If \"R\" is a ring and \"G\" is a left \"R\"-module, the dual group formula_3 will become a right \"R\"-module; in this way we can also see that discrete left \"R\"-modules will be Pontryagin dual to compact right \"R\"-modules. The ring End(\"G\") of endomorphisms in LCA is changed by duality into its opposite ring (change the multiplication to the other order). For example, if \"G\" is an infinite cyclic discrete group, formula_3 is a circle group: the former has formula_168 so this is true also of the latter.\n\nSuch a theory cannot exist in the same form for non-commutative groups \"G\", since in that case the appropriate dual object formula_3 of isomorphism classes of representations cannot only contain one-dimensional representations, and will fail to be a group. The generalisation that has been found useful in category theory is called Tannaka–Krein duality; but this diverges from the connection with harmonic analysis, which needs to tackle the question of the Plancherel measure on formula_3.\n\nThere are analogues of duality theory for noncommutative groups, some of which are formulated in the language of C*-algebras.\n\nWhen \"G\" is a Hausdorff abelian topological group, the group formula_3 with the compact-open topology is a Hausdorff abelian topological group and the natural mapping from \"G\" to its double-dual \"G^^\" makes sense. If this mapping is an isomorphism, we say that \"G\" satisfies Pontryagin duality. This has been extended in a number directions beyond the case that \"G\" is locally compact.\n\n\nHowever, there is a fundamental aspect that changes if we want to consider Pontryagin duality beyond the locally compact case. In E. Martin-Peinador, \"A reflexible admissible topological group must be locally compact\", Proc. Amer. Math. Soc. 123 (1995), 3563–3566, it is proved that if \"G\" is a Hausdorff abelian topological group that satisfies Pontryagin duality and the natural evaluation pairing:\n\nis continuous, then \"G\" is locally compact. Thus any non-locally compact example of Pontryagin duality is a group where the natural evaluation pairing of \"G\" and formula_3 is not continuous.\n\n\nThe following books have chapters on locally compact abelian groups, duality and Fourier transform. The Dixmier reference (also available in English translation) has material on non-commutative harmonic analysis.\n\n"}
{"id": "764468", "url": "https://en.wikipedia.org/wiki?curid=764468", "title": "Post's theorem", "text": "Post's theorem\n\nIn computability theory Post's theorem, named after Emil Post, describes the connection between the arithmetical hierarchy and the Turing degrees. \n\nThe statement of Post's theorem uses several concepts relating to definability and recursion theory. This section gives a brief overview of these concepts, which are covered in depth in their respective articles. \n\nThe arithmetical hierarchy classifies certain sets of natural numbers that are definable in the language of Peano arithmetic. A formula is said to be formula_1 if it is an existential statement in prenex normal form (all quantifiers at the front) with formula_2 alternations between existential and universal quantifiers applied to a formula with bounded quantifiers only. Formally a formula formula_3 in the language of Peano arithmetic is a formula_1 formula if it is of the form\nwhere formula_6 contains only bounded quantifiers and \"Q\" is formula_7 if \"m\" is even and formula_8 if \"m\" is odd.\n\nA set of natural numbers \"A\" is said to be formula_9 if it is definable by a formula_9 formula, that is, if there is a formula_9 formula formula_3 such that each number \"n\" is in \"A\" if and only if formula_13 holds. It is known that if a set is formula_9 then it is formula_15 for any formula_16, but for each \"m\" there is a formula_17 set that is not formula_9. Thus the number of quantifier alternations required to define a set gives a measure of the complexity of the set.\n\nPost's theorem uses the relativized arithmetical hierarchy as well as the unrelativized hierarchy just defined. A set \"A\" of natural numbers is said to be formula_9 relative to a set \"B\", written formula_20, if\n\"A\" is definable by a formula_9 formula in an extended language that includes a predicate for membership in \"B\".\n\nWhile the arithmetical hierarchy measures definability of sets of natural numbers, Turing degrees measure the level of uncomputability of sets of natural numbers. A set \"A\" is said to be Turing reducible to a set \"B\", written formula_22, if there is an oracle Turing machine that, given an oracle for \"B\", computes the characteristic function of \"A\".\nThe Turing jump of a set \"A\" is a form of the Halting problem relative to \"A\". Given a set \"A\",\nthe Turing jump formula_23 is the set of indices of oracle Turing machines that halt on input \"0\" when run with oracle \"A\". It is known that every set \"A\" is Turing reducible to its Turing jump, but the Turing jump of a set is never Turing reducible to the original set. \n\nPost's theorem uses finitely iterated Turing jumps. For any set \"A\" of natural numbers, the notation\nformula_24 indicates the \"n\"-fold iterated Turing jump of \"A\". Thus formula_25 is just \"A\", and formula_26 is the Turing jump of formula_24.\n\nPost's theorem establishes a close connection between the arithmetical hierarchy and the Turing degrees of the form formula_28, that is, finitely iterated Turing jumps of the empty set. (The empty set could be replaced with any other computable set without changing the truth of the theorem.)\n\nPost's theorem states:\n\nPost's theorem has many corollaries that expose additional relationships between the arithmetical\nhierarchy and the Turing degrees. These include:\n\nThe operation of a Turing machine T on input n can be formalized logically in first-order arithmetic. For example, we may use symbols A, B and C for the tape configuration, machine state and location along the tape after k steps, respectively. T's transition system determines the relation between (A,B,C) and (A,B,C); their initial values (for k=0) are the input, the initial state and zero, respectively. The machine halts if and only if there is a number k such that B is the halting state.\n\nThe exact relation depends on the specific implementation of the notion of Turing machine (e.g. their alphabet, allowed mode of motion along the tape, etc.)\n\nIn case T halts at time n, the relation between (A,B,C) and (A,B,C) must be satisfied only for k bounded from above by n.\n\nThus there is a formula formula_46 in first-order arithmetic with no unbounded quantifiers, such that T halts on input n at time n at most if and only if formula_46 is satisfied.\n\nFor example, for a prefix-free Turing machine with binary alphabet and no blank symbol, we may use the following notations:\n\nFor a prefix-free Turing machine we may use, for input n, the initial tape configuration formula_48 where cat stands for concatenation; thus t(n) is a log(n)-length string of 1-s followed by 0 and then by n. \n\nThe operation of the Turing machine at the first n steps can thus be written as the conjunction of the initial conditions and the following formulas, quantified over k for all k<n:\n\nT halts on input n at time n at most if and only if formula_46 is satisfied, where:\n\nThis is a first-order arithmetic formula with no unbounded quantifiers, i.e. it is in formula_54.\n\nLet S be a set that can be recursively enumerated by a Turing machine. Then there is a Turing machine T that for every n in S, T halts when given n as an input.\n\nThis can be formalized by the first-order arithmetical formula presented above. The members of S are the numbers n satisfying the following formula:\n\nformula_55\n\nThis formula is in formula_56. Therefore, S is in formula_56.\nThus every recursively enumerable set is in formula_56. \n\nThe converse is true as well: for every formula formula_59 in formula_56 with k existential quantifiers, we may enumerate the k-tuples of natural numbers and run a Turing machine that goes through all of them until it finds the formula is satisfied. This Turing machine halts on precisely the set of natural numbers satisfying formula_59, and thus enumerates its corresponding set.\n\nSimilarly, the operation of an oracle machine T with an oracle O that halts after at most n steps on input n can be described by a first-order formula formula_62, except that the formula formula_63 now includes:\n\nIf the oracle is for a decision problem, O is always \"Yes\" or \"No\", which we may formalize as 0 or 1. Suppose the decision problem itself can be formalized by a first-order arithmetic formula formula_65.\nThen T halts on n after at most n steps if and only if the following formula is satisfied:\nformula_66\n\nwhere formula_67 is a first-order formula with no unbounded quantifiers.\n\nIf O is an oracle to the halting problem of a machine T', then formula_65 is the same as \"there exists m such that T' starting with input m is at the halting state after m steps\". \nThus:\nformula_69\nwhere formula_70 is a first-order formula that formalizes T'. If T' is a Turing machine (with no oracle), formula_70 is in formula_72 (i.e. it has no unbounded quantifiers).\n\nSince there is a finite number of numbers m satisfying formula_64, we may choose the same number of steps for all of them: there is a number m, such that T' halts after m steps precisely on those inputs formula_64 for which it halts at all.\n\nMoving to prenex normal form, we get that the oracle machine halts on input n if and only if the following formula is satisfied:\nformula_75\n\n(informally, there is a \"maximal number of steps\" m such every oracle that does not halt within the first m steps does not stop at all; however, for every m, each oracle that halts after m steps does halt).\n\nNote that we may replace both n and m by a single number - their maximum - without changing the truth value of formula_59. Thus we may write:\nformula_77\n\nFor the oracle to the halting problem over Turing machines, formula_70 is in formula_79 and formula_59 is in formula_81. Thus every set that is recursively enumerable by an oracle machine with an oracle for formula_82, is in formula_81.\n\nThe converse is true as well: Suppose formula_59 is a formula in formula_81 with k existential quantifiers followed by k universal quantifiers. Equivalently, formula_59 has k existential quantifiers followed by a negation of a formula in formula_56; the latter formula can be enumerated by a Turing machine and can thus be checked immediately by an oracle for formula_82.\n\nWe may thus enumerate the k-tuples of natural numbers and run an oracle machine with an oracle for formula_82 that goes through all of them until it finds a satisfaction for the formula. This oracle machine halts on precisely the set of natural numbers satisfying formula_59, and thus enumerates its corresponding set.\n\nMore generally, suppose every set that is recursively enumerable by an oracle machine with an oracle for formula_91 is in formula_92. Then for an oracle machine with an oracle for formula_93, formula_69 is in formula_92.\n\nSince formula_65 is the same as formula_59 for the previous Turing jump, it can be constructed (as we have just done with formula_59 above) so that formula_70 in formula_100. After moving to prenex formal form the new formula_59 is in formula_102.\n\nBy induction, every set that is recursively enumerable by an oracle machine with an oracle for formula_91, is in formula_92.\n\nThe other direction can be proven by induction as well: Suppose every formula in formula_92 can be enumerated by an oracle machine with an oracle for formula_91.\n\nNow Suppose formula_59 is a formula in formula_102 with k existential quantifiers followed by k universal quantifiers etc. Equivalently, formula_59 has k existential quantifiers followed by a negation of a formula in formula_92; the latter formula can be enumerated by an oracle machine with an oracle for formula_91 and can thus be checked immediately by an oracle for formula_93.\n\nWe may thus enumerate the k-tuples of natural numbers and run an oracle machine with an oracle for formula_93 that goes through all of them until it finds a satisfaction for the formula. This oracle machine halts on precisely the set of natural numbers satisfying formula_59, and thus enumerates its corresponding set.\n\nRogers, H. \"The Theory of Recursive Functions and Effective Computability\", MIT Press. ; \n\nSoare, R. \"Recursively enumerable sets and degrees.\" Perspectives in Mathematical Logic. Springer-Verlag, Berlin, 1987. \n"}
{"id": "23756", "url": "https://en.wikipedia.org/wiki?curid=23756", "title": "Presburger arithmetic", "text": "Presburger arithmetic\n\nPresburger arithmetic is the first-order theory of the natural numbers with addition, named in honor of Mojżesz Presburger, who introduced it in 1929. The signature of Presburger arithmetic contains only the addition operation and equality, omitting the multiplication operation entirely. The axioms include a schema of induction.\n\nPresburger arithmetic is much weaker than Peano arithmetic, which includes both addition and multiplication operations. Unlike Peano arithmetic, Presburger arithmetic is a decidable theory. This means it is possible to algorithmically determine, for any sentence in the language of Presburger arithmetic, whether that sentence is provable from the axioms of Presburger arithmetic. The asymptotic running-time computational complexity of this decision problem is at least doubly exponential, however, as shown by .\n\nThe language of Presburger arithmetic contains constants 0 and 1 and a binary function +, interpreted as addition. In this language, the axioms of Presburger arithmetic are the universal closures of the following:\n\n(5) is an axiom schema of induction, representing infinitely many axioms. Since the axioms in the schema in (5) cannot be replaced by any finite number of axioms, Presburger arithmetic is not finitely axiomatizable in first-order logic.\n\nPresburger arithmetic cannot formalize concepts such as divisibility or prime number. Generally, any number concept leading to multiplication cannot be defined in Presburger arithmetic, since that leads to incompleteness and undecidability. However, it can formulate individual instances of divisibility; for example, it proves \"for all \"x\", there exists \"y\" : (\"y\" + \"y\" = \"x\") ∨ (\"y\" + \"y\" + 1 = \"x\")\". This states that every number is either even or odd.\n\nMojżesz Presburger proved Presburger arithmetic to be:\n\nThe decidability of Presburger arithmetic can be shown using quantifier elimination, supplemented by reasoning about arithmetical congruence (Enderton 2001, p. 188).\n\nPeano arithmetic, which is Presburger arithmetic augmented with multiplication, is not decidable, as a consequence of the negative answer to the Entscheidungsproblem. By Gödel's incompleteness theorem, Peano arithmetic is incomplete and its consistency is not internally provable (but see Gentzen's consistency proof).\n\nThe decision problem for Presburger arithmetic is an interesting example in computational complexity theory and computation. Let \"n\" be the length of a statement in Presburger arithmetic. Then Fischer and Rabin (1974) proved that any decision algorithm for Presburger arithmetic has a worst-case runtime of at least formula_1, for some constant \"c\">0. Hence, the decision problem for Presburger arithmetic is an example of a decision problem that has been proved to require more than exponential run time. Fischer and Rabin also proved that for any reasonable axiomatization (defined precisely in their paper), there exist theorems of length \"n\" which have doubly exponential length proofs. Intuitively, this means there are computational limits on what can be proven by computer programs. Fischer and Rabin's work also implies that Presburger arithmetic can be used to define formulas which correctly calculate any algorithm as long as the inputs are less than relatively large bounds. The bounds can be increased, but only by using new formulas. On the other hand, a triply exponential upper bound on a decision procedure for Presburger Arithmetic was proved by Oppen (1978). A more tight complexity bound was shown using alternating complexity classes by .\n\nThe set of true statements in Presburger arithmetic (PA) is complete for TimeAlternations(2, n). Thus, its complexity is between double exponential nondeterministic time (2-NEXP) and double exponential space (2-EXPSPACE). Completeness is under polynomial time many-to-one reductions. (Also, note that while Presburger arithmetic is commonly abbreviated PA, in mathematics in general PA usually means Peano arithmetic.)\n\nFor a more fine-grained result, let PA(i) be the set of true Σ PA statements, and PA(i, j) the set of true Σ PA statements with each quantifier block limited to j variables. '<' is considered to be quantifier-free; here, bounded quantifiers are counted as quantifiers.\nPA(1, j) is in P, while PA(1) is NP-complete.\nFor i > 0 and j > 2, PA(i + 1, j) is Σ-complete. The hardness result only needs j>2 (as opposed to j=1) in the last quantifier block.\nFor i>0, PA(i+1) is Σ-complete (and is TimeAlternations(2, i)-complete). \n\nBecause Presburger arithmetic is decidable, automatic theorem provers for Presburger arithmetic exist. For example, the Coq proof assistant system features the tactic omega for Presburger arithmetic and the Isabelle proof assistant contains a verified quantifier elimination procedure by Nipkow (2010). The double exponential complexity of the theory makes it infeasible to use the theorem provers on complicated formulas, but this behavior occurs only in the presence of nested quantifiers: Oppen and Nelson (1980) describe an automatic theorem prover which uses the simplex algorithm on an extended Presburger arithmetic without nested quantifiers to prove some of the instances of quantifier-free Presburger arithmetic formulas. More recent satisfiability modulo theories solvers use complete integer programming techniques to handle quantifier-free fragment of Presburger arithmetic theory (King, Barrett, Tinelli 2014).\n\nPresburger arithmetic can be extended to include multiplication by constants, since multiplication is repeated addition. Most array subscript calculations then fall within the region of decidable problems. This approach is the basis of at least five proof-of-correctness systems for computer programs, beginning with the Stanford Pascal Verifier in the late 1970s and continuing through to Microsoft's Spec# system of 2005.\n\nSome properties are now given about integer relations definable in Presburger Arithmetic. For the sake of simplicity, all relations considered in this sections are over natural integers.\n\nA relation is Presburger-definable if and only if it is a semilinear set.\n\nA unary integer relation formula_2, that is, a set of natural integers, is Presburger-definable if and only if it is ultimately periodic. That is, if there exists a threshold formula_3 and a positive period formula_4 such that, for all integer formula_5 such that formula_6, formula_7 if and only if formula_8.\n\nBy the Cobham–Semenov theorem, a relation is Presburger-definable if and only if it is definable in Büchi arithmetic of base formula_9 for all formula_10. A relation definable in Büchi arithmetic of base formula_9 and formula_12 for formula_9 and formula_12 being multiplicatively independent integers is Presburger definable.\n\nAn integer relation formula_2 is Presburger-definable if and only if all sets of integers which are definable in first order logic with addition and formula_2 (that is, Presburger Arithmetic plus a predicate for formula_2) are Presburger-definable. Equivalently, for each relation formula_2 which is not Presburger-definable, there exists a first-order formula with addition and formula_2 which defines a set of integers which is not definable using only addition.\n\nPresburger-definable relations admit another characterization: by Muchnik's theorem. It is more complicated to state, but led to the proof of the two former characterizations. Before Muchnik's theorem can be stated, some additional definitions must be introduced.\n\nLet formula_20 be a set, the section formula_21 of formula_2, for formula_23 and formula_24 is defined as \n\nGiven two sets formula_26 and a formula_27-tuple of integers formula_28, the set formula_2 is called formula_30-periodic in formula_31 if, for all formula_32 such that formula_33 then formula_34 if and only if formula_35. For formula_36, the set formula_2 is said to be formula_38-periodic in formula_31 if it is formula_40-periodic for some formula_41 such that \n\ndenote the cube of size formula_9 whose lesser corner is formula_45.\n\nIntuitively, the integer formula_38 represents the length of a shift, the integer formula_9 is the size of the cubes and formula_59 is the threshold before the periodicity. This result remains true when the condition \n\nis replaced either by formula_61 or by formula_62.\n\nThis characterization led to the so-called \"definable criterion for definability in Presburger arithmetic\", that is: there exists a first-order formula with addition and a formula_27-ary predicate formula_2 which holds if and only if formula_2 is interpreted by a Presburger-definable relation. Muchnik's theorem also allows one to prove that it is decidable whether an automatic sequence accepts a Presburger-definable set.\n\n\n\n"}
{"id": "56899835", "url": "https://en.wikipedia.org/wiki?curid=56899835", "title": "Projection (measure theory)", "text": "Projection (measure theory)\n\nIn measure theory, projection map plays an important role in treating product spaces: The product sigma-algebra of measurable spaces is defined to be the finest such that the projection mappings will be measurable. Sometimes for some reasons product spaces are equipped with sigma-algebra different than \"the\" product sigma-algebra. In these cases the projections need not be measurable at all.\n\nThe projected set of a measurable set is called analytic set and need not be a measurable set. However, in some cases, either relatively to the product sigma-algebra or relatively to some other sigma-algebra, projected set of measurable set is indeed measurable.\n\nHenri Lebesgue himself, one of the founders of measure theory, was mistaken about that fact. In a paper from 1905 he wrote that the projection of Borel set in the plane onto the real line is again a Borel set. The mathematician Mikhail Yakovlevich Suslin found that error about ten years later, and his following research has led to descriptive set theory. The fundamental mistake of Lebesgue was to think that projection commutes with decreasing intersection, while there are simple counterexamples to that.\n\nAs an example for not measurable projection, one can take the space formula_1 with the sigma-algebra formula_2 and the space formula_1 with the sigma-algebra formula_4. The diagonal set formula_5 is not measurable relatively to formula_6, although the both projections are measurable sets.\n\nThe common example for a non-measurable set which is a projection of a measurable set, is in Lebesgue sigma-algebra. Let formula_7 be Lebesgue sigma-algebra of formula_8 and let formula_9 be the Lebesgue sigma-algebra of formula_10. For any bounded formula_11 not in formula_7, the set formula_13 is in formula_9, since Lebesgue measure is complete and the product set is contained in a set of zero measure.\n\nStill one can see that formula_9 is not the product sigma-algebra formula_16 but its completion. As for such example in product sigma-algebra, one can take the space formula_17 (or any product along a set with cardinality greater than continuum) with the product sigma-algebra formula_18 where formula_19 for every formula_20. In fact, in this case \"most\" of the projected sets are not measurable, since the cardinality of formula_21 is formula_22, whereas the cardinality of the projected sets is formula_23. There are also examples of Borel sets in the plane which their projection to the real line is not a Borel set, as suslin showed.\n\nThe following theorem gives a sufficient condition for the projection of measurable sets to be measurable.\n\nLet formula_24 be a measurable space and let formula_25 be a polish space where formula_26 is its Borel sigma-algebra. Then for every set in the product sigma-algebra formula_27, the projected set onto formula_28 is in universally measurable set relatively to formula_21.\n\nAn important spacial case of this theorem is that the projection of any Borel set ot formula_30 onto formula_31 where formula_32 is Lebesgue-measurable, even though it is not necessarily a Borel set. In addition, it means that the former example of non-Lebesgue-measurable set of formula_8 which is a projection of some measurable set of formula_10, is the only sort of such example.\n\n\n"}
{"id": "2327355", "url": "https://en.wikipedia.org/wiki?curid=2327355", "title": "Pseudo-Anosov map", "text": "Pseudo-Anosov map\n\nIn mathematics, specifically in topology, a pseudo-Anosov map is a type of a diffeomorphism or homeomorphism of a surface. It is a generalization of a linear Anosov diffeomorphism of the torus. Its definition relies on the notion of a measured foliation introduced by William Thurston, who also coined the term \"pseudo-Anosov diffeomorphism\" when he proved his classification of diffeomorphisms of a surface.\n\nA measured foliation \"F\" on a closed surface \"S\" is a geometric structure on \"S\" which consists of a singular foliation and a measure in the transverse direction. In some neighborhood of a regular point of \"F\", there is a \"flow box\" \"φ\": \"U\" → R which sends the leaves of \"F\" to the horizontal lines in R. If two such neighborhoods \"U\" and \"U\" overlap then there is a transition function \"φ\" defined on \"φ\"(\"U\"), with the standard property\n\nwhich must have the form\n\nfor some constant \"c\". This assures that along a simple curve, the variation in \"y\"-coordinate, measured locally in every chart, is a geometric quantity (i.e. independent of the chart) and permits the definition of a total variation along a simple closed curve on \"S\". A finite number of singularities of \"F\" of the type of \"\"p\"-pronged saddle\", \"p\"≥3, are allowed. At such a singular point, the differentiable structure of the surface is modified to make the point into a conical point with the total angle \"πp\". The notion of a diffeomorphism of \"S\" is redefined with respect to this modified differentiable structure. With some technical modifications, these definitions extend to the case of a surface with boundary.\n\nA homeomorphism\n\nof a closed surface \"S\" is called pseudo-Anosov if there exists a transverse pair of measured foliations on \"S\", \"F\" (stable) and \"F\" (unstable), and a real number \"λ\" > 1 such that the foliations are preserved by \"f\" and their transverse measures are multiplied by 1/\"λ\" and \"λ\". The number \"λ\" is called the stretch factor or dilatation of \"f\".\n\nThurston constructed a compactification of the Teichmüller space \"T\"(\"S\") of a surface \"S\" such that the action induced on \"T\"(\"S\") by any diffeomorphism \"f\" of \"S\" extends to a homeomorphism of the Thurston compactification. The dynamics of this homeomorphism is the simplest when \"f\" is a pseudo-Anosov map: in this case, there are two fixed points on the Thurston boundary, one attracting and one repelling, and the homeomorphism behaves similarly to a hyperbolic automorphism of the Poincaré half-plane. A \"generic\" diffeomorphism of a surface of genus at least two is isotopic to a pseudo-Anosov diffeomorphism.\n\nUsing the theory of train tracks, the notion of a pseudo-Anosov map has been extended to self-maps of graphs (on the topological side) and outer automorphisms of free groups (on the algebraic side). This leads to an analogue of Thurston classification for the case of automorphisms of free groups, developed by Bestvina and Handel.\n\n"}
{"id": "51910", "url": "https://en.wikipedia.org/wiki?curid=51910", "title": "Quantum key distribution", "text": "Quantum key distribution\n\nQuantum key distribution (QKD) is a secure communication method which implements a cryptographic protocol involving components of quantum mechanics. It enables two parties to produce a shared random secret key known only to them, which can then be used to encrypt and decrypt messages. It is often incorrectly called quantum cryptography, as it is the best-known example of a quantum cryptographic task.\n\nAn important and unique property of quantum key distribution is the ability of the two communicating users to detect the presence of any third party trying to gain knowledge of the key. This results from a fundamental aspect of quantum mechanics: the process of measuring a quantum system in general disturbs the system. A third party trying to eavesdrop on the key must in some way measure it, thus introducing detectable anomalies. By using quantum superpositions or quantum entanglement and transmitting information in quantum states, a communication system can be implemented that detects eavesdropping. If the level of eavesdropping is below a certain threshold, a key can be produced that is guaranteed to be secure (i.e. the eavesdropper has no information about it), otherwise no secure key is possible and communication is aborted.\n\nThe security of encryption that uses quantum key distribution relies on the foundations of quantum mechanics, in contrast to traditional public key cryptography, which relies on the computational difficulty of certain mathematical functions, and cannot provide any mathematical proof as to the actual complexity of reversing the one-way functions used. QKD has provable security based on information theory, and forward secrecy.\n\nQuantum key distribution is only used to produce and distribute a key, not to transmit any message data. This key can then be used with any chosen encryption algorithm to encrypt (and decrypt) a message, which can then be transmitted over a standard communication channel. The algorithm most commonly associated with QKD is the one-time pad, as it is provably secure when used with a secret, random key. In real-world situations, it is often also used with encryption using symmetric key algorithms like the Advanced Encryption Standard algorithm.\n\nQuantum communication involves encoding information in quantum states, or qubits, as opposed to classical communication's use of bits. Usually, photons are used for these quantum states. Quantum key distribution exploits certain properties of these quantum states to ensure its security. There are several different approaches to quantum key distribution, but they can be divided into two main categories depending on which property they exploit.\n\n\n\nThese two approaches can each be further divided into three families of protocols: discrete variable, continuous variable and distributed phase reference coding. Discrete variable protocols were the first to be invented, and they remain the most widely implemented. The other two families are mainly concerned with overcoming practical limitations of experiments. The two protocols described below both use discrete variable coding.\n\nThis protocol, known as BB84 after its inventors and year of publication, was originally described using photon polarization states to transmit the information. However, any two pairs of conjugate states can be used for the protocol, and many optical fibre based implementations described as BB84 use phase encoded states. The sender (traditionally referred to as Alice) and the receiver (Bob) are connected by a quantum communication channel which allows quantum states to be transmitted. In the case of photons this channel is generally either an optical fibre or simply free space. In addition they communicate via a public classical channel, for example using broadcast radio or the internet. The protocol is designed with the assumption that an eavesdropper (referred to as Eve) can interfere in any way with the quantum channel, while the classical channel needs to be authenticated.\n\nThe security of the protocol comes from encoding the information in non-orthogonal states. Quantum indeterminacy means that these states cannot in general be measured without disturbing the original state (see No cloning theorem). BB84 uses two pairs of states, with each pair conjugate to the other pair, and the two states within a pair orthogonal to each other. Pairs of orthogonal states are referred to as a basis. The usual polarization state pairs used are either the rectilinear basis of vertical (0°) and horizontal (90°), the diagonal basis of 45° and 135° or the circular basis of left- and right-handedness. Any two of these bases are conjugate to each other, and so any two can be used in the protocol. Below the rectilinear and diagonal bases are used.\n\nThe first step in BB84 is quantum transmission. Alice creates a random bit (0 or 1) and then randomly selects one of her two bases (rectilinear or diagonal in this case) to transmit it in. She then prepares a photon polarization state depending both on the bit value and basis, as shown in the adjacent table. So for example a 0 is encoded in the rectilinear basis (+) as a vertical polarization state, and a 1 is encoded in the diagonal basis (x) as a 135° state. Alice then transmits a single photon in the state specified to Bob, using the quantum channel. This process is then repeated from the random bit stage, with Alice recording the state, basis and time of each photon sent.\n\nAccording to quantum mechanics (particularly quantum indeterminacy), no possible measurement distinguishes between the 4 different polarization states, as they are not all orthogonal. The only possible measurement is between any two orthogonal states (an orthonormal basis). So, for example, measuring in the rectilinear basis gives a result of horizontal or vertical. If the photon was created as horizontal or vertical (as a rectilinear eigenstate) then this measures the correct state, but if it was created as 45° or 135° (diagonal eigenstates) then the rectilinear measurement instead returns either horizontal or vertical at random. Furthermore, after this measurement the photon is polarized in the state it was measured in (horizontal or vertical), with all information about its initial polarization lost.\n\nAs Bob does not know the basis the photons were encoded in, all he can do is to select a basis at random to measure in, either rectilinear or diagonal. He does this for each photon he receives, recording the time, measurement basis used and measurement result. After Bob has measured all the photons, he communicates with Alice over the public classical channel. Alice broadcasts the basis each photon was sent in, and Bob the basis each was measured in. They both discard photon measurements (bits) where Bob used a different basis, which is half on average, leaving half the bits as a shared key.\n\nTo check for the presence of an eavesdropper, Alice and Bob now compare a predetermined subset of their remaining bit strings. If a third party (usually referred to as Eve, for \"eavesdropper\") has gained any information about the photons' polarization, this introduces errors in Bob's measurements. Other environmental conditions can cause errors in a similar fashion. If more than formula_1 bits differ they abort the key and try again, possibly with a different quantum channel, as the security of the key cannot be guaranteed. formula_1 is chosen so that if the number of bits known to Eve is less than this, privacy amplification can be used to reduce Eve's knowledge of the key to an arbitrarily small amount at the cost of reducing the length of the key.\n\nArtur Ekert's scheme uses entangled pairs of photons. These can be created by Alice, by Bob, or by some source separate from both of them, including eavesdropper Eve. The photons are distributed so that Alice and Bob each end up with one photon from each pair.\n\nThe scheme relies on two properties of entanglement. First, the entangled states are perfectly correlated in the sense that if Alice and Bob both measure whether their particles have vertical or horizontal polarizations, they always get the same answer with 100% probability. The same is true if they both measure any other pair of complementary (orthogonal) polarizations. This necessitates that the two distant parties have exact directionality synchronization. However, the particular results are completely random; it is impossible for Alice to predict if she (and thus Bob) will get vertical polarization or horizontal polarization. Second, any attempt at eavesdropping by Eve destroys these correlations in a way that Alice and Bob can detect.\n\nSimilarly to BB84, the protocol involves a private measurement protocol before detecting the presence of Eve. The measurement stage involves Alice measuring each photon she receives using some basis from the set formula_3 while Bob chooses from formula_4 where formula_5 is the formula_6 of their key bits (thus discarding them as key bits, as they are no longer secret) the probability they find disagreement and identify the presence of Eve is\n\nformula_7\n\nSo to detect an eavesdropper with probability formula_8 Alice and Bob need to compare formula_9 key bits.\n\nQuantum key distribution is vulnerable to a man-in-the-middle attack when used without authentication to the same extent as any classical protocol, since no known principle of quantum mechanics can distinguish friend from foe. As in the classical case, Alice and Bob cannot authenticate each other and establish a secure connection without some means of verifying each other's identities (such as an initial shared secret). If Alice and Bob have an initial shared secret then they can use an unconditionally secure authentication scheme (such as Carter-Wegman,) along with quantum key distribution to exponentially expand this key, using a small amount of the new key to authenticate the next session. Several methods to create this initial shared secret have been proposed, for example using a 3rd party or chaos theory. Nevertheless, only \"almost strongly universal\" family of hash functions can be used for unconditionally secure authentication.\n\nIn the BB84 protocol Alice sends quantum states to Bob using single photons. In practice many implementations use laser pulses attenuated to a very low level to send the quantum states. These laser pulses contain a very small number of photons, for example 0.2 photons per pulse, which are distributed according to a Poissonian distribution. This means most pulses actually contain no photons (no pulse is sent), some pulses contain 1 photon (which is desired) and a few pulses contain 2 or more photons. If the pulse contains more than one photon, then Eve can split off the extra photons and transmit the remaining single photon to Bob. This is the basis of the photon number splitting attack, where Eve stores these extra photons in a quantum memory until Bob detects the remaining single photon and Alice reveals the encoding basis. Eve can then measure her photons in the correct basis and obtain information on the key without introducing detectable errors.\n\nEven with the possibility of a PNS attack a secure key can still be generated, as shown in the GLLP security proof; however, a much higher amount of privacy amplification is needed reducing the secure key rate significantly (with PNS the rate scales as formula_10 as compared to formula_11 for a single photon sources, where formula_11 is the transmittance of the quantum channel).\n\nThere are several solutions to this problem. The most obvious is to use a true single photon \nsource instead of an attenuated laser. While such sources are still at a developmental stage QKD has been carried out successfully with them. However, as current sources operate at a low efficiency and frequency key rates and transmission distances are limited. Another solution is to modify the BB84 protocol, as is done for example in the SARG04 protocol, in which the secure key rate scales as formula_13. The most promising solution is the decoy states in which Alice randomly sends some of her laser pulses with a lower average photon number. These decoy states can be used to detect a PNS attack, as Eve has no way to tell which pulses are signal and which decoy. Using this idea the secure key rate scales as formula_11, the same as for a single photon source. This idea has been implemented successfully first at the University of Toronto, and in several follow-up QKD experiments, allowing for high key rates secure against all known attacks.\n\nBecause currently a dedicated fibre optic line (or line of sight in free space) is required between the two points linked by quantum key distribution, a denial of service attack can be mounted by simply cutting or blocking the line. This is one of the motivations for the development of quantum key distribution networks, which would route communication via alternate links in case of disruption.\n\nA quantum key distribution system may be probed by Eve by sending in bright light from the quantum channel and analyzing the back-reflections in a Trojan-horse attack. In a recent research study it has been shown that Eve discerns Bob's secret basis choice with higher than 90% probability, breaching the security of the system.\n\nIf Eve is assumed to have unlimited resources, for example both classical and quantum computing power, there are many more attacks possible. BB84 has been proven secure against any attacks allowed by quantum mechanics, both for sending information using an ideal photon source which only ever emits a single photon at a time, and also using practical photon sources which sometimes emit multiphoton pulses. These proofs are unconditionally secure in the sense that no conditions are imposed on the resources available to the eavesdropper; however, there are other conditions required:\n\nHacking attacks target vulnerabilities in the operation of a QKD protocol or deficiencies in the components of the physical devices used in construction of the QKD system. If the equipment used in quantum key distribution can be tampered with, it could be made to generate keys that were not secure using a random number generator attack. Another common class of attacks is the Trojan horse attack which does not require physical access to the endpoints: rather than attempt to read Alice and Bob's single photons, Eve sends a large pulse of light back to Alice in between transmitted photons. Alice's equipment reflects some of Eve's light, revealing the state of Alice's basis (e.g., a polarizer). This attack can be detected, e.g. by using a classical detector to check the non-legitimate signals (i.e. light from Eve) entering Alice's system. It is also conjectured that most hacking attacks can similarly be defeated by modifying the implementation, though there is no formal proof.\n\nSeveral other attacks including faked-state attacks, phase remapping attacks, and time-shift attacks are now known. The time-shift attack has even been demonstrated on a commercial quantum cryptosystem. This is the first demonstration of quantum hacking against a non-homemade quantum key distribution system. Later on, the phase-remapping attack was also demonstrated on a specially configured, research oriented open QKD system (made and provided by the Swiss company Id Quantique under their Quantum Hacking program). It is one of the first ‘intercept-and-resend’ attacks on top of a widely used QKD implementation in commercial QKD systems. This work has been widely reported in media.\n\nThe first attack that claimed to be able to eavesdrop the whole key without leaving any trace was demonstrated in 2010. It was experimentally shown that the single-photon detectors in two commercial devices could be fully remote-controlled using specially tailored bright illumination. In a spree of publications thereafter, the collaboration between the Norwegian University of Science and Technology in Norway and Max Planck Institute for the Science of Light in Germany, has now demonstrated several methods to successfully eavesdrop on commercial QKD systems based on weaknesses of Avalanche photodiodes (APDs) operating in gated mode. This has sparked research on new approaches to securing communications networks.\n\nThe task of distributing a secret key could be achieved even when the particle (on which the secret information, e.g. polarization, has been encoded) does not traverse through the quantum channel using a protocol developed by Tae-Gon Noh. Here Alice generates a photon which randomly takes either path (a) or path (b). Path (a) stays inside Alice's secure device and path (b) goes to Bob. By rejecting the photons that Bob receives and only accepting the ones he doesn't receive, Bob & Alice can set up a secure channel, i.e. Eve's attempts to read the \"counterfactual\" photons would still be detected. This protocol uses the quantum phenomenon whereby the possibility that a photon can be sent has an effect even when it isn't sent. So-called interaction-free measurement also uses this quantum effect, as for example in the bomb testing problem, whereby you can determine which bombs are not duds without setting them off, except in a counterfactual sense.\n\nQuantum cryptography was proposed first by Stephen Wiesner, then at Columbia University in New York, who, in the early 1970s, introduced the concept of quantum conjugate coding. His seminal paper titled \"Conjugate Coding\" was rejected by IEEE Information Theory but was eventually published in 1983 in SIGACT News (15:1 pp. 78–88, 1983). In this paper he showed how to store or transmit two messages by encoding them in two \"conjugate observables\", such as linear and circular polarization of light, so that either, but not both, of which may be received and decoded. He illustrated his idea with a design of unforgeable bank notes. A decade later, building upon this work, Charles H. Bennett, of the IBM Thomas J. Watson Research Center, and Gilles Brassard, of the University of Montreal, proposed a method for secure communication based on Wiesner’s \"conjugate observables\". In 1990, Artur Ekert, then a Ph.D. student at Wolfson College, University of Oxford, developed a different approach to quantum key distribution based on peculiar quantum correlations known as quantum entanglement.\n\nThe current commercial systems are aimed mainly at governments and corporations with high security requirements. Key distribution by courier is typically used in such cases, where traditional key distribution schemes are not believed to offer enough guarantee. This has the advantage of not being intrinsically distance limited, and despite long travel times the transfer rate can be high due to the availability of large capacity portable storage devices. The major difference of quantum key distribution is the ability to detect any interception of the key, whereas with courier the key security cannot be proven or tested. QKD (Quantum Key Distribution) systems also have the advantage of being automatic, with greater reliability and lower operating costs than a secure human courier network.\n\nFactors preventing wide adoption of quantum key distribution outside high security areas include the cost of equipment, and the lack of a demonstrated threat to existing key exchange protocols. However, with optic fibre networks already present in many countries the infrastructure is in place for a more widespread use.\n\nAn Industry Specification Group (ISG) of the European Telecommunications Standards Institute (ETSI) has been set up to address standardisation issues in quantum cryptography. A European Metrology Research Programme project ‘Metrology for Industrial Communications’ is developing the measurements required to characterise the optical components of faint-pulse QKD systems.\n\n\n\n\n\n\n\n\n\n"}
{"id": "5784135", "url": "https://en.wikipedia.org/wiki?curid=5784135", "title": "Rational representation", "text": "Rational representation\n\nIn mathematics, in the representation theory of algebraic groups, a linear representation of an algebraic group is said to be rational if, viewed as a map from the group to the general linear group, it is a rational map of algebraic varieties.\n\nFinite direct sums and products of rational representations are rational.\n\nA rational formula_1 module is a module that can be expressed as a sum (not necessarily direct) of rational representations.\n\n"}
{"id": "27832980", "url": "https://en.wikipedia.org/wiki?curid=27832980", "title": "Rigid transformation", "text": "Rigid transformation\n\nIn mathematics, a rigid transformation (also called Euclidean transformation or Euclidean isometry) is a geometric transformation of a Euclidean space that preserves the Euclidean distance between every pair of points.\n\nThe rigid transformations include rotations, translations, reflections, or their combination. Sometimes reflections are excluded from the definition of a rigid transformation by imposing that the transformation also preserve the handedness of figures in the Euclidean space (a reflection would not preserve handedness; for instance, it would transform a left hand into a right hand). To avoid ambiguity, this smaller class of transformations is known as proper rigid transformations (informally, also known as roto-translations). In general, any proper rigid transformation can be decomposed as a rotation followed by a translation, while any rigid transformation can be decomposed as an improper rotation followed by a translation (or as a sequence of reflections).\n\nAny object will keep the same shape and size after a proper rigid transformation.\n\nAll rigid transformations are examples of affine transformations. The set of all (proper and improper) rigid transformations is a group called the Euclidean group, denoted E(\"n\") for \"n\"-dimensional Euclidean spaces. The set of proper rigid transformation is called special Euclidean group, denoted SE(\"n\").\n\nIn kinematics, proper rigid transformations in a 3-dimensional Euclidean space, denoted SE(3), are used to represent the linear and angular displacement of rigid bodies. According to Chasles' theorem, every rigid transformation can be expressed as a screw displacement.\n\nA rigid transformation is formally defined as a transformation that, when acting on any vector v, produces a transformed vector \"T\"(v) of the form\nwhere \"R\" = \"R\" (i.e., \"R\" is an orthogonal transformation), and t is a vector giving the translation of the origin.\n\nA proper rigid transformation has, in addition,\nwhich means that \"R\" does not produce a reflection, and hence it represents a rotation (an orientation-preserving orthogonal transformation). Indeed, when an orthogonal transformation matrix produces a reflection, its determinant is –1.\n\nA measure of distance between points, or metric, is needed in order to confirm that a transformation is rigid. The Euclidean distance formula for R is the generalization of the Pythagorean theorem. The formula gives the distance squared between two points X and Y as the sum of the squares of the distances along the coordinate axes, that is\nwhere X=(X, X, ..., X) and Y=(Y, Y, ..., Y), and the dot denotes the scalar product.\n\nUsing this distance formula, a rigid transformation \"g\":R→R has the property,\n\nA translation of a vector space adds a vector d to every vector in the space, which means it is the transformation \"g\"(v):v→v+d. It is easy to show that this is a rigid transformation by computing,\n\nA linear transformation of a vector space, \"L\":R→ R, has the property that the transformation of a vector, V=av+bw, is the sum of the transformations of its components, that is,\nEach linear transformation \"L\" can be formulated as a matrix operation, which means \"L\":v→[L]v, where [L] is an nxn matrix.\n\nA linear transformation is a rigid transformation if it satisfies the condition,\nthat is\nNow use the fact that the scalar product of two vectors v.w can be written as the matrix operation vw, where the T denotes the matrix transpose, we have\nThus, the linear transformation \"L\" is rigid if its matrix satisfies the condition\nwhere [I] is the identity matrix. Matrices that satisfy this condition are called \"orthogonal matrices.\" This condition actually requires the columns of these matrices to be orthogonal unit vectors.\n\nMatrices that satisfy this condition form a mathematical group under the operation of matrix multiplication called the \"orthogonal group of nxn matrices\" and denoted \"O(n)\".\n\nCompute the determinant of the condition for an orthogonal matrix to obtain\nwhich shows that the matrix [L] can have a determinant of either +1 or -1. Orthogonal matrices with determinant -1 are reflections, and those with determinant +1 are rotations. Notice that the set of orthogonal matrices can be viewed as consisting of two manifolds in R separated by the set of singular matrices.\n\nThe set of rotation matrices is called the \"special orthogonal group,\" and denoted \"SO(n).\" It is an example of a Lie group because it has the structure of a manifold.\n"}
{"id": "6679463", "url": "https://en.wikipedia.org/wiki?curid=6679463", "title": "Sublime number", "text": "Sublime number\n\nIn number theory, a sublime number is a positive integer which has a perfect number of positive factors (including itself), and whose positive factors add up to another perfect number.\n\nThe number 12, for example, is a sublime number. It has a perfect number of positive factors (6): 1, 2, 3, 4, 6, and 12, and the sum of these is again a perfect number: 1 + 2 + 3 + 4 + 6 + 12 = 28.\n\nThere are only two known sublime numbers, 12 and (2)(2 − 1)(2 − 1)(2 − 1)(2 − 1)(2 − 1)(2 − 1) . The second of these has 76 decimal digits:\n"}
{"id": "669090", "url": "https://en.wikipedia.org/wiki?curid=669090", "title": "Trigonometric constants expressed in real radicals", "text": "Trigonometric constants expressed in real radicals\n\nExact algebraic expressions for trigonometric values are sometimes useful, mainly for simplifying solutions into radical forms which allow further simplification.\n\nAll trigonometric numbers – sines or cosines of rational multiples of 360° – are algebraic numbers (solutions of polynomial equations with integer coefficients); moreover they may be expressed in terms of radicals of complex numbers; but not all of these are expressible in terms of \"real\" radicals. When they are, they are expressible more specifically in terms of square roots.\n\nAll values of the sines, cosines, and tangents of angles at 3° increments are expressible in terms of square roots, using identities – the half-angle identity, the double-angle identity, and the angle addition/subtraction identity – and using values for 0°, 30°, 36°, and 45°. For an angle of an integer number of degrees that is not a multiple 3° ( radians), the values of sine, cosine, and tangent cannot be expressed in terms of real radicals.\n\nAccording to Niven's theorem, the only rational values of the sine function for which the argument is a rational number of degrees are 0, ,  1, −, and −1.\n\nAccording to Baker's theorem, if the value of a sine, a cosine or a tangent is algebraic, then the angle is either a rational number of degrees or a transcendental number of degrees. That is, if the angle is an algebraic, but non-rational, number of degrees, the trigonometric functions all have transcendental values.\n\nThe list in this article is incomplete in several senses. First, the trigonometric functions of all angles that are integer multiples of those given can also be expressed in radicals, but some are omitted here.\n\nSecond, it is always possible to apply the half-angle formula to find an expression in radicals for a trigonometric function of one-half of any angle on the list, then half of that angle, etc.\n\nThird, expressions in real radicals exist for a trigonometric function of a rational multiple of if and only if the denominator of the fully reduced rational multiple is a power of 2 by itself or the product of a power of 2 with the product of distinct Fermat primes, of which the known ones are 3, 5, 17, 257, and 65537.\n\nFourth, this article only deals with trigonometric function values when the expression in radicals is in \"real\" radicals – roots of real numbers. Many other trigonometric function values are expressible in, for example, cube roots of complex numbers that cannot be rewritten in terms of roots of real numbers. For example, the trigonometric function values of any angle that is one-third of an angle \"θ\" considered in this article can be expressed in cube roots and square roots by using the cubic equation formula to solve\n\nbut in general the solution for the cosine of the one-third angle involves the cube root of a complex number (giving \"casus irreducibilis\").\n\nIn practice, all values of sines, cosines, and tangents not found in this article are approximated using the techniques described at \"Trigonometric tables\".\n\nSeveral different units of angle measure are widely used, including degrees, radians, and gradians (gons):\nThe following table shows the conversions and values for some common angles:\n\nValues outside the [0°, 45°] angle range are trivially derived from these values, using circle axis reflection symmetry. (See List of trigonometric identities.)\n\nIn the entries below, when a certain number of degrees is related to a regular polygon, the relation is that the number of degrees in each angle of the polygon is (\"n\" – 2) times the indicated number of degrees (where \"n\" is the number of sides). This is because the sum of the angles of any \"n\"-gon is 180° × (\"n\" – 2) and so the measure of each angle of any regular \"n\"-gon is 180° × (\"n\" – 2) ÷ \"n\". Thus for example the entry \"45°: square\" means that, with \"n\" = 4, 180° ÷ \"n\" = 45°, and the number of degrees in each angle of a square is (\"n\" – 2) × 45° = 90°.\n\nFor cube roots of non-real numbers that appear in this table, one has to take the principal value, that is the cube root with the largest real part; this largest real part is always positive. Therefore, the sums of cube roots that appear in the table are all positive real numbers.\n\nformula_116\n\nAs an example of the use of these constants, consider the volume of a regular dodecahedron, where \"a\" is the length of an edge:\n\nUsing\nthis can be simplified to:\n\nThe derivation of sine, cosine, and tangent constants into radial forms is based upon the constructibility of right triangles.\n\nHere right triangles made from symmetry sections of regular polygons are used to calculate fundamental trigonometric ratios. Each right triangle represents three points in a regular polygon: a vertex, an edge center containing that vertex, and the polygon center. An \"n\"-gon can be divided into 2\"n\" right triangles with angles of , 90 − , 90} degrees, for \"n\" in 3, 4, 5, …\n\nConstructibility of 3, 4, 5, and 15-sided polygons are the basis, and angle bisectors allow multiples of two to also be derived.\n\nIn degree format, sin and cos of 0, 30, 45, 60, and 90 can be calculated from their right angled triangles, using the Pythagorean theorem.\n\nIn radian format, sin and cos of / 2 can be expressed in radical format by recursively applying the following:\n\nFor example:\nand so on.\n\nand so on.\n\nand so on.\n\nand so on.\n\nIf formula_166 and formula_167 then\nTherefore, applying induction:\n\nThe induction above can be applied in the same way to all the remaining Fermat primes (F=2+1=2+1=257 and F=2+1=2+1=65537), the factors of whose cos and sin radical expressions are known to exist but are very long to express here.\n\nD = 2 - 1 = 4,294,967,295 is the largest odd integer denominator for which radical forms for sin(/D) and cos (/D) are known to exist.\n\nUsing the radical form values from the sections above, and applying cos(A-B) = cosA cosB + sinA sinB, followed by induction, we get -\n\nTherefore, using the radical form values from the sections above, and applying cos(A-B) = cosA cosB + sinA sinB, followed by induction, we get -\n\nFinally, using the radical form values from the sections above, and applying cos(A-B) = cosA cosB + sinA sinB, followed by induction, we get -\n\nThe radical form expansion of the above is very large, hence expressed in the simpler form above.\n\nApplying Ptolemy's theorem to the cyclic quadrilateral ABCD defined by four successive vertices of the pentagon, we can find that:\n\nwhich is the reciprocal of the golden ratio. crd is the chord function,\n\nThus\n\n(Alternatively, without using Ptolemy's theorem, label as X the intersection of AC and BD, and note by considering angles that triangle AXB is isosceles, so AX = AB = \"a\". Triangles AXD and CXB are similar, because AD is parallel to BC. So XC = \"a\"·(). But AX + XC = AC, so \"a\" +  = \"b\". Solving this gives  = , as above).\n\nSimilarly\n\nso\n\nIf θ is 18° or -54°, then 2θ and 3θ add up to 5θ = 90° or -270°, therefore sin 2θ is equal to cos 3θ.\nTherefore,\n\nAlternately, the multiple-angle formulas for functions of 5\"x\", where \"x\" ∈ {18, 36, 54, 72, 90} and 5\"x\" ∈ {90, 180, 270, 360, 450}, can be solved for the functions of \"x\", since we know the function values of 5\"x\". The multiple-angle formulas are:\n\nIn general nested radicals cannot be reduced. But if\nwith \"a\", \"b\", and \"c\" rational, we have\nis rational, then both\nare rational; then we have\nFor example,\n\n\n\n"}
{"id": "16296098", "url": "https://en.wikipedia.org/wiki?curid=16296098", "title": "X mark", "text": "X mark\n\nAn X mark (also known as a cross, x, ex , X, ✕, ☓, ✖, ✗, ✘, etc.) used to indicate the concept of negation (for example \"no, this has not been verified\" or \"no, I don't agree\") as well as an indicator (for example in election ballot papers or in x marks the spot). Its opposite is often considered to be the check mark or tick (or the O mark used in Japan, Korea and Taiwan). In Japanese, the X mark (❌) is called \"batsu\" (ばつ) and can be expressed by someone by crossing their arms.\n\nIn some countries such as France it is common for people to check a square box with a cross rather than a check mark, while in others the check mark (✓) or even a v mark is used.\n\nIt is also used as a replacement for a signature for a person who is blind or illiterate and thus cannot write his or her name. Typically, the writing of an X used for this purpose must be witnessed to be valid.\n\nAs a verb, to ex (or x, notably one of the shortest English words) off/out or to cross off/out means to add such a mark. It is quite common, especially on printed forms and document, for there to be squares in which to place x marks, or interchangeably checks.\n\nIt is also traditionally used on maps to indicate locations, most famously on treasure maps.\n\nUnicode provides various related symbols, including:\nThe mark is generally rendered with a less symmetrical form than the following cross-shaped mathematical symbols:\n"}
