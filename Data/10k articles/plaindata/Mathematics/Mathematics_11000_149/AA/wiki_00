{"id": "17311242", "url": "https://en.wikipedia.org/wiki?curid=17311242", "title": "Alexandra Bellow", "text": "Alexandra Bellow\n\nAlexandra Bellow (formerly Alexandra Ionescu Tulcea; born 30 August 1935) is a mathematician from Bucharest, Romania, who has made contributions to the fields of ergodic theory, probability and analysis.\n\nBellow was born in Bucharest, Romania, on August 30, 1935, as Alexandra Bagdasar. Her parents were both physicians. Her mother, , was a child psychiatrist. Her father, , was a neurosurgeon (in fact, he founded the Romanian school of neurosurgery, after having obtained his training in Boston, at the clinic of the world pioneer of neurosurgery, Dr. Harvey Cushing). She received her M.S. in mathematics from the University of Bucharest in 1957, where she met and married her first husband, Cassius Ionescu-Tulcea. She accompanied her husband to the United States in 1957 and received her Ph.D from Yale University in 1959 under the direction of Shizuo Kakutani. After receiving her degree, she worked as a research associate at Yale from 1959 until 1961, and as an Assistant professor at the University of Pennsylvania from 1962 to 1964. From 1964 until 1967 she was an Associate professor at the University of Illinois at Urbana–Champaign. In 1967 she moved to Northwestern University as a professor of mathematics. She was at Northwestern until her retirement in 1996, when she became Professor Emeritus.\n\nDuring her marriage to Cassius Ionescu-Tulcea (1956–1969) she and her husband wrote a number of papers together, as well as the research monograph [25] on lifting theory.\n\nAlexandra's second husband was the writer Saul Bellow who was awarded the Nobel Prize (1976), during this marriage (1975–1985). Alexandra features in Bellow's writings; she is portrayed lovingly in his memoir \"To Jerusalem and Back\" (1976), and, his novel \"The Dean's December\" (1982), more critically, satirically in his last novel \"Ravelstein\" (2000) - which was written many years after their divorce. The decade of the nineties was for Alexandra a period of personal and professional fulfillment, brought about by her marriage in 1989 to the mathematician, Alberto P. Calderón. For more details about her personal and professional life see her autobiographical article. See also her recent interview.\n\nSome of her early work involved properties and consequences of lifting. Lifting theory, which had started with the pioneering papers of John von Neumann and later Dorothy Maharam, came into its own in the 1960s and 70's with the work of the Ionescu Tulceas and provided the definitive treatment for the representation theory of linear operators arising in probability, the process of disintegration of measures. The Ergebnisse monograph became a standard reference in this area.\n\nBy applying a lifting to a stochastic process, A. Ionescu Tulcea and C. Ionescu Tulcea obtained a ‘separable’ process; this gives a rapid proof of Doob's theorem concerning the existence of a separable modification of a stochastic process (also a ‘canonical’ way of obtaining the separable modification).\n\nBy applying a lifting to a ‘weakly’ measurable function with values in a weakly compact set of a Banach space, one obtains a strongly measurable function; this gives a one line proof of Phillips's classical theorem (also a ‘canonical’ way of obtaining the strongly measurable version).\n\nWe say that a set H of measurable functions satisfies the \"separation property\" if any two distinct functions in H belong to distinct equivalence classes. The range of a lifting is always a set of measurable functions with the \"separation property\". The following ‘metrization criterion’ gives some idea why the functions in the range of a lifting are so much better behaved:\n\nLet H be a set of measurable functions with the following properties : (I) H is compact (for the topology of pointwise convergence); (II) H is convex; (III) H satisfies the \"separation property\". Then H is metrizable.\n\nThe proof of the existence of a lifting commuting with the left translations of an arbitrary locally compact group, by A. Ionescu Tulcea and C. Ionescu Tulcea, is highly non-trivial. It makes use of approximation by Lie groups, and martingale-type arguments tailored to the group structure.\n\nIn the early 1960s she worked with C Ionescu Tulcea on martingales taking values in a Banach space. In a certain sense paper this work launched the study of vector-valued martingales, with the first proof of the ‘strong’ almost everywhere convergence for martingales taking values in a Banach space with (what later became known as) the Radon–Nikodym property; this, by the way, opened the doors to a new area of analysis, the \"geometry of Banach spaces\". These ideas were later extended by Bellow to the theory of ‘uniform amarts’,(in the context of Banach spaces, uniform amarts are the natural generalization of martingales, quasi-martingales and possess remarkable stability properties, such as optional sampling), now an important chapter in probability theory.\n\nIn 1960 D. S. Ornstein constructed an example of a non-singular transformation on the Lebesgue space of the unit interval, which does not admit a σ – finite invariant measure equivalent to Lebesgue measure, thus solving a long-standing problem in ergodic theory. A few years later, R. V. Chacón gave an example of a positive (linear) isometry of \"L\" for which the individual ergodic theorem fails in \"L\". Her work unifies and extends these two remarkable results. It shows, by methods of Baire Category, that the seemingly isolated examples of non-singular transformations first discovered by Ornstein and later by Chacón, were in fact the typical case.\n\nBeginning in the early 1980s Bellow began a series of papers that has brought about a revival of that important area of ergodic theory dealing with limit theorems and the delicate question of pointwise a.e. convergence. This was accomplished by exploiting the interplay with probability and harmonic analysis, in the modern context (the Central limit theorem, transference principles, square functions and other singular integral techniques are now part of the daily arsenal of people working in this area of ergodic theory) and by attracting a number of talented mathematicians who have been very active in this area.\n\nOne of the two problems that she raised at the Oberwolfach meeting on \"Measure Theory\" in 1981, was the question of the validity, for \"ƒ\" in \"L\", of the pointwise ergodic theorem along the ‘sequence of squares’, and along the ‘sequence of primes’ (A similar question was raised independently, a year later, by H. Furstenberg). This problem was solved several years later by J. Bourgain, for \"f\" in \"L\", \"p\" > 1 in the case of the ‘squares’ and for \"p\" > (1 + )/2 in the case of the ‘primes’ (the argument was pushed through to \"p\" > 1 by M. Wierdl; the case of \"L\" however had remained open). Bourgain was awarded the Fields Medal in 1994, in part for this work in ergodic theory.\n\nIt was U. Krengel who first gave, in 1971, an ingenious construction of an increasing sequence of positive integers along which the pointwise ergodic theorem fails in \"L\" for every ergodic transformation. The existence of such a \"bad universal sequence\" came as a surprise. Bellow showed that every lacunary sequence of integers is in fact a \"bad universal sequence\" in \"L\". Thus lacunary sequences are ‘canonical’ examples of \"bad universal sequences\".\n\nLater she was able to show that from the point of view of the pointwise ergodic theorem, a sequence of positive integers may be \"good universal\" in \"L\", but \"bad universal\" in \"L\", for all 1 ≤ \"q\" < \"p\". This was rather startling and answered a question raised by R. Jones.\n\nA place in this area of research is occupied by the \"strong sweeping out property\" (that a sequence of linear operators may exhibit). This describes the situation when almost everywhere convergence breaks down even in \"L\" and in the worst possible way. Instances of this appear in several of her papers, see for example (59, 61, 63, 65, 66) in her vita. Paper 65 was an extensive and systematic study of the \"strong sweeping out\" property (s.s.o.), giving various criteria and numerous examples of (s.s.o.). This project involved many authors and a long period of time to complete.\n\nWorking with U. Krengel, she was able to give a negative answer to a long-standing conjecture of E. Hopf. Later, Bellow and Krengel working with A. P. Calderón were able to show that in fact the Hopf operators have the \"strong sweeping out\" property.\n\nIn the study of aperiodic flows, sampling at nearly periodic times, as for example, \"t\" = \"n\" + \"ε\"(\"n\"), where \"ε\" is positive and tends to zero, does not lead to a.e. convergence; in fact strong sweeping out occurs. This shows the possibility of serious errors when using the ergodic theorem for the study of physical systems. Such results can be of practical value for statisticians and other scientists.\n\nIn the study of discrete ergodic systems, which can be observed only over certain blocks of time [a,b], one has the following dichotomy of behavior of the corresponding averages: either the averages converge a.e. for all functions in \"L\", or the strong sweeping out property holds. This depends on the geometric properties of the blocks, see.\n\nThe following are some examples of the work of A. Bellow with other mathematicians.\n\nMathematicians, who in their papers, answered questions raised by A. Bellow:\n\nThe \"strong sweeping out property\", a notion formalized by A. Bellow, plays a role in this area of research.\n\n\n\n"}
{"id": "25371502", "url": "https://en.wikipedia.org/wiki?curid=25371502", "title": "Andreas Speiser", "text": "Andreas Speiser\n\nAndreas Speiser (June 10, 1885 – October 12, 1970) was a Swiss Mathematician and Philosopher of science.\n\nSpeiser studied since 1904 in Göttingen notably with David Hilbert, Felix Klein, Hermann Minkowski. In 1917 he became full-time professor at the University of Zurich but later relocated in Basel. During 1924/25 he was president of the Swiss Mathematical Association.\n\nSpeiser worked on number theory, group theory, and the theory of Riemann surfaces. He organized the translation of Leonard Dickson's seminal 1923 book \"Algebras and Their Arithmetics\" (\"Algebren und ihre Zahlentheorie\", 1927), which was heavily influenced by the work on the theory of algebras done by the schools of Emmy Noether and Helmut Hasse. Speiser also added an appendix on ideal theory to Dickson's book. Speiser's book \"Theorie der Gruppen endlicher Ordnung\" is a classic, richly illustrated work on group theory. In this book, there are group theoretical applications in Galois theory, elementary number theory, and Platonic solids, as well as extensive studies of ornaments, such as those that Speiser studied on a 1928 trip to Egypt.\n\nSpeiser also worked on the history of mathematics and was the chief editor for the Euler Commission's edition of Leonhard Euler's \"Opera Omnia\" and the editor of the works of Johann Heinrich Lambert. As a philosopher Speiser was chiefly concerned with Plato and wrote a commentary on the Parmenides Dialogue, but he was also an expert of the philosophies of Plotinus and Hegel.\n\n\n\n"}
{"id": "34170016", "url": "https://en.wikipedia.org/wiki?curid=34170016", "title": "Artin conductor", "text": "Artin conductor\n\nIn mathematics, the Artin conductor is a number or ideal associated to a character of a Galois group of a local or global field, introduced by as an expression appearing in the functional equation of an Artin L-function.\n\nSuppose that \"L\" is a finite Galois extension of the local field \"K\", with Galois group \"G\". If χ is a character of \"G\", then the Artin conductor of χ is the number\nwhere \"G\" is the \"i\"-th ramification group (in lower numbering), of order \"g\", and χ(\"G\") is the average value of χ on \"G\". By a result of Artin, the local conductor is an integer. If χ is unramified, then its Artin conductor is zero. If \"L\" is unramified over \"K\", then the Artin conductors of all χ are zero.\n\nThe \"wild invariant\" or \"Swan conductor\" of the character is\n\nin other words, the sum of the higher order terms with \"i\" > 0.\n\nThe global Artin conductor of a representation χ of the Galois group \"G\" of a finite extension \"L\"/\"K\" of global fields is an ideal of \"K\", defined to be\n\nwhere the product is over the primes \"p\" of \"K\", and \"f\"(χ,\"p\") is the local Artin conductor of the restriction of χ to the decomposition group of some prime of \"L\" lying over \"p\". Since the local Artin conductor is zero at unramified primes, the above product only need be taken over primes that ramify in \"L\"/\"K\".\n\nSuppose that \"L\" is a finite Galois extension of the local field \"K\", with Galois group \"G\". The Artin character \"a\" of \"G\" is the character\n\nand the Artin representation \"A\" is the complex linear representation of \"G\" with this character. asked for a direct construction of the Artin representation. showed that the Artin representation can be realized over the local field Q, for any prime \"l\" not equal to the residue characteristic \"p\". showed that it can be realized over the corresponding ring of Witt vectors. It cannot in general be realized over the rationals or over the local field Q, suggesting that there is no easy way to construct the Artin representation explicitly.\n\nThe Swan character \"sw\" is given by\n\nwhere \"r\" is the character of the regular representation and 1 is the character of the trivial representation. The Swan character is the character of a representation of \"G\". showed that there is a unique projective representation of \"G\" over the \"l\"-adic integers with character the Swan character.\n\nThe Artin conductor appears in the conductor-discriminant formula for the discriminant of a global field.\n\nThe optimal level in the Serre modularity conjecture is expressed in terms of the Artin conductor.\n\nThe Artin conductor appears in the functional equation of the Artin L-function.\n\nThe Artin and Swan representations are used to defined the conductor of an elliptic curve or abelian variety.\n\n"}
{"id": "1829970", "url": "https://en.wikipedia.org/wiki?curid=1829970", "title": "Beta-dual space", "text": "Beta-dual space\n\nIn functional analysis and related areas of mathematics, the beta-dual or -dual is a certain linear subspace of the algebraic dual of a sequence space.\n\nGiven a sequence space the -dual of is defined as\n\nIf is an FK-space then each in defines a continuous linear form on \n\n\nThe beta-dual of an FK-space is a linear subspace of the continuous dual of . If is an FK-AK space then the beta dual is linear isomorphic to the continuous dual.\n"}
{"id": "4692157", "url": "https://en.wikipedia.org/wiki?curid=4692157", "title": "Bird–Meertens formalism", "text": "Bird–Meertens formalism\n\nThe Bird–Meertens formalism (BMF) is a calculus for deriving programs from specifications (in a functional-programming setting) by a process of equational reasoning. It was devised by Richard Bird and Lambert Meertens as part of their work within IFIP Working Group 2.1.\n\nIt is sometimes referred to in publications as BMF, as a nod to Backus–Naur form. Facetiously it is also referred to as \"Squiggol\", as a nod to ALGOL, which was also in the remit of WG 2.1, and because of the \"squiggly\" symbols it uses. A less-used variant name, but actually the first one suggested, is \"SQUIGOL\".\n\nMap is a well-known second-order function that applies a given function to every element of a list; in BMF, it is written formula_1:\n\nLikewise, reduce is a function that collapses a list into a single value by repeated application of a binary operator. It is written / in BMF.\nTaking formula_3 as a suitable binary operator with neutral element \"e\", we have\n\nUsing those two operators and the primitives formula_5 (as the usual addition), and formula_6 (for list concatenation), we can easily express the sum of all elements of a list, and the flatten function, as formula_7 and formula_8, in\ncurrified notation. We have:\n\nSimilarly, writing formula_11 for functional composition and formula_12 for conjunction, it is easy to write a function testing that all elements of a list satisfy a predicate \"p\", simply as formula_13:\n\nA function \"h\" on lists is a list homomorphism if there exists an associative binary operator formula_3 and neutral element formula_16 such that the following holds:\n\nThe \"homomorphism lemma\" states that \"h\" is a homomorphism if and only if there exists an operator formula_3 and a function \"f\" such that formula_19.\n\nA point of great interest for this lemma is its application to the derivation of highly parallel implementations of computations. Indeed, it is trivial to see that formula_20 has a highly parallel implementation, and so does formula_21 — most obviously as a binary tree. Thus for any list homomorphism \"h\", there exists a parallel implementation. That implementation cuts the list into chunks, which are assigned to different computers; each computes the result on its own chunk. It is those results that transit on the network and are finally combined into one. In any application where the list is enormous and the result is a very simple type – say an integer – the benefits of parallelisation are considerable. This is the basis of the map-reduce approach.\n\n\n"}
{"id": "5824808", "url": "https://en.wikipedia.org/wiki?curid=5824808", "title": "Bounded quantifier", "text": "Bounded quantifier\n\nIn the study of formal theories in mathematical logic, bounded quantifiers are often included in a formal language in addition to the standard quantifiers \"∀\" and \"∃\". Bounded quantifiers differ from \"∀\" and \"∃\" in that bounded quantifiers restrict the range of the quantified variable. The study of bounded quantifiers is motivated by the fact that determining whether a sentence with only bounded quantifiers is true is often not as difficult as determining whether an arbitrary sentence is true.\n\nExamples of bounded quantifiers in the context of real analysis include \"∀\"x\">0\", \"∃\"y\"<0\", and \"∀\"x\" ∊ ℝ\". Informally \"∀\"x\">0\" says \"for all \"x\" where \"x\" is larger than 0\", \"∃\"y\"<0\" says \"there exists a \"y\" where \"y\" is less than 0\" and \"∀\"x\" ∊ ℝ\" says \"for all \"x\" where \"x\" is a real number\". For example, says \"every positive number is the square of a negative number\".\n\nSuppose that \"L\" is the language of Peano arithmetic (the language of second-order arithmetic or arithmetic in all finite types would work as well). There are two types of bounded quantifiers: formula_1 and formula_2.\nThese quantifiers bind the number variable \"n\" and contain a numeric term \"t\" which may not mention \"n\" but which may have other free variables. (\"Numeric terms\" here means terms such as \"1 + 1\", \"2\", \"2 × 3\", \"\"m\" + 3\", etc.)\n\nThese quantifiers are defined by the following rules (formula_3 denotes formulas):\n\nThere are several motivations for these quantifiers.\n\nIn general, a relation on natural numbers is definable by a bounded formula if and only if it is computable in the linear-time hierarchy, which is defined similarly to the polynomial hierarchy, but with linear time bounds instead of polynomial. Consequently, all predicates definable by a bounded formula are Kalmár elementary, context-sensitive, and primitive recursive.\n\nIn the arithmetical hierarchy, an arithmetical formula which contains only bounded quantifiers is called formula_10, formula_11, and formula_12. The superscript 0 is sometimes omitted.\n\nSuppose that \"L\" is the language formula_13 of the Zermelo–Fraenkel set theory, where the ellipsis may be replaced by term-forming operations such as a symbol for the powerset operation. There are two bounded quantifiers: formula_14 and formula_15. These quantifiers bind the set variable \"x\" and contain a term \"t\" which may not mention \"x\" but which may have other free variables.\n\nThe semantics of these quantifiers is determined by the following rules:\n\nA ZF formula which contains only bounded quantifiers is called formula_18, formula_19, and formula_20. This forms the basis of the Lévy hierarchy, which is defined analogously with the arithmetical hierarchy.\n\nBounded quantifiers are important in Kripke–Platek set theory and constructive set theory, where only Δ separation is included. That is, it includes separation for formulas with only bounded quantifiers, but not separation for other formulas. In KP the motivation is the fact that whether a set \"x\" satisfies a bounded quantifier formula only depends on the collection of sets that are close in rank to \"x\" (as the powerset operation can only be applied finitely many times to form a term). In constructive set theory, it is motivated on predicative grounds.\n\n\n"}
{"id": "56508282", "url": "https://en.wikipedia.org/wiki?curid=56508282", "title": "Brigitte Servatius", "text": "Brigitte Servatius\n\nBrigitte Irma Servatius (b. 1954) is a mathematician specializing in matroids and structural rigidity. She is a professor of mathematics at Worcester Polytechnic Institute, and has been the editor-in-chief of the \"Pi Mu Epsilon Journal\" since 1999.\n\nServatius is originally from Graz in Austria.\nAs a student at an all-girl gymnasium in Graz that specialized in language studies rather than mathematics, her interest in mathematics was sparked by her participation in a national mathematical olympiad,\nand she went on to earn master's degrees in mathematics and physics at the University of Graz.\n\nShe became a high school mathematics and science teacher in Leibnitz. She moved to the US in 1981, to begin doctoral studies at Syracuse University. She completed her Ph.D. in 1987, and joined the Worcester Polytechnic Institute faculty in the same year. Her dissertation, \"Planar Rigidity\", was supervised by Jack Graver.\n\nWhile still in Austria, Servatius began working on combinatorial group theory, and her first publication (appearing while she was a graduate student) is in that subject.\nShe switched to the theory of structural rigidity for her doctoral research,\nand later became the author (with Jack Graver and Herman Servatius) of the book \"Combinatorial Rigidity\" (1993).\nAnother well-cited paper of hers in this area characterizes the planar Laman graphs, the minimally rigid graphs that can be embedded without crossings in the plane, as the graphs of pseudotriangulations, partitions of a plane region into subregions with three convex corners studied in computational geometry.\n\nServatius is also the co-editor of a book on matroid theory.\nWith Tomaž Pisanski she wrote the book \"Configurations from a Graphical Viewpoint\" (2013), on configurations of points and lines in the plane with the same number of points touching each two lines and the same number of lines touching each two points. Other topics in her research include graph duality and the triconnected components of infinite graphs.\n\n"}
{"id": "49155297", "url": "https://en.wikipedia.org/wiki?curid=49155297", "title": "Calderón projector", "text": "Calderón projector\n\nIn applied mathematics, the Calderón projector is a pseudo-differential operator used widely in boundary element methods. It is named after Alberto Calderón.\n\nThe interior Calderón projector is defined to be:\n\nformula_1\n\nwhere formula_2 is formula_3 almost everywhere, formula_4 is the identity boundary operator, formula_5 is the double layer boundary operator, formula_6 is the single layer boundary operator, formula_7 is the adjoint double layer boundary operator and formula_8 is the hypersingular boundary operator.\n\nThe exterior Calderón projector is defined to be:\n"}
{"id": "1462712", "url": "https://en.wikipedia.org/wiki?curid=1462712", "title": "Centrality", "text": "Centrality\n\nIn graph theory and network analysis, indicators of centrality identify the most important vertices within a graph. Applications include identifying the most influential person(s) in a social network, key infrastructure nodes in the Internet or urban networks, and super-spreaders of disease. Centrality concepts were first developed in social network analysis, and many of the terms used to measure centrality reflect their sociological origin.\nThey should not be confused with node influence metrics, which seek to quantify the influence of every node in the network.\n\nCentrality indices are answers to the question \"What characterizes an important vertex?\" The answer is given in terms of a real-valued function on the vertices of a graph, where the values produced are expected to provide a ranking which identifies the most important nodes.\n\nThe word \"importance\" has a wide number of meanings, leading to many different definitions of centrality. Two categorization schemes have been proposed.\n\"Importance\" can be conceived in relation to a type of flow or transfer across the network. This allows centralities to be classified by the type of flow they consider important. \"Importance\" can alternatively be conceived as involvement in the cohesiveness of the network. This allows centralities to be classified based on how they measure cohesiveness. Both of these approaches divide centralities in distinct categories. A further conclusion is that a centrality which is appropriate for one category will often \"get it wrong\" when applied to a different category.\n\nWhen centralities are categorized by their approach to cohesiveness, it becomes apparent that the majority of centralities inhabit one category. The count of the number of walks starting from a given vertex differs only in how walks are defined and counted. Restricting consideration to this group allows for a soft characterization which places centralities on a spectrum from walks of length one (degree centrality) to infinite walks (eigenvalue centrality). The observation that many centralities share this familial relationships perhaps explains the high rank correlations between these indices.\n\nA network can be considered a description of the paths along which something flows. This allows a characterization based on the type of flow and the type of path encoded by the centrality. A flow can be based on transfers, where each undivisible item goes from one node to another, like a package delivery which goes from the delivery site to the client's house. A second case is the serial duplication, where this is a replication of the item which goes to the next node, so both the source and the target have it. An example is the propagation of information through gossip, with the information being propagated in a private way and with both the source and the target nodes being informed at the end of the process. The last case is the parallel duplication, with the item being duplicated to several links at the same time, like a radio broadcast which provides the same information to many listeners at once.\n\nLikewise, the type of path can be constrained to: Geodesics (shortest paths), paths (no vertex is visited more than once), trails (vertices can be visited multiple times, no edge is traversed more than once), or walks (vertices and edges can be visited/traversed multiple times).\n\nAn alternative classification can be derived from how the centrality is constructed. This again splits into two classes. Centralities are either \"Radial\" or \"Medial.\"\nRadial centralities count walks which start/end from the given vertex. The degree and eigenvalue centralities are examples of radial centralities, counting the number of walks of length one or length infinity. Medial centralities count walks which pass through the given vertex. The canonical example is Freeman's betweenness centrality, the number of shortest paths which pass through the given vertex.\n\nLikewise, the counting can capture either the \"volume\" or the \"length\" of walks. Volume is the total number of walks of the given type. The three examples from the previous paragraph fall into this category. Length captures the distance from the given vertex to the remaining vertices in the graph. Freeman's closeness centrality, the total geodesic distance from a given vertex to all other vertices, is the best known example. Note that this classification is independent of the type of walk counted (i.e. walk, trail, path, geodesic).\n\nBorgatti and Everett propose that this typology provides insight into how best to compare centrality measures. Centralities placed in the same box in this 2×2 classification are similar enough to make plausible alternatives; one can reasonably compare which is better for a given application. Measures from different boxes, however, are categorically distinct. Any evaluation of relative fitness can only occur within the context of predetermining which category is more applicable, rendering the comparison moot.\n\nThe characterization by walk structure shows that almost all centralities in wide use are radial-volume measures. These encode the belief that a vertex's centrality is a function of the centrality of the vertices it is associated with. Centralities distinguish themselves on how association is defined.\n\nBonacich showed that if association is defined in terms of walks, then a family of centralities can be defined based on the length of walk considered. The degree counts walks of length one, the eigenvalue centrality counts walks of length infinity. Alternative definitions of association are also reasonable. The alpha centrality allows vertices to have an external source of influence. Estrada's subgraph centrality proposes only counting closed paths (triangles, squares, ...).\n\nThe heart of such measures is the observation that powers of the graph's adjacency matrix gives the number of walks of length given by that power. Similarly, the matrix exponential is also closely related to the number of walks of a given length. An initial transformation of the adjacency matrix allows differing definition of the type of walk counted. Under either approach, the centrality of a vertex can be expressed as an infinite sum, either\n\nfor matrix powers or\n\nfor matrix exponentials, where\n\n\nBonacich's family of measures does not transform the adjacency matrix. The alpha centrality replaces the adjacency matrix with its resolvent. The subgraph centrality replaces the adjacency matrix with its trace. A startling conclusion is that regardless of the initial transformation of the adjacency matrix, all such approaches have common limiting behavior. As formula_5 approaches zero, the indices converge to the degree centrality. As formula_5 approaches its maximal value, the indices converge to the eigenvalue centrality.\n\nThe common feature of most of the aforementioned standard measures is that they assess the\nimportance of a node by focusing only on the role that a node plays by itself. However,\nin many applications such an approach is inadequate because of synergies that may occur\nif the functioning of nodes is considered in groups.\nFor example, let's consider problem of stopping epidemic. Looking at above image of network, which nodes should we vaccinate? Based on previously described measures, we want to recognize nodes that are the most important in disease spreading. Approaches based only on centralities, that focuse on indiviudal features of nodes, may not be good idea. Nodes in the red square, individually cannot stop disease spreading, but considering them as a group, we clearly see that they can stop disease if it has started in nodes formula_8,formula_9,formula_10. Game-Theoretic Centralities try to consult described problems and opportunities, using tools from game-theory. Approach proposed in uses Shapley Value. Because of time-complexity hardness of Shapley value calculation, most efforts in this domain are driven into implementing new algorithms and methods which rely on peculiar topology of network and special character of problem. Such a approach may lead to reducing time-complexity from exponential to polynomial.\n\nCentrality indices have two important limitations, one obvious and the other subtle. The obvious limitation is that a centrality which is optimal for one application is often sub-optimal for a different application. Indeed, if this were not so, we would not need so many different centralities. An illustration of this phenomenon is provided by the Krackhardt kite graph, for which three different notions of centrality give three different choices of the most central vertex.\n\nThe more subtle limitation is the commonly held fallacy that vertex centrality indicates the relative importance of vertices. Centrality indices are explicitly designed to produce a ranking which allows indication of the most important vertices. This they do well, under the limitation just noted. They are not designed to measure the influence of nodes in general. Recently, network physicists have begun developing node influence metrics to address this problem.\n\nThe error is two-fold. Firstly, a ranking only orders vertices by importance, it does not quantify the difference in importance between different levels of the ranking. This may be mitigated by applying Freeman centralization to the centrality measure in question, which provide some insight to the importance of nodes depending on the differences of their centralization scores. Furthermore, Freeman centralization enables one to compare several networks by comparing their highest centralization scores. This approach, however, is seldom seen in practice.\n\nSecondly, the features which (correctly) identify the most important vertices in a given network/application do not necessarily generalize to the remaining vertices. \nFor the majority of other network nodes the rankings may be meaningless. This explains why, for example, only the first few results of a Google image search appear in a reasonable order. The pagerank is a highly unstable measure, showing frequent rank reversals after small adjustments of the jump parameter\nWhile the failure of centrality indices to generalize to the rest of the network may at first seem counter-intuitive, it follows directly from the above definitions.\nComplex networks have heterogeneous topology. To the extent that the optimal measure depends on the network structure of the most important vertices, a measure which is optimal for such vertices is sub-optimal for the remainder of the network.\n\nHistorically first and conceptually simplest is degree centrality, which is defined as the number of links incident upon a node (i.e., the number of ties that a node has). The degree can be interpreted in terms of the immediate risk of a node for catching whatever is flowing through the network (such as a virus, or some information). In the case of a directed network (where ties have direction), we usually define two separate measures of degree centrality, namely indegree and outdegree. Accordingly, indegree is a count of the number of ties directed to the node and outdegree is the number of ties that the node directs to others. When ties are associated to some positive aspects such as friendship or collaboration, indegree is often interpreted as a form of popularity, and outdegree as gregariousness.\n\nThe degree centrality of a vertex formula_11, for a given graph formula_12 with formula_13 vertices and formula_14 edges, is defined as\n\nCalculating degree centrality for all the nodes in a graph takes formula_16 in a dense adjacency matrix representation of the graph, and for edges takes formula_17 in a sparse matrix representation.\n\nThe definition of centrality on the node level can be extended to the whole graph, in which case we are speaking of \"graph centralization\". Let formula_18 be the node with highest degree centrality in formula_19. Let formula_20 be the formula_21 node connected graph that maximizes the following quantity (with formula_22 being the node with highest degree centrality in formula_23):\n\nCorrespondingly, the degree centralization of the graph formula_19 is as follows:\n\nThe value of formula_27 is maximized when the graph formula_23 contains one central node to which all other nodes are connected (a star graph), and in this case \n\nSo, for any graph formula_30 \n\nIn a connected graph, the normalized closeness centrality (or closeness) of a node is the average length of the shortest path between the node and all other nodes in the graph. Thus the more central a node is, the closer it is to all other nodes.\n\nCloseness was defined by Bavelas (1950) as the reciprocal of the farness, that is:\n\nwhere formula_33 is the distance between vertices formula_34 and formula_35. However, when speaking of closeness centrality, people usually refer to its normalized form, generally given by the previous formula multiplied by formula_36, where formula_37 is the number of nodes in the graph. This adjustment allows comparisons between nodes of graphs of different sizes.\n\nTaking distances \"from\" or \"to\" all other nodes is irrelevant in undirected graphs, whereas it can produce totally different results in directed graphs (e.g. a website can have a high closeness centrality from outgoing link, but low closeness centrality from incoming links).\n\nIn a (not necessarily connected) graph, the harmonic centrality reverses the sum and reciprocal operations in the definition of closeness centrality:\n\nwhere formula_39 if there is no path from formula_35 to formula_34. Harmonic centrality can be normalized by dividing by formula_36, where formula_37 is the number of nodes in the graph.\n\nHarmonic centrality was proposed by Marchiori and Latora (2000) and then independently by Dekker (2005), using the name \"valued centrality,\" and by Rochat (2009).\n\nBetweenness is a centrality measure of a vertex within a graph (there is also edge betweenness, which is not discussed here). Betweenness centrality quantifies the number of times a node acts as a bridge along the shortest path between two other nodes. It was introduced as a measure for quantifying the control of a human on the communication between other humans in a social network by Linton Freeman In his conception, vertices that have a high probability to occur on a randomly chosen shortest path between two randomly chosen vertices have a high betweenness.\n\nThe betweenness of a vertex formula_11 in a graph formula_12 with formula_46 vertices is computed as follows:\n\n\nMore compactly the betweenness can be represented as:\n\nwhere formula_48 is total number of shortest paths from node formula_49 to node formula_50 and formula_51 is the number of those paths that pass through formula_11. The betweenness may be normalised by dividing through the number of pairs of vertices not including \"v\", which for directed graphs is formula_53 and for undirected graphs is formula_54. For example, in an undirected star graph, the center vertex (which is contained in every possible shortest path) would have a betweenness of formula_54 (1, if normalised) while the leaves (which are contained in no shortest paths) would have a betweenness of 0.\n\nFrom a calculation aspect, both betweenness and closeness centralities of all vertices in a graph involve calculating the shortest paths between all pairs of vertices on a graph, which requires formula_56 time with the Floyd–Warshall algorithm. However, on sparse graphs, Johnson's algorithm may be more efficient, taking formula_57 time. In the case of unweighted graphs the calculations can be done with Brandes' algorithm which takes formula_58 time. Normally, these algorithms assume that graphs are undirected and connected with the allowance of loops and multiple edges. When specifically dealing with network graphs, often graphs are without loops or multiple edges to maintain simple relationships (where edges represent connections between two people or vertices). In this case, using Brandes' algorithm will divide final centrality scores by 2 to account for each shortest path being counted twice.\n\nEigenvector centrality (also called eigencentrality) is a measure of the influence of a node in a network. It assigns relative scores to all nodes in the network based on the concept that connections to high-scoring nodes contribute more to the score of the node in question than equal connections to low-scoring nodes. Google's PageRank and the Katz centrality are variants of the eigenvector centrality.\n\nFor a given graph formula_12 with formula_13 number of vertices let formula_61 be the adjacency matrix, i.e. formula_62 if vertex formula_11 is linked to vertex formula_50, and formula_65 otherwise. The relative centrality score of vertex formula_11 can be defined as:\n\nwhere formula_68 is a set of the neighbors of formula_11 and formula_70 is a constant. With a small rearrangement this can be rewritten in vector notation as the eigenvector equation\n\nIn general, there will be many different eigenvalues formula_70 for which a non-zero eigenvector solution exists. Since the entries in the adjacency matrix are non-negative, there is a unique largest eigenvalue, which is real and positive, by the Perron–Frobenius theorem. This greatest eigenvalue results in the desired centrality measure. The formula_73 component of the related eigenvector then gives the relative centrality score of the vertex formula_11 in the network. The eigenvector is only defined up to a common factor, so only the ratios of the centralities of the vertices are well defined. To define an absolute score one must normalise the eigen vector e.g. such that the sum over all vertices is 1 or the total number of vertices \"n\". Power iteration is one of many eigenvalue algorithms that may be used to find this dominant eigenvector. Furthermore, this can be generalized so that the entries in \"A\" can be real numbers representing connection strengths, as in a stochastic matrix.\n\nKatz centrality is a generalization of degree centrality. Degree centrality measures the number of direct neighbors, and Katz centrality measures the number of all nodes that can be connected through a path, while the contributions of distant nodes are penalized. Mathematically, it is defined as \n\nwhere formula_76 is an attenuation factor in formula_77.\n\nKatz centrality can be viewed as a variant of eigenvector centrality. Another form of Katz centrality is \n\nCompared to the expression of eigenvector centrality, formula_79 is replaced by formula_80\n\nIt is shown that the principal eigenvector (associated with the largest eigenvalue of formula_81, the adjacency matrix) is the limit of Katz centrality as formula_76 approaches formula_83 from below.\n\nPageRank satisfies the following equation\n\nwhere \n\nis the number of neighbors of node formula_86 (or number of outbound links in a directed graph). Compared to eigenvector centrality and Katz centrality, one major difference is the scaling factor formula_87. Another difference between PageRank and eigenvector centrality is that the PageRank vector is a left hand eigenvector (note the factor formula_88 has indices reversed).\n\nA slew of centrality measures exist to determine the ‘importance’ of a single node in a complex network. However, these measures quantify the importance of a node in purely topological terms, and the value of the node does not depend on the ‘state’ of the node in any way. It remains constant regardless of network dynamics. This is true even for the weighted betweenness measures. However, a node may very well be centrally located in terms of betweenness centrality or another centrality measure, but may not be ‘centrally’ located in the context of a network in which there is percolation. Percolation of a ‘contagion’ occurs in complex networks in a number of scenarios. For example, viral or bacterial infection can spread over social networks of people, known as contact networks. The spread of disease can also be considered at a higher level of abstraction, by contemplating a network of towns or population centres, connected by road, rail or air links. Computer viruses can spread over computer networks. Rumours or news about business offers and deals can also spread via social networks of people. In all of these scenarios, a ‘contagion’ spreads over the links of a complex network, altering the ‘states’ of the nodes as it spreads, either recoverably or otherwise. For example, in an epidemiological scenario, individuals go from ‘susceptible’ to ‘infected’ state as the infection spreads. The states the individual nodes can take in the above examples could be binary (such as received/not received a piece of news), discrete (susceptible/infected/recovered), or even continuous (such as the proportion of infected people in a town), as the contagion spreads. The common feature in all these scenarios is that the spread of contagion results in the change of node states in networks. Percolation centrality (PC) was proposed with this in mind, which specifically measures the importance of nodes in terms of aiding the percolation through the network. This measure was proposed by Piraveenan et al.\n\nThe Percolation Centrality is defined for a given node, at a given time, as the proportion of ‘percolated paths’ that go through that node. A ‘percolated path’ is a shortest path between a pair of nodes, where the source node is percolated (e.g., infected). The target node can be percolated or non-percolated, or in a partially percolated state.\n\n"}
{"id": "4184621", "url": "https://en.wikipedia.org/wiki?curid=4184621", "title": "Chetaev instability theorem", "text": "Chetaev instability theorem\n\nThe Chetaev instability theorem for dynamical systems states that if there exists, for the system formula_1 with an equilibrium point at the origin, a continuously differentiable function V(x) such that\n\nthen the origin is an unstable equilibrium point of the system.\n\nThis theorem is somewhat less restrictive than the Lyapunov instability theorems, since a complete sphere (circle) around the origin for which V and formula_6 both are of the same sign does not have to be produced.\n\nIt is named after Nicolai Gurevich Chetaev.\n\n"}
{"id": "46802", "url": "https://en.wikipedia.org/wiki?curid=46802", "title": "Continued fraction", "text": "Continued fraction\n\n}</math>\n\nIn mathematics, a continued fraction is an expression obtained through an iterative process of representing a number as the sum of its integer part and the reciprocal of another number, then writing this other number as the sum of its integer part and another reciprocal, and so on. In a finite continued fraction (or terminated continued fraction), the iteration/recursion is terminated after finitely many steps by using an integer in lieu of another continued fraction. In contrast, an infinite continued fraction is an infinite expression. In either case, all integers in the sequence, other than the first, must be positive. The integers formula_3 are called the coefficients or terms of the continued fraction.\n\nContinued fractions have a number of remarkable properties related to the Euclidean algorithm for integers or real numbers. Every rational number has two closely related expressions as a finite continued fraction, whose coefficients can be determined by applying the Euclidean algorithm to formula_6. The numerical value of an infinite continued fraction is irrational; it is defined from its infinite sequence of integers as the limit of a sequence of values for finite continued fractions. Each finite continued fraction of the sequence is obtained by using a finite prefix of the infinite continued fraction's defining sequence of integers. Moreover, every irrational number formula_7 is the value of a \"unique\" infinite continued fraction, whose coefficients can be found using the non-terminating version of the Euclidean algorithm applied to the incommensurable values formula_7 and 1. This way of expressing real numbers (rational and irrational) is called their \"continued fraction representation\".\n\nIt is generally assumed that the numerator of all of the fractions is 1. If arbitrary values and/or functions are used in place of one or more of the numerators or the integers in the denominators, the resulting expression is a generalized continued fraction. When it is necessary to distinguish the first form from generalized continued fractions, the former may be called a simple or regular continued fraction, or said to be in canonical form.\n\nThe term \"continued fraction\" may also refer to representations of rational functions, arising in their analytic theory. For this use of the term, see Padé approximation and Chebyshev rational functions.\n\nConsider, for example, the rational number , which is around 4.4624. As a first approximation, start with 4, which is the integer part; . Note that the fractional part is the reciprocal of which is about 2.1628. Use the integer part, 2, as an approximation for the reciprocal to obtain a second approximation of .\nThe remaining fractional part, , is the reciprocal of , and is around 6.1429. Use 6 as an approximation for this to obtain as an approximation for and , about 4.4615, as the third approximation; . Finally, the fractional part, , is the reciprocal of 7, so its approximation in this scheme, 7, is exact () and produces the exact expression for .\n\nThe expression is called the continued fraction representation of . This can be represented by the abbreviated notation = [4; 2, 6, 7]. (Note that it is customary to replace only the \"first\" comma by a semicolon.) Some older textbooks use all commas in the -tuple, for example, [4, 2, 6, 7].\n\nIf the starting number is rational, then this process exactly parallels the Euclidean algorithm. In particular, it must terminate and produce a finite continued fraction representation of the number. If the starting number is irrational, then the process continues indefinitely. This produces a sequence of approximations, all of which are rational numbers, and these converge to the starting number as a limit. This is the (infinite) continued fraction representation of the number. Examples of continued fraction representations of irrational numbers are:\n\nContinued fractions are, in some ways, more \"mathematically natural\" representations of a real number than other representations such as decimal representations, and they have several desirable properties:\n\nA continued fraction is an expression of the form\n\nwhere a and b are either rational numbers, real numbers, or complex numbers.\nIf b = 1 for all \"i\" the expression is called a \"simple\" continued fraction.\nIf the expression contains a finite number of terms, it is called a \"finite\" continued fraction.\nIf the expression contains an infinite number of terms, it is called an \"infinite\" continued fraction.\n\nThus, all of the following illustrate valid finite simple continued fractions:\n\nConsider a real number .\nLet formula_10 be the integer part of and let\nformula_11 be the fractional part of .\nThen the continued fraction representation of is\nformula_12, where formula_13 is the continued fraction representation of formula_14.\n\nTo calculate a continued fraction representation of a number , write down the integer part (technically the floor) of . Subtract this integer part from . If the difference is 0, stop; otherwise find the reciprocal of the difference and repeat. The procedure will halt if and only if is rational. This process can be efficiently implemented using the Euclidean algorithm when the number is rational. The table below shows an implementation of this procedure for the number 3.245, resulting in the continued fraction expansion [3; 4,12,4].\n\nThe integers formula_2, formula_16 etc., are called the \"coefficients\" or \"terms\" of the continued fraction. One can abbreviate the continued fraction\n\nin the notation of Carl Friedrich Gauss\n\nor as\n\nor in the notation of Pringsheim as\n\nor in another related notation as\n\nSometimes angle brackets are used, like this:\n\nThe semicolon in the square and angle bracket notations is sometimes replaced by a comma.\n\nOne may also define \"infinite simple continued fractions\" as limits:\n\nThis limit exists for any choice of formula_2 and positive integers formula_25\n\nEvery finite continued fraction represents a rational number, and every rational number can be represented in precisely two different ways as a finite continued fraction, with the conditions that the first coefficient is an integer and other coefficients being positive integers. These two representations agree except in their final terms. In the longer representation the final term in the continued fraction is 1; the shorter representation drops the final 1, but increases the new final term by 1. The final element in the short representation is therefore always greater than 1, if present. In symbols:\n\nThe continued fraction representations of a positive rational number and its reciprocal are identical except for a shift one place left or right depending on whether the number is less than or greater than one respectively. In other words, the numbers represented by\nformula_26 and formula_27 are reciprocals. For instance if formula_28 is an integer and formula_29 then\n\nIf formula_32 then\n\nThe last number that generates the remainder of the continued fraction is the same for both formula_35 and its reciprocal.\n\nFor example,\n\nEvery infinite continued fraction is irrational, and every irrational number can be represented in precisely one way as an infinite continued fraction.\n\nAn infinite continued fraction representation for an irrational number is useful because its initial segments provide rational approximations to the number. These rational numbers are called the convergents of the continued fraction. The larger a term is in the continued fraction, the closer the corresponding convergent is to the irrational number being approximated. Numbers like π have occasional large terms in their continued fraction, which makes them easy to approximate with rational numbers. Other numbers like \"e\" have only small terms early in their continued fraction, which makes them more difficult to approximate rationally. The golden ratio ϕ has terms equal to 1 everywhere—the smallest values possible—which makes ϕ the most difficult number to approximate rationally. In this sense, therefore, it is the \"most irrational\" of all irrational numbers. Even-numbered convergents are smaller than the original number, while odd-numbered ones are larger.\n\nFor a continued fraction , the first four convergents (numbered 0 through 3) are\n\nIn words, the numerator of the third convergent is formed by multiplying the numerator of the second convergent by the third quotient, and adding the numerator of the first convergent. The denominators are formed similarly. Therefore, each convergent can be expressed explicitly in terms of the continued fraction as the ratio of certain multivariate polynomials called \"continuants\".\n\nIf successive convergents are found, with numerators , , … and denominators , , … then the relevant recursive relation is:\n\nThe successive convergents are given by the formula\n\nThus to incorporate a new term into a rational approximation, only the two previous convergents are necessary. The initial \"convergents\" (required for the first two terms) are ⁄ and ⁄. For example, here are the convergents for [0;1,5,2,2].\n\nWhen using the Babylonian method to generate successive approximations to the square root of an integer, if one starts with the lowest integer as first approximant, the rationals generated all appear in the list of convergents for the continued fraction. Specifically, the approximants will appear on the convergents list in positions 0, 1, 3, 7, 15, … , , ... For example, the continued fraction expansion for is [1;1,2,1,2,1,2,1,2,…]. Comparing the convergents with the approximants derived from the Babylonian method:\n\nBaire space is a topological space on infinite sequences of natural numbers. The infinite continued fraction provides a homeomorphism from Baire space to the space of irrational real numbers (with the subspace topology inherited from the usual topology on the reals). The infinite continued fraction also provides a map between the quadratic irrationals and the dyadic rationals, and from other irrationals to the set of infinite strings of binary numbers (i.e. the Cantor set); this map is called the Minkowski question mark function. The mapping has interesting self-similar fractal properties; these are given by the modular group, which is the subgroup of Möbius transformations having integer values in the transform. Roughly speaking, continued fraction convergents can be taken to be Möbius transformations acting on the (hyperbolic) upper half-plane; this is what leads to the fractal self-symmetry.\n\nIf formula_2, formula_16, formula_40, formula_41 is an infinite sequence of positive integers, define the sequences formula_42 and formula_43 recursively:\nTheorem 1. For any positive real number formula_44\n\nTheorem 2. The convergents of [formula_2; formula_16, formula_40, formula_41] are given by\n\nTheorem 3. If the formula_1th convergent to a continued fraction is formula_42/formula_43, then\n\nCorollary 1: Each convergent is in its lowest terms (for if formula_42 and formula_43 had a nontrivial common divisor it would divide formula_57, which is impossible).\n\nCorollary 2: The difference between successive convergents is a fraction whose numerator is unity:\n\nCorollary 3: The continued fraction is equivalent to a series of alternating terms:\n\nCorollary 4: The matrix\nhas determinant plus or minus one, and thus belongs to the group of\nformula_61 unimodular matrices formula_62.\n\nTheorem 4. Each (formula_63th) convergent is nearer to a subsequent (formula_1th) convergent than any preceding (formula_65th) convergent is. In symbols, if the formula_1th convergent is taken to be formula_67, then\nfor all formula_69.\n\nCorollary 1: The even convergents (before the formula_1th) continually increase, but are always less than formula_71.\n\nCorollary 2: The odd convergents (before the formula_1th) continually decrease, but are always greater than formula_71.\n\nTheorem 5.\n\nCorollary 1: Any convergent is nearer to the continued fraction than any other fraction whose denominator is less than that of the convergent.\n\nCorollary 2: Any convergent which immediately precedes a large quotient is a near approximation to the continued fraction.\n\nIf\n\nare consecutive convergents, then any fractions of the form\n\nwhere formula_77 is an integer such that formula_78, are called \"semiconvergents\", \"secondary convergents\", or \"intermediate fractions\". The formula_79-st semiconvergent equals the mediant of the formula_77-th one and the convergent formula_81. It follows that semiconvergents represent a monotonic sequence of fractions between the convergents formula_82 (corresponding to formula_83) and formula_84 (corresponding to formula_85). Sometimes the term is taken to mean that being a semiconvergent excludes the possibility of being a convergent (i.e., formula_86), rather than that a convergent is a kind of semiconvergent.\n\nThe semiconvergents to the continued fraction expansion of a real number formula_35 include all the rational approximations that are better than any approximation with a smaller denominator. Another useful property is that consecutive semiconvergents formula_88 and formula_89 are such that\nformula_90.\n\nOne can choose to define a \"best rational approximation\" to a real number as a rational number , , that is closer to than any approximation with a smaller or equal denominator. The simple continued fraction for generates \"all\" of the best rational approximations for according to three rules:\n\n\nFor example, 0.84375 has continued fraction [0;1,5,2,2]. Here are all of its best rational approximations.\n\nThe strictly monotonic increase in the denominators as additional terms are included permits an algorithm to impose a limit, either on size of denominator or closeness of approximation.\n\nThe \"half rule\" mentioned above is that when is even, the halved term /2 is admissible if and only if This is equivalent to:\n\nThe convergents to are best approximations in an even stronger sense: / is a convergent for if and only if is the least among all approximations / with ; that is, we have so long as . (Note also that as .)\n\nA rational that falls within the interval , for , can be found with the continued fractions for and . When both and are irrational and\nwhere and have identical continued fraction expansions up through , a rational that falls within the interval is given by the finite continued fraction,\nThis rational will be best in the sense that no other rational in will have a smaller numerator or a smaller denominator.\n\nIf is rational, it will have \"two\" continued fraction representations that are \"finite\", and , and similarly a rational  will have two representations, and . The coefficients beyond the last in any of these representations should be interpreted as ; and the best rational will be one of , , , or .\n\nFor example, the decimal representation 3.1416 could be rounded from any number in the interval . The continued fraction representations of 3.14155 and 3.14165 are\nand the best rational between these two is\nThus, is the best rational number corresponding to the rounded decimal number 3.1416, in the sense that no other rational number that would be rounded to 3.1416 will have a smaller numerator or a smaller denominator.\n\nA rational number, which can be expressed as finite continued fraction in two ways,\nwill be one of the convergents for the continued fraction expansion of a number, if and only if the number is strictly between\nThe numbers and are formed by incrementing the last coefficient in the two representations for . It is the case that when is even, and when is odd.\n\nFor example, the number has the continued fraction representations\nand thus is a convergent of any number strictly between\n\nConsider and . If is the smallest index for which is unequal to then if and otherwise.\n\nIf there is no such , but one expansion is shorter than the other, say and with for , then if is even and if is odd.\n\nTo calculate the convergents of we may set , define and , and , . Continuing like this, one can determine the infinite continued fraction of as\nThe fourth convergent of is [3;7,15,1] = = 3.14159292035..., sometimes called Milü, which is fairly close to the true value of .\n\nLet us suppose that the quotients found are, as above, [3;7,15,1]. The following is a rule by which we can write down at once the convergent fractions which result from these quotients without developing the continued fraction.\n\nThe first quotient, supposed divided by unity, will give the first fraction, which will be too small, namely, . Then, multiplying the numerator and denominator of this fraction by the second quotient and adding unity to the numerator, we shall have the second fraction, , which will be too large. Multiplying in like manner the numerator and denominator of this fraction by the third quotient, and adding to the numerator the numerator of the preceding fraction, and to the denominator the denominator of the preceding fraction, we shall have the third fraction, which will be too small. Thus, the third quotient being 15, we have for our numerator , and for our denominator, . The third convergent, therefore, is . We proceed in the same manner for the fourth convergent. The fourth quotient being 1, we say 333 times 1 is 333, and this plus 22, the numerator of the fraction preceding, is 355; similarly, 106 times 1 is 106, and this plus 7 is 113.\n\nIn this manner, by employing the four quotients [3;7,15,1], we obtain the four fractions:\n\nThese convergents are alternately smaller and larger than the true value of , and approach nearer and nearer to . The difference between a given convergent and is less than the reciprocal of the product of the denominators of that convergent and the next convergent. For example, the fraction is greater than , but − is less than  =  (in fact, − is just more than = ).\n\nThe demonstration of the foregoing properties is deduced from the fact that if we seek the difference between one of the convergent fractions and the next adjacent to it we shall obtain a fraction of which the numerator is always unity and the denominator the product of the two denominators. Thus the difference between and is , in excess; between and , , in deficit; between and , , in excess; and so on. The result being, that by employing this series of differences we can express in another and very simple manner the fractions with which we are here concerned, by means of a second series of fractions of which the numerators are all unity and the denominators successively be the product of every two adjacent denominators. Instead of the fractions written above, we have thus the series:\n\nThe first term, as we see, is the first fraction; the first and second together give the second fraction, ; the first, the second and the third give the third fraction , and so on with the rest; the result being that the series entire is equivalent to the original value.\n\nA generalized continued fraction is an expression of the form\n\nwhere the \"a\" (\"n\" > 0) are the partial numerators, the \"b\" are the partial denominators, and the leading term \"b\" is called the \"integer\" part of the continued fraction.\n\nTo illustrate the use of generalized continued fractions, consider the following example. The sequence of partial denominators of the simple continued fraction of does not show any obvious pattern:\n\nor\n\nHowever, several generalized continued fractions for have a perfectly regular structure, such as:\n\nThe first two of these are special cases of the function with = 4 arctan (1).\n\nThe above continued fraction of pi consisting of cubes uses the Nilakantha series and an exploit from Leonhard Euler.\n\nThe numbers with periodic continued fraction expansion are precisely the irrational solutions of quadratic equations with rational coefficients; rational solutions have finite continued fraction expansions as previously stated. The simplest examples are the golden ratio φ = [1;1,1,1,1,1,…] and = [1;2,2,2,2,…], while = [3;1,2,1,6,1,2,1,6…] and = [6;2,12,2,12,2,12…]. All irrational square roots of integers have a special form for the period; a symmetrical string, like the empty string (for ) or 1,2,1 (for ), followed by the double of the leading integer.\n\nBecause the continued fraction expansion for φ doesn't use any integers greater than 1, φ is one of the most \"difficult\" real numbers to approximate with rational numbers. Hurwitz's theorem states that any irrational number can be approximated by infinitely many rational with\n\nWhile virtually all real numbers will eventually have infinitely many convergents whose distance from is significantly smaller than this limit, the convergents for φ (i.e., the numbers , , , , etc.) consistently \"toe the boundary\", keeping a distance of almost exactly formula_99 away from φ, thus never producing an approximation nearly as impressive as, for example, for . It can also be shown that every real number of the form , where , , , and are integers such that , shares this property with the golden ratio φ; and that all other real numbers can be more closely approximated.\n\nWhile there is no discernable pattern in the simple continued fraction expansion of , there is one for , the base of the natural logarithm:\n\nwhich is a special case of this general expression for positive integer :\n\nAnother, more complex pattern appears in this continued fraction expansion for positive odd :\n\nwith a special case for :\n\nOther continued fractions of this sort are\n\nwhere is a positive integer; also, for integer :\n\nwith a special case for :\n\nIf is the modified, or hyperbolic, Bessel function of the first kind, we may define a function on the rationals by\n\nwhich is defined for all rational numbers, with and in lowest terms. Then for all nonnegative rationals, we have\n\nwith similar formulas for negative rationals; in particular we have\n\nMany of the formulas can be proved using Gauss's continued fraction.\n\nMost irrational numbers do not have any periodic or regular behavior in their continued fraction expansion. Nevertheless, Khinchin proved that for almost all real numbers , the (for ) have an astonishing property: their geometric mean is a constant (known as Khinchin's constant, ) independent of the value of . Paul Lévy showed that the th root of the denominator of the th convergent of the continued fraction expansion of almost all real numbers approaches an asymptotic limit, approximately 3.27582, which is known as Lévy's constant. Lochs' theorem states that th convergent of the continued fraction expansion of almost all real numbers determines the number to an average accuracy of just over decimal places.\n\nGeneralized continued fractions are used in a method for computing square roots.\n\nThe identity\nleads via recursion to the generalized continued fraction for any square root:\n\nContinued fractions play an essential role in the solution of Pell's equation. For example, for positive integers and , and non-square , it is true that if and only if is a convergent of the regular continued fraction for .\n\nContinued fractions also play a role in the study of dynamical systems, where they tie together the Farey fractions which are seen in the Mandelbrot set with Minkowski's question mark function and the modular group Gamma.\n\nThe backwards shift operator for continued fractions is the map called the Gauss map, which lops off digits of a continued fraction expansion: . The transfer operator of this map is called the Gauss–Kuzmin–Wirsing operator. The distribution of the digits in continued fractions is given by the zero'th eigenvector of this operator, and is called the Gauss–Kuzmin distribution.\n\nThe Lanczos algorithm uses a continued fraction expansion to iteratively approximate the eigenvalues and eigenvectors of a large sparse matrix.\n\nContinued fraction has also been used in modelling optimization problems for wireless network virtualization to find a route between a source and a destination.\n\n\n\n\n"}
{"id": "52080279", "url": "https://en.wikipedia.org/wiki?curid=52080279", "title": "Coxeter decompositions of hyperbolic polygons", "text": "Coxeter decompositions of hyperbolic polygons\n\nA Coxeter decomposition of a polygon is a decomposition into a finite number of polygons in which any two sharing a side are reflections of each other along that side. Hyperbolic polygons are the analogues of Euclidean polygons in hyperbolic geometry. A hyperbolic \"n\"-gon is an area bounded by \"n\" segments, rays, or entire straight lines. The standard model for this geometry is the Poincaré disk model. A major difference between Euclidean and hyperbolic polygons is that the sum of internal angles of a hyperbolic polygon is not the same as Euclidean polygons. In particular, the sum of the angles of a hyperbolic triangle is less than 180 degrees. Coxeter decompositions are named after Harold Scott MacDonald Coxeter, an accomplished 20th century geometer. He introduced the Coxeter group, an abstract group generated by reflections. These groups have many uses, including producing the rotations of Platonic solids and tessellating the plane.\n\nGiven a polygon \"P\", a group \"G\" can be generated by reflecting \"P\" around its sides. If the angles of \"P\" are /\"k\" for natural numbers \"k\", then \"G\" will be discrete. A Coxeter decomposition of a polygon is a decomposition into a finite number of polygons in which any two sharing a side are reflections of each other along that side.\n\nThe goal of a Coxeter decomposition is to break up a polygon into a composition of congruent triangles reflected on its sides.\n\nIf triangle ABC can undergo Coxeter decomposition and has angles formula_1, where formula_2 is the number of times the formula_3th angle is broken up, the triangle ABC can be written as formula_4. Several properties of these fundamental polygons are known for hyperbolic triangles.\n\n\nQuadrilaterals may also have Coxeter decompositions.\n"}
{"id": "26147867", "url": "https://en.wikipedia.org/wiki?curid=26147867", "title": "Detlef Laugwitz", "text": "Detlef Laugwitz\n\nDetlef Laugwitz (1932–2000) was a German mathematician and historian, who worked in differential geometry, history of mathematics, functional analysis, and non-standard analysis.\n\nHe was born on 11 May 1932 in Breslau, Germany. Starting in 1949, he studied mathematics, physics, and philosophy at the Georg-August-University at Göttingen, where he received his doctorate in 1954. Until 1956 he worked in the Mathematical Research Institute of Oberwolfach. In 1958 he became a lecturer at the Technical University of Munich, where he obtained his Habilitation. In 1958 he moved to the Technical University of Darmstadt, where in 1962 he became a professor, and remained until his retirement. From 1976 to 1984 he was a visiting professor at Caltech.\n\nLaugwitz worked in differential geometry of infinite dimensional vector spaces (his dissertation) and in Finsler geometry. In 1958 he and Curt Schmieden developed their own approach to infinitesimals through field extensions, independently of Abraham Robinson. They described this as \"infinitesimal mathematics\" and leading back to the historical roots in Leibniz. In 1996 he published the standard biography of Bernhard Riemann.\n\n"}
{"id": "357371", "url": "https://en.wikipedia.org/wiki?curid=357371", "title": "Dickson's lemma", "text": "Dickson's lemma\n\nIn mathematics, Dickson's lemma states that every set of formula_1-tuples of natural numbers has finitely many minimal elements. This simple fact from combinatorics has become attributed to the American algebraist L. E. Dickson, who used it to prove a result in number theory about perfect numbers. However, the lemma was certainly known earlier, for example to Paul Gordan in his research on invariant theory.\n\nLet formula_2 be a fixed number, and let formula_3 be the set of pairs of numbers whose product is at least formula_2. When defined over the positive real numbers, formula_5 has infinitely many minimal elements of the form formula_6, one for each positive number formula_7; this set of points forms one of the branches of a hyperbola. The pairs on this hyperbola are minimal, because it is not possible for a different pair that belongs to formula_5 to be less than or equal to formula_6 in both of its coordinates. However, Dickson's lemma concerns only tuples of natural numbers, and over the natural numbers there are only finitely many minimal pairs. Every minimal pair formula_10 of natural numbers has formula_11 and formula_12, for if \"x\" were greater than \"K\" then (\"x\" −1,\"y\") would also belong to \"S\", contradicting the minimality of (\"x\",\"y\"), and symmetrically if \"y\" were greater than \"K\" then (\"x\",\"y\" −1) would also belong to \"S\". Therefore, over the natural numbers, formula_5 has at most formula_14 minimal elements, a finite number.\n\nLet formula_15 be the set of non-negative integers (natural numbers), let \"n\" be any fixed constant, and let formula_16 be the set of formula_1-tuples of natural numbers. These tuples may be given a pointwise partial order, the product order, in which formula_18 if and only if, for every formula_19, formula_20.\nThe set of tuples that are greater than or equal to some particular tuple formula_21 forms a positive orthant with its apex at the given tuple.\n\nWith this notation, Dickson's lemma may be stated in several equivalent forms:\n\nDickson used his lemma to prove that, for any given number formula_1, there can exist only a finite number of odd perfect numbers that have at most formula_1 prime factors. However, it remains open whether there exist any odd perfect numbers at all.\n\nThe divisibility relation among the \"P\"-smooth numbers, natural numbers whose prime factors all belong to the finite set \"P\", gives these numbers the structure of a partially ordered set isomorphic to formula_37. Thus, for any set \"S\" of \"P\"-smooth numbers, there is a finite subset of \"S\" such that every element of \"S\" is divisible by one of the numbers in this subset. This fact has been used, for instance, to show that there exists an algorithm for classifying the winning and losing moves from the initial position in the game of Sylver coinage, even though the algorithm itself remains unknown.\n\nThe tuples formula_21 in formula_16 correspond one-for-one with the monomials formula_40 over a set of formula_1 variables formula_42. Under this correspondence, Dickson's lemma may be seen as a special case of Hilbert's basis theorem stating that every polynomial ideal has a finite basis, for the ideals generated by monomials. Indeed, Paul Gordan used this restatement of Dickson's lemma in 1899 as part of a proof of Hilbert's basis theorem.\n\n"}
{"id": "55823152", "url": "https://en.wikipedia.org/wiki?curid=55823152", "title": "Diffeomorphometry", "text": "Diffeomorphometry\n\nDiffeomorphometry is the metric study of imagery, shape and form in the discipline of computational anatomy (CA) in medical imaging. The study of images in computational anatomy rely on high-dimensional diffeomorphism groups formula_1 which generate orbits of the form formula_2, in which images formula_3 can be dense scalar magnetic resonance or computed axial tomography images. For deformable shapes these are the collection of manifolds formula_4, points, curves and surfaces. The diffeomorphisms move the images and shapes through the orbit according to formula_5 which are defined as the group actions of computational anatomy.\n\nThe orbit of shapes and forms is made into a metric space by inducing a metric on the group of diffeomorphisms. The study of metrics on groups of diffeomorphisms and the study of metrics between manifolds and surfaces has been an area of significant investigation. In Computational anatomy, the diffeomorphometry metric measures how close and far two shapes or images are from each other. Informally, the metric is constructed by defining a flow of diffemorphisms formula_6 which connect the group elements from one to another, so for formula_7 then formula_8. The metric between two coordinate systems or diffeomorphisms is then the shortest length or geodesic flow connecting them. The metric on the space associated to the geodesics is given byformula_9. The metrics on the orbits formula_10 are inherited from the metric induced on the diffeomorphism group.\n\nThe group formula_1 is thusly made into a smooth Riemannian manifold with Riemannian metric formula_12 associated to the tangent spaces at all formula_13. The Riemannian metric satisfies at every point of the manifold formula_14 there is an inner product inducing the norm on the tangent space formula_15 that varies smoothly across formula_16.\n\nOftentimes, the familiar Euclidean metric is not directly applicable because the patterns of shapes and images don't form a vector space. In the Riemannian orbit model of Computational anatomy, diffeomorphisms acting on the forms formula_17 don't act linearly. There are many ways to define metrics, and for the sets associated to shapes the Hausdorff metric is another. The method used to induce the Riemannian metric is to induce the metric on the orbit of shapes by defining it in terms of the metric length between diffeomorphic coordinate system transformations of the flows. Measuring the lengths of the geodesic flow between coordinates systems in the orbit of shapes is called diffeomorphometry.\n\nThe diffeomorphisms in computational anatomy are generated to satisfy the Lagrangian and Eulerian specification of the flow fields, formula_18, generated via the ordinary differential equation\n\nwith the Eulerian vector fields formula_19 in formula_20 for formula_21. The inverse for the flow is given by\nformula_22\nand the formula_23 Jacobian matrix for flows in formula_24 given as formula_25\n\nTo ensure smooth flows of diffeomorphisms with inverse, the vector fields formula_20 must be at least 1-time continuously differentiable in space which are modelled as elements of the Hilbert space formula_27 using the Sobolev embedding theorems so that each element formula_28 has 3-square-integrable derivatives thusly implies formula_27 embeds smoothly in 1-time continuously differentiable functions. The diffeomorphism group are flows with vector fields absolutely integrable in Sobolev norm:\n\nShapes in Computational Anatomy (CA) are studied via the use of diffeomorphic mapping for establishing correspondences between anatomical coordinate systems. In this setting, 3-dimensional medical images are modelled as diffemorphic transformations of some exemplar, termed the template formula_30, resulting in the observed images to be elements of the random orbit model of CA. For images these are defined as formula_31, with for charts representing sub-manifolds denoted as formula_32.\n\nThe orbit of shapes and forms in Computational Anatomy are generated by the group action formula_33 , formula_34. These are made into a Riemannian orbits by introducing a metric associated to each point and associated tangent space. For this a metric is defined on the group which induces the metric on the orbit. Take as the metric for Computational anatomy at each element of the tangent space formula_35 in the group of diffeomorphisms\n\nwith the vector fields modelled to be in a Hilbert space with the norm in the Hilbert space formula_27. We model formula_38 as a reproducing kernel Hilbert space (RKHS) defined by a 1-1, differential operator formula_39, where formula_40 is the dual-space. In general, formula_41 is a generalized function or distribution, the linear form associated to the inner-product and norm for generalized functions are interpreted by integration by parts according to for formula_42,\n\nWhen formula_44, a vector density, formula_45\n\nThe differential operator is selected so that the Green's kernel associated to the inverse is sufficiently smooth so that the vector fields support 1-continuous derivative. The Sobolev embedding theorem arguments were made in demonstrating that 1-continuous derivative is required for smooth flows. The Green's operator generated from the Green's function(scalar case) associated to the differential operator smooths.\n\nFor proper choice of formula_46 then formula_47 is an RKHS with the operator formula_48. The Green's kernels associated to the differential operator smooths since for controlling enough derivatives in the square-integral sense the kernel formula_49 is continuously differentiable in both variables implying\n\n(I,J)=\\inf_{\\phi \\in \\operatorname{Diff}_V: \\phi \\cdot I = J } d_{\\operatorname{Diff}_V}(id,\\phi) \\ ;\n\nThe distance on shapes and forms, formula_51,\n\nFor calculating the metric, the geodesics are a dynamical system, the flow of coordinates formula_52 and the control the vector field formula_53 related via formula_54 The Hamiltonian view\n\nThe Pontryagin maximum principle gives the Hamiltonian formula_59\nThe optimizing vector field formula_60 with dynamics formula_61. Along the geodesic the Hamiltonian is constant:\nformula_62. The metric distance between coordinate systems connected via the geodesic determined by the induced distance between identity and group element:\n\nFor landmarks, formula_64, the Hamiltonian momentum\n\nwith Hamiltonian dynamics taking the form\n\nwith\nThe metric between landmarks formula_68\n\nThe dynamics associated to these geodesics is shown in the accompanying figure.\n\nFor surfaces, the Hamiltonian momentum is defined across the surface has Hamiltonian\n\nand dynamics\n\nFor volumes the Hamiltonian\n"}
{"id": "35675213", "url": "https://en.wikipedia.org/wiki?curid=35675213", "title": "Eleven-point conic", "text": "Eleven-point conic\n\nIn geometry, an eleven-point conic is a conic associated to four points and a line, containing 11 special points.\n\n\n"}
{"id": "34289077", "url": "https://en.wikipedia.org/wiki?curid=34289077", "title": "Euclidean random matrix", "text": "Euclidean random matrix\n\nAn \"N\"×\"N\" Euclidean random matrix Â is defined with the help of an arbitrary deterministic function \"f\"(r, r′) and of \"N\" points {r} randomly distributed in a region \"V\" of \"d\"-dimensional Euclidean space. The element A of the matrix is equal to \"f\"(r, r): A = \"f\"(r, r).\n\nEuclidean random matrices were first introduced in 1999. They studied a special case of functions \"f\" that depend only on the distances between the pairs of points: \"f\"(r, r′) = \"f\"(r - r′) and imposed an additional condition on the diagonal elements A,\n\nmotivated by the physical context in which they studied the matrix.\nA Euclidean distance matrix is a particular example of Euclidean random matrix with either \"f\"(r - r) = |r - r| or \"f\"(r - r) = |r - r|.\n\nFor example, in many biological networks, the strength of interaction between two nodes depends on the physical proximity of those nodes. Spatial interactions between nodes can be modelled as a Euclidean random matrix, if nodes are placed randomly in space.\n\nBecause the positions of the points {r} are random, the matrix elements A are random too. Moreover, because the \"N\"×\"N\" elements are completely determined by only \"N\" points and, typically, one is interested in \"N\"≫\"d\", strong correlations exist between different elements.\n\nHermitian Euclidean random matrices appear in various physical contexts, including supercooled liquids, phonons in disordered systems, and waves in random media.\n\n\"Example 1:\" Consider the matrix Â generated by the function \"f\"(r, r′) = sin(\"k\"|r-r′|)/(\"k\"|r-r′|), with \"k\" = 2π/λ. This matrix is Hermitian and its eigenvalues Λ are real. For \"N\" points distributed randomly in a cube of side \"L\" and volume \"V\" = \"L\", one can show that the probability distribution of Λ is approximately given by the Marchenko-Pastur law, if the density of points ρ = \"N\"/\"V\" obeys ρλ ≤ 1 and 2.8\"N\"/(\"k\" \"L\") < 1 (see figure).\n\nA theory for the eigenvalue density of large (\"N\"≫1) non-Hermitian Euclidean random matrices has been developed and has been applied to study the problem of random laser.\n\n\"Example 2:\" Consider the matrix Â generated by the function \"f\"(r, r′) = exp(\"ik\"|r-r′|)/(\"k\"|r-r′|), with \"k\" = 2π/λ and \"f\"(r= r′) = 0. This matrix is not Hermitian and its eigenvalues Λ are complex. The probability distribution of Λ can be found analytically if the density of point ρ = \"N\"/\"V\" obeys ρλ ≤ 1 and 9\"N\"/(8\"k\" \"R\") < 1 (see figure).\n"}
{"id": "12684962", "url": "https://en.wikipedia.org/wiki?curid=12684962", "title": "Fisher–Yates shuffle", "text": "Fisher–Yates shuffle\n\nThe Fisher–Yates shuffle is an algorithm for generating a random permutation of a finite sequence—in plain terms, the algorithm shuffles the sequence. The algorithm effectively puts all the elements into a hat; it continually determines the next element by randomly drawing an element from the hat until no elements remain. The algorithm produces an unbiased permutation: every permutation is equally likely. The modern version of the algorithm is efficient: it takes time proportional to the number of items being shuffled and shuffles them in place.\n\nThe Fisher–Yates shuffle is named after Ronald Fisher and Frank Yates, who first described it, and is also known as the Knuth shuffle after Donald Knuth. A variant of the Fisher–Yates shuffle, known as Sattolo's algorithm, may be used to generate random cyclic permutations of length \"n\" instead of random permutations.\n\nThe Fisher–Yates shuffle, in its original form, was described in 1938 by Ronald Fisher and Frank Yates in their book \"Statistical tables for biological, agricultural and medical research\". Their description of the algorithm used pencil and paper; a table of random numbers provided the randomness. The basic method given for generating a random permutation of the numbers 1 through \"N\" goes as follows:\n\n\nProvided that the random numbers picked in step 2 above are truly random and unbiased, so will the resulting permutation be. Fisher and Yates took care to describe how to obtain such random numbers in any desired range from the supplied tables in a manner which avoids any bias. They also suggested the possibility of using a simpler method — picking random numbers from one to \"N\" and discarding any duplicates—to generate the first half of the permutation, and only applying the more complex algorithm to the remaining half, where picking a duplicate number would otherwise become frustratingly common.\n\nThe modern version of the Fisher–Yates shuffle, designed for computer use, was introduced by Richard Durstenfeld in 1964 and popularized by Donald E. Knuth in \"The Art of Computer Programming\" as \"Algorithm P (Shuffling)\". Neither Durstenfeld's article nor Knuth's first edition of \"The Art of Computer Programming\" acknowledged the work of Fisher and Yates; they may not have been aware of it. Subsequent editions of Knuth's \"The Art of Computer Programming\" mention Fisher and Yates' contribution.\n\nThe algorithm described by Durstenfeld differs from that given by Fisher and Yates in a small but significant way. Whereas a naive computer implementation of Fisher and Yates' method would spend needless time counting the remaining numbers in step 3 above, Durstenfeld's solution is to move the \"struck\" numbers to the end of the list by swapping them with the last unstruck number at each iteration. This reduces the algorithm's time complexity to \"O\"(\"n\"), compared to \"O\"(\"n\") for the naïve implementation. This change gives the following algorithm (for a zero-based array).\n\nAn equivalent version which shuffles the array in the opposite direction (from lowest index to highest) is:\n\nAs an example, we'll permute the numbers from 1 to 8 using Fisher and Yates' original method. We'll start by writing the numbers out on a piece of scratch paper:\n\nNow we roll a random number \"k\" from 1 to 8—let's make it 3—and strike out the \"k\"th (i.e. third) number (3, of course) on the scratch pad and write it down as the result:\n\nNow we pick a second random number, this time from 1 to 7: it turns out to be 4. Now we strike out the fourth number \"not yet struck\" off the scratch pad—that's number 5—and add it to the result:\n\nNow we pick the next random number from 1 to 6, and then from 1 to 5, and so on, always repeating the strike-out process as above:\n\nWe'll now do the same thing using Durstenfeld's version of the algorithm: this time, instead of striking out the chosen numbers and copying them elsewhere, we'll swap them with the last number not yet chosen. We'll start by writing out the numbers from 1 to 8 as before:\n\nFor our first roll, we roll a random number from 1 to 8: this time it is 6, so we swap the 6th and 8th numbers in the list:\n\nThe next random number we roll from 1 to 7, and turns out to be 2. Thus, we swap the 2nd and 7th numbers and move on:\n\nThe next random number we roll is from 1 to 6, and just happens to be 6, which means we leave the 6th number in the list (which, after the swap above, is now number 8) in place and just move to the next step. Again, we proceed the same way until the permutation is complete:\n\nAt this point there's nothing more that can be done, so the resulting permutation is 7 5 4 3 1 8 2 6.\n\nThe Fisher–Yates shuffle, as implemented by Durstenfeld, is an \"in-place shuffle\". That is, given a preinitialized array, it shuffles the elements of the array in place, rather than producing a shuffled copy of the array. This can be an advantage if the array to be shuffled is large.\n\nTo simultaneously initialize and shuffle an array, a bit more efficiency can be attained by doing an \"inside-out\" version of the shuffle. In this version, one successively places element number \"i\" into a random position among the first \"i\" positions in the array, after moving the element previously occupying that position to position \"i\". In case the random position happens to be number \"i\", this \"move\" (to the same place) involves an uninitialised value, but that does not matter, as the value is then immediately overwritten. No separate initialization is needed, and no exchange is performed. In the common case where \"source\" is defined by some simple function, such as the integers from 0 to \"n\" − 1, \"source\" can simply be replaced with the function since \"source\" is never altered during execution.\n\nThe inside-out shuffle can be seen to be correct by induction. Assuming a perfect random number generator, every one of the \"n\"! different sequences of random numbers that could be obtained from the calls of \"random\" will produce a different permutation of the values, so all of these are obtained exactly once. The condition that checks if \"j\" ≠ \"i\" may be omitted in languages that have no problems accessing uninitialized array values, and for which assigning is cheaper than comparing.\n\nAnother advantage of this technique is that the algorithm can be modified so that even when we do not know \"n\", the number of elements in \"source\", we can still generate a uniformly distributed random permutation of the \"source\" data. Below the array \"a\" is built iteratively starting from empty, and \"a\".length represents the current number of elements seen.\n\nA very similar algorithm was published in 1986 by Sandra Sattolo for generating uniformly distributed cycles of (maximal) length \"n\". The only difference between Durstenfeld's and Sattolo's algorithms is that in the latter, in step 2 above, the random number \"j\" is chosen from the range between 1 and \"i\"−1 (rather than between 1 and \"i\") inclusive. This simple change modifies the algorithm so that the resulting permutation always consists of a single cycle.\n\nIn fact, as described below, it is quite easy to \"accidentally\" implement Sattolo's algorithm when the ordinary Fisher–Yates shuffle is intended. This will bias the results by causing the permutations to be picked from the smaller set of (\"n\"−1)! cycles of length \"N\", instead of from the full set of all \"n\"! possible permutations.\n\nThe fact that Sattolo's algorithm always produces a cycle of length \"n\" can be shown by induction. Assume by induction that after the initial iteration of the loop, the remaining iterations permute the first \"n\" − 1 elements according to a cycle of length \"n\" − 1 (those remaining iterations are just Sattolo's algorithm applied to those first \"n\" − 1 elements). This means that tracing the initial element to its new position \"p\", then the element originally at position \"p\" to its new position, and so forth, one only gets back to the initial position after having visited all other positions. Suppose the initial iteration swapped the final element with the one at (non-final) position \"k\", and that the subsequent permutation of first \"n\" − 1 elements then moved it to position \"l\"; we compare the permutation \"π\" of all \"n\" elements with that remaining permutation \"σ\" of the first \"n\" − 1 elements. Tracing successive positions as just mentioned, there is no difference between \"π\" and \"σ\" until arriving at position \"k\". But then, under \"π\" the element originally at position \"k\" is moved to the final position rather than to position \"l\", and the element originally at the final position is moved to position \"l\". From there on, the sequence of positions for \"π\" again follows the sequence for \"σ\", and all positions will have been visited before getting back to the initial position, as required.\n\nAs for the equal probability of the permutations, it suffices to observe that the modified algorithm involves (\"n\"−1)! distinct possible sequences of random numbers produced, each of which clearly produces a different permutation, and each of which occurs—assuming the random number source is unbiased—with equal probability. The (\"n\"−1)! different permutations so produced precisely exhaust the set of cycles of length \"n\": each such cycle has a unique cycle notation with the value \"n\" in the final position, which allows for (\"n\"−1)! permutations of the remaining values to fill the other positions of the cycle notation.\n\nA sample implementation of Sattolo's algorithm in Python is:\n\nThe asymptotic time and space complexity of the Fisher–Yates shuffle are optimal. Combined with a high-quality unbiased random number source, it is also guaranteed to produce unbiased results. Compared to some other solutions, it also has the advantage that, if only part of the resulting permutation is needed, it can be stopped halfway through, or even stopped and restarted repeatedly, generating the permutation incrementally as needed.\n\nAn alternative method assigns a random number to each element of the set to be shuffled and then sorts the set according to the assigned numbers. The sorting method has the same asymptotic time complexity as Fisher–Yates: although general sorting is \"O\"(\"n\" log \"n\"), numbers are efficiently sorted using Radix sort in \"O\"(\"n\") time. Like the Fisher–Yates shuffle, the sorting method produces unbiased results. However, care must be taken to ensure that the assigned random numbers are never duplicated, since sorting algorithms typically don't order elements randomly in case of a tie. Additionally, this method requires asymptotically larger space: \"O\"(\"n\") additional storage space for the random numbers, versus \"O\"(1) space for the Fisher–Yates shuffle. Finally, we note that the sorting method has a simple parallel implementation, unlike the Fisher–Yates shuffle, which is sequential.\n\nA variant of the above method that has seen some use in languages that support sorting with user-specified comparison functions is to shuffle a list by sorting it with a comparison function that returns random values. However, \"this is an extremely bad method\": it is very likely to produce highly non-uniform distributions, which in addition depends heavily on the sorting algorithm used.\nFor instance suppose quicksort is used as sorting algorithm, with a fixed element selected as first pivot element. The algorithm starts comparing the pivot with all other elements to separate them into those less and those greater than it, and the relative sizes of those groups will determine the final place of the pivot element. For a uniformly distributed random permutation, each possible final position should be equally likely for the pivot element, but if each of the initial comparisons returns \"less\" or \"greater\" with equal probability, then that position will have a binomial distribution for \"p\" = 1/2, which gives positions near the middle of the sequence with a much higher probability for than positions near the ends. Randomized comparison functions applied to other sorting methods like merge sort may produce results that appear more uniform, but are not quite so either, since merging two sequences by repeatedly choosing one of them with equal probability (until the choice is forced by the exhaustion of one sequence) does not produce results with a uniform distribution; instead the probability to choose a sequence should be proportional to the number of elements left in it. In fact no method that uses only two-way random events with equal probability (\"coin flipping\"), repeated a bounded number of times, can produce permutations of a sequence (of more than two elements) with a uniform distribution, because every execution path will have as probability a rational number with as denominator a power of 2, while the required probability 1/\"n\"! for each possible permutation is not of that form.\n\nIn principle this shuffling method can even result in program failures like endless loops or access violations, because the correctness of a sorting algorithm may depend on properties of the order relation (like transitivity) that a comparison producing random values will certainly not have.\nWhile this kind of behaviour should not occur with sorting routines that never perform a comparison whose outcome can be predicted with certainty (based on previous comparisons), there can be valid reasons for deliberately making such comparisons. For instance the fact that any element should compare equal to itself allows using them as sentinel value for efficiency reasons, and if this is the case, a random comparison function would break the sorting algorithm.\n\nCare must be taken when implementing the Fisher–Yates shuffle, both in the implementation of the algorithm itself and in the generation of the random numbers it is built on, otherwise the results may show detectable bias. A number of common sources of bias have been listed below.\n\nA common error when implementing the Fisher–Yates shuffle is to pick the random numbers from the wrong range. The flawed algorithm may appear to work correctly, but it will not produce each possible permutation with equal probability, and it may not produce certain permutations at all. For example, a common off-by-one error would be choosing the index \"j\" of the entry to swap in the example above to be always strictly less than the index \"i\" of the entry it will be swapped with. This turns the Fisher–Yates shuffle into Sattolo's algorithm, which produces only permutations consisting of a single cycle involving all elements: in particular, with this modification, no element of the array can ever end up in its original position.\n\nSimilarly, always selecting \"j\" from the entire range of valid array indices on \"every\" iteration also produces a result which is biased, albeit less obviously so. This can be seen from the fact that doing so yields \"n\" distinct possible sequences of swaps, whereas there are only \"n\"! possible permutations of an \"n\"-element array. Since \"n\" can never be evenly divisible by \"n\"! when \"n\" > 2 (as the latter is divisible by \"n\"−1, which shares no prime factors with \"n\"), some permutations must be produced by more of the \"n\" sequences of swaps than others. As a concrete example of this bias, observe the distribution of possible outcomes of shuffling a three-element array [1, 2, 3]. There are 6 possible permutations of this array (3! = 6), but the algorithm produces 27 possible shuffles (3 = 27). In this case, [1, 2, 3], [3, 1, 2], and [3, 2, 1] each result from 4 of the 27 shuffles, while each of the remaining 3 permutations occurs in 5 of the 27 shuffles.\n\nThe matrix to the right shows the probability of each element in a list of length 7 ending up in any other position. Observe that for most elements, ending up in their original position (the matrix's main diagonal) has lowest probability, and moving one slot backwards has highest probability.\n\nDoing a Fisher–Yates shuffle involves picking uniformly distributed random integers from various ranges. Most random number generators, however — whether true or pseudorandom — will only directly provide numbers in a fixed range from 0 to RAND_MAX, and in some libraries, RAND_MAX may be as low as 32767. A simple and commonly used way to force such numbers into a desired range is to apply the modulo operator; that is, to divide them by the size of the range and take the remainder. However, the need in a Fisher–Yates shuffle to generate random numbers in every range from 0–1 to 0–\"n\" pretty much guarantees that some of these ranges will not evenly divide the natural range of the random number generator. Thus, the remainders will not always be evenly distributed and, worse yet, the bias will be systematically in favor of small remainders.\n\nFor example, assume that your random number source gives numbers from 0 to 99 (as was the case for Fisher and Yates' original tables), and that you wish to obtain an unbiased random number from 0 to 15. If you simply divide the numbers by 16 and take the remainder, you'll find that the numbers 0–3 occur about 17% more often than others. This is because 16 does not evenly divide 100: the largest multiple of 16 less than or equal to 100 is 6×16 = 96, and it is the numbers in the incomplete range 96–99 that cause the bias. The simplest way to fix the problem is to discard those numbers before taking the remainder and to keep trying again until a number in the suitable range comes up. While in principle this could, in the worst case, take forever, the expected number of retries will always be less than one.\n\nA related problem occurs with implementations that first generate a random floating-point number—usually in the range [0,1)—and then multiply it by the size of the desired range and round down. The problem here is that random floating-point numbers, however carefully generated, always have only finite precision. This means that there are only a finite number of possible floating point values in any given range, and if the range is divided into a number of segments that doesn't divide this number evenly, some segments will end up with more possible values than others. While the resulting bias will not show the same systematic downward trend as in the previous case, it will still be there.\n\nAn additional problem occurs when the Fisher–Yates shuffle is used with a pseudorandom number generator or PRNG: as the sequence of numbers output by such a generator is entirely determined by its internal state at the start of a sequence, a shuffle driven by such a generator cannot possibly produce more distinct permutations than the generator has distinct possible states. Even when the number of possible states exceeds the number of permutations, the irregular nature of the mapping from sequences of numbers to permutations means that some permutations will occur more often than others. Thus, to minimize bias, the number of states of the PRNG should exceed the number of permutations by at least several orders of magnitude.\n\nFor example, the built-in pseudorandom number generator provided by many programming languages and/or libraries may often have only 32 bits of internal state, which means it can only produce 2 different sequences of numbers. If such a generator is used to shuffle a deck of 52 playing cards, it can only ever produce a very small fraction of the 52! ≈ 2 possible permutations. It is impossible for a generator with less than 226 bits of internal state to produce all the possible permutations of a 52-card deck.\n\nNo pseudorandom number generator can produce more distinct sequences, starting from the point of initialization, than there are distinct seed values it may be initialized with. Thus, a generator that has 1024 bits of internal state but which is initialized with a 32-bit seed can still only produce 2 different permutations right after initialization. It can produce more permutations if one exercises the generator a great many times before starting to use it for generating permutations, but this is a very inefficient way of increasing randomness: supposing one can arrange to use the generator a random number of up to a billion, say 2 for simplicity, times between initialization and generating permutations, then the number of possible permutations is still only 2.\n\nA further problem occurs when a simple linear congruential PRNG is used with the divide-and-take-remainder method of range reduction described above. The problem here is that the low-order bits of a linear congruential PRNG with modulo 2 are less random than the high-order ones : the low \"n\" bits of the generator themselves have a period of at most 2. When the divisor is a power of two, taking the remainder essentially means throwing away the high-order bits, such that one ends up with a significantly less random value. Different rules apply if the LCG has prime modulo, but such generators are uncommon. This is an example of the general rule that a poor-quality RNG or PRNG will produce poor-quality shuffles.\n\n\n"}
{"id": "8370929", "url": "https://en.wikipedia.org/wiki?curid=8370929", "title": "Fitness model (network theory)", "text": "Fitness model (network theory)\n\nIn complex network theory, the fitness model is a model of the evolution of a network: how the links between nodes change over time depends on the fitness of nodes. Fitter nodes attract more links at the expense of less fit nodes.\n\nIt has been used to model the network structure of the World Wide Web.\n\nThe model is based on the idea of fitness, an inherent competitive factor that nodes may have, capable of affecting the network's evolution. According to this idea, the nodes' intrinsic ability to attract links in the network varies from node to node, the most efficient (or \"fit\") being able to gather more edges in the expense of others. In that sense, not all nodes are identical to each other, and they claim their degree increase according to the fitness they possess every time. The fitness factors of all the nodes composing the network may form a distribution ρ(η) characteristic of the system been studied.\n\nBianconi and Barabási proposed a new model called Bianconi-Barabasi model, a variant to the Barabási-Albert model (BA model), where the probability for a node to connect to another one is supplied with a term expressing the fitness of the node involved. The fitness parameter is time independent and is multiplicative to the probability\n\nThus, the system of equations for the time evolution of the degrees formula_2 according to the continuum theory introduced by the same model will have the form\n\nwhere \"m\" the number of edges the newly coming node has. If we require the solution to have a similar form to the one it had without the insertion of the fitness factors (to avoid ruining the power-law degree distribution of scale-free networks), then the exponent of the solution has to change and become fitness dependent\n\nwhere\n\nHence, the more fit nodes increase their degree faster than the less ones. This characteristic attributes the network with a different behavior regarding its evolution. Without the introduction of the fitness property, all nodes had the same exponent in the power-law degree evolution formula. This means that the older nodes in the system would have more edges compared to newcoming ones. After the fitness property is introduced, this exponent, and accordingly, the slope of formula_6 change, giving thus the opportunity to newcoming nodes to dominate the system.\n\nIt was seen through this example how can a network's evolution change behavior through the introduction of a new parameter in the model. However, we require the network to preserve its overall scale-free character. By forcing the fitness dependence to be accumulated in the exponent only, the degree-distribution will still be a power-law relationship, composed though by a weighted sum of different power-law in degree-evolution formulas\n\nwhere formula_8 is the fitness distribution depending on the system's composition\n\nThe fitness model can be extended to in corporate additional processes, such as internal edges, which affect the exponents.\n\nAnother model where the fitness is not coupled to preferential attachment has been introduced by Caldarelli et al. Here a link is created between two vertices formula_9 with a probability given by a linking function formula_10 of the fitnesses of the vertices involved.\nThe degree of a vertex i is given by \n\nIf formula_12 is an invertible and increasing function of formula_13, then\nthe probability distribution formula_14 is given by\n\nAs a result if the fitnesses formula_16 are distributed as a power law, then also the node degree does.\n\nLess intuitively with a fast decaying probability distribution as\nformula_17 together with a linking function of the kind\n\nwith formula_19 a constant and formula_20 the Heavyside function, we also obtain\nscale-free networks.\n\nSuch model has been successfully applied to describe trade between nations by using GDP as fitness for the various nodes formula_9 and a linking function of the kind\n\nThe fitness model has been used to model the network structure of the World Wide Web. In a PNAS article, Kong et al. extended the fitness model to include random node deletion, a common phenomena in the Web. When the deletion rate of the web pages are accounted for, they found that the overall fitness distribution is exponential. Nonetheless, even this small variance in the fitness is amplified through the preferential attachment mechanism, leading to a heavy-tailed distribution of incoming links on the Web.\n"}
{"id": "11795634", "url": "https://en.wikipedia.org/wiki?curid=11795634", "title": "Fourier algebra", "text": "Fourier algebra\n\nFourier and related algebras occur naturally in the harmonic analysis of locally compact groups. They play an important role in the duality theories of these groups. The Fourier–Stieltjes algebra and the Fourier–Stieltjes transform on the Fourier algebra of a locally compact group were introduced by Pierre Eymard in 1964.\n\nLet G be a locally compact abelian group, and Ĝ the dual group of G. Then the Fourier transform of functions in formula_1, the group algebra of formula_2, is a sub-algebra A(G) of CB(G), the space of bounded continuous complex-valued functions on G with pointwise multiplication called the Fourier algebra of\nG, and the Fourier-Stieltjes transform of measures in formula_3, the measure algebra of formula_2, also a subalgebra of CB(G), called the Fourier-Stieltjes algebra of G.\n\nLet formula_5 be a Fourier–Stieltjes algebra and formula_6 be a Fourier algebra such that the locally compact group formula_7 is abelian. Let formula_3 be the measure algebra of finite measures on formula_9 and let formula_1 be the convolution algebra of integrable functions on formula_9, where formula_12 is the character group of the Abelian group formula_7.\n\nThe Fourier–Stieltjes transform of a finite measure formula_14 on formula_12 is the function formula_16 on formula_7 defined by\n\nThe space formula_5 of these functions is an algebra under pointwise multiplication is isomorphic to the measure algebra formula_3. Restricted to formula_1, viewed as a subspace of formula_3, the Fourier–Stieltjes transform is the Fourier transform on formula_1 and its image is, by definition, the Fourier algebra formula_6. The generalized Bochner theorem states that a measurable function on formula_7 is equal, almost everywhere, to the Fourier–Stieltjes transform of a non-negative finite measure on formula_9 if and only if it is positive definite. Thus, formula_5 can be defined as the linear span of the set of continuous positive-definite functions on formula_7. This definition is still valid when formula_7 is not Abelian.\n\nLet A(G) be the Fourier algebra of a compact group G. Building upon the work of Wiener, Lévy, Gelfand, and Beurling, in 1959 Helson, Kahane, Katznelson, and Rudin proved that, when G is compact and abelian, a function f defined on a closed convex subset of the plane operates in A(G) if and only if f is real analytic. In 1969 Dunkl proved the result holds when G is compact and contains an infinite abelian subgroup.\n\n"}
{"id": "4767368", "url": "https://en.wikipedia.org/wiki?curid=4767368", "title": "Freivalds' algorithm", "text": "Freivalds' algorithm\n\nFreivalds' algorithm (named after Rūsiņš Mārtiņš Freivalds) is a probabilistic randomized algorithm used to verify matrix multiplication. Given three \"n\" × \"n\" matrices formula_1, formula_2, and formula_3, a general problem is to verify whether formula_4. A naïve algorithm would compute the product formula_5 explicitly and compare term by term whether this product equals formula_3. However, the best known matrix multiplication algorithm runs in formula_7 time. Freivalds' algorithm utilizes randomization in order to reduce this time bound to formula_8\n\nwith high probability. In formula_9 time the algorithm can verify a matrix product with probability of failure less than formula_10.\n\nThree \"n\" × \"n\" matrices formula_1, formula_2, and formula_3.\n\nYes, if formula_4; No, otherwise.\n\n\nIf formula_4, then the algorithm always returns \"Yes\". If formula_19, then the probability that the algorithm returns \"Yes\" is less than or equal to one half. This is called one-sided error.\n\nBy iterating the algorithm \"k\" times and returning \"Yes\" only if all iterations yield \"Yes\", a runtime of formula_9 and error probability of formula_21 is achieved.\n\nSuppose one wished to determine whether:\nA random two-element vector with entries equal to 0 or 1 is selected — say formula_23 — and used to compute:\nThis yields the zero vector, suggesting the possibility that AB = C. However, if in a second trial the vector formula_25 is selected, the result becomes:\nThe result is nonzero, proving that in fact AB ≠ C.\n\nThere are four two-element 0/1 vectors, and half of them give the zero vector in this case (formula_27 and formula_23), so the chance of randomly selecting these in two trials (and falsely concluding that AB=C) is 1/2 or 1/4. In the general case, the proportion of \"r\" yielding the zero vector may be less than 1/2, and a larger number of trials (such as 20) would be used, rendering the probability of error very small.\n\nLet \"p\" equal the probability of error. We claim that if \"A\" × \"B\" = \"C\", then \"p\" = 0, and if \"A\" × \"B\" ≠ \"C\", then \"p\" ≤ 1/2.\n\nThis is regardless of the value of formula_15, since it uses only that formula_31. Hence the probability for error in this case is:\n\nLet formula_33 such that\n\nWhere\n\nSince formula_19, we have that some element of formula_33 is nonzero. Suppose that the element formula_38. By the definition of matrix multiplication, we have:\n\nFor some constant formula_40.\nUsing Bayes' Theorem, we can partition over formula_40:\n\nWe use that:\n\nPlugging these in the equation (), we get:\n\nTherefore, \nThis completes the proof.\n\nSimple algorithmic analysis shows that the running time of this algorithm is O(\"n\"), beating the classical deterministic algorithm's bound of O(\"n\"). The error analysis also shows that if we run our algorithm \"k\" times, we can achieve an error bound of less than formula_46, an exponentially small quantity. The algorithm is also fast in practice due to wide availability of fast implementations for matrix-vector products. Therefore, utilization of randomized algorithms can speed up a very slow deterministic algorithm. In fact, the best known deterministic matrix multiplication algorithm known at the current time is a variant of the Coppersmith–Winograd algorithm with an asymptotic running time of O(\"n\").\n\nFreivalds' algorithm frequently arises in introductions to probabilistic algorithms due to its simplicity and how it illustrates the superiority of probabilistic algorithms in practice for some problems.\n\n\n"}
{"id": "3992708", "url": "https://en.wikipedia.org/wiki?curid=3992708", "title": "G. B. Halsted", "text": "G. B. Halsted\n\nGeorge Bruce Halsted (November 25, 1853 – March 16, 1922), usually cited as G. B. Halsted, was an American mathematician who explored foundations of geometry and introduced non-Euclidean geometry into the United States through his own work and his many important translations. Especially noteworthy were his translations and commentaries relating to non-Euclidean geometry, including works by Bolyai, Lobachevski, Saccheri, and Poincaré. He wrote an elementary geometry text, \"Rational Geometry\", based on Hilbert's axioms, which was translated into French, German, and Japanese.\n\nHalsted was a tutor and instructor at Princeton University. He held a mathematical fellowship while a student at Princeton. Halsted was a fourth generation Princeton graduate, earning his Bachelor's degree in 1875 and his Master's in 1878. He went on to Johns Hopkins University where he was J. J. Sylvester's first student, receiving his Ph.D. in 1879. After graduation, Halsted served as an instructor in mathematics at Princeton until beginning his post at the University of Texas at Austin in 1884.\n\nFrom 1884 to 1903, Halsted was a member of the University of Texas at Austin Department of Pure and Applied Mathematics, eventually becoming its chair. He taught mathematicians R. L. Moore and L. E. Dickson, among other students, who frequently joked that his primary criterion for the rationality of a geometric system was the simplicity of the terms in which it could express the closed space figure formed by the contours of his mustache. He explored the foundations of geometry and explored many alternatives to Euclid's development, culminating with his \"Rational Geometry\". \nIn the interest of hyperbolic geometry in 1891 he translated the work of Nicolai Lobachevsky on theory of parallels. In 1893 in Chicago, Halsted read a paper \"Some salient points in the history of non-Euclidean and hyper-spaces\" at the International Mathematical Congress held in connection with the World's Columbian Exposition. Halsted frequently contributed to the early American Mathematical Monthly. In one article he championed the role of J. Bolyai in the development of non-Euclidean geometry and criticized C. F. Gauss. See also on 3 September 1912.\n\nIn 1903, Halsted was fired from UT Austin after having published several articles that criticized the university for having passed over R. L. Moore, at that time a young and promising mathematician whom Halsted hoped to have as an assistant, for an instructor post in favor of a well-connected but less qualified candidate with roots in the area. \nHe completed his teaching career at St. John's College, Annapolis; Kenyon College, Gambier, Ohio (1903-1906); and the Colorado State Teachers College, Greeley (1906-1914).\nHalsted was a member of the American Mathematical Society and served as vice president of the American Association for the Advancement of Science. He was elected Fellow of the Royal Astronomical Society in 1905.\n\n\n\n\n"}
{"id": "1360654", "url": "https://en.wikipedia.org/wiki?curid=1360654", "title": "Gauss–Kuzmin–Wirsing operator", "text": "Gauss–Kuzmin–Wirsing operator\n\nIn mathematics, the Gauss–Kuzmin–Wirsing operator is the transfer operator of the Gauss map. \n\nIt is named after:\nIt occurs in the study of continued fractions; it is also related to the Riemann zeta function.\n\nThe Gauss function (map) h is : \n\nwhere:\n\nIt has an infinite number of jump discontinuities at x = 1/n, for positive integers n. It is hard to approximate it by a single smooth polynomial.\n\nThe Gauss–Kuzmin–Wirsing operator formula_3 acts on functions formula_4 as\n\nThe first eigenfunction of this operator is\n\nwhich corresponds to an eigenvalue of \"λ\"=1. This eigenfunction gives the probability of the occurrence of a given integer in a continued fraction expansion, and is known as the Gauss–Kuzmin distribution. This follows in part because the Gauss map acts as a truncating shift operator for the continued fractions: if\n\nis the continued fraction representation of a number 0 < \"x\" < 1, then\n\nAdditional eigenvalues can be computed numerically; the next eigenvalue is \"λ\" = −0.3036630029... \nand its absolute value is known as the Gauss–Kuzmin–Wirsing constant. Analytic forms for additional eigenfunctions are not known. It is not known if the eigenvalues are irrational.\n\nLet us arrange the eigenvalues of the Gauss–Kuzmin–Wirsing operator according to an absolute value:\n\nIt was conjectured in 1995 by Philippe Flajolet and Brigitte Vallée that\n\nIn 2014, Giedrius Alkauskas proved this conjecture. Moreover, the following asymptotic result holds:\n\nhere the function formula_12 is bounded, and formula_13 is the Riemann zeta function.\n\nThe eigenvalues form a discrete spectrum, when the operator is limited to act on functions on the unit interval of the real number line. More broadly, since the Gauss map is the shift operator on Baire space formula_14, the GKW operator can also be viewed as an operator on the function space formula_15 (considered as a Banach space, with basis functions taken to be the indicator functions on the cylinders of the product topology). In the later case, it has a continuous spectrum, with eigenvalues in the unit disk formula_16 of the complex plane. That is, given the cylinder formula_17, the operator G shifts it to the left: formula_18. Taking formula_19 to be the indicator function which is 1 on the cylinder (when formula_20), and zero otherwise, one has that formula_21. The series\n\nthen is an eigenfunction with eigenvalue formula_23. That is, one has formula_24 whenever the summation converges: that is, when formula_16.\n\nA special case arises when one wishes to consider the Haar measure of the shift operator, that is, a function that is invariant under shifts. This is given by the Minkowski measure formula_26. That is, one has that formula_27.\n\nThe GKW operator is related to the Riemann zeta function. Note that the zeta function can be written as\n\nwhich implies that\n\nby change-of-variable.\n\nConsider the Taylor series expansions at x=1 for a function \"f\"(\"x\") and formula_30. That is, let\n\nand write likewise for \"g\"(\"x\"). The expansion is made about \"x\" = 1 because the GKW operator is poorly behaved at \"x\" = 0. The expansion is made about 1-x so that we can keep \"x\" a positive number, 0 ≤ \"x\" ≤ 1. Then the GKW operator acts on the Taylor coefficients as\n\nwhere the matrix elements of the GKW operator are given by\n\nThis operator is extremely well formed, and thus very numerically tractable. The Gauss–Kuzmin constant is easily computed to high precision by numerically diagonalizing the upper-left \"n\" by \"n\" portion. There is no known closed-form expression that diagonalizes this operator; that is, there are no closed-form expressions known for the eigenvectors.\n\nThe Riemann zeta can be written as\n\nwhere the formula_35 are given by the matrix elements above:\n\nPerforming the summations, one gets:\n\nwhere formula_38 is the Euler–Mascheroni constant. These formula_35 play the analog of the Stieltjes constants, but for the falling factorial expansion. By writing\n\none gets: \"a\" = −0.0772156... and \"a\" = −0.00474863... and so on. The values get small quickly but are oscillatory. Some explicit sums on these values can be performed. They can be explicitly related to the Stieltjes constants by re-expressing the falling factorial as a polynomial with Stirling number coefficients, and then solving. More generally, the Riemann zeta can be re-expressed as an expansion in terms of Sheffer sequences of polynomials.\n\nThis expansion of the Riemann zeta is investigated in the following references. The coefficients are decreasing as\n\n\n\n"}
{"id": "21256722", "url": "https://en.wikipedia.org/wiki?curid=21256722", "title": "Gordon–Newell theorem", "text": "Gordon–Newell theorem\n\nIn queueing theory, a discipline within the mathematical theory of probability, the Gordon–Newell theorem is an extension of Jackson's theorem from open queueing networks to closed queueing networks of exponential servers where customers cannot leave the network. Jackson's theorem cannot be applied to closed networks because the queue length at a node in the closed network is limited by the population of the network. The Gordon–Newell theorem calculates the open network solution and then eliminates the infeasible states by renormalizing the probabilities. Calculation of the normalizing constant makes the treatment more awkward as the whole state space must be enumerated. Buzen's algorithm or mean value analysis can be used to calculate the normalizing constant more efficiently.\n\nA network of \"m\" interconnected queues is known as a Gordon–Newell network or closed Jackson network if it meets the following conditions:\n\n\nIn a closed Gordon–Newell network of \"m\" queues, with a total population of \"K\" individuals, write formula_4 (where \"k\" is the length of queue \"i\") for the state of the network and \"S\"(\"K\", \"m\") for the state space\n\nThen the equilibrium state probability distribution exists and is given by\n\nwhere service times at queue \"i\" are exponentially distributed with parameter \"μ\". The normalizing constant \"G\"(\"K\") is given by\n\nand \"e\" is the visit ratio, calculated by solving the simultaneous equations\n\n"}
{"id": "7999626", "url": "https://en.wikipedia.org/wiki?curid=7999626", "title": "Gårding's inequality", "text": "Gårding's inequality\n\nIn mathematics, Gårding's inequality is a result that gives a lower bound for the bilinear form induced by a real linear elliptic partial differential operator. The inequality is named after Lars Gårding.\n\nLet Ω be a bounded, open domain in \"n\"-dimensional Euclidean space and let \"H\"(Ω) denote the Sobolev space of \"k\"-times weakly differentiable functions \"u\" : Ω → R with weak derivatives in \"L\". Assume that Ω satisfies the \"k\"-extension property, i.e., that there exists a bounded linear operator \"E\" : \"H\"(Ω) → \"H\"(R) such that (\"Eu\")| = \"u\" for all \"u\" in \"H\"(Ω).\n\nLet \"L\" be a linear partial differential operator of even order \"2k\", written in divergence form\n\nand suppose that \"L\" is uniformly elliptic, i.e., there exists a constant \"θ\" > 0 such that\n\nFinally, suppose that the coefficients \"A\" are bounded, continuous functions on the closure of Ω for |\"α\"| = |\"β\"| = \"k\" and that\n\nThen Gårding's inequality holds: there exist constants \"C\" > 0 and \"G\" ≥ 0\n\nwhere\n\nis the bilinear form associated to the operator \"L\".\n\nBe careful, in this application, Garding's Inequality seems useless here as the final result is a direct consequence of Poincaré's Inequality, or Friedrich Inequality. (See talk on the article). \n\nAs a simple example, consider the Laplace operator Δ. More specifically, suppose that one wishes to solve, for \"f\" ∈ \"L\"(Ω) the Poisson equation\n\nwhere Ω is a bounded Lipschitz domain in R. The corresponding weak form of the problem is to find \"u\" in the Sobolev space \"H\"(Ω) such that\n\nwhere\n\nThe Lax–Milgram lemma ensures that if the bilinear form \"B\" is both continuous and elliptic with respect to the norm on \"H\"(Ω), then, for each \"f\" ∈ \"L\"(Ω), a unique solution \"u\" must exist in \"H\"(Ω). The hypotheses of Gårding's inequality are easy to verify for the Laplace operator Δ, so there exist constants \"C\" and \"G\" ≥ 0\n\nApplying the Poincaré inequality allows the two terms on the right-hand side to be combined, yielding a new constant \"K\" > 0 with\n\nwhich is precisely the statement that \"B\" is elliptic. The continuity of \"B\" is even easier to see: simply apply the Cauchy–Schwarz inequality and the fact that the Sobolev norm is controlled by the \"L\" norm of the gradient.\n\n"}
{"id": "22154619", "url": "https://en.wikipedia.org/wiki?curid=22154619", "title": "Hilbert metric", "text": "Hilbert metric\n\nIn mathematics, the Hilbert metric, also known as the Hilbert projective metric, is an explicitly defined distance function on a bounded convex subset of the \"n\"-dimensional Euclidean space R. It was introduced by as a generalization of the Cayley's formula for the distance in the Cayley–Klein model of hyperbolic geometry, where the convex set is the \"n\"-dimensional open unit ball. Hilbert's metric has been applied to Perron–Frobenius theory and to constructing Gromov hyperbolic spaces.\n\nLet Ω be a convex open domain in a Euclidean space that does not contain a line. Given two distinct points \"A\" and \"B\" of Ω, let \"X\" and \"Y\" be the points at which the straight line \"AB\" intersects the boundary of Ω, where the order of the points is \"X\", \"A\", \"B\", \"Y\". Then the Hilbert distance \"d\"(\"A\", \"B\") is the logarithm of the cross-ratio of this quadruple of points:\n\nThe function \"d\" is extended to all pairs of points by letting \"d\"(\"A\", \"A\") = 0 and defines a metric on Ω. If one of the points \"A\" and \"B\" lies on the boundary of Ω then \"d\" can be formally defined to be +∞, corresponding to a limiting case of the above formula \nwhen one of the denominators is zero.\nHilbert balls in convex polygonal domains have varying combinatorial complexity.\n\nA variant of this construction arises for a closed convex cone \"K\" in a Banach space \"V\" (possibly, infinite-dimensional). In addition, the cone \"K\" is assumed to be \"pointed\", i.e. \"K\" ∩ (−\"K\") = {0} and thus \"K\" determines a partial order formula_2 on \"V\". Given any vectors \"v\" and \"w\" in \"K\" \\ {0}, one first defines\n\nThe / notation is to emphasize that : formula_4.\n\nThe Hilbert pseudometric on \"K\" \\ {0} is then defined by the formula\n\nIt is invariant under the rescaling of \"v\" and \"w\" by positive constants and so descends to a metric on the space of rays of \"K\", which is interpreted as the projectivization of \"K\" (in order for \"d\" to be finite, one needs to restrict to the interior of \"K\"). Moreover, if \"K\" ⊂ R × \"V\" is the cone over a convex set Ω,\n\nthen the space of rays of \"K\" is canonically isomorphic to Ω. If \"v\" and \"w\" are vectors in rays in \"K\" corresponding to the points \"A\", \"B\" ∈ Ω then these two formulas for \"d\" yield the same value of the distance.\n\n\n\n"}
{"id": "846733", "url": "https://en.wikipedia.org/wiki?curid=846733", "title": "Hundredth", "text": "Hundredth\n\nIn arithmetic, a hundredth is a single part of something that has been divided equally into a hundred parts. For example, a hundredth of 675 is 6.75. In this manner it is used with the prefix \"centi\" such as in centimeter.\n\nA hundredth is the reciprocal of 100.\n\nA hundredth is written as a decimal fraction as 0.01, and as a vulgar fraction as 1/100.\n\n“Hundredth” is also the ordinal number that follows “ninety-ninth” and precedes “hundred and first.” It is written as 100th.\n\n"}
{"id": "12144610", "url": "https://en.wikipedia.org/wiki?curid=12144610", "title": "Increment theorem", "text": "Increment theorem\n\nIn non-standard analysis, a field of mathematics, the increment theorem states the following: Suppose a function \"y\" = \"f\"(\"x\") is differentiable at \"x\" and that Δ\"x\" is infinitesimal. Then\n\nfor some infinitesimal ε, where\n\nIf formula_3 then we may write\n\nwhich implies that formula_5, or in other words that formula_6 is infinitely close to formula_7, or formula_7 is the standard part of formula_6.\n\nA similar theorem exists in standard Calculus. Again assume that \"y\" = \"f\"(\"x\") is differentiable, but now let Δ\"x\" be a nonzero standard real number. Then the same equation\nholds with the same definition of Δ\"y\", but instead of ε being infinitesimal, we have\n(treating \"x\" and \"f\" as given so that ε is a function of Δ\"x\" alone).\n\n\n"}
{"id": "15466849", "url": "https://en.wikipedia.org/wiki?curid=15466849", "title": "Interdisciplinary Contest in Modeling", "text": "Interdisciplinary Contest in Modeling\n\nThe Interdisciplinary Contest in Modeling is a multi-day mathematics competition held annually by COMAP and sponsored by SIAM, the NSA, and INFORMS. It is distinguished from other major mathematical competitions such as Putnam by its strong focus on research, originality, teamwork, communication and justification of results.\n\nAround 1500 international teams of three undergraduates compete to produce original mathematical papers in response to an interdisciplinary modeling problem. Once the problem is posted, teams are given 96 hours (usually Thursday to Monday) to research and submit solutions.\n\n\n"}
{"id": "239290", "url": "https://en.wikipedia.org/wiki?curid=239290", "title": "John Wallis", "text": "John Wallis\n\nJohn Wallis (; 3 December 1616 – 8 November 1703) was an English clergyman and mathematician who is given partial credit for the development of infinitesimal calculus. Between 1643 and 1689 he served as chief cryptographer for Parliament and, later, the royal court. He is credited with introducing the symbol ∞ to represent the concept of infinity. He similarly used 1/∞ for an infinitesimal.\n\nJohn Wallis was born in Ashford, Kent, the third of five children of Reverend John Wallis and Joanna Chapman. He was initially educated at a school in Ashford but moved to James Movat's school in Tenterden in 1625 following an outbreak of plague. Wallis was first exposed to mathematics in 1631, at Martin Holbeach's school in Felsted; he enjoyed maths, but his study was erratic, since \"mathematics, at that time with us, were scarce looked on as academical studies, but rather mechanical\" (Scriba 1970).\n\nAs it was intended he should be a doctor, he was sent in 1632 to Emmanuel College, Cambridge. While there, he kept an \"act\" on the doctrine of the circulation of the blood; that was said to have been the first occasion in Europe on which this theory was publicly maintained in a disputation. His interests, however, centred on mathematics. He received his Bachelor of Arts degree in 1637 and a Master's in 1640, afterwards entering the priesthood. From 1643 to 1649, he served as a nonvoting scribe at the Westminster Assembly. He was elected to a fellowship at Queens' College, Cambridge in 1644, from which he had to resign following his marriage.\n\nThroughout this time, Wallis had been close to the Parliamentarian party, perhaps as a result of his exposure to Holbeach at Felsted School. He rendered them great practical assistance in deciphering Royalist dispatches. The quality of cryptography at that time was mixed; despite the individual successes of mathematicians such as François Viète, the principles underlying cipher design and analysis were very poorly understood. Most ciphers were ad hoc methods relying on a secret algorithm, as opposed to systems based on a variable key. Wallis realised that the latter were far more secure – even describing them as \"unbreakable\", though he was not confident enough in this assertion to encourage revealing cryptographic algorithms. He was also concerned about the use of ciphers by foreign powers, refusing, for example, Gottfried Leibniz's request of 1697 to teach Hanoverian students about cryptography.\n\nReturning to London – he had been made chaplain at St Gabriel Fenchurch in 1643 – Wallis joined the group of scientists that was later to evolve into the Royal Society. He was finally able to indulge his mathematical interests, mastering William Oughtred's \"Clavis Mathematicae\" in a few weeks in 1647. He soon began to write his own treatises, dealing with a wide range of topics, which he continued for the rest of his life.\n\nWallis joined the moderate Presbyterians in signing the remonstrance against the execution of Charles I, by which he incurred the lasting hostility of the Independents. In spite of their opposition he was appointed in 1649 to the Savilian Chair of Geometry at Oxford University, where he lived until his death on 28 October 1703 (O.S.). In 1661, he was one of twelve Presbyterian representatives at the Savoy Conference.\n\nBesides his mathematical works he wrote on theology, logic, English grammar and philosophy, and he was involved in devising a system for teaching a deaf boy to speak at Littlecote House. William Holder had earlier taught a deaf man, Alexander Popham, to speak \"plainly and distinctly, and with a good and graceful tone\". Wallis later claimed credit for this, leading Holder to accuse Wallis of \"rifling his Neighbours, and adorning himself with their spoyls\".\n\nWallis made significant contributions to trigonometry, calculus, geometry, and the analysis of infinite series. In his \"Opera Mathematica\" I (1695) he introduced the term \"continued fraction\".\n\nWallis rejected as absurd the now usual idea of a negative number as being less than nothing, but accepted the view that it is something greater than infinity. (The argument that negative numbers are greater than infinity involves the quotient formula_1 and considering what happens as \"x\" approaches and then crosses the point \"x\" = 0 from the positive side.) Despite this he is generally credited as the originator of the idea of the number line, in which numbers are represented geometrically in a line with the negative numbers represented by lengths opposite in direction to lengths of positive numbers.\n\nIn 1655, Wallis published a treatise on conic sections in which they were defined analytically. This was the earliest book in which these curves are considered and defined as curves of the second degree. It helped to remove some of the perceived difficulty and obscurity of René Descartes' work on analytic geometry.\nIn the \"Treatise on the Conic Sections\" Wallis popularised the symbol ∞ for infinity. He wrote, “I suppose any plane (following the \"Geometry of Indivisibles\" of Cavalieri) to be made up of an infinite number of parallel lines, or as I would prefer, of an infinite number of parallelograms of the same altitude; (let the altitude of each one of these be an infinitely small part 1/∞ of the whole altitude, and let the symbol ∞ denote Infinity) and the altitude of all to make up the altitude of the figure.”\n\n\"Arithmetica Infinitorum\", the most important of Wallis's works, was published in 1656. In this treatise the methods of analysis of Descartes and Cavalieri were systematised and extended, but some ideas were open to criticism. He began, after a short tract on conic sections, by developing the standard notation for powers, extending them from positive integers to rational numbers:\n\nLeaving the numerous algebraic applications of this discovery, he next proceeded to find, by integration, the area enclosed between the curve \"y\" = \"x\", the axis of \"x\", and any ordinate \"x\" = \"h\", and he proved that the ratio of this area to that of the parallelogram on the same base and of the same height is 1/(\"m\" + 1), extending Cavalieri's quadrature formula. He apparently assumed that the same result would be true also for the curve \"y\" = \"ax\", where \"a\" is any constant, and \"m\" any number positive or negative, but he discussed only the case of the parabola in which \"m\" = 2 and the hyperbola in which \"m\" = −1. In the latter case, his interpretation of the result is incorrect. He then showed that similar results may be written down for any curve of the form\n\nand hence that, if the ordinate \"y\" of a curve can be expanded in powers of \"x\", its area can be determined: thus he says that if the equation of the curve is \"y\" = \"x\" + \"x\" + \"x\" + ..., its area would be \"x\" + x/2 + \"x\"/3 + ... He then applied this to the quadrature of the curves \"y\" = (\"x\" − \"x\"), \"y\" = (\"x\" − \"x\"), \"y\" = (\"x\" − \"x\"), etc., taken between the limits \"x\" = 0 and \"x\" = 1. He shows that the areas are, respectively, 1, 1/6, 1/30, 1/140, etc. He next considered curves of the form \"y\" = \"x\" and established the theorem that the area bounded by this curve and the lines \"x\" = 0 and \"x\" = 1 is equal to the area of the rectangle on the same base and of the same altitude as \"m\" : \"m\" + 1. This is equivalent to computing\n\nHe illustrated this by the parabola, in which case \"m\" = 2. He stated, but did not prove, the corresponding result for a curve of the form \"y\" = \"x\".\n\nWallis showed considerable ingenuity in reducing the equations of curves to the forms given above, but, as he was unacquainted with the binomial theorem, he could not effect the quadrature of the circle, whose equation is formula_11, since he was unable to expand this in powers of \"x\". He laid down, however, the principle of interpolation. Thus, as the ordinate of the circle formula_11 is the geometrical mean of the ordinates of the curves formula_13 and formula_14, it might be supposed that, as an approximation, the area of the semicircle formula_15 which is formula_16 might be taken as the geometrical mean of the values of\n\nthat is, 1 and formula_18; this is equivalent to taking formula_19 or 3.26... as the value of π. But, Wallis argued, we have in fact a series formula_20... and therefore the term interpolated between 1 and formula_21 ought to be chosen so as to obey the law of this series. This, by an elaborate method that is not described here in detail, leads to a value for the interpolated term which is equivalent to taking\n(which is now known as the Wallis product).\n\nIn this work also the formation and properties of continued fractions are discussed, the subject having been brought into prominence by Brouncker's use of these fractions.\n\nA few years later, in 1659, Wallis published a tract containing the solution of the problems on the cycloid which had been proposed by Blaise Pascal. In this he incidentally explained how the principles laid down in his \"Arithmetica Infinitorum\" could be used for the rectification of algebraic curves and gave a solution of the problem to rectify (i.e., find the length of) the semicubical parabola \"x\" = \"ay\", which had been discovered in 1657 by his pupil William Neile. Since all attempts to rectify the ellipse and hyperbola had been (necessarily) ineffectual, it had been supposed that no curves could be rectified, as indeed Descartes had definitely asserted to be the case. The logarithmic spiral had been rectified by Evangelista Torricelli and was the first curved line (other than the circle) whose length was determined, but the extension by Neile and Wallis to an algebraic curve was novel. The cycloid was the next curve rectified; this was done by Christopher Wren in 1658.\n\nEarly in 1658 a similar discovery, independent of that of Neile, was made by van Heuraët, and this was published by van Schooten in his edition of Descartes's \"Geometria\" in 1659. Van Heuraët's method is as follows. He supposes the curve to be referred to rectangular axes; if this be so, and if (\"x\", \"y\") be the coordinates of any point on it, and \"n\" be the length of the normal, and if another point whose coordinates are (\"x\", η) be taken such that η : \"h\" = \"n\" : \"y\", where \"h\" is a constant; then, if \"ds\" be the element of the length of the required curve, we have by similar triangles \"ds\" : \"dx\" = \"n\" : \"y\". Therefore, \"h ds\" = η \"dx\". Hence, if the area of the locus of the point (\"x\", η) can be found, the first curve can be rectified. In this way van Heuraët effected the rectification of the curve \"y\" = \"ax\" but added that the rectification of the parabola \"y\" = \"ax\" is impossible. since it requires the quadrature of the hyperbola. The solutions given by Neile and Wallis are somewhat similar to that given by van Heuraët, though no general rule is enunciated, and the analysis is clumsy. A third method was suggested by Fermat in 1660, but it is inelegant and laborious.\n\nThe theory of the collision of bodies was propounded by the Royal Society in 1668 for the consideration of mathematicians. Wallis, Christopher Wren, and Christian Huygens sent correct and similar solutions, all depending on what is now called the conservation of momentum; but, while Wren and Huygens confined their theory to perfectly elastic bodies (elastic collision), Wallis considered also imperfectly elastic bodies (inelastic collision). This was followed in 1669 by a work on statics (centres of gravity), and in 1670 by one on dynamics: these provide a convenient synopsis of what was then known on the subject.\n\nIn 1685 Wallis published \"Algebra\", preceded by a historical account of the development of the subject, which contains a great deal of valuable information. The second edition, issued in 1693 and forming the second volume of his \"Opera\", was considerably enlarged. This algebra is noteworthy as containing the first systematic use of formulae. A given magnitude is here represented by the numerical ratio which it bears to the unit of the same kind of magnitude: thus, when Wallis wants to compare two lengths he regards each as containing so many units of length. This perhaps will be made clearer by noting that the relation between the space described in any time by a particle moving with a uniform velocity is denoted by Wallis by the formula\n\nwhere \"s\" is the number representing the ratio of the space described to the unit of length; while the previous writers would have denoted the same relation by stating what is equivalent to the proposition\n\nHe is usually credited with the proof of the Pythagorean theorem using similar triangles. However, Thabit Ibn Qurra (AD 901), an Arab mathematician, had produced a generalisation of the Pythagorean theorem applicable to all triangles six centuries earlier. It is a reasonable conjecture that Wallis was aware of Thabit's work.\n\nWallis was also inspired by the works of Islamic mathematician Sadr al-Tusi, the son of Nasir al-Din al-Tusi, particularly by al-Tusi's book written in AD 1298 on the parallel postulate. The book was based on his father's thoughts which presented one of the earliest arguments for a non-Euclidean hypothesis equivalent to the parallel postulate. After reading this, Wallis then wrote about his ideas as he developed his own thoughts about the postulate, trying to prove it also with similar triangles.\n\nHe found that Euclid's fifth postulate is equivalent to the one currently named \"Wallis postulate\" after him. This postulate states that \"On a given finite straight line it is always possible to construct a triangle similar to a given triangle\". This result was encompassed in a trend trying to deduce Euclid's fifth from the other four postulates which today is known to be impossible. Unlike other authors, he realised that the unbounded growth of a triangle was not guaranteed by the four first postulates.\n\nAnother aspect of Wallis's mathematical skills was his ability to do mental calculations. He slept badly and often did mental calculations as he lay awake in his bed. One night he calculated in his head the square root of a number with 53 digits. In the morning he dictated the 27-digit square root of the number, still entirely from memory. It was a feat that was considered remarkable, and Henry Oldenburg, the Secretary of the Royal Society, sent a colleague to investigate how Wallis did it. It was considered important enough to merit discussion in the \"Philosophical Transactions\" of the Royal Society of 1685.\n\nA long-running debate between Wallis and Thomas Hobbes arose in the mid-1650s, when mathematicians criticised errors in the work \"De corpore\" by Hobbes. It continued into the 1670s, having gathered in the later claims of Hobbes on squaring the circle, and the wider beliefs on both sides.\n\nWallis translated into Latin works of Ptolemy, Bryennius, and Porphyrius's commentary on Ptolemy. He also published three letters to Henry Oldenburg concerning tuning. He approved of equal temperament that was being used in England's organs.\n\nHis \"Institutio logicae\", published in 1687, was very popular. The \"Grammatica linguae Anglicanae\" was a work on English grammar, that remained in print well into the eighteenth century. He also published on theology.\n\nOn 14 March 1645 he married Susanna Glynde (16?? – 16 March 1687), They had three children:\n\n\n\n"}
{"id": "890862", "url": "https://en.wikipedia.org/wiki?curid=890862", "title": "L-attributed grammar", "text": "L-attributed grammar\n\nL-attributed grammars are a special type of attribute grammars. They allow the attributes to be evaluated in one depth-first left-to-right traversal of the abstract syntax tree. As a result, attribute evaluation in L-attributed grammars can be incorporated conveniently in top-down parsing. \n\nA syntax-directed definition is L-attributed if each <ins>inherited</ins> attribute of formula_1 on the right side of formula_2 depends only on \n\n\nEvery S-attributed syntax-directed definition is also L-attributed.\n\nImplementing L-attributed definitions in Bottom-Up parsers requires rewriting L-attributed definitions into translation schemes.\n\nMany programming languages are L-attributed. Special types of compilers, the narrow compilers, are based on some form of L-attributed grammar. These are a strict superset of S-attributed grammars. Used for code synthesis.\n\nEither \"inherited attributes\" or \"synthesized attributes\" associated with the occurrence of symbol formula_5.\n"}
{"id": "40400729", "url": "https://en.wikipedia.org/wiki?curid=40400729", "title": "Limiting case (mathematics)", "text": "Limiting case (mathematics)\n\nIn mathematics, a limiting case of a mathematical object is a special case that arises when one or more components of the object take on their most extreme possible values. For example:\n\n\nA limiting case is sometimes a degenerate case in which some qualitative properties differ from the corresponding properties of the generic case. For example:\n\n"}
{"id": "8997770", "url": "https://en.wikipedia.org/wiki?curid=8997770", "title": "List of graphs", "text": "List of graphs\n\nThis partial list of graphs contains definitions of graphs and graph families which are known by particular names, but do not have a Wikipedia article of their own.\n\nFor collected definitions of graph theory terms that do not refer to individual graph types, such as \"vertex\" and \"path\", see Glossary of graph theory. For links to existing articles about particular kinds of graphs, see .\n\nA gear graph, denoted \"G\" is a graph obtained by inserting an extra vertex between each pair of adjacent vertices on the perimeter of a wheel graph \"W\". Thus, \"G\" has 2\"n\"+1 vertices and 3\"n\" edges. Gear graphs are examples of squaregraphs, and play a key role in the forbidden graph characterization of squaregraphs. Gear graphs are also known as cogwheels and bipartite wheels.\n\nA grid graph is a unit distance graph corresponding to the square lattice, so that it is isomorphic to the graph having a vertex corresponding to every pair of integers (\"a\", \"b\"), and an edge connecting (\"a\", \"b\") to (\"a\"+1, \"b\") and (\"a\", \"b\"+1). The finite grid graph \"G\" is an \"m\"×\"n\" rectangular graph isomorphic to the one obtained by restricting the ordered pairs to the range 0 ≤ \"a\" < \"m\", 0 ≤ \"b\" < \"n\". Grid graphs can be obtained as the Cartesian product of two paths: \"G\" = \"P\" × \"P\". Every grid graph is a median graph.\n\nA helm graph, denoted H is a graph obtained by attaching a single edge and node to each node of the outer circuit of a wheel graph W.\n\nA lobster graph is a tree in which all the vertices are within distance 2 of a central path. Compare \"caterpillar\".\n\nThe web graph \"W\" is a graph consisting of \"r\" concentric copies of the cycle graph \"C\", with corresponding vertices connected by \"spokes\". Thus \"W\" is the same graph as \"C\", and \"W\" is a prism.\n\nA web graph has also been defined as a prism graph \"Y\", with the edges of the outer cycle removed.\n\nGallery of named graphs\n"}
{"id": "9084906", "url": "https://en.wikipedia.org/wiki?curid=9084906", "title": "Lobachevsky Prize", "text": "Lobachevsky Prize\n\nThe Lobachevsky Prize, awarded by the Russian Academy of Sciences, and the Lobachevsky Medal, awarded by the Kazan State University, are mathematical awards in honor of Nikolai Ivanovich Lobachevsky.\n\nThe Lobachevsky Prize was established in 1896 by the Kazan Physical and Mathematical Society, in honor of the famous Russian mathematician Nikolai Ivanovich Lobachevsky, who had been a professor at Kazan University and spent almost all of his mathematical life there. The prize was first awarded in 1897. \nBetween the October revolution of 1917 and World War II the Lobachevsky Prize was awarded only twice, by the Kazan State University, in 1927 and 1937.\nIn 1947, by a decree of the Council of Ministers of the USSR, the jurisdiction over awarding the Lobachevsky Prize was transferred to the USSR Academy of Sciences. The 1947 decree specified that there be two prizes, awarded every five years: the main, international, Lobachevsky Prize, for which both Soviet and foreign scientists would be eligible, and an honorable mention prize, for Soviet mathematicians only.\nIn a 2003 article, B. N. Shapukov, a professor at the Kazan State University, writes that the 1947 decree also specified that awarding of the prize by the USSR Academy of Sciences should be done in consultation with the Kazan State University, but that this condition was not subsequently followed in practice.\n\nAnother decree of the Council of Ministers of the USSR, in 1956, specified that there be only one, international, Lobachevsky Prize, to be awarded every three years.\n\nWith the dissolution of the Soviet Union at the end of 1991, the Russian Academy of Sciences became the legatee of the USSR Academy of Sciences. The Russian Academy of Sciences continued awarding the Lobachevsky Prize, awarding it in 1992, 1996 and 2000. As of January 2010, the Lobachevsky Prize is listed among its awards at the Russian Academy of Sciences website.\n\nIn 1990-1991, while preparing the 1992 celebration of Lobachevsky's 200th anniversary, the Kazan State University organizers of this celebration lobbied the Soviet government to establish a special Kazan State University award in honor of Lobachevsky. A June 1991 decree of the Cabinet of Ministers of the USSR established the Lobachevsky Medal, for outstanding contributions to geometry, to be awarded by the Kazan State University. The Lobachevsky Medal was awarded by the university in 1992, 1997 and 2002. The article of Shapukov mentions that during the 1997 competition for the Lobachevsky Medal, the Mathematics section of the Russian Academy of Sciences complained about the fact and the process of awarding the Medal.\nThe Kazan State University website for the Lobachevsky Medal contains a list of recipients of the Lobachesky Prize from 1897 to 1989, which excludes the 1992, 1996 and 2000 Russian Academy of Sciences awards. The Russian Academy of Sciences website for the Lobachevsky Prize contains a list of recipients of the prize from 1897 to 2000 and does not mention Kazan State University's Lobachevsky Medal.\n\n\nIn 1906, Beppo Levi received an honorable mention. The prize itself was not awarded.\n\n\n\n\nIn 1997, Valery N. Berestovsky (Russia), Idjad Kh. Sabitov (Russia) and Boris Rosenfeld (USA) received an honorable mention.\n\n"}
{"id": "1033045", "url": "https://en.wikipedia.org/wiki?curid=1033045", "title": "Marcinkiewicz interpolation theorem", "text": "Marcinkiewicz interpolation theorem\n\nIn mathematics, the Marcinkiewicz interpolation theorem, discovered by , is a result bounding the norms of non-linear operators acting on \"L\" spaces.\n\nMarcinkiewicz' theorem is similar to the Riesz–Thorin theorem about linear operators, but also applies to non-linear operators.\n\nLet \"f\" be a measurable function with real or complex values, defined on a measure space (\"X\", \"F\", ω). The distribution function of \"f\" is defined by\n\nThen \"f\" is called weak formula_2 if there exists a constant \"C\" such that the distribution of \"f\" satisfies the following inequality for all \"t\" > 0:\n\nThe smallest constant \"C\" in the inequality above is called the weak formula_2 norm and is usually denoted by ||\"f\"|| or ||\"f\"||. Similarly the space is usually denoted by \"L\" or \"L\".\n\nAny formula_2 function belongs to \"L\" and in addition one has the inequality\n\nThis is nothing but Markov's inequality (aka Chebyshev's Inequality). The converse is not true. For example, the function 1/\"x\" belongs to \"L\" but not to \"L\".\n\nSimilarly, one may define the weak formula_10 space as the space of all functions \"f\" such that formula_11 belong to \"L\", and the weak formula_10 norm using\n\nMore directly, the \"L\" norm is defined as the best constant \"C\" in the inequality\n\nfor all \"t\" > 0.\n\nInformally, Marcinkiewicz's theorem is\n\nTheorem: \"Let T be a bounded linear operator from formula_10 to formula_16 and at the same time from formula_17 to formula_18. Then T is also a bounded operator from formula_19 to formula_19 for any r between p and q.\"\n\nIn other words, even if you only require weak boundedness on the extremes \"p\" and \"q\", you still get regular boundedness inside. To make this more formal, one has to explain that \"T\" is bounded only on a dense subset and can be completed. See Riesz-Thorin theorem for these details.\n\nWhere Marcinkiewicz's theorem is weaker than the Riesz-Thorin theorem is in the estimates of the norm. The theorem gives bounds for the formula_19 norm of \"T\" but this bound increases to infinity as \"r\" converges to either \"p\" or \"q\". Specifically , suppose that\nso that the operator norm of \"T\" from \"L\" to \"L\" is at most \"N\", and the operator norm of \"T\" from \"L\" to \"L\" is at most \"N\". Then the following interpolation inequality holds for all \"r\" between \"p\" and \"q\" and all \"f\" ∈ \"L\":\nwhere\nand\nThe constants δ and γ can also be given for \"q\" = ∞ by passing to the limit.\n\nA version of the theorem also holds more generally if \"T\" is only assumed to be a quasilinear operator in the following sense: there exists a constant \"C\" > 0 such that \"T\" satisfies\nfor almost every \"x\". The theorem holds precisely as stated, except with γ replaced by\n\nAn operator \"T\" (possibly quasilinear) satisfying an estimate of the form\nis said to be of weak type (\"p\",\"q\"). An operator is simply of type (\"p\",\"q\") if \"T\" is a bounded transformation from \"L\" to \"L\":\nA more general formulation of the interpolation theorem is as follows:\nThe latter formulation follows from the former through an application of Hölder's inequality and a duality argument.\n\nA famous application example is the Hilbert transform. Viewed as a multiplier, the Hilbert transform of a function \"f\" can be computed by first taking the Fourier transform of \"f\", then multiplying by the sign function, and finally applying the inverse Fourier transform.\n\nHence Parseval's theorem easily shows that the Hilbert transform is bounded from formula_32 to formula_32. A much less obvious fact is that it is bounded from formula_2 to formula_35. Hence Marcinkiewicz's theorem shows that it is bounded from formula_10 to formula_10 for any 1 < \"p\" < 2. Duality arguments show that it is also bounded for 2 < \"p\" < ∞. In fact, the Hilbert transform is really unbounded for \"p\" equal to 1 or ∞.\n\nAnother famous example is the Hardy–Littlewood maximal function, which is only sublinear operator rather than linear. While formula_10 to formula_10 bounds can be derived immediately from the formula_2 to weak formula_2 estimate by a clever change of variables, Marcinkiewicz interpolation is a more intuitive approach. Since the Hardy–Littlewood Maximal Function is trivially bounded from formula_42 to formula_42, strong boundedness for all formula_44 follows immediately from the weak (1,1) estimate and interpolation. The weak (1,1) estimate can be obtained from the Vitali covering lemma.\n\nThe theorem was first announced by , who showed this result to Antoni Zygmund shortly before he died in World War II. The theorem was almost forgotten by Zygmund, and was absent from his original works on the theory of singular integral operators. Later realized that Marcinkiewicz's result could greatly simplify his work, at which time he published his former student's theorem together with a generalization of his own.\n\n\n"}
{"id": "8532654", "url": "https://en.wikipedia.org/wiki?curid=8532654", "title": "Mice problem", "text": "Mice problem\n\nIn mathematics, the mice problem is a problem in which a number of mice (or insects, dogs, missiles, etc.) are placed at the corners of a regular polygon. Each mouse begins to move towards its immediate neighbour (clockwise or anticlockwise). It must be determined when the mice meet.\n\nThe most common version has the mice starting at the corners of a unit square, moving at unit speed. In this case they meet after a time of one unit, because the distance between two neighboring mice always decreases at a speed of one unit. More generally, for a regular polygon of \"n\" sides, the distance between neighboring mice decreases at a speed of 1  − cos(2π/\"n\"), so they meet after a time of 1/(1 − cos(2π/\"n\")).\n\nFor all regular polygons, the mice trace out a logarithmic spiral, which meets in the center of the polygon (as shown on the right). When additional mice are added and the mice move towards non-immediate neighbours, the paths they trace become more complex.\n\n\n"}
{"id": "5503348", "url": "https://en.wikipedia.org/wiki?curid=5503348", "title": "Negative flag", "text": "Negative flag\n\nIn a computer processor the negative flag or sign flag is a single bit in a system status (flag) register used to indicate whether the result of the last mathematical operation resulted in a value in which the most significant bit was set. In a two's complement interpretation of the result, the negative flag is set if the result was negative.\n\nFor example, in an 8-bit signed number system, -37 will be represented as 1101 1011 in binary (the most significant bit is 1), while +37 will be represented as 0010 0101 (the most significant bit is 0).\n\nThe negative flag is set according to the result in the x86 series processors by the following instructions (referring to the Intel 80386 manual ): \n\nif result is negative sign flag is set {1}."}
{"id": "47986929", "url": "https://en.wikipedia.org/wiki?curid=47986929", "title": "Operator ideal", "text": "Operator ideal\n\nIn functional analysis, a branch of mathematics, an operator ideal is a special kind of class of continuous linear operators between Banach spaces. If an operator formula_1 belongs to an operator ideal formula_2, then for any operators formula_3 and formula_4 which can be composed with formula_1 as formula_6, then formula_6 is class formula_2 as well. Additionally, in order for formula_2 to be an operator ideal, it must contain the class of all finite-rank Banach space operators.\n\nLet formula_10 denote the class of continuous linear operators acting between two Banach spaces. For any subclass formula_2 of formula_10 and any two Banach spaces formula_13 and formula_14 over the same field formula_15, denote by formula_16 the set of continuous linear operators of the form formula_17. In this case, we say that formula_16 is a component of formula_2. An operator ideal is a subclass formula_2 of formula_10, containing every identity operator acting on a 1-dimensional Banach space, such that for any two Banach spaces formula_13 and formula_14 over the same field formula_15, the following two conditions for formula_16 are satisfied: (1) If formula_26 then formula_27; and (2) if formula_28 and formula_29 are Banach spaces over formula_15 with formula_31 and formula_32, and if formula_33, then formula_34.\n\nOperator ideals enjoy the following nice properties.\n\n\nFurthermore, some very well-known classes are norm-closed operator ideals, i.e., operator ideals whose components are always norm-closed. These include but are not limited to the following.\n\n\n"}
{"id": "245206", "url": "https://en.wikipedia.org/wiki?curid=245206", "title": "Plus and minus signs", "text": "Plus and minus signs\n\nThe plus and minus signs (+ and −) are mathematical symbols used to represent the notions of positive and negative as well as the operations of addition and subtraction. Their use has been extended to many other meanings, more or less analogous. \"Plus\" and \"minus\" are Latin terms meaning \"more\" and \"less\", respectively.\n\nThough the signs now seem as familiar as the alphabet or the Hindu-Arabic numerals, they are not of great antiquity. The Egyptian hieroglyphic sign for addition, for example, resembled a pair of legs walking in the direction in which the text was written (Egyptian could be written either from right to left or left to right), with the reverse sign indicating subtraction:\nNicole Oresme's manuscripts from the 14th century show what may be one of the earliest uses of the plus sign \"+\".\n\nIn Europe in the early 15th century the letters \"P\" and \"M\" were generally used.\nThe symbols (P with line \"p̄\" for \"più\", i.e., plus, and M with line \"m̄\" for \"meno\", i.e., minus) appeared for the first time in Luca Pacioli’s mathematics compendium, \"Summa de arithmetica, geometria, proportioni et proportionalità\", first printed and published in Venice in 1494. The + is a simplification of the Latin \"et\" (comparable to the ampersand &). The − may be derived from a tilde written over m when used to indicate subtraction; or it may come from a shorthand version of the letter m itself. In his 1489 treatise Johannes Widmann referred to the symbols − and + as \"minus\" and \"mer\" (Modern German \"mehr\"; \"more\"): \"was − ist, das ist minus, und das + ist das mer\". They weren't used for addition and subtraction here, but to indicate surplus and deficit; their first use in their modern sense appears in a book by Henricus Grammateus in 1518.\n\nRobert Recorde, the designer of the equals sign, introduced plus and minus to Britain in 1557 in \"The Whetstone of Witte\": \"There be other 2 signes in often use of which the first is made thus + and betokeneth more: the other is thus made – and betokeneth lesse.\"\n\nThe plus sign (+) is a binary operator that indicates addition, as in 2 + 3 = 5. It can also serve as a unary operator that leaves its operand unchanged (+\"x\" means the same as \"x\"). This notation may be used when it is desired to emphasize the positiveness of a number, especially when contrasting with the negative (+5 versus −5).\n\nThe plus sign can also indicate many other operations, depending on the mathematical system under consideration. Many algebraic structures have some operation which is called, or is equivalent to, addition. It is conventional to use the plus sign to only denote commutative operations. Moreover, the symbolism has been extended to very different operations; plus can also mean:\n\nThe minus sign (−) has three main uses in mathematics:\n\nAll three uses can be referred to as \"minus\" in everyday speech. In most English-speaking countries, −5 (for example) is normally pronounced \"minus five\", but in modern US usage it is instead usually pronounced \"negative five\"; here, \"minus\" may be used by speakers born before 1950, and is still popular in some contexts, but \"negative\" is usually taught as the only correct reading. Further, a few textbooks in the United States encourage −\"x\" to be read as \"the opposite of \"x\"\" or \"the additive inverse of \"x\"\" to avoid giving the impression that −\"x\" is necessarily negative.\n\nIn some contexts, different glyphs are used for these meanings; for instance in the computer language APL and the expression language used by Texas Instruments graphing calculators (definitely at least the early models including the TI-81 and TI-82) a raised minus sign is used in negative numbers (as in 2 − 5 shows 3), but such usage is uncommon.\n\nIn mathematics and most programming languages, the rules for the order of operations mean that −5 is equal to −25: Powers bind more strongly than the unary minus, which binds more strongly than multiplication or division. However, in some programming languages and Microsoft Excel in particular, unary operators bind strongest, so in those cases −5^2 is 25 but 0−5^2 is −25.\n\nSome elementary teachers use raised plus and minus signs before numbers to show they are positive or negative numbers. For example, subtracting −5 from 3 might be read as \"positive three take away negative 5\" and be shown as\n\nor even as\n\nIn grading systems (such as examination marks), the plus sign indicates a grade one level higher and the minus sign a grade lower. For example, B− (\"B minus\") is one grade lower than B. Sometimes this is extended to two plus or minus signs; for example A++ is two grades higher than A.\n\nPositive and negative are sometimes abbreviated as +ve and −ve.\n\nIn mathematics the one-sided limit \"x\"→\"a\" means \"x\" approaches \"a\" from the right, and \"x\"→\"a\" means \"x\" approaches \"a\" from the left. For example, when calculating what \"x\" is when \"x\" approaches 0, because \"x\"→+∞ when \"x\"→0 but \"x\"→−∞ when \"x\"→0.\n\nBlood types are often qualified with a plus or minus to indicate the presence or absence of the Rh factor; for instance, A+ means A-type blood with the Rh factor present, while B− means B-type blood with the Rh factor absent.\n\nIn music, augmented chords are symbolized with a plus sign, although this practice is not universal as there are other methods for spelling those chords. For example, \"C+\" is read \"C augmented chord\". Also used as superscript.\n\nAs well as the normal mathematical usage plus and minus may be used for a number of other purposes in computing.\n\nPlus and minus signs are often used in tree view on a computer screen to show if a folder is collapsed or not.\n\nIn some programming languages, concatenation of strings is written codice_1, and results in codice_2.\n\nIn most programming languages, subtraction and negation are indicated with the ASCII hyphen-minus character, codice_3. In APL a raised minus sign (Unicode U+00AF) is used to denote a negative number, as in codice_4. While in J a negative number is denoted by an underscore, as in codice_5.\n\nIn C and some other computer programming languages, two plus signs indicate the increment operator and two minus signs a decrement; the position of the operator before or after the variable indicates whether the new or old value is read from it. For example, if x equals 6, then codice_6 increments x to 7 but sets y to 6, whereas codice_7 would set both x and y to 7. By extension, \"++\" is sometimes used in computing terminology to signify an improvement, as in the name of the language C++.\n\nIn regular expressions, \"+\" is often used to indicate \"1 or more\" in a pattern to be matched. For example, \"x+\" means \"one or more of the letter x\".\n\nThere is no concept of negative zero in mathematics, but in computing −0 may have a separate representation from zero. In the IEEE floating-point standard, 1 / −0 is negative infinity (−∞) whereas 1 / 0 is positive infinity (∞).\n\nIn chemistry, superscripted plus and minus signs are used to indicate an ion with a positive or negative charge of 1 (for example, NH). If the charge is greater than 1, a number indicating the charge is written before the sign (SO). The minus sign is also used (rather than an en dash) for a single covalent bond between two atoms, as in the skeletal formula.\n\nSubscripted plus and minus signs are used as diacritics in the International Phonetic Alphabet to indicate advanced or retracted articulations of speech sounds.\n\nThe minus sign is also used as tone letter in the orthographies of Dan, Krumen, Karaboro, Mwan, Wan, Yaouré, Wè, Nyabwa and Godié. The Unicode character used for the tone letter (U+02D7) is different from the mathematical minus sign.\n\nIn the algebraic notation used to record games of chess, the plus sign (+) is used to denote a move that puts the opponent into check. A double plus (++) is sometimes used to denote double check. Combinations of the plus and minus signs are used to evaluate a move (+/−, +/=, =/+, −/+).\n\nThe hyphen-minus sign (-) is the ASCII alternative/version of the minus sign, and doubles as a hyphen. It is usually shorter in length than the plus sign and sometimes at a different height. It can be used as a substitute for the true minus sign when the character set is limited to ASCII. Most programming languages and other computer readable languages do this, since ASCII is generally available as a subset of most character encodings, while U+2212 is a Unicode feature only.\n\nThere is a commercial minus sign (⁒), which looks somewhat like an obelus, at U+2052 (HTML &#x2052;).\n\nThe codice_8 entity is HTML 5.\n\nA Jewish tradition that dates from at least the 19th century is to write \"plus\" using a symbol like an inverted T. This practice was adopted into Israeli schools and is still commonplace today in elementary schools (including secular schools) but in fewer secondary schools. It is also used occasionally in books by religious authors, but most books for adults use the international symbol \"+\". The reason for this practice is that it avoids the writing of a symbol \"+\" that looks like a Christian cross. Unicode has this symbol at position .\n\n\n"}
{"id": "1250665", "url": "https://en.wikipedia.org/wiki?curid=1250665", "title": "Proof calculus", "text": "Proof calculus\n\nIn mathematical logic, a proof calculus or a proof system is built to prove statements.\n\nA proof system includes the components:\n\nUsually a given proof calculus encompasses more than a single particular formal system, since many proof calculi are under-determined and can be used for radically different logics. For example, a paradigmatic case is the sequent calculus, which can be used to express the consequence relations of both intuitionistic logic and relevance logic. Thus, loosely speaking, a proof calculus is a template or design pattern, characterized by a certain style of formal inference, that may be specialized to produce specific formal systems, namely by specifying the actual inference rules for such a system. There is no consensus among logicians on how best to define the term.\n\nThe most widely known proof calculi are those classical calculi that are still in widespread use:\n\nMany other proof calculi were, or might have been, seminal, but are not widely used today.\n\n\nModern research in logic teems with rival proof calculi:\n\n"}
{"id": "3067624", "url": "https://en.wikipedia.org/wiki?curid=3067624", "title": "Quantitative psychology", "text": "Quantitative psychology\n\nQuantitative psychology is a field of scientific study that focuses on the mathematical modeling, research design and methodology, and statistical analysis of human or animal psychological processes. It includes tests and other devices for measuring human abilities. Quantitative psychologists develop and analyze a wide variety of research methods, including those of psychometrics, a field concerned with the theory and technique of psychological measurement.\n\nPsychologists have long contributed to statistical and mathematical analysis, and quantitative psychology is now a specialty recognized by the American Psychological Association. Doctoral degrees are awarded in this field in a number of universities in Europe and North America, and quantitative psychologists have been in high demand in industry, government, and academia. Their training in both social science and quantitative methodology provides a unique skill set for solving both applied and theoretical problems in a variety of areas.\n\nQuantitative psychology has its roots in early experimental psychology when, in the nineteenth century, the scientific method was first systematically applied to psychological phenomena. Notable contributions included E. H. Weber's studies of tactile sensitivity (1930s), Fechner's development and use of the psychophysical methods (1850-1860), and Helmholtz's research on vision and audition beginning after 1850. Wilhelm Wundt is often called the \"founder of experimental psychology\", because he called himself a psychologist and opened a psychological laboratory in 1879 where many researchers came to study. The work of these and many others helped put to rest the assertion, by theorists such as Immanuel Kant, that psychology could not become a science because precise experiments on the human mind were impossible.\n\nIntelligence testing has long been an important branch of quantitative psychology. The nineteenth-century English statistician Francis Galton, a pioneer in psychometrics, was the first to create a standardized test of intelligence, and he was among the first to apply statistical methods to the study of human differences and their inheritance. He came to believe that intelligence is largely determined by heredity, and he also hypothesized that other measures such as the speed of reflexes, muscle strength, and head size are correlated with intelligence. He established the world's first mental testing center in 1882 in the following year he published his observations and theories in \"Inquiries into Human Faculty and Its Development\".\n\nStatistical methods are the quantitative tools most used by psychologists. Pearson introduced the correlation coefficient and the chi-squared test. The 1900–1920 period saw the t-test (Student, 1908), the ANOVA (Fisher, 1925) and a non-parametric correlation coefficient (Spearman, 1904). A large number of tests were developed in the latter half of the 20th century (e.g., all the multivariate tests). Popular techniques (such as Hierarchical Linear Model, Arnold, 1992, Structural Equation Modeling, Byrne, 1996 and Independent Component Analysis, Hyvarinën, Karhunen and Oja, 2001) are relatively recent.\n\nIn 1946, psychologist Stanley Smith Stevens organized levels of measurement into four scales: Nominal, Ordinal, Ratio, and Interval in a paper that is still often cited. Jacob Cohen, a New York University professor of psychology, analyzed quantitative methods involving statistical power and effect size, which helped to lay foundations for current statistical meta-analysis and the methods of estimation statistics. He gave his name to Cohen's kappa and Cohen's d.\n\nIn 1990, an influential paper titled \"Graduate Training in Statistics, Methodology, and Measurement in Psychology\" was published in the American Psychologist journal. This article discussed the need for increased and up-to-date training in quantitative methods for psychology graduate programs in the United States.\n\nTraining for quantitative psychology can begin informally at the undergraduate level. Many graduate schools recommend that students have some coursework in psychology and complete the full college sequence of calculus (including multivariate calculus) and a course in linear algebra. Quantitative coursework in other fields such as economics and research methods and statistics courses for psychology majors are also helpful. Historically, however, students without all these courses have been accepted if other aspects of their application show promise. Some schools also offer formal minors in areas related to quantitative psychology. For example, the University of Kansas offers a minor in \"Social and Behavioral Sciences Methodology\" that provides advanced training in research methodology, applied data analysis, and practical research experience relevant to quantitative psychology. Coursework in computer science is also useful. Mastery of an object-oriented programming language or learning to write code in SPSS or R is useful for the type of data analysis performed in graduate school.\n\nQuantitative psychologists may possess a doctoral degree or a master's degree. Due to its interdisciplinary nature and depending on the research focus of the university, these programs may be housed in a school's college of education or in their psychology department. Programs that focus especially in educational research and psychometrics are often part of education or educational psychology departments. These programs may therefore have different names mentioning \"research methods\" or \"quantitative methods\", such as the \"Research and Evaluation Methodology\" Ph.D from the University of Florida or the \"Quantitative Methods\" degree at the University of Pennsylvania. However, some universities may have separate programs in their two colleges. For example, the University of Washington has a \"Quantitative psychology\" degree in their psychology department and a separate \"Measurement & Statistics\" Ph.D in their college of education. Others, such as Vanderbilt University's Ph.D in Psychological Sciences is jointly housed across its two psychology departments.\n\nUniversities with a mathematical focus include McGill University's \"Quantitative Psychology and Modeling\" program and Purdue University's \"Mathematical and Computational Cognitive Science\" degrees. Students with an interest in modeling biological or functional data may go into related fields such as biostatistics or computational neuroscience.\n\nDoctoral programs typical accept students with only bachelor's degrees, although some schools may require a master's degree before applying. After the first two years of studies, graduate students typically earn a Master of Arts in Psychology, Master of Science in Statistics or Applied statistics, or both.\n\nAdditionally, several universities offer minor concentrations in quantitative methods, such as New York University.\n\nCompanies that produce standardized tests such as College Board, Educational Testing Service, and American College Testing are some of the biggest private sector employers of quantitative psychologists. These companies also often provide internships to students in graduate school.\n\nIn August 2005, the American Psychological Association expressed the need for more quantitative psychologists in the industry—for every PhD awarded in the subject, there were about 2.5 quantitative psychologist position openings. Due to a lack of applicants in the field, the APA created a Task Force to study the state of quantitative psychology and predict its future. Domestic U.S. applicants are especially lacking. The majority of international applicants come from Asian countries, especially South Korea and China. In response to the lack of qualified applicants, the APA Council of Representatives authorized a special task force in 2006. The task force was chaired by Leona S. Aiken from Arizona State University.\n\nQuantitative psychologists generally have a main area of interest. Notable research areas in psychometrics include item response theory and computer adaptive testing, which focus on education and intelligence testing. Other research areas include modeling psychological processes through time series analysis, such as in fMRI data collection, and structural equation modeling, social network analysis, human decision science, and statistical genetics.\n\nTwo common types of psychometric tests are: aptitude tests, which are supposed to measure raw intellectual ability, and personality tests that aim to assess your character, temperament, and how you deal with problems.\n\nItem response theory is based on the application of related mathematical models to testing data. Because it is generally regarded as superior to classical test theory, it is the preferred method for developing scales in the United States, especially when optimal decisions are demanded, as in so-called high-stakes tests, e.g., the Graduate Record Examination (GRE) and Graduate Management Admission Test (GMAT).\nQuantitative psychology is served by several scientific organizations. These include the Psychometric Society, Division 5 of the American Psychological Association (Evaluation, Measurement and Statistics), the Society of Multivariate Experimental Psychology, and the European Society for Methodology. Associated disciplines include statistics, mathematics, educational measurement, educational statistics, sociology, and political science. Several scholarly journals reflect the efforts of scientists in these areas, notably \"Psychometrika\", \"Multivariate Behavioral Research\", \"Structural Equation Modeling\" and \"Psychological Methods\".\n\nThe following is a select list of quantitative psychologists or people who have contributed to the field:\n\n"}
{"id": "29317092", "url": "https://en.wikipedia.org/wiki?curid=29317092", "title": "Regular semi-algebraic system", "text": "Regular semi-algebraic system\n\nIn computer algebra, a regular semi-algebraic system is a particular kind of triangular system of multivariate polynomials over a real closed field.\n\nRegular chains and triangular decompositions are fundamental and well-developed tools for describing the complex solutions of polynomial systems. The notion of a regular semi-algebraic system is an adaptation of the concept of a regular chain focusing on solutions of the real analogue: semi-algebraic systems.\n\nAny semi-algebraic system formula_1 can be decomposed into finitely many regular semi-algebraic systems formula_2 such that a point (with real coordinates) is a solution of formula_1 if and only if it is a solution of one of the systems formula_2.\n\nLet formula_5 be a regular chain of formula_6 for some ordering of the variables formula_7 and a real closed field formula_8. Let formula_9 and formula_10 designate respectively the variables of formula_11 that are free and algebraic with respect to formula_5. Let formula_13 be finite such that each polynomial in formula_14 is regular with respect to the saturated ideal of formula_5. Define formula_16. Let formula_17 be a quantifier-free formula of formula_18 involving only the variables of formula_19. We say that formula_20 is a regular semi-algebraic system if the following three conditions hold.\n\n\nThe zero set of formula_30, denoted by formula_31, is defined as the set of points formula_32 such that formula_33 is true and formula_34, for all formula_35and all formula_36. Observe that formula_31 has dimension formula_38 in the affine space formula_39.\n\n"}
{"id": "43076291", "url": "https://en.wikipedia.org/wiki?curid=43076291", "title": "Richard L. Bishop", "text": "Richard L. Bishop\n\nRichard Lawrence Bishop (born 1932) is an American mathematician, a professor emeritus of mathematics at the University of Illinois at Urbana–Champaign. The Bishop–Gromov inequality in Riemannian geometry is named after him (with Mikhail Gromov).\n\nBishop went to Case Institute of Technology as an undergraduate, earning a B.S. in 1954. Next he earned his Ph.D. from the Massachusetts Institute of Technology in 1959, and immediately joined the UIUC faculty. His thesis, \"On Imbeddings and Holonomy\", was supervised by Isadore Singer. At UIUC, his doctoral students included future UIUC colleague Stephanie B. Alexander. He is the author of \"Geometry of Manifolds\" (with Richard J. Crittenden, AMS Chelsea Publishing, 1964, translated into Russian 1967 and reprinted 2001) and \"Tensor Analysis on Manifolds\" (with Samuel I. Goldberg, Macmillan, 1968, reprinted by Dover Books on Mathematics, 1980).\n\nIn 2013, Bishop became one of the inaugural fellows of the American Mathematical Society.\n"}
{"id": "329400", "url": "https://en.wikipedia.org/wiki?curid=329400", "title": "Solid of revolution", "text": "Solid of revolution\n\nIn mathematics, engineering, and manufacturing, a solid of revolution is a solid figure obtained by rotating a plane curve around some straight line (the \"axis of revolution\") that lies on the same plane.\n\nAssuming that the curve does not cross the axis, the solid's volume is equal to the length of the circle described by the figure's centroid multiplied by the figure's area (Pappus's second centroid Theorem).\n\nA representative disk is a three-dimensional volume element of a solid of revolution. The element is created by rotating a line segment (of length ) around some axis (located units away), so that a cylindrical volume of units is enclosed.\n\nTwo common methods for finding the volume of a solid of revolution are the disc method and the shell method of integration. To apply these methods, it is easiest to draw the graph in question; identify the area that is to be revolved about the axis of revolution; determine the volume of either a disc-shaped slice of the solid, with thickness , or a cylindrical shell of width ; and then find the limiting sum of these volumes as approaches 0, a value which may be found by evaluating a suitable integral.\n\nThe disk method is used when the slice that was drawn is \"perpendicular to\" the axis of revolution; i.e. when integrating \"parallel to\" the axis of revolution.\n\nThe volume of the solid formed by rotating the area between the curves of and and the lines and about the -axis is given by\nIf (e.g. revolving an area between the curve and the -axis), this reduces to:\n\nThe method can be visualized by considering a thin horizontal rectangle at between on top and on the bottom, and revolving it about the -axis; it forms a ring (or disc in the case that ), with outer radius and inner radius . The area of a ring is , where is the outer radius (in this case ), and is the inner radius (in this case ). The volume of each infinitesimal disc is therefore . The limit of the Riemann sum of the volumes of the discs between and becomes integral (1).\n\nThe cylinder method is used when the slice that was drawn is \"parallel to\" the axis of revolution; i.e. when integrating \"perpendicular to\" the axis of revolution.\n\nThe volume of the solid formed by rotating the area between the curves of and and the lines and about the -axis is given by\nIf (e.g. revolving an area between curve and -axis), this reduces to:\n\nThe method can be visualized by considering a thin vertical rectangle at with height , and revolving it about the -axis; it forms a cylindrical shell. The lateral surface area of a cylinder is , where is the radius (in this case ), and is the height (in this case ). Summing up all of the surface areas along the interval gives the total volume.\n\nWhen a curve is defined by its parametric form in some interval , the volumes of the solids generated by revolving the curve around the -axis or the -axis are given by\n\nUnder the same circumstances the areas of the surfaces of the solids generated by revolving the curve around the -axis or the -axis are given by\n\n\n"}
{"id": "15560112", "url": "https://en.wikipedia.org/wiki?curid=15560112", "title": "Stability spectrum", "text": "Stability spectrum\n\nIn model theory, a branch of mathematical logic, a complete first-order theory \"T\" is called stable in λ (an infinite cardinal number), if the Stone space of every model of \"T\" of size ≤ λ has itself size ≤ λ. \"T\" is called a stable theory if there is no upper bound for the cardinals κ such that \"T\" is stable in κ. The stability spectrum of \"T\" is the class of all cardinals κ such that \"T\" is stable in κ.\n\nFor countable theories there are only four possible stability spectra. The corresponding dividing lines are those for total transcendentality, superstability and stability. This result is due to Saharon Shelah, who also defined stability and superstability.\n\nTheorem.\nEvery countable complete first-order theory \"T\" falls into one of the following classes:\n\nThe condition on λ in the third case holds for cardinals of the form λ = κ, but not for cardinals λ of cofinality ω (because λ < λ).\n\nA complete first-order theory \"T\" is called totally transcendental if every formula has bounded Morley rank, i.e. if RM(φ) < ∞ for every formula φ(\"x\") with parameters in a model of \"T\", where \"x\" may be a tuple of variables. It is sufficient to check that RM(\"x\"=\"x\") < ∞, where \"x\" is a single variable.\n\nFor countable theories total transcendence is equivalent to stability in ω, and therefore countable totally transcendental theories are often called ω-stable for brevity. A totally transcendental theory is stable in every λ ≥ |\"T\"|, hence a countable ω-stable theory is stable in all infinite cardinals.\n\nEvery uncountably categorical countable theory is totally transcendental. This includes complete theories of vector spaces or algebraically closed fields. The theories of groups of finite Morley rank are another important example of totally transcendental theories.\n\nA complete first-order theory \"T\" is superstable if there is a rank function on complete types that has essentially the same properties as Morley rank in a totally transcendental theory. Every totally transcendental theory is superstable. A theory \"T\" is superstable if and only if it is stable in all cardinals λ ≥ 2.\n\nA theory that is stable in one cardinal λ ≥ |\"T\"| is stable in all cardinals λ that satisfy λ = λ. Therefore a theory is stable if and only if it is stable in some cardinal λ ≥ |\"T\"|.\n\nMost mathematically interesting theories fall into this category, including complicated theories such as any complete extension of ZF set theory, and relatively tame theories such as the theory of real closed fields. This shows that the stability spectrum is a relatively blunt tool. To get somewhat finer results one can look at the exact cardinalities of the Stone spaces over models of size ≤ λ, rather than just asking whether they are at most λ.\n\nFor a general stable theory \"T\" in a possibly uncountable language, the stability spectrum is determined by two cardinals κ and λ, such that \"T\" is stable in λ exactly when λ ≥ λ and λ = λ for all μ<κ. So λ is the smallest infinite cardinal for which \"T\" is stable. These invariants satisfy the inequalities\n\nWhen |\"T\"| is countable the 4 possibilities for its stability spectrum correspond to the following values of these cardinals:\n\n\n"}
{"id": "604111", "url": "https://en.wikipedia.org/wiki?curid=604111", "title": "Stone's representation theorem for Boolean algebras", "text": "Stone's representation theorem for Boolean algebras\n\nIn mathematics, Stone's representation theorem for Boolean algebras states that every Boolean algebra is isomorphic to a certain field of sets. The theorem is fundamental to the deeper understanding of Boolean algebra that emerged in the first half of the 20th century. The theorem was first proved by Marshall H. Stone (1936). Stone was led to it by his study of the spectral theory of operators on a Hilbert space.\n\nEach Boolean algebra \"B\" has an associated topological space, denoted here \"S\"(\"B\"), called its Stone space. The points in \"S\"(\"B\") are the ultrafilters on \"B\", or equivalently the homomorphisms from \"B\" to the two-element Boolean algebra. The topology on \"S\"(\"B\") is generated by a (closed) basis consisting of all sets of the form\nwhere \"b\" is an element of \"B\". This is the topology of pointwise convergence of nets of homomorphisms into the two-element Boolean algebra.\n\nFor every Boolean algebra \"B\", \"S\"(\"B\") is a compact totally disconnected Hausdorff space; such spaces are called Stone spaces (also \"profinite spaces\"). Conversely, given any topological space \"X\", the collection of subsets of \"X\" that are clopen (both closed and open) is a Boolean algebra.\n\nA simple version of Stone's representation theorem states that every Boolean algebra \"B\" is isomorphic to the algebra of clopen subsets of its Stone space \"S\"(\"B\"). The isomorphism sends an element \"b\"∈\"B\" to the set of all ultrafilters that contain \"b\". This is a clopen set because of the choice of topology on \"S\"(\"B\") and because \"B\" is a Boolean algebra. \n\nRestating the theorem using the language of category theory; the theorem states that there is a duality between the category of Boolean algebras and the category of Stone spaces. This duality means that in addition to the correspondence between Boolean algebras and their Stone spaces, each homomorphism from a Boolean algebra \"A\" to a Boolean algebra \"B\" corresponds in a natural way to a continuous function from \"S\"(\"B\") to \"S\"(\"A\"). In other words, there is a contravariant functor that gives an equivalence between the categories. This was an early example of a nontrivial duality of categories.\n\nThe theorem is a special case of Stone duality, a more general framework for dualities between topological spaces and partially ordered sets.\n\nThe proof requires either the axiom of choice or a weakened form of it. Specifically, the theorem is equivalent to the Boolean prime ideal theorem, a weakened choice principle that states that every Boolean algebra has a prime ideal.\n\nAn extension of the classical Stone duality to the category of Boolean spaces (= zero-dimensional locally compact Hausdorff spaces) and continuous maps (respectively, perfect maps) was obtained by G. D. Dimov (respectively, by H. P. Doctor) (see the references below).\n\n\n"}
{"id": "246188", "url": "https://en.wikipedia.org/wiki?curid=246188", "title": "Summation by parts", "text": "Summation by parts\n\nIn mathematics, summation by parts transforms the summation of products of sequences into other summations, often simplifying the computation or (especially) estimation of certain types of sums. The summation by parts formula is sometimes called Abel's lemma or Abel transformation.\n\nSuppose formula_1 and formula_2 are two sequences. Then,\n\nUsing the forward difference operator formula_4, it can be stated more succinctly as\n\nNote that summation by parts is an analogue to the integration by parts formula,\n\nAn alternative statement is\nwhich is analogous to the integration by parts formula for semimartingales.\n\nNote also that although applications almost always deal with convergence of sequences, the statement is purely algebraic and will work in any field. It will also work when one sequence is in a vector space, and the other is in the relevant field of scalars.\n\nThe formula is sometimes given in one of these - slightly different - forms\n\nwhich represent a special case (formula_9) of the more general rule\n\nboth result from iterated application of the initial formula. The auxiliary quantities are Newton series:\n\nand \n\nA remarkable, particular (formula_14) result is the noteworthy identity\n\nHere, formula_16 is the binomial coefficient.\n\nFor two given sequences formula_17 and formula_18, with formula_19, one wants to study the sum of the following series:<br>\nformula_20\n\nIf we define formula_21 \nthen for every formula_22  formula_23  and\n\nFinally  formula_26\n\nThis process, called an Abel transformation, can be used to prove several criteria of convergence for formula_27 .\n\nThe formula for an integration by parts is formula_28<br>\nBeside the boundary conditions, we notice that the first integral contains two multiplied functions, one which is integrated in the final integral ( formula_29 becomes formula_30 ) and one which is differentiated ( formula_31 becomes formula_32 ).\n\nThe process of the \"Abel transformation\" is similar, since one of the two initial sequences is summed ( formula_33 becomes formula_34 ) and the other one is differenced ( formula_35 becomes formula_36 ).\n\n\nSummation by parts gives\nwhere \"a\" is the limit of formula_38. As formula_37 is convergent, formula_43 is bounded independently of formula_44, say by formula_45. As formula_46 go to zero, so go the first two terms. The third term goes to zero by the Cauchy criterion for formula_37. The remaining sum is bounded by\n\nby the monotonicity of formula_38, and also goes to zero as formula_50.\n\nthen formula_20 converges.\n\nIn both cases, the sum of the series satisfies:\nformula_58\n\n"}
{"id": "296838", "url": "https://en.wikipedia.org/wiki?curid=296838", "title": "Universe (mathematics)", "text": "Universe (mathematics)\n\nIn mathematics, and particularly in set theory, category theory, type theory, and the foundations of mathematics, a universe is a collection that contains all the entities one wishes to consider in a given situation. It is closely related to the concept of a domain of discourse in philosophy. \n\nIn set theory, universes are often classes that contain (as elements) all sets for which one hopes to prove a particular theorem. These classes can serve as inner models for various axiomatic systems such as ZFC or Morse–Kelley set theory. Universes are of critical importance to formalizing concepts in category theory inside set-theoretical foundations. For instance, the canonical motivating example of a category is Set, the category of all sets, which cannot be formalized in a set theory without some notion of a universe.\n\nIn type theory, a universe is a type whose elements are types.\n\nPerhaps the simplest version is that \"any\" set can be a universe, so long as the object of study is confined to that particular set. If the object of study is formed by the real numbers, then the real line R, which is the real number set, could be the universe under consideration. Implicitly, this is the universe that Georg Cantor was using when he first developed modern naive set theory and cardinality in the 1870s and 1880s in applications to real analysis. The only sets that Cantor was originally interested in were subsets of R.\n\nThis concept of a universe is reflected in the use of Venn diagrams. In a Venn diagram, the action traditionally takes place inside a large rectangle that represents the universe \"U\". One generally says that sets are represented by circles; but these sets can only be subsets of \"U\". The complement of a set \"A\" is then given by that portion of the rectangle outside of \"A\"'s circle. Strictly speaking, this is the relative complement \"U\" \\ \"A\" of \"A\" relative to \"U\"; but in a context where \"U\" is the universe, it can be regarded as the absolute complement \"A\" of \"A\". Similarly, there is a notion of the nullary intersection, that is the intersection of zero sets (meaning no sets, not null sets).\n\nWithout a universe, the nullary intersection would be the set of absolutely everything, which is generally regarded as impossible; but with the universe in mind, the nullary intersection can be treated as the set of everything under consideration, which is simply \"U\". These conventions are quite useful in the algebraic approach to basic set theory, based on Boolean lattices. Except in some non-standard forms of axiomatic set theory (such as New Foundations), the class of all sets is not a Boolean lattice (it is only a relatively complemented lattice).\n\nIn contrast, the class of all subsets of \"U\", called the power set of \"U\", is a Boolean lattice. The absolute complement described above is the complement operation in the Boolean lattice; and \"U\", as the nullary intersection, serves as the top element (or nullary meet) in the Boolean lattice. Then De Morgan's laws, which deal with complements of meets and joins (which are unions in set theory) apply, and apply even to the nullary meet and the nullary join (which is the empty set).\n\nHowever, once subsets of a given set \"X\" (in Cantor's case, \"X\" = R) are considered, the universe may need to be a set of subsets of \"X\". (For example, a topology on \"X\" is a set of subsets of \"X\".) The various sets of subsets of \"X\" will not themselves be subsets of \"X\" but will instead be subsets of PX\", the power set of \"X\". This may be continued; the object of study may next consist of such sets of subsets of \"X\", and so on, in which case the universe will be P(PX\"). In another direction, the binary relations on \"X\" (subsets of the Cartesian product may be considered, or functions from \"X\" to itself, requiring universes like or \"X\".\n\nThus, even if the primary interest is \"X\", the universe may need to be considerably larger than \"X\". Following the above ideas, one may want the superstructure over \"X\" as the universe. This can be defined by structural recursion as follows:\nThen the superstructure over \"X\", written S\"X\", is the union of S\"X\", S\"X\", S\"X\", and so on; or\n\nNo matter what set \"X\" is the starting point, the empty set {} will belong to S\"X\". The empty set is the von Neumann ordinal [0].\nThen {[0]}, the set whose only element is the empty set, will belong to S\"X\"; this is the von Neumann ordinal [1]. Similarly, {[1]} will belong to S\"X\", and thus so will {[0],[1]}, as the union of {[0]} and {[1]}; this is the von Neumann ordinal [2]. Continuing this process, every natural number is represented in the superstructure by its von Neumann ordinal. Next, if \"x\" and \"y\" belong to the superstructure, then so does , which represents the ordered pair (\"x\",\"y\"). Thus the superstructure will contain the various desired Cartesian products. Then the superstructure also contains functions and relations, since these may be represented as subsets of Cartesian products. The process also gives ordered \"n\"-tuples, represented as functions whose domain is the von Neumann ordinal [\"n\"], and so on.\n\nSo if the starting point is just \"X\" = {}, a great deal of the sets needed for mathematics appear as elements of the superstructure over {}. But each of the elements of S{} will be a finite set. Each of the natural numbers belongs to it, but the set N of \"all\" natural numbers does not (although it is a \"subset\" of S{}). In fact, the superstructure over {} consists of all of the hereditarily finite sets. As such, it can be considered the \"universe of finitist mathematics\". Speaking anachronistically, one could suggest that the 19th-century finitist Leopold Kronecker was working in this universe; he believed that each natural number existed but that the set N (a \"completed infinity\") did not.\n\nHowever, S{} is unsatisfactory for ordinary mathematicians (who are not finitists), because even though N may be available as a subset of S{}, still the power set of N is not. In particular, arbitrary sets of real numbers are not available. So it may be necessary to start the process all over again and form S(S{}). However, to keep things simple, one can take the set N of natural numbers as given and form SN, the superstructure over N. This is often considered the \"universe of ordinary mathematics\". The idea is that all of the mathematics that is ordinarily studied refers to elements of this universe. For example, any of the usual constructions of the real numbers (say by Dedekind cuts) belongs to SN. Even non-standard analysis can be done in the superstructure over a non-standard model of the natural numbers.\n\nThere is a slight shift in philosophy from the previous section, where the universe was any set \"U\" of interest. There, the sets being studied were \"subset\"s of the universe; now, they are \"members\" of the universe. Thus although P(SX\") is a Boolean lattice, what is relevant is that SX\" itself is not. Consequently, it is rare to apply the notions of Boolean lattices and Venn diagrams directly to the superstructure universe as they were to the power-set universes of the previous section. Instead, one can work with the individual Boolean lattices PA\", where \"A\" is any relevant set belonging to SX\"; then PA\" is a subset of SX\" (and in fact belongs to S\"X\"). In Cantor's case \"X\" = R in particular, arbitrary sets of real numbers are not available, so there it may indeed be necessary to start the process all over again.\n\nIt is possible to give a precise meaning to the claim that SN is the universe of ordinary mathematics; it is a model of Zermelo set theory, the axiomatic set theory originally developed by Ernst Zermelo in 1908. Zermelo set theory was successful precisely because it was capable of axiomatising \"ordinary\" mathematics, fulfilling the programme begun by Cantor over 30 years earlier. But Zermelo set theory proved insufficient for the further development of axiomatic set theory and other work in the foundations of mathematics, especially model theory.\n\nFor a dramatic example, the description of the superstructure process above cannot itself be carried out in Zermelo set theory. The final step, forming S as an infinitary union, requires the axiom of replacement, which was added to Zermelo set theory in 1922 to form Zermelo–Fraenkel set theory, the set of axioms most widely accepted today. So while ordinary mathematics may be done \"in\" SN, discussion \"of\" SN goes beyond the \"ordinary\", into metamathematics.\n\nBut if high-powered set theory is brought in, the superstructure process above reveals itself to be merely the beginning of a transfinite recursion.\nGoing back to \"X\" = {}, the empty set, and introducing the (standard) notation \"V\" for S{}, \"V\" = {}, \"V\" = P{}, and so on as before. But what used to be called \"superstructure\" is now just the next item on the list: \"V\", where ω is the first infinite ordinal number. This can be extended to arbitrary ordinal numbers:\nEvery individual \"V\" is a set, but their union \"V\" is a proper class. The axiom of foundation, which was added to ZF set theory at around the same time as the axiom of replacement, says that \"every\" set belongs to \"V\".\n\nThere is another approach to universes which is historically connected with category theory. This is the idea of a Grothendieck universe. Roughly speaking, a Grothendieck universe is a set inside which all the usual operations of set theory can be performed. This version of a universe is defined to be any set for which the following axioms hold:\n\nThe advantage of a Grothendieck universe is that it is actually a \"set\", and never a proper class. The disadvantage is that if one tries hard enough, one can leave a Grothendieck universe.\n\nThe most common use of a Grothendieck universe \"U\" is to take \"U\" as a replacement for the category of all sets. One says that a set \"S\" is U-small if \"S\" ∈\"U\", and U-large otherwise. The category \"U\"-Set of all \"U\"-small sets has as objects all \"U\"-small sets and as morphisms all functions between these sets. Both the object set and the morphism set are sets, so it becomes possible to discuss the category of \"all\" sets without invoking proper classes. Then it becomes possible to define other categories in terms of this new category. For example, the category of all \"U\"-small categories is the category of all categories whose object set and whose morphism set are in \"U\". Then the usual arguments of set theory are applicable to the category of all categories, and one does not have to worry about accidentally talking about proper classes. Because Grothendieck universes are extremely large, this suffices in almost all applications.\n\nOften when working with Grothendieck universes, mathematicians assume the Axiom of Universes: \"For any set \"x\", there exists a universe \"U\" such that \"x\" ∈\"U\".\" The point of this axiom is that any set one encounters is then \"U\"-small for some \"U\", so any argument done in a general Grothendieck universe can be applied. This axiom is closely related to the existence of strongly inaccessible cardinals.\n\nIn some type theories, especially in systems with dependent types, types themselves can be regarded as terms. There is a type called the universe (often denoted formula_17) which has types as its elements. To avoid paradoxes such as Girard's paradox (an analogue of Russell's paradox for type theory), type theories are often equipped with a countably infinite hierarchy of such universes, with each universe being a term of the next one. \n\nThere are at least two kinds of universes that one can consider in type theory: Russell-style universes (named after Bertrand Russell) and Tarski-style universes (named after Alfred Tarski). A Russell-style universe is a type whose terms are types. A Tarski-style universe is a type together with an interpretation operation allowing us to regard its terms as types.\n\n\n\n"}
{"id": "48221297", "url": "https://en.wikipedia.org/wiki?curid=48221297", "title": "Viviane Baladi", "text": "Viviane Baladi\n\nViviane Baladi (born May 23, 1963) is a mathematician who works as a director of research at the Centre national de la recherche scientifique (CNRS) in France. Originally Swiss, she has become a naturalized citizen of France. Her research concerns dynamical systems.\n\nBaladi earned master's degrees in mathematics and computer science in 1986 from the University of Geneva. She stayed in Geneva for her doctoral studies, finishing a Ph.D. in 1989 under the supervision of Jean-Pierre Eckmann, with a dissertation concerning the zeta functions of dynamical systems. She worked at CNRS beginning in 1990, with a leave of absence from 1993 to 1999 when she taught at ETH Zurich and the University of Geneva. She also spent a year as a professor at the University of Copenhagen in 2012–2013.\n\nShe is the author of the book \"Positive Transfer Operators and Decay of Correlation\" (Advanced Series in Nonlinear Dynamics 16, World Scientific, 2000) and of \"Dynamical Zeta Functions and Dynamical Determinants for Hyperbolic Maps: A Functional Approach\" (Ergebnisse der Mathematik und ihrer Grenzgebiete 68, Springer, 2018).\n\nShe was an invited speaker at the International Congress of Mathematicians in 2014, speaking in the section on \"Dynamical Systems and Ordinary Differential Equations\".\n\n"}
{"id": "104482", "url": "https://en.wikipedia.org/wiki?curid=104482", "title": "Vladimir Voevodsky", "text": "Vladimir Voevodsky\n\nVladimir Alexandrovich Voevodsky (; , 4 June 1966 – 30 September 2017) was a Russian-American mathematician. His work in developing a homotopy theory for algebraic varieties and formulating motivic cohomology led to the award of a Fields Medal in 2002. He is also known for the proof of the Milnor conjecture and motivic Bloch-Kato conjectures and for the univalent foundations of mathematics and homotopy type theory.\n\nVladimir Voevodsky's father, Aleksander Voevodsky, was head of the Laboratory of High Energy Leptons in the Institute for Nuclear Research at the Russian Academy of Sciences. His mother Tatyana, was a chemist. Voevodsky attended Moscow State University for a while, but was forced to leave without a diploma for refusing to attend classes and failing academically. He received his Ph.D. in mathematics from Harvard University in 1992 after being recommended without ever applying, following several independent publications; he was advised there by David Kazhdan.\n\nWhile he was a first year undergraduate, he was given a copy of Esquisse d'un Programme (submitted a few months earlier by Alexander Grothendieck to CNRS) by his advisor George Shabat. He learned the French language \"with the sole purpose of being able to read this text\" and started his research on some of the themes mentioned there.\n\nVoevodsky's work was in the intersection of algebraic geometry with algebraic topology. Along with Fabien Morel, Voevodsky introduced a homotopy theory for schemes. He also formulated what is now believed to be the correct form of motivic cohomology, and used this new tool to prove Milnor's conjecture relating the Milnor K-theory of a field to its étale cohomology. For the above, he received the Fields Medal at the 24th International Congress of Mathematicians held in Beijing, China.\n\nIn 1998 he gave a plenary lecture (\"A-Homotopy Theory\") at the International Congress of Mathematicians in Berlin. He coauthored (with Andrei Suslin and Eric M. Friedlander) \"Cycles, Transfers and Motivic Homology Theories\", which develops the theory of motivic cohomology in some detail.\n\nFrom 2002, Voevodsky was a professor at the Institute for Advanced Study in Princeton, New Jersey.\n\nIn January 2009, at an anniversary conference in honor of Alexander Grothendieck, held at the Institut des Hautes Études Scientifiques, Voevodsky announced a proof of the full Bloch-Kato conjectures.\n\nIn 2009, he constructed the univalent model of Martin-Löf type theory in simplicial sets. This led to important advances in type theory and in the development of new Univalent foundations of mathematics that Voevodsky worked on in his final years. He worked on a Coq library \"UniMath\" using univalent ideas.\n\nIn April 2016, the University of Gothenburg awarded an honorary doctorate to Voevodsky.\n\nVoevodsky died on 30 September 2017 at his home in Princeton, from an aneurysm. He is survived by daughters Diana Yasmine Voevodsky and Natalia Dalia Shalaby.\n\n\n\n"}
