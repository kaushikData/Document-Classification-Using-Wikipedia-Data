{"id": "37498758", "url": "https://en.wikipedia.org/wiki?curid=37498758", "title": "Andrea Bertozzi", "text": "Andrea Bertozzi\n\nAndrea Louise Bertozzi (born 1965) is an American mathematician. Her research interests are in non-linear partial differential equations and applied mathematics.\n\nShe earned her bachelor's and master's degrees from Princeton University, followed by her PhD from Princeton in 1991; her dissertation was titled \"Existence, Uniqueness, and a Characterization of Solutions to the Contour Dynamics Equation\". Prior to joining UCLA in 2003, Bertozzi was an L. E. Dickson Instructor at the University of Chicago, and then Professor of Mathematics and Physics at Duke University. At the University of Chicago she first began to study the mathematics of thin films. She spent one year at Argonne National Laboratory as the Maria Goeppert-Mayer Distinguished Scholar. She coauthored the book \"Vorticity and Incompressible Flow\", which was published in 2000.\n\nShe is a member of the faculty of the University of California, Los Angeles, as a Professor of Mathematics (since 2003) and Mechanical and Aerospace Engineering (since 2018) and Director of Applied Mathematics (since 2005). She is a member of the California NanoSystems Institute. At UCLA, among other things, she has worked with Jeffrey Brantingham and other colleagues to apply mathematics to the patterns of urban crime, research which was the cover feature in the March 2, 2010 issue of Proceedings of the National Academy of Sciences. Bertozzi also spoke about the mathematics of crime at the 2010 annual meeting of the American Association for the Advancement of Science.\n\nShe is the older sister of the chemist Carolyn Bertozzi. Her father, William Bertozzi, was a professor of physics at the Massachusetts Institute of Technology.\n\nIn 1995 Bertozzi received a research fellowship from the Sloan Foundation. In 1996 she received the Presidential Early Career Award for Scientists and Engineers from the U.S. Office of Naval Research. She is featured in the book \"Encyclopedia of World Scientists\", by Elizabeth H. Oakes, published in 2007. She was also awarded the 2009 Association for Women in Mathematics-Society for Industrial and Applied Mathematics Sonia Kovalevsky Lecture, and was elected a Society for Industrial and Applied Mathematics Fellow in 2010.\n\nIn 2010 she was elected to the American Academy of Arts and Sciences. \nIn 2012 she became a fellow of the American Mathematical Society. \nIn 2013 she was named the Betsy Wood Knapp Chair for Innovation and Creativity at UCLA.\nIn 2014 she won a SIAM Outstanding Paper Prize (joint with Arjuna Flenner).\nIn 2016 she became a Fellow of the American Physical Society.\nIn 2015 and 2016 she was named a Thomson-Reuters/Clarivate Analytics 'highly cited' researcher.\nIn 2017 she became a Simons Investigator.\nIn 2018 she was elected to the US National Academy of Sciences.\n\n"}
{"id": "31680438", "url": "https://en.wikipedia.org/wiki?curid=31680438", "title": "Annals of Functional Analysis", "text": "Annals of Functional Analysis\n\nThe Annals of Functional Analysis is a peer-reviewed mathematics journal published by the Tusi Mathematical Research Group. The journal was established in 2009 and covers functional analysis and operator theory. It is indexed by Science Citation Index Expanded, \"Mathematical Reviews\" and \"Zentralblatt MATH\". The journal has an MCQ-2012 of 0.36 by MathSciNet (All Journal MCQ is 0.35).\n"}
{"id": "52245438", "url": "https://en.wikipedia.org/wiki?curid=52245438", "title": "Bernoulli's triangle", "text": "Bernoulli's triangle\n\nBernoulli's triangle is an array of partial sums of the binomial coefficients. For any non-negative integer \"n\" and for any integer \"k\" included between 0 and \"n\", the component in row \"n\" and column \"k\" is given by:\n\ni.e., the sum of the first \"k\" \"n\"th-order binomial coefficients. The first rows of Bernoulli's triangle are:\n\nSimilarly to Pascal's triangle, each component of Bernoulli's triangle is the sum of two components of the previous row, except for the last number of each row, which is double the last number of the previous row. For example, if formula_3 denotes the component in row \"n\" and column \"k\", then:\n\nAs in Pascal's triangle and other similarly constructed triangles, sums of components along diagonal paths in Bernoulli's triangle result in the Fibonacci numbers.\n\n"}
{"id": "9496834", "url": "https://en.wikipedia.org/wiki?curid=9496834", "title": "Carry operator", "text": "Carry operator\n\nThe carry operator, symbolized by the ¢ sign, is an abstraction of the operation of determining whether a portion of an adder network generates or propagates a carry. It is defined as follows:\n\n"}
{"id": "525149", "url": "https://en.wikipedia.org/wiki?curid=525149", "title": "Codimension", "text": "Codimension\n\nIn mathematics, codimension is a basic geometric idea that applies to subspaces in vector spaces, to submanifolds in manifolds, and suitable subsets of algebraic varieties.\n\nFor affine and projective algebraic varieties, the codimension equals the height of the defining ideal. For this reason, the height of an ideal is often called its codimension.\n\nThe dual concept is relative dimension.\n\nCodimension is a \"relative\" concept: it is only defined for one object \"inside\" another. There is no “codimension of a vector space (in isolation)”, only the codimension of a vector \"sub\"space.\n\nIf \"W\" is a linear subspace of a finite-dimensional vector space \"V\", then the codimension of \"W\" in \"V\" is the difference between the dimensions:\nIt is the complement of the dimension of \"W,\" in that, with the dimension of \"W,\" it adds up to the dimension of the ambient space \"V:\"\n\nSimilarly, if \"N\" is a submanifold or subvariety in \"M\", then the codimension of \"N\" in \"M\" is\nJust as the dimension of a submanifold is the dimension of the tangent bundle (the number of dimensions that you can move \"on\" the submanifold), the codimension is the dimension of the normal bundle (the number of dimensions you can move \"off\" the submanifold).\n\nMore generally, if \"W\" is a linear subspace of a (possibly infinite dimensional) vector space \"V\" then the codimension of \"W\" in \"V\" is the dimension (possibly infinite) of the quotient space \"V\"/\"W\", which is more abstractly known as the cokernel of the inclusion. For finite-dimensional vector spaces, this agrees with the previous definition\nand is dual to the relative dimension as the dimension of the kernel.\n\nFinite-codimensional subspaces of infinite-dimensional spaces are often useful in the study of topological vector spaces.\n\nThe fundamental property of codimension lies in its relation to intersection: if \"W\" has codimension \"k\", and \"W\" has codimension \"k\", then if \"U\" is their intersection with codimension \"j\" we have\n\nIn fact \"j\" may take any integer value in this range. This statement is more perspicuous than the translation in terms of dimensions, because the RHS is just the sum of the codimensions. In words\n\nThis statement is called dimension counting, particularly in intersection theory.\n\nIn terms of the dual space, it is quite evident why dimensions add. The subspaces can be defined by the vanishing of a certain number of linear functionals, which if we take to be linearly independent, their number is the codimension. Therefore, we see that \"U\" is defined by taking the union of the sets of linear functionals defining the \"W\". That union may introduce some degree of linear dependence: the possible values of \"j\" express that dependence, with the RHS sum being the case where there is no dependence. This definition of codimension in terms of the number of functions needed to cut out a subspace extends to situations in which both the ambient space and subspace are infinite dimensional.\n\nIn other language, which is basic for any kind of intersection theory, we are taking the union of a certain number of constraints. We have two phenomena to look out for:\n\n\nThe first of these is often expressed as the principle of counting constraints: if we have a number \"N\" of parameters to adjust (i.e. we have \"N\" degrees of freedom), and a constraint means we have to 'consume' a parameter to satisfy it, then the codimension of the solution set is \"at most\" the number of constraints. We do not expect to be able to find a solution if the predicted codimension, i.e. the number of \"independent\" constraints, exceeds \"N\" (in the linear algebra case, there is always a \"trivial\", null vector solution, which is therefore discounted).\n\nThe second is a matter of geometry, on the model of parallel lines; it is something that can be discussed for linear problems by methods of linear algebra, and for non-linear problems in projective space, over the complex number field.\n\nCodimension also has some clear meaning in geometric topology: on a manifold, codimension 1 is the dimension of topological disconnection by a submanifold, while codimension 2 is the dimension of ramification and knot theory. In fact, the theory of high-dimensional manifolds, which starts in dimension 5 and above, can alternatively be said to start in codimension 3, because higher codimensions avoid the phenomenon of knots. Since surgery theory requires working up to the middle dimension, once one is in dimension 5, the middle dimension has codimension greater than 2, and hence one avoids knots.\n\nThis quip is not vacuous: the study of embeddings in codimension 2 is knot theory, and difficult, while the study of embeddings in codimension 3 or more is amenable to the tools of high-dimensional geometric topology, and hence considerably easier.\n\n"}
{"id": "292231", "url": "https://en.wikipedia.org/wiki?curid=292231", "title": "Combinatorial game theory", "text": "Combinatorial game theory\n\nCombinatorial game theory (CGT) is a branch of mathematics and theoretical computer science that typically studies sequential games with perfect information. Study has been largely confined to two-player games that have a \"position\" in which the players take turns changing in defined ways or \"moves\" to achieve a defined winning condition. CGT has not traditionally studied games of chance or those that use imperfect or incomplete information, favoring games that offer perfect information in which the state of the game and the set of available moves is always known by both players. However, as mathematical techniques advance, the types of game that can be mathematically analyzed expands, thus the boundaries of the field are ever changing. Scholars will generally define what they mean by a \"game\" at the beginning of a paper, and these definitions often vary as they are specific to the game being analyzed and are not meant to represent the entire scope of the field.\n\nCombinatorial games include well-known games such as chess, checkers, and Go, which are regarded as non-trivial, and tic-tac-toe, which is considered as trivial in the sense of being \"easy to solve\". Some combinatorial games may also have an unbounded playing area, such as infinite chess. In CGT, the moves in these and other games are represented as a game tree. \n\nCombinatorial games also include one-player combinatorial puzzles such as Sudoku, and no-player automata, such as Conway's Game of Life, (although in the strictest definition, \"games\" can be said to require more than one participant, thus the designations of \"puzzle\" and \"automata\".) \n\nGame theory in general includes games of chance, games of imperfect knowledge, and games in which players can move simultaneously, and they tend to represent real-life decision making situations.\n\nCGT has a different emphasis than \"traditional\" or \"economic\" game theory, which was initially developed to study games with simple combinatorial structure, but with elements of chance (although it also considers sequential moves, see extensive-form game). Essentially, CGT has contributed new methods for analyzing game trees, for example using surreal numbers, which are a subclass of all two-player perfect-information games. The type of games studied by CGT is also of interest in artificial intelligence, particularly for automated planning and scheduling. In CGT there has been less emphasis on refining practical search algorithms (such as the alpha–beta pruning heuristic included in most artificial intelligence textbooks), but more emphasis on descriptive theoretical results (such as measures of game complexity or proofs of optimal solution existence without necessarily specifying an algorithm, such as the strategy-stealing argument).\n\nAn important notion in CGT is that of the solved game. For example, tic-tac-toe is considered a solved game, as it can be proven that any game will result in a draw if both players play optimally. Deriving similar results for games with rich combinatorial structures is difficult. For instance, in 2007 it was announced that checkers has been weakly solved—optimal play by both sides also leads to a draw—but this result was a computer-assisted proof. Other real world games are mostly too complicated to allow complete analysis today, although the theory has had some recent successes in analyzing Go endgames. Applying CGT to a \"position\" attempts to determine the optimum sequence of moves for both players until the game ends, and by doing so discover the optimum move in any position. In practice, this process is torturously difficult unless the game is very simple.\n\nIt can be helpful to distinguish between combinatorial \"mathgames\" of interest primarily to mathematicians and scientists to ponder and solve, and combinatorial \"playgames\" of interest to the general population as a form of entertainment and competition. However, a number of games fall into both categories. Nim, for instance, is a playgame instrumental in the foundation of CGT, and one of the first computerized games. Tic-tac-toe is still used to teach basic principles of game AI design to computer science students. \nCGT arose in relation to the theory of impartial games, in which any play available to one player must be available to the other as well. One such game is nim, which can be solved completely. Nim is an impartial game for two players, and subject to the \"normal play condition\", which means that a player who cannot move loses. In the 1930s, the Sprague–Grundy theorem showed that all impartial games are equivalent to heaps in nim, thus showing that major unifications are possible in games considered at a combinatorial level, in which detailed strategies matter, not just pay-offs.\n\nIn the 1960s, Elwyn R. Berlekamp, John H. Conway and Richard K. Guy jointly introduced the theory of a partisan game, in which the requirement that a play available to one player be available to both is relaxed. Their results were published in their book \"Winning Ways for your Mathematical Plays\" in 1982. However, the first work published on the subject was Conway's 1976 book \"On Numbers and Games\", also known as ONAG, which introduced the concept of surreal numbers and the generalization to games. \"On Numbers and Games\" was also a fruit of the collaboration between Berlekamp, Conway, and Guy.\n\nCombinatorial games are generally, by convention, put into a form where one player wins when the other has no moves remaining. It is easy to convert any finite game with only two possible results into an equivalent one where this convention applies. One of the most important concepts in the theory of combinatorial games is that of the sum of two games, which is a game where each player may choose to move either in one game or the other at any point in the game, and a player wins when his opponent has no move in either game. This way of combining games leads to a rich and powerful mathematical structure.\n\nJohn Conway states in ONAG that the inspiration for the theory of partisan games was based on his observation of the play in go endgames, which can often be decomposed into sums of simpler endgames isolated from each other in different parts of the board.\n\nThe introductory text \"Winning Ways\" introduced a large number of games, but the following were used as motivating examples for the introductory theory:\n\nThe classic game Go was influential on the early combinatorial game theory, and Berlekamp and Wolfe subsequently developed an endgame and \"temperature\" theory for it (see references). Armed with this they were able to construct plausible Go endgame positions from which they could give expert Go players a choice of sides and then defeat them either way.\n\nAnother game studied in the context of combinatorial game theory is chess. In 1953 Alan Turing wrote of the game, \"If one can explain quite unambiguously in English, with the aid of mathematical symbols if required, how a calculation is to be done, then it is always possible to programme any digital computer to do that calculation, provided the storage capacity is adequate.\" In a 1950 paper, Claude Shannon estimated the lower bound of the game-tree complexity of chess to be 10, and today this is referred to as the Shannon number. Chess remains unsolved, although extensive study, including work involving the use of supercomputers has created chess end-game tablebases, which shows the result of perfect play for all end-games with seven pieces or less. Infinite chess has an even greater combinatorial complexity than chess (unless only limited end-games, or composed positions with a small number of pieces are being studied).\n\nA game, in its simplest terms, is a list of possible \"moves\" that two players, called \"left\" and \"right\", can make. The game position resulting from any move can be considered to be another game. This idea of viewing games in terms of their possible moves to other games leads to a recursive mathematical definition of games that is standard in combinatorial game theory. In this definition, each game has the notation {L|R}. formula_1 is the set of game positions that the left player can move to, and formula_2 is the set of game positions that the right player can move to; each position in L and R is defined as a game using the same notation.\n\nUsing Domineering as an example, label each of the sixteen boxes of the four-by-four board by \"A1\" for the upper leftmost square, \"C2\" for the third box from the left on the second row from the top, and so on. We use e.g. (D3, D4) to stand for the game position in which a vertical domino has been placed in the bottom right corner. Then, the initial position can be described in combinatorial game theory notation as\n\nIn standard Cross-Cram play, the players alternate turns, but this alternation is handled implicitly by the definitions of combinatorial game theory rather than being encoded within the game states.\n\nThe zero game is a loss for the first player.\n\nThe sum of number games behaves like the integers, for example 3 + -2 = 1.\n\n\"Star\", written as * or {0|0}, is a first-player win since either player must (if first to move in the game) move to a zero game, and therefore win.\n\nThe game * is neither positive nor negative; it and all other games in which the first player wins (regardless of which side the player is on) are said to be \"fuzzy with\" or \"confused with\" 0; symbolically, we write * || 0.\n\n\"Up\", written as ↑, is a position in combinatorial game theory. In standard notation, ↑ = {0|*}.\n\nUp is strictly positive (↑ > 0), but is infinitesimal. Up is defined in \"Winning Ways for your Mathematical Plays\".\n\n\"Down\", written as ↓, is a position in combinatorial game theory. In standard notation, ↓ = {*|0}.\n\nDown is strictly negative (↓ < 0), but is infinitesimal. Down is defined in Winning Ways for your Mathematical Plays.\n\nConsider the game {1|-1}. Both moves in this game are an advantage for the player who makes them; so the game is said to be \"hot;\" it is greater than any number less than -1, less than any number greater than 1, and fuzzy with any number in between. It is written as ±1. It can be added to numbers, or multiplied by positive ones, in the expected fashion; for example, 4 ± 1 = {5|3}.\n\nAn impartial game is one where, at every position of the game, the same moves are available to both players. For instance, Nim is impartial, as any set of objects that can be removed by one player can be removed by the other. However, domineering is not impartial, because one player places horizontal dominoes and the other places vertical ones. Likewise Checkers is not impartial, since the players own different colored pieces. For any ordinal number, one can define an impartial game generalizing Nim in which, on each move, either player may replace the number with any smaller ordinal number; the games defined in this way are known as nimbers. The Sprague–Grundy theorem states that every impartial game is equivalent to a nimber.\n\nThe \"smallest\" nimbers - the simplest and least under the usual ordering of the ordinals - are 0 and *.\n\n\n\n"}
{"id": "41125081", "url": "https://en.wikipedia.org/wiki?curid=41125081", "title": "Connective spectrum", "text": "Connective spectrum\n\nIn algebraic topology, a branch of mathematics, a connective spectrum is a spectrum whose homotopy sets formula_1 of negative degrees are zero.\n\n"}
{"id": "4321829", "url": "https://en.wikipedia.org/wiki?curid=4321829", "title": "David Harel", "text": "David Harel\n\nDavid Harel (; born 12 April 1950) is a computer scientist at the Weizmann Institute of Science in Israel, and holds the William Sussman Professorial Chair of Mathematics. Born in London, England, he was Dean of the Faculty of Mathematics and Computer Science at the institute for seven years. He currently also serves as Vice-President of the Israel Academy of Sciences and Humanities.\n\nHarel is best known for his work on dynamic logic, computability, database theory, software engineering and modelling biological systems. In the 1980s he invented the graphical language of Statecharts for specifying and programming reactive systems, which has been adopted as part of the UML standard. Since the late 1990s he has concentrated on a scenario-based approach to programming such systems, launched by his co-invention (with W. Damm) of Live Sequence Charts. He has published expository accounts of computer science, such as his award winning 1987 book \"Algorithmics: The Spirit of Computing\" and his 2000 book \"Computers Ltd.: What They \"Really\" Can’t do\", and has presented series on computer science for Israeli radio and television. He has also worked on other diverse topics, such as graph layout, computer science education and the analysis and communication of odors.\n\nHarel completed his PhD at MIT between 1976 and 1978. In 1987, he co-founded the software company I-Logix, which in 2006 became part of IBM.\n\nHe has advocated building a full computer model of the Caenorhabditis elegans nematode, which was the first multicellular organism to have its genome completely sequenced. The eventual completeness of such a model depends on his updated version of the Turing test.\n\nHe is a fellow of the ACM, the IEEE, the AAAS, and EATCS.\n\nHarel is active in several peace and human rights organisations in Israel.\n\n\n\n"}
{"id": "8286632", "url": "https://en.wikipedia.org/wiki?curid=8286632", "title": "Digital root", "text": "Digital root\n\nThe digital root (also repeated digital sum) of a non-negative integer is the (single digit) value obtained by an iterative process of summing digits, on each iteration using the result from the previous iteration to compute a digit sum. The process continues until a single-digit number is reached.\n\nFor example, the digital root of 65,536 is 7, because and \n\nDigital roots can be calculated with congruences in modular arithmetic rather than by adding up all the digits, a procedure that can save time in the case of very large numbers.\n\nDigital roots can be used as a sort of checksum, to check that a sum has been performed correctly. If it has, then the digital root of the sum of the given numbers will equal the digital root of the sum of the digital roots of the given numbers. This check, which involves only single-digit calculations, can catch many errors in calculation.\n\nDigital roots are used in Western numerology, but certain numbers deemed to have occult significance (such as 11 and 22) are not always completely reduced to a single digit.\n\nThe number of times the digits must be summed to reach the digital root is called a number's additive persistence; in the above example, the additive persistence of 65,536 is 2.\n\nIt helps to see the digital root of a positive integer as the position it holds with respect to the largest multiple of 9 less than the number itself. For example, the digital root of 11 is 2, which means that 11 is the second number after 9. Likewise, the digital root of 2035 is 1, which means that 2035 − 1 is a multiple of 9. If a number produces a digital root of exactly 9, then the number is a multiple of 9.\n\nWith this in mind the digital root of a positive integer formula_1 may be defined by using floor function formula_2, as\n\nThe table below shows the digital roots produced by the familiar multiplication table in the decimal system.\nThe table shows a number of interesting patterns and symmetries and is known as the Vedic square.\n\nLet formula_4 denote the sum of the digits of formula_1 and let the composition of formula_4 be as follows:\nEventually the sequence formula_8 becomes a one digit number. Let formula_9 (the digital root of formula_1) represent this one digit number.\n\nLet us find the digital root of formula_11.\n\nThus,\n\nFor simplicity let us agree simply that\n\nHow do we know that the sequence formula_8 eventually becomes a one digit number? Here's a proof:\n\nLet formula_17, for all formula_18, formula_19 is an integer greater than or equal to 0 and less than 10. Then, formula_20. This means that formula_21, unless formula_22, in which case formula_1 is a one digit number. Thus, repeatedly using the formula_4 function would cause formula_1 to decrease by at least 1, until it becomes a one digit number, at which point it will stay constant, as formula_26.\n\nThe formula is:\nor,\n\nTo generalize the concept of digital roots to other bases \"b\", one can simply change the 9 in the formula to \"b\" - 1.\n\nThe digital root is the value modulo 9 because formula_29 and thus formula_30 so regardless of position, the value mod 9 is the same – formula_31 – which is why digits can be meaningfully added. Concretely, for a three-digit number,\n\nTo obtain the modular value with respect to other numbers \"n,\" one can take weighted sums, where the weight on the \"k\"th digit corresponds to the value of formula_33 modulo \"n,\" or analogously for formula_34 for different bases. This is simplest for 2, 5, and 10, where higher digits vanish (since 2 and 5 divide 10), which corresponds to the familiar fact that the divisibility of a decimal number with respect to 2, 5, and 10 can be checked by the last digit (even numbers end in 0, 2, 4, 6, or 8).\n\nAlso of note is the modulus 11: since formula_35 and thus formula_36 taking the \"alternating\" sum of digits yields the value modulo 11.\n\nThe digital root of a number is zero if and only if the number is itself zero.\nThe digital root of a number is a positive integer if and only if the number is itself a positive integer.\nThe digital root of formula_1 is formula_1 itself if and only if the number has exactly one digit.\nThe digital root of formula_1 is less than formula_1 if and only if the number is greater than or equal to 10.\nThe digital root of formula_45 + formula_46 is digital root of the sum of the digital root of formula_45 and the digital root of formula_46.\nThe digital root of formula_45 - formula_46 is congruent with the difference of the digital root of formula_45 and the digital root of formula_46 modulo 9.\nEspecially, we can define the digital root of minus formula_1 as follows:\nThe digital root of formula_45 × formula_46 is digital root of the product of the digital root of formula_45 and the digital root of formula_46.\n\nThis article is about the digital root in decimal or base ten, hence it is the number mod 9. It is nothing different as the number converted to base 9 and then only the last digit taken.\nIn other radixes the digital root is number mod (base-1) so in base 12 a digital root of a number is the number mod 11 (Ɛ), for example, 1972 is 1 + 9 + 7 + 2 = 19 = 17 which is 1 + 7 = 8, while in decimal the root of the same number (3110) is 5; and in base 16 a digital root of a number is the number mod 15 (0xF), for example, 0x7DF is 7 + 13 + 15 = 35 = 0x23 which is 2 + 3 = 5, while in decimal the root of the same number (2015) is 8.\n\nDigital roots form an important mechanic in the visual novel adventure game Nine Hours, Nine Persons, Nine Doors.\n\n\n"}
{"id": "11071463", "url": "https://en.wikipedia.org/wiki?curid=11071463", "title": "Entropy rate", "text": "Entropy rate\n\nIn the mathematical theory of probability, the entropy rate or source information rate of a stochastic process is, informally, the time density of the average information in a stochastic process. For stochastic processes with a countable index, the entropy rate formula_1 is the limit of the joint entropy of formula_2 members of the process formula_3 divided by formula_2, as formula_2 tends to infinity:\n\nwhen the limit exists. An alternative, related quantity is:\n\nFor strongly stationary stochastic processes, formula_8. The entropy rate can be thought of as a general property of stochastic sources; this is the asymptotic equipartition property. The entropy rate may be used to estimate the complexity of stochastic processes. It is used in diverse applications ranging from characterizing the complexity of languages, blind source separation, through to optimizing quantizers and data compression algorithms. For example, a maximum entropy rate criterion may be used for feature selection in machine learning .\n\nSince a stochastic process defined by a Markov chain that is irreducible, aperiodic\nand positive recurrent has a stationary distribution, the entropy rate is independent of the initial distribution.\n\nFor example, for such a Markov chain formula_9 defined on a countable number of states, given the transition matrix formula_10, formula_11 is given by:\n\nwhere formula_13 is the asymptotic distribution of the chain.\n\nA simple consequence of this definition is that an i.i.d. stochastic process has an entropy rate that is the same as the entropy of any individual member of the process.\n\n\n"}
{"id": "4522019", "url": "https://en.wikipedia.org/wiki?curid=4522019", "title": "Factorization lemma", "text": "Factorization lemma\n\nIn measure theory, the factorization lemma allows us to express a function \"f\" with another function \"T\" if \"f\" is measurable with respect to \"T\". An application of this is regression analysis.\n\nLet formula_1 be a function of a set formula_2 in a measure space formula_3 and let formula_4 be a scalar function on formula_2. Then formula_6 is measurable with respect to the σ-algebra formula_7 generated by formula_8 in formula_2 if and only if there exists a measurable function formula_10 such that formula_11, where formula_12 denotes the Borel set of the real numbers. If formula_6 only takes finite values, then formula_14 also only takes finite values.\n\nFirst, if formula_11, then \"f\" is formula_16 measurable because it is the composition of a formula_17 and of a formula_18 measurable function. The proof of the converse falls into four parts: (1)\"f\" is a step function, (2)\"f\" is a positive function, (3) \"f\" is any scalar function, (4) \"f\" only takes finite values.\n\nSuppose formula_19 is a step function, i.e. formula_20 and formula_21. As \"T\" is a measurable function, for all \"i\", there exists formula_22 such that formula_23. formula_24 fulfils the requirements.\n\nIf \"f\" takes only positive values, it is the limit, for pointwise convergence, of a increasing sequence formula_25 of step functions. For each of these, by (1), there exists formula_26 such that formula_27. The function formula_28, which exists on the image of T for pointwise convergence because formula_25 is monotonic, fulfils the requirements.\n\nWe can decompose \"f\" in a positive part formula_30 and a negative part formula_31. We can then find formula_32 and formula_33 such that formula_34 and formula_35. The problem is that the difference formula_36 is not defined on the set formula_37. Fortunately, formula_38 because formula_39 always implies formula_40\nWe define formula_41 and formula_42. formula_43 fulfils the requirements.\n\nIf \"f\" takes finite values only, we will show that \"g\" also only takes finite values. Let formula_44. Then formula_45 fulfils the requirements because formula_46.\n\nIf the function formula_47 is not scalar, but takes values in a different measurable space, such as formula_48 with its trivial σ-algebra (the empty set, and the whole real line) instead of formula_49, then the lemma becomes false (as the restrictions on formula_47 are much weaker).\n\n"}
{"id": "47732", "url": "https://en.wikipedia.org/wiki?curid=47732", "title": "Fourier-transform spectroscopy", "text": "Fourier-transform spectroscopy\n\nFourier-transform spectroscopy is a measurement technique whereby spectra are collected based on measurements of the coherence of a radiative source, using time-domain or space-domain measurements of the electromagnetic radiation or other type of radiation.\nIt can be applied to a variety of types of spectroscopy including optical spectroscopy, infrared spectroscopy (FTIR, FT-NIRS), nuclear magnetic resonance (NMR) and magnetic resonance spectroscopic imaging (MRSI), mass spectrometry and electron spin resonance spectroscopy. There are several methods for measuring the temporal coherence of the light (see: field-autocorrelation), including the continuous wave \"Michelson\" or \"Fourier-transform\" spectrometer and the pulsed Fourier-transform spectrograph (which is more sensitive and has a much shorter sampling time than conventional spectroscopic techniques, but is only applicable in a laboratory environment).\n\nThe term \"Fourier-transform spectroscopy\" reflects the fact that in all these techniques, a Fourier transform is required to turn the raw data into the actual spectrum, and in many of the cases in optics involving interferometers, is based on the Wiener–Khinchin theorem.\n\nOne of the most basic tasks in spectroscopy is to characterize the spectrum of a light source: how much light is emitted at each different wavelength. The most straightforward way to measure a spectrum is to pass the light through a monochromator, an instrument that blocks all of the light \"except\" the light at a certain wavelength (the un-blocked wavelength is set by a knob on the monochromator). Then the intensity of this remaining (single-wavelength) light is measured. The measured intensity directly indicates how much light is emitted at that wavelength. By varying the monochromator's wavelength setting, the full spectrum can be measured. This simple scheme in fact describes how \"some\" spectrometers work.\n\nFourier-transform spectroscopy is a less intuitive way to get the same information. Rather than allowing only one wavelength at a time to pass through to the detector, this technique lets through a beam containing many different wavelengths of light at once, and measures the \"total\" beam intensity. Next, the beam is modified to contain a \"different\" combination of wavelengths, giving a second data point. This process is repeated many times. Afterwards, a computer takes all this data and works backwards to infer how much light there is at each wavelength.\n\nTo be more specific, between the light source and the detector, there is a certain configuration of mirrors that allows some wavelengths to pass through but blocks others (due to wave interference). The beam is modified for each new data point by moving one of the mirrors; this changes the set of wavelengths that can pass through.\n\nAs mentioned, computer processing is required to turn the raw data (light intensity for each mirror position) into the desired result (light intensity for each wavelength). The processing required turns out to be a common algorithm called the Fourier transform (hence the name, \"Fourier-transform spectroscopy\"). The raw data is sometimes called an \"interferogram\". Because of the existing computer equipment requirements, and the ability of light to analyze very small amounts of substance, it is often beneficial to automate many aspects of the sample preparation. The sample can be better preserved and the results are much easier to replicate. Both of these benefits are important, for instance, in testing situations that may later involve legal action, such as those involving drug specimens.\n\nThe method of Fourier-transform spectroscopy can also be used for absorption spectroscopy. The primary example is \"FTIR Spectroscopy\", a common technique in chemistry.\n\nIn general, the goal of absorption spectroscopy is to measure how well a sample absorbs or transmits light at each different wavelength. Although absorption spectroscopy and emission spectroscopy are different in principle, they are closely related in practice; any technique for emission spectroscopy can also be used for absorption spectroscopy. First, the emission spectrum of a broadband lamp is measured (this is called the \"background spectrum\"). Second, the emission spectrum of the same lamp \"shining through the sample\" is measured (this is called the \"sample spectrum\"). The sample will absorb some of the light, causing the spectra to be different. The ratio of the \"sample spectrum\" to the \"background spectrum\" is directly related to the sample's absorption spectrum.\n\nAccordingly, the technique of \"Fourier-transform spectroscopy\" can be used both for measuring emission spectra (for example, the emission spectrum of a star), \"and\" absorption spectra (for example, the absorption spectrum of a liquid).\n\nThe Michelson spectrograph is similar to the instrument used in the Michelson–Morley experiment. Light from the source is split into two beams by a half-silvered mirror, one is reflected off a fixed mirror and one off a movable mirror, which introduces a time delay—the Fourier-transform spectrometer is just a Michelson interferometer with a movable mirror. The beams interfere, allowing the temporal coherence of the light to be measured at each different time delay setting, effectively converting the time domain into a spatial coordinate. By making measurements of the signal at many discrete positions of the movable mirror, the spectrum can be reconstructed using a Fourier transform of the temporal coherence of the light. Michelson spectrographs are capable of very high spectral resolution observations of very bright sources.\nThe Michelson or Fourier-transform spectrograph was popular for infra-red applications at a time when infra-red astronomy only had single-pixel detectors. Imaging Michelson spectrometers are a possibility, but in general have been supplanted by imaging Fabry–Pérot instruments, which are easier to construct.\n\nThe intensity as a function of the path length difference (also denoted as retardation) in the interferometer formula_1 and wavenumber formula_2 is \n\nwhere formula_4 is the spectrum to be determined. Note that it is not necessary for formula_4 to be modulated by the sample before the interferometer. In fact, most FTIR spectrometers place the sample after the interferometer in the optical path. The total intensity at the detector is\n\nThis is just a Fourier cosine transform. The inverse gives us our desired result in terms of the measured quantity formula_7:\n\nA pulsed Fourier-transform spectrometer does not employ transmittance techniques. In the most general description of pulsed FT spectrometry, a sample is exposed to an energizing event which causes a periodic response. The frequency of the periodic response, as governed by the field conditions in the spectrometer, is indicative of the measured properties of the analyte.\n\nIn magnetic spectroscopy (EPR, NMR), a Microwave pulse (EPR) or a radio frequency pulse (NMR) in a strong ambient magnetic field is used as the energizing event. This turns the magnetic particles at an angle to the ambient field, resulting in gyration. The gyrating spins then induce a periodic current in a detector coil. Each spin exhibits a characteristic frequency of gyration (relative to the field strength) which reveals information about the analyte.\n\nIn Fourier-transform mass spectrometry, the energizing event is the injection of the charged sample into the strong electromagnetic field of a cyclotron. These particles travel in circles, inducing a current in a fixed coil on one point in their circle. Each traveling particle exhibits a characteristic cyclotron frequency-field ratio revealing the masses in the sample.\n\nPulsed FT spectrometry gives the advantage of requiring a single, time-dependent measurement which can easily deconvolute a set of similar but distinct signals. The resulting composite signal, is called a \"free induction decay,\" because typically the signal will decay due to inhomogeneities in sample frequency, or simply unrecoverable loss of signal due to entropic loss of the property being measured.\n\nPulsed sources allow for the utilization of Fourier-transform spectroscopy principles in scanning near-field optical microscopy techniques. Particularly in nano-FTIR, where the scattering from a sharp probe-tip is used to perform spectroscopy of samples with nanoscale spatial resolution, a high-power illumination from pulsed infrared lasers makes up for a relatively small scattering efficiency (often < 1%) of the probe.\n\nIn addition to the scanning forms of Fourier-transform spectrometers, there are a number of stationary or self-scanned forms. While the analysis of the interferometric output is similar to that of the typical scanning interferometer, significant differences apply, as shown in the published analyses. Some stationary forms retain the Fellgett multiplex advantage, and their use in the spectral region where detector noise limits apply is similar to the scanning forms of the FTS. In the photon-noise limited region, the application of stationary interferometers is dictated by specific consideration for the spectral region and the application.\n\nOne of the most important advantages of Fourier-transform spectroscopy was shown by P. B. Fellgett, an early advocate of the method. The Fellgett advantage, also known as the multiplex principle, states that when obtaining a spectrum when measurement noise is dominated by detector noise (which is independent of the power of radiation incident on the detector), a multiplex spectrometer such as a Fourier-transform spectrometer will produce a relative improvement in signal-to-noise ratio, compared to an equivalent scanning monochromator, of the order of the square root of \"m\", where \"m\" is the number of sample points comprising the spectrum. However, if the detector is shot-noise dominated, the noise will be proportional to the square root of the power, thus for a broad boxcar spectrum (continuous broadband source), the noise is proportional to the square root of \"m\", thus precisely offset the Fellgett's advantage. Shot noise is the main reason Fourier-transform spectrometry was never popular for ultraviolet (UV) and visible spectra.\n\n"}
{"id": "11490", "url": "https://en.wikipedia.org/wiki?curid=11490", "title": "Fundamental frequency", "text": "Fundamental frequency\n\nThe fundamental frequency, often referred to simply as the fundamental, is defined as the lowest frequency of a periodic waveform. In music, the fundamental is the musical pitch of a note that is perceived as the lowest partial present. In terms of a superposition of sinusoids (e.g. Fourier series), the fundamental frequency is the lowest frequency sinusoidal in the sum. In some contexts, the fundamental is usually abbreviated as f\" (or FF), indicating the lowest frequency counting from zero. In other contexts, it is more common to abbreviate it as f\", the first harmonic. (The second harmonic is then f = 2⋅f, etc. In this context, the zeroth harmonic would be 0 Hz.)\n\nAll sinusoidal and many non-sinusoidal waveforms are periodic, which is to say they repeat exactly over time. The period of a waveform is the formula_1 for which the following equation is true:\n\nWhere formula_3 is the value of the waveform at formula_4. This means that this equation and a definition of the waveforms values over any interval of length formula_1 is all that is required to describe the waveform completely.\n\nEvery waveform may be described using any multiple of this period. There exists a smallest period over which the function may be described completely and this period is the fundamental period. The fundamental frequency is defined as its reciprocal:\n\nSince the period is measured in units of time, then the units for frequency are 1/time. When the time units are seconds, the frequency is in formula_7, also known as Hertz.\n\nFor a tube of length formula_8 with one end closed and the other end open the wavelength of the fundamental harmonic is formula_9, as indicated by the first two animations. Hence,\n\nTherefore, using the relation\nwhere formula_12 is the speed of the wave, we can find the fundamental frequency in terms of the speed of the wave and the length of the tube:\n\nIf the ends of the same tube are now both closed or both opened as in the last two animations, the wavelength of the fundamental harmonic becomes formula_14. By the same method as above, the fundamental frequency is found to be\n\nAt 20 °C (68 °F) the speed of sound in air is 343 m/s (1129 ft/s). This speed is temperature dependent and increases at a rate of 0.6 m/s for each degree Celsius increase in temperature (1.1 ft/s for every increase of 1 °F).\n\nThe velocity of a sound wave at different temperatures:-\n\nIn music, the fundamental is the musical pitch of a note that is perceived as the lowest partial present. The fundamental may be created by vibration over the full length of a string or air column, or a higher harmonic chosen by the player. The fundamental is one of the harmonics. A harmonic is any member of the harmonic series, an ideal set of frequencies that are positive integer multiples of a common fundamental frequency. The reason a fundamental is also considered a harmonic is because it is 1 times itself.\nThe fundamental is the frequency at which the entire wave vibrates. Overtones are other sinusoidal components present at frequencies above the fundamental. All of the frequency components that make up the total waveform, including the fundamental and the overtones, are called partials. Together they form the harmonic series. Overtones which are perfect integer multiples of the fundamental are called harmonics. When an overtone is near to being harmonic, but not exact, it is sometimes called a harmonic partial, although they are often referred to simply as harmonics. Sometimes overtones are created that are not anywhere near a harmonic, and are just called partials or inharmonic overtones.\n\nThe fundamental frequency is considered the \"first harmonic\" and the \"first partial.\" The numbering of the partials and harmonics is then usually the same; the second partial is the second harmonic, etc. But if there are inharmonic partials, the numbering no longer coincides. Overtones are numbered as they appear \"above\" the fundamental. So strictly speaking, the \"first\" overtone is the \"second\" partial (and usually the \"second\" harmonic). As this can result in confusion, only harmonics are usually referred to by their numbers, and overtones and partials are described by their relationships to those harmonics.\n\nConsider a spring, fixed at one end and having a mass attached to the other; this would be a single degree of freedom (SDoF) oscillator. Once set into motion, it will oscillate at its natural frequency. For a single degree of freedom oscillator, a system in which the motion can be described by a single coordinate, the natural frequency depends on two system properties: mass and stiffness; (providing the system is undamped). The radian frequency, \"ω\", can be found using the following equation:\n\nWhere:\n\"k\" = stiffness of the spring\n\"m\" = mass \n\"ω\" = radian frequency (radians per second)\n\nFrom the radian frequency, the natural frequency, \"f\", can be found by simply dividing \"ω\" by 2\"π\". Without first finding the radian frequency, the natural frequency can be found directly using:\n\nWhere:\n\"f\" = natural frequency in hertz (cycles/second)\n\"k\" = stiffness of the spring (Newtons/meter or N/m)\n\"m\" = mass(kg) \nwhile doing the modal analysis of structures and mechanical equipment, the frequency of 1st mode is called fundamental frequency.\n\n"}
{"id": "37595171", "url": "https://en.wikipedia.org/wiki?curid=37595171", "title": "Georges Glaeser", "text": "Georges Glaeser\n\nGeorges Glaeser ( 1918–2002 ) was a French mathematician who was director of the IREM of Strasbourg.\nHe worked in analysis and mathematical education and introduced Glaeser's composition theorem and Glaeser's continuity theorem.\n\nGlaeser was a Ph.D. student of Laurent Schwartz.\n\nOn July 3, 1973, Glaeser filed a complaint against Vichy collaborator Paul Touvier in the Lyon Court, charging him with crimes against humanity. Glaeser accused Touvier of the 1944 massacre at Rillieux-la-Pape, in which Glaeser's father was murdered. Touvier was eventually imprisoned for life on this charge in 1994.\n\n\n\n"}
{"id": "7277012", "url": "https://en.wikipedia.org/wiki?curid=7277012", "title": "Greedy algorithm for Egyptian fractions", "text": "Greedy algorithm for Egyptian fractions\n\nIn mathematics, the greedy algorithm for Egyptian fractions is a greedy algorithm, first described by Fibonacci, for transforming rational numbers into Egyptian fractions. An Egyptian fraction is a representation of an irreducible fraction as a sum of distinct unit fractions, as e.g. 5/6 = 1/2 + 1/3. As the name indicates, these representations have been used as long ago as ancient Egypt, but the first published systematic method for constructing such expansions is described in the Liber Abaci (1202) of Leonardo of Pisa (Fibonacci). It is called a greedy algorithm because at each step the algorithm chooses greedily the largest possible unit fraction that can be used in any representation of the remaining fraction.\n\nFibonacci actually lists several different methods for constructing Egyptian fraction representations (, chapter II.7). He includes the greedy method as a last resort for situations when several simpler methods fail; see Egyptian fraction for a more detailed listing of these methods. As Salzer (1948) details, the greedy method, and extensions of it for the approximation of irrational numbers, have been rediscovered several times by modern mathematicians, earliest and most notably by ; see for instance and . A closely related expansion method that produces closer approximations at each step by allowing some unit fractions in the sum to be negative dates back to .\n\nThe expansion produced by this method for a number \"x\" is called the greedy Egyptian expansion, Sylvester expansion, or Fibonacci–Sylvester expansion of \"x\". However, the term \"Fibonacci expansion\" usually refers, not to this method, but to representation of integers as sums of Fibonacci numbers.\n\nFibonacci's algorithm expands the fraction \"x\"/\"y\" to be represented, by repeatedly performing the replacement\n(simplifying the second term in this replacement as necessary). For instance:\nin this expansion, the denominator 3 of the first unit fraction is the result of rounding 15/7 up to the next larger integer, and the remaining fraction 2/15 is the result of simplifying (-15 mod 7)/(15×3) = 6/45. The denominator of the second unit fraction, 8, is the result of rounding 15/2 up to the next larger integer, and the remaining fraction 1/120 is what is left from 7/15 after subtracting both 1/3 and 1/8.\n\nAs each expansion step reduces the numerator of the remaining fraction to be expanded, this method always terminates with a finite expansion; however, compared to ancient Egyptian expansions or to more modern methods, this method may produce expansions that are quite long, with large denominators. For instance, this method expands\nwhile other methods lead to the much better expansion\n\nSylvester's sequence 2, 3, 7, 43, 1807, ... can be viewed as generated by an infinite greedy expansion of this type for the number one, where at each step we choose the denominator formula_5 instead of formula_6. Truncating this sequence to \"k\" terms and forming the corresponding Egyptian fraction, e.g. (for \"k\" = 4)\nresults in the closest possible underestimate of 1 by any \"k\"-term Egyptian fraction (; ). That is, for example, any Egyptian fraction for a number in the open interval (1805/1806,1) requires at least five terms. describes an application of these closest-approximation results in lower-bounding the number of divisors of a perfect number, while describes applications in group theory.\n\nAny fraction \"x\"/\"y\" requires at most \"x\" terms in its greedy expansion. and examine the conditions under which the greedy method produces an expansion of \"x\"/\"y\" with exactly \"x\" terms; these can be described in terms of congruence conditions on \"y\".\n\n\n\nMore generally the sequence of fractions \"x\"/\"y\" that have \"x\"-term greedy expansions and that have the smallest possible denominator \"y\" for each \"x\" is\n\n and describe a method of finding an accurate approximation for the roots of a polynomial based on the greedy method. Their algorithm computes the greedy expansion of a root; at each step in this expansion it maintains an auxiliary polynomial that has as its root the remaining fraction to be expanded. Consider as an example applying this method to find the greedy expansion of the golden ratio, one of the two solutions of the polynomial equation \"P\"(\"x\") = \"x\" - x - 1 = 0. The algorithm of Stratemeyer and Salzer performs the following sequence of steps:\n\n\nContinuing this approximation process eventually produces the greedy expansion for the golden ratio,\n\nThe length, minimum denominator, and maximum denominator of the greedy expansion for all fractions with small numerators and denominators can be found in the On-Line Encyclopedia of Integer Sequences as sequences , , and , respectively. In addition, the greedy expansion of any irrational number leads to an infinite increasing sequence of integers, and the OEIS contains expansions of several well known constants. Some additional entries in the OEIS, though not labeled as being produced by the greedy algorithm, appear to be of the same type.\n\nIn general, if one wants an Egyptian fraction expansion in which the denominators are constrained in some way, it is possible to define a greedy algorithm in which at each step one chooses the expansion\nwhere \"d\" is chosen, among all possible values satisfying the constraints, as small as possible such that \"xd\" > \"y\" and such that \"d\" is distinct from all previously chosen denominators. For instance, the Engel expansion can be viewed as an algorithm of this type in which each successive denominator must be a multiple of the previous one. However, it may be difficult to determine whether an algorithm of this type can always succeed in finding a finite expansion. In particular, the odd greedy expansion of a fraction \"x\"/\"y\" is formed by a greedy algorithm of this type in which all denominators are constrained to be odd numbers; it is known that, whenever \"y\" is odd, there is a finite Egyptian fraction expansion in which all denominators are odd, but it is not known whether the odd greedy expansion is always finite.\n\n"}
{"id": "24958527", "url": "https://en.wikipedia.org/wiki?curid=24958527", "title": "Group testing", "text": "Group testing\n\nIn statistics and combinatorial mathematics, group testing is any procedure that breaks up the task of identifying certain objects into tests on groups of items, rather than on individual ones. First studied by Robert Dorfman in 1943, group testing is a relatively new field of applied mathematics that is an active area of research today and is useful for a wide range of practical applications.\n\nA familiar example of group testing involves a string of light bulbs connected in series, where exactly one of the bulbs is known to be broken. The objective is to find the broken bulb using the smallest number of tests (a test is when some of the bulbs are connected to a power supply). A simple approach is to test each bulb individually. However, when there are a large number of bulbs it would be much more efficient to pool the bulbs into groups. For example, in connecting the first half of the bulbs at once, it can be determined which half the broken bulb is in, ruling out half of the bulbs in just one test. When searching among six bulbs connected in series, this allows completion of the test in at worst three, and at best two steps, compared to a worst-case of six and best-case of one tests, when testing each bulb individually.\n\nSchemes for carrying out such group testing can be simple or complex and the tests involved at each stage may be different. Schemes in which the tests for the next stage depend on the results of the previous stages are called \"adaptive procedures\", while schemes designed so that all the tests are known beforehand are called \"non-adaptive procedures\". The structure of the scheme of the tests involved in a non-adaptive procedure is known as a \"pooling design\".\n\nGroup testing has many applications, including statistics, biology, computer science, medicine, engineering and cyber security. Modern interest in these testing schemes has been rekindled by the Human Genome Project.\nUnlike many areas of mathematics, the origins of group testing can be traced back to a single report written by a single person: Robert Dorfman. The motivation arose during the Second World War when the United States Public Health Service and the Selective service embarked upon a large-scale project to weed out all syphilitic men called up for induction. Testing an individual for syphilis involves drawing a blood sample from them and then analysing the sample to determine the presence or absence of syphilis. At the time, performing this test was expensive, and testing every soldier individually would have been very expensive and inefficient.\n\nSupposing there are formula_1 soldiers, this method of testing leads to formula_1 separate tests. If a large proportion of the people are infected then this method would be reasonable. However, in the more likely case that only a very small proportion of the men are infected, a much more efficient testing scheme can be achieved. The feasibility of a more effective testing scheme hinges on the following property: the soldiers can be pooled into groups, and in each group the blood samples can be combined together. The combined sample can then be tested to check if at least one soldier in the group has syphilis. This is the central idea behind group testing. If one or more of the soldiers in this group has syphilis, then a test is wasted (more tests need to be performed to find which soldier(s) it was). On the other hand, if no one in the pool has syphilis then many tests are saved, since every soldier in that group can be eliminated with just one test.\n\nThe items that cause a group to test positive are generally called \"defective items\" (these are the broken lightbulbs, syphilitic men, etc.). Often, the total number of items is denoted as formula_1 and formula_4 represents the number of defectives if it is assumed to be known.\n\nThere are two independent classifications for group-testing problems; every group-testing problem is either adaptive or non-adaptive, and either probabilistic or combinatorial.\n\nIn probabilistic models, the defective items are assumed to follow some probability distribution and the aim is to minimise the expected number of tests needed to identify the defectiveness of every item. On the other hand, with combinatorial group testing, the goal is to minimise the number of tests needed in a 'worst-case scenario' – that is, create a minmax algorithm – and no knowledge of the distribution of defectives is assumed.\n\nThe other classification, adaptivity, concerns what information can be used when choosing which items to group into a test. In general, the choice of which items to test can depend on the results of previous tests, as in the above lightbulb problem. An algorithm that proceeds by performing a test, and then using the result (and all past results) to decide which next test to perform, is called adaptive. Conversely, in non-adaptive algorithms, all tests are decided in advance. This idea can be generalised to multistage algorithms, where tests are divided into stages, and every test in the next stage must be decided in advance, with only the knowledge of the results of tests in previous stages.\nAlthough adaptive algorithms offer much more freedom in design, it is known that adaptive group-testing algorithms do not improve upon non-adaptive ones by more than a constant factor in the number of tests required to identify the set of defective items. In addition to this, non-adaptive methods are often useful in practice because one can proceed with successive tests without first analysing the results of all previous tests, allowing for the effective distribution of the testing process.\n\nThere are many ways to extend the problem of group testing. One of the most important is called \"noisy\" group testing, and deals with a big assumption of the original problem: that testing is error-free. A group-testing problem is called noisy when there is some chance that the result of a group test is erroneous (e.g. comes out positive when the test contained no defectives). The \"Bernoulli noise model\" assumes this probability is some constant, formula_5, but in general it can depend on the true number of defectives in the test and the number of items tested. For example, the effect of dilution can be modelled by saying a positive result is more likely when there are more defectives (or more defectives as a fraction of the number tested), present in the test. A noisy algorithm will always have a non-zero probability of making an error (that is, mislabeling an item).\n\nGroup testing can be extended by considering scenarios in which there are more than two possible outcomes of a test. For example, a test may have the outcomes formula_6 and formula_7, corresponding to there being no defectives, a single defective, or an unknown number of defectives larger than one. More generally, it is possible to consider the outcome-set of a test to be formula_8 for some formula_9.\n\nAnother extension is to consider geometric restrictions on which sets can be tested. The above lightbulb problem is an example of this kind of restriction: only bulbs that appear consecutively can be tested. Similarly, the items my be arranged in a circle, or in general, a net, where the tests are available paths on the graph. Another kind of geometric restriction would be on the maximum number of items that can be tested in a group, or the group sizes might have to be even and so on. In a similar way, it may be useful to consider the restriction that any given item can only appear in a certain number of tests.\n\nThere are endless ways to continue remixing the basic formula of group testing. The following elaborations will give an idea of some of the more exotic variants. In the 'good–mediocre–bad' model, each item is one of 'good', 'mediocre' or 'bad', and the result of a test is the type of the 'worst' item in the group. In threshold group testing, the result of a test is positive if the number of defective items in the group is greater than some threshold value or proportion. Group testing with inhibitors is a variant with applications in molecular biology. Here, there is a third class of items called inhibitors, and the result of a test is positive if it contains at least one defective and no inhibitors.\n\nThe concept of group testing was first introduced by Robert Dorfman in 1943 in a short report published in the Notes section of \"Annals of Mathematical Statistics\". Dorfman's report – as with all the early work on group testing – focused on the probabilistic problem, and aimed to use the novel idea of group testing to reduce the expected number of tests needed to weed out all syphilitic men in a given pool of soldiers. The method was simple: put the soldiers into groups of a given size, and use individual testing (testing items in groups of size one) on the positive groups to find which were infected. Dorfman tabulated the optimum group sizes for this strategy against the prevalence rate of defectiveness in the population.\n\nAfter 1943, group testing remained largely untouched for a number of years. Then in 1957, Sterrett produced an improvement on Dorfman’s procedure. This newer process starts by again performing individual testing on the positive groups, but stopping as soon as a defective is identified. Then, the remaining items in the group are tested together, since it is very likely that none of them are defective.\n\nThe first thorough treatment of group testing was given by Sobel and Groll in their formative 1959 paper on the subject. They described five new procedures – in addition to generalisations for when the prevalence rate is unknown – and for the most optimal one, they provided an explicit formula for the expected number of tests it would use. The paper also made the connection between group testing and information theory for the first time, as well as discussing several generalisations of the group-testing problem and providing some new applications of the theory.\n\nGroup testing was first studied in the combinatorial context by Li in 1962, with the introduction of \"Li’s formula_10-stage algorithm\". Li proposed an extension of\nDorfman's '2-stage algorithm' to an arbitrary number of stages that required no more\nthan formula_11 tests to be guaranteed to find formula_4 or fewer defectives among formula_1 items.\nThe idea was to remove all the items in negative tests, and divide the remaining items into groups as was done with the initial pool. This was to be done formula_14 times before performing individual testing.\n\nCombinatorial group testing in general was later studied more fully by Katona in 1973. Katona introduced the matrix representation of non-adaptive group-testing and produced a procedure for finding the defective in the non-adaptive 1-defective case in no more than formula_15tests, which he also proved to be optimal.\n\nIn general, finding optimal algorithms for adaptive combinatorial group testing is difficult, and although the computational complexity of group testing has not been determined, it is suspected to be hard in some complexity class. However, an important breakthrough occurred in 1972, with the introduction of the generalised binary-splitting algorithm. The generalised binary-splitting algorithm works by performing a binary search on groups that test positive, and is a simple algorithm that finds a single defective in no more than the information-lower-bound number of tests.\n\nIn scenarios where there are two or more defectives, the generalised binary-splitting algorithm still produces near-optimal results, requiring at most formula_16 tests above the information lower bound where formula_4 is the number of defectives. Considerable improvements to this were made in 2013 by Allemann, getting the required number of tests to less than formula_18 above the information lower bound when formula_19 and formula_20. This was achieved by changing the binary search in the binary-splitting algorithm to a complex set of sub-algorithms with overlapping test groups. As such, the problem of adaptive combinatorial group testing – with a known number or upper bound on the number of defectives – has essentially been solved, with little room for further improvement.\n\nThere is an open question as to when individual testing is minmax. Hu, Hwang and Wang showed in 1981 that individual testing is minmax when formula_21, and that it is not minmax when formula_22. It is currently conjectured that this bound is sharp: that is, individual testing is minmax if and only if formula_23. Some progress was made in 2000 by Ricccio and Colbourn, who showed that for large formula_1, individual testing is minmax when formula_25.\n\nTurning now to non-adaptive group testing, significant gains can be made by not requiring that the group-testing procedure be certain to succeed, but rather to have some non-zero probability of mis-labelling an item. In particular, it is known that zero-error algorithms require significantly more tests (as the number of defective items approaches the total number of items) than algorithms that allow asymptotically small probabilities of error.\n\nIn this vein, Chan \"et al.\" introduced COMP in 2011, a fast, explicit algorithm that requires no more than formula_26 tests to find up to formula_4 defectives in formula_1 items with a probability of error no more than formula_29. This is within a constant factor of the formula_30 lower bound. They also provided a generalisation of this algorithm to a simple noisy model, and similarly produced an explicit performance bound, which was again only a constant (dependent on the likelihood of a failed test) above the corresponding lower bound. In general, the number of tests required in the Bernoulli noise case is a constant factor larger than in the noiseless case.\n\nAn extension of the COMP algorithm that added additional post-processing steps was produced Aldridge, Baldassini and Johnson in 2014. They showed that performance guarantees of this new algorithm, called DD, strictly exceed those of COMP, and that DD is 'essentially optimal' in scenarios where formula_31, by comparing it to a hypothetical algorithm that defines a reasonable optimum. The performance of this hypothetical algorithm suggests that there is possible room for improvement when formula_32, as well as suggesting how much improvement this might be.\n\nThis section formally defines the notions and terms relating to group testing.\n\n\nformula_37 is intended to describe the (unknown) set of defective items. The key property of formula_37 is that it is an \"implicit input\". That is to say, there is no direct knowledge of what the entries of formula_37 are, other than that which can be inferred via some series of 'tests'. This leads on to the next definition.\n\n\nTherefore the goal of group testing is to come up with a method for choosing a 'short' series of tests that allow formula_37 to be determined, either exactly or with a high degree of certainty.\n\n\nSince it is always possible to resort to individual testing by setting formula_49 for each formula_50, it must be that that formula_51. Also, since any non-adaptive testing procedure can be written as an adaptive algorithm by simply performing all the tests without regard to their outcome, formula_52. Finally, when formula_53, there is at least one item whose defectiveness must be determined (by at least one test), and so formula_54.\n\nIn summary (when assuming formula_53), formula_56.\n\nA lower bound on the number of tests needed can be described using the notion of \"sample space\", denoted formula_57, which is simply the set of possible placements of defectives. For any group testing problem with sample space formula_57 and any group-testing algorithm, it can be shown that formula_59, where formula_60 is the minimum number of tests required to identify all defectives with a zero probability of error. This is called the \"information lower bound\". This bound is derived from the fact that after each test, formula_57 is split into two disjoint subsets, each corresponding to one of the two possible outcomes of the test.\n\nHowever, the information lower bound itself is usually unachievable, even for small problems. This is because the splitting of formula_57 is not arbitrary, since it must be realisable by some test.\n\nIn fact, the information lower bound can be generalised to the case where there is a non-zero probability that the algorithm makes an error. In this form, the theorem gives us an upper bound on the probability of success based on the number of tests. For any group-testing algorithm that performs formula_60 tests, the probability of success, formula_64, satisfies formula_65. This can be strengthened to: formula_66.\n\nAlgorithms for non-adaptive group testing consist of two distinct phases. First, it is decided how many tests to perform and which items to include in each test. In the second phase, often called the decoding step, the results of each group test are analysed to determine which items are likely to be defective. The first phase is usually encoded in a matrix as follows.\n\n\nThus each column of formula_71 represents an item and each row represents a test, with a formula_75 in the formula_76 entry indicating that the formula_77 test included the formula_78 item and a formula_79 indicating otherwise.\n\nAs well as the vector formula_37 (of length formula_1) that describes the unknown defective set, it is common to introduce the result vector, which describes the results of each test.\n\n\nWith these definitions, the non-adaptive problem can be reframed as follows: first a testing matrix is chosen, formula_71, after which the vector formula_89 is returned. Then the problem is to analyse formula_89 to find some estimate for formula_37.\n\nIn the simplest noisy case, where there is a constant probability, formula_5, that a group test will have an erroneous result, one considers a random binary vector, formula_93, where each entry has a probability formula_5 of being formula_75, and is formula_79 otherwise. The vector that is returned is then formula_97, with the usual addition on formula_98 (equivalently this is the element-wise XOR operation). A noisy algorithm must estimate formula_37 using formula_100 (that is, without direct knowledge of formula_89).\n\nThe matrix representation makes it possible to prove some bounds on non-adaptive group testing. The approach mirrors that of many deterministic designs, where formula_4-separable matrices are considered, as defined below.\n\n\nWhen formula_71 is a testing matrix, the property of being formula_4-separable (formula_106-separable) is equivalent to being able to distinguish between (up to) formula_4 defectives. However, it does not guarantee that this will be straightforward. A stronger property, called \"formula_4-disjunctness\" does.\n\n\nA useful property of formula_4-disjunct testing matrices is that, with up to formula_4 defectives, every non-defective item will appear in at least one test whose outcome is negative. This means there is a simple procedure for finding the defectives: just remove every item that appears in a negative test.\n\nUsing the properties of formula_4-separable and formula_4-disjunct matrices the following can be shown for the problem of identifying formula_4 defectives among formula_1 total items.\n\nThe generalised binary-splitting algorithm is an essentially-optimal adaptive group-testing algorithm that finds formula_4 or fewer defectives among formula_1 items as follows:\n\n\nThe generalised binary-splitting algorithm requires no more than formula_140 tests where\nformula_141.\n\nFor formula_142 large, it can be shown that formula_143, which compares favorably to the formula_144 tests required for Li's formula_10-stage algorithm. In fact, the generalised binary-splitting algorithm is close to optimal in the following sense. When formula_146 it can be shown that formula_147, where formula_148 is the information lower bound.\n\nNon-adaptive group-testing algorithms tend to assume that the number of defectives, or at least a good upper bound on them, is known. This quantity is denoted formula_4 in this section. If no bounds are known, there are non-adaptive algorithms with low query complexity that can help estimate formula_4.\n\nCombinatorial Orthogonal Matching Pursuit, or COMP, is a simple non-adaptive group-testing algorithm that forms the basis for the more complicated algorithms that follow in this section.\n\nFirst, each entry of the testing matrix is chosen i.i.d. to be formula_75 with probability formula_152 and formula_79 otherwise.\n\nThe decoding step proceeds column-wise (i.e. by item). If every test in which an item appears is positive, then the item is declared defective; otherwise the item is assumed to be non-defective. Or equivalently, if an item appears in any test whose outcome is negative, the item is declared non-defective; otherwise the item is assumed to be defective. An important property of this algorithm is that it never creates false negatives, though a false positive occurs when all locations with ones in the \"j\"-th column of formula_71 (corresponding to a non-defective item \"j\") are \"hidden\" by the ones of other columns corresponding to defective items.\n\nThe COMP algorithm requires no more than formula_155 tests to have an error probability less than or equal to formula_29. This is within a constant factor of the lower bound for the average probability of error above.\n\nIn the noisy case, one relaxes the requirement in the original COMP algorithm that the set of locations of ones in any column of formula_71 corresponding to a positive item be entirely contained in the set of locations of ones in the result vector. Instead, one allows for a certain number of “mismatches” – this number of mismatches depends on both the number of ones in each column, and also the noise parameter, formula_5. This noisy COMP algorithm requires no more than formula_159 tests to achieve an error probability at most formula_29.\n\nThe definite defectives method (DD) is an extension of the COMP algorithm that attempts to remove any false positives. Performance guarantees for DD have been shown to strictly exceed those of COMP.\n\nThe decoding step uses a useful property of the COMP algorithm: that every item that COMP declares non-defective is certainly non-defective (that is, there are no false negatives). It proceeds as follows.\n\n\nNote that steps 1 and 2 never make a mistake, so the algorithm can only make a mistake if it declares a defective item to be non-defective. Thus the DD algorithm can only create false negatives.\n\nSCOMP (Sequential COMP) is an algorithm that makes use of the fact that DD makes no mistakes until the last step, where it is assumed that the remaining items are non-defective. Let the set of declared defectives be formula_161. A positive test is called \"explained\" by formula_161 if it contains at least one item in formula_161. The key observation with SCOMP is that the set of defectives found by DD may not explain every positive test, and that every unexplained test must contain a hidden defective.\n\nThe algorithm proceeds as follows.\n\nIn simulations, SCOMP has been shown to perform close to optimally.\n\nThe generality of the theory of group testing lends it to many diverse applications, including clone screening, locating electrical shorts; high speed computer networks; medical examination, quantity searching, statistics; machine learning, DNA sequencing; cryptography; and data forensics. This section provides a brief overview of a small selection of these applications.\n\nA multiaccess channel is a communication channel that connects many users at once. Every user can listen and transmit on the channel, but if more than one user transmits at the same time, the signals collide, and are reduced to unintelligible noise. Multiaccess channels are important for various real-world applications, notably wireless computer networks and phone networks.\n\nA prominent problem with multiaccess channels is how to assign transmission times to the users so that their messages do not collide. A simple method is to give each user their own time slot in which to transmit, requiring formula_1 slots. (This is called \"time division multiplexing\", or TDM.) However, this is very inefficient, since it will assign transmission slots to users that may not have a message, and it is usually assumed that only a few users will want to transmit at any given time – otherwise a multiaccess channel is not practical in the first place.\n\nIn the context of group testing, this problem is usually tackled by dividing time into 'epochs' in the following way. A user is called 'active' if they have a message at the start of an epoch. (If a message is generated during an epoch, the user only becomes active at the start of the next one.) An epoch ends when every active user has successfully transmitted their message. The problem is then to find all the active users in a given epoch, and schedule a time for them to transmit (if they have not already done so successfully). Here, a test on a set of users corresponds to those users attempting a transmission. The results of the test are the number of users that attempted to transmit, formula_169 and formula_7, corresponding respectively to no active users, exactly one active user (message successful) or more than one active user (message collision). Therefore, using an adaptive group testing algorithm with outcomes formula_171, it can be determined which users wish to transmit in the epoch. Then, any user that has not yet made a successful transmission can now be assigned a slot to transmit, without wastefully assigning times to inactive users.\n\nMachine learning is a field of computer science that has many software applications such as DNA classification, fraud detection and targeted advertising. One of the main subfields of machine learning is the 'learning by examples' problem, where the task is to approximate some unknown function when given its value at a number of specific points. As outlined in this section, this function learning problem can be tackled with a group-testing approach.\n\nIn a simple version of the problem, there is some unknown function, formula_172 where formula_173, and formula_174 (using logical arithmetic: addition is logical OR and multiplication is logical AND). Here formula_175 is 'formula_4 sparse', which means that at most formula_177 of its entries are formula_75. The aim is to construct an approximation to formula_179 using formula_60 point evaluations, where formula_60 is as small as possible. (Exactly recovering formula_179 corresponds to zero-error algorithms, whereas formula_179 is approximated by algorithms that have a non-zero probability of error.)\n\nIn this problem, recovering formula_179 is equivalent to finding formula_175. Moreover, formula_186 if and only if there is some index, formula_1, where formula_188. Thus this problem is analogous to a group-testing problem with formula_4 defectives and formula_1 total items. The entries of formula_175 are the items, which are defective if they are formula_75, formula_193 specifies a test, and a test is positive if and only if formula_186.\n\nIn reality, one will often be interested in functions that are more complicated, such as formula_195, again where formula_173. Compressed sensing, which is closely related to group testing, can be used to solve this problem.\n\nIn compressed sensing, the goal is to reconstruct a signal, formula_197, by taking a number of measurements. These measurements are modelled as taking the dot product of formula_198 with a chosen vector. The aim is to use a small number of measurements, though this is typically not possible unless something is assumed about the signal. One such assumption (which is common) is that only a small number of entries of formula_198 are \"significant\", meaning that they have a large magnitude. Since the measurements are dot products of formula_198, the equation formula_201 holds, where formula_71 is a formula_203 matrix that describes the set of measurements that have been chosen and formula_204 is the set of measurement results. This construction shows that compressed sensing is a kind of 'continuous' group testing.\n\nThe primary difficulty in compressed sensing is identifying which entries are significant. Once that is done, there are a variety of methods to estimate the actual values of the entries. This task of identification can be approached with a simple application of group testing. Here a group test produces a complex number: the sum of the entries that are tested. The outcome of a test is called positive if it produces a complex number with a large magnitude, which, given the assumption that the significant entries are sparse, indicates that at least one significant entry is contained in the test.\n\nThere are explicit deterministic constructions for this type of combinatorial search algorithm, requiring formula_205 measurements. However, as with group-testing, these are sub-optimal, and random constructions (such as COMP) can often recover formula_179 sub-linearly in formula_207.\n\nData forensics is a field dedicated to finding methods for compiling digital evidence of a crime. Such crimes typically involve an adversary modifying the data, documents or databases of a victim, with examples including the altering of tax records, a virus hiding its presence, or an identity thief modifying personal data.\n\nA common tool in data forensics is the one-way cryptographic hash. This is a function that takes the data, and through a difficult-to-reverse procedure, produces a unique number called a hash. Hashes, which are often much shorter than the data, allow us to check if the data has been changed without having to wastefully store complete copies of the information: the hash for the current data can be compared with a past hash to determine if any changes have occurred. An unfortunate property of this method is that, although it is easy to tell if the data has been modified, there is no way of determining how: that is, it is impossible to recover which part of the data has changed.\n\nOne way to get around this limitation is to store more hashes – now of subsets of the data structure – to narrow down where the attack has occurred. However, to find the exact location of the attack with a naive approach, a hash would need to be stored for every datum in the structure, which would defeat the point of the hashes in the first place. (One may as well store a regular copy of the data.) Group testing can be used to dramatically reduce the number of hashes that need to be stored. A test becomes a comparison between the stored and current hashes, which is positive when there is a mismatch. This indicates that at least one edited datum (which is taken as defectiveness in this model) is contained in the group that generated the current hash.\n\nIn fact, the amount of hashes needed is so low that they, along with the testing matrix they refer to, can even be stored within the organisational structure of the data itself. This means that as far as memory is concerned the test can be performed 'for free'. (This is true with the exception of a master-key/password that is used to secretly determine the hashing function.)\n\n\n"}
{"id": "45088464", "url": "https://en.wikipedia.org/wiki?curid=45088464", "title": "Heliographic copier", "text": "Heliographic copier\n\nA heliographic copier or heliographic duplicator is an apparatus used in the world of reprography for making contact prints on paper from original drawings made with that purpose on tracing paper, parchment paper or any other transparent or translucent material using different procedures. In general terms some type of \"heliographic copier\" is used for making: Hectographic prints, Ferrogallic prints, Gel-lithographs or Silver halide prints. All of them, until a certain size, can be achieved using a contact printer with an appropriate lamp (ultraviolet, etc...) but for big engineering and architectural plans, the \"heliographic copiers\" used with the cyanotype and the diazotype technologies, are of the roller type, which makes them completely different from contact printers.\n\nIn the \"argot\" of engineers, architects and designers, the resulting plan copies coming from any type of \" heliographic copier\" no matter they were either blue or white, were traditionally called blueprints, name derived from the blue background color of the cyanotype technique, which was the previous process for obtaining blueprints, When the diazo based compounds changed the background color to white, in technical environments, -by tradition-, the name for copies of technical drawings remained Blueprint , although in English-speaking countries, it was intended, without much success, to change the name from \"Blueprint\" to \"Whiteprint\". Depending on the color or the line and background, the appropriate paper may be developed for blueprints (blue background), whiteprints (blue line and white background), black line (white background) and sepia, using the same machine and process. \n\nUsing the right compound some \"cyano copiers\" could be adapted to be used as \"diazo copiers\".\n\nThe light sensitivity of certain chemicals used in the cyanotype process, was already known when the English scientist and astronomer Sir John Herschel discovered the procedure in 1842 and several other related printing processes were patented by the 1890s . When Herschel developed the process, he considered it mainly as a means of reproducing notes and diagrams, as its use in blueprints.\n\nThis is a simple process for the reproduction of any light transmitting document. Engineers and architects used to draw their designs on cartridge paper; these were then traced by hand on to tracing paper using Indian ink, which were kept to be reproduced with the \"cyano-copie\"r whenever they were needed.\n\nIntroduction of the blueprint process eliminated the high expenses of photolithographic reproduction or of hand-tracing of original drawings. By the latter 1890s in American architectural offices, a blueprint was one-tenth the cost of a hand-traced reproduction. The blueprint process is still used for special artistic and photographic effects, on paper and fabrics.\n\nDifferent blueprint processes based on photosensitive ferric compounds have been used. The best known is probably a process using ammonium ferric citrate and potassium ferricyanide. In this procedure a distinctly blue compound is formed and the process is also known as cyanotype. The paper is impregnated with a solution of ammonium ferric citrate and dried. When the paper is illuminated a photoreaction turns the trivalent (ferric) iron into divalent (ferrous) iron. The image is then developed using a solution of potassium ferricyanide forming insoluble ferroferricyanide (Turnbell's blue identical to Prussian blue) with the divalent iron. Excess ammonium ferric citrate and potassium ferricyanide are then washed away.\n\nThe process of diazotype (Whiteprints) replaced the cyanotype process (blueprints) for reproducing architectural and engineering drawings because the process was simpler and involved fewer toxic chemicals. \n\nA blue-line print is not permanent and will fade if exposed to light for weeks or months, but a drawing print that lasts only a few months is sufficient for many purposes (test prints)\n\nThe different names \"blue-line copier\", \"whiteprint copier\" or \" diazo copier\", were given, due to the nature of the process, which consists in exposing to an ultraviolet light a previously sensitized paper with a component called diazo, and finally developing it in a bath (a solution of ammonia in water) which converts the parts not exposed to light, to a dark blue colour (blue-line) over an almost white background.\n\nA little smell of ammonia and a faintly purplish paper colour are the main characteristics of a whiteprint. The dark lines in the original are converted to a dark violet colour, while the white parts degrade to a light purplish colour. The back of the drawings is a cream colour in which the folds are degraded to a lighter colour.\n\nThe diazo copies are of different sizes and for this reason the diazo paper is obtainable in standard sizes that vary from 30 cm to 60 cm wide, after de process the copied paper can be cut to the desired size.\n\nThe paper used for the diazo copies is usually a bond paper or similar type, with a diazo coating sensitive to the UV light.\n\nThe original plan and the sensitized paper , are introduced, in perfect contact, within the copier rollers that pull and expose them to a source of ultraviolet light, typically a blacklight lamp, similar to the manual action to expose both sheets strongly bonded directly to the sunlight, and once exposed:\n\n\n\n\n"}
{"id": "945503", "url": "https://en.wikipedia.org/wiki?curid=945503", "title": "Hellenic Mathematical Society", "text": "Hellenic Mathematical Society\n\nThe Hellenic Mathematical Society (HMS) (Greek: Ελληνική Μαθηματική Εταιρεία) is a learned society which promotes the study of mathematics in Greece. It was founded in 1918, and published the \"Bulletin of the Greek Mathematical Society\".\n\nIt is a member of the European Mathematical Society. \n\n\n\n"}
{"id": "15095266", "url": "https://en.wikipedia.org/wiki?curid=15095266", "title": "Horst Sachs", "text": "Horst Sachs\n\nHorst Sachs (27 March 1927 – 25 April 2016) was a German mathematician, an expert in graph theory, a recipient of the Euler Medal (2000).\n\nHe earned the degree of Doctor of Science (Dr. rer. nat.) from the Martin-Luther-Universität Halle-Wittenberg in 1958. Following his retirement in 1992, he was professor emeritus at the Institute of Mathematics of the Technische Universität Ilmenau.\n\nHis encyclopedic book in spectral graph theory, \"Spectra of Graphs. Theory and Applications\" (with Dragos Cvetković and Michael Doob) has several editions and was translated in several languages.\n\nTwo theorems in graph theory bear his name. One of them relates the coefficients of the characteristic polynomial of a graph to certain structural features of the graph. Another one is a simple relation between the characteristic polynomials of a graph and its line graph.\n"}
{"id": "39078438", "url": "https://en.wikipedia.org/wiki?curid=39078438", "title": "Invariant convex cone", "text": "Invariant convex cone\n\nIn mathematics, an invariant convex cone is a closed convex cone in a Lie algebra of a connected Lie group that is invariant under inner automorphisms. The study of such cones was initiated by Ernest Vinberg and Bertram Kostant.\n\nFor a simple Lie algebra, the existence of an invariant convex cone forces the Lie algebra to have a Hermitian structure, i.e. the maximal compact subgroup has center isomorphic to the circle group. The invariant convex cone generated by a generator of the Lie algebra of the center is closed and is the minimal invariant convex cone (up to a sign). The dual cone with respect to the Killing form is the maximal invariant convex cone. Any intermediate cone is uniquely determined by its intersection with the Lie algebra of a maximal torus in a maximal compact subgroup. The intersection is invariant under the Weyl group of the maximal torus and the orbit of every point in the interior of the cone intersects the interior of the Weyl group invariant cone.\n\nFor the real symplectic group, the maximal and minimal cone coincide, so there is only one invariant convex cone. When one is properly contained in the other, there is a continuum of intermediate invariant convex cones.\n\nInvariant convex cones arise in the analysis of holomorphic semigroups in the complexification of the Lie group, first studied by Grigori Olshanskii. They are naturally associated with Hermitian symmetric spaces and their associated holomorphic discrete series. The semigroup is made up of those elements in the complexification which, when acting on the Hermitian symmetric space of compact type, leave invariant the bounded domain corresponding to the noncompact dual. The semigroup acts by contraction operators on the holomorphic discrete series; its interior acts by Hilbert–Schmidt operators. The unitary part of their polar decomposition is the operator corresponding to an element in the original real Lie group, while the positive part is the exponential of an imaginary multiple of the infinitesimal operator corresponding to an element in the maximal cone. A similar decomposition already occurs in the semigroup.\n\nThe oscillator semigroup of Roger Howe concerns the special case of this theory for the real symplectic group. Historically this has been one of the most important applications and has been generalized to infinite dimensions. This article treats in detail the example of the invariant convex cone for the symplectic group and its use in the study of the symplectic Olshanskii semigroup.\n\nThe Lie algebra of the symplectic group on R has a unique invariant convex cone. It is self-dual. The cone and its properties can be derived directly using the description of the symplectic Lie algebra provided by the Weyl calculus in quantum mechanics. Let the variables in R be \"x\", ..., \"x\", \"y\", ..., \"y\". Taking the standard inner product on R, the symplectic form corresponds to the matrix\n\nThe real polynomials on R form an infinite-dimensional Lie algebra under the Poisson bracket\n\nThe polynomials of degree ≤ 2 form a finite-dimensional Lie algebra with center the constant polynomials. The homogeneous polynomials of degree 2 form a Lie subalgebra isomorphic to the symplectic Lie algebra. The symplectic group acts naturally on this subalgebra by reparametrization and this yields the adjoint representation. Homogeneous polynomials of degree 2 on the other hand are just symmetric bilinear forms on R. They therefore correspond to symmetric 2\"n\" × 2\"n\" matrices. The Killing form on the Lie algebra is proportional to the trace form Tr \"AB\". \nThe positive definite symmetric bilinear forms give an open invariant convex cone with closure the set \"P\" of positive semi-definite symmetric bilinear forms. Because the Killing form is the trace form, the cone \"P\" is self-dual.\nAny positive symmetric bilinear form defines a new inner product on R. The symplectic from defines an invertible skew-adjoint operator \"T\" with respect to this inner product with –\"T\" a positive operator. An orthonormal basis can be chose so that \"T\" has 2 × 2 skew-symmetric matrices down the diagonal. Scaling the orthonormal basis, it follows that there is a symplectic basis for R diagonalizing the original positive symmetric bilinear form. Thus every positive symmetric bilinear form lies in the orbit of a diagonal form under the symplectic group.\n\nIf \"C\" is any other invariant convex cone then it is invariant under the closed subgroup \"U\" of the symplectic group consisting of orthogonal transformations commuting with \"J\". Identifying R with the complex inner product space C using the complex structure \"J\", \"U\" can be identified with \"U\"(\"n\"). Taking any non-zero point in \"C\". the average over \"U\" with respect to Haar measure lies in \"C\" and is non-zero. The corresponding quadratic form is a multiple of the standard inner product. Replacing \"C\" by –\"C\" this multiple can be taken to be positive. There is a copy of SL(2,R) in the symplectic group acting only on the variables \"x\" and \"y\". These operators can be used to transform\nquadratic forms and . By convexity it contains all diagonal positive symmetric bilinear forms. Since any positive symmetric bilinear form is in the orbit of a diagonal form, \"C\" contains the cone of non-negative symmetric bilinear forms. By duality the dual cone \"C\"* is contained in \"P\". If \"C\" is a proper cone, the previous argument shows that \"C\"* = \"P\" and hence that \"C\" = \"P\".\n\nThis argument shows that every positive definite symmetric form is in the orbit of a form with corresponding quadratic form\n\nwith \"a\" > 0. This corresponds to a cone in the Lie algebra of the (diagonal) maximal torus of \"U\".\n\nSince every element of \"P\" is diagonalizable, the stabilizer of a positive element in the symplectic group is contained in a conjugate of \"U\". On the other hand, if \"K\" is another compact subgroup of the symplectic group, averaging over Haar measure shows that it leaves invariant a positive element of \"P\". Thus \"K\" is contained in a conjugate of \"U\". It follows that \"U\" is a maximal compact subgroup of the symplectic group and that any other such subgroup must be a conjugate of \"U\".\n\nThe complex symplectic group acts by Möbius transformations on \"X\", the complex symmetric matrices with operator norm less than or equal to one. Representing an element as a 2 × 2 block matrix\nformula_4, the action is given by\n\nThere is a period 2 automorphism σ of the complex symplectic group with fixed point subgroup the real symplectic group. Then \"x\" = σ(x)^{-1} is an antiautomorphism of \"H\" which induces the inverse on the real symplectic group \"G\". If \"g\" is in the open Olshanski semigroup \"H\", let \"h\" = \"g\"\"g\". By Brouwer's fixed point theorem applied to the compact convex set \"X\", \"g\" has a fixed point in \"X\". Since \"g\" carries \"X\" into its interior, the fixed point is an interior point. Since \"G\" acts transitively on the interior of \"X\", post-multiplying by an element of \"G\" if necessary, it can be assumed that \"h\" fixes 0. Since \"h\" = \"h\", it follows that \"b\" = \"c\" = 0. Conjugating by an element in \"K\" ⊂ SU(1,1), \"a\" and \"d\" can be diagonalized. It has positive eigenvalues, so there is a unique positive diagonal operator \"h\" with square \"h\". By uniqueness (\"h\") = \"h\". Since \"h\" is diagonal, the theory for SU(1,1) and SL(2,C) acting on the unit disk in C shows that \"h\" lies in exp \"C\". On the other hand, \"k\" = \"g\" (\"h\") satisfies \"k\"\"k\" = 1 so that σ(\"k\") = \"k\". Thus \"k\" lies in \"G\" and therefore, using the invariance of \"C\", \"H\" admits the decomposition\n\nIn fact there is a similar decomposition for the closed Olshanski symplectic semigroup:\n\nMoreover, the map (\"g\",\"x\") ↦ \"g\" exp \"x\" is a homeomorphism.\n\nIn fact if \"X\" is in \"C\", it is diagonalizable with real eigenvalues. So that exp \"X\" has strictly positive eigenvalues. By continuity if \"X\" is in the closure of \"C\", it has real eigenvalues and exp \"X\" has strictly positive eigenvalues. Any invertible operator that is a limit of \nsuch exp \"X\" will also have strictly positive eigenvalues. By the holomorphic functional calculus the exponential map on the space of operators with real spectrum defines a homeomorphism onto the space of operators with strictly positive spectrum, with an analytic inverse given by the logarithm. It follows that \nformula_8 is closed in the complex symplectic group.\n\nIf \"g\" exp \"X\" tends to \"h\", then exp 2\"X\" tends to \"h\"\"h\". Since formula_8 is closed, \"h\"\"h\" = exp 2\"X\" for some \"X\" and hence \"h\" exp –\"X\" lies in \"G\". So the closure of\nformula_10 is closed and coincides with formula_11. Similarly if \"g\" exp \"X\" tends to \"g\" exp \"X\", then exp 2 \"X\" tends to exp 2\"X\". Hence \"X\" tends to \"X\". But then\nexp \"X\" tends to exp \"X\", so that \"g\" tends to \"g\".\n\nThe use of the Brouwer fixed-point theorem can be avoided by applying more direct fixed-point theorems for holomorphic mappings, such as the Earle–Hamilton fixed point theorem and its variants. In fact a Möbius transformation \"f\" taking {\"z\": ||\"z\"|| < 1, \"z\" = \"z\"} into a compact subset has a unique fixed point \"z\" with \"f\"(\"z\") → \"z\" for any \"z\".\n\n\"Uniqueness\" follows because, if \"f\" has a fixed point, after conjugating by an element of the real symplectic group, it can be assumed to be 0. Then \"f\" has the form \"f\"(\"z\") = \"az\"(1 + \"cz\")\"a\", where \"c\" = \"c\", with iterates\n\"f\"(\"z\") = \"a\"\"z\"(1 + \"c\"\"z\")(\"a\") with \"c\" = \"c\" + \"a\"\"ca\" + ⋅⋅⋅ + (\"a\")\"ca\". Here \"a\" and \"c\" all have operator norm less than one. Thus for ||\"z\"|| ≤ \"r\" < 1, \"f\"(\"z\") tends to 0 uniformly, so that in particular 0 is the unique fixed point and it is obtained by applying iterates of \"f\".\n\n\"Existence\" of a fixed point for \"f\" follows by noting that is an increasing sequence \"n\" such that \"f\" and \"f\" are both uniformly convergent on compacta, to \"h\" and \"g\" respectively. This follows because real symplectic transformations \"g\" can be chosen so that \"h\" = \"g\" ∘ \"f\" fixes 0, with a subsequence of \"g\"'s convergent precisely when the corresponding subsequence of \"f\"(0) is convergent. Since the transformations \"h\" can be written as \"h\"(\"z\") = \"a\"\"z\"(1 + \"b\"\"z\") (\"a\"), convergent subsequences can be chosen. By construction \"g\" ∘ \"h\" = \"h\". So points in the image of \"h\" are fixed by \"g\". Now \"g\" and \"h\" are either constant or have the form \"az\"(1 + \"cz\")\"a\" followed by a real symplectic transformation. Since the image of \"h\" is connected and a non-constant map has just one fixed point, the image of \"h\" is a single point \"z\", fixed by \"g\". Since \"g\" commutes with \"f\", \"f\"(\"z\") is also fixed by \"g\" and hence \"f\"(\"z\")= \"z\", so that \"z\" is a fixed point of \"f\".\n\nThe symplectic group acts transitively by Möbius transformations on the complex symmetric matrices with operator norm less than one. The open Olshanski semigroup consists of Möbius transformations in the complex symplectic group which take the space complex symmetric matrices of norm ≤ 1 into complex symmetric matrices of norm < 1. Its closure is a maximal proper semigroup in the complex symplectic group.\n\nIn two dimensions this follows from a general argument of which also applies in one dimension. Let \"G\" = SL(2,R) act by Möbius transformations on the extended real line and let \"H\" be the open semigroup consisting of transformations carrying [–1,1] into (–1,1). Its closure formula_11 is the closed semigroup of transformations carrying [–1,1] into itself. Maximality of formula_11 is proved by first showing that any strictly larger semigroup \"S\" contains an element \"g\" sending |\"t\"| < 1 onto |\"t\"| > 1. In fact if \"x\" is in \"S\" but not in formula_11, then there is an interval \"I\" in \"I\" = (–1,1) such that \"x\" \"I\" lies in [–1,1]. Then for some \"h\" in \"H\", \"I\" = \"hI\". Similarly \"yxI\" = [–1,1] for some \"y\" in \"H\". So \"g\" = \"yxh\" lies in \"S\" and sends \"I\" onto [–1,1]. It follows that \"g\" fixes \"I\", so that \"g\" lies in \"S\". If \"z\" lies in \"H\" then \"z\" \"g\" \"I\" contains \"g\" \"I\". Hence \"g\"\"z\" \"g\" lies in formula_11. So \"z\" lies in \"S\" and therefore \"S\" contains an open neighbourhood of \"1\". Hence \"S\" = SL(2,R).\n\nMaximality can be deduced for the Olshanski symplectic semigroup in SL(2,C) from the maximality of this semigroup in SL(2,R). It suffices to show that the closed semigroup contains SL(2,R), because the scaling transformations lie in the interior of the Olshanski symplectic semigroup. So if their inverses lie in the symplectic semigroup, it contains a neighbourhood of the identity and hence the whole of SL(2,C). If \"S\" is a semigroup properly containing the symplectic semigroup, it contains an element carrying the closed unit disk outside itself. Pre- and post-composing with elements of SU(1,1), it can be assumed that the element \"g\" of \"S\" carries 0 into \"r\" > 1. Precomposing with a scaling transformation, it can be assumed that \"g\" carries the closed unit disk onto a small neighbourhood of \"r\". Pre-composing with an element of SU(1,1), the inverse image of the real axis can be taken to be the diameter joining –1 and 1. But in that case, \"g\" must lie in SL(2,R). From the maximality result for semigroups in SL(2,R), \"S\" must contain SL(2,R) and hence must be the whole of SL(2,C).\n\nAutonne–Takagi factorization states that for any complex symmetric matrix \"M\", there is a unitary matrix \"U\" such that \"UMU\" is diagonal. If\"S\" is a semigroup properly containing the closure of the Olshanki semigroup, then it contains an element \"g\" such that \"z\" = \"g\"(0) with 1< ||\"z\"|| < ∞.\n\nIndeed, there is an embedding due to Harish-Chandra of the space of complex symmetric \"n\" by \"n\" matrices as a dense open subset of the compact Grassmannian of Langrangian subspaces of C. Morevoer this embedding is equivariant for the action of the real symplectic group. In fact, with the standard complex inner product on C, the Grassmannian of \"n\"-dimensional subspaces has a continuous transitive action of SL(2\"n\",C) and its maximal compact subpgroup SU(2\"n\"). It can be identified with the space of orthogonal rank \"n\" projections, a compact subspace of M(C).\nTaking coordinates (\"z\"...,\"z\",\"w\"...,\"w\") on C, the symplectic form is given by\n\nAn \"n\"-dimensional subspace \"U\" is called Lagrangian if \"B\" vanishes on \"U\". The Lagrangian subpaces form a closed subset of the Grassmannian on which the complex symplectic group and the unitary symplectic group act transitively. This is the Lagrangian Grassmannian. The subspace \"U\" formed of vectors with \"z\" = 0 is Lagrangian. The set of Langrangian subspaces \"U\" for which the restriction of the orthogonal projection onto \"U\" is an isomorphism forms an open dense subset Ω of the Lagrangian Grassmannian. Any such subspace has a canonical basis whose column vectors form a 2\"n\" by \"n\" matrix formula_17 where \"Z\" is a complex symmetric \"n\" by \"n\" matrix and \"I\" is the \"n\" by \"n\" identity matrix. Under this correspondence elements of the complex symplectic group, viewed as block matrices \nformula_18 act as Möbius transformations,\n\"g\"(\"Z\") = (\"AZ\" + \"B\")(\"CZ\" + \"D\"). The unit ball for the operator norm and its closure are left invariant under the corresponding real form of the symplectic group.\n\nIf an element \"g\" of the complex symplectic group does not lie in the closure of Olshanski semigroup, it must carry some point \"W\" of the open unit ball into the complement of its closure. If \"g\"(\"W\") does not lie in Ω then the image of a small ball about \"W\" must contain points with in Ω \nwith arbitrarily large operator norm. Precomposing \"g\" with a suitable element in \"G\", it follows that \"Z\" = \"g\"(0) will have operator norm greater than 1. If \"g\"(\"W\") already lies in Ω, it will also have operator norm greater than 1 and \"W\" can be then be taken to be 0 by precomposing with a suitable element of \"G\".\n\nPre-composing \"g\" with a scaling transformation and post-composing \"g\" with a unitary transformation, it can be assumed that \"g\"(0) is a diagonal matrix with entries λ ≥ 0 with \"r\" = λ > 0 and that the image of the unit ball is contained in a small ball around this point. The entries λ with \"i\" ≥ 2 can be separately scaled byelements of the Olshanki semigroup so that λ < 1; and then they can be sent to 0 by elements of \"G\" lying in commuting copies of SU(1,1). So \"g\"(0) is a diagonal matrix with entries \"r\", 0...,0, where \"r\" > 1.\n\n\n"}
{"id": "36594635", "url": "https://en.wikipedia.org/wiki?curid=36594635", "title": "Karen Parshall", "text": "Karen Parshall\n\nKaren Hunger Parshall (born 1955, Virginia; \"née\" Karen Virginia Hunger) is an American historian of mathematics.\n\nParshall studied Romance languages, such as French, and mathematics at the University of Virginia, where she earned her master's degree in mathematics in 1978. She earned her PhD in 1982 in the history of mathematics from the University of Chicago under the direction of the historian Allen G. Debus (1926–2009) and the mathematician Israel Herstein. The subject of her dissertation is the history of the theory of algebras, especially the work of Joseph Wedderburn (\"The contributions of J. H. M. Wedderburn to the theory of algebras, 1900–1910\").\n\nFrom 1982 to 1987 Parshall was an assistant professor at Sweet Briar College and in 1987/88 at the University of Illinois at Urbana-Champaign. Since 1988 she has taught the history of mathematics, and also mathematics and the history of science, at the University of Virginia, where she became in 1988 an assistant professor, in 1993 an associate professor and in 1999 a professor. She was a visiting professor at the Australian National University in Canberra, at the École des Hautes Etudes en Sciences Sociales (1985 and 2010) and at the Pierre and Marie Curie University in Paris (2016).\n\nParshall's academic specialty is the development of mathematics in the USA in the late 19th century and early 20th century (particularly the Chicago School). As one example, she has studied the work of Leonard Dickson, who was greatly influenced by contact with German mathematicians such as Felix Klein at the time of the Columbian Exposition of 1893. She has also focused on the history of algebra. She edited the correspondence of James Joseph Sylvester published by Oxford University Press and wrote a biography of Sylvester.\n\nIn the academic year 1996/97 Parshall was a Guggenheim Fellow. In 1994 she was an invited speaker at the International Congress of Mathematicians (ICM) in Zürich (\"Mathematics in National Contexts (1875–1900): An International Overview\"). Since 2002 she has been a corresponding member of the Académie internationale d’histoire des sciences in Paris. From 1996 to 1999, she was editor of the journal \"Historia Mathematica\". Parshall was in the governing body of the History of Science Society and from 1998 to 2001 of the American Mathematical Society (AMS).\n\nIn 2012, she became a fellow of the American Mathematical Society.\nShe is the 2018 winner of the Albert Leon Whiteman Memorial Prize of the American Mathematical Society \"for her outstanding work in the history of mathematics, and in particular, for her work on the evolution of mathematics in the USA and on the history of algebra, as well as for her substantial contribution to the international life of her discipline through students, editorial work, and conferences.\"\n\n\n\n"}
{"id": "48312994", "url": "https://en.wikipedia.org/wiki?curid=48312994", "title": "Kernel-independent component analysis", "text": "Kernel-independent component analysis\n\nIn statistics, kernel-independent component analysis (kernel ICA) is an efficient algorithm for independent component analysis which estimates source components by optimizing a \"generalized variance\" contrast function, which is based on representations in a reproducing kernel Hilbert space. Those contrast functions use the notion of mutual information as a measure of statistical independence.\n\nKernel ICA is based on the idea that correlations between two random variables can be represented in a reproducing kernel Hilbert space (RKHS), denoted by formula_1, associated with a feature map formula_2 defined for a fixed formula_3. The formula_1-correlation between two random variables formula_5 and formula_6 is defined as\n\nwhere the functions formula_8 range over formula_1 and\n\nfor fixed formula_11. Note that the reproducing property implies that formula_12 for fixed formula_3 and formula_14. It follows then that the formula_1-correlation between two independent random variables is zero.\n\nThis notion of formula_1-correlations is used for defining \"contrast\" functions that are optimized in the Kernel ICA algorithm. Specifically, if formula_17 is a prewhitened data matrix, that is, the sample mean of each column is zero and the sample covariance of the rows is the formula_18 dimensional identity matrix, Kernel ICA estimates a formula_18 dimensional orthogonal matrix formula_20 so as to minimize finite-sample formula_1-correlations between the columns of formula_22.\n"}
{"id": "408555", "url": "https://en.wikipedia.org/wiki?curid=408555", "title": "Kissing number problem", "text": "Kissing number problem\n\nIn geometry, a kissing number is defined as the number of non-overlapping unit spheres that can be arranged such that they each touch a common unit sphere. For a lattice packing the kissing number is the same for every sphere, but for an arbitrary sphere packing the kissing number may vary from one sphere to another. Other names for kissing number that have been used are Newton number (after the originator of the problem), and contact number.\n\nIn general, the kissing number problem seeks the maximum possible kissing number for \"n\"-dimensional spheres in (\"n\" + 1)-dimensional Euclidean space. Ordinary spheres correspond to two-dimensional closed surfaces in three-dimensional space.\n\nFinding the kissing number when centers of spheres are confined to a line (the one-dimensional case) or a plane (two-dimensional case) is trivial. Proving a solution to the three-dimensional case, despite being easy to conceptualise and model in the physical world, eluded mathematicians until the mid-20th century. Solutions in higher dimensions are considerably more challenging, and only a handful of cases have been solved exactly. For others investigations have determined upper and lower bounds, but not exact solutions.\n\nIn one dimension, the kissing number is 2:\n\nIn two dimensions, the kissing number is 6:\n\nProof: Consider a circle with center \"C\" that is touched by circles with centers \"C\", \"C\", ... Consider the rays \"C\" \"C\". These rays all emanate from the same center \"C\", so the sum of angles between adjacent rays is 360°.\n\nAssume by contradiction that there are more than six touching circles. Then at least two adjacent rays, say \"C\" \"C\" and \"C\" \"C\", are separated by an angle of less than 60°. The segments \"C C\" have the same length – 2\"r\" – for all \"i\". Therefore, the triangle \"C\" \"C\" \"C\" is isosceles, and its third side – \"C\" \"C\" – has a side length of less than 2\"r\". Therefore, the circles 1 and 2 intersect – a contradiction.\n\nIn three dimensions, the kissing number is 12, but the correct value was much more difficult to establish than in dimensions one and two. It is easy to arrange 12 spheres so that each touches a central sphere, but there is a lot of space left over, and it is not obvious that there is no way to pack in a 13th sphere. (In fact, there is so much extra space that any two of the 12 outer spheres can exchange places through a continuous movement without any of the outer spheres losing contact with the center one.) This was the subject of a famous disagreement between mathematicians Isaac Newton and David Gregory. Newton correctly thought that the limit was 12; Gregory thought that a 13th could fit. Some incomplete proofs that Newton was correct were offered in the nineteenth century, most notably one by Reinhold Hoppe, but the first correct proof (according to Brass, Moser, and Pach) did not appear until 1953.\n\nThe twelve neighbors of the central sphere correspond to the maximum bulk coordination number of an atom in a crystal lattice in which all atoms have the same size (as in a chemical element). A coordination number of 12 is found in a cubic close-packed or a hexagonal close-packed structure.\n\nIn four dimensions, it was known for some time that the answer was either 24 or 25. It is easy to produce a packing of 24 spheres around a central sphere (one can place the spheres at the vertices of a suitably scaled 24-cell centered at the origin). As in the three-dimensional case, there is a lot of space left over—even more, in fact, than for \"n\" = 3—so the situation was even less clear. In 2003, Oleg Musin proved the kissing number for \"n\" = 4 to be 24, using a subtle trick.\n\nThe kissing number in \"n\" dimensions is unknown for \"n\" > 4, except for \"n\" = 8 (240), and \"n\" = 24 (196,560). The results in these dimensions stem from the existence of highly symmetrical lattices: the \"E\" lattice and the Leech lattice.\n\nIf arrangements are restricted to \"lattice\" arrangements, in which the centres of the spheres all lie on points in a lattice, then this restricted kissing number is known for \"n\" = 1 to 9 and \"n\" = 24 dimensions. For 5, 6, and 7 dimensions the arrangement with the highest known kissing number found so far is the optimal lattice arrangement, but the existence of a non-lattice arrangement with a higher kissing number has not been excluded.\n\nThe following table lists some known bounds on the kissing number in various dimensions. The dimensions in which the kissing number is known are listed in boldface.\n\nThe kissing number problem can be generalized to the problem of finding the maximum number of non-overlapping congruent copies of any convex body that touch a given copy of the body. There are different versions of the problem depending on whether the copies are only required to be congruent to the original body, translates of the original body, or translated by a lattice. For the regular tetrahedron, for example, it is known that both the lattice kissing number and the translative kissing number are equal to 18, whereas the congruent kissing number is at least 56.\n\nThere are several approximation algorithms on intersection graphs where the approximation ratio depends on the kissing number. For example, there is \na polynomial-time 10-approximation algorithm to find a maximum non-intersecting subset of a set of rotated unit squares.\n\nThe kissing number problem can be stated as the existence of a solution to a set of inequalities. Let formula_1 be a set of \"N\" \"D\"-dimensional position vectors of the centres of the spheres. The condition that this set of spheres can lie round the centre sphere without overlapping is:\n\nThus the problem for each dimension can be expressed in the existential theory of the reals. However, general methods of solving problems in this form take at least exponential time which is why this problem has only been solved up to 4 dimensions. By adding additional variables, formula_3 this can be converted to a single quartic equation in \"N\"(\"N\"-1)/2 + \"DN\" variables:\n\nTherefore, to solve the case in \"D\" = 5 dimensions and \"N\" = 40+1 vectors would be equivalent to determining the existence of real solutions to a quartic polynomial in 1025 variables. For the \"D\" = 24 dimensions and \"N\" = 196560+1, the quartic would have 19,322,732,544 variables. An alternative statement in terms of distance geometry is given by the distances squared formula_5 between then \"m\" and \"n\" sphere.\n\nThis must be supplemented with the condition that the Cayley–Menger Determinant is zero for any set of points which forms an (\"D\"+1) simplex in \"D\" dimensions, since that volume must be zero. Setting formula_7 gives a set of simultaneous polynomial equations in just \"y\" which must be solved for real values only. The two methods, being entirely equivalent, have various different uses. For example, in the second case one can randomly alter the values of the \"y\" by small amounts to try and minimise the polynomial in terms of the \"y\".\n\n\n"}
{"id": "94158", "url": "https://en.wikipedia.org/wiki?curid=94158", "title": "Lagrange inversion theorem", "text": "Lagrange inversion theorem\n\nIn mathematical analysis, the Lagrange inversion theorem, also known as the Lagrange–Bürmann formula, gives the Taylor series expansion of the inverse function of an analytic function.\n\nSuppose \"z\" is defined as a function of \"w\" by an equation of the form\n\nwhere \"f\" is analytic at a point \"a\" and . Then it is possible to \"invert\" or \"solve\" the equation for \"w\", expressing it in the form formula_2 given by a power series\nwhere\n\nThe theorem further states that this series has a non-zero radius of convergence, i.e., formula_5 represents an analytic function of \"z\" in a neighbourhood of formula_6. This is also called reversion of series.\n\nIf the assertions about analyticity are omitted, the formula is also valid for formal power series and can be generalized in various ways. It can be formulated for functions of several variables, it can be extended to provide a ready formula for \"F\"(\"g\"(\"z\")) for any analytic function \"F\", and it can be generalized to the case \"f\" '(\"a\") = 0, where the inverse \"g\" is a multivalued function.\n\nThe theorem was proved by Lagrange and generalized by Hans Heinrich Bürmann, both in the late 18th century. There is a straightforward derivation using complex analysis and contour integration; the complex formal power series version is a consequence of knowing the formula for polynomials, so the theory of analytic functions may be applied. Actually, the machinery from analytic function theory enters only in a formal way in this proof, in that what is really needed is some property of the formal residue, and a more direct formal proof is available.\n\nIf \"f\" is a formal power series, then the above formula does not give the coefficients of the compositional inverse series \"g\" directly in terms for the coefficients of the series \"f\". If one can express the functions \"f\" and \"g\" in formal power series as\n\nwith \"f = 0\" and \"f ≠ 0\", then an explicit form of inverse coefficients can be given in term of Bell polynomials:\n\nwith formula_9    formula_10   and   formula_11  being the rising factorial. \n\nWhen \"f = 1\", the last formula can be interpreted in terms of the faces of associahedra \n\nwhere formula_13 for each face formula_14 of the associahedron formula_15.\n\nFor instance, the algebraic equation of degree \"p\" \ncan be solved for \"x\" by means of the Lagrange inversion formula for the function \"f\"(\"x\") = \"x\" − \"x\", yielding to a formal series solution\n\nBy convergence tests, this series is in fact convergent for |\"z\"| ≤ (\"p\" − 1)\"p\", which is also the largest disk in which a local inverse to \"f\" can be defined.\n\nThere is a special case of Lagrange inversion theorem that is used in combinatorics and applies when formula_18 for some analytic formula_19 with formula_20 Take formula_21 to obtain formula_22 Then for the inverse formula_5 (satisfying formula_24), we have\n\nwhich can be written alternatively as\n\nwhere formula_28 is an operator which extracts the coefficient of formula_29 in the Taylor series of a function of w.\n\nA useful generalization of the formula is known as the Lagrange–Bürmann formula:\n\nwhere is an arbitrary analytic function.\n\nSometimes, the derivative \"H' (w)\" can be quite complicated. A simpler version of the formula replaces \"H' (w)\" with \"H (w)(1-φ'(w)/φ(w))\" to get \n\nwhich involves \"φ'(w)\" instead of \"H' (w)\".\n\nThe Lambert \"W\" function is the function formula_32 that is implicitly defined by the equation\n\nWe may use the theorem to compute the Taylor series of formula_32 at formula_35\nWe take formula_36 and formula_37 Recognizing that\nthis gives\n\nThe radius of convergence of this series is formula_40 (this example refers to the principal branch of the Lambert function).\n\nA series that converges for larger \"z\" (though not for all \"z\") can also be derived by series inversion. The function formula_41 satisfies the equation\n\nThen formula_43 can be expanded into a power series and inverted. This gives a series for formula_44:\n\nformula_46 can be computed by substituting formula_47 for \"z\" in the above series. For example, substituting −1 for \"z\" gives the value of formula_48.\n\nConsider the set formula_49 of unlabelled binary trees.\nAn element of formula_49 is either a leaf of size zero, or a root node with two subtrees. Denote by formula_51 the number of binary trees on \"n\" nodes.\n\nNote that removing the root splits a binary tree into two trees of smaller size. This yields the functional equation on the generating function formula_52:\n\nNow let formula_54, one has thus formula_55 Now apply the theorem with formula_56\n\nWe conclude that formula_51 is the Catalan number.\n\nIn the Laplace-Erdelyi theorem that gives the asymptotic approximation for Laplace-type integrals, the function inversion is taken as a crucial step.\n\n\n"}
{"id": "54888506", "url": "https://en.wikipedia.org/wiki?curid=54888506", "title": "Lia Bronsard", "text": "Lia Bronsard\n\nLia Bronsard (b. 14 March 1963) is a Canadian mathematician, the 2010 winner of the Krieger–Nelson Prize and the former president of the Canadian Mathematical Society. She is a professor of mathematics at McMaster University. In her research, she has used geometric flows to model the interface dynamics of reaction–diffusion systems.\nOther topics in her research include pattern formation, grain boundaries, and vortices in superfluids.\n\nBronsard is originally from Québec. She did her undergraduate studies at the Université de Montréal, graduating in 1983,\nand earned her PhD in 1988 from New York University under the supervision of Robert V. Kohn.\nAfter short-term positions at Brown University, the Institute for Advanced Study, and Carnegie Mellon University, she moved to McMaster in 1992. She was president of the Canadian Mathematical Society for 2014–2016.\n\n\n"}
{"id": "30563979", "url": "https://en.wikipedia.org/wiki?curid=30563979", "title": "Lusin's separation theorem", "text": "Lusin's separation theorem\n\nIn descriptive set theory and mathematical logic, Lusin's separation theorem states that if \"A\" and \"B\" are disjoint analytic subsets of Polish space, then there is a Borel set \"C\" in the space such that \"A\" ⊆ \"C\" and \"B\" ∩ \"C\" = ∅. It is named after Nikolai Luzin, who proved it in 1927.\n\nThe theorem can be generalized to show that for each sequence (\"A\") of disjoint analytic sets there is a sequence (\"B\") of disjoint Borel sets such that \"A\" ⊆ \"B\" for each \"n\". \n\nAn immediate consequence is Suslin's theorem, which states that if a set and its complement are both analytic, then the set is Borel.\n\n"}
{"id": "46573575", "url": "https://en.wikipedia.org/wiki?curid=46573575", "title": "McLaughlin graph", "text": "McLaughlin graph\n\nIn the mathematical field of graph theory, the McLaughlin graph is a strongly regular graph with parameters (275,112,30,56), and is the only such graph.\n\nThe group theorist Jack McLaughlin discovered that the automorphism group of this graph had a subgroup of index 2 which was a previously undiscovered finite simple group, now called the McLaughlin sporadic group.\n\nThe automorphism group has rank 3, meaning that its point stabilizer subgroup divides the remaining 274 vertices into two orbits. Those orbits contain 112 and 162 vertices. The former is the colinearity graph of the generalized quadrangle GQ(3,9). The latter is a strongly regular graph called the local McLaughlin graph.\n"}
{"id": "28310124", "url": "https://en.wikipedia.org/wiki?curid=28310124", "title": "Modular decomposition", "text": "Modular decomposition\n\nIn graph theory, the modular decomposition is a decomposition of a graph into subsets of vertices called modules. A module is a generalization of a connected component of a graph. Unlike connected components, however, one module can be a proper subset of another. Modules therefore lead to a recursive (hierarchical) decomposition of the graph, instead of just a partition.\n\nThere are variants of modular decomposition for undirected graphs and directed graphs. For each undirected graph, this decomposition is unique.\n\nThis notion can be generalized to other structures (for example directed graphs) and is useful to design efficient algorithms for the recognition of some graph classes, for finding transitive orientations of comparability graphs, for optimization problems on graphs, and for graph drawing.\n\nAs the notion of modules has been rediscovered in many areas, \"modules\" have also been called \"autonomous sets\", \"homogeneous sets\", \"intervals\", and \"partitive sets\". Perhaps the earliest reference to them, and the first description of modular quotients and the graph decomposition they give rise to appeared in (Gallai 1967).\n\nA \"module\" of a graph is a generalization of a connected component. A connected component has the property that it is a set formula_1 of vertices such that every member of formula_1 is a non-neighbor of every vertex not in formula_1. (It is a union of connected components if and only if it has this property.) More generally, formula_1 is a module if, for each vertex formula_5, either every member of formula_1 is a non-neighbor of formula_7 or every member of formula_1 is a neighbor of formula_7.\n\nEquivalently, formula_1 is a module if all members of formula_1 have the same set of neighbors among vertices not in formula_1.\n\nContrary to the connected components, the modules of a graph are the same as the modules of its complement, and modules can be \"nested\": one module can be a proper subset of another. Note that the set formula_13 of vertices of a graph is a module, as are its one-element subsets and the empty set; these are called the trivial modules. A graph may or may not have other modules. A graph is called prime if all of its modules are trivial.\n\nDespite these differences, modules preserve a desirable property of connected components, which is that many properties of the subgraph formula_14 induced by a connected component formula_1 are independent of the rest of the graph. A similar phenomenon also applies to the subgraphs induced by modules.\n\nThe modules of a graph are therefore of great algorithmic interest. A set of nested modules, of which the modular decomposition is an example, can be used to guide the recursive solution of many combinatorial problems on graphs, such as recognizing and transitively orienting comparability graphs, recognizing and finding permutation representations of permutation graphs, recognizing whether a graph is a cograph and finding a certificate of the answer to the question, recognizing interval graphs and finding interval representations for them, defining distance-hereditary graphs (Spinrad, 2003) and for graph drawing (Papadoupoulos, 2006). They play an important role in Lovász's celebrated proof of the perfect graph theorem (Golumbic, 1980).\n\nFor recognizing distance-hereditary graphs and circle graphs, a further generalization of modular decomposition, called the split decomposition, is especially useful (Spinrad, 2003).\n\nTo avoid the possibility of ambiguity in the above definitions, we give the following formal definitions of modules. \nformula_16.\nformula_17 is a module of formula_18 if:\n\nformula_30, formula_13 and all the singletons formula_32 for formula_33 are modules, and are called trivial modules. A graph is prime if all its modules are trivial. Connected components of a graph formula_18, or of its complement graph are also modules of formula_18.\n\nformula_19 is a strong module of a graph formula_18 if it does not overlap any other module of formula_18:\nformula_39 module of formula_18, either formula_41 or formula_42 or formula_43.\n\nIf formula_1 and formula_45 are disjoint modules, then it is easy to see that either every member of formula_1 is a neighbor of every element of formula_45, or no member of formula_1 is adjacent to any member of formula_45. Thus, the relationship between two disjoint modules is either \"adjacent\" or \"nonadjacent\". No relationship intermediate between these two extremes can exist.\n\nBecause of this, modular partitions of formula_13 where each partition class is a module are of particular interest. Suppose formula_51 is a modular partition. Since the partition classes are disjoint, their adjacencies constitute a new graph, a quotient graph formula_52, whose vertices are the members of formula_51. That is, each vertex of formula_52 is a module of G, and the adjacencies of these modules are the edges of formula_52.\n\nIn the figure below, vertex 1, vertices 2 through 4, vertex 5, vertices 6 and 7, and vertices 8 through 11 are a modular partition. In the upper right diagram, the edges between these sets depict the quotient given by this partition, while the edges internal to the sets depict the corresponding factors.\n\nThe partitions formula_56 and formula_57 are the trivial modular partitions. formula_58 is just the one-vertex graph, while formula_59. Suppose formula_1 is a nontrivial module. Then formula_1 and the one-elements subsets of formula_62 are a nontrivial modular partition of formula_13. Thus, the existence of \"any\" nontrivial modules implies the existence of nontrivial modular partitions. In general, many or all members of formula_51 can be nontrivial modules.\n\nIf formula_51 is a nontrivial modular partition, then formula_52 is a compact representation of all the edges that have endpoints in different partition classes of formula_51. For each partition class formula_1 in formula_51, the subgraph formula_14 induced by formula_1 is called a factor and gives a representation of all edges with both endpoints in formula_1. Therefore, the edges of formula_18 can be reconstructed given only the quotient graph formula_52 and its factors. The term \"prime\" graph comes from the fact that a prime graph has only trivial quotients and factors.\n\nWhen formula_14 is a factor of a modular quotient formula_52, it is possible that formula_14 can be recursively decomposed into factors and quotients. Each level of the recursion gives rise to a quotient. As a base case, the graph has only one vertex. Collectively, formula_18 can be reconstructed inductively by reconstructing the factors from the bottom up, inverting the steps of the decomposition by combining factors with the quotient at each level.\n\nIn the figure below, such a recursive decomposition is represented by a tree that depicts one way of recursively decomposing factors of an initial modular partition into smaller modular partitions.\n\nA way to recursively decompose a graph into factors and quotients may not be unique. (For example, all subsets of the vertices of a complete graph are modules, which means that there are many different ways of decomposing it recursively.) Some ways may be more useful than others.\n\nFortunately, there exists such a recursive decomposition of a graph that implicitly represents all ways of decomposing it; this is the modular decomposition. It is itself a way of decomposing a graph recursively into quotients, but it subsumes all others. The decomposition depicted in the figure below is this special decomposition for the given graph.\n\nThe following is a key observation in understanding the modular decomposition:\n\nIf formula_1 is a module of formula_18 and formula_45 is a subset of formula_1, then formula_45 is a module of formula_18, if and only if it is a module of formula_14.\n\nIn (Gallai, 1967), Gallai defined the modular decomposition recursively on a graph with vertex set formula_13, as follows:\n\n\nThe final tree has one-element sets of vertices of formula_18 as its leaves, due to the base case. A set formula_45 of vertices of formula_18 is a module if and only if it is a node of the tree or a union of children of a series or parallel node. This implicitly gives all modular partitions of formula_13. It is in this sense that the modular decomposition tree \"subsumes\" all other ways of recursively decomposing formula_18 into quotients.\n\nA data structure for representing the modular decomposition tree should support the operation that inputs a node and returns the set of vertices of formula_18 that the node represents. An obvious way to do this is to assign to each node a list of the formula_116 vertices of formula_18 that it represents. Given a pointer to a node, this structure could return the set of vertices of formula_18 that it represents in formula_119 time. However, this data structure would require formula_120 space in the worst case. \n\nAn formula_121-space alternative that matches this performance is obtained by representing the modular decomposition tree using any standard formula_121 rooted-tree data structure and labeling each leaf with the vertex of formula_18 that it represents. The set represented by an internal node formula_7 is given by the set of labels of its leaf descendants. It is well known that any rooted tree with formula_116 leaves has at most formula_126 internal nodes. One can use a depth-first search starting at formula_7 to report the labels of leaf-descendants of formula_7 in formula_119 time.\n\nEach node formula_1 is a set of vertices of formula_18 and, if formula_1 is an internal node, the set formula_51 of children of formula_1 is a partition of formula_1 where each partition class is a module. They therefore induce the quotient formula_136 in formula_14. The vertices of this quotient are the elements of formula_51, so formula_136 can be represented by installing edges among the children of formula_1. If formula_45 and formula_142 are two members of formula_51 and formula_144 and formula_145, then formula_23 and formula_7 are adjacent in formula_18 if and only if formula_45 and formula_142 are adjacent in this quotient. For any pair formula_151 of vertices of formula_18, this is determined by the quotient at children of the least common ancestor of formula_153 and formula_32 in the modular decomposition tree. Therefore, the modular decomposition, labeled in this way with quotients, gives a complete representation of formula_18.\n\nMany combinatorial problems can be solved on formula_18 by solving the problem separately on each of these quotients. For example, formula_18 is a comparability graph if and only if each of these quotients is a comparability graph (Gallai, 67; Möhring, 85). Therefore, to find whether a graph is a comparability graph, one need only find whether each of the quotients is. In fact, to find a transitive orientation of a comparability graph, it suffices to transitively orient each of these quotients of its modular decomposition (Gallai, 67; Möhring, 85). A similar phenomenon applies for permutation graphs, (McConnell and Spinrad '94), interval graphs (Hsu and Ma '99), perfect graphs, and other graph classes. Some important combinatorial optimization problems on graphs can be solved using a similar strategy (Möhring, 85).\n\nCographs are the graphs that only have parallel or series nodes in their modular decomposition tree.\n\nThe first polynomial algorithm to compute the modular decomposition tree of a graph was published in 1972 (James, Stanton & Cowan 1972) and now linear algorithms are available (McConnell & Spinrad 1999, Tedder et al. 2007, Cournier & Habib 1994).\n\nModular decomposition of directed graphs can be done in linear time .\n\nWith a small number of simple exceptions, every graph with a nontrivial modular decomposition also has a skew partition .\n\n\n"}
{"id": "44779201", "url": "https://en.wikipedia.org/wiki?curid=44779201", "title": "Nine-point conic", "text": "Nine-point conic\n\nIn geometry, the nine-point conic of a complete quadrangle is a conic that passes through the three diagonal points and the six midpoints of sides of the complete quadrangle.\n\nThe nine-point conic was described by Maxime Bôcher in 1892. The better-known nine-point circle is an instance of Bôcher's conic. The nine-point hyperbola is another instance.\n\nBôcher used the four points of the complete quadrangle as three vertices of a triangle with one independent point:\nThe conic is an ellipse if \"P\" lies in the interior of \"ABC\" or in one of the regions of the plane separated from the interior by two sides of the triangle, otherwise the conic is a hyperbola. Bôcher notes that when \"P\" is the orthocenter, one obtains the nine-point circle, and when \"P\" is on the circumcircle of \"ABC\", then the conic is an equilateral hyperbola.\n\nIn 1912 Maud Minthorn showed that the nine-point conic is the locus of the center of a conic through four given points.\n\n\n"}
{"id": "341086", "url": "https://en.wikipedia.org/wiki?curid=341086", "title": "Non-monotonic logic", "text": "Non-monotonic logic\n\nA non-monotonic logic is a formal logic whose consequence relation is not monotonic. In other words, non-monotonic logics are devised to capture and represent defeasible inferences (cf. defeasible reasoning), i.e., a kind of inference in which reasoners draw tentative conclusions, enabling reasoners to retract their conclusion(s) based on further evidence.\nMost studied formal logics have a monotonic consequence relation, meaning that adding a formula to a theory never produces a reduction of its set of consequences. Intuitively, monotonicity indicates that learning a new piece of knowledge cannot reduce the set of what is known. A monotonic logic cannot handle various reasoning tasks such as reasoning by default (consequences may be derived only because of lack of evidence of the contrary), abductive reasoning (consequences are only deduced as most likely explanations), some important approaches to reasoning about knowledge (the ignorance of a consequence must be retracted when the consequence becomes known), and similarly, belief revision (new knowledge may contradict old beliefs).\n\nAbductive reasoning is the process of deriving the most likely explanations of the known facts. An abductive logic should not be monotonic because the most likely explanations are not necessarily correct. For example, the most likely explanation for seeing wet grass is that it rained; however, this explanation has to be retracted when learning that the real cause of the grass being wet was a sprinkler. Since the old explanation (it rained) is retracted because of the addition of a piece of knowledge (a sprinkler was active), any logic that models explanations is non-monotonic.\n\nIf a logic includes formulae that mean that something is not known, this logic should not be monotonic. Indeed, learning something that was previously not known leads to the removal of the formula specifying that this piece of knowledge is not known. This second change (a removal caused by an addition) violates the condition of monotonicity. A logic for reasoning about knowledge is the autoepistemic logic.\n\nBelief revision is the process of changing beliefs to accommodate a new belief that might be inconsistent with the old ones. In the assumption that the new belief is correct, some of the old ones have to be retracted in order to maintain consistency. This retraction in response to an addition of a new belief makes any logic for belief revision to be non-monotonic. The belief revision approach is alternative to paraconsistent logics, which tolerate inconsistency rather than attempting to remove it.\n\nProof-theoretic formalization of a non-monotonic logic begins with adoption of certain non-monotonic rules of inference, and then prescribes contexts in which these non-monotonic rules may be applied in admissible deductions. This typically is accomplished by means of fixed-point equations that relate the sets of premises and the sets of their non-monotonic conclusions. Default logic and autoepistemic logic are the most common examples of non-monotonic logics that have been formalized that way.\n\nModel-theoretic formalization of a non-monotonic logic begins with restriction of the semantics of a suitable monotonic logic to some special models, for instance, to minimal models, and then derives the set of non-monotonic rules of inference, possibly with some restrictions in which contexts these rules may be applied, so that the resulting deductive system is sound and complete with respect to the restricted semantics. Unlike some proof-theoretic formalizations that suffered from well-known paradoxes and were often hard to evaluate with respect of their consistency with the intuitions they were supposed to capture, model-theoretic formalizations were paradox-free and left little, if any, room for confusion about what non-monotonic patterns of reasoning they covered. Examples of proof-theoretic formalizations of non-monotonic reasoning, which revealed some undesirable or paradoxical properties or did not capture the desired intuitive comprehensions, that have been successfully (consistent with respective intuitive comprehensions and with no paradoxical properties, that is) formalized by model-theoretic means include first-order circumscription, closed-world assumption, and autoepistemic logic.\n\n\n\n"}
{"id": "40814966", "url": "https://en.wikipedia.org/wiki?curid=40814966", "title": "Nonabelian cohomology", "text": "Nonabelian cohomology\n\nIn mathematics, a nonabelian cohomology is any cohomology with coefficients in a nonabelian group, a sheaf of nonabelian groups or even in a topological space.\n\nIf homology is thought of as the abelianization of homotopy (cf. Hurewicz theorem), then the nonabelian cohomology may be thought of as a dual of homotopy groups.\n\nSee: Nonabelian Poincare Duality (Lecture 8)\n\n\n"}
{"id": "1457116", "url": "https://en.wikipedia.org/wiki?curid=1457116", "title": "Numéraire", "text": "Numéraire\n\nThe numéraire (or numeraire) is a basic standard by which value is computed. In mathematical economics it is a tradeable economic entity in terms of whose price the relative prices of all other tradeables are expressed. In a monetary economy, acting as the numéraire is one of the functions of money, to serve as a unit of account: to provide a common benchmark relative to which the worths of various goods and services are measured. Using a numeraire, whether monetary or some consumable good, facilitates value comparisons when only the relative prices are relevant, as in general equilibrium theory. When economic analysis refers to a particular good as the numéraire, one says that all other prices are normalized by the price of that good. For example, if a unit of good \"g\" has twice the market value of a unit of the numeraire, then the (relative) price of \"g\" is 2. Since the value of one unit of the numeraire relative to one unit of itself is 1, the price of the numeraire is always 1.\n\nIn a financial market with traded securities, one may use a change of numéraire to price assets. For instance, if formula_1 is the price at time formula_2 of $1 that was invested in the money market at time 0, then the Fundamental Theorem of Asset Pricing says that all assets (say formula_3), priced in terms of the money market, are martingales with respect to the risk-neutral measure, (say formula_4). That is\n\nNow, suppose that formula_6 is another strictly positive traded asset (and hence a martingale when priced in terms of the money market). Then, we can define a new probability measure formula_7 by the Radon–Nikodym derivative\n\nThen, by using the abstract Bayes' Rule it can be shown that formula_3 is a martingale under formula_7 when priced in terms of the new numéraire, formula_11:\n\nThis technique has many important applications in LIBOR and swap market models, as well as commodity markets. Jamshidian (1989) first used it in the context of the Vasicek model for interest rates in order to calculate bond options prices. Geman, El Karoui and Rochet (1995) introduced the general formal framework for the change of numéraire technique. See for example Brigo and Mercurio (2001) for a change of numéraire toolkit.\n\n\n"}
{"id": "11870817", "url": "https://en.wikipedia.org/wiki?curid=11870817", "title": "Octacube (sculpture)", "text": "Octacube (sculpture)\n\nThe Octacube is a large, steel sculpture of a mathematical object: the 24-cell or \"octacube\". Because a real 24-cell is four-dimensional, the artwork is actually a projection into the three-dimensional world. \"Octacube\" has very high intrinsic symmetry, which matches features in chemistry (molecular symmetry) and physics (quantum field theory).\n\nThe sculpture was designed by Adrian Ocneanu, a mathematics professor at Pennsylvania State University. The university's machine shop spent over a year completing the intricate metal-work. \"Octacube\" was funded by an alumna in memory of her husband, Kermit Anderson, who died in the September 11 attacks. The sculpture is displayed in the lobby of Penn State's math department.\n\nThe \"Octacube's\" metal skeleton measures about 6 feet (2 meters) in all three dimensions. It is a complex arrangement of unpainted, tri-cornered flanges. The base is a 3-foot (1 meter) high granite block, with some engraving.\n\nThe artwork was designed by Adrian Ocneanu, a Penn State mathematics professor. He supplied the specifications for the sculpture's 96 triangular pieces of stainless steel and for their assembly. Fabrication was done by Penn State's machine shop, led by Jerry Anderson. The work took over a year, involving bending and welding as well as cutting. Discussing the construction, Ocneanu said: It's very hard to make 12 steel sheets meet perfectly—and conformally—at each of the 23 vertices, with no trace of welding left. The people who built it are really world-class experts and perfectionists—artists in steel.\nBecause of the reflective metal at different angles, the appearance is pleasantly strange. In some cases, the mirror-like surfaces create an illusion of transparency by showing reflections from unexpected sides of the structure. The sculpture's mathematician creator commented: When I saw the actual sculpture, I had quite a shock. I never imagined the play of light on the surfaces. There are subtle optical effects that you can feel but can't quite put your finger on.\n\nThe Platonic solids are three-dimensional shapes with special, high, symmetry. They are the next step up in dimension from the two-dimensional regular polygons (squares, equilateral triangles, etc.). The five Platonic solids are the tetrahedron (4 faces), cube (6 faces), octahedron (8 faces), dodecahedron (12 faces), and icosahedron (20 faces). They have been known since the time of the Ancient Greeks and valued for their aesthetic appeal and philosophical, even mystical, import. (See also the \"Timaeus\", a dialogue of Plato.)\n\nIn higher dimensions, the counterparts of the Platonic solids are the regular polytopes. These shapes were first described in the mid-19th century by a Swiss mathematician, Ludwig Schläfli. In four dimensions, there are six of them: the pentachoron (5-cell), tesseract (8-cell), hexadecachoron (16-cell), octacube (24-cell), hecatonicosachoron (120-cell), and the hexacosichoron (600-cell).\n\nThe 24-cell consists of 24 octahedrons, joined in 4-dimensional space. The 24-cell's vertex figure (the 3-D shape formed when a 4-D corner is cut off) is a cube. Despite its suggestive name, the octacube is not the 4-D analog of either the octahedron or the cube. In fact, it is the only one of the six 4-D regular polytopes that lacks a corresponding Platonic solid.\n\nOcneanu explains the conceptual challenge in working in the fourth dimension: \"Although mathematicians can work with a fourth dimension abstractly by adding a fourth coordinate to the three that we use to describe a point in space, a fourth spatial dimension is difficult to visualize.\"\n\nAlthough it is impossible to see or make 4-dimensional objects, it is possible to map them into lower dimensions to get some impressions of them. An analogy for converting the 4-D 24-cell into its 3-D sculpture is cartographic projection, where the surface of the 3-D Earth (or a globe) is reduced to a flat 2-D plane (a portable map). This is done either with light 'casting a shadow' from the globe onto the map or with some mathematical transformation. Many different types of map projection exist: the familiar rectangular Mercator (used for navigation), the circular gnomonic (first projection invented), and several others. All of them have limitations in that they show some features in a distorted manner—'you can't flatten an orange peel without damaging it'—but they are useful visual aids and convenient references.\n\nIn the same manner that the exterior of the Earth is a 2-D skin (bent into the third dimension), the exterior of a 4-dimensionsal shape is a 3-D space (but folded through hyperspace, the fourth dimension). However, just as the surface of Earth's globe can not be mapped onto a plane without some distortions, neither can the exterior 3-D shape of the 24-cell 4-D hyper-shape. In the image on the right a 24-cell is shown projected into space as a 3-D object (and then the image is a 2-D rendering of it, with perspective to aid the eye). Some of the distortions:\n\nTo map the 24-cell, Ocneanu uses a related projection which he calls \"windowed radial stereographic projection\". As with the stereographic projection, there are curved lines shown in 3-D space. Instead of using semitransparent surfaces, \"windows\" are cut into the faces of the cells so that interior cells can be seen. Also, only 23 vertices are physically present. The 24th vertice \"occurs at infinity\" because of the projection; what one sees is the 8 legs and arms of the sculpture diverging outwards from the center of the 3-D sculpture.\n\nThe \"Octacube\" sculpture has very high symmetry. The stainless steel structure has the same amount of symmetry as a cube or an octahedron. The artwork can be visualized as related to a cube: the arms and legs of the structure extend to the corners. Imagining an octahedron is more difficult; it involves thinking of the faces of the visualized cube forming the corners of an octahedron. The cube and octahedron have the same amount and type of symmetry: octahedral symmetry, called O (order 48) in mathematical notation. Some, but not all, of the symmetry elements are\n\nMany molecules have the same symmetry as the \"Octacube\" sculpture. The organic molecule, cubane (CH) is one example. The arms and legs of the sculpture are similar to the outward projecting hydrogen atoms. Sulfur hexafluoride (or any molecule with exact octahedral molecular geometry) also shares the same symmetry although the resemblance is not as similar.\n\nThe \"Octacube\" also shows parallels to concepts in theoretical physics. Creator Ocneanu researches mathematical aspects of quantum field theory (QFT). The subject has been described by a Fields medal winner, Ed Witten, as the most difficult area in physics. Part of Ocneanu's work is to build theoretical, and even physical, models of the symmetry features in QFT. Ocneanu cites the relationship of the inner and outer halves of the structure as analogous to the relationship of spin 1/2 particles (e.g. electrons) and spin 1 particles (e.g. photons).\n\n\"Octacube\" was commissioned and funded by Jill Anderson, a 1965 PSU math grad, in memory of her husband, Kermit, another 1965 math grad, who was killed in the 9-11 terrorist attacks. Summarizing the memorial, Anderson said: I hope that the sculpture will encourage students, faculty, administrators, alumnae, and friends to ponder and appreciate the wonderful world of mathematics. I also hope that all who view the sculpture will begin to grasp the sobering fact that everyone is vulnerable to something terrible happening to them and that we all must learn to live one day at a time, making the very best of what has been given to us. It would be great if everyone who views the \"Octacube\" walks away with the feeling that being kind to others is a good way to live.\n\nAnderson also funded a math scholarship in Kermit's name, at the same time the sculpture project went forward.\n\nA more complete explanation of the sculpture, including how it came to be made, how its construction was funded and its role in mathematics and physics, has been made available by Penn State. In addition, Ocneanu has provided his own commentary.\n\nArtists:\n\nMath:\n\nNotes\nCitations\n"}
{"id": "41162601", "url": "https://en.wikipedia.org/wiki?curid=41162601", "title": "Opaque forest problem", "text": "Opaque forest problem\n\nIn computational geometry, The opaque forest problem can be stated as follows: \"\"Given a convex polygon \"C\" in the plane, determine the minimal forest \"T\" of closed, bounded line segments such that every line through \"C\" also intersects \"T\". \"T\" is said to be the \"opaque forest\", or \"barrier\" of \"C\". \"C\" is said to be the \"coverage\" of \"T\". While any forest that covers \"C\" is a barrier of \"C\", we wish to find the one with shortest length.\n\nIt may be the case that \"T\" is constrained to be strictly interior or exterior to \"C\". In this case, we specifically refer to a barrier as \"interior\" or \"exterior\". Otherwise, the barrier is assumed to have no constraints on its location.\n\nThe opaque forest problem was originally introduced by Mazurkiewicz in 1916. \nSince then, not much progress has been made with respect to the original problem. There does not exist \"any\" verified general solution to the problem. In fact, the optimal solution for even simple fixed inputs such as the unit square or equilateral triangle are unknown. There exist conjectured optimal solutions to both of these instances, but we currently lack the tooling to prove that they are optimal.\nWhile general solutions to the problem have been claimed by several individuals,\nthey either haven't been peer reviewed or have been demonstrated to be incorrect.\n\nGiven a convex polygon \"C\" with perimeter \"p\" it is possible to bound the value of the optimal solution in terms of \"p\". These bounds are individually tight in general, but due to the various shapes that can be provided, are quite loose with respect to each other.\n\nIn general, one can prove that \"p\"/2 ≤ |OPT| ≤ \"p\".\n\nTracing the perimeter of \"C\" is always sufficient to cover it. Therefore, \"p\" is an upper bound for any \"C\". For internal barriers, this bound is tight in the limiting case of when \"C\" is a circle; every point \"q\" on the perimeter of the circle must be contained in \"T\", or else a tangent of \"C\" can be drawn through \"q\" without intersecting \"T\". However, for any other convex polygon, this is suboptimal, meaning that this is not a particularly good upper bound for most inputs.\n\nFor most inputs, a slightly better upper bound for convex polygons can be found in the length of the perimeter, less the longest edge (which is the minimum spanning tree). Even better, one can take the minimum Steiner tree of the vertices of the polygon. For internal barriers, the only way to improve this bound is to make a disconnected barrier.\n\nVarious proofs of the lower bound can be found in .\nTo see that this is tight in general, one can consider the case of a stretching out a very long and thin rectangle. Any opaque forest for this shape must be at least as long as the rectangle, or else there is a hole through which vertical lines can pass through. As the rectangle becomes longer and thinner, this value approaches \"p\"/2. Therefore, this bound is tight in general. However, for any shape that actually has a positive area, some extra length will need to be allocated to span the shape in other directions. Hence this is not a particularly good lower bound for most inputs.\n\nFor the unit square, these bounds evaluate to 2 and 4 respectively. However, slightly improved lower bounds of 2 + 10 for barriers that satisfy a locality constraint, and 2 + 10 for internal barriers, have been shown.\n\nDue to the difficulty faced in finding an optimal barrier for even simple examples, it has become very desirable to find a barrier that approximates the optimal within some constant factor.\n\nMost barrier constructions are such that the fact that it covers the desired region is guaranteed. However, given an arbitrary barrier \"T\", it would be desirable to confirm that it covers the desired area \"C\".\n\nAs a simple first pass, one can compare the convex hulls of \"C\" and \"T\". \"T\" covers at most its convex hull, so if the convex hull of \"T\" does not strictly contain \"C\", then it cannot possibly cover \"T\". This provides a simple O(\"n\" log \"n\") first-pass algorithm for verifying a barrier. If \"T\" consists of a single connected component, then it covers exactly its convex hull, and this algorithm is sufficient. However, If \"T\" contains more than one connected component, it may cover less. So this test is not sufficient in general.\n\nThe problem of determining exactly what regions any given forest \"T\" consisting of \"m\" connected components and \"n\" line segments actually covers, can be solved in Θ(\"m\"\"n\") time.\nThe basic procedure for doing this is simple: first, simplify each connected component by replacing it with its own convex hull. Then, for vertex \"p\" of each convex hull, perform a circular plane-sweep with a line centered at \"p\", tracking when the line is or isn't piercing a convex hull (not including the point \"p\" itself). The orientations of the sweep-line during which an intersection occurred produce a \"sun\" shaped set of points (a collection of double-wedges centred at \"p\"). The coverage of \"T\" is exactly the intersection of all of these \"suns\" for all choices of \"p\".\n\nWhile this algorithm is worst-case optimal, it often does a lot of useless work when it doesn't need to. In particular, when the convex hulls are first computed, many of them may overlap. If they do, they can be replaced by their combined convex hull without loss of generality. If after merging all overlapping hulls, a single barrier has resulted, then the more general algorithm need not be run; the coverage of a barrier is at most its convex hull, and we have just determined that its coverage \"is\" its convex hull. The merged hulls can be computed in O(\"n\"log\"n\") time. Should more than one hull remain, the original algorithm can be run on the new simplified set of hulls, for a reduced running time.\n\n"}
{"id": "19975724", "url": "https://en.wikipedia.org/wiki?curid=19975724", "title": "Open Options Corporation", "text": "Open Options Corporation\n\nOpen Options Corporation is a privately owned business strategy consulting company that specializes in applied game theory and business war games where there are multiple stakeholders who can all influence the final outcome of a particular situation.\n\nOpen Options’ original software development was funded by the CIA through a company called Waterloo Engineering Software. The software was designed to help the American government better anticipate the collapse of the Soviet Union into many different breakaway republics. The software was based on academic research by Dr. Niall Fraser from the University of Waterloo. Dr. Fraser has written several books on game theory including Mathematical Modeling of Resolutions of Conflict and Conflict Analysis: Models and Resolutions.\n\nOpen Options uses a type of game theory called Ordinal Non-cooperative Game Theory which is a less well-known sub-section of game theory. However, this approach and proprietary software enables Open Options to analyze millions of possible outcomes.\n\nbusiness ideas\n\n"}
{"id": "38707083", "url": "https://en.wikipedia.org/wiki?curid=38707083", "title": "Order-7 heptagonal tiling", "text": "Order-7 heptagonal tiling\n\nIn geometry, the order-7 heptagonal tiling is a regular tiling of the hyperbolic plane. It has Schläfli symbol of {7,7}, constructed from seven heptagons around every vertex. As such, it is self-dual.\n\n\n\n"}
{"id": "872374", "url": "https://en.wikipedia.org/wiki?curid=872374", "title": "Parastatistics", "text": "Parastatistics\n\nIn quantum mechanics and statistical mechanics, parastatistics is one of several alternatives to the better known particle statistics models (Bose–Einstein statistics, Fermi–Dirac statistics and Maxwell–Boltzmann statistics). Other alternatives include anyonic statistics and braid statistics, both of these involving lower spacetime dimensions.\n\nConsider the operator algebra of a system of \"N\" identical particles. This is a *-algebra. There is an \"S\" group (symmetric group of order \"N\") acting upon the operator algebra with the intended interpretation of permuting the \"N\" particles. Quantum mechanics requires focus on observables having a physical meaning, and the observables would have to be invariant under all possible permutations of the \"N\" particles. For example, in the case \"N\" = 2, \"R\" − \"R\" cannot be an observable because it changes sign if we switch the two particles, but the distance between the two particles : |\"R\" − \"R\"| is a legitimate observable.\n\nIn other words, the observable algebra would have to be a *-subalgebra invariant under the action of \"S\" (noting that this does not mean that every element of the operator algebra invariant under \"S\" is an observable). Therefore, we can have different superselection sectors, each parameterized by a Young diagram of \"S\".\n\nIn particular:\n\n\nA paraboson field of order \"p\", formula_1 where if \"x\" and \"y\" are spacelike-separated points, formula_2 and formula_3 if formula_4 where [,] is the commutator and {,} is the anticommutator. Note that this disagrees with the spin-statistics theorem, which is for bosons and not parabosons. There might be a group such as the symmetric group \"S\" acting upon the \"φ\"s. Observables would have to be operators which are invariant under the group in question. However, the existence of such a symmetry is not essential.\n\nA parafermion field formula_5 of order \"p\", where if \"x\" and \"y\" are spacelike-separated points, formula_6 and formula_7 if formula_4. The same comment about observables would apply together with the requirement that they have even grading under the grading where the \"ψ\"s have odd grading.\n\nThe \"parafermionic and parabosonic algebras\" are generated by elements that obey the commutation and anticommutation relations. They generalize the usual \"fermionic algebra\" and the \"bosonic algebra\" of quantum mechanics. The Dirac algebra and the Duffin–Kemmer–Petiau algebra appear as special cases of the parafermionic algebra for order p=1 and p=2, respectively.\n\nNote that if \"x\" and \"y\" are spacelike-separated points, \"φ\"(\"x\") and \"φ\"(\"y\") neither commute nor anticommute unless \"p\"=1. The same comment applies to \"ψ\"(\"x\") and \"ψ\"(\"y\"). So, if we have \"n\" spacelike separated points \"x\", ..., \"x\",\n\ncorresponds to creating \"n\" identical parabosons at \"x\"..., \"x\". Similarly,\n\ncorresponds to creating \"n\" identical parafermions. Because these fields neither commute nor anticommute\n\nand\n\ngives distinct states for each permutation π in \"S\".\n\nWe can define a permutation operator formula_13 by\n\nand\n\nrespectively. This can be shown to be well-defined as long as formula_13 is only restricted to states spanned by the vectors given above (essentially the states with \"n\" identical particles). It is also unitary. Moreover, formula_17 is an operator-valued representation of the symmetric group \"S\" and as such, we can interpret it as the action of \"S\" upon the \"n\"-particle Hilbert space itself, turning it into a unitary representation.\n\nQCD can be reformulated using parastatistics with the quarks being parafermions of order 3 and the gluons being parabosons of order 8. Note this is different from the conventional approach where quarks always obey anticommutation relations and gluons commutation relations.\n\nH.S. (Bert) Green is credited with the invention/discovery of parastatistics in 1953.\n\n"}
{"id": "585271", "url": "https://en.wikipedia.org/wiki?curid=585271", "title": "Perfect group", "text": "Perfect group\n\nIn mathematics, more specifically in the area of modern algebra known as group theory, a group is said to be perfect if it equals its own commutator subgroup, or equivalently, if the group has no nontrivial abelian quotients (equivalently, its abelianization, which is the universal abelian quotient, is trivial). In symbols, a perfect group is one such that \"G\" = \"G\" (the commutator subgroup equals the group), or equivalently one such that \"G\" = {1} (its abelianization is trivial).\n\nThe smallest (non-trivial) perfect group is the alternating group \"A\". More generally, any non-abelian simple group is perfect since the commutator subgroup is a normal subgroup with abelian quotient. Conversely, a perfect group need not be simple; for example, the special linear group over the field with 5 elements, SL(2,5) (or the binary icosahedral group which is isomorphic to it) is perfect but not simple (it has a non-trivial center containing formula_1).\n\nMore generally, a quasisimple group (a perfect central extension of a simple group) which is a non-trivial extension (i.e., not a simple group itself) is perfect but not simple; this includes all the insoluble non-simple finite special linear groups SL(\"n\",\"q\") as extensions of the projective special linear group PSL(\"n\",\"q\") (SL(2,5) is an extension of PSL(2,5), which is isomorphic to \"A\"). Similarly, the special linear group over the real and complex numbers is perfect, but the general linear group GL is never perfect (except when trivial or over F, where it equals the special linear group), as the determinant gives a non-trivial abelianization and indeed the commutator subgroup is SL.\n\nA non-trivial perfect group, however, is necessarily not solvable; and 4 divides its order (if finite), moreover, if 8 does not divide the order, then 3 does.\n\nEvery acyclic group is perfect, but the converse is not true: \"A\" is perfect but not acyclic (in fact, not even superperfect), see . In fact, for \"n\" ≥ 5 the alternating group \"A\" is perfect but not superperfect, with \"H\"(\"A\", Z) = Z/2 for \"n\" ≥ 8.\n\nAny quotient of a perfect group is perfect. A non-trivial finite perfect group which is not simple must then be an extension of at least one smaller simple non-abelian group. But it can be the extension of more than one simple group. In fact, the direct product of perfect groups is also perfect.\n\nEvery perfect group \"G\" determines another perfect group \"E\" (its universal central extension) together with a surjection \"f:E\" → \"G\" whose kernel is in the center of \"E,\"\nsuch that \"f\" is universal with this property. The kernel of \"f\" is called the Schur multiplier of \"G\" because it was first studied by Schur in 1904; it is isomorphic to the\nhomology group \"H(G)\".\n\nIn the plus construction of algebraic K-theory, if we consider the group formula_2 for a commutative ring formula_3, then the subgroup of elementary matrices formula_4 forms a perfect subgroup.\n\nAs the commutator subgroup is \"generated\" by commutators, a perfect group may contain elements that are products of commutators but not themselves commutators. Øystein Ore proved in 1951 that the alternating groups on five or more elements contained only commutators, and made the conjecture that this was so for all the finite non-abelian simple groups. Ore's conjecture was finally proven in 2008. The proof relies on the classification theorem.\n\nA basic fact about perfect groups is Grün's lemma from : the quotient of a perfect group by its center is centerless (has trivial center).\n\nProof: If \"G\" is a perfect group, let \"Z\" and \"Z\" denote the first two terms of the upper central series of \"G\" (i.e., \"Z\" is the center of \"G\", and \"Z\"/\"Z\" is the center of \"G\"/\"Z\"). If \"H\" and \"K\" are subgroups of \"G\", denote the commutator of \"H\" and \"K\" by [\"H\", \"K\"] and note that [\"Z\", \"G\"] = 1 and [\"Z\", \"G\"] ⊆ \"Z\", and consequently (the convention that [\"X\", \"Y\", \"Z\"] = [[\"X\", \"Y\"], \"Z\"] is followed):\n\nBy the [[three subgroups lemma]] (or equivalently, by the [[Commutator#Identities (group theory)|Hall-Witt identity]]), it follows that [\"G\", \"Z\"] = [[\"G\", \"G\"], \"Z\"] = [\"G\", \"G\", \"Z\"] = {1}. Therefore, \"Z\" ⊆ \"Z\" = \"Z\"(\"G\"), and the center of the quotient group \"G\" ⁄ \"Z\"(\"G\") is the [[trivial group]].\n\nAs a consequence, all [[Center (group theory)#Higher centers|higher centers]] (that is, higher terms in the [[upper central series]]) of a perfect group equal the center.\n\nIn terms of [[group homology]], a perfect group is precisely one whose first homology group vanishes: \"H\"(\"G\", Z) = 0, as the first homology group of a group is exactly the abelianization of the group, and perfect means trivial abelianization. An advantage of this definition is that it admits strengthening:\n\nEspecially in the field of [[algebraic K-theory]], a group is said to be quasi-perfect if its commutator subgroup is perfect; in symbols, a quasi-perfect group is one such that \"G\" = \"G\" (the commutator of the commutator subgroup is the commutator subgroup), while a perfect group is one such that \"G\" = \"G\" (the commutator subgroup is the whole group). See and .\n\n\n[[Category:Properties of groups]]\n[[Category:Lemmas]]"}
{"id": "404130", "url": "https://en.wikipedia.org/wiki?curid=404130", "title": "Piecewise", "text": "Piecewise\n\nIn mathematics, a piecewise-defined function (also called a piecewise function or a hybrid function) is a function defined by multiple sub-functions, each sub-function applying to a certain interval of the main function's domain, a sub-domain. Piecewise is actually a way of expressing the function, rather than a characteristic of the function itself, but with additional qualification, it can describe the nature of the function. For example, a piecewise polynomial function is a function that is a polynomial on each of its sub-domains, but possibly a different one on each.\n\nThe word \"piecewise\" is also used to describe any property of a piecewise-defined function that holds for each piece but not necessarily hold for the whole domain of the function. A function is piecewise differentiable or piecewise continuously differentiable if each piece is differentiable throughout its subdomain, even though the whole function may not be differentiable at the points between the pieces. In convex analysis, the notion of a derivative may be replaced by that of the subderivative for piecewise functions. Although the \"pieces\" in a piecewise definition need not be intervals, a function is not called \"piecewise linear\" or \"piecewise continuous\" or \"piecewise differentiable\" unless the pieces are intervals.\n\nPiecewise functions are defined using the common functional notation, where the body of the function is an array of functions and associated subdomains. Crucially, in most settings, there must only be a \"finite\" number of subdomains, each of which must be an interval, in order for the overall function to be called \"piecewise\". For example, consider the piecewise definition of the absolute value function:\nFor all values of \"x\" less than zero, the first function (−\"x\") is used, which negates the sign of the input value, making negative numbers positive. For all values of \"x\" greater than or equal to zero, the second function (\"x\") is used, which evaluates trivially to the input value itself.\n\nConsider the piecewise function \"f\"(\"x\") evaluated at certain values of \"x\":\nThus, in order to evaluate a piecewise function at a given input value, the appropriate subdomain needs to be chosen in order to select the correct function and produce the correct output value.\n\nA piecewise function is continuous on a given interval if the following conditions are met:\n\nThe pictured function, for example, is piecewise continuous throughout its subdomains, but is not continuous on the entire domain, as it contains a jump discontinuity at formula_2. The filled circle indicates that the value of the right function piece is used in this position.\n\nIn applied mathematical analysis, piecewise functions have been found to be consistent with many models of the human visual system, where images are perceived at a first stage as consisting of smooth regions separated by edges.\nIn particular, shearlets have been used as a representation system to provide sparse approximations of this model class in 2D and 3D.\n\nSpecific instances of piecewise functions include:\n\n"}
{"id": "47164545", "url": "https://en.wikipedia.org/wiki?curid=47164545", "title": "Polynomial decomposition", "text": "Polynomial decomposition\n\nIn mathematics, a polynomial decomposition expresses a polynomial \"f\" as the functional composition formula_1 of polynomials \"g\" and \"h\", where \"g\" and \"h\" have degree greater than 1. Algorithms are known for decomposing polynomials in polynomial time.\n\nPolynomials which are decomposable in this way are composite polynomials; those which are not are prime or indecomposable polynomials (not to be confused with irreducible polynomials, which cannot be factored into products of polynomials).\n\nIn the simplest case, one of the polynomials is a monomial. For example,\n\ndecomposes into\n\nsince\n\nLess trivially,\n\nA polynomial may have distinct decompositions into indecomposable polynomials where formula_6 where formula_7 for some formula_8. The restriction in the definition to polynomials of degree greater than one excludes the infinitely many decompositions possible with linear polynomials.\n\nJoseph Ritt proved that formula_9, and the degrees of the components are the same, but possibly in different order; this is Ritt's polynomial decomposition theorem. For example, formula_10.\n\nA polynomial decomposition may enable more efficient evaluation of a polynomial. For example,\ncan be calculated with only 3 multiplications using the decomposition, while Horner's method would require 7.\n\nA polynomial decomposition enables calculation of symbolic roots using radicals, even for some irreducible polynomials. This technique is used in many computer algebra systems. For example, using the decomposition\n\nthe roots of this irreducible polynomial can be calculated as\n\nEven in the case of quartic polynomials, where there is an explicit formula for the roots, solving using the decomposition often gives a simpler form. For example, the decomposition\n\ngives the roots\n\nbut straightforward application of the quartic formula gives equivalent results but in a form that is difficult to simplify and difficult to understand:\n\nThe first algorithm for polynomial decomposition was published in 1985, though it had been discovered in 1976 and implemented in the Macsyma computer algebra system. That algorithm took worst-case exponential time but worked independently of the characteristic of the underlying field.\n\nMore recent algorithms ran in polynomial time but with restrictions on the characteristic.\n\nThe most recent algorithm calculates a decomposition in polynomial time and without restrictions on the characteristic.\n\n"}
{"id": "5064309", "url": "https://en.wikipedia.org/wiki?curid=5064309", "title": "Postage stamp problem", "text": "Postage stamp problem\n\nThe postage stamp problem is a mathematical riddle that asks what is the smallest postage value which cannot be placed on an envelope, if the latter can hold only a limited number of stamps, and these may only have certain specified face values.\n\nFor example, suppose the envelope can hold only three stamps, and the available stamp values are 1 cent, 2 cents, 5 cents, and 20 cents. Then the solution is 13 cents; since any smaller value can be obtained with at most three stamps (e.g. 4 = 2 + 2, 8 = 5 + 2 + 1, etc.), but to get 13 cents one must use at least four stamps.\n\nMathematically, the problem can be formulated as follows: \n\nThis problem can be solved by brute force search or backtracking with maximum time proportional to |\"V\" |, where |\"V\" | is the number of distinct stamp values allowed. Therefore, if the capacity of the envelope \"m\" is fixed, it is a polynomial time problem. If the capacity \"m\" is arbitrary, the problem is known to be NP-hard.\n\n\n"}
{"id": "1596638", "url": "https://en.wikipedia.org/wiki?curid=1596638", "title": "Proof procedure", "text": "Proof procedure\n\nIn logic, and in particular proof theory, a proof procedure for a given logic is a systematic method for producing proofs in some proof calculus of (provable) statements.\n\nThere are several types of proof calculi. The most popular are natural deduction, sequent calculi (i.e., Gentzen type systems), Hilbert systems, and semantic tableaux or trees. A given proof procedure will target a specific proof calculus, but can often be reformulated so as to produce proofs in other proof styles.\n\nA proof procedure for a logic is \"complete\" if it produces a proof for each provable statement. The theorems of logical systems are typically recursively enumerable, which implies the existence of a complete but extremely inefficient proof procedure; however, a proof procedure is only of interest if it is reasonably efficient.\n\nFaced with an unprovable statement, a complete proof procedure may sometimes succeed in detecting and signalling its unprovability. In the general case, where provability is a semidecidable property, this is not possible, and instead the procedure will diverge (not terminate).\n\n"}
{"id": "371317", "url": "https://en.wikipedia.org/wiki?curid=371317", "title": "Quantitative psychological research", "text": "Quantitative psychological research\n\nQuantitative psychological research is defined as psychological research which performs mathematical modeling and statistical estimation or statistical inference or a means for testing objective theories by examining the relationship between variables. The first definition distinguishes it from qualitative psychological research; however, there has been a long debate on the difference between quantitative and qualitative research. It has been argued that because this debated has not found an end, the differences are enough that both quantitative and qualitative research is valuable in ways that both should be used in the gathering of data.\n\nStatistics is widely used in quantitative psychological research. Typically a project begins with the collection of data based on a theory or hypothesis, followed by the application of descriptive or inferential statistical methods. Often it is necessary to collect a very large volume of data, which require validating, verifying and recording. Software packages such as SPSS and R are typically used for this purpose, and for subsequent analysis. Causal relationships are studied by manipulating factors thought to influence the phenomena of interest while controlling other variables relevant to the experimental outcomes. Researchers might measure and study the relationship between education and measurable psychological effects, whilst controlling for other key variables. Quantitatively based surveys are widely used by psychologists, and statistics such as the proportion of respondents who display one or more psychological traits reported. In such surveys, respondents are asked a set of structured questions and their responses are tabulated. The software can then perform correlation analysis or other procedures on the data. Surveys are a common example of how statistics and quantitative research are utilized to gather data.\n\nQuantitative research falls under the category of empirical studies (or statistical studies). Research designs include experimental studies, quasi-experimental studies, pretest-postest designs, and others. Randomization, the control of variables, and valid, reliable measures are used to relate the results of the smaller subject pool to the population.\n\n"}
{"id": "691277", "url": "https://en.wikipedia.org/wiki?curid=691277", "title": "Quantity", "text": "Quantity\n\nQuantity is a property that can exist as a multitude or magnitude. Quantities can be compared in terms of \"more\", \"less\", or \"equal\", or by assigning a numerical value in terms of a unit of measurement. Quantity is among the basic classes of things along with quality, substance, change, and relation. Some quantities are such by their inner nature (as number), while others are functioning as states (properties, dimensions, attributes) of things such as heavy and light, long and short, broad and narrow, small and great, or much and little. \n\nTwo basic divisions of quantity, magnitude and multitude, imply the principal distinction between continuity (continuum) and discontinuity.\n\nUnder the name of multitude comes what is discontinuous and discrete and divisible ultimately into indivisibles, such as: \"army, fleet, flock, government, company, party, people, mess (military), chorus, crowd\", and \"number\"; all which are cases of collective nouns. Under the name of magnitude comes what is continuous and unified and divisible only into smaller divisibles, such as: \"matter, mass, energy, liquid, material\"—all cases of non-collective nouns.\n\nAlong with analyzing its nature and classification, the issues of quantity involve such closely related topics as the relation of magnitudes and multitudes, dimensionality, equality, proportion, the measurements of quantities, the units of measurements, number and numbering systems, the types of numbers and their relations to each other as numerical ratios.\n\nThus quantity is a property that exists in a range of magnitudes or multitudes. Mass, time, distance, heat, and angular separation are among the familiar examples of quantitative properties.\n\nIn mathematics, the concept of quantity is an ancient one extending back to the time of Aristotle and earlier. Aristotle regarded quantity as a fundamental ontological and scientific category. In Aristotle's ontology, quantity or quantum was classified into two different types, which he characterized as follows:\n\nIn his \"Elements\", Euclid developed the theory of ratios of magnitudes without studying the nature of magnitudes, as Archimedes, but giving the following significant definitions:\n\nFor Aristotle and Euclid, relations were conceived as whole numbers (Michell, 1993). John Wallis later conceived of ratios of magnitudes as real numbers as reflected in the following:\n\nThat is, the ratio of magnitudes of any quantity, whether volume, mass, heat and so on, is a number. Following this, Newton then defined number, and the relationship between quantity and number, in the following terms: \"By \"number\" we understand not so much a multitude of unities, as the abstracted ratio of any quantity to another quantity of the same kind, which we take for unity\" (Newton, 1728).\n\nContinuous quantities possess a particular structure that was first explicitly characterized by Hölder (1901) as a set of axioms that define such features as \"identities\" and \"relations\" between magnitudes. In science, quantitative structure is the subject of empirical investigation and cannot be assumed to exist \"a priori\" for any given property. The linear continuum represents the prototype of continuous quantitative structure as characterized by Hölder (1901) (translated in Michell & Ernst, 1996). A fundamental feature of any type of quantity is that the relationships of equality or inequality can in principle be stated in comparisons between particular magnitudes, unlike quality, which is marked by likeness, similarity and difference, diversity. Another fundamental feature is additivity. Additivity may involve concatenation, such as adding two lengths A and B to obtain a third A + B. Additivity is not, however, restricted to extensive quantities but may also entail relations between magnitudes that can be established through experiments that permit tests of hypothesized observable manifestations of the additive relations of magnitudes. Another feature is continuity, on which Michell (1999, p. 51) says of length, as a type of quantitative attribute, \"what continuity means is that if any arbitrary length, a, is selected as a unit, then for every positive real number, \"r\", there is a length b such that b = \"r\"a\". A further generalization is given by the theory of conjoint measurement, independently developed by French economist Gérard Debreu (1960) and by the American mathematical psychologist R. Duncan Luce and statistician John Tukey (1964).\n\nMagnitude (how much) and multitude (how many), the two principal types of quantities, are further divided as mathematical and physical. In formal terms, quantities—their ratios, proportions, order and formal relationships of equality and inequality—are studied by mathematics. The essential part of mathematical quantities consists of having a collection of variables, each assuming a set of values. These can be a set of a single quantity, referred to as a scalar when represented by real numbers, or have multiple quantities as do vectors and tensors, two kinds of geometric objects.\n\nThe mathematical usage of a quantity can then be varied and so is situationally dependent. Quantities can be used as being infinitesimal, arguments of a function, variables in an expression (independent or dependent), or probabilistic as in random and stochastic quantities. In mathematics, magnitudes and multitudes are also not only two distinct kinds of quantity but furthermore relatable to each other.\n\nNumber theory covers the topics of the discrete quantities as numbers: number systems with their kinds and relations. Geometry studies the issues of spatial magnitudes: straight lines, curved lines, surfaces and solids, all with their respective measurements and relationships.\n\nA traditional philosophy of mathematics, stemming from Aristotle and remaining popular until the eighteenth century, held that mathematics is the \"science of quantity\". Quantity was considered to be divided into the discrete (studied by arithmetic) and the continuous (studied by geometry and later calculus). The theory fits reasonably well elementary or school mathematics but less well the abstract topological and algebraic structures of modern mathematics.\n\nEstablishing quantitative structure and relationships \"between\" different quantities is the cornerstone of modern physical sciences. Physics is fundamentally a quantitative science. Its progress is chiefly achieved due to rendering the abstract qualities of material entities into physical quantities, by postulating that all material bodies marked by quantitative properties or physical dimensions are subject to some measurements and observations. Setting the units of measurement, physics covers such fundamental quantities as space (length, breadth, and depth) and time, mass and force, temperature, energy, and quanta.\n\nA distinction has also been made between intensive quantity and extensive quantity as two types of quantitative property, state or relation. The magnitude of an \"intensive quantity\" does not depend on the size, or extent, of the object or system of which the quantity is a property, whereas magnitudes of an \"extensive quantity\" are additive for parts of an entity or subsystems. Thus, magnitude does depend on the extent of the entity or system in the case of extensive quantity. Examples of intensive quantities are density and pressure, while examples of extensive quantities are energy, volume, and mass.\n\nIn human languages, including English, number is a syntactic category, along with person and gender. The quantity is expressed by identifiers, definite and indefinite, and quantifiers, definite and indefinite, as well as by three types of nouns: 1. count unit nouns or countables; 2. mass nouns, uncountables, referring to the indefinite, unidentified amounts; 3. nouns of multitude (collective nouns). The word ‘number’ belongs to a noun of multitude standing either for a single entity or for the individuals making the whole. An amount in general is expressed by a special class of words called identifiers, indefinite and definite and quantifiers, definite and indefinite. The amount may be expressed by: singular form and plural from, ordinal numbers before a count noun singular (first, second, third...), the demonstratives; definite and indefinite numbers and measurements (hundred/hundreds, million/millions), or cardinal numbers before count nouns. The set of language quantifiers covers \"a few, a great number, many, several (for count names); a bit of, a little, less, a great deal (amount) of, much (for mass names); all, plenty of, a lot of, enough, more, most, some, any, both, each, either, neither, every, no\". For the complex case of unidentified amounts, the parts and examples of a mass are indicated with respect to the following: a measure of a mass (two kilos of rice and twenty bottles of milk or ten pieces of paper); a piece or part of a mass (part, element, atom, item, article, drop); or a shape of a container (a basket, box, case, cup, bottle, vessel, jar).\n\nSome further examples of quantities are:\n\n\n\n"}
{"id": "1106042", "url": "https://en.wikipedia.org/wiki?curid=1106042", "title": "Reciprocal polynomial", "text": "Reciprocal polynomial\n\nIn algebra, the reciprocal polynomial, or reflected polynomial or , of a polynomial of degree with coefficients from an arbitrary field, such as\nis the polynomial\n\nEssentially, the coefficients are written in reverse order. They arise naturally in linear algebra as the characteristic polynomial of the inverse of a matrix.\n\nIn the special case that the polynomial has complex coefficients, that is,\n\nthe conjugate reciprocal polynomial, given by,\n\nwhere formula_5 denotes the complex conjugate of formula_6, is also called the reciprocal polynomial when no confusion can arise.\n\nA polynomial is called self-reciprocal or palindromic if .\nThe coefficients of a self-reciprocal polynomial satisfy . In the conjugate reciprocal case, the coefficients must be real to satisfy the condition.\n\nReciprocal polynomials have several connections with their original polynomials, including:\n\nOther properties of reciprocal polynomials may be obtained, for instance:\n\nA self-reciprocal polynomial is also called palindromic because its coefficients, when the polynomial is written in the order of ascending or descending powers, form a palindrome. That is, if\nis a polynomial of degree , then is \"palindromic\" if for . Some authors use the terms \"palindromic\" and \"reciprocal\" interchangeably.\n\nSimilarly, , a polynomial of degree , is called antipalindromic if for . That is, a polynomial is \"antipalindromic\" if .\n\nFrom the properties of the binomial coefficients, it follows that the polynomials are palindromic for all positive integers , while the polynomials are palindromic when is even and antipalindromic when is odd.\n\nOther examples of palindromic polynomials include cyclotomic polynomials and Eulerian polynomials.\n\n\nA polynomial with real coefficients all of whose complex roots lie on the unit circle in the complex plane (all the roots are unimodular) is either palindromic or antipalindromic.\n\nA polynomial is conjugate reciprocal if formula_8 and self-inversive if formula_9 for a scale factor on the unit circle.\n\nIf is the minimal polynomial of with , and has real coefficients, then is self-reciprocal. This follows because\n\nSo is a root of the polynomial formula_11 which has degree . But, the minimal polynomial is unique, hence \nfor some constant , i.e. formula_13. Sum from to and note that 1 is not a root of . We conclude that .\n\nA consequence is that the cyclotomic polynomials are self-reciprocal for . This is used in the special number field sieve to allow numbers of the form and to be factored taking advantage of the algebraic factors by using polynomials of degree 5, 6, 4 and 6 respectively – note that (Euler's totient function) of the exponents are 10, 12, 8 and 12.\n\nThe reciprocal polynomial finds a use in the theory of cyclic error correcting codes. Suppose can be factored into the product of two polynomials, say . When generates a cyclic code , then the reciprocal polynomial generates , the orthogonal complement of .\nAlso, is \"self-orthogonal\" (that is, , if and only if divides .\n\n\n"}
{"id": "39560800", "url": "https://en.wikipedia.org/wiki?curid=39560800", "title": "Six operations", "text": "Six operations\n\nIn mathematics, Grothendieck's six operations, named after Alexander Grothendieck, is a formalism in homological algebra. It originally sprang from the relations in étale cohomology that arise from a morphism of schemes . The basic insight was that many of the elementary facts relating cohomology on \"X\" and \"Y\" were formal consequences of a small number of axioms. These axioms hold in many cases completely unrelated to the original context, and therefore the formal consequences also hold. The six operations formalism has since been shown to apply to contexts such as \"D\"-modules on algebraic varieties, sheaves on locally compact topological spaces, and motives.\n\nThe operations are six functors. Usually these are functors between derived categories and so are actually left and right derived functors.\n\n\nThe functors formula_2 and formula_1 form an adjoint functor pair, as do formula_3 and formula_4. Similarly, internal tensor product is left adjoint to internal Hom.\n\nLet be a morphism of schemes. The morphism \"f\" induces several functors. Specifically, it gives adjoint functors \"f\" and \"f\" between the categories of sheaves on \"X\" and \"Y\", and it gives the functor \"f\" of direct image with proper support. In the derived category, \"Rf\" admits a right adjoint \"f\". Finally, when working with abelian sheaves, there is a tensor product functor ⊗ and an internal Hom functor, and these are adjoint. The six operations are the corresponding functors on the derived category: , , , , , and .\n\nSuppose that we restrict ourselves to a category of formula_9-adic torsion sheaves, where formula_9 is coprime to the characteristic of \"X\" and of \"Y\". In SGA 4 III, Grothendieck and Artin proved that if \"f\" is a smooth morphism, then \"Lf\" is isomorphic to , where denote the \"d\"th inverse Tate twist and denotes a shift in degree by . Furthermore, suppose that \"f\" is separated and of finite type. If is another morphism of schemes, if denotes the base change of \"X\" by \"g\", and if \"f\"′ and \"g\"′ denote the base changes of \"f\" and \"g\" by \"g\" and \"f\", respectively, then there exist natural isomorphisms:\nAgain assuming that \"f\" is separated and of finite type, for any objects \"M\" in the derived category of \"X\" and \"N\" in the derived category of \"Y\", there exist natural isomorphisms:\n\nIf \"i\" is a closed immersion of \"Z\" into \"S\" with complementary open immersion \"j\", then there is a distinguished triangle in the derived category:\nwhere the first two maps are the counit and unit, respectively of the adjunctions. If \"Z\" and \"S\" are regular, then there is an isomorphism:\nwhere and are the units of the tensor product operations (which vary depending on which category of formula_9-adic torsion sheaves is under consideration).\n\nIf \"S\" is regular and , and if \"K\" is an invertible object in the derived category on \"S\" with respect to , then define \"D\" to be the functor . Then, for objects \"M\" and \"M\"′ in the derived category on \"X\", the canonical maps:\nare isomorphisms. Finally, if is a morphism of \"S\"-schemes, and if \"M\" and \"N\" are objects in the derived categories of \"X\" and \"Y\", then there are natural isomorphisms:\n\n\n\n"}
{"id": "47243080", "url": "https://en.wikipedia.org/wiki?curid=47243080", "title": "The Art of Mathematics", "text": "The Art of Mathematics\n\nThe Art of Mathematics (), written by Hong Sung-Dae (), is a series of mathematics textbooks for high school students in South Korea. First published in 1966, it is aguably the best-selling mathematics textbook series in South Korea, with about 37 million copies sold as of 2006. In Jeongeup, North Jeolla Province, the hometown of Hong Sung-Dae, a street is named in honor of the author.'\n\nChanges in the 11th edition, published 2013-2015, reflect the 2009 revision of South Korea's National Curriculum ( icheon-gu gaejeong gyoyuggwajeong). Each of the six volumes consist of two versions, one for average students ( Gibon-pyeon) and one for higher-ability students ().\n\n\n\n\n\n\n\n"}
{"id": "37721302", "url": "https://en.wikipedia.org/wiki?curid=37721302", "title": "Thread automaton", "text": "Thread automaton\n\nIn automata theory, the thread automaton (plural: automata) is an extended type of finite-state automata that recognizes a mildly context-sensitive language class above the tree-adjoining languages. \n\nA thread automaton consists of\n\nA path \"u\"...\"u\" ∈ \"U\" is a string of path components \"u\" ∈ \"U\"; \"n\" may be 0, with the empty path denoted by ε.\nA thread has the form \"u\"...\"u\":\"A\", where \"u\"...\"u\" ∈ \"U\" is a path, and \"A\" ∈ \"N\" is a state.\nA thread store \"S\" is a finite set of threads, viewed as a partial function from \"U\" to \"N\", such that \"dom\"(\"S\") is closed by prefix.\n\nA thread automaton configuration is a triple ‹\"l\",\"p\",\"S\"›, where \"l\" denotes the current position in the input string, \"p\" is the active thread, and \"S\" is a thread store containing \"p\".\nThe initial configuration is ‹0,ε,{ε:\"A\"}›.\nThe final configuration is ‹\"n\",\"u\",{ε:\"A\",\"u\":\"A\"}›, where \"n\" is the length of the input string and \"u\" abbreviates δ(\"A\").\nA transition in the set Θ may have one of the following forms, and changes the current automaton configuration in the following way:\nOne may prove that δ(\"B\")=\"u\" for POP and SPOP transitions, and δ(\"C\")=⊥ for SPUSH transitions.\n\nAn input string is accepted by the automaton if there is a sequence of transitions changing the initial into the final configuration.\n"}
{"id": "19374248", "url": "https://en.wikipedia.org/wiki?curid=19374248", "title": "Timeline of geometry", "text": "Timeline of geometry\n\nA timeline of algebra and geometry\n\n\n\n\n\n\n\n\n\n"}
{"id": "34140027", "url": "https://en.wikipedia.org/wiki?curid=34140027", "title": "Totative", "text": "Totative\n\nIn number theory, a totative of a given positive integer is an integer such that and is coprime to . Euler's totient function φ(\"n\") counts the number of totatives of \"n\". The totatives under multiplication modulo \"n\" form the multiplicative group of integers modulo \"n\".\n\nThe distribution of totatives has been a subject of study. Paul Erdős conjectured that, writing the totatives of \"n\" as\n\nthe mean square gap satisfies\n\nfor some constant \"C\" and this was proven by Bob Vaughan and Hugh Montgomery.\n\n\n"}
