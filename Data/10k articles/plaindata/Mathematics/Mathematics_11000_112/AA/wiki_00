{"id": "9822856", "url": "https://en.wikipedia.org/wiki?curid=9822856", "title": "2 × 2 real matrices", "text": "2 × 2 real matrices\n\nIn mathematics, the associative algebra of real matrices is denoted by M(2, R). Two matrices \"p\" and \"q\" in M(2, R) have a sum \"p\" + \"q\" given by matrix addition. The product matrix is formed from the dot product of the rows and columns of its factors through matrix multiplication. For\n\nlet\n\nThen \"q q\" = \"q\" \"q\" = (\"ad\" − \"bc\") , where is the identity matrix. The real number \"ad\" − \"bc\" is called the determinant of \"q\". When \"ad\" − \"bc\" ≠ 0, \"q\" is an invertible matrix, and then\n\nThe collection of all such invertible matrices constitutes the general linear group GL(2, R). In terms of abstract algebra, M(2, R) with the associated addition and multiplication operations forms a ring, and GL(2, R) is its group of units. M(2, R) is also a four-dimensional vector space, so it is considered an associative algebra. It is ring-isomorphic to the coquaternions, but has a different profile.\n\nThe real matrices are in one-one correspondence with the linear mappings of the two-dimensional Cartesian coordinate system into itself by the rule\n\nWithin M(2, R), the multiples by real numbers of the identity matrix may be considered a real line. This real line is the place where all commutative subrings come together:\n\nLet \"P\" = {\"x\" + \"ym\" : \"x\", \"y\" ∈ R} where \"m\" ∈ {−, 0,  }. Then \"P\" is a commutative subring and M(2, R) = ⋃\"P\"  where the union is over all \"m\" such that \"m\" ∈ {−, 0,  }.\n\nTo identify such \"m\", first square the generic matrix:\n\nWhen \"a\" + \"d\" = 0 this square is a diagonal matrix.\n\nThus one assumes \"d\" = −\"a\" when looking for \"m\" to form commutative subrings. When \"mm\" = −, then \"bc\" = −1 − \"aa\", an equation describing a hyperbolic paraboloid in the space of parameters (\"a\", \"b\", \"c\"). Such an \"m\" serves as an imaginary unit. In this case P is isomorphic to the field of (ordinary) complex numbers.\n\nWhen \"mm\" = +, \"m\" is an involutory matrix. Then \"bc\" = +1 − \"aa\", also giving a hyperbolic paraboloid. If a matrix is an idempotent matrix, it must lie in such a P and in this case P is isomorphic to the ring of split-complex numbers.\n\nThe case of a nilpotent matrix, \"mm\" = 0, arises when only one of \"b\" or \"c\" is non-zero, and the commutative subring P is then a copy of the dual number plane.\n\nWhen M(2, R) is reconfigured with a change of basis, this profile changes to the profile of split-quaternions where the sets of square roots of and − take a symmetrical shape as hyperboloids.\n\nFirst transform one differential vector into another:\n\nAreas are measured with \"density\" formula_7, a differential 2-form which involves the use of exterior algebra. The transformed density is\n\nThus the equi-areal mappings are identified with SL(2, R) = {\"g\" ∈ M(2, R) : det(\"g\") = 1}, the special linear group. Given the profile above, every such \"g\" lies in a commutative subring P representing a type of complex plane according to the square of \"m\". Since \"g g\" = , one of the following three alternatives occurs:\n\nWriting about planar affine mapping, Rafael Artzy made a similar trichotomy of planar, linear mapping in his book \"Linear Geometry\" (1965).\n\nThe commutative subrings of M(2, R) determine the function theory; in particular the three types of subplanes have their own algebraic structures which set the value of algebraic expressions. Consideration of the square root function and the logarithm function serves to illustrate the constraints implied by the special properties of each type of subplane P described in the above profile.\nThe concept of identity component of the group of units of P leads to the polar decomposition of elements of the group of units:\n\nIn the first case exp(θ \"m\") = cos(θ) + \"m\" sin(θ). In the case of the dual numbers exp(\"s m\") = 1 + \"s m\". Finally, in the case of split complex numbers there are four components in the group of units. The identity component is parameterized by ρ and exp(\"a m\") = cosh(\"a\") + \"m\" sinh(\"a\").\nNow formula_9 regardless of the subplane P, but the argument of the function must be taken from the \"identity component of its group of units\". Half the plane is lost in the case of the dual number structure; three-quarters of the plane must be excluded in the case of the split-complex number structure.\n\nSimilarly, if ρ exp(\"a m\") is an element of the identity component of the group of units of a plane associated with matrix \"m\", then the logarithm function results in a value log ρ + \"a m\". The domain of the logarithm function suffers the same constraints as does the square root function described above: half or three-quarters of P must be excluded in the cases \"mm\" = 0 or \"mm\" = .\n\nFurther function theory can be seen in the article complex functions for the C structure, or in the article motor variable for the split-complex structure.\n\nEvery real matrix can be interpreted as one of three types of (generalized) complex numbers: standard complex numbers, dual numbers, and split-complex numbers. Above, the algebra of matrices is profiled as a union of complex planes, all sharing the same real axis. These planes are presented as commutative subrings \"P\". We can determine to which complex plane a given matrix belongs as follows and classify which kind of complex number that plane represents.\n\nConsider the matrix\n\nWe seek the complex plane \"P\" containing \"z\".\n\nAs noted above, the square of the matrix \"z\" is diagonal when \"a\" + \"d\" = 0. The matrix \"z\" must be expressed as the sum of a multiple of the identity matrix and a matrix in the hyperplane \"a\" + \"d\" = 0. Projecting \"z\" alternately onto these subspaces of R yields\n\nFurthermore, \n\nNow \"z\" is one of three types of complex number:\n\nSimilarly, a matrix can also be expressed in polar coordinates with the caveat that there are two connected components of the group of units in the dual number plane, and four components in the split-complex number plane.\n\n"}
{"id": "22647091", "url": "https://en.wikipedia.org/wiki?curid=22647091", "title": "Albert Muchnik", "text": "Albert Muchnik\n\nAlbert Abramovich Muchnik (born 1934) is a Russian mathematician who worked in the field of foundations and mathematical logic.\n\nHe received his Ph.D from Moscow State Pedagogical Institute in 1959 under the advisorship of Pyotr Novikov. Muchnik's most significant contribution was on the subject of relative computability. He and Richard Friedberg, independently introduced the priority method which gave an affirmative answer to Post's Problem regarding the existence of re Turing degrees between 0 and 0' . This groundbreaking result, now known as the Friedberg-Muchnik Theorem, opened a wide study of the Turing degrees of the recursively enumerable sets which turned out to possess a very complicated and non-trivial structure. He also has a significant contribution in the subject of mass problems where he introduced the generalisation of Turing degrees, called \"Muchnik degrees\" in his work \"On Strong and Weak Reducibilities of Algorithmic Problems\" published in 1963. Muchnik also elaborated Kolmogorov's proposal of viewing intuitionism as \"calculus of problems\" and proved that the lattice of Muchnik degrees is Brouwerian.\n\nMuchnik is married to the Russian mathematician Nadezhda Ermolaeva, and their son Andrej, who died in 2007, was also a mathematician working in foundations of mathematics.\n\n\n"}
{"id": "9092767", "url": "https://en.wikipedia.org/wiki?curid=9092767", "title": "Alfred Kempe", "text": "Alfred Kempe\n\nSir Alfred Bray Kempe DCL FRS (6 July 1849, Kensington, London – 21 April 1922, London) was a mathematician best known for his work on linkages and the four colour theorem.\n\nKempe was the son of the Rector of St James's Church, Piccadilly, the Rev. John Edward Kempe. He was educated at St Paul's School, London and then studied at Trinity College, Cambridge where Arthur Cayley was one of his teachers. He graduated BA (22nd wrangler) in 1872. Despite his interest in mathematics he became a barrister, specialising in the ecclesiastical law. He was knighted in 1913, the same year he became the Chancellor for the Diocese of London. He was also Chancellor of the dioceses of Newcastle, Southwell, St Albans, Peterborough, Chichester, and Chelmsford. He received the honorary degree DCL from the University of Durham and he was elected a Bencher of the Inner Temple in 1909.\n\nIn 1876 he published his article \"On a General Method of describing Plane Curves of the nth degree by Linkwork,\" which showed that for an arbitrary algebraic plane curve a linkage can be constructed that draws the curve. This direct connection between linkages and algebraic curves was recently named the Kempe's Universality Theorem that any bounded subset of an algebraic curve may be traced out by the motion of one of the joints in a suitably chosen linkage. Kempe's proof was flawed, and the first complete proof was provided in 2002, based on his ideas.\n\nIn 1877 Kempe discovered new straight line linkages and published his influential lectures on the subject. In 1879 Kempe wrote his famous \"proof\" of the four colour theorem, shown incorrect by Percy Heawood in 1890. Much later, his work led to fundamental concepts such as the Kempe chain and unavoidable sets.\n\nKempe (1886) revealed a rather marked philosophical bent, and much influenced Charles Sanders Peirce. Kempe also discovered what are now called multisets, although this fact was not noted until long after his death.\n\nKempe was elected a fellow of the Royal Society in 1881. He was Treasurer and Vice-President of the Royal Society 1899–1919. He was a president of the London Mathematical Society from 1892 to 1894. He was also a mountain climber, mostly in Switzerland.\nHis first wife was Mary, daughter of Sir William Bowman, 1st Baronet; she died in 1893. He then married, in 1897, Ida, daughter of His Honour Judge Meadows White, QC. He had two sons and one daughter.\n\n"}
{"id": "56608952", "url": "https://en.wikipedia.org/wiki?curid=56608952", "title": "Algebraic representation", "text": "Algebraic representation\n\nIn mathematics, an algebraic representation of a group \"G\" on a \"k\"-algebra \"A\" is a linear representation formula_1 such that, for each \"g\" in \"G\", formula_2 is an algebra automorphism. Equipped with such a representation, the algebra \"A\" is then called a \"G\"-algebra.\n\nFor example, if \"V\" is a linear representation of a group \"G\", then the representation put on the tensor algebra formula_3 is an algebraic represention of \"G\".\n\nIf \"A\" is a commutative \"G\"-algebra, then formula_4 is an affine \"G\"-scheme.\n\n"}
{"id": "2791110", "url": "https://en.wikipedia.org/wiki?curid=2791110", "title": "Anonymous matching", "text": "Anonymous matching\n\nAnonymous matching is a matchmaking method facilitated by computer databases, in which each user confidentially selects people they are interested in dating and the computer identifies and reports matches to pairs of users who share a mutual attraction. Protocols for anonymous matchmaking date back to the 1980s, and one of the earliest papers on the topic is by Baldwin and Gramlich, published in 1985. From a technical perspective, the problem and solution are trivial and likely predate even this paper. The problem becomes interesting and requires more sophisticated cryptography when the matchmaker (central server) isn't trusted.\n\nThe purpose of the protocol is to allow people to initiate romantic relationships while avoiding the risk of embarrassment, awkwardness, and other negative consequences associated with unwanted romantic overtures and rejection. The general concept was patented on September 7, 1999 by David J. Blumberg and DoYouDo chief executive officer Gil S. Sudai, but several websites were already employing the methodology by that date, and thus apparently were allowed to continue using it. United States Patent 5,950,200 points out several potential flaws in traditional courtship and in conventional dating systems in which strangers meet online, promoting anonymous matching of friends and acquaintances as a better alternative:\n\nSome of the most notable implementations of the idea have been:\n\nThese commercial implementations all trust the central server, simplifying the solution and implementation drastically. Baldwin and Gramlich solved this case in 1985, as well as the more notable and challenging case in which the central server isn't trusted.\n\neCRUSH, DoYOU2.com, the LiveJournal Secret Crush meme, and SecretAdmirer.com are examples of anonymous matching services using viral marketing to increase their membership. Users are encouraged to send an anonymous email to their crush so that they will visit the site and enter their own crushes, facilitating a match. In the case of SecretAdmirer.com, the email is mandatory; this represents a more aggressive type of viral marketing.\n\nAt least one site, CrushLink, was accused by eCRUSH of sending spam emails disguised as crush notifications. According to a Salon article, \"What makes SomeoneLikesYou and Crushlink different from the rest of the sites in the genre is this: they bait hopeful visitors to hand over as many e-mail addresses as possible by trading clues for e-mail addresses\". Both sites are now defunct.\n\n"}
{"id": "44127276", "url": "https://en.wikipedia.org/wiki?curid=44127276", "title": "Berkeley Madonna", "text": "Berkeley Madonna\n\nBerkeley Madonna is a mathematical modelling software package, developed at the University of California at Berkeley by Robert Macey and George Oster. It numerically solves ordinary differential equations and difference equations, originally developed to execute STELLA programs.\n\nBerkeley Madonna is arguably the fastest differential equation solver, originally developed for modeling and visualization of chemical reactions.\n\nIts strength lies in a relatively simple syntax to define differential equations coupled with a simple yet powerful user interface. In particular, Berkeley Madonna provides the facility of putting parameters onto a slider that can in turn be moved by a user to change the value. Such visualizations enable quick assessments of whether or not a particular model class is suitable to describe the data to be analyzed and modeled, and, later, communicating models easily to other disciplines such as medical decision makers.\n\nIt has become a standard in the development and communication of pharmacometric models describing drug concentration and its effects in drug development\n, modeling of physiological processes. \nA user community exists in the form of a LinkedIn user group with currently more than 500 members.\n\nThe use of system dynamics modeling has expanded into other areas such as epidemiology, environmental health, and population ecology.\n\nThere are two versions of Berkeley Madonna: a free version with slightly limited functionality and a licensed version that is registered to individuals.\n\n"}
{"id": "34421338", "url": "https://en.wikipedia.org/wiki?curid=34421338", "title": "Blumenthal Award", "text": "Blumenthal Award\n\nThe Blumenthal Award was founded by the American Mathematical Society in 1993 in memory of Leonard M. and Eleanor B. Blumenthal. \nThe award was presented to the individual deemed to have made the most substantial contribution in research in the field of pure mathematics, and who was deemed to have the potential for future production of distinguished research in such field. It was awarded every four years for the most substantial Ph.D. thesis produced in the four year interval between awards. The fund that supported the award was discontinued and, thus, the award is no longer being made.\n\n\n"}
{"id": "27908226", "url": "https://en.wikipedia.org/wiki?curid=27908226", "title": "Bochner measurable function", "text": "Bochner measurable function\n\nIn mathematics – specifically, in functional analysis – a Bochner-measurable function taking values in a Banach space is a function that equals a.e. the limit of a sequence of measurable countably-valued functions, i.e.,\n\nwhere the functions formula_2 each have a countable range and for which the pre-image formula_3 is measurable for each \"x\". The concept is named after Salomon Bochner.\n\nBochner-measurable functions are sometimes called strongly measurable, formula_4-measurable or just measurable (or uniformly measurable in case that the Banach space is the space of continuous linear operators between Banach spaces).\n\nThe relationship between measurability and weak measurability is given by the following result, known as Pettis' theorem or Pettis measurability theorem.\nFunction \"f\" is almost surely separably valued (or essentially separably valued) if there exists a subset \"N\" ⊆ \"X\" with \"μ\"(\"N\") = 0 such that \"f\"(\"X\" \\ \"N\") ⊆ \"B\" is separable.\n\nA function f  : \"X\" → \"B\" defined on a measure space (\"X\", Σ, \"μ\") and taking values in a Banach space \"B\" is (strongly) measurable (with respect to Σ and the Borel algebra on \"B\") if and only if it is both weakly measurable and almost surely separably valued. \nIn the case that \"B\" is separable, since any subset of a separable Banach space is itself separable, one can take \"N\" above to be empty, and it follows that the notions of weak and strong measurability agree when \"B\" is separable.\n\n"}
{"id": "7723", "url": "https://en.wikipedia.org/wiki?curid=7723", "title": "Carmichael number", "text": "Carmichael number\n\nIn number theory, a Carmichael number is a composite number formula_1 which satisfies the modular arithmetic congruence relation:\nfor all integers formula_3 which are relatively prime to formula_1.\n\nThey are named for Robert Carmichael.\nThe Carmichael numbers are the subset \"K\" of the Knödel numbers.\n\nEquivalently, a Carmichael number is a composite number formula_1 for which\nfor all integers formula_3.\nFermat's little theorem states that if \"p\" is a prime number, then for any integer \"b\", the number \"b\" − \"b\" is an integer multiple of \"p\". Carmichael numbers are composite numbers which have this property. Carmichael numbers are also called Fermat pseudoprimes or absolute Fermat pseudoprimes. A Carmichael number will pass a Fermat primality test to every base \"b\" relatively prime to the number, even though it is not actually prime.\nThis makes tests based on Fermat's Little Theorem less effective than strong probable prime tests such as the Baillie-PSW primality test and the Miller–Rabin primality test.\n\nHowever, no Carmichael number is either an Euler-Jacobi pseudoprime or a strong pseudoprime to every base relatively prime to it\n\nso, in theory, either an Euler or a strong probable prime test could prove that a Carmichael number is, in fact, composite.\n\nAs numbers become larger, Carmichael numbers become increasingly rare. For example, there are 20,138,200 Carmichael numbers between 1 and 10 (approximately one in 50 trillion (5·10) numbers).\n\nAn alternative and equivalent definition of Carmichael numbers is given by Korselt's criterion.\n\nIt follows from this theorem that all Carmichael numbers are odd, since any even composite number that is square-free (and hence has only one prime factor of two) will have at least one odd prime factor, and thus formula_13 results in an even dividing an odd, a contradiction. (The oddness of Carmichael numbers also follows from the fact that formula_14 is a Fermat witness for any even composite number.)\nFrom the criterion it also follows that Carmichael numbers are cyclic. Additionally, it follows that there are no Carmichael numbers with exactly two prime factors.\n\nKorselt was the first who observed the basic properties of Carmichael numbers, but he did not give any examples. In 1910, Carmichael found the first and smallest such number, 561, which explains the name \"Carmichael number\".\n\nThat 561 is a Carmichael number can be seen with Korselt's criterion. Indeed, formula_15 is square-free and formula_16, formula_17 and formula_18.\n\nThe next six Carmichael numbers are :\n\nThese first seven Carmichael numbers, from 561 to 8911, were all found by the Czech mathematician Václav Šimerka in 1885 (thus preceding not just Carmichael but also Korselt, although Šimerka did not find anything like Korselt's criterion). His work, however, remained unnoticed.\n\nJ. Chernick proved a theorem in 1939 which can be used to construct a subset of Carmichael numbers. The number formula_25 is a Carmichael number if its three factors are all prime. Whether this formula produces an infinite quantity of Carmichael numbers is an open question (though it is implied by Dickson's conjecture).\n\nPaul Erdős heuristically argued there should be infinitely many Carmichael numbers. In 1994 W. R. (Red) Alford, Andrew Granville and Carl Pomerance used a bound on Olson's constant to show that there really do exist infinitely many Carmichael numbers. Specifically, they showed that for sufficiently large formula_1, there are at least formula_27 Carmichael numbers between 1 and formula_1.\n\nLöh and Niebuhr in 1992 found some very large Carmichael numbers, including one with 1,101,518 factors and over 16 million digits.\n\nCarmichael numbers have at least three positive prime factors. For some fixed \"R\", there are infinitely many Carmichael numbers with exactly \"R\" factors; in fact, there are infinitely many such R.\n\nThe first Carmichael numbers with formula_29 prime factors are :\n\nThe first Carmichael numbers with 4 prime factors are :\n\nThe second Carmichael number (1105) can be expressed as the sum of two squares in more ways than any smaller number. The third Carmichael number (1729) is the Hardy-Ramanujan Number: the smallest number that can be expressed as the sum of two cubes (of positive numbers) in two different ways.\n\nLet formula_30 denote the number of Carmichael numbers less than or equal to formula_31. The distribution of Carmichael numbers by powers of 10 :\n\nIn 1953, Knödel proved the upper bound:\n\nfor some constant formula_33.\n\nIn 1956, Erdős improved the bound to\n\nfor some constant formula_35. He further gave a heuristic argument suggesting that this upper bound should be close to the true growth rate of formula_30. The table below gives approximate minimal values for the constant \"k\" in the Erdős bound for formula_37 as \"n\" grows:\n\nIn the other direction, Alford, Granville and Pomerance proved in 1994 that for sufficiently large \"X\",\n\nIn 2005, this bound was further improved by Harman to\n\nwho subsequently improved the exponent to formula_40.\nRegarding the asymptotic distribution of Carmichael numbers, there have been several conjectures. In 1956, Erdős conjectured that there were formula_41 Carmichael numbers for \"X\" sufficiently large. In 1981, Pomerance sharpened Erdős' heuristic arguments to conjecture that there are\n\nCarmichael numbers up to \"X\". However, inside current computational ranges (such as the counts of Carmichael numbers performed by Pinch up to 10), these conjectures are not yet borne out by the data.\n\nThe notion of Carmichael number generalizes to a Carmichael ideal in any number field \"K\". For any nonzero prime ideal formula_43 in formula_44, we have <math>\\alpha^\n"}
{"id": "39127833", "url": "https://en.wikipedia.org/wiki?curid=39127833", "title": "Cat state", "text": "Cat state\n\nIn quantum computing, the cat state, named after Schrödinger's cat, is a quantum superposition of two macroscopically distinct states. The individual states being superposed could be classical or quantum, but their macroscopicity is an important criterion. A cat state could be of one or more modes or particles, and does not necessarily need entanglement, especially for the single-particle case. This is in contrast to the Greenberger–Horne–Zeilinger state, which by definition consists of multiple distinct particles or modes and their entanglement.\n\nIn other quantum mechanics contexts, according to \"The New York Times\" for example, physicists view the cat state as composed of two diametrically opposed conditions \"at the same time\", such as the possibilities that a cat be alive and dead at the same time. This is sometimes connected to the many worlds hypothesis by proponents of the many worlds interpretation of quantum mechanics. More prosaically, a cat state might be the possibilities that six atoms be \"spin up\" and \"spin down\", as published by a team led by David Wineland at NIST, December 1, 2005. Large cat states have also been experimentally created using photons by a team led by Jian-Wei Pan at University of Science and Technology of China, for instance, four-photon entanglement, five-photon entanglement, six-photon entanglement, eight-photon entanglement, and five-photon ten-qubit cat state. This spin up/down formulation was proposed by David Bohm, who conceived of spin as an observable in a version of thought experiments formulated in the 1935 EPR paradox.\n\nIn quantum optics, a cat state is defined as the coherent superposition of two coherent states with opposite phase:\nwhere\nand\nare coherent states defined in the number (Fock) basis. Notice that if we add the two states together, the resulting cat state only contains even Fock state terms:\nAs a result of this property, the above cat state is often referred to as an \"even\" cat state. Alternatively, we can define an \"odd\" cat state as\nwhich only contains odd Fock states\n\nEven and odd coherent states were first introduced by Dodonov, Malkin, and Man'ko in 1974.\n\nA simple example of a \"cat state\" is a linear superposition of coherent states with opposite phases, when each state has the same weight:\nThe larger the value of α, the lower the overlap between the two macroscopic classical coherent states exp(-2α), and the better it approaches an ideal cat state. However, the production of cat states with a large mean photon number (=|α|) is difficult. A typical way to produce approximate cat states is through photon subtraction from a squeezed vacuum state. This method usually is restricted to small values of α, and such states have been referred to as Schrödinger \"kitten\" states in the literature. Methods have been proposed to produce larger coherent state superpositions through multiphoton subtraction or through ancilla-assisted subtraction.\n\nCoherent state superpositions have been proposed for quantum computing by Sanders.\n"}
{"id": "42944052", "url": "https://en.wikipedia.org/wiki?curid=42944052", "title": "Causal fermion system", "text": "Causal fermion system\n\nThe theory of causal fermion systems is an approach to describe fundamental physics. Its proponents claim it gives quantum mechanics, general relativity and quantum field theory as limiting cases and is therefore a candidate for a unified physical theory.\n\nInstead of introducing physical objects on a preexisting space-time manifold, the general concept is to derive space-time as well as all the objects therein as secondary objects from the structures of an underlying causal fermion system. This concept also makes it possible to generalize notions of differential geometry to the non-smooth setting. In particular, one can describe situations when space-time no longer has a manifold structure on the microscopic scale (like a space-time lattice or other discrete or continuous structures on the Planck scale). As a result, the theory of causal fermion systems is a proposal for quantum geometry and an approach to quantum gravity.\n\nCausal fermion systems were introduced by Felix Finster and collaborators.\n\nThe physical starting point is the fact that the Dirac equation in Minkowski space has solutions of negative energy which are usually associated to the Dirac sea. Taking the concept seriously that the states of the Dirac sea form an integral part of the physical system, one finds that many structures (like the causal and metric structures as well as the bosonic fields) can be recovered from the wave functions of the sea states. This leads to the idea that the wave functions of all occupied states (including the sea states) should be regarded as the basic physical objects, and that all structures in space-time arise as a result of the collective interaction of the sea states with each other and with the additional particles and \"holes\" in the sea. Implementing this picture mathematically leads to the framework of causal fermion systems.\n\nMore precisely, the correspondence between the above physical situation and the mathematical framework is obtained as follows. All occupied states span a Hilbert space of wave functions in Minkowski space formula_1. The observable information on the distribution of the wave functions in space-time is encoded in the \"local correlation operators\" formula_2 which in an orthonormal basis formula_3 have the matrix representation\n(where formula_5 is the adjoint spinor).\nIn order to make the wave functions into the basic physical objects, one considers the set formula_6 as a set of linear operators on an \"abstract\" Hilbert space. The structures of Minkowski space are all disregarded, except for the volume measure formula_7, which is transformed to a corresponding measure on the linear operators (the \"universal measure\"). The resulting structures, namely a Hilbert space together with a measure on the linear operators thereon, are the basic ingredients of a causal fermion system.\n\nThe above construction can also be carried out in more general space-times. Moreover, taking the abstract definition as the starting point, causal fermion systems allow for the description of generalized \"quantum space-times.\" The physical picture is that one causal fermion system describes a space-time together with all structures and objects therein (like the causal and the metric structures, wave functions and quantum fields). In order to single out the physically admissible causal fermion systems, one must formulate physical equations. In analogy to the Lagrangian formulation of classical field theory, the physical equations for causal fermion systems are formulated via a variational principle, the so-called \"causal action principle\". Since one works with different basic objects, the causal action principle has a novel mathematical structure where one minimizes a positive action under variations of the universal measure. The connection to conventional physical equations is obtained in a certain limiting case (the continuum limit) in which the interaction can be described effectively by gauge fields coupled to particles and antiparticles, whereas the Dirac sea is no longer apparent.\n\nIn this section the mathematical framework of causal fermion systems is introduced.\n\nA causal fermion system of spin dimension formula_8 is a triple formula_9 where\nThe measure formula_15 is referred to as the universal measure.\n\nAs will be outlined below, this definition is rich enough to encode analogs of the mathematical structures needed to formulate physical theories. In particular, a causal fermion system gives rise to a space-time together with additional structures that generalize objects like spinors, the metric and curvature. Moreover, it comprises quantum objects like wave functions and a fermionic Fock state.\n\nInspired by the Langrangian formulation of classical field theory, the dynamics on a causal fermion system is described by a variational principle defined as follows.\n\nGiven a Hilbert space formula_10 and the spin dimension formula_13, the set formula_11 is defined as above. Then for any formula_21, the product formula_22 is an operator of rank at most formula_23. It is not necessarily self-adjoint because in general formula_24. We denote the non-trivial eigenvalues of the operator formula_22 (counting algebraic multiplicities) by\nMoreover, the spectral weight formula_27 is defined by\nThe Lagrangian is introduced by\nThe causal action is defined by\n\nThe causal action principle is to minimize formula_31 under variations of formula_15 within the class of (positive) Borel measures under the following constraints:\nHere on formula_37 one considers the topology induced by the formula_38-norm on the bounded linear operators on formula_39.\n\nThe constraints prevent trivial minimizers and ensure existence, provided that formula_39 is finite-dimensional.\nThis variational principle also makes sense in the case that the total volume formula_36 is infinite if one considers variations formula_42 of bounded variation with formula_43.\n\nIn contemporary physical theories, the word space-time refers to a Lorentzian manifold formula_44. This means that space-time is a set of points enriched by topological and geometric structures. In the context of causal fermion systems, space-time does not need to have a manifold structure. Instead, space-time formula_45 is a set of operators on a Hilbert space (a subset of formula_11). This implies additional inherent structures that correspond to and generalize usual objects on a space-time manifold.\n\nFor a causal fermion system formula_9,\nwe define space-time formula_45 as the support of the universal measure,\nWith the topology induced by formula_50,\nspace-time formula_45 is a topological space.\n\nFor formula_52, we denote the non-trivial eigenvalues of the operator formula_22 (counting algebraic multiplicities) by formula_54.\nThe points formula_55 and formula_56 are defined to be spacelike separated if all the formula_57 have the same absolute value. They are timelike separated if the formula_57 do not all have the same absolute value and are all real. In all other cases, the points formula_55 and formula_56 are lightlike separated.\n\nThis notion of causality fits together with the \"causality\" of the above causal action in the sense that if two space-time points formula_52 are space-like separated, then the Lagrangian formula_62 vanishes. This corresponds to the physical notion of causality that spatially separated space-time points do not interact. This causal structure is the reason for the notion \"causal\" in causal fermion system and causal action.\n\nLet formula_63 denote the orthogonal projection on the subspace formula_64. Then the sign of the functional\ndistinguishes the future from the past. In contrast to the structure of a partially ordered set, the relation \"lies in the future of\" is in general not transitive. But it is transitive on the macroscopic scale in typical examples.\n\nFor every formula_66 the spin space is defined by formula_67; it is a subspace of formula_39 of dimension at most formula_23. The spin scalar product formula_70 defined by\nis an indefinite inner product on formula_72 of signature formula_73 with formula_74.\n\nA wave function formula_75 is a mapping\nOn wave functions for which the norm formula_77 defined by\nis finite (where formula_79 is the absolute value of the symmetric operator formula_55), one can define the inner product\nTogether with the topology induced by the norm formula_77, one obtains a Krein space formula_83.\n\nTo any vector formula_84 we can associate the wave function\n(where formula_86 is again the orthogonal projection to the spin space).\nThis gives rise to a distinguished family of wave functions, referred to as the\nwave functions of the occupied states.\n\nThe kernel of the fermionic projector formula_87 is defined by\n(where formula_86 is again the orthogonal projection on the spin space,\nand formula_90 denotes the restriction to formula_91). The fermionic projector formula_92 is the operator\nwhich has the dense domain of definition given by all vectors formula_94 satisfying the conditions\nAs a consequence of the causal action principle, the kernel of the fermionic projector has additional normalization properties which justify the name projector.\n\nBeing an operator from one spin space to another, the kernel of the fermionic projector gives relations between different space-time points. This fact can be used to introduce a spin connection\nThe basic idea is to take a polar decomposition of formula_87. The construction becomes more involved by the fact that the spin connection should induce a corresponding metric connection\nwhere the tangent space formula_99 is a specific subspace of the linear operators on formula_72 endowed with a Lorentzian metric.\nThe spin curvature is defined as the holonomy of the spin connection,\nSimilarly, the metric connection gives rise to metric curvature. These geometric structures give rise to a proposal for a quantum geometry.\n\nIf formula_39 has finite dimension formula_103, choosing an orthonormal basis formula_104 of formula_39 and taking the wedge product of the corresponding wave functions\ngives a state of an formula_103-particle fermionic Fock space. Due to the total anti-symmetrization, this state depends on the choice of the basis of formula_39 only by a phase factor. This correspondence explains why the vectors in the particle space are to be interpreted as fermions. It also motivates the name causal fermion system.\n\nCausal fermion systems incorporate several physical principles in a specific way:\n\nCausal fermion systems have mathematically sound limiting cases that give a connection to conventional physical structures.\n\nStarting on any globally hyperbolic Lorentzian spin manifold formula_118 with spinor bundle formula_119, one gets into the framework of causal fermion systems by choosing formula_120 as a subspace of the solution space of the Dirac equation. Defining the so-called local correlation operator formula_121 for formula_122 by\n(where formula_124 is the inner product on the fibre formula_125) and introducing the universal measure as the push-forward of the volume measure on formula_1,\none obtains a causal fermion system. For the local correlation operators to be well-defined, formula_39 must consist of continuous sections, typically making it necessary to introduce a regularization on the microscopic scale formula_129. In the limit formula_130, all the intrinsic structures on the causal fermion system (like the causal structure, connection and curvature) go over to the corresponding structures on the Lorentzian spin manifold. Thus the geometry of space-time is encoded completely in the corresponding causal fermion systems.\n\nThe Euler-Lagrange equations corresponding to the causal action principle have a well-defined limit if the space-times formula_131 of the causal fermion systems go over to Minkowski space. More specifically, one considers a sequence of causal fermion systems (for example with formula_39 finite-dimensional in order to ensure the existence of the fermionick Fock state as well as of minimizers of the causal action), such that the corresponding wave functions go over to a configuration of interacting Dirac seas involving additional particle states or \"holes\" in the seas. This procedure, referred to as the continuum limit, gives effective equations having the structure of the Dirac equation coupled to classical field equations. For example, for a simplified model involving three elementary fermionic particles\nin spin dimension two, one obtains an interaction via a classical axial gauge field formula_133 described by the coupled Dirac- and Yang-Mills equations\nTaking the non-relativistic limit of the Dirac equation, one obtains the Pauli equation or the Schrödinger equation, giving the correspondence to quantum mechanics. Here formula_135 and formula_136 depend on the regularization and determine the coupling constant as well as the rest mass.\n\nLikewise, for a system involving neutrinos in spin dimension 4, one gets effectively a massive formula_137 gauge field coupled to the left-handed component of the Dirac spinors. The fermion configuration of the standard model can be described in spin dimension 16.\n\nFor the just-mentioned system involving neutrinos, the continuum limit also yields the Einstein field equations coupled to the Dirac spinors,\nup to corrections of higher order in the curvature tensor. Here the cosmological constant formula_139 is undetermined, and formula_140 denotes the energy-momentum tensor of the spinors and the formula_137 gauge field. The gravitation constant formula_142 depends on the regularization length.\n\nStarting from the coupled system of equations obtained in the continuum limit and expanding in powers of the coupling constant, one obtains integrals which correspond to Feynman diagrams on the tree level. Fermionic loop diagrams arise due to the interaction with the sea states, whereas bosonic loop diagrams appear when taking averages over the microscopic (in generally non-smooth) space-time structure of a causal fermion system (method of microscopic mixing). The detailed analysis and comparison with standard quantum field theory is work in progress.\n\n"}
{"id": "9337483", "url": "https://en.wikipedia.org/wiki?curid=9337483", "title": "Charles Loewner", "text": "Charles Loewner\n\nCharles Loewner (29 May 1893 – 8 January 1968) was an American mathematician. His name was Karel Löwner in Czech and Karl Löwner in German.\n\nKarl Loewner was born into a Jewish family in Lany, about 30 km from Prague, where his father Sigmund Löwner was a store owner.\n\nLoewner received his Ph.D. from the University of Prague in 1917 under supervision of Georg Pick.\nOne of his central mathematical contributions is the proof of the Bieberbach conjecture in the first highly nontrivial case of the third coefficient. The technique he introduced, the Loewner differential equation, has had far-reaching implications in geometric function theory; it was used in the final solution of the Bieberbach conjecture by Louis de Branges in 1985. Loewner worked at the University of Berlin, University of Prague, University of Louisville, Brown University, Syracuse University and eventually at Stanford University. His students include Lipman Bers, Roger Horn, Adriano Garsia, and P. M. Pu.\n\nIn 1949 Loewner proved his torus inequality, to the effect that every metric on the 2-torus satisfies the optimal inequality\n\nwhere sys is its systole. The boundary case of equality is attained if and only if the metric is flat and homothetic to the so-called \"equilateral torus\", i.e. torus whose group of deck transformations is precisely the hexagonal lattice spanned by the cube roots of unity in formula_2.\n\nThe Loewner matrix (in linear algebra) is a square matrix or, more specifically, a linear operator (of real formula_3 functions) associated with 2 input parameters consisting of (1) a real continuously differentiable function on a subinterval of the real numbers and (2) an formula_4-dimensional vector with elements chosen from the subinterval; the 2 input parameters are assigned an output parameter consisting of an formula_5 matrix.\n\nLet formula_6 be a real-valued function that is continuously differentiable on the open interval formula_7.\n\nFor any formula_8 define the divided difference of formula_6 at formula_10 as\nGiven formula_15, the Loewner matrix formula_16 associated with formula_6 for formula_18 is defined as the formula_5 matrix whose formula_20-entry is formula_21.\n\nIn his fundamental 1934 paper, Loewner proved that for each positive integer formula_4, formula_6 is formula_4-monotone on formula_7 if and only if formula_16 is positive semidefinite for any choice of formula_27. Most significantly, using this equivalence, he proved that formula_6 is formula_4-monotone on formula_7 for all formula_4 if and only if formula_6 is real analytic with an analytic continuation to the upper half plane that has a positive imaginary part on the upper plane.\n\n\n\n\n"}
{"id": "596282", "url": "https://en.wikipedia.org/wiki?curid=596282", "title": "Class number problem", "text": "Class number problem\n\nIn mathematics, the Gauss class number problem (for imaginary quadratic fields), as usually understood, is to provide for each \"n\" ≥ 1 a complete list of imaginary quadratic fields formula_1 (for negative integers \"d\") having class number \"n\". It is named after Carl Friedrich Gauss. It can also be stated in terms of discriminants. There are related questions for real quadratic fields and for the behavior as formula_2.\n\nThe difficulty is in effective computation of bounds: for a given discriminant, it is easy to compute the class number, and there are several ineffective lower bounds on class number (meaning that they involve a constant that is not computed), but effective bounds (and explicit proofs of completeness of lists) are harder.\n\nThe problems are posed in Gauss's Disquisitiones Arithmeticae of 1801 (Section V, Articles 303 and 304).\n\nGauss discusses imaginary quadratic fields in Article 303, stating the first two conjectures, and discusses real quadratic fields in Article 304, stating the third conjecture.\n\nThe original Gauss class number problem for imaginary quadratic fields is significantly different and easier than the modern statement: he restricted to even discriminants, and allowed non-fundamental discriminants.\n\n\nFor imaginary quadratic number fields, the (fundamental) discriminants of class number 1 are:\nThe non-fundamental discriminants of class number 1 are:\nThus, the even discriminants of class number 1, fundamental and non-fundamental (Gauss's original question) are:\n\nIn 1934, Hans Heilbronn proved the Gauss Conjecture. Equivalently, for any given class number, there are only finitely many imaginary quadratic number fields with that class number.\n\nAlso in 1934, Heilbronn and Edward Linfoot showed that there were at most 10 imaginary quadratic number fields with class number 1 (the 9 known ones, and at most one further).\nThe result was ineffective (see effective results in number theory): it did not give bounds on the size of the remaining field.\n\nIn later developments, the case \"n\" = 1 was first discussed by Kurt Heegner, using modular forms and modular equations to show that no further such field could exist. This work was not initially accepted; only with later work of Harold Stark and Bryan Birch was the position clarified, and Heegner's work understood. See Stark–Heegner theorem, Heegner number. Practically simultaneously, Alan Baker proved what we now know as Baker's theorem on linear forms in logarithms of algebraic numbers, which resolved the problem by a completely different method. The case \"n\" = 2 was tackled shortly afterwards, at least in principle, as an application of Baker's work. (see .)\n\nThe complete list of imaginary quadratic fields with class number one is formula_7 with \"k\" one of\n\nThe general case awaited the discovery of Dorian Goldfeld that the class number problem could be connected to the L-functions of elliptic curves. This reduced the question, in principle, of effective determination, to one about establishing the existence of a multiple zero of such an L-function. This could be done on the basis of the later Gross-Zagier theorem. So at that point one could specify a finite calculation, the result of which would be a complete list for a given class number. In fact in practice such lists that are \"probably\" complete can be made by relatively simple methods; what is at issue is certainty. The cases up to \"n\" = 100 have now (2004) been done: see Watkins (2004).\n\nThe contrasting case of \"real\" quadratic fields is very different, and much less is known. That is because what enters the analytic formula for the class number is not \"h\", the class number, on its own — but \"h\" log \"ε\", where \"ε\" is a fundamental unit. This extra factor is hard to control. It may well be the case that class number 1 for real quadratic fields occurs infinitely often.\n\nThe Cohen-Lenstra heuristics are a set of more precise conjectures about the structure of class groups of quadratic fields. For real fields they predict that about 75.446% of the fields obtained by adjoining the square root of a prime will have class number 1, a result that agrees with computations.\n\n\n"}
{"id": "1428810", "url": "https://en.wikipedia.org/wiki?curid=1428810", "title": "Complex adaptive system", "text": "Complex adaptive system\n\nA complex adaptive system is a system in which a perfect understanding of the individual parts does not automatically convey a perfect understanding of the whole system's behavior. The study of complex adaptive systems, a subset of nonlinear dynamical systems, is highly interdisciplinary and blends insights from the natural and social sciences to develop system-level models and insights that allow for heterogeneous agents, phase transition, and emergent behavior.\n\nThey are \"complex\" in that they are dynamic networks of interactions, and their relationships are not aggregations of the individual static entities, i.e., the behavior of the ensemble is not predicted by the behavior of the components. They are \"adaptive\" in that the individual and collective behavior mutate and self-organize corresponding to the change-initiating micro-event or collection of events. They are a \"complex macroscopic collection\" of relatively \"similar and partially connected micro-structures\" formed in order to adapt to the changing environment and increase their survivability as a macro-structure.\n\nThe term \"complex adaptive systems\", or \"complexity science\", is often used to describe the loosely organized academic field that has grown up around the study of such systems. Complexity science is not a single theory—it encompasses more than one theoretical framework and is highly interdisciplinary, seeking the answers to some fundamental questions about living, adaptable, changeable systems. The study of CAS focuses on complex, emergent and macroscopic properties of the system. John H. Holland said that CAS \"are systems that have a large numbers of components, often called agents, that interact and adapt or learn.\"\n\nTypical examples of complex adaptive systems include: climate; cities; firms; markets; governments; industries; ecosystems; social networks; power grids; animal swarms; traffic flows; social insect (e.g. ant) colonies; the brain and the immune system; and the cell and the developing embryo. Human social group-based endeavors, such as political parties, communities, geopolitical organizations, war, and terrorist networks are also considered CAS. The internet and cyberspace—composed, collaborated, and managed by a complex mix of human–computer interactions, is also regarded as a complex adaptive system. CAS can be hierarchical, but more often exhibit aspects of \"self-organization.\"\n\nWhat distinguishes a CAS from a pure multi-agent system (MAS) is the focus on top-level properties and features like self-similarity, complexity, emergence and self-organization. A MAS is defined as a system composed of multiple interacting agents; whereas in CAS, the agents as well as the system are adaptive and the system is self-similar. A CAS is a complex, self-similar collectivity of interacting, adaptive agents. Complex Adaptive Systems are characterized by a high degree of adaptive capacity, giving them resilience in the face of perturbation.\n\nOther important properties are adaptation (or homeostasis), communication, cooperation, specialization, spatial and temporal organization, and reproduction. They can be found on all levels: cells specialize, adapt and reproduce themselves just like larger organisms do. Communication and cooperation take place on all levels, from the agent to the system level. The forces driving co-operation between agents in such a system, in some cases, can be analyzed with game theory.\n\nSome of the most important characteristics of complex systems are:\n\n\nRobert Axelrod & Michael D. Cohen identify a series of key terms from a modeling perspective:\n\nCAS are occasionally modeled by means of agent-based models and complex network-based models. Agent-based models are developed by means of various methods and tools primarily by means of first identifying the different agents inside the model. Another method of developing models for CAS involves developing complex network models by means of using interaction data of various CAS components.\n\nIn 2013 SpringerOpen/BioMed Central has launched an online open-access journal on the topic of \"complex adaptive systems modeling\" (CASM).\n\nLiving organisms are complex adaptive systems. Although complexity is hard to quantify in biology, evolution has produced some remarkably complex organisms. This observation has led to the common misconception of evolution being progressive and leading towards what are viewed as \"higher organisms\".\n\nIf this were generally true, evolution would possess an active trend towards complexity. As shown below, in this type of process the value of the most common amount of complexity would increase over time. Indeed, some artificial life simulations have suggested that the generation of CAS is an inescapable feature of evolution.\n\nHowever, the idea of a general trend towards complexity in evolution can also be explained through a passive process. This involves an increase in variance but the most common value, the mode, does not change. Thus, the maximum level of complexity increases over time, but only as an indirect product of there being more organisms in total. This type of random process is also called a bounded random walk.\n\nIn this hypothesis, the apparent trend towards more complex organisms is an illusion resulting from concentrating on the small number of large, very complex organisms that inhabit the right-hand tail of the complexity distribution and ignoring simpler and much more common organisms. This passive model emphasizes that the overwhelming majority of species are microscopic prokaryotes, which comprise about half the world's biomass and constitute the vast majority of Earth's biodiversity. Therefore, simple life remains dominant on Earth, and complex life appears more diverse only because of sampling bias.\n\nIf there is a lack of an overall trend towards complexity in biology, this would not preclude the existence of forces driving systems towards complexity in a subset of cases. These minor trends would be balanced by other evolutionary pressures that drive systems towards less complex states.\n\n\n"}
{"id": "1802169", "url": "https://en.wikipedia.org/wiki?curid=1802169", "title": "Continuous linear operator", "text": "Continuous linear operator\n\nIn functional analysis and related areas of mathematics, a continuous linear operator or continuous linear mapping is a continuous linear transformation between topological vector spaces.\n\nAn operator between two normed spaces is a bounded linear operator if and only if it is a continuous linear operator.\n\nIf for every formula_1 there exists a formula_2 such that\nformula_3\nwe say the operator formula_4 is continuous.\n\nA continuous linear operator maps bounded sets into bounded sets. A linear functional is continuous if and only if its kernel is closed. Every linear function on a finite-dimensional space is continuous.\n\nThe following are equivalent: given a linear operator \"A\" between topological spaces \"X\" and \"Y\":\n\nThe proof uses the facts that the translation of an open set in a linear topological space is again an open set, and the equality\nfor any set \"D\" in \"Y\" and any \"x\" in \"X\", which is true due to the additivity of \"A\".\n"}
{"id": "43466503", "url": "https://en.wikipedia.org/wiki?curid=43466503", "title": "Convenient vector space", "text": "Convenient vector space\n\nIn mathematics, convenient vector spaces are locally convex vector spaces satisfying a very mild completeness condition. \nTraditional differential calculus is effective in the analysis of finite-dimensional vector spaces and for Banach spaces. Beyond Banach spaces, difficulties begin to arise; in particular, composition of continuous linear mappings stop being jointly continuous at the level of Banach spaces, for any compatible topology on the spaces of continuous linear mappings.\n\nMappings between convenient vector spaces are smooth or if they map smooth curves to smooth curves. This leads to a Cartesian closed category of smooth mappings between -open subsets of convenient vector spaces (see property 6 below). The corresponding calculus of smooth mappings is called \"convenient calculus\".\nIt is weaker than any other reasonable notion of differentiability, it is easy to apply, but there are smooth mappings which are not continuous (see Note 1).\nThis type of calculus alone is not useful in solving equations.\n\nLet be a locally convex vector space. A curve is called \"smooth\" or if all derivatives exist and are continuous. Let \ntopology of \"E\", only on its associated bornology (system of bounded sets); see [KM], 2.11.\nThe final topologies with respect to the following sets of mappings into coincide; see [KM], 2.13.\nThis topology is called the -\"topology\" on and we write \nIn general \n(on the space of smooth functions with compact support on the real line, for example) it is finer \nthan the given locally convex topology, it is not a vector space \ntopology, since addition is no longer jointly continuous. Namely, even \nThe finest among all locally convex topologies on \nwhich are coarser than is the bornologification of the \ngiven locally convex topology. If is a Fréchet space, then .\n\nA locally convex vector space is said to be a \"convenient vector space\" if one of the following equivalent conditions holds (called -completeness); see [KM], 2.14.\nHere a mapping is called if all \nderivatives up to order exist and are Lipschitz, locally on .\n\nLet and be convenient vector spaces, \nand let be -open. \nA mapping is called \"smooth\" or \n, if the composition for all . See[KM], 3.11.\n\n1. For maps on Fréchet spaces this notion of smoothness coincides with all other reasonable definitions. On this is a non-trivial theorem, proved by Boman, 1967. See also [KM], 3.4.\n\n2. Multilinear mappings are smooth if and only if they are bounded ([KM], 5.5).\n\n3. If is smooth then the derivative is smooth, and also is smooth where denotes the space of all bounded linear mappings with the topology of uniform convergence on bounded subsets; see [KM], 3.18.\n\n4. The chain rule holds ([KM], 3.18).\n\n5. The space of all smooth mappings is again a convenient vector space where the structure is given by the following injection, where carries the topology of compact convergence in each derivative separately; see [KM], 3.11 and 3.7.\n\n6. The \"exponential law\" holds ([KM], 3.12): For -open the following mapping is a linear diffeomorphism of convenient vector spaces. \nThis is the main assumption of variational calculus. Here it is a theorem. This property is the source of the name \"convenient\", which was borrowed from (Steenrod 1967).\n\n7. \"Smooth uniform boundedness theorem\" ([KM], theorem 5.26). \nA linear mapping is smooth (by (2) equivalent to bounded) if and only if is smooth for each .\n\n8. The following canonical mappings are smooth. This follows from the exponential law by simple categorical reasonings, see [KM], 3.13.\n\nConvenient calculus of smooth mappings appeared for the first time in [Frölicher, 1981], [Kriegl 1982, 1983].\nConvenient calculus (having properties 6 and 7) exists also for:\nThe corresponding notion of convenient vector space is the same (for their underlying real vector space in the complex case) for all these theories.\n\nThe exponential law 6 of convenient calculus allows for very simple proofs of the basic facts about manifolds of mappings. \nLet and be finite dimensional smooth manifolds where is compact. We use an \nauxiliary Riemann metric formula_4 on . The Riemannian exponential mapping of formula_4 is described in the following diagram:\nIt induces an atlas of charts on the space of all smooth mappings as follows.\nA chart centered at , is:\nNow the basics facts follow in easily.\nTrivializing the pull back vector bundle and applying the exponential law 6 leads to the diffeomorphism\nAll chart change mappings are smooth () since they map smooth curves to smooth curves: \nThus is a smooth manifold modelled on Fréchet spaces. The space of all smooth curves in this manifold is given by\nSince it visibly maps smooth curves to smooth curves, \"composition\"\nis smooth. As a consequence of the chart structure, the tangent bundle of the manifold of mappings is given by \n\nLet be a connected smooth Lie group modeled on convenient vector spaces, with Lie algebra \nformula_14. Multiplication and inversion are denoted by:\nThe notion of a regular Lie group is originally due to Omori et al. for Fréchet Lie groups, was weakened and made more transparent by J.Milnor, and was then carried over to convenient Lie groups; see [KM], 38.4.\n\nA Lie group is called \"regular\" if the following two conditions hold:\nIf is the unique solution for the curve required above, we denote \nIf is a constant curve in the Lie algebra, then is the group exponential mapping.\n\nTheorem. For each compact manifold , the diffeomorphism group is a regular Lie group. Its Lie algebra is the space formula_21 of all smooth vector fields on , with the negative of the usual bracket as Lie bracket.\n\n\"Proof:\" The diffeomorphism group is a smooth manifold since it is an open subset in . Composition is smooth by restriction. Inversion is smooth: If is a smooth curve in , then satisfies the implicit equation \nformula_22, so by the finite dimensional implicit function theorem, is smooth. So inversion maps smooth curves to smooth curves, and thus inversion is smooth.\nLet be a time dependent vector field on (in formula_23).\nThen the flow operator of the corresponding autonomous vector field formula_24 on induces the evolution operator via\nwhich satisfies the ordinary differential equation \nGiven a smooth curve in the Lie algebra, formula_27,\nthen the solution of the ordinary differential equation depends smoothly also on the further variable ,\nthus maps smooth curves of time dependent vector fields to smooth curves of \ndiffeomorphism. QED.\n\nFor finite dimensional manifolds and with compact, the space of all smooth embeddings of into , is open in , so it is a smooth manifold. The diffeomorphism group acts freely and smoothly from the right on .\n\nTheorem: is a principal fiber bundle with structure group .\n\n\"Proof:\" One uses again an auxiliary Riemannian metric formula_4 on . Given , view as a submanifold of , and split the restriction of the tangent bundle to into the subbundle normal to and tangential to as\nformula_29. Choose a tubular neighborhood \nIf is -near to , then \nThis is the required local splitting. QED\n\nAn overview of applications using geometry of shape spaces and diffeomorphism groups can be found in [Bauer, Bruveris, Michor, 2014].\n\n"}
{"id": "23429522", "url": "https://en.wikipedia.org/wiki?curid=23429522", "title": "Corner-point grid", "text": "Corner-point grid\n\nIn geometry, a corner-point grid is a tessellation of a Euclidean 3D volume where the base cell has 6 faces (hexahedron).\n\nA set of straight lines defined by their end points define the \"pillars\" of the corner-point grid. The pillars have a lexicographical ordering that determines neighbouring pillars. On each pillar, a constant number of nodes (corner-points) is defined. A corner-point cell is now the volume between 4 neighbouring pillars and two neighbouring points on each pillar. \n\nEach cell can be identified by integer coordinates formula_1, where the formula_2 coordinate runs along the pillars, and formula_3 and formula_4 span each layer. The cells are ordered naturally, where the index formula_3 runs the fastest and formula_2 the slowest.\n\nData within the interior of such cells can be computed by trilinear interpolation from the boundary values at the 8 corners, 12 edges, and 6 faces.\n\nIn the special case of all pillars being vertical, the top and bottom face of each corner-point cell are described by bilinear surfaces and the side faces are planes.\n\nCorner-point grids are supported by most reservoir simulation software, and has become an industry standard.\n\nA main feature of the format is the ability to define erosion surfaces in geological modelling, effectively done by collapsing nodes along each pillar. This means that the corner-point cells degenerate and may have less than 6 faces.\n\nFor the corner-point grids non-neighboring connections are supported, meaning that grid cells that are not neighboring in ijk-space can be defined as neighboring. This feature allows for representation of faults with significant throw/displacement. Moreover, the neighboring grid cells do not need to have matching cell faces (just overlap).\n\n"}
{"id": "55906881", "url": "https://en.wikipedia.org/wiki?curid=55906881", "title": "Cyborg data mining", "text": "Cyborg data mining\n\nCyborg data mining is the practice of collecting data produced by an implantable device that monitors bodily processes for commercial interests. As an android is a human-like robot, a cyborg, on the other hand, is an organism whose physiological functioning is aided by or dependent upon a mechanical/electronic device that relies on some sort of feedback.\n\nImplantable cybernetics and biomechatronics are on course to be proliferated among the global population within the twenty-first century as the markets for implantable electronics are already huge and growing. The global market for artificial cardiac pacemakers (PMs) and implantable cardioverter-defibrillators (ICDs) was approximately €8 billion in 2015, and is growing at 10% per year. Over 350 million people worldwide experience endemic diseases, diabetes, cardiac and renal failure, hearing disorders, and neurological disorders, thus making implantable technologies specific to these uses susceptible to increasingly higher demand. However, for the millions of cyborgs already equipped with body-enhancing technologies, namely PMs and ICDs, the data mining of these technologies pertains to broader topics of data sovereignty, data ownership rights, privacy and security, and medical research and development.\n\nAccording to European Directive 90/385/EEC, an \"active implantable medical device\" is any device that is intended to be used for human beings in the: 1) diagnosis, prevention, monitoring, treatment, or alleviation of disease or injury; 2) investigation, replacement, or modification of the anatomy or of a physiological process; and 3) control of conception. The implantable device is to be totally or partially introduced, surgically or medically, into the human body, and is intended to remain after the procedure.\n\nAccording to one definition of the term cyborg, basic technologies, such as implantable medical devices, that humanity has physical attachments with have already made humans into cyborgs. These technologies are responsible for enhancing people’s cognitive abilities, or more importantly, keeping them alive. The three most common implantable technologies are cochlear implants, PMs, and ICDs. Cochlear implants aid in the process of hearing, and are used by more than 200,000 patients worldwide. PMs and ICDs keep people alive through the measurement of bodily voltage levels, measurement of regular and irregular heartbeats, and the delivery of electric impulses when irregularities are sensed in order to keep the person alive. There are about 3 million people worldwide with pacemakers, and each year 600,000 pacemakers are implanted. The data collected from these technologies, however, is not owned by the person whose body the technology is in, but rather the company who owns the intellectual property to that technology, as well as other third-parties.\n\nCompanies are now able to mine the data exhaust from internet-enabled wearable and implantable technologies, such as medical and fitness tracking devices (Fitbit, Apple Watch Nike+, etc.), sensors, PMs, RFID (Radio Frequency Identification) microchips, and so forth. However, consumers in the U.S. do not have agency over their data due to current intellectual property law and the third-party doctrine. Intellectual property owners of the software, as well as the patented hardware and processes, of these devices acquire the data from the cyborg’s bodily processes via these implantable devices, which become property of the owner, not the cyborg. Consumers surrender these massive pools of data via an End-User-License-Agreement (EULAs), terms of service agreements, and so forth. Companies then algorithmically arrange data, and consumers lose ownership of their data to the intellectual property owners and data brokerage firms to commodify, thus becoming a part of the larger Big Data economy. In a $300 billion-a-year industry, currently no legislation specific to the regulation of third-party data broker firms exists. Third-party data broker firms are not restricted by Federal Trade Commission regulations, including the Fair Credit Reporting Act, as well as the Gramm-Leach-Bliley Act. It is very difficult for consumers to opt-in and out of having data collected about them, whereby their data ownership rights become very limited.\n\nUnder the third-party doctrine, an individual does not have a reasonable expectation of privacy with respect to information they voluntarily disclose to a third party. In the context of the PM, which monitors a patient’s heartbeat, blood temperature, breathing, and heart electrical activity, this extracted data is voluntarily given to a third party, and thus subject to the third-party doctrine. The five largest PM manufacturers in the world are the U.S.’s Medtronic ($1.9 billion in global PM sales in 2013), St. Jude Medical (nearly $1 billion in global PM sales in 2013), and Boston Scientific ($514 million in global PM sales in 2013), Germany's Biotronik ($397 million in global PM sales in 2013), and Italy's Sorin Group ($219 million global PM sales in 2013). PM users have no agency over their data, nor over who has the ability to access it, and current laws impose no requirement for manufacturers to allow PM users access to their own data. The notable activist, Hugo Campo, has been fighting for the right to access the data collected by his own defibrillator for years without success due to the logic of PM data being covered by the third-party doctrine.\n\nIn April 2016, the European Union tabled legislation for the General Data Protection Regulation (GDPR), which will replace the Data Protection Directive 95/46/EC of 1995, and comes into force on May 25, 2018. Internet-enabled wearable and future implantable technologies will fall under the purview of this legislation. The directive will implement tougher fines for non-compliance and breaches, and gives consumers more control over how their personal data is used. Some of these consumer rights are, but are not limited to, having the right:\n\n\nAs cyborgs are comprehensive data subjects, they can also be used as a powerful instrument to facilitate surveillance and sousveillance through optical recording technology. Some data collection from cyborgs can be harmless, namely posting pictures to Facebook, or recording one’s life experiences. However, cyborgs can serve as a means of surveillance on the overall populace via sousveillance. Sousveillance is the notion of a populace watching the state from below, whereby the notable University of Toronto professor and cyborg, Steve Mann, has advocated that sousveillant devices can “invert the panopticon” and challenge and balance the hypocrisy and corruption that is otherwise inherent in a surveillance-only society. Sousveillant technologies secure the cyborg and individuals, especially by deterring and documenting crime, but potentially infringes privacy on cyborgs and non-cyborgs as well. Google Glass, for example, is an optical recording device that presents concerns toward privacy in public. The ability for cyborgs to record everyday routines and interactions with others thus present the question of how society and laws are to respond to the advent of cyborgs being subjects and instruments of surveillance and sousveillance.\n\nData mined from bodily processes are able to help companies in their research and development endeavours in developing better technologies, and conducting invaluable medical research for identifying and managing various conditions. PMs and ICDs offered by major companies come with wireless capabilities that communicate with home transmitters, which then relay data to the physician, and thus allow for remote patient follow-up and monitoring. These systems for remote follow-up are used widely across the U.S. and Europe. Major PM and ICD producers have their own remote monitoring system networks, such as Biotronik’s Home Monitoring, Medtronic’s CareLink Network, Boston Scientific’s Latitude Patient Management system, and St. Jude Medical’s Merlin.net. The benefits of this data collection include a reduction of in-clinic visits, improved patient safety, increased patient satisfaction, and potential cost savings for consumers. The ability to remotely follow up because of PM and ICD data collection allows for tracking product performance in a large number of patients, and may allow earlier identification of issues with specific models.\n\nThe data collected from PMs and ICDs have the potential to facilitate critical medical research. Namely, Medtronic collects and analyzes the data generated by its pacemakers and defibrillators via the CareLink system. Medtronic is using the collected PM data and is working with researchers at Johns Hopkins Hospital and Washington University School of Medicine in order to help answer specific questions about heart disease, such as whether weak hearts cause arrhythmias or vice versa. Although this aspect of the technology is not widely proliferated yet, scientists and industry developers say that wireless devices could trigger an automatic treatment, which could range from electrical stimulation to the release of drugs.\n"}
{"id": "34794178", "url": "https://en.wikipedia.org/wiki?curid=34794178", "title": "Derek Corneil", "text": "Derek Corneil\n\nDerek Gordon Corneil is a Canadian mathematician and computer scientist, a professor \"emeritus\" of computer science at the University of Toronto, and an expert in graph algorithms and graph theory.\n\nWhen he was leaving high school, Corneil was told by his English teacher that doing a degree in mathematics and physics was a bad idea, and that the best he could hope for was to go to a technical college. His interest in computer science began when, as an undergraduate student at Queens College, he heard that a computer was purchased by the London Life insurance company in London, Ontario, where his father worked. As a freshman, he took a summer job operating the UNIVAC Mark II at the company. One of his main responsibilities was to operate a printer. An opportunity for a programming job with the company sponsoring his college scholarship appeared soon after. It was a chance that Corneil jumped at after being denied a similar position at London Life. There was an initial mix-up at his job as his overseer thought that he knew how to program the UNIVAC Mark II, and so he would easily transition to doing the same for the company's newly acquired IBM 1401 machine. However, Corneil did not have the assumed programming background. Thus, in the two-week window that Corneil had been given to learn how to grasp programming the IBM 1401, he learned how to write code from scratch by relying heavily on the instruction manual. This experience pushed him further on his way as did a number of projects he worked on in that position later on.\n\nCorneil went on to earn a bachelor's degree in mathematics and physics from Queen's University in 1964. Initially he had planned to do his graduate studies before becoming a high school teacher, but his acceptance into the brand new graduate program in computer science at the University of Toronto changed that. At the University of Toronto, Corneil earned a master's degree and then in 1968 a doctorate in computer science under the supervision of Calvin Gotlieb. (His post-doctoral supervisor was Jaap Seidel.) It was during this time that Corneil became interested in graph theory. He and Gotlieb eventually became good friends. After postdoctoral studies at the Eindhoven University of Technology, Corneil returned to Toronto as a faculty member in 1970. Before his retirement in 2010, Corneil held many positions at the University of Toronto, including Department Chair of the Computer Science department (July 1985 to June 1990), Director of Research Initiatives of the Faculty of Arts and Science (July 1991 to March 1998), and Acting Vice President of Research and International Relations (September to December 1993). During his time as a professor, he was also a visiting professor at universities such as the University of British Columbia, Simon Fraser University, the Université de Grenoble and the Université de Montpellier.\n\nCorneil did his research in algorithmic graph theory and graph theory in general. He has overseen 49 theses and published over 100 papers on his own or with co-authors. These papers include:\n\nAs a professor \"emeritus\", Corneil still does research and is also an editor of several publications such as \"Ars Combinatoria\" and \"SIAM Monographs on Discrete Mathematics and Applications\".\n\nHe was inducted as a Fields Institute Fellow in 2004.\n\n"}
{"id": "7921", "url": "https://en.wikipedia.org/wiki?curid=7921", "title": "Derivative", "text": "Derivative\n\nThe derivative of a function of a real variable measures the sensitivity to change of the function value (output value) with respect to a change in its argument (input value). Derivatives are a fundamental tool of calculus. For example, the derivative of the position of a moving object with respect to time is the object's velocity: this measures how quickly the position of the object changes when time advances.\n\nThe derivative of a function of a single variable at a chosen input value, when it exists, is the slope of the tangent line to the graph of the function at that point. The tangent line is the best linear approximation of the function near that input value. For this reason, the derivative is often described as the \"instantaneous rate of change\", the ratio of the instantaneous change in the dependent variable to that of the independent variable.\n\nDerivatives may be generalized to functions of several real variables. In this generalization, the derivative is reinterpreted as a linear transformation whose graph is (after an appropriate translation) the best linear approximation to the graph of the original function. The Jacobian matrix is the matrix that represents this linear transformation with respect to the basis given by the choice of independent and dependent variables. It can be calculated in terms of the partial derivatives with respect to the independent variables. For a real-valued function of several variables, the Jacobian matrix reduces to the gradient vector.\n\nThe process of finding a derivative is called differentiation. The reverse process is called \"antidifferentiation\". The fundamental theorem of calculus states that antidifferentiation is the same as integration. Differentiation and integration constitute the two fundamental operations in single-variable calculus.\n\n\"Differentiation\" is the action of computing a derivative. The derivative of a function of a variable is a measure of the rate at which the value of the function changes with respect to the change of the variable . It is called the \"derivative\" of with respect to . If and are real numbers, and if the graph of is plotted against , the derivative is the slope of this graph at each point.\nThe simplest case, apart from the trivial case of a constant function, is when is a linear function of , meaning that the graph of is a line. In this case, , for real numbers and , and the slope is given by\nwhere the symbol (Delta) is an abbreviation for \"change in\". This formula is true because\nThus, since\nit follows that\n\nThis gives an exact value for the slope of a line.\nIf the function is not linear (i.e. its graph is not a straight line), however, then the change in divided by the change in varies: differentiation is a method to find an exact value for this rate of change at any given value of .\n\nThe idea, illustrated by Figures 1 to 3, is to compute the rate of change as the limit value of the ratio of the differences as becomes infinitely small.\n\nTwo distinct notations are commonly used for the derivative, one deriving from Leibniz and the other from Joseph Louis Lagrange.\n\nIn Leibniz's notation, an infinitesimal change in is denoted by , and the derivative of with respect to is written\nsuggesting the ratio of two infinitesimal quantities. (The above expression is read as \"the derivative of \"y\" with respect to \"x\", \"dy\" by \"dx\", or \"dy\" over \"dx\". The oral form \"dy\" \"dx\" is often used conversationally, although it may lead to confusion.)\n\nIn Lagrange's notation, the derivative with respect to of a function is denoted (read as \"f\" prime of \"x\") or (read as \"f\" prime \"x\" of \"x\"\"), in case of ambiguity of the variable implied by the derivation. Lagrange's notation is sometimes incorrectly attributed to Newton.\n\nThe most common approach to turn this intuitive idea into a precise definition is to define the derivative as a limit of difference quotients of real numbers. This is the approach described below.\n\nLet be a real valued function defined in an open neighborhood of a real number . In classical geometry, the tangent line to the graph of the function at was the unique line through the point that did \"not\" meet the graph of transversally, meaning that the line did not pass straight through the graph. The derivative of with respect to at is, geometrically, the slope of the tangent line to the graph of at . The slope of the tangent line is very close to the slope of the line through and a nearby point on the graph, for example . These lines are called secant lines. A value of close to zero gives a good approximation to the slope of the tangent line, and smaller values (in absolute value) of will, in general, give better approximations. The slope of the secant line is the difference between the values of these points divided by the difference between the values, that is, \n\nThis expression is Newton's difference quotient. Passing from an approximation to an exact answer is done using a limit. Geometrically, the limit of the secant lines is the tangent line. Therefore, the limit of the difference quotient as approaches zero, if it exists, should represent the slope of the tangent line to . This limit is defined to be the derivative of the function at :\n\nWhen the limit exists, is said to be \"differentiable\" at . Here is one of several common notations for the derivative (see below).\n\nEquivalently, the derivative satisfies the property that\nwhich has the intuitive interpretation (see Figure 1) that the tangent line to at gives the \"best linear approximation\"\nto near (i.e., for small ). This interpretation is the easiest to generalize to other settings (see below).\n\nSubstituting 0 for in the difference quotient causes division by zero, so the slope of the tangent line cannot be found directly using this method. Instead, define to be the difference quotient as a function of :\n\nIn practice, the existence of a continuous extension of the difference quotient to is shown by modifying the numerator to cancel in the denominator. Such manipulations can make the limit value of for small clear even though is still not defined at . This process can be long and tedious for complicated functions, and many shortcuts are commonly used to simplify the process.\n\nRelative to a hyperreal extension of the real numbers, the derivative of a real function at a real point can be defined as the shadow of the quotient for infinitesimal , where . Here the natural extension of to the hyperreals is still denoted . Here the derivative is said to exist if the shadow is independent of the infinitesimal chosen.\n\nThe squaring function given by is differentiable at , and its derivative there is 6. This result is established by calculating the limit as approaches zero of the difference quotient of :\n\nThe last expression shows that the difference quotient equals when and is undefined when , because of the definition of the difference quotient. However, the definition of the limit says the difference quotient does not need to be defined when . The limit is the result of letting go to zero, meaning it is the value that tends to as becomes very small:\n\nHence the slope of the graph of the squaring function at the point is , and so its derivative at is .\n\nMore generally, a similar computation shows that the derivative of the squaring function at is :\n\nIf is differentiable at , then must also be continuous at . As an example, choose a point and let be the step function that returns the value 1 for all less than , and returns a different value 10 for all greater than or equal to . cannot have a derivative at . If is negative, then is on the low part of the step, so the secant line from to is very steep, and as tends to zero the slope tends to infinity. If is positive, then is on the high part of the step, so the secant line from to has slope zero. Consequently, the secant lines do not approach any single slope, so the limit of the difference quotient does not exist.\n\nHowever, even if a function is continuous at a point, it may not be differentiable there. For example, the absolute value function given by is continuous at , but it is not differentiable there. If is positive, then the slope of the secant line from 0 to is one, whereas if is negative, then the slope of the secant line from 0 to is negative one. This can be seen graphically as a \"kink\" or a \"cusp\" in the graph at . Even a function with a smooth graph is not differentiable at a point where its tangent is vertical: For instance, the function given by is not differentiable at .\n\nIn summary: for a function to have a derivative it is \"necessary\" for the function to be continuous, but continuity alone is not \"sufficient\".\n\nMost functions that occur in practice have derivatives at all points or at almost every point. Early in the history of calculus, many mathematicians assumed that a continuous function was differentiable at most points. Under mild conditions, for example if the function is a monotone function or a Lipschitz function, this is true. However, in 1872 Weierstrass found the first example of a function that is continuous everywhere but differentiable nowhere. This example is now known as the Weierstrass function. In 1931, Stefan Banach proved that the set of functions that have a derivative at some point is a meager set in the space of all continuous functions. Informally, this means that hardly do any random continuous functions have a derivative at even one point.\n\nLet be a function that has a derivative at every point in its domain. We can then define a function that maps every point formula_14 to the value of the derivative of formula_15 at formula_14. This function is written and is called the \"derivative function\" or the \"derivative of\" .\n\nSometimes has a derivative at most, but not all, points of its domain. The function whose value at equals whenever is defined and elsewhere is undefined is also called the derivative of . It is still a function, but its domain is strictly smaller than the domain of .\n\nUsing this idea, differentiation becomes a function of functions: The derivative is an operator whose domain is the set of all functions that have derivatives at every point of their domain and whose range is a set of functions. If we denote this operator by , then is the function . Since is a function, it can be evaluated at a point . By the definition of the derivative function, .\n\nFor comparison, consider the doubling function given by ; is a real-valued function of a real number, meaning that it takes numbers as inputs and has numbers as outputs:\nThe operator , however, is not defined on individual numbers. It is only defined on functions:\nBecause the output of is a function, the output of can be evaluated at a point. For instance, when is applied to the squaring function, , outputs the doubling function , which we named . This output function can then be evaluated to get , , and so on.\n\nLet be a differentiable function, and let be its derivative. The derivative of (if it has one) is written and is called the \"second derivative of \". Similarly, the derivative of the second derivative, if it exists, is written and is called the \"third derivative of \". Continuing this process, one can define, if it exists, the th derivative as the derivative of the th derivative. These repeated derivatives are called \"higher-order derivatives\". The th derivative is also called the derivative of order .\n\nIf represents the position of an object at time , then the higher-order derivatives of have physical interpretations. The second derivative of is the derivative of , the velocity, and by definition this is the object's acceleration. The third derivative of is defined to be the jerk, and the fourth derivative is defined to be the jounce.\n\nA function need not have a derivative (for example, if it is not continuous). Similarly, even if does have a derivative, it may not have a second derivative. For example, let\nCalculation shows that is a differentiable function whose derivative at formula_14 is given by\n\nOn the real line, every polynomial function is infinitely differentiable. By standard differentiation rules, if a polynomial of degree is differentiated times, then it becomes a constant function. All of its subsequent derivatives are identically zero. In particular, they exist, so polynomials are smooth functions.\n\nThe derivatives of a function at a point provide polynomial approximations to that function near . For example, if is twice differentiable, then\nin the sense that\nIf is infinitely differentiable, then this is the beginning of the Taylor series for evaluated at around .\n\nA point where the second derivative of a function changes sign is called an \"inflection point\". At an inflection point, the second derivative may be zero, as in the case of the inflection point of the function given by formula_25, or it may fail to exist, as in the case of the inflection point of the function given by formula_26. At an inflection point, a function switches from being a convex function to being a concave function or vice versa.\n\nThe symbols formula_27, formula_28, and formula_29 were introduced by Gottfried Wilhelm Leibniz in 1675. It is still commonly used when the equation is viewed as a functional relationship between dependent and independent variables. Then the first derivative is denoted by\n\nand was once thought of as an infinitesimal quotient. Higher derivatives are expressed using the notation\n\nfor the \"n\"th derivative of formula_32. These are abbreviations for multiple applications of the derivative operator. For example,\n\nWith Leibniz's notation, we can write the derivative of formula_34 at the point formula_35 in two different ways:\n\nLeibniz's notation allows one to specify the variable for differentiation (in the denominator), which is relevant in partial differentiation. It also makes the chain rule easier to remember:\n\nSometimes referred to as \"prime notation\", one of the most common modern notation for differentiation is due to Joseph-Louis Lagrange and uses the prime mark, so that the derivative of a function formula_15 is denoted formula_39. Similarly, the second and third derivatives are denoted\nTo denote the number of derivatives beyond this point, some authors use Roman numerals in superscript, whereas others place the number in parentheses:\nThe latter notation generalizes to yield the notation formula_44 for the \"n\"th derivative of formula_15 – this notation is most useful when we wish to talk about the derivative as being a function itself, as in this case the Leibniz notation can become cumbersome.\n\nNewton's notation for differentiation, also called the dot notation, places a dot over the function name to represent a time derivative. If formula_46, then\ndenote, respectively, the first and second derivatives of formula_34. This notation is used exclusively for derivatives with respect to time or arc length. It is very common in physics, differential equations, and differential geometry. While the notation becomes unmanageable for high-order derivatives, in practice only few derivatives are needed.\n\nEuler's notation uses a differential operator formula_50, which is applied to a function formula_15 to give the first derivative formula_52. The \"n\"th derivative is denoted formula_53.\n\nIf is a dependent variable, then often the subscript \"x\" is attached to the \"D\" to clarify the independent variable \"x\".\nEuler's notation is then written\nalthough this subscript is often omitted when the variable \"x\" is understood, for instance when this is the only variable present in the expression.\n\nEuler's notation is useful for stating and solving linear differential equations.\n\nThe derivative of a function can, in principle, be computed from the definition by considering the difference quotient, and computing its limit. In practice, once the derivatives of a few simple functions are known, the derivatives of other functions are more easily computed using \"rules\" for obtaining derivatives of more complicated functions from simpler ones.\n\nMost derivative computations eventually require taking the derivative of some common functions. The following incomplete list gives some of the most frequently used functions of a single real variable and their derivatives.\n\n\nwhere \"r\" is any real number, then\n\nwherever this is defined. For example, if formula_58, then\n\nand the derivative function is defined only for positive \"x\", not for . When , this rule implies that \"f\"′(\"x\") is zero for , which is almost the constant rule (stated below).\n\n\nIn many cases, complicated limit calculations by direct application of Newton's difference quotient can be avoided using differentiation rules. Some of the most basic rules are the following.\n\n\nThe derivative of the function given by\n\nis\n\nHere the second term was computed using the chain rule and third using the product rule. The known derivatives of the elementary functions \"x\", \"x\", sin(\"x\"), ln(\"x\") and , as well as the constant 7, were also used.\n\nA vector-valued function y of a real variable sends real numbers to vectors in some vector space R. A vector-valued function can be split up into its coordinate functions \"y\"(\"t\"), \"y\"(\"t\"), …, \"y\"(\"t\"), meaning that . This includes, for example, parametric curves in R or R. The coordinate functions are real valued functions, so the above definition of derivative applies to them. The derivative of y(\"t\") is defined to be the vector, called the tangent vector, whose coordinates are the derivatives of the coordinate functions. That is,\n\nEquivalently,\n\nif the limit exists. The subtraction in the numerator is the subtraction of vectors, not scalars. If the derivative of y exists for every value of \"t\", then y′ is another vector-valued function.\n\nIf e, …, e is the standard basis for R, then y(\"t\") can also be written as . If we assume that the derivative of a vector-valued function retains the linearity property, then the derivative of y(\"t\") must be\nbecause each of the basis vectors is a constant.\n\nThis generalization is useful, for example, if y(\"t\") is the position vector of a particle at time \"t\"; then the derivative y′(\"t\") is the velocity vector of the particle at time \"t\".\n\nSuppose that \"f\" is a function that depends on more than one variable—for instance,\n\"f\" can be reinterpreted as a family of functions of one variable indexed by the other variables:\nIn other words, every value of \"x\" chooses a function, denoted \"f\", which is a function of one real number. That is,\nOnce a value of \"x\" is chosen, say \"a\", then determines a function \"f\" that sends \"y\" to :\nIn this expression, \"a\" is a \"constant\", not a \"variable\", so \"f\" is a function of only one real variable. Consequently, the definition of the derivative for a function of one variable applies:\nThe above procedure can be performed for any choice of \"a\". Assembling the derivatives together into a function gives a function that describes the variation of \"f\" in the \"y\" direction:\nThis is the partial derivative of \"f\" with respect to \"y\". Here ∂ is a rounded \"d\" called the partial derivative symbol. To distinguish it from the letter \"d\", ∂ is sometimes pronounced \"der\", \"del\", or \"partial\" instead of \"dee\".\n\nIn general, the partial derivative of a function in the direction \"x\" at the point (\"a\", …, \"a\") is defined to be:\nIn the above difference quotient, all the variables except \"x\" are held fixed. That choice of fixed values determines a function of one variable\nand, by definition,\nIn other words, the different choices of \"a\" index a family of one-variable functions just as in the example above. This expression also shows that the computation of partial derivatives reduces to the computation of one-variable derivatives.\n\nAn important example of a function of several variables is the case of a scalar-valued function on a domain in Euclidean space R (e.g., on R or R). In this case \"f\" has a partial derivative ∂\"f\"/∂\"x\" with respect to each variable \"x\". At the point (\"a\", …, \"a\"), these partial derivatives define the vector\nThis vector is called the gradient of \"f\" at \"a\". If \"f\" is differentiable at every point in some domain, then the gradient is a vector-valued function ∇\"f\" that takes the point (\"a\", …, \"a\") to the vector ∇\"f\"(\"a\", …, \"a\"). Consequently, the gradient determines a vector field.\n\nIf \"f\" is a real-valued function on R, then the partial derivatives of \"f\" measure its variation in the direction of the coordinate axes. For example, if \"f\" is a function of \"x\" and \"y\", then its partial derivatives measure the variation in \"f\" in the \"x\" direction and the \"y\" direction. They do not, however, directly measure the variation of \"f\" in any other direction, such as along the diagonal line . These are measured using directional derivatives. Choose a vector\nThe directional derivative of \"f\" in the direction of v at the point x is the limit\nIn some cases it may be easier to compute or estimate the directional derivative after changing the length of the vector. Often this is done to turn the problem into the computation of a directional derivative in the direction of a unit vector. To see how this works, suppose that . Substitute into the difference quotient. The difference quotient becomes:\nThis is \"λ\" times the difference quotient for the directional derivative of \"f\" with respect to u. Furthermore, taking the limit as \"h\" tends to zero is the same as taking the limit as \"k\" tends to zero because \"h\" and \"k\" are multiples of each other. Therefore, . Because of this rescaling property, directional derivatives are frequently considered only for unit vectors.\n\nIf all the partial derivatives of \"f\" exist and are continuous at x, then they determine the directional derivative of \"f\" in the direction v by the formula:\nThis is a consequence of the definition of the total derivative. It follows that the directional derivative is linear in v, meaning that .\n\nThe same definition also works when \"f\" is a function with values in R. The above definition is applied to each component of the vectors. In this case, the directional derivative is a vector in R.\n\nWhen \"f\" is a function from an open subset of R to R, then the directional derivative of \"f\" in a chosen direction is the best linear approximation to \"f\" at that point and in that direction. But when , no single directional derivative can give a complete picture of the behavior of \"f\". The total derivative gives a complete picture by considering all directions at once. That is, for any vector v starting at a, the linear approximation formula holds:\nJust like the single-variable derivative, is chosen so that the error in this approximation is as small as possible.\n\nIf \"n\" and \"m\" are both one, then the derivative is a number and the expression is the product of two numbers. But in higher dimensions, it is impossible for to be a number. If it were a number, then would be a vector in R while the other terms would be vectors in R, and therefore the formula would not make sense. For the linear approximation formula to make sense, must be a function that sends vectors in R to vectors in R, and must denote this function evaluated at v.\n\nTo determine what kind of function it is, notice that the linear approximation formula can be rewritten as\nNotice that if we choose another vector w, then this approximate equation determines another approximate equation by substituting w for v. It determines a third approximate equation by substituting both w for v and for a. By subtracting these two new equations, we get\nIf we assume that v is small and that the derivative varies continuously in a, then is approximately equal to , and therefore the right-hand side is approximately zero. The left-hand side can be rewritten in a different way using the linear approximation formula with substituted for v. The linear approximation formula implies:\nThis suggests that is a linear transformation from the vector space R to the vector space R. In fact, it is possible to make this a precise derivation by measuring the error in the approximations. Assume that the error in these linear approximation formula is bounded by a constant times ||v||, where the constant is independent of v but depends continuously on a. Then, after adding an appropriate error term, all of the above approximate equalities can be rephrased as inequalities. In particular, is a linear transformation up to a small error term. In the limit as v and w tend to zero, it must therefore be a linear transformation. Since we define the total derivative by taking a limit as v goes to zero, must be a linear transformation.\n\nIn one variable, the fact that the derivative is the best linear approximation is expressed by the fact that it is the limit of difference quotients. However, the usual difference quotient does not make sense in higher dimensions because it is not usually possible to divide vectors. In particular, the numerator and denominator of the difference quotient are not even in the same vector space: The numerator lies in the codomain R while the denominator lies in the domain R. Furthermore, the derivative is a linear transformation, a different type of object from both the numerator and denominator. To make precise the idea that is the best linear approximation, it is necessary to adapt a different formula for the one-variable derivative in which these problems disappear. If , then the usual definition of the derivative may be manipulated to show that the derivative of \"f\" at \"a\" is the unique number such that\nThis is equivalent to\nbecause the limit of a function tends to zero if and only if the limit of the absolute value of the function tends to zero. This last formula can be adapted to the many-variable situation by replacing the absolute values with norms.\n\nThe definition of the total derivative of \"f\" at a, therefore, is that it is the unique linear transformation such that\nHere h is a vector in R, so the norm in the denominator is the standard length on R. However, \"f\"′(a)h is a vector in R, and the norm in the numerator is the standard length on R. If \"v\" is a vector starting at \"a\", then is called the pushforward of v by \"f\" and is sometimes written .\n\nIf the total derivative exists at a, then all the partial derivatives and directional derivatives of \"f\" exist at a, and for all v, is the directional derivative of \"f\" in the direction v. If we write \"f\" using coordinate functions, so that , then the total derivative can be expressed using the partial derivatives as a matrix. This matrix is called the Jacobian matrix of \"f\" at a:\n\nThe existence of the total derivative \"f\"′(a) is strictly stronger than the existence of all the partial derivatives, but if the partial derivatives exist and are continuous, then the total derivative exists, is given by the Jacobian, and depends continuously on a.\n\nThe definition of the total derivative subsumes the definition of the derivative in one variable. That is, if \"f\" is a real-valued function of a real variable, then the total derivative exists if and only if the usual derivative exists. The Jacobian matrix reduces to a 1×1 matrix whose only entry is the derivative \"f\"′(\"x\"). This 1×1 matrix satisfies the property that is approximately zero, in other words that\n\nUp to changing variables, this is the statement that the function formula_110 is the best linear approximation to \"f\" at \"a\".\n\nThe total derivative of a function does not give another function in the same way as the one-variable case. This is because the total derivative of a multivariable function has to record much more information than the derivative of a single-variable function. Instead, the total derivative gives a function from the tangent bundle of the source to the tangent bundle of the target.\n\nThe natural analog of second, third, and higher-order total derivatives is not a linear transformation, is not a function on the tangent bundle, and is not built by repeatedly taking the total derivative. The analog of a higher-order derivative, called a jet, cannot be a linear transformation because higher-order derivatives reflect subtle geometric information, such as concavity, which cannot be described in terms of linear data such as vectors. It cannot be a function on the tangent bundle because the tangent bundle only has room for the base space and the directional derivatives. Because jets capture higher-order information, they take as arguments additional coordinates representing higher-order changes in direction. The space determined by these additional coordinates is called the jet bundle. The relation between the total derivative and the partial derivatives of a function is paralleled in the relation between the \"k\"th order jet of a function and its partial derivatives of order less than or equal to \"k\".\n\nBy repeatedly taking the total derivative, one obtains higher versions of the Fréchet derivative, specialized to R. The \"k\"th order total derivative may be interpreted as a map\nwhich takes a point x in R and assigns to it an element of the space of \"k\"-linear maps from R to R – the \"best\" (in a certain precise sense) \"k\"-linear approximation to \"f\" at that point. By precomposing it with the diagonal map Δ, , a generalized Taylor series may be begun as\nwhere f(a) is identified with a constant function, are the components of the vector , and and are the components of and as linear transformations.\n\nThe concept of a derivative can be extended to many other settings. The common thread is that the derivative of a function at a point serves as a linear approximation of the function at that point.\n\nCalculus, known in its early history as \"infinitesimal calculus\", is a mathematical discipline focused on limits, functions, derivatives, integrals, and infinite series. Isaac Newton and Gottfried Leibniz independently discovered calculus in the mid-17th century. However, each inventor claimed the other stole his work in a bitter dispute that continued until the end of their lives.\n\n\n\n\n"}
{"id": "4606682", "url": "https://en.wikipedia.org/wiki?curid=4606682", "title": "Diagonal morphism", "text": "Diagonal morphism\n\nIn category theory, a branch of mathematics, for any object formula_1 in any category formula_2 where the product formula_3 exists, there exists the diagonal morphism \n\nsatisfying \n\nwhere formula_7 is the canonical projection morphism to the formula_8-th component. The existence of this morphism is a consequence of the universal property which characterizes the product (up to isomorphism). The restriction to binary products here is for ease of notation; diagonal morphisms exist similarly for arbitrary products. The image of a diagonal morphism in the category of sets, as a subset of the Cartesian product, is a relation on the domain, namely equality.\n\nFor concrete categories, the diagonal morphism can be simply described by its action on elements formula_9 of the object formula_1. Namely, formula_11, the ordered pair formed from formula_9. The reason for the name is that the image of such a diagonal morphism is diagonal (whenever it makes sense), for example the image of the diagonal morphism formula_13 on the real line is given by the line which is a graph of the equation formula_14. The diagonal morphism into the infinite product formula_15 may provide an injection into the space of sequences valued in formula_16; each element maps to the constant sequence at that element. However, most notions of sequence spaces have convergence restrictions which the image of the diagonal map will fail to satisfy.\n\n"}
{"id": "16300987", "url": "https://en.wikipedia.org/wiki?curid=16300987", "title": "Enthought", "text": "Enthought\n\nEnthought, Inc. is a software company based in Austin, Texas, United States that develops scientific and analytic computing solutions using primarily the Python programming language. It is best known for the early development and maintenance of the SciPy library of mathematics, science, and engineering algorithms and for its Python for scientific computing distribution Enthought Canopy (formerly EPD).\n\nThe company was founded in 2001 by Travis Vaught and Eric Jones.\n\nEnthought publishes a large portion of the code as open-source software under a BSD-style license.\n\nEnthought Canopy is a Python for scientific and analytic computing distribution and analysis environment, available for free and under a commercial license.\n\nThe Enthought Tool Suite open source software projects include:\n\n\n"}
{"id": "26269427", "url": "https://en.wikipedia.org/wiki?curid=26269427", "title": "Five points determine a conic", "text": "Five points determine a conic\n\nIn Euclidean and projective geometry, just as two (distinct) points determine a line (a degree-1 plane curve), five points determine a conic (a degree-2 plane curve). There are additional subtleties for conics that do not exist for lines, and thus the statement and its proof for conics are both more technical than for lines. \n\nFormally, given any five points in the plane in general linear position, meaning no three collinear, there is a unique conic passing through them, which will be non-degenerate; this is true over both the Euclidean plane and any pappian projective plane. Indeed, given any five points there is a conic passing through them, but if three of the points are collinear the conic will be degenerate (reducible, because it contains a line), and may not be unique; see further discussion.\n\nThis result can be proven numerous different ways; the dimension counting argument is most direct, and generalizes to higher degree, while other proofs are special to conics.\n\nIntuitively, passing through five points in general linear position specifies five independent linear constraints on the (projective) linear space of conics, and hence specifies a unique conic, though this brief statement ignores subtleties.\n\nMore precisely, this is seen as follows:\n\nThe two subtleties in the above analysis are that the resulting point is a quadratic equation (not a linear equation), and that the constraints are independent. The first is simple: if \"A\", \"B\", and \"C\" all vanish, then the equation formula_5 defines a line, and any 3 points on this (indeed any number of points) lie on a line – thus general linear position ensures a conic. The second, that the constraints are independent, is significantly subtler: it corresponds to the fact that given five points in general linear position in the plane, their images in formula_6 under the Veronese map are in general linear position, which is true because the Veronese map is biregular: i.e., if the image of five points satisfy a relation, then the relation can be pulled back and the original points must also satisfy a relation. The Veronese map has coordinates formula_7 and the target formula_6 is \"dual\" to the formula_9 formula_6 of conics. The Veronese map corresponds to \"evaluation of a conic at a point\", and the statement about independence of constraints is exactly a geometric statement about this map.\n\nThat five points determine a conic can be proven by synthetic geometry—i.e., in terms of lines and points in the plane—in addition to the analytic (algebraic) proof given above. Such a proof can be given using a theorem of Jakob Steiner, which states:\nThis can be shown by taking the points \"X\" and \"Y\" to the standard points formula_12 and formula_13 by a projective transformation, in which case the pencils of lines correspond to the horizontal and vertical lines in the plane, and the intersections of corresponding lines to the graph of a function, which (must be shown) is a hyperbola, hence a conic, hence the original curve \"C\" is a conic.\n\nNow given five points \"X, Y, A, B, C,\" the three lines formula_14 can be taken to the three lines formula_15 by a unique projective transform, since projective transforms are simply 3-transitive on lines (they are simply 3-transitive on points, hence by projective duality they are 3-transitive on lines). Under this map \"X\" maps to \"Y,\" since these are the unique intersection points of these lines, and thus satisfy the hypothesis of Steiner’s theorem. The resulting conic thus contains all five points, and is the unique such conic, as desired.\n\nGiven five points, one can construct the conic containing them in various ways.\n\nAnalytically, given the coordinates of the five points, the equation for the conic can be found by linear algebra, by writing and solving the five equations in the coefficients, substituting the variables with the values of the coordinates: five equations, six unknowns, but homogeneous so scaling removes one dimension; concretely, setting one of the coefficients to 1 accomplishes this.\n\nSynthetically, the conic can be constructed by the , by applying the Braikenridge–Maclaurin theorem, which is the converse of Pascal's theorem. Pascal's theorem states that given \"6\" points on a conic (a hexagon), the lines defined by opposite sides intersect in three collinear points. This can be reversed to construct the possible locations for a 6th point, given 5 existing ones.\n\nThe natural generalization is to ask for what value of \"k\" a configuration of \"k\" points (in general position) in \"n\"-space determines a variety of degree \"d\" and dimension \"m\", which is a fundamental question in enumerative geometry.\n\nA simple case of this is for a hypersurface (a codimension 1 subvariety, the zeros of a single polynomial, the case formula_16), of which plane curves are an example.\n\nIn the case of a hypersurface, the answer is given in terms of the multiset coefficient, more familiarly the binomial coefficient, or more elegantly the rising factorial, as:\nThis is via the analogous analysis of the Veronese map: \"k\" points in general position impose \"k\" independent linear conditions on a variety (because the Veronese map is biregular), and the number of monomials of degree \"d\" in formula_18 variables (\"n\"-dimensional projective space has formula_18 homogeneous coordinates) is formula_20 from which 1 is subtracted because of projectivization: multiplying a polynomial by a constant does not change its zeros.\n\nIn the above formula, the number of points \"k\" is a polynomial in \"d\" of degree \"n,\" with leading coefficient formula_21\n\nIn the case of plane curves, where formula_22 the formula becomes:\nwhose values for formula_24 are formula_25 – there are no curves of degree 0 (a single point is determines a point, which is codimension 2), 2 points determine a line, 5 points determine a conic, 9 points determine a cubic, 14 points determine a quartic, and so forth.\n\nWhile five points determine a conic, sets of six or more points on a conic are not in general position, that is, they are constrained as is demonstrated in Pascal's theorem.\n\nSimilarly, while nine points determine a cubic, if the nine points lie on more than one cubic—i.e., they are the intersection of two cubics—then they are not in general position, and indeed satisfy an addition constraint, as stated in the Cayley–Bacharach theorem.\n\nFour points do not determine a conic, but rather a pencil, the 1-dimensional linear system of conics which all pass through the four points (formally, have the four points as base locus). Similarly, three points determine a 2-dimensional linear system (net), two points determine a 3-dimensional linear system (web), one point determines a 4-dimensional linear system, and zero points place no constraints on the 5-dimensional linear system of all conics.\nAs is well known, three non-collinear points determine a circle in Euclidean geometry and two distinct points determine a pencil of circles such as the Apollonian circles. These results seem to run counter the general result since circles are special cases of conics. However, in a pappian projective plane a conic is a circle only if it passes through two specific points on the line at infinity, so a circle is determined by five non-collinear points, three in the affine plane and these two special points. Similar considerations explain the smaller than expected number of points needed to define pencils of circles.\n\nInstead of passing through points, a different condition on a curve is being tangent to a given line. Being tangent to five given lines also determines a conic, by projective duality, but from the algebraic point of view tangency to a line is a \"quadratic\" constraint, so naive dimension counting yields 2 = 32 conics tangent to five given lines, of which 31 must be ascribed to degenerate conics, as described in fudge factors in enumerative geometry; formalizing this intuition requires significant further development to justify.\n\nAnother classic problem in enumerative geometry, of similar vintage to conics, is the Problem of Apollonius: a circle that is tangent to three circles in general determines eight circles, as each of these is a quadratic condition and 2 = 8. As a question in real geometry, a full analysis involves many special cases, and the actual number of circles may be any number between 0 and 8, except for 7.\n\n\n"}
{"id": "57182888", "url": "https://en.wikipedia.org/wiki?curid=57182888", "title": "Francois Deruyts Prize", "text": "Francois Deruyts Prize\n\nThe Francois Deruyts Prize, or Prix Francois Deruyts, is awarded every four years to recognize progress in synthetic or analytic superior geometry. It was established in 1902 by the Academie Royale de Belgique, Classe des Sciences, and carries a monetary award. Originally recipients had to be Belgian, but currently EU nationals are eligible.\n\nThe recipients of the Francois Deruyts Prize are:\n\n"}
{"id": "56782000", "url": "https://en.wikipedia.org/wiki?curid=56782000", "title": "GKM variety", "text": "GKM variety\n\nIn algebraic geometry, a GKM variety is a complex algebraic variety equipped with a torus action that meets certain conditions. The concept was introduced by Mark Goresky, Robert Kottwitz, and Robert MacPherson in 1998. The torus action of a GKM variety must be \"skeletal\": both the set of fixed points of the action, and the number of one-dimensional orbits of the action, must be finite. In addition, the action must be \"equivariantly formal\", a condition that can be phrased in terms of the torus' rational cohomology.\n\n"}
{"id": "4076831", "url": "https://en.wikipedia.org/wiki?curid=4076831", "title": "Gentzen's consistency proof", "text": "Gentzen's consistency proof\n\nGentzen's consistency proof is a result of proof theory in mathematical logic, published by Gerhard Gentzen in 1936. It shows that the Peano axioms of first-order arithmetic do not contain a contradiction (i.e. are \"consistent\"), as long as a certain other system used in the proof does not contain any contradictions either. This other system, today called \"primitive recursive arithmetic with the additional principle of quantifier-free transfinite induction up to the ordinal ε\", is neither weaker nor stronger than the system of Peano axioms. Gentzen argued that it avoids the questionable modes of inference contained in Peano arithmetic and that its consistency is therefore less controversial.\n\nGentzen's theorem is concerned with first-order arithmetic: the theory of the natural numbers, including their addition and multiplication, axiomatized by the first-order Peano axioms. This is a \"first-order\" theory: the quantifiers extend over natural numbers, but not over sets or functions of natural numbers. The theory is strong enough to describe recursively defined integer functions such as exponentiation, factorials or the Fibonacci sequence.\n\nGentzen showed that the consistency of the first-order Peano axioms is provable, over the base theory of primitive recursive arithmetic with the additional principle of quantifier-free transfinite induction up to the ordinal ε. Primitive recursive arithmetic is a much simplified form of arithmetic that is rather uncontroversial. The additional principle means, informally, that there is a well-ordering on the set of finite rooted trees. Formally, ε is the first ordinal formula_1 such that formula_2, i.e. the limit of the sequence:\nTo express ordinals in the language of arithmetic, an ordinal notation is needed, i.e. a way to assign natural numbers to ordinals less than ε. This can be done in various ways, one example provided by Cantor's normal form theorem. We require for any quantifier-free formula A(x): if there is an ordinal \"a\"< ε for which A(a) is false, then there is a least such ordinal.\n\nGentzen defines a notion of \"reduction procedure\" for proofs in Peano arithmetic. For a given proof, such a procedure produces a tree of proofs, with the given one serving as the root of the tree, and the other proofs being, in a sense, \"simpler\" than the given one. This increasing simplicity is formalized by attaching an ordinal < ε to every proof, and showing that, as one moves down the tree, these ordinals get smaller with every step. He then shows that if there were a proof of a contradiction, the reduction procedure would result in an infinite descending sequence of ordinals smaller than ε produced by a primitive recursive operation on proofs corresponding to a quantifier-free formula.\n\nIt is possible to interpret Gentzen's proof in game-theoretic terms .\n\nGentzen's proof highlights one commonly missed aspect of Gödel's second incompleteness theorem. It is sometimes claimed that the consistency of a theory can only be proved in a stronger theory. Gentzen's theory obtained by adding quantifier-free transfinite induction to primitive recursive arithmetic proves the consistency of first-order Peano arithmetic (PA) but does not contain PA. For example, it does not prove ordinary mathematical induction for all formulae, whereas PA does (since all instance of induction are axioms of PA). Gentzen's theory is not contained in PA, either, however, since it can prove a number-theoretical fact—the consistency of PA—that PA cannot. Therefore, the two theories are, in one sense, incomparable.\n\nThat said, there are other, more powerful ways to compare the strength of theories, the most important of which is defined in terms of the notion of interpretability. It can be shown that, if one theory T is interpretable in another B, then T is consistent if B is. (Indeed, this is a large point of the notion of interpretability.) And, assuming that T is not extremely weak, T itself will be able to prove this very conditional: If B is consistent, then so is T. Hence, T cannot prove that B is consistent, by the second incompleteness theorem, whereas B may well be able to prove that T is consistent. This is what motivates the idea of using interpretability to compare theories, i.e., the thought that, if B interprets T, then B is at least as strong (in the sense of 'consistency strength') as T is.\n\nA strong form of the second incompleteness theorem, proved by Pavel Pudlák, who was building on earlier work by Solomon Feferman, states that no consistent theory T that contains Robinson arithmetic, Q, can interpret Q plus Con(T), the statement that T is consistent. By contrast, Q+Con(T) \"does\" interpret T, by a strong form of the arithmetized completeness theorem. So Q+Con(T) is always stronger (in one good sense) than T is. But Gentzen's theory trivially interprets Q+Con(PA), since it contains Q and proves Con(PA), and so Gentzen's theory interprets PA. But, by Pudlák's result, PA \"cannot\" interpret Gentzen's theory, since Gentzen's theory (as just said) interprets Q+Con(PA), and interpretability is transitive. That is: If PA did interpret Gentzen's theory, then it would also interpret Q+Con(PA) and so would be inconsistent, by Pudlák's result. So, in the sense of consistency strength, as characterized by interpretability, Gentzen's theory is stronger than Peano arithmetic.\n\nHermann Weyl made the following comment in 1946 regarding the significance of Gentzen's consistency result following the devastating impact of Gödel's 1931 incompleteness result on Hilbert's plan to prove the consistency of mathematics.\n\nGentzen's first version of his consistency proof was not published during his lifetime because Paul Bernays had objected to a method implicitly used in the proof. The modified proof, described above, was published in 1936 in the \"Annals\". Gentzen went on to publish two more consistency proofs, one in 1938 and one in 1943. All of these are contained in .\n\nIn 1940 Wilhelm Ackermann published another consistency proof for Peano arithmetic, also using the ordinal ε.\n\nGentzen's proof is the first example of what is called proof-theoretical ordinal analysis. In ordinal analysis one gauges the strength of theories by measuring how large the (constructive) ordinals are that can be proven to be well-ordered, or equivalently for how large a (constructive) ordinal can transfinite induction be proven. A constructive ordinal is the order type of a recursive well-ordering of natural numbers.\n\nLaurence Kirby and Jeff Paris proved in 1982 that Goodstein's theorem cannot be proven in Peano arithmetic. Their proof was based on Gentzen's theorem.\n\n"}
{"id": "505526", "url": "https://en.wikipedia.org/wiki?curid=505526", "title": "HAKMEM", "text": "HAKMEM\n\nHAKMEM, alternatively known as AI Memo 239, is a February 1972 \"memo\" (technical report) of the MIT AI Lab containing a wide variety of hacks, including useful and clever algorithms for mathematical computation, some number theory and schematic diagrams for hardware — in Guy L. Steele's words, \"a bizarre and eclectic potpourri of technical trivia\".\nContributors included about two dozen members and associates of the AI Lab. The title of the report is short for \"hacks memo\", abbreviated to six upper case characters that would fit in a single PDP-10 machine word (using a six-bit character set).\n\nHAKMEM is notable as an early compendium of algorithmic technique, particularly for its practical bent, and as an illustration of the wide-ranging interests of AI Lab people of the time, which included almost anything other than AI research.\n\nHAKMEM contains original work in some fields, notably continued fractions.\n\n\n"}
{"id": "15967005", "url": "https://en.wikipedia.org/wiki?curid=15967005", "title": "Hadamard's lemma", "text": "Hadamard's lemma\n\nIn mathematics, Hadamard's lemma, named after Jacques Hadamard, is essentially a first-order form of Taylor's theorem, in which we can express a smooth, real-valued function exactly in a convenient manner.\n\nLet ƒ be a smooth, real-valued function defined on an open, star-convex neighborhood \"U\" of a point \"a\" in \"n\"-dimensional Euclidean space. Then ƒ(\"x\") can be expressed, for all \"x\" in \"U\", in the form:\n\nwhere each \"g\" is a smooth function on \"U\", \"a\" = (\"a\",…,\"a\"), and \"x\" = (\"x\",…,\"x\").\n\nLet \"x\" be in \"U\". Let \"h\" be the map from [0,1] to the real numbers defined by\n\nThen since\n\nwe have\n\nBut, additionally, \"h\"(1) − \"h\"(0) = \"f\"(\"x\") − \"f\"(\"a\"), so if we let\n\nwe have proven the theorem.\n"}
{"id": "1752415", "url": "https://en.wikipedia.org/wiki?curid=1752415", "title": "Hans Lewy", "text": "Hans Lewy\n\nHans Lewy (20 October 1904 – 23 August 1988) was a Jewish German born American mathematician, known for his work on partial differential equations and on the theory of functions of several complex variables.\n\nLewy was born in Breslau, Germany (now Wrocław, Poland), on October 20, 1904. He began his studies at the University of Göttingen in 1922, after being advised to avoid the more local University of Breslau because it was too old-fashioned, supporting himself during the Weimar hyperinflation by a side job doing railroad track maintenance. At Göttingen, he studied both mathematics and physics; his teachers there included Max Born, Richard Courant, James Franck, David Hilbert, Edmund Landau, Emmy Noether, and Alexander Ostrowski. He earned his doctorate in 1926, at which time he and his friend Kurt Otto Friedrichs both became assistants to Courant and privatdozents at Göttingen.\n\nAt the recommendation of Courant, Lewy was granted a Rockefeller Fellowship, which he used in 1929 to travel to Rome and study algebraic geometry with Tullio Levi-Civita and Federigo Enriques, and then in 1930 to travel to Paris, where he attended the seminar of Jacques Hadamard. After Hitler's election as chancellor in 1933, Lewy was advised by Herbert Busemann to leave Germany again. He was offered a position in Madrid, but declined it, fearing for the future there under Francisco Franco. He revisited Italy and France, but then at the invitation of the Emergency Committee in Aid of Displaced Foreign Scholars and with the assistance of Hadamard found a two-year position in America at Brown University. At the end of that term, in 1935, he moved to the University of California, Berkeley.\n\nDuring World War II, Lewy obtained a pilot's license, but then worked at the Aberdeen Proving Ground. He married Helen Crosby in 1947.\n\nIn 1950, Lewy was fired from Berkeley for refusing to sign a loyalty oath. He taught at Harvard University and Stanford University in 1952 and 1953 before being reinstated by the California Supreme Court case \"Tolman v. Underhill\".\n\nHe retired from Berkeley in 1972, and in 1973 became one of two Ordway Professors of Mathematics at the University of Minnesota. He died on August 23, 1988, in Berkeley.\n\nLewy was elected to the National Academy of Sciences in 1964, and was also a member of the American Academy of Arts and Sciences. He became a foreign member of the Accademia dei Lincei in 1972.\nHe was awarded a Leroy P. Steele Prize in 1979, and a Wolf Prize in Mathematics in 1986 for his work on partial differential equations. In 1986, the University of Bonn gave him an honorary doctorate.\n\nA selection of his work, edited by David Kinderlehrer and including his most important works, was published as the two volume work and \nThe following works are included in his \"Selecta\" in their original language or translated form.\n\n\n"}
{"id": "763708", "url": "https://en.wikipedia.org/wiki?curid=763708", "title": "Herman Goldstine", "text": "Herman Goldstine\n\nHerman Heine Goldstine (September 13, 1913 – June 16, 2004) was a mathematician and computer scientist, who was one of the original developers of ENIAC, the first of the modern electronic digital computers.\n\nHerman Heine Goldstine was born in Chicago in 1913 to Jewish parents. He attended the University of Chicago, where he joined the Phi Beta Kappa fraternity, and graduated with a degree in Mathematics in 1933, a master's degree in 1934, and a PhD in 1936. For three years he was a research assistant under Gilbert Ames Bliss, an authority on the mathematical theory of external ballistics. In 1939 Goldstine began a teaching career at the University of Michigan, until the United States' entry into World War II, when he joined the U. S. Army. In 1941 he married Adele Katz, who was an ENIAC programmer and who wrote the technical description for ENIAC. He had a daughter and a son with Adele, who died in 1964. Two years later he married secondly Ellen Watson.\n\nIn retirement Goldstine became executive director of the American Philosophical Society in Philadelphia between 1985 and 1997, in which capacity he was able to attract many prestigious visitors and speakers.\n\nGoldstine died on June 16, 2004, at his home in Bryn Mawr, Pennsylvania, after a long struggle with Parkinson's disease. His death was announced by the Thomas J. Watson Research Center in Yorktown Heights, New York, where a post-doctoral fellowship was renamed in his honor.\n\nAs a result of the United States' entering World War II, Goldstine left the University of Michigan where he was a professor in July, 1942 to enlist in the Army. He was commissioned a lieutenant and worked as an ordnance mathematician calculating firing tables at the Ballistic Research Laboratory (BRL) at Aberdeen Proving Ground, Maryland. The firing tables were used in battle to find the appropriate elevation and azimuth for aiming artillery, which had a range of several miles.\n\nThe firing table calculations were accomplished by about one hundred women operating mechanical desk calculators. Each combination of gun, round and geographical region required a unique set of firing tables. It took about 750 calculations to compute a single trajectory and each table had about 3,000 trajectories. It took human computer at least 7 hours to calculate one trajectory. To increase production, BRL enlisted the computing facilities of the Moore School of Electrical Engineering at the University of Pennsylvania and Goldstine was the liaison between BRL and the university.\n\nWhile making some adjustments to the Moore School's mechanical differential analyzer, engineer Joseph Chapline suggested Goldstine visit John Mauchly, a physics instructor at the Moore School, who had distributed a memorandum proposing that the calculations could be done thousands of times faster with an electronic computer using vacuum tubes. Mauchly wrote a proposal and in June 1943 he and Goldstine secured funding from the Army for the project. The ENIAC was built in 30 months with 200,000 man hours. The ENIAC was huge, measuring 30 by 60 feet and weighing 30 tons with 18,000 vacuum tubes. The device could only store 20 numbers and took days to program. It was completed in late 1945 as World War II was coming to an end.\n\nIn spite of disappointment that ENIAC had not contributed to the war effort, interest remained strong in the Army to develop an electronic computer. Prior even to the ENIAC's completion, the Army procured a second contract from the Moore School to build a successor machine known as the EDVAC. Goldstine, Mauchly, J. Presper Eckert and Arthur Burks began to study the development of the new machine in the hopes of correcting the deficiencies of the ENIAC.\n\nIn the summer of 1944 Goldstine had a chance encounter with the prominent mathematician John von Neumann on a railway platform in Aberdeen, Maryland, and Goldstine described his project at the University of Pennsylvania. Unknown to Goldstine, von Neumann was then working on the Manhattan Project, which was aiming to build the first atomic bomb. The calculations needed for this project were also daunting.\n\nAs a result of his conversations with Goldstine, von Neumann joined the study group and wrote a memo called \"First Draft of a Report on the EDVAC\". von Neumann intended this to be a memo to the study group, but Goldstine typed it up into a 101-page document that named von Neumann as the sole author. On June 25, 1946, Goldstine forwarded 24 copies of the document to those intimately involved in the EDVAC project; dozens or perhaps hundreds of mimeographs of the report were forwarded to von Neumann's colleagues at universities in the United States and in Great Britain in the weeks that followed. While incomplete, the paper was very well received and became a blueprint for building electronic digital computers. Due to von Neumann's prominence as a major American mathematician, the EDVAC architecture became known as the von Neumann architecture.\n\nOne of the key ideas in the \"first draft\" was that the programmer could store a program in the computer's electronic memory, rather than program the computer using mechanical switches and patch cables. This and other ideas in the paper had been discussed in the EDVAC study group before von Neumann joined the group. The fact that Eckert and Mauchly, the actual inventors and designers of the ENIAC, were not named as co-authors created resentment that led to the group's dissolution at the end of the war.\n\nEckert and Mauchly went on to form the Eckert-Mauchly Computer Corporation, a company that in part survives today as the Unisys Corporation, while von Neumann, Goldstine and Burks moved on into academic life at the Institute for Advanced Study. In Summer 1946, all of them were reunited to give presentations at the first computer course, which has come to be known as the Moore School Lectures; Goldstine's presentations, given without notes, covered deeply and rigorously numerical mathematical methods useful in programs for digital computers.\n\nAfter World War II Goldstine joined von Neumann and Burks at the Institute for Advanced Study at Princeton, where they built a computer referred to as the IAS machine. Goldstine was appointed as assistant director of the project and was later its director, after 1954.\n\nThe IAS machine influenced the design of IBM's early computers through von Neumann, who was a consultant to IBM. When von Neumann died in 1958, the IAS computer project was terminated. Goldstine went on to become the founding director of the Mathematical Sciences Department at IBM's Watson Research Center in Yorktown Heights, New York.\n\nAt IBM one of Goldstine's most significant roles was in fostering relations between IBM researchers and the academic community. In 1969 he was appointed an IBM Fellow, the company's most prestigious technical honor, and a consultant to the director of research. As a fellow Goldstine developed an interest in the history of computing and mathematical sciences. He wrote three books on the topic; \"The Computer from Pascal to von Neumann\", \"History of Numerical Analysis from the 16th Through the 19th Century\" and \"History of the Calculus of Variations from the Seventeenth Through the Nineteenth Century\". As the title implies, in \"The Computer from Pascal to von Neumann\", Goldstine leaves little doubt that in his opinion von Neumann played a critical role in developing modern theories of computing.\n\n\n\n"}
{"id": "158371", "url": "https://en.wikipedia.org/wiki?curid=158371", "title": "John Edensor Littlewood", "text": "John Edensor Littlewood\n\nJohn Edensor Littlewood FRS LLD (9 June 1885 – 6 September 1977) was an English mathematician. He worked on topics relating to analysis, number theory,\nand differential equations, and had a lengthy collaboration with G. H. Hardy.\n\nLittlewood was born on 1885 in Rochester, Kent, the eldest son of Edward Thornton Littlewood and Sylvia Maud (nee Ackland). In 1892, his father accepted the headmastership of a school in Wynberg, Cape Town in South Africa, taking his family there. Littlewood returned to England in 1900 to attend St Paul's School in London, studying under Francis Sowerby Macaulay, an influential algebraic geometer.\n\nIn 1903, Littlewood entered the University of Cambridge, studying in Trinity College. He spent his first two years preparing for the Tripos examinations which qualify undergraduates for a bachelor's degree (where he emerged in 1905 as Senior Wrangler, the person who obtained the highest mark in Part 1 of the Tripos). In 1906, after completing the second part of the Tripos, he started his research under Ernest Barnes. One of the problems that Barnes suggested to Littlewood was to prove the Riemann hypothesis, an assignment at which he did not succeed. He was elected a Fellow of Trinity College in 1908 and, apart from three years as Richardson Lecturer in the University of Manchester, the balance of his career was spent at the University of Cambridge. He was appointed Rouse Ball Professor of Mathematics in 1928, retiring in 1950. He was elected a Fellow of the Royal Society in 1916, awarded the Royal Medal in 1929, the Sylvester Medal in 1943 and the Copley Medal in 1958. He was president of the London Mathematical Society from 1941 to 1943, and was awarded the De Morgan Medal in 1938 and the Senior Berwick Prize in 1960.\n\nMost of Littlewood's work was in the field of mathematical analysis. He began research under the supervision of Ernest William Barnes, who suggested that he attempt to prove the Riemann hypothesis: Littlewood showed that if the Riemann hypothesis is true then the prime number theorem follows and obtained the error term. This work won him his Trinity fellowship. However, the link between the Riemann hypothesis and the prime number theorem had been known before in Continental Europe, and Littlewood wrote later in his book, \"A Mathematician's Miscellany\" that his rediscovery of the result did not shed a positive light on the isolated nature of British mathematics at the time.\n\nHe coined Littlewood's law, which states that individuals can expect \"miracles\" to happen to them, at the rate of about one per month.\n\nHe continued to write papers into his eighties, particularly in analytical areas of what would become the theory of dynamical systems.\n\nLittlewood is also remembered for his book of reminiscences, \"A Mathematician's Miscellany\" (new edition published in 1986).\n\nAmong his own PhD students were Sarvadaman Chowla, Harold Davenport, and Donald C. Spencer. Spencer reported that in 1941 when he (Spencer) was about to get on the boat that would take him home to the United States, Littlewood reminded him: \"\"n\", \"n\" alpha, \"n\" beta!\" (referring to Littlewood's conjecture).\n\nLittlewood's collaborative work, carried out by correspondence, covered fields in Diophantine approximation and Waring's problem, in particular. In his other work, he collaborated with Raymond Paley on Littlewood–Paley theory in Fourier theory, and with Cyril Offord in combinatorial work on random sums, in developments that opened up fields that are still intensively studied.\n\nHe worked with Mary Cartwright on problems in differential equations arising out of early research on radar: their work foreshadowed the modern theory of dynamical systems. Littlewood's 4/3 inequality on bilinear forms was a forerunner of the later Grothendieck tensor norm theory.\n\nLittlewood collaborated for many years with G. H. Hardy. Together they devised the first Hardy–Littlewood conjecture, a strong form of the twin prime conjecture, and the second Hardy–Littlewood conjecture.\n\nHe also, with Hardy, identified the work of the Indian mathematician Srinivasa Ramanujan as that of a genius, and supported him in travelling to the UK and working at Cambridge. A self-taught mathematician, Ramanujan later became a Fellow of the Royal Society, Fellow of Trinity College, Cambridge, and widely recognised as on a par with other geniuses such as Euler and Jacobi.\n\nIn a 1947 lecture, the Danish mathematician Harald Bohr said, \"To illustrate to what extent Hardy and Littlewood in the course of the years came to be considered as the leaders of recent English mathematical research, I may report what an excellent colleague once jokingly said: 'Nowadays, there are only three really great English mathematicians: Hardy, Littlewood, and Hardy–Littlewood.'\"\n\nThere is a story (related in the Miscellany) that at a conference Littlewood met a German mathematician who said he was most interested to discover that Littlewood really existed, as he had always assumed that Littlewood was a name used by Hardy for lesser work which he did not want to put out under his own name; Littlewood apparently roared with laughter. There are versions of this story involving both Norbert Wiener and Edmund Landau, who, it is claimed, \"so doubted the existence of Littlewood that he made a special trip to Great Britain to see the man with his own eyes\".\n\nJohn Littlewood is depicted in two films covering the life of Ramanujan - \" Ramanujan\" in 2014 portrayed by Michael Lieber and \"The Man Who Knew Infinity\", in 2015 portrayed by Toby Jones.\n\n\n\nand\n\n"}
{"id": "1252308", "url": "https://en.wikipedia.org/wiki?curid=1252308", "title": "Law of thought", "text": "Law of thought\n\nThe laws of thought are fundamental axiomatic rules upon which rational discourse itself is often considered to be based. The formulation and clarification of such rules have a long tradition in the history of philosophy and logic. Generally they are taken as laws that guide and underlie everyone's thinking, thoughts, expressions, discussions, etc. However, such classical ideas are often questioned or rejected in more recent developments, such as intuitionistic logic, dialetheism and fuzzy logic.\n\nAccording to the 1999 Cambridge Dictionary of Philosophy, laws of thought are laws by which or in accordance with which valid thought proceeds, or that justify valid inference, or to which all valid deduction is reducible. Laws of thought are rules that apply without exception to any subject matter of thought, etc.; sometimes they are said to be the object of logic. The term, rarely used in exactly the same sense by different authors, has long been associated with three equally ambiguous expressions: the law of identity (ID), the law of contradiction (or non-contradiction; NC), and the law of excluded middle (EM).\nSometimes, these three expressions are taken as propositions of formal ontology having the widest possible subject matter, propositions that apply to entities as such: (ID), everything is (i.e., is identical to) itself; (NC) no thing having a given quality also has the negative of that quality (e.g., no even number is non-even); (EM) every thing either has a given quality or has the negative of that quality (e.g., every number is either even or non-even). Equally common in older works is the use of these expressions for principles of metalogic about propositions: (ID) every proposition implies itself; (NC) no proposition is both true and false; (EM) every proposition is either true or false.\n\nBeginning in the middle to late 1800s, these expressions have been used to denote propositions of Boolean Algebra about classes: (ID) every class includes itself; (NC) every class is such that its intersection (\"product\") with its own complement is the null class; (EM) every class is such that its union (\"sum\") with its own complement is the universal class. More recently, the last two of the three expressions have been used in connection with the classical propositional logic and with the so-called protothetic or quantified propositional logic; in both cases the law of non-contradiction involves the negation of the conjunction (\"and\") of something with its own negation, ¬(A∧¬A), and the law of excluded middle involves the disjunction (\"or\") of something with its own negation, A∨¬A. In the case of propositional logic, the \"something\" is a schematic letter serving as a place-holder, whereas in the case of protothetic logic the \"something\" is a genuine variable. The expressions \"law of non-contradiction\" and \"law of excluded middle\" are also used for semantic principles of model theory concerning sentences and interpretations: (NC) under no interpretation is a given sentence both true and false, (EM) under any interpretation, a given sentence is either true or false.\n\nThe expressions mentioned above all have been used in many other ways. Many other propositions have also been mentioned as laws of thought, including the dictum de omni et nullo attributed to Aristotle, the substitutivity of identicals (or equals) attributed to Euclid, the so-called identity of indiscernibles attributed to Gottfried Wilhelm Leibniz, and other \"logical truths\".\n\nThe expression \"laws of thought\" gained added prominence through its use by Boole (1815–64) to denote theorems of his \"algebra of logic\"; in fact, he named his second logic book \"An Investigation of the Laws of Thought on Which are Founded the Mathematical Theories of Logic and Probabilities\" (1854). Modern logicians, in almost unanimous disagreement with Boole, take this expression to be a misnomer; none of the above propositions classed under \"laws of thought\" are explicitly about thought per se, a mental phenomenon studied by psychology, nor do they involve explicit reference to a thinker or knower as would be the case in pragmatics or in epistemology. The distinction between psychology (as a study of mental phenomena) and logic (as a study of valid inference) is widely accepted.\n\nHamilton offers a history of the three traditional laws that begins with Plato, proceeds through Aristotle, and ends with the schoolmen of the Middle Ages; in addition he offers a fourth law (see entry below, under Hamilton):\n\nThe following will state the three traditional \"laws\" in the words of Bertrand Russell (1912):\n\nThe law of identity: 'Whatever is, is.'\n\nFor all a: a = a.\n\nRegarding this law, Aristotle wrote:\nMore than two millennia later, George Boole alluded to the very same principle as did Aristotle when Boole made the following observation with respect to the nature of language and those principles that must inhere naturally within them: \n\nThe law of non-contradiction (alternately the 'law of contradiction'): 'Nothing can both be and not be.'\n\nIn other words: \"two or more contradictory statements cannot both be true in the same sense at the same time\": ¬(A∧¬A).\n\nIn the words of Aristotle, that \"one cannot say of something that it is and that it is not in the same respect and at the same time\". As an illustration of this law, he wrote:\n\nThe law of excluded middle: 'Everything must either be or not be.\"\n\nIn accordance with the law of excluded middle or excluded third, for every proposition, either its positive or negative form is true: A∨¬A.\n\nRegarding the law of excluded middle, Aristotle wrote:\nAs the quotations from Hamilton above indicate, in particular the \"law of identity\" entry, the rationale for and expression of the \"laws of thought\" have been fertile ground for philosophic debate since Plato. Today the debate—about how we \"come to know\" the world of things and our thoughts—continues; for examples of rationales see the entries, below.\n\nto originally research?|data=february 2019}}-->\n\nIn one of Plato's Socratic dialogues, Socrates described three principles derived from introspection: \n\nThe law of non-contradiction is found in ancient Indian logic as a meta-rule in the \"Shrauta Sutras\", the grammar of Pāṇini, and the \"Brahma Sutras\" attributed to Vyasa. It was later elaborated on by medieval commentators such as Madhvacharya.\n\nJohn Locke claimed that the principles of identity and contradiction (i.e. the law of identity and the law of non-contradiction) were general ideas and only occurred to people after considerable abstract, philosophical thought. He characterized the principle of identity as \"Whatsoever is, is.\" He stated the principle of contradiction as \"It is impossible for the same thing to be and not to be.\" To Locke, these were not innate or \"a priori\" principles.\n\nGottfried Leibniz formulated two additional principles, either or both of which may sometimes be counted as a law of thought:\n\nIn Leibniz's thought, as well as generally in the approach of rationalism, the latter two principles are regarded as clear and incontestable axioms. They were widely recognized in European thought of the 17th, 18th, and 19th centuries, although they were subject to greater debate in the 19th century. As turned out to be the case with the law of continuity, these two laws involve matters which, in contemporary terms, are subject to much debate and analysis (respectively on determinism and extensionality). Leibniz's principles were particularly influential in German thought. In France, the \"Port-Royal Logic\" was less swayed by them. Hegel quarrelled with the identity of indiscernibles in his \"Science of Logic\" (1812–1816).\n\n\"The primary laws of thought, or the conditions of the thinkable, are four: – 1. The law of identity [A is A]. 2. The law of contradiction. 3. The law of exclusion; or excluded middle. 4. The law of sufficient reason.\" (Thomas Hughes, \"The Ideal Theory of Berkeley and the Real World\", Part II, Section XV, Footnote, p. 38)\n\nArthur Schopenhauer discussed the laws of thought and tried to demonstrate that they are the basis of reason. He listed them in the following way in his \"On the Fourfold Root of the Principle of Sufficient Reason\", §33:\n\nAlso:\nTo show that they are the foundation of reason, he gave the following explanation:\nSchopenhauer's four laws can be schematically presented in the following manner:\n\nLater, in 1844, Schopenhauer claimed that the four laws of thought could be reduced to two. In the ninth chapter of the second volume of \"The World as Will and Representation\", he wrote:\nThe title of George Boole's 1854 treatise on logic, \"An Investigation on the Laws of Thought\", indicates an alternate path. The laws are now incorporated into an algebraic representation of his \"laws of the mind\", honed over the years into modern Boolean algebra.\n\nBoole begins his chapter I \"Nature and design of this Work\" with a discussion of what characteristic distinguishes, generally, \"laws of the mind\" from \"laws of nature\":\n\nContrasted with this are what he calls \"laws of the mind\": Boole asserts these are known in their first instance, without need of repetition: \n\nBoole begins with the notion of \"signs\" representing \"classes\", \"operations\" and \"identity\":\n\nBoole then clarifies what a \"literal symbol\" e.g. x, y, z... represents—a name applied to a collection of instances into \"classes\". For example, \"bird\" represents the entire class of feathered winged warm-blooded creatures. For his purposes he extends the notion of class to represent membership of \"one\", or \"nothing\", or \"the universe\" i.e. totality of all individuals: \n\nHe then defines what the string of symbols e.g. xy means [modern logical &, conjunction]:\n\nGiven these definitions he now lists his laws with their justification plus examples (derived from Boole):\nLogical OR: Boole defines the \"collecting of parts into a whole or separate a whole into its parts\" (Boole 1854:32). Here the connective \"and\" is used disjunctively, as is \"or\"; he presents a commutative law (3) and a distributive law (4) for the notion of \"collecting\". The notion of \"separating\" a part from the whole he symbolizes with the \"-\" operation; he defines a commutative (5) and distributive law (6) for this notion:\nLastly is a notion of \"identity\" symbolized by \"=\". This allows for two axioms: (axiom 1): equals added to equals results in equals, (axiom 2): equals subtracted from equals results in equals.\n\nNothing \"0\" and Universe \"1\": He observes that the only two numbers that satisfy xx = x are 0 and 1. He then observes that 0 represents \"Nothing\" while \"1\" represents the \"Universe\" (of discourse).\n\nThe logical NOT: Boole defines the contrary (logical NOT) as follows (his Proposition III): \n\nThe notion of a particular as opposed to a universal: To represent the notion of \"some men\", Boole writes the small letter \"v\" before the predicate-symbol \"vx\" some men.\n\nExclusive- and inclusive-OR: Boole does not use these modern names, but he defines these as follows x(1-y) + y(1-x) and x + y(1-x), respectively; these agree with the formulas derived by means of the modern Boolean algebra.\n\nArmed with his \"system\" he derives the \"principle of [non]contradiction\" starting with his law of identity: x = x. He subtracts x from both sides (his axiom 2), yielding x - x = 0. He then factors out the x: x(x - 1) = 0. For example, if x = \"men\" then 1 - x represents NOT-men. So we have an example of the \"Law of Contradiction\":\n\nThis notion is found throughout Boole's \"Laws of Thought\" e.g. 1854:28, where the symbol \"1\" (the integer 1) is used to represent \"Universe\" and \"0\" to represent \"Nothing\", and in far more detail later (pages 42ff):\n\nIn his chapter \"The Predicate Calculus\" Kleene observes that the specification of the \"domain\" of discourse is \"not a trivial assumption, since it is not always clearly satisfied in ordinary discourse . . . in mathematics likewise, logic can become pretty slippery when no D [domain] has been specified explicitly or implicitly, or the specification of a D [domain] is too vague\" (Kleene 1967:84).\n\nAs noted above, Hamilton specifies \"four\" laws—the three traditional plus the fourth \"Law of Reason and Consequent\"—as follows:\n\nHamilton opines that thought comes in two forms: \"necessary\" and \"contingent\" (Hamilton 1860:17). With regards the \"necessary\" form he defines its study as \"logic\": “Logic is the science of the necessary forms of thought” (Hamilton 1860:17). To define \"necessary\" he asserts that it implies the following four “qualities”:\n\nHere's Hamilton's fourth law from his LECT. V. LOGIC. 60-61:\n\nIn the 19th century, the Aristotelian laws of thoughts, as well as sometimes the Leibnizian laws of thought, were standard material in logic textbooks, and J. Welton described them in this way:\nThe sequel to Bertrand Russell's 1903 \"The Principles of Mathematics\" became the three volume work named Principia Mathematica (hereafter PM), written jointly with Alfred North Whitehead. Immediately after he and Whitehead published PM he wrote his 1912 \"The Problems of Philosophy\". His \"Problems\" reflects \"the central ideas of Russell's logic\".\n\nIn his 1903 \"Principles\" Russell defines Symbolic or Formal Logic (he uses the terms synonymously) as \"the study of the various general types of deduction\" (Russell 1903:11). He asserts that \"Symbolic Logic is essentially concerned with inference in general\" (Russell 1903:12) and with a footnote indicates that he does not distinguish between inference and deduction; moreover he considers induction \"to be either disguised deduction or a mere method of making plausible guesses\" (Russell 1903:11). This opinion will change by 1912, when he deems his \"principle of induction\" to be par with the various \"logical principles\" that include the \"Laws of Thought\".\n\nIn his Part I \"The Indefinables of Mathematics\" Chapter II \"Symbolic Logic\" Part A \"The Propositional Calculus\" Russell reduces deduction (\"propositional calculus\") to 2 \"indefinables\" and 10 axioms:\n\nFrom these he \"claims\" to be able to \"derive\" the law of excluded middle and the law of contradiction but does not exhibit his derivations (Russell 1903:17). Subsequently, he and Whitehead honed these \"primitive principles\" and axioms into the nine found in PM, and here Russell actually \"exhibits\" these two derivations at ❋1.71 and ❋3.24, respectively.\n\nBy 1912 Russell in his \"Problems\" pays close attention to \"induction\" (inductive reasoning) as well as \"deduction\" (inference), both of which represent just two \"examples\" of \"self-evident logical principles\" that include the \"Laws of Thought.\"\n\nInduction principle: Russell devotes a chapter to his \"induction principle\". He describes it as coming in two parts: firstly, as a repeated collection of evidence (with no failures of association known) and therefore increasing probability that whenever A happens B follows; secondly, in a fresh instance when indeed A happens, B will indeed follow: i.e. \"a sufficient number of cases of association will make the probability of a fresh association nearly a certainty, and will make it approach certainty without limit.\"\n\nHe then collects all the cases (instances) of the induction principle (e.g. case 1: A = \"the rising sun\", B = \"the eastern sky\"; case 2: A = \"the setting sun\", B = \"the western sky\"; case 3: etc.) into a \"general\" law of induction which he expresses as follows:\n\nHe makes an argument that this induction principle can neither be disproved or proved by experience, the failure of disproof occurring because the law deals with \"probability\" of success rather than certainty; the failure of proof occurring because of unexamined cases that are yet to be experienced, i.e. they will occur (or not) in the future. \"Thus we must either accept the inductive principle on the ground of its intrinsic evidence, or forgo all justification of our expectations about the future\".\n\nIn his next chapter (\"On Our Knowledge of General Principles\") Russell offers other principles that have this similar property: \"which cannot be proved or disproved by experience, but are used in arguments which start from what is experienced.\" He asserts that these \"have even greater evidence than the principle of induction . . . the knowledge of them has the same degree of certainty as the knowledge of the existence of sense-data. They constitute the means of drawing inferences from what is given in sensation\".\n\nInference principle: Russell then offers an example that he calls a \"logical\" principle. Twice previously he has asserted this principle, first as the 4th axiom in his 1903 and then as his first \"primitive proposition\" of PM: \"❋1.1 Anything implied by a true elementary proposition is true\". Now he repeats it in his 1912 in a refined form: \"Thus our principle states that if this implies that, and this is true, then that is true. In other words, 'anything implied by a true proposition is true', or 'whatever follows from a true proposition is true'. This principle he places great stress upon, stating that \"this principle is really involved -- at least, concrete instances of it are involved -- in all demonstrations\".\n\nHe does not call his inference principle \"modus ponens\", but his formal, symbolic expression of it in PM (2nd edition 1927) is that of \"modus ponens\"; modern logic calls this a \"rule\" as opposed to a \"law\". In the quotation that follows, the symbol \"⊦\" is the \"assertion-sign\" (cf PM:92); “⊦\" means \"it is true that\", therefore “⊦p” where \"p\" is \"the sun is rising\" means \"it is true that the sun is rising\", alternately \"The statement 'The sun is rising' is true\". The \"implication\" symbol \"⊃\" is commonly read \"if p then q\", or \"p implies q\" (cf PM:7). Embedded in this notion of \"implication\" are two \"primitive ideas\", \"the Contradictory Function\" (symbolized by NOT, \"~\") and \"the Logical Sum or Disjunction\" (symbolized by OR, \"⋁\"); these appear as \"primitive propositions\" ❋1.7 and ❋1.71 in PM (PM:97). With these two \"primitive propositions\" Russell defines \"p ⊃ q\" to have the formal logical equivalence \"NOT-p OR q\" symbolized by \"~p ⋁ q\":\n\nIn other words, in a long \"string\" of inferences, after each inference we can detach the \"consequent\" “⊦q” from the symbol string “⊦p, ⊦(p⊃q)” and not carry these symbols forward in an ever-lengthening string of symbols. \nThe three traditional \"laws\" (principles) of thought: Russell goes on to assert other principles, of which the above logical principle is \"only one\". He asserts that \"some of these must be granted before any argument or proof becomes possible. When some of them have been granted, others can be proved.\" Of these various \"laws\" he asserts that \"for no very good reason, three of these principles have been singled out by tradition under the name of 'Laws of Thought'. And these he lists as follows:\n\nRationale: Russell opines that \"the name 'laws of thought' is ... misleading, for what is important is not the fact that we think in accordance with these laws, but the fact that things behave in accordance with them; in other words, the fact that when we think in accordance with them we think \"truly\".\" But he rates this a \"large question\" and expands it in two following chapters where he begins with an investigation of the notion of \"a priori\" (innate, built-in) knowledge, and ultimately arrives at his acceptance of the Platonic \"world of universals\". In his investigation he comes back now and then to the three traditional laws of thought, singling out the law of contradiction in particular: \"The conclusion that the law of contradiction is a law of \"thought\" is nevertheless erroneous . . . [rather], the law of contradiction is about things, and not merely about thoughts . . . a fact concerning the things in the world.\"\n\nHis argument begins with the statement that the three traditional laws of thought are \"samples of self-evident principles\". For Russell the matter of \"self-evident\" merely introduces the larger question of how we derive our knowledge of the world. He cites the \"historic controversy . . . between the two schools called respectively 'empiricists' [ Locke, Berkeley, and Hume ] and 'rationalists' [ Descartes and Leibniz]\" (these philosophers are his examples). Russell asserts that the rationalists \"maintained that, in addition to what we know by experience, there are certain 'innate ideas' and 'innate principles', which we know independently of experience\"; to eliminate the possibility of babies having innate knowledge of the \"laws of thought\", Russell renames this sort of knowledge \"a priori\". And while Russell agrees with the empiricists that \"Nothing can be known to \"exist\" except by the help of experience,\", he also agrees with the rationalists that some knowledge is \"a priori\", specifically \"the propositions of logic and pure mathematics, as well as the fundamental propositions of ethics\".\n\nThis question of how such \"a priori\" knowledge can exist directs Russell to an investigation into the philosophy of Immanuel Kant, which after careful consideration he rejects as follows:\n\nHis objections to Kant then leads Russell to accept the 'theory of ideas' of Plato, \"in my opinion . . . one of the most successful attempts hitherto made.\"; he asserts that \" . . . we must examine our knowledge of universals . . . where we shall find that [this consideration] solves the problem of \"a priori\" knowledge.\".\n\nUnfortunately, Russell's \"Problems\" does not offer an example of a \"minimum set\" of principles that would apply to human reasoning, both inductive and deductive. But PM does at least provide \"an\" example set (but not the minimum; see Post below) that is sufficient for \"deductive\" reasoning by means of the propositional calculus (as opposed to reasoning by means of the more-complicated predicate calculus)—a total of 8 principles at the start of \"Part I: Mathematical Logic\". Each of the formulas :❋1.2 to :❋1.6 is a tautology (true no matter what the truth-value of p, q, r ... is). What is missing in PM's treatment is a formal rule of substitution; in his 1921 PhD thesis Emil Post fixes this deficiency (see Post below). In what follows the formulas are written in a more modern format than that used in PM; the names are given in PM). \nRussell sums up these principles with \"This completes the list of primitive propositions required for the theory of deduction as applied to elementary propositions\" (PM:97).\n\nStarting from these eight tautologies and a tacit use of the \"rule\" of substitution, PM then derives over a hundred different formulas, among which are the Law of Excluded Middle ❋1.71, and the Law of Contradiction ❋3.24 (this latter requiring a definition of logical AND symbolized by the modern ⋀: (p ⋀ q) = ~(~p ⋁ ~q). (PM uses the \"dot\" symbol for logical AND)).\n\nAt about the same time (1912) that Russell and Whitehead were finishing the last volume of their Principia Mathematica, and the publishing of Russell's \"The Problems of Philosophy\" at least two logicians (Louis Couturat, Christine Ladd-Franklin) were asserting that two \"laws\" (principles) of contradiction\" and \"excluded middle\" are necessary to specify \"contradictories\"; Ladd-Franklin renamed these the principles of exclusion and exhaustion. The following appears as a footnote on page 23 of Couturat 1914:\n\nIn other words, the creation of \"contradictories\" represents a dichotomy, i.e. the \"splitting\" of a universe of discourse into two classes (collections) that have the following two properties: they are (i) mutually exclusive and (ii) (collectively) exhaustive. In other words, no one thing (drawn from the universe of discourse) can simultaneously be a member of both classes (law of non-contradiction), \"but\" [and] every single thing (in the universe of discourse) must be a member of one class or the other (law of excluded middle).\n\nAs part of his PhD thesis \"Introduction to a general theory of elementary propositions\" Emil Post proved \"the system of elementary propositions of Principia [PM]\" i.e. its \"propositional calculus\" described by PM's first 8 \"primitive propositions\" to be \"consistent\". The definition of \"consistent\" is this: that by means of the deductive \"system\" at hand (its stated axioms, laws, rules) it is impossible to derive (display) both a formula S and its contradictory ~S (i.e. its logical negation) (Nagel and Newman 1958:50). To demonstrate this formally, Post had to add a primitive proposition to the 8 primitive propositions of PM, a \"rule\" that specified the notion of \"substitution\" that was missing in the original PM of 1910.\n\nGiven PM's tiny set of \"primitive propositions\" and the proof of their consistency, Post then proves that this system (\"propositional calculus\" of PM) is \"complete\", meaning every possible truth table can be generated in the \"system\":\n\nThen there is the matter of \"independence\" of the axioms. In his commentary before Post 1921, van Heijenoort states that Paul Bernays solved the matter in 1918 (but published in 1926) -- the formula ❋1.5 Associative Principle: p ⋁ (q ⋁ r) ⊃ q ⋁ (p ⋁ r) can be proved with the other four. As to what system of \"primitive-propositions\" is the minimum, van Heijenoort states that the matter was \"investigated by Zylinski (1925), Post himself (1941), and Wernick (1942)\" but van Heijenoort does not answer the question.\n\nKleene (1967:33) observes that \"logic\" can be \"founded\" in two ways, first as a \"model theory\", or second by a formal \"proof\" or \"axiomatic theory\"; \"the two formulations, that of model theory and that of proof theory, give equivalent results\"(Kleene 1967:33). This foundational choice, and their equivalence also applies to \"predicate logic\" (Kleene 1967:318). \nIn his introduction to Post 1921, van Heijenoort observes that both the \"truth-table and the axiomatic approaches are clearly presented\". This matter of a proof of consistency both ways (by a model theory, by axiomatic proof theory) comes up in the more-congenial version of Post's consistency proof that can be found in Nagel and Newman 1958 in their chapter V \"An Example of a Successful Absolute Proof of Consistency\". In the main body of the text they use a model to achieve their consistency proof (they also state that the system is complete but do not offer a proof) (Nagel & Newman 1958:45-56). But their text promises the reader a proof that is axiomatic rather than relying on a model, and in the Appendix they deliver this proof based on the notions of a division of formulas into two classes K and K that are mutually exclusive and exhaustive (Nagel & Newman 1958:109-113)\nThe (restricted) \"first-order predicate calculus\" is the \"system of logic\" that adds to the propositional logic (cf Post, above) the notion of \"subject-predicate\" i.e. the subject x is drawn from a domain (universe) of discourse and the predicate is a logical function f(x): x as subject and f(x) as predicate (Kleene 1967:74). Although Gödel's proof involves the same notion of \"completeness\" as does the proof of Post, Gödel's proof is far more difficult; what follows is a discussion of the axiom set.\n\nKurt Gödel in his 1930 doctoral dissertation \"The completeness of the axioms of the functional calculus of logic\" proved that in this \"calculus\" (i.e. restricted predicate logic with or without equality) that every valid formula is \"either refutable or satisfiable\" or what amounts to the same thing: every valid formula is provable and therefore the logic is complete. Here is Gödel's definition of whether or not the \"restricted functional calculus\" is \"complete\":\n\nThis particular predicate calculus is \"restricted to the first order\". To the propositional calculus it adds two special symbols that symbolize the generalizations \"for all\" and \"there exists (at least one)\" that extend over the domain of discourse. The calculus requires only the first notion \"for all\", but typically includes both: (1) the notion \"for all x\" or \"for every x\" is symbolized in the literature as variously as (x), ∀x, ∏x etc., and the (2) notion of \"there exists (at least one x)\" variously symbolized as Ex, ∃x.\n\nThe restriction is that the generalization \"for all\" applies only to the \"variables\" (objects x, y, z etc. drawn from the domain of discourse) and not to functions, in other words the calculus will permit ∀xf(x) (\"for all creatures x, x is a bird\") but not ∀f∀x(f(x)) [but if \"equality\" is added to the calculus it will permit ∀f:f(x); see below under Tarski]. Example:\n\nKleene remarks that \"the predicate calculus (without or with equality) fully accomplishes (for first order theories) what has been conceived to be the role of logic\" (Kleene 1967:322).\n\nThis first half of this axiom -- \"the maxim of all\" will appear as the first of two additional axioms in Gödel's axiom set. The \"dictum of Aristotle\" (dictum de omni et nullo) is sometimes called \"the maxim of all and none\" but is really two \"maxims\" that assert: \"What is true of all (members of the domain) is true of some (members of the domain)\", and \"What is not true of all (members of the domain) is true of none (of the members of the domain)\".\n\nThe \"dictum\" appears in Boole 1854 a couple places:\n\nBut later he seems to argue against it:\n\nBut the first half of this \"dictum\" (\"dictum de omni\") is taken up by Russell and Whitehead in PM, and by Hilbert in his version (1927) of the \"first order predicate logic\"; his (system) includes a principle that Hilbert calls \"Aristotle's dictum\" \n\nThis axiom also appears in the modern axiom set offered by Kleene (Kleene 1967:387), as his \"∀-schema\", one of two axioms (he calls them \"postulates\") required for the predicate calculus; the other being the \"∃-schema\" f(y) ⊃ ∃xf(x) that reasons from the particular f(y) to the existence of at least one subject x that satisfies the predicate f(x); both of these requires adherence to a defined domain (universe) of discourse.\n\nTo supplement the four (down from five; see Post) axioms of the propositional calculus, Gödel 1930 adds the \"dictum de omni\" as the first of two additional axioms. Both this \"dictum\" and the second axiom, he claims in a footnote, derive from \"Principia Mathematica\". Indeed, PM includes both as\nThe latter asserts that the logical sum (i.e. ⋁, OR) of a simple proposition p and a predicate ∀xf(x) implies the logical sum of each separately. But PM derives both of these from six primitive propositions of ❋9, which in the second edition of PM is discarded and replaced with four new \"Pp\" (primitive principles) of ❋8 (see in particular ❋8.2, and Hilbert derives the first from his \"logical ε-axiom\" in his 1927 and does not mention the second. How Hilbert and Gödel came to adopt these two as axioms is unclear.\n\nAlso required are two more \"rules\" of detachment (\"modus ponens\") applicable to predicates.\n\nAlfred Tarski in his 1946 (2nd edition) \"Introduction to Logic and to the Methodology of the Deductive Sciences\" cites a number of what he deems \"universal laws\" of the sentential calculus, three \"rules\" of inference, and one fundamental law of identity (from which he derives four more laws). The traditional \"laws of thought\" are included in his long listing of \"laws\" and \"rules\". His treatment is, as the title of his book suggests, limited to the \"Methodology of the Deductive Sciences\".\n\nRationale: In his introduction (2nd edition) he observes that what began with an application of logic to mathematics has been widened to \"the whole of human knowledge\": \nTo add the notion of \"equality\" to the \"propositional calculus\" (this new notion not to be confused with \"logical\" equivalence symbolized by ↔, ⇄, \"if and only if (iff)\", \"biconditional\", etc.) Tarski (cf p54-57) symbolizes what he calls \"Leibniz's law\" with the symbol \"=\". This extends the domain (universe) of discourse and the types of functions to numbers and mathematical formulas (Kleene 1967:148ff, Tarski 1946:54ff).\n\nIn a nutshell: given that \"x has every property that y has\", we can write \"x = y\", and this formula will have a truth value of \"truth\" or \"falsity\". Tarski states this Leibniz's Law as follows:\nHe then derives some other \"laws\" from this law: \n\nPrincipia Mathematica \"defines\" the notion of equality as follows (in modern symbols); note that the generalization \"for all\" extends over predicate-functions f( ):\n\nHilbert 1927:467 adds only two axioms of equality, the first is x = x, the second is (x = y) → ((f(x) → f(y)); the \"for all f\" is missing (or implied). Gödel 1930 defines equality similarly to PM :❋13.01. Kleene 1967 adopts the two from Hilbert 1927 plus two more (Kleene 1967:387).\n\nAll of the above \"systems of logic\" are considered to be \"classical\" meaning propositions and predicate expressions are two-valued, with either the truth value \"truth\" or \"falsity\" but not both(Kleene 1967:8 and 83). While intuitionistic logic falls into the \"classical\" category, it objects to extending the \"for all\" operator to the Law of Excluded Middle; it allows instances of the \"Law\", but not its generalization to an infinite domain of discourse.\n\n'Intuitionistic logic', sometimes more generally called constructive logic, is a paracomplete symbolic logic that differs from classical logic by replacing the traditional concept of truth with the concept of constructive provability.\n\nThe \"generalized\" law of the excluded middle is not part of the execution of intuitionistic logic, but neither is it negated. Intuitionistic logic merely forbids the use of the operation as part of what it defines as a \"constructive proof\", which is not the same as demonstrating it invalid (this is comparable to the use of a particular building style in which screws are forbidden and only nails are allowed; it does not necessarily disprove or even question the existence or usefulness of screws, but merely demonstrates what can be built without them).\n\n'Paraconsistent logic' refers to so-called contradiction-tolerant logical systems in which a contradiction does not necessarily result in trivialism. In other words, the principle of explosion is not valid in such logics. Some (namely the dialetheists) argue that the law of non-contradiction is denied by dialetheic logic. They are motivated by certain paradoxes which seem to imply a limit of the law of non-contradiction, namely the Liar Paradox. In order to avoid a trivial logical system and still allow certain contradictions to be true, dialetheists will employ a paraconsistent logic of some kind.\n\nTBD cf Three-valued logic\ntry this \nA Ternary Arithmetic and Logic - Semantic Scholar\nhttps://pdfs.semanticscholar.org/.../11ef6cfbc237a4655af2eee550e6d...\n\nOldal lefordítása\nÍrta: I Profeanu - 2010 - Idézetek száma: 10 - Kapcsolódó cikkek\ngradually reveal. For the beginning I propose a ternary arithmetic and logic and I will bring arguments for their use. Index Terms—free energy, ternary arithmetic, ternary logic. I. INTRODUCTION. It is well known that the numeral system used in most if not all modern computer systems is the binary numerical system, as again ...\n\n(cf Kleene 1967:49): These \"calculi\" include the symbols ⎕A, meaning \"A is necessary\" and ◊A meaning \"A is possible\". Kleene states that:\n\n'Fuzzy logic' is a form of many-valued logic; it deals with reasoning that is approximate rather than fixed and exact.\n\n\n\n"}
{"id": "534959", "url": "https://en.wikipedia.org/wiki?curid=534959", "title": "List of multivariable calculus topics", "text": "List of multivariable calculus topics\n\nThis is a list of multivariable calculus topics. See also multivariable calculus, vector calculus, list of real analysis topics, list of calculus topics. \n\n"}
{"id": "44788496", "url": "https://en.wikipedia.org/wiki?curid=44788496", "title": "Matthias Aschenbrenner", "text": "Matthias Aschenbrenner\n\nMatthias Aschenbrenner is an American mathematician. He is a professor of mathematics at the University of California, Los Angeles. His research interests include differential algebra and model theory.\n\nAschenbrenner received his Ph.D. from the University of Illinois at Urbana-Champaign in 2001, where he was a student of Lou van den Dries. For his dissertation, he was awarded the 2001 Sacks Prize by the Association for Symbolic Logic. In 2012, Aschenbrenner became a Fellow of the American Mathematical Society. He was jointly awarded the 2018 Karp Prize with Lou van den Dries and Joris van der Hoeven \"for their work in model theory, especially on asymptotic differential algebra and the model theory of transseries.\"\n"}
{"id": "8015680", "url": "https://en.wikipedia.org/wiki?curid=8015680", "title": "Monadic predicate calculus", "text": "Monadic predicate calculus\n\nIn logic, the monadic predicate calculus (also called monadic first-order logic) is the fragment of first-order logic in which all relation symbols in the signature are monadic (that is, they take only one argument), and there are no function symbols. All atomic formulas are thus of the form formula_1, where formula_2 is a relation symbol and formula_3 is a variable.\n\nMonadic predicate calculus can be contrasted with polyadic predicate calculus, which allows relation symbols that take two or more arguments. \n\nThe absence of polyadic relation symbols severely restricts what can be expressed in the monadic predicate calculus. It is so weak that, unlike the full predicate calculus, it is decidable—there is a decision procedure that determines whether a given formula of monadic predicate calculus is logically valid (true for all nonempty domains). Adding a single binary relation symbol to monadic logic, however, results in an undecidable logic.\n\nThe need to go beyond monadic logic was not appreciated until the work on the logic of relations, by Augustus De Morgan and Charles Sanders Peirce in the nineteenth century, and by Frege in his 1879 \"Begriffsschrifft\". Prior to the work of these three men, term logic (syllogistic logic) was widely considered adequate for formal deductive reasoning.\n\nInferences in term logic can all be represented in the monadic predicate calculus. For example the syllogism\ncan be notated in the language of monadic predicate calculus as\nwhere formula_5, formula_6 and formula_7 denote the predicates of being, respectively, a dog, a mammal, and a bird.\n\nConversely, monadic predicate calculus is not significantly more expressive than term logic. Each formula in the monadic predicate calculus is equivalent to a formula in which quantifiers appear only in closed subformulas of the form\nor\nThese formulas slightly generalize the basic judgements considered in term logic. For example, this form allows statements such as \"Every mammal is either a herbivore or a carnivore (or both)\", formula_10. Reasoning about such statements can, however, still be handled within the framework of term logic, although not by the 19 classical Aristotelian syllogisms alone.\n\nTaking propositional logic as given, every formula in the monadic predicate calculus expresses something that can likewise be formulated in term logic. On the other hand, a modern view of the problem of multiple generality in traditional logic concludes that quantifiers cannot nest usefully if there are no polyadic predicates to relate the bound variables.\n\nThe formal system described above is sometimes called the pure monadic predicate calculus, where \"pure\" signifies the absence of function letters. Allowing monadic function letters changes the logic only superficially, whereas admitting even a single binary function letter results in an undecidable logic. \n\nMonadic second-order logic allows predicates of higher arity in formulas, but restricts second-order quantification to unary predicates, i.e. the only second-order variables allowed are subset variables.\n"}
{"id": "38358615", "url": "https://en.wikipedia.org/wiki?curid=38358615", "title": "Order-5 pentagonal tiling", "text": "Order-5 pentagonal tiling\n\nIn geometry, the order-5 pentagonal tiling is a regular tiling of the hyperbolic plane. It has Schläfli symbol of {5,5}, constructed from five pentagons around every vertex. As such, it is self-dual.\n\nThis tiling is topologically related as a part of sequence of regular polyhedra and tilings with vertex figure (5).\n\n\n\n"}
{"id": "27499693", "url": "https://en.wikipedia.org/wiki?curid=27499693", "title": "Partial permutation", "text": "Partial permutation\n\nIn combinatorial mathematics, a partial permutation, or sequence without repetition, on a finite set \"S\"\nis a bijection between two specified subsets of \"S\". That is, it is defined by two subsets \"U\" and \"V\" of equal size, and a one-to-one mapping from \"U\" to \"V\". Equivalently, it is a partial function on \"S\" that can be extended to a permutation.\n\nIt is common to consider the case when the set \"S\" is simply the set {1, 2, ..., \"n\"} of the first \"n\" integers. In this case, a partial permutation may be represented by a string of \"n\" symbols, some of which are distinct numbers in the range from 1 to formula_1 and the remaining ones of which are a special \"hole\" symbol ◊. In this formulation, the domain \"U\" of the partial permutation consists of the positions in the string that do not contain a hole, and each such position is mapped to the number in that position. For instance, the string \"1 ◊ 2\" would represent the partial permutation that maps 1 to itself and maps 3 to 2.\nThe seven partial permutations on two items are\nThe number of partial permutations on \"n\" items, for \"n\" = 0, 1, 2, ..., is given by the integer sequence\nwhere the \"n\"th item in the sequence is given by the summation formula\nin which the \"i\"th term counts the number of partial permutations with support of size \"i\", that is, the number of partial permutations with \"i\" non-hole entries.\nAlternatively, it can be computed by a recurrence relation\nThis is determined as follows:\n\nSome authors restrict partial permutations so that either the domain \nor the range of the bijection is forced to consist of the first \"k\" items in the set of \"n\" items being permuted, for some \"k\". In the former case, a partial permutation of length \"k\" from an \"n\"-set is just a sequence of \"k\" terms from the \"n\"-set without repetition. (In elementary combinatorics, these objects are sometimes confusingly called \"\"k\"-permutations\" of the \"n\"-set.)\n"}
{"id": "225965", "url": "https://en.wikipedia.org/wiki?curid=225965", "title": "Percentile rank", "text": "Percentile rank\n\nThe percentile rank of a score is the percentage of scores in its frequency distribution that are equal to or lower than it. For example, a test score that is greater than 75% of the scores of people taking the test is said to be at the 75th percentile, where 75 is the percentile rank. In educational measurement, a range of percentile ranks, often appearing on a score report, shows the range within which the test taker’s “true” percentile rank probably occurs. The “true” value refers to the rank the test taker would obtain if there were no random errors involved in the testing process. \nPercentile ranks are commonly used to clarify the interpretation of scores on standardized tests. For the test theory, the percentile rank of a raw score is interpreted as the percentages of examinees in the norm group who scored at or below the score of interest. \n\nPercentile ranks are not on an equal-interval scale; that is, the difference between any two scores is not the same between any other two scores whose difference in percentile ranks is the same. For example, 50 − 25 = 25 is not the same distance as 60 − 35 = 25 because of the bell-curve shape of the distribution. Some percentile ranks are closer to some than others. Percentile rank 30 is closer on the bell curve to 40 than it is to 20.\n\nThe mathematical formula is\n\nwhere c is the count of all scores less than the score of interest, \"ƒ\" is the frequency of the score of interest, and \"N\" is the number of examinees in the sample. If the distribution is normally distributed, the percentile rank can be inferred from the standard score.\n\n"}
{"id": "35098363", "url": "https://en.wikipedia.org/wiki?curid=35098363", "title": "Peter M. Gruber", "text": "Peter M. Gruber\n\nPeter Manfred Gruber (28 August 1941, Klagenfurt – 7 March 2017, Vienna) was an Austrian mathematician working in geometric number theory as well as in convex and discrete geometry.\n\nGruber obtained his PhD at the University of Vienna in 1966, under the supervision of Nikolaus Hofreiter. From 1971, he was Professor at the University of Linz, and from 1976, at the TU Wien. He was a member of the Austrian Academy of Sciences, a foreign member of the Russian Academy of Sciences, and a corresponding member of the Bavarian Academy of Sciences and Humanities.\n\nHis past doctoral students include Monika Ludwig.\n\n\n"}
{"id": "34789789", "url": "https://en.wikipedia.org/wiki?curid=34789789", "title": "Pierre Lelong", "text": "Pierre Lelong\n\nPierre Lelong (14 March 1912 Paris – 12 October 2011) was a French mathematician who introduced the Poincaré–Lelong equation, the Lelong number and the concept of plurisubharmonic function.\n\nLelong earned his doctorate in 1941 from the École Normale Supérieure, under the supervision of Paul Montel. On June 5, 1981 Lelong received an honorary doctorate from the Faculty of \nMathematics and Science at Uppsala University, Sweden.\nHe was a member of the French Academy of Sciences since 1985.\n\nHe married another mathematician, Jacqueline Ferrand, in 1947; they separated in 1977.\n"}
{"id": "166521", "url": "https://en.wikipedia.org/wiki?curid=166521", "title": "Proof (play)", "text": "Proof (play)\n\nProof is a 2000 play by the American playwright David Auburn. \"Proof\" was developed at George Street Playhouse in New Brunswick, New Jersey, during the 1999 Next Stage Series of new plays. The play premiered Off-Broadway in May 2000 and transferred to Broadway in October 2000. The play won the 2001 Pulitzer Prize for Drama and the Tony Award for Best Play.\n\nThe play concerns Catherine, the daughter of Robert, a recently deceased mathematical genius in his fifties and professor at the University of Chicago, and her struggle with mathematical genius and mental illness. Catherine had cared for her father through a lengthy mental illness. Upon Robert's death, his ex-graduate student Hal discovers a paradigm-shifting proof about prime numbers in Robert's office. The title refers both to that proof and to the play's central question: Can Catherine prove the proof's authorship? Along with demonstrating the proof's authenticity, the daughter also finds herself in a relationship with 28-year-old Hal. Throughout, the play explores Catherine's fear of following in her father's footsteps, both mathematically and mentally and her desperate attempts to stay in control.\n\n\nThe play opens with Catherine sitting alone in the backyard of her large, old house. Robert, her father, approaches her with a bottle of champagne to celebrate her 25th birthday. Catherine complains that she hasn't done any worthwhile work in the field of mathematics, at least not to the same level as her father, a well-known math genius. He reassures her that she can still do good work as long as she stops lying in bed till all hours and wasting time reading magazines. Catherine confesses she's worried about inheriting Robert's inclination towards mental instability. He begins to comfort her but then alludes to a \"bad sign\" when he points out that he is, in fact, dead. He died a week ago. Robert disappears as Catherine dozes off. She awakens when Hal, one of Robert's students, exits the house. He's been studying the hundreds of notebooks Robert left behind after his death, looking for any work that could be published. Catherine assures him that the notebooks are filled with scribbles and nonsense since her father wrote them when he was at his most delusional. Hal, attempting to flirt, invites her to go see his band later that night. Catherine becomes suspicious of him and demands to see what's in his backpack. She roots through it to find nothing but becomes infuriated when a notebook falls out of Hal's jacket. She dials the police while accusing him of trying to steal her father's work and pass it off as his own. He admits that he was sneaking it away but only to give it back to her later as a birthday present. He opens to a page that Robert wrote during a time when he was lucid. In it, Robert writes it's a \"good day\" and thanks to Catherine for taking care of him and expresses hope for the future. Hal leaves Catherine with the notebook. She begins to cry until she hears police sirens.\n\nThe next day Claire, Catherine's sister who just flew in from New York, is setting up a large brunch for them in the backyard. Catherine enters and Claire tries to goad her into idle chitchat as Catherine quietly seethes. Claire declares she's getting married and invites Catherine to stay with her and her fiance in New York. Catherine assures her she'll come in January for the wedding, but Claire keeps pressing her to go earlier. When Catherine demands to know why Claire is inundating her with questions, Claire tells her the police came over earlier to check in on Catherine. Catherine admits to calling the police the previous night and tries to explain her altercation with Hal but only ends up sounding unhinged to the dubious Claire. Hal appears and asks to continue his work sorting the notebooks. Catherine lets him inside and Claire drops a hint for Catherine to try flirting with Hal by offering a bagel. Catherine storms into the house.\n\nLater that night, after the funeral, Claire holds a party in the house for her friends as well as Hal and Robert's students. Catherine escapes to the porch where Hal finds her and offers her a beer. Hal confesses that he's not so sure about his own mathematical abilities since he considers math to be a \"young man's game\". Catherine tries to reassure him with a quote from Gauss. Hal responds by kissing her, much to Catherine's surprise. He apologizes for trying to steal the notebook and she apologizes for calling the police. They kiss again and Hal asks Catherine if she remembers meeting him years earlier. She says she does and recalls she thought he was \"not boring\". They continue to kiss.\n\nThe next morning Catherine sits outside. Hal exits the house and tells her he'd like to spend the rest of the day with her. Catherine gives him a key to Robert's desk and tells him to look inside. He goes into the house. A moment later, Claire comes into the backyard, extremely hungover. Catherine, now in a good mood, tries to make nice with Claire. Claire takes the opportunity to continue to push Catherine to moving to New York. Catherine asks why she would move to New York to which Claire confesses that she's selling the house. Catherine becomes enraged at the idea and she accuses Claire of abandoning her to take care of their sick father alone. Claire insists that the reason she did so was to keep working to pay for the house as well as Catherine's education. Catherine reveals that she had to quit school to tend to Robert and then accuses Claire of trying to have her committed. Claire admits that she's researched doctors and facilities for Catherine but insists that she wasn't planning on having her committed. In the middle of the row, Hal appears clutching a notebook, barely containing his excitement. He tells Claire that Catherine is in possession of one of Robert's notebooks which holds a very important proof. Claire asks Catherine where she found it and Catherine tells them she didn't find it. She wrote it.\n\n\nWe flashback to years earlier, with Robert sitting in the backyard. Catherine tells him she thinks he's getting better and he agrees. She blurts out that she's decided to go to college in a couple months, funded by Claire, but promises she'll be only a short drive away if he were to need her again. Robert protests and demands to know why she waited so long to tell him. When she points out that he hadn't been well until recently and was, at one point, trying to decode extraterrestrial messages in library books, he becomes upset. Hal interrupts, much to his embarrassment, to present his final dissertation to Robert. Robert assures him they'll work out the problem points together, then suddenly realizes he's forgotten Catherine's birthday. He apologizes and offers to take her out to dinner. Catherine invites Hal along but he says he can't go. Catherine shows Hal out and Robert sits down to write a notebook entry, declaring it to be a \"good day\".\n\nWe flash forward to where Act I left off. Catherine declares she was the one who wrote the proof and is met with incredulity by both Hal and her sister. The handwriting is very much like Robert's and Hal questions Catherine's mathematical abilities given that she only had a few months' education at Northwestern. Catherine tells him that her real education was living with Robert. When Hal offers to show it to other math experts to confirm the authenticity of the proof, Catherine refuses. She tells Hal she trusted him and then accuses him of having no talent and being past his prime. Hal storms off and Catherine begins to rip the notebook apart. Claire gets it away from her and Catherine runs into the house.\n\nLater, Hal attempts to visit Catherine and apologize for his behavior. Claire stops him and tells him Catherine won't talk to her, let alone Hal. Claire accuses him of sleeping with Catherine despite her being unstable. Hal argues that he had no bad intentions and insists Catherine is stronger than Claire thinks. He requests to have the notebook to verify its authenticity with fellow mathematicians. Claire gives it to him and tells him she's taking Catherine with her to New York the next day. She expresses concern for Catherine's future mental stability.\n\nWe flashback to Robert in the backyard, sitting in the cold and writing furiously. Catherine enters and reprimands him for sitting in the cold with no jacket. Robert tells her it's too hot in the house and that the cold is better for helping him work. Catherine is shocked that he's working again and he assures her that he's sharper than ever. She's ecstatic that his previous mental instability has passed and asks to see his work. He says he'd love for her to take a look and asks if she'd like to take time off school to work with him. Before she decides, Robert insists she look at his latest idea and thrusts a notebook into her hands. Catherine glances at it and becomes quiet. She tells him they need to go inside and Robert explodes with fury. He yells at her to read what he's written. She reads aloud, a nonsensical, rambling paragraph about winter and books and the cold. It's obvious that Robert's mind is deteriorating as it had been before. Catherine begins to cry as Robert descends into confusion and begins to shiver uncontrollably. Catherine tries to take him inside when he asks her not to leave. She promises she won't.\n\nWe flash forward to Claire in the backyard. Catherine enters with her suitcase. She asks Claire about life in New York. Claire mentions potential schools or jobs for Catherine but Catherine is quick to mock her by making ridiculous demands for a Freudian psychiatrist who will listen as she blames all her problems on Claire. Claire begins to cry and throws Catherine's plane ticket in front of her before storming off. Hal enters and tells Catherine that the proof checks out and apologizes for not believing her. Catherine tells him there's no proof that she wrote it and he can claim it as his own if he wants. Hal tells her he believes she's the one who wrote it and offers to read through it with her. Catherine admits she knows she's like her father but is terrified of becoming like her father. Hal reassures her that maybe she'll be better. Catherine opens the proof and begins to talk through it with Hal.\n\n\nOriginally produced by the Manhattan Theatre Club, opening on May 23, 2000, the play transferred to Broadway at the Walter Kerr Theatre on October 24, 2000. Directed by Daniel J. Sullivan, the production starred Mary-Louise Parker as Catherine, Larry Bryggman as Robert, Ben Shenkman as Hal, and Johanna Day as Claire. Later during the Broadway run, Jennifer Jason Leigh (September 13, 2001, to June 30, 2002) and Anne Heche (as of July 5, 2002) took over the lead role. Josh Hamilton and Neil Patrick Harris subsequently played the role of Hal. Mary-Louise Parker won the Tony Award for her performance, and Daniel Sullivan won the Tony Award, Best Direction of a Play. The play closed on January 5, 2003 after 917 performances.\n\n\"Proof\" premiered in the West End at the Donmar Warehouse in May 2002, to June 15, 2002. Directed by John Madden, the cast starred Gwyneth Paltrow as Catherine, with Ronald Pickup as Robert, Sara Stewart as Claire, and Richard Coyle as Hal. London's Menier Chocolate Factory produced the play from March 13, 2013, to April 27, 2013. It featured Mariah Gale in the role of Catherine, and Polly Findlay directed.\n\nThe play premiered in Australia at the Sydney Opera House in 2003 starring Jacqueline Mckenzie and Barry Otto and directed by George Ogilvie as a Sydney Theatre Company production.\n\nIn April and May 2013, a new production by the Whitmore Eclectic Theater Group opened in Los Angeles at the Hayworth Theatre for a limited run. James Whitmore Jr., son of the award-winning iconic actor James Whitmore, starred; and his daughter Aliah Whitmore directed.\n\nA production in May 2013 opened at Carolina Actors Studio Theatre in Charlotte, North Carolina.\n\nIn September to October 2013, \"Proof\" was directed by Emily Mann at the McCarter Theatre in Princeton, New Jersey.\n\nLondon's Tabard Theatre produced the play from 29 September to 24 October 2015, directed by Sebastien Blanc (son of Raymond Blanc). It featured Tim Hardy (Royal Shakespeare Company Marat/Sade) as Robert, Julia Papp as Catherine, Mary-Ann Cafferkey as Claire and Ian Charleson Awards nominee Kim Hardy as Hal.\n\nA Seattle production opened in January 2017 at Strawberry Theatre Workshop with three TPS Gregory Award winning actors, Anastasia Higham, Charles Leggett, and Allison Standley in principal roles. \"Proof\" was directed by Greg Carter on the Mainstage at 12th Ave Arts. It was the first professional production in Seattle since 2004.\n\nA 2005 film adaptation was directed by John Madden, starring Gwyneth Paltrow as Catherine, along with Anthony Hopkins, Hope Davis, and Jake Gyllenhaal. Adapted by Rebecca Miller, the film version added more characters (in minor supporting roles), whereas the play has only four.\n\n\n\n"}
{"id": "359970", "url": "https://en.wikipedia.org/wiki?curid=359970", "title": "Proof by exhaustion", "text": "Proof by exhaustion\n\nProof by exhaustion, also known as proof by cases, proof by case analysis, complete induction, or the brute force method, is a method of mathematical proof in which the statement to be proved is split into a finite number of cases or sets of equivalent cases and each type of case is checked to see if the proposition in question holds. This is a method of direct proof. A proof by exhaustion contains two stages:\n\n\nThe prevalence of digital computers has greatly increased the convenience of using the method of exhaustion. Computer expert systems can be used to arrive at answers to many of the questions posed to them. In theory, the proof by exhaustion method can be used whenever the number of cases is finite. However, because most mathematical sets are infinite, this method is rarely used to derive general mathematical results.\n\nIn the Curry–Howard isomorphism, proof by exhaustion and case analysis are related to ML-style pattern matching.\n\nTo prove that every integer that is a perfect cube is a multiple of 9, or is 1 more than a multiple of 9, or is 1 less than a multiple of 9.\n\nProof:\n<br>Each cube number is the cube of some integer \"n\". Every integer \"n\" is either a multiple of 3, or 1 more or 1 less than a multiple of 3. So these 3 cases are exhaustive:\n\nMathematicians prefer to avoid proofs by exhaustion with large numbers of cases, which are viewed as inelegant. An illustration of how such proofs might be inelegant is to prove that every year in which the modern Summer Olympic Games is held is divisible by 4.\n\nProof: the first modern Summer Olympics were held in 1896, and then every 4 years thereafter (neglecting years in which the games were not held due to World War I and World War II). Since 1896 = 474 × 4 is divisible by 4, the next Olympics would be in year 474 × 4 + 4 = (474 + 1) × 4, which is also divisible by four, and so on (this is a proof by mathematical induction). Therefore the statement is proved.\n\nThe statement can also be proved by exhaustion by listing out every year in which the Summer Olympics were held, and checking that every one of them can be divided by four. With 28 total Summer Olympics as of 2016, this is a proof by exhaustion with 28 cases. In addition to being more elegant, the proof by mathematical induction also proves the statement indefinitely into the future, while after each new Summer Olympics the proof by exhaustion will require an extra case.\n\nThere is no upper limit to the number of cases allowed in a proof by exhaustion. Sometimes there are only two or three cases. Sometimes there may be thousands or even millions. For example, rigorously solving an endgame puzzle in chess might involve considering a very large number of possible positions in the game tree of that problem.\n\nThe first proof of the four colour theorem was a proof by exhaustion with 1,936 cases. This proof was controversial because the majority of the cases were checked by a computer program, not by hand. The shortest known proof of the four colour theorem today still has over 600 cases.\n\nIn general the probability of an error in the whole proof increases with the number of cases. A proof with a large number of cases leaves an impression that the theorem is only true by coincidence, and not because of some underlying principle or connection. Other types of proofs—such as proof by induction (mathematical induction)—are considered more elegant. However, there are some important theorems for which no other method of proof has been found, such as\n\n"}
{"id": "1089270", "url": "https://en.wikipedia.org/wiki?curid=1089270", "title": "Random sample consensus", "text": "Random sample consensus\n\nRandom sample consensus (RANSAC) is an iterative method to estimate parameters of a mathematical model from a set of observed data that contains outliers, when outliers are to be accorded no influence on the values of the estimates. Therefore, it also can be interpreted as an outlier detection method. It is a non-deterministic algorithm in the sense that it produces a reasonable result only with a certain probability, with this probability increasing as more iterations are allowed. The algorithm was first published by Fischler and Bolles at SRI International in 1981. They used RANSAC to solve the Location Determination Problem (LDP), where the goal is to determine the points in the space that project onto an image into a set of landmarks with known locations.\n\nA basic assumption is that the data consists of \"inliers\", i.e., data whose distribution can be explained by some set of model parameters, though may be subject to noise, and \"outliers\" which are data that do not fit the model. The outliers can come, for example, from extreme values of the noise or from erroneous measurements or incorrect hypotheses about the interpretation of data. RANSAC also assumes that, given a (usually small) set of inliers, there exists a procedure which can estimate the parameters of a model that optimally explains or fits this data.\n\nA simple example is fitting a line in two dimensions to a set of observations. Assuming that this set contains both inliers, i.e., points which approximately can be fitted to a line, and outliers, points which cannot be fitted to this line, a simple least squares method for line fitting will generally produce a line with a bad fit to the inliers. The reason is that it is optimally fitted to all points, including the outliers. RANSAC, on the other hand, attempts to exclude the outliers and find a linear model that only uses the inliers in its calculation. This is done by fitting linear models to several random samplings of the data and returning the model that has the best fit to a subset of the data. Since the inliers tend to be more linearly related than a random mixture of inliers and outliers, a random subset that consists entirely of inliers will have the best model fit. In practice, there is no guarantee that a subset of inliers will be randomly sampled, and the probability of the algorithm succeeding depends on the proportion of inliers in the data as well as the choice of several algorithm parameters.\n\nThe RANSAC algorithm is a learning technique to estimate parameters of a model by random sampling of observed data. Given a dataset whose data elements contain both inliers and outliers, RANSAC uses the voting scheme to find the optimal fitting result. Data elements in the dataset are used to vote for one or multiple models. The implementation of this voting scheme is based on two assumptions: that the noisy features will not vote consistently for any single model (few outliers) and there are enough features to agree on a good model (few missing data).\nThe RANSAC algorithm is essentially composed of two steps that are iteratively repeated:\nThe set of inliers obtained for the fitting model is called consensus set. The RANSAC algorithm will iteratively repeat the above two steps until the obtained consensus set in certain iteration has enough inliers.\n\nThe input to the RANSAC algorithm is a set of observed data values, a way of fitting some kind of model to the observations, and some confidence parameters. RANSAC achieves its goal by repeating the following steps:\n\n\nThis procedure is repeated a fixed number of times, each time producing either a model which is rejected because too few points are part of the consensus set, or a refined model together with a corresponding consensus set size. In the latter case, we keep the refined model if its consensus set is larger than the previously saved model.\n\nThe generic RANSAC algorithm works as follows:\n\nA Matlab implementation of 2D line fitting using the RANSAC algorithm:\n\nThe threshold value to determine when a data point fits a model , and the number of close data points required to assert that a model fits well to data are determined based on specific requirements of the application and the dataset, and possibly based on experimental evaluation. The number of iterations , however, can be determined as a function of the desired probability of success using a theoretical result. Let be the desired probability that the RANSAC algorithm provides a useful result after running. RANSAC returns a successful result if in some iteration it selects only inliers from the input data set when it chooses the points from which the model parameters are estimated. Let formula_1 be the probability of choosing an inlier each time a single point is selected, that is,\n\nA common case is that formula_1 is not well known beforehand, but some rough value can be given. Assuming that the points needed for estimating a model are selected independently, formula_4 is the probability that all \"n\" points are inliers and formula_5 is the probability that at least one of the points is an outlier, a case which implies that a bad model will be estimated from this point set. That probability to the power of is the probability that the algorithm never selects a set of points which all are inliers and this must be the same as formula_6. Consequently,\n\nwhich, after taking the logarithm of both sides, leads to\n\nThis result assumes that the data points are selected independently, that is, a point which has been selected once is replaced and can be selected again in the same iteration. This is often not a reasonable approach and the derived value for should be taken as an upper limit in the case that the points are selected without replacement. For example, in the case of finding a line which fits the data set illustrated in the above figure, the RANSAC algorithm typically chooses two points in each iteration and computes codice_1 as the line between the points and it is then critical that the two points are distinct.\n\nTo gain additional confidence, the standard deviation or multiples thereof can be added to . The standard deviation of is defined as\n\nAn advantage of RANSAC is its ability to do robust estimation of the model parameters, i.e., it can estimate the parameters with a high degree of accuracy even when a significant number of outliers are present in the data set. A disadvantage of RANSAC is that there is no upper bound on the time it takes to compute these parameters (except exhaustion). When the number of iterations computed is limited the solution obtained may not be optimal, and it may not even be one that fits the data in a good way. In this way RANSAC offers a trade-off; by computing a greater number of iterations the probability of a reasonable model being produced is increased. Moreover, RANSAC is not always able to find the optimal set even for moderately contaminated sets and it usually performs badly when the number of inliers is less than 50%. Optimal RANSAC was proposed to handle both these problems and is capable of finding the optimal set for heavily contaminated sets, even for an inlier ratio under 5%. Another disadvantage of RANSAC is that it requires the setting of problem-specific thresholds.\n\nRANSAC can only estimate one model for a particular data set. As for any one-model approach when two (or more) model instances exist, RANSAC may fail to find either one. The Hough transform is one alternative robust estimation technique that may be useful when more than one model instance is present. Another approach for multi model fitting is known as PEARL, which combines model sampling from data points as in RANSAC with iterative re-estimation of inliers and the multi-model fitting being formulated as an optimization problem with a global energy functional describing the quality of the overall solution.\n\nThe RANSAC algorithm is often used in computer vision, e.g., to simultaneously solve the correspondence problem and estimate the fundamental matrix related to a pair of stereo cameras.\n\nSince 1981 RANSAC has become a fundamental tool in the computer vision and image processing community. In 2006, for the 25th anniversary of the algorithm, a workshop was organized at the International Conference on Computer Vision and Pattern Recognition (CVPR) to summarize the most recent contributions and variations to the original algorithm, mostly meant to improve the speed of the algorithm, the robustness and accuracy of the estimated solution and to decrease the dependency from user defined constants.\n\nRANSAC can be sensitive to the choice of the correct noise threshold that defines which data points fit a model instantiated with a certain set of parameters. If such threshold is too large, then all the hypotheses tend to be ranked equally (good). On the other hand, when the noise threshold is too small, the estimated parameters tend to be unstable ( i.e. by simply adding or removing a datum to the set of inliers, the estimate of the parameters may fluctuate). To partially compensate for this undesirable effect, Torr et al. proposed two modification of RANSAC called MSAC (M-estimator SAmple and Consensus) and MLESAC (Maximum Likelihood Estimation SAmple and Consensus). The main idea is to evaluate the quality of the consensus set ( i.e. the data that fit a model and a certain set of parameters) calculating its likelihood (whereas in the original formulation by Fischler and Bolles the rank was the cardinality of such set). An extension to MLESAC which takes into account the prior probabilities associated to the input dataset is proposed by Tordoff. The resulting algorithm is dubbed Guided-MLESAC. Along similar lines, Chum proposed to guide the sampling procedure if some a priori information regarding the input data is known, i.e. whether a datum is likely to be an inlier or an outlier. The proposed approach is called PROSAC, PROgressive SAmple Consensus.\n\nChum et al. also proposed a randomized version of RANSAC called R-RANSAC to reduce the computational burden to identify a good CS. The basic idea is to initially evaluate the goodness of the currently instantiated model using only a reduced set of points instead of the entire dataset. A sound strategy will tell with high confidence when it is the case to evaluate the fitting of the entire dataset or when the model can be readily discarded. It is reasonable to think that the impact of this approach is more relevant in cases where the percentage of inliers is large. The type of strategy proposed by Chum et al. is called preemption scheme. Nistér proposed a paradigm called Preemptive RANSAC that allows real time robust estimation of the structure of a scene and of the motion of the camera. The core idea of the approach consists in generating a fixed number of hypothesis so that the\ncomparison happens with respect to the quality of the generated hypothesis rather than against some absolute quality metric.\n\nOther researchers tried to cope with difficult situations where the noise scale is not known and/or multiple model instances are present. The first problem has been tackled in the work by Wang and Suter. Toldo et al. represent each datum with the characteristic function of the set of random models that fit the point. Then multiple models are revealed as clusters which group the points supporting the same model. The clustering algorithm, called J-linkage, does not require prior specification of the number of models, nor does it necessitate manual parameters tuning.\n\nRANSAC has also been tailored for recursive state estimation applications, where the input measurements are corrupted by outliers and Kalman filter approaches, which rely on a Gaussian distribution of the measurement error, are doomed to fail. Such an approach is dubbed KALMANSAC.\n\n\n\n"}
{"id": "7770362", "url": "https://en.wikipedia.org/wiki?curid=7770362", "title": "Retiming", "text": "Retiming\n\nRetiming is the technique of moving the structural location of latches or registers in a digital circuit to improve its performance, area, and/or power characteristics in such a way that preserves its functional behavior at its outputs. Retiming was first described by Charles E. Leiserson and James B. Saxe in 1983.\n\nThe technique uses a directed graph where the vertices represent asynchronous combinational blocks and the directed edges represent a series of registers or latches (the number of registers or latches can be zero). Each vertex has a value corresponding to the delay through the combinational circuit it represents. After doing this, one can attempt to optimize the circuit by pushing registers from output to input and vice versa - much like bubble pushing. Two operations can be used - deleting a register from each input of a vertex while adding a register to all outputs, and conversely adding a register to each input of vertex and deleting a register from all outputs. In all cases, if the rules are followed, the circuit will have the same functional behavior as it did before retiming.\n\nThe initial formulation of the retiming problem as described by Leiserson and Saxe is as follows. Given a directed graph formula_1 whose vertices represent logic gates or combinational delay elements in a circuit, assume there is a directed edge formula_2 between two elements that are connected directly or through one or more registers. Let the \"weight\" of each edge formula_3 be the number of registers present along edge formula_4 in the initial circuit. Let formula_5 be the propagation delay through vertex formula_6. The goal in retiming is to compute an integer \"lag\" value formula_7 for each vertex such that the retimed weight formula_8 of every edge is non-negative. There is a proof that this preserves the output functionality.\n\nThe most common use of retiming is to minimize the clock period. A simple technique to optimize the clock period is to search for the minimum feasible period (e.g. using binary search).\n\nThe feasibility of a clock period formula_9 can be checked in one of several ways. The linear program below is feasible if and only if formula_9 is a feasible clock period. Let formula_11 be the minimum number of registers along any path from formula_12 to formula_6 (if such a path exists), and formula_14 is the maximum delay along any path from formula_12 to formula_6 with W(u,v) registers. The dual of this program is a minimum cost circulation problem, which can be solved efficiently as a network problem. The limitations of this approach arise from the enumeration and size of the formula_17 and formula_18 matrices.\n\nAlternatively, feasibility of a clock period formula_9 can be expressed as a mixed-integer linear program (MILP). A solution will exist and a valid lag function formula_7 will be returned if and only if the period is feasible.\n\nAlternate formulations allow the minimization of the register count and the minimization of the register count under a delay constraint. The initial paper includes extensions that allow the consideration of fan-out sharing and a more general delay model. Subsequent work has addressed the inclusion of register delays, load-dependent delay models, and hold constraints.\n\nRetiming has found industrial use, albeit sporadic. Its primary drawback is that the state encoding of the circuit is destroyed, making debugging, testing, and verification substantially more difficult. Some retimings may also require complicated initialization logic to have the circuit start in an identical initial state. Finally, the changes in the circuit's topology have consequences in other logical and physical synthesis steps that make design closure difficult.\n\nClock skew scheduling is a related technique for optimizing sequential circuits. Whereas retiming relocates the structural position of the registers, clock skew scheduling moves their temporal position by scheduling the arrival time of the clock signals. The lower bound of the achievable minimum clock period of both techniques is the maximum mean cycle time (i.e. the total combinational delay along any path divided by the number of registers along it).\n\n\n"}
{"id": "31717575", "url": "https://en.wikipedia.org/wiki?curid=31717575", "title": "Richart E. Slusher", "text": "Richart E. Slusher\n\nRichart Elliott Slusher (born 1938) is a regents researcher and a principal research scientist at the Georgia Tech Research Institute, and the director of the Georgia Tech Quantum Institute.\n\nSlusher received a Ph.D. in physics from the University of California at Berkeley in 1965.\n\nSlusher worked at Bell Laboratories from 1965 to 2007, where he directed a research department focused on optical and quantum device physics from 1977 to 2005. Since 2005, he has worked at the Georgia Tech Research Institute.\n\nSlusher received the 1989 Einstein Prize for Laser Science, the 1995 Arthur L. Schawlow Prize in Laser Science from the American Physical Society and the 2006 Max Born Award from the Optical Society of America.\n\n"}
{"id": "1258371", "url": "https://en.wikipedia.org/wiki?curid=1258371", "title": "Scott core theorem", "text": "Scott core theorem\n\nIn mathematics, the Scott core theorem is a theorem about the finite presentability of fundamental groups of 3-manifolds due to G. Peter Scott, . The precise statement is as follows:\n\nGiven a 3-manifold (not necessarily compact) with finitely generated fundamental group, there is a compact three-dimensional submanifold, called the compact core or Scott core, such that its inclusion map induces an isomorphism on fundamental groups. In particular, this means a finitely generated 3-manifold group is finitely presentable.\n\nA simplified proof is given in , and a stronger uniqueness statement is proven in .\n\n"}
{"id": "29368", "url": "https://en.wikipedia.org/wiki?curid=29368", "title": "Slope", "text": "Slope\n\nIn mathematics, the slope or gradient of a line is a number that describes both the \"direction\" and the \"steepness\" of the line. Slope is often denoted by the letter \"m\"; there is no clear answer to the question why the letter \"m\" is used for slope, but it might be from the \"m for multiple\" in the equation of a straight line \"y = mx + b\" or \"y = mx + c\".\n\nSlope is calculated by finding the ratio of the \"vertical change\" to the \"horizontal change\" between (any) two distinct points on a line. Sometimes the ratio is expressed as a quotient (\"rise over run\"), giving the same number for every two distinct points on the same line. A line that is decreasing has a negative \"rise\". The line may be practical - as set by a road surveyor, or in a diagram that models a road or a roof either as a description or as a plan.\n\nThe \"steepness\", incline, or grade of a line is measured by the absolute value of the slope. A slope with a greater absolute value indicates a steeper line. The \"direction\" of a line is either increasing, decreasing, horizontal or vertical. \n\nThe rise of a road between two points is the difference between the altitude of the road at those two points, say \"y\" and \"y\", or in other words, the rise is (\"y\" − \"y\") = Δ\"y\". For relatively short distances - where the earth's curvature may be neglected, the run is the difference in distance from a fixed point measured along a level, horizontal line, or in other words, the run is (\"x\" − \"x\") = Δ\"x\". Here the slope of the road between the two points is simply described as the ratio of the altitude change to the horizontal distance between any two points on the line.\n\nIn mathematical language, the slope \"m\" of the line is\n\nThe concept of slope applies directly to grades or gradients in geography and civil engineering. Through trigonometry, the slope \"m\" of a line is related to its angle of incline \"θ\" by the tangent function\n\nThus, a 45° rising line has a slope of +1 and a 45° falling line has a slope of −1.\n\nAs a generalization of this practical description, the mathematics of differential calculus defines the slope of a curve at a point as the slope of the tangent line at that point. When the curve is given by a series of points in a diagram or in a list of the coordinates of points, the slope may be calculated not at a point but between any two given points. When the curve is given as a continuous function, perhaps as an algebraic formula, then the differential calculus provides rules giving a formula for the slope of the curve at any point in the middle of the curve.\n\nThis generalization of the concept of slope allows very complex constructions to be planned and built that go well beyond static structures that are either horizontals or verticals, but can change in time, move in curves, and change depending on the rate of change of other factors. Thereby, the simple idea of slope becomes one of the main basis of the modern world in terms of both technology and the built environment.\n\nThe slope of a line in the plane containing the \"x\" and \"y\" axes is generally represented by the letter \"m\", and is defined as the change in the \"y\" coordinate divided by the corresponding change in the \"x\" coordinate, between two distinct points on the line. This is described by the following equation:\n\nGiven two points (\"x\",\"y\") and (\"x\",\"y\"), the change in \"x\" from one to the other is (\"run\"), while the change in \"y\" is (\"rise\"). Substituting both quantities into the above equation generates the formula:\nThe formula fails for a vertical line, parallel to the \"y\" axis (see Division by zero), where the slope can be taken as infinite, so the slope of a vertical line is considered undefined.\n\nSuppose a line runs through two points: \"P\" = (1, 2) and \"Q\" = (13, 8). By dividing the difference in \"y\"-coordinates by the difference in \"x\"-coordinates, one can obtain the slope of the line:\n\nAs another example, consider a line which runs through the points (4, 15) and (3, 21). Then, the slope of the line is \n\n\n\n\n\nFor example, consider a line running through the points (2,8) and (3,20). This line has a slope, , of \nOne can then write the line's equation, in point-slope form:\nor: \nThe angle θ between -90° and 90° that this line makes with the -axis is \n\nConsider the two lines: and . Both lines have slope . They are not the same line. So they are parallel lines.\n\nConsider the two lines and . The slope of the first line is . The slope of the second line is . The product of these two slopes is −1. So these two lines are perpendicular.\n\nIn statistical mathematics, the gradient of the line of best fit for a given distribution of data which is linear, numerical, and free of outliers, is usually written as formula_19, where formula_20 is defined as the gradient (in statistics), formula_21 is Pearson's correlation coefficient, formula_22 is the standard deviation of the y-values and formula_23 is the standard deviation of the x-values.\n\nIn this equation formula_24 for the least-squares regression line, formula_20 is the slope and formula_26 is the intercept.\n\nThere are two common ways to describe the steepness of a road or railroad. One is by the angle between 0° and 90° (in degrees), and the other is by the slope in a percentage. See also steep grade railway and rack railway.\n\nThe formulae for converting a slope given as a percentage into an angle in degrees and vice versa are: \nwhere \"angle\" is in degrees and the trigonometric functions operate in degrees. For example, a slope of 100% or 1000‰ is an angle of 45°.\n\nA third way is to give one unit of rise in say 10, 20, 50 or 100 horizontal units, e.g. 1:10. 1:20, 1:50 or 1:100 (or \"1 in 10\", \"1 in 20\" etc.) Note that 1:10 is steeper than 1:20. For example, steepness of 20% means 1:5 or an incline with angle 11,3°.\n\nRoads and railways have both longitudinal slopes and cross slopes.\n\nThe concept of a slope is central to differential calculus. For non-linear functions, the rate of change varies along the curve. The derivative of the function at a point is the slope of the line tangent to the curve at the point, and is thus equal to the rate of change of the function at that point.\n\nIf we let Δ\"x\" and Δ\"y\" be the distances (along the \"x\" and \"y\" axes, respectively) between two points on a curve, then the slope given by the above definition,\n\nis the slope of a secant line to the curve. For a line, the secant between any two points is the line itself, but this is not the case for any other type of curve.\n\nFor example, the slope of the secant intersecting \"y\" = \"x\" at (0,0) and (3,9) is 3. (The slope of the tangent at is also 3—\"a\" consequence of the mean value theorem.)\n\nBy moving the two points closer together so that Δ\"y\" and Δ\"x\" decrease, the secant line more closely approximates a tangent line to the curve, and as such the slope of the secant approaches that of the tangent. Using differential calculus, we can determine the limit, or the value that Δ\"y\"/Δ\"x\" approaches as Δ\"y\" and Δ\"x\" get closer to zero; it follows that this limit is the exact slope of the tangent. If \"y\" is dependent on \"x\", then it is sufficient to take the limit where only Δ\"x\" approaches zero. Therefore, the slope of the tangent is the limit of Δ\"y\"/Δ\"x\" as Δ\"x\" approaches zero, or \"dy\"/\"dx\". We call this limit the derivative.\n\nIts value at a point on the function gives us the slope of the tangent at that point. For example, let \"y\"=\"x\". A point on this function is (-2,4). The derivative of this function is /=2\"x\". So the slope of the line tangent to \"y\" at (-2,4) is 2·(-2) = -4. The equation of this tangent line is: \"y\"-4=(-4)(\"x\"-(-2)) or \"y\" = -4\"x\" - 4.\n\nThe concept of slope can be generalized to functions of more than one variable and is more often referred to as gradient.\n\n\n"}
{"id": "26041032", "url": "https://en.wikipedia.org/wiki?curid=26041032", "title": "The Construction and Principal Uses of Mathematical Instruments", "text": "The Construction and Principal Uses of Mathematical Instruments\n\nThe Construction and Principal Uses of Mathematical Instruments () is a book by Nicholas Bion, first published in 1709. It was translated into English in 1723 by Edmund Stone.\n\nIt was described as \"the most famous book devoted to instruments\" by historian of science David M. Knight.\n\nNicholas Bion ( ; 1652–1733) was a French instrument maker and author with workshops in Paris. He was king’s engineer for mathematical instruments. He died in Paris in 1733 aged 81.\n\nBion is author of the following:\n\n"}
{"id": "30247317", "url": "https://en.wikipedia.org/wiki?curid=30247317", "title": "Trémaux tree", "text": "Trémaux tree\n\nIn graph theory, a Trémaux tree of an undirected graph \"G\" is a spanning tree of \"G\", rooted at one of its vertices, with the property that every two adjacent vertices in \"G\" are related to each other as an ancestor and descendant in the tree. All depth-first search trees and all Hamiltonian paths are Trémaux trees.\nTrémaux trees are named after Charles Pierre Trémaux, a 19th-century French author who used a form of depth-first search as a strategy for solving mazes. They have also been called normal spanning trees, especially in the context of infinite graphs.\n\nIn finite graphs, although depth-first search itself is inherently sequential, Trémaux trees can be constructed by a randomized parallel algorithm in the complexity class RNC. They can be used to define the tree-depth of a graph, and as part of the left-right planarity test for testing whether a graph is a planar graph.\nA characterization of Trémaux trees in the monadic second-order logic of graphs allows graph properties involving orientations to be recognized efficiently for graphs of bounded treewidth using Courcelle's theorem.\n\nNot every infinite graph has a Trémaux tree, and the graphs that do have them can be characterized by their forbidden minors.\nA Trémaux tree exists in every graph with countably many vertices, even when an infinite form of depth-first search would not succeed in exploring every vertex of the graph.\nIn an infinite graph, a Trémaux tree must have exactly one infinite path for each end of the graph, and the existence of a Trémaux tree characterizes the graphs whose topological completions, formed by adding a point at infinity for each end, are metric spaces.\n\nIn the graph shown below, the tree with edges 1–3, 2–3, and 3–4 is a Trémaux tree when it is rooted at vertex 1 or vertex 2: every edge of the graph belongs to the tree except for the edge 1–2, which (for these choices of root) connects an ancestor-descendant pair.\n\nHowever, rooting the same tree at vertex 3 or vertex 4 produces a rooted tree that is not a Trémaux tree, because with this root 1 and 2 are no longer ancestor and descendant.\n\nEvery finite connected undirected graph has at least one Trémaux tree. One can construct such a tree by performing a depth-first search and connecting each vertex (other than the starting vertex of the search) to the earlier vertex from which it was discovered. The tree constructed in this way is known as a depth-first search tree. If \"uv\" is an arbitrary edge in the graph, and \"u\" is the earlier of the two vertices to be reached by the search, then \"v\" must belong to the subtree descending from \"u\" in the depth-first search tree, because the search will necessarily discover \"v\" while it is exploring this subtree, either from one of the other vertices in the subtree or, failing that, from \"u\" directly. Every finite Trémaux tree can be generated as a depth-first search tree: If \"T\" is a Trémaux tree of a finite graph, and a depth-first search explores the children in \"T\" of each vertex prior to exploring any other vertices, it will necessarily generate \"T\" as its depth-first search tree.\n\nIt is P-complete to find the Trémaux tree that would be found by a sequential depth-first search algorithm, in which the neighbors of each vertex are searched in order by their identities. Nevertheless it is possible to find a different Trémaux tree by a randomized parallel algorithm, showing that the construction of Trémaux trees belongs to the complexity class RNC. As of 1997, it remained unknown whether Trémaux tree construction could be performed by a deterministic parallel algorithm, in the complexity class NC.\n\nIt is possible to express the property that a set \"T\" of edges with a choice of root vertex \"r\" forms a Trémaux tree, in the monadic second-order logic of graphs, and more specifically in the form of this logic called MSO, which allows quantification over both vertex and edge sets. This property can be expressed as the conjunction of the following properties:\nOnce a Trémaux tree has been identified in this way, one can describe an orientation of the given graph, also in monadic second-order logic, by specifying the set of edges whose orientation is from the ancestral endpoint to the descendant endpoint. The remaining edges outside this set must be oriented in the other direction. This technique allows graph properties involving orientations to be specified in monadic second order logic, allowing these properties to be tested efficiently on graphs of bounded treewidth using Courcelle's theorem.\n\nIf a graph has a Hamiltonian path, then that path (rooted at one of its endpoints) is also a Trémaux tree. The undirected graphs for which every Trémaux tree has this form are the cycle graphs, complete graphs, and balanced complete bipartite graphs.\n\nTrémaux trees are closely related to the concept of tree-depth. The tree-depth of a graph \"G\" can be defined as the smallest number \"d\" such that \"G\" can be embedded as a subgraph of a graph \"H\" that has a Trémaux tree \"T\" of depth \"d\". Bounded tree-depth, in a family of graphs, is equivalent to the existence of a path that cannot occur as a graph minor of the graphs in the family. Many hard computational problems on graphs have algorithms that are fixed-parameter tractable when parameterized by the tree-depth of their inputs.\n\nTrémaux trees also play a key role in the Fraysseix–Rosenstiehl planarity criterion for testing whether a given graph is planar. According to this criterion, a graph \"G\" is planar if, for a given Trémaux tree \"T\" of \"G\", the remaining edges can be placed in a consistent way to the left or the right of the tree, subject to constraints that prevent edges with the same placement from crossing each other.\n\nNot every infinite graph has a normal spanning tree. For instance, a complete graph on an uncountable set of vertices does not have one: a normal spanning tree in a complete graph can only be a path, but a path has only a countable number of vertices. However, every graph on a countable set of vertices does have a normal spanning tree.\n\nEven in countable graphs, a depth-first search might not succeed in eventually exploring the entire graph, and not every normal spanning tree can be generated by a depth-first search: to be a depth-first search tree, a countable normal spanning tree must have only one infinite path or one node with infinitely many children (and not both).\n\nIf an infinite graph \"G\" has a normal spanning tree, so does every connected graph minor of \"G\". It follows from this that the graphs that have normal spanning trees have a characterization by forbidden minors. One of the two classes of forbidden minors consists of bipartite graphs in which one side of the bipartition is countable, the other side is uncountable, and every vertex has infinite degree. The other class of forbidden minors consists of certain graphs derived from Aronszajn trees.\n\nThe details of this characterization depend on the choice of set-theoretic axiomatization used to formalize mathematics. In particular, in models of set theory for which Martin's axiom is true and the continuum hypothesis is false, the class of bipartite graphs in this characterization can be replaced by a single forbidden minor. However, for models in which the continuum hypothesis is true, this class contains graphs which are incomparable with each other in the minor ordering.\n\nNormal spanning trees are also closely related to the ends of an infinite graph, equivalence classes of infinite paths that, intuitively, go to infinity in the same direction. If a graph has a normal spanning tree, this tree must have exactly one infinite path for each of the graph's ends.\n\nAn infinite graph can be used to form a topological space by viewing the graph itself as a simplicial complex and adding a point at infinity for each end of the graph. With this topology, a graph has a normal spanning tree if and only if its set of vertices can be decomposed into a countable union of closed sets. Additionally, this topological space can be represented by a metric space if and only if the graph has a normal spanning tree.\n"}
