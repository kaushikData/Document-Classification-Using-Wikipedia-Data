{"id": "208288", "url": "https://en.wikipedia.org/wiki?curid=208288", "title": "17 (number)", "text": "17 (number)\n\n17 (seventeen) is the natural number following 16 and preceding 18. It is a prime number.\n\nIn English speech, the numbers 17 and 70 are sometimes confused, as they sound very similar.\n\nThe number 17 has wide significance in pure mathematics, as well as in applied sciences, law, music, religion, sports, and other cultural phenomena. Seventeen is the sum of the first four prime numbers.\n\nSeventeen is the seventh prime number. The next prime is nineteen, with which it forms a twin prime.\n\nSeventeen is a permutable prime and a supersingular prime.\n\nSeventeen is the third Fermat prime, as it is of the form 2 + 1, specifically with \"n\" = 2. Since 17 is a Fermat prime, regular heptadecagons can be constructed with a compass and unmarked ruler. This was proven by Carl Friedrich Gauss.\n\nThere are exactly 17 two-dimensional space (plane symmetry) groups. These are sometimes called wallpaper groups, as they represent the seventeen possible symmetry types that can be used for wallpaper.\n\nLike 41, the number 17 is a prime that yields primes in the polynomial \"n\" + \"n\" + \"p\", for all positive \"n\" < \"p\" − 1.\n\nEither 16 or 18 unit squares can be formed into rectangles with perimeter equal to the area; and there are no other natural numbers with this property. The Platonists regarded this as a sign of their peculiar propriety; and Plutarch notes it when writing that the Pythagoreans \"utterly abominate\" 17, which \"bars them off from each other and disjoins them\".\n\nSeventeen is the minimum possible number of givens for a sudoku puzzle with a unique solution. This was long conjectured, and was proved in 2012.\n\nThere are 17 orthogonal curvilinear coordinate systems (to within a conformal symmetry) in which the three-variable Laplace equation can be solved using the separation of variables technique.\n\nSeventeen is the sixth Mersenne prime exponent, yielding 131071.\n\nSeventeen is the first number that can be written as the sum of a positive cube and a positive square in two different ways; that is, the smallest \"n\" such that \"x\" + \"y\" = \"n\" has two different solutions for \"x\" and \"y\" positive integers. The next such number is 65.\n\nSeventeen is the minimum number of vertices on a graph such that, if the edges are coloured with three different colours, there is bound to be a monochromatic triangle. (See .)\n\nSeventeen is the only prime number which is the sum of four consecutive primes (2,3,5,7). Any other four consecutive primes summed would always produce an even number, thereby divisible by 2 and so not prime.\n\n\nIn Catalan, 17 is the first compound number (\"disset\"). The numbers 11 (\"onze\") through 16 (\"setze\") have their own names.\n\nIn French, 17 is the first compound number (\"dix-sept\"). The numbers 11 (\"onze\") through 16 (\"seize\") have their own names.\n\nIn Italian, 17 is also the first compound number (\"diciassette\"), whereas sixteen is \"sedici\".\n\n\n\n\n\n\n\n\n\n\n\n\nSeventeen is:\n\n\n\n"}
{"id": "403332", "url": "https://en.wikipedia.org/wiki?curid=403332", "title": "67 (number)", "text": "67 (number)\n\n67 (sixty-seven) is the natural number following 66 and preceding 68. It is an odd number.\n\n67 is:\n\n\n\n\n\nSixty-seven is:\n\n"}
{"id": "2324576", "url": "https://en.wikipedia.org/wiki?curid=2324576", "title": "A Symbolic Analysis of Relay and Switching Circuits", "text": "A Symbolic Analysis of Relay and Switching Circuits\n\nA Symbolic Analysis of Relay and Switching Circuits is the title of a master's thesis written by computer science pioneer Claude E. Shannon while attending the Massachusetts Institute of Technology (MIT) in 1937. In his thesis, Shannon, a dual degree graduate of the University of Michigan, proved that Boolean algebra could be used to simplify the arrangement of the relays that were the building blocks of the electromechanical automatic telephone exchanges of the day. Shannon went on to prove that it should also be possible to use arrangements of relays to solve Boolean algebra problems.\n\nThe utilization of the binary properties of electrical switches to perform logic functions is the basic concept that underlies all electronic digital computer designs. Shannon's thesis became the foundation of practical digital circuit design when it became widely known among the electrical engineering community during and after World War II. At the time, the methods employed to design logic circuits were \"ad hoc\" in nature and lacked the theoretical discipline that Shannon's paper supplied to later projects.\n\nPsychologist Howard Gardner described Shannon's thesis as \"possibly the most important, and also the most famous, master's thesis of the century\". A version of the paper was published in the 1938 issue of the \"Transactions of the American Institute of Electrical Engineers\", and in 1940, it earned Shannon the Alfred Noble American Institute of American Engineers Award.\n\n"}
{"id": "30350487", "url": "https://en.wikipedia.org/wiki?curid=30350487", "title": "Auslander algebra", "text": "Auslander algebra\n\nIn mathematics, the Auslander algebra of an algebra \"A\" is the endomorphism ring of the sum of the indecomposable modules of \"A\". It was introduced by .\n\nAn Artin algebra Γ is called an Auslander algebra if gl dim Γ ≤ 2 and if 0→Γ→\"I\"→\"J\"→\"K\"→0 is a minimal injective resolution of Γ then \"I\" and \"J\" are projective Γ-modules;\n"}
{"id": "32485057", "url": "https://en.wikipedia.org/wiki?curid=32485057", "title": "Bass conjecture", "text": "Bass conjecture\n\nIn mathematics, especially algebraic geometry, the Bass conjecture says that certain algebraic \"K\"-groups are supposed to be finitely generated. The conjecture was proposed by Hyman Bass.\n\nAny of the following equivalent statements is referred to as the Bass conjecture.\n\nThe equivalence of these statements follows from the agreement of \"K\"- and \"K<nowiki>'</nowiki>\"-theory for regular rings and the localization sequence for \"K<nowiki>'</nowiki>\"-theory.\n\nDaniel Quillen showed that the Bass conjecture holds for all (regular, depending on the version of the conjecture) rings or schemes of dimension ≤ 1, i.e., algebraic curves over finite fields and the spectrum of the ring of integers in a number field.\n\nThe (non-regular) ring \"A\" = Z[x, y]/x has an infinitely generated \"K\"(\"A\").\n\nThe Bass conjecture is known to imply the Beilinson–Soulé vanishing conjecture.\n\n"}
{"id": "21413563", "url": "https://en.wikipedia.org/wiki?curid=21413563", "title": "Christian Heinrich von Nagel", "text": "Christian Heinrich von Nagel\n\nChristian Heinrich von Nagel (28 February 1803 in Stuttgart, Germany – 27 October 1882 in Ulm, Germany) was a German geometer.\n\nAfter attending the gymnasium, Nagel went in 1817 to Evangelical Seminaries of Maulbronn and Blaubeuren. From 1821 to 1825, he took a four-year course of theology at the Tübinger Stift. Soon after his graduation, he became interested in mathematics. He became mathematics and science teacher at the Lyceum and at the Secondary school in Tübingen. Already in 1826, he earned doctorate at the local Faculty of Philosophy on a theme \"De triangulis rectangulis ex algebraica aequatione construendis\" (About right triangles construable from an algebraic equation). Until 1830, he held post of a private lecturer in Tübingen. In that year, he moved to Ulm where he had a better-paid job as a teacher at the Gymnasium in Ulm. Later he was rector of the affiliated Realschule. He was ennobled in 1875.\n\nHis best known results are from triangle geometry. One of the notable triangle points, Nagel point, is named after him.\n\n\n"}
{"id": "3313777", "url": "https://en.wikipedia.org/wiki?curid=3313777", "title": "Continuation map", "text": "Continuation map\n\nIn differential topology, given a family of Morse-Smale functions on a smooth manifold \"X\" parameterized by a closed interval \"I\", one can construct a Morse-Smale vector field on \"X\" × \"I\" whose critical points occur only on the boundary. The Morse differential defines a chain map from the Morse complexes at the boundaries of the family, the continuation map. This can be shown to descend to an isomorphism on Morse homology, proving its invariance of Morse homology of a smooth manifold. \n\nContinuation maps were defined by Andreas Floer to prove the invariance of Floer homology in infinite dimensional analogues of the situation described above; in the case of finite-dimensional Morse theory, invariance may be proved by proving that Morse homology is isomorphic to singular homology, which is known to be invariant. However, Floer homology is not always isomorphic to a familiar invariant, so continuation maps yield an a priori proof of invariance. \n\nIn finite-dimensional Morse theory, different choices made in constructing the vector field on \"X\" × \"I\" yield distinct but chain homotopic maps and thus descend to the same isomorphism on homology. However, in certain infinite dimensional cases, this does not hold, and these techniques may be used to produce invariants of one-parameter families of objects (such as contact structures or Legendrian knots).\n\n"}
{"id": "879358", "url": "https://en.wikipedia.org/wiki?curid=879358", "title": "Coordinate vector", "text": "Coordinate vector\n\nIn linear algebra, a coordinate vector is a representation of a vector as an ordered list of numbers that describes the vector in terms of a particular ordered basis. Coordinates are always specified relative to an ordered basis. Bases and their associated coordinate representations let one realize vector spaces and linear transformations concretely as column vectors, row vectors, and matrices, hence are useful in calculations.\n\nThe idea of a coordinate vector can also be used for infinite-dimensional vector spaces, as addressed below.\n\nLet \"V\" be a vector space of dimension \"n\" over a field \"F\" and let \nbe an ordered basis for \"V\".\nThen for every formula_2 there is a unique linear combination of the basis vectors that equals \"v\":\nThe coordinate vector of \"v\" relative to \"B\" is the sequence of coordinates\nThis is also called the \"representation of v with respect of B\", or the \"B representation of v\". The α-s are called the \"coordinates of v\". The order of the basis becomes important here, since it determines the order in which the coefficients are listed in the coordinate vector.\n\nCoordinate vectors of finite-dimensional vector spaces can be represented by matrices as column or row vectors. In the above notation, one can write\nor\n\nWe can mechanize the above transformation by defining a function formula_7, called the \"standard representation of V with respect to B\", that takes every vector to its coordinate representation: formula_8. Then formula_7 is a linear transformation from \"V\" to \"F\". In fact, it is an isomorphism, and its inverse formula_10 is simply\n\nAlternatively, we could have defined formula_12 to be the above function from the beginning, realized that formula_12 is an isomorphism, and defined formula_7 to be its inverse.\n\nLet P3 be the space of all the algebraic polynomials in degree at most 3 (i.e. the highest exponent of \"x\" can be 3). This space is linear and spanned by the following polynomials:\n\nmatching\n\nthen the coordinate vector corresponding to the polynomial\n\nis\n\nAccording to that representation, the differentiation operator d/dx which we shall mark D will be represented by the following matrix:\n\nUsing that method it is easy to explore the properties of the operator: such as invertibility, hermitian or anti-hermitian or none, spectrum and eigenvalues and more.\n\nThe Pauli matrices which represent the spin operator when transforming the spin eigenstates into vector coordinates.\n\nLet \"B\" and \"C\" be two different bases of a vector space \"V\", and let us mark with formula_20 the matrix which has columns consisting of the \"C\" representation of basis vectors \"b, b, …, b\": \n\nThis matrix is referred to as the basis transformation matrix from \"B\" to \"C\". It can be regarded as an automorphism over \"V\". Any vector \"v\" represented in \"B\" can be transformed to a representation in \"C\" as follows:\n\nIf \"E\" is the standard basis, the notation can be simplified by omitting it, with the transformation from \"B\" to \"E\" being represented:\n\nwhere \n\nUnder the transformation of basis, notice that the superscript on the transformation matrix, \"M\", and the subscript on the coordinate vector, \"v\", are the same, and seemingly cancel, leaving the remaining subscript. While this may serve as a memory aid, it is important to note that no such cancellation, or similar mathematical operation, is taking place.\n\nThe matrix \"M\" is an invertible matrix and \"M\" is the basis transformation matrix from \"C\" to \"B\". In other words,\n\nSuppose \"V\" is an infinite-dimensional vector space over a field \"F\". If the dimension is κ, then there is some basis of κ elements for \"V\". After an order is chosen, the basis can be considered an ordered basis. The elements of \"V\" are finite linear combinations of elements in the basis, which give rise to unique coordinate representations exactly as described before. The only change is that the indexing set for the coordinates is not finite. Since a given vector \"v\" is a \"finite\" linear combination of basis elements, the only nonzero entries of the coordinate vector for \"v\" will be the nonzero coefficients of the linear combination representing \"v\". Thus the coordinate vector for \"v\" is zero except in finitely many entries.\n\nThe linear transformations between (possibly) infinite-dimensional vector spaces can be modeled, analogously to the finite-dimensional case, with infinite matrices. The special case of the transformations from \"V\" into \"V\" is described in the full linear ring article.\n\n"}
{"id": "292506", "url": "https://en.wikipedia.org/wiki?curid=292506", "title": "Cube root", "text": "Cube root\n\nIn mathematics, a cube root of a number \"x\" is a number \"y\" such that \"y\" = \"x\". All real numbers (except zero) have exactly one real cube root and a pair of complex conjugate cube roots, and all nonzero complex numbers have three distinct complex cube roots. For example, the real cube root of 8, denoted , is 2, because 2 = 8, while the other cube roots of 8 are −1 + \"i\" and −1 − \"i\". The three cube roots of −27\"i\" are \n\nThe cube root operation is not distributive with addition or subtraction.\n\nIn some contexts, particularly when the number whose cube root is to be taken is a real number, one of the cube roots (in this particular case the real one) is referred to as the \"principal cube root\", denoted with the radical sign . The cube root operation is associative with exponentiation and distributive with multiplication and division if considering only real numbers, but not always if considering complex numbers: for example, the cube of any cube root of 8 is 8, but the three cube roots of 8 are 8, −4 + 4\"i\", and −4 − 4\"i\".\n\nThe cube roots of a number \"x\" are the numbers \"y\" which satisfy the equation\n\nFor any real number \"x\", there is \"one\" real number \"y\" such that \"y\" = \"x\". The cube function is increasing, so does not give the same result for two different inputs, plus it covers all real numbers. In other words, it is a bijection, or one-to-one. Then we can define an inverse function that is also one-to-one. For real numbers, we can define a unique cube root of all real numbers. If this definition is used, the cube root of a negative number is a negative number.\nIf \"x\" and \"y\" are allowed to be complex, then there are three solutions (if \"x\" is non-zero) and so \"x\" has three cube roots. A real number has one real cube root and two further cube roots which form a complex conjugate pair. For instance, the cube roots of 1 are:\n\nThe last two of these roots lead to a relationship between all roots of any real or complex number. If a number is one cube root of a particular real or complex number, the other two cube roots can be found by multiplying that cube root by one or the other of the two complex cube roots of 1.\n\nFor complex numbers, the principal cube root is usually defined as the cube root that has the greatest real part, or, equivalently, the cube root whose argument has the least absolute value. It is related to the principal value of the natural logarithm by the formula\n\nIf we write \"x\" as\n\nwhere \"r\" is a non-negative real number and \"θ\" lies in the range\n\nthen the principal complex cube root is\n\nThis means that in polar coordinates, we are taking the cube root of the radius and dividing the polar angle by three in order to define a cube root. With this definition, the principal cube root of a negative number is a complex number, and for instance will not be −2, but rather .\n\nThis difficulty can also be solved by considering the cube root as a multivalued function: if we write the original complex number \"x\" in three equivalent forms, namely\n\nThe principal complex cube roots of these three forms are then respectively\n\nUnless , these three complex numbers are distinct, even though the three representations of \"x\" were equivalent. For example, may then be calculated to be −2, , or .\n\nThis is related with the concept of monodromy: if one follows by continuity the function \"cube root\" along a closed path around zero, after a turn the value of the cube root is multiplied (or divided) by formula_10\n\nCube roots arise in the problem of finding an angle whose measure is one third that of a given angle (angle trisection) and in the problem of finding the edge of a cube whose volume is twice that of a cube with a given edge (doubling the cube). In 1837 Pierre Wantzel proved that neither of these can be done with a compass-and-straightedge construction.\n\nNewton's method is an iterative method that can be used to calculate the cube root. For real floating-point numbers this method reduces to the following iterative algorithm to produce successively better approximations of the cube root of \"a\":\n\nThe method is simply averaging three factors chosen such that\nat each iteration.\n\nHalley's method improves upon this with an algorithm that converges more quickly with each step, albeit consuming more multiplication operations:\n\nWith either method a poor initial approximation of \"x\" can give very poor algorithm performance, and coming up with a good initial approximation is somewhat of a black art. Some implementations manipulate the exponent bits of the floating-point number; i.e. they arrive at an\ninitial approximation by dividing the exponent by 3.\n\nCubic equations, which are polynomial equations of the third degree (meaning the highest power of the unknown is 3) can always be solved for their three solutions in terms of cube roots and square roots (although simpler expressions only in terms of square roots exist for all three solutions, if at least one of them is a rational number). If two of the solutions are complex numbers, then all three solution expressions involve the real cube root of a real number, while if all three solutions are real numbers then they may be expressed in terms of the complex cube root of a complex number. \n\nQuartic equations can also be solved in terms of cube roots and square roots.\n\nThe calculation of cube roots can be to traced back to Babylonian mathematicians from as early as 1800 BCE. In the fourth century BCE Plato posed the problem of doubling the cube, which required a compass-and-straightedge construction of the edge of a cube with twice the volume of a given cube; this required the construction, now known to be impossible, of the length .\n\nA method for extracting cube roots appears in \"The Nine Chapters on the Mathematical Art\", a Chinese mathematical text compiled around the 2nd century BCE and commented on by Liu Hui in the 3rd century CE. The Greek mathematician Hero of Alexandria devised a method for calculating cube roots in the 1st century CE. His formula is again mentioned by Eutokios in a commentary on Archimedes. In 499 CE Aryabhata, a mathematician-astronomer from the classical age of Indian mathematics and Indian astronomy, gave a method for finding the cube root of numbers having many digits in the \"Aryabhatiya\" (section 2.5).\n\n\n"}
{"id": "21486302", "url": "https://en.wikipedia.org/wiki?curid=21486302", "title": "Cycle double cover", "text": "Cycle double cover\n\nIn graph-theoretic mathematics, a cycle double cover is a collection of cycles in an undirected graph that together include each edge of the graph exactly twice. For instance, for any polyhedral graph, the faces of a convex polyhedron that represents the graph provide a double cover of the graph: each edge belongs to exactly two faces.\n\nIt is an unsolved problem, posed by George Szekeres and Paul Seymour and known as the cycle double cover conjecture, whether every bridgeless graph has a cycle double cover. The conjecture can equivalently be formulated in terms of graph embeddings, and in that context is also known as the circular embedding conjecture.\n\nThe usual formulation of the cycle double cover conjecture asks whether every bridgeless undirected graph has a collection of cycles such that each edge of the graph is contained in exactly two of the cycles. The requirement that the graph be bridgeless is an obvious necessary condition for such a set of cycles to exist, because a bridge cannot belong to any cycle. A collection of cycles satisfying the condition of the cycle double cover conjecture is called a cycle double cover. Some graphs such as cycle graphs and bridgeless cactus graphs can only be covered by using the same cycle more than once, so this sort of duplication is allowed in a cycle double cover.\n\nA snark is a special case of a bridgeless graph, having the additional properties that every vertex has exactly three incident edges (that is, the graph is cubic) and that it is not possible to partition the edges of the graph into three perfect matchings (that is, the graph has no 3-edge coloring, and by Vizing's theorem has chromatic index 4). It turns out that snarks form the only difficult case of the cycle double cover conjecture: if the conjecture is true for snarks, it is true for any graph.\n\nOne possible attack on the cycle double cover problem would be to show that there cannot exist a minimum counterexample, by proving that any graph contains a \"reducible configuration\", a subgraph that can be replaced by a smaller subgraph in a way that would preserve the existence or nonexistence of a cycle double cover. For instance, if a cubic graph contains a triangle, a Δ-Y transform will replace the triangle by a single vertex; any cycle double cover of the smaller graph can be extended back to a cycle double cover of the original cubic graph. Therefore, a minimal counterexample to the cycle double cover conjecture must be a triangle-free graph, ruling out some snarks such as Tietze's graph which contain triangles. Through computer searches, it is known that every cycle of length 11 or less in a cubic graph forms a reducible configuration, and therefore that any minimal counterexample to the cycle double cover conjecture must have girth at least 12.\n\nUnfortunately, it is not possible to prove the cycle double cover conjecture using a finite set of reducible configurations. Every reducible configuration contains a cycle, so for every finite set \"S\" of reducible configurations there is a number γ such that all configurations in the set contain a cycle of length at most γ. However, there exist snarks with arbitrarily high girth, that is, with arbitrarily high bounds on the length of their shortest cycle. A snark \"G\" with girth greater than γ cannot contain any of the configurations in the set \"S\", so the reductions in \"S\" are not strong enough to rule out the possibility that \"G\" might be a minimal counterexample.\n\nIf a graph has a cycle double cover, the cycles of the cover can be used to form the 2-cells of a graph embedding onto a two-dimensional cell complex. In the case of a cubic graph, this complex always forms a manifold. The graph is said to be \"circularly embedded\" onto the manifold, in that every face of the embedding is a simple cycle in the graph. However, a cycle double cover of a graph with degree greater than three may not correspond to an embedding on a manifold: the cell complex formed by the cycles of the cover may have non-manifold topology at its vertices. The circular embedding conjecture or strong embedding conjecture states that every biconnected graph has a circular embedding onto a manifold. If so, the graph also has a cycle double cover, formed by the faces of the embedding.\n\nFor cubic graphs, biconnectivity and bridgelessness are equivalent. Therefore, the circular embedding conjecture is clearly at least as strong as the cycle double cover conjecture. However, it turns out to be no stronger. If the vertices of a graph \"G\" are expanded to form a cubic graph, which is then circularly embedded, and the expansions are undone by contracting the added edges, the result will be a circular embedding of \"G\" itself. Therefore, if the cycle double cover conjecture is true, every biconnected graph has a circular embedding. That is, the cycle double cover conjecture is equivalent to the circular embedding conjecture, even though a cycle double cover and a circular embedding are not always the same thing.\n\nIf a circular embedding exists, it might not be on a surface of minimal genus: Nguyen Huy Xuong described a biconnected toroidal graph none of whose circular embeddings lie on a torus.\n\nA stronger version of the circular embedding conjecture that has also been considered is the conjecture that every biconnected graph has a circular embedding on an orientable manifold. In terms of the cycle double cover conjecture, this is equivalent to the conjecture that there exists a cycle double cover, and an orientation for each of the cycles in the cover, such that for every edge \"e\" the two cycles that cover \"e\" are oriented in opposite directions through \"e\".\n\nAlternatively, strengthenings of the conjecture that involve colorings of the cycles in the cover have also been considered. The strongest of these is a conjecture that every bridgeless graph has a circular embedding on an orientable manifold in which the faces can be 5-colored. If true, this would imply a conjecture of W. T. Tutte that every bridgeless graph has a nowhere-zero 5-flow.\n\nA stronger type of embedding than a circular embedding is a \"polyhedral embedding\", an embedding of a graph on a surface in such a way that every face is a simple cycle and every two faces that intersect do so in either a single vertex or a single edge. (In the case of a cubic graph, this can be simplified to a requirement that every two faces that intersect do so in a single edge.) Thus, in view of the reduction of the cycle double cover conjecture to snarks, it is of interest to investigate polyhedral embeddings of snarks. Unable to find such embeddings, Branko Grünbaum conjectured that they do not exist, but disproved Grünbaum's conjecture by finding a snark with a polyhedral embedding.\n\nSee also Petersen coloring conjecture.\n\n\n"}
{"id": "53880775", "url": "https://en.wikipedia.org/wiki?curid=53880775", "title": "Dan Abramovich", "text": "Dan Abramovich\n\nDan Abramovich is an American mathematician currently at Brown University and an Elected Fellow of the American Mathematical Society.\n\nAbramovich graduated from Tel Aviv University in 1997. He holds a PhD degree from Harvard University in algebraic Geometry. He has been serving as a Professor at Brown University since 2003. Previously he served as an Associate Professor at Boston University from 1999 to 2003.\n"}
{"id": "1577298", "url": "https://en.wikipedia.org/wiki?curid=1577298", "title": "Decile", "text": "Decile\n\nIn descriptive statistics, a decile is any of the nine values that divide the sorted data into ten equal parts, so that each part represents 1/10 of the sample or population. A decile is one possible form of a quantile; others include the quartile and percentile.\n\n"}
{"id": "22431956", "url": "https://en.wikipedia.org/wiki?curid=22431956", "title": "Douglas N. Arnold", "text": "Douglas N. Arnold\n\nDouglas Norman \"Doug\" Arnold is a mathematician whose research focuses on the numerical analysis of partial differential equations with applications in mechanics and other fields in physics. , he is McKnight Presidential Professor of Mathematics at the University of Minnesota.\n\nArnold studied mathematics as an undergraduate at Brown University, gaining his B.A. in 1975. He continued his studies at the University of Chicago, where he received a Ph.D. in 1979. He then moved to work at the University of Maryland, College Park. In 1989, he moved to Penn State University where he occupied a chair until 2002. He became director of the Institute for Mathematics and its Applications (IMA) in Minnesota in 2001. At the end of his term as director of the IMA, he becomes the McKnight Presidential Professor of Mathematics at the University of Minnesota. He served as president of the Society for Industrial and Applied Mathematics in 2009 and 2010.\n\nArnold's research initially focused on the finite element method for the solution of problems in elasticity. This expanded to take other applications into account, like the collision of black holes. \nEspecially well known is Arnold's development of the finite element exterior calculus, a discrete version of exterior calculus that can be used to analyze the stability of finite element methods. This was the topic of the plenary lecture Arnold gave at the 2002 International Congress of Mathematicians.\n\nTogether with a colleague (Jonathan Rogness), Arnold produced a video explaining Möbius transformations which won an honorable mention in a contest sponsored by Science magazine and the National Science Foundation in 2007.\nThe video was watched over a million times at YouTube.\nOther honors include the award of a Guggenheim Fellowship in 2008 and election as a foreign member of the Norwegian Academy of Science and Letters in 2009. In 2012 he became a fellow of the American Mathematical Society. In 2009, he was named a fellow of the Society for Industrial and Applied Mathematics.\n\nArnold is married to Maria-Carme Calderer, a professor of mathematics at University of Minnesota.\n\n"}
{"id": "2541664", "url": "https://en.wikipedia.org/wiki?curid=2541664", "title": "Equidistribution theorem", "text": "Equidistribution theorem\n\nIn mathematics, the equidistribution theorem is the statement that the sequence\n\nis uniformly distributed on the circle formula_1, when \"a\" is an irrational number. It is a special case of the ergodic theorem where one takes the normalized angle measure formula_2.\n\nWhile this theorem was proved in 1909 and 1910 separately by Hermann Weyl, Wacław Sierpiński and Piers Bohl, variants of this theorem continue to be studied to this day.\n\nIn 1916, Weyl proved that the sequence \"a\", 2\"a\", 3\"a\", ... mod 1 is uniformly distributed on the unit interval. In 1935, Ivan Vinogradov proved that the sequence \"p\" \"a\" mod 1 is uniformly distributed, where \"p\" is the \"n\"th prime. Vinogradov's proof was a byproduct of the odd Goldbach conjecture, that every sufficiently large odd number is the sum of three primes.\n\nGeorge Birkhoff, in 1931, and Aleksandr Khinchin, in 1933, proved that the generalization \"x\" + \"na\", for almost all \"x\", is equidistributed on any Lebesgue measurable subset of the unit interval. The corresponding generalizations for the Weyl and Vinogradov results were proven by Jean Bourgain in 1988.\n\nSpecifically, Khinchin showed that the identity\n\nholds for almost all \"x\" and any Lebesgue integrable function ƒ. In modern formulations, it is asked under what conditions the identity\n\nmight hold, given some general sequence \"b\".\n\nOne noteworthy result is that the sequence 2\"a\" mod 1 is uniformly distributed for almost all, but not all, irrational \"a\". Similarly, for the sequence \"b\" = 2, for every irrational \"a\", and almost all \"x\", there exists a function ƒ for which the sum diverges. In this sense, this sequence is considered to be a universally bad averaging sequence, as opposed to \"b\" = \"k\", which is termed a universally good averaging sequence, because it does not have the latter shortcoming.\n\nA powerful general result is Weyl's criterion, which shows that equidistribution is equivalent to having a non-trivial estimate for the exponential sums formed with the sequence as exponents. For the case of multiples of \"a\", Weyl's criterion reduces the problem to summing finite geometric series.\n\n\n\n"}
{"id": "1732661", "url": "https://en.wikipedia.org/wiki?curid=1732661", "title": "Federigo Enriques", "text": "Federigo Enriques\n\nAbramo Giulio Umberto Federigo Enriques (5 January 1871 – 14 June 1946) was an Italian mathematician, now known principally as the first to give a classification of algebraic surfaces in birational geometry, and other contributions in algebraic geometry.\n\nEnriques was born in Livorno, and brought up in Pisa, in a Sephardi Jewish family of Portuguese descent. He became a student of Guido Castelnuovo, and became an important member of the Italian school of algebraic geometry. He also worked on differential geometry. He collaborated with Castelnuovo, Corrado Segre and Francesco Severi. He had positions at the University of Bologna, and then the University of Rome La Sapienza. He lost his position in 1938, when the Fascist government enacted the \"leggi razziali\" (racial laws), which in particular banned Jews from holding professorships in Universities.\n\nThe Enriques classification, of complex algebraic surfaces up to birational equivalence, was into five main classes, and was background to further work until Kunihiko Kodaira reconsidered the matter in the 1950s. The largest class, in some sense, was that of surfaces of general type: those for which the consideration of differential forms provides linear systems that are large enough to make all the geometry visible. The work of the Italian school had provided enough insight to recognise the other main birational classes. Rational surfaces and more generally ruled surfaces (these include quadrics and cubic surfaces in projective 3-space) have the simplest geometry. Quartic surfaces in 3-spaces are now classified (when non-singular) as cases of K3 surfaces; the classical approach was to look at the Kummer surfaces, which are singular at 16 points. Abelian surfaces give rise to Kummer surfaces as quotients. There remains the class of elliptic surfaces, which are fiber bundles over a curve with elliptic curves as fiber, having a finite number of modifications (so there is a bundle that is locally trivial actually over a curve less some points). The question of classification is to show that any surface, lying in projective space of any dimension, is in the birational sense (after blowing up and blowing down of some curves, that is) accounted for by the models already mentioned.\n\nNo more than other work in the Italian school would the proofs by Enriques now be counted as complete and rigorous. Not enough was known about some of the technical issues: the geometers worked by a mixture of inspired guesswork and close familiarity with examples. Oscar Zariski started to work in the 1930s on a more refined theory of birational mappings, incorporating commutative algebra methods. He also began work on the question of the classification for characteristic p, where new phenomena arise. The schools of Kunihiko Kodaira and Igor Shafarevich had put Enriques' work on a sound footing by about 1960.\n\n\nOn \"Scientia\". \n\n"}
{"id": "18443436", "url": "https://en.wikipedia.org/wiki?curid=18443436", "title": "Financial models with long-tailed distributions and volatility clustering", "text": "Financial models with long-tailed distributions and volatility clustering\n\nFinancial models with long-tailed distributions and volatility clustering have been introduced to overcome problems with the realism of classical financial models. These classical models of financial time series typically assume homoskedasticity and normality cannot explain stylized phenomena such as skewness, heavy tails, and volatility clustering of the empirical asset returns in finance. In 1963, Benoit Mandelbrot first used the stable (or formula_1-stable) distribution to model the empirical distributions which have the skewness and heavy-tail property. Since formula_1-stable distributions have infinite formula_3-th moments for all formula_4, the tempered stable processes have been proposed for overcoming this limitation of the stable distribution.\n\nOn the other hand, GARCH models have been developed to explain the volatility clustering. In the GARCH model, the innovation (or residual) distributions are assumed to be a standard normal distribution, despite the fact that this assumption is often rejected empirically. For this reason, GARCH models with non-normal innovation distribution have been developed.\n\nMany financial models with stable and tempered stable distributions together with volatility clustering have been developed and applied to risk management, option pricing, and portfolio selection.\n\nA random variable formula_5 is called \"infinitely divisible\" if,\nfor each formula_6, there are independent and identically-distributed random variables\n\nsuch that\n\nwhere formula_9 denotes equality in distribution.\n\nA Borel measure formula_10 on formula_11 is called a \"Lévy measure\" if formula_12 and\n\nIf formula_5 is infinitely divisible, then the characteristic function\nformula_15 is given by\n\nwhere formula_17 and formula_18.\n\nThis distribution was first introduced by under\nthe name of \"Truncated Lévy Flights\" and has been called the \"tempered stable\" or the \"KoBoL\" distribution. In particular, if\nformula_19, then this distribution is called the CGMY\ndistribution which has been used for\nfinancial modeling.\n\nThe characteristic function formula_20 for a tempered stable\ndistribution is given by\n\nfor some formula_22. Moreover, formula_20 can be extended to the\nregion formula_24.\n\nRosiński [6] generalized the CTS distribution under the name of the\n\"tempered stable distribution\". The KR distribution, which is a subclass of the Rosiński's generalized tempered stable distributions, is used in finance.\n\nAn infinitely divisible distribution is called a \"modified tempered stable (MTS) distribution\" with parameter formula_25,\nif its Lévy triplet formula_26 is given by\nformula_27, formula_28 and\nwhere formula_30 and \nHere formula_32 is the modified Bessel function of the second kind.\nThe MTS distribution is not included in the class of Rosiński's generalized tempered stable distributions.\n\nIn order to describe the volatility clustering effect of the return process of an asset, the GARCH model can be used. In the GARCH model, innovation (formula_33) is assumed that formula_34, where\nformula_35 and where\nthe series formula_36 are modeled by\n\nand where formula_38 and formula_39.\n\nHowever, the assumption of formula_35 is often rejected empirically. For that reason, new GARCH models with stable or tempered stable distributed innovation have been developed. GARCH models with formula_1-stable innovations have been introduced. Subsequently, GARCH Models with tempered stable innovations have been developed.\n\nObjections against the use of stable distributions in Financial models are given in \n\n"}
{"id": "13001206", "url": "https://en.wikipedia.org/wiki?curid=13001206", "title": "Flip (mathematics)", "text": "Flip (mathematics)\n\nIn algebraic geometry, flips and flops are codimension-2 surgery operations arising in the minimal model program, given by blowing up along a relative canonical ring. In dimension 3 flips are used to construct minimal models, and any two birationally equivalent minimal models are connected by a sequence of flops. It is conjectured that the same is true in higher dimensions.\n\nThe minimal model program can be summarised very briefly as follows: given a variety formula_1, we construct a sequence of contractions formula_2, each of which contracts some curves on which the canonical divisor formula_3 is negative. Eventually, formula_4 should become nef (at least in the case of nonnegative Kodaira dimension), which is the desired result. The major technical problem is that, at some stage, the variety formula_5 may become 'too singular', in the sense that the canonical divisor formula_3 is no longer Cartier, so the intersection number formula_7 with a curve formula_8 is not even defined.\nThe (conjectural) solution to this problem is the \"flip\". Given a problematic formula_5 as above, the flip of formula_5 is a birational map (in fact an isomorphism in codimension 1) formula_11 to a variety whose singularities are 'better' than those of formula_5. So we can put formula_13, and continue the process.\n\nTwo major problems concerning flips are to show that they exist and to show that one cannot have an infinite sequence of flips. If both of these problems can be solved then the minimal model program can be carried out. \nThe existence of flips for 3-folds was proved by . The existence of log flips, a more general kind of flip, in dimension three and four were proved by \nwhose work was fundamental to the solution of the existence of log flips and other problems in higher dimension. \nThe existence of log flips in higher dimensions has been settled by . On the other hand, the problem of termination—proving that there can be no infinite sequence of flips—is still open in dimensions greater than 3.\n\nIf \"f\":\"X\"→\"Y\" is a morphism, and \"K\" is the canonical bundle of \"X\", then the relative canonical ring of \"f\" is\nand is a sheaf of graded algebras over the sheaf \"O\" of regular functions on \"Y\".\nThe blowup \"f\"\nof \"Y\" along the relative canonical ring is a morphism to \"Y\". If the relative canonical ring is finitely generated (as an algebra over \"O\") then the morphism \"f\" is called the flip of \"f\" if −\"K\" is relatively ample, and the flop of \"f\" if \"K\" is relatively trivial. (Sometimes the induced birational morphism from \"X\" to \"X\" is called a flip or flop.)\n\nIn applications, \"f\" is often a small contraction of an extremal ray, which implies several extra properties:\n\nThe first example of a flop, known as the Atiyah flop, was found in .\nLet \"Y\" be the zeros of \"xy\" = \"zw\" in A, and let \"V\" be the blowup of \"Y\" at the origin. \nThe exceptional locus of this blowup is isomorphic to P×P, and can be blown down to P in 2 different ways, giving varieties \"X\" and \"X\". The natural birational map from \"X\" to \"X\" is the Atiyah flop.\n\n\"xy\" = (\"z\"+\"w\")(\"z\"−\"w\").\n\n"}
{"id": "7428961", "url": "https://en.wikipedia.org/wiki?curid=7428961", "title": "Hadamard code", "text": "Hadamard code\n\nThe Hadamard code is an error-correcting code named after Jacques Hadamard that is used for error detection and correction when transmitting messages over very noisy or unreliable channels. In 1971, the code was used to transmit photos of Mars back to Earth from the NASA space probe Mariner 9. Because of its unique mathematical properties, the Hadamard code is not only used by engineers, but also intensely studied in coding theory, mathematics, and theoretical computer science.\nThe Hadamard code is named after the French mathematician Jacques Hadamard.\nIt is also known under the names Walsh code, Walsh family, and Walsh–Hadamard code in recognition of the American mathematician Joseph Leonard Walsh.\n\nThe Hadamard code is an example of a linear code over a binary alphabet that maps messages of length formula_1 to codewords of length formula_2.\nIt is unique in that each non-zero codeword has a Hamming weight of exactly formula_3, which implies that the distance of the code is also formula_3.\nIn standard coding theory notation for block codes, the Hadamard code is a formula_5-code, that is, it is a linear code over a binary alphabet, has block length formula_2, message length (or dimension) formula_1, and minimum distance formula_8.\nThe block length is very large compared to the message length, but on the other hand, errors can be corrected even in extremely noisy conditions.\nThe punctured Hadamard code is a slightly improved version of the Hadamard code; it is a formula_9-code and thus has a slightly better rate while maintaining the relative distance of formula_10, and is thus preferred in practical applications.\nThe punctured Hadamard code is the same as the first order Reed–Muller code over the binary alphabet.\n\nNormally, Hadamard codes are based on Sylvester's construction of Hadamard matrices, but the term “Hadamard code” is also used to refer to codes constructed from arbitrary Hadamard matrices, which are not necessarily of Sylvester type.\nIn general, such a code is not linear.\nSuch codes were first constructed by R. C. Bose and S. S. Shrikhande in 1959.\nIf \"n\" is the size of the Hadamard matrix, the code has parameters formula_11, meaning it is a not-necessarily-linear binary code with 2\"n\" codewords of block length \"n\" and minimal distance \"n\"/2. The construction and decoding scheme described below apply for general \"n\", but the property of linearity and the identification with Reed–Muller codes require that \"n\" be a power of 2 and that the Hadamard matrix be equivalent to the matrix constructed by Sylvester's method.\n\nThe Hadamard code is a locally decodable code, which provides a way to recover parts of the original message with high probability, while only looking at a small fraction of the received word. This gives rise to applications in computational complexity theory and particularly in the design of probabilistically checkable proofs.\nSince the relative distance of the Hadamard code is 1/2, normally one can only hope to recover from at most a 1/4 fraction of error. Using list decoding, however, it is possible to compute a short list of possible candidate messages as long as fewer than formula_12 of the bits in the received word have been corrupted.\n\nIn code division multiple access (CDMA) communication, the Hadamard code is referred to as Walsh Code, and is used to define individual communication channels. It is usual in the CDMA literature to refer to codewords as “codes”. Each user will use a different codeword, or “code”, to modulate their signal. Because Walsh codewords are mathematically orthogonal, a Walsh-encoded signal appears as random noise to a CDMA capable mobile terminal, unless that terminal uses the same codeword as the one used to encode the incoming signal.\n\n\"Hadamard code\" is the name that is most commonly used for this code in the literature. However, in modern use these error correcting codes are referred to as Walsh–Hadamard codes.\n\nThere is a reason for this:\n\nJacques Hadamard did not invent the code himself, but he defined Hadamard matrices around 1893, long before the first error-correcting code, the Hamming code, was developed in the 1940s.\n\nThe Hadamard code is based on Hadamard matrices, and while there are many different Hadamard matrices that could be used here, normally only Sylvester's construction of Hadamard matrices is used to obtain the codewords of the Hadamard code.\n\nJames Joseph Sylvester developed his construction of Hadamard matrices in 1867, which actually predates Hadamard's work on Hadamard matrices. Hence the name \"Hadamard code\" is disputed and sometimes the code is called \"Walsh code\", honoring the American mathematician Joseph Leonard Walsh.\n\nA Hadamard code was used during the 1971 Mariner 9 mission to correct for picture transmission errors. The data words used during this mission were 6 bits long, which represented 64 grayscale values.\n\nBecause of limitations of the quality of the alignment of the transmitter at the time (due to Doppler Tracking Loop issues) the maximum useful data length was about 30 bits. Instead of using a repetition code, a [32, 6, 16] Hadamard code was used.\n\nErrors of up to 7 bits per word could be corrected using this scheme. Compared to a 5-repetition code, the error correcting properties of this Hadamard code are much better, yet its rate is comparable. The efficient decoding algorithm was an important factor in the decision to use this code.\n\nThe circuitry used was called the \"Green Machine\". It employed the fast Fourier transform which can increase the decoding speed by a factor of three. Since the 1990s use of this code by space programs has more or less ceased, and the Deep Space Network does not support this error correction scheme for its dishes that are greater than 26 m.\n\nWhile all Hadamard codes are based on Hadamard matrices, the constructions differ in subtle ways for different scientific fields, authors, and uses. Engineers, who use the codes for data transmission, and coding theorists, who analyse extremal properties of codes, typically want the rate of the code to be as high as possible, even if this means that the construction becomes mathematically slightly less elegant.\n\nOn the other hand, for many applications of Hadamard codes in theoretical computer science it is not so important to achieve the optimal rate, and hence simpler constructions of Hadamard codes are preferred since they can be analyzed more elegantly.\n\nWhen given a binary message formula_13 of length formula_1, the Hadamard code encodes the message into a codeword formula_15 using an encoding function formula_16\nThis function makes use of the inner product formula_17 of two vectors formula_18, which is defined as follows:\nThen the Hadamard encoding of formula_20 is defined as the sequence of \"all\" inner products with formula_20:\n\nAs mentioned above, the \"punctured\" Hadamard code is used in practice since the Hadamard code itself is somewhat wasteful.\nThis is because, if the first bit of formula_23 is zero, formula_24, then the inner product contains no information whatsoever about formula_25, and hence, it is impossible to fully decode formula_20 from those positions of the codeword alone.\nOn the other hand, when the codeword is restricted to the positions where formula_27, it is still possible to fully decode formula_20.\nHence it makes sense to restrict the Hadamard code to these positions, which gives rise to the \"punctured\" Hadamard encoding of formula_20; that is, formula_30.\n\nThe Hadamard code is a linear code, and all linear codes can be generated by a generator matrix formula_31. This is a matrix such that formula_32 holds for all formula_13, where the message formula_20 is viewed as a row vector and the vector-matrix product is understood in the vector space over the finite field formula_35. In particular, an equivalent way to write the inner product definition for the Hadamard code arises by using the generator matrix whose columns consist of \"all\" strings formula_23 of length formula_1, that is,\n\nwhere formula_39 is the formula_40-th binary vector in lexicographical order.\nFor example, the generator matrix for the Hadamard code of dimension formula_41 is:\n\nThe matrix formula_31 is a formula_44-matrix and gives rise to the linear operator formula_45.\n\nThe generator matrix of the \"punctured\" Hadamard code is obtained by restricting the matrix formula_31 to the columns whose first entry is one.\nFor example, the generator matrix for the punctured Hadamard code of dimension formula_41 is:\n\nThen formula_49 is a linear mapping with formula_50.\n\nFor general formula_1, the generator matrix of the punctured Hadamard code is a parity-check matrix for the extended Hamming code of length formula_3 and dimension formula_53, which makes the punctured Hadamard code the dual code of the extended Hamming code.\nHence an alternative way to define the Hadamard code is in terms of its parity-check matrix: the parity-check matrix of the Hadamard code is equal to the generator matrix of the Hamming code.\n\nGeneralized Hadamard codes are obtained from an \"n\"-by-\"n\" Hadamard matrix \"H\". In particular, the 2\"n\" codewords of the code are the rows of \"H\" and the rows of −\"H\". To obtain a code over the alphabet {0,1}, the mapping −1 ↦ 1, 1 ↦ 0, or, equivalently, \"x\" ↦ (1 − \"x\")/2, is applied to the matrix elements. That the minimum distance of the code is \"n\"/2 follows from the defining property of Hadamard matrices, namely that their rows are mutually orthogonal. This implies that two distinct rows of a Hadamard matrix differ in exactly \"n\"/2 positions, and, since negation of a row does not affect orthogonality, that any row of \"H\" differs from any row of −\"H\" in \"n\"/2 positions as well, except when the rows correspond, in which case they differ in \"n\" positions.\n\nTo get the punctured Hadamard code above with formula_54, the chosen Hadamard matrix \"H\" has to be of Sylvester type, which gives rise to a message length of formula_55.\n\nThe distance of a code is the minimum Hamming distance between any two distinct codewords, i.e., the minimum number of positions at which two distinct codewords differ. Since the Walsh–Hadamard code is a linear code, the distance is equal to the minimum Hamming weight among all of its non-zero codewords. All non-zero codewords of the Walsh–Hadamard code have a Hamming weight of exactly formula_3 by the following argument.\n\nLet formula_13 be a non-zero message. Then the following value is exactly equal to the fraction of positions in the codeword that are equal to one:\n\nThe fact that the latter value is exactly formula_10 is called the \"random subsum principle\". To see that it is true, assume without loss of generality that formula_60.\nThen, when conditioned on the values of formula_61, the event is equivalent to formula_62 for some formula_63 depending on formula_64 and formula_61. The probability that formula_66 happens is exactly formula_10. Thus, in fact, \"all\" non-zero codewords of the Hadamard code have relative Hamming weight formula_10, and thus, its relative distance is formula_10.\n\nThe relative distance of the \"punctured\" Hadamard code is formula_10 as well, but it no longer has the property that every non-zero codeword has weight exactly formula_10 since the all formula_72s vector formula_73 is a codeword of the punctured Hadamard code. This is because the vector formula_74 encodes to formula_75. Furthermore, whenever formula_20 is non-zero and not the vector formula_77, the random subsum principle applies again, and the relative weight of formula_15 is exactly formula_10.\n\nA locally decodable code is a code that allows a single bit of the original message to be recovered with high probability by only looking at a small portion of the received word.\n\nA code is formula_80-query locally decodable if a message bit, formula_81, can be recovered by checking formula_80 bits of the received word. More formally, a code, formula_83, is formula_84-locally decodable, if there exists a probabilistic decoder, formula_85, such that \"(Note: formula_86 represents the Hamming distance between vectors formula_20 and formula_23)\":\n\nformula_89, formula_90 implies that formula_91\n\nTheorem 1: The Walsh–Hadamard code is formula_92-locally decodable for formula_93.\n\nLemma 1: For all codewords, formula_94 in a Walsh–Hadamard code, formula_95, formula_96, where formula_97 represent the bits in formula_94 in positions formula_40 and formula_100 respectively, and formula_101 represents the bit at position formula_102.\n\nLet formula_103 be the codeword in formula_95 corresponding to message formula_20.\n\nLet formula_106 be the generator matrix of formula_95.\n\nBy definition, formula_108. From this, formula_109. By the construction of formula_31, formula_111. Therefore, by substitution, formula_112.\n\nTo prove theorem 1 we will construct a decoding algorithm and prove its correctness.\n\nInput: Received word formula_113\n\nFor each formula_114:\n\nOutput: Message formula_122\n\nFor any message, formula_20, and received word formula_23 such that formula_23 differs from formula_126 on at most formula_127 fraction of bits, formula_81 can be decoded with probability at least formula_129.\n\nBy lemma 1, formula_130. Since formula_100 and formula_1 are picked uniformly, the probability that formula_133 is at most formula_127. Similarly, the probability that formula_135 is at most formula_127. By the union bound, the probability that either formula_137 or formula_138 do not match the corresponding bits in formula_94 is at most formula_140. If both formula_137 and formula_138 correspond to formula_94, then lemma 1 will apply, and therefore, the proper value of formula_81 will be computed. Therefore, the probability formula_81 is decoded properly is at least formula_146. Therefore, formula_147 and for formula_148 to be positive, formula_149.\n\nTherefore, the Walsh–Hadamard code is formula_92 locally decodable for formula_93\n\nFor \"k\" ≤ 7 the linear Hadamard codes have been proven optimal in the sense of minimum distance.\n\n\n"}
{"id": "13527566", "url": "https://en.wikipedia.org/wiki?curid=13527566", "title": "Heine's identity", "text": "Heine's identity\n\nIn mathematical analysis, Heine's identity, named after Heinrich Eduard Heine is a Fourier expansion of a reciprocal square root which Heine presented as\n\nwhere formula_2 is a Legendre function of the second kind, which has degree, \"m\" − /, a half-integer, and argument, \"z\", real and greater than one. This expression can be generalized for arbitrary half-integer powers as follows\n\nwhere formula_4 is the Gamma function.\n"}
{"id": "9871405", "url": "https://en.wikipedia.org/wiki?curid=9871405", "title": "Image rectification", "text": "Image rectification\n\nImage rectification is a transformation process used to project images onto a common image plane. This process has several degrees of freedom and there are many strategies for transforming images to the common plane.\nComputer stereo vision takes two or more images with known relative camera positions that show an object from different viewpoints. For each pixel it then determines the corresponding scene point's depth (i.e. distance from the camera) by first finding matching pixels (i.e. pixels showing the same scene point) in the other image(s) and then applying triangulation to the found matches to determine their depth.\nFinding matches in stereo vision is restricted by epipolar geometry: Each pixel's match in another image can only be found on a line called the epipolar line.\nIf two images are coplanar, i.e. they were taken such that the right camera is only offset horizontally compared to the left camera (not being moved towards the object or rotated), then each pixel's epipolar line is horizontal and at the same vertical position as that pixel. However, in general settings (the camera did move towards the object or rotate) the epipolar lines are slanted. Image rectification warps both images such that they appear as if they have been taken with only a horizontal displacement and as a consequence all epipolar lines are horizontal, which slightly simplifies the stereo matching process. Note however, that rectification does not fundamentally change the stereo matching process: It searches on lines, slanted ones before and horizontal ones after rectification.\n\nImage rectification is also an equivalent (and more often used) alternative to perfect camera coplanarity. Even with high-precision equipment, image rectification is usually performed because it may be impractical to maintain perfect coplanarity between cameras.\n\nImage rectification can only be performed with two images at a time and simultaneous rectification of more than two images is generally impossible.\n\nIf the images to be rectified are taken from camera pairs without geometric distortion, this calculation can easily be made with a linear transformation. X & Y rotation puts the images on the same plane, scaling makes the image frames be the same size and Z rotation & skew adjustments make the image pixel rows directly line up. The rigid alignment of the cameras needs to be known (by calibration) and the calibration coefficients are used by the transform.\n\nIn performing the transform, if the cameras themselves are calibrated for internal parameters, an essential matrix provides the relationship between the cameras. The more general case (without camera calibration) is represented by the fundamental matrix. If the fundamental matrix is not known, it is necessary to find preliminary point correspondences between stereo images to facilitate its extraction.\n\nThere are three main categories for image rectification algorithms: planar rectification, cylindrical rectification and polar rectification.\n\nAll rectified images satisfy the following two properties:\n\nIn order to transform the original image pair into a rectified image pair, it is necessary to find a projective transformation \"H\". Constraints are placed on \"H\" to satisfy the two properties above. For example, constraining the epipolar lines to be parallel with the horizontal axis means that epipoles must be mapped to the infinite point \"[1,0,0]\" in homogeneous coordinates. Even with these constraints, \"H\" still has four degrees of freedom. It is also necessary to find a matching \"H' \" to rectify the second image of an image pair. Poor choices of \"H\" and \"H' \" can result in rectified images that are dramatically changed in scale or severely distorted.\n\nThere are many different strategies for choosing a projective transform \"H\" for each image from all possible solutions. One advanced method is minimizing the disparity or least-square difference of corresponding points on the horizontal axis of the rectified image pair. Another method is separating \"H\" into a specialized projective transform, similarity transform, and shearing transform to minimize image distortion. One simple method is to rotate both images to look perpendicular to the line joining their collective optical centers, twist the optical axes so the horizontal axis of each image points in the direction of the other image's optical center, and finally scale the smaller image to match for line-to-line correspondence. This process is demonstrated in the following example.\n\nOur model for this example is based on a pair of images that observe a 3D point \"P\", which corresponds to \"p\" and \"p' \" in the pixel coordinates of each image. \"O\" and \"O' \" represent the optical centers of each camera, with known camera matrices formula_1 and formula_2 (we assume the world origin is at the first camera). We will briefly outline and depict the results for a simple approach to find a \"H\" and \"H' \" projective transformation that rectify the image pair from the example scene.\n\nFirst, we compute the epipoles, \"e\" and \"e' \" in each image:\n\nSecond, we find a projective transformation \"H\" that rotates our first image to be perpendicular to the baseline connecting \"O\" and \"O' \" (row 2, column 1 of 2D image set). This rotation can be found by using the cross product between the original and the desired optical axes. Next, we find the projective transformation \"H\" that takes the rotated image and twists it so that the horizontal axis aligns with the baseline. If calculated correctly, this second transformation should map the \"e\" to infinity on the x axis (row 3, column 1 of 2D image set). Finally, define formula_5 as the projective transformation for rectifying the first image.\n\nThird, through an equivalent operation, we can find \"H' \" to rectify the second image (column 2 of 2D image set). Note that \"H'\" should rotate the second image's optical axis to be parallel with the transformed optical axis of the first image. One strategy is to pick a plane parallel to the line where the two original optical axes intersect to minimize distortion from the reprojection process. In this example, we simply define \"H' \" using the rotation matrix \"R\" and initial projective transformation \"H\" as formula_6.\n\nFinally, we scale both images to the same approximate resolution and align the now horizontal epipoles for easier horizontal scanning for correspondences (row 4 of 2D image set).\n\nNote that it is possible to perform this and similar algorithms without having the camera parameter matrices \"M\" and \"M' \". All that is required is a set of seven or more image to image correspondences to compute the fundamental matrices and epipoles.\n\nImage rectification in GIS converts images to a standard map coordinate system. This is done by matching ground control points (GCP) in the mapping system to points in the image. These GCPs calculate necessary image transforms.\n\nPrimary difficulties in the process occur \n\nThe maps that are used with rectified images are non-topographical. However, the images to be used may contain distortion from terrain. Image orthorectification additionally removes these effects.\n\nImage rectification is a standard feature available with GIS software packages.\n\nThis section provides external links to reference implementations of image rectification.\n\n"}
{"id": "26786801", "url": "https://en.wikipedia.org/wiki?curid=26786801", "title": "Imaginary (exhibition)", "text": "Imaginary (exhibition)\n\nImaginary is an interactive traveling exhibition by the Mathematischen Forschungsinstituts Oberwolfach created for the Year of Mathematics 2008 in Germany. Its intention is to display visualizations, interactive installations, virtual realities, 3D objects and their theoretical background in algebraic geometry and in singularity theory in an attractive and understandable manner.\n\nThe exhibition was held in 23 countries, and attracted over 1 million visitors.\n\nA complete list of the exhibitions, past, current and future, can be found on the official website.\n"}
{"id": "346030", "url": "https://en.wikipedia.org/wiki?curid=346030", "title": "Improper integral", "text": "Improper integral\n\nIn mathematical analysis, an improper integral is the limit of a definite integral as an endpoint of the interval(s) of integration approaches either a specified real number, formula_1, formula_2, or in some instances as both endpoints approach limits. Such an integral is often written symbolically just like a standard definite integral, in some cases with \"infinity\" as a limit of integration.\n\nSpecifically, an improper integral is a limit of the form:\nor\nin which one takes a limit in one or the other (or sometimes both) endpoints .\n\nBy abuse of notation, improper integrals are often written symbolically just like standard definite integrals, perhaps with \"infinity\" among the limits of integration. When the definite integral exists (in the sense of either the Riemann integral or the more advanced Lebesgue integral), this ambiguity is resolved as both the proper and improper integral will coincide in value.\n\nOften one is able to compute values for improper integrals, even when the function is not integrable in the conventional sense (as a Riemann integral, for instance) because of a singularity in the function or because one of the bounds of integration is infinite.\n\nThe original definition of the Riemann integral does not apply to a function such as formula_5 on the interval [1, ∞), because in this case the domain of integration is unbounded. However, the Riemann integral can often be extended by continuity, by defining the improper integral instead as a limit\n\nThe narrow definition of the Riemann integral also does not cover the function formula_7 on the interval [0, 1]. The problem here is that the integrand is unbounded in the domain of integration (the definition requires that both the domain of integration and the integrand be bounded). However, the improper integral does exist if understood as the limit\n\nSometimes integrals may have two singularities where they are improper. Consider, for example, the function integrated from 0 to (shown right). At the lower bound, as goes to 0 the function goes to , and the upper bound is itself , though the function goes to 0. Thus this is a doubly improper integral. Integrated, say, from 1 to 3, an ordinary Riemann sum suffices to produce a result of /6. To integrate from 1 to , a Riemann sum is not possible. However, any finite upper bound, say (with ), gives a well-defined result, . This has a finite limit as goes to infinity, namely /2. Similarly, the integral from 1/3 to 1 allows a Riemann sum as well, coincidentally again producing /6. Replacing 1/3 by an arbitrary positive value (with ) is equally safe, giving . This, too, has a finite limit as goes to zero, namely /2. Combining the limits of the two fragments, the result of this improper integral is\nThis process does not guarantee success; a limit might fail to exist, or might be infinite. For example, over the bounded interval from 0 to 1 the integral of 1/ does not converge; and over the unbounded interval from 1 to the integral of 1/ does not converge.\nIt might also happen that an integrand is unbounded near an interior point, in which case the integral must be split at that point. For the integral as a whole to converge, the limit integrals on both sides must exist and must be bounded. For example:\nBut the similar integral\ncannot be assigned a value in this way, as the integrals above and below zero do not independently converge. (However, see Cauchy principal value.)\n\nAn improper integral converges if the limit defining it exists. Thus for example one says that the improper integral\nexists and is equal to \"L\" if the integrals under the limit exist for all sufficiently large \"t\", and the value of the limit is equal to \"L\".\n\nIt is also possible for an improper integral to diverge to infinity. In that case, one may assign the value of ∞ (or -∞) to the integral. For instance \nHowever, other improper integrals may simply diverge in no particular direction, such as\nwhich does not exist, even as an extended real number. This is called divergence by oscillation.\n\nA limitation of the technique of improper integration is that the limit must be taken with respect to one endpoint at a time. Thus, for instance, an improper integral of the form\n\ncan be defined by taking two separate limits; to wit\n\nprovided the double limit is finite. It can also be defined as a pair of distinct improper integrals of the first kind:\n\nwhere \"c\" is any convenient point at which to start the integration. This definition also applies when one of these integrals is infinite, or both if they have the same sign.\n\nAn example of an improper integrals where both endpoints are infinite is the Gaussian integral formula_18. An example which evaluates to infinity is formula_19. But one cannot even define other integrals of this kind unambiguously, such as formula_20, since the double limit is infinite and the two-integral method\n\nyields formula_22. In this case, one can however define an improper integral in the sense of Cauchy principal value:\n\nThe questions one must address in determining an improper integral are:\n\n\nThe first question is an issue of mathematical analysis. The second one can be addressed by calculus techniques, but also in some cases by contour integration, Fourier transforms and other more advanced methods.\n\nThere is more than one theory of integration. From the point of view of calculus, the Riemann integral theory is usually assumed as the default theory. In using improper integrals, it can matter which integration theory is in play.\n\n\nIn some cases, the integral\n\ncan be defined as an integral (a Lebesgue integral, for instance) without reference to the limit\n\nbut cannot otherwise be conveniently computed. This often happens when the function \"f\" being integrated from \"a\" to \"c\" has a vertical asymptote at \"c\", or if \"c\" = ∞ (see Figures 1 and 2). In such cases, the improper Riemann integral allows one to calculate the Lebesgue integral of the function. Specifically, the following theorem holds :\n\n\nFor example, the integral\ncan be interpreted alternatively as the improper integral\nor it may be interpreted instead as a Lebesgue integral over the set (0, ∞). Since both of these kinds of integral agree, one is free to choose the first method to calculate the value of the integral, even if one ultimately wishes to regard it as a Lebesgue integral. Thus improper integrals are clearly useful tools for obtaining the actual values of integrals.\n\nIn other cases, however, a Lebesgue integral between finite endpoints may not even be defined, because the integrals of the positive and negative parts of \"f\" are both infinite, but the improper Riemann integral may still exist. Such cases are \"properly improper\" integrals, i.e. their values cannot be defined except as such limits. For example,\n\ncannot be interpreted as a Lebesgue integral, since\n\nBut formula_35 is nevertheless integrable between any two finite endpoints, and its integral between 0 and ∞ is usually understood as the limit of the integral:\n\nOne can speak of the \"singularities\" of an improper integral, meaning those points of the extended real number line at which limits are used.\n\nConsider the difference in values of two limits:\n\nThe former is the Cauchy principal value of the otherwise ill-defined expression\n\nSimilarly, we have\n\nbut\n\nThe former is the principal value of the otherwise ill-defined expression\n\nAll of the above limits are cases of the indeterminate form ∞ − ∞.\n\nThese pathologies do not affect \"Lebesgue-integrable\" functions, that is, functions the integrals of whose absolute values are finite.\n\nAn improper integral may diverge in the sense that the limit defining it may not exist. In this case, there are more sophisticated definitions of the limit which can produce a convergent value for the improper integral. These are called summability methods.\n\nOne summability method, popular in Fourier analysis, is that of Cesàro summation. The integral\n\nis Cesàro summable (C, α) if\n\nexists and is finite . The value of this limit, should it exist, is the (C, α) sum of the integral.\n\nAn integral is (C, 0) summable precisely when it exists as an improper integral. However, there are integrals which are (C, α) summable for α > 0 which fail to converge as improper integrals (in the sense of Riemann or Lebesgue). One example is the integral\n\nwhich fails to exist as an improper integral, but is (C,α) summable for every α > 0. This is an integral version of Grandi's series.\n\nThe improper integral can also be defined for functions of several variables. The definition is slightly different, depending on whether one requires integrating over an unbounded domain, such as formula_46, or is integrating a function with singularities, like formula_47.\n\nIf formula_48 is a non-negative function that is Riemann integrable over every compact cube of the form formula_49, for formula_50, then the improper integral of \"f\" over formula_51 is defined to be the limit\nprovided it exists.\n\nA function on an arbitrary domain \"A\" in formula_53 is extended to a function formula_54 on formula_51 by zero outside of \"A\":\nThe Riemann integral of a function over a bounded domain \"A\" is then defined as the integral of the extended function formula_54 over a cube formula_49 containing \"A\": \nMore generally, if \"A\" is unbounded, then the improper Riemann integral over an arbitrary domain in formula_53 is defined as the limit:\n\nIf \"f\" is a non-negative function which is unbounded in a domain \"A\", then the improper integral of \"f\" is defined by truncating \"f\" at some cutoff \"M\", integrating the resulting function, and then taking the limit as \"M\" tends to infinity. That is for formula_62, set formula_63. Then define\nprovided this limit exists.\n\nThese definitions apply for functions that are non-negative. A more general function \"f\" can be decomposed as a difference of its positive part formula_65 and negative part formula_66, so\nwith formula_68 and formula_69 both non-negative functions. The function \"f\" has an improper Riemann integral if each of formula_68 and formula_69 has one, in which case the value of that improper integral is defined by\nIn order to exist in this sense, the improper integral necessarily converges absolutely, since\n\n\n"}
{"id": "199014", "url": "https://en.wikipedia.org/wiki?curid=199014", "title": "Infix notation", "text": "Infix notation\n\nInfix notation is the notation commonly used in arithmetical and logical formulae and statements. It is characterized by the placement of operators between operands—\"infixed operators\"—such as the plus sign in 2 + 2.\n\nInfix notation is more difficult to parse by computers than prefix notation (e.g. + 2 2) or postfix notation (e.g. 2 2 +). However many programming languages use it due to its familiarity. It is more used in arithmetic, e.g. 5 × 6.\n\nIn infix notation, unlike in prefix or postfix notations, parentheses surrounding groups of operands and operators are necessary to indicate the intended order in which operations are to be performed. In the absence of parentheses, certain precedence rules determine the order of operations.\n\nInfix notation may also be distinguished from function notation, where the name of a function suggests a particular operation, and its arguments are the operands. An example of such a function notation would be S(1, 3) in which the function S denotes addition: S(1, 3) = 1 + 3 = 4.\n\n\n"}
{"id": "5642731", "url": "https://en.wikipedia.org/wiki?curid=5642731", "title": "Information theory and measure theory", "text": "Information theory and measure theory\n\nThis article discusses how information theory (a branch of mathematics studying the transmission, processing and storage of information) is related to measure theory (a branch of mathematics related to integration and probability).\n\nMany of the concepts in information theory have separate definitions and formulas for continuous and discrete cases. For example, entropy formula_1 is usually defined for discrete random variables, whereas for continuous random variables the related concept of differential entropy, written formula_2, is used (see Cover and Thomas, 2006, chapter 8). Both these concepts are mathematical expectations, but the expectation is defined with an integral for the continuous case, and a sum for the discrete case.\n\nThese separate definitions can be more closely related in terms of measure theory. For discrete random variables, probability mass functions can be considered density functions with respect to the counting measure. Thinking of both the integral and the sum as integration on a measure space allows for a unified treatment. \n\nConsider the formula for the differential entropy of a continuous random variable formula_3 with range formula_4 and probability density function formula_5:\nThis can usually be interpreted as the following Riemann–Stieltjes integral:\nwhere formula_8 is the Lebesgue measure.\n\nIf instead, formula_3 is discrete, with range formula_10 a finite set, formula_11 is a probability mass function on formula_10, and formula_13 is the counting measure on formula_10, we can write:\nThe integral expression, and the general concept, are identical in the continuous case; the only difference is the measure used. In both cases the probability density function formula_11 is the Radon–Nikodym derivative of the probability measure with respect to the measure against which the integral is taken.\n\nIf formula_17 is the probability measure induced by formula_3, then the integral can also be taken directly with respect to formula_17:\n\nIf instead of the underlying measure μ we take another probability measure formula_21, we are led to the Kullback–Leibler divergence: let formula_22 and formula_23 be probability measures over the same space. Then if formula_22 is absolutely continuous with respect to formula_23, written formula_26 the Radon–Nikodym derivative formula_27 exists and the Kullback–Leibler divergence can be expressed in its full generality:\n\nwhere the integral runs over the support of formula_29 Note that we have dropped the negative sign: the Kullback–Leibler divergence is always non-negative due to Gibbs' inequality.\n\nThere is an analogy between Shannon's basic \"measures\" of the information content of random variables and a measure over sets. Namely the joint entropy, conditional entropy, and mutual information can be considered as the measure of a set union, set difference, and set intersection, respectively (Reza pp. 106–108).\n\nIf we associate the existence of abstract sets formula_30 and formula_31 to arbitrary discrete random variables \"X\" and \"Y\", somehow representing the information borne by \"X\" and \"Y\", respectively, such that:\n\nwhere formula_8 is a signed measure over these sets, and we set:\n\nwe find that Shannon's \"measure\" of information content satisfies all the postulates and basic properties of a formal signed measure over sets, as commonly illustrated in an information diagram. This allows the sum of two measures to be written:\n\nand the analog of Bayes' theorem (formula_37) allows the difference of two measures to be written:\n\nThis can be a handy mnemonic device in some situations, e.g.\n\nNote that measures (expectation values of the logarithm) of true probabilities are called \"entropy\" and generally represented by the letter \"H\", while other measures are often referred to as \"information\" or \"correlation\" and generally represented by the letter \"I\". For notational simplicity, the letter \"I\" is sometimes used for all measures.\n\nCertain extensions to the definitions of Shannon's basic measures of information are necessary to deal with the σ-algebra generated by the sets that would be associated to three or more arbitrary random variables. (See Reza pp. 106–108 for an informal but rather complete discussion.) Namely formula_40 needs to be defined in the obvious way as the entropy of a joint distribution, and a multivariate mutual information formula_41 defined in a suitable manner so that we can set:\n\nin order to define the (signed) measure over the whole σ-algebra. There is no single universally accepted definition for the mutivariate mutual information, but the one that corresponds here to the measure of a set intersection is due to Fano (1966: p. 57-59). The definition is recursive. As a base case the mutual information of a single random variable is defined to be its entropy: formula_43. Then for formula_44 we set\nwhere the conditional mutual information is defined as\nThe first step in the recursion yields Shannon's definition formula_47 The multivariate mutual information (same as interaction information but for a change in sign) of three or more random variables can be negative as well as positive: Let \"X\" and \"Y\" be two independent fair coin flips, and let \"Z\" be their exclusive or. Then formula_48 bit.\n\nMany other variations are possible for three or more random variables: for example, formula_49 is the mutual information of the joint distribution of \"X\" and \"Y\" relative to \"Z\", and can be interpreted as formula_50 Many more complicated expressions can be built this way, and still have meaning, e.g. formula_51 or formula_52\n\n\n"}
{"id": "27908099", "url": "https://en.wikipedia.org/wiki?curid=27908099", "title": "Jakobson's functions of language", "text": "Jakobson's functions of language\n\nRoman Jakobson defined six functions of language (or communication functions), according to which an effective act of verbal communication can be described. Each of the functions has an associated factor. For this work, Jakobson was influenced by Karl Bühler's organon model, to which he added the poetic, phatic and metalingual functions.\n\n\n"}
{"id": "56081515", "url": "https://en.wikipedia.org/wiki?curid=56081515", "title": "John Baras", "text": "John Baras\n\nJohn S. Baras is a Greek American electrical engineer. He is a Professor and Lockheed Martin Chair in Systems Engineering, University of Maryland, College Park.\n\nHe was included in the 2019 class of fellows of the American Mathematical Society \"for contributions to the mathematical foundations and applications of systems theory, stochastic systems, stochastic control, network security and trust, mentoring and academic leadership\".\n"}
{"id": "29238744", "url": "https://en.wikipedia.org/wiki?curid=29238744", "title": "Jon Folkman", "text": "Jon Folkman\n\nJon Hal Folkman (December 8, 1938 – January 23, 1969) was an American mathematician, a student of John Milnor, and a researcher at the RAND Corporation.\n\nFolkman was a Putnam Fellow in 1960. He received his Ph.D. in 1964 from Princeton University, under the supervision of Milnor, with a thesis entitled \"Equivariant Maps of Spheres into the Classical Groups\".\n\nJon Folkman contributed important theorems in many areas of combinatorics.\n\nIn geometric combinatorics, Folkman is known for his pioneering and posthumously-published studies of oriented matroids; in particular, the Folkman–Lawrence topological representation theorem is \"one of the cornerstones of the theory of oriented matroids\". In lattice theory, Folkman solved an open problem on the foundations of combinatorics by proving a conjecture of Gian–Carlo Rota; in proving Rota's conjecture, Folkman characterized the structure of the homology groups of \"geometric lattices\" in terms of the free Abelian groups of finite rank. In graph theory, he was the first to study semi-symmetric graphs, and he discovered the semi-symmetric graph with the fewest possible vertices, now known as the Folkman graph. He proved the existence, for every positive \"h\", of a finite \"K\"-free graph which has a monocolored \"K\" in every 2-coloring of the edges, settling a problem previously posed by Paul Erdős and András Hajnal. He further proved that if \"G\" is a finite graph such that every set \"S\" of vertices contains an independent set of size (|\"S\"| − \"k\")/2 then the chromatic number of \"G\" is at most \"k\" + 2.\n\nIn convex geometry, Folkman worked with his RAND colleague Lloyd Shapley to prove the Shapley–Folkman lemma and theorem: Their results suggest that sums of sets are approximately convex; in mathematical economics their results are used to explain why economies with many agents have approximate equilibria, despite individual nonconvexities.\n\nIn additive combinatorics, Folkman's theorem states that for each assignment of finitely many colors to the positive integers, there exist arbitrarily large sets of integers all of whose nonempty sums have the same color; the name was chosen as a memorial to Folkman by his friends. In Ramsey theory, the Rado–Folkman–Sanders theorem describes \"partition regular\" sets.\n\nFor r > max{p, q}, let F(p, q; r) denote the minimum number of\nvertices in a graph G that has the following properties:\n\nSome results are\n\nIn the late 1960s, Folkman suffered from brain cancer; while hospitalized, Folkman was visited repeatedly by Ronald Graham and Paul Erdős. After his brain surgery, Folkman was despairing that he had lost his mathematical skills. As soon as Folkman received Graham and Erdős at the hospital, Erdős challenged Folkman with mathematical problems, helping to rebuild his confidence.\n\nFolkman later purchased a gun and killed himself. Folkman's supervisor at RAND, Delbert Ray Fulkerson, blamed himself for failing to notice suicidal behaviors in Folkman. Years later Fulkerson also killed himself.\n"}
{"id": "12220009", "url": "https://en.wikipedia.org/wiki?curid=12220009", "title": "Logic redundancy", "text": "Logic redundancy\n\nLogic redundancy occurs in a digital gate network containing circuitry that does not affect the static logic function. There are several reasons why logic redundancy may exist. One reason is that it may have been added deliberately to suppress transient glitches (thus causing a race condition) in the output signals by having two or more product terms overlap with a third one.\n\nConsider the following equation:\n\nThe third product term formula_2 is a redundant consensus term. If formula_3 switches from 1 to 0 while formula_4 and formula_5, formula_6 remains 1. During the transition of signal formula_3 in logic gates, both the first and second term may be 0 momentarily. The third term prevents a glitch since its value of 1 in this case is not affected by the transition of signal formula_3.\n\nAnother reason for logic redundancy is poor design practices which unintentionally result in logically redundantly terms. This causes an unnecessary increase in network complexity, and possibly hampering the ability to test manufactured designs using traditional test methods (single stuck-at fault models). (Note: testing might be possible using IDDQ models.)\n\nLogic redundancy is, in general, not desired.\nRedundancy, by definition, requires extra parts (in this case: logical terms) which raises the cost of implementation (either actual cost of physical parts or CPU time to process).\nLogic redundancy can be removed by several well-known techniques, such as Karnaugh maps, the Quine–McCluskey algorithm, and the heuristic computer method.\n\nIn some cases it may be desirable to \"add\" logic redundancy. One of those cases is to avoid race conditions whereby an output can fluctuate because different terms are \"racing\" to turn off and on. To explain this in more concrete terms the Karnaugh map to the right shows the minterms and maxterms for the following function:\n\nThe boxes represent the minimal AND/OR terms needed to implement this function:\n\nThe k-map visually shows where race conditions occur in the minimal expression by having gaps between minterms or gaps between maxterms. For example, the gap between the blue and green rectangles. If the input formula_11 were to change to formula_12 then a race will occur between formula_13 turning off and formula_14 turning off.\nIf the blue term switches off before the green turns on then the output will fluctuate and may register as 0.\nAnother race condition is between the blue and the red for transition of formula_11 to formula_16.\n\nThe race condition is removed by adding in logic redundancy, which is contrary to the aims of using a k-map in the first place.\nBoth minterm race conditions are covered by addition of the yellow term formula_17.\n\nIn this case, the addition of logic redundancy has stabilized the output to avoid output fluctuations because terms are racing each other to change state.\n\n 3\n"}
{"id": "41701177", "url": "https://en.wikipedia.org/wiki?curid=41701177", "title": "Maximum disjoint set", "text": "Maximum disjoint set\n\nIn computational geometry, a maximum disjoint set (MDS) is a largest set of non-overlapping geometric shapes selected from a given set of candidate shapes.\n\nFinding an MDS is important in applications such as automatic label placement, VLSI circuit design, and cellular frequency division multiplexing.\n\nEvery set of non-overlapping shapes is an independent set in the intersection graph of the shapes. Therefore, the MDS problem is a special case of the maximum independent set (MIS) problem. Both problems are NP complete, but finding a MDS may be easier than finding a MIS in two respects:\n\nThe MDS problem can be generalized by assigning a different weight to each shape and searching for a disjoint set with a maximum total weight.\n\nIn the following text, MDS(\"C\") denotes the maximum disjoint set in a set \"C\".\n\nGiven a set \"C\" of shapes, an approximation to MDS(\"C\") can be found by the following greedy algorithm:\n\n\nFor every shape \"x\" that we add to \"S\", we lose the shapes in \"N(x)\", because they are intersected by \"x\" and thus cannot be added to \"S\" later on. However, some of these shapes themselves intersect each other, and thus in any case it is not possible that they all be in the optimal solution \"MDS(S)\". The largest subset of shapes that \"can\" all be in the optimal solution is \"MDS(N(x))\". Therefore, selecting an \"x\" that minimizes \"|MDS(N(x))|\" minimizes the loss from adding \"x\" to \"S\".\n\nIn particular, if we can guarantee that there is an \"x\" for which \"|MDS(N(x))|\" is bounded by a constant (say, \"M\"), then this greedy algorithm yields a constant \"M\"-factor approximation, as we can guarantee that:\n\nformula_1\n\nSuch an upper bound \"M\" exists for several interesting cases:\n\nWhen \"C\" is a set of intervals on a line, \"M\"=1, and thus the greedy algorithm finds the exact MDS. To see this, assume w.l.o.g. that the intervals are vertical, and let \"x\" be the interval with the \"highest bottom endpoint\". All other intervals intersected by \"x\" must cross its bottom endpoint. Therefore, all intervals in \"N(x)\" intersect each other, and \"MDS(N(x))\" has a size of at most 1 (see figure).\n\nTherefore, in the 1-dimensional case, the MDS can be found exactly in time \"O\"(\"n\" log \"n\"): \n\nThis algorithm is analogous to the earliest deadline first scheduling solution to the interval scheduling problem.\n\nIn contrast to the 1-dimensional case, in 2 or more dimensions the MDS problem becomes NP-complete, and thus has either exact super-polynomial algorithms or approximate polynomial algorithms.\n\nWhen \"C\" is a set of unit disks, \"M\"=3, because the leftmost disk (the disk whose center has the smallest \"x\" coordinate) intersects at most 3 other disjoint disks (see figure). Therefore the greedy algorithm yields a 3-approximation, i.e., it finds a disjoint set with a size of at least \"MDS(C)\"/3.\n\nSimilarly, when \"C\" is a set of axis-parallel unit squares, \"M\"=2.\nWhen \"C\" is a set of arbitrary-size disks, \"M\"=5, because the disk with the smallest radius intersects at most 5 other disjoint disks (see figure).\n\nSimilarly, when \"C\" is a set of arbitrary-size axis-parallel squares, \"M\"=4.\n\nOther constants can be calculated for other regular polygons.\n\nThe most common approach to finding a MDS is divide-and-conquer. A typical algorithm in this approach looks like the following:\n\n\nThe main challenge with this approach is to find a geometric way to divide the set into subsets. This may require to discard a small number of shapes that do not fit into any one of the subsets, as explained in the following subsections.\n\nLet \"C\" be a set of \"n\" axis-parallel rectangles in the plane. The following algorithm finds a disjoint set with a size of at least formula_2 in time \nformula_3:\n\n\nIt is provable by induction that, at the last step, either formula_17 or formula_19 have a cardinality of at least formula_2.\n\nThe approximation factor has been reduced to formula_27 and generalized to the case in which rectangles have different weights.\n\nLet \"C\" be a set of \"n\" axis-parallel rectangles in the plane, all with the same height \"H\" but with varying lengths. The following algorithm finds a disjoint set with a size of at least |MDS(\"C\")|/2 in time \"O\"(\"n\" log \"n\"):\n\n\nLet \"C\" be a set of \"n\" axis-parallel rectangles in the plane, all with the same height but with varying lengths. There is an algorithm that finds a disjoint set with a size of at least |MDS(\"C\")|/(1 + 1/\"k\") in time \"O\"(\"n\"), for every constant \"k\" > 1.\n\nThe algorithm is an improvement of the above-mentioned 2-approximation, by combining dynamic programming with the shifting technique of.\n\nThis algorithm can be generalized to \"d\" dimensions. If the labels have the same size in all dimensions except one, it is possible to find a similar approximation by applying dynamic programming along one of the dimensions. This also reduces the time to n^O(1/e).\n\nLet \"C\" be a set of \"n\" squares or circles of identical size. There is a polynomial-time approximation scheme for finding an MDS using a simple shifted-grid strategy. It finds a solution within (1 − \"e\") of the maximum in time \"n\" time and linear space. The strategy generalizes to any collection of fat objects of roughly the same size (i.e., when the maximum-to-minimum size ratio is bounded by a constant).\n\nLet \"C\" be a set of \"n\" fat objects (e.g. squares or circles) of arbitrary sizes. There is a PTAS for finding an MDS based on multi-level grid alignment. It has been discovered by two groups in approximately the same time, and described in two different ways.\n\nVersion 1 finds a disjoint set with a size of at least (1 − 1/\"k\") · |MDS(\"C\")| in time \"n\", for every constant \"k\" > 1:\n\nScale the disks so that the smallest disk has diameter 1. Partition the disks to levels, based on the logarithm of their size. I.e., the \"j\"-th level contains all disks with diameter between (\"k\" + 1) and (\"k\" + 1), for \"j\" ≤ 0 (the smallest disk is in level 0).\n\nFor each level \"j\", impose a grid on the plane that consists of lines that are (\"k\" + 1) apart from each other. By construction, every disk can intersect at most one horizontal line and one vertical line from its level.\n\nFor every \"r\", \"s\" between 0 and \"k\", define \"D\"(\"r\",\"s\") as the subset of disks that are not intersected by any horizontal line whose index modulo \"k\" is \"r\", nor by any vertical line whose index modulu \"k\" is \"s\". By the pigeonhole principle, there is at least one pair \"(r,s)\" such that formula_36, i.e., we can find the MDS only in \"D\"(\"r\",\"s\") and miss only a small fraction of the disks in the optimal solution:\n\nVersion 2 finds a disjoint set with a size of at least (1 − 2/\"k\")·|MDS(\"C\")| in time \"n\", for every constant \"k\" > 1.\n\nThe algorithm uses shifted quadtrees. The key concept of the algorithm is \"alignment\" to the quadtree grid. An object of size \"r\" is called \"k-aligned\" (where \"k\" ≥ 1 is a constant) if it is inside a quadtree cell of size at most \"kr\" (\"R\" ≤ \"kr\").\n\nBy definition, a \"k\"-aligned object that intersects the boundary of a quatree cell of size \"R\" must have a size of at least \"R\"/\"k\" (\"r\" > \"R\"/\"k\"). The boundary of a cell of size \"R\" can be covered by 4\"k\" squares of size \"R\"/\"k\"; hence the number of disjoint fat objects intersecting the boundary of that cell is at most 4\"kc\", where \"c\" is a constant measuring the fatness of the objects.\n\nTherefore, if all objects are fat and \"k\"-aligned, it is possible to find the exact maximum disjoint set in time \"n\" using a divide-and-conquer algorithm. Start with a quadtree cell that contains all objects. Then recursively divide it to smaller quadtree cells, find the maximum in each smaller cell, and combine the results to get the maximum in the larger cell. Since the number of disjoint fat objects intersecting the boundary of every quadtree cell is bounded by 4\"kc\", we can simply \"guess\" which objects intersect the boundary in the optimal solution, and then apply divide-and-conquer to the objects inside.\n\nIf \"almost\" all objects are \"k\"-aligned, we can just discard the objects that are not \"k\"-aligned, and find a maximum disjoint set of the remaining objects in time \"n\". This results in a (1 − \"e\") approximation, where e is the fraction of objects that are not \"k\"-aligned.\n\nIf most objects are not \"k\"-aligned, we can try to make them \"k\"-aligned by \"shifting\" the grid in multiples of (1/\"k\",1/\"k\"). First, scale the objects such that they are all contained in the unit square. Then, consider \"k\" shifts of the grid: (0,0), (1/\"k\",1/\"k\"), (2/\"k\",2/\"k\"), ..., ((\"k\" − 1)/\"k\",(\"k\" − 1)/\"k\"). I.e., for each \"j\" in {0...,\"k\" − 1}, consider a shift of the grid in (j/k,j/k). It is possible to prove that every label will be 2\"k\"-aligned for at least \"k\" − 2 values of \"j\". Now, for every \"j\", discard the objects that are not \"k\"-aligned in the (\"j\"/\"k\",\"j\"/\"k\") shift, and find a maximum disjoint set of the remaining objects. Call that set \"A\"(\"j\"). Call the real maximum disjoint set is \"A\"*. Then:\n\nformula_37\n\nTherefore, the largest \"A\"(\"j\") has a size of at least: (1 − 2/\"k\")|\"A\"*|. The return value of the algorithm is the largest \"A\"(\"j\"); the approximation factor is (1 − 2/\"k\"), and the run time is \"n\". We can make the approximation factor as small as we want, so this is a PTAS.\n\nBoth versions can be generalized to \"d\" dimensions (with different approximation ratios) and to the weighted case.\n\nSeveral divide-and-conquer algorithms are based on a certain geometric separator theorem. A geometric separator is a line or shape that separates a given set of shapes to two smaller subsets, such that the number of shapes lost during the division is relatively small. This allows both PTASs and sub-exponential exact algorithms, as explained below.\n\nLet \"C\" be a set of \"n\" fat objects of arbitrary sizes. The following algorithm finds a disjoint set with a size of at least (1 − \"O\"())·|MDS(\"C\")| in time \"n\", for every constant \"b\" > 1.\n\nThe algorithm is based on the following geometric separator theorem, which can be proved similarly to the proof of the existence of geometric separator for disjoint squares:\n\nwhere \"a\" and \"c\" are constants. If we could calculate MDS(\"C\") exactly, we could make the constant \"a\" as low as 2/3 by a proper selection of the separator rectangle. But since we can only approximate MDS(\"C\") by a constant factor, the constant \"a\" must be larger. Fortunately, \"a\" remains a constant independent of |\"C\"|.\n\nThis separator theorem allows to build the following PTAS:\n\nSelect a constant \"b\". Check all possible combinations of up to \"b\" + 1 labels.\n\nLet \"E\"(\"m\") be the error of the above algorithm when the optimal MDS size is MDS(\"C\") = \"m\". When \"m\" ≤ \"b\", the error is 0 because the maximum disjoint set is calculated exactly; when \"m\" > \"b\", the error increases by at most \"c\" the number of labels intersected by the separator. The worst case for the algorithm is when the split in each step is in the maximum possible ratio which is \"a\":(1 − \"a\"). Therefore the error function satisfies the following recurrence relation:\n\nThe solution to this recurrence is:\n\ni.e., formula_41. We can make the approximation factor as small as we want by a proper selection of \"b\".\n\nThis PTAS is more space-efficient than the PTAS based on quadtrees, and can handle a generalization where the objects may slide, but it cannot handle the weighted case.\n\nLet \"C\" be a set of \"n\" disks, such that the ratio between the largest radius and the smallest radius is at most \"r\". The following algorithm finds MDS(\"C\") exactly in time formula_42.\n\nThe algorithm is based on a width-bounded geometric separator on the set \"Q\" of the centers of all disks in \"C\". This separator theorem allows to build the following exact algorithm:\n\nThe run time of this algorithm satisfies the following recurrence relation:\n\nThe solution to this recurrence is:\n\nA \"pseudo-disks-set\" is a set of objects in which the boundaries of every pair of objects intersect at most twice. (Note that this definition relates to a whole collection, and does not say anything about the shapes of the specific objects in the collection). A pseudo-disks-set has a bounded union complexity, i.e., the number of intersection points on the boundary of the union of all objects is linear in the number of objects.\n\nLet \"C\" be a pseudo-disks-set with \"n\" objects. The following local search algorithm finds a disjoint set of size at least formula_47 in time formula_48, for every integer constant formula_49:\nEvery exchange in the search step increases the size of \"S\" by at least 1, and thus can happen at most \"n\" times.\n\nThe algorithm is very simple; the difficult part is to prove the approximation ratio.\n\nSee also.\n\nLet \"C\" be a pseudo-disks-set with \"n\" objects and union complexity \"u\". Using linear programming relaxation, it is possible to find a disjoint set of size at least formula_55. This is possible either with a randomized algorithm that has a high probability of success and run time formula_56, or a deterministic algorithm with a slower run time (but still polynomial). This algorithm can be generalized to the weighted case.\n\n\n"}
{"id": "9610679", "url": "https://en.wikipedia.org/wiki?curid=9610679", "title": "Morse–Palais lemma", "text": "Morse–Palais lemma\n\nIn mathematics, the Morse–Palais lemma is a result in the calculus of variations and theory of Hilbert spaces. Roughly speaking, it states that a smooth enough function near a critical point can be expressed as a quadratic form after a suitable change of coordinates.\n\nThe Morse–Palais lemma was originally proved in the finite-dimensional case by the American mathematician Marston Morse, using the Gram–Schmidt orthogonalization process. This result plays a crucial role in Morse theory. The generalization to Hilbert spaces is due to Richard Palais and Stephen Smale.\n\nLet (\"H\", 〈 , 〉) be a real Hilbert space, and let \"U\" be an open neighbourhood of 0 in \"H\". Let \"f\" : \"U\" → R be a (\"k\" + 2)-times continuously differentiable function with \"k\" ≥ 1, i.e. \"f\" ∈ \"C\"(\"U\"; R). Assume that \"f\"(0) = 0 and that 0 is a non-degenerate critical point of \"f\", i.e. the second derivative D\"f\"(0) defines an isomorphism of \"H\" with its continuous dual space \"H\" by\n\nThen there exists a subneighbourhood \"V\" of 0 in \"U\", a diffeomorphism \"φ\" : \"V\" → \"V\" that is \"C\" with \"C\" inverse, and an invertible symmetric operator \"A\" : \"H\" → \"H\", such that\n\nfor all \"x\" ∈ \"V\".\n\nLet \"f\" : \"U\" → R be \"C\" such that 0 is a non-degenerate critical point. Then there exists a \"C\"-with-\"C\"-inverse diffeomorphism \"ψ\" : \"V\" → \"V\" and an orthogonal decomposition\n\nsuch that, if one writes\n\nthen\n\nfor all \"x\" ∈ \"V\".\n"}
{"id": "44450362", "url": "https://en.wikipedia.org/wiki?curid=44450362", "title": "Network medicine", "text": "Network medicine\n\nNetwork medicine is the application of network science towards identifying, preventing, and treating diseases. This field focuses on using network topology and network dynamics towards identifying diseases and developing medical drugs. Biological networks, such as protein-protein interactions and metabolic pathways, are utilized by network medicine. Disease networks, which map relationships between diseases and biological factors, also play an important role in the field. Epidemiology is extensively studied using network science as well; social networks and transportation networks are used to model the spreading of disease across populations. Network medicine is a medically focused area of systems biology.\n\nThe term \"network medicine\" was coined and popularized in a scientific article by Albert-László Barabási called \"Network Medicine – From Obesity to the \"Diseasome\", published in The New England Journal of Medicine, in 2007. Barabási states that biological systems, similarly to social and technological systems, contain many components that are connected in complicated relationships but are organized by simple principles. Using the recent development of network theory, the organizing principles can be comprehensively analyzed by representing systems as complex networks, which are collections of nodes linked together by a particular relationship. For networks pertaining to medicine, nodes represent biological factors (biomolecules, diseases, phenotypes, etc.) and links (edges) represent their relationships (physical interactions, shared metabolic pathway, shared gene, shared trait, etc.).\n\nThree key networks for understanding human disease are the metabolic network, the disease network, and the social network. The network medicine is based on the idea that understanding complexity of gene regulation, metabolic reactions, and protein-protein interactions and that representing these as complex networks will shed light on the causes and mechanisms of diseases. It is possible, for example, to infer a bipartite graph representing the connections of diseases to their associated genes using the OMIM database. The projection of the diseases, called the human disease network (HDN), is a network of diseases connected to each other if they share a common gene. Using the HDN, diseases can be classified and analyzed through the genetic relationships between them.\n\nThe whole set of molecular interactions in the human cell, also known as the interactome, can be used for disease identification and prevention. These networks have been technically classified as scale-free, disassortative, small-world networks, having a high betweenness centrality.\n\nProtein-protein interactions have been mapped, using proteins as nodes and their interactions between each other as links. These maps utilize databases such as BioGRID and the Human Protein Reference Database. The metabolic network encompasses the biochemical reactions in metabolic pathways, connecting two metabolites if they are in the same pathway. Researchers have used databases such as KEGG to map these networks. Others networks include cell signaling networks, gene regulatory networks, and RNA networks.\n\nUsing interactome networks, one can discover and classify diseases, as well as develop treatments through knowledge of its associations and their role in the networks. One observation is that diseases can be classified not by their principle phenotypes (pathophenotype) but by their disease module, which is a neighborhood or group of components in the interactome that, if disrupted, results in a specific pathophenotype. Disease modules can be used in a variety of ways, such as predicting disease genes that have not been discovered yet. Therefore, network medicine looks to identify the disease module for a specific pathophenotype using clustering algorithms.\n\nHuman disease networks, also called the diseasome, are networks in which the nodes are diseases and the links, the strength of correlation between them. This correlation is commonly quantified based on associated cellular components that two diseases share. The first-published human disease network (HDN) looked at genes, finding that many of the disease associated genes are non-essential genes, as these are the genes that do not completely disrupt the network and are able to be passed down generations. Metabolic disease networks (MDN), in which two diseases are connected by a shared metabolite or metabolic pathway, have also been extensively studied and is especially relevant in the case of metabolic disorders.\n\nThree representations of the diseasome are:\n\nSome disease networks connect diseases to associated factors outside the human cell. Networks of environmental and genetic etiological factors linked with shared diseases, called the \"etiome\", can be also used to assess the clustering of environmental factors in these networks and understand the role of the environment on the interactome. The human symptom-disease network (HSDN), published in June 2014, showed that the symptoms of disease and disease associated cellular components were strongly correlated and that diseases of the same categories tend to form highly connected communities, with respect to their symptoms.\n\nNetwork pharmacology is a developing field based in systems pharmacology that looks at the effect of drugs on both the interactome and the diseasome. The drug-target network (DTN) can play an important role in understanding the mechanisms of action of approved and experimental drugs. The network theory view of pharmaceuticals is based on the effect of the drug in the interactome, especially the region that the drug target occupies. Combination therapy for a complex disease (polypharmacology) is suggested in this field since one active pharmaceutical ingredient (API) aimed at one target may not effect the entire disease module. The concept of disease modules can be used to aid in drug discovery, drug design, and the development of biomarkers for disease detection. There can be a variety of ways to identifying drugs using network pharmacology; a simple example of this is the \"guilt by association\" method. This states if two diseases are treated by the same drug, a drug that treats one disease may treat the other. Drug repurposing, drug-drug interactions and drug side-effects have also been studied in this field.\n\nNetwork epidemics has been built by applying network science to existing epidemic models, as many transportation networks and social networks play a role in the spread of disease. Social networks have been used to assess the role of social ties in the spread of obesity in populations. Epidemic models and concepts, such as spreading and contact tracing, have been adapted to be used in network analysis. These models can be used in public health policies, in order to implement strategies such as targeted immunization and has been recently used to model the spread of the Ebola virus epidemic in West Africa across countries and continents.\n\nThe development of organs and other biological systems can be modelled as network structures where the clinical (e.g., radiographic, functional) characteristics can be represented as nodes and the relationships between these characteristics are represented as the links\namong such nodes. Therefore, it is possible to use networks to model how organ systems dynamically interact.\n\nThe Channing Division of Network Medicine at Brigham and Women's Hospital was created in 2012 to study, reclassify, and develop treatments for complex diseases using network science and systems biology. It focuses on three areas: \n\nMassachusetts Institute of Technology offers an undergraduate course called \"Network Medicine: Using Systems Biology and Signaling Networks to Create Novel Cancer Therapeutics\". Also, Harvard Catalyst (The Harvard Clinical and Translational Science Center) offers a three-day course entitled \"Introduction to Network Medicine\", open to clinical and science professionals with doctorate degrees.\n"}
{"id": "55972236", "url": "https://en.wikipedia.org/wiki?curid=55972236", "title": "Oberwolfach problem", "text": "Oberwolfach problem\n\nThe Oberwolfach problem is an unsolved problem in mathematics that may be formulated either as a problem scheduling seating assignments for diners,\nor more abstractly as a problem in graph theory, on the edge cycle covers of complete graphs. It is named after the Mathematical Research Institute of Oberwolfach, where the problem was posed in 1967 by Gerhard Ringel.\n\nIn conferences held at Oberwolfach, it is the custom for the participants to dine together in a room with circular tables, not all the same size, and with assigned seating that rearranges the participants from meal to meal. The Oberwolfach problem asks how to make a seating chart for a given set of tables so that all tables are full at each meal and all pairs of conference participants are seated next to each other exactly once. An instance of the problem can be denoted as formula_1 where formula_2 are the given table sizes. Alternatively, when some table sizes are repeated, they may be denoted using exponential notation; for instance, formula_3 describes an instance with three tables of size five.\n\nFormulated as a problem in graph theory, the pairs of people sitting next to each other at a single meal can be represented as a disjoint union of cycle graphs formula_4 of the specified lengths, with one cycle for each of the dining tables. This union of cycles is a 2-regular graph, and every 2-regular graph has this form. If formula_5 is this 2-regular graph and has formula_6 vertices, the question is whether the complete graph formula_7 can be represented as an edge-disjoint union of copies of formula_5.\n\nIn order for a solution to exist, the total number of conference participants (or equivalently, the total capacity of the tables, or the total number of vertices of the given cycle graphs) must be an odd number. For, at each meal, each participant sits next to two neighbors, so the total number of neighbors of each participant must be even, and this is only possible when the total number of participants is odd. The problem has, however, also been extended to even values of formula_6 by asking, for those formula_6, whether all of the edges of the complete graph except for a perfect matching can be covered by copies of the given 2-regular graph. Like the ménage problem (a different mathematical problem involving seating arrangements of diners and tables), this variant of the problem can be formulated by supposing that the formula_6 diners are arranged into formula_12 married couples, and that the seating arrangements should place each diner next to each other diner except their own spouse exactly once.\n\nThe only instances of the Oberwolfach problem that are known not to be solvable are formula_13, formula_14, formula_15, and formula_16. It is widely believed that all other instances have a solution, but only special cases have been provable to be solvable.\n\nThe cases for which a solution is known include:\n\nKirkman's schoolgirl problem, of grouping fifteen schoolgirls into rows of three in seven different ways so that each pair of girls appears once in each triple, is a special case of the Oberwolfach problem, formula_25. The problem of Hamiltonian decomposition of a complete graph formula_7 is another special case, formula_27.\n\nAlspach's conjecture, on the decomposition of a complete graph into cycles of given sizes, is related to the Oberwolfach problem, but neither is a special case of the other.\nIf formula_5 is a 2-regular graph, with formula_6 vertices, formed from a disjoint union of cycles of certain lengths, then a solution to the Oberwolfach problem for formula_5 would also provide a decomposition of the complete graph into formula_31 copies of each of the cycles of formula_5. However, not every decomposition of formula_7 into this many cycles of each size can be grouped into disjoint cycles that form copies of formula_5, and on the other hand not every instance of Alspach's conjecture involves sets of cycles that have formula_31 copies of each cycle.\n"}
{"id": "29943244", "url": "https://en.wikipedia.org/wiki?curid=29943244", "title": "Orbital overlap", "text": "Orbital overlap\n\nIn chemical bonds, an orbital overlap is the concentration of orbitals on adjacent atoms in the same regions of space. Orbital overlap can lead to bond formation. The importance of orbital overlap was emphasized by Linus Pauling to explain the molecular bond angles observed through experimentation and is the basis for the concept of orbital hybridization. Since \"s\" orbitals are spherical (and have no directionality) and \"p\" orbitals are oriented 90° to each other, a theory was needed to explain why molecules such as methane (CH) had observed bond angles of 109.5°. Pauling proposed that s and p orbitals on the carbon atom can combine to form hybrids (sp in the case of methane) which are directed toward the hydrogen atoms. The carbon hybrid orbitals have greater overlap with the hydrogen orbitals, and can therefore form stronger C–H bonds.\n\nA quantitative measure of the overlap of two atomic orbitals Ψ and Ψ on atoms A and B is their overlap integral, defined as\n\nwhere the integration extends over all space. The star on the first orbital wavefunction indicates the complex conjugate of the function, which in general may be complex-valued.\n\nThe overlap matrix is a square matrix, used in quantum chemistry to describe the inter-relationship of a set of basis vectors of a quantum system, such as an atomic orbital basis set used in molecular electronic structure calculations. In particular, if the vectors are orthogonal to one another, the overlap matrix will be diagonal. In addition, if the basis vectors form an orthonormal set, the overlap matrix will be the identity matrix. The overlap matrix is always \"n\"×\"n\", where \"n\" is the number of basis functions used. It is a kind of Gramian matrix.\n\nIn general, each overlap matrix element is defined as an overlap integral:\n\nwhere\n\nIn particular, if the set is normalized (though not necessarily orthogonal) then the diagonal elements will be identically 1 and the magnitude of the off-diagonal elements less than or equal to one with equality if and only if there is linear dependence in the basis set as per the Cauchy–Schwarz inequality. Moreover, the matrix is always positive definite; that is to say, the eigenvalues are all strictly positive.\n\n\n\"Quantum Chemistry: Fifth Edition\", Ira N. Levine, 2000\n"}
{"id": "10950869", "url": "https://en.wikipedia.org/wiki?curid=10950869", "title": "Orthonormal function system", "text": "Orthonormal function system\n\nAn orthonormal function system (ONS) is an orthonormal basis in a vector space of functions.\n\nSee basis (linear algebra), Fourier analysis, square-integrable, Hilbert space for more.\n"}
{"id": "1832537", "url": "https://en.wikipedia.org/wiki?curid=1832537", "title": "Parametric derivative", "text": "Parametric derivative\n\nIn calculus, a parametric derivative is a derivative of a dependent variable \"y\" with respect to an independent variable \"x\" that is taken when both variables depend on an independent third variable \"t\", usually thought of as \"time\" (that is, when \"x\" and \"y\" are given by parametric equations in \"t\" ).\n\nLet formula_1 and formula_2 be the coordinates of the points of the curve expressed as functions of a variable \"t\":\n\nThe first derivative implied by these parametric equations is \n\nwhere the notation formula_5 denotes the derivative of \"x\" with respect to \"t\", for example. This can be derived using the chain rule for derivatives:\n\nand dividing both sides by formula_7 to give the equation above.\n\nIn general all of these derivatives — \"dy / dt\", \"dx / dt\", and \"dy / dx\" — are themselves functions of \"t\" and so can be written more explicitly as, for example, formula_8\n\nThe second derivative implied by a parametric equation is given by\nby making use of the quotient rule for derivatives. The latter result is useful in the computation of curvature.\n\nFor example, consider the set of functions where:\nand\n\nDifferentiating both functions with respect to \"t\" leads to\n\nand\n\nrespectively. Substituting these into the formula for the parametric derivative, we obtain\n\nwhere formula_14 and formula_15 are understood to be functions of \"t\".\n\n\n"}
{"id": "18978005", "url": "https://en.wikipedia.org/wiki?curid=18978005", "title": "Planar separator theorem", "text": "Planar separator theorem\n\nIn graph theory, the planar separator theorem is a form of isoperimetric inequality for planar graphs, that states that any planar graph can be split into smaller pieces by removing a small number of vertices. Specifically, the removal of O(√\"n\") vertices from an \"n\"-vertex graph (where the \"O\" invokes big O notation) can partition the graph into disjoint subgraphs each of which has at most 2\"n\"/3 vertices.\n\nA weaker form of the separator theorem with O(√\"n\" log \"n\") vertices in the separator instead of O(√\"n\") was originally proven by , and the form with the tight asymptotic bound on the separator size was first proven by . Since their work, the separator theorem has been reproven in several different ways, the constant in the O(√\"n\") term of the theorem has been improved, and it has been extended to certain classes of nonplanar graphs.\n\nRepeated application of the separator theorem produces a separator hierarchy which may take the form of either a tree decomposition or a branch-decomposition of the graph. Separator hierarchies may be used to devise efficient divide and conquer algorithms for planar graphs, and dynamic programming on these hierarchies can be used to devise exponential time and fixed-parameter tractable algorithms for solving NP-hard optimization problems on these graphs. Separator hierarchies may also be used in nested dissection, an efficient variant of Gaussian elimination for solving sparse systems of linear equations arising from finite element methods.\n\nBidimensionality theory of Demaine, Fomin, Hajiaghayi, and Thilikos generalizes and greatly expands the applicability of the separator theorem \nfor a vast set of minimization problems on planar graphs and more generally graphs excluding a fixed minor.\n\nAs it is usually stated, the separator theorem states that, in any \"n\"-vertex planar graph \"G\" = (\"V\",\"E\"), there exists a partition of the vertices of \"G\" into three sets \"A\", \"S\", and \"B\", such that each of \"A\" and \"B\" has at most 2\"n\"/3 vertices, \"S\" has O(√\"n\") vertices, and there are no edges with one endpoint in \"A\" and one endpoint in \"B\". It is not required that \"A\" or \"B\" form connected subgraphs of \"G\". \"S\" is called the separator for this partition.\n\nAn equivalent formulation is that the edges of any \"n\"-vertex planar graph \"G\" may be subdivided into two edge-disjoint subgraphs \"G\" and \"G\" in such a way that both subgraphs have at least \"n\"/3 vertices and such that the intersection of the vertex sets of the two subgraphs has O(√\"n\") vertices in it. Such a partition is known as a separation. If a separation is given, then the intersection of the vertex sets forms a separator, and the vertices that belong to one subgraph but not the other form the separated subsets of at most 2\"n\"/3 vertices. In the other direction, if one is given a partition into three sets \"A\", \"S\", and \"B\" that meet the conditions of the planar separator theorem, then one may form a separation in which the edges with an endpoint in \"A\" belong to \"G\", the edges with an endpoint in \"B\" belong to \"G\", and the remaining edges (with both endpoints in \"S\") are partitioned arbitrarily.\n\nThe constant 2/3 in the statement of the separator theorem is arbitrary and may be replaced by any other number in the open interval (1/2,1) without changing the form of the theorem: a partition into more equal subsets may be obtained from a less-even partition by repeatedly splitting the larger sets in the uneven partition and regrouping the resulting connected components.\n\nConsider a grid graph with \"r\" rows and \"c\" columns; the number \"n\" of vertices equals \"rc\". For instance, in the illustration, \"r\" = 5, \"c\" = 8, and \"n\" = 40. If \"r\" is odd, there is a single central row, and otherwise there are two rows equally close to the center; similarly, if \"c\" is odd, there is a single central column, and otherwise there are two columns equally close to the center. Choosing \"S\" to be any of these central rows or columns, and removing \"S\" from the graph, partitions the graph into two smaller connected subgraphs \"A\" and \"B\", each of which has at most \"n\"/2 vertices. If \"r\" ≤ \"c\" (as in the illustration), then choosing a central column will give a separator \"S\" with \"r\" ≤ √\"n\" vertices, and similarly if \"c\" ≤ \"r\" then choosing a central row will give a separator with at most √\"n\" vertices. Thus, every grid graph has a separator \"S\" of size at most √\"n\", the removal of which partitions it into two connected components, each of size at most \"n\"/2.\n\nThe planar separator theorem states that a similar partition can be constructed in any planar graph. The case of arbitrary planar graphs differs from the case of grid graphs in that the separator has size O(√\"n\") but may be larger than √\"n\", the bound on the size of the two subsets \"A\" and \"B\" (in the most common versions of the theorem) is 2\"n\"/3 rather than \"n\"/2, and the two subsets \"A\" and \"B\" need not themselves form connected subgraphs.\n\n augment the given planar graph by additional edges, if necessary, so that it becomes maximal planar (every face in a planar embedding is a triangle). They then perform a breadth-first search, rooted at an arbitrary vertex \"v\", and partition the vertices into levels by their distance from \"v\". If \"l\" is the median level (the level such that the numbers of vertices at higher and lower levels are both at most \"n\"/2) then there must be levels \"l\" and \"l\" that are O(√\"n\") steps above and below \"l\" respectively and that contain O(√\"n\") vertices, respectively, for otherwise there would be more than \"n\" vertices in the levels near \"l\". They show that there must be a separator \"S\" formed by the union of \"l\" and \"l\", the endpoints \"e\" of an edge of \"G\" that does not belong to the breadth-first search tree and that lies between the two levels, and the vertices on the two breadth-first search tree paths from \"e\" back up to level \"l\". The size of the separator \"S\" constructed in this way is at most √8√\"n\", or approximately 2.83√\"n\". The vertices of the separator and the two disjoint subgraphs can be found in linear time.\n\nThis proof of the separator theorem applies as well to weighted planar graphs, in which each vertex has a non-negative cost. The graph may be partitioned into three sets \"A\", \"S\", and \"B\" such that \"A\" and \"B\" each have at most 2/3 of the total cost and \"S\" has O(√\"n\") vertices, with no edges from \"A\" to \"B\". By analysing a similar separator construction more carefully, shows that the bound on the size of \"S\" can be reduced to √6√\"n\", or approximately 2.45√\"n\".\n\nFor a graph that is already maximal planar it is possible to show a stronger construction of a simple cycle separator, a cycle of small length such that the inside and the outside of the cycle (in the unique planar embedding of the graph) each have at most 2\"n\"/3 vertices. proves this (with a separator size of √8√\"n\") by using the Lipton–Tarjan technique for a modified version of breadth first search in which the levels of the search form simple cycles.\n\nAccording to the Koebe–Andreev–Thurston circle-packing theorem, any planar graph may be represented by a packing of circular disks in the plane with disjoint interiors, such that two vertices in the graph are adjacent if and only if the corresponding pair of disks are mutually tangent. As show, for such a packing, there exists a circle that has at most 3\"n\"/4 disks touching or inside it, at most 3\"n\"/4 disks touching or outside it, and that crosses O(√\"n\") disks.\n\nTo prove this, Miller et al. use stereographic projection to map the packing onto the surface of a unit sphere in three dimensions. By choosing the projection carefully, the center of the sphere can be made into a centerpoint of the disk centers on its surface, so that any plane through the center of the sphere partitions it into two halfspaces that each contain or cross at most 3\"n\"/4 of the disks. If a plane through the center is chosen uniformly at random, a disk will be crossed with probability proportional to its radius. Therefore, the expected number of disks that are crossed is proportional to the sum of the radii of the disks. However, the sum of the squares of the radii is proportional to the total area of the disks, which is at most the total surface area of the unit sphere, a constant. An argument involving Jensen's inequality shows that, when the sum of squares of \"n\" non-negative real numbers is bounded by a constant, the sum of the numbers themselves is O(√\"n\"). Therefore, the expected number of disks crossed by a random plane is O(√\"n\") and there exists a plane that crosses at most that many disks. This plane intersects the sphere in a great circle, which projects back down to a circle in the plane with the desired properties. The O(√\"n\") disks crossed by this circle correspond to the vertices of a planar graph separator that separates the vertices whose disks are inside the circle from the vertices whose disks are outside the circle, with at most 3\"n\"/4 vertices in each of these two subsets.\n\nThis method leads to a randomized algorithm that finds such a separator in linear time, and a less-practical deterministic algorithm with the same linear time bound. By analyzing this algorithm carefully using known bounds on the packing density of circle packings, it can be shown to find separators of size at most\nAlthough this improved separator size bound comes at the expense of a more-uneven partition of the graph, argue that it provides an improved constant factor in the time bounds for nested dissection compared to the separators of . The size of the separators it produces can be further improved, in practice, by using a nonuniform distribution for the random cutting planes.\n\nThe stereographic projection in the Miller et al. argument can be avoided by considering the smallest circle containing a constant fraction of the centers of the disks and then expanding it by a constant picked uniformly in the range [1,2]. It is easy to argue, as in Miller et al., that the disks intersecting the expanded circle form a valid separator, and that, in expectation, the separator is of the right size. The resulting constants are somewhat worse.\n\nSpectral clustering methods, in which the vertices of a graph are grouped by the coordinates of the eigenvectors of matrices derived from the graph, have long been used as a heuristic for graph partitioning problems for nonplanar graphs. As show, spectral clustering can also be used to derive an alternative proof for a weakened form of the planar separator theorem that applies to planar graphs with bounded degree. In their method, the vertices of a given planar graph are sorted by the second coordinates of the eigenvectors of the Laplacian matrix of the graph, and this sorted order is partitioned at the point that minimizes the ratio of the number of edges cut by the partition to the number of vertices on the smaller side of the partition. As they show, every planar graph of bounded degree has a partition of this type in which the ratio is O(1/√\"n\"). Although this partition may not be balanced, repeating the partition within the larger of the two sides and taking the union of the cuts formed at each repetition will eventually lead to a balanced partition with O(√\"n\") edges. The endpoints of these edges form a separator of size O(√\"n\").\n\nA variation of the planar separator theorem involves edge separators, small sets of edges forming a cut between two subsets \"A\" and \"B\" of the vertices of the graph. The two sets \"A\" and \"B\" must each have size at most a constant fraction of the number \"n\" of vertices of the graph (conventionally, both sets have size at most 2\"n\"/3), and each vertex of the graph belongs to exactly one of \"A\" and \"B\". The separator consists of the edges that have one endpoint in \"A\" and one endpoint in \"B\". Bounds on the size of an edge separator involve the degree of the vertices as well as the number of vertices in the graph: the planar graphs in which one vertex has degree \"n\" − 1, including the wheel graphs and star graphs, have no edge separator with a sublinear number of edges, because any edge separator would have to include all the edges connecting the high degree vertex to the vertices on the other side of the cut. However, every planar graph with maximum degree Δ has an edge separator of size O(√(Δ\"n\")).\n\nA simple cycle separator in the dual graph of a planar graph forms an edge separator in the original graph.\nApplying the simple cycle separator theorem of to the dual graph of a given planar graph strengthens the O(√(Δ\"n\")) bound for the size of an edge separator by showing that every planar graph has an edge separator whose size is proportional to the Euclidean norm of its vector of vertex degrees.\n\nIn a √\"n\" × √\"n\" grid graph, a set \"S\" of \"s\" < √\"n\" points can enclose a subset of at most \"s\"(\"s\" − 1)/2 grid points, where the maximum is achieved by arranging \"S\" in a diagonal line near a corner of the grid. Therefore, in order to form a separator that separates at least \"n\"/3 of the points from the remaining grid, \"s\" needs to be at least √(2\"n\"/3), approximately 0.82√\"n\".\n\nThere exist \"n\"-vertex planar graphs (for arbitrarily large values of \"n\") such that, for every separator \"S\" that partitions the remaining graph into subgraphs of at most 2\"n\"/3 vertices, \"S\" has at least √(4π√3)√\"n\" vertices, approximately 1.56√\"n\". The construction involves approximating a sphere by a convex polyhedron, replacing each of the faces of the polyhedron by a triangular mesh, and applying isoperimetric theorems for the surface of the sphere.\n\nSeparators may be combined into a separator hierarchy of a planar graph, a recursive decomposition into smaller graphs. A separator hierarchy may be represented by a binary tree in which the root node represents the given graph itself, and the two children of the root are the roots of recursively constructed separator hierarchies for the induced subgraphs formed from the two subsets \"A\" and \"B\" of a separator.\n\nA separator hierarchy of this type forms the basis for a tree decomposition of the given graph, in which the set of vertices associated with each tree node is the union of the separators on the path from that node to the root of the tree. Since the sizes of the graphs go down by a constant factor at each level of the tree, the upper bounds on the sizes of the separators also go down by a constant factor at each level, so the sizes of the separators on these paths add in a geometric series to O(√\"n\"). That is, a separator formed in this way has width O(√\"n\"), and can be used to show that every planar graph has treewidth O(√\"n\").\n\nConstructing a separator hierarchy directly, by traversing the binary tree top down and applying a linear-time planar separator algorithm to each of the induced subgraphs associated with each node of the binary tree, would take a total of O(\"n\" log \"n\") time. However, it is possible to construct an entire separator hierarchy in linear time, by using the Lipton–Tarjan breadth-first layering approach and by using appropriate data structures to perform each partition step in sublinear time.\n\nIf one forms a related type of hierarchy based on separations instead of separators, in which the two children of the root node are roots of recursively constructed hierarchies for the two subgraphs \"G\" and \"G\" of a separation of the given graph, then the overall structure forms a branch-decomposition instead of a tree decomposition. The width of any separation in this decomposition is, again, bounded by the sum of the sizes of the separators on a path from any node to the root of the hierarchy, so any branch-decomposition formed in this way has width O(√\"n\") and any planar graph has branchwidth O(√\"n\"). Although many other related graph partitioning problems are NP-complete, even for planar graphs, it is possible to find a minimum-width branch-decomposition of a planar graph in polynomial time.\n\nBy applying methods of more directly in the construction of branch-decompositions, show that every planar graph has branchwidth at most 2.12√\"n\", with the same constant as the one in the simple cycle separator theorem of Alon et al. Since the treewidth of any graph is at most 3/2 its branchwidth, this also shows that planar graphs have treewidth at most 3.18√\"n\".\n\nSome sparse graphs do not have separators of sublinear size: in an expander graph, deleting up to a constant fraction of the vertices still leaves only one connected component.\n\nPossibly the earliest known separator theorem is a result of that any tree can be partitioned into subtrees of at most \"n\"/2 vertices each by the removal of a single vertex. In particular, the vertex that minimizes the maximum component size has this property, for if it did not then its neighbor in the unique large subtree would form an even better partition. By applying the same technique to a tree decomposition of an arbitrary graph, it is possible to show that any graph has a separator of size at most equal to its treewidth.\n\nIf a graph \"G\" is not planar, but can be embedded on a surface of genus \"g\", then it has a separator with O((\"gn\")) vertices. prove this by using a similar approach to that of . They group the vertices of the graph into breadth-first levels and find two levels the removal of which leaves at most one large component consisting of a small number of levels. This remaining component can be made planar by removing a number of breadth-first paths proportional to the genus, after which the Lipton–Tarjan method can be applied to the remaining planar graph. The result follows from a careful balancing of the size of the removed two levels against the number of levels between them. If the graph embedding is given as part of the input, its separator can be found in linear time. Graphs of genus \"g\" also have edge separators of size O((\"g\"Δ\"n\")).\n\nGraphs of bounded genus form an example of a family of graphs closed under the operation of taking minors, and separator theorems also apply to arbitrary minor-closed graph families. In particular, if a graph family has a forbidden minor with \"h\" vertices, then it has a separator with O(\"h\"√\"n\") vertices, and such a separator can be found in time O(\"n\") for any ε > 0.\nThe circle separator method of generalizes to the intersection graphs of any system of \"d\"-dimensional balls with the property that any point in space is covered by at most some constant number \"k\" of balls, to \"k\"-nearest-neighbor graphs in \"d\" dimensions, and to the graphs arising from finite element meshes. The sphere separators constructed in this way partition the input graph into subgraphs of at most vertices. The size of the separators for \"k\"-ply ball intersection graphs and for \"k\"-nearest-neighbor graphs is O(\"k\"\"n\").\n\nSeparator decompositions can be of use in designing efficient divide and conquer algorithms for solving problems on planar graphs. As an example, one problem that can be solved in this way is to find the shortest cycle in a weighted planar digraph. This may be solved by the following steps:\nThe time for the two recursive calls to \"A\" and \"B\" in this algorithm is dominated by the time to perform the O(√\"n\") calls to Dijkstra's algorithm, so this algorithm finds the shortest cycle in O(\"n\" log \"n\") time.\n\nA faster algorithm for the same shortest cycle problem, running in time O(\"n\" log\"n\"), was given by . His algorithm uses the same separator-based divide and conquer structure, but uses simple cycle separators rather than arbitrary separators, so that the vertices of \"S\" belong to a single face of the graphs inside and outside the cycle separator. He then replaces the O(√\"n\") separate calls to Dijkstra's algorithm with more sophisticated algorithms to find shortest paths from all vertices on a single face of a planar graph and to combine the distances from the two subgraphs. For weighted but undirected planar graphs, the shortest cycle is equivalent to the minimum cut in the dual graph and can be found in O(\"n\" log log \"n\") time, and the shortest cycle in an unweighted undirected planar graph (its girth) may be found in time O(\"n\"). (However, the faster algorithm for unweighted graphs is not based on the separator theorem.)\n\nFrederickson proposed another faster algorithm for single source shortest paths by implementing separator theorem in planar graphs in 1986. This is an improvement of Dijkstra's algorithm with iterative search on a carefully selected subset of the vertices. This version takes O(\"n\" √(log \"n\")) time in an \"n\"-vertex graph. Separators are used to find a division of a graph, that is, a partition of the edge-set into two or more subsets, called regions. A node is said to be contained in a region if some edge of the region is incident to the node. A node contained in more that one region is called a boundary node of the regions containing it. The method uses the notion of a \"r\"-division of an \"n\"-node graph that is a graph division into O(\"n\"/\"r\") regions, each containing O(\"r\") nodes including O(√\"r\") boundary nodes. Frederickson showed that an \"r\"-division can be found in O(\"n\" log \"n\") time by recursive application of separator theorem.\n\nThe sketch of his algorithm to solve the problem is as follows.\n\n1. Preprocessing Phase: Partition the graph into carefully selected subsets of vertices and determine the shortest paths between all pairs of vertices in these subsets, where intermediate vertices on this path are not in the subset. This phase requires a planar graph \"G\"  to be transformed into \"G\" with no vertex having degree greater than 3. From a corollary of Euler's formula, the number of vertices in the resulting graph will be \"n\" ≤ 6\"n\" -12, where \"n\"  is the number of vertices in \"G\" . This phase also ensures the following properties of a suitable \"r\"-division. A suitable \"r\"-division of a planar graph is an \"r\"-division such that,\n\n2. Search Phase:\n\nHenzinger et. al. extended Frederickson's \"r\"-division technique for the single source shortest path algorithm in planar graphs for nonnegative edge-lengths and proposed a linear time algorithm. Their method generalizes Frederickson's notion of graph-divisions such that now an (\"r\",\"s\")-division of an \"n\"-node graph be a division into O(\"n\"/\"r\") regions, each containing \"r\"  nodes, each having at most \"s\" boundary nodes. If an (\"r\", \"s\")-division is repeatedly divided into smaller regions, that is called get a recursive division. This algorithm uses approximately log*\"n\" levels of divisions. The recursive division is represented by a rooted tree whose leaves are labeled by distinct edge of \"G\". The root of the tree represents the region consisting of full-\"G\", the children of the root represent the subregions into which that region is divided and so on. Each leaf (atomic region) represents a region containing exactly one edge.\n\nNested dissection is a separator based divide and conquer variation of Gaussian elimination for solving sparse symmetric systems of linear equations with a planar graph structure, such as the ones arising from the finite element method. It involves finding a separator for the graph describing the system of equations, recursively eliminating the variables in the two subproblems separated from each other by the separator, and then eliminating the variables in the separator. The fill-in of this method (the number of nonzero coefficients of the resulting Cholesky decomposition of the matrix) is O(\"n\" log \"n\"), allowing this method to be competitive with iterative methods for the same problems.\n\nKlein, Mozes and Weimann gave an O(\"n\" log \"n\")-time, linear-space algorithm to find the shortest path distances from \"s\" to all nodes for a directed planar graph with positive and negative arc-lengths containing no negative cycles. Their algorithm uses planar graph separators to find a Jordan curve \"C\" that passes through O(√\"n\") nodes (and no arcs) such that between \"n\"/3 and 2\"n\"/3 nodes are enclosed by \"C\". Nodes through which \"C\" passes are boundary nodes. The original graph \"G\" is separated into two subgraphs \"G\"  and \"G\"  by cutting the planar embedding along \"C\" and duplicating the boundary nodes. For \"i\" = 0 and 1, in \"G\"  the boundary nodes lie on the boundary of a single face \"F\" .\n\nThe overview of their approach is given below. \n\nAn important part of this algorithm is the use of Price Functions and Reduced Lengths. For a directed graph \"G\" with arc-lengths ι(·), a price function is a function φ from the nodes of \"G\" to the real numbers. For an arc \"uv\", the reduced length with respect to φ is ιφ(\"uv\") = ι(\"uv\") + φ(\"u\") − φ(\"v\"). A feasible price function is a price function that induces nonnegative reduced lengths on all arcs of \"G\". It is useful in transforming a shortest-path problem involving positive and negative lengths into one involving only nonnegative lengths, which can then be solved using Dijkstra’s algorithm.\n\nThe separator based divide and conquer paradigm has also been used to design data structures for dynamic graph algorithms and point location, algorithms for polygon triangulation, shortest paths, and the construction of nearest neighbor graphs, and approximation algorithms for the maximum independent set of a planar graph.\n\nBy using dynamic programming on a tree decomposition or branch-decomposition of a planar graph, many NP-hard optimization problems may be solved in time exponential in √\"n\" or √\"n\" log \"n\". For instance, bounds of this form are known for finding maximum independent sets, Steiner trees, and Hamiltonian cycles, and for solving the travelling salesman problem on planar graphs. Similar methods involving separator theorems for geometric graphs may be used to solve Euclidean travelling salesman problem and Steiner tree construction problems in time bounds of the same form.\n\nFor parameterized problems that admit a kernelization that preserves planarity and reduces the input graph to a kernel of size linear in the input parameter, this approach can be used to design fixed-parameter tractable algorithms the running time of which depends polynomially on the size of the input graph and exponentially on √\"k\", where \"k\" is the parameter of the algorithm. For instance, time bounds of this form are known for finding vertex covers and dominating sets of size \"k\".\n\n observed that the separator theorem may be used to obtain polynomial time approximation schemes for NP-hard optimization problems on planar graphs such as finding the maximum independent set. Specifically, by truncating a separator hierarchy at an appropriate level, one may find a separator of size O(\"n\"/√log \"n\") the removal of which partitions the graph into subgraphs of size \"c\" log \"n\", for any constant \"c\". By the four-color theorem, there exists an independent set of size at least \"n\"/4, so the removed nodes form a negligible fraction of the maximum independent set, and the maximum independent sets in the remaining subgraphs can be found independently in time exponential in their size. By combining this approach with later linear-time methods for separator hierarchy construction and with table lookup to share the computation of independent sets between isomorphic subgraphs, it can be made to construct independent sets of size within a factor of 1 − O(1/√log \"n\") of optimal, in linear time. However, for approximation ratios even closer to 1 than this factor, a later approach of (based on tree-decomposition but not on planar separators) provides better tradeoffs of time versus approximation quality.\n\nSimilar separator-based approximation schemes have also been used to approximate other hard problems such as vertex cover. use separators in a different way to approximate the travelling salesman problem for the shortest path metric on weighted planar graphs; their algorithm uses dynamic programming to find the shortest tour that, at each level of a separator hierarchy, crosses the separator a bounded number of times, and they show that as the crossing bound increases the tours constructed in this way have lengths that approximate the optimal tour.\n\nSeparators have been used as part of data compression algorithms for representing planar graphs and other separable graphs using a small number of bits. The basic principle of these algorithms is to choose a number \"k\" and repeatedly subdivide the given planar graph using separators into O(\"n\"/\"k\") subgraphs of size at most \"k\", with O(\"n\"/√\"k\") vertices in the separators. With an appropriate choice of \"k\" (at most proportional to the logarithm of \"n\") the number of non-isomorphic \"k\"-vertex planar subgraphs is significantly less than the number of subgraphs in the decomposition, so the graph can be compressed by constructing a table of all the possible non-isomorphic subgraphs and representing each subgraph in the separator decomposition by its index into the table. The remainder of the graph, formed by the separator vertices, may be represented explicitly or by using a recursive version of the same data structure. Using this method, planar graphs and many more restricted families of graphs may be encoded using a number of bits that is information-theoretically optimal: if there are \"P\" \"n\"-vertex graphs in the family of graphs to be represented, then an individual graph in the family can be represented using only (1 + o(\"n\"))log\"P\" bits. It is also possible to construct representations of this type in which one may test adjacency between vertices, determine the degree of a vertex, and list neighbors of vertices in constant time per query, by augmenting the table of subgraphs with additional tabular information representing the answers to the queries.\n\nA universal graph for a family \"F\" of graphs is a graph that contains every member of \"F\" as a subgraphs. Separators can be used to show that the \"n\"-vertex planar graphs have universal graphs with \"n\" vertices and O(\"n\") edges.\n\nThe construction involves a strengthened form of the separator theorem in which the size of the three subsets of vertices in the separator does not depend on the graph structure: there exists a number \"c\", the magnitude of which at most a constant times √\"n\", such that the vertices of every \"n\"-vertex planar graph can be separated into subsets \"A\", \"S\", and \"B\", with no edges from \"A\" to \"B\", with |\"S\"| = \"c\", and with |\"A\"| = |\"B\"| = (\"n\" − \"c\")/2. This may be shown by using the usual form of the separator theorem repeatedly to partition the graph until all the components of the partition can be arranged into two subsets of fewer than \"n\"/2 vertices, and then moving vertices from these subsets into the separator as necessary until it has the given size.\n\nOnce a separator theorem of this type is shown, it can be used to produce a separator hierarchy for \"n\"-vertex planar graphs that again does not depend on the graph structure: the tree-decomposition formed from this hierarchy has width O(√\"n\") and can be used for any planar graph. The set of all pairs of vertices in this tree-decomposition that both belong to a common node of the tree-decomposition forms a trivially perfect graph with O(\"n\") vertices that contains every \"n\"-vertex planar graph as a subgraph. A similar construction shows that bounded-degree planar graphs have universal graphs with O(\"n\" log \"n\") edges, where the constant hidden in the O notation depends on the degree bound. Any universal graph for planar graphs (or even for trees of unbounded degree) must have Ω(\"n\" log \"n\") edges, but it remains unknown whether this lower bound or the O(\"n\") upper bound is tight for universal graphs for arbitrary planar graphs.\n\n\n"}
{"id": "2214583", "url": "https://en.wikipedia.org/wiki?curid=2214583", "title": "Poisson kernel", "text": "Poisson kernel\n\nIn potential theory, the Poisson kernel is an integral kernel, used for solving the two-dimensional Laplace equation, given Dirichlet boundary conditions on the unit disc. The kernel can be understood as the derivative of the Green's function for the Laplace equation. It is named for Siméon Poisson.\n\nPoisson kernels commonly find applications in control theory and two-dimensional problems in electrostatics.\nIn practice, the definition of Poisson kernels are often extended to \"n\"-dimensional problems.\n\nIn the complex plane, the Poisson kernel for the unit disc is given by\n\nThis can be thought of in two ways: either as a function of \"r\" and \"θ\", or as a family of functions of \"θ\" indexed by \"r\".\n\nIf formula_2 is the open unit disc in C, T is the boundary of the disc, and \"f\" a function on T that lies in \"L\"(T), then the function \"u\" given by\n\nis harmonic in D and has a radial limit that agrees with \"f\" almost everywhere on the boundary T of the disc.\n\nThat the boundary value of \"u\" is \"f\" can be argued using the fact that as \"r\" → 1, the functions \"P\"(\"θ\") form an approximate unit in the convolution algebra \"L\"(T). As linear operators, they tend to the Dirac delta function pointwise on \"L\"(T). By the maximum principle, \"u\" is the only such harmonic function on \"D\".\n\nConvolutions with this approximate unit gives an example of a summability kernel for the Fourier series of a function in \"L\"(T) . Let \"f\" ∈ \"L\"(T) have Fourier series {\"f\"}. After the Fourier transform, convolution with \"P\"(\"θ\") becomes multiplication by the sequence {\"r\"} ∈ \"l\"(Z). Taking the inverse Fourier transform of the resulting product {\"rf\"} gives the Abel means \"Af\" of \"f\":\n\nRearranging this absolutely convergent series shows that \"f\" is the boundary value of \"g\" + \"h\", where \"g\" (resp. \"h\") is a holomorphic (resp. antiholomorphic) function on \"D\".\n\nWhen one also asks for the harmonic extension to be holomorphic, then the solutions are elements of a Hardy space. This is true when the negative Fourier coefficients of \"f\" all vanish. In particular, the Poisson kernel is commonly used to demonstrate the equivalence of the Hardy spaces on the unit disk, and the unit circle.\n\nThe space of functions that are the limits on T of functions in \"H\"(\"z\") may be called \"H\"(T). It is a closed subspace of \"L\"(T) (at least for \"p\"≥1). Since \"L\"(T) is a Banach space (for 1 ≤ \"p\" ≤ ∞), so is \"H\"(T).\n\nThe unit disk may be conformally mapped to the upper half-plane by means of certain Möbius transformations. Since the conformal map of a harmonic function is also harmonic, the Poisson kernel carries over to the upper half-plane. In this case, the Poisson integral equation takes the form\n\nThe kernel itself is given by\n\nGiven a function formula_7, the \"L\" space of integrable functions on the real line, \"u\" can be understood as a harmonic extension of \"f\" into the upper half-plane. In analogy to the situation for the disk, when \"u\" is holomorphic in the upper half-plane, then \"u\" is an element of the Hardy space, formula_8 and in particular,\n\nThus, again, the Hardy space \"H\" on the upper half-plane is a Banach space, and, in particular, its restriction to the real axis is a closed subspace of formula_10 The situation is only analogous to the case for the unit disk; the Lebesgue measure for the unit circle is finite, whereas that for the real line is not.\n\nFor the ball of radius formula_11 the Poisson kernel takes the form\n\nwhere formula_13 (the surface of formula_14), and formula_15 is the surface area of the unit (\"n\"−1)-sphere.\n\nThen, if \"u\"(\"x\") is a continuous function defined on \"S\", the corresponding Poisson integral is the function \"P\"[\"u\"](\"x\") defined by\n\nIt can be shown that \"P\"[\"u\"](\"x\") is harmonic on the ball formula_14 and that \"P\"[\"u\"](\"x\") extends to a continuous function on the closed ball of radius \"r\", and the boundary function coincides with the original function \"u\".\n\nAn expression for the Poisson kernel of an upper half-space can also be obtained. Denote the standard Cartesian coordinates of R by\nThe upper half-space is the set defined by\nThe Poisson kernel for \"H\" is given by\nwhere\n\nThe Poisson kernel for the upper half-space appears naturally as the Fourier transform of the Abel kernel\nin which \"t\" assumes the role of an auxiliary parameter. To wit,\nIn particular, it is clear from the properties of the Fourier transform that, at least formally, the convolution\nis a solution of Laplace's equation in the upper half-plane. One can also show that as \"t\" → 0, \"P\"[\"u\"](\"t\",\"x\") → \"u\"(\"x\") in a suitable sense.\n\n\n"}
{"id": "1205310", "url": "https://en.wikipedia.org/wiki?curid=1205310", "title": "Practical number", "text": "Practical number\n\nIn number theory, a practical number or panarithmic number is a positive integer \"n\" such that all smaller positive integers can be represented as sums of distinct divisors of \"n\". For example, 12 is a practical number because all the numbers from 1 to 11 can be expressed as sums of its divisors 1, 2, 3, 4, and 6: as well as these divisors themselves, we have 5 = 3 + 2, 7 = 6 + 1, 8 = 6 + 2, 9 = 6 + 3, 10 = 6 + 3 + 1, and 11 = 6 + 3 + 2.\n\nThe sequence of practical numbers begins\n\nPractical numbers were used by Fibonacci in his Liber Abaci (1202) in connection with the problem of representing rational numbers as Egyptian fractions. Fibonacci does not formally define practical numbers, but he gives a table of Egyptian fraction expansions for fractions with practical denominators.\n\nThe name \"practical number\" is due to . He noted that \"the subdivisions of money, weights, and measures involve numbers like 4, 12, 16, 20 and 28 which are usually supposed to be so inconvenient as to deserve replacement by powers of 10.\" He rediscovered the number theoretical property of such numbers and was the first to attempt a classification of these numbers that was completed by and . This characterization makes it possible to determine whether a number is practical by examining its prime factorization. Every even perfect number and every power of two is also a practical number.\n\nPractical numbers have also been shown to be analogous with prime numbers in many of their properties.\n\nThe original characterisation by stated that a practical number cannot be a deficient number, that is one of which the sum of all divisors (including 1 and itself) is less than twice the number unless the deficiency is one. If the ordered set of all divisors of the practical number formula_1 is formula_2 with formula_3 and formula_4, then Srinivasan's statement can be expressed by the inequality\n\nIn other words, the ordered sequence of all divisors formula_6 of a practical number has to be a complete sub-sequence.\n\nThis partial characterization was extended and completed by and who showed that it is straightforward to determine whether a number is practical from its prime factorization.\nA positive integer greater than one with prime factorization formula_7 (with the primes in sorted order formula_8) is practical if and only if each of its prime factors formula_9 is small enough for formula_10 to have a representation as a sum of smaller divisors. For this to be true, the first prime formula_11 must equal 2 and, for every from 2 to , each successive prime formula_9 must obey the inequality\nwhere formula_14 denotes the sum of the divisors of \"x\". For example, 2 × 3 × 29 × 823 = 429606 is practical, because the inequality above holds for each of its prime factors: 3 ≤ σ(2) + 1 = 4, 29 ≤ σ(2 × 3) + 1 = 40, and 823 ≤ σ(2 × 3 × 29) + 1 = 1171.\n\nThe condition stated above is necessary and sufficient for a number to be practical. In one direction, this condition is necessary in order to be able to represent formula_10 as a sum of divisors of \"n\", because if the inequality failed to be true then even adding together all the smaller divisors would give a sum too small to reach formula_10. In the other direction, the condition is sufficient, as can be shown by induction. \nMore strongly, if the factorization of \"n\" satisfies the condition above, then any formula_17 can be represented as a sum of divisors of \"n\", by the following sequence of steps:\n\n\nSeveral other notable sets of integers consist only of practical numbers:\n\nIf \"n\" is practical, then any rational number of the form \"m\"/\"n\" with \"m\" < \"n\" may be represented as a sum ∑\"d\"/\"n\" where each \"d\" is a distinct divisor of \"n\". Each term in this sum simplifies to a unit fraction, so such a sum provides a representation of \"m\"/\"n\" as an Egyptian fraction. For instance,\n\nFibonacci, in his 1202 book \"Liber Abaci\" lists several methods for finding Egyptian fraction representations of a rational number. Of these, the first is to test whether the number is itself already a unit fraction, but the second is to search for a representation of the numerator as a sum of divisors of the denominator, as described above. This method is only guaranteed to succeed for denominators that are practical. Fibonacci provides tables of these representations for fractions having as denominators the practical numbers 6, 8, 12, 20, 24, 60, and 100.\n\nAccording to a September 2015 conjecture by Zhi-Wei Sun, every positive rational number has an Egyptian fraction representation in which every denominator is a practical number. There is a proof for the conjecture on David Eppstein's blog.\n\nOne reason for interest in practical numbers is that many of their properties are similar to properties of the prime numbers. \nIndeed, theorems analogous to Goldbach's conjecture and the twin prime conjecture are known for practical numbers: every positive even integer is the sum of two practical numbers, and there exist infinitely many triples of practical numbers \"x\" − 2, \"x\", \"x\" + 2. Melfi also showed that there are infinitely many practical Fibonacci numbers ; the analogous question of the existence of infinitely many Fibonacci primes is open. showed that there always exists a practical number in the interval [\"x\",(\"x\" + 1)] for any positive real \"x\", a result analogous to Legendre's conjecture for primes.\n\nLet \"p\"(\"x\") count how many practical numbers are at most \"x\".\n\nfor formula_33 and some constant formula_34.\n\n\n"}
{"id": "15889648", "url": "https://en.wikipedia.org/wiki?curid=15889648", "title": "Proof by intimidation", "text": "Proof by intimidation\n\nProof by intimidation (or argumentum verbosium) is a jocular phrase used mainly in mathematics to refer to a style of presenting a purported mathematical proof by giving an argument loaded with jargon and appeal to obscure results, so that the audience is simply obliged to accept it, lest they have to admit their ignorance and lack of understanding.\n\nThe phrase is also used when the author is an authority in their field presenting their proof to people who respect \"a priori\" the author's insistence that the proof is valid or when the author claims that their statement is true because it is trivial or because they simply say so. Usage of this phrase is for the most part in good humour, though it also appears in serious criticism.\n\n\"Proof by intimidation\" is also cited by critics of junk science to describe cases in which scientific evidence is thrown aside in favour of a litany of tragic individual cases presented to the public by articulate advocates who pose as experts in their field.\n\nGian-Carlo Rota claimed in a memoir that the expression \"proof by intimidation\" was coined by Mark Kac to describe a technique used by William Feller in his lectures.\n\n"}
{"id": "5806643", "url": "https://en.wikipedia.org/wiki?curid=5806643", "title": "Quasitopological space", "text": "Quasitopological space\n\nIn mathematics, a quasi-topology on a set \"X\" is a function that associates to every compact Hausdorff space \"C\" a collection of mappings from \"C\" to \"X\" satisfying certain natural conditions. A set with a quasi-topology is called a quasitopological space\n\nThey were introduced by Spanier, who showed that there is a natural quasi-topology on the space of continuous maps from one space to another.\n"}
{"id": "1548091", "url": "https://en.wikipedia.org/wiki?curid=1548091", "title": "Rafael Bombelli", "text": "Rafael Bombelli\n\nRafael Bombelli (baptised on 20 January 1526; died 1572) was an Italian mathematician.\n\nBorn in Bologna, he is the author of a treatise on algebra and is a central figure in the understanding of imaginary numbers.\n\nHe was the one who finally managed to address the problem with imaginary numbers. In his 1572 book, \"L'Algebra\", Bombelli solved equations using the method of del Ferro/Tartaglia. He introduced the rhetoric that preceded the representative symbols +\"i\" and -\"i\" and described how they both worked.\n\nThe lunar crater Bombelli is named after him.\n\nRafael Bombelli was baptised on 20 January 1526 in Bologna, Papal States. He was born to Antonio Mazzoli, a wool merchant, and Diamante Scudieri, a tailor's daughter. The Mazzoli family was once quite powerful in Bologna. When Pope Julius II came to power, in 1506, he exiled the ruling family, the Bentivoglios. The Bentivoglio family attempted to retake Bologna in 1508, but failed. Rafael's grandfather participated in the coup attempt, and was captured and executed. Later, Antonio was able to return to Bologna, having changed his surname to Bombelli to escape the reputation of the Mazzoli family. Rafael was the oldest of six children. Rafael received no college education, but was instead taught by an engineer-architect by the name of Pier Francesco Clementi.\n\nRafael Bombelli felt that none of the works on algebra by the leading mathematicians of his day provided a careful and thorough exposition of the subject. Instead of another convoluted treatise that only mathematicians could comprehend, Rafael decided to write a book on algebra that could be understood by anyone. His text would be self-contained and easily read by those without higher education.\n\nRafael Bombelli died in 1572 in Rome, Italy.\n\nIn the book that was published in 1572, entitled \"Algebra\", Bombelli gave a comprehensive account of the algebra known at the time. He was the first European to write down the way of performing computations with negative numbers. The following is an excerpt from the text:\n\nAs was intended, Bombelli used simple language as can be seen above so that anybody could understand it. But at the same time, he was thorough.\n\nPerhaps more importantly than his work with algebra, however, the book also includes Bombelli's monumental contributions to complex number theory. Before he writes about complex numbers, he points out that they occur in solutions of equations of the form formula_1 given that formula_2 which is another way of stating that the discriminant of the cubic is negative. The solution of this kind of equation requires taking the cube root of the sum of one number and the square root of some negative number.\n\nBefore Bombelli delves into using imaginary numbers practically, he goes into a detailed explanation of the properties of complex numbers. Right away, he makes it clear that the rules of arithmetic for imaginary numbers are not the same as for real numbers. This was a big accomplishment, as even numerous subsequent mathematicians were extremely confused on the topic.\n\nBombelli avoided confusion by giving a special name to square roots of negative numbers, instead of just trying to deal with them as regular radicals like other mathematicians did. This made it clear that these numbers were neither positive nor negative. This kind of system avoids the confusion that Euler encountered. Bombelli called the imaginary number \"i\" “plus of minus” and used “minus of minus” for -\"i\".\n\nBombelli had the foresight to see that imaginary numbers were crucial and necessary to solving quartic and cubic equations. At the time, people cared about complex numbers only as tools to solve practical equations. As such, Bombelli was able to get solutions using Scipione del Ferro's rule, even in the irreducible case, where other mathematicians such as Cardano had given up.\n\nIn his book, Bombelli explains complex arithmetic as follows:\n\nAfter dealing with the multiplication of real and imaginary numbers, Bombelli goes on to talk about the rules of addition and subtraction. He is careful to point out that real parts add to real parts, and imaginary parts add to imaginary parts.\n\nBombelli is generally regarded as the inventor of complex numbers, as no one before him had made rules for dealing with such numbers, and no one believed that working with imaginary numbers would have useful results. Upon reading Bombelli's \"Algebra\", Leibniz praised Bombelli as an \". . . outstanding master of the analytical art.\" Crossley writes in his book, \"Thus we have an engineer, Bombelli, making practical use of complex numbers perhaps because they gave him useful results, while Cardan found the square roots of negative numbers useless. Bombelli is the first to give a treatment of any complex numbers. . . It is remarkable how thorough he is in his presentation of the laws of calculation of complex numbers. . .\"[3]\n\nIn honor of his accomplishments, a moon crater was named Bombelli.\n\nBombelli used a method related to continued fractions to calculate square roots. His method for finding formula_3 begins with formula_4 with formula_5, from which it can be shown that formula_6. Repeated substitution of the expression on the right hand side for formula_7 into itself yields a continued fraction\n\nfor the root but Bombelli is more concerned with better approximations for formula_7. The value chosen for formula_10 is either of the whole numbers whose squares formula_11 lies between. The method gives the following convergents for formula_12 while the actual value is 3.605551275... :\n\nThe last convergent equals 3.605550883... . Bombelli's method should be compared with formulas and results used by Heros and Archimedes. The result formula_14 used by Archimedes in his determination of the value of formula_15 can be found by using 1 and 0 for the initial values of formula_7.\n\n\n\n"}
{"id": "18673900", "url": "https://en.wikipedia.org/wiki?curid=18673900", "title": "Riesz transform", "text": "Riesz transform\n\nIn the mathematical theory of harmonic analysis, the Riesz transforms are a family of generalizations of the Hilbert transform to Euclidean spaces of dimension \"d\" > 1. They are a type of singular integral operator, meaning that they are given by a convolution of one function with another function having a singularity at the origin. Specifically, the Riesz transforms of a complex-valued function ƒ on R are defined by\nfor \"j\" = 1,2...,\"d\". The constant \"c\" is a dimensional normalization given by\n\nwhere ω is the volume of the (\"d\" − 1)-ball. The limit is written in various ways, often as a principal value, or as a convolution with the tempered distribution\n\nIn this form, the Riesz transforms are seen to be generalizations of the Hilbert transform. The kernel is a distribution which is homogeneous of degree zero. A particular consequence of this last observation is that the Riesz transform defines a bounded linear operator from \"L\"(R) to itself.\n\nThis homogeneity property can also be stated more directly without the aid of the Fourier transform. If σ is the dilation on R by the scalar \"s\", that is σ\"x\" = \"sx\", then σ defines an action on functions via pullback:\n\nThe Riesz transforms commute with σ:\n\nSimilarly, the Riesz transforms commute with translations. Let τ be the translation on R along the vector \"a\"; that is, τ(\"x\") = \"x\" + \"a\". Then\n\nFor the final property, it is convenient to regard the Riesz transforms as a single vectorial entity \"R\"ƒ = (\"R\"ƒ,…,\"R\"ƒ). Consider a rotation ρ in R. The rotation acts on spatial variables, and thus on functions via pullback. But it also can act on the spatial vector \"R\"ƒ. The final transformation property asserts that the Riesz transform is equivariant with respect to these two actions; that is,\n\nThese three properties in fact characterize the Riesz transform in the following sense. Let \"T\"=(\"T\",…,\"T\") be a \"d\"-tuple of bounded linear operators from \"L\"(R) to \"L\"(R) such that\n\n\nThen, for some constant \"c\", \"T\" = \"cR\".\n\nSomewhat imprecisely, the Riesz transforms of ƒ give the first partial derivatives of a solution of the equation\n\nwhere Δ is the Laplacian. Thus the Riesz transform of ƒ can be written as:\n\nIn particular, one should also have\nso that the Riesz transforms give a way of recovering information about the entire hessian of a function from knowledge of only its Laplacian.\n\nThis is now made more precise. Suppose that \"u\" is a Schwartz function. Then indeed by the explicit form of the Fourier multiplier, one has\nThe identity is not generally true in the sense of distributions. For instance, if \"u\" is a tempered distribution such that Δ\"u\" ∈ \"L\"(R), then one can only conclude that\nfor some polynomial \"P\".\n\n\n"}
{"id": "33459997", "url": "https://en.wikipedia.org/wiki?curid=33459997", "title": "Siddhānta Shiromani", "text": "Siddhānta Shiromani\n\nSiddhānta Śiromani (Sanskrit: सिद्धांत शिरोमणी for \"Crown of treatises\") is the major treatise of Indian mathematician Bhāskara II. He wrote the \"Siddhānta Śiromani\" in 1150 when he was 36 years old. The work is composed in Sanskrit Language in 1450 verses.\n\nThe name of the book comes from his daughter, Leelāvati. It is the first volume of the \"Siddhānta Śiromani\". The book contains thirteen chapters, 278 verses, mainly arithmetic and measurement.\n\nIt is the second volume of \"Siddhānta Shiromani\". It is divided into six parts, contains 213 verses and is devoted to algebra.\n\nGanitadhyaya and Goladhyaya of Siddhanta Shiromani are devoted to astronomy. All put together there are about 900 verses.(Ganitadhyaya has 451 and Goladhyaya has 501 verses).\n\n"}
{"id": "9428917", "url": "https://en.wikipedia.org/wiki?curid=9428917", "title": "Snub (geometry)", "text": "Snub (geometry)\n\nIn geometry, a snub is an operation applied to a polyhedron. The term originates from Kepler's names of two Archimedean solids, for the snub cube (cubus simus) and snub dodecahedron (dodecaedron simum). In general, snubs have chiral symmetry with two forms, with clockwise or counterclockwise orientations. By Kepler's names, a snub can be seen as an expansion of a regular polyhedron, with the faces moved apart, and twists on their centers, adding new polygons centered on the original vertices, and pairs of triangles fitting between the original edges.\n\nThe terminology was generalized by Coxeter, with a slightly different definition, for a wider set of uniform polytopes.\n\nJohn Conway explored generalized polyhedron operators, defining what is now called Conway polyhedron notation, which can be applied to polyhedra and tilings. Conway calls Coxeter's operation a \"semi-snub\".\n\nIn this notation, snub is defined by the dual and gyro operators, as \"s\" = \"dg\", and it is equivalent to an alternation of a truncation of an ambo operator. Conway's notation itself avoids Coxeter's alternation (half) operation since it only applies for polyhedra with only even-sided faces.\n\nIn 4-dimensions, Conway suggests the snub 24-cell should be called a \"semi-snub 24-cell\" because it doesn't represent an alternated omnitruncated 24-cell like his 3-dimensional polyhedron usage. It is instead actually an alternated truncated 24-cell.\n\nCoxeter's snub terminology is slightly different, meaning an alternated truncation, deriving the snub cube as a \"snub cuboctahedron\", and the snub dodecahedron as a \"snub icosidodecahedron\". This definition is used in the naming two Johnson solids: snub disphenoid, and snub square antiprism, as well as higher dimensional polytopes such as the 4-dimensional snub 24-cell, or s{3,4,3}.\n\nA regular polyhedron (or tiling) with Schläfli symbol, formula_1, and Coxeter diagram , has truncation defined as formula_2, and and snub defined as an alternated truncation formula_3, and Coxeter diagram . This construction requires \"q\" to be even.\n\nA quasiregular polyhedron formula_4 or \"r\"{\"p\",\"q\"}, with Coxeter diagram or has a quasiregular truncation defined as formula_5 or \"tr\"{\"p\",\"q\"}, and Coxeter diagram or and quasiregular snub defined as an alternated truncated rectification formula_6 or \"htr\"{\"p\",\"q\"} = \"sr\"{\"p\",\"q\"}, and Coxeter diagram or .\n\nFor example, Kepler's snub cube is derived from the quasiregular cuboctahedron, with a vertical Schläfli symbol formula_7, and Coxeter diagram , and so is more explicitly called a snub cuboctahedron, expressed by a vertical Schläfli symbol formula_8 and Coxeter diagram . The snub cuboctahedron is the alternation of the \"truncated cuboctahedron\", formula_9 and .\n\nRegular polyhedra with even-order vertices to also be snubbed as alternated trunction, like a \"snub octahedron\", formula_10, (and \"snub tetratetrahedron\", as formula_11, ) represents the pseudoicosahedron, a regular icosahedron with pyritohedral symmetry. The \"snub octahedron\" is the alternation of the truncated octahedron, formula_12 and , or tetrahedral symmetry form: formula_13 and .\nCoxeter's snub operation also allows n-antiprisms to be defined as formula_14 or formula_15, based on n-prisms formula_16 or formula_17, while formula_18 is a regular n-hosohedron, a degenerate polyhedron, but a valid tiling on the sphere with digon or lune-shaped faces.\nThe same process applies for snub tilings:\n\nNonuniform polyhedra with all even-valance vertices can be snubbed, including some infinite sets, for example:\n\nSnub star-polyhedra are constructed by their Schwarz triangle (p q r), with rational ordered mirror-angles, and all mirrors active and alternated.\n\nIn general, a regular polychora with Schläfli symbol, formula_19, and Coxeter diagram , has a snub with extended Schläfli symbol formula_20, and .\n\nA rectified polychora formula_21 = r{p,q,r}, and has snub symbol formula_22 = sr{p,q,r}, and .\n\nThere is only one uniform snub in 4-dimensions, the snub 24-cell. The regular 24-cell has Schläfli symbol, formula_23, and Coxeter diagram , and the snub 24-cell is represented by formula_24, Coxeter diagram . It also has an index 6 lower symmetry constructions as formula_25 or s{3} and , and an index 3 subsymmetry as formula_26 or sr{3,3,4}, and or .\n\nThe related snub 24-cell honeycomb can be seen as a formula_27 or s{3,4,3,3}, and , and lower symmetry formula_28 or sr{3,3,4,3} and or , and lowest symmetry form as formula_29 or s{3} and .\n\nA Euclidean honeycomb is an alternated hexagonal slab honeycomb, s{2,6,3}, and or sr{2,3,6}, and or sr{2,3}, and .\n\nAnother Euclidean (scaliform) honeycomb is an alternated square slab honeycomb, s{2,4,4}, and or sr{2,4} and :\n\nThe only uniform snub hyperbolic uniform honeycomb is the \"snub hexagonal tiling honeycomb\", as s{3,6,3} and , which can also be constructed as an alternated hexagonal tiling honeycomb, h{6,3,3}, . It is also constructed as s{3} and .\n\nAnother hyperbolic (scaliform) honeycomb is an snub order-4 octahedral honeycomb, s{3,4,4}, and .\n\n\n"}
{"id": "10393861", "url": "https://en.wikipedia.org/wiki?curid=10393861", "title": "Spherical polyhedron", "text": "Spherical polyhedron\n\nIn mathematics, a spherical polyhedron or spherical tiling is a tiling of the sphere in which the surface is divided or partitioned by great arcs into bounded regions called spherical polygons. Much of the theory of symmetrical polyhedra is most conveniently derived in this way.\n\nThe most familiar spherical polyhedron is the soccer ball (outside the United States, Canada, and Australia, a football), thought of as a spherical truncated icosahedron. The next most popular spherical polyhedron is the beach ball, thought of as a hosohedron. \n\nSome \"improper\" polyhedra, such as the hosohedra and their duals the dihedra, exist as spherical polyhedra but have no flat-faced analogue. In the examples below, {2, 6} is a hosohedron and {6, 2} is the dual dihedron.\n\nThe first known man-made polyhedra are spherical polyhedra carved in stone. Many have been found in Scotland, and appear to date from the neolithic period (the New Stone Age).\n\nDuring the 10th Century, the Islamic scholar Abū al-Wafā' Būzjānī (Abu'l Wafa) wrote the first serious study of spherical polyhedra.\n\nTwo hundred years ago, at the start of the 19th Century, Poinsot used spherical polyhedra to discover the four regular star polyhedra.\n\nIn the middle of the 20th Century, Coxeter used them to enumerate all but one of the uniform polyhedra, through the construction of kaleidoscopes (Wythoff construction).\n\nAll the regular, semiregular polyhedra and their duals can be projected onto the sphere as tilings. Given by their Schläfli symbol {p, q} or vertex figure a.b.c. ...:\n\nSpherical tilings allow cases that polyhedra do not, namely the hosohedra, regular figures as {2,n}, and dihedra, regular figures as {n,2}.\n\nSpherical polyhedra having at least one inversive symmetry are related to projective polyhedra (tessellations of the real projective plane) – just as the sphere has a 2-to-1 covering map of the projective plane, projective polyhedra correspond under 2-fold cover to spherical polyhedra that are symmetric under reflection through the origin.\n\nThe best-known examples of projective polyhedra are the regular projective polyhedra, the quotients of the centrally symmetric Platonic solids, as well as two infinite classes of even dihedra and hosohedra:\n\n\n"}
{"id": "324806", "url": "https://en.wikipedia.org/wiki?curid=324806", "title": "Sporadic group", "text": "Sporadic group\n\nIn group theory, a sporadic group is one of the 26 exceptional groups found in the classification of finite simple groups.\n\nA simple group is a group \"G\" that does not have any normal subgroups except for the trivial group and \"G\" itself. The classification theorem states that the list of finite simple groups consists of 18 countably infinite families, plus 26 exceptions that do not follow such a systematic pattern. These are the sporadic groups. They are also known as the sporadic simple groups, or the sporadic finite groups. Because it is not strictly a group of Lie type, the Tits group is sometimes regarded as a sporadic group, in which case the sporadic groups number 27.\n\nThe monster group is the largest of the sporadic groups and contains all but six of the other sporadic groups as subgroups or subquotients.\n\nFive of the sporadic groups were discovered by Mathieu in the 1860s and the other 21 were found between 1965 and 1975. Several of these groups were predicted to exist before they were constructed. Most of the groups are named after the mathematician(s) who first predicted their existence. The full list is:\n\n\nThe Tits group \"T\" is sometimes also regarded as a sporadic group (it is almost but not strictly a group of Lie type), which is why in some sources the number of sporadic groups is given as 27 instead of 26. In some other sources, the Tits group is regarded as neither sporadic nor of Lie type. Anyway, it is the of the infinite family of commutator groups all of them finite simple groups. For \"n\">0 they coincide with the groups of Lie type But for the derived subgroup , called Tits group, has an index 2 in the group of Lie type.\n\nMatrix representations over finite fields for all the sporadic groups have been constructed.\n\nThe earliest use of the term \"sporadic group\" may be where he comments about the Mathieu groups: \"These apparently sporadic simple groups would probably repay a closer examination than they have yet received\".\n\nThe diagram on the right is based on the diagram given in . The sporadic groups also have a lot of subgroups which are not sporadic but these are not shown on the diagram because they are too numerous.\n\nOf the 26 sporadic groups, 20 can be seen inside the Monster group as subgroups or quotients of subgroups (sections).\n\nThe six exceptions are \"J\", \"J\", \"J\", \"O'N\", \"Ru\" and \"Ly\". These six are sometimes known as the pariahs.\n\nThe remaining twenty have been called the \"Happy Family\" by Robert Griess, and can be organized into three generations.\n\nM for \"n\" = 11, 12, 22, 23 and 24 are multiply transitive permutation groups on \"n\" points. They are all subgroups of M, which is a permutation group on 24 points.\n\nAll the subquotients of the automorphism group of a lattice in 24 dimensions called the Leech lattice:\n\nConsists of subgroups which are closely related to the Monster group \"M\":\n\nThe Tits group also belongs in this generation: there is a subgroup S ×F(2)′ normalising a 2C subgroup of \"B\", giving rise to a subgroup\n2·S ×F(2)′ normalising a certain Q subgroup of the Monster.\nF(2)′ is also a subgroup of the Fischer groups \"Fi\", \"Fi\" and \"Fi\"′, and of the Baby Monster \"B\".\nF(2)′ is also a subgroup of the (pariah) Rudvalis group \"Ru\", and has\nno involvements in sporadic simple groups except the containments we have already mentioned.\n\n"}
{"id": "10963217", "url": "https://en.wikipedia.org/wiki?curid=10963217", "title": "Subtract a square", "text": "Subtract a square\n\nSubtract-a-square (also referred to as take-a-square) is a two-player mathematical game of strategy. It is played by two people with a pile of coins (or other tokens) between them. The players take turns removing coins from the pile, always removing a non-zero square number of coins. The game is usually played as a \"normal play\" game, which means that the player who removes the last coin wins. It is an impartial game, meaning that the set of moves available from any position does not depend on whose turn it is. Solomon W. Golomb credits the invention of this game to Richard A. Epstein.\n\nA normal play game starting with 13 coins is a win for the first player provided (s)he starts with a subtraction of 1:\nPlayer 2 now has three choices: subtract 1, 4 or 9. In each of these cases, player 1 can ensure that within a few moves the number 2 gets passed on to player 2: \n\nNow player 2 has to subtract 1, and player 1 subsequently does the same:\n\nIn the above example, the number '13' represents a winning or 'hot' position, whilst the number '2' represents a losing or 'cold' position. Given an integer list with each integer labeled 'hot' or 'cold', the strategy of the game is simple: try to pass on a 'cold' number to your opponent. This is always possible provided you are being presented a 'hot' number. Which numbers are 'hot' and which numbers are 'cold' can be determined recursively:\n\nUsing this algorithm, a list of cold numbers is easily derived:\nA faster divide and conquer algorithm can compute the same sequence of numbers, up to any threshold formula_1, in time formula_2.\n\nThere are infinitely many cold numbers. More strongly, the number of cold numbers up to some threshold formula_1 must be at least proportional to the square root of formula_1, for otherwise there would not be enough of them to provide winning moves from all the hot numbers.\nCold numbers tend to end in 0, 2, 4, 5, 7, or 9. Cold values that end with other digits are quite uncommon. This holds in particular for cold numbers ending in 6. Out of all the over 180,000 cold numbers less than 40 million, only one ends in a 6: 11,356. \n\nNo two cold numbers can differ by a square, because if they did then a move from the larger of the two to the smaller would be winning, contradicting the assumption that they are both cold. Therefore, by the Furstenberg–Sárközy theorem, the natural density of the cold numbers is zero. That is, for every formula_5, and for all sufficiently large formula_1, the fraction of the numbers up to formula_1 that are cold is less than formula_8.\nMore strongly, for every formula_1 there are\ncold numbers up to The exact growth rate of the cold numbers remains unknown, but experimentally the number of cold positions up to any given threshold formula_1 appears to be roughly formula_12.\n\nThe game subtract-a-square can also be played with multiple numbers. At each turn the player to make a move first selects one of the numbers, and then subtracts a square from it. Such a 'sum of normal games' can be analysed using the Sprague–Grundy theorem. This theorem states that each position in the game subtract-a-square may be mapped onto an equivalent nim heap size. Optimal play consists of moving to a collection of numbers such that the nim-sum of their equivalent nim heap sizes is zero, when this is possible. The equivalent nim heap size of a position may be calculated as the minimum excluded value of the equivalent sizes of the positions that can be reached by a single move.\nFor subtract-a-square positions of values 0, 1, 2, ... the equivalent nim heap sizes are\nIn particular, a position of subtract-a-square is cold if and only if its equivalent nim heap size is zero.\n\nIt is also possible to play variants of this game using other allowed moves than the square numbers. For instance, Golomb defined an analogous game based on the Moser–de Bruijn sequence, a sequence that grows at a similar asymptotic rate to the squares, for which it is possible to determine more easily the set of cold positions and to define an easily computed optimal move strategy.\n\nSubtract-a-square can also be played as a \"misère\" game, in which the player to make the last subtraction loses. The recursive algorithm to determine 'hot' and 'cold' numbers for the misère game is the same as that for the normal game, except that for the misère game the number 1 is 'cold' whilst 2 is 'hot'. It follows that the cold numbers for the misère variant are the cold numbers for the normal game shifted by 1:\n\n"}
{"id": "47404029", "url": "https://en.wikipedia.org/wiki?curid=47404029", "title": "Trachette Jackson", "text": "Trachette Jackson\n\nTrachette Levon Jackson (born July 24, 1972) is an African-American mathematician who works as a professor of mathematics at the University of Michigan and is known for work in mathematical oncology. She uses many different approaches, including continuous and discrete mathematical models, numerical simulations, and experiments to study tumor growth and treatment. Specifically, her lab is interested in \"molecular pathways associated with intratumoral angiogenesis\", \"cell-tissue interactions associated with tumor-induced angiogenesis,\" and \"tumor heterogeneity and cancer stem cells\".\n\nJackson's parents were in the military and traveled frequently through her childhood; as a teenager, she lived in Mesa, Arizona. There, in a summer calculus course, her talent for mathematics brought her to the attention of Arizona State University mathematics professor Joaquín Bustoz, Jr. She went on to undergraduate studies at ASU, originally intending to study engineering, but steered to mathematics by Bustoz. From there, her interest in pure math developed into an interest in mathematical biology when she attended a talk by her future PhD advisor, James D. Murray, on the mathematics of pattern formation and, \"how the leopard got its spots.\" She graduated in 1994, and earned her master's and Ph.D. at the University of Washington in 1996 and 1998.\nAfter postdoctoral research at the University of Minnesota, Environmental Protection Agency, and Duke University, she joined the Michigan faculty in 2000, and was promoted to full professor in 2008.\n\nShe was awarded a Sloan Research Fellowship in 2003, becoming the second African-American woman to become a Sloan Fellow in mathematics.\nShe won the James S. McDonnell 21st Century Scientist Award in 2005, and won the Blackwell-Tapia Prize in 2010. In 2017, she was selected as a fellow of the Association for Women in Mathematics in the inaugural class.\n"}
{"id": "39817", "url": "https://en.wikipedia.org/wiki?curid=39817", "title": "William Rowan Hamilton", "text": "William Rowan Hamilton\n\nSir William Rowan Hamilton MRIA (4 August 1805 – 2 September 1865) was an Irish mathematician. While still an undergraduate he was appointed Andrews professor of Astronomy and Royal Astronomer of Ireland, and lived at Dunsink Observatory. He made important contributions to optics, classical mechanics and algebra. Although Hamilton was not a physicist–he regarded himself as a pure mathematician–his work was of major importance to physics, particularly his reformulation of Newtonian mechanics, now called Hamiltonian mechanics. This work has proven central to the modern study of classical field theories such as electromagnetism, and to the development of quantum mechanics. In pure mathematics, he is best known as the inventor of quaternions.\n\nHamilton is said to have shown immense talent at a very early age. Astronomer Bishop Dr. John Brinkley remarked of the 18-year-old Hamilton, 'This young man, I do not say \"will be\", but \"is\", the first mathematician of his age.'\n\nWilliam Rowan Hamilton's scientific career included the study of geometrical optics, classical mechanics, adaptation of dynamic methods in optical systems, applying quaternion and vector methods to problems in mechanics and in geometry, development of theories of conjugate algebraic couple functions (in which complex numbers are constructed as ordered pairs of real numbers), solvability of polynomial equations and general quintic polynomial solvable by radicals, the analysis on Fluctuating Functions (and the ideas from Fourier analysis), linear operators on quaternions and proving a result for linear operators on the space of quaternions (which is a special case of the general theorem which today is known as the \"Cayley–Hamilton theorem\"). Hamilton also invented \"icosian calculus\", which he used to investigate closed edge paths on a dodecahedron that visit each vertex exactly once.\n\nHamilton was the fourth of nine children born to Sarah Hutton (1780–1817) and Archibald Hamilton (1778–1819), who lived in Dublin at 29 Dominick Street, later renumbered to 36. Hamilton's father, who was from Dublin, worked as a solicitor. By the age of three, Hamilton had been sent to live with his uncle James Hamilton, a graduate of Trinity College who ran a school in Talbots Castle in Trim, Co. Meath.\n\nHis uncle soon discovered that Hamilton had a remarkable ability to learn languages, and from a young age, had displayed an uncanny ability to acquire them (although this is disputed by some historians, who claim he had only a very basic understanding of them). At the age of seven, he had already made very considerable progress in Hebrew, and before he was thirteen he had acquired, under the care of his uncle (a linguist), almost as many languages as he had years of age. These included the classical and modern European languages, and Persian, Arabic, Hindustani, Sanskrit, and even Marathi and Malay. He retained much of his knowledge of languages to the end of his life, often reading Persian and Arabic in his spare time, although he had long since stopped studying languages, and used them just for relaxation.\n\nIn September 1813, the American calculating prodigy Zerah Colburn was being exhibited in Dublin. Colburn was 9, a year older than Hamilton. The two were pitted against each other in a mental arithmetic contest with Colburn emerging the clear victor. In reaction to his defeat, Hamilton dedicated less time to studying languages and more time to studying mathematics.\n\nHamilton was part of a small but well-regarded school of mathematicians associated with Trinity College in Dublin, which he entered at age 18. The college awarded him two Optimes, or off-the-chart grades. He studied both classics and mathematics, and was appointed Professor of Astronomy just prior to his graduation (BA, 1827, he was awarded MA in 1837). He then took up residence at Dunsink Observatory where he spent the rest of his life.\n\nWhile attending Trinity College, Hamilton proposed to his friend's sister, who rejected him. Hamilton, being a sensitive young man, became sick and depressed, and almost committed suicide. He was rejected again in 1831 by Aubrey De Vere (1814-1902). Luckily, Hamilton found a woman who would accept his proposal. She was Helen Marie Bayly, a country preacher's daughter, and they married in 1833. Hamilton had three children with Bayly: William Edwin Hamilton (born 1834), Archibald Henry (born 1835), and Helen Elizabeth (born 1840). Hamilton's married life turned out to be difficult and unhappy as Bayly proved to be pious, shy, timid, and chronically ill.\n\nHamilton made important contributions to optics and to classical mechanics. His first discovery was in an early paper that he communicated in 1823 to Dr. Brinkley, who presented it under the title of \"\"Caustics\" in 1824 to the Royal Irish Academy. It was referred as usual to a committee. While their report acknowledged its novelty and value, they recommended further development and simplification before publication. Between 1825 and 1828 the paper grew to an immense size, mostly by the additional details that the committee had suggested. But it also became more intelligible, and the features of the new method were now easily seen. Until this period Hamilton himself seems not to have fully understood either the nature or importance of optics, as later he intended to apply his method to dynamics.\n\nIn 1827, Hamilton presented a theory of a single function, now known as Hamilton's principal function, that brings together mechanics, optics, and mathematics, and which helped to establish the wave theory of light. He proposed it when he first predicted its existence in the third supplement to his \"Systems of Rays\", read in 1832. The Royal Irish Academy paper was finally entitled \"Theory of Systems of Rays\" (23 April 1827), and the first part was printed in 1828 in the \"Transactions of the Royal Irish Academy\". The more important contents of the second and third parts appeared in the three voluminous supplements (to the first part) which were published in the same Transactions, and in the two papers \"On a General Method in Dynamics\", which appeared in the Philosophical Transactions in 1834 and 1835. In these papers, Hamilton developed his great principle of \"Varying Action\". The most remarkable result of this work is the prediction that a single ray of light entering a biaxial crystal at a certain angle would emerge as a hollow cone of rays. This discovery is still known by its original name, \"conical refraction\".\n\nThe step from optics to dynamics in the application of the method of \"Varying Action\" was made in 1827, and communicated to the Royal Society, in whose \"Philosophical Transactions\" for 1834 and 1835 there are two papers on the subject, which, like the \"Systems of Rays\", display a mastery over symbols and a flow of mathematical language almost unequaled. The common thread running through all this work is Hamilton's principle of \"Varying Action\". Although it is based on the calculus of variations and may be said to belong to the general class of problems included under the principle of least action which had been studied earlier by Pierre Louis Maupertuis, Euler, Joseph Louis Lagrange, and others, Hamilton's analysis revealed much deeper mathematical structure than had been previously understood, in particular the symmetry between momentum and position. Paradoxically, the credit for discovering the quantity now called the Lagrangian and Lagrange's equations belongs to Hamilton. Hamilton's advances enlarged greatly the class of mechanical problems that could be solved, and they represent perhaps the greatest addition which dynamics had received since the work of Isaac Newton and Lagrange. Many scientists, including Liouville, Jacobi, Darboux, Poincaré, Kolmogorov, and Arnold, have extended Hamilton's work, thereby expanding our knowledge of mechanics and differential equations.\n\nWhile Hamilton's reformulation of classical mechanics is based on the same physical principles as the mechanics of Newton and Lagrange, it provides a powerful new technique for working with the equations of motion. More importantly, both the Lagrangian and Hamiltonian approaches, which were initially developed to describe the motion of discrete systems, have proven critical to the study of continuous classical systems in physics, and even quantum mechanical systems. In this way, the techniques find use in electromagnetism, quantum mechanics, quantum relativity theory, and quantum field theory.\n\nHamilton's mathematical studies seem to have been undertaken and carried to their full development without any assistance whatsoever, and the result is that his writings do not belong to any particular \"school\". Not only was Hamilton an expert as an arithmetic calculator, but he seems to have occasionally had fun in working out the result of some calculation to an enormous number of decimal places. At the age of eight Hamilton engaged Zerah Colburn, the American \"calculating boy\"\", who was then being exhibited as a curiosity in Dublin. Two years later, aged ten, Hamilton stumbled across a Latin copy of Euclid, which he eagerly devoured; and at twelve he studied Newton's \"Arithmetica Universalis\". This was his introduction to modern analysis. Hamilton soon began to read the \"Principia\", and at sixteen Hamilton had mastered a great part of it, as well as some more modern works on analytical geometry and the differential calculus.\n\nAround this time Hamilton was also preparing to enter Trinity College, Dublin, and therefore had to devote some time to classics. In mid-1822 he began a systematic study of Laplace's \"Mécanique Céleste\".\n\nFrom that time Hamilton appears to have devoted himself almost wholly to mathematics, though he always kept himself well acquainted with the progress of science both in Britain and abroad. Hamilton found an important defect in one of Laplace's demonstrations, and he was induced by a friend to write out his remarks, so that they could be shown to Dr. John Brinkley, then the first Royal Astronomer of Ireland, and an accomplished mathematician. Brinkley seems to have immediately perceived Hamilton's talents, and to have encouraged him in the kindest way.\n\nHamilton's career at College was perhaps unexampled. Amongst a number of extraordinary competitors, he was first in every subject and at every examination. He achieved the rare distinction of obtaining an optime both for Greek and for physics. Hamilton might have attained many more such honours (he was expected to win both the gold medals at the degree examination), if his career as a student had not been cut short by an unprecedented event. This was Hamilton's appointment to the Andrews Professorship of Astronomy in the University of Dublin, vacated by Dr. Brinkley in 1827. The chair was not exactly offered to him, as has been sometimes asserted, but the electors, having met and talked over the subject, authorised Hamilton's personal friend (also an elector) to urge Hamilton to become a candidate, a step which Hamilton's modesty had prevented him from taking. Thus, when barely 22, Hamilton was established at the Dunsink Observatory, near Dublin.\n\nHamilton was not especially suited for the post, because although he had a profound acquaintance with theoretical astronomy, he had paid little attention to the regular work of the practical astronomer. Hamilton's time was better employed in original investigations than it would have been spent in observations made even with the best of instruments. Hamilton was intended by the university authorities who elected him to the professorship of astronomy to spend his time as he best could for the advancement of science, without being tied down to any particular branch. If Hamilton had devoted himself to practical astronomy, the University of Dublin would assuredly have furnished him with instruments and an adequate staff of assistants.\n\nHe was twice awarded the Cunningham Medal of the Royal Irish Academy. The first award, in 1834, was for his work on conical refraction, for which he also received the Royal Medal of the Royal Society the following year. He was to win it again in 1848.\n\nIn 1835, being secretary to the meeting of the British Association which was held that year in Dublin, he was knighted by the lord-lieutenant. Other honours rapidly succeeded, among which his election in 1837 to the president's chair in the Royal Irish Academy, and the rare distinction of being made a corresponding member of the Saint Petersburg Academy of Sciences. Later, in 1864, the newly established United States National Academy of Sciences elected its first Foreign Associates, and decided to put Hamilton's name on top of their list.\n\nThe other great contribution Hamilton made to mathematical science was his discovery of quaternions in 1843. However, in 1840, Benjamin Olinde Rodrigues had already reached a result that amounted to their discovery in all but name.\n\nHamilton was looking for ways of extending complex numbers (which can be viewed as points on a 2-dimensional plane) to higher spatial dimensions. \nHe failed to find a useful 3-dimensional system (in modern terminology, he failed to find a real, three-dimensional skew-field), but in working with four dimensions he created quaternions. According to Hamilton, on 16 October he was out walking along the Royal Canal in Dublin with his wife when the solution in the form of the equation\n\nsuddenly occurred to him; Hamilton then promptly carved this equation using his penknife into the side of the nearby Broom Bridge (which Hamilton called Brougham Bridge). This event marks the discovery of the quaternion group.\n\nA plaque under the bridge was unveiled by the Taoiseach Éamon de Valera, himself a mathematician and student of quaternions, on 13 November 1958. Since 1989, the National University of Ireland, Maynooth has organised a pilgrimage called the \"Hamilton Walk\", in which mathematicians take a walk from Dunsink Observatory to the bridge, where no trace of the carving remains, though a stone plaque does commemorate the discovery.\n\nThe quaternion involved abandoning commutativity, a radical step for the time. Not only this, but Hamilton had in a sense invented the cross and dot products of vector algebra. Hamilton also described a quaternion as an ordered four-element multiple of real numbers, and described the first element as the 'scalar' part, and the remaining three as the 'vector' part.\n\nHamilton introduced, as a method of analysis, both quaternions and biquaternions, the extension to eight dimensions by introduction of complex number coefficients. When his work was assembled in 1853, the book \"Lectures on Quaternions\" had \"formed the subject of successive courses of lectures, delivered in 1848 and subsequent years, in the Halls of Trinity College, Dublin\". Hamilton confidently declared that quaternions would be found to have a powerful influence as an instrument of research. \nWhen he died, Hamilton was working on a definitive statement of quaternion science. His son William Edwin Hamilton brought the \"Elements of Quaternions\", a hefty volume of 762 pages, to publication in 1866. As copies ran short, a second edition was prepared by Charles Jasper Joly, when the book was split into two volumes, the first appearing 1899 and the second in 1901. The subject index and footnotes in this second edition improved the \"Elements\" accessibility.\n\nOne of the features of Hamilton's quaternion system was the differential operator del which could be used to express the gradient of a vector field or to express the curl. These operations were applied by Clerk Maxwell to the electrical and magnetic studies of Michael Faraday in Maxwell's Treatise on Electricity and Magnetism (1873). Though the del operator continues to be used, the real quaternions fall short as a representation of spacetime. On the other hand, the biquaternion algebra, in the hands of Arthur W. Conway and Ludwik Silberstein, provided representational tools for Minkowski space and the Lorentz group early in the twentieth century.\n\nToday, the quaternions are used in computer graphics, control theory, signal processing, and orbital mechanics, mainly for representing rotations/orientations. For example, it is common for spacecraft attitude-control systems to be commanded in terms of quaternions, which are also used to telemeter their current attitude. The rationale is that combining quaternion transformations is more numerically stable than combining many matrix transformations. In control and modelling applications, quaternions do not have a computational singularity (undefined division by zero) that can occur for quarter-turn rotations (90 degrees) that are achievable by many Air, Sea and Space vehicles. In pure mathematics, quaternions show up significantly as one of the four finite-dimensional normed division algebras over the real numbers, with applications throughout algebra and geometry.\n\nIt is believed by some modern mathematicians that Hamilton's work on quaternions was satirized by Charles Lutwidge Dodgson in Alice in Wonderland. In particular, the Mad Hatter's tea party was meant to represent the folly of quaternions and the need to revert to Euclidean geometry.\n\nHamilton originally matured his ideas before putting pen to paper. The discoveries, papers, and treatises previously mentioned might well have formed the whole work of a long and laborious life. But not to speak of his enormous collection of books, full to overflowing with new and original matter, which have been handed over to Trinity College, Dublin, the previous mentioned works barely form the greater portion of what Hamilton has published. Hamilton developed the variational principle, which was reformulated later by Carl Gustav Jacob Jacobi. He also introduced the icosian game or \"Hamilton's puzzle\" which can be solved using the concept of a Hamiltonian path.\n\nHamilton's extraordinary investigations connected with the solution of algebraic equations of the fifth degree, and his examination of the results arrived at by N. H. Abel, G. B. Jerrard, and others in their researches on this subject, form another contribution to science. There is next Hamilton's paper on fluctuating functions, a subject which, since the time of Joseph Fourier, has been of immense and ever increasing value in physical applications of mathematics. There is also the extremely ingenious invention of the hodograph. Of his extensive investigations into the solutions (especially by numerical approximation) of certain classes of physical differential equations, only a few items have been published, at intervals, in the \"Philosophical Magazine\".\n\nBesides all this, Hamilton was a voluminous correspondent. Often a single letter of Hamilton's occupied from fifty to a hundred or more closely written pages, all devoted to the minute consideration of every feature of some particular problem; for it was one of the peculiar characteristics of Hamilton's mind never to be satisfied with a general understanding of a question; Hamilton pursued the problem until he knew it in all its details. Hamilton was ever courteous and kind in answering applications for assistance in the study of his works, even when his compliance must have cost him much time. He was excessively precise and hard to please with reference to the final polish of his own works for publication; and it was probably for this reason that he published so little compared with the extent of his investigations.\n\nHamilton retained his faculties unimpaired to the very last, and steadily continued the task of finishing the \"Elements of Quaternions\" which had occupied the last six years of his life. He died on 2 September 1865, following a severe attack of gout precipitated by excessive drinking and overeating. He is buried in Mount Jerome Cemetery in Dublin. He had married Helen Bayly and had several children.\n\nHamilton is recognised as one of Ireland's leading scientists and, as Ireland becomes more aware of its scientific heritage, he is increasingly celebrated. The Hamilton Institute is an applied mathematics research institute at NUI Maynooth and the Royal Irish Academy holds an annual public Hamilton lecture at which Murray Gell-Mann, Frank Wilczek, Andrew Wiles, and Timothy Gowers have all spoken. The year 2005 was the 200th anniversary of Hamilton's birth and the Irish government designated that the \"Hamilton Year, celebrating Irish science\". Trinity College Dublin marked the year by launching the Hamilton Mathematics Institute.\n\nTwo commemorative stamps were issued by Ireland in 1943 to mark the centenary of the announcement of quaternions. A 10 Euros commemorative silver Proof coin was issued by the Central Bank of Ireland in 2005 to commemorate 200 years since his birth.\n\nThe newest maintenance depot for the Dublin LUAS tram system has been named after him. It is located adjacent to the Broombridge stop on the Green Line.\n\n\n\n\n\n\n"}
{"id": "86113", "url": "https://en.wikipedia.org/wiki?curid=86113", "title": "Winding number", "text": "Winding number\n\nIn mathematics, the winding number of a closed curve in the plane around a given point is an integer representing the total number of times that curve travels counterclockwise around the point. The winding number depends on the orientation of the curve, and is negative if the curve travels around the point clockwise.\n\nWinding numbers are fundamental objects of study in algebraic topology, and they play an important role in vector calculus, complex analysis, geometric topology, differential geometry, and physics, including string theory.\n\nSuppose we are given a closed, oriented curve in the \"xy\" plane. We can imagine the curve as the path of motion of some object, with the orientation indicating the direction in which the object moves. Then the winding number of the curve is equal to the total number of counterclockwise turns that the object makes around the origin.\n\nWhen counting the total number of turns, counterclockwise motion counts as positive, while clockwise motion counts as negative. For example, if the object first circles the origin four times counterclockwise, and then circles the origin once clockwise, then the total winding number of the curve is three.\n\nUsing this scheme, a curve that does not travel around the origin at all has winding number zero, while a curve that travels clockwise around the origin has negative winding number. Therefore, the winding number of a curve may be any integer. The following pictures show curves with winding numbers between −2 and 3:\n\nA curve in the \"xy\" plane can be defined by parametric equations:\n\nIf we think of the parameter \"t\" as time, then these equations specify the motion of an object in the plane between and . The path of this motion is a curve as long as the functions \"x\"(\"t\") and \"y\"(\"t\") are continuous. This curve is closed as long as the position of the object is the same at and .\n\nWe can define the winding number of such a curve using the polar coordinate system. Assuming the curve does not pass through the origin, we can rewrite the parametric equations in polar form:\n\nThe functions \"r\"(\"t\") and \"θ\"(\"t\") are required to be continuous, with . Because the initial and final positions are the same, \"θ\"(0) and \"θ\"(1) must differ by an integer multiple of 2\"π\". This integer is the winding number:\n\nThis defines the winding number of a curve around the origin in the \"xy\" plane. By translating the coordinate system, we can extend this definition to include winding numbers around any point \"p\".\n\nWinding number is often defined in different ways in various parts of mathematics. All of the definitions below are equivalent to the one given above:\n\nA simple combinatorial rule for defining the winding number was proposed by August Ferdinand Möbius in 1865\nand again independently by James Waddell Alexander II in 1928.\nAny curve partitions the plane into several connected regions, one of which is unbounded. The winding numbers of the curve around two points in the same region are equal. The winding number around (any point in) the unbounded region is zero. Finally, the winding numbers for any two adjacent regions differ by exactly 1; the region with the larger winding number appears on the left side of the curve (with respect to motion down the curve).\n\nIn differential geometry, parametric equations are usually assumed to be differentiable (or at least piecewise differentiable). In this case, the polar coordinate \"θ\" is related to the rectangular coordinates \"x\" and \"y\" by the equation:\nWhich is found by differentiating the following definition for θ:\nBy the fundamental theorem of calculus, the total change in \"θ\" is equal to the integral of \"dθ\". We can therefore express the winding number of a differentiable curve as a line integral:\nThe one-form \"dθ\" (defined on the complement of the origin) is closed but not exact, and it generates the first de Rham cohomology group of the punctured plane. In particular, if \"ω\" is any closed differentiable one-form defined on the complement of the origin, then the integral of \"ω\" along closed loops gives a multiple of the winding number.\n\nWinding numbers play a very important role throughout complex analysis (c.f. the statement of the residue theorem). In the context of complex analysis, the winding number of a closed curve formula_7 in the complex plane can be expressed in terms of the complex coordinate . Specifically, if we write \"z\" = \"re\", then\n\nand therefore\n\nThe total change in ln(\"r\") is zero, and thus the integral of formula_10 is equal to formula_11 multiplied by the total change in formula_12. Therefore, the winding number of closed path formula_7 about the origin is given by the expression\n\nMore generally, if formula_7 is a closed curve parameterized by formula_16, the winding number of formula_7 about formula_18, also known as the \"index\" of formula_18 with respect to formula_7, is defined for complex formula_21 as\n\nThis is a special case of the famous Cauchy integral formula. \n\nSome of the basic properties of the winding number in the complex plane are given by the following theorem: \n\nTheorem. \"Let formula_23 be a closed path and let formula_24 be the set complement of the image of formula_7, that is, formula_26. Then the index of formula_18 with respect to formula_7,\"\"formula_29,\"\"is (i) integer-valued, i.e., formula_30 for all formula_31; (ii) constant over each component (i.e., maximal connected subset) of formula_24; and (iii) zero if formula_18 is in the unbounded component of formula_24.\"\n\nAs an immediate corollary, this theorem gives the winding number of a circular path formula_7 about a point formula_18. As expected, the winding number counts the number of (counterclockwise) loops formula_7 makes around formula_18:\n\nCorollary. \"If formula_7 is the path defined by formula_40, then\" formula_41\nIn topology, the winding number is an alternate term for the degree of a continuous mapping. In physics, winding numbers are frequently called topological quantum numbers. In both cases, the same concept applies.\n\nThe above example of a curve winding around a point has a simple topological interpretation. The complement of a point in the plane is homotopy equivalent to the circle, such that maps from the circle to itself are really all that need to be considered. It can be shown that each such map can be continuously deformed to (is homotopic to) one of the standard maps formula_42, where multiplication in the circle is defined by identifying it with the complex unit circle. The set of homotopy classes of maps from a circle to a topological space form a group, which is called the first homotopy group or fundamental group of that space. The fundamental group of the circle is the group of the integers, Z; and the winding number of a complex curve is just its homotopy class.\n\nMaps from the 3-sphere to itself are also classified by an integer which is also called the winding number or sometimes Pontryagin index.\n\nIn polygons, the winding number is referred to as the polygon density. For convex polygons, and more generally simple polygons (not self-intersecting), the density is 1, by the Jordan curve theorem. By contrast, for a regular star polygon {\"p\"/\"q\"}, the density is \"q\".\n\nOne can also consider the winding number of the path with respect to the tangent of the path itself. As a path followed through time, this would be the winding number with respect to the origin of the velocity vector. In this case the example illustrated at the beginning of this article has a winding number of 3, because the small loop \"is\" counted.\n\nThis is only defined for immersed paths (i.e., for differentiable paths with nowhere vanishing derivatives), and is the degree of the tangential Gauss map.\n\nThis is called the turning number, and can be computed as the total curvature divided by 2\"π\".\n\nThe winding number is closely related with the (2 + 1)-dimensional continuous Heisenberg ferromagnet equations and its integrable extensions: the Ishimori equation etc. Solutions of the last equations are classified by the winding number or topological charge (topological invariant and/or topological quantum number).\n\n"}
