{"id": "403312", "url": "https://en.wikipedia.org/wiki?curid=403312", "title": "62 (number)", "text": "62 (number)\n\n62 (sixty-two) is a natural number following 61 and preceding 63.\n\n62 is:\n\n\n\n62 is the code for international direct dial calls to Indonesia.\n\nIn the 1998 Home Run Race, Mark McGwire hit his 62nd home run on September 8, breaking the single-season record. Sammy Sosa hit his 62nd home run just days later on September 13.\n"}
{"id": "15846301", "url": "https://en.wikipedia.org/wiki?curid=15846301", "title": "Adams filtration", "text": "Adams filtration\n\nIn mathematics, especially in the area of algebraic topology known as stable homotopy theory, the Adams filtration and the Adams-Novikov filtration allow a stable homotopy group to be understood as built from layers, the \"n\"th layer containing just those maps which require at most \"n\" auxiliary spaces in order to be a composition of homologically trivial maps. These filtrations are of particular interest because the Adams (-Novikov) spectral sequence converges to them.\n\nThe group of stable homotopy classes [\"X\",\"Y\"] between two spectra \"X\" and \"Y\" can be given a filtration by saying that a map \"f\": \"X\" → \"Y\" has filtration \"n\" if it can be written as a composite of maps \"X\" = \"X\" → \"X\" → ... → \"X\" = \"Y\" such that each individual map \"X\" → \"X\" induces the zero map in some fixed homology theory \"E\". If \"E\" is ordinary mod-\"p\" homology, this filtration is called the Adams filtration, otherwise the Adams-Novikov filtration.\n"}
{"id": "3259", "url": "https://en.wikipedia.org/wiki?curid=3259", "title": "Amicable numbers", "text": "Amicable numbers\n\nAmicable numbers are two different numbers so related that the sum of the proper divisors of each is equal to the other number. (A proper divisor of a number is a positive factor of that number other than the number itself. For example, the proper divisors of 6 are 1, 2, and 3.) A pair of amicable numbers constitutes an aliquot sequence of period 2. A related concept is that of a perfect number, which is a number that equals the sum of \"its own\" proper divisors, in other words a number which forms an aliquot sequence of period 1. Numbers that are members of an aliquot sequence with period greater than 2 are known as sociable numbers.\n\nThe smallest pair of amicable numbers is (220, 284). They are amicable because the proper divisors of 220 are 1, 2, 4, 5, 10, 11, 20, 22, 44, 55 and 110, of which the sum is 284; and the proper divisors of 284 are 1, 2, 4, 71 and 142, of which the sum is 220.\n\nThe first ten amicable pairs are: (220, 284), (1184, 1210), (2620, 2924), (5020, 5564), (6232, 6368), (10744, 10856), (12285, 14595), (17296, 18416), (63020, 76084), and (66928, 66992). . (Also see and )\n\nAmicable numbers were known to the Pythagoreans, who credited them with many mystical properties. A general formula by which some of these numbers could be derived was invented circa 850 by the Iraqi mathematician Thābit ibn Qurra (826–901). Other Arab mathematicians who studied amicable numbers are al-Majriti (died 1007), al-Baghdadi (980–1037), and al-Fārisī (1260–1320). The Iranian mathematician Muhammad Baqir Yazdi (16th century) discovered the pair (9363584, 9437056), though this has often been attributed to Descartes. Much of the work of Eastern mathematicians in this area has been forgotten.\n\nThābit ibn Qurra's formula was rediscovered by Fermat (1601–1665) and Descartes (1596–1650), to whom it is sometimes ascribed, and extended by Euler (1707–1783). It was extended further by Borho in 1972. Fermat and Descartes also rediscovered pairs of amicable numbers known to Arab mathematicians. Euler also discovered dozens of new pairs. The second smallest pair, (1184, 1210), was discovered in 1866 by a then teenage B. Nicolò I. Paganini (not to be confused with the composer and violinist), having been overlooked by earlier mathematicians.\n\nBy 1946 there were 390 known pairs, but the advent of computers has allowed the discovery of many thousands since then. Exhaustive searches have been carried out to find all pairs less than a given bound, this bound being extended from 10 in 1970, to 10 in 1986, 10 in 1993, 10 in 2015, and to 10 in 2016.\n\n, there are over 1,222,214,178 known amicable pairs.\n\nWhile these rules do generate some pairs of amicable numbers, many other pairs are known, so these rules are by no means comprehensive.\n\nIn particular, the two rules below produce only even amicable pairs, so they are of no interest for the open problem of finding amicable pairs coprime to 210 = 2·3·5·7, while over 1000 pairs coprime to 30 = 2·3·5 are known [García, Pedersen & te Riele (2003), Sándor & Crstici (2004)]. \nThe Thābit ibn Qurra theorem is a method for discovering amicable numbers invented in the ninth century by the Arab mathematician Thābit ibn Qurra.\n\nIt states that if\nwhere is an integer and , , and are prime numbers, then and are a pair of amicable numbers. This formula gives the pairs for , for , and for , but no other such pairs are known. Numbers of the form are known as Thabit numbers. In order for Ibn Qurra's formula to produce an amicable pair, two consecutive Thabit numbers must be prime; this severely restricts the possible values of .\n\nTo establish the theorem, Thâbit ibn Qurra proved nine lemmas divided into two groups. The first three lemmas deal with the determination of the aliquot parts of a natural integer. The second group of lemmas deals more specifically with the formation of perfect, abundant and deficient numbers.\n\n\"Euler's rule\" is a generalization of the Thâbit ibn Qurra theorem. It states that if\nwhere are integers and , , and are prime numbers, then and are a pair of amicable numbers. Thābit ibn Qurra's theorem corresponds to the case . Euler's rule creates additional amicable pairs for with no others being known. Euler (1747 & 1750) overall found 58 new pairs to make all the by then existing pairs into 61.\n\nLet (, ) be a pair of amicable numbers with , and write and where is the greatest common divisor of and . If and are both coprime to and square free then the pair (, ) is said to be regular , otherwise it is called irregular or exotic. If (, ) is regular and and have and prime factors respectively, then is said to be of type .\n\nFor example, with , the greatest common divisor is and so and . Therefore, is regular of type .\n\nAn amicable pair is twin if there are no integers between and belonging to any other amicable pair .\n\nIn every known case, the numbers of a pair are either both even or both odd. It is not known whether an even-odd pair of amicable numbers exists, but if it does, the even number must either be a square number or twice one, and the odd number must be a square number. However, amicable numbers where the two members have different smallest prime factors do exist: there are seven such pairs known. Also, every known pair shares at least one common prime factor. It is not known whether a pair of coprime amicable numbers exists, though if any does, the product of the two must be greater than 10. Also, a pair of coprime amicable numbers cannot be generated by Thabit's formula (above), nor by any similar formula.\n\nIn 1955, Paul Erdős showed that the density of amicable numbers, relative to the positive integers, was 0.\n\nAccording to the sum of amicable pairs conjecture, as the number of the amicable numbers approaches infinity, the percentage of the sums of the amicable pairs divisible by ten approaches 100% .\n\n\nAmicable numbers formula_1 satisfy formula_2 and formula_3 which can be written together as formula_4. This can be generalized to larger tuples, say formula_5, where we require\n\nFor example, (1980, 2016, 2556) is an amicable triple , and (3270960, 3361680, 3461040, 3834000) is an amicable quadruple .\n\nAmicable multisets are defined analogously and generalizes this a bit further .\n\nSociable numbers are the numbers in cyclic lists of numbers (with a length greater than 2) where each number is the sum of the proper divisors of the preceding number. For example, formula_7 are sociable numbers of order 4.\n\nThe aliquot sequence can be represented as a directed graph, formula_8, for a given integer formula_9, where formula_10 denotes the\nsum of the proper divisors of formula_11.\nCycles in formula_8 represent sociable numbers within the interval formula_13. Two special cases are loops that represent perfect numbers and cycles of length two that represent amicable pairs.\n\n\n\n"}
{"id": "2320078", "url": "https://en.wikipedia.org/wiki?curid=2320078", "title": "Anosov diffeomorphism", "text": "Anosov diffeomorphism\n\nIn mathematics, more particularly in the fields of dynamical systems and geometric topology, an Anosov map on a manifold \"M\" is a certain type of mapping, from \"M\" to itself, with rather clearly marked local directions of \"expansion\" and \"contraction\". Anosov systems are a special case of Axiom A systems.\n\nAnosov diffeomorphisms were introduced by Dmitri Victorovich Anosov, who proved that their behaviour was in an appropriate sense \"generic\" (when they exist at all).\n\nThree closely related definitions must be distinguished:\n\nA classical example of Anosov diffeomorphism is the Arnold's cat map.\n\nAnosov proved that Anosov diffeomorphisms are structurally stable and form an open subset of mappings (flows) with the \"C\" topology.\n\nNot every manifold admits an Anosov diffeomorphism; for example, there are no such diffeomorphisms on the sphere . The simplest examples of compact manifolds admitting them are the tori: they admit the so-called linear Anosov diffeomorphisms, which are isomorphisms having no eigenvalue of modulus 1. It was proved that any other Anosov diffeomorphism on a torus is topologically conjugate to one of this kind.\n\nThe problem of classifying manifolds that admit Anosov diffeomorphisms turned out to be very difficult, and still has no answer. The only known examples are infranil manifolds, and it is conjectured that they are the only ones.\n\nA sufficient condition for transitivity is that all points are nonwandering: formula_1.\n\nAlso, it is unknown if every formula_2 volume-preserving Anosov diffeomorphism is ergodic. Anosov proved it under a formula_3 assumption. It is also true for formula_4 volume-preserving Anosov diffeomorphisms.\n\nFor formula_3 transitive Anosov diffeomorphism formula_6 there exists a unique SRB measure (the acronym stands for Sinai, Ruelle and Bowen) formula_7 supported on formula_8 such that its basin formula_9 is of full volume, where \n\nAs an example, this section develops the case of the Anosov flow on the tangent bundle of a Riemann surface of negative curvature. This flow can be understood in terms of the flow on the tangent bundle of the Poincaré half-plane model of hyperbolic geometry. Riemann surfaces of negative curvature may be defined as Fuchsian models, that is, as the quotients of the upper half-plane and a Fuchsian group. For the following, let \"H\" be the upper half-plane; let Γ be a Fuchsian group; let \"M\" = \"H\"/Γ be a Riemann surface of negative curvature as the quotient of \"M\" by the action of the group Γ, and let formula_11 be the tangent bundle of unit-length vectors on the manifold \"M\", and let formula_12 be the tangent bundle of unit-length vectors on \"H\". Note that a bundle of unit-length vectors on a surface is the principal bundle of a complex line bundle.\n\nOne starts by noting that formula_12 is isomorphic to the Lie group PSL(2,R). This group is the group of orientation-preserving isometries of the upper half-plane. The Lie algebra of PSL(2,R) is sl(2,R), and is represented by the matrices\n\nwhich have the algebra\n\nThe exponential maps\n\ndefine right-invariant flows on the manifold of \"T\"\"H\" = PSL(2,R), and likewise on \"T\"\"M\". Defining \"P\" = \"T\"\"H\" and \"Q\" = \"T\"\"M\", these flows define vector fields on \"P\" and \"Q\", whose vectors lie in \"TP\" and \"TQ\". These are just the standard, ordinary Lie vector fields on the manifold of a Lie group, and the presentation above is a standard exposition of a Lie vector field.\n\nThe connection to the Anosov flow comes from the realization that formula_17 is the geodesic flow on \"P\" and \"Q\". Lie vector fields being (by definition) left invariant under the action of a group element, one has that these fields are left invariant under the specific elements formula_17 of the geodesic flow. In other words, the spaces \"TP\" and \"TQ\" are split into three one-dimensional spaces, or subbundles, each of which are invariant under the geodesic flow. The final step is to notice that vector fields in one subbundle expand (and expand exponentially), those in another are unchanged, and those in a third shrink (and do so exponentially).\n\nMore precisely, the tangent bundle \"TQ\" may be written as the direct sum \n\nor, at a point formula_20, the direct sum\n\ncorresponding to the Lie algebra generators \"Y\", \"J\" and \"X\", respectively, carried, by the left action of group element \"g\", from the origin \"e\" to the point \"q\". That is, one has formula_22 and formula_23. These spaces are each subbundles, and are preserved (are invariant) under the action of the geodesic flow; that is, under the action of group elements formula_24.\n\nTo compare the lengths of vectors in formula_25 at different points \"q\", one needs a metric. Any inner product at formula_26 extends to a left-invariant Riemannian metric on \"P\", and thus to a Riemannian metric on \"Q\". The length of a vector formula_27 expands exponentially as exp(t) under the action of formula_17. The length of a vector formula_29 shrinks exponentially as exp(-t) under the action of formula_17. Vectors in formula_31 are unchanged. This may be seen by examining how the group elements commute. The geodesic flow is invariant,\n\nbut the other two shrink and expand:\n\nand \n\nwhere we recall that a tangent vector in formula_35 is given by the derivative, with respect to \"t\", of the curve formula_36, the setting \"t\" = 0.\n\nWhen acting on the point \"z\" = \"i\" of the upper half-plane, formula_17 corresponds to a geodesic on the upper half plane, passing through the point \"z\" = \"i\". The action is the standard Möbius transformation action of SL(2,R) on the upper half-plane, so that \n\nA general geodesic is given by\n\nwith \"a\", \"b\", \"c\" and \"d\" real, with \"ad\" − \"bc\" = 1. The curves formula_40 and formula_36 are called horocycles. Horocycles correspond to the motion of the normal vectors of a horosphere on the upper half-plane.\n\n\n"}
{"id": "344913", "url": "https://en.wikipedia.org/wiki?curid=344913", "title": "Asymmetry", "text": "Asymmetry\n\nAsymmetry is the absence of, or a violation of, symmetry (the property of an object being invariant to a transformation, such as reflection). Symmetry is an important property of both physical and abstract systems and it may be displayed in precise terms or in more aesthetic terms. The absence of or violation of symmetry that are either expected or desired can have important consequences for a system.\n\nDue to how cells divide in organisms, asymmetry in organisms is fairly usual in at least one dimension, with biological symmetry also being common in at least one dimension.\n\nLouis Pasteur proposed that biological molecules are asymmetric because the cosmic [i.e. physical] forces that preside over their formation are themselves asymmetric. While at his time, and even now, the symmetry of physical processes are highlighted, it is known that there are fundamental physical asymmetries, starting with time.\n\nAsymmetry is an important and widespread trait, having evolved numerous times in many organisms and at many levels of organisation (ranging from individual cells, through organs, to entire body-shapes). Benefits of asymmetry sometimes have to do with improved spatial arrangements, such as the left human lung being smaller, and having one fewer lobes than the right lung to make room for the asymmetrical heart. In other examples, division of function between the right and left half may have been beneficial and has driven the asymmetry to become stronger. Such an explanation is usually given for mammal hand or paw preference (Handedness), an asymmetry in skill development in mammals. Training the neural pathways in a skill with one hand (or paw) may take less effort than doing the same with both hands.\n\nNature also provides several examples of handedness in traits that are usually symmetric. The following are examples of animals with obvious left-right asymmetries:\n\n\n\n\nSince birth defects and injuries are likely to indicate poor health of the organism, defects resulting in asymmetry often put an animal at a disadvantage when it comes to finding a mate. In particular, a degree of facial symmetry is associated with physical attractiveness, but complete symmetry is both impossible and probably unattractive.\n\nPre-modern architectural styles tended to place an emphasis on symmetry, except where extreme site conditions or historical developments lead away from this classical ideal. To the contrary, modernist and postmodern architects became much more free to use asymmetry as a design element.\n\nWhile most bridges employ a symmetrical form due to intrinsic simplicities of design, analysis and fabrication and economical use of materials, a number of modern bridges have deliberately departed from this, either in response to site-specific considerations or to create a dramatic design statement.\n\nSome asymmetrical structures\nIn fire-resistance rated wall assemblies, used in passive fire protection, including, but not limited to, high-voltage transformer fire barriers, asymmetry is a crucial aspect of design. When designing a facility, it is not always certain, that in the event of fire, which \"side\" a fire may come from. Therefore, many building codes and fire test standards outline, that a symmetrical assembly, need only be tested from one side, because both sides are the same. However, as soon as an assembly is asymmetrical, both sides must be tested and the test report is required to state the results for each side. In practical use, the lowest result achieved is the one that turns up in certification listings. Neither the test sponsor, nor the laboratory can go by an opinion or deduction as to which side was in more peril as a result of contemplated testing and then test only one side. Both must be tested in order to be compliant with test standards and building codes\n\nThere are no \"a\" and \"b\" such that \"a\" < \"b\" and \"b\" < \"a\". This form of asymmetry is an asymmetrical relation.\n\nCertain molecules are chiral; that is, they cannot be superposed upon their mirror image. Chemically identical molecules with different chirality are called \"enantiomers\"; this difference in orientation can lead to different properties in the way they react with biological systems.\n\nAsymmetry arises in physics in a number of different realms.\n\nThe original non-statistical formulation of thermodynamics was asymmetrical in time: it claimed that the entropy in a closed system can only increase with time. This was derived from the Second Law (any of the two, Clausius' or Lord Kelvin's statement can be used since they are equivalent) and using the Clausius' Theorem (see Kerson Huang ). The later theory of statistical mechanics, however, is symmetric in time. Although it states that a system significantly below maximum entropy is very likely to evolve \"towards\" higher entropy, it also states that such a system is very likely to have evolved \"from\" higher entropy.\n\nSymmetry is one of the most powerful tools in particle physics, because it has become evident that practically all laws of nature originate in symmetries. Violations of symmetry therefore present theoretical and experimental puzzles that lead to a deeper understanding of nature. Asymmetries in experimental measurements also provide powerful handles that are often relatively free from background or systematic uncertainties.\n\nUntil the 1950s, it was believed that fundamental physics was left-right symmetric; i.e., that interactions were invariant under parity. Although parity is conserved in electromagnetism, strong interactions and gravity, it turns out to be violated in weak interactions. The Standard Model incorporates parity violation by expressing the weak interaction as a chiral gauge interaction. Only the left-handed components of particles and right-handed components of antiparticles participate in weak interactions in the Standard Model. A consequence of parity violation in particle physics is that neutrinos have only been observed as left-handed particles (and antineutrinos as right-handed particles).\n\nIn 1956-1957 Chien-Shiung Wu, E. Ambler, R. W. Hayward, D. D. Hoppes, and R. P. Hudson found a clear violation of parity conservation in the beta decay of cobalt-60. Simultaneously, R. L. Garwin, Leon Lederman, and R. Weinrich modified an existing cyclotron experiment and immediately verified parity violation.\n\nAfter the discovery of the violation of parity in 1956-57, it was believed that the combined symmetry of parity (P) and simultaneous charge conjugation (C), called \"CP\", was preserved. For example, CP transforms a left-handed neutrino into a right-handed antineutrino. In 1964, however, James Cronin and Val Fitch provided clear evidence that CP symmetry was also violated in an experiment with neutral kaons.\n\nCP violation is one of the necessary conditions for the generation of a baryon asymmetry in the universe.\n\nCombining the CP symmetry with simultaneous time reversal (T) produces a combined symmetry called CPT symmetry. CPT symmetry must be preserved in any Lorentz invariant local quantum field theory with a Hermitian Hamiltonian. As of 2006, no violations of CPT symmetry have been observed.\n\nThe baryons (i.e., the protons and neutrons and the atoms that they comprise) observed so far in the universe are overwhelmingly matter as opposed to anti-matter. This asymmetry is called the baryon asymmetry of the universe.\n\nIsospin is the symmetry transformation of the weak interactions. The concept was first introduced by Werner Heisenberg in nuclear physics based on the observations that the masses of the neutron and the proton are almost identical and that the strength of the strong interaction between any pair of nucleons is the same, independent of whether they are protons or neutrons. This symmetry arises at a more fundamental level as a symmetry between up-type and down-type quarks. Isospin symmetry in the strong interactions can be considered as a subset of a larger flavor symmetry group, in which the strong interactions are invariant under interchange of different types of quarks. Including the strange quark in this scheme gives rise to the Eightfold Way scheme for classifying mesons and baryons.\n\nIsospin is violated by the fact that the masses of the up and down quarks are different, as well as by their different electric charges. Because this violation is only a small effect in most processes that involve the strong interactions, isospin symmetry remains a useful calculational tool, and its violation introduces corrections to the isospin-symmetric results.\n\nBecause the weak interactions violate parity, collider processes that can involve the weak interactions typically exhibit asymmetries in the distributions of the final-state particles. These asymmetries are typically sensitive to the \"difference\" in the interaction between particles and antiparticles, or between left-handed and right-handed particles. They can thus be used as a sensitive measurement of differences in interaction strength and/or to distinguish a small asymmetric signal from a large but symmetric background.\n\nAsymmetry is also relevant to grammar and linguistics, especially in the contexts of lexical analysis and transformational grammar.\n\nEnumeration example:\nIn English, there are grammatical rules for specifying coordinate items in an enumeration or series. Similar rules exist for programming languages and mathematical notation. These rules vary, and some require lexical asymmetry to be considered grammatically correct.\n\nFor example, in standard written English:\n\n\n"}
{"id": "19442806", "url": "https://en.wikipedia.org/wiki?curid=19442806", "title": "Australian Association of Mathematics Teachers", "text": "Australian Association of Mathematics Teachers\n\nThe Australian Association of Mathematics Teachers is the main representative organisation of mathematics teachers in Australia. Membership is via affiliated state organisations. The AAMT conducts a number of activities including \"Reach for the stars\", an activity for students, as well as submissions to government bodies and reports on issues relevant to mathematics teaching.\n\nThe AAMT is a federation of 8 affiliated associations of teachers of mathematics, one from each Australian State and Territory: \n\nThe AAMT is governed by a council made up of a representatives from each of these associations, as well as an elected President, Treasurer, and either a President Elect or an Immediate Past President.\n\nThe day-to-day affairs of the association are managed by an office staff based primarily in Adelaide, South Australia.\n\nAAMT does not direct membership; members join their local affiliated association and are then automatically a member of AAMT. AAMT has approximately 4000 individual and institutional members.\n\nThe AAMT publish \"Standards for Excellence in Teaching Mathematics in Australian Schools\" as a guide for the improvement and maintenance of teaching standards in mathematics in Australian schools.\n\nThe AAMT publishes three journals:\n"}
{"id": "6693", "url": "https://en.wikipedia.org/wiki?curid=6693", "title": "Cofinality", "text": "Cofinality\n\nIn mathematics, especially in order theory, the cofinality cf(\"A\") of a partially ordered set \"A\" is the least of the cardinalities of the cofinal subsets of \"A\".\n\nThis definition of cofinality relies on the axiom of choice, as it uses the fact that every non-empty set of cardinal numbers has a least member. The cofinality of a partially ordered set \"A\" can alternatively be defined as the least ordinal \"x\" such that there is a function from \"x\" to \"A\" with cofinal image. This second definition makes sense without the axiom of choice. If the axiom of choice is assumed, as will be the case in the rest of this article, then the two definitions are equivalent.\n\nCofinality can be similarly defined for a directed set and is used to generalize the notion of a subsequence in a net.\n\n\nIf \"A\" admits a totally ordered cofinal subset, then we can find a subset \"B\" which is well-ordered and cofinal in \"A\". Any subset of \"B\" is also well-ordered. If two cofinal subsets of \"B\" have minimal cardinality (i.e. their cardinality is the cofinality of \"B\"), then they are order isomorphic to each other.\n\nThe cofinality of an ordinal α is the smallest ordinal δ which is the order type of a cofinal subset of α. The cofinality of a set of ordinals or any other well-ordered set is the cofinality of the order type of that set.\n\nThus for a limit ordinal α, there exists a δ-indexed strictly increasing sequence with limit α. For example, the cofinality of ω² is ω, because the sequence ω·\"m\" (where \"m\" ranges over the natural numbers) tends to ω²; but, more generally, any countable limit ordinal has cofinality ω. An uncountable limit ordinal may have either cofinality ω as does ω or an uncountable cofinality.\n\nThe cofinality of 0 is 0. The cofinality of any successor ordinal is 1. The cofinality of any nonzero limit ordinal is an infinite regular cardinal.\n\nA regular ordinal is an ordinal which is equal to its cofinality. A singular ordinal is any ordinal which is not regular.\n\nEvery regular ordinal is the initial ordinal of a cardinal. Any limit of regular ordinals is a limit of initial ordinals and thus is also initial but need not be regular. Assuming the axiom of choice, formula_1 is regular for each α. In this case, the ordinals 0, 1, formula_2, formula_3, and formula_4 are regular, whereas 2, 3, formula_5, and ω are initial ordinals which are not regular.\n\nThe cofinality of any ordinal \"α\" is a regular ordinal, i.e. the cofinality of the cofinality of \"α\" is the same as the cofinality of \"α\". So the cofinality operation is idempotent.\n\nIf κ is an infinite cardinal number, then cf(κ) is the least cardinal such that there is an unbounded function from cf(κ) to κ; cf(κ) is also the cardinality of the smallest set of strictly smaller cardinals whose sum is κ; more precisely\n\nThat the set above is nonempty comes from the fact that\n\ni.e. the disjoint union of κ singleton sets. This implies immediately that cf(κ) ≤ κ.\nThe cofinality of any totally ordered set is regular, so one has cf(κ) = cf(cf(κ)).\n\nUsing König's theorem, one can prove κ < κ and κ < cf(2) for any infinite cardinal κ.\n\nThe last inequality implies that the cofinality of the cardinality of the continuum must be uncountable. On the other hand,\n\nthe ordinal number ω being the first infinite ordinal, so that the cofinality of formula_9 is card(ω) = formula_10. (In particular, formula_9 is singular.) Therefore,\n\nGeneralizing this argument, one can prove that for a limit ordinal δ\n\n\n"}
{"id": "31591175", "url": "https://en.wikipedia.org/wiki?curid=31591175", "title": "Corners theorem", "text": "Corners theorem\n\nIn mathematics, the corners theorem is a result, proved by Miklós Ajtai and Endre Szemerédi, of a statement in arithmetic combinatorics. It states that for every \"ε\" > 0 there exists \"N\" such that given at least \"εN\" points in the \"N\" × \"N\" grid {1, ..., \"N\"} × {1, ..., \"N\"}, there exists a corner, i.e., three points in the form (\"x\", \"y\"), (\"x\" + \"h\", \"y\"), and (\"x\", \"y\" + \"h\"). Later, gave a simpler proof, based on the triangle removal lemma. The corners theorem implies Roth's theorem.\n\n\n"}
{"id": "2346823", "url": "https://en.wikipedia.org/wiki?curid=2346823", "title": "De Bruijn graph", "text": "De Bruijn graph\n\nIn graph theory, an \"n\"-dimensional De Bruijn graph of \"m\" symbols is a directed graph representing overlaps between sequences of symbols. It has \"m\" vertices, consisting of all possible length-\"n\" sequences of the given symbols; the same symbol may appear multiple times in a sequence. If we have the set of \"m\" symbols formula_1 then the set of vertices is:\n\nIf one of the vertices can be expressed as another vertex by shifting all its symbols by one place to the left and adding a new symbol at the end of this vertex, then the latter has a directed edge to the former vertex. Thus the set of arcs (aka directed edges) is\n\nAlthough De Bruijn graphs are named after Nicolaas Govert de Bruijn, they were discovered independently by both De Bruijn and I. J. Good. Much earlier, Camille Flye Sainte-Marie implicitly used their properties.\n\n\nThe line graph construction of the three smallest binary De Bruijn graphs is depicted below. As can be seen in the illustration, each vertex of the formula_8-dimensional De Bruijn graph corresponds to an edge of the formula_9-dimensional De Bruijn graph, and each edge in the formula_8-dimensional De Bruijn graph corresponds to a two-edge path in the formula_9-dimensional De Bruijn graph.\nBinary De Bruijn graphs can be drawn (below, left) in such a way that they resemble objects from the theory of dynamical systems, such as the Lorenz attractor (below, right):\nThis analogy can be made rigorous: the \"n\"-dimensional \"m\"-symbol De Bruijn graph is a model of the Bernoulli map\n\nThe Bernoulli map (also called the 2x mod 1 map for \"m\" = 2) is an ergodic dynamical system, which can be understood to be a single shift of a m-adic number. The trajectories of this dynamical system correspond to walks in the De Bruijn graph, where the correspondence is given by mapping each real \"x\" in the interval [0,1) to the vertex corresponding to the first \"n\" digits in the base-\"m\" representation of \"x\". Equivalently, walks in the De Bruijn graph correspond to trajectories in a one-sided subshift of finite type.\n\n\n\n"}
{"id": "20758913", "url": "https://en.wikipedia.org/wiki?curid=20758913", "title": "Distance sampling", "text": "Distance sampling\n\nDistance sampling is a widely used group of closely related methods for estimating the density and/or abundance of populations. The main methods are based on line transects or point transects. In this method of sampling, the data collected are the distances of the objects being surveyed from these randomly placed lines or points, and the objective is to estimate the average density of the objects within a region.\n\n"}
{"id": "584406", "url": "https://en.wikipedia.org/wiki?curid=584406", "title": "Edge-transitive graph", "text": "Edge-transitive graph\n\nIn the mathematical field of graph theory, an edge-transitive graph is a graph \"G\" such that, given any two edges \"e\" and \"e\" of \"G\", there is an \nautomorphism of \"G\" that maps \"e\" to \"e\".\n\nIn other words, a graph is edge-transitive if its automorphism group acts transitively upon its edges.\n\nEdge-transitive graphs include any complete bipartite graph formula_1, and any symmetric graph, such as the vertices and edges of the cube. Symmetric graphs are also vertex-transitive (if they are connected), but in general edge-transitive graphs need not be vertex-transitive. The Gray graph is an example of a graph which is edge-transitive but not vertex-transitive. All such graphs are bipartite, and hence can be colored with only two colors.\n\nAn edge-transitive graph that is also regular, but not vertex-transitive, is called semi-symmetric. The Gray graph again provides an example.\nEvery edge-transitive graph that is not vertex-transitive must be bipartite and either semi-symmetric or biregular.\n\n"}
{"id": "5897031", "url": "https://en.wikipedia.org/wiki?curid=5897031", "title": "Elementary matrix", "text": "Elementary matrix\n\nIn mathematics, an elementary matrix is a matrix which differs from the identity matrix by one single elementary row operation. The elementary matrices generate the general linear group of invertible matrices. Left multiplication (pre-multiplication) by an elementary matrix represents elementary row operations, while right multiplication (post-multiplication) represents elementary column operations.\n\nElementary row operations are used in Gaussian elimination to reduce a matrix to row echelon form. They are also used in Gauss-Jordan elimination to further reduce the matrix to reduced row echelon form.\n\nThere are three types of elementary matrices, which correspond to three types of row operations (respectively, column operations):\n\n\n\n\nIf \"E\" is an elementary matrix, as described below, to apply the elementary row operation to a matrix \"A\", one multiplies \"A\" by the elementary matrix on the left, \"EA\". The elementary matrix for any row operation is obtained by executing the operation on the identity matrix.\n\nThe first type of row operation on a matrix \"A\" switches all matrix elements on row \"i\" with their counterparts on row \"j\". The corresponding elementary matrix is obtained by swapping row \"i\" and row \"j\" of the identity matrix. \n\nSo \"T\"\"A\" is the matrix produced by exchanging row \"i\" and row \"j\" of \"A\".\n\n\nThe next type of row operation on a matrix \"A\" multiplies all elements on row \"i\" by \"m\" where \"m\" is a non-zero scalar (usually a real number). The corresponding elementary matrix is a diagonal matrix, with diagonal entries 1 everywhere except in the \"i\"th position, where it is \"m\".\n\nSo \"D\"(\"m\")\"A\" is the matrix produced from \"A\" by multiplying row \"i\" by \"m\".\n\n\nThe final type of row operation on a matrix \"A\" adds row \"i\" multiplied by a scalar \"m\" to row \"j\". The corresponding elementary matrix is the identity matrix but with an \"m\" in the (\"j\", \"i\") position.\n\nSo \"L\"(\"m\")\"A\" is the matrix produced from \"A\" by adding \"m\" times row \"i\" to row \"j\". \n\n\n\n"}
{"id": "8605007", "url": "https://en.wikipedia.org/wiki?curid=8605007", "title": "Entropic security", "text": "Entropic security\n\nEntropic security is a security definition used in the field of cryptography. Modern encryption schemes are generally required to protect communications even when the attacker has substantial information about the messages being encrypted. For example, even if an attacker knows that an intercepted ciphertext encrypts either the message \"Attack\" or the message \"Retreat\", a semantically secure encryption scheme will prevent the attacker from learning which of the two messages is encrypted. However, definitions such as semantic security are too strong to achieve with certain specialized encryption schemes. Entropic security is a weaker definition that can be used in the special case where an attacker has very little information about the messages being encrypted.\n\nIt is well known that certain types of encryption algorithm cannot satisfy definitions such as semantic security: for example, deterministic encryption algorithms can never be semantically secure. Entropic security definitions relax these definitions to cases where the message space has substantial entropy (from an adversary's point of view). Under this definition it is possible to prove security of deterministic encryption.\n\nNote that in practice entropically-secure encryption algorithms are only \"secure\" provided that the message distribution possesses high entropy from any reasonable adversary's perspective. This is an unrealistic assumption for a general encryption scheme, since one cannot assume that all likely users will encrypt high-entropy messages. For these schemes, stronger definitions (such as semantic security or indistinguishability under adaptive chosen ciphertext attack) are appropriate. However, there are special cases in which it is reasonable to require high entropy messages. For example, encryption schemes that encrypt only secret key material (e.g., key encapsulation or Key Wrap schemes) can be considered under an entropic security definition. A practical application of this result is the use of deterministic encryption algorithms for secure encryption of secret key material.\n\nRussell and Wang formalized a definition of \"entropic security\" for encryption. Their definition resembles the semantic security definition when message spaces have highly-entropic distribution. In one formalization, the definition implies that an adversary given the ciphetext will be unable to compute any predicate on the ciphertext with (substantially) greater probability than an adversary who does not possess the ciphertext. Dodis and Smith later proposed alternate definitions and showed equivalence.\n\n"}
{"id": "1704142", "url": "https://en.wikipedia.org/wiki?curid=1704142", "title": "Escalation of commitment", "text": "Escalation of commitment\n\nEscalation of commitment is a human behavior pattern in which an individual or group facing increasingly negative outcomes from some decision, action, or investment nevertheless continues the same behavior rather than alter course. The actor maintains behaviors that are irrational, but align with previous decisions and actions.\n\nEconomists and behavioral scientists use a related term, \"sunk-cost fallacy\", to describe the justification of increased investment of money, time, lives, etc. in a decision, based on the cumulative prior investment (\"sunk cost\") despite new evidence suggesting that the cost, beginning immediately, of continuing the decision outweighs the expected benefit.\n\nIn sociology, \"irrational escalation of commitment\" or \"commitment bias\" describe similar behaviours. The phenomenon and the sentiment underlying it are reflected in such proverbial images as \"throwing good money after bad\", \"in for a penny, in for a pound\", and \"if at first you don't succeed, try, try again\".\n\nEscalation of commitment was first described by Barry M. Staw in his 1976 paper, \"Knee deep in the big muddy: A study of escalating commitment to a chosen course of action\".\n\nResearchers, inspired by the work of Staw, conducted studies that tested factors, situations and causes of escalation of commitment. The research introduced other analyses of situations and how people approach problems and make decisions. Some of the earliest work stemmed from events in which this phenomenon had an effect and help explain the phenomenon.\n\nOver the past few decades, researchers have followed and analyzed many examples of the escalation of commitment to a situation. The heightened situations are explained in three elements. Firstly, a situation has a costly amount of resources such as time, money and people invested in the project. Next, past behavior leads up to an apex in time where the project has not met expectations or could be in a cautious state of decline. Lastly, these problems all force a decision-maker to make choices that include the options of continuing to pursue a project until completion by adding additional costs, or canceling the project altogether.\n\nResearchers have also developed an argument regarding how escalation of commitment is observed in two different categories. Many researchers believe that the need to escalate resources is linked to expectancy theory. \"According to such a viewpoint, decision makers assess the probability that additional resource allocations will lead to goal attainment, as well as the value of goal attainment (i.e., rewards minus costs), and thereby generate a subjective expected utility associated with the decision to allocate additional resources.\" The next phase of the escalation process is self-justification and rationalizing if the decision the leader made used resources well, if the resources being used were used to make positive change, and assuring themselves that the decision they chose was right. Leaders must balance costs and benefits of any problem to produce a final decision. What matters most often in assuring the leader that they made the right choice regardless of the final outcome is if their decision is what others believed in.\n\nResearch conducted on the topic has been taken from many other forms and theories of psychology. Many believe that what researchers have done thus far to explain this behavior is best analyzed and tested through situational development and resource allocation.\n\nOther research has identified circumstances that lead to the opposite of escalation of commitment, namely deescalation of commitment. This research explains the factors that influence whether escalation or deescalation of commitment is more likely to occur through the role of budgeting and mental accounting.\n\nEscalation of commitment can many times cause behavior change by means of locking into resources. One of the first examples of escalation of commitment was described by George Ball, who wrote to President Lyndon Johnson to explain to him the predictions of the war outcome:\n\nSelf-justification thought process is a part of commitment decisions of leaders and managers of a group and can therefore cause a rise in commitment levels. This attitude provides \"one explanation for why people escalate commitment to their past investments.\" Managers make decisions that reflect previous behavior. Managers tend to recall and follow information that is aligned to their behavior to create consistency for their current and future decisions. If a group member or outside party recognizes inconsistent decision making, this can alter the leadership role of the manager. Managers have external influence from society, peers, and authority, which can significantly alter a manager's perception on what factors realistically matter when making decisions.\n\nProspect theory helps to describe the natural reactions and processes involved in making a decision in a risk-taking situation. Prospect theory makes the argument for how levels of wealth, whether that is people, money, or time, affect how a decision is made. Researchers were particularly interested in how one perceives a situation based on costs and benefits. The framing to how the problem is introduced is crucial for understanding and thinking of the probability that the situation will either work in favor or against you and how to prepare for those changes. \"Whyte (1986) argued that prospect theory provides the psychological mechanism by which to explain escalating commitment to a failing course of action without the need to invoke self-justification processes. (Fiegenbaum & Thomas, 1988: 99)\" Prospect theorists believe that one's use of this process is when there is a negative downfall in the stakes that will affect the outcome of the project. To ensure they will not fail, the individual may add more resources to assure them that they will succeed. Although this theory seems realistic, researchers \"Davis and Bobko (1986) found no effect of personal responsibility on continued commitment to the previous course of action in the positive frame condition.\" Which means that escalation of commitment will be lower in the higher responsibility situation.\n\nThe attribution theory, originating from Fritz Heider, \"attempts to find causal explanations for events and human behaviors.\" This theory approaches two methods of inquiry including locus of causality and stability. Locus of causality reflects on internal characteristics of an individual, such as intelligence levels and attention seeking, with the relationship of the external space such as weather forecasts and task difficulty. Aspects of control become a significant factor in how a manager justifies a decision made. Managers will use the relationship between internal and external factors to describe why they made a decision to a single point of view. Managers may justify their actions by explaining that this was out of their personal control of the event, or they could believe that the decision could not be controlled by anyone else. Research suggests that \"the type of attribution made by an employee across these dimensions is likely to impact an employee's tendency to engage in the negative emotional activity referred to as escalation of commitment.\"\n\nIdentity is a large part of how we move through the world. Private thoughts and opinions as well as the effect of others create the social identity theory. People make connections between their use of groups and their own view of themselves, which researchers have discovered motivates people to keep their social status and to defend it whenever it is endangered.\n\nGroups engage in temporal comparisons, which means that you compare actions and behaviors at \"different points in time.\" This is a form of social identity scenario. This type of comparison can be made when a decision seems unproductive and forces team members to consider any threat to the group.\n\n\"The aggregate model's emphasis is upon the accumulation and balance of forces rather than the ordering of effects over time.\" The model is general and can provide an ideal view as to how. The effects whether positive or negative are defined by micro and macro forces. This model goes by situation rather than what researchers have defined as the norm. There is no process to follow, which makes it very useful for researchers because they can understand a situation more clearly as well as see the bigger picture of the situation.\n\nThe main drivers of the tendency to continue investing in losing propositions are project, psychological, social, and structural.\n\nProject determinants are those that refer to the original commitments and decisions made at a project's beginning. This includes general project characteristics and initial financial costs. Among them, decision risk, opportunity cost information, and information acquisition have been found to have negative relationships with escalation of commitment. Decision uncertainty, positive performance trend information, and expressed preference for initial decision have been found to have positive relationships.\n\nHigh costs of ending a project or changing its course, potential financial gain upon completion, and extensive structure can factor in to escalation of commitment, making it difficult to walk away from the project. Preventing future monetary loss, potential savings, and having clearly available alternatives can allow for avoidance of the behavior. In studies by Teger and later Ross and Staw, situations where ending an action costs more than completing it resulted in decision makers being trapped in their current, costly behaviors.\n\nPsychological determinants are those that refer to internal views on the actions and information involved in a project. This can include cognitive factors, personality, and emotions as they relate to project elements. Of these, sunk costs, time investment, decision maker experience and expertise, self-efficacy and confidence, personal responsibility for the initial decision, ego threat, and proximity to project completion have been found to have positive relationships with escalation of commitment, while anticipated regret and positive information framing have been found to have negative relationships.\n\nOptimism and belief that one has control over what will still be positive outcomes lead to continued self-justification of commitment to actions. People add to their initial personal investments in the hope they will overcome currently negative results. This was illustrated in a case study by Staw, where providing business students with manipulated responsibilities for initial decisions and their outcomes resulted in the greatest commitment to increased actions and resources when the initial decision assigned was made directly by the student with poor outcomes. In these instances, people take further risk in an attempt to avoid further problems. This is even more likely when subjects view current issues as having unstable reasoning rather than stable reasoning, or when the individual is unwilling to admit mistakes. They then believe the situation will stabilize or turn around. Confirmation bias can also lead to escalation of commitment as individuals are then less likely to recognize the negative results of their decisions. On the other hand, if the results are recognized, they can be blamed on unforeseeable events occurring during the course of the project.\n\nThe effect of sunk costs is often seen escalating commitment. When the amount of investment is greater and can not be recovered, the desire to avoid complete loss of those resources and keeping with impression management prompts continued investment over pulling out. Relatedly, as invested resources can include time, closeness to completion of a project yields similar results. More value is placed on project when they are early or late in their process due to the clarity of completion goals at those points. It's more likely that risks will be taken at these points than in a project closer to a visible midpoint.\n\nSocial determinants are those that refer to the expectations and influence of other individuals or groups on project actions. Included in these, group identity or cohesive strength has been found to have the most influence on escalation of commitment while public evaluation of decision and resistance to decision from others has little significance in relation.\n\nIndividuals present themselves cautiously to others in the environment. They don't concentrate on themselves and what they believe is accurate and competent—they are only interested in their organization appearing competent. Escalation of commitment is about decision making that usually comes in a form of a group or individual deciding a course of action. Managers have a responsibility to choose the fate of what a group of people have been working on. A manager who decides to back a team out of a project isn't concerned that the project failed, they are concerned that team members may think the manager is incompetent. Studies that tested this phenomenon included factors such as policy resistance, job insecurity (Fox & Staw 1979), and audience size (Rubin & Lang 1981). All showed a spike in commitment when these realistic factors are present. This mental and emotional response is referred to as the face-saving effect. Individuals who are responsible for others are constantly checking themselves to assure that their actions and beliefs are parallel to the expectations for their viewers. One's social identity to the public can decide your fate. For example, a team can identify a level of commitment and personal connection to an idea or project. Team members consistently use statements like \"that project is Bob's baby,\" or \"oh, we had the same idea.\" Both ends of the spectrum are crucial to how others view and analyze a situation, especially something that failed.\n\nLeaders are responsible for guiding a team through the difficult problems into a solution. Although many times, the negative aspects of commitments can discussed in more detail and importance, there are many social rewards that are positive. One example of this phenomenon is persistence. A project on the verge of failure is a manager's responsibility to revive, but with persistence, a manager can get rewarded to turning a bad project into something great. Rewards are earned for turning around a team to produce something successful. When managers stick to their goals, and get their team to produce responsibly.\n\nStructural determinants are those that refer to the cultural and environmental features and resources of the project's overall organization. The minimal research available on them indicates that agency problems most influence escalation of commitment.\n\nThere are macro-level variables that affect the organizational structure of a team and how the make decisions. Decisions are made based on individuals in a group setting with specific roles and responsibilities and effect decisions that are a product of interaction in the team. The determinant that affects escalation of commitment is institutional inertia. This phenomenon is used to describe how individual attitudes and behavior changes frequently, so therefore, a group's stature is also unpredictable and controllable. \"Organizations have very imperfect sensory systems, making them relatively impervious to changes in their environments.\"\n\nThis is one factor that plays a role in how issues are addressed. When there are a group of individuals involved in communication, decisions, and change, with the lack of consistency in the group, most of the tasks fail to complete. This phenomenon occurs in situations such as policy change, rulings and procedures.\n\nThis issue can also cause havoc for an organization when they are too attached to a policy, thought process or identity because they are not free to change. \"On occasion, a project, product, or policy can become so closely tied to the values and purposes of the organization that it becomes almost unthinkable to consider withdrawal.\" One primary example of this phenomenon is the downfall of the Pan American World Airways company, commonly known as Pan Am. Pan Am was a well known airlines and hotel with hundreds of employees. With the turn of industry, airline policies were deregulated and caused financial downfall for Pan Am. The company over time made cuts to their company to stay afloat. The company believed that their image of being an airline was more important than being a successful company that they removed all of the assets that were in fact making them the largest amount of revenue only to save the image they thought they needed to remain to be Pan Am.\n\nIn groups, it can be more difficult to attribute issues to a single, simpler determinant. While determinants are still applicable, oftentimes there are more complex factors when organizational goals and actions misalign. Groups, especially as they grow larger, can be resistant to changing course.\n\nEven if the need to change course or cease action is recognized, communication speed, policies, and business politics can be hindrances. A larger organization, especially one with a spread of subgroups, has to communicate the argument and decision to go against previous actions across the appropriate levels. If this communication does not occur in a timely manner or is blocked, the final decision may not be made and acted upon. A decision that goes against existing rules and processes may also reach resistance even with support from those within the organization. Individuals and groups that are directly employed due to a project, have financial stake in it may provide enough opposition to prevent changes from being made as well. They feel personally responsible for the parts they've worked on and can also feel that they too are being replaced or terminated. Escalation of commitment can then occur in any of these situations. External groups can play an even larger part in escalating commitment if their power is greater than that of the group taking action and they use that power to directly lead and influence.\n\nWith a larger number of decision makers included, groups have the opportunity for greater productivity than single individuals, but they also have the opportunity for greater losses and escalation. Members can eliminate some of the escalation potential if they come to a better decision earlier on in the process and avoid the need to change course dramatically. Yet they can also hold onto a larger base of support for their initial actions to the point where escalation potential is increased. In this case, groupthink assists in keeping with the original decision and pressures towards conformity rather than dividing the group with other options. Also, a group whose members are not cohesive can have decreased escalation potential due to conflicts and varying levels of inclusion in the process.\n\nOrganizations that are family businesses are especially prone to escalation of commitment due to the added level of going through the family structure in addition to the business structure, allowing for further conflicts between the two. Business reputation, customer and share loss, and financial loss become risks.\n\n\n"}
{"id": "58516365", "url": "https://en.wikipedia.org/wiki?curid=58516365", "title": "F. Michael Christ", "text": "F. Michael Christ\n\nFrancis Michael Christ (born 7 June 1955) is an American mathematician, specializing in harmonic analysis, partial differential equations, and several complex variables. He is known for the Christ–Kiselev maximal inequality.\n\nHe received in 1977 from Harvey Mudd College his bachelor's degree and in 1982 from the University of Chicago his PhD under the supervision of Alberto Calderón with thesis \"Restrictions of the Fourier transform to submanifolds of low codimension\". At Princeton University (where he worked with Elias M. Stein), Christ was from 1982 to 1984 an instructor and from 1984 to 1986 an assistant professor. He was at the University of California, Los Angeles (UCLA) from 1986 to 1988 an associate professor and from 1988 to 1996 a full professor. In 1996 he became a full professor at the University of California, Berkeley.\n\nChrist was a Sloan Fellow for the academic year 1986–1987. In 1990 in Kyoto he was an invited speaker at the International Congress of Mathematicians. In 1997 he shared with David E. Barrett the Stefan Bergman Prize.\n\n\n"}
{"id": "34672901", "url": "https://en.wikipedia.org/wiki?curid=34672901", "title": "Fay's trisecant identity", "text": "Fay's trisecant identity\n\nIn algebraic geometry, Fay's trisecant identity is an identity between theta functions of Riemann surfaces introduced by . Fay's identity holds for theta functions of Jacobians of curves, but not for theta functions of general abelian varieties.\n\nThe name \"trisecant identity\" refers to the geometric interpretation given by , who used it to show that the Kummer variety of a genus \"g\" Riemann surface, given by the image of the map from the Jacobian to projective space of dimension 2 – 1 induced by theta functions of order 2, has a 4-dimensional space of trisecants.\n\nSuppose that\n\nThe Fay's identity states that\n\nformula_1\n\nwith\n\nformula_2\n\n"}
{"id": "27106625", "url": "https://en.wikipedia.org/wiki?curid=27106625", "title": "Fox H-function", "text": "Fox H-function\n\nIn mathematics, the Fox H-function \"H\"(\"x\") is a generalization of the Meijer G-function introduced by .\nIt is defined by a Mellin–Barnes integral\nwhere \"L\" is a certain contour separating the poles of the two factors in the numerator.\nAnother generalization of Fox H-function is given by Innayat Hussain .\nFor a further generalization of this function, useful in Physics and Statistics, see .\n\nThe special case for which the Fox H-function reduces to the Meijer G-function is \"A\" = \"B\" = \"C\", \"C\" > 0 for \"j\" = 1...\"p\" and \"k\" = 1...\"q\" :\n\n"}
{"id": "3171371", "url": "https://en.wikipedia.org/wiki?curid=3171371", "title": "Generator (mathematics)", "text": "Generator (mathematics)\n\nIn mathematics and physics, the term generator or generating set may refer to any of a number of related concepts. The underlying concept in each case is that of a smaller set of objects, together with a set of operations that can be applied to it, that result in the creation of a larger collection of objects, called the generated set. The larger set is then said to be generated by the smaller set. It is commonly the case that the generating set has a simpler set of properties than the generated set, thus making it easier to discuss and examine. It is usually the case that properties of the generating set are in some way preserved by the act of generation; likewise, the properties of the generated set are often reflected in the generating set.\n\nA list of examples of generating sets follow.\n\n\nIn the study of differential equations, and commonly those occurring in physics, one has the idea of a set of infinitesimal displacements that can be extended to obtain a manifold, or at least, a local part of it, by means of integration. The general concept is of using the exponential map to take the vectors in the tangent space and extend them, as geodesics, to an open set surrounding the tangent point. In this case, it is not unusual to call the elements of the tangent space the \"generators\" of the manifold. When the manifold possesses some sort of symmetry, there is also the related notion of a \"charge\" or \"current\", which is sometimes also called the generator, although, strictly speaking, charges are not elements of the tangent space.\n\n\n\n"}
{"id": "12281", "url": "https://en.wikipedia.org/wiki?curid=12281", "title": "Gottfried Wilhelm Leibniz", "text": "Gottfried Wilhelm Leibniz\n\nGottfried Wilhelm (von) Leibniz (sometimes spelled Leibnitz) (; or ; ; – 14 November 1716) was a prominent German polymath and philosopher in the history of mathematics and the history of philosophy. His most notable accomplishment was conceiving the ideas of differential and integral calculus, independently of Isaac Newton's contemporaneous developments. Mathematical works have always favored Leibniz's notation as the conventional expression of calculus, while Newton's notation became unused. It was only in the 20th century that Leibniz's law of continuity and transcendental law of homogeneity found mathematical implementation (by means of non-standard analysis). He became one of the most prolific inventors in the field of mechanical calculators. While working on adding automatic multiplication and division to Pascal's calculator, he was the first to describe a pinwheel calculator in 1685 and invented the Leibniz wheel, used in the arithmometer, the first mass-produced mechanical calculator. He also refined the binary number system, which is the foundation of all digital computers.\n\nIn philosophy, Leibniz is most noted for his optimism, i.e. his conclusion that our universe is, in a restricted sense, the best possible one that God could have created, an idea that was often lampooned by others such as Voltaire. Leibniz, along with René Descartes and Baruch Spinoza, was one of the three great 17th-century advocates of rationalism. The work of Leibniz anticipated modern logic and analytic philosophy, but his philosophy also looks back to the scholastic tradition, in which conclusions are produced by applying reason to first principles or prior definitions rather than to empirical evidence.\n\nLeibniz made major contributions to physics and technology, and anticipated notions that surfaced much later in philosophy, probability theory, biology, medicine, geology, psychology, linguistics, and computer science. He wrote works on philosophy, politics, law, ethics, theology, history, and philology. Leibniz also contributed to the field of library science. While serving as overseer of the Wolfenbüttel library in Germany, he devised a cataloging system that would serve as a guide for many of Europe's largest libraries. Leibniz's contributions to this vast array of subjects were scattered in various learned journals, in tens of thousands of letters, and in unpublished manuscripts. He wrote in several languages, but primarily in Latin, French, and German. There is no complete gathering of the writings of Leibniz translated into English.\n\nGottfried Leibniz was born on 1 July 1646, toward the end of the Thirty Years' War, in Leipzig, Saxony, to Friedrich Leibniz and Catharina Schmuck. Friedrich noted in his family journal:\n\nIn English:\nLeibniz was baptized on 3 July of that year at St. Nicholas Church, Leipzig; his godfather was the Lutheran theologian . His father died when he was six years old, and from that point on he was raised by his mother.\n\nLeibniz's father had been a Professor of Moral Philosophy at the University of Leipzig, and the boy later inherited his father's personal library. He was given free access to it from the age of seven. While Leibniz's schoolwork was largely confined to the study of a small canon of authorities, his father's library enabled him to study a wide variety of advanced philosophical and theological works—ones that he would not have otherwise been able to read until his college years. Access to his father's library, largely written in Latin, also led to his proficiency in the Latin language, which he achieved by the age of 12. He also composed 300 hexameters of Latin verse, in a single morning, for a special event at school at the age of 13.\n\nIn April 1661 he enrolled in his father's former university at age 14, and completed his bachelor's degree in Philosophy in December 1662. He defended his \"Disputatio Metaphysica de Principio Individui\" (\"Metaphysical Disputation on the Principle of Individuation\"), which addressed the principle of individuation, on 9 June 1663. Leibniz earned his master's degree in Philosophy on 7 February 1664. He published and defended a dissertation \"Specimen Quaestionum Philosophicarum ex Jure collectarum\" (\"An Essay of Collected Philosophical Problems of Right\"), arguing for both a theoretical and a pedagogical relationship between philosophy and law, in December 1664. After one year of legal studies, he was awarded his bachelor's degree in Law on 28 September 1665. His dissertation was titled \"De conditionibus\" (\"On Conditions\").\n\nIn early 1666, at age 19, Leibniz wrote his first book, \"De Arte Combinatoria\" (\"On the Combinatorial Art\"), the first part of which was also his habilitation thesis in Philosophy, which he defended in March 1666. His next goal was to earn his license and Doctorate in Law, which normally required three years of study. In 1666, the University of Leipzig turned down Leibniz's doctoral application and refused to grant him a Doctorate in Law, most likely due to his relative youth. Leibniz subsequently left Leipzig.\n\nLeibniz then enrolled in the University of Altdorf and quickly submitted a thesis, which he had probably been working on earlier in Leipzig. The title of his thesis was \"Disputatio Inauguralis de Casibus Perplexis in Jure\" (\"Inaugural Disputation on Ambiguous Legal Cases\"). Leibniz earned his license to practice law and his Doctorate in Law in November 1666. He next declined the offer of an academic appointment at Altdorf, saying that \"my thoughts were turned in an entirely different direction\".\n\nAs an adult, Leibniz often introduced himself as \"Gottfried von Leibniz\". Many posthumously published editions of his writings presented his name on the title page as \"Freiherr G. W. von Leibniz.\" However, no document has ever been found from any contemporary government that stated his appointment to any form of nobility.\n\nLeibniz's first position was as a salaried secretary to an alchemical society in Nuremberg. He knew fairly little about the subject at that time but presented himself as deeply learned. He soon met Johann Christian von Boyneburg (1622–1672), the dismissed chief minister of the Elector of Mainz, Johann Philipp von Schönborn. Von Boyneburg hired Leibniz as an assistant, and shortly thereafter reconciled with the Elector and introduced Leibniz to him. Leibniz then dedicated an essay on law to the Elector in the hope of obtaining employment. The stratagem worked; the Elector asked Leibniz to assist with the redrafting of the legal code for the Electorate. In 1669, Leibniz was appointed assessor in the Court of Appeal. Although von Boyneburg died late in 1672, Leibniz remained under the employment of his widow until she dismissed him in 1674.\n\nVon Boyneburg did much to promote Leibniz's reputation, and the latter's memoranda and letters began to attract favorable notice. After Leibniz's service to the Elector there soon followed a diplomatic role. He published an essay, under the pseudonym of a fictitious Polish nobleman, arguing (unsuccessfully) for the German candidate for the Polish crown. The main force in European geopolitics during Leibniz's adult life was the ambition of Louis XIV of France, backed by French military and economic might. Meanwhile, the Thirty Years' War had left German-speaking Europe exhausted, fragmented, and economically backward. Leibniz proposed to protect German-speaking Europe by distracting Louis as follows. France would be invited to take Egypt as a stepping stone towards an eventual conquest of the Dutch East Indies. In return, France would agree to leave Germany and the Netherlands undisturbed. This plan obtained the Elector's cautious support. In 1672, the French government invited Leibniz to Paris for discussion, but the plan was soon overtaken by the outbreak of the Franco-Dutch War and became irrelevant. Napoleon's failed invasion of Egypt in 1798 can be seen as an unwitting, late implementation of Leibniz's plan, after the Eastern hemisphere colonial supremacy in Europe had already passed from the Dutch to the British.\n\nThus Leibniz went to Paris in 1672. Soon after arriving, he met Dutch physicist and mathematician Christiaan Huygens and realised that his own knowledge of mathematics and physics was patchy. With Huygens as his mentor, he began a program of self-study that soon pushed him to making major contributions to both subjects, including discovering his version of the differential and integral calculus. He met Nicolas Malebranche and Antoine Arnauld, the leading French philosophers of the day, and studied the writings of Descartes and Pascal, unpublished as well as published. He befriended a German mathematician, Ehrenfried Walther von Tschirnhaus; they corresponded for the rest of their lives.\n\nWhen it became clear that France would not implement its part of Leibniz's Egyptian plan, the Elector sent his nephew, escorted by Leibniz, on a related mission to the English government in London, early in 1673. There Leibniz came into acquaintance of Henry Oldenburg and John Collins. He met with the Royal Society where he demonstrated a calculating machine that he had designed and had been building since 1670. The machine was able to execute all four basic operations (adding, subtracting, multiplying, and dividing), and the society quickly made him an external member.\n\nThe mission ended abruptly when news of the Elector's death (12 February 1673) reached them. Leibniz promptly returned to Paris and not, as had been planned, to Mainz. The sudden deaths of his two patrons in the same winter meant that Leibniz had to find a new basis for his career.\n\nIn this regard, a 1669 invitation from the John Frederick of Brunswick to visit Hanover proved to have been fateful. Leibniz had declined the invitation, but had begun corresponding with the duke in 1671. In 1673, the duke offered Leibniz the post of counsellor. Leibniz very reluctantly accepted the position two years later, only after it became clear that no employment in Paris, whose intellectual stimulation he relished, or with the Habsburg imperial court, was forthcoming.\n\nIn 1675 he tried to get admitted to the French Academy of Sciences as a foreign honorary member, but it was considered that there were already enough foreigners there and so no invitation came. He left Paris in October 1676.\n\nLeibniz managed to delay his arrival in Hanover until the end of 1676 after making one more short journey to London, where Newton accused him of having seen Newton's unpublished work on calculus in advance. This was alleged to be evidence supporting the accusation, made decades later, that he had stolen calculus from Newton. On the journey from London to Hanover, Leibniz stopped in The Hague where he met van Leeuwenhoek, the discoverer of microorganisms. He also spent several days in intense discussion with Spinoza, who had just completed his masterwork, the \"Ethics\".\n\nIn 1677, he was promoted, at his request, to Privy Counselor of Justice, a post he held for the rest of his life. Leibniz served three consecutive rulers of the House of Brunswick as historian, political adviser, and most consequentially, as librarian of the ducal library. He thenceforth employed his pen on all the various political, historical, and theological matters involving the House of Brunswick; the resulting documents form a valuable part of the historical record for the period.\n\nLeibniz began promoting a project to use windmills to improve the mining operations in the Harz Mountains. This project did little to improve mining operations and was shut down by Duke Ernst August in 1685.\n\nAmong the few people in north Germany to accept Leibniz were the Electress Sophia of Hanover (1630–1714), her daughter Sophia Charlotte of Hanover (1668–1705), the Queen of Prussia and his avowed disciple, and Caroline of Ansbach, the consort of her grandson, the future George II. To each of these women he was correspondent, adviser, and friend. In turn, they all approved of Leibniz more than did their spouses and the future king George I of Great Britain.\n\nThe population of Hanover was only about 10,000, and its provinciality eventually grated on Leibniz. Nevertheless, to be a major courtier to the House of Brunswick was quite an honor, especially in light of the meteoric rise in the prestige of that House during Leibniz's association with it. In 1692, the Duke of Brunswick became a hereditary Elector of the Holy Roman Empire. The British Act of Settlement 1701 designated the Electress Sophia and her descent as the royal family of England, once both King William III and his sister-in-law and successor, Queen Anne, were dead. Leibniz played a role in the initiatives and negotiations leading up to that Act, but not always an effective one. For example, something he published anonymously in England, thinking to promote the Brunswick cause, was formally censured by the British Parliament.\n\nThe Brunswicks tolerated the enormous effort Leibniz devoted to intellectual pursuits unrelated to his duties as a courtier, pursuits such as perfecting calculus, writing about other mathematics, logic, physics, and philosophy, and keeping up a vast correspondence. He began working on calculus in 1674; the earliest evidence of its use in his surviving notebooks is 1675. By 1677 he had a coherent system in hand, but did not publish it until 1684. Leibniz's most important mathematical papers were published between 1682 and 1692, usually in a journal which he and Otto Mencke founded in 1682, the \"Acta Eruditorum\". That journal played a key role in advancing his mathematical and scientific reputation, which in turn enhanced his eminence in diplomacy, history, theology, and philosophy.\n\nThe Elector Ernest Augustus commissioned Leibniz to write a history of the House of Brunswick, going back to the time of Charlemagne or earlier, hoping that the resulting book would advance his dynastic ambitions. From 1687 to 1690, Leibniz traveled extensively in Germany, Austria, and Italy, seeking and finding archival materials bearing on this project. Decades went by but no history appeared; the next Elector became quite annoyed at Leibniz's apparent dilatoriness. Leibniz never finished the project, in part because of his huge output on many other fronts, but also because he insisted on writing a meticulously researched and erudite book based on archival sources, when his patrons would have been quite happy with a short popular book, one perhaps little more than a genealogy with commentary, to be completed in three years or less. They never knew that he had in fact carried out a fair part of his assigned task: when the material Leibniz had written and collected for his history of the House of Brunswick was finally published in the 19th century, it filled three volumes.\n\nLeibniz was appointed Librarian of the Herzog August Library in Wolfenbüttel, Lower Saxony, in 1691.\n\nIn 1708, John Keill, writing in the journal of the Royal Society and with Newton's presumed blessing, accused Leibniz of having plagiarised Newton's calculus. Thus began the calculus priority dispute which darkened the remainder of Leibniz's life. A formal investigation by the Royal Society (in which Newton was an unacknowledged participant), undertaken in response to Leibniz's demand for a retraction, upheld Keill's charge. Historians of mathematics writing since 1900 or so have tended to acquit Leibniz, pointing to important differences between Leibniz's and Newton's versions of calculus.\n\nIn 1711, while traveling in northern Europe, the Russian Tsar Peter the Great stopped in Hanover and met Leibniz, who then took some interest in Russian matters for the rest of his life. In 1712, Leibniz began a two-year residence in Vienna, where he was appointed Imperial Court Councillor to the Habsburgs. On the death of Queen Anne in 1714, Elector George Louis became King George I of Great Britain, under the terms of the 1701 Act of Settlement. Even though Leibniz had done much to bring about this happy event, it was not to be his hour of glory. Despite the intercession of the Princess of Wales, Caroline of Ansbach, George I forbade Leibniz to join him in London until he completed at least one volume of the history of the Brunswick family his father had commissioned nearly 30 years earlier. Moreover, for George I to include Leibniz in his London court would have been deemed insulting to Newton, who was seen as having won the calculus priority dispute and whose standing in British official circles could not have been higher. Finally, his dear friend and defender, the Dowager Electress Sophia, died in 1714.\n\nLeibniz died in Hanover in 1716: at the time, he was so out of favor that neither George I (who happened to be near Hanover at that time) nor any fellow courtier other than his personal secretary attended the funeral. Even though Leibniz was a life member of the Royal Society and the Berlin Academy of Sciences, neither organization saw fit to honor his death. His grave went unmarked for more than 50 years. Leibniz was eulogized by Fontenelle, before the French Academy of Sciences in Paris, which had admitted him as a foreign member in 1700. The eulogy was composed at the behest of the Duchess of Orleans, a niece of the Electress Sophia.\n\nLeibniz never married. He complained on occasion about money, but the fair sum he left to his sole heir, his sister's stepson, proved that the Brunswicks had, by and large, paid him well. In his diplomatic endeavors, he at times verged on the unscrupulous, as was all too often the case with professional diplomats of his day. On several occasions, Leibniz backdated and altered personal manuscripts, actions which put him in a bad light during the calculus controversy. On the other hand, he was charming, well-mannered, and not without humor and imagination. He had many friends and admirers all over Europe. On Leibniz's religious views, though he was a protestant, Leibniz learned to appreciate aspects of Catholicism through his patrons and colleagues. He never admitted the Protestant view of Pope as an Antichrist. Leibniz was claimed as a philosophical theist. Leibniz remained committed to Trinitarian Christianity throughout his life.\n\nLeibniz's philosophical thinking appears fragmented, because his philosophical writings consist mainly of a multitude of short pieces: journal articles, manuscripts published long after his death, and many letters to many correspondents. He wrote only two book-length philosophical treatises, of which only the \"Théodicée\" of 1710 was published in his lifetime.\n\nLeibniz dated his beginning as a philosopher to his \"Discourse on Metaphysics\", which he composed in 1686 as a commentary on a running dispute between Nicolas Malebranche and Antoine Arnauld. This led to an extensive and valuable correspondence with Arnauld; it and the \"Discourse\" were not published until the 19th century. In 1695, Leibniz made his public entrée into European philosophy with a journal article titled \"New System of the Nature and Communication of Substances\". Between 1695 and 1705, he composed his \"New Essays on Human Understanding\", a lengthy commentary on John Locke's 1690 \"An Essay Concerning Human Understanding\", but upon learning of Locke's 1704 death, lost the desire to publish it, so that the \"New Essays\" were not published until 1765. The \"Monadologie\", composed in 1714 and published posthumously, consists of 90 aphorisms.\n\nLeibniz met Spinoza in 1676, read some of his unpublished writings, and has since been suspected of appropriating some of Spinoza's ideas. While Leibniz admired Spinoza's powerful intellect, he was also forthrightly dismayed by Spinoza's conclusions, especially when these were inconsistent with Christian orthodoxy.\n\nUnlike Descartes and Spinoza, Leibniz had a thorough university education in philosophy. He was influenced by his Leipzig professor Jakob Thomasius, who also supervised his BA thesis in philosophy. Leibniz also eagerly read Francisco Suárez, a Spanish Jesuit respected even in Lutheran universities. Leibniz was deeply interested in the new methods and conclusions of Descartes, Huygens, Newton, and Boyle, but viewed their work through a lens heavily tinted by scholastic notions. Yet it remains the case that Leibniz's methods and concerns often anticipate the logic, and analytic and linguistic philosophy of the 20th century.\n\nLeibniz variously invoked one or another of seven fundamental philosophical Principles:\n\nLeibniz would on occasion give a rational defense of a specific principle, but more often took them for granted.\n\nLeibniz's best known contribution to metaphysics is his theory of monads, as exposited in \"Monadologie\". He proposes his theory that the universe is made of an infinite number of simple substances known as monads. Monads can also be compared to the corpuscles of the Mechanical Philosophy of René Descartes and others. These simple substances or monads are the \"ultimate units of existence in nature\". Monads have no parts but still exist by the qualities that they have. These qualities are continuously changing over time, and each monad is unique. They are also not affected by time and are subject to only creation and annihilation. Monads are centers of force; substance is force, while space, matter, and motion are merely phenomenal.\n\nLeibniz's proof of God can be summarized in the \"Théodicée\". Reason is governed by the principle of contradiction and the principle of sufficient reason. Using the principle of reasoning, Leibniz concluded that the first reason of all things is God. All that we see and experience are subject to change, and the fact that this world is contingent can be explained by the possibility of the world being arranged differently in space and time. The contingent world must have some necessary reason for its existence. Leibniz uses a geometry book as an example to explain his reasoning. If this book was copied from an infinite chain of copies, there must be a some reason for the content of the book. Leibniz concluded that there must be the \"monas monadum\" or God.\n\nThe ontological essence of a monad is its irreducible simplicity. Unlike atoms, monads possess no material or spatial character. They also differ from atoms by their complete mutual independence, so that interactions among monads are only apparent. Instead, by virtue of the principle of pre-established harmony, each monad follows a preprogrammed set of \"instructions\" peculiar to itself, so that a monad \"knows\" what to do at each moment. By virtue of these intrinsic instructions, each monad is like a little mirror of the universe. Monads need not be \"small\"; e.g., each human being constitutes a monad, in which case free will is problematic.\n\nMonads are purported to have gotten rid of the problematic:\n\nThe \"Theodicy\" tries to justify the apparent imperfections of the world by claiming that it is optimal among all possible worlds. It must be the best possible and most balanced world, because it was created by an all powerful and all knowing God, who would not choose to create an imperfect world if a better world could be known to him or possible to exist. In effect, apparent flaws that can be identified in this world must exist in every possible world, because otherwise God would have chosen to create the world that excluded those flaws.\n\nLeibniz asserted that the truths of theology (religion) and philosophy cannot contradict each other, since reason and faith are both \"gifts of God\" so that their conflict would imply God contending against himself. The \"Theodicy\" is Leibniz's attempt to reconcile his personal philosophical system with his interpretation of the tenets of Christianity. This project was motivated in part by Leibniz's belief, shared by many conservative philosophers and theologians during the Enlightenment, in the rational and enlightened nature of the Christian religion as compared to its purportedly less advanced non-Western counterparts. It was also shaped by Leibniz's belief in the perfectibility of human nature (if humanity relied on correct philosophy and religion as a guide), and by his belief that metaphysical necessity must have a rational or logical foundation, even if this metaphysical causality seemed inexplicable in terms of physical necessity (the natural laws identified by science).\n\nBecause reason and faith must be entirely reconciled, any tenet of faith which could not be defended by reason must be rejected. Leibniz then approached one of the central criticisms of Christian theism: if God is all good, all wise and all powerful, how did evil come into the world? The answer (according to Leibniz) is that, while God is indeed unlimited in wisdom and power, his human creations, as creations, are limited both in their wisdom and in their will (power to act). This predisposes humans to false beliefs, wrong decisions and ineffective actions in the exercise of their free will. God does not arbitrarily inflict pain and suffering on humans; rather he permits both \"moral evil\" (sin) and \"physical evil\" (pain and suffering) as the necessary consequences of \"metaphysical evil\" (imperfection), as a means by which humans can identify and correct their erroneous decisions, and as a contrast to true good.\n\nFurther, although human actions flow from prior causes that ultimately arise in God, and therefore are known as a metaphysical certainty to God, an individual's free will is exercised within natural laws, where choices are merely contingently necessary, to be decided in the event by a \"wonderful spontaneity\" that provides individuals an escape from rigorous predestination.\n\nFor Leibniz, \"God is an absolutely perfect being.\" He describes this perfection later in section VI as the simplest form of something with the most substantial outcome (VI). Along these lines, he declares that every type of perfection \"pertains to him (God) in the highest degree\" (I). Even though his types of perfections are not specifically drawn out, Leibniz highlights the one thing that, to him, does certify imperfections and proves that God is perfect: \"that one acts imperfectly if he acts with less perfection than he is capable of\", and since God is a perfect being, he cannot act imperfectly (III). Because God cannot act imperfectly, the decisions he makes pertaining to the world must be perfect. Leibniz also comforts readers, stating that because he has done everything to the most perfect degree; those who love him cannot be injured. However, to love God is a subject of difficulty as Leibniz believes that we are \"not disposed to wish for that which God desires\" because we have the ability to alter our disposition (IV). In accordance with this, many act as rebels, but Leibniz says that the only way we can truly love God is by being content \"with all that comes to us according to his will\" (IV).\n\nBecause God is \"an absolutely perfect being\" (I), Leibniz argues that God would be acting imperfectly if he acted with any less perfection than what he is able of (III). His syllogism then ends with the statement that God has made the world perfectly in all ways. This also affects how we should view God and his will. Leibniz states that, in lieu of God’s will, we have to understand that God \"is the best of all masters\" and he will know when his good succeeds, so we, therefore, must act in conformity to his good will—or as much of it as we understand (IV). In our view of God, Leibniz declares that we cannot admire the work solely because of the maker, lest we mar the glory and love God in doing so. Instead, we must admire the maker for the work he has done (II). Effectively, Leibniz states that if we say the earth is good because of the will of God, and not good according to some standards of goodness, then how can we praise God for what he has done if contrary actions are also praiseworthy by this definition (II). Leibniz then asserts that different principles and geometry cannot simply be from the will of God, but must follow from his understanding.\n\nLeibniz wrote: \"Why is there something rather than nothing? The sufficient reason ... is found in a substance which ... is a necessary being bearing the reason for its existence within itself.\" Martin Heidegger called this question \"the fundamental question of metaphysics\".\n\nLeibniz believed that much of human reasoning could be reduced to calculations of a sort, and that such calculations could resolve many differences of opinion:\n\nThe only way to rectify our reasonings is to make them as tangible as those of the Mathematicians, so that we can find our error at a glance, and when there are disputes among persons, we can simply say: Let us calculate [\"calculemus\"], without further ado, to see who is right.\n\nLeibniz's calculus ratiocinator, which resembles symbolic logic, can be viewed as a way of making such calculations feasible. Leibniz wrote memoranda that can now be read as groping attempts to get symbolic logic—and thus his \"calculus\"—off the ground. These writings remained unpublished until the appearance of a selection edited by C.I. Gerhardt (1859). L. Couturat published a selection in 1901; by this time the main developments of modern logic had been created by Charles Sanders Peirce and by Gottlob Frege.\n\nLeibniz thought symbols were important for human understanding. He attached so much importance to the development of good notations that he attributed all his discoveries in mathematics to this. His notation for calculus is an example of his skill in this regard. Peirce, a 19th-century pioneer of semiotics, shared Leibniz's passion for symbols and notation, and his belief that these are essential to a well-running logic and mathematics.\n\nBut Leibniz took his speculations much further. Defining a character as any written sign, he then defined a \"real\" character as one that represents an idea directly and not simply as the word embodying the idea. Some real characters, such as the notation of logic, serve only to facilitate reasoning. Many characters well known in his day, including Egyptian hieroglyphics, Chinese characters, and the symbols of astronomy and chemistry, he deemed not real. Instead, he proposed the creation of a \"characteristica universalis\" or \"universal characteristic\", built on an alphabet of human thought in which each fundamental concept would be represented by a unique \"real\" character:\n\nIt is obvious that if we could find characters or signs suited for expressing all our thoughts as clearly and as exactly as arithmetic expresses numbers or geometry expresses lines, we could do in all matters \"insofar as they are subject to reasoning\" all that we can do in arithmetic and geometry. For all investigations which depend on reasoning would be carried out by transposing these characters and by a species of calculus.\n\nComplex thoughts would be represented by combining characters for simpler thoughts. Leibniz saw that the uniqueness of prime factorization suggests a central role for prime numbers in the universal characteristic, a striking anticipation of Gödel numbering. Granted, there is no intuitive or mnemonic way to number any set of elementary concepts using the prime numbers. \n\nBecause Leibniz was a mathematical novice when he first wrote about the \"characteristic\", at first he did not conceive it as an algebra but rather as a universal language or script. Only in 1676 did he conceive of a kind of \"algebra of thought\", modeled on and including conventional algebra and its notation. The resulting \"characteristic\" included a logical calculus, some combinatorics, algebra, his \"analysis situs\" (geometry of situation), a universal concept language, and more. What Leibniz actually intended by his \"characteristica universalis\" and calculus ratiocinator, and the extent to which modern formal logic does justice to calculus, may never be established. Leibniz's idea of reasoning through a universal language of symbols and calculations remarkably foreshadows great 20th-century developments in formal systems, such as Turing completeness, where computation was used to define equivalent universal languages (see Turing degree).\n\nLeibniz has been noted as one of the most important logicians between the times of Aristotle and Gottlob Frege. Leibniz enunciated the principal properties of what we now call conjunction, disjunction, negation, identity, set inclusion, and the empty set. The principles of Leibniz's logic and, arguably, of his whole philosophy, reduce to two:\n\n\nThe formal logic that emerged early in the 20th century also requires, at minimum, unary negation and quantified variables ranging over some universe of discourse.\n\nLeibniz published nothing on formal logic in his lifetime; most of what he wrote on the subject consists of working drafts. In his book \"History of Western Philosophy\", Bertrand Russell went so far as to claim that Leibniz had developed logic in his unpublished writings to a level which was reached only 200 years later.\n\nRussell's principal work on Leibniz found that many of Leibniz's most startling philosophical ideas and claims (e.g., that each of the fundamental monads mirrors the whole universe) follow logically from Leibniz's conscious choice to reject \"relations\" between things as unreal. He regarded such relations as (real) \"qualities\" of things (Leibniz admitted unary predicates only): For him \"Mary is the mother of John\" describes separate qualities of Mary and of John. This view contrasts with the relational logic of De Morgan, Peirce, Schröder and Russell himself, now standard in predicate logic. Notably, Leibniz also declared space and time to be inherently relational.\n\nAlthough the mathematical notion of function was implicit in trigonometric and logarithmic tables, which existed in his day, Leibniz was the first, in 1692 and 1694, to employ it explicitly, to denote any of several geometric concepts derived from a curve, such as abscissa, ordinate, tangent, chord, and the perpendicular. In the 18th century, \"function\" lost these geometrical associations. Leibniz also believed that the sum of an infinite number of zeros would equal to one half using the analogy of the creation of the world from nothing. Leibniz was also one of the pioneers in actuarial science, calculating the purchase price of life annuities and the liquidation of a state's debt.\n\nLeibniz's discoveries of Boolean algebra and of symbolic logic, also relevant to mathematics, are discussed in the preceding section. The best overview of Leibniz's writings on calculus may be found in Bos (1974).\n\nLeibniz arranged the coefficients of a system of linear equations into an array, now called a matrix, in order to find a solution to the system if it existed. This method was later called Gaussian elimination. Leibniz laid down the foundations and theory of determinants, although Seki Kowa discovered determinants well before Leibniz. His works show calculating the determinants using cofactors. Calculating the determinant using cofactors is named the Leibniz formula. Finding the determinant of a matrix using this method proves impractical with large \"n\", requiring to calculate \"n!\" products and the number of n-permutations. He also solved systems of linear equations using determinants, which is now called Cramer's rule. This method for solving systems of linear equations based on determinants was found in 1684 by Leibniz (Cramer published his findings in 1750). Although Gaussian elimination requires formula_1 arithmetic operations, linear algebra textbooks still teach cofactor expansion before LU factorization.\n\nThe Leibniz formula for states that\nLeibniz wrote that circles \"can most simply be expressed by this series, that is, the aggregate of fractions alternately added and subtracted.\" However this formula is only accurate with a large number of terms, using 10,000,000 terms to obtain the correct value of to 8 decimal places. Leibniz attempted to create a definition for a straight line while attempting to prove the parallel postulate. While most mathematicians defined a straight line as the shortest line between two points, Leibniz believed that this was merely a property of a straight line rather than the definition.\n\nLeibniz is credited, along with Sir Isaac Newton, with the discovery of calculus (differential and integral calculus). According to Leibniz's notebooks, a critical breakthrough occurred on 11 November 1675, when he employed integral calculus for the first time to find the area under the graph of a function . He introduced several notations used to this day, for instance the integral sign , representing an elongated S, from the Latin word \"summa\", and the used for differentials, from the Latin word \"differentia\". Leibniz did not publish anything about his calculus until 1684. Leibniz expressed the inverse relation of integration and differentiation, later called the fundamental theorem of calculus, by means of a figure in his 1693 paper \"Supplementum geometriae dimensoriae...\". However, James Gregory is credited for the theorem's discovery in geometric form, Isaac Barrow proved a more generalized geometric version, and Newton developed supporting theory. The concept became more transparent as developed through Leibniz's formalism and new notation. The product rule of differential calculus is still called \"Leibniz's law\". In addition, the theorem that tells how and when to differentiate under the integral sign is called the Leibniz integral rule.\n\nLeibniz exploited infinitesimals in developing calculus, manipulating them in ways suggesting that they had paradoxical algebraic properties. George Berkeley, in a tract called \"The Analyst\" and also in \"De Motu\", criticized these. A recent study argues that Leibnizian calculus was free of contradictions, and was better grounded than Berkeley's empiricist criticisms.\nFrom 1711 until his death, Leibniz was engaged in a dispute with John Keill, Newton and others, over whether Leibniz had invented calculus independently of Newton. This subject is treated at length in the article Leibniz–Newton calculus controversy.\n\nThe use of infinitesimals in mathematics was frowned upon by followers of Karl Weierstrass, but survived in science and engineering, and even in rigorous mathematics, via the fundamental computational device known as the differential. Beginning in 1960, Abraham Robinson worked out a rigorous foundation for Leibniz's infinitesimals, using model theory, in the context of a field of hyperreal numbers. The resulting non-standard analysis can be seen as a belated vindication of Leibniz's mathematical reasoning. Robinson's transfer principle is a mathematical implementation of Leibniz's heuristic law of continuity, while the standard part function implements the Leibnizian transcendental law of homogeneity.\n\nLeibniz was the first to use the term \"analysis situs\", later used in the 19th century to refer to what is now known as topology. There are two takes on this situation. On the one hand, Mates, citing a 1954 paper in German by Jacob Freudenthal, argues:\n\nAlthough for Leibniz the situs of a sequence of points is completely determined by the distance between them and is altered if those distances are altered, his admirer Euler, in the famous 1736 paper solving the Königsberg Bridge Problem and its generalizations, used the term \"geometria situs\" in such a sense that the situs remains unchanged under topological deformations. He mistakenly credits Leibniz with originating this concept. ... [It] is sometimes not realized that Leibniz used the term in an entirely different sense and hence can hardly be considered the founder of that part of mathematics.\n\nBut Hideaki Hirano argues differently, quoting Mandelbrot:\n\nTo sample Leibniz' scientific works is a sobering experience. Next to calculus, and to other thoughts that have been carried out to completion, the number and variety of premonitory thrusts is overwhelming. We saw examples in \"packing\", ... My Leibniz mania is further reinforced by finding that for one moment its hero attached importance to geometric scaling. In \"Euclidis Prota\" ..., which is an attempt to tighten Euclid's axioms, he states ...: \"I have diverse definitions for the straight line. The straight line is a curve, any part of which is similar to the whole, and it alone has this property, not only among curves but among sets.\" This claim can be proved today.\n\nThus the fractal geometry promoted by Mandelbrot drew on Leibniz's notions of self-similarity and the principle of continuity: \"Natura non facit saltus\". We also see that when Leibniz wrote, in a metaphysical vein, that \"the straight line is a curve, any part of which is similar to the whole\", he was anticipating topology by more than two centuries. As for \"packing\", Leibniz told his friend and correspondent Des Bosses to imagine a circle, then to inscribe within it three congruent circles with maximum radius; the latter smaller circles could be filled with three even smaller circles by the same procedure. This process can be continued infinitely, from which arises a good idea of self-similarity. Leibniz's improvement of Euclid's axiom contains the same concept.\n\nLeibniz's writings are currently discussed, not only for their anticipations and possible discoveries not yet recognized, but as ways of advancing present knowledge. Much of his writing on physics is included in Gerhardt's \"Mathematical Writings\".\n\nLeibniz contributed a fair amount to the statics and dynamics emerging around him, often disagreeing with Descartes and Newton. He devised a new theory of motion (dynamics) based on kinetic energy and potential energy, which posited space as relative, whereas Newton was thoroughly convinced that space was absolute. An important example of Leibniz's mature physical thinking is his \"Specimen Dynamicum\" of 1695.\n\nUntil the discovery of subatomic particles and the quantum mechanics governing them, many of Leibniz's speculative ideas about aspects of nature not reducible to statics and dynamics made little sense. For instance, he anticipated Albert Einstein by arguing, against Newton, that space, time and motion are relative, not absolute: \"As for my own opinion, I have said more than once, that I hold space to be something merely relative, as time is, that I hold it to be an order of coexistences, as time is an order of successions.\"\n\nLeibniz held a relationist notion of space and time, against Newton's substantivalist views. According to Newton's substantivalism, space and time are entities in their own right, existing independently of things. Leibniz's relationism, on the other hand, describes space and time as systems of relations that exist between objects. The rise of general relativity and subsequent work in the history of physics has put Leibniz's stance in a more favorable light.\n\nOne of Leibniz's projects was to recast Newton's theory as a vortex theory. However, his project went beyond vortex theory, since at its heart there was an attempt to explain one of the most difficult problems in physics, that of the origin of the cohesion of matter.\n\nThe principle of sufficient reason has been invoked in recent cosmology, and his identity of indiscernibles in quantum mechanics, a field some even credit him with having anticipated in some sense. Those who advocate digital philosophy, a recent direction in cosmology, claim Leibniz as a precursor. In addition to his theories about the nature of reality, Leibniz's contributions to the development of calculus have also had a major impact on physics.\n\nLeibniz's \"vis viva\" (Latin for \"living force\") is , twice the modern kinetic energy. He realized that the total energy would be conserved in certain mechanical systems, so he considered it an innate motive characteristic of matter. Here too his thinking gave rise to another regrettable nationalistic dispute. His \"vis viva\" was seen as rivaling the conservation of momentum championed by Newton in England and by Descartes in France; hence academics in those countries tended to neglect Leibniz's idea. In reality, both energy and momentum are conserved, so the two approaches are equally valid.\n\nBy proposing that the earth has a molten core, he anticipated modern geology. In embryology, he was a preformationist, but also proposed that organisms are the outcome of a combination of an infinite number of possible microstructures and of their powers. In the life sciences and paleontology, he revealed an amazing transformist intuition, fueled by his study of comparative anatomy and fossils. One of his principal works on this subject, \"Protogaea\", unpublished in his lifetime, has recently been published in English for the first time. He worked out a primal organismic theory. In medicine, he exhorted the physicians of his time—with some results—to ground their theories in detailed comparative observations and verified experiments, and to distinguish firmly scientific and metaphysical points of view.\n\nPsychology had been a central interest of Leibniz. He appears to be an \"underappreciated pioneer of psychology\" He wrote on topics which are now regarded as fields of psychology: attention and consciousness, memory, learning (association), motivation (the act of \"striving\"), emergent individuality, the general dynamics of development (evolutionary psychology). His discussions in the \"New Essays\" and \"Monadology\" often rely on everyday observations such as the behaviour of a dog or the noise of the sea, and he develops intuitive analogies (the synchronous running of clocks or the balance spring of a clock). He also devised postulates and principles that apply to psychology: the continuum of the unnoticed \"petite perceptions\" to the distinct, self-aware apperception, and psychophysical parallelism from the point of view of causality and of purpose: “Souls act according to the laws of final causes, through aspirations, ends and means. Bodies act according to the laws of efficient causes, i.e. the laws of motion. And these two realms, that of efficient causes and that of final causes, harmonize with one another.” This idea refers to the mind-body problem, stating that the mind and brain do not act upon each other, but act alongside each other separately but in harmony. Leibniz, however, did not use the term \"psychologia\".\nLeibniz’ epistemological position—against John Locke and English empiricism (sensualism)—was made clear: “Nihil est in intellectu quod non fuerit in sensu, nisi intellectu ipse.” – “Nothing is in the intellect that was not first in the senses, except the intellect itself.” Principles that are not present in sensory impressions can be recognised in human perception and consciousness: logical inferences, categories of thought, the principle of causality and the principle of purpose (teleology). \nLeibniz found his most important interpreter in Wilhelm Wundt, founder of psychology as a discipline. Wundt used the \"… nisi intellectu ipse\" quotation 1862 on the title page of his \"Beiträge zur Theorie der Sinneswahrnehmung\" (Contributions on the Theory of Sensory Perception) and published a detailed and aspiring monograph on Leibniz Wundt shaped the term apperception, introduced by Leibniz, into an experimental psychologically based apperception psychology that included neuropsychological modelling – an excellent example of how a concept created by a great philosopher could stimulate a psychological research program. One principle in the thinking of Leibniz played a fundamental role: “the principle of equality of separate but corresponding viewpoints.” Wundt characterized this style of thought (perspectivism) in a way that also applied for him—viewpoints that \"supplement one another, while also being able to appear as opposites that only resolve themselves when considered more deeply.\" \nMuch of Leibniz's work went on to have a great impact on the field of psychology. Leibniz thought that there are many petites perceptions, or small perceptions of which we perceive but of which we are unaware. He believed that by the principle that phenomena found in nature were continuous by default, it was likely that the transition between conscious and unconscious states had intermediary steps. For this to be true, there must also be a portion of the mind of which we are unaware at any given time. His theory regarding consciousness in relation to the principle of continuity can be seen as an early theory regarding the stages of sleep. In this way, Leibniz's theory of perception can be viewed as one of many theories leading up to the idea of the unconscious. Leibniz was a direct influence on Ernst Platner, who is credited with originally coining the term Unbewußtseyn (unconscious). Additionally, the idea of subliminal stimuli can be traced back to his theory of small perceptions. Leibniz's ideas regarding music and tonal perception went on to influence the laboratory studies of Wilhelm Wundt.\n\nIn public health, he advocated establishing a medical administrative authority, with powers over epidemiology and veterinary medicine. He worked to set up a coherent medical training program, oriented towards public health and preventive measures. In economic policy, he proposed tax reforms and a national insurance program, and discussed the balance of trade. He even proposed something akin to what much later emerged as game theory. In sociology he laid the ground for communication theory.\n\nIn 1906, Garland published a volume of Leibniz's writings bearing on his many practical inventions and engineering work. To date, few of these writings have been translated into English. Nevertheless, it is well understood that Leibniz was a serious inventor, engineer, and applied scientist, with great respect for practical life. Following the motto \"theoria cum praxi\", he urged that theory be combined with practical application, and thus has been claimed as the father of applied science. He designed wind-driven propellers and water pumps, mining machines to extract ore, hydraulic presses, lamps, submarines, clocks, etc. With Denis Papin, he invented a steam engine. He even proposed a method for desalinating water. From 1680 to 1685, he struggled to overcome the chronic flooding that afflicted the ducal silver mines in the Harz Mountains, but did not succeed.\n\nLeibniz may have been the first computer scientist and information theorist. Early in life, he documented the binary numeral system (base 2), then revisited that system throughout his career. While Leibniz was examining other cultures to compare his metaphysical views, he encountered an ancient Chinese book \"I Ching\". Leibniz interpreted a diagram which showed yin and yang and corresponded it to a zero and one. More information can be found in the Sinophile section. Leibniz may have plagiarized Juan Caramuel y Lobkowitz and Thomas Harriot, who independently developed the binary system, as he was familiar with their works on the binary system. Juan Caramuel y Lobkowitz worked extensively on logarithms including logarithms with base 2. Thomas Harriot's manuscripts contained a table of binary numbers and their notation, which demonstrated that any number could be written on a base 2 system. Regardless, Leibniz simplified the binary system and articulated logical properties such as conjunction, disjunction, negation, identity, inclusion, and the empty set. He anticipated Lagrangian interpolation and algorithmic information theory. His calculus ratiocinator anticipated aspects of the universal Turing machine. In 1961, Norbert Wiener suggested that Leibniz should be considered the patron saint of cybernetics.\n\nIn 1671, Leibniz began to invent a machine that could execute all four arithmetic operations, gradually improving it over a number of years. This \"stepped reckoner\" attracted fair attention and was the basis of his election to the Royal Society in 1673. A number of such machines were made during his years in Hanover by a craftsman working under his supervision. They were not an unambiguous success because they did not fully mechanize the carry operation. Couturat reported finding an unpublished note by Leibniz, dated 1674, describing a machine capable of performing some algebraic operations. Leibniz also devised a (now reproduced) cipher machine, recovered by Nicholas Rescher in 2010. In 1693, Leibniz described a design of a machine which could, in theory, integrate differential equations, which he called \"integraph\".\n\nLeibniz was groping towards hardware and software concepts worked out much later by Charles Babbage and Ada Lovelace. In 1679, while mulling over his binary arithmetic, Leibniz imagined a machine in which binary numbers were represented by marbles, governed by a rudimentary sort of punched cards. Modern electronic digital computers replace Leibniz's marbles moving by gravity with shift registers, voltage gradients, and pulses of electrons, but otherwise they run roughly as Leibniz envisioned in 1679.\n\nLater in Leibniz’s career (after the death of von Boinburg), Leibniz moved to Paris and accepted a position as a librarian in the Hanoverian court of Johann Friedrich, Duke of Brunswick-Luneburg. Leibniz’s predecessor, Tobias Fleischer, had already created a cataloging system for the Duke’s library but it was a clumsy attempt. At this library, Leibniz focused more on advancing the library than on the cataloging. For instance, within a month of taking the new position, he developed a comprehensive plan to expand the library. He was one of the first to consider developing a core collection for a library and felt “that a library for display and ostentation is a luxury and indeed superfluous, but a well-stocked and organized library is important and useful for all areas of human endeavor and is to be regarded on the same level as schools and churches”. Unfortunately, Leibniz lacked the funds to develop the library in this manner. After working at this library, by the end of 1690 Leibnez was appointed as privy-councilor and librarian of the Bibliotheca Augusta at Wolfenbuettel. It was an extensive library with at least 25,946 printed volumes. At this library, Leibniz sought to improve the catalog. He was not allowed to make complete changes to the existing closed catalog, but was allowed to improve upon it so he started on that task immediately. He created an alphabetical author catalog and had also created other cataloging methods that were not implemented. While serving as librarian of the ducal libraries in Hanover and Wolfenbuettel, Leibniz effectively became one of the founders of library science. He also designed a book indexing system in ignorance of the only other such system then extant, that of the Bodleian Library at Oxford University. He also called on publishers to distribute abstracts of all new titles they produced each year, in a standard form that would facilitate indexing. He hoped that this abstracting project would eventually include everything printed from his day back to Gutenberg. Neither proposal met with success at the time, but something like them became standard practice among English language publishers during the 20th century, under the aegis of the Library of Congress and the British Library.\n\nHe called for the creation of an empirical database as a way to further all sciences. His \"characteristica universalis\", calculus ratiocinator, and a \"community of minds\"—intended, among other things, to bring political and religious unity to Europe—can be seen as distant unwitting anticipations of artificial languages (e.g., Esperanto and its rivals), symbolic logic, even the World Wide Web.\n\nLeibniz emphasized that research was a collaborative endeavor. Hence he warmly advocated the formation of national scientific societies along the lines of the British Royal Society and the French Academie Royale des Sciences. More specifically, in his correspondence and travels he urged the creation of such societies in Dresden, Saint Petersburg, Vienna, and Berlin. Only one such project came to fruition; in 1700, the Berlin Academy of Sciences was created. Leibniz drew up its first statutes, and served as its first President for the remainder of his life. That Academy evolved into the German Academy of Sciences, the publisher of the ongoing critical edition of his works.\n\nWith the possible exception of Marcus Aurelius, no philosopher has ever had as much experience with practical affairs of state as Leibniz. Leibniz's writings on law, ethics, and politics were long overlooked by English-speaking scholars, but this has changed of late.\n\nWhile Leibniz was no apologist for absolute monarchy like Hobbes, or for tyranny in any form, neither did he echo the political and constitutional views of his contemporary John Locke, views invoked in support of liberalism, in 18th-century America and later elsewhere. The following excerpt from a 1695 letter to Baron J. C. Boyneburg's son Philipp is very revealing of Leibniz's political sentiments:\n\nAs for ... the great question of the power of sovereigns and the obedience their peoples owe them, I usually say that it would be good for princes to be persuaded that their people have the right to resist them, and for the people, on the other hand, to be persuaded to obey them passively. I am, however, quite of the opinion of Grotius, that one ought to obey as a rule, the evil of revolution being greater beyond comparison than the evils causing it. Yet I recognize that a prince can go to such excess, and place the well-being of the state in such danger, that the obligation to endure ceases. This is most rare, however, and the theologian who authorizes violence under this pretext should take care against excess; excess being infinitely more dangerous than deficiency.\n\nIn 1677, Leibniz called for a European confederation, governed by a council or senate, whose members would represent entire nations and would be free to vote their consciences; this is sometimes considered an anticipation of the European Union. He believed that Europe would adopt a uniform religion. He reiterated these proposals in 1715.\n\nBut at the same time, he arrived to propose an interreligious and multicultural project to create a universal system of justice, which required from him a broad interdisciplinary perspective. In order to propose it, he combined linguistics, especially sinology, moral and law philosophy, management, economics, and politics.\n\nLeibniz devoted considerable intellectual and diplomatic effort to what would now be called ecumenical endeavor, seeking to reconcile first the Roman Catholic and Lutheran churches, and later the Lutheran and Reformed churches. In this respect, he followed the example of his early patrons, Baron von Boyneburg and the Duke John Frederick—both cradle Lutherans who converted to Catholicism as adults—who did what they could to encourage the reunion of the two faiths, and who warmly welcomed such endeavors by others. (The House of Brunswick remained Lutheran because the Duke's children did not follow their father.) These efforts included corresponding with the French bishop Jacques-Bénigne Bossuet, and involved Leibniz in some theological controversy. He evidently thought that the thoroughgoing application of reason would suffice to heal the breach caused by the Reformation.\n\nLeibniz the philologist was an avid student of languages, eagerly latching on to any information about vocabulary and grammar that came his way. He refuted the belief, widely held by Christian scholars in his day, that Hebrew was the primeval language of the human race. He also refuted the argument, advanced by Swedish scholars in his day, that a form of proto-Swedish was the ancestor of the Germanic languages. He puzzled over the origins of the Slavic languages and was fascinated by classical Chinese. Leibniz was also an expert in the Sanskrit language.\n\nHe published the \"princeps editio\" (first modern edition) of the late medieval \"Chronicon Holtzatiae\", a Latin chronicle of the County of Holstein.\n\nLeibniz was perhaps the first major European intellectual to take a close interest in Chinese civilization, which he knew by corresponding with, and reading other works by, European Christian missionaries posted in China. Having read \"Confucius Sinarum Philosophus\" on the first year of its publication, he concluded that Europeans could learn much from the Confucian ethical tradition. He mulled over the possibility that the Chinese characters were an unwitting form of his universal characteristic. He noted with fascination how the \"I Ching\" hexagrams correspond to the binary numbers from 000000 to 111111, and concluded that this mapping was evidence of major Chinese accomplishments in the sort of philosophical mathematics he admired. Leibniz communicated his ideas of the binary system representing Christianity to the Emperor of China hoping it would convert him. Leibniz may be the only major Western philosopher who attempted to accommodate Confucian ideas to prevailing European beliefs.\n\nLeibniz's attraction to Chinese philosophy originates from his perception that Chinese philosophy was similar to his own. The historian E.R. Hughes suggests that Leibniz's ideas of \"simple substance\" and \"pre-established harmony\" were directly influenced by Confucianism, pointing to the fact that they were conceived during the period that he was reading \"Confucius Sinarum Philosophus\".\n\nWhile making his grand tour of European archives to research the Brunswick family history that he never completed, Leibniz stopped in Vienna between May 1688 and February 1689, where he did much legal and diplomatic work for the Brunswicks. He visited mines, talked with mine engineers, and tried to negotiate export contracts for lead from the ducal mines in the Harz mountains. His proposal that the streets of Vienna be lit with lamps burning rapeseed oil was implemented. During a formal audience with the Austrian Emperor and in subsequent memoranda, he advocated reorganizing the Austrian economy, reforming the coinage of much of central Europe, negotiating a Concordat between the Habsburgs and the Vatican, and creating an imperial research library, official archive, and public insurance fund. He wrote and published an important paper on mechanics.\n\nLeibniz also wrote a short paper, \"Primae veritates\", first published by Louis Couturat in 1903 (pp. 518–523) summarizing his views on metaphysics. The paper is undated; that he wrote it while in Vienna in 1689 was determined only in 1999, when the ongoing critical edition finally published Leibniz's philosophical writings for the period 1677–90. Couturat's reading of this paper was the launching point for much 20th-century thinking about Leibniz, especially among analytic philosophers. But after a meticulous study of all of Leibniz's philosophical writings up to 1688—a study the 1999 additions to the critical edition made possible—Mercer (2001) begged to differ with Couturat's reading; the jury is still out.\n\nWhen Leibniz died, his reputation was in decline. He was remembered for only one book, the \"Théodicée\", whose supposed central argument Voltaire lampooned in his popular book \"Candide\", which concludes with the character Candide saying, \"Non liquet\" (it is not clear), a term that was applied during the Roman Republic to a legal verdict of \"not proven\". Voltaire's depiction of Leibniz's ideas was so influential that many believed it to be an accurate description. Thus Voltaire and his \"Candide\" bear some of the blame for the lingering failure to appreciate and understand Leibniz's ideas. Leibniz had an ardent disciple, Christian Wolff, whose dogmatic and facile outlook did Leibniz's reputation much harm. He also influenced David Hume who read his \"Théodicée\" and used some of his ideas. In any event, philosophical fashion was moving away from the rationalism and system building of the 17th century, of which Leibniz had been such an ardent proponent. His work on law, diplomacy, and history was seen as of ephemeral interest. The vastness and richness of his correspondence went unrecognized.\n\nMuch of Europe came to doubt that Leibniz had discovered calculus independently of Newton, and hence his whole work in mathematics and physics was neglected. Voltaire, an admirer of Newton, also wrote \"Candide\" at least in part to discredit Leibniz's claim to having discovered calculus and Leibniz's charge that Newton's theory of universal gravitation was incorrect.\n\nLeibniz's long march to his present glory began with the 1765 publication of the \"Nouveaux Essais\", which Kant read closely. In 1768, Louis Dutens edited the first multi-volume edition of Leibniz's writings, followed in the 19th century by a number of editions, including those edited by Erdmann, Foucher de Careil, Gerhardt, Gerland, Klopp, and Mollat. Publication of Leibniz's correspondence with notables such as Antoine Arnauld, Samuel Clarke, Sophia of Hanover, and her daughter Sophia Charlotte of Hanover, began.\n\nIn 1900, Bertrand Russell published a critical study of Leibniz's metaphysics. Shortly thereafter, Louis Couturat published an important study of Leibniz, and edited a volume of Leibniz's heretofore unpublished writings, mainly on logic. They made Leibniz somewhat respectable among 20th-century analytical and linguistic philosophers in the English-speaking world (Leibniz had already been of great influence to many Germans such as Bernhard Riemann). For example, Leibniz's phrase \"salva veritate\", meaning interchangeability without loss of or compromising the truth, recurs in Willard Quine's writings. Nevertheless, the secondary literature on Leibniz did not really blossom until after World War II. This is especially true of English speaking countries; in Gregory Brown's bibliography fewer than 30 of the English language entries were published before 1946. American Leibniz studies owe much to Leroy Loemker (1904–1985) through his translations and his interpretive essays in LeClerc (1973).\n\nNicholas Jolley has surmised that Leibniz's reputation as a philosopher is now perhaps higher than at any time since he was alive. Analytic and contemporary philosophy continue to invoke his notions of identity, individuation, and possible worlds. Work in the history of 17th- and 18th-century ideas has revealed more clearly the 17th-century \"Intellectual Revolution\" that preceded the better-known Industrial and commercial revolutions of the 18th and 19th centuries.\n\nIn 1985, the German government created the Leibniz Prize, offering an annual award of 1.55 million euros for experimental results and 770,000 euros for theoretical ones. It was the worlds largest prize for scientific achievement prior to the Fundamental Physics Prize.\n\nThe collection of manuscript papers of Leibniz at the Gottfried Wilhelm Leibniz Bibliothek – Niedersächische Landesbibliothek were inscribed on UNESCO's Memory of the World Register in 2007.\n\nLeibniz mainly wrote in three languages: scholastic Latin, French and German. During his lifetime, he published many pamphlets and scholarly articles, but only two \"philosophical\" books, the \"Combinatorial Art\" and the \"Théodicée\". (He published numerous pamphlets, often anonymous, on behalf of the House of Brunswick-Lüneburg, most notably the \"De jure suprematum\" a major consideration of the nature of sovereignty.) One substantial book appeared posthumously, his \"Nouveaux essais sur l'entendement humain\", which Leibniz had withheld from publication after the death of John Locke. Only in 1895, when Bodemann completed his catalogue of Leibniz's manuscripts and correspondence, did the enormous extent of Leibniz's \"Nachlass\" become clear: about 15,000 letters to more than 1000 recipients plus more than 40,000 other items. Moreover, quite a few of these letters are of essay length. Much of his vast correspondence, especially the letters dated after 1700, remains unpublished, and much of what is published has been so only in recent decades. The amount, variety, and disorder of Leibniz's writings are a predictable result of a situation he described in a letter as follows:\n\nI cannot tell you how extraordinarily distracted and spread out I am. I am trying to find various things in the archives; I look at old papers and hunt up unpublished documents. From these I hope to shed some light on the history of the [House of] Brunswick. I receive and answer a huge number of letters. At the same time, I have so many mathematical results, philosophical thoughts, and other literary innovations that should not be allowed to vanish that I often do not know where to begin.\n\nThe extant parts of the critical edition of Leibniz's writings are organized as follows:\n\nThe systematic cataloguing of all of Leibniz's \"Nachlass\" began in 1901. It was hampered by two world wars and decades of German division in two states with the cold war's \"iron curtain\" in between, separating scholars, and also scattering portions of his literary estates. The ambitious project has had to deal with seven languages contained in some 200,000 pages of written and printed paper. In 1985 it was reorganized and included in a joint program of German federal and state (\"Länder\") academies. Since then the branches in Potsdam, Münster, Hanover and Berlin have jointly published 57 volumes of the critical edition, with an average of 870 pages, and prepared index and concordance works.\n\nThe year given is usually that in which the work was completed, not of its eventual publication.\nMeditationes de cognitione, veritate et ideis\n\n\nSix important collections of English translations are Wiener (1951), Parkinson (1966), Loemker (1969), Ariew and Garber (1989), Woolhouse and Francks (1998), and Strickland (2006). The ongoing critical edition of all of Leibniz's writings is \"Sämtliche Schriften und Briefe\".\n\nLeibniz is receiving popular attention. The Google Doodle for July 1, 2018 celebrated Leibniz's 372nd birthday. Using a quill, his hand is shown writing \"Google\" in binary ASCII code.\n\nOne of the earliest popular but indirect exposition of Leibniz was Voltaire's satire \"Candide\", published in 1759. Leibniz was lampooned as Professor Pangloss, described as \"the greatest philosopher of the Holy Roman Empire\".\n\nLeibniz also appears as one of the main historical figures in Neal Stephenson's series of novels \"The Baroque Cycle\". Stephenson credits readings and discussions concerning Leibniz for inspiring him to write the series.\n\n\n\nAn updated bibliography of more than 25.000 titles is available at Leibniz Bibliographie.\n\n\n\n\n"}
{"id": "41964210", "url": "https://en.wikipedia.org/wiki?curid=41964210", "title": "Great 120-cell honeycomb", "text": "Great 120-cell honeycomb\n\nIn the geometry of hyperbolic 4-space, the great 120-cell honeycomb is one of four regular star-honeycombs. With Schläfli symbol {5,5/2,5,3}, it has three great 120-cells around each face. It is dual to the order-5 icosahedral 120-cell honeycomb.\n\nIt can be seen as a greatening of the 120-cell honeycomb, and is thus analogous to the three-dimensional great dodecahedron {5,5/2} and four-dimensional great 120-cell {5,5/2,5}. It has density 10.\n\n\n"}
{"id": "48633254", "url": "https://en.wikipedia.org/wiki?curid=48633254", "title": "History of arithmetic", "text": "History of arithmetic\n\nThe history of arithmetic includes the period from the emergence of counting before the formal definition of numbers and arithmetic operations over them by means of a system of axioms. Arithmetic — the science of numbers, their properties and their relations — is one of the main mathematical sciences. It is closely connected with algebra and the theory of numbers.\n\nThe practical need for counting, elementary measurements and calculations became the reason for the emergence of arithmetic. The first authentic data on arithmetic knowledge are found in the historical monuments of Babylon and Ancient Egypt in the third and second millennia BC. The big contribution to the development of arithmetic was made by the ancient Greek mathematicians, in particular Pythagoreans, who tried to define all regularities of the world in terms of numbers. In the Middle Ages trade and approximate calculations were the main scope of arithmetic. Arithmetic developed first of all in India and the countries of Islam and only then came to Western Europe. In the seventeenth century the needs of astronomy, mechanics, and more difficult commercial calculations put before arithmetic new challenges regarding methods of calculation and gave an impetus to further development.\n\nTheoretical justifications of the idea of number are connected first of all with the definition of \"natural number\" and Peano's axioms formulated in 1889. They were followed by strict definitions of rational, real, negative and complex numbers. Further expansion of the concept of number is possible only if one of the arithmetic laws is rejected.\n\nIf in two sets of subjects each element of one set has only one corresponding element in the other set, these sets are one-to-one. Such actual comparison when subjects were displayed in two ranks, was used by primitive tribes in trade. This approach gives the opportunity to establish quantitative ratios between groups of objects and doesn't demand the concept of number.\n\nFurther there were natural standards for counting, for example, fingers of hands, and then sets of standards, such as hands. The advent of the standards symbolizing concrete numbers also is connected to the emergence of the concept of number. Thus the number of things to be counted was compared to the Moon in the sky, the number of eyes, and the number of fingers on a hand. Later numerous standards were replaced with one of the most convenient, usually fingers of hands and/or feet.\n"}
{"id": "28770638", "url": "https://en.wikipedia.org/wiki?curid=28770638", "title": "Irrigation informatics", "text": "Irrigation informatics\n\nIrrigation informatics is a newly emerging academic field that is a cross-disciplinary science using informatics to study the information flows and data management related to irrigation. The field is one of many new informatics sub-specialities that uses the science of information, the practice of information processing, and the engineering of information systems to advance a biophysical science or engineering field.\n\nAgricultural productivity increases are eagerly sought by governments and industry, spurred by the realisation that world food production must double in the 21st century to feed growing populations and that as irrigation makes up 36% of global food production, but that new land for irrigation growth is very limited, irrigation efficiency must increase. Since irrigation science is a mature and stable field, irrigation researchers are looking to cross-disciplinary science to bring about production gains and informatics is one such science along with others such as social science. Much of the driver for work in the area of irrigation informatics is the perceived success of other informatics fields such as health informatics.\n\nIrrigation informatics is very much a part of the wider research into irrigation wherever information technology or data systems are used, however the term \"informatics\" is not always used to describe research involving computer systems and data management so that \"information science\" or \"information technology\" may alternatively be used. This leads to a great number of irrigation informatics articles not using the term \"irrigation informatics\". There are currently no formal publications (journals) that focus on \"irrigation informatics\" with the publication most likely to present articles on the topic being Computers and electronics in Agriculture or one of the many irrigation science journals such as Irrigation Science.\n\nRecent work in the general area of irrigation informatics has mentioned the exact phrase \"Irrigation Informatics\" with at least one publication in scientific conference proceedings using it in its title.\n\nMeteorological informatics, as with all informatics, are increasingly being used to handle the growing volumes of data that are available from sensors, remote sensing and scientific models. The Australian Bureau of Meteorology has recently implemented an XML data format, known as the Water Data Transfer Format (WDTF) and standard to be used by Australian government agencies and meteorological data suppliers when delivering data to the Bureau. This format includes specifications for evapotranspiration and other weather parameters that are useful for irrigation and may be used through implementations of irrigation informatics.\n\n"}
{"id": "8454563", "url": "https://en.wikipedia.org/wiki?curid=8454563", "title": "István Orosz", "text": "István Orosz\n\nIstván Orosz (born 24 October 1951 in Kecskemét) is a Hungarian painter, printmaker, graphic designer and animated film director. He is known for his mathematically inspired works, impossible objects, optical illusions, double-meaning images and anamorphoses. The geometric art of István Orosz, with forced perspectives and optical illusions, has been compared to works by M. C. Escher.\n\nStudied at the Hungarian University of Arts and Design (now Moholy-Nagy University of Art and Design) in Budapest as pupil of István Balogh and Ernő Rubik. After graduating in 1975 he began to deal with theatre as stage designer and animated film as animator and film director. He is known as painter, printmaker, poster designer, and illustrator as well. He likes to use visual paradox, double meaning images and illusionistic approaches while following traditional printing techniques such as woodcutting and etching. He also tries to renew the technique of anamorphosis. He is a regular participant in the major international biennials of posters and graphic art and his works has been shown in individual and group exhibitions in Hungary and abroad. Film director at the PannóniaFilm Studio in Budapest, Habil. professor at University of West Hungary in Sopron, co-founder of Hungarian Poster Association, member of Alliance Graphique International (AGI) and Hungarian Art Academie. He often uses ΟΥΤΙΣ, or Utisz, (pronounced: outis) (No one) as artist's pseudonym.\n\n\nDuring the last two decades - when most of the works shown here were made – the activities of the poster designer, the printmaker, the illustrator, and the film director have completed each other. Many motive, stylistic features, technical solutions appeared in all of the media and for Orosz it seemingly did not cause any problem to cross the borders of the different genres. When he was drawing a poster usually he did it with the preciseness of illustrators, when he was illustrating a book, he did it with the narrative mood of filmmakers, if he was animating films, sometimes he used the several layers approach of etchers and engravers and for prints he often chose the emblematic simplifying way of depiction of posters. If we call him only a poster designer based on his functional prints, we narrow down his field of activity, we go closer to the truth if we associate him with „postering\" as a way of thinking, or if we call his many sided image depicting ourselves and our age as the poster-mirror of István Orosz. \"(Guy d'Obonner: Transfiguration of Poster - detail)\"\n\nIstván Orosz was known as poster designer in the first part of his career. He made mainly cultural posters for theatres, movies, galleries, museums and publishing houses At the time of the revolutions of 1989 in Eastern Europe he drew some political posters too. His \"Tovarishi Adieu\" (also used with text \"Tovarishi Koniec\" – that means Comrades it is over) appeared in many countries and it was known as symbolic image of changes in the area.\n\nArtists who design anamorphosis (anamorphosis is Greek for \"re-transformation\") play with perspective to create a distorted image that appears normal only when viewed from the correct angle or with the aid of curved mirrors. The technique was often used by Renaissance-era artists. Orosz tries to renew the technique of anamorphosis and his aim is to develop it as well when he gives a meaning to the distorted image, too. It is not an amorph picture any more, but a meaningful depiction that is independent from the result that appears in the mirror or viewed from a special point of view.\n\nThis approach of anamorphoses is suitable for expressing more sophisticated messages, and it fits to show more amusing fun.\n\n\n\n\n\n\n"}
{"id": "17956207", "url": "https://en.wikipedia.org/wiki?curid=17956207", "title": "KOV-21", "text": "KOV-21\n\nThe KOV-21 is a cryptographic PC card module developed under the auspices of the U.S. National Security Agency and manufactured by Sypris Electronics LLC. It is intended to be the cryptographic engine for next generation key management devices, such as the AN/PYQ-10 key loader, as part of the U.S. Government’s Cryptographic Modernization Initiative. Sypris was awarded a contract for production of KOV-21 units in November, 2007.\n"}
{"id": "35226515", "url": "https://en.wikipedia.org/wiki?curid=35226515", "title": "MAgPIE", "text": "MAgPIE\n\nMAgPIE is a non-linear, recursive, dynamic-optimization, global land and water-use model with a cost-minimization objective function.\nMAgPIE was developed and is employed by the land-use group working at the Potsdam Institute for Climate Impact Research (PIK). It links regional economic information with grid-based biophysical constraints simulated by the dynamic vegetation and hydrology model LPJmL. MAgPIE considers spatially-explicit patterns of production, land use change and water constraints in different world regions, consistently linking economic development with food and energy demand.\n\nThe model is based on static yield functions in order to model potential crop productivity and its related water use. For the biophysical supply simulation, spatially explicit 0.5° data is aggregated to a consistent number of clusters. Ten world regions represent the demand side of the model. Required calories for the demand categories (food and non-food energy intake) are determined by a cross-sectional country regression based on population and income projections. In order to fulfill the demand, the model allocates 19 cropping and 5 livestock activities to the spatially-explicit land and water resources, subject to resource, management and cost constraints. From 1995 MAgPIE simulates time-steps of 10 years. For each period the optimal land use pattern from the previous period is used as a starting point. \n\nThe demand for agricultural products is fixed for every region and every time-step. The drivers of agricultural demand are: time, income and population growth. Total demand is composed of: food demand, material demand, feed demand and seed demand. Food demand depends on food energy demand, and the share of crop and livestock products in the diet. Within livestock products, the share of different products (Ruminant meat, chicken meat, other meat, milk, eggs) is fixed at 1995 levels. The same is valid for the share of crops within total food calories and material demand. The share of livestock products in the total consumed food calories is an important driver for the land-use sector. Different statistical models are used to estimate plausible future scenarios. A calibration is used to reach the livestock shares of the Food Balance Sheets for 1995 for each region.\n\nFeed for livestock is produced as a mixture of concentrates, fodder, livestock products (e.g. bone meal), pasture, crop residues and conversion by-products (e.g. rapeseed cake) at predefined proportions. These differences in the livestock systems cause different emission levels from livestock.\n\nThe biophysical inputs for the simulations are obtained from the grid-based model LPJmL. The global vegetation model with managed land (LPJmL) also delivers values for water availability and requirements for each grid cell as well as the carbon content of the different vegetation types. Cropland, pasture, and irrigation water are fixed inputs in limited supply in each grid cell.\n\nMAgPIE takes four different cost types into account: production costs for crop and livestock production, investments in technological change, land conversion costs and intra-regional transport costs. By minimizing these four cost components on a global scale for the current time step, the model solution is obtained. Production costs in MAgPIE imply costs for labor, capital and intermediate inputs. They are specific for all crop and livestock types and are implemented as costs per area for crops (US$/ha) and costs per production unit of livestock (US$/ton). \n\nMAgPIE has two options to increase total production in agriculture at additional costs: land expansion and intensification. In MAgPIE the latter can be achieved by investments in technological change (TC). Investing in technological change triggers yield increases which lead then to a higher total production. At the same time the corresponding increases in agricultural land-use intensity raises costs for further yield increases. The reason is that intensification on land which is already used intensively is more expensive than intensification on extensively-used land.\n\nTo increase production another alternative is to expand cropland into non-agricultural land. The conversion causes additional costs for the preparation of new land and basic infrastructure investments, which are also taken into account. Intraregional transport costs arise for each commodity unit as a function of the distance to intraregional markets and therefore restrict land expansion in MAgPIE. This depends on the quality and accessibility of infrastructure. Intra-regional transport costs are higher for less accessible areas than for more accessible regions. This leads to higher overall costs of cropland expansion in those cases.\n"}
{"id": "13084071", "url": "https://en.wikipedia.org/wiki?curid=13084071", "title": "Magnetic anisotropy", "text": "Magnetic anisotropy\n\nMagnetic anisotropy is the directional dependence of a material's magnetic properties. The magnetic moment of magnetically anisotropic materials will tend to align with an easy axis, which is an energetically favorable direction of spontaneous magnetization. The two opposite directions along an easy axis are usually equivalent, and the actual direction of magnetization can be along either of them (see spontaneous symmetry breaking).\n\nIn contrast, a magnetically isotropic material has no preferential direction for its magnetic moment unless there is an applied magnetic field.\n\nMagnetic anisotropy is a prerequisite for hysteresis in ferromagnets: without it, a ferromagnet is superparamagnetic.\n\nThere are several sources of magnetic anisotropy:\n\nSuppose that a ferromagnet is single-domain in the strictest sense: the magnetization is uniform and rotates in unison. If the magnetic moment is formula_1 and the volume of the particle is formula_2, the magnetization is formula_3, where formula_4 is the saturation magnetization and formula_5 are direction cosines (components of a unit vector) so formula_6. The energy associated with magnetic anisotropy can depend on the direction cosines in various ways, the most common of which are discussed below.\n\nA magnetic particle with uniaxial anisotropy has one easy axis. If the easy axis is in the formula_7 direction, the anisotropy energy can be expressed as one of the forms:\n\nwhere formula_2 is the volume, formula_10 the anisotropy constant, and formula_11 the angle between the easy axis and the particle's magnetization. When shape anisotropy is explicitly considered, the symbol formula_12 is often used to indicate the anisotropy constant, instead of formula_10. In the widely used Stoner–Wohlfarth model, the anisotropy is uniaxial.\n\nA magnetic particle with triaxial anisotropy still has a single easy axis, but it also has a hard axis (direction of maximum energy) and an intermediate axis (direction associated with a saddle point in the energy). The coordinates can be chosen so the energy has the form\n\nIf formula_15 the easy axis is the formula_7 direction, the intermediate axis is the formula_17 direction and the hard axis is the formula_18 direction.\n\nA magnetic particle with cubic anisotropy has three or four easy axes, depending on the anisotropy parameters. The energy has the form\n\nIf formula_20 the easy axes are the formula_21 and formula_7 axes. If formula_23 there are four easy axes characterized by formula_24.\n\n"}
{"id": "48967169", "url": "https://en.wikipedia.org/wiki?curid=48967169", "title": "Miggy Biller", "text": "Miggy Biller\n\nMargherita Joan (Miggy) Biller is a British mathematics teacher, the head of mathematics at York College. She was named an MBE in the 2016 New Year Honours \"for services to mathematics in further education\".\n\nBiller taught mathematics at St Peter's School, York before moving to York College in 1988. At York College, she taught mathematics prodigy Daniel Lightwing, after whom the main character of the film \"X+Y\" was modeled.\nHer husband, Peter Biller, is a historian at the University of York.\n"}
{"id": "5498479", "url": "https://en.wikipedia.org/wiki?curid=5498479", "title": "Notation for theoretic scheduling problems", "text": "Notation for theoretic scheduling problems\n\nA convenient notation for theoretic scheduling problems was introduced by Ronald Graham, Eugene Lawler, Jan Karel Lenstra and Alexander Rinnooy Kan in. It consists of three fields: α, β and γ.\n\nEach field may be a comma separated list of words. The α field describes the machine environment, β the job characteristics and constraints, and γ the objective function.\n\nSince its introduction in the late 70's the notation has been constantly extended, sometimes inconsistently. As a result today there are some problems that appear with distinct notations in several papers. \n\nEach job comes with a given processing time.\n\n\nThese letters might be followed by the number of machines which is then fixed, here formula_1 stands then for a fixed number. For example formula_12 is the problem of assigning each of the formula_13 given jobs to one of the 2 given machines so to minimize the maximum total processing time over the machines.\n\n\nThe processing time may be equal for all jobs (formula_37, or formula_38) or even of unit length (formula_39, or formula_40). All processing times are assumed to be integers. In some older research papers however they are assumed to be rationals.\n\n\nPrecedence relations might be given for the jobs, in form of a partial order, meaning that if i is a predecessor of i' in that order, i' can start only when i is completed.\n\nIn the presence of a precedence relation one might in addition assume \"time lags\". Let formula_51 denote the start time of a job and formula_52 its completion time. Then the precedence relation formula_53 implies the constraint formula_54. If no time lag formula_55 is specified then it is assumed to be zero. Time lags can be positive or negative numbers.\n\n\nUsually the goal is to minimize some objective value. One difference is the notation formula_112 where the goal is to maximize the number of jobs that complete before their deadline. This is also called the \"throughput\". The objective value can be sum, possibly weighted by some given priority weights formula_113 per job.\n\nAdapted from \n\n\n\n\n"}
{"id": "396507", "url": "https://en.wikipedia.org/wiki?curid=396507", "title": "Percy Williams Bridgman", "text": "Percy Williams Bridgman\n\nPercy Williams Bridgman (21 April 1882 – 20 August 1961) was an American physicist who received the 1946 Nobel Prize in Physics for his work on the physics of high pressures. He also wrote extensively on the scientific method and on other aspects of the philosophy of science. The Bridgman effect and the Bridgman–Stockbarger technique are named after him.\n\nKnown to family and friends as \"Peter\", Bridgman was born in Cambridge, Massachusetts, and grew up in nearby Auburndale, Massachusetts.\n\nBridgman's parents were both born in New England. His father, Raymond Landon Bridgman, was \"profoundly religious and idealistic\" and worked as a newspaper reporter assigned to state politics. His mother, Mary Ann Maria Williams, was described as \"more conventional, sprightly, and competitive\".\n\nBridgman attended both elementary and high school in Auburndale, where he excelled at competitions in the classroom, on the playground, and while playing chess. Described as both shy and proud, his home life consisted of family music, card games, and domestic and garden chores. The family was deeply religious; reading the Bible each morning and attending a Congregational Church. However, Bridgman later became an atheist.\n\nBridgman entered Harvard University in 1900, and studied physics through to his Ph.D. From 1910 until his retirement, he taught at Harvard, becoming a full professor in 1919. In 1905, he began investigating the properties of matter under high pressure. A machinery malfunction led him to modify his pressure apparatus; the result was a new device enabling him to create pressures eventually exceeding 100,000 kgf/cm (10 GPa; 100,000 atmospheres). This was a huge improvement over previous machinery, which could achieve pressures of only 3,000 kgf/cm (0.3 GPa). This new apparatus led to an abundance of new findings, including a study of the compressibility, electric and thermal conductivity, tensile strength and viscosity of more than 100 different compounds. Bridgman is also known for his studies of electrical conduction in metals and properties of crystals. He developed the Bridgman seal and is the eponym for Bridgman's thermodynamic equations.\n\nBridgman made many improvements to his high-pressure apparatus over the years, and unsuccessfully attempted the synthesis of diamond many times.\n\nHis philosophy of science book \"The Logic of Modern Physics\" (1927) advocated operationalism and coined the term operational definition. In 1938 he participated in the International Committee composed to organise the International Congresses for the Unity of Science. He was also one of the 11 signatories to the Russell–Einstein Manifesto.\n\nBridgman married Olive Ware, of Hartford, Connecticut, in 1912. Ware's father, Edmund Asa Ware, was the founder and first president of Atlanta University. The couple had two children and were married for 50 years, living most of that time in Cambridge. The family also had a summer home in Randolph, New Hampshire, where Bridgman was known as a skilled mountain climber.\n\nBridgman was a \"penetrating analytical thinker\" with a \"fertile mechanical imagination\" and exceptional manual dexterity. He was a skilled plumber and carpenter, known to shun the assistance of professionals in these matters. He was also fond of music and played the piano, and took pride in his flower and vegetable gardens.\n\nBridgman committed suicide by gunshot after suffering from metastatic cancer for some time. His suicide note read in part, \"It isn't decent for society to make a man do this thing himself. Probably this is the last day I will be able to do it myself.\" Bridgman's words have been quoted by many in the assisted suicide debate.\n\nBridgman received Doctors, \"honoris causa\" from Stevens Institute (1934), Harvard (1939), Brooklyn Polytechnic (1941), Princeton (1950), Paris (1950), and Yale (1951). He received the Bingham Medal (1951) from the Society of Rheology, the Rumford Prize from the American Academy of Arts and Sciences (1919), the Elliott Cresson Medal (1932) from the Franklin Institute, the Gold Medal from Bakhuys Roozeboom Fund (founder Hendrik Willem Bakhuis Roozeboom) (1933) from the Royal Netherlands Academy of Arts and Sciences, and the Comstock Prize (1933) of the National Academy of Sciences.\n\nBridgman was a member of the American Physical Society and was its President in 1942. He was also a member of the American Association for the Advancement of Science, the American Academy of Arts and Sciences, the American Philosophical Society, and the National Academy of Sciences. He was a Foreign Member of the Royal Society and Honorary Fellow of the Physical Society of London.\n\nThe Percy W. Bridgman House, in Massachusetts, is a U.S. National Historic Landmark designated in 1975.\n\nIn 2014, the Commission on New Minerals, Nomenclature and Classification (CNMNC) of the International Mineralogical Association (IMA) approved the name bridgmanite for perovskite-structured (Mg,Fe)SiO, the Earth's most abundant mineral, in honor of his high-pressure research.\n\n\n\n\n"}
{"id": "339183", "url": "https://en.wikipedia.org/wiki?curid=339183", "title": "Points of the compass", "text": "Points of the compass\n\nThe points of the compass mark the divisions on a compass, which is primarily divided into four points: north, south, east, and west. These cardinal directions are further subdivided by the addition of the four intercardinal (or ordinal) directions—northeast (NE), southeast (SE), southwest (SW), and northwest (NW)—to indicate the eight principal winds. In meteorological usage, further intermediate points between cardinal and ordinal points, such as north-northeast (NNE) are added to give the 16 points of a compass rose.\nAt the most complete division are the full thirty-two points of the mariner's compass, which adds points such as north by east (NbE) between north and north-northeast, and northeast by north (NEbN) between north-northeast and northeast. A compass point allows reference to a specific course (or azimuth) in a colloquial fashion, without having to compute or remember degrees.\n\nThe European nautical tradition retained the term \"one point\" to describe of a circle in such phrases as \"two points to starboard\". By the middle of the 18th century, the 32-point system was extended with half- and quarter-points to allow 128 directions to be differentiated.\n\nThe names of the compass point directions follow these rules:\n\n\n\nIn summary, the 32-wind compass rose is yielded from the eight principal winds, eight half-winds and sixteen quarter-winds combined together, with each compass direction point at an ° angle from the next.\n\nIn the mariner's exercise of boxing the compass, all thirty-two points of the compass are named in clockwise order.\n\nThe traditional compass rose of eight winds (and its 16-wind and 32-wind derivatives) was invented by seafarers in the Mediterranean Sea during the Middle Ages (with no obvious connection to the twelve classical compass winds of the ancient Greeks and Romans). The traditional mariner's wind names were expressed in Italian, or more precisely, the Italianate Mediterranean lingua franca common among sailors in the 13th and 14th centuries, which was principally composed of Genoese (Ligurian), mixed with Venetian, Sicilian, Provençal, Catalan, Greek and Arabic terms from around the Mediterranean basin.\nThis Italianate patois was used to designate the names of the principal winds on the compass rose found in mariners' compasses and portolan charts of the 14th and 15th centuries. The \"traditional\" names of the eight principal winds are:\n\nLocal spelling variations are far more numerous than listed, e.g. Tramutana, Gregale, Grecho, Sirocco, Xaloc, Lebeg, Libezo, Leveche, Mezzodi, Migjorn, Magistro, Mestre, etc. Traditional compass roses will typically have the initials T, G, L, S, O, L, P, and M on the main points. Portolan charts also colour-coded the compass winds: black for the eight principal winds, green for the eight half-winds, and red for the sixteen quarter-winds.\n\nEach half-wind name is simply a combination of the two principal winds that it bisects, with the shortest name usually placed first, for example: NNE is \"Greco-Tramontana\"; ENE is \"Greco-Levante\"; SSE is \"Ostro-Scirocco\", etc. The quarter winds are expressed with an Italian phrase, \"\"Quarto di \"X\" verso \"Y\" ( one quarter from X towards Y), or \"X \"al\" Y\" (X to Y) or \"X \"per\" Y\" (X by Y). There are no irregularities to trip over; the closest principal wind always comes first, the more distant one second, for example: north-by-east is \"\"Quarto di Tramontana verso Greco\"; and northeast-by-north is \"Quarto di Greco verso Tramontana\"\".\n\nThe table below shows how the 32 compass points are named.\n\nEach point has an angular range of 11.250 degrees where: middle azimuth is the horizontal angular direction (from north) of the given compass bearing; minimum is the lower angular limit of the compass point; and maximum is the upper angular limit of the compass point.\n\nBy the middle of the 18th century, the 32-point system had been further extended by using half- and quarter-points to give a total of 128 directions.\nThese fractional points are named by appending, for example east, east, or east to the name of one of the 32 points. Each of the 96 fractional points can be named in two ways, depending on which of the two adjoining whole points is used, for example, NE is equivalent to NbEN. Either form is easily understood but alternative conventions as to correct usage developed in different countries and organisations. \"It is the custom in the United States Navy to box \"from\" north and south \"toward\" east and west, with the exception that divisions adjacent to a cardinal or inter-cardinal point are always referred to that point.\" The Royal Navy used the additional \"rule that quarter points were never read from a point beginning and ending with the same letter.\"\n\nCompass roses very rarely named the fractional points and only showed small, unlabelled markers as a guide for helmsmen.\n\nThe table below shows how each of the 128 directions are named. The first two columns give the number of points and degrees clockwise from north. The third gives the equivalent bearing to the nearest degree from north or south towards east or west. The \"CW\" column gives the fractional-point bearings increasing in the clockwise direction and \"CCW\" counterclockwise. The final three columns show three common naming conventions: No \"by\" avoids the use of \"by\" with fractional points; \"USN\" is the system used by the US Navy; and \"RN\" is the Royal Navy system. Colour coding shows whether each of the three naming systems matches the \"CW\" or \"CCW\" column.\n\n\n"}
{"id": "24910", "url": "https://en.wikipedia.org/wiki?curid=24910", "title": "Product topology", "text": "Product topology\n\nIn topology and related areas of mathematics, a product space is the Cartesian product of a family of topological spaces equipped with a natural topology called the product topology. This topology differs from another, perhaps more obvious, topology called the box topology, which can also be given to a product space and which agrees with the product topology when the product is over only finitely many spaces. However, the product topology is \"correct\" in that it makes the product space a categorical product of its factors, whereas the box topology is too fine; this is the sense in which the product topology is \"natural\".\n\nGiven \"X\" such that\n\nis the Cartesian product of the topological spaces \"X\", indexed by formula_2, and the canonical projections \"p\" : \"X\" → \"X\", the product topology on \"X\" is defined to be the coarsest topology (i.e. the topology with the fewest open sets) for which all the projections \"p\" are continuous. The product topology is sometimes called the Tychonoff topology.\n\nThe open sets in the product topology are unions (finite or infinite) of sets of the form formula_3, where each \"U\" is open in \"X\" and \"U\" ≠ \"X\" for only finitely many \"i\". In particular, for a finite product (in particular, for the product of two topological spaces), the set of all Cartesian products between one basis elements from each \"X\" gives a basis for the product topology of formula_4. That is, for a finite product, the set of all formula_3, where formula_6 is an element of the (chosen) basis of formula_7, is a basis for the product topology of formula_4.\n\nThe product topology on \"X\" is the topology generated by sets of the form \"p\"(\"U\"), where \"i\" is in \"I \" and \"U\" is an open subset of \"X\". In other words, the sets {\"p\"(\"U\")} form a subbase for the topology on \"X\". A subset of \"X\" is open if and only if it is a (possibly infinite) union of intersections of finitely many sets of the form \"p\"(\"U\"). The \"p\"(\"U\") are sometimes called open cylinders, and their intersections are cylinder sets.\n\nIn general, the product of the topologies of each \"X\" forms a basis for what is called the box topology on \"X\". In general, the box topology is finer than the product topology, but for finite products they coincide.\n\nIf one starts with the standard topology on the real line R and defines a topology on the product of \"n\" copies of R in this fashion, one obtains the ordinary Euclidean topology on R.\n\nThe Cantor set is homeomorphic to the product of countably many copies of the discrete space {0,1} and the space of irrational numbers is homeomorphic to the product of countably many copies of the natural numbers, where again each copy carries the discrete topology.\n\nSeveral additional examples are given in the article on the initial topology.\n\nThe product space \"X\", together with the canonical projections, can be characterized by the following universal property: If \"Y\" is a topological space, and for every \"i\" in \"I\", \"f\" : \"Y\" → \"X\" is a continuous map, then there exists \"precisely one\" continuous map \"f\" : \"Y\" → \"X\" such that for each \"i\" in \"I\" the following diagram commutes:\n\nThis shows that the product space is a product in the category of topological spaces. It follows from the above universal property that a map \"f\" : \"Y\" → \"X\" is continuous if and only if \"f\" = \"p\" o \"f\" is continuous for all \"i\" in \"I\". In many cases it is easier to check that the component functions \"f\" are continuous. Checking whether a map \"f\" : \"Y\" → \"X\" is continuous is usually more difficult; one tries to use the fact that the \"p\" are continuous in some way.\n\nIn addition to being continuous, the canonical projections \"p\" : \"X\" → \"X\" are open maps. This means that any open subset of the product space remains open when projected down to the \"X\". The converse is not true: if \"W\" is a subspace of the product space whose projections down to all the \"X\" are open, then \"W\" need not be open in \"X\". (Consider for instance \"W\" = R \\ (0,1).) The canonical projections are not generally closed maps (consider for example the closed set formula_9 whose projections onto both axes are R \\ {0}).\n\nThe product topology is also called the \"topology of pointwise convergence\" because of the following fact: a sequence (or net) in \"X\" converges if and only if all its projections to the spaces \"X\" converge. In particular, if one considers the space \"X\" = R of all real valued functions on \"I\", convergence in the product topology is the same as pointwise convergence of functions.\n\nAny product of closed subsets of \"X\" is a closed set in \"X\".\n\nAn important theorem about the product topology is Tychonoff's theorem: any product of compact spaces is compact. This is easy to show for finite products, while the general statement is equivalent to the axiom of choice.\n\n\nThe axiom of choice is equivalent to the statement that the product of a collection of non-empty sets is non-empty. The proof is easy enough: one needs only to pick an element from each set to find a representative in the product. Conversely, a representative of the product is a set which contains exactly one element from each component.\n\nThe axiom of choice occurs again in the study of (topological) product spaces; for example, Tychonoff's theorem on compact sets is a more complex and subtle example of a statement that is equivalent to the axiom of choice.\n\n"}
{"id": "20332029", "url": "https://en.wikipedia.org/wiki?curid=20332029", "title": "Rigid origami", "text": "Rigid origami\n\nRigid origami is a branch of origami which is concerned with folding structures using flat rigid sheets joined by hinges. That is, unlike with paper origami, the sheets cannot bend during the folding process; they must remain flat at all times. However, there is no requirement that the structure start as a single flat sheet – for instance shopping bags with flat bottoms are studied as part of rigid origami.\n\nRigid origami is a part of the study of the mathematics of paper folding, and rigid origami structures can be considered as a type of mechanical linkage. Rigid origami has great practical utility.\n\nThe number of standard origami bases that can be folded using rigid origami is restricted by its rules.\nRigid origami does not have to follow the Huzita–Hatori axioms, the fold lines can be calculated rather than having to be constructed from existing lines and points. When folding rigid origami flat, Kawasaki's theorem and Maekawa's theorem restrict the folding patterns that are possible, just as they do in conventional origami, but they no longer form an exact characterization: some patterns that can be folded flat in conventional origami cannot be folded flat rigidly.\n\nThe Bellows theorem says that a flexible polyhedron has constant volume when flexed rigidly.\n\nThe napkin folding problem asks whether it is possible to fold a square so the perimeter of the resulting flat figure is increased. That this can be solved within rigid origami was proved by A.S. Tarasov in 2004.\n\nThe Miura fold is a rigid fold that has been used to pack large solar panel arrays for space satellites, which have to be folded before deployment.\n\nRobert J. Lang has applied origami to the problem of folding a space telescope.\n\nFolding paper shopping bags is a problem where the rigidity requirement means the classic solution does not work.\n\nMartin Gardner has popularised flexagons which are a form of rigid origami and the flexatube.\n\nKaleidocycles are toys, usually made of paper, which give an effect similar to a kaleidoscope when convoluted.\n"}
{"id": "42563941", "url": "https://en.wikipedia.org/wiki?curid=42563941", "title": "Seminar for Applied Mathematics", "text": "Seminar for Applied Mathematics\n\nThe Seminar for Applied Mathematics (SAM; from 1948 to 1969 Institute for Applied Mathematics) was founded in 1948 by Prof. Eduard Stiefel. It is part of the Department of Mathematics (D-MATH) of the Swiss Federal Institute of Technology, ETH Zurich. The Seminar consists of four regular professorships (as of 2014), two assistant professorships, two permanent senior scientists, approximately 14 positions for assistants which are either filled by senior assistants, postdoctoral fellows or Ph.D. students, as well as secretarial staff and a systems administrator. It is represented by the Head of SAM. The SAM is a center for research and teaching in numerical mathematics, mathematical modelling and computing in Science and Technology in the D-MATH of the ETH Zürich.\n\n\n\n"}
{"id": "201359", "url": "https://en.wikipedia.org/wiki?curid=201359", "title": "Squaring the circle", "text": "Squaring the circle\n\nSquaring the circle is a problem proposed by ancient geometers. It is the challenge of constructing a square with the same area as a given circle by using only a finite number of steps with compass and straightedge. It may be taken to ask whether specified axioms of Euclidean geometry concerning the existence of lines and circles entail the existence of such a square.\n\nIn 1882, the task was proven to be impossible, as a consequence of the Lindemann–Weierstrass theorem which proves that pi () is a transcendental, rather than an algebraic irrational number; that is, it is not the root of any polynomial with rational coefficients. It had been known for some decades before then that the construction would be impossible if were transcendental, but was not proven transcendental until 1882. Approximate squaring to any given non-perfect accuracy, in contrast, is possible in a finite number of steps, since there are rational numbers arbitrarily close to .\n\nThe expression \"squaring the circle\" is sometimes used as a metaphor for trying to do the impossible.\n\nThe term \"quadrature of the circle\" is sometimes used to mean the same thing as squaring the circle, but it may also refer to approximate or numerical methods for finding the area of a circle.\n\nMethods to approximate the area of a given circle with a square, which can be thought of as a precursor problem to squaring the circle, were known already to Babylonian mathematicians. The Egyptian Rhind papyrus of 1800 BC gives the area of a circle as (64/81) , where is the diameter of the circle. In modern terms, this is equivalent to approximating pi() by 256/81, a number that appears in the older Moscow Mathematical Papyrus and is used for volume approximations (i.e. hekat). Indian mathematicians also found an approximate method, though less accurate, documented in the \"Shulba Sutras\". Archimedes proved the formula for the area of a circle (, where is the radius of the circle) and showed that the value of lay between 3 + 1/7 (approximately 3.1429) and 3 + 10/71 (approximately 3.1408). See Numerical approximations of π for more on the history.\n\nThe first known Greek to be associated with the problem was Anaxagoras, who worked on it while in prison. Hippocrates of Chios squared certain lunes, in the hope that it would lead to a solution — see Lune of Hippocrates. Antiphon the Sophist believed that inscribing regular polygons within a circle and doubling the number of sides will eventually fill up the area of the circle, and since a polygon can be squared, it means the circle can be squared. Even then there were skeptics—Eudemus argued that magnitudes cannot be divided up without limit, so the area of the circle will never be used up. The problem was even mentioned in Aristophanes's play \"The Birds\".\n\nIt is believed that Oenopides was the first Greek who required a plane solution (that is, using only a compass and straightedge). James Gregory attempted a proof of its impossibility in \"Vera Circuli et Hyperbolae Quadratura\" (The True Squaring of the Circle and of the Hyperbola) in 1667. Although his proof was faulty, it was the first paper to attempt to solve the problem using algebraic properties of . It was not until 1882 that Ferdinand von Lindemann rigorously proved its impossibility.\n\nThe first of these two misguided visionaries filled me with a great ambition to do a feat I have never heard of as accomplished by man, namely to convince a circle squarer of his error! The value my friend selected for Pi was 3.2: the enormous error tempted me with the idea that it could be easily demonstrated to BE an error. More than a score of letters were interchanged before I became sadly convinced that I had no chance.\nA ridiculing of circle-squaring appears in Augustus de Morgan's A Budget of Paradoxes published posthumously by his widow in 1872. Having originally published the work as a series of articles in the \"Athenæum\", he was revising it for publication at the time of his death. Circle squaring was very popular in the nineteenth century, but hardly anyone indulges in it today and it is believed that de Morgan's work helped bring this about.\n\nThe solution of the problem of squaring the circle by compass and straightedge requires the construction of the number formula_1. If formula_1 is constructible, it follows from standard constructions that formula_3 would also be constructible. In 1837, Pierre Wantzel showed that lengths that could be constructed with compass and straightedge had to be solutions of certain polynomial equations with rational coefficients. Thus, constructible lengths must be algebraic numbers. If the problem of the quadrature of the circle could be solved using only compass and straightedge, then would have to be an algebraic number. Johann Heinrich Lambert conjectured that was not algebraic, that is, a transcendental number, in 1761. He did this in the same paper in which he proved its irrationality, even before the general existence of transcendental numbers had been proven. It was not until 1882 that Ferdinand von Lindemann proved the transcendence of and so showed the impossibility of this construction.\n\nThe transcendence of implies the impossibility of exactly \"circling\" the square, as well as of squaring the circle.\n\nIt is possible to construct a square with an area arbitrarily close to that of a given circle. If a rational number is used as an approximation of , then squaring the circle becomes possible, depending on the values chosen. However, this is only an approximation and does not meet the constraints of the ancient rules for solving the problem. Several mathematicians have demonstrated workable procedures based on a variety of approximations.\n\nBending the rules by allowing an infinite number of compass-and-straightedge operations or by performing the operations in certain non-Euclidean geometries also makes squaring the circle possible in some sense. For example, although the circle cannot be squared in Euclidean space, it sometimes can be in hyperbolic geometry under suitable interpretations of the terms. As there are no squares in the hyperbolic plane, their role needs to be taken by \"regular quadrilaterals\", meaning quadrilaterals with all sides congruent and all angles congruent (but these angles are strictly smaller than right angles).\nThere exist, in the hyperbolic plane, (countably) infinitely many pairs of constructible circles and constructible regular quadrilaterals of equal area, which, however, are constructed simultaneously.\nThere is no method for starting with a regular quadrilateral and constructing the circle of equal area, and there is no method for starting with a circle and constructing a regular quadrilateral of equal area (even when the circle has small enough radius such that a regular quadrilateral of equal area exists).\n\nThough squaring the circle is an impossible problem using only compass and straightedge, approximations to squaring the circle can be given by constructing lengths close to .\nIt takes only minimal knowledge of elementary geometry to convert any given rational approximation of into a corresponding compass-and-straightedge construction, but constructions made in this way tend to be very long-winded in comparison to the accuracy they achieve. After the exact problem was proven unsolvable, some mathematicians applied their ingenuity to finding elegant approximations to squaring the circle, defined roughly and informally as constructions that are particularly simple among other imaginable constructions that give similar precision.\n\nAmong the modern approximate constructions was one by E. W. Hobson in 1913. This was a fairly accurate construction which was based on constructing the approximate value of 3.14164079..., which is accurate to 4 decimals (i.e. it differs from by about ).\n\nIndian mathematician Srinivasa Ramanujan in 1913, Carl Olds in 1963, Martin Gardner in 1966, and Benjamin Bold in 1982 all gave geometric constructions for\n\nwhich is accurate to six decimal places of .\n\nSrinivasa Ramanujan in 1914 gave a ruler-and-compass construction which was equivalent to taking the approximate value for to be\n\ngiving eight decimal places of . \nHe describes his construction till line segment OS as follows.\n\nIn this quadrature, Ramanujan did not construct the side length of the square, it was enough for him to show the line segment . In the following continuation of the construction, the line segment is used together with the line segment to represent the mean proportionals (red line segment ).\n\nContinuation of the construction up to the searched side length a of the square:\n\nExtend beyond A and beat the circular arc b around O with radius , it results S'. Bisect the line segment in D and draw the semicircle b over D. Draw a straight line from O through C up to the semicircle b, it cuts b in E. The line segment is the mean proportional between and , also called geometric mean. Extend the line segment beyond O and transfer twice more, it results F and A, and thus the length of the line segment with the above described approximation value of formula_3, the half circumference of the circle. Bisect the line segment in G and draw the semicircle b over G. Transfer the distance from A to the line segment , it results H. Create a vertical from H up to the semicircle b on , it results B. Connect A to B, thus the searched side formula_7 of the square ABCD is constructed, which has nearly the same area as the given circle.\n\nExamples to illustrate the errors:\nIn 1991, Robert Dixon gave constructions for\n\nseven decimal places are equal to those of formula_10 respectively equal to those of formula_11\n\nThe problem of finding the area under a curve, known as integration in calculus, or quadrature in numerical analysis, was known as \"squaring\" before the invention of calculus. Since the techniques of calculus were unknown, it was generally presumed that a squaring should be done via geometric constructions, that is, by compass and straightedge. For example, Newton wrote to Oldenburg in 1676 \"I believe M. Leibnitz will not dislike the Theorem towards the beginning of my letter pag. 4 for squaring Curve lines Geometrically\" (emphasis added). After Newton and Leibniz invented calculus, they still referred to this integration problem as squaring a curve.\n\nThe mathematical proof that the quadrature of the circle is impossible using only compass and straightedge has not proved to be a hindrance to the many people who have invested years in this problem anyway. Having squared the circle is a famous crank assertion. (\"See also\" pseudomathematics.) In his old age, the English philosopher Thomas Hobbes convinced himself that he had succeeded in squaring the circle.\n\nDuring the 18th and 19th century, the notion that the problem of squaring the circle was somehow related to the longitude problem seems to have become prevalent among would-be circle squarers. Using \"cyclometer\" for circle-squarer, Augustus de Morgan wrote in 1872:\nMontucla says, speaking of France, that he finds three notions prevalent among cyclometers: 1. That there is a large reward offered for success; 2. That the longitude problem depends on that success; 3. That the solution is the great end and object of geometry. The same three notions are equally prevalent among the same class in England. No reward has ever been offered by the government of either country.\n\nAlthough from 1714 to 1828 the British government did indeed sponsor a £20,000 prize for finding a solution to the longitude problem, exactly why the connection was made to squaring the circle is not clear; especially since two non-geometric methods (the astronomical method of lunar distances and the mechanical chronometer) had been found by the late 1760s. De Morgan goes on to say that \"[t]he longitude problem in no way depends upon perfect solution; existing approximations are sufficient to a point of accuracy far beyond what can be wanted.\" In his book, de Morgan also mentions receiving many threatening letters from would-be circle squarers, accusing him of trying to \"cheat them out of their prize\".\n\nEven after it had been proved impossible, in 1894, amateur mathematician Edwin J. Goodwin claimed that he had developed a method to square the circle. The technique he developed did not accurately square the circle, and provided an incorrect area of the circle which essentially redefined pi as equal to 3.2. Goodwin then proposed the Indiana Pi Bill in the Indiana state legislature allowing the state to use his method in education without paying royalties to him. The bill passed with no objections in the state house, but the bill was tabled and never voted on in the Senate, amid increasing ridicule from the press.\n\nThe problem of squaring the circle has been mentioned by poets such as Dante and Alexander Pope, with varied metaphorical meanings. Its literary use dates back at least to 414 BC, when the play The Birds by Aristophanes was first performed. In it, the character Meton of Athens mentions squaring the circle, possibly to indicate the paradoxical nature of his utopian city.\n\nDante's \"Paradise\" canto XXXIII lines 133–135 contain the verses:\n<poem style=\"margin-left: 2em\">\nAs the geometer his mind applies\nTo square the circle, nor for all his wit\nFinds the right formula, howe'er he tries</poem>\nFor Dante, squaring the circle represents a task beyond human comprehension, which he compares to his own inability to comprehend Paradise.\n\nBy 1742, when Alexander Pope published the fourth book of his Dunciad, attempts at circle-squaring had come to be seen as \"wild and fruitless\":\n<poem style=\"margin-left: 2em\">Mad Mathesis alone was unconfined,\nToo mad for mere material chains to bind,\nNow to pure space lifts her ecstatic stare,\nNow, running round the circle, finds it square.</poem>\n\nSimilarly, the Gilbert and Sullivan comic opera \"Princess Ida\" features a song which satirically lists the impossible goals of the women's university run by the title character, such as finding perpetual motion. One of these goals is \"And the circle – they will square it/Some fine day.\"\n\nThe sestina, a poetic form first used in the 12th century by Arnaut Daniel, has been said to square the circle in its use of a square number of lines (six stanzas of six lines each) with a circular scheme of six repeated words. writes that this form invokes a symbolic meaning in which the circle stands for heaven and the square stands for the earth.\nA similar metaphor was used in \"Squaring The Circle\", a 1908 short story by O. Henry, about a long-running family feud. In the title of this story, the circle represents the natural world, while the square represents the city, the world of man.\n\nIn James Joyce's novel \"Ulysses\", Leopold Bloom dreams of becoming wealthy by squaring the circle, unaware that the quadrature of the circle had been proved impossible 22 years earlier and that the British government had never offered a reward for its solution.\n\nIn Thomas Mann's \"The Magic Mountain\", Lawyer Paravant is tragically obsessed with squaring the circle, probably in response to a failed love affair. He becomes lost in his \"obsessive attempt to square the circle, an impossible operation which he believes he will be able to perform at the Berghof, because of the sanitarium's vertiginous elevation above the 'flatland'\". His endeavour \"takes a dissipatory turn, and is eventually denounced at the sanitarium as 'mystic hocus-pocus'\".\n\n\n"}
{"id": "5717356", "url": "https://en.wikipedia.org/wiki?curid=5717356", "title": "Steane code", "text": "Steane code\n\nThe Steane code is a tool in quantum error correction introduced by Andrew Steane in 1996. It is a perfect CSS code (Calderbank-Shor-Steane), using the classical binary [7,4,3] Hamming code to correct for qubit flip errors (X errors) and the dual of the Hamming code, the [7,3,4] code, to correct for phase flip errors (Z errors). The Steane code is able to correct arbitrary single qubit errors. \n\nIn the stabilizer formalism, the Steane code has 6 generators, and the check matrix in standard form is \n\nwhere H is the parity-check matrix of the Hamming code and is given by\n\nThe formula_3 Steane code is the first in the family of quantum Hamming codes, codes with parameters formula_4 for integers formula_5. It is also a quantum color code. \n"}
{"id": "453395", "url": "https://en.wikipedia.org/wiki?curid=453395", "title": "Tagged union", "text": "Tagged union\n\nIn computer science, a tagged union, also called a variant, variant record, choice type, discriminated union, disjoint union, or sum type, is a data structure used to hold a value that could take on several different, but fixed, types. Only one of the types can be in use at any one time, and a tag field explicitly indicates which one is in use. It can be thought of as a type that has several \"cases,\" each of which should be handled correctly when that type is manipulated. Like ordinary unions, tagged unions can save storage by overlapping storage areas for each type, since only one is in use at a time.\n\nTagged unions are most important in functional languages such as ML and Haskell, where they are called datatypes (see algebraic data type) and the compiler is able to verify that all cases of a tagged union are always handled, avoiding many types of errors. They can, however, be constructed in nearly any language, and are much safer than untagged unions, often simply called unions, which are similar but do not explicitly keep track of which member of the union is currently in use.\n\nTagged unions are often accompanied by the concept of a type constructor, which is similar but not the same as a constructor for a class. Type constructors produce a tagged union type, given the initial tag type and the corresponding type.\n\nMathematically, tagged unions correspond to \"disjoint\" or \"discriminated unions\", usually written using +. Given an element of a disjoint union \"A\" + \"B\", it is possible to determine whether it came from \"A\" or \"B\". If an element lies in both, there will be two effectively distinct copies of the value in \"A\" + \"B\", one from \"A\" and one from \"B\".\n\nIn type theory, a tagged union is called a sum type. Sum types are the dual of product types. Notations vary, but usually the sum type formula_1 comes with two introduction forms formula_2 and formula_3. The elimination form is case analysis, known as pattern matching in ML-style programming languages: if formula_4 has type formula_1 and formula_6 and formula_7 have type formula_8 under the assumptions formula_9 and formula_10 respectively, then the term \nformula_11 has type formula_8. The sum type corresponds to intuitionistic logical disjunction under the Curry–Howard correspondence.\n\nAn enumerated type can be seen as a degenerate case: a tagged union of unit types. It corresponds to a set of nullary constructors and may be implemented as a simple tag variable, since it holds no additional data besides the value of the tag.\n\nMany programming techniques and data structures –\nincluding rope (data structure), lazy evaluation, class hierarchy (see below), arbitrary-precision arithmetic, CDR coding, the indirection bit and other kinds of tagged pointers, etc. –\nare usually implemented using some sort of tagged union.\n\nA tagged union can be seen as the simplest kind of self-describing data format.\nThe tag of the tagged union can be seen as the simplest kind of metadata.\n\nThe primary advantage of a tagged union over an untagged union is that all accesses are safe, and the compiler can even check that all cases are handled. Untagged unions depend on program logic to correctly identify the currently active field, which may result in strange behavior and hard-to-find bugs if that logic fails.\n\nThe primary advantage of a tagged union over a simple record containing a field for each type is that it saves storage by overlapping storage for all the types. Some implementations reserve enough storage for the largest type, while others dynamically adjust the size of a tagged union value as needed. When the value is immutable, it is simple to allocate just as much storage as is needed.\n\nThe main disadvantage of tagged unions is that the tag occupies space. Since there are usually a small number of alternatives, the tag can often be squeezed into 2 or 3 bits wherever space can be found, but sometimes even these bits are not available. In this case, a helpful alternative may be folded, computed or encoded tags, where the tag value is dynamically computed from the contents of the union field. Common examples of this are the use of \"reserved values\", where, for example, a function returning a positive number may return -1 to indicate failure, and sentinel values, most often used in tagged pointers.\n\nSometimes, untagged unions are used to perform bit-level conversions between types, called reinterpret casts in C++. Tagged unions are not intended for this purpose; typically a new value is assigned whenever the tag is changed.\n\nMany languages support, to some extent, a universal data type, which is a type that includes every value of every other type, and often a way is provided to test the actual type of a value of the universal type. These are sometimes referred to as \"variants\". While universal data types are comparable to tagged unions in their formal definition, typical tagged unions include a relatively small number of cases, and these cases form different ways of expressing a single coherent concept, such as a data structure node or instruction. Also, there is an expectation that every possible case of a tagged union will be dealt with when it is used. The values of a universal data type are not related and there is no feasible way to deal with them all.\n\nLike option types and exception handling, tagged unions are sometimes used to handle the occurrence of exceptional results. Often these tags are folded into the type as \"reserved values\", and their occurrence is not consistently checked: this is a fairly common source of programming errors. This use of tagged unions can be formalized as a monad with the following functions:\n\nwhere \"value\" and \"err\" are the constructors of the union type, \"A\" and \"B\" are valid result types and \"E\" is the type of error conditions. Alternately, the same monad may be described by \"return\" and two additional functions, \"fmap\" and \"join\":\n\nSay we wanted to build a binary tree of integers. In ML, we would do this by creating a datatype like this:\n\nThis is a tagged union with two cases: one, the leaf, is used to terminate a path of the tree, and functions much like a null value would in imperative languages. The other branch holds a node, which contains an integer and a left and right subtree. Leaf and Node are the constructors, which enable us to actually produce a particular tree, such as:\n\nwhich corresponds to this tree:\n\nNow we can easily write a typesafe function that, say, counts the number of nodes in the tree:\n\nIn ALGOL 68, tagged unions are called \"united modes\", the tag is implicit, and the codice_1 construct is used to determine which field is tagged:\n\ncodice_2\n\nUsage example for codice_3 codice_1 of codice_5:\n\nAlthough primarily only functional languages such as ML and Haskell (from 1990s) give a central role to tagged unions and have the power to check that all cases are handled, other languages have support for tagged unions as well. However, in practice they can be less efficient in non-functional languages due to optimizations enabled by functional language compilers that can eliminate explicit tag checks and avoid explicit storage of tags.\n\nPascal, Ada, and Modula-2 call them variant records (formally discriminated type in Ada), and require the tag field to be manually created and the tag values specified, as in this Pascal example:\n\nand this Ada equivalent:\nIn C and C++, a tagged union can be created from untagged unions using a strict access discipline where the tag is always checked:\n\nAs long as the union fields are only accessed through the functions, the accesses will be safe and correct. The same approach can be used for encoded tags; we simply decode the tag and then check it on each access. If the inefficiency of these tag checks is a concern, they may be automatically removed in the final version.\n\nC and C++ also have language support for one particular tagged union: the possibly-null pointer. This may be compared to the codice_6 type in ML or the codice_7 type in Haskell, and can be seen as a tagged pointer: a tagged union (with an encoded tag) of two types:\nUnfortunately, C compilers do not verify that the null case is always handled, and this is a particularly prevalent source of errors in C code, since there is a tendency to ignore exceptional cases.\n\nOne advanced dialect of C called Cyclone has extensive built-in support for tagged unions.\n\nThe enum types in the Rust and Swift languages also work as tagged unions.\n\nThe variant library from Boost has demonstrated it was possible to implement a safe tagged union as a library in C++, visitable using function objects.\nScala has case classes:\nBecause the class hierarchy is sealed, the compiler can check that all cases are handled in a pattern match:\nScala's case classes also permit reuse through subtyping:\n\nF# has discriminated unions:\n\nBecause the defined cases are exhaustive, the compiler can check that all cases are handled in a pattern match:\nHaxe's enums also work as tagged unions:\nThese can be matched using a switch expression:\n\nThe Rust language has extensive support for tagged unions, called enums. For example:\nIt also allows matching on unions:\n\nRust's error handling model relies extensively on these tagged unions, especially the codice_9 type, which is either codice_10 or codice_11, and the codice_12 type, which is either codice_13 or codice_14.\n\nIn a typical class hierarchy in object-oriented programming, each subclass can encapsulate data unique to that class. The metadata used to perform virtual method lookup (for example, the object's vtable pointer in most C++ implementations) identifies the subclass and so effectively acts as a tag identifying the particular data stored by the instance (see RTTI).\nAn object's constructor sets this tag, and it remains constant throughout the object's lifetime.\n\nNevertheless, a class hierarchy involves true subtype polymorphism; it can be extended by creating further subclasses of the same base type, which could not be handled correctly under a tag/dispatch model. Hence, it is usually not possible to do case analysis or dispatch on a subobject's 'tag' as one would for tagged unions. Some languages such as Scala allow base classes to be \"sealed\", and unify tagged unions with sealed base classes.\n\n\n"}
{"id": "1416915", "url": "https://en.wikipedia.org/wiki?curid=1416915", "title": "Tangent cone", "text": "Tangent cone\n\nIn geometry, the tangent cone is a generalization of the notion of the tangent space to a manifold to the case of certain spaces with singularities.\n\nIn nonlinear analysis, there are many definitions for a tangent cone, including the adjacent cone, Bouligand's contingent cone, and the Clarke tangent cone. These three cones coincide for a convex set, but they can differ on more general sets.\n\nLet \"K\" be a closed convex subset of a real vector space \"V\" and ∂\"K\" be the boundary of \"K\". The solid tangent cone to \"K\" at a point \"x\" ∈ ∂\"K\" is the closure of the cone formed by all half-lines (or rays) emanating from \"x\" and intersecting \"K\" in at least one point \"y\" distinct from \"x\". It is a convex cone in \"V\" and can also be defined as the intersection of the closed half-spaces of \"V\" containing \"K\" and bounded by the supporting hyperplanes of \"K\" at \"x\". The boundary \"T\" of the solid tangent cone is the tangent cone to \"K\" and ∂\"K\" at \"x\". If this is an affine subspace of \"V\" then the point \"x\" is called a smooth point of ∂\"K\" and ∂\"K\" is said to be differentiable at \"x\" and \"T\" is the ordinary tangent space to ∂\"K\" at \"x\".\n\nLet \"X\" be an affine algebraic variety embedded into the affine space formula_1, with defining ideal formula_2. For any polynomial \"f\", let formula_3 be the homogeneous component of \"f\" of the lowest degree, the \"initial term\" of \"f\", and let \n\nbe the homogeneous ideal which is formed by the initial terms formula_3 for all formula_6, the \"initial ideal\" of \"I\". The tangent cone to \"X\" at the origin is the Zariski closed subset of formula_1 defined by the ideal formula_8. By shifting the coordinate system, this definition extends to an arbitrary point of formula_1 in place of the origin. The tangent cone serves as the extension of the notion of the tangent space to \"X\" at a regular point, where \"X\" most closely resembles a differentiable manifold, to all of \"X\". (The tangent cone at a point of formula_1 that is not contained in \"X\" is empty.)\n\nFor example, the nodal curve\n\nis singular at the origin, because both partial derivatives of \"f\"(\"x\", \"y\") = \"y\" − \"x\" − \"x\" vanish at (0, 0). Thus the Zariski tangent space to \"C\" at the origin is the whole plane, and has higher dimension than the curve itself (two versus one). On the other hand, the tangent cone is the union of the tangent lines to the two branches of \"C\" at the origin,\n\nIts defining ideal is the principal ideal of \"k\"[\"x\"] generated by the initial term of \"f\", namely \"y\" − \"x\" = 0.\n\nThe definition of the tangent cone can be extended to abstract algebraic varieties, and even to general Noetherian schemes. Let \"X\" be an algebraic variety, \"x\" a point of \"X\", and (\"O\", \"m\") be the local ring of \"X\" at \"x\". Then the tangent cone to \"X\" at \"x\" is the spectrum of the associated graded ring of \"O\" with respect to the \"m\"-adic filtration:\nIf we look at our previous example, then we can see that graded pieces contain the same information. So let\nthen if we expand out the associated graded ring\nwe can see that the polynomial defining our variety\n\n"}
{"id": "4377114", "url": "https://en.wikipedia.org/wiki?curid=4377114", "title": "Texture mapping unit", "text": "Texture mapping unit\n\nA texture mapping unit (TMU) is a component in modern graphics processing units (GPUs). Historically it was a separate physical processor. A TMU is able to rotate, resize, and distort a bitmap image (performing texture sampling), to be placed onto an arbitrary plane of a given 3D model as a texture. This process is called texture mapping. In modern graphics cards it is implemented as a discrete stage in a graphics pipeline, whereas when first introduced it was implemented as a separate processor, e.g. as seen on the Voodoo2 graphics card.\n\nThe TMU came about due to the compute demands of sampling and transforming a flat image (as the texture map) to the correct angle and perspective it would need to be in 3D space. The compute operation is a large matrix multiply, which CPUs of the time (early Pentiums) could not cope with at acceptable performance.\n\nToday (2013), TMUs are part of the shader pipeline and decoupled from the Render Output Pipelines (ROPs). For example, in AMD's Cypress GPU, each shader pipeline (of which there are 20) has four TMUs, giving the GPU 80 TMUs. This is done by chip designers to closely couple shaders and the texture engines they will be working with.\n\n3D scenes are generally composed of two things: 3D geometry, and the textures that cover that geometry. Texture units in a video card take a texture and 'map' it to a piece of geometry. That is, they wrap the texture around the geometry and produce textured pixels which can then be written to the screen.\nTextures can be an actual image, a lightmap, or even normal maps for advanced surface lighting effects.\n\nTo render a 3D scene, textures are mapped over the top of polygon meshes. This is called texture mapping and is accomplished by texture mapping units (TMUs) on the videocard. Texture fill rate is a measure of the speed with which a particular card can perform texture mapping.\n\nThough pixel shader processing is becoming more important, this number still holds some weight. Best example of this is the X1600 XT. This card has a 3 to 1 ratio of pixel shader processors/texture mapping units. As a result, the X1600 XT achieves lower performance when compared to other GPUs of the same era and class (such as nVidia's 7600GT) . In the mid range, texture mapping can still very much be a bottleneck.\nHowever, at the high end, the X1900 XTX has this same 3 to 1 ratio, but does just fine because screen resolutions top out and it has more than enough texture mapping power to handle any display.\n\nTextures need to be addressed and filtered. This job is done by TMUs that work in conjunction with pixel and vertex shader units. It is the TMU's job to apply texture operations to pixels. The number of texture units in a graphics processor is used when comparing two different cards for texturing performance. It is reasonable to assume that the card with more TMUs will be faster at processing texture information.\nIn modern GPUs TMUs contain Texture Address Units(TA) and Texture Filtering Units(TF). Texture Address Units map texels to pixels and can perform texture addressing modes. Texture Filtering Units optionally perform hardware based texture filtering.\n\nA pipeline is the graphics card's architecture, which provides a generally accurate idea of the computing power of a graphics processor.\n\nA pipeline isn't formally accepted as a technical term. There are different pipelines within a graphics processor as there are separate functions being performed at any given time. Historically, it has been referred to as a pixel processor that is attached to a dedicated TMU. A Geforce 3 had four pixel pipelines, each of which had two TMUs. The rest of the pipeline handled things like depth and blending operations.\n\nThe ATI Radeon 9700 was first to break this mould, by placing a number vertex shader engines independent of the pixel shaders . The R300 GPU used in the Radeon 9700 had four global vertex shaders, but split the rest of the rendering pipeline in half (it was, so to speak, dual core) each half, called a quad, had four pixel shaders, four TMUs and four ROPs. \n\nSome units are used more than others, and in an effort to increase the processor's entire performance, they attempted to find a \"sweet spot\" in the number of units needed for optimum efficiency without the need for excess silicon. In this architecture the name pixel pipeline lost its meaning as pixel processors were no longer attached to single TMUs.\n\nThe vertex shader had long been decoupled, starting with the R300, but the pixel shader was not so easily done, as it required colour data (e.g. texture samples) to work with, and hence needed to be closely coupled to a TMU.\n\nSaid coupling remains to this day, where the shader engine, made of units able to run either vertex or pixel data, is tightly coupled to a TMU but has a crossbar dispatcher between its output and the bank of ROPs.\n\nThe Render Output Pipeline is an inherited term, and more often referred to as the render output unit. Its job is to control the sampling of pixels (each pixel is a dimensionless point), so it controls antialiasing, when more than one sample is merged into one pixel. All data rendered has to travel through the ROP in order to be written to the framebuffer, from there it can be transmitted to the display.\n\nTherefore, the ROP is where the GPU's output is assembled into a bitmapped image ready for display.\n\nIn GPGPU, texture maps in 1,2, or 3 dimensions may be used to store arbitrary data. By providing interpolation, the texture mapping unit provides a convenient means of approximating arbitrary functions with data tables.\n\n\n"}
{"id": "48445265", "url": "https://en.wikipedia.org/wiki?curid=48445265", "title": "Theorem of the three geodesics", "text": "Theorem of the three geodesics\n\nIn differential geometry the theorem of the three geodesics states that every Riemannian manifold with the topology of a sphere has at least three closed geodesics that form simple closed curves without self-intersections. The result can also be extended to quasigeodesics on a convex polyhedron.\n\nThis result stems from the mathematics of ocean navigation, where the surface of the earth can be modeled accurately by an ellipsoid, and from the study of the geodesics on an ellipsoid, the shortest paths for ships to travel. In particular, a nearly-spherical triaxial ellipsoid has only three simple closed geodesics, its equators. In 1905, Henri Poincaré conjectured that every smooth surface topologically equivalent to a sphere likewise contains at least three simple closed geodesics, and in 1929 Lazar Lyusternik and Lev Schnirelmann published a proof of the conjecture, which was later found to be flawed.\nThe proof was repaired by Hans Werner Ballmann in 1978.\n\nOne proof of this conjecture examines the homology of the space of smooth curves on the sphere, and uses the curve-shortening flow to find a simple closed geodesic that represents each of the three nontrivial homology classes of this space.\n\nMore strongly, there necessarily exist three simple closed geodesics whose length is at most proportional to the diameter of the surface.\n\nThe number of closed geodesics of length at most \"L\" on a smooth topological sphere grows in proportion to \"L\"/log \"L\", but not all such geodesics can be guaranteed to be simple.\n\nOn compact hyperbolic Riemann surfaces, there are infinitely many simple closed geodesics, but only finitely many with a given length bound. They are encoded analytically by the Selberg zeta function. The growth rate of the number of simple closed geodesics, as a function of their length, was investigated by Maryam Mirzakhani.\n\nIt is also possible to define geodesics on some surfaces that are not smooth everywhere, such as convex polyhedra. Although some polyhedra have simple closed geodesics (for instance, the regular tetrahedron and disphenoids have infinitely many closed geodesics, all simple) others do not. In particular, a simple closed geodesic of a convex polyhedron would necessarily bisect the total angular defect of the vertices, and almost all polyhedra do not have such bisectors.\n\nNevertheless, the theorem of the three geodesics can be extended to convex polyhedra by considering quasigeodesics, curves that are geodesic except at the vertices of the polyhedra and that have angles less than on both sides at each vertex they cross. A version of the theorem of the three geodesics for convex polyhedra states that all polyhedra have at least three simple closed quasigeodesics; this can be proved by approximating the polyhedron by a smooth surface and applying the theorem of the three geodesics to this surface. It is an open problem whether any of these quasigeodesics can be constructed in polynomial time.\n"}
{"id": "7803789", "url": "https://en.wikipedia.org/wiki?curid=7803789", "title": "Thomas Kirkman", "text": "Thomas Kirkman\n\nThomas Penyngton Kirkman FRS (31 March 1806 – 3 February 1895) was a British mathematician and ordained minister of the Church of England. Despite being primarily a churchman, he maintained an active interest in research-level mathematics, and was listed by Alexander Macfarlane as one of ten leading 19th-century British mathematicians. In the 1840s, he obtained an existence theorem for Steiner triple systems that founded the field of combinatorial design theory, while the related Kirkman's schoolgirl problem is named after him.\n\nKirkman was born 31 March 1806 in Bolton, in the north west of England, the son of a local cotton dealer. In his schooling at the Bolton Grammar School, he studied classics, but no mathematics was taught in the school. He was recognised as the best scholar at the school, and the local vicar guaranteed him a scholarship at Cambridge, but his father would not allow him to go. Instead, he left school at age 14 to work in his father's office.\n\nNine years later, defying his father, he went to Trinity College Dublin, working as a private tutor to support himself during his studies. There, among other subjects, he first began learning mathematics. He earned a B.A. in 1833 and returned to England in 1835.\n\nOn his return to England, Kirkman was ordained into the ministry of the Church of England and became the curate in Bury and then in Lymm. In 1839 he was invited to become rector of Croft with Southworth, a newly founded parish in Lancashire, where he would stay for 52 years until his retirement in 1892. Theologically, Kirkman supported the anti-literalist position of John William Colenso, and was also strongly opposed to materialism. He published many tracts and pamphlets on theology, as well as a book \"Philosophy Without Assumptions\" (1876).\n\nKirkman married Eliza Wright in 1841; they had seven children. To support them, Kirkman supplemented his income with tutoring, until Eliza inherited enough property to secure their living. The rectorship itself did not demand much from Kirkman, so from this point forward he had time to devote to mathematics.\n\nKirkman died 4 February 1895 in Bowdon. His wife died ten days later.\n\nKirkman's first mathematical publication was in the \"Cambridge and Dublin Mathematical Journal\" in 1846, on a problem involving Steiner triple systems that had been published two years earlier in \"The Lady's and Gentleman's Diary\" by Wesley S. B. Woolhouse. Despite Kirkman's and Woolhouse's contributions to the problem, Steiner triple systems were named after Jakob Steiner who wrote a later paper in 1853. Kirkman's second research paper, in 1848, concerned pluquaternions.\n\nIn 1848, Kirkman published \"First Mnemonical Lessons\", a book on mathematical mnemonics for schoolchildren. It was not successful, and Augustus de Morgan criticised it as \"the most curious crochet I ever saw\".\n\nNext, in 1849, Kirkman studied the Pascal lines determined by the intersection points of opposite sides of a hexagon inscribed within a conic section. Any six points on a conic may be joined into a hexagon in 60 different ways, forming 60 different Pascal lines. Extending previous work of Steiner, Kirkman showed that these lines intersect in triples to form 60 points (now known as the Kirkman points), so that each line contains three of the points and each point lies on three of the lines. That is, these lines and points form a projective configuration of type 6060.\n\nIn 1850, Kirkman observed that his 1846 solution to Woolhouse's problem had an additional property, which he set out as a puzzle in \"The Lady's and Gentleman's Diary\":\n\nThis problem became known as Kirkman's schoolgirl problem, subsequently to become Kirkman's most famous result. He published several additional works on combinatorial design theory in later years.\n\nIn 1848 Kirkman wrote \"On Pluquaternions and Homoid Products of \"n\" Squares\".\nGeneralizing the quaternions and octonions, Kirkman called a pluquaternion Q a representative of a system with \"a\" imaginary units, \"a\" > 3.\nKirkman's paper was dedicated to confirming Cayley's assertions concerning two equations among triple-products of units as sufficient to determine the system in case \"a\" = 3 but not \"a\" = 4. By 1900 these number systems were called hypercomplex numbers, and later treated as part of the theory of associative algebras.\n\nBeginning in 1853, Kirkman began working on combinatorial enumeration problems concerning polyhedra, beginning with a proof of Euler's formula and concentrating on simple polyhedra (the polyhedra in which each vertex has three incident edges). He also studied Hamiltonian cycles in polyhedra, and provided an example of a polyhedron with no Hamiltonian cycle, prior to the work of William Rowan Hamilton on the Icosian game. He enumerated cubic Halin graphs, over a century before the work of Halin on these graphs. He showed that every polyhedron can be generated from a pyramid by face-splitting and vertex-splitting operations, and he studied self-dual polyhedra.\n\nKirkman was inspired to work in group theory by a prize offered beginning in 1858 (but in the end never awarded) by the French Academy of Sciences. His contributions in this area include an enumeration of the transitive group actions on sets of up to ten elements. However, as with much of his work on polyhedra, Kirkman's work in this area was weighed down by newly invented terminology and, perhaps because of this, did not significantly influence later researchers.\n\nIn the early 1860s, Kirkman fell out with the mathematical establishment and in particular with Arthur Cayley and James Joseph Sylvester, over the poor reception of his works on polyhedra and groups and over issues of priority. Much of his later mathematical work was published (often in doggerel) in the problem section of the \"Educational Times\" and in the obscure \"Proceedings of the Literary and Philosophical Society of Liverpool\". However, in 1884 he began serious work on knot theory, and with Peter Guthrie Tait published an enumeration of the knots with up to ten crossings. He remained active in mathematics even after retirement, until his death in 1895.\n\nIn 1857, Kirkman was elected as a fellow of the Royal Society for his research on pluquaternions and partitions. He was also an honorary member of the\nLiterary and Philosophical Society of Manchester and the Literary and Philosophical Society of Liverpool, and a foreign member of the Dutch Society of Science.\n\nSince 1994, the Institute of Combinatorics and its Applications has handed out an annual Kirkman medal, named after Kirkman, to recognise outstanding combinatorial research by a mathematician within four years of receiving a doctorate.\n"}
{"id": "16569551", "url": "https://en.wikipedia.org/wiki?curid=16569551", "title": "Thomas Rawson Birks", "text": "Thomas Rawson Birks\n\nThomas Rawson Birks (28 September 1810 – 19 July 1883) was an English theologian and controversialist, who figured in the debate to try to resolve theology and science. He rose to be Knightbridge Professor of Moral Philosophy at the University of Cambridge. His discussions led to much controversy: in one book he proposed that stars cannot have planets as this would reduce the importance of Christ's appearance on this planet.\n\nBirks was born on 28 September 1810 in Staveley in Derbyshire, England, where his father was a tenant farmer under the Duke of Devonshire. The family being nonconformists, Birks was educated at Chesterfield and then at the Dissenting College at Mill Hill. He won a sizarship and a scholarship at Trinity College, Cambridge, and in his third year gained the chief English declamation prize. As the holder of this prize he delivered the customary oration in the college hall. The subject chosen was \"Mathematical and Moral Certainty\" and Dr William Whewell spoke very highly of this oration. In 1834, like Whewell before him, Birks became second wrangler and second Smith's prizeman.\n\nHaving joined the Church of England on leaving the university, Birks settled at Watton-at-Stone as tutor and then curate to the Reverend Edward Bickersteth. During his stay there he studied the prophetic scriptures, and took the affirmative side in the warm controversy which arose on the subject of the premillennial theory of the Lord's return. In 1843-4 Birks won the Seatonian prize for the best English poem at Trinity College. Some years before he had been elected a fellow of this college. He engaged in many religious controversies, and one of these, on the future of the Lost, led to the severance of private friendships and religious connections. In his views on this subject he was equally opposed to the universalists and the annihilationists. In 1844 Birks married Bickersteth's daughter, Elizabeth, and accepted the living of Kelshall in Hertfordshire.\n\nBirks published \"Modern Astronomy\" in 1830 to demonstrate a harmony between science and religion; in it he attempts to join together theology and a modern understanding of astronomy. He deals with such subjects as the insignificance of man, if we are but one race alone in the universe except for the angels. How can Christ's importance be squared with the unimportance of the human race in a large universe with a multitude of stars and planets? Birks' solution was to decide that the existence of planets around other stars is only conjecture.\n\nIn 1850 he published his edition of William Paley's \"Horae Paulinae\", or the \"Truth of the Scripture History of St Paul\" with notes and a supplementary treatise entitled \"Horae Apostolicae\".\n\nIn 1856 Birks' wife, Elizabeth, died at the age of 46. His widowhood led to the suspension of his writing for several years. Nevertheless, \"The Bible and Modern Thought\" was published in 1861 at the request of the committee of the Religious Tract Society. Birks subsequently enlarged his work by a series of notes on the evidential school of theology, the limits of religious thought, the Bible and ancient Egypt, the human element in Scripture, and Genesis and geology. In 1862 he published \"On Matter and Ether, Or, The Secret Laws of Physical Change\" which dealt with issues of physics.\n\nBirks left Kelshall in 1864. In 1866 he accepted the important charge of Holy Trinity Church, Cambridge; and on 17 May 1866 married his second wife, Georgina Agnes Beresford, widow of Major James Douglas.\n\nAt the time of the disestablishment of the Church of Ireland, Birks came forward with a lengthy treatise on \"Church and State\" which was an elaboration of a treatise written thirty years before, and was now republished as bearing upon the ecclesiastical change proposed by William Ewart Gladstone and carried into effect by Parliament. Birks was installed honorary canon of Ely Cathedral in 1871, and in 1872, on the death of the Rev. Frederick Maurice, he was elected Knightbridge Professor of Philosophy. This appointment led to a stormy controversy. It was regarded as a retrograde step by the large body of liberal thinkers who sympathised with the views of Frederick Maurice. As pastor at Cambridge, Birks gave religious instruction to the undergraduates, to older members of the university, and also to the residents in the town. In the year of his appointment he published his \"Scripture Doctrine of Creation \" and \"The Philosophy of Human Responsibility\". His inaugural lecture as professor of moral philosophy was on \"The Present Importance of Moral Science\"(1872).\n\nIn 1873 Birks published his \"First Principles of Moral Science\" which was a course of lectures delivered during his professorship. This work was followed in 1874 by \"Modern Utilitarianism\" in which the systems of William Paley, Jeremy Bentham and John Stuart Mill were examined and compared. In 1876 he delivered the annual address to the Victoria Institute, his subject being \"The Uncertainties of Modern Physical Science\".\n\nIn 1876 he published his work on \"Modern Physical Fatalism and the Doctrine of Evolution\". It contained the substance of a course of lectures devoted to the examination of the philosophy unfolded in Herbert Spencer's \"First Principles\". Birks held the views expressed by Spencer to be unsound and opposed to the fundamental doctrines of Christianity and even the existence of moral science. To the strictures upon his \"First Principles\" Spencer replied at length, and this led to the re-publication, in 1882, of Birks's treatise, with an introduction by Charles Pritchard, Savilian professor of astronomy at Oxford, in which Spencer's rejoinder was dealt with, and the original arguments of Birks illustrated and further explained.\n\nBirks resigned the vicarage of Trinity in 1877, and in the same year published a volume on \"Manuscript Evidence in the Text of the New Testament\", which was an endeavour to bring \"mathematical reasoning to bear on the probable value of the manuscripts of different ages, with a general inference in favour of the high value of the cursive manuscripts as a class\". In the same year Birks issued his \"Supernatural Revelation\", being an answer to a work on \" Supernatural Religion\" which had given rise to much criticism. Birks's treatise was again republished at a later period by Pritchard, with a reply to objections that had been urged against it.\n\nFor twenty-one years Birks served as honorary secretary to the Evangelical Alliance, but he resigned when the committee failed to agree with his views on eternal punishment. He was an examiner for the theological examination at Cambridge in 1867 and 1868, and was a member of the board of theological studies. He took an active part in all university affairs during his connection with Cambridge, was appointed to preach the Ramsden sermon in 1867, and was frequently a select preacher before the university.\n\nEarly in 1875 Birks suffered from a paralytic seizure, and this was followed by a second stroke in 1877. He still took a deep interest in questions of the day, and was able to dictate various works, pamphlets, and letters bearing upon these questions.\nIn April 1880, while residing in the New Forest, he was paralysed for a week, his third attack. He was conveyed home to Cambridge, where he lingered for three years incapable of intellectual effort. He died on 19 July 1883.\n\nBy his first marriage to Elizabeth Bickersteth, Birks had eight children. His eldest son, Edward Bickersteth Birks, also became a theologian and succeeded him as a fellow of Trinity.\n\nIn addition to the works named in the course of this article, Birks was the author of a considerable number of treatises on prophecy and other subjects connected with the older revelation, as well as his \"Memoir of the Rev. Edward Bickersteth\".\n"}
{"id": "1651618", "url": "https://en.wikipedia.org/wiki?curid=1651618", "title": "Tibor Gallai", "text": "Tibor Gallai\n\nTibor Gallai (born Tibor Grünwald, 15 July 1912 – 2 January 1992) was a Hungarian mathematician. He worked in combinatorics, especially in graph theory, and was a lifelong friend and collaborator of Paul Erdős. He was a student of Dénes Kőnig and an advisor of László Lovász. He was a corresponding member of the Hungarian Academy of Sciences (1991).\n\nThe Edmonds–Gallai decomposition theorem, which was proved independently by Gallai and Jack Edmonds, describes finite graphs from the point of view of matchings. Gallai also proved, with Milgram, Dilworth's theorem in 1947, but as they hesitated to publish the result, Dilworth independently discovered and published it.\n\nGallai was the first to prove the higher-dimensional version of van der Waerden's theorem.\n\nWith Paul Erdős he gave a necessary and sufficient condition for a sequence to be the degree sequence of a graph, known as the Erdős–Gallai theorem.\n\n"}
{"id": "58764", "url": "https://en.wikipedia.org/wiki?curid=58764", "title": "Timeline of classical mechanics", "text": "Timeline of classical mechanics\n\nTimeline of classical mechanics:\n\n\n"}
{"id": "57665268", "url": "https://en.wikipedia.org/wiki?curid=57665268", "title": "Tits metric", "text": "Tits metric\n\nIn mathematics, the Tits metric is a metric defined on the ideal boundary of an Hadamard space (also called a complete CAT(0) space). It is named after Jacques Tits.\n\nLet (\"X\", \"d\") be an Hadamard space. Two geodesic rays \"c\", \"c\" : [0, ∞] → \"X\" are called asymptotic if they stay within a certain distance when traveling, i.e.\nEquivalently, the Hausdorff distance between the two rays is finite.\n\nThe asymptotic property defines an equivalence relation on the set of geodesic rays, and the set of equivalence classes is called the ideal boundary ∂\"X\" of \"X\". An equivalence class of geodesic rays is called a boundary point of \"X\". For any equivalence class of rays and any point \"p\" in \"X\", there is a unique ray in the class that issues from \"p\".\n\nFirst we define an angle between boundary points with respect to a point \"p\" in \"X\". For any two boundary points formula_2 in ∂\"X\", take the two geodesic rays \"c\", \"c\" issuing from \"p\" corresponding to the two boundary points respectively. One can define an angle of the two rays at \"p\" called the Alexandrov angle. Intuitively, take the triangle with vertices \"p\", \"c\"(\"t\"), \"c\"(\"t\") for a small \"t\", and construct a triangle in the flat plane with the same side lengths as this triangle. Consider the angle at the vertex of the flat triangle corresponding to \"p\". The limit of this angle when \"t\" goes to zero is defined as the Alexandrov angle of the two rays at \"p\". (By definition of a CAT(0) space, the angle monotonically decreases as \"t\" decreases, so the limit exists.) Now we define formula_3 to be this angle.\n\nTo define the angular metric on the boundary ∂\"X\" that does not depend on the choice of \"p\", we take the supremum over all points in \"X\"\n\nThe Tits metric \"d\" is the length metric associated to the angular metric, that is for any two boundary points, the Tits distance between them is the infimum of lengths of all the curves on the boundary that connect them measured in the angular metric. If there is no such curve with finite length, the Tits distance between the two points is defined as infinity.\n\nThe ideal boundary of \"X\" equipped with the Tits metric is called the Tits boundary, denoted as ∂\"X\". \n\nFor a complete CAT(0) space, it can be shown that its ideal boundary with the angular metric is a complete CAT(1) space, and its Tits boundary is also a complete CAT(1) space. Thus for any two boundary points formula_2 such that formula_6, we have \nand the points can be joined by a unique geodesic segment on the boundary. If the space is proper, then any two boundary points at finite Tits distance apart can be joined by a geodesic segment on the boundary.\n\n\n"}
{"id": "44645174", "url": "https://en.wikipedia.org/wiki?curid=44645174", "title": "Token reconfiguration", "text": "Token reconfiguration\n\nIn computational complexity theory and combinatorics, the token reconfiguration problem is an optimization problem on a graph with both an initial and desired state for tokens.\n\nGiven a graph formula_1, an initial state of tokens is defined by a subset formula_2 of the vertices of the graph; let formula_3. Moving a token from vertex formula_4 to vertex formula_5 is valid if formula_4 and formula_5 are joined by a path in formula_1 that does not contain any other tokens; note that the distance traveled within the graph is inconsequential, and moving a token across multiple edges sequentially is considered a single move. A desired end state is defined as another subset formula_9. The goal is to minimize the number of valid moves to reach the end state from the initial state.\n\nThe problem is motivated by so-called sliding puzzles, which are in fact a variant of this problem, often restricted to rectangular grid graphs with no holes. The most famous such puzzle, the 15 puzzle, is a variant of this problem on a 4 by 4 grid graph such that formula_10. One key difference between sliding block puzzles and the token reconfiguration problem is that in the original token reconfiguration problem, the tokens are indistinguishable. As a result, if the graph is connected, the token reconfiguration problem is always solvable; this is not necessarily the case for sliding block puzzles.\n\nCalinescu, Dumitrescu, and Pach have shown several results regarding both the optimization and approximation of this problem on various types of graphs.\n\nFirstly, reducing to the case of trees, there is always a solution in at most formula_11 moves, with at most one move per token. Furthermore, an optimal solution can be found in time linear in the size of the tree. Clearly, the first result extends to arbitrary graphs; the latter does not.\n\nA sketch of the optimal algorithm for trees is as follows. First, we obtain an algorithm that moves each node exactly once, which may not be optimal. Do this recursively: consider any leaf of the smallest tree in the graph containing both the initial and desired sets. If a leaf of this tree is in both, remove it and recurse down. If a leaf is in the initial set only, find a path from it to a vertex in the desired set that does not pass through any other vertices in the desired set. Remove this path (it'll be the last move), and recurse down. The other case, where the leaf is in the desired set only, is symmetric. To extend to an algorithm that achieves the optimum, consider any token in both the initial and desired sets. If removing it would split the graph into subtrees, all of which have the same number of elements from the initial and desired sets, then do so and recurse. If there is no such token, then each token must move exactly once, and so the solution that moves all tokens exactly once must be optimal.\n\nWhile the algorithm for finding the optimum on trees is linear time, finding the optimum for general graphs is NP-complete, a leap up in difficulty. It is in NP; the certificate is a sequence of moves, which is at most linear size, so it remains to show the problem is NP-hard as well. This is done via reduction from set cover.\n\nConsider an instance of set cover, where we wish to cover all elements formula_12 in a universe formula_13 using subsets formula_14 of formula_13 using the minimum number of subsets. Construct a graph as follows:\n\nMake a vertex for each of the elements in the universe and each of the subsets. Connect a subset vertex to an element vertex if the subset contains that element. Create a long path of size formula_11, and attach one end to every subset vertex. The initial set is the added path plus every subset vertex, and the final set is every subset vertex plus every element vertex.\n\nTo see why this is a reduction, consider the selection of which subset vertex tokens to move. Clearly, we must open up paths to each of the element vertices, and we do so by moving some of the subset vertex tokens. After doing so, each token on the long path must move once. Thus, the optimum cost is equal to the number of selected subsets plus the number of elements (the latter of which is notably a constant). So we have a polynomial-time reduction from set cover, which is NP-complete, to token reconfiguration. Thus token reconfiguration is also NP-complete on general graphs.\n\nThe token reconfiguration problem is APX-complete, meaning that in some sense, it is as hard to approximate as any problem that has a constant-factor approximation algorithm. The reduction is the same one as above, from set cover. However, the set cover problem is restricted to subsets of size at most 3, which is an APX-hard problem.\n\nUsing the exact same structure as above, we obtain an L-reduction, as the distance of any solution from optimum is equal between the set cover instance and the transformed token reconfiguration problem. The only change is the addition of the number of elements in the universe. Furthermore, the set cover optimum is at least 1/3 of the number of elements, due to the bounded subset size. Thus, the constants from the L-reduction are formula_17.\n\nOne can, in fact, modify the reduction to work for labeled token reconfiguration as well. To do so, attach a new vertex to each of the subset vertices, which is neither an initial nor desired vertex. Label the vertices on the long path 1 through formula_11, and do the same for the element vertices. Now, the solution consists of 'moving aside' each chosen subset vertex token, correctly placing the labeled vertices from the path, and returning the subset vertex tokens to the initial locations. This is an L-reduction with formula_19.\n\nCalinescu, Dumitrescu, and Pach have also shown that there exists a 3-approximation for unlabeled token reconfiguration, so the problem is in APX as well and thus APX-complete. The proof is much more complicated and omitted here.\n"}
{"id": "6129269", "url": "https://en.wikipedia.org/wiki?curid=6129269", "title": "Transport of structure", "text": "Transport of structure\n\nIn mathematics, transport of structure is the definition of a new structure on an object by reference to another object on which a similar structure already exists. Definitions by transport of structure are regarded as canonical.\n\nSince mathematical structures are often defined in reference to an underlying space, many examples of transport of structure involve spaces and mappings between them. For example, if \"V\" and \"W\" are vector spaces, and if formula_1 is an isomorphism, and if formula_2 is an inner product on formula_3, then we can define an inner product formula_4 on \"V\" by\nAlthough the equation makes sense even when formula_6 is not an isomorphism, it only defines an inner product on \"V\" when formula_6 is, since otherwise it will cause formula_8 to be degenerate. The idea is that formula_6 allows us to consider \"V\" and \"W\" as \"the same\" vector space, and if we follow this analogy, we can transport an inner product from one to the other.\n\nA more involved example comes from differential topology, in which we have the notion of a smooth manifold. If \"M\" is such a manifold, and if \"X\" is any topological space which is homeomorphic to \"M\", we can consider \"X\" as a smooth manifold as well. That is, let formula_10 be a homeomorphism; we must define coordinate charts on \"X\", which we will do by \"pulling back\" coordinate charts on \"M\" through formula_6. Recall that a coordinate chart on formula_12 is an open set \"U\" together with an injective map\nfor some \"n\"; to get such a chart on \"X\", we let\nFurthermore, it is required that the charts cover \"M\", we must check that the transported charts cover \"X\", which follows immediately from the fact that formula_6 is a bijection. Finally, since \"M\" is a \"smooth\" manifold, we have that if \"U\" and \"V\", with their maps\nare two charts on \"M\", then the composition, the \"transition map\"\nis smooth. We must check this for our transported charts on \"X\". We have\nand therefore\nTherefore the transition map for formula_24 and formula_25 is the same as that for \"U\" and \"V\", hence smooth. Therefore \"X\" is a smooth manifold via transport of structure.\n\nAlthough the second example involved considerably more checking, the principle was the same, and any experienced mathematician would have no difficulty performing the necessary verifications. Therefore when such an operation is indicated, it is invoked merely as \"transport of structure\" and the details left to the reader, if desired.\n\nThe second example also illustrates why \"transport of structure\" is not always desirable. Namely, we can take \"M\" to be the plane, and we can take \"X\" to be an infinite one-sided cone. By \"flattening\" the cone we achieve a homeomorphism of \"X\" and \"M\", and therefore the structure of a smooth manifold on \"X\", but the cone is not \"naturally\" a smooth manifold. That is, we can consider \"X\" as a subspace of 3-space, in which context it is not smooth at the cone point. A more surprising example is that of exotic spheres, discovered by Milnor, which states that there are exactly 28 smooth manifolds which are homeomorphic (but by definition \"not\" diffeomorphic) to formula_26, the 7-dimensional sphere in 8-space. Thus, transport of structure is most productive when there exists a canonical isomorphism between the two objects.\n"}
{"id": "58151408", "url": "https://en.wikipedia.org/wiki?curid=58151408", "title": "Transseries", "text": "Transseries\n\nIn mathematics, the field formula_1 of logarithmic-exponential transseries is a non-Archimedean ordered differential field which extends comparability of asymptotic growth rates of elementary nontrigonometric functions to a much broader class of objects. Each log-exp transseries represents a formal asymptotic behavior, and it can be manipulated formally, and when it converges (or in every case if using special semantics such as through infinite surreal numbers), corresponds to actual behavior. Transseries can also be convenient for representing functions. Through their inclusion of exponentiation and logarithms, transseries are a strong generalization of the power series at infinity (formula_2) and other similar asymptotic expansions.\n\nThe field formula_1 was introduced independently by Dahn-Göring and Ecalle in the respective contexts of model theory or exponential fields and of the study of analytic singularity and proof by Ecalle of the Dulac conjectures. It constitutes a formal object, extending the field of exp-log functions of Hardy and the field of accelerando-summable series of Ecalle.\n\nThe field formula_1 enjoys a rich structure: an ordered field with a notion of generalized series and sums, with a compatible derivation with distinguished antiderivation, compatible exponential and logarithm functions and a notion of formal composition of series.\n\nInformally speaking, exp-log transseries are \"well-based\" (i.e. reverse well-ordered) formal Hahn series of real powers of the posive infinite indeterminate formula_5, exponentials, logarithms and their compositions, with real coefficients. Two important additionnal conditions are that the exponential and logarithmic depth of an exp-log transseries formula_6 that is the maximal numbers of iterations of exp and log occurring in formula_6 must be finite.\n\nThe following formal series are log-exp transseries:\n\nThe following formal series are \"not\" log-exp transseries:\n\nIt is possible to define differential fields of transseries containing the two last series, they belong respectively to formula_13 and formula_14 (see the paragraph \"Using surreal numbers\" below).\n\nA remarkable fact is that asymptotic growth rates of elementary nontrigonometric functions and even all functions definable in the model theoretic structure formula_15 of the ordered exponential field of real numbers are all comparable: \nFor all such formula_16 and formula_17, we have formula_18 or formula_19, where formula_20 means formula_21. The equivalence class of formula_16 under the relation formula_23 is the asymptotic behavior of formula_16, also called the \"germ\" of formula_16 (or the germ of formula_16 at infinity).\n\nThe field of transseries can be intuitively viewed as a formal generalization these growth rates: In addition to the elementary operations, transseries are closed under \"limits\" for appropriate sequences with bounded exponential and logarithmic depth. However, a complication is that growth rates are non-Archimedean and hence do not have the least upper bound property. We can address this by associating a sequence with the least upper bound of minimal complexity, analogously to construction of surreal numbers. For example, formula_27 is associated with formula_28 rather than formula_29 because formula_30 decays too quickly, and if we identify fast decay with complexity, it has greater complexity than necessary (also, because we care only about asymptotic behavior, pointwise convergence is not dispositive).\n\nBecause of the comparability, transseries do not include oscillatory growth rates (such as formula_31). On the other hand, there are transseries such as formula_32 that do not directly correspond to convergent series or real valued functions. Another limitation of transseries is that each of them is bounded by a tower of exponentials, i.e. a finite iteration formula_33 of formula_34, thereby excluding tetration and other transexponential functions, i.e. functions which grow faster than any tower of exponentials. There are ways to construct fields of generalized transseries including formal transsexponential terms, for instance formal solutions formula_35 of the Abel equation formula_36.\n\nTransseries can be defined as formal (potentially infinite) expressions, with rules defining which expressions are valid, comparison of transseries, arithmetic operations, and even differentiation. Appropriate transseries can then be assigned to corresponding functions or germs, but there are subtleties involving convergence. Even transseries that diverge can often be meaningfully (and uniquely) assigned actual growth rates (that agree with the formal operations on transseries) using accelero-summation, which is a generalization of Borel summation.\n\nTransseries can be formalized in several equivalent ways; we use one of the simplest ones here.\n\nA \"transseries\" is a well-based sum, \n\nwith finite exponential depth, where each formula_38 is a nonzero real number and formula_39 is a monic transmonomial (formula_40 is a transmonomial but is not monic unless the \"coefficient\" formula_41; each formula_39 is different; the order of the summands is irrelevant).\n\nThe sum might be infinite or transfinite; it is usually written in the order of decreasing formula_39.\n\nHere, \"well-based\" means that there is no infinite ascending sequence formula_44 (see well-ordering).\n\nA \"monic transmonomial\" is one of 1, \"x\", log \"x\", log log \"x\", ..., \"e\".\n\nA \"purely large transseries\" is a nonempty transseries formula_49 with every formula_50.\n\nTransseries have \"finite exponential depth\", where each level of nesting of \"e\" or log increases depth by 1 (so we cannot have \"x\" + log \"x\" + log log \"x\" + ...).\n\nAddition of transseries is termwise: formula_51 (absence of a term is equated with a zero coefficient).\n\n\"Comparison:\"\n\nThe most significant term of formula_49 is formula_40 for the largest formula_39 (because the sum is well-based, this exists for nonzero transseries). formula_49 is positive iff the coefficient of the most significant term is positive (this is why we used 'purely large' above). \"X\" > \"Y\" iff \"X\" − \"Y\" is positive.\n\n\"Comparison of monic transmonomials:\"\n\n\"Multiplication:\"\n\nThis essentially applies the distributive law to the product; because the series is well-based, the inner sum is always finite.\n\n\"Differentiation:\"\n\nWith these definitions, transseries is an ordered differential field. Transseries is also a valued field, with the valuation formula_67 given by the leading monic transmonomial, and the corresponding asymptotic relation defined for formula_68 by formula_69 if formula_70 (where formula_71 is the absolute value).\n\nWe first define the subfield formula_72 of formula_1 of so-called \"log-free transseries\". Those are transseries which exclude any logarithmic term.\n\n\"Inductive definition:\"\n\nFor formula_74 we will define a linearly ordered multiplicative group of \"monomials\" formula_75. We then let formula_76 denote the field of \"well-based series\" formula_77. This is the set of maps formula_78 with well-based (i.e. reverse well-ordered) support, equipped with pointwise sum and Cauchy product (see Hahn series). In formula_76, we distinguish the (non-unital) subring formula_80 of \"purely large transseries\", which are series whose support contains only monomials lying strictly above formula_81.\n\nThe field formula_106 is a proper subfield of the field formula_107 of well-based series with real coefficients and monomials in formula_108. Indeed, every series formula_16 in formula_106 has a bounded exponential depth, i.e. the least positive integer formula_111 such that formula_112, whereas the series \n\nhas no such bound.\n\n\"Exponentiation on formula_106:\"\n\nThe field of log-free transseries is equipped with an exponential function which is a specific morphism formula_115. Let formula_16 be a log-free transseries and let formula_117 be the exponential depth of formula_16, so formula_112. Write formula_16 as the sum formula_121 in formula_122 where formula_91, formula_124 is a real number and formula_125 is infinitesimal (any of them could be zero). Then the formal Hahn sum \n\nconverges in formula_76, and we define formula_128 where formula_129 is the value of the real exponential function at formula_124.\n\n\"Right-composition with formula_34:\"\n\nA right composition formula_132 with the series formula_34 can be defined by induction on the exponential depth by \n\nwith formula_135. It follows inductively that monomials are preserved by formula_136 so at each inductive step the sums are well-based and thus well defined.\n\n\"Definition:\"\n\nThe function formula_137 defined above is not onto formula_138 so the logarithm is only partially defined on formula_139: for instance the series formula_5 has no logarithm. Moreover, every positive infinite log-free transseries is greater than some positive power of formula_5. In order to move from formula_106 to formula_1, one can simply \"plug\" into the variable formula_5 of series formal iterated logarithms formula_145 which will behave like the formal reciprocal of the formula_111-fold iterated exponential term denoted formula_147.\n\nFor formula_148 let formula_149 denote the set of formal expressions formula_150 where formula_151. We turn this into an ordered group by defining formula_152, and defining formula_153 when formula_154. We define formula_155. If formula_156 and formula_157 we embed formula_149 into formula_159 by identifying an element formula_150 with the term \n\nWe then obtain formula_1 as the directed union \n\nOn formula_164 the right-composition formula_165 with formula_166 is naturally defined by \n\n\"Exponential and logarithm:\"\n\nExponentiation can be defined on formula_1 in a similar way as for log-free transseries, but here also formula_137 has a reciprocal formula_46 on formula_171. Indeed, for a strictly positive series formula_172, write formula_173 where formula_174 is the dominant monomial of formula_16 (largest element of its support), formula_124 is the corresponding positive real coefficient, and formula_177 is infinitesimal. The formal Hahn sum \n\nconverges in formula_179. Write formula_180 where formula_151 itself has the form formula_182 where formula_183 and formula_90. We define formula_185. We finally set \n\nOne may also define the field of log-exp transseries as a subfield of the ordered field formula_187 of surreal numbers. The field formula_187 is equipped with Gonshor-Kruskal's exponential and logarithm functions and with its natural structure of field of well-based series under Conway normal form.\n\nDefine formula_189, the subfield of formula_187 generated by formula_191 and the simplest positive infinite surreal number formula_192 (which corresponds naturally to the ordinal formula_192, and as a transseries to the series formula_5). Then, for formula_117, define formula_196 as the field generated by formula_197, exponentials of elements of formula_197 and logarithms of strictly positive elements of formula_197, as well as (Hahn) sums of summable families in formula_197. The union formula_201 is naturally isomorphic to formula_1. In fact, there is a unique such isomorphism which sends formula_192 to formula_5 and commutes with exponentiation and sums of summable families in formula_205 lying in formula_206.\n\n\n\nThe Berarducci-Mantova derivation on formula_187 coincides on formula_1 with its natural derivation, and is unique to satisfy compatibility relations with the exponential ordered field structure and generalized series field structure of formula_13 and formula_225\n\nContrary to formula_164 the derivation in formula_13 and formula_228 is not surjective: for instance the series \n\ndoesn't have an antiderivative in formula_13 or formula_231 (this is linked to the fact that those fields contain no transsexponential function).\n\nTransseries have very strong closure properties, and many operations can be defined on transseries:\n\nNote 1. The last two properties mean that formula_1 is \"Liouville closed\".\n\nNote 2. Just like an elementary nontrigonometric function, each positive infinite transseries formula_16 has integral exponentiality, even in this strong sense: \n\nThe number formula_243 is unique, it is called the \"exponentiality\" of formula_16.\n\nAn original property of formula_1 is that it admits a composition formula_246 (where formula_247 is the set of positive infinite log-exp transseries) which enables us to see each log-exp transseries formula_16 as a function on formula_247. Informally speaking, for formula_250 and formula_237, the series formula_252 is obtained by replacing each occurrence of the variable formula_5 in formula_16 by formula_17.\n\n\nThe formula_289 theory of formula_1 is decidable and can be axiomatized as follows (this is Theorem 2.2 of Aschenbrenner et al.):\n\nIn this theory, exponentiation is essentially defined for functions (using differentiation) but not constants; in fact, every definable subset of formula_298 is semialgebraic.\n\nThe formula_299 theory of formula_1 is that of the exponential real ordered exponential field formula_301, which is model complete by Wilkie's theorem.\n\nformula_302 is the field of accelero-summable transseries, and using accelero-summation, we have the corresponding Hardy field, which is conjectured to be the maximal Hardy field corresponding to a subfield of formula_303. (This conjecture is informal since we have not defined which isomorphisms of Hardy fields into differential subfields of formula_303 are permitted.) formula_302 is conjectured to satisfy the above axioms of formula_303. Without defining accelero-summation, we note that when operations on convergent transseries produce a divergent one while the same operations on the corresponding germs produce a valid germ, we can then associate the divergent transseries with that germ.\n\nA Hardy field is said \"maximal\" if it is properly contained in no Hardy field. By an application of Zorn's lemma, every Hardy field is contained in a maximal Hardy field. It is conjectured that all maximal Hardy fields are elementary equivalent as differential fields, and indeed have the same first order theory as formula_1. Logarithmic-transseries do not themselves correspond to a maximal Hardy field for not every transseries corresponds to a real function, and maximal Hardy fields always contain transsexponential functions.\n\n\n"}
{"id": "48539749", "url": "https://en.wikipedia.org/wiki?curid=48539749", "title": "Wojciech Szpankowski", "text": "Wojciech Szpankowski\n\nWojciech Szpankowski is the Saul Rosen Professor of Computer Science at the Purdue University. He is known for his work in analytic combinatorics, analysis of algorithms and analytic information theory. He is the director of the NSF Science and Technology Center for Science of Information.\n\nSzpankowski received his MS and PhD in Electrical Engineering and Computer Science from the Technical University of Gdańsk in 1970 and 1980 respectively.\n\n"}
{"id": "14927756", "url": "https://en.wikipedia.org/wiki?curid=14927756", "title": "X-Machine Testing", "text": "X-Machine Testing\n\nThe (Stream) X-Machine Testing Methodology is a \"complete\" functional testing approach to software- and hardware testing that exploits the scalability of the Stream X-Machine model of computation. \nUsing this methodology, it is likely to identify a finite test-set that exhaustively determines whether the tested system's implementation matches its specification. This goal is achieved by a divide-and-conquer approach, in which the design is decomposed by refinement into a collection of Stream X-Machines, which are implemented as separate modules, then tested bottom-up. At each integration stage, the testing method guarantees that the tested components are correctly integrated.\n\nThe methodology overcomes formal undecidability limitations by requiring that certain design for test principles are followed during specification and implementation. The resulting scalability means that practical software and hardware systems consisting of hundreds of thousands of states and millions of transitions have been tested successfully.\n\nMuch software testing is merely hopeful, seeking to exercise the software system in various ways to see whether any faults can be detected. Testing may indeed reveal some faults, but can never guarantee that the system is correct, once testing is over. \nFunctional testing methods seek to improve on this situation, by developing a formal specification describing the intended behaviour of the system, against which the implementation is later tested (a kind of conformance testing). The specification can be validated against the user-requirements and later proven to be consistent and complete by mathematical reasoning (eliminating any logical design flaws). Complete functional testing methods exploit the specification systematically, generating test-sets which exercise the implemented software system \"exhaustively\", to determine whether it conforms to the specification. In particular:\nThis level of testing can be difficult to achieve, since software systems are extremely complex, with hundreds of thousands of states and millions of transitions. What is needed is a way of breaking down the specification and testing problem into parts which can be addressed separately.\n\nMike Holcombe first proposed using Samuel Eilenberg's theoretical X-machine model as the basis for software specification in the late 1980s.\nThis is because the model cleanly separates the \"control flow\" of a system from the \"processing\" carried out by the system. At a given level of abstraction, the system can be viewed as a simple finite state machine consisting of a few states and transitions. The more complex processing is delegated to the \"processing functions\" on the transitions, which modify the underlying fundamental data type \"X\". Later, each processing function may be separately exposed and characterized by another X-machine, modelling the behaviour of that system operation.\n\nThis supports a divide-and-conquer approach, in which the overall system architecture is specified first, then each major system operation is specified next, followed by subroutines, and so forth. At each step, the level of complexity is manageable, because of the independence of each layer. In particular, it is easy for software engineers to validate the simple finite state machines against user requirements.\n\nGilbert Laycock first proposed a particular kind of X-machine, the Stream X-Machine, as the basis for the testing method. The advantage of this variant was the way in which testing could be controlled. In a Stream X-Machine, the fundamental data type has a particular form: \"X\" = \"Out\"* × \"Mem\" × \"In\"*, where \"In\"* is an input stream, \"Out\"* is an output stream, and \"Mem\" is the internal memory. The transitions of a Stream X-Machine are labelled with processing functions of the form φ: \"Mem\" × \"In\" → \"Out\" × \"Mem\", that is, they consume one input from the input stream, possibly modify memory, and produce one output on the output stream (see the associated article for more details).\n\nThe benefits for testing are that software systems designed in this way are \"observable\" at each step. For each input, the machine takes one step, producing an output, such that input/output pairs may be matched exactly. This contrasts with other approaches in which the system \"runs to completion\" (taking multiple steps) before any observation is made. Furthermore, layered Stream X-Machines offer a convenient abstraction. At each level, the tester may \"forget\" about the details of the processing functions and consider the (sub-)system just as a simple finite state machine. Powerful methods exist for testing systems that conform to finite state specifications, such as Chow's W-method.\n\nWhen following the (Stream) X-Machine methodology, the first stage is to identify the various types of data to be processed. For example, a word processor will use basic types \"Character\" (keyboard input), \"Position\" (mouse cursor position) and \"Command\" (mouse or menu command). There may be other constructed types, such as \"Text\" ::= \"Character\"* (a sequence of characters), \"Selection\" ::= \"Position\" × \"Position\" (the start and end of the selection) and \"Document\" ::= \"Text\" × \"Selection\" × \"Boolean\" (the text, a possible selection, and a flag to signal if the document has been modified).\n\nThe top-level specification is a Stream X-Machine describing the main user interaction with the system. For example, the word processor will exist in a number of states, in which keystrokes and commands will have different effects. Suppose that this word processor exists in the states {Writing, Selecting, Filing, Editing}. We expect the word processor to start in the initial Writing state, but to move to the Selecting state if either the mouse is \"dragged\", or the \"shift-key\" is held down. Once the selection is established, it should return to the Writing state. Likewise, if a menu option is chosen, this should enter the Editing or Filing state. In these states, certain keystrokes may have different meanings. The word processor eventually returns to the Writing state, when any menu command has finished. This state machine is designed and labelled informally with the various actions that cause it to change state.\n\nThe input, memory and output types for the top-level machine are now formalised. Suppose that the memory type of the simple word processor is the type \"Document\" defined above. This treats a document as a text string, with two positions marking a possible selection and a flag to indicate modification since the last \"save\"-command. A more complex word processor might support undoable editing, with a sequence of document states: \"Document\" ::= (\"Text\" × \"Selection\")*, which are collapsed to one document every time a \"save\"-command is performed.\n\nSuppose that the input type for the machine is: \"In\" ::= \"Command\" × \"Character\" × \"Position\". This recognises that every interaction could be a simple character insertion, a menu command or a cursor placement. Any given interaction is a 3-tuple, but some places may be empty. For example, (\"Insert\", 'a', ε) would represent typing the character 'a'; while (\"Position\", ε, 32) would mean placing the cursor between characters 32 and 33; and (\"Select\", ε, 32) would mean selecting the text between the current cursor position and the place between characters 32 and 33.\n\nThe output type for the machine is designed so that it is possible to determine from the output \"which\" processing function was executed, in response to a given input. This relates to the property of \"output distinguishability\", described below.\n\nIf a system is complex, then it will most likely be decomposed into several Stream X-Machines. The most common kind of refinement is to take each of the major processing functions (which were the labels on the high-level machine) and treat these as separate Stream X-Machines. In this case, the input, memory and output types for the low-level machines will be different from those defined for the high-level machine. Either, this is treated as an expansion of the data sets used at the high level, or there is a translation from more abstract data sets at the high level into more detailed data sets at the lower level. For example, a command \"Select\" at the high level could be decomposed into three events: \"MouseDown\", \"MouseMove\", \"MouseUp\" at the lower level.\n\nIpate and Holcombe mention several kinds of refinement, including \"functional refinement\", in which the behaviour of the processing functions is elaborated in more detail, and \"state refinement\", in which a simple state-space is partitioned into a more complex state-space. Ipate proves these two kinds of refinement to be eventually equivalent\n\nSystems are otherwise specified down to the level at which the designer is prepared to trust the primitive operations supported by the implementation environment. It is also possible to test small units exhaustively by other testing methods.\n\nThe (Stream) X-Machine methodology requires the designer to observe certain design for test conditions. These are typically not too difficult to satisfy. For each Stream X-Machine in the specification, we must obtain:\n\n\nA minimal machine is the machine with the fewest states and transitions for some given behaviour. Keeping the specification minimal simply ensures that the test sets are as small as possible. A deterministic machine is required for systems that are predictable. Otherwise, an implementation could make an arbitrary choice regarding which transition was taken. Some recent work has relaxed this assumption to allow testing of non-deterministic machines.\n\nTest completeness is needed to ensure that the implementation is testable within tractable time. For example, if a system has distant, or hard-to-reach states that are only entered after memory has reached a certain limiting value, then special test inputs should be added to allow memory to be bypassed, forcing the state machine into the distant state. This allows all (abstract) states to be covered quickly during testing. Output distinguishability is the key property supporting the scalable testing method. It allows the tester to treat the processing functions φ as simple labels, whose detailed behaviour may be safely ignored, while testing the state machine of the next integration layer. The unique outputs are witness values, which guarantee that a particular function was invoked.\n\nThe (Stream) X-Machine Testing Method assumes that both the design and the implementation can be considered as (a collection of) Stream X-Machines. For each pair of corresponding machines (\"Spec\", \"Imp\"), the purpose of testing is to determine whether the behaviour of \"Imp\", the machine of the implementation, exactly matches the behaviour of \"Spec\", the machine of the specification. Note that \"Imp\" need not be a minimal machine - it may have more states and transitions than \"Spec\" and still behave in an identical way.\n\nTo test all behaviours, it must be possible to drive a machine into all of its states, then attempt all possible transitions (those which should succeed, and those which should be blocked) to achieve full \"positive\" and \"negative\" testing (see above). For transitions which succeed, the destination state must also be verified. Note that if \"Spec\" and \"Imp\" have the same number of states, the above describes the smallest test-set that achieves the objective. If \"Imp\" has more states and transitions than \"Spec\", longer test sequences are needed to guarantee that \"redundant\" states in \"Imp\" also behave as expected.\n\nThe basis for the test generation strategy is Tsun S. Chow's W-Method for testing finite state automata, chosen because it supports the testing of redundant implementations. Chow's method assumes simple finite state machines with observable inputs and outputs, but no directly observable states. To map onto Chow's formalism, the functions φ on the transitions of the Stream X-Machines are treated simply as labels (inputs, in Chow's terms) and the distinguishing outputs are used directly. (Later, a mapping from real inputs and memory (\"mem\", \"in\") is chosen to trigger each function φ, according to its domain).\n\nTo identify specific states in \"Imp\", Chow chooses a \"characterization set, W\", the smallest set of test sequences that uniquely characterizes each state in \"Spec\". That is, when starting in a given state, exercising the sequences in \"W\" should yield at least one observable difference, compared to starting in any other state.\n\nTo reach each state expected in \"Spec\", the tester constructs the \"state cover, C\", the smallest set of test sequences that reaches every state. This can be constructed by automatic breadth-first exploration of \"Spec\". The test-set which validates all the states of a minimal \"Imp\" is then: \"C\" formula_1 \"W\", where formula_1 denotes the \"concatenated product\" of the two sets. For example, if \"C\" = {<\"a\">, <\"b\">} and \"W\" = {<\"c\">, <\"d\">}, then \"C\" formula_1 \"W\" = {<\"ac\">, <\"ad\">,<\"bc\">, <\"bd\">}.\n\nThe above test-set determines whether a minimal \"Imp\" has the same states as \"Spec\". To determine whether a minimal \"Imp\" also has the same transition behaviour as \"Spec\", the tester constructs the \"transition cover, K\". This is the smallest set of test sequences that reaches every state and then attempts every possible transition once, from that state. Now, the input alphabet consists of (the labels of) every function φ in Φ. Let us construct a set of length-1 test sequences, consisting of single functions chosen from Φ, and call this Φ. The transition cover is defined as \"K\" codice_1 \"C\" formula_4 \"C\" formula_1 Φ.\n\nThis will attempt every possible transition from every state. For those which succeed, we must validate the destination states. So, the smallest test-set \"T\" which completely validates the behaviour of a minimal \"Imp\" is given by: \"T\" ::= \"C\" formula_1 \"W\" formula_4 \"C\" formula_1 Φ formula_1 \"W\". This formula can be rearranged as:\n\nwhere Φ is the set containing the empty sequence {<>}.\nIf \"Imp\" has more states than \"Spec\", the above test-set may not be sufficient to guarantee the conformant behaviour of replicated states in \"Imp\". So, sets of longer test sequences are chosen, consisting of all pairs of functions Φ, all triples of functions Φ up to some limit Φ, when the tester is satisfied that \"Imp\" cannot contain chains of duplicated states longer than \"k\"-1. The final test formula is given by:\n\nThis test-set completely validates the behaviour of a non-minimal \"Imp\" in which chains of duplicated states are expected to be no longer than \"k\"-1. For most practical purposes, testing up to \"k\"=2, or \"k\"=3 is quite exhaustive, revealing all state-related faults in really poor implementations.\n"}
