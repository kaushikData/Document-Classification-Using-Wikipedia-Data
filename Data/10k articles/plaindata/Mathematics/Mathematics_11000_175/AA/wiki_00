{"id": "463020", "url": "https://en.wikipedia.org/wiki?curid=463020", "title": "144 (number)", "text": "144 (number)\n\n144 (one hundred [and] forty-four) is the natural number following 143 and preceding 145. 144 is a dozen dozens, or one gross.\n\n144 is the twelfth Fibonacci number, and the largest one to also be a square, as the square of 12 (which is also its index in the Fibonacci sequence), following 89 and preceding 233.\n\n144 is the smallest number with exactly 15 divisors, but it is not highly composite since a smaller number 120 has 16 divisors.\n\n144 is divisible by the value of its φ function, which returns 48 in this case. Also, there are 21 solutions to the equation φ(\"x\") = 144, more than any integer below 144, making it a highly totient number.\n\n144 = 27 + 84 + 110 + 133, the smallest number whose fifth power is a sum of four (smaller) fifth powers. This solution was found in 1966 by L. J. Lander and T. R. Parkin, and disproved Euler's sum of powers conjecture.\n\nThe maximum determinant in a 9 by 9 matrix of zeroes and ones is 144.\n\n144 is in base 10 a sum-product number, as well as a Harshad number. 144 is a repdigit in bases 15, 17, 23, 35, 47, 71, and 143. 144 is the sum of a twin prime pair (71 + 73)\n\n\n\n\n144 is also:\n\n\n\n"}
{"id": "1206236", "url": "https://en.wikipedia.org/wiki?curid=1206236", "title": "Abramowitz and Stegun", "text": "Abramowitz and Stegun\n\nAbramowitz and Stegun (AS) is the informal name of a mathematical reference work edited by Milton Abramowitz and Irene Stegun of the United States National Bureau of Standards (NBS), now the \"National Institute of Standards and Technology\" (NIST). Its full title is Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. A digital successor to the Handbook was released as the \"Digital Library of Mathematical Functions\" (DLMF) on May 11, 2010, along with a printed version, the \"NIST Handbook of Mathematical Functions\", published by Cambridge University Press.\n\nSince it was first published in 1964, the 1046 page \"Handbook\" has been one of the most comprehensive sources of information on special functions, containing definitions, identities, approximations, plots, and tables of values of numerous functions used in virtually all fields of applied mathematics. The notation used in the \"Handbook\" is the \"de facto\" standard for much of applied mathematics today.\n\nAt the time of its publication, the \"Handbook\" was an essential resource for practitioners. Nowadays, computer algebra systems have replaced the function tables, but the \"Handbook\" remains an important reference source. The foreword discusses a meeting in 1954 in which it was agreed that \"the advent of high-speed computing equipment changed the task of table making but definitely did not remove the need for tables\".\n\nBecause the \"Handbook\" is the work of U.S. federal government employees acting in their official capacity, it is not protected by copyright in the United States. While it could be ordered from the Government Printing Office, it has also been reprinted by commercial publishers, most notably Dover Publications (), and can be legally viewed on and downloaded from the web.\n\nWhile there was only one edition of the work, it went through many print runs including a growing number of corrections.\n\nOriginal NBS edition:\n\n\nReprint edition by Dover Publications:\n\n\nUp to the tenth printing of the original NBS edition in December 1972, corrections were incorporated on pages 2–3, 6–8, 10, 15, 19–20, 25, 76, 85, 91, 102, 187, 189–197, 218, 223, 225, 233, 250, 255, 260–263, 268, 271–273, 292, 302, 328, 332, 333–337, 362, 365, 415, 423, 438–440, 443, 445, 447, 449, 451, 484, 498, 505–506, 509–510, 543, 556, 558, 562, 571, 595, 599, 600, 722–723, 739, 742, 744, 746, 752, 756, 760–765, 774, 777–785, 790, 797, 801, 822–823, 832, 835, 844, 886–889, 897, 914, 915, 920, 930–931, 936, 940–941, 944–950, 953, 960, 963, 989–990, 1010 and 1026.\n\nThe ninth reprint edition by Dover Publications incorporates additional corrections on pages 18, 79, 80, 82, 408, 450, 786, 825 and 934.\n\nAs a side-note, the Dover paperback edition (SBN 468-61272-4) cover names the second editor \"Irene A. Segun\" instead of Stegun. This error is sometimes used to illustrate the human trait of looking in every place except the most obvious one. \n\nUnresolved errata:\n\nMichael Danos and Johann Rafelski edited the “\"Pocketbook of Mathematical Functions\"”, published by Verlag Harri Deutsch in 1984. The book is an abridged version of Abramowitz's and Stegun's Handbook, retaining most of the formulas (except for the first and the two last original chapters, which were dropped), but reducing the numerical tables to a minimum, which, by this time, could be easily calculated with scientific pocket calculators. The references were removed as well. Most known errata were incorporated, the physical constants updated and the now-first chapter saw some slight enlargement compared to the former second chapter. The numbering of formulas was kept for easier cross-reference.\n\nA digital successor to the Handbook, long under development at NIST, was released as the “Digital Library of Mathematical Functions” (DLMF) on May 11, 2010, along with a printed version, the \"NIST Handbook of Mathematical Functions\", published by Cambridge University Press.\n\n\n\n"}
{"id": "2575836", "url": "https://en.wikipedia.org/wiki?curid=2575836", "title": "Abstract state machine", "text": "Abstract state machine\n\nIn computer science, an abstract state machine (ASM) is a state machine operating on states that are arbitrary data structures (structure in the sense of mathematical logic, that is a nonempty set together with a number of functions (operations) and relations over the set).\n\nThe ASM Method is a practical and scientifically well-founded systems engineering method that bridges the gap between the two ends of system development:\n\nThe method builds upon three basic concepts: \n\nIn the original conception of ASMs, a single agent executes a program in a sequence of steps, possibly interacting with its environment. This notion was extended to capture distributed computations, in which multiple agents execute their programs concurrently.\n\nSince ASMs model algorithms at arbitrary levels of abstraction, they can provide high-level, low-level and mid-level views of a hardware or software design. ASM specifications often consist of a series of ASM models, starting with an abstract \"ground model\" and proceeding to greater levels of detail in successive refinements or coarsenings.\n\nDue to the algorithmic and mathematical nature of these three concepts, ASM models and their properties of interest can be analyzed using any rigorous form of verification (by reasoning) or validation (by experimentation, testing model executions).\n\nThe concept of ASMs is due to Yuri Gurevich, who first proposed it in the mid-1980s as a way of improving on Turing's thesis that every algorithm is simulated by an appropriate Turing machine. He formulated the \"ASM Thesis\": every algorithm, no matter how abstract, is step-for-step emulated by an appropriate ASM. In 2000, Gurevich axiomatized the notion of sequential algorithms, and proved the ASM thesis for them. Roughly stated, the axioms are as follows: states are structures, the state transition involves only a bounded part of the state, and everything is invariant under isomorphisms of structures. (Structures can be viewed as algebras, which explains the original name \"evolving algebras\" for ASMs.) The axiomatization and characterization of sequential algorithms have been extended to parallel and interactive algorithms.\n\nIn the 1990s, by a community effort, the ASM method was developed, using ASMs for the formal specification and analysis (verification and validation) of computer hardware and software. Comprehensive ASM specifications of programming languages (including Prolog, C, and Java) and design languages (UML and SDL) have been developed. \nA detailed historical account can be found in the AsmBook (Chapter 9) or in\nthis article.\n\nA number of software tools for ASM execution and analysis are available.\n\n\n\n\n\n"}
{"id": "5023947", "url": "https://en.wikipedia.org/wiki?curid=5023947", "title": "Apollonian circles", "text": "Apollonian circles\n\nApollonian circles are two families of circles such that every circle in the first family intersects every circle in the second family orthogonally, and vice versa. These circles form the basis for bipolar coordinates. They were discovered by Apollonius of Perga, a renowned Greek geometer.\n\nThe Apollonian circles are defined in two different ways by a line segment denoted \"CD\".\n\nEach circle in the first family (the blue circles in the figure) is associated with a positive real number \"r\", and is defined as the locus of points \"X\" such that the ratio of distances from \"X\" to \"C\" and to \"D\" equals \"r\",\nFor values of \"r\" close to zero, the corresponding circle is close to \"C\", while for values of \"r\" close to ∞, the corresponding circle is close to \"D\"; for the intermediate value \"r\" = 1, the circle degenerates to a line, the perpendicular bisector of \"CD\". The equation defining these circles as a locus can be generalized to define the Fermat–Apollonius circles of larger sets of weighted points.\n\nEach circle in the second family (the red circles in the figure) is associated with an angle \"θ\", and is defined as the locus of points \"X\" such that the inscribed angle CXD equals \"θ\",\n\nScanning \"θ\" from 0 to \"π\" generates the set of all circles passing through the two points \"C\" and \"D\".\n\nThe two points where all the red circles cross are the limiting points of pairs of circles in the blue family.\n\nA given blue circle and a given red circle intersect in two points. In order to obtain bipolar coordinates, a method is required to specify which point is the right one. An isoptic arc is the locus of points \"X\" that sees points \"C\" and \"D\" under a given oriented angle of vectors i.e. \nSuch an arc is contained into a red circle and is bounded by points \"C\" and \"D\". The remaining part of the corresponding red circle is formula_4. When we really want the whole red circle, a description using oriented angles of straight lines has to be used\n\nBoth of the families of Apollonian circles are called pencils of circles. More generally, there is a natural correspondence between circles in the plane and points in three-dimensional projective space; a line in this space corresponds to a one-dimensional continuous family of circles called a pencil.\n\nSpecifically, the equation of a circle of radius \"r\" centered at a point (\"p\",\"q\"),\nmay be rewritten as\nwhere α = 1, β = \"p\", γ = \"q\", and δ = \"p\" + \"q\" − \"r\". \nIn this form, multiplying the quadruple (α,β,γ,δ) by a scalar produces a different quadruple that represents the same circle; thus, these quadruples may be considered to be homogeneous coordinates for the space of circles. Straight lines may also be represented with an equation of this type in which α = 0 and should be thought of as being a degenerate form of a circle. When α ≠ 0, we may solve for \"p\" = β/α, \"q\" = γ/α, and \"r\" =√((−δ − β − γ)/α); note, however, that the latter formula may give \"r\" = 0 (in which case the circle degenerates to a point) or \"r\" equal to an imaginary number (in which case the quadruple (α,β,γ,δ) is said to represent an \"imaginary circle\").\n\nThe set of affine combinations of two circles (α,β,γ,δ), (α,β,γ,δ), that is, the set of circles represented by the quadruple\nfor some value of the parameter \"z\",\nforms a pencil; the two circles are called \"generators\" of the pencil. There are three types of pencil:\n\nA family of concentric circles centered at a single focus \"C\" forms a special case of a hyperbolic pencil, in which the other focus is the point at infinity of the complex projective line. The corresponding elliptic pencil consists of the family of straight lines through \"C\"; these should be interpreted as circles that all pass through the point at infinity.\n\nExcept for the two special cases of a pencil of concentric circles and a pencil of coincident lines,\nany two circles within a pencil have the same radical axis, and all circles in the pencil have collinear centers. Any three or more circles from the same family are called coaxal circles or coaxial circles.\n\nThe elliptic pencil of circles passing through the two points \"C\" and \"D\" (the set of red circles, in the figure) has the line \"CD\" as its radical axis. The centers of the circles in this pencil lie on the perpendicular bisector of \"CD\".\nThe hyperbolic pencil defined by points \"C\" and \"D\" (the blue circles) has its radical axis on the perpendicular bisector of line \"CD\", and all its circle centers on line \"CD\".\n\nThe radical axis of any pencil of circles, interpreted as an infinite-radius circle, belongs to the pencil.\nAny three circles belong to a common pencil whenever all three pairs share the same radical axis and their centers are collinear.\n\nCircle inversion transforms the plane in a way that maps circles into circles, and pencils of circles into pencils of circles. The type of the pencil is preserved: the inversion of an elliptic pencil is another elliptic pencil, the inversion of a hyperbolic pencil is another hyperbolic pencil, and the inversion of a parabolic pencil is another parabolic pencil.\n\nIt is relatively easy to show using inversion that, in the Apollonian circles, every blue circle intersects every red circle orthogonally, i.e., at a right angle. Inversion of the blue Apollonian circles with respect to a circle centered on point \"C\" results in a pencil of concentric circles centered at the image of point \"D\". The same inversion transforms the red circles into a set of straight lines that all contain the image of \"D\". Thus, this inversion transforms the bipolar coordinate system defined by the Apollonian circles into a polar coordinate system.\nObviously, the transformed pencils meet at right angles. Since inversion is a conformal transformation, it preserves the angles between the curves it transforms, so the original Apollonian circles also meet at right angles.\n\nAlternatively, the orthogonal property of the two pencils follows from the defining property of the radical axis, that from any point \"X\" on the radical axis of a pencil \"P\" the lengths of the tangents from \"X\" to each circle in \"P\" are all equal. It follows from this that the circle centered at \"X\" with length equal to these tangents crosses all circles of \"P\" perpendicularly. The same construction can be applied for each \"X\" on the radical axis of \"P\", forming another pencil of circles perpendicular to \"P\".\n\nMore generally, for every pencil of circles there exists a unique pencil consisting of the circles that are perpendicular to the first pencil. If one pencil is elliptic, its perpendicular pencil is hyperbolic, and vice versa; in this case the two pencils form a set of Apollonian circles. The pencil of circles perpendicular to a parabolic pencil is also parabolic; it consists of the circles that have the same common tangent point but with a perpendicular tangent line at that point.\n\n\n"}
{"id": "23834912", "url": "https://en.wikipedia.org/wiki?curid=23834912", "title": "Arithmetic topology", "text": "Arithmetic topology\n\nArithmetic topology is an area of mathematics that is a combination of algebraic number theory and topology. It establishes an analogy between number fields and closed, orientable 3-manifolds.\n\nThe following are some of the analogies used by mathematicians between number fields and 3-manifolds:\n\nExpanding on the last two examples, there is an analogy between knots and prime numbers in which one considers \"links\" between primes. The triple of primes are \"linked\" modulo 2 (the Rédei symbol is −1) but are \"pairwise unlinked\" modulo 2 (the Legendre symbols are all 1). Therefore these primes have been called a \"proper Borromean triple modulo 2\" or \"mod 2 Borromean primes\".\n\nIn the 1960s topological interpretations of class field theory were given by John Tate based on Galois cohomology, and also by Michael Artin and Jean-Louis Verdier based on Étale cohomology. Then David Mumford (and independently Yuri Manin) came up with an analogy between prime ideals and knots which was further explored by Barry Mazur. In the 1990s Reznikov and Kapranov began studying these analogies, coining the term arithmetic topology for this area of study.\n\n\n\n"}
{"id": "42020168", "url": "https://en.wikipedia.org/wiki?curid=42020168", "title": "Carl S. Herz", "text": "Carl S. Herz\n\nCarl Samuel Herz (10 April 1930, Rockville Centre, Long Island, New York – 1 May 1995) was an American-Canadian mathematician, specializing in harmonic analysis. His name is attached to the Herz–Schur multiplier.\n\nHerz received his bachelor's degree from Cornell University in 1950 and then became a mathematics graduate student at Princeton University. There he received a Ph.D. under the supervision of Salomon Bochner in 1953 with the dissertation \"Bessel Functions of Matrix Argument\". According to Tom H. Koornwinder, Herz's dissertation (published in the Annals of Mathematics in May 1955) \"was a pioneering paper in the field of special functions in several variables associated with Lie groups and with root systems.\" Herz returned to Cornell, where he became in 1953 an instructor, in 1955 an assistant professor, in 1958 an associate professor, and in 1963 a full professor and remained at Cornell until 1969. He worked for the academic year 1969–1970 at Brandeis University and then became in 1970 a professor at McGill University, where he remained until his death in 1995. For the academic year 1962–1963 Herz was a Sloan Fellow at Université de Paris-Sud at Orsay; in subsequent years he was a frequent academic visitor at Orsay for a month or two each year. In the academic years 1957-1958 and 1976–1977 he was a visiting scholar at the Institute for Advanced Study. Herz did mathematical research on spectral synthesis, positive definite functions, Fourier transforms on convex sets, potential theory, H, and BMO. According to Nicholas Varopoulos, Herz made contributions \"to the theory of symmetric spaces, Lie groups and the heat kernel on these; among other things he succeeded in classifying all faithful representations of Lie groups by contact transformations of a compact manifold.\"\n\nIn 1978 he was elected of a Fellow of the Royal Society of Canada. In 1986 he was awarded the Jeffery–Williams Prize by the Canadian Mathematical Society. The Institut des sciences mathématiques (a consortium of eight Quebec universities) established a prize in honour of Carl Herz. He was the president of the Canadian Mathematical Society in 1987–1989.\n\n"}
{"id": "70662", "url": "https://en.wikipedia.org/wiki?curid=70662", "title": "Chaffing and winnowing", "text": "Chaffing and winnowing\n\nChaffing and winnowing is a cryptographic technique to achieve confidentiality without using encryption when sending data over an insecure channel. The name is derived from agriculture: after grain has been harvested and threshed, it remains mixed together with inedible fibrous chaff. The chaff and grain are then separated by winnowing, and the chaff is discarded. The cryptographic technique was conceived by Ron Rivest and published in an on-line article on 18 March 1998. Although it bears similarities to both traditional encryption and steganography, it cannot be classified under either category.\n\nThis technique allows the sender to deny responsibility for encrypting their message. When using chaffing and winnowing, the sender transmits the message unencrypted, in clear text. Although the sender and the receiver share a secret key, they use it only for authentication. However, a third party can make their communication confidential by simultaneously sending specially crafted messages through the same channel.\n\nThe sender (Alice) wants to send a message to the receiver (Bob). In the simplest setup, Alice enumerates the symbols (usually bits) in her message and sends out each in a separate packet. In general the method requires each symbol to arrive in-order and to be authenticated by the receiver. When implemented over networks that may change the order of packets, the sender places the symbol's serial number in the packet, the symbol itself (both unencrypted), and a message authentication code (MAC). Many MACs use a secret key Alice shares with Bob, but it is sufficient that the receiver has a method to authenticate the packets. Charles, who transmits Alice's packets to Bob, interleaves the packets with corresponding bogus packets (called \"chaff\") with corresponding serial numbers, arbitrary symbols, and a random number in place of the MAC. Charles does not need to know the key to do that (real MAC are large enough that it is extremely unlikely to generate a valid one by chance, unlike in the example). Bob uses the MAC to find the authentic messages and drops the \"chaff\" messages. This process is called \"winnowing\".\n\nAn eavesdropper located between Alice and Charles can easily read Alice's message. But an eavesdropper between Charles and Bob would have to tell which packets are bogus and which are real (i.e. to winnow, or \"separate the wheat from the chaff\"). That is infeasible if the MAC used is secure and Charles does not leak any information on packet authenticity (e.g. via timing).\n\nIf a fourth party, Dave, (anyone other than Alice, Charles, or Bob) requires Alice to disclose her secret key, she can defend with the argument that she used the key merely for authentication and did not intend to make the message confidential. If Dave cannot force Alice to disclose an authentication key (the knowledge of which would enable him to forge messages from Alice), then her messages will remain confidential. On the other hand, Charles does not even possess any secret keys that he could be ordered to disclose.\n\nThe simple variant of the chaffing and winnowing technique described above adds many bits of overhead per bit of original message. To make the transmission more efficient, Alice can process her message with an all-or-nothing transform and then send it out in much larger chunks. The chaff packets will have to be modified accordingly. Because the original message can be reconstructed only by knowing all of its chunks, Charles needs to send only enough chaff packets to make finding the correct combination of packets computationally infeasible.\n\nChaffing and winnowing lends itself especially well to use in packet-switched network environments such as the Internet, where each message (whose payload is typically small) is sent in a separate network packet. In another variant of the technique, Charles carefully interleaves packets coming from multiple senders. That eliminates the need for Charles to generate and inject bogus packets in the communication. However, the text of Alice's message cannot be well protected from other parties who are communicating via Charles at the same time. This variant also helps protect against information leakage and traffic analysis.\n\nRon Rivest suggests that laws related to cryptography, including export controls, would not apply to \"chaffing and winnowing\" because it does not employ any encryption at all.\n\nThe author of the paper proposes that the security implications of handing everyone's authentication keys to the government for law-enforcement purposes would be far too risky, since possession of the key would enable someone to masquerade and communicate as another entity, such as an airline controller. Furthermore, Ron Rivest contemplates the possibility of rogue law enforcement officials framing up innocent parties by introducing the chaff into their communications, concluding that drafting a law restricting \"chaffing and winnowing\" would be far too difficult.\n\nThe term \"winnowing\" was suggested by Ronald Rivest's father. Before the publication of Rivest's paper in 1998 other people brought to his attention a 1965 novel, Rex Stout's \"The Doorbell Rang\", which describes the same concept and was thus included in the paper's references.\n\n"}
{"id": "977983", "url": "https://en.wikipedia.org/wiki?curid=977983", "title": "Charge qubit", "text": "Charge qubit\n\nIn quantum computing, a charge qubit is a qubit whose basis states are charge states (e.g. states which represent the presence or absence of excess Cooper pairs in the island). In superconducting quantum computing, a charge qubit is formed by a tiny superconducting island (also known as a Cooper-pair box) coupled by a Josephson junction to a superconducting reservoir (see figure). The state of the qubit is determined by the number of Cooper pairs which have tunneled across the junction. In contrast with the charge state of an atomic or molecular ion, the charge states of such an \"island\" involve a macroscopic number of conduction electrons of the island. The quantum superposition of charge states can be achieved by tuning the gate voltage U that controls the chemical potential of the island. The charge qubit is typically read-out by electrostatically coupling the island to an extremely sensitive electrometer such as the radio-frequency single-electron transistor.\n\nTypical T coherence times for a charge qubit are on the order of 1–2 μs. Recent work has shown T times approaching 100 μs using a type of charge qubit known as a transmon inside a three-dimensional superconducting cavity. Understanding the limits of T is an active area of research in the field of superconducting quantum computing.\n\nTo-date, the realizations of qubits that have had the most success are ion traps and NMR, with Shor's algorithm even being implemented using NMR. However, it is hard to see these two methods being scaled to the hundreds, thousands, or millions of qubits necessary to create a quantum computer. Solid-state representations of qubits are much more easily scalable, but they themselves have their own problem: decoherence. Superconductors, however, have the advantage of being more easily scaled, and they are more coherent than normal solid-state systems.\n\nSuperconducting charge qubits have been progressing quickly. They were first suggested in 1997 by Shnirman, and by 2001 coherent oscillations were observed.\n"}
{"id": "1442505", "url": "https://en.wikipedia.org/wiki?curid=1442505", "title": "Complex conjugate vector space", "text": "Complex conjugate vector space\n\nIn mathematics, the complex conjugate of a complex vector space formula_1 is a complex vector space formula_2, which has the same elements and additive group structure as formula_3, but whose scalar multiplication involves conjugation of the scalars. In other words, the scalar multiplication of formula_2 satisfies\nwhere formula_6 is the scalar multiplication of formula_2 and formula_8 is the scalar multiplication of formula_3.\nThe letter formula_10 stands for a vector in formula_1, formula_12 is a complex number, and formula_13 denotes the complex conjugate of formula_12.\n\nMore concretely, the complex conjugate vector space is the same underlying \"real\" vector space (same set of points, same vector addition and real scalar multiplication) with the conjugate linear complex structure \"J\" (different multiplication by \"i\").\n\nIf formula_1 and formula_16 are complex vector spaces, a function formula_17 is antilinear if\nWith the use of the conjugate vector space formula_2, an antilinear map formula_20 can be regarded as an ordinary linear map of type formula_21. The linearity is checked by noting:\nConversely, any linear map defined on formula_2 gives rise to an antilinear map on formula_1.\n\nThis is the same underlying principle as in defining opposite ring so that a right formula_25-module can be regarded as a left formula_26-module, or that of an opposite category so that a contravariant functor formula_27 can be regarded as an ordinary functor of type formula_28.\n\nA linear map formula_29 gives rise to a corresponding linear map formula_30 which has the same action as formula_31. Note that formula_32 preserves scalar multiplication because\nThus, complex conjugation formula_34 and formula_35 define a functor from the category of complex vector spaces to itself.\n\nIf formula_1 and formula_16 are finite-dimensional and the map formula_38 is described by the complex matrix formula_39 with respect to the bases formula_40 of formula_1 and formula_42 of formula_16, then the map formula_32 is described by the complex conjugate of formula_39 with respect to the bases formula_46 of formula_2 and formula_48 of formula_49.\n\nThe vector spaces formula_1 and formula_2 have the same dimension over the complex numbers and are therefore isomorphic as complex vector spaces. However, there is no natural isomorphism from formula_1 to formula_2.\n\nThe double conjugate formula_54 is identical to formula_3.\n\nGiven a Hilbert space formula_56 (either finite or infinite dimensional), its complex conjugate formula_57 is the same vector space as its continuous dual space formula_58.\nThere is one-to-one antilinear correspondence between continuous linear functionals and vectors.\nIn other words, any continuous linear functional on formula_56 is an inner multiplication to some fixed vector, and vice versa.\n\nThus, the complex conjugate to a vector formula_60, particularly in finite dimension case, may be denoted as formula_61 (v-star, a row vector which is the conjugate transpose to a column vector formula_60).\nIn quantum mechanics, the conjugate to a \"ket vector\" formula_63 is denoted as formula_64 – a \"bra vector\" (see bra–ket notation).\n\n\n"}
{"id": "21205307", "url": "https://en.wikipedia.org/wiki?curid=21205307", "title": "Component (UML)", "text": "Component (UML)\n\nA component in the Unified Modeling Language \"represents a modular part of a system, that encapsulates its content and whose manifestation is replaceable within its environment. A component defines its behavior in terms of \"provided\" and \"required\" interfaces\".\n\nA component may be replaced by another if and only if their provided and required interfaces are identical. This idea is the underpinning for the plug-and-play capability of component-based systems and promotes software reuse.\n\nAs can be seen from the above definition, UML places no restriction on the granularity of a component. Thus, a component may be as small as a \"figures-to-words converter\", or as large as an entire \"document management system\".\n\nLarger pieces of a system's functionality may be assembled by reusing components as parts in an encompassing component or assembly of components, and wiring together their required and provided interfaces.\" Such assemblies are illustrated by means of component diagrams.\n\n\n"}
{"id": "12527335", "url": "https://en.wikipedia.org/wiki?curid=12527335", "title": "Cosmic time", "text": "Cosmic time\n\nCosmic time (also known as time since the big bang) is the time coordinate commonly used in the Big Bang models of physical cosmology. It is defined for homogeneous, expanding universes as follows: Choose a time coordinate so that the universe has the same density everywhere at each moment in time (the fact that this is possible means that the universe is, by definition, homogeneous). Measure the passage of time using clocks moving with the Hubble flow. Choose the big bang singularity as the origin of the time coordinate. \n\nCosmic time formula_1 is a measure of time by a physical clock with zero peculiar velocity in the absence of matter over-/under-densities (to prevent time dilation due to relativistic effects or confusions caused by expansion of the universe). Unlike other measures of time such as temperature, redshift, particle horizon, or Hubble horizon, the cosmic time (similar and complementary to the comoving coordinates) is blind to the expansion of the universe. \n\nThere are two main ways for establishing a reference point for the cosmic time. The most trivial way is to take the present time as the cosmic reference point (sometimes referred to as the lookback time) or alternatively, take the Big Bang as formula_2 (also referred to as age of the universe). The big bang doesn't necessarily have to correspond to a physical event but rather it refers to the point at which the scale factor would vanish for a standard cosmological model such as ΛCDM. For instance, in the case of inflation, i.e. a non-standard cosmology, the hypothetical moment of big bang is still determined using the benchmark cosmological models which may coincide with the end of the inflationary epoch. For inflationary models, it is not possible to establish a well defined origin of time before the big bang since the universe does not require a beginning event in such models. For technical purposes, concepts such as the average temperature of the universe (in units of eV) or the particle horizon are used when the early universe is the objective of a study since understanding the interaction among particles is more relevant than their time coordinate or age. \n\nCosmic time is the standard time coordinate for specifying the Friedmann–Lemaître–Robertson–Walker solutions of Einstein's equations.\n\n"}
{"id": "16175342", "url": "https://en.wikipedia.org/wiki?curid=16175342", "title": "De divina proportione", "text": "De divina proportione\n\nDe divina proportione (\"On the Divine Proportion\") is a book on mathematics written by Luca Pacioli and illustrated by Leonardo da Vinci, composed around 1498 in Milan and first printed in 1509. Its subject was mathematical proportions (the title refers to the golden ratio) and their applications to geometry, visual art through perspective, and architecture. The clarity of the written material and Leonardo's excellent diagrams helped the book to achieve an impact beyond mathematical circles, popularizing contemporary geometric concepts and images.\n\nThe book consists of three separate manuscripts, which Pacioli worked on between 1496 and 1498.\n\nThe first part, \"Compendio divina proportione\" (\"Compendium on the Divine Proportion\"), studies the golden ratio from a mathematical perspective (following the relevant work of Euclid) and explores its applications to various arts, in seventy-one chapters. It also contains a discourse on the regular and semiregular polyhedra, as well as a discussion of the use of geometric perspective by painters such as Piero della Francesca, Melozzo da Forlì and Marco Palmezzano.\n\nThe second part, \"Trattato dell'architettura\" (\"Treatise on Architecture\"), discusses the ideas of Vitruvius (from his \"De architectura\") on the application of mathematics to architecture in twenty chapters. The text compares the proportions of the human body to those of artificial structures, with examples from classical Greco-Roman architecture.\n\nThe third part, \"Libellus in tres partiales divisus\" (\"Book divided into three parts\"), is mainly an Italian translation of Piero della Francesca's Latin writings \"On [the] Five Regular Solids\" (\"De quinque corporibus regularibus\") and mathematical examples. In 1550 Giorgio Vasari wrote a biography of della Francesca, in which he accused Pacioli of plagiarism and claimed that he stole della Francesca's work on perspective, on arithmetic and on geometry.\n\nAfter these three parts are appended two sections of illustrations, the first showing twenty-three capital letters drawn with a ruler and compass by Pacioli and the second with some sixty illustrations in woodcut after drawings by Leonardo da Vinci. Leonardo drew the illustrations of the regular solids while he lived with and took mathematics lessons from Pacioli. Leonardo's drawings are probably the first illustrations of skeletonic solids which allowed an easy distinction between front and back.\n\nPacioli produced three manuscripts of the treatise by different scribes. He gave the first copy with a dedication to the Duke of Milan, Ludovico il Moro; this manuscript is now preserved in Switzerland at the Bibliothèque de Genève in Geneva. A second copy was donated to Galeazzo da Sanseverino and now rests at the Biblioteca Ambrosiana in Milan. The third, which has gone missing, was given to Pier Soderini, the Gonfaloniere of Florence. On 1 June 1509 the first printed edition was published in Venice by Paganino Paganini; it has since been reprinted several times.\nThe book was displayed as part of an exhibition in Milan between October 2005 and October 2006 together with the Codex Atlanticus. The \"M\" logo used by the Metropolitan Museum of Art in New York is adapted from one in \"De divina proportione\".\n\n\n"}
{"id": "753145", "url": "https://en.wikipedia.org/wiki?curid=753145", "title": "Del in cylindrical and spherical coordinates", "text": "Del in cylindrical and spherical coordinates\n\nThis is a list of some vector calculus formulae for working with common curvilinear coordinate systems.\n\n\nformula_12\nformula_13\n\nThe expressions for formula_14 and formula_15 are found in the same way.\n\nformula_16\n\nformula_17\n\nformula_18\n\nformula_19\n\nformula_20\n\nformula_21\n\nformula_22\n\nformula_23\n\nformula_24\n\nformula_25\n\nThe unit vector of a coordinate parameter u is defined in such a way that a small positive change in u causes the position vector formula_26 to change in formula_27 direction.\n\nTherefore, formula_28 where s is the arc length parameter.\n\nFor two sets of coordinate systems formula_29 and formula_30, according to chain rule, formula_31\n\nNow, let all of formula_32 but one and then divide both sides by the corresponding differential of that coordinate parameter, we find:\n\nformula_33\n\n\n"}
{"id": "58683384", "url": "https://en.wikipedia.org/wiki?curid=58683384", "title": "Diagrammatic Monte Carlo", "text": "Diagrammatic Monte Carlo\n\nIn mathematical physics, the diagrammatic Monte Carlo method is based on stochastic summation of Feynman diagrams with controllable error bars. It was developed by Boris Svistunov and Nikolay Prokof'ev. It was proposed as a generic approach to overcome the numerical sign problem that precludes simulations of many-body fermionic problems.\n"}
{"id": "37279084", "url": "https://en.wikipedia.org/wiki?curid=37279084", "title": "Element (UML)", "text": "Element (UML)\n\nIn the Unified Modeling Language (UML), an Element is an abstract class with no superclass. It is used as the superclass or base class, as known by object oriented programmers, for all the metaclasses in the UML infrastructure library. All other elements in the UML inherit, directly or indirectly from Element. An Element has a derived composition association to itself to support the general capability for elements to own other elements. As such, it has no additional attributes as part of its specification.\n\nAn association describes a set of tuples of typed instances. \n\nThe Element class belongs to the base package in the UML called the Kernel. This is the package that contains the superclasses that make up the superstructure of the UML.\n\nSubclasses of Element provide semantics appropriate to the concept they represent. The comments for an Element add no semantics but may represent information useful to the reader of the model.\n\nUpdated for the UML version 2.4.1\n\n"}
{"id": "18617907", "url": "https://en.wikipedia.org/wiki?curid=18617907", "title": "Essential manifold", "text": "Essential manifold\n\nEssential manifold a special type of closed manifolds. \nThe notion was first introduced explicitly by Mikhail Gromov.\n\nA closed manifold \"M\" is called essential if its fundamental class [\"M\"] defines a nonzero element in the homology of its fundamental group \"π\", or more precisely in the homology of the corresponding Eilenberg–MacLane space \"K\"(\"π\", 1), via the natural homomorphism\nwhere \"n\" is the dimension of \"M\". Here the fundamental class is taken in homology with integer coefficients if the manifold is orientable, and in coefficients modulo 2, otherwise.\n\n\n\n"}
{"id": "16465515", "url": "https://en.wikipedia.org/wiki?curid=16465515", "title": "Flat spline", "text": "Flat spline\n\nA spline, or the more modern term flexible curve, consists of a long strip fixed in position at a number of points that relaxes to form and hold a smooth curve passing through those points for the purpose of transferring that curve to another material.\n\nBefore computers were used for creating engineering designs, drafting tools were employed by designers drawing by hand. To draw curves, especially for shipbuilding, draftsmen often used long, thin, flexible strips of wood, plastic, or metal called splines (or laths, not to be confused with lathes). The splines were held in place with lead weights (called ducks because of their duck-like shape). The elasticity of the spline material combined with the constraint of the control points, or knots, would cause the strip to take the shape that minimized the energy required for bending it between the fixed points, this being the smoothest possible shape.\nSplines are more recently referred to as flexible curves and perform much of the original function. The main difference between splines and flexible curves is that the control points of flexible curves are entirely internal in their housing. This has one advantage over splines: whereas the draftsman had to first set up the spline and control points before moving the object to be marked to the spline in that order only, with flexible curves the draftsman can also set up the flexible curve before carefully moving the flexible curve to the object to be marked.\n\nOne can recreate an original draftsman's spline device with weights and a length of thin stiff plastic or rubber tubing. The weights are attached to the tube (by gluing or pinning). The tubing is then placed over drawing paper. Crosses are marked on the paper to designate the knots or control points. The tube is then adjusted so that it passes over the control points.\n\nIn 1946, mathematicians started studying the spline shape and derived the piecewise polynomial formula known as the spline curve, or spline function. This has led to the widespread use of such functions in computer-aided design, especially in the surface designs of vehicles. I. J. Schoenberg gave the spline function its name after its resemblance to the mechanical spline used by draftsmen.\n\nThe origins of the spline in wood-working may show in the conjectured etymology, which connects the word \"spline\" to the word \"splinter\". Later craftsmen have made splines out of rubber, steel, and other elastomeric materials.\n\nSpline devices help bend the wood for pianos, violins, violas, etc. The Wright brothers used one to shape the wings of their aircraft.\n\n"}
{"id": "9597906", "url": "https://en.wikipedia.org/wiki?curid=9597906", "title": "Franco P. Preparata", "text": "Franco P. Preparata\n\nFranco P. Preparata is a computer scientist, the An Wang Professor, Emeritus, of Computer Science at Brown University.\n\nHe is best known for his 1985 book \"Computational Geometry: An Introduction\" into which he blended salient parts of M. I. Shamos' doctoral thesis (Shamos appears as a co-author of the book). This book, which represents a snapshot of the disciplines as of 1985, has been for many years the standard textbook in the field, and has been translated into four foreign Languages (Russian, Japanese, Chinese, and Polish). He has made several contributions to the computational geometry, the most recent being the notion of \"algorithmic degree\" as a key feature to control robust implementations of geometric algorithms.\n\nIn addition, Preparata has worked in many other areas of, or closely related to, computer science.\n\nHis initial work was in coding theory, where he (independently and simultaneously) contributed the Berlekamp-Preparata codes (optimal convolution codes for burst-error correction) and the Preparata codes, the first known systematic class of nonlinear binary codes, with higher information content than corresponding linear BCH codes of the same length. Thirty years later these codes have been found relevant to quantum coding theory.\n\nIn 1967, he substantially contributed to a model of system-level fault diagnosis, known today as the PMC (Preparata-Metze-Chien) model, which is a main issue in the design of highly dependable processing systems. This model is still the object of intense research today (as attested by the literature).\n\nOver the years, he was also active in research in parallel computation and VLSI theory. His 1979 paper (with J. Vuillemin), still highly cited, presented the cube-connected-cycles (CCC), a parallel architecture that optimally emulates the hypercube interconnection. This interconnection was closely reflected in the architecture of the CM2 of Thinking Machines Inc., the first massive-parallel system in the VLSI era. His 1991 paper with Zhou and Kang on interconnection delays in VLSI was awarded the 1993 \"Darlington Best Paper Award\" by the IEEE Circuits and Systems Society. In the late nineties, (in joint work with G. Bilardi) he confronted the problem of the physical limitations (space and speed) of parallel computation, and formulated the conclusion that mesh connections are ultimately the only scalable massively parallel architectures.\n\nMore recently the focus of his research has been Computational Biology. Among other results, he contributed (with Eli Upfal) a novel approach to DNA Sequencing by Hybridization, achieving sequencing lengths that are the square of what was previously known, which has attracted media coverage.\n\nThe unifying character of these results in diverse research areas is the methodological approach, based on the construction of precise mathematical models and the use of sophisticated mathematical techniques.\n\nPreparata was born in Italy in December, 1935. He received a doctorate from the University of Rome, Italy in 1959. After a postdoctorate at CNR and several years of working in industry, he joined the faculty of the University of Illinois at Urbana-Champaign in 1965, where he achieved the rank of Professor in 1970. He stayed at the UIUC for many years, advising 16 Ph.D. students there. He received his Italian Libera Docenza in 1969. In 1991, Preparata moved from Illinois to Brown University where he has remained active in research, teaching, and student advising until his retirement at the end of 2013. He is the author (or co-author) of three books and nearly 250 articles. In 1997, the University of Padova awarded Preparata an honorary doctorate in Information Engineering. Preparata is an IEEE Fellow (1978),an ACM Fellow (1993), and was a Fellow of the Japan Society for the Advancement of Science.\n\n\n\n"}
{"id": "3862616", "url": "https://en.wikipedia.org/wiki?curid=3862616", "title": "French mathematical seminars", "text": "French mathematical seminars\n\nFrench mathematical seminars have been an important type of institution combining research and exposition, active since the beginning of the twentieth century. \n\nFrom 1909 to 1937, the Séminaire Hadamard has gathered many participants (f. i. André Weil) around the presentation of international research papers and work in progress. The Séminaire Julia focussed on yearly themes and impulsed the Bourbaki movement. The Séminaire Nicolas Bourbaki is the most famous, but is atypical in a number of ways: it attempts to cover, if selectively, the whole of pure mathematics, and its talks are now, by convention, reports and surveys on research by someone not directly involved. More standard is a working group organised around a specialist area, with research talks given and written up 'from the horse's mouth'.\n\nHistorically speaking, the Séminaire Cartan of the late 1940s and early 1950s, around Henri Cartan, was one of the most influential. Publication in those days was by means of the duplicated \"exemplaire\" (limited distribution and not peer-reviewed). The seminar model was tested, almost to destruction, by the SGA series of Alexander Grothendieck.\n\n\n"}
{"id": "48440954", "url": "https://en.wikipedia.org/wiki?curid=48440954", "title": "Gauss Lectureship", "text": "Gauss Lectureship\n\nThe Gauss Lectureship (\"Gauß-Vorlesung\") is an annually awarded mathematical distinction, named in honor of Carl Friedrich Gauss. It was established in 2001 by the German Mathematical Society with a series of lectures for a broad audience.\n\nEach Gauss Lecture is paired with another presentation on the history of mathematics.\n\n"}
{"id": "6233334", "url": "https://en.wikipedia.org/wiki?curid=6233334", "title": "George Pólya Prize", "text": "George Pólya Prize\n\nThe George Pólya Prize is a prize in mathematics, awarded by the Society for Industrial and Applied Mathematics. First given in 1969, the prize is named after Hungarian mathematician George Pólya. It is now awarded in evenly numbered years. Starting in 1969 the prize money was provided by Frank Harary, who donated the profits from his Graph Theory book. At some point SIAM took it over. The first SIAM selection committee is listed as 2002.\n\nThe prize is given every two years, alternately in two categories: (1) for a notable application of combinatorial theory; (2) for a notable contribution in another area of interest to George Pólya such as approximation theory, complex analysis, number theory, orthogonal polynomials, probability theory, or mathematical discovery and learning.\n\nThe prize is broadly intended to recognize specific recent work. Prize committees may occasionally consider an award for cumulative work, but such awards should be rare.\n\n"}
{"id": "13552465", "url": "https://en.wikipedia.org/wiki?curid=13552465", "title": "Hereditary property", "text": "Hereditary property\n\nIn mathematics, a hereditary property is a property of an object, that inherits to all its \"subobjects\", where the term subobject depends on the context. These properties are particularly considered in topology and graph theory, but also in set theory.\n\nIn topology, a topological property is said to be \"hereditary\" if whenever a topological space has that property, then so does every subspace of it. If the latter is true only for closed subspaces, then the property is called \"weakly hereditary\" or\n\"closed-hereditary\".\n\nFor example, second countability and metrisability are hereditary properties. Sequentiality and Hausdorff compactness are weakly hereditary, but not hereditary. Connectivity is not weakly hereditary.\n\nIf \"P\" is a property of a topological space \"X\" and every subspace also has property \"P\", then \"X\" is said to be \"hereditarily \"P\"\".\n\nIn graph theory, a \"hereditary property\" is a property of a graph which also holds for (is \"inherited\" by) its induced subgraphs. Alternately, a hereditary property is preserved by the removal of vertices. A graph class formula_1 is said hereditary if it is closed under induced subgraphs. Examples of hereditary graph classes are independent graphs (graphs with no edges), which is a special case (with \"c\" = 1) of being \"c\"-colorable for some number \"c\", being forests, planar, complete, complete multipartite etc.\n\nIn some cases, the term \"hereditary\" has been defined with reference to graph minors, but this is more properly called a minor-hereditary property. The Robertson–Seymour theorem implies that a minor-hereditary property may be characterized in terms of a finite set of forbidden minors.\n\nThe term \"hereditary\" has been also used for graph properties that are closed with respect to taking subgraphs. In such a case, properties, that are closed with respect to taking induced subgraphs, are called induced-hereditary. This approach is used by the members of the scientific society Hereditarnia Club. The language of hereditary properties and induced-hereditary properties provides a powerful tool for study of structural properties of various types of generalized colourings. The most important result from this area is the Unique Factorisation Theorem. \nThere is no consensus for the meaning of \"monotone property\" in graph theory. Examples of definitions are:\n\nThe complementary property of a property that is preserved by the removal of edges is preserved under the addition of edges. Hence some authors avoid this ambiguity by saying a property A is monotone if A or A (the complement of A) is monotone. Some authors choose to resolve this by using the term \"increasing\" monotone for properties preserved under the addition of some object, and \"decreasing\" monotone for those preserved under the removal of the same object.\n\nIn planning and problem solving, or more formally one-person games, the search space is seen as a directed graph with \"states\" as nodes, and \"transitions\" as edges. States can have properties, and such a property P is hereditary if \"for each state S that has P, each state that can be reached from S also has P\".\n\nThe subset of all states that have P plus the subset of all states that have ~P form a partition of the set of states called a hereditary partition. This notion can trivially be extended to more discriminating partitions by instead of properties, considering \"aspects\" of states and their domains. If states have an aspect A, with d ⊂ D a partition of the domain D of A, then the subsets of states for which A∈d form a hereditary partition of the total set of states.\n\nIf the current state and the goal state are in different elements of a hereditary partition, there is no path from the current state to the goal state — the problem has no solution.\n\nCan a checkers board be covered with domino tiles, each of which covers exactly two adjacent fields? Yes. What if we remove the top left and the bottom right field? Then no covering is possible any more, because the difference between number of uncovered white fields and the number of uncovered black fields is 2, and adding a domino tile (which covers one white and one black field) keeps that number at 2. For a total covering the number is 0, so a total covering cannot be reached from the start position.\n\nThis notion was first introduced by Laurent Siklóssy and Roach.\n\nIn model theory and universal algebra, a class \"K\" of structures of a given signature is said to have the \"hereditary property\" if every substructure of a structure in \"K\" is again in \"K\". A variant of this definition is used in connection with Fraïssé's theorem: A class \"K\" of finitely generated structures has the \"hereditary property\" if every finitely generated substructure is again in \"K\". See age.\n\nIn a matroid, every subset of an independent set is again independent. This is also sometimes called the \"hereditary property.\"\n\nRecursive definitions using the adjective \"hereditary\" are often encountered in set theory.\n\nA set is said to be hereditary (or \"pure\") if all of its elements are hereditary sets. It is vacuously true that the empty set is a hereditary set, and thus the set formula_2 containing only the empty set formula_3 is a hereditary set, and recursively so is formula_4, for example. In formulations of set theory that are intended to be interpreted in the von Neumann universe or to express the content of Zermelo–Fraenkel set theory, all sets are hereditary, because the only sort of object that is even a candidate to be an element of a set is another set. Thus the notion of hereditary set is interesting only in a context in which there may be urelements.\n\nA couple of notions are defined analogously:\n\nBased on the above, it follows that in ZFC a more general notion can be defined for any predicate formula_5. A set \"x\" is said to have \"hereditarily\" the property formula_5 if \"x\" itself and all members of its transitive closure satisfy formula_7, i.e. formula_8. Equivalently, \"x\" hereditarily satisfies formula_5 iff it is a member of a transitive subset of formula_10. A property (of a set) is thus said to be hereditary if is inherited by every subset. For example, being well-ordered is a hereditary property, and so it being finite.\n\nIf we instantiate in the above schema formula_5 with \"\"x\" has cardinality less than κ\", we obtain the more general notion of a set being \"hereditarily of cardinality less than κ\", usually denoted by formula_12 or formula_13. We regain the two simple notions we introduced above as formula_14 being the set of hereditarily finite sets and formula_15 being the set of hereditarily countable sets. (formula_16 is the first uncountable ordinal.)\n"}
{"id": "3570709", "url": "https://en.wikipedia.org/wiki?curid=3570709", "title": "Hinge theorem", "text": "Hinge theorem\n\nIn geometry, the hinge theorem states that if two sides of one triangle are congruent to two sides of another triangle, and the included angle of the first is larger than the included angle of the second, then the third side of the first triangle is longer than the third side of the second triangle. This theorem is actually Propositions 24 of Book 1 of Euclid's Elements (sometimes called the open mouth theorem). The theorem states the following:\n\nThe hinge theorem holds in Euclidean spaces and more generally in simply connected non-positively curved space forms.\n\nIt can be also extended from plane Euclidean geometry to higher dimension Euclidean spaces (e.g., to tetrahedra and more generally to simplices), as has been done for orthocentric tetrahedra (i.e., tetrahedra in which altitudes are concurrent) and more generally for orthocentric simplices (i.e., simplices in which altitudes are concurrent).\n\nThe converse of the hinge theorem is also true: If the two sides of one triangle are congruent to two sides of another triangle, and the third side of the first triangle is greater than the third side of the second triangle, then the included angle of the first triangle is larger than the included angle of the second triangle. \n\nIn some textbooks, the theorem and its converse are written as the SAS Inequality Theorem and the SSS Inequality Theorem respectively.\n"}
{"id": "49729992", "url": "https://en.wikipedia.org/wiki?curid=49729992", "title": "Hybrid argument (Cryptography)", "text": "Hybrid argument (Cryptography)\n\nIn cryptography, the hybrid argument is a proof technique used to show that two distributions are computationally indistinguishable.\n\nFormally, to show two distributions \"D\" and \"D\" are computationally indistinguishable, we can define a sequence of \"hybrid distributions\" \"D\" := \"H\", \"H\", ..., \"H\" =: \"D\" where \"t\" is polynomial in the security parameter. Define the advantage of any probabilistic efficient (polynomial-bounded time) algorithm A as\n\nwhere the dollar symbol ($) denotes that we sample an element from the distribution at random.\n\nBy triangle inequality, it is clear that for any probabilistic polynomial time algorithm A,\n\nThus there must exist some \"k\" s.t. 0 ≤ \"k\" < \"t\" and\n\nSince \"t\" is polynomial-bounded, for any such algorithm A, if we can show that its advantage to distinguish the distributions \"H\" and \"H\" is negligible for every \"i\", then it immediately follows that its advantage to distinguish the distributions \"D\" = \"H\" and \"D\" = \"H\" must also be negligible. This fact gives rise to the hybrid argument: it suffices to find such a sequence of hybrid distributions and show each pair of them is computationally indistinguishable.\n\nThe hybrid argument is extensively used in cryptography. Some simple proofs using hybrid arguments are:\n\n"}
{"id": "11017352", "url": "https://en.wikipedia.org/wiki?curid=11017352", "title": "Hypertranscendental function", "text": "Hypertranscendental function\n\nA hypertranscendental function or transcendentally transcendental function is an analytic function which is not the solution of an algebraic differential equation with coefficients in Z (the integers) and with algebraic initial conditions. All hypertranscendental functions are transcendental functions.\n\nThe term 'transcendentally transcendental' was introduced by E. H. Moore in 1896; the term 'hypertranscendental' was introduced by D. D. Morduhai-Boltovskoi in 1914.\n\nOne standard definition (there are slight variants) defines solutions of differential equations of the form\nwhere formula_2 is a polynomial with constant coefficients, as \"algebraically transcendental\" or \"differentially algebraic\". Transcendental functions which are not \"algebraically transcendental\" are \"transcendentally transcendental\". Hölder's theorem shows that the gamma function is in this category.\n\nHypertranscendental functions usually arise as the solutions to functional equations, for example the gamma function.\n\n\n\n\n\n"}
{"id": "38714058", "url": "https://en.wikipedia.org/wiki?curid=38714058", "title": "International Conference on Theory and Applications of Models of Computation", "text": "International Conference on Theory and Applications of Models of Computation\n\nThe International Conference on Theory and Applications of Models of Computation (TAMC) is an academic conference in the field of theoretical computer science. TAMC has been organized annually since 2004. Previous editors of the TAMC conference proceedings include Manindra Agrawal and Petr Kolman. The conference proceedings are published in the \"Lecture Notes in Computer Science\" (LNCS) series by Springer.\n\n"}
{"id": "7635201", "url": "https://en.wikipedia.org/wiki?curid=7635201", "title": "Invariant measure", "text": "Invariant measure\n\nIn mathematics, an invariant measure is a measure that is preserved by some function. Ergodic theory is the study of invariant measures in dynamical systems. The Krylov–Bogolyubov theorem proves the existence of invariant measures under certain conditions on the function and space under consideration.\n\nLet (\"X\", Σ) be a measurable space and let \"f\" be a measurable function from \"X\" to itself. A measure \"μ\" on (\"X\", Σ) is said to be invariant under \"f\" if, for every measurable set \"A\" in Σ,\n\nIn terms of the push forward, this states that \"f\"(\"μ\") = \"μ\".\n\nThe collection of measures (usually probability measures) on \"X\" that are invariant under \"f\" is sometimes denoted \"M\"(\"X\"). The collection of ergodic measures, \"E\"(\"X\"), is a subset of \"M\"(\"X\"). Moreover, any convex combination of two invariant measures is also invariant, so \"M\"(\"X\") is a convex set; \"E\"(\"X\") consists precisely of the extreme points of \"M\"(\"X\").\n\nIn the case of a dynamical system (\"X\", \"T\", \"φ\"), where (\"X\", Σ) is a measurable space as before, \"T\" is a monoid and \"φ\" : \"T\" × \"X\" → \"X\" is the flow map, a measure \"μ\" on (\"X\", Σ) is said to be an invariant measure if it is an invariant measure for each map \"φ\" : \"X\" → \"X\". Explicitly, \"μ\" is invariant if and only if\n\nPut another way, \"μ\" is an invariant measure for a sequence of random variables (\"Z\") (perhaps a Markov chain or the solution to a stochastic differential equation) if, whenever the initial condition \"Z\" is distributed according to \"μ\", so is \"Z\" for any later time \"t\".\n\nWhen the dynamical system can be described by a transfer operator, then the invariant measure is an eigenvector of the operator, corresponding to an eigenvalue of 1, this being the largest eigenvalue as given by the Frobenius-Perron theorem.\n\n\n\n\n\n"}
{"id": "35784363", "url": "https://en.wikipedia.org/wiki?curid=35784363", "title": "Jeans's theorem", "text": "Jeans's theorem\n\nIn astrophysics and statistical mechanics, Jeans's theorem, named after James Jeans, states that any steady-state solution of the collisionless Boltzmann equation depends on the phase space coordinates only through integrals of motion in the given potential, and conversely any function of the integrals is a steady-state solution.\n\nJeans's theorem is most often discussed in the context of potentials characterized by three, global integrals. In such potentials, all of the orbits are regular, i.e. non-chaotic; the Kepler potential is one example. In generic potentials, some orbits respect only one or two integrals and the corresponding motion is chaotic. Jeans's theorem can be generalized to such potentials as follows:\n\nThe phase-space density of a stationary stellar system is constant within every well-connected region.\n\nA well-connected region is one that cannot be decomposed into two finite regions such that all trajectories lie, for all time, in either one or the other. Invariant tori of regular orbits are such regions, but so are the more complex parts of phase space associated with chaotic trajectories. Integrability of the motion is therefore not required for a steady state.\n"}
{"id": "3099132", "url": "https://en.wikipedia.org/wiki?curid=3099132", "title": "Kerala School of Astronomy and Mathematics", "text": "Kerala School of Astronomy and Mathematics\n\nThe Kerala School of Astronomy and Mathematics was a school of mathematics and astronomy founded by Madhava of Sangamagrama in Kerala, India, which included among its members: Parameshvara, Neelakanta Somayaji, Jyeshtadeva, Achyuta Pisharati, Melpathur Narayana Bhattathiri and Achyuta Panikkar. The school flourished between the 14th and 16th centuries and the original discoveries of the school seems to have ended with Narayana Bhattathiri (1559–1632). In attempting to solve astronomical problems, the Kerala school independently created a number of important mathematics concepts. Their most important results—series expansion for trigonometric functions—were described in Sanskrit verse in a book by Neelakanta called \"Tantrasangraha\", and again in a commentary on this work, called \"Tantrasangraha-vakhya\", of unknown authorship. The theorems were stated without proof, but proofs for the series for sine, cosine, and inverse tangent were provided a century later in the work \"Yuktibhasa\" (), written in Malayalam, by Jyesthadeva, and also in a commentary on \"Tantrasangraha\".\n\nTheir work, completed two centuries before the invention of calculus in Europe, provided what is now considered the first example of a power series (apart from geometric series). However, they did not formulate a systematic theory of differentiation and integration, nor is there any direct evidence of their results being transmitted outside Kerala.\n\nThe Kerala school has made a number of contributions to the fields of infinite series and calculus. These include the following (infinite) geometric series:\n\nThe Kerala school made intuitive use of mathematical induction, though the inductive hypothesis was not yet formulated or employed in proofs. They used this to discover a semi-rigorous proof of the result:\n\nfor large \"n\". \n\nThey applied ideas from (what was to become) differential and integral calculus to obtain (Taylor–Maclaurin) infinite series for formula_1, formula_2, and formula_3. The \"Tantrasangraha-vakhya\" gives the series in verse, which when translated to mathematical notation, can be written as:\n\nwhere, for formula_4 the series reduce to the standard power series for these trigonometric functions, for example:\n\nThe Kerala school made use of the rectification (computation of length) of the arc of a circle to give a proof of these results. (The later method of Leibniz, using quadrature (\"i.e.\" computation of area under the arc of the circle), was not yet developed.) They also made use of the series expansion of formula_5 to obtain an infinite series expression (later known as Gregory series) for formula_6:\nTheir rational approximation of the \"error\" for the finite sum of their series are of particular interest. For example, the error, formula_7, (for \"n\" odd, and \"i = 1, 2, 3\") for the series:\n\nThey manipulated the terms, using the partial fraction expansion of :formula_8 to obtain a more rapidly converging series for formula_6:\n\nThey used the improved series to derive a rational expression, formula_10 for formula_6 correct up to nine decimal places, i.e. formula_12. They made use of an intuitive notion of a limit to compute these results. The Kerala school mathematicians also gave a semi-rigorous method of differentiation of some trigonometric functions, though the notion of a function, or of exponential or logarithmic functions, was not yet formulated.\n\nIn 1825 John Warren published a memoir on the division of time in southern India, called the \"Kala Sankalita\", which briefly mentions the discovery of infinite series by Kerala astronomers. \n\nThe works of the Kerala school were first written up for the Western world by Englishman C. M. Whish in 1835. According to Whish, the Kerala mathematicians had \"laid the foundation for a complete system of fluxions\" and these works abounded \"with fluxional forms and series to be found in no work of foreign countries\". However, Whish's results were almost completely neglected, until over a century later, when the discoveries of the Kerala school were investigated again by C. T. Rajagopal and his associates. Their work includes commentaries on the proofs of the arctan series in \"Yuktibhasa\" given in two papers, a commentary on the \"Yuktibhasa\"s proof of the sine and cosine series and two papers that provide the Sanskrit verses of the \"Tantrasangrahavakhya\" for the series for arctan, sin, and cosine (with English translation and commentary).\n\nIn 1952 Otto Neugebauer wrote on Tamil astronomy. \n\nIn 1972 K. V. Sarma published his A History of the Kerala School of Hindu Astronomy which described features of the School such as the continuity of knowledge transmission from the 13th to the 17th century: Govinda Bhattathiri to Parameshvara to Damodara to Nilakantha Somayaji to Jyesthadeva to Acyuta Pisarati. Transmission from teacher to pupil conserved knowledge in \"a practical, demonstrative discipline like astronomy at a time when there was not a proliferation of printed books and public schools.\"\n\nIn 1994 it was argued that the heliocentric model had been adopted about 1500 A.D. in Kerala.\n\nA. K. Bag suggested in 1979 that knowledge of these results might have been transmitted to Europe through the trade route from Kerala by traders and Jesuit missionaries. Kerala was in continuous contact with China and Arabia, and Europe. The suggestion of some communication routes and a chronology by some scholars could make such a transmission a possibility; however, there is no direct evidence by way of relevant manuscripts that such a transmission took place. According to David Bressoud, \"there is no evidence that the Indian work of series was known beyond India, or even outside of Kerala, until the nineteenth century\".\n\nBoth Arab and Indian scholars made discoveries before the 17th century that are now considered a part of calculus. According to V.J. Katz, they were yet to \"combine many differing ideas under the two unifying themes of the derivative and the integral, show the connection between the two, and turn calculus into the great problem-solving tool we have today\", like Newton and Leibniz. The intellectual careers of both Newton and Leibniz are well-documented and there is no indication of their work not being their own; however, it is not known with certainty whether the immediate \"predecessors\" of Newton and Leibniz, \"including, in particular, Fermat and Roberval, learned of some of the ideas of the Islamic and Indian mathematicians through sources of which we are not now aware\". This is an active area of current research, especially in the manuscript collections of Spain and Maghreb, research that is now being pursued, among other places, at the Centre national de la recherche scientifique in Paris.\n\n\n</div>\n\n"}
{"id": "3815666", "url": "https://en.wikipedia.org/wiki?curid=3815666", "title": "Lattice model (finance)", "text": "Lattice model (finance)\n\nIn finance, a lattice model is a technique applied to the valuation of derivatives, where a discrete time model is required. For equity options, a typical example would be pricing an American option, where a decision as to option exercise is required at \"all\" times (any time) before and including maturity. A continuous model, on the other hand, such as Black–Scholes, would only allow for the valuation of European options, where exercise is on the option's maturity date. For interest rate derivatives lattices are additionally useful in that they address many of the issues encountered with continuous models, such as pull to par. The method is also used for valuing certain exotic options, where because of path dependence in the payoff, Monte Carlo methods for option pricing fail to account for optimal decisions to terminate the derivative by early exercise.\n\nIn general the approach is to divide time between now and the option's expiration into \"N\" discrete periods. At the specific time \"n\", the model has a finite number of outcomes at time \"n\" + 1 such that every possible change in the state of the world between \"n\" and \"n\" + 1 is captured in a branch. This process is iterated until every possible path between \"n\" = 0 and \"n\" = \"N\" is mapped. Probabilities are then estimated for every \"n\" to \"n\" + 1 path. The outcomes and probabilities flow backwards through the tree until a fair value of the option today is calculated.\n\nFor equity and commodities the application is as follows. The first step is to trace the evolution of the option's key underlying variable(s), starting with today's spot price, such that this process is consistent with its volatility; log-normal Brownian motion with constant volatility is usually assumed. The next step is to value the option recursively, stepping backwards from the final time-step, and applying risk neutral valuation at each node, where option value is the probability-weighted present value of the up- and down-nodes in the later time-step. See for more detail, as well as for logic and formulae derivation.\n\nAs above, the lattice approach is particularly useful in valuing American options, where the choice whether to exercise the option early, or to hold the option, may be modeled at each discrete time/price combination; this is true also for Bermudan options. For similar reasons, real options and employee stock options are often modeled using a lattice framework, though with modified assumptions. In each of these cases, a third step is to determine whether the option is to be exercised or held, and to then apply this value at the node in question. Some exotic options, such as barrier options, are also easily modeled here; note though that for other Path-Dependent Options, simulation would be preferred. (Although, tree-based methods have been developed)\n\nThe simplest lattice model is the binomial options pricing model; the standard (\"canonical\") method is that proposed by Cox, Ross and Rubinstein (CRR) in 1979; see diagram for formulae. Over 20 other methods have been developed, with each \"derived under a variety of assumptions\" as regards the development of the underlying's price. In the limit, as the number of time-steps increases, these converge to the Log-normal distribution, and hence produce the \"same\" option price as Black-Scholes: to achieve this, these will variously seek to agree with the underlying's central moments, raw moments and / or log-moments at each time-step, as measured discretely. Further enhancements are designed to achieve stability relative to Black-Scholes as the number of time-steps changes. More recent models, in fact, are designed around direct convergence to Black-Scholes.\n\nA variant on the Binomial, is the Trinomial tree, developed by Phelim Boyle in 1986, where valuation is based on the value of the option at the up-, down- and middle-nodes in the later time-step. The chief conceptual difference here, being that the price may also remain unchanged over the time-step. As for the binomial, a similar (although smaller) range of methods exist. Note that the trinomial model is considered to produce more accurate results than the binomial model when fewer time steps are modelled, and is therefore used when computational speed or resources may be an issue. For vanilla options, as the number of steps increases, the results rapidly converge, and the binomial model is then preferred due to its simpler implementation. For exotic options the trinomial model (or adaptations) is sometimes more stable and accurate, regardless of step-size.\n\nVarious of the Greeks can be estimated directly on the lattice, where the sensitivities are calculated using finite differences. Delta and gamma, being sensitivities of option value w.r.t. price, are approximated given differences between option prices - with their related spot - in the same time step. Theta, sensitivity to time, is likewise estimated given the option price at the first node in the tree and the option price for the same spot in a later time step. (Second time step for trinomial, third for binomial. Note also, that depending on method, if the \"down factor\" is not the inverse of the \"up factor\", this method will not be precise.) For rho, sensitivity to interest rates, and vega, sensitivity to input volatility, the measurement is indirect, as the value must be calculated a second time on a new lattice built with these inputs slightly altered - and the sensitivity here is likewise returned via finite difference. See also Fugit - the estimated time to exercise - which is typically calculated using a lattice.\n\nWhen it is important to incorporate the volatility smile, or surface, implied trees can be constructed. Here, the tree is solved such that it successfully reproduces selected (all) market prices, across various strikes and expirations. These trees thus \"ensure that all European standard options (with strikes and maturities coinciding with the tree nodes) will have theoretical values which match their market prices\". Using the calibrated lattice one can then price options with strike / maturity combinations not quoted in the market, such that these prices are consistent with observed volatility patterns. There exist both implied binomial trees, often Rubinstein IBTs (R-IBT), and Implied trinomial trees, often Derman-Kani-Chriss (DKC; superseding the DK-IBT). The former is easier built, but is consistent with one maturity only; the latter will be consistent with, but at the same time requires, known (or interpolated) prices at all time-steps and nodes. (DKC is effectively a discretized local volatility model.)\n\nAs regards the construction, for an R-IBT the first step is to recover the \"Implied Ending Risk-Neutral Probabilities\" of spot prices. Then by the assumption that all paths which lead to the same ending node have the same risk-neutral probability, a \"path probability\" is attached to each ending node. Thereafter \"it's as simple as One-Two-Three\", and a three step backwards recursion allows for the node probabilities to be recovered for each time step. Option valuation then proceeds as standard, with these substituted for \"p\". For DKC, the first step is to recover the state prices corresponding to each node in the tree, such that these are consistent with observed option prices (i.e. with the volatility surface). Thereafter the up-, down- and middle-probabilities are found for each node such that: these sum to 1; spot prices adjacent time-step-wise evolve risk neutrally, incorporating dividend yield; state prices similarly \"grow\" at the risk free rate. (The solution here is iterative per time step as opposed to simultaneous.) As for R-IBTs, option valuation is then by standard backward recursion.\n\nAs an alternative, Edgeworth binomial trees allow for an analyst-specified skew and kurtosis in spot price returns; see Edgeworth series. This approach is useful when the underlying's behavior departs (markedly) from normality. A related use is to calibrate the tree to the volatility smile (or surface), by a \"judicious choice\" of parameter values—priced here, options with differing strikes will return differing implied volatilities. For pricing American options, an Edgeworth-generated ending distribution may be combined with an R-IBT. Note that this approach is limited as to the set of skewness and kurtosis pairs for which valid distributions are available. One recent proposal, Johnson binomial trees, is to use N. L. Johnson's system of distributions, as this is capable of accommodating all possible pairs; see Johnson SU distribution.\n\nFor multiple underlyers, multinomial lattices can be built, although the number of nodes increases exponentially with the number of underlyers. As an alternative, Basket options, for example, can be priced using an \"approximating distribution\" via an Edgeworth (or Johnson) tree.\n\nLattices are commonly used in valuing bond options, Swaptions, and other interest rate derivatives In these cases the valuation is largely as above, but requires an additional, zeroeth, step of constructing an interest rate tree, on which the price of the underlying is then based. Note that the next step also differs: the underlying' price here is built via \"backward induction\" i.e. flows backwards from maturity, accumulating the present value of scheduled cash flows at each node, as opposed to flowing forwards from valuation date as above. The final step, option valuation, then proceeds as standard. See aside.\n\nThe initial lattice is built by discretizing either a short-rate model, such as Hull-White or Black Derman Toy, or a forward rate-based model, such as the LIBOR market model or HJM. As for equity, trinomial trees may also be employed for these models; this is usually the case for Hull-White trees.\n\nUnder HJM, the condition of no arbitrage implies that there exists a martingale probability measure, as well as a corresponding restriction on the \"drift coefficients\" of the forward rates. These, in turn, are functions of the volatility(s) of the forward rates. A \"simple\" discretized expression for the drift then allows for forward rates to be expressed in a binomial lattice. Note that for these forward rate-based models, dependent on volatility assumptions, the lattice might not recombine. This means that an \"up-move\" followed by a \"down-move\" will not give the same result as a \"down-move\" followed by an \"up-move\". In this case, the Lattice is sometimes referred to as a \"bush\", and the number of nodes grows exponentially as a function of number of time-steps. A recombining binomial tree methodology is also available for the Libor Market Model.\n\nAs regards the short-rate models, these are, in turn, further categorized: these will be either equilibrium-based (Vasicek and CIR) or arbitrage-free (Ho–Lee and subsequent). This distinction means that for equilibrium-based models the yield curve is an \"output\" from the model, while for arbitrage-free models the yield curve is an \"input\" to the model. \nIn the former case, the approach is to \"calibrate\" the model parameters, such that bond prices produced by the model, in its continuous form, best fit observed market prices. The tree is then built as a function of these parameters.\nIn the latter case, the calibration is directly on the lattice: the fit is to both the current term structure of interest rates (i.e. the yield curve), and the corresponding volatility structure. Here, calibration means that the interest-rate-tree reproduces the prices of the zero-coupon bonds—and any other interest-rate sensitive securities—used in constructing the yield curve; note the parallel to implied trees above, and compare Bootstrapping (finance). For models assuming a normal distribution (such as Ho-Lee), calibration may be performed analytically, while for log-normal models the calibration is via a root-finding algorithm; see boxed-description under Black–Derman–Toy model.\n\nThe volatility structure—i.e. vertical node-spacing—here reflects the volatility of rates during the quarter, or other period, corresponding to the lattice time-step. (Some analysts use \"realized volatility\", i.e. of the rates applicable \"historically\" for the time-step; others prefer to use \"current\" interest rate cap prices, and the implied volatility for the Black-76-prices of each component caplet; see Interest rate cap#Implied Volatilities.) Given this functional link to volatility, note the resultant difference in the construction relative to implied trees above: here, the volatility is known for each time-step, and the node-values (i.e. interest rates) must be solved for specified risk neutral probabilities; for implied trees, on the other hand, a single volatility cannot be specified per time-step, i.e. we have a \"smile\", and the tree is built by solving for the probabilities corresponding to specified values of the underlying at each node.\n\nOnce calibrated, the interest rate lattice is then used in the valuation of various of the fixed income instruments and derivatives. The approach for bond options is described aside—note that this approach addresses the problem of pull to par experienced under closed form approaches; see . For swaptions the logic is almost identical, substituting swaps for bonds in step 1, and swaptions for bond options in step 2. For caps (and floors) step 1 and 2 are combined: at each node the value is based on the relevant nodes at the later step, plus, for any caplet (floorlet) maturing in the time-step, the difference between its reference-rate and the short-rate at the node (and reflecting the corresponding day count fraction and notional-value exchanged). For callable- and putable bonds a third step would be required: at each node in the time-step incorporate the effect of the embedded option on the bond price and / or the option price there before stepping-backwards one time-step. (And noting that these options are not mutually exclusive, and so a bond may have several options embedded; hybrid securities are treated below.) For other, more exotic interest rate derivatives, similar adjustments are made to steps 1 and onward. For the \"Greeks\" see under next section.\n\nAn alternative approach to modeling (American) bond options, particularly those struck on yield to maturity (YTM), employs modified equity-lattice methods. Here the analyst builds a CRR tree of YTM, applying a constant volatility assumption, and then calculates the bond price as a function of this yield at each node; prices here are thus pulling-to-par. The second step is to then incorporate any term structure of volatility by building a corresponding DKC tree (based on every second time-step in the CRR tree: as DKC is trinomial whereas CRR is binomial) and then using this for option valuation.\n\nSince the 2007–2012 global financial crisis, swap pricing is (generally) under a \"multi-curve\" framework, whereas previously it was off a single, \"self discounting\", curve; see . Here, payoffs are set as a function of LIBOR specific to the tenor in question, while discounting is at the OIS rate. To accommodate this in the lattice framework, the OIS rate and the relevant LIBOR rate are jointly modeled in a three-dimensional tree, constructed such that LIBOR swap rates are matched.\n\nHybrid securities, incorporating both equity- and bond-like features are also valued using trees.\n\nFor convertible bonds (CBs) the approach of Tsiveriotis and Fernandes (1998) is to divide the value of the bond at each node into an \"equity\" component, arising from situations where the CB will be converted, and a \"debt\" component, arising from situations where CB is redeemed. Correspondingly, twin trees are constructed where discounting is at the risk free and credit risk adjusted rate respectively, with the sum being the value of the CB. There are other methods, which similarly combine an equity-type tree with a short-rate tree. An alternate approach, originally published by Goldman Sachs (1994), does not decouple the components, rather, discounting is at a conversion-probability-weighted risk-free and risky interest rate within a single tree. See , Contingent convertible bond.\n\nAdditional to the calculation of \"Greeks\" as for equity, lattices can be used here to estimate sensitivities related to overall changes in interest rates. For a bond with an embedded option, the standard yield to maturity based calculations of duration and convexity do not consider how changes in interest rates will alter the cash flows due to option exercise. To address this, \"effective\" duration and -convexity are introduced. Here, similar to rho and vega above, the interest rate tree is rebuilt for an upward and then downward parallel shift in the yield curve and these measures are calculated numerically given the corresponding changes in bond value. Similar calculations can be applied to interest rate derivatives above, as regards Greeks and duration.\n\nMore generally, equity can be viewed as a call option on the firm: where the value of the firm is less than the value of the outstanding debt shareholders would choose not to repay the firm's debt; they would choose to repay—and not to liquidate (i.e. exercise their option)—otherwise. Lattice models have been developed for equity analysis here, particularly as relates to distressed firms. Relatedly, as regards corporate debt pricing, the relationship between equity holders' limited liability and potential Chapter 11 proceedings has also been modelled via lattice.\n\n"}
{"id": "36344163", "url": "https://en.wikipedia.org/wiki?curid=36344163", "title": "Linear arboricity", "text": "Linear arboricity\n\nIn graph theory, a branch of mathematics, the linear arboricity of an undirected graph is the smallest number of linear forests its edges can be partitioned into. Here, a linear forest is an acyclic graph with maximum degree two, i.e., a disjoint union of path graphs.\nThe linear arboricity of a graph formula_1 with maximum degree formula_2 is always at least formula_3, because each linear forest can use only two of the edges at a maximum-degree vertex. The linear arboricity conjecture of is that this lower bound is also tight: according to their conjecture, every graph has linear arboricity at most formula_4. However, this remains unproven, with the best proven upper bound on the linear arboricity being somewhat larger, \nformula_5 for some constant formula_6 due to Ferber, Fox and Jain.\n\nIn a regular graph, the linear arboricity cannot equal formula_7 because the endpoints of each path in one of the linear forests would not have two adjacent edges used by that forest. Therefore, for regular graphs, the linear arboricity conjecture implies that the linear arboricity is exactly formula_4.\n\nLinear arboricity is a variation of arboricity, the minimum number of forests that the edges of a graph can be partitioned into. Researchers have also studied linear -arboricity, a variant of linear arboricity in which each path in the linear forest can have at most edges.\n\nUnlike arboricity, which can be determined in polynomial time, linear arboricity is NP-hard. Even recognizing the graphs of linear arboricity two is NP-complete. However, for cubic graphs and other graphs of maximum degree three, the linear arboricity is always two, and a decomposition into two linear forests can be found in linear time using an algorithm based on depth-first search.\n"}
{"id": "6368430", "url": "https://en.wikipedia.org/wiki?curid=6368430", "title": "Linear programming relaxation", "text": "Linear programming relaxation\n\nIn mathematics, the relaxation of a (mixed) integer linear program is the problem that arises by removing the integrality constraint of each variable.\n\nFor example, in a 0-1 integer program, all constraints are of the form\n\nThe relaxation of the original integer program instead uses a collection of linear constraints\n\nThe resulting relaxation is a linear program, hence the name. This relaxation technique transforms an NP-hard optimization problem (integer programming) into a related problem that is solvable in polynomial time (linear programming); the solution to the relaxed linear program can be used to gain information about the solution to the original integer program.\n\nConsider the set cover problem, the linear programming relaxation of which was first considered by . In this problem, one is given as input a family of sets \"F\" = {\"S\", \"S\", ...}; the task is to find a subfamily, with as few sets as possible, having the same union as \"F\".\n\nTo formulate this as a 0-1 integer program, form an indicator variable \"x\" for each set \"S\", that takes the value 1 when \"S\" belongs to the chosen subfamily and 0 when it does not. Then a valid cover can be described by an assignment of values to the indicator variables satisfying the constraints\n(that is, only the specified indicator variable values are allowed) and, for each element \"e\" of the union of \"F\",\n(that is, each element is covered). The minimum set cover corresponds to the assignment of indicator variables satisfying these constraints and minimizing the linear objective function\nThe linear programming relaxation of the set cover problem describes a \"fractional cover\" in which the input sets are assigned weights such that the total weight of the sets containing each element is at least one and the total weight of all sets is minimized.\n\nAs a specific example of the set cover problem, consider the instance \"F\" = . There are three optimal set covers, each of which includes two of the three given sets. Thus, the optimal value of the objective function of the corresponding 0-1 integer program is 2, the number of sets in the optimal covers. However, there is a fractional solution in which each set is assigned the weight 1/2, and for which the total value of the objective function is 3/2. Thus, in this example, the linear programming relaxation has a value differing from that of the unrelaxed 0-1 integer program.\n\nThe linear programming relaxation of an integer program may be solved using any standard linear programming technique. If the optimal solution to the linear program happens to have all variables either 0 or 1, it will also be an optimal solution to the original integer program. However, this is generally not true, except for some special cases (e.g., \nproblems with totally unimodular matrix specifications.)\n\nIn all cases, though, the solution quality of the linear program is at least as good as that of the integer program, because any integer program solution would also be a valid linear program solution. That is, in a maximization problem, the relaxed program has a value greater than or equal to that of the original program, while in a minimization problem such as the set cover problem the relaxed program has a value smaller than or equal to that of the original program. Thus, the relaxation provides an optimistic bound on the integer program's solution.\n\nIn the example instance of the set cover problem described above, in which the relaxation has an optimal solution value of 3/2, we can deduce that the optimal solution value of the unrelaxed integer program is at least as large. Since the set cover problem has solution values that are integers (the numbers of sets chosen in the subfamily), the optimal solution quality must be at least as large as the next larger integer, 2. Thus, in this instance, despite having a different value from the unrelaxed problem, the linear programming relaxation gives us a tight lower bound on the solution quality of the original problem.\n\nLinear programming relaxation is a standard technique for designing approximation algorithms for hard optimization problems. In this application, an important concept is the integrality gap, the maximum ratio between the solution quality of the integer program and of its relaxation. In an instance of a minimization problem, if the real minimum (the minimum of the integer problem) is formula_6, and the relaxed minimum (the minimum of the linear programming relaxation) is formula_7, then the integrality gap of that instance is formula_8. In a maximization problem the fraction is reversed. The integrality gap is always at least 1. In the example above, the instance \"F\" = shows an integrality gap of 4/3.\n\nTypically, the integrality gap translates into the approximation ratio of an approximation algorithm. This is because an approximation algorithm relies on some rounding strategy that finds, for every relaxed solution of size formula_7, an integer solution of size at most formula_10 (where \"RR\" is the rounding ratio). If there is an instance with integrality gap \"IG\", then \"every\" rounding strategy will return, on that instance, a rounded solution of size at least formula_11. Therefore necessarily formula_12. The rounding ratio \"RR\" is only an upper bound on the approximation ratio, so in theory the actual approximation ratio may be lower than \"IG\", but this may be hard to prove. In practice, a large \"IG\" usually implies that the approximation ratio in the linear programming relaxation might be bad, and it may be better to look for other approximation schemes for that problem.\n\nFor the set cover problem, Lovász proved that the integrality gap for an instance with \"n\" elements is \"H\", the \"n\"th harmonic number. One can turn the linear programming relaxation for this problem into an approximate solution of the original unrelaxed set cover instance via the technique of randomized rounding . Given a fractional cover, in which each set \"S\" has weight \"w\", choose randomly the value of each 0-1 indicator variable \"x\" to be 1 with probability \"w\" ×(ln \"n\" +1), and 0 otherwise. Then any element \"e\" has probability less than 1/(\"e\"×\"n\") of remaining uncovered, so with constant probability all elements are covered. The cover generated by this technique has total size, with high probability, (1+o(1))(ln \"n\")\"W\", where \"W\" is the total weight of the fractional solution. Thus, this technique leads to a randomized approximation algorithm that finds a set cover within a logarithmic factor of the optimum. As showed, both the random part of this algorithm and the need to construct an explicit solution to the linear programming relaxation may be eliminated using the method of conditional probabilities, leading to a deterministic greedy algorithm for set cover, known already to Lovász, that repeatedly selects the set that covers the largest possible number of remaining uncovered elements. This greedy algorithm approximates the set cover to within the same \"H\" factor that Lovász proved as the integrality gap for set cover. There are strong complexity-theoretic reasons for believing that no polynomial time approximation algorithm can achieve a significantly better approximation ratio .\n\nSimilar randomized rounding techniques, and derandomized approximation algorithms, may be used in conjunction with linear programming relaxation to develop approximation algorithms for many other problems, as described by Raghavan, Tompson, and Young.\n\nAs well as its uses in approximation, linear programming plays an important role in branch and bound algorithms for computing the true optimum solution to hard optimization problems.\n\nIf some variables in the optimal solution have fractional values, we may start a branch and bound type process, in which we recursively solve subproblems in which some of the fractional variables have their values fixed to either zero or one. In each step of an algorithm of this type, we consider a subproblem of the original 0-1 integer program in which some of the variables have values assigned to them, either 0 or 1, and the remaining variables are still free to take on either value. In subproblem \"i\", let \"V\" denote the set of remaining variables. The process begins by considering a subproblem in which no variable values have been assigned, and in which \"V\" is the whole set of variables of the original problem. Then, for each subproblem \"i\", it performs the following steps.\n\nAlthough it is difficult to prove theoretical bounds on the performance of algorithms of this type, they can be very effective in practice.\n\nTwo 0-1 integer programs that are equivalent, in that they have the same objective function and the same set of feasible solutions, may have quite different linear programming relaxations: a linear programming relaxation can be viewed geometrically, as a convex polytope that includes all feasible solutions and excludes all other 0-1 vectors, and infinitely many different polytopes have this property. Ideally, one would like to use as a relaxation the convex hull of the feasible solutions; linear programming on this polytope would automatically yield the correct solution to the original integer program. However, in general, this polytope will have exponentially many facets and be difficult to construct. Typical relaxations, such as the relaxation of the set cover problem discussed earlier, form a polytope that strictly contains the convex hull and has vertices other than the 0-1 vectors that solve the unrelaxed problem.\n\nThe cutting-plane method for solving 0-1 integer programs, first introduced for the traveling salesman problem by and generalized to other integer programs by , takes advantage of this multiplicity of possible relaxations by finding a sequence of relaxations that more tightly constrain the solution space until eventually an integer solution is obtained. This method starts from any relaxation of the given program, and finds an optimal solution using a linear programming solver. If the solution assigns integer values to all variables, it is also the optimal solution to the unrelaxed problem. Otherwise, an additional linear constraint (a \"cutting plane\" or \"cut\") is found that separates the resulting fractional solution from the convex hull of the integer solutions, and the method repeats on this new more tightly constrained problem.\n\nProblem-specific methods are needed to find the cuts used by this method. It is especially desirable to find cutting planes that form facets of the convex hull of the integer solutions, as these planes are the ones that most tightly constrain the solution space; there always exists a cutting plane of this type that separates any fractional solution from the integer solutions. Much research has been performed on methods for finding these facets for different types of combinatorial optimization problems, under the framework of polyhedral combinatorics .\n\nThe related branch and cut method combines the cutting plane and branch and bound methods. In any subproblem, it runs the cutting plane method until no more cutting planes can be found, and then branches on one of the remaining fractional variables.\n\n\n"}
{"id": "521969", "url": "https://en.wikipedia.org/wiki?curid=521969", "title": "List of numeral system topics", "text": "List of numeral system topics\n\nThis is a list of Wikipedia articles on topics of numeral system and \"numeric representations\"\n\nSee also: computer numbering formats and number names.\n\n\n\n"}
{"id": "43547876", "url": "https://en.wikipedia.org/wiki?curid=43547876", "title": "Louis Bachelier Prize", "text": "Louis Bachelier Prize\n\nThe Louis Bachelier Prize is a biennial prize in applied mathematics jointly awarded by the London Mathematical Society, the Natixis Foundation for Quantitative Research and the Société de Mathématiques Appliquées et Industrielles (SMAI) in recognition for \"exceptional contributions to mathematical modelling in finance, insurance, risk management and/or scientific computing applied to finance and insurance.\" \n\nThe prize is named in honor of French mathematician Louis Bachelier, a pioneer in the field of probability and its use in financial modeling.\n\nThe Louis Bachelier Prize was created in 2007 by the Société de Mathématiques Appliquées et Industrielles [Society for Applied and Industrial Mathematics] in collaboration with the Natixis Quantitative Research Foundation and the French Academy of Sciences.. The prize of 20,000, is awarded biennially to a scientist with less than 25 years of postdoctoral experience. The candidates must be permanent residents of a country of the European Union.\nFrom its creation to 2015, the Louis Bachelier Prize was awarded by the French Academy of Sciences.\nSince 2015, the prize is administered by the London Mathematical Society. \n"}
{"id": "30159410", "url": "https://en.wikipedia.org/wiki?curid=30159410", "title": "Minimal axioms for Boolean algebra", "text": "Minimal axioms for Boolean algebra\n\nIn mathematical logic, minimal axioms for Boolean algebra are assumptions which are equivalent to the axioms of Boolean algebra (or propositional calculus), chosen to be as short as possible. For example, an axiom with six NAND operations and three variables is equivalent to Boolean algebra:\n\nwhere the vertical bar represents the NAND logical operation (also known as the Sheffer stroke).\n\nStephen Wolfram, and separately, a group of researchers including William McCune, Branden Fitelson, and Larry Wos, identified this axiom by testing 25 candidate axioms, the set of Sheffer identities of length less or equal to 15 elements (excluding mirror images) that have no noncommutative models with four or fewer variables. MathWorld, a site associated with Wolfram, has named the axiom the \"Wolfram axiom\". McCune et al. also found a longer single axiom for Boolean algebra based on disjunction and negation.\n\nIn 1933, Edward Vermilye Huntington identified the axiom\nas being equivalent to Boolean algebra, when combined with the commutativity of the OR operation, formula_3, and the assumption of associativity, formula_4. Herbert Robbins conjectured that Huntington's axiom could be replaced by\nwhich requires one fewer use of the logical negation operator formula_6. Neither Robbins nor Huntington could prove this conjecture; nor could Alfred Tarski, who took considerable interest in it later. The conjecture was eventually proved in 1996 with the aid of theorem-proving software. This proof established that the Robbins axiom, together with associativity and commutativity, form a 3-basis for Boolean algebra. The existence of a 2-basis was established in 1967 by Carew Arthur Meredith:\nThe following year, Meredith found a 2-basis in terms of the Sheffer stroke:\n\nIn 1973, Padmanabhan and Quackenbush demonstrated a method that, in principle, would yield a 1-basis for Boolean algebra. Applying this method in a straightforward manner yielded \"axioms of enormous length\", thereby prompting the question of how shorter axioms might be found. This search yielded the 1-basis in terms of the Sheffer stroke given above, as well as the 1-basis\nwhich is written in terms of OR and NOT.\n"}
{"id": "29353987", "url": "https://en.wikipedia.org/wiki?curid=29353987", "title": "Minuscule representation", "text": "Minuscule representation\n\nIn mathematical representation theory, a minuscule representation of a semisimple Lie algebra or group is an irreducible representation such that the Weyl group acts transitively on the weights. Some authors exclude the trivial representation. A quasi-minuscule representation (also called a basic representation) is an irreducible representation such that all non-zero weights are in the same orbit under the Weyl group; each simple Lie algebra has a unique quasi-minuscule representation that is not minuscule, and the multiplicity of the zero weight is the number of short nodes of the Dynkin diagram.\n\nThe minuscule representations are indexed by the weight lattice modulo the root lattice, or equivalently by irreducible representations of the center of the simply connected compact group. For the simple Lie algebras, the dimensions of the minuscule representations are given as follows.\n\n"}
{"id": "46846453", "url": "https://en.wikipedia.org/wiki?curid=46846453", "title": "Non-linear preferential attachment", "text": "Non-linear preferential attachment\n\nIn network science, preferential attachment means that nodes of a network tend to connect to those nodes which have more links. If the network is growing and new nodes tend to connect to existing ones with linear probability in the degree of the existing nodes then preferential attachment leads to a scale-free network. If this probability is sub-linear then the network’s degree distribution is stretched exponential and hubs are much smaller than in a scale-free network. If this probability is super-linear then almost all nodes are connected to a few hubs. According to Kunegis, Blattner, and Moser several online networks follow a non-linear preferential attachment model. Communication networks and online contact networks are sub-linear while interaction networks are super-linear. The co-author network among scientists also shows the signs of sub-linear preferential attachment.\n\nFor simplicity it can be assumed that the probability with which a new node connects to an existing one follows a power function of the existing nodes’ degree \"k\":\n\nwhere \"α\" > 0. This is a good approximation for a lot of real networks such as the Internet, the citation network or the actor network. If \"α\" = 1 then the preferential attachment is linear. If \"α\" < 1 then it is sub-linear while if \"α\" > 1 then it is super-linear.\n\nIn measuring preferential attachment from real networks, the above log-linearity functional form \"k\" can be relaxed to a free form function, i.e. (\"k\") can be measured for each \"k\" without any assumptions on the functional form of (\"k\"). This is believed to be more flexible, and allows the discovery of non-log-linearity of preferential attachment in real networks.\n\nIn this case the new nodes still tend to connect to the nodes with higher degree but this effect is smaller than in the case of linear preferential attachment. There are less hubs and their size is also smaller than in a scale-free network. The size of the largest component logarithmically depends on the number of nodes:\n\nso it is smaller than the polynomial dependence.\n\nIf \"α\" > 1 then a few nodes tend to connect to every other node in the network. For \"α\" > 2 this process happens more extremely, the number of connections between other nodes is still finite in the limit when \"n\" goes to infinity. So the degree of the largest hub is proportional to the system size:\n"}
{"id": "24067", "url": "https://en.wikipedia.org/wiki?curid=24067", "title": "Paolo Uccello", "text": "Paolo Uccello\n\nPaolo Uccello (; 1397 – 10 December 1475), born Paolo di Dono, was an Italian painter and mathematician who was notable for his pioneering work on visual perspective in art. In his book \"Lives of the Artists\" Giorgio Vasari wrote that Uccello was obsessed by his interest in perspective and would stay up all night in his study trying to grasp the exact vanishing point. While his contemporaries used perspective to narrate different or succeeding stories, Uccello used perspective to create a feeling of depth in his paintings. His best known works are the three paintings representing the battle of San Romano, which were wrongly entitled the \"Battle of Sant' Egidio of 1416\" for a long period of time.\n\nPaolo worked in the Late Gothic tradition, emphasizing colour and pageantry rather than the classical realism that other artists were pioneering. His style is best described as idiosyncratic, and he left no school of followers. He has had some influence on twentieth-century art and literary criticism (e.g., in the \"Vies imaginaires\" by Marcel Schwob, \"Uccello le poil\" by Antonin Artaud and \"O Mundo Como Ideia\" by Bruno Tolentino).\n\nThe sources for Paolo Uccello’s life are few: Giorgio Vasari’s biography, written 75 years after Paolo’s death, and a few contemporary official documents. Due to the lack of sources, even his date of birth is questionable. It is believed that Uccello was born in Pratovecchio in 1397, and his tax declarations for some years indicate that he was born in 1397, but in 1446 he claimed to be born in 1396. His father, Dono di Paolo, was a barber-surgeon from Pratovecchio near Arezzo; his mother, Antonia, was a high-born Florentine. His nickname \"Uccello\" came from his fondness for painting birds.\n\nFrom 1412 until 1416 he was apprenticed to the famous sculptor Lorenzo Ghiberti. Ghiberti was the designer of the doors of the Florence Baptistery and his workshop was the premier centre for Florentine art at the time. Ghiberti's late-Gothic, narrative style and sculptural composition greatly influenced Paolo. It was also around this time that Paolo began his lifelong friendship with Donatello. In 1414, Uccello was admitted to the painters' guild, \"Compagnia di San Luca,\" and just one year later, in 1415, he joined the official painter's guild of Florence \"Arte dei Medici e degli Speziali\". Although the young Uccello had probably left Ghiberti's workshop by the mid 1420s, he stayed on good terms with his master and may have been privy to the designs for Ghiberti's second set of Baptistery doors, \"The Gates of Paradise.\" These featured a battle scene \"that might well have impressed itself in the mind of the young Uccello,\" and thus influenced \"The Battle of San Romano\".\n\nAccording to Vasari, Uccello’s first painting was a Saint Anthony between the saints Cosmas and Damianus, a commission for the hospital of Lelmo. Next, he painted two figures in the convent of Annalena. Shortly afterwards, he painted three frescoes with scenes from \"the life of Saint Francis\" above the left door of the Santa Trinita church. For the Santa Maria Maggiore church, he painted a fresco of the Annunciation. In this fresco, he painted a large building with columns in perspective. According to Vasari, people found this to be a great and beautiful achievement because this was the first example of how lines could be expertly used to demonstrate perspective and size. As a result, this work became a model for artists who wished to craft illusions of space in order to enhance the realness of their paintings.\n\nPaolo painted \"the Lives of the Church Fathers\" in the cloisters of the church of San Miniato, which sat on a hill overlooking Florence. According to Vasari, Paolo protested against the monotonous meals of cheese pies and cheese soup served by the abbot by running away, and returned to finish the job only after the abbot promised him a more varied diet.\n\nUccello was asked to paint a number of scenes of distempered animals for the house of the Medici. The scene most appreciated by Vasari was his depiction of a fierce lion fighting with a venom-spouting snake. Uccello loved to paint animals and he kept a wide variety of pictures of animals, especially birds, at home. This love for birds is what led to his nickname, Paolo Uccelli (Paul of the birds).\n\nBy 1424, Paolo was earning his own living as a painter. In that year, he proved his artistic maturity by painting episodes of the \"Creation and Expulsion,\" which are now badly damaged, for the Green Cloister (\"Chiostro Verde\") of Santa Maria Novella in Florence. Again, this assignment allowed him to paint a large number of animals in a lively manner. He also succeeded in painting trees in their natural colours. This was a skill that was difficult for many of his predecessors, so Uccello also began to acquire a reputation for painting landscapes. He continued with scenes from the Deluge, the story of Noah's Ark, Noah's sacrifice and Noah's drunkenness. These scenes brought him great fame in Florence.\n\nIn 1425, Uccello travelled to Venice, where he worked on the mosaics for the façade of San Marco, which have all since been lost. During this time, he also painted some frescoes in the Prato Cathedral and Bologna. Some suggest he visited Rome with his friend Donatello before returning to Florence in 1431. After he returned, Uccello remained in Florence for most of the rest of his life, executing works for various churches and patrons, most notably the Duomo.\n\nDespite his leave from Florence, interest in Uccello did not diminish. In 1432, the Office of Works asked the Florentine ambassador in Venice to enquire after Uccello’s reputation as an artist. In 1436, he was given the commission for the monochromatic fresco of \"Sir John Hawkwood\". This equestrian monument exemplified his keen interest in perspective. The condottiere and his horse are presented as if the fresco was a sculpture seen from below.\n\nIt is widely thought that he is the author of the frescoes \"Stories of the Virgin\" and \"Story of Saint Stephen\" in the Cappella dell'Assunta, Florence, so he likely visited nearby Prato sometime between 1435 and 1440. Later, in 1443, he painted the figures on the clock of the Duomo. In that same year and continuing into 1444, he designed a few stained glass windows for the same church. In 1444 he was also at work in Padua, and he travelled to Padua again in 1445 at Donatello’s invitation.\nBack in Florence in 1446, he painted the \"Green Stations of the Cross\", again for the cloister of the church Santa Maria Novella. Around 1447–1454 he painted \"Scenes of Monastic Life\" for the church San Miniato al Monte, Florence.\n\nAround the mid-1450s, he painted his three most famous paintings, the panels depicting \"The Battle of San Romano\" for the Palazzo Medici in Florence, commemorating the victory of the Florentine army over the Sienese in 1432. The extraordinarily foreshortened forms extending in many planes accentuate Uccello's virtuosity as a draftsman, and provides a controlled visual structure to the chaos of the battle scene.\n\nBy 1453, Uccello was married to Tommasa Malifici. This is known because, in that year Donato (named after Donatello), was born. Three years later, in 1456, his wife gave birth to their daughter, Antonia. Antonia Uccello (1456–1491) was a Carmelite nun, whom Giorgio Vasari called \"a daughter who knew how to draw.\" She was even noted as a \"pittoressa\", a painter, on her death certificate. Her style and her skill remains a mystery as none of her work is extant.\n\nFrom 1465 to 1469, Uccello was in Urbino with his son Donato working for the Confraternity of Corpus Domini, a brotherhood of laymen. During this time, he painted the predella for their new altarpiece with the \"Miracle of the Profaned Host.\" (The main panel representing the \"Communion of the Apostles\" was commissioned to Justus van Ghent and finished in 1474). Uccello's predella is composed of six meticulous, naturalistic scenes related to the antisemitic myth of host desecration, which was based upon an event that supposedly occurred in Paris in 1290. It has been suggested that the subject of the main panel, on which Duke Frederick of Montefeltro of Urbino appears in the background conversing with an Asian, is related to the antisemitic intention of the predella. However, Federico did allow a small Jewish community to live in Urbino and not all of these scenes are unanimously attributed to Paolo Uccello.\n\nIn his Florentine tax return of August 1469, Uccello declared, “I find myself old and ailing, my wife is ill, and I can no longer work.” In the last years of his life, Paolo was a lonesome and forgotten man who was afraid of hardship in life. His last known work is \"The Hunt\", c. 1470. He made his testament on 11 November 1475 and died shortly afterwards on 10 December 1475 at the hospital of Florence, at the age of 78. He was buried in his father’s tomb in the Florentine church of Santo Spirito.\n\nWith his precise and analytical mind, Paolo Uccello tried to apply a scientific method to depict objects in three-dimensional space. In particular, some of his studies of the perspective foreshortening of the torus are preserved, and one standard display of drawing skill was his depiction of the mazzocchio. In the words of G. C. Argan: \"Paolo's rigour is similar to the rigour of Cubists in the early 20th century, whose images were more \"true\" when they were less \"true to life\". Paolo constructs space through perspective, and historic event through the structure of space; if the resulting image is unnatural and unrealistic, so much the worse for nature and history.\" The perspective in his paintings has influenced many famous painters, such as Piero della Francesca, Albrecht Dürer and Leonardo da Vinci, to name a few.\n\nPope-Hennessy is far more conservative than the Italian authors: he attributes some of the works below to a \"Prato Master\" and a \"Karlsruhe Master\". Most of the dates in the list (taken from Borsi and Borsi) are derived from stylistic comparison rather than from documentation.\n\n\n"}
{"id": "509331", "url": "https://en.wikipedia.org/wiki?curid=509331", "title": "Pappus's centroid theorem", "text": "Pappus's centroid theorem\n\nIn mathematics, Pappus's centroid theorem (also known as the Guldinus theorem, Pappus–Guldinus theorem or Pappus's theorem) is either of two related theorems dealing with the surface areas and volumes of surfaces and solids of revolution.\n\nThe theorems are attributed to Pappus of Alexandria and Paul Guldin.\n\nThe first theorem states that the surface area \"A\" of a surface of revolution generated by rotating a plane curve \"C\" about an axis external to \"C\" and on the same plane is equal to the product of the arc length \"s\" of \"C\" and the distance \"d\" traveled by the geometric centroid of \"C\":\n\nFor example, the surface area of the torus with minor radius \"r\" and major radius \"R\" is\n\nThe second theorem states that the volume \"V\" of a solid of revolution generated by rotating a plane figure \"F\" about an external axis is equal to the product of the area \"A\" of \"F\" and the distance \"d\" traveled by the geometric centroid of \"F\". (Note that the centroid of \"F\" is usually different from the centroid of its boundary curve \"C\".) That is:\n\nFor example, the volume of the torus with minor radius \"r\" and major radius \"R\" is\n\nThis special case was derived by Johannes Kepler using infinitesimals.\n\nLet formula_5 be the area of formula_6, formula_7 the solid of revolution of formula_6, and formula_9 the volume of formula_7. Suppose formula_6 starts in the formula_12-plane and rotates around the formula_13-axis. The distance of the centroid of formula_6 from the formula_13-axis is its formula_16-coordinate\n\nand the theorem states that\n\nTo show this, let formula_6 be in the \"xz\"-plane, parametrized by formula_20 for formula_21, a parameter region. Since formula_22 is essentially a mapping from formula_23 to formula_23, the area of formula_6 is given by the change of variables formula:\n\nwhere formula_27 is the determinant of the Jacobian matrix of the change of variables. \n\nThe solid formula_7 has the toroidal parametrization formula_29 for formula_30 in the parameter region formula_31; and its volume is\n\nExpanding,\n\nThe last equality holds because the axis of rotation must be external to formula_6, meaning formula_36. Now,\n\nby change of variables.\n\nThe theorems can be generalized for arbitrary curves and shapes, under appropriate conditions. \n\nGoodman & Goodman generalize the second theorem as follows. If the figure \"F\" moves through space so that it remains perpendicular to the curve \"L\" traced by the centroid of \"F\", then it sweeps out a solid of volume \"V\" = \"Ad\", where \"A\" is the area of \"F\" and \"d\" is the length of \"L\". (This assumes the solid does not intersect itself.) In particular, \"F\" may rotate about its centroid during the motion. \n\nHowever, the corresponding generalization of the first theorem is only true if the curve \"L\" traced by the centroid lies in a plane perpendicular to the plane of \"C\".\n"}
{"id": "16856774", "url": "https://en.wikipedia.org/wiki?curid=16856774", "title": "Pappus chain", "text": "Pappus chain\n\nIn geometry, the Pappus chain is a ring of circles between two tangent circles investigated by Pappus of Alexandria in the 3rd century AD.\n\nThe arbelos is defined by two circles, \"C\" and \"C\", which are tangent at the point A and where \"C\" is enclosed by \"C\". Let the radii of these two circles be denoted as \"r\" and \"r\", respectively, and let their respective centers be the points U and V. The Pappus chain consists of the circles in the shaded grey region, which are externally tangent to \"C\" (the inner circle) and internally tangent to \"C\" (the outer circle). Let the radius, diameter and center point of the \"n\" circle of the Pappus chain be denoted as \"r\", \"d\" and P, respectively.\n\nAll the centers of the circles in the Pappus chain are located on a common ellipse, for the following reason. The sum of the distances from the \"n\" circle of the Pappus chain to the two centers U and V of the arbelos circles equals a constant\n\nThus, the foci of this ellipse are U and V, the centers of the two circles that define the arbelos; these points correspond to the midpoints of the line segments AB and AC, respectively.\n\nIf \"r\" = \"AC\"/\"AB\", then the center of the \"n\"th circle in the chain is:\n\nIf \"r\" = \"AC\"/\"AB\", then the radius of the \"n\"th circle in the chain is:\n\nThe height \"h\" of the center of the \"n\" circle above the base diameter ACB equals \"n\" times \"d\". This may be shown by inverting in a circle centered on the tangent point A. The circle of inversion is chosen to intersect the \"n\" circle perpendicularly, so that the \"n\" circle is transformed into itself. The two arbelos circles, \"C\" and \"C\", are transformed into parallel lines tangent to and sandwiching the \"n\" circle; hence, the other circles of the Pappus chain are transformed into similarly sandwiched circles of the same diameter. The initial circle \"C\" and the final circle \"C\" each contribute ½\"d\" to the height \"h\", whereas the circles \"C\"–\"C\" each contribute \"d\". Adding these contributions together yields the equation \"h\" = \"n\" \"d\".\n\nThe same inversion can be used to show that the points where the circles of the Pappus chain are tangent to one another lie on a common circle. As noted above, the inversion centered at point A transforms the arbelos circles \"C\" and \"C\" into two parallel lines, and the circles of the Pappus chain into a stack of equally sized circles sandwiched between the two parallel lines. Hence, the points of tangency between the transformed circles lie on a line midway between the two parallel lines. Undoing the inversion in the circle, this line of tangent points is transformed back into a circle. \n\nIn these properties of having centers on an ellipse and tangencies on a circle, the Pappus chain is analogous to the Steiner chain, in which finitely many circles are tangent to two circles.\n\n\n"}
{"id": "29661763", "url": "https://en.wikipedia.org/wiki?curid=29661763", "title": "Quasisymmetric function", "text": "Quasisymmetric function\n\nIn algebra and in particular in algebraic combinatorics, a quasisymmetric function is any element in the ring of quasisymmetric functions which is in turn a subring of the formal power series ring with a countable number of variables. This ring generalizes the ring of symmetric functions. This ring can be realized as a specific limit of the rings of quasisymmetric polynomials in \"n\" variables, as \"n\" goes to infinity. This ring serves as universal structure in which relations between quasisymmetric polynomials can be expressed in a way independent of the number \"n\" of variables (but its elements are neither polynomials nor functions).\n\nThe ring of quasisymmetric functions, denoted QSym, can be defined over any commutative ring \"R\" such as the integers. \nQuasisymmetric \nfunctions are power series of bounded degree in variables formula_1 with coefficients in \"R\", which are shift invariant in the sense that the coefficient of the monomial formula_2 is equal to the coefficient of the monomial formula_3 for any strictly increasing sequence of positive integers \nformula_4 indexing the variables and any positive integer sequence formula_5 of exponents.\nMuch of the study of quasisymmetric functions is based on that of symmetric functions.\n\nA quasisymmetric function in finitely many variables is a \"quasisymmetric polynomial\".\nBoth symmetric and quasisymmetric polynomials may be characterized in terms of actions of the symmetric group formula_6\non a polynomial ring in formula_7 variables formula_8. \nOne such action of formula_6 permutes variables,\nchanging a polynomial formula_10 by iteratively swapping pairs formula_11\nof variables having consecutive indices.\nThose polynomials unchanged by all such swaps\nform the subring of symmetric polynomials.\nA second action of formula_6 conditionally permutes variables, \nchanging a polynomial formula_13\nby swapping pairs formula_11 of variables\n\"except\" in monomials containing both variables.\nThose polynomials unchanged by all such conditional swaps form\nthe subring of quasisymmetric polynomials. One quasisymmetric function in four variables is the polynomial\n\nThe simplest symmetric function containing all of these monomials is\n\nQSym is a graded \"R\"-algebra, decomposing as \n\nwhere formula_18 is the formula_19-span of all quasisymmetric functions that are homogeneous of degree formula_7. Two natural bases for formula_18 are the monomial basis formula_22 and the fundamental basis formula_23 indexed by compositions formula_24 of formula_7, denoted formula_26. The monomial basis consists of formula_27 and all formal power series \n\nThe fundamental basis consists formula_29 and all formal power series \n\nwhere formula_31 means we can obtain formula_32 by adding together adjacent parts of formula_33, for example, (3,2,4,2) formula_34 (3,1,1,1,2,1,2). Thus, when the ring formula_19 is the ring of rational numbers, one has\n\nThen one can define the algebra of symmetric functions formula_37 as the subalgebra of QSym spanned by the monomial symmetric functions formula_38 and all formal power series formula_39 where the sum is over all compositions formula_32 which rearrange to the partition formula_41. Moreover, we have formula_42. For example, formula_43 and formula_44\n\nOther important bases for quasisymmetric functions include the basis of quasisymmetric Schur functions, and bases related to enumeration in matroids.\n\nQuasisymmetric functions have been applied in enumerative combinatorics, symmetric function theory, representation theory, and number theory. Applications of\nquasisymmetric functions include enumeration of P-partitions,\npermutations, tableaux, chains of posets, reduced decompositions in finite Coxeter groups (via Stanley symmetric functions), and parking functions. In symmetric function theory and representation theory, applications include the study of Schubert polynomials, Macdonald polynomials,\nHecke algebras, and Kazhdan–Lusztig polynomials. Often quasisymmetric functions provide a powerful bridge between combinatorial structures and symmetric functions.\n\nAs a graded Hopf algebra, the dual of the ring of quasisymmetric functions is the ring of noncommutative symmetric functions. \nEvery symmetric function is also a quasisymmetric function, and hence the ring of symmetric functions is a subalgebra of the ring of quasisymmetric functions.\n\nThe ring of quasisymmetric functions is the terminal object in category of graded Hopf algebras with a single character.\nHence any such Hopf algebra has a morphism to the ring of quasisymmetric functions.\n\nOne example of this is the peak algebra.\n\nThe Malvenuto–Reutenauer algebra is a Hopf algebra based on permutations that relates the rings of symmetric functions, quasisymmetric functions, and noncommutative symmetric functions, (denoted Sym, QSym, and NSym respectively), as depicted the following commutative diagram. The duality between QSym and NSym mentioned above is reflected in the main diagonal of this diagram.\n\nMany related Hopf algebras were constructed from Hopf monoids in the category of species by Aguiar and Majahan\n\nOne can also construct the ring of quasisymmetric functions in noncommuting variables.\n\n"}
{"id": "146806", "url": "https://en.wikipedia.org/wiki?curid=146806", "title": "Recurrence relation", "text": "Recurrence relation\n\nIn mathematics, a recurrence relation is an equation that recursively defines a sequence or multidimensional array of values, once one or more initial terms are given: each further term of the sequence or array is defined as a function of the preceding terms.\n\nThe term difference equation sometimes (and for the purposes of this article) refers to a specific type of recurrence relation. However, \"difference equation\" is frequently used to refer to \"any\" recurrence relation.\n\nAn example of a recurrence relation is the logistic map:\n\nwith a given constant \"r\"; given the initial term \"x\" each subsequent term is determined by this relation.\n\nSome simply defined recurrence relations can have very complex (chaotic) behaviours, and they are a part of the field of mathematics known as nonlinear analysis.\n\nSolving a recurrence relation means obtaining a closed-form solution: a non-recursive function of \"n\".\n\nThe recurrence satisfied by the Fibonacci numbers is the archetype of a homogeneous linear recurrence relation with constant coefficients (see below). The Fibonacci sequence is defined using the recurrence\n\nwith initial conditions (seed values)\n\nExplicitly, the recurrence yields the equations\netc.\n\nWe obtain the sequence of Fibonacci numbers, which begins\n\nThe recurrence can be solved by methods described below yielding Binet's formula, which involves powers of the two roots of the characteristic polynomial \"t\" = \"t\" + 1; the generating function of the sequence is the rational function\n\nA simple example of a multidimensional recurrence relation is given by the binomial coefficients formula_9, which count the number of ways of selecting \"k\" elements out of a set of \"n\" elements.\nThey can be computed by the recurrence relation\nwith the base cases formula_11. Using this formula to compute the values of all binomial coefficients generates an infinite array called Pascal's triangle. The same values can also be computed directly by a different formula that is not a recurrence, but that requires multiplication and not just addition to compute:\nformula_12\n\nGiven an ordered sequence formula_13 of real numbers: the first difference formula_14 is defined as\n\nThe second difference formula_16 is defined as\n\nwhich can be simplified to\n\nMore generally: the \"k\"-th difference of the sequence \"a\" written as formula_19 is defined recursively as\n\nActually, it is easily seen that,\n\nThus, a difference equation can be defined as an equation that involves \n\"a\", \"a\", \"a\" etc. (or equivalently\n\"a\", \"a\", \"a\" etc.)\n\nSince difference equations are a very common form of recurrence, some authors use the two terms interchangeably. For example, the difference equation\n\nis equivalent to the recurrence relation\n\nThus one can solve many recurrence relations by rephrasing them as difference equations, and then solving the difference equation, analogously to how one solves ordinary differential equations. However, the Ackermann numbers are an example of a recurrence relation that do not map to a difference equation, much less points on the solution to a differential equation.\n\nSee time scale calculus for a unification of the theory of difference equations with that of differential equations.\n\nSummation equations relate to difference equations as integral equations relate to differential equations.\n\nSingle-variable or one-dimensional recurrence relations are about sequences (i.e. functions defined on one-dimensional grids). Multi-variable or n-dimensional recurrence relations are about n-dimensional grids. Functions defined on n-grids can also be studied with partial difference equations.\n\nAn order-\"d\" homogeneous linear recurrence with constant coefficients is an equation of the form\n\nwhere the \"d\" coefficients \"c\" (for all \"i\") are constants, and formula_25.\n\nA constant-recursive sequence is a sequence satisfying a recurrence of this form. There are \"d\" degrees of freedom for solutions to this recurrence, i.e., the initial values formula_26 can be taken to be any values but then the recurrence determines the sequence uniquely.\n\nThe same coefficients yield the characteristic polynomial (also \"auxiliary polynomial\")\n\nwhose \"d\" roots play a crucial role in finding and understanding the sequences satisfying the recurrence. If the roots \"r\", \"r\", ... are all distinct, then each solution to the recurrence takes the form\nwhere the coefficients \"k\" are determined in order to fit the initial conditions of the recurrence. When the same roots occur multiple times, the terms in this formula corresponding to the second and later occurrences of the same root are multiplied by increasing powers of \"n\". For instance, if the characteristic polynomial can be factored as (\"x\"−\"r\"), with the same root \"r\" occurring three times, then the solution would take the form\n\nAs well as the Fibonacci numbers, other constant-recursive sequences include the Lucas numbers and Lucas sequences, the Jacobsthal numbers, the Pell numbers and more generally the solutions to Pell's equation.\n\nFor order 1, the recurrence\nhas the solution \"a\" = \"r\" with \"a\" = 1 and the most general solution is \"a\" = \"kr\" with \"a\" = \"k\". The characteristic polynomial equated to zero (the characteristic equation) is simply \"t\" − \"r\" = 0.\n\nSolutions to such recurrence relations of higher order are found by systematic means, often using the fact that \"a\" = \"r\" is a solution for the recurrence exactly when \"t\" = \"r\" is a root of the characteristic polynomial. This can be approached directly or using generating functions (formal power series) or matrices.\n\nConsider, for example, a recurrence relation of the form\n\nWhen does it have a solution of the same general form as \"a\" = \"r\"? Substituting this guess (ansatz) in the recurrence relation, we find that\n\nmust be true for all \"n\" > 1.\n\nDividing through by \"r\", we get that all these equations reduce to the same thing:\n\nwhich is the characteristic equation of the recurrence relation. Solve for \"r\" to obtain the two roots \"λ\", \"λ\": these roots are known as the characteristic roots or eigenvalues of the characteristic equation. Different solutions are obtained depending on the nature of the roots: If these roots are distinct, we have the general solution\n\nwhile if they are identical (when \"A\" + 4\"B\" = 0), we have\n\nThis is the most general solution; the two constants \"C\" and \"D\" can be chosen based on two given initial conditions \"a\" and \"a\" to produce a specific solution.\n\nIn the case of complex eigenvalues (which also gives rise to complex values for the solution parameters \"C\" and \"D\"), the use of complex numbers can be eliminated by rewriting the solution in trigonometric form. In this case we can write the eigenvalues as formula_37 Then it can be shown that\n\ncan be rewritten as\n\nwhere\n\nHere \"E\" and \"F\" (or equivalently, \"G\" and δ) are real constants which depend on the initial conditions. Using \n\none may simplify the solution given above as\n\nwhere \"a\" and \"a\" are the initial conditions and\n\nIn this way there is no need to solve for λ and λ.\n\nIn all cases—real distinct eigenvalues, real duplicated eigenvalues, and complex conjugate eigenvalues—the equation is stable (that is, the variable \"a\" converges to a fixed value [specifically, zero]) if and only if \"both\" eigenvalues are smaller than one in absolute value. In this second-order case, this condition on the eigenvalues can be shown to be equivalent to |\"A\"| < 1 − \"B\" < 2, which is equivalent to |\"B\"| < 1 and |\"A\"| < 1 − \"B\".\n\nThe equation in the above example was homogeneous, in that there was no constant term. If one starts with the non-homogeneous recurrence\n\nwith constant term \"K\", this can be converted into homogeneous form as follows: The steady state is found by setting \"b\" = \"b\" = \"b\" = \"b\"* to obtain\n\nThen the non-homogeneous recurrence can be rewritten in homogeneous form as\n\nwhich can be solved as above.\n\nThe stability condition stated above in terms of eigenvalues for the second-order case remains valid for the general \"n\"-order case: the equation is stable if and only if all eigenvalues of the characteristic equation are less than one in absolute value.\n\nGiven a homogeneous linear recurrence relation with constant coefficients of order \"d\", let \"p\"(\"t\") be the characteristic polynomial (also \"auxiliary polynomial\")\n\nsuch that each \"c\" corresponds to each \"c\" in the original recurrence relation (see the general form above). Suppose λ is a root of \"p\"(\"t\") having multiplicity \"r\". This is to say that (\"t\"−λ) divides \"p\"(\"t\"). The following two properties hold:\n\n\nAs a result of this theorem a homogeneous linear recurrence relation with constant coefficients can be solved in the following manner:\n\n\nThe method for solving linear differential equations is similar to the method above—the \"intelligent guess\" (ansatz) for linear differential equations with constant coefficients is \"e\" where λ is a complex number that is determined by substituting the guess into the differential equation.\n\nThis is not a coincidence. Considering the Taylor series of the solution to a linear differential equation:\n\nit can be seen that the coefficients of the series are given by the \"n\" derivative of \"f\"(\"x\") evaluated at the point \"a\". The differential equation provides a linear difference equation relating these coefficients.\n\nThis equivalence can be used to quickly solve for the recurrence relationship for the coefficients in the power series solution of a linear differential equation.\n\nThe rule of thumb (for equations in which the polynomial multiplying the first term is non-zero at zero) is that:\n\nand more generally\n\nExample: The recurrence relationship for the Taylor series coefficients of the equation:\n\nis given by\n\nor\n\nThis example shows how problems generally solved using the power series solution method taught in normal differential equation classes can be solved in a much easier way.\n\nExample: The differential equation\n\nhas solution\n\nThe conversion of the differential equation to a difference equation of the Taylor coefficients is\n\nIt is easy to see that the \"n\"th derivative of \"e\" evaluated at 0 is \"a\"\n\nA linearly recursive sequence y of order n\n\nis identical to\n\nExpanded with n-1 identities of kind formula_66 this n-th order equation is translated into a matrix difference equation system of n first-order linear equations,\n\nObserve that the vector formula_68 can be computed by \"n\" applications of the companion matrix, C, to the initial state vector, formula_69. Thereby, n-th entry of the sought sequence y, is the top component of formula_70.\n\nEigendecomposition, formula_71 into eigenvalues, formula_72, and eigenvectors, formula_73, is used to compute formula_74 Thanks to the crucial fact that system \"C\" time-shifts every eigenvector, \"e\", by simply scaling its components \"λ\" times,\n\nthat is, time-shifted version of eigenvector,\"e\", has components \"λ\" times larger, the eigenvector components are powers of \"λ\", formula_76 and, thus, recurrent homogeneous linear equation solution is a combination of exponential functions, formula_77. The components formula_78 can be determined out of initial conditions:\n\nSolving for coefficients,\n\nThis also works with arbitrary boundary conditions formula_81, not necessary the initial ones,\n\nThis description is really no different from general method above, however it is more succinct. It also works nicely for situations like\nwhere there are several linked recurrences.\n\nCertain difference equations - in particular, linear constant coefficient difference equations - can be solved using z-transforms. The \"z\"-transforms are a class of integral transforms that lead to more convenient algebraic manipulations and more straightforward solutions. There are cases in which obtaining a direct solution would be all but impossible, yet solving the problem via a thoughtfully chosen integral transform is straightforward.\n\nIf the recurrence is non-homogeneous, a particular solution can be found by the method of undetermined coefficients and the solution is the sum of the solution of the homogeneous and the particular solutions. Another method to solve a non-homogeneous recurrence is the method of \"symbolic differentiation\". For example, consider the following recurrence:\n\nThis is a non-homogeneous recurrence. If we substitute \"n\" ↦ \"n\"+1, we obtain the recurrence\n\nSubtracting the original recurrence from this equation yields\n\nor equivalently\n\nThis is a homogeneous recurrence, which can be solved by the methods explained above. In general, if a linear recurrence has the form\n\nwhere formula_90 are constant coefficients and \"p\"(\"n\") is the inhomogeneity, then if \"p\"(\"n\") is a polynomial with degree \"r\", then this non-homogeneous recurrence can be reduced to a homogeneous recurrence by applying the method of symbolic differencing \"r\" times.\n\nIf\nis the generating function of the inhomogeneity, the generating function\nof the non-homogeneous recurrence \nwith constant coefficients is derived from\nIf \"P\"(\"x\") is a rational generating function, \"A\"(\"x\") is also one. The case discussed above, where \"p\" = \"K\" is a constant, emerges as one example of this formula, with \"P\"(\"x\") = \"K\"/(1−\"x\"). Another example, the recurrence formula_95 with linear inhomogeneity, arises in the definition of the schizophrenic numbers. The solution of homogeneous recurrences is incorporated as \"p\" = \"P\" = 0.\n\nMoreover, for the general first-order non-homogeneous linear recurrence relation with variable coefficients:\n\nthere is also a nice method to solve it:\n\nLet \nThen \n\nIf we apply the formula to formula_105 and take the limit h→0, we get the formula for first order linear differential equations with variable coefficients; the sum becomes an integral, and the product becomes the exponential function of an integral.\n\nMany homogeneous linear recurrence relations may be solved by means of the generalized hypergeometric series. Special cases of these lead to recurrence relations for the orthogonal polynomials, and many special functions. For example, the solution to\n\nis given by\n\nthe Bessel function, while\n\nis solved by\n\nthe confluent hypergeometric series. Sequences which are the solutions of linear difference equations with polynomial coefficients are called P-recursive. For these specific recurrence equations algorithms are known which find polynomial, rational or hypergeometric solutions. \n\nA first order rational difference equation has the form formula_110. Such an equation can be solved by writing formula_111 as a nonlinear transformation of another variable formula_112 which itself evolves linearly. Then standard methods can be used to solve the linear difference equation in formula_112.\n\nThe linear recurrence of order \"d\",\n\nhas the characteristic equation\n\nThe recurrence is stable, meaning that the iterates converge asymptotically to a fixed value, if and only if the eigenvalues (i.e., the roots of the characteristic equation), whether real or complex, are all less than unity in absolute value.\n\nIn the first-order matrix difference equation\n\nwith state vector \"x\" and transition matrix \"A\", \"x\" converges asymptotically to the steady state vector \"x\"* if and only if all eigenvalues of the transition matrix \"A\" (whether real or complex) have an absolute value which is less than 1.\n\nConsider the nonlinear first-order recurrence\n\nThis recurrence is locally stable, meaning that it converges to a fixed point \"x\"* from points sufficiently close to \"x\"*, if the slope of \"f\" in the neighborhood of \"x\"* is smaller than unity in absolute value: that is,\n\nA nonlinear recurrence could have multiple fixed points, in which case some fixed points may be locally stable and others locally unstable; for continuous \"f\" two adjacent fixed points cannot both be locally stable.\n\nA nonlinear recurrence relation could also have a cycle of period \"k\" for \"k\" > 1. Such a cycle is stable, meaning that it attracts a set of initial conditions of positive measure, if the composite function\n\nwith \"f\" appearing \"k\" times is locally stable according to the same criterion:\n\nwhere \"x\"* is any point on the cycle.\n\nIn a chaotic recurrence relation, the variable \"x\" stays in a bounded region but never converges to a fixed point or an attracting cycle; any fixed points or cycles of the equation are unstable. See also logistic map, dyadic transformation, and tent map.\n\nWhen solving an ordinary differential equation numerically, one typically encounters a recurrence relation. For example, when solving the initial value problem\n\nwith Euler's method and a step size \"h\", one calculates the values\n\nby the recurrence\n\nSystems of linear first order differential equations can be discretized exactly analytically using the methods shown in the discretization article.\n\nSome of the best-known difference equations have their origins in the attempt to model population dynamics. For example, the Fibonacci numbers were once used as a model for the growth of a rabbit population.\n\nThe logistic map is used either directly to model population growth, or as a starting point for more detailed models of population dynamics. In this context, coupled difference equations are often used to model the interaction of two or more populations. For example, the Nicholson-Bailey model for a host-parasite interaction is given by\n\nwith \"N\" representing the hosts, and \"P\" the parasites, at time \"t\".\n\nIntegrodifference equations are a form of recurrence relation important to spatial ecology. These and other difference equations are particularly suited to modeling univoltine populations.\n\nRecurrence relations are also of fundamental importance in analysis of algorithms. If an algorithm is designed so that it will break a problem into smaller subproblems (divide and conquer), its running time is described by a recurrence relation.\n\nA simple example is the time an algorithm takes to find an element in an ordered vector with formula_126 elements, in the worst case.\n\nA naive algorithm will search from left to right, one element at a time. The worst possible scenario is when the required element is the last, so the number of comparisons is formula_126.\n\nA better algorithm is called binary search. However, it requires a sorted vector. It will first check if the element is at the middle of the vector. If not, then it will check if the middle element is greater or lesser than the sought element. At this point, half of the vector can be discarded, and the algorithm can be run again on the other half. The number of comparisons will be given by\n\nthe time complexity of which will be formula_130.\n\nIn digital signal processing, recurrence relations can model feedback in a system, where outputs at one time become inputs for future time. They thus arise in infinite impulse response (IIR) digital filters.\n\nFor example, the equation for a \"feedforward\" IIR comb filter of delay \"T\" is:\n\nWhere formula_112 is the input at time \"t\", formula_133 is the output at time \"t\", and α controls how much of the delayed signal is fed back into the output. From this we can see that\n\netc.\n\nRecurrence relations, especially linear recurrence relations, are used extensively in both theoretical and empirical economics. In particular, in macroeconomics one might develop a model of various broad sectors of the economy (the financial sector, the goods sector, the labor market, etc.) in which some agents' actions depend on lagged variables. The model would then be solved for current values of key variables (interest rate, real GDP, etc.) in terms of exogenous variables and lagged endogenous variables. See also time series analysis.\n\n\n"}
{"id": "15622495", "url": "https://en.wikipedia.org/wiki?curid=15622495", "title": "Robert Bartnik", "text": "Robert Bartnik\n\nRobert Bartnik is an Australian mathematician based at Monash University, where he holds the position of Professor of Pure Mathematics. He is known for his contribution to general relativity, particularly for demonstrating that the ADM mass of an asymptotically flat spacetime is a well-defined quantity. He gained his bachelor's and master's degrees from Melbourne University and his PhD from Princeton University in 1983. His dissertation subject was \"Existence of Maximal Hypersurfaces\". In 2004 he was elected as a fellow of the Australian Academy of Science at which time it was noted He is perhaps best known for his work with John McKinnon on particle-like solutions of the Einstein Yang–Mills equation, but he has worked widely on applications of geometry and analysis to the study of spacetime structure. \n\nHe was a visiting scholar at the Institute for Advanced Study in 1980-81.\n\n"}
{"id": "28782", "url": "https://en.wikipedia.org/wiki?curid=28782", "title": "Self-similarity", "text": "Self-similarity\n\nIn mathematics, a self-similar object is exactly or approximately similar to a part of itself (i.e. the whole has the same shape as one or more of the parts). Many objects in the real world, such as coastlines, are statistically self-similar: parts of them show the same statistical properties at many scales. Self-similarity is a typical property of artificial fractals. Scale invariance is an exact form of self-similarity where at any magnification there is a smaller piece of the object that is similar to the whole. For instance, a side of the Koch snowflake is both symmetrical and scale-invariant; it can be continually magnified 3x without changing shape. The non-trivial similarity evident in fractals is distinguished by their fine structure, or detail on arbitrarily small scales. As a counterexample, whereas any portion of a straight line may resemble the whole, further detail is not revealed.\n\nA time developing phenomenon is said to exhibit self-similarity if the numerical value of certain observable quantity \nformula_1 measured at different times are different but the corresponding dimensionless quantity at given value of formula_2 remain invariant. It happens if the quantity formula_1 exhibits dynamic scaling. The idea is just an extension of the idea of similarity of two triangles. Note that two triangles are similar if the numerical values of their sides are different however the corresponding dimensionless quantities, such as their angles, coincide.\n\nIn mathematics, self-affinity is a feature of a fractal whose pieces are scaled by different amounts in the x- and y-directions. This means that to appreciate the self similarity of these fractal objects, they have to be rescaled using an anisotropic affine transformation.\n\nA compact topological space \"X\" is self-similar if there exists a finite set \"S\" indexing a set of non-surjective homeomorphisms formula_4 for which\n\nIf formula_6, we call \"X\" self-similar if it is the only non-empty subset of \"Y\" such that the equation above holds for formula_7. We call\n\na \"self-similar structure\". The homeomorphisms may be iterated, resulting in an iterated function system. The composition of functions creates the algebraic structure of a monoid. When the set \"S\" has only two elements, the monoid is known as the dyadic monoid. The dyadic monoid can be visualized as an infinite binary tree; more generally, if the set \"S\" has \"p\" elements, then the monoid may be represented as a p-adic tree.\n\nThe automorphisms of the dyadic monoid is the modular group; the automorphisms can be pictured as hyperbolic rotations of the binary tree.\n\nA more general notion than self-similarity is Self-affinity.\n\nThe Mandelbrot set is also self-similar around Misiurewicz points.\n\nSelf-similarity has important consequences for the design of computer networks, as typical network traffic has self-similar properties. For example, in teletraffic engineering, packet switched data traffic patterns seem to be statistically self-similar. This property means that simple models using a Poisson distribution are inaccurate, and networks designed without taking self-similarity into account are likely to function in unexpected ways.\n\nSimilarly, stock market movements are described as displaying self-affinity, i.e. they appear self-similar when transformed via an appropriate affine transformation for the level of detail being shown. Andrew Lo describes stock market log return self-similarity in econometrics.\n\nFinite subdivision rules are a powerful technique for building self-similar sets, including the Cantor set and the Sierpinski triangle.\n\nThe Viable System Model of Stafford Beer is an organizational model with an affine self-similar hierarchy, where a given viable system is one element of the System One of a viable system one recursive level higher up, and for whom the elements of its System One are viable systems one recursive level lower down.\n\nSelf-similarity can be found in nature, as well. To the right is a mathematically generated, perfectly self-similar image of a fern, which bears a marked resemblance to natural ferns. Other plants, such as Romanesco broccoli, exhibit strong self-similarity.\n\n\n\n"}
{"id": "419250", "url": "https://en.wikipedia.org/wiki?curid=419250", "title": "Significand", "text": "Significand\n\nThe significand (also mantissa or coefficient, sometimes also argument or fraction) is part of a number in scientific notation or a floating-point number, consisting of its significant digits. Depending on the interpretation of the exponent, the significand may represent an integer or a fraction. The word \"mantissa\" seems to have been introduced by Arthur Burks in 1946 writing for the Institute for Advanced Study at Princeton, although this use of the word is discouraged by the IEEE floating-point standard committee as well as some professionals such as the creator of the standard, William Kahan.\n\nThe number 123.45 can be represented as a decimal floating-point number with the integer 12345 as the significand and a 10 power term, also called characteristics, where −2 is the exponent (and 10 is the base). Its value is given by the following arithmetic:\n\nThis same value can also be represented in normalized form with 1.2345 as the fractional coefficient, and +2 as the exponent (and 10 as the base):\n\nSchmid, however, called this representation with a significand ranging between 1.0 and 10 a modified normalized form.\n\nFor base 2, this 1.xxxx form is also called a normalized significand.\n\nFinally, the value can be represented in the format given by the Language Independent Arithmetic standard and several programming language standards, including Ada, C, Fortran and Modula-2, as\n\nSchmid called this representation with a significand ranging between 0.1 and 1.0 the true normalized form.\n\nThis later 0.xxxx form is called a normed significand.\n\nFor a normalized number, the most significant digit is always non-zero. When working in binary, this uniquely determines this digit to always be 1; as such, it doesn't need to be explicitly stored, being called the \"hidden bit\". The significand is characterized by its width in (binary) digits, and depending on the context, the hidden bit may or may not be counted towards the width of the significand. For example, the same IEEE 754 double-precision format is commonly described as having either a 53-bit significand, including the hidden bit, or a 52-bit significand, excluding the hidden bit. IEEE 754 defines the precision \"p\" to be the number of digits in the significand, including any implicit leading bit (e.g., \"p\" = 53 for the double-precision format).\n\nIn American English, the original word for this seems to have been \"mantissa\" (Burks \"et al.\"), and this usage remains common in computing and among computer scientists. However, the term \"significand\" was introduced by George Forsythe and Cleve Moler in 1967, and the use of \"mantissa\" for this purpose is discouraged by the IEEE floating-point standard committee and by some professionals such as William Kahan and Donald Knuth, because it conflicts with the pre-existing use of \"mantissa\" for the fractional part of a logarithm (see also common logarithm). For instance, Knuth adopts the third representation 0.12345 × 10 in the example above and calls 0.12345 the \"fraction\" part of the number; he adds: \"it is an abuse of terminology to call the fraction part a mantissa, since this concept has quite a different meaning in connection with logarithms\".\n\nThe confusion is because scientific notation and floating-point representation are log-linear, not logarithmic. To multiply two numbers, given their logarithms, one just adds the characteristic (integer part) and the mantissa (fractional part). By contrast, to multiply two floating-point numbers, one adds the exponent (which is logarithmic) and \"multiplies\" the significand (which is linear).\n\n"}
{"id": "5213747", "url": "https://en.wikipedia.org/wiki?curid=5213747", "title": "Slutsky's theorem", "text": "Slutsky's theorem\n\nIn probability theory, Slutsky’s theorem extends some properties of algebraic operations on convergent sequences of real numbers to sequences of random variables.\n\nThe theorem was named after Eugen Slutsky. Slutsky’s theorem is also attributed to Harald Cramér.\n\nLet {\"X\"}, {\"Y\"} be sequences of scalar/vector/matrix random elements.\n\nIf \"X\" converges in distribution to a random element \"X\";\n\nand \"Y\" converges in probability to a constant \"c\", then\n\nwhere formula_4 denotes convergence in distribution.\n\nNotes:\n\nThis theorem follows from the fact that if \"X\" converges in distribution to \"X\" and \"Y\" converges in probability to a constant \"c\", then the joint vector (\"X\", \"Y\") converges in distribution to (\"X\", \"c\") (see here).\n\nNext we apply the continuous mapping theorem, recognizing the functions \"g\"(\"x\",\"y\") = \"x\" + \"y\", \"g\"(\"x\",\"y\") = \"xy\", and \"g\"(\"x\",\"y\") = \"x\" \"y\" as continuous (for the last function to be continuous, \"y\" has to be invertible).\n\n"}
{"id": "3257847", "url": "https://en.wikipedia.org/wiki?curid=3257847", "title": "Structured criticality", "text": "Structured criticality\n\nStructured criticality is a property of complex systems in which small events may trigger larger events due to subtle interdependencies between elements. This often gives rise to a form of stratified chaos where the general behavior of the system can be modeled on one scale while smaller- and larger-scale behaviors remain unpredictable.\n\nFor example:\n\nConsider a pile of sand. If you drop one grain of sand on top of this pile every second, the pile will continue to grow in the shape of a cone. The general shape, size, and growth of this cone is fairly easy to model as a function of the rate at which new sand grains are added, the size and shape of the grains, and the number of grains in the pile.\n\nThe pile retains its shape because occasionally a new grain of sand will trigger an avalanche which causes some number of grains to slide down the side of the cone into new positions. \n\nThese avalanches are chaotic. It is nearly impossible to predict if the next grain of sand will cause an avalanche, where that avalanche will occur on the pile, how many grains of sand will be involved in the event, and so on. \n\nHowever, the aggregate behavior of avalanches can be modeled statistically with some accuracy. For example, you can reasonably predict the frequency of avalanche events of different sizes.\n\nThe avalanches are caused when the impact of a new grain of sand is sufficient to dislodge some group of sand grains. If that group is dislodged then its motion may be sufficient to cause a cascade failure in some neighboring groups, while other groups that are nearby may be strong enough to absorb the energy of the event without being disturbed.\n\nEach group of sand grains can be thought of as a sub-system with its own state, and each sub-system can be made up of other sub-systems, and so on. In this way you can imagine the sand pile as a complex system made up of sub-systems ultimately made up of individual grains of sand (yet another sub-system). Each of these sub-systems are more or less likely to suffer a cascade failure. Those that are likely to fail and reorganize can be said to be in a critical state.\n\nPut another way, the likelihood that any particular sub-system will fail (or experience a particular event) can be called its criticality. (See: Self-organized criticality)\n\nSo then, the pile of sand can be viewed as a network of interconnected systems, each with its own criticality. The relationships between these groups impose a structure on this network which has a profound effect on the probability and scope of a cascade failure in response to some other event. In other words - structured criticality.\n\nStructured criticality is found just about everywhere. Some other examples are:\n\n"}
{"id": "11120026", "url": "https://en.wikipedia.org/wiki?curid=11120026", "title": "Sum-free set", "text": "Sum-free set\n\nIn additive combinatorics and number theory, a subset \"A\" of an abelian group \"G\" is said to be sum-free if the sumset \"A\"⊕\"A\" is disjoint from \"A\". In other words, \"A\" is sum-free if the equation formula_1 has no solution with formula_2.\n\nFor example, the set of odd numbers is a sum-free subset of the integers, and the set {\"N\"+1, ..., 2\"N\"} forms a large sum-free subset of the set {1...,2\"N\"}. Fermat's Last Theorem is the statement that, for a given integer \"n\" > 2, the set of all nonzero \"n\" powers of the integers is a sum-free subset.\n\nSome basic questions that have been asked about sum-free sets are:\n\n\nA sum-free set is said to be maximal if it is not a proper subset of another sum-free set.\n"}
{"id": "34157817", "url": "https://en.wikipedia.org/wiki?curid=34157817", "title": "Tommaso Boggio", "text": "Tommaso Boggio\n\nTommaso Boggio (22 December 1877 – 25 May 1963) was an Italian mathematician. Boggio worked in mathematical physics, differential geometry, analysis, and financial mathematics. He was an invited speaker in International Congress of Mathematicians 1908 in Rome. He wrote, with Burali-Forti, \"Meccanica Razionale\", published in 1921 by S. Lattes & Compagnia.\n\n"}
