{"id": "19216956", "url": "https://en.wikipedia.org/wiki?curid=19216956", "title": "(ε, δ)-definition of limit", "text": "(ε, δ)-definition of limit\n\nIn calculus, the (ε, δ)-definition of limit (\"epsilon–delta definition of limit\") is a formalization of the notion of limit. The concept is due to Augustin-Louis Cauchy, who never gave an (formula_1) definition of limit in his \"Cours d'Analyse\", but occasionally used formula_1 arguments in proofs. It was first given as a formal definition by Bernard Bolzano in 1817, and the definitive modern statement was ultimately provided by Karl Weierstrass. It makes rigorous the following informal notion: the dependent expression \"f\"(\"x\") approaches the value \"L\" as the variable \"x\" approaches the value \"c\" if \"f\"(\"x\") can be made as close as desired to \"L\" by taking \"x\" sufficiently close to \"c\".\n\nAlthough the Greeks examined limiting process, such as the Babylonian method, they probably had no concept similar to the modern limit. The need for the concept of a limit came into force in the 17th century when Pierre de Fermat attempted to find the slope of the tangent line at a point formula_3 of a function such as formula_4. Using a non-zero, but almost zero quantity, formula_5, Fermat performed the following calculation:\n\nThe key to the above calculation is that since formula_5 is non-zero one can divide formula_8 by formula_5, but since formula_5 is close to 0, formula_11 is essentially formula_12. Quantities such as formula_5 are called infinitesimals. The problem with this calculation is that mathematicians of the era were unable to rigorously define a quantity with properties of formula_14 although it was common practice to 'neglect' higher power infinitesimals and this seemed to yield correct results.\n\nThis problem reappeared later in the 1600s at the center of the development of calculus because calculations such as Fermat's are important to the calculation of derivatives. Isaac Newton first developed calculus via an infinitesimal quantity called a fluxion. He developed them in reference to the idea of an \"infinitely small moment in time...\" However, Newton later rejected fluxions in favor of a theory of ratios that is close to the modern formula_15 definition of the limit. Moreover, Newton was aware that the limit of the ratio of vanishing quantities was \"not\" itself a ratio, as he wrote:\nAdditionally, Newton occasionally explained limits in terms similar to the epsilon–delta definition. Gottfried Wilhelm Leibniz developed an infinitesimal of his own and tried to provide it with a rigorous footing, but it was still greeted with unease by some mathematicians and philosophers.\n\nAugustin-Louis Cauchy gave a definition of limit in terms of a more primitive notion he called a \"variable quantity\". He never gave an epsilon–delta definition of limit (Grabiner 1981). Some of Cauchy's proofs contain indications of the epsilon–delta method. Whether or not his foundational approach can be considered a harbinger of Weierstrass's is a subject of scholarly dispute. Grabiner feels that it is, while Schubring (2005) disagrees. Nakane concludes that Cauchy and Weierstrass gave the same name to different notions of limit.\n\nEventually, Weierstrass and Bolzano are credited with providing a rigorous footing for calculus in the form of the modern formula_16 definition of the limit.\n\nThis is not to say that the limiting definition was free of problems as, although it removed the need for infinitesimals, it did require the construction of the real numbers by Richard Dedekind. This is also not to say that infinitesimals have no place in modern mathematics as later mathematicians were able to rigorously create infinitesimal quantities as part of the hyperreal number or surreal number systems. Moreover, it is possible to rigorously develop calculus with these quantities and they have other mathematical uses.\n\nA viable intuitive or provisional definition is that a \"function \"f\" approaches the limit \"L\" near \"a\" (symbolically, formula_19) if we can make \"f(x)\" as close as we like to \"L\" by requiring that \"x\" be sufficiently close to, but unequal to, \"a\".\"\n\nWhen we say that two things are close (such as \"f(x)\" and \"L\" or \"x\" and \"a\") we mean that the distance between them is small. When \"f(x)\", \"L\", \"x\", and \"a\" are real numbers, the distance between two numbers is the absolute value of the difference of the two. Thus, when we say \"f(x)\" is close to \"L\" we mean formula_20 is small. When we say that \"x\" and \"a\" are close, we mean that formula_21 is small.\n\nWhen we say that we can make \"f(x)\" as close as we like to \"L\", we mean that for all non-zero distances, formula_22, we can make the distance between \"f(x)\" and \"L\" smaller than formula_22.\n\nWhen we say that we can make \"f(x)\" as close as we like to \"L\" by requiring that \"x\" be sufficiently close to, but, unequal to, \"a\", we mean that for every non-zero distance formula_24, there is some non-zero distance formula_25 such that if the distance between \"x\" and \"a\" is less than formula_25 then the distance between \"f(x)\" and \"L\" is smaller than formula_24.\n\nThe aspect that must be grasped is that the definition requires the following conversation. One is provided with any challenge formula_28 for a given \"f\",\"a\", and \"L\". One must answer with a formula_29 such that formula_30 implies that formula_31. If one can provide an answer for any challenge, one has proven that the limit exists.\n\nThe formula_32 definition of the limit of a function is as follows:\n\nLet formula_33 be a real-valued function defined on a subset formula_34 of the real numbers. Let formula_35 be a limit point of formula_34 and let formula_37 be a real number. We say that\n\nif for every formula_28 there exists a formula_40 such that, for all formula_41, if formula_42, then formula_43.\n\nSymbolically:\n\nIf formula_45 or formula_46, then the condition that formula_47 is a limit point is automatically met because closed real intervals and the entire real line are perfect sets.\n\nThe definition can be generalized to functions that map between metric spaces. These spaces come with a function, called a metric, that takes two points in the space and returns a real number that represents the distance between the two points. The generalized definition is as follows:\n\nSuppose formula_33 is defined on a subset formula_34 of a metric space formula_50 with a metric formula_51 and maps into a metric space formula_52 with a metric formula_53. Let formula_35 be a limit point of formula_34 and let formula_37 be a point of formula_52.\n\nWe say that\n\nif for every formula_28 there exists a formula_40 such that, for all formula_41, if formula_62, then formula_63.\n\nSince formula_64 is a metric on the real numbers, one can show that this definition generalizes the first definition for real functions.\n\nThe negation of the definition is a follows:\n\nSuppose formula_33 is defined on a subset formula_34 of a metric space formula_50 with a metric formula_51 and maps into a metric space formula_52 with a metric formula_53. Let formula_35 be a limit point of formula_34 and let formula_37 be a point of formula_52.\n\nWe say that\n\nif there exists an formula_28 such that for all formula_29 there is an formula_41 such that formula_62 and formula_80.\n\nWe say that formula_81 does not exist if for all formula_82, formula_75.\n\nFor the negation of a real valued function defined on the real numbers, simply set formula_84.\n\nThe precise statement for limits at infinity is as follows:\n\nSuppose formula_33 is defined on a subset formula_34 of a metric space formula_50 with a metric formula_51 and maps into a metric space formula_52 with a metric formula_53. Let formula_82.\n\nWe say that\n\nif for every formula_93, there is a real number formula_94 such that there is an formula_41 where formula_96 and such that if formula_96 and formula_41, then formula_99.\n\nWe will show that\n\nWe let formula_93 be given. We need to find a formula_102 such that formula_103 implies formula_104.\n\nSince sine is bounded above by 1 and below by -1,\n\nformula_105\n\nThus, if we take formula_106, then formula_107 implies formula_108, which completes the proof.\n\nLet us prove the statement that\n\nfor any real number formula_110.\n\nLet formula_111 be given. We will find a formula_112 such that formula_113 implies formula_114.\n\nWe start by factoring:\n\nWe recognize that formula_116 is the term bounded by formula_117 so we can presuppose a bound of 1 and later pick something smaller than that for formula_117.\n\nSo we suppose formula_119. Since formula_120 holds in general for real numbers formula_3 and formula_122, we have\n\nThus,\n\nThus via the triangle inequality,\n\nThus, if we further suppose that\n\nthen\n\nIn summary, we set\n\nSo, if formula_129, then\n\nThus, we have found a formula_117 such that formula_132 implies formula_133. Thus, we have shown that\nfor any real number formula_110.\n\nLet us prove the statement that\n\nThis is easily shown through graphical understandings of the limit, and as such serves as a strong basis for introduction to proof. According to the formal definition above, a limit statement is correct if and only if confining formula_3 to formula_117 units of formula_35 will inevitably confine formula_140 to formula_141 units of formula_37. In this specific case, this means that the statement is true if and only if confining formula_3 to formula_117 units of 5 will inevitably confine\n\nto formula_141 units of 12. The overall key to showing this implication is to demonstrate how formula_117 and formula_141 must be related to each other such that the implication holds. Mathematically, we want to show that\n\nSimplifying, factoring, and dividing 3 on the right hand side of the implication yields\n\nwhich immediately gives the required result if we choose\n\nThus the proof is completed. The key to the proof lies in the ability of one to choose boundaries in formula_3, and then conclude corresponding boundaries in formula_140, which in this case were related by a factor of 3, which is entirely due to the slope of 3 in the line\n\nA function \"f\" is said to be continuous at \"c\" if it is both defined at \"c\" and its value at \"c\" equals the limit of \"f\" as \"x\" approaches \"c\":\n\nIf the condition 0 < |\"x\" − \"c\"| is left out of the definition of limit, then requiring \"f\"(\"x\") to have a limit at \"c\" would be the same as requiring \"f\"(\"x\") to be continuous at \"c\".\n\n\"f\" is said to be continuous on an interval \"I\" if it is continuous at every point \"c\" of \"I\".\n\nKeisler proved that a hyperreal definition of limit reduces the quantifier complexity by two quantifiers. Namely, formula_140 converges to a limit \"L\" as formula_3 tends to \"a\" if and only if for every infinitesimal \"e\", the value formula_158 is infinitely close to \"L\"; see microcontinuity for a related definition of continuity, essentially due to Cauchy. Infinitesimal calculus textbooks based on Robinson's approach provide definitions of continuity, derivative, and integral at standard points in terms of infinitesimals. Once notions such as continuity have been thoroughly explained via the approach using microcontinuity, the epsilon–delta approach is presented as well. Karel Hrbáček argues that the definitions of continuity, derivative, and integration in Robinson-style non-standard analysis must be grounded in the ε–δ method in order to cover also non-standard values of the input. Błaszczyk et al. argue that microcontinuity is useful in developing a transparent definition of uniform continuity, and characterize the criticism by Hrbáček as a \"dubious lament\". Hrbáček proposes an alternative non-standard analysis, which (unlike Robinson's) has many \"levels\" of infinitesimals, so that limits at one level can be defined in terms of infinitesimals at the next level.\n\n\n"}
{"id": "498590", "url": "https://en.wikipedia.org/wiki?curid=498590", "title": "115 (number)", "text": "115 (number)\n\n115 (one hundred [and] fifteen) is the natural number following 114 and preceding 116.\n\n115 has a square sum of divisors:\n\nThere are 115 different rooted trees with exactly eight nodes, 115 inequivalent ways of placing six rooks on a 6 × 6 chess board in such a way that no two of the rooks attack each other, and 115 solutions to the stamp folding problem for a strip of seven stamps.\n\n115 is also a heptagonal pyramidal number. The 115th Woodall number,\nis a prime number.\n\n\n115 is also the fire service emergency number in Mauritius and Italy,\nand the ambulance emergency number in Vietnam.\n\n"}
{"id": "28177884", "url": "https://en.wikipedia.org/wiki?curid=28177884", "title": "Analyst's traveling salesman theorem", "text": "Analyst's traveling salesman theorem\n\nThe analyst's traveling salesman problem is an analog of the traveling salesman problem in combinatorial optimization. In its simplest and original form, it asks under what conditions may a set \"E\" in two-dimensional Euclidean space formula_1 be contained inside a rectifiable curve of finite length. So while in the original traveling salesman problem, one asks for the shortest way to visit every vertex in a graph with a discrete path, this analytical version requires the curve to visit perhaps infinitely many points.\n\nA posteriori, for \"E\" to be contained in a rectifiable curve Γ, since Γ has tangents at \"H\"-almost every point in Γ (where \"H\" denotes one-dimensional Hausdorff measure), \"E\" must look \"flat\" when you zoom in on points in \"E\". This suggests that a condition that would tell us whether a set could be contained in a curve must somehow incorporate information about how flat \"E\" is when we zoom in on points of \"E\" at different scales.\n\nThis discussion motivates the definition of the following quantity: \n\nWhere \"Q\" is any square, formula_3 is the sidelength of \"Q\", and dist(\"x\", \"L\") measures the distance from \"x\" to the line \"L\". Intuitively, formula_4 is the width of the smallest rectangle containing the portion of \"E\" inside \"Q\", and hence formula_5 gives us a scale invariant notion of \"flatness\".\n\nLet Δ denote the collection of dyadic squares, that is,\n\nwhere formula_7 denotes the set of integers. For a set formula_8, define\n\nwhere diam \"E\" is the diameter of \"E\". Then Peter Jones' analyst's traveling salesman theorem may be stated as follows:\n\n\nThe Traveling Salesman Theorem was shown to hold in general Euclidean spaces by Kate Okikiolu, that is, the same theorem above holds for sets formula_10, \"d\" > 1, where Δ is now the collection of dyadic cubes in formula_11 defined in a similar way as dyadic squares. In her proof, the constant \"C\" grows exponentially with the dimension \"d\".\n\nWith some slight modifications to the definition of \"β\"(\"E\"), Raanan Schul showed Traveling Salesman Theorem also holds for sets \"E\" that lie in any Hilbert Space, and in particular, implies the theorems of Jones and Okikiolu, where now the constant \"C\" is independent of dimension. (In particular, this involves using \"β\"-numbers of balls instead of cubes).\n\nHahlomaa further adjusted the definition of \"β\"(\"E\") to get a condition for when a set \"E\" of an arbitrary metric space may be contained in the Lipschitz-image of a subset formula_12 of positive measure. For this, he had to redefine the definition of the \"β\"-numbers using menger curvature (since in a metric space there isn't necessarily a notion of a cube or a straight line).\n\nMenger curvature, as in the previous example, can be used to give numerical estimates that determine whether a set contains a rectifiable subset, and the proofs of these results frequently depend on \"β\"-numbers.\n\nThe Denjoy–Riesz theorem gives general conditions under which a point set can be covered by the homeomorphic image of a curve. This is true, in particular, for every compact totally disconnected subset of the Euclidean plane. However, it may be necessary for such an arc to have infinite length, failing to meet the conditions of the analyst's traveling salesman theorem.\n"}
{"id": "454450", "url": "https://en.wikipedia.org/wiki?curid=454450", "title": "Analytical mechanics", "text": "Analytical mechanics\n\nIn theoretical physics and mathematical physics, analytical mechanics, or theoretical mechanics is a collection of closely related alternative formulations of classical mechanics. It was developed by many scientists and mathematicians during the 18th century and onward, after Newtonian mechanics. Since Newtonian mechanics considers vector quantities of motion, particularly accelerations, momenta, forces, of the constituents of the system, an alternative name for the mechanics governed by Newton's laws and Euler's laws is \"vectorial mechanics\".\n\nBy contrast, analytical mechanics uses \"scalar\" properties of motion representing the system as a whole—usually its total kinetic energy and potential energy—not Newton's vectorial forces of individual particles. A scalar is a quantity, whereas a vector is represented by quantity and direction. The equations of motion are derived from the scalar quantity by some underlying principle about the scalar's variation.\n\nAnalytical mechanics takes advantage of a system's \"constraints\" to solve problems. The constraints limit the degrees of freedom the system can have, and can be used to reduce the number of coordinates needed to solve for the motion. The formalism is well suited to arbitrary choices of coordinates, known in the context as generalized coordinates. The kinetic and potential energies of the system are expressed using these generalized coordinates or momenta, and the equations of motion can be readily set up, thus analytical mechanics allows numerous mechanical problems to be solved with greater efficiency than fully vectorial methods. It does not always work for non-conservative forces or dissipative forces like friction, in which case one may revert to Newtonian mechanics or use the Udwadia–Kalaba equation.\n\nTwo dominant branches of analytical mechanics are Lagrangian mechanics (using generalized coordinates and corresponding generalized velocities in configuration space) and Hamiltonian mechanics (using coordinates and corresponding momenta in phase space). Both formulations are equivalent by a Legendre transformation on the generalized coordinates, velocities and momenta, therefore both contain the same information for describing the dynamics of a system. There are other formulations such as Hamilton–Jacobi theory, Routhian mechanics, and Appell's equation of motion. All equations of motion for particles and fields, in any formalism, can be derived from the widely applicable result called the principle of least action. One result is Noether's theorem, a statement which connects conservation laws to their associated symmetries.\n\nAnalytical mechanics does not introduce new physics and is not more general than Newtonian mechanics. Rather it is a collection of equivalent formalisms which have broad application. In fact the same principles and formalisms can be used in relativistic mechanics and general relativity, and with some modification, quantum mechanics and quantum field theory.\n\nAnalytical mechanics is used widely, from fundamental physics to applied mathematics, particularly chaos theory.\n\nThe methods of analytical mechanics apply to discrete particles, each with a finite number of degrees of freedom. They can be modified to describe continuous fields or fluids, which have infinite degrees of freedom. The definitions and equations have a close analogy with those of mechanics.\n\n\nIn Newtonian mechanics, one customarily uses all three Cartesian coordinates, or other 3D coordinate system, to refer to a body's position during its motion. In physical systems, however, some structure or other system usually constrains the body's motion from taking certain directions and pathways. So a full set of Cartesian coordinates is often unneeded, as the constraints determine the evolving relations among the coordinates, which relations can be modeled by equations corresponding to the constraints. In the Lagrangian and Hamiltonian formalisms, the constraints are incorporated into the motion's geometry, reducing the number of coordinates to the minimum needed to model the motion. These are known as \"generalized coordinates\", denoted \"q\" (\"i\" = 1, 2, 3...).\n\nDifference between curvillinear and generalized coordinates\n\nGeneralized coordinates incorporate constraints on the system. There is one generalized coordinate \"q\" for each degree of freedom (for convenience labelled by an index \"i\" = 1, 2...\"N\"), i.e. each way the system can change its configuration; as curvilinear lengths or angles of rotation. Generalized coordinates are not the same as curvilinear coordinates. The number of \"curvilinear\" coordinates equals the dimension of the position space in question (usually 3 for 3d space), while the number of \"generalized\" coordinates is not necessarily equal to this dimension; constraints can reduce the number of degrees of freedom (hence the number of generalized coordinates required to define the configuration of the system), following the general rule:\n\nFor a system with \"N\" degrees of freedom, the generalized coordinates can be collected into an \"N\"-tuple:\n\nand the time derivative (here denoted by an overdot) of this tuple give the \"generalized velocities\":\n\n\nThe foundation which the subject is built on is \"D'Alembert's principle\".\n\nThis principle states that infinitesimal \"virtual work\" done by a force across reversible displacements is zero, which is the work done by a force consistent with ideal constraints of the system. The idea of a constraint is useful - since this limits what the system can do, and can provide steps to solving for the motion of the system. The equation for D'Alembert's principle is:\n\nwhere\n\nare the generalized forces (script Q instead of ordinary Q is used here to prevent conflict with canonical transformations below) and q are the generalized coordinates. This leads to the generalized form of Newton's laws in the language of analytical mechanics:\n\nwhere \"T\" is the total kinetic energy of the system, and the notation\n\nis a useful shorthand (see matrix calculus for this notation).\nHolonomic constraints\n\nIf the curvilinear coordinate system is defined by the standard position vector r, and if the position vector can be written in terms of the generalized coordinates q and time \"t\" in the form:\n\nand this relation holds for all times \"t\", then q are called \"Holonomic constraints\". Vector r is explicitly dependent on \"t\" in cases when the constraints vary with time, not just because of q(\"t\"). For time-independent situations, the constraints are also called scleronomic, for time-dependent cases they are called rheonomic.\n\nLagrangian and Euler–Lagrange equations\n\nThe introduction of generalized coordinates and the fundamental Lagrangian function:\n\nwhere \"T\" is the total kinetic energy and \"V\" is the total potential energy of the entire system, then either following the calculus of variations or using the above formula - lead to the Euler–Lagrange equations;\n\nwhich are a set of \"N\" second-order ordinary differential equations, one for each \"q\"(\"t\").\n\nThis formulation identifies the actual path followed by the motion as a selection of the path over which the time integral of kinetic energy is least, assuming the total energy to be fixed, and imposing no conditions on the time of transit.\n\nConfiguration space\n\nThe Lagrangian formulation uses the configuration space of the system, the set of all possible generalized coordinates:\n\nwhere formula_11 is \"N\"-dimensional real space (see also set-builder notation). The particular solution to the Euler–Lagrange equations is called a \"(configuration) path or trajectory\", i.e. one particular q(\"t\") subject to the required initial conditions. The general solutions form a set of possible configurations as functions of time:\n\nThe configuration space can be defined more generally, and indeed more deeply, in terms of topological manifolds and the tangent bundle.\n\nHamiltonian and Hamilton's equations\n\nThe Legendre transformation of the Lagrangian replaces the generalized coordinates and velocities (q, q̇) with (q, p); the generalized coordinates and the \"generalized momenta\" conjugate to the generalized coordinates:\n\nand introduces the Hamiltonian (which is in terms of generalized coordinates and momenta):\n\nwhere • denotes the dot product, also leading to Hamilton's equations:\n\nwhich are now a set of 2\"N\" first-order ordinary differential equations, one for each \"q\"(\"t\") and \"p\"(\"t\"). Another result from the Legendre transformation relates the time derivatives of the Lagrangian and Hamiltonian:\n\nwhich is often considered one of Hamilton's equations of motion additionally to the others. The generalized momenta can be written in terms of the generalized forces in the same way as Newton's second law:\n\nGeneralized momentum space\n\nAnalogous to the configuration space, the set of all momenta is the \"momentum space\" (technically in this context; \"generalized momentum space\"):\n\n\"Momentum space\" also refers to \"k-space\"; the set of all wave vectors (given by De Broglie relations) as used in quantum mechanics and theory of waves: this is not referred to in this context.\n\nPhase space\n\nThe set of all positions and momenta form the \"phase space\";\n\nthat is, the Cartesian product × of the configuration space and generalized momentum space.\n\nA particular solution to Hamilton's equations is called a \"phase path\", a particular curve (q(\"t\"),p(\"t\")) subject to the required initial conditions. The set of all phase paths, the general solution to the differential equations, is the \"phase portrait\":\n\n\nAll dynamical variables can be derived from position r, momentum p, and time \"t\", and written as a function of these: \"A\" = \"A\"(q, p, \"t\"). If \"A\"(q, p, \"t\") and \"B\"(q, p, \"t\") are two scalar valued dynamical variables, the \"Poisson bracket\" is defined by the generalized coordinates and momenta:\n\nCalculating the total derivative of one of these, say \"A\", and substituting Hamilton's equations into the result leads to the time evolution of \"A\":\n\nThis equation in \"A\" is closely related to the equation of motion in the Heisenberg picture of quantum mechanics, in which classical dynamical variables become quantum operators (indicated by hats (^)), and the Poisson bracket is replaced by the commutator of operators via Dirac's canonical quantization:\n\nFollowing are overlapping properties between the Lagrangian and Hamiltonian functions.\n\n\n\n\n\nAction is another quantity in analytical mechanics defined as a functional of the Lagrangian:\n\nA general way to find the equations of motion from the action is the \"principle of least action\":\n\nwhere the departure \"t\" and arrival \"t\" times are fixed. The term \"path\" or \"trajectory\" refers to the time evolution of the system as a path through configuration space formula_31, in other words q(\"t\") tracing out a path in formula_31. The path for which action is least is the path taken by the system.\n\nFrom this principle, \"all\" equations of motion in classical mechanics can be derived. This approach can be extended to fields rather than a system of particles (see below), and underlies the path integral formulation of quantum mechanics, and is used for calculating geodesic motion in general relativity.\n\n\nThe invariance of the Hamiltonian (under addition of the partial time derivative of an arbitrary function of p, q, and \"t\") allows the Hamiltonian in one set of coordinates q and momenta p to be transformed into a new set Q = Q(q, p, \"t\") and P = P(q, p, \"t\"), in four possible ways:\n\nWith the restriction on P and Q such that the transformed Hamiltonian system is:\n\nthe above transformations are called \"canonical transformations\", each function \"G\" is called a generating function of the \"\"n\"th kind\" or \"type-\"n\"\". The transformation of coordinates and momenta can allow simplification for solving Hamilton's equations for a given problem.\n\nThe choice of Q and P is completely arbitrary, but not every choice leads to a canonical transformation. One simple criterion for a transformation q → Q and p → P to be canonical is the Poisson bracket be unity,\n\nfor all \"i\" = 1, 2...\"N\". If this does not hold then the transformation is not canonical.\n\n\nBy setting the canonically transformed Hamiltonian \"K\" = 0, and the type-2 generating function equal to Hamilton's principal function (also the action formula_36) plus an arbitrary constant \"C\":\n\nthe generalized momenta become:\n\nand P is constant, then the Hamiltonian-Jacobi equation (HJE) can be derived from the type-2 canonical transformation:\n\nwhere \"H\" is the Hamiltonian as before:\n\nAnother related function is Hamilton's characteristic function\n\nused to solve the HJE by additive separation of variables for a time-independent Hamiltonian \"H\".\n\nThe study of the solutions of the Hamilton–Jacobi equations leads naturally to the study of symplectic manifolds and symplectic topology. In this formulation, the solutions of the Hamilton–Jacobi equations are the integral curves of Hamiltonian vector fields.\n\nRouthian mechanics is a hybrid formulation of Lagrangian and Hamiltonian mechanics, not often used but especially useful for removing cyclic coordinates. If the Lagrangian of a system has \"s\" cyclic coordinates q = \"q\", \"q\", ... \"q\" with conjugate momenta p = \"p\", \"p\", ... \"p\", with the rest of the coordinates non-cyclic and denoted ζ = \"ζ\", \"ζ\", ..., \"ζ\", they can be removed by introducing the \"Routhian\":\n\nwhich leads to a set of 2\"s\" Hamiltonian equations for the cyclic coordinates q,\n\nand \"N\" − \"s\" Lagrangian equations in the non cyclic coordinates ζ.\n\nSet up in this way, although the Routhian has the form of the Hamiltonian, it can be thought of a Lagrangian with \"N\" − \"s\" degrees of freedom.\n\nThe coordinates q do not have to be cyclic, the partition between which coordinates enter the Hamiltonian equations and those which enter the Lagrangian equations is arbitrary. It is simply convenient to let the Hamiltonian equations remove the cyclic coordinates, leaving the non cyclic coordinates to the Lagrangian equations of motion.\n\nAppell's equation of motion involve generalized accelerations, the second time derivatives of the generalized coordinates:\n\nas well as generalized forces mentioned above in D'Alembert's principle. The equations are\n\nwhere\n\nis the acceleration of the \"k\" particle, the second time derivative of its position vector. Each acceleration a is expressed in terms of the generalized accelerations \"α\", likewise each r are expressed in terms the generalized coordinates \"q\".\n\n\nGeneralized coordinates apply to discrete particles. For \"N\" scalar fields \"φ\"(r, \"t\") where \"i\" = 1, 2, ... \"N\", the Lagrangian density is a function of these fields and their space and time derivatives, and possibly the space and time coordinates themselves:\n\nformula_48\n\nand the Euler–Lagrange equations have an analogue for fields:\n\nwhere \"∂\" denotes the 4-gradient and the summation convention has been used. For \"N\" scalar fields, these Lagranian field equations are a set of \"N\" second order partial differential equations in the fields, which in general will be coupled and nonlinear.\n\nThis scalar field formulation can be extended to vector fields, tensor fields, and spinor fields.\n\nThe Lagrangian is the volume integral of the Lagrangian density:\n\nOriginally developed for classical fields, the above formulation is applicable to all physical fields in classical, quantum, and relativistic situations: such as Newtonian gravity, classical electromagnetism, general relativity, and quantum field theory. It is a question of determining the correct Lagrangian density to generate the correct field equation.\n\n\nThe corresponding \"momentum\" field densities conjugate to the \"N\" scalar fields \"φ\"(r, \"t\") are:\n\nwhere in this context the overdot denotes a partial time derivative, not a total time derivative. The Hamiltonian density formula_52 is defined by analogy with mechanics:\n\nThe equations of motion are:\n\nwhere the variational derivative\n\nmust be used instead of merely partial derivatives. For \"N\" fields, these Hamiltonian field equations are a set of 2\"N\" first order partial differential equations, which in general will be coupled and nonlinear.\n\nAgain, the volume integral of the Hamiltonian density is the Hamiltonian\n\n\nEach transformation can be described by an operator (i.e. function acting on the position r or momentum p variables to change them). The following are the cases when the operator does not change r or p, i.e. symmetries.\n\nwhere \"R\"(n̂, θ) is the rotation matrix about an axis defined by the unit vector n̂ and angle θ.\n\n\nNoether's theorem states that a continuous symmetry transformation of the action corresponds to a conservation law, i.e. the action (and hence the Lagrangian) doesn't change under a transformation parameterized by a parameter \"s\":\n\nthe Lagrangian describes the same motion independent of \"s\", which can be length, angle of rotation, or time. The corresponding momenta to \"q\" will be conserved.\n\n"}
{"id": "34995804", "url": "https://en.wikipedia.org/wiki?curid=34995804", "title": "Atiyah–Jones conjecture", "text": "Atiyah–Jones conjecture\n\nIn mathematics, the Atiyah–Jones conjecture is a conjecture about the homology of the moduli space of instantons over a sphere. It was introduced by and proved by .\n\n"}
{"id": "32295384", "url": "https://en.wikipedia.org/wiki?curid=32295384", "title": "Bernstein–Zelevinsky classification", "text": "Bernstein–Zelevinsky classification\n\nIn mathematics, the Bernstein–Zelevinsky classification, introduced by and , classifies the irreducible complex smooth representations of a general linear group over a local field in terms of cuspidal representations.\n\n"}
{"id": "27258886", "url": "https://en.wikipedia.org/wiki?curid=27258886", "title": "Caterpillar tree", "text": "Caterpillar tree\n\nIn graph theory, a caterpillar or caterpillar tree is a tree in which all the vertices are within distance 1 of a central path.\n\nCaterpillars were first studied in a series of papers by Harary and Schwenk. The name was suggested by A. Hobbs. As colorfully write, \"A caterpillar is a tree which metamorphoses into a path when its cocoon of endpoints is removed.\"\n\nThe following characterizations all describe the caterpillar trees:\n\nA \"k\"-tree is a chordal graph with exactly maximal cliques, each containing vertices; in a \"k\"-tree that is not itself a , each maximal clique either separates the graph into two or more components, or it contains a single leaf vertex, a vertex that belongs to only a single maximal clique. A \"k\"-path is a \"k\"-tree with at most two leaves, and a \"k\"-caterpillar is a \"k\"-tree that can be partitioned into a \"k\"-path and some \"k\"-leaves, each adjacent to a separator \"k\"-clique of the \"k\"-path. In this terminology, a 1-caterpillar is the same thing as a caterpillar tree, and \"k\"-caterpillars are the edge-maximal graphs with pathwidth \"k\".\n\nA lobster graph is a tree in which all the vertices are within distance 2 of a central path.\n\nCaterpillars provide one of the rare graph enumeration problems for which a precise formula can be given: when \"n\" ≥ 3, the number of caterpillars with \"n\" unlabeled vertices is \nFor \"n\" = 1, 2, 3, ... the numbers of \"n\"-vertex caterpillars are\n\nFinding a spanning caterpillar in a graph is NP-complete. A related optimization problem is the Minimum Spanning Caterpillar Problem (MSCP), where a graph has dual costs over its edges and the goal is to find a caterpillar tree that spans the input graph and has the smallest overall cost. Here the cost of the caterpillar is defined as the sum of the costs of its edges, where each edge takes one of the two costs based on its role as a leaf edge or an internal one. There is no f(n)-approximation algorithm for the MSCP unless P = NP. Here f(n) is any polynomial-time computable function of n, the number of vertices of a graph.\n\nThere is a parametrized algorithm that finds an optimal solution for the MSCP in bounded treewidth graphs. So both the Spanning Caterpillar Problem and the MSCP have linear time algorithms if a graph is an outerplanar, a series-parallel, or a Halin graph.\n\nCaterpillar trees have been used in chemical graph theory to represent the structure of benzenoid hydrocarbon molecules. In this representation, one forms a caterpillar in which each edge corresponds to a 6-carbon ring in the molecular structure, and two edges are incident at a vertex whenever the corresponding rings belong to a sequence of rings connected end-to-end in the structure. writes, \"It is amazing that nearly all graphs that played an important role in what is now called \"chemical graph theory\" may be related to caterpillar trees.\" In this context, caterpillar trees are also known as benzenoid trees and Gutman trees, after the work of Ivan Gutman in this area.\n"}
{"id": "36465461", "url": "https://en.wikipedia.org/wiki?curid=36465461", "title": "Caucher Birkar", "text": "Caucher Birkar\n\nCaucher Birkar (; born Faraydoun Derakhshani (); July 1978, in Marivan County, Kurdistan Province, Iran) is a UK-based Kurdish-Iranian mathematician and a professor at the University of Cambridge. \n\nBirkar is an important contributor to modern birational geometry. In 2010 he received the Leverhulme Prize in mathematics and statistics for his contributions to algebraic geometry, and in 2016, the AMS Moore Prize for the article \"Existence of minimal models for varieties of log general type\", \"Journal of the AMS\" (2010) (joint with P. Cascini, C. Hacon and J. McKernan). He was awarded the Fields Medal in 2018, \"for his proof of boundedness of Fano varieties and contributions to the minimal model problem\".\n\nBirkar is Kurdish, and was born in 1978 in Marivan County, Kurdistan Province, Iran, on a subsistence farm, and raised during the Iran-Iraq War. He studied mathematics at the University of Tehran where he received his bachelor's degree. He was awarded the third prize in the International Mathematics Competition for University Students in 2000 and, shortly after, while still studying in the University, relocated to the UK as a refugee and asked for political asylum. In 2001–2004 Birkar was a PhD student at the University of Nottingham. In 2003 he was awarded the Cecil King Travel Scholarship by the London Mathematical Society as the most promising PhD student. \n\nUpon emigrating to the UK he changed his name to Caucher Birkar, which means \"migrant mathematician\" in Kurdish.\n\nTogether with Paolo Cascini, Christopher Hacon and James McKernan, Birkar settled several conjectures including existence of log flips, finite generation of log canonical rings, and existence of minimal models for varieties of log general type, building upon earlier work of Vyacheslav Shokurov and of Hacon and McKernan. \n\nIn the setting of log canonical singularities, he proved existence of log flips along with key cases of the minimal model and abundance conjectures. (This was also proved independently by Hacon and Chenyang Xu.)\n\nIn a different direction, he studied the old problem of Iitaka on effectivity of Iitaka fibrations induced by pluri-canonical systems on varieties of non-negative Kodaira dimension. The problem consists of two halves: one related to general fibres of the fibration and one related to the base of the fibration. Birkar and Zhang co-solved the second half of the problem, hence essentially reducing Iitaka's problem to the special case of Kodaira dimension zero.\n\nIn more recent work, Birkar studied Fano varieties and singularities of linear systems. He proved several fundamental problems such as Shokurov's conjecture on boundedness of complements and Borisov–Alexeev–Borisov conjecture on boundedness of Fano varieties. In 2018, Birkar was given the Fields Medal for his Fano varieties and his other contributions the minimal model problem. In a video made available by the Simons Foundation, Birkar expressed hope that his Fields Medal will put “just a little smile on the lips” of the world’s estimated 40 million Kurds. Birkar's Fields Medal was stolen on the same day it was awarded to him. In a special ceremony at ICM 2018, Birkar was presented with a replacement medal.\n\nBirkar is also active in the field of birational geometry over fields of positive characteristic. His work together with work of Hacon-Xu nearly completes the minimal model program for 3-folds over fields of characteristic at least 7.\n\n\n\n"}
{"id": "145661", "url": "https://en.wikipedia.org/wiki?curid=145661", "title": "Centralizer and normalizer", "text": "Centralizer and normalizer\n\nIn mathematics, especially group theory, the centralizer (also called commutant) of a subset \"S\" of a group \"G\" is the set of elements of \"G\" that commute with each element of \"S\", and the normalizer of \"S\" are elements that satisfy a weaker condition. The centralizer and normalizer of \"S\" are subgroups of \"G\", and can provide insight into the structure of \"G\".\n\nThe definitions also apply to monoids and semigroups.\n\nIn ring theory, the centralizer of a subset of a ring is defined with respect to the semigroup (multiplication) operation of the ring. The centralizer of a subset of a ring \"R\" is a subring of \"R\". This article also deals with centralizers and normalizers in Lie algebra.\n\nThe idealizer in a semigroup or ring is another construction that is in the same vein as the centralizer and normalizer.\n\nThe centralizer of a subset \"S\" of group (or semigroup) \"G\" is defined to be\n\nSometimes if there is no ambiguity about the group in question, the \"G\" is suppressed from the notation entirely. When \"S\" = {\"a\"} is a singleton set, then C({\"a\"}) can be abbreviated to C(\"a\"). Another less common notation for the centralizer is Z(\"a\"), which parallels the notation for the center of a group. With this latter notation, one must be careful to avoid confusion between the center of a group \"G\", Z(\"G\"), and the \"centralizer\" of an \"element\" \"g\" in \"G\", given by Z(\"g\").\n\nThe normalizer of \"S\" in the group (or semigroup) \"G\" is defined to be\n\nThe definitions are similar but not identical. If \"g\" is in the centralizer of \"S\" and \"s\" is in \"S\", then it must be that , however if \"g\" is in the normalizer, for some \"t\" in \"S\", potentially different from \"s\". The same conventions mentioned previously about suppressing \"G\" and suppressing braces from singleton sets also apply to the normalizer notation. The normalizer should not be confused with the normal closure.\n\nIf \"R\" is a ring or an algebra over a field, and \"S\" is a subset of \"R\", then the centralizer of \"S\" is exactly as defined for groups, with \"R\" in the place of \"G\".\n\nIf formula_3 is a Lie algebra (or Lie ring) with Lie product [\"x\",\"y\"], then the centralizer of a subset \"S\" of formula_3 is defined to be\n\nThe definition of centralizers for Lie rings is linked to the definition for rings in the following way. If \"R\" is an associative ring, then \"R\" can be given the bracket product . Of course then if and only if . If we denote the set \"R\" with the bracket product as L, then clearly the \"ring centralizer\" of \"S\" in \"R\" is equal to the \"Lie ring centralizer\" of \"S\" in L.\n\nThe normalizer of a subset \"S\" of a Lie algebra (or Lie ring) formula_3 is given by\nWhile this is the standard usage of the term \"normalizer\" in Lie algebra, this construction is actually the idealizer of the set \"S\" in formula_3. If \"S\" is an additive subgroup of formula_3, then formula_10 is the largest Lie subring (or Lie subalgebra, as the case may be) in which \"S\" is a Lie ideal.\n\nLet formula_11 denote the centralizer of formula_12 in the semigroup formula_13, i.e. formula_14 Then:\n\nSource:\n\nSource:\n\n\n"}
{"id": "991784", "url": "https://en.wikipedia.org/wiki?curid=991784", "title": "Circumscribed sphere", "text": "Circumscribed sphere\n\nIn geometry, a circumscribed sphere of a polyhedron is a sphere that contains the polyhedron and touches each of the polyhedron's vertices. The word circumsphere is sometimes used to mean the same thing. As in the case of two-dimensional circumscribed circles, the radius of a sphere circumscribed around a polyhedron \"P\" is called the circumradius of \"P\", and the center point of this sphere is called the circumcenter of \"P\".\n\nWhen it exists, a circumscribed sphere need not be the smallest sphere containing the polyhedron; for instance, the tetrahedron formed by a vertex of a cube and its three neighbors has the same circumsphere as the cube itself, but can be contained within a smaller sphere having the three neighboring vertices on its equator. However, the smallest sphere containing a given polyhedron is always the circumsphere of the convex hull of a subset of the vertices of the polyhedron.\n\nThe circumscribed sphere is the three-dimensional analogue of the circumscribed circle.\nAll regular polyhedra have circumscribed spheres, but most irregular polyhedra do not have one, since in general not all vertices lie on a common sphere. The circumscribed sphere (when it exists) is an example of a bounding sphere, a sphere that contains a given shape. It is possible to define the smallest bounding sphere for any polyhedron, and compute it in linear time.\n\nOther spheres defined for some but not all polyhedra include a midsphere, a sphere tangent to all edges of a polyhedron, and an inscribed sphere, a sphere tangent to all faces of a polyhedron. In the regular polyhedra, the inscribed sphere, midsphere, and circumscribed sphere all exist and are concentric.\n"}
{"id": "1633290", "url": "https://en.wikipedia.org/wiki?curid=1633290", "title": "Cofinal (mathematics)", "text": "Cofinal (mathematics)\n\nIn mathematics, let \"A\" be a set and let ≤ be a binary relation on \"A\". Then a subset \"B\" of \"A\" is said to be cofinal if it satisfies the following condition:\nThis definition is most commonly applied when \"A\" is a partially ordered set or directed set under the relation ≤.\n\nCofinal subsets are very important in the theory of directed sets and nets, where “cofinal subnet” is the appropriate generalization of “subsequence”. They are also important in order theory, including the theory of cardinal numbers, where the minimum possible cardinality of a cofinal subset of \"A\" is referred to as the cofinality of \"A\".\n\nA subset \"B\" of \"A\" is said to be coinitial (or dense in the sense of forcing) if it satisfies the following condition:\nThis is the order-theoretic dual to the notion of cofinal subset.\n\nNote that cofinal and coinitial subsets are both dense in the sense of appropriate (right- or left-) order topology.\n\nThe cofinal relation over partially ordered sets (\"poset\") is reflexive: every poset is cofinal in itself. It is also transitive: if \"B\" is a cofinal subset of a poset \"A\", and \"C\" is a cofinal subset of \"B\" (with the partial ordering of \"A\" applied to \"B\"), then \"C\" is also a cofinal subset of \"A\".\n\nFor a partially ordered set with maximal elements, every cofinal subset must contain all maximal elements, otherwise a maximal element which is not in the subset would fail to be \"less than\" any element of the subset, violating the definition of cofinal. For a partially ordered set with a greatest element, a subset is cofinal if and only if it contains that greatest element (this follows, since a greatest element is necessarily a maximal element). Partially ordered sets without greatest element or maximal elements admit disjoint cofinal subsets. For example, the even and odd natural numbers form disjoint cofinal subsets of the set of all natural numbers.\n\nIf a partially ordered set \"A\" admits a totally ordered cofinal subset, then we can find a subset \"B\" which is well-ordered and cofinal in \"A\".\n\nA particular but important case is given if \"A\" is a subset of the power set \"P\"(\"E\") of some set \"E\", ordered by reverse inclusion (⊃). Given this ordering of \"A\", a subset \"B\" of \"A\" is cofinal in \"A\" if for every \"a\" ∈ \"A\" there is a \"b\" ∈ \"B\" such that \"a\" ⊃ \"b\".\n\nFor example, let \"E\" be a group and let \"A\" be the set of normal subgroups of finite index. The profinite completion of \"E\" is defined to be the inverse limit of the inverse system of finite quotients of \"E\" (which are parametrized by the set \"A\"). \nIn this situation, every cofinal subset of \"A\" is sufficient to construct and describe the profinite completion of \"E\".\n\nA map f: \"X\" → \"A\" between two directed sets is said to be final if the range f(\"X\") of f is a cofinal subset of \"A\".\n\n"}
{"id": "794342", "url": "https://en.wikipedia.org/wiki?curid=794342", "title": "Construct validity", "text": "Construct validity\n\nConstruct validity is \"the degree to which a test measures what it claims, or purports, to be measuring.\" In the classical model of test validity, construct validity is one of three main types of validity evidence, alongside content validity and criterion validity. Modern validity theory defines construct validity as the overarching concern of validity research, subsuming all other types of validity evidence.\n\nConstruct validity is the appropriateness of inferences made on the basis of observations or measurements (often test scores), specifically whether a test measures the intended construct. Constructs are abstractions that are deliberately created by researchers in order to conceptualize the latent variable, which is correlated with scores on a given measure (although it is not directly observable). Construct validity examines the question: Does the measure behave like the theory says a measure of that construct should behave?\n\nConstruct validity is essential to the perceived overall validity of the test. Construct validity is particularly important in the social sciences, psychology, psychometrics and language studies.\n\nPsychologists such as Samuel Messick (1998) have pushed for a unified view of construct validity \"...as an integrated evaluative judgment of the degree to which empirical evidence and theoretical rationales support the adequacy and appropriateness of inferences and actions based on test scores...\" Key to construct validity are the theoretical ideas behind the trait under consideration, i.e. the concepts that organize how aspects of personality, intelligence, etc. are viewed. Paul Meehl states that, \"The best construct is the one around which we can build the greatest number of inferences, in the most direct fashion.\"\n\nScale purification, i.e. \"the process of eliminating items from multi-item scales\" (Wieland et al., 2017) can influence construct validity. A framework presented by Wieland et al. (2017) highlights that both statistical and judgmental criteria need to be taken under consideration when making scale purification decision.\n\nThroughout the 1940s scientists had been trying to come up with ways to validate experiments prior to publishing them. The result of this was a myriad of different validities (intrinsic validity, face validity, logical validity, empirical validity, etc.). This made it difficult to tell which ones were actually the same and which ones were not useful at all. Until the middle of the 1950s there were very few universally accepted methods to validate psychological experiments. The main reason for this was because no one had figured out exactly which qualities of the experiments should be looked at before publishing. Between 1950 and 1954 the APA Committee on Psychological Tests met and discussed the issues surrounding the validation of psychological experiments.\n\nAround this time the term construct validity was first coined by Paul Meehl and Lee Cronbach in their seminal article \" Construct Validity In Psychological Tests\". They noted the idea that construct validity was not new at that point. Rather, it was a combinations of many different types of validity dealing with theoretical concepts. They proposed the following three steps to evaluate construct validity:\n\nMany psychologists note that an important role of construct validation in psychometrics was that it place more emphasis on theory as opposed to validation. The core issue with validation was that a test could be validated, but that did not necessarily show that it measured the theoretical construct it purported to measure. Construct validity has three aspects or components: the substantive component, structural component, and external component. They are related close to three stages in the test construction process: constitution of the pool of items, analysis and selection of the internal structure of the pool of items, and correlation of test scores with criteria and other variables.\n\nIn the 1970s there was growing debate between theorist who began to see construct validity as the dominant model pushing towards a more unified theory of validity and those who continued to work from multiple validity frameworks. Many psychologists and education researchers saw \"predictive, concurrent, and content validities as essentially \"ad hoc\", construct validity was the whole of validity from a scientific point of view\" In the 1974 version of \"The Standards for Educational and Psychological Testing\" the inter-relatedness of the three different aspects of validity was recognized: \"These aspects of validity can be discussed independently, but only for convenience. They are interrelated operationally and logically; only rarely is one of them alone important in a particular situation\".\n\nIn 1989 Messick presented a new conceptualization of construct validity as a unified and multi-faceted concept. Under this framework, all forms of validity are connected to and are dependent on the quality of the construct. He noted that a unified theory was not his own idea, but rather the culmination of debate and discussion within the scientific community over the preceding decades. There are six aspects of construct validity in Messick's unified theory of construct validity. They examine six items that measure the quality of a test's construct validity:\n\n\nHow construct validity should be properly viewed is still a subject of debate for validity theorists. The core of the difference lies in an epistemological difference between positivist and postpositivist theorists.\n\nEvaluation of construct validity requires that the correlations of the measure be examined in regard to variables that are known to be related to the construct (purportedly measured by the instrument being evaluated or for which there are theoretical grounds for expecting it to be related). This is consistent with the multitrait-multimethod matrix (MTMM) of examining construct validity described in Campbell and Fiske's landmark paper (1959). There are other methods to evaluate construct validity besides MTMM. It can be evaluated through different forms of factor analysis, structural equation modeling (SEM), and other statistical evaluations. It is important to note that a single study does not prove construct validity. Rather it is a continuous process of evaluation, reevaluation, refinement, and development. Correlations that fit the expected pattern contribute evidence of construct validity. Construct validity is a judgment based on the accumulation of correlations from numerous studies using the instrument being evaluated.\n\nMost researchers attempt to test the construct validity before the main research. To do this pilot studies may be utilized. Pilot studies are small scale preliminary studies aimed at testing the feasibility of a full-scale test. These pilot studies establish the strength of their research and allow them to make any necessary adjustments. Another method is the known-groups technique, which involves administering the measurement instrument to groups expected to differ due to known characteristics. Hypothesized relationship testing involves logical analysis based on theory or prior research. Intervention studies are yet another method of evaluating construct validity. Intervention studies where a group with low scores in the construct is tested, taught the construct, and then re-measured can demonstrate a test's construct validity. If there is a significant difference pre-test and post-test, which are analyzed by statistical tests, then this may demonstrate good construct validity.\n\nConvergent and discriminant validity are the two subtypes of validity that make up construct validity. Convergent validity refers to the degree to which two measures of constructs that theoretically should be related, are in fact related. In contrast discriminant validity tests whether concepts or measurements that are supposed to be unrelated are, in fact, unrelated. Take, for example, a construct of general happiness. If a measure of general happiness had convergent validity, then constructs similar to happiness (satisfaction, contentment, cheerfulness, etc.) should relate closely to the measure of general happiness. If this measure has discriminate validity, then constructs that are not supposed to be related to general happiness (sadness, depression, despair, etc.) should not relate to the measure of general happiness. Measures can have one of the subtypes of construct validity and not the other. Using the example of general happiness, a researcher could create an inventory where there is a very high positive correlation between general happiness and contentment, but if there is also a significant positive correlation between happiness and depression, then the measure's construct validity is called into question. The test has convergent validity but not discriminant validity.\n\nLee Cronbach and Paul Meehl (1955) proposed that the development of a nomological net was essential to measurement of a test's construct validity. A nomological network defines a construct by illustrating its relation to other constructs and behaviors. It is a representation of the concepts (constructs) of interest in a study, their observable manifestations and the interrelationship among them. It examines whether the relationships between similar construct are considered with relationships between the observed measures of the constructs. Thorough observation of constructs relationships to each other it can generate new constructs. For example, intelligence and working memory are considered highly related constructs. Through the observation of their underlying components psychologists developed new theoretical constructs such as: controlled attention and short term loading. Creating a nomological net can also make the observation and measurement of existing constructs more efficient by pinpointing errors. Researchers have found that studying the bumps on the human skull (phrenology) are not indicators of intelligence, but volume of the brain is. Removing the theory of phrenology from the nomological net of intelligence and adding the theory of brain mass evolution, constructs of intelligence are made more efficient and more powerful. The weaving of all of these interrelated concepts and their observable traits creates a \"net\" that supports their theoretical concept. For example, in the nomological network for academic achievement, we would expect observable traits of academic achievement (i.e. GPA, SAT, and ACT scores) to relate to the observable traits for studiousness (hours spent studying, attentiveness in class, detail of notes). If they do not then there is a problem with measurement (of academic achievement or studiousness), or with the purported theory of achievement. If they are indicators of one another then the nomological network, and therefore the constructed theory, of academic achievement is strengthened. Although the nomological network proposed a theory of how to strengthen constructs, it doesn't tell us how we can assess the construct validity in a study.\n\nThe multitrait-multimethod matrix (MTMM) is an approach to examining construct validity developed by Campbell and Fiske (1959). This model examines convergence (evidence that different measurement methods of a construct give similar results) and discriminability (ability to differentiate the construct from other related constructs). It measures six traits: the evaluation of convergent validity, the evaluation of discriminant (divergent) validity, trait-method units, multitrait-multimethods, truly different methodologies, and trait characteristics. This design allows investigators to test for: \"convergence across different measures...of the same ‘thing’...and for divergence between measures...of related but conceptually distinct 'things'.\n\nApparent construct validity can be misleading due to a range of problems in hypothesis formulation and experimental design.\n\nAn in-depth exploration of the threats to construct validity is presented in Trochim.\n\n\n"}
{"id": "89246", "url": "https://en.wikipedia.org/wiki?curid=89246", "title": "Curve", "text": "Curve\n\nIn mathematics, a curve (also called a curved line in older texts) is, generally speaking, an object similar to a line but that need not be straight. Thus, a curve is a generalization of a line, in that its curvature need not be zero.\n\nVarious disciplines within mathematics have given the term different meanings depending on the area of study, so the precise meaning depends on context. However, many of these meanings are special instances of the definition which follows. A curve is a topological space which is locally homeomorphic to a line. In everyday language, this means that a curve is a set of points which, near each of its points, looks like a line, up to a deformation. A simple example of a curve is the parabola, shown to the right. A large number of other curves have been studied in multiple mathematical fields.\n\nA closed curve is a curve that forms a path whose starting point is also its ending point—that is, a path from any of its points to the same point.\n\nClosely related meanings include the graph of a function (for example, Phillips curve) and a two-dimensional graph.\n\nInterest in curves began long before they were the subject of mathematical study. This can be seen in numerous examples of their decorative use in art and on everyday objects dating back to prehistoric\ntimes. Curves, or at least their graphical representations, are simple to create, for example by a stick in the sand on a beach.\n\nHistorically, the term \"line\" was used in place of the more modern term \"curve\". Hence the phrases \"straight line\" and \"right line\" were used to distinguish what are today called lines from \"curved lines\". For example, in Book I of Euclid's Elements, a line is defined as a \"breadthless length\" (Def. 2), while a \"straight\" line is defined as \"a line that lies evenly with the points on itself\" (Def. 4). Euclid's idea of a line is perhaps clarified by the statement \"The extremities of a line are points,\" (Def. 3). Later commentators further classified lines according to various schemes. For example:\nThe Greek geometers had studied many other kinds of curves. One reason was their interest in solving geometrical problems that could not be solved using standard compass and straightedge construction.\nThese curves include:\nA fundamental advance in the theory of curves was the advent of analytic geometry in the seventeenth century. This enabled a curve to be described using an equation rather than an elaborate geometrical construction. This not only allowed new curves to be defined and studied, but it enabled a formal distinction to be made between curves that can be defined using algebraic equations, algebraic curves, and those that cannot, transcendental curves. Previously, curves had been described as \"geometrical\" or \"mechanical\" according to how they were, or supposedly could be, generated.\n\nConic sections were applied in astronomy by Kepler.\nNewton also worked on an early example in the calculus of variations. Solutions to variational problems, such as the brachistochrone and tautochrone questions, introduced properties of curves in new ways (in this case, the cycloid). The catenary gets its name as the solution to the problem of a hanging chain, the sort of question that became routinely accessible by means of differential calculus.\n\nIn the eighteenth century came the beginnings of the theory of plane algebraic curves, in general. Newton had studied the cubic curves, in the general description of the real points into 'ovals'. The statement of Bézout's theorem showed a number of aspects which were not directly accessible to the geometry of the time, to do with singular points and complex solutions.\n\nSince the nineteenth century there has not been a separate theory of curves, but rather the appearance of curves as the one-dimensional aspect of projective geometry, and differential geometry; and later topology, when for example the Jordan curve theorem was understood to lie quite deep, as well as being required in complex analysis. The era of the space-filling curves finally provoked the modern definitions of curve.\n\nIn general, a curve is defined through a continuous function formula_1 from an interval of the real numbers into a topological space . Depending on the context, it is either formula_2 or its image formula_3 which is called a curve.\n\nIn general topology, when non-differentiable functions are considered, it is the map formula_2, which is called a curve, because its image may look very differently from what is commonly called a curve. For example, the image of the Peano curve completely fills the square. On the other hand, when one considers curves defined by a differentiable function (or, at least, a piecewise differentiable function), this is commonly the image of the function which is called a curve.\n\nA plane curve is a curve for which formula_22 is the Euclidean plane—these are the examples first encountered—or in some cases the projective plane. A space curve is a curve for which formula_22 is of three dimensions, usually Euclidean space; a skew curve is a space curve which lies in no plane. These definitions of plane, space and skew curves apply also to real algebraic curves, although the above definition of a curve does not apply (a real algebraic curve may be disconnected).\n\nThis definition of curve captures our intuitive notion of a curve as a connected, continuous geometric figure that is \"like\" a line, without thickness and drawn without interruption, although it also includes figures that can hardly be called curves in common usage. For example, the image of a curve can cover a square in the plane (space-filling curve). The image of simple plane curve can have Hausdorff dimension bigger than one (see Koch snowflake) and even positive Lebesgue measure (the last example can be obtained by small variation of the Peano curve construction). The dragon curve is another unusual example.\n\nRoughly speaking a differentiable curve is a curve that is defined as being locally the image of an injective differentiable function formula_1 from an interval of the real numbers into a differentiable manifold , often formula_25\n\nMore precisely, a differentiable curve is a subset of where every point of has a neighborhood such that formula_26 is diffeomorphic to an interval of the real numbers. In other words, a differentiable curve is a differentiable manifold of dimension one.\n\nIf formula_27 is the formula_28-dimensional Euclidean space, and if formula_29 is an injective and continuously differentiable function, then the length of formula_30 is defined as the quantity\nThe length of a curve is independent of the parametrization formula_30.\n\nIn particular, the length formula_33 of the graph of a continuously differentiable function formula_34 defined on a closed interval formula_35 is\n\nMore generally, if formula_37 is a metric space with metric formula_38, then we can define the length of a curve formula_39 by\nwhere the supremum is taken over all formula_41 and all partitions formula_42 of formula_43.\n\nA rectifiable curve is a curve with finite length. A curve formula_39 is called natural (or unit-speed or parametrized by arc length) if for any formula_45 such that formula_46, we have\n\nIf formula_39 is a Lipschitz-continuous function, then it is automatically rectifiable. Moreover, in this case, one can define the speed (or metric derivative) of formula_30 at formula_50 as\nand then show that\n\nWhile the first examples of curves that are met are mostly plane curves (that is, in everyday words, \"curved lines\" in \"two-dimensional space\"), there are obvious examples such as the helix which exist naturally in three dimensions. The needs of geometry, and also for example classical mechanics are to have a notion of curve in space of any number of dimensions. In general relativity, a world line is a curve in spacetime.\n\nIf formula_22 is a differentiable manifold, then we can define the notion of \"differentiable curve\" in formula_22. This general idea is enough to cover many of the applications of curves in mathematics. From a local point of view one can take formula_22 to be Euclidean space. On the other hand, it is useful to be more general, in that (for example) it is possible to define the tangent vectors to formula_22 by means of this notion of curve.\n\nIf formula_22 is a smooth manifold, a \"smooth curve\" in formula_22 is a smooth map\n\nThis is a basic notion. There are less and more restricted ideas, too. If formula_22 is a formula_61 manifold (i.e., a manifold whose charts are formula_62 times continuously differentiable), then a formula_61 curve in formula_22 is such a curve which is only assumed to be formula_61 (i.e. formula_62 times continuously differentiable). If formula_22 is an analytic manifold (i.e. infinitely differentiable and charts are expressible as power series), and formula_2 is an analytic map, then formula_2 is said to be an \"analytic curve\".\n\nA differentiable curve is said to be \"regular\" if its derivative never vanishes. (In words, a regular curve never slows to a stop or backtracks on itself.) Two formula_61 differentiable curves\n\nare said to be \"equivalent\" if there is a bijective formula_61 map\n\nsuch that the inverse map\n\nis also formula_61, and\n\nfor all formula_78. The map formula_79 is called a \"reparametrisation\" of formula_80; and this makes an equivalence relation on the set of all formula_61 differentiable curves in formula_22. A formula_61 \"arc\" is an equivalence class of formula_61 curves under the relation of reparametrisation.\n\nAlgebraic curves are the curves considered in algebraic geometry. A plane algebraic curve is the set of the points of coordinates \"x\", \"y\" such that \"f\"(\"x\", \"y\") = 0, where \"f\" is a polynomial in two variables defined over some field \"F\". Algebraic geometry normally looks not only on points with coordinates in \"F\" but on all the points with coordinates in an algebraically closed field \"K\". If \"C\" is a curve defined by a polynomial \"f\" with coefficients in \"F\", the curve is said to be defined over \"F\". The points of the curve \"C\" with coordinates in a field \"G\" are said to be rational over \"G\" and can be denoted \"C\"(\"G\")). When \"G\" is the field of the rational numbers, one simply talks of \"rational points\". For example, Fermat's Last Theorem may be restated as: \"For n\" > 2, \"every rational point of the Fermat curve of degree n has a zero coordinate\".\n\nAlgebraic curves can also be space curves, or curves in a space of higher dimension, say . They are defined as algebraic varieties of dimension one. They may be obtained as the common solutions of at least polynomial equations in variables. If polynomials are sufficient to define a curve in a space of dimension , the curve is said to be a complete intersection. By eliminating variables (by any tool of elimination theory), an algebraic curve may be projected onto a plane algebraic curve, which however may introduce new singularities such as cusps or double points.\n\nA plane curve may also be completed in a curve in the projective plane: if a curve is defined by a polynomial \"f\" of total degree \"d\", then \"w\"\"f\"(\"u\"/\"w\", \"v\"/\"w\") simplifies to a homogeneous polynomial \"g\"(\"u\", \"v\", \"w\") of degree \"d\". The values of \"u\", \"v\", \"w\" such that \"g\"(\"u\", \"v\", \"w\") = 0 are the homogeneous coordinates of the points of the completion of the curve in the projective plane and the points of the initial curve are those such \"w\" is not zero. An example is the Fermat curve \"u\" + \"v\" = \"w\", which has an affine form \"x\" + \"y\" = 1. A similar process of homogenization may be defined for curves in higher dimensional spaces\n\nImportant examples of algebraic curves are the conics, which are nonsingular curves of degree two and genus zero, and elliptic curves, which are nonsingular curves of genus one studied in number theory and which have important applications to cryptography. Because algebraic curves in fields of characteristic zero are most often studied over the complex numbers, algebraic curves in algebraic geometry may be considered as real surfaces. In particular, the nonsingular complex projective algebraic curves are called Riemann surfaces.\n\n\n"}
{"id": "580297", "url": "https://en.wikipedia.org/wiki?curid=580297", "title": "Diplomatic bag", "text": "Diplomatic bag\n\nA diplomatic bag, also known as a diplomatic pouch, is a container with certain legal protections used for carrying official correspondence or other items between a diplomatic mission and its home government or other diplomatic, consular, or otherwise official entity. The physical concept of a \"diplomatic bag\" is flexible and therefore can take many forms (e.g., a cardboard box, briefcase, duffel bag, large suitcase, crate or even a shipping container). \n\nAdditionally, a diplomatic bag usually has some form of lock and/or tamper-evident seal attached to it in order to deter or detect interference by unauthorized third parties. The most important point is that as long as it is externally marked to show its status, the \"bag\" has diplomatic immunity from search or seizure, as codified in article 27 of the 1961 Vienna Convention on Diplomatic Relations. It may only contain articles intended for official use. It is often escorted by a diplomatic courier, who is similarly immune from arrest and detention.\n\nIn discussions of cryptography, the diplomatic bag is conventionally used as an example of the ultimate secure channel used to exchange keys, codebooks, and other necessarily secret materials. In contemporary practice, diplomatic bags are indeed used for exactly this purpose.\n\n\n\n"}
{"id": "2924436", "url": "https://en.wikipedia.org/wiki?curid=2924436", "title": "Electromagnetic wave equation", "text": "Electromagnetic wave equation\n\nThe electromagnetic wave equation is a second-order partial differential equation that describes the propagation of electromagnetic waves through a medium or in a vacuum. It is a three-dimensional form of the wave equation. The homogeneous form of the equation, written in terms of either the electric field or the magnetic field , takes the form:\n\nwhere \n\nis the speed of light (i.e. phase velocity) in a medium with permeability , and permittivity , and is the Laplace operator. In a vacuum, meters per second, a fundamental physical constant. The electromagnetic wave equation derives from Maxwell's equations. In most older literature, is called the \"magnetic flux density\" or \"magnetic induction\".\n\nIn his 1865 paper titled A Dynamical Theory of the Electromagnetic Field, Maxwell utilized the correction to Ampère's circuital law that he had made in part III of his 1861 paper On Physical Lines of Force. In \"Part VI\" of his 1864 paper titled \"Electromagnetic Theory of Light\", Maxwell combined displacement current with some of the other equations of electromagnetism and he obtained a wave equation with a speed equal to the speed of light. He commented:\n\n\"The agreement of the results seems to show that light and magnetism are affections of the same substance, and that light is an electromagnetic disturbance propagated through the field according to electromagnetic laws.\" \n\nMaxwell's derivation of the electromagnetic wave equation has been replaced in modern physics education by a much less cumbersome method involving combining the corrected version of Ampère's circuital law with Faraday's law of induction.\n\nTo obtain the electromagnetic wave equation in a vacuum using the modern method, we begin with the modern 'Heaviside' form of Maxwell's equations. In a vacuum- and charge-free space, these equations are:\n\nThese are the general Maxwell's equations specialized to the case with charge and current both set to zero.\nTaking the curl of the curl equations gives:\n\nWe can use the vector identity\n\nwhere is any vector function of space. And\n\nwhere is a dyadic which when operated on by the divergence operator yields a vector. Since \n\nthen the first term on the right in the identity vanishes and we obtain the wave equations:\n\nwhere \n\nis the speed of light in free space.\n\nThese relativistic equations can be written in contravariant form as\n\nwhere the electromagnetic four-potential is\n\nwith the Lorenz gauge condition:\n\nand where\n\nis the d'Alembert operator.\n\nThe electromagnetic wave equation is modified in two ways, the derivative is replaced with the covariant derivative and a new term that depends on the curvature appears.\n\nwhere formula_15 is the Ricci curvature tensor and the semicolon indicates covariant differentiation.\n\nThe generalization of the Lorenz gauge condition in curved spacetime is assumed:\n\nLocalized time-varying charge and current densities can act as sources of electromagnetic waves in a vacuum. Maxwell's equations can be written in the form of a wave equation with sources. The addition of sources to the wave equations makes the partial differential equations inhomogeneous.\n\nThe general solution to the electromagnetic wave equation is a linear superposition of waves of the form\n\nfor virtually \"any\" well-behaved function of dimensionless argument , where is the angular frequency (in radians per second), and is the wave vector (in radians per meter).\n\nAlthough the function can be and often is a monochromatic sine wave, it does not have to be sinusoidal, or even periodic. In practice, cannot have infinite periodicity because any real electromagnetic wave must always have a finite extent in time and space. As a result, and based on the theory of Fourier decomposition, a real wave must consist of the superposition of an infinite set of sinusoidal frequencies.\n\nIn addition, for a valid solution, the wave vector and the angular frequency are not independent; they must adhere to the dispersion relation:\n\nwhere is the wavenumber and is the wavelength. The variable can only be used in this equation when the electromagnetic wave is in a vacuum.\n\nThe simplest set of solutions to the wave equation result from assuming sinusoidal waveforms of a single frequency in separable form:\n\nwhere\n\nConsider a plane defined by a unit normal vector \n\nThen planar traveling wave solutions of the wave equations are\n\nwhere is the position vector (in meters).\n\nThese solutions represent planar waves traveling in the direction of the normal vector . If we define the z direction as the direction of . and the x direction as the direction of , then by Faraday's Law the magnetic field lies in the y direction and is related to the electric field by the relation \n\nBecause the divergence of the electric and magnetic fields are zero, there are no fields in the direction of propagation.\n\nThis solution is the linearly polarized solution of the wave equations. There are also circularly polarized solutions in which the fields rotate about the normal vector.\n\nBecause of the linearity of Maxwell's equations in a vacuum, solutions can be decomposed into a superposition of sinusoids. This is the basis for the Fourier transform method for the solution of differential equations. The sinusoidal solution to the electromagnetic wave equation takes the form\n\nwhere\nThe wave vector is related to the angular frequency by\n\nwhere is the wavenumber and is the wavelength.\n\nThe electromagnetic spectrum is a plot of the field magnitudes (or energies) as a function of wavelength.\n\nAssuming monochromatic fields varying in time as formula_30, if one uses Maxwell's Equations to eliminate , the electromagnetic wave equation reduces to the Helmholtz Equation for :\n\nwith \"k = ω/c\" as given above. Alternatively, one can eliminate in favor of to obtain:\n\nA generic electromagnetic field with frequency can be written as a sum of solutions to these two equations. The three-dimensional solutions of the Helmholtz Equation can be expressed as expansions in spherical harmonics with coefficients proportional to the spherical Bessel functions. However, applying this expansion to each vector component of or will give solutions that are not generically divergence-free (), and therefore require additional restrictions on the coefficients.\n\nThe multipole expansion circumvents this difficulty by expanding not or , but or into spherical harmonics. These expansions still solve the original Helmholtz equations for and because for a divergence-free field , . The resulting expressions for a generic electromagnetic field are:\n\nwhere formula_35 and formula_36 are the \"electric multipole fields of order (l, m)\", and formula_37 and formula_38 are the corresponding \"magnetic multipole fields\", and and are the coefficients of the expansion. The multipole fields are given by\n\nwhere \"h(x)\" are the spherical Hankel functions, \"E\" and \"B\" are determined by boundary conditions, and \n\nare vector spherical harmonics normalized so that\n\nThe multipole expansion of the electromagnetic field finds application in a number of problems involving spherical symmetry, for example antennae radiation patterns, or nuclear gamma decay. In these applications, one is often interested in the power radiated in the far-field. In this regions, the and fields asymptote to\n\nThe angular distribution of the time-averaged radiated power is then given by\n\n\n\n\n\n\n"}
{"id": "1309542", "url": "https://en.wikipedia.org/wiki?curid=1309542", "title": "Finite thickness", "text": "Finite thickness\n\nIn formal language theory, in particular in algorithmic learning theory, a class \"C\" of languages has finite thickness if every string is contained in at most finitely many languages in \"C\". This condition was introduced by Dana Angluin as a sufficient condition for \"C\" being identifiable in the limit.\n\nGiven a language \"L\" and an indexed class \"C\" = { \"L\", \"L\", \"L\", ... } of languages, a member language \"L\" ∈ \"C\" is called a minimal concept of \"L\" within \"C\" if \"L\" ⊆ \"L\", but not \"L\" ⊊ \"L\" ⊆ \"L\" for any \"L\" ∈ \"C\".\nThe class \"C\" is said to satisfy the MEF-condition if every finite subset \"D\" of a member language \"L\" ∈ \"C\" has a minimal concept \"L\" ⊆ \"L\". Symmetrically, \"C\" is said to satisfy the MFF-condition if every nonempty finite set \"D\" has at most finitely many minimal concepts in \"C\". Finally, \"C\" is said to have M-finite thickness if it satisfies both the MEF- and the MFF-condition.\n\nFinite thickness implies M-finite thickness. However, there are classes that are of M-finite thickness but not of finite thickness (for example, any class of languages \"C\" = { \"L\", \"L\", \"L\", ... } such that \"L\" ⊆ \"L\" ⊆ \"L\" ⊆ ...).\n"}
{"id": "138214", "url": "https://en.wikipedia.org/wiki?curid=138214", "title": "Five lemma", "text": "Five lemma\n\nIn mathematics, especially homological algebra and other applications of abelian category theory, the five lemma is an important and widely used lemma about commutative diagrams.\nThe five lemma is not only valid for abelian categories but also works in the category of groups, for example.\n\nThe five lemma can be thought of as a combination of two other theorems, the four lemmas, which are dual to each other.\n\nConsider the following commutative diagram in any abelian category (such as the category of abelian groups or the category of vector spaces over a given field) or in the category of groups.\n\nThe five lemma states that, if the rows are exact, \"m\" and \"p\" are isomorphisms, \"l\" is an epimorphism, and \"q\" is a monomorphism, then \"n\" is also an isomorphism.\n\nThe two four-lemmas state:<br>\n(1) If the rows in the commutative diagram\n\nare exact and \"m\" and \"p\" are epimorphisms and \"q\" is a monomorphism, then \"n\" is an epimorphism.\n\n(2) If the rows in the commutative diagram\n\nare exact and \"m\" and \"p\" are monomorphisms and \"l\" is an epimorphism, then \"n\" is a monomorphism.\n\nThe method of proof we shall use is commonly referred to as diagram chasing. We shall prove the five lemma by individually proving each of the two four lemmas.\n\nTo perform diagram chasing, we assume that we are in a category of modules over some ring, so that we may speak of \"elements\" of the objects in the diagram and think of the morphisms of the diagram as \"functions\" (in fact, homomorphisms) acting on those elements.\nThen a morphism is a monomorphism if and only if it is injective, and it is an epimorphism if and only if it is surjective.\nSimilarly, to deal with exactness, we can think of kernels and images in a function-theoretic sense.\nThe proof will still apply to any (small) abelian category because of Mitchell's embedding theorem, which states that any small abelian category can be represented as a category of modules over some ring.\nFor the category of groups, just turn all additive notation below into multiplicative notation, and note that commutativity of abelian group is never used.\n\nSo, to prove (1), assume that \"m\" and \"p\" are surjective and \"q\" is injective.\n\nThen, to prove (2), assume that \"m\" and \"p\" are injective and is surjective.\n\nCombining the two four lemmas now proves the entire five lemma.\n\nThe five lemma is often applied to long exact sequences: when computing homology or cohomology of a given object, one typically employs a simpler subobject whose homology/cohomology is known, and arrives at a long exact sequence which involves the unknown homology groups of the original object. This alone is often not sufficient to determine the unknown homology groups, but if one can compare the original object and sub object to well-understood ones via morphisms, then a morphism between the respective long exact sequences is induced, and the five lemma can then be used to determine the unknown homology groups.\n\n\n"}
{"id": "25116039", "url": "https://en.wikipedia.org/wiki?curid=25116039", "title": "Free Poisson distribution", "text": "Free Poisson distribution\n\nIn the mathematics of free probability theory, the free Poisson distribution is a counterpart of the Poisson distribution in conventional probability theory.\n\nThe free Poisson distribution with jump size formula_1 and rate formula_2 arises in free probability theory as the limit of repeated free convolution\n\nas \"N\" → ∞.\n\nIn other words, let formula_4 be random variables so that formula_4 has value formula_1 with probability formula_7 and value 0 with the remaining probability. Assume also that the family formula_8 are freely independent. Then the limit as formula_9 of the law of formula_10\nis given by the Free Poisson law with parameters formula_11.\n\nThis definition is analogous to one of the ways in which the classical Poisson distribution is obtained from a (classical) Poisson process. \n\nThe measure associated to the free Poisson law is given by\n\nwhere\n\nand has support formula_14.\n\nThis law also arises in random matrix theory as the Marchenko–Pastur law. Its free cumulants are equal to formula_15.\n\nWe give values of some important transforms of the free Poisson law; the computation can be found in e.g. in the book \"Lectures on the Combinatorics of Free Probability\" by A. Nica and R. Speicher\n\nThe R-transform of the free Poisson law is given by\n\nThe Cauchy transform (which is the negative of the Stieltjes transformation) is given by\n\nThe S-transform is given by\n\nin the case that formula_19.\n"}
{"id": "35690430", "url": "https://en.wikipedia.org/wiki?curid=35690430", "title": "Fuzzy extractor", "text": "Fuzzy extractor\n\nFuzzy extractors are a method that allows to use biometric data as inputs to standard cryptographic techniques for biometric security. \"Fuzzy\", in this context, refers to the fact that the fixed values required for cryptography will be extracted from values close but not identical to the original one, without compromising the security required. One application is to encrypt and authenticate users records, using the biometric inputs of the user as a key.\n\nFuzzy extractors are a biometric tool that allows to authenticate a user using for the key a biometric template constructed from his biometric data. They extract a uniform and random string formula_1 from an input formula_2 with a tolerance for noise. If the input changes to formula_3 but is still close to formula_2, the same string formula_1 will be re-constructed. To achieve this, during the initial computation of formula_1 the process also outputs a helper string formula_7which will be stored to recover formula_1 later and can be made public without compromising the security of formula_1. The security of the process is ensured also when an adversary modifies formula_7. Once the fixed string formula_1has been calculated, it can be used for example for key agreement between a user and a server based only on a biometric input.\n\nHistorically, the first biometric system of this kind was designed by Juels and Wattenberg and was called \"Fuzzy commitment\", where the cryptographic key is decommitted using biometric data. Later, Juels and Sudan came up with Fuzzy vault schemes which are order invariant for the fuzzy commitment scheme but uses a Reed–Solomon code. Codeword is evaluated by polynomial and the secret message is inserted as the coefficients of the polynomial. The polynomial is evaluated for different values of a set of features of the biometric data. So Fuzzy commitment and Fuzzy Vault were precursors to Fuzzy extractors.\n\nThis description is based on the papers \"Fuzzy Extractors: A Brief Survey of Results from 2004 to 2006\" and \"Fuzzy Extractors: How to Generate Strong Keys from Biometrics and Other Noisy Data\" by Yevgeniy Dodis, Rafail Ostrovsky, Leonid Reyzin and Adam Smith\n\nIn order for fuzzy extractors to generate strong keys from Biometrics and other Noisy Data, cryptography paradigms will be applied to this biometric data which means they must be allow to \n\n(1) Limit the number of assumptions required about the content of the biometric data (these data comes from a variety of sources and in order to avoid an adversary to exploit them, it's best to assume the input is unpredictable) \n\n(2) Apply usual cryptographic techniques from the input. (for this fuzzy extractors convert biometric data into secret, uniformly random and reliably reproducible random string).\n\nThe paper \"Fuzzy Extractors: How to Generate Strong Keys from Biometrics and Other Noisy Data\" by Yevgeniy Dodis, Rafail Ostrovsky, Leonid Reyzin and Adam Smith shows that these techniques can also have other broader applications for other type of noisy inputs such as approximative data from human memory, images used as passwords, keys from quantum channel. According to the Differential Privacy paper by Cynthia Dwork (ICALP 2006), fuzzy extractors also have applications in the proof of impossibility of the strong notions of privacy for statistical databases.\n\nPredictability indicates the probability that adversary can guess a secret key. Mathematically speaking, the predictability of a random variable formula_12 is formula_13.\n\nFor example, given a pair of random variable formula_12 and formula_15, if the adversary knows formula_16 of formula_15, then the predictability of formula_12 will be formula_19. So, an adversary can predict formula_12 with formula_21. We use the average over formula_15 as it is not under adversary control, but since knowing formula_16 makes the prediction of formula_12 adversarial, we take the worst case over formula_12.\n\nMin-entropy indicates the worst-case entropy. Mathematically speaking, it is defined as formula_26 .\n\nA random variable with a min-entropy at least of formula_27 is called a formula_27-source.\n\nStatistical distance is a measure of distinguishability. Mathematically speaking, it is expressed for two probability distributions formula_12 and formula_15 as formula_31 = formula_32. In any system, if formula_12 is replaced by formula_15, it will behave as the original system with a probability at least of formula_35.\n\nSetting formula_36 as a strong randomness extractor. The randomized function Ext: formula_37 with randomness of length formula_38 is a formula_39 strong extractor if for all formula_27-sources formula_41 on formula_42 where formula_43 is independent of formula_41. \n\nThe output of the extractor is a key generated from formula_45 with the seed formula_46. It behaves independently of other parts of the system with the probability of formula_47. Strong extractors can extract at most formula_48 bits from an arbitrary formula_49-source.\n\nSecure sketch makes it possible to reconstruct noisy input, so that if the input is formula_2 and the sketch is formula_51, given formula_51 and a value formula_3close to formula_2, formula_2 can be recovered. But the sketch formula_51 must not reveal information about formula_2, in order to keep it secure.\n\nIf formula_58 is a metric space with the distance function dis, Secure sketch recovers the string formula_59 from any close string formula_60 without disclosing formula_2.\n\nAn formula_62 secure sketch is a pair of efficient randomized procedures (the Sketch noted SS, the Recover noted Rec) such that : \n\n(1) The sketching procedure SS applied on input formula_63 returns a string formula_64.\n\nThe recovery procedure Rec uses as input two elements formula_65 and formula_66.\n\n(2) Correctness: If formula_67 then formula_68.\n\n(3) Security: For any formula_27-source over formula_70, the min-entropy of formula_41 given formula_51 is high: \n\nfor any formula_73, if formula_74, then formula_75.\n\nFuzzy extractors do not recover the original input but generate a string formula_1 (which is close to uniform) from formula_2 and allow its subsequent reproduction (using helper string formula_7) given any formula_3 close to formula_2. Strong extractors are a special case of fuzzy extractors when formula_81 = 0 and formula_82.\n\nAn formula_83 fuzzy extractor is a pair of efficient randomized procedures (Gen – Generate and Rep – Reproduce) such that:\n\n(1) Gen, given formula_63, outputs an extracted string formula_85 and a helper string formula_86.\n\n(2) Correctness: If formula_87 and formula_88, then formula_89.\n\n(3) Security: For all m-sources formula_41 over formula_70, the string formula_1 is nearly uniform even given formula_7, So formula_94, then formula_95.\n\nSo Fuzzy extractors output almost uniform random sequences of bits which are a prerequisite for using cryptographic applications (as secret keys). Since the output bits are slightly non-uniform, there's a risk of a decreased security, but the distance from an uniform distribution is no more than formula_96 and as long as this distance is sufficiently small, the security will remains adequate.\n\nSecure sketches can be used to construct fuzzy extractors. Like applying SS to formula_2 to obtain formula_51 and strong extractor Ext with randomness formula_99 to formula_2 to get formula_1. formula_102 can be stored as helper string formula_7. formula_1 can be reproduced by formula_3 and formula_106. formula_107 can recover formula_2 and formula_109 can reproduce formula_1.\nFollowing Lemma formalize this.\n\nAssume (SS,Rec) is an formula_111 secure sketch and let Ext be an average-case formula_112 strong extractor. Then the following (Gen, Rep) is an formula_113 fuzzy extractor:\n(1) Gen formula_114: set formula_115 and output formula_116.\n(2) Rep formula_117: recover formula_118 and output formula_119.\n\nProof: From the definition of secure sketch (Definition 2), \nformula_120. And since Ext is an average-case formula_121-strong extractor. formula_122\n\nIf (SS,Rec) is an formula_111 secure sketch and Ext is an formula_124 strong extractor, then the above construction (Gen,Rep) is a formula_125 fuzzy extractor.\n\nThe reference paper \"Fuzzy Extractors: How to Generate Strong Keys from Biometrics and Other Noisy Data\" by Yevgeniy Dodis, Rafail Ostrovsky, Leonid Reyzin and Adam Smith (2008) includes many generic combinatorial bounds on secure sketches and fuzzy extractors\n\nDue to their error tolerant properties, a secure sketches can be treated, analyzed, and constructed like a formula_126 general error correcting code or formula_127 for linear codes, where formula_128 is the length of codewords, formula_129 is the length of the message to be codded, formula_130 is the distance between codewords, and formula_131 is the alphabet. If formula_132 is the universe of possible words then it may be possible to find an error correcting code formula_133 that has a unique codeword formula_134 for every formula_135 and have a Hamming distance of formula_136. The first step for constructing a secure sketch is determining the type of errors that will likely occur and then choosing a distance to measure.\n\nWhen there is no risk of data being deleted and only of it being corrupted then the best measurement to use for error correction is the Hamming distance. There are two common constructions for correcting Hamming errors depending on whether the code is linear or not. Both constructions start with an error correcting code that has a distance of formula_137 where formula_138 is the number of tolerated errors.\n\nWhen using a formula_139 general code, assign a uniformly random codeword formula_134 to each formula_141, then let formula_142 which is the shift needed to change formula_143 into formula_141. To fix errors in formula_145 subtract formula_146 from formula_145 then correct the errors in the resulting incorrect codeword to get formula_143 and finally add formula_146 to formula_143 to get formula_141. This means formula_152. This construction can achieve the best possible tradeoff between error tolerance and entropy loss when formula_153 and a Reed–Solomon code is used resulting in an entropy loss of formula_154. The only way to improve upon would be to find a code better than Reed–Solomon.\n\nWhen using a formula_155 linear code let the formula_156 be the syndrome of formula_141. To correct formula_145 find a vector formula_159 such that formula_160, then formula_161.\n\nWhen working with a very large alphabet or very long strings resulting in a very large universe formula_162, it may be more efficient to treat formula_141 and formula_145 as sets and look at set differences to correct errors. To work with a large set formula_141 it is useful to look at its characteristic vector formula_166, which is a binary vector of length formula_128 that has a value of 1 when an element formula_168 and formula_169, or 0 when formula_170. The best way to decrease the size of a secure sketch when formula_128 is large is make formula_129 large since the size is determined by formula_173. A good code to base this construction on is a formula_174 BCH code where formula_175 and formula_176 so formula_177, it is also useful that BCH codes can be decode in sub-linear time.\n\nLet formula_178. To correct formula_145 first find formula_180, then find a set v where formula_181, finally compute the symmetric difference to get formula_182. While this is not the only construction than can be used to set the difference, it is the easiest one.\n\nWhen data can be corrupted or deleted, the best measurement to use is edit distance. To make a construction based on edit distance, the easiest is to start with a construction for set difference or hamming distance as an intermediate correction step and then build the edit distance construction around that.\n\nThere are many other types of errors and distances that can be used to model other situations. Most of these other possible constructions are built upon simpler constructions, like edit distance constructions.\n\nIt can be shown that the error-tolerance of a secure sketch can be improved by applying a probabilistic method to error correction and only requesting errors to be correctable with a high probability. This allows to exceed the Plotkin bound which limits to correcting formula_183 errors, and to approach Shannon’s bound allowing for nearly formula_184 corrections. To achieve this enhanced error correction, a less restrictive error distribution model must be used.\n\nFor this most restrictive model use a BSCformula_185 to create a formula_145 that a probability formula_187 at each position in formula_145 that the bit received is wrong. This model can show that entropy loss is limited to formula_189, where formula_190 is the binary entropy function, and if min-entropy formula_191 then formula_192 errors can be tolerated, for some constant formula_193.\n\nFor this model errors do not have a known distribution and can be from an adversary, the only constraints are formula_194 and that a corrupted word depends only on the input formula_141 and not on the secure sketch. It can be shown for this error model that there will never be more than formula_196 errors since this model can account for all complex noise processes, meaning that Shannon’s bound can be reached, to do this a random permutation is prepended to the secure sketch that will reduce entropy loss.\n\nThis differs from the input dependent model by having errors that depend on both the input formula_141 and the secure sketch, and an adversary is limited to polynomial time algorithms for introducing errors. Since algorithms that can run in better than polynomial time are not currently feasible in the real world, then a positive result using this error model would guarantee that any errors can be fixed. This is the least restrictive model the only known way to approach Shannon’s bound is to use list-decodable codes although this may not always be useful in practice since returning a list instead of a single codeword may not always be acceptable.\n\nIn general a secure system attempts to leak as little information as possible to an adversary. In the case of biometrics if information about the biometric reading is leaked the adversary may be able to learn personal information about a user. For example an adversary notices that there is a certain pattern in the helper strings that implies the ethnicity of the user. We can consider this additional information a function formula_198. If an adversary were to learn a helper string, it must be ensured that, from this data he can not infer any data about the person from which the biometric reading was taken.\n\nIdeally the helper string formula_199 would reveal no information about the biometric input formula_141. This is only possible when every subsequent biometric reading formula_145 is identical to the original formula_141. In this case there is actually no need for the helper string, so it is easy to generate a string that is in no way correlated to formula_141. \n\nSince it is desirable to accept biometric input formula_145 similar to formula_141 the helper string formula_199 must be somehow correlated. The more different formula_141 and formula_145 are allowed to be, the more correlation there will be between formula_199 and formula_141, the more correlated they are the more information formula_199 reveals about formula_141. We can consider this information to be a function formula_198. The best possible solution is to make sure the adversary can't learn anything useful from the helper string.\n\nA probabilistic map formula_214 hides the results of functions with a small amount of leakage formula_215. The leakage is the difference in probability two adversaries have of guessing some function when one knows the probabilistic map and one does not. Formally:\n\nIf the function formula_217 is a probabilistic map, then even if an adversary knows both the helper string formula_199 and the secret string formula_1 they are only negligibly more likely figure something out about the subject as if they knew nothing. The string formula_220 is supposed to kept secret, so even if it is leaked (which should be very unlikely) the adversary can still figure out nothing useful about the subject, as long as formula_96 is small. We can consider formula_222 to be any correlation between the biometric input and some physical characteristic of the person. Setting formula_223 in the above equation changes it to:\n\nThis means that if one adversary formula_225 has formula_116 and a second adversary formula_227 knows nothing, their best guesses at formula_222 are only formula_96 apart.\n\nUniform fuzzy extractors are a special case of fuzzy extractors, where the output formula_230 of formula_217 are negligibly different from strings picked from the uniform distribution, i.e. formula_232\n\nSince secure sketches imply fuzzy extractors, constructing a uniform secure sketch allows for the easy construction of a uniform fuzzy extractor. In a uniform secure sketch the sketch procedure formula_233 is a randomness extractor formula_234. Where formula_141 is the biometric input and formula_236 is the random seed. Since randomness extractors output a string that appears to be from a uniform distribution they hide all the information about their input.\n\nExtractor sketches can be used to construct formula_237-fuzzy perfectly one-way hash functions. When used as a hash function the input formula_141 is the object you want to hash. The formula_239 that formula_240 outputs is the hash value. If one wanted to verify that a formula_145 within formula_81 from the original formula_2, they would verify that formula_244. formula_237-fuzzy perfectly one-way hash functions are special hash functions where they accept any input with at most formula_196 errors, compared to traditional hash functions which only accept when the input matches the original exactly. Traditional cryptographic hash functions attempt to guarantee that is it is computationally infeasible to find two different inputs that hash to the same value. Fuzzy perfectly one-way hash functions make an analogous claim. They make it computationally infeasible two find two inputs, that are more than formula_196 Hamming distance apart and hash to the same value.\n\nAn active attack could be one where the adversary can modify the helper string formula_199. If the adversary is able to change formula_199 to another string that is also acceptable to the reproduce function formula_250, it causes formula_250 to output an incorrect secret string formula_252. Robust fuzzy extractors solve this problem by allowing the reproduce function to fail, if a modified helper string is provided as input.\n\nOne method of constructing robust fuzzy extractors is to use hash functions. This construction requires two hash functions formula_253 and formula_254. The formula_217 functions produces the helper string formula_199 by appending the output of a secure sketch formula_257 to the hash of both the reading formula_141 and secure sketch formula_146. It generates the secret string formula_220 by applying the second hash function to formula_141 and formula_146. Formally:\nformula_263\n\nThe reproduce function formula_250 also makes use of the hash functions formula_253 and formula_254. In addition to verifying the biometric input is similar enough to the one recovered using the formula_267 function, it also verifies that hash in the second part of formula_199 was actually derived from formula_141 and formula_146. If both of those conditions are met it returns formula_220 which is itself the second hash function applied to formula_141 and formula_146. Formally:\n\nformula_274 Get formula_275 and formula_276 from formula_277\nIf formula_278 and formula_279 then formula_280 else formula_281\nIf formula_199 has been tampered with it will be obvious because, formula_283 will output fail with very high probability. To cause the algorithm accept a different formula_199 an adversary would have to find a formula_285 such that formula_286. Since hash function are believed to be one way functions, it is computationally infeasible to find such a formula_285.\nSeeing formula_199 would provide the adversary with no useful information. Since, again, hash function are one way functions, it is computationally infeasible for the adversary to reverse the hash function and figure out formula_141. Part of formula_199 is the secure sketch, but by definition the sketch reveals negligible information about its input. Similarly seeing formula_220(even though it should never see it) would provide the adversary with no useful information as the adversary wouldn't be able to reverse the hash function and see the biometric input.\n\n"}
{"id": "4325474", "url": "https://en.wikipedia.org/wiki?curid=4325474", "title": "Geometric analysis", "text": "Geometric analysis\n\nGeometric analysis is a mathematical discipline at the interface of differential geometry and differential equations.\n\nIt includes both the use of geometrical methods in the study of partial differential equations (when it is also known as \"geometric PDE\"), and the application of the theory of partial differential equations to geometry. It incorporates problems involving curves and surfaces, or domains with curved boundaries, but also the study of Riemannian manifolds in arbitrary dimension. The calculus of variations is sometimes regarded as part of geometric analysis, because differential equations arising from variational principles have a strong geometric content. Geometric analysis also includes global analysis, which concerns the study of differential equations on manifolds, and the relationship between differential equations and topology.\n\n"}
{"id": "3216491", "url": "https://en.wikipedia.org/wiki?curid=3216491", "title": "Hamburger moment problem", "text": "Hamburger moment problem\n\nIn mathematics, the Hamburger moment problem, named after Hans Ludwig Hamburger, is formulated as follows: given a sequence { \"m\" : \"n\" = 1, 2, 3, ... }, does there exist a positive Borel measure \"μ\" (for instance, the cumulative distribution function of a random variable) on the real line such that\n\nIn other words, an affirmative answer to the problem means that { \"m\" : \"n\" = 0, 1, 2, ... } is the sequence of moments of some positive Borel measure \"μ\".\n\nThe Stieltjes moment problem, Vorobyev moment problem, and the Hausdorff moment problem are similar but replace the real line by formula_2 (Stieltjes and Vorobyev; but Vorobyev formulates the problem in the terms of matrix theory), or a bounded interval (Hausdorff).\n\nThe Hamburger moment problem is solvable (that is, {\"m\"} is a sequence of moments) if and only if the corresponding Hankel kernel on the nonnegative integers\n\nis positive definite, i.e.,\n\nfor an arbitrary sequence {\"c\"} of complex numbers with finite support (i.e. \n\"c\" = 0 except for finitely many values of \"j\").\n\nFor the \"only if\" part of the claims simply note that\n\nwhich is non-negative if formula_6 is non-negative.\n\nWe sketch an argument for the converse. Let Z be the nonnegative integers and \"F\"(Z) denote the family of complex valued sequences with finite support. The positive Hankel kernel \"A\" induces a (possibly degenerate) sesquilinear product on the family of complex valued sequences with finite support. This in turn gives a Hilbert space\n\nwhose typical element is an equivalence class denoted by [\"f\"].\n\nLet \"e\" be the element in \"F\"(Z) defined by \"e\"(\"m\") = \"δ\". One notices that\n\nTherefore, the \"shift\" operator \"T\" on formula_9, with \"T\"[\"e\"] = [\"e\"], is symmetric.\n\nOn the other hand, the desired expression\n\nsuggests that \"μ\" is the spectral measure of a self-adjoint operator. (More precisely stated, \"μ\" is the spectral measure for an operator formula_11 defined below and the vector [1], ). If we can find a \"function model\" such that the symmetric operator \"T\" is multiplication by \"x\", then the spectral resolution of a self-adjoint extension of \"T\" proves the claim.\n\nA function model is given by the natural isomorphism from \"F\"(Z) to the family of polynomials, in one single real variable and complex coefficients: for \"n\" ≥ 0, identify \"e\" with \"x\". In the model, the operator \"T\" is multiplication by \"x\" and a densely defined symmetric operator. It can be shown that \"T\" always has self-adjoint extensions. Let\n\nbe one of them and \"μ\" be its spectral measure. So\n\nOn the other hand,\n\nFor an alternative proof of the existence that only uses Stieltjes integrals, see also , in particular theorem 3.2. \n\nThe solutions form a convex set, so the problem has either infinitely many solutions or a unique solution.\n\nConsider the (\"n\" + 1)×(\"n\" + 1) Hankel matrix\n\nPositivity of \"A\" means that for each \"n\", det(Δ) ≥ 0. If det(Δ) = 0, for some \"n\", then\n\nis finite-dimensional and \"T\" is self-adjoint. So in this case the solution to the Hamburger moment problem is unique and \"μ\", being the spectral measure of \"T\", has finite support.\n\nMore generally, the solution is unique if there are constants \"C\" and \"D\" such that for all \"n\", |m|≤ \"CD\"\"n\"! . This follows from the more general Carleman's condition.\n\nThere are examples where the solution is not unique, see e.g. .\n\nOne can see that the Hamburger moment problem is intimately related to orthogonal polynomials on the real line. The Gram–Schmidt procedure gives a basis of orthogonal polynomials in which the operator: formula_17 has a tridiagonal \"Jacobi matrix representation\". This in turn leads to a \"tridiagonal model\" of positive Hankel kernels.\n\nAn explicit calculation of the Cayley transform of \"T\" shows the connection with what is called the \"Nevanlinna class\" of analytic functions on the left half plane. Passing to the non-commutative setting, this motivates \"Krein's formula\" which parametrizes the extensions of partial isometries.\n\nThe cumulative distribution function and the probability density function can often be found by applying the inverse Laplace transform to the moment generating function\nprovided that this function converges.\n\n"}
{"id": "56874", "url": "https://en.wikipedia.org/wiki?curid=56874", "title": "Horizontal line test", "text": "Horizontal line test\n\nIn mathematics, the horizontal line test is a test used to determine whether a function is injective (i.e., one-to-one).\n\nA \"horizontal line\" is a straight, flat line that goes from left to right. Given a function formula_1 (i.e. from the real numbers to the real numbers), we can decide if it is injective by looking at horizontal lines that intersect the function's graph. If any horizontal line formula_2 intersects the graph in more than one point, the function is not injective. To see this, note that the points of intersection have the same y-value (because they lie on the line formula_2) but different x values, which by definition means the function cannot be injective.\n\nVariations of the horizontal line test can be used to determine whether a function is surjective or bijective:\n\nConsider a function formula_4 with its corresponding graph as a subset of the Cartesian product formula_5. Consider the horizontal lines in formula_5 :formula_7. The function \"f\" is injective if and only if each horizontal line intersects the graph at most once. In this case the graph is said to pass the horizontal line test. If any horizontal line intersects the graph more than once, the function fails the horizontal line test and is not injective.\n\n"}
{"id": "36237048", "url": "https://en.wikipedia.org/wiki?curid=36237048", "title": "Kneser's theorem (combinatorics)", "text": "Kneser's theorem (combinatorics)\n\nIn mathematics, Kneser's theorem is an inequality among the sizes of certain sumsets in abelian groups. It belongs to the field of additive combinatorics, and is named after Martin Kneser, who published it in 1953. It may be regarded as an extension of the Cauchy–Davenport theorem, which also concerns sumsets in groups but is restricted to groups whose order is a prime number.\n\nLet \"G\" be an abelian group and \"A\", \"B\" finite non-empty subsets. If |\"A\"| + |\"B\"| ≤ |\"G\"| then there is a finite subgroup \"H\" of \"G\" such that\n\nThe subgroup \"H\" can be taken to be the \"stabiliser\" of \"A\"+\"B\"\n\n"}
{"id": "285773", "url": "https://en.wikipedia.org/wiki?curid=285773", "title": "Limit of a sequence", "text": "Limit of a sequence\n\nIn mathematics, the limit of a sequence is the value that the terms of a sequence \"tend to\". If such a limit exists, the sequence is called convergent. A sequence which does not converge is said to be divergent. The limit of a sequence is said to be the fundamental notion on which the whole of analysis ultimately rests.\n\nLimits can be defined in any metric or topological space, but are usually first encountered in the real numbers.\n\nThe Greek philosopher Zeno of Elea is famous for formulating paradoxes that involve limiting processes.\n\nLeucippus, Democritus, Antiphon, Eudoxus and Archimedes developed the method of exhaustion, which uses an infinite sequence of approximations to determine an area or a volume. Archimedes succeeded in summing what is now called a geometric series.\n\nNewton dealt with series in his works on \"Analysis with infinite series\" (written in 1669, circulated in manuscript, published in 1711), \"Method of fluxions and infinite series\" (written in 1671, published in English translation in 1736, Latin original published much later) and \"Tractatus de Quadratura Curvarum\" (written in 1693, published in 1704 as an Appendix to his \"Optiks\"). In the latter work, Newton considers the binomial expansion of (\"x\" + \"o\") which he then linearizes by \"taking limits\" (letting \"o\" → 0).\n\nIn the 18th century, mathematicians such as Euler succeeded in summing some \"divergent\" series by stopping at the right moment; they did not much care whether a limit existed, as long as it could be calculated. At the end of the century, Lagrange in his \"Théorie des fonctions analytiques\" (1797) opined that the lack of rigour precluded further development in calculus. Gauss in his etude of hypergeometric series (1813) for the first time rigorously investigated under which conditions a series converged to a limit.\n\nThe modern definition of a limit (for any ε there exists an index \"N\" so that ...) was given by Bernhard Bolzano (\"Der binomische Lehrsatz\", Prague 1816, little noticed at the time) and by Karl Weierstrass in the 1870s.\n\nIn the real numbers, a number formula_1 is the limit of the sequence formula_2 if the numbers in the sequence become closer and closer to formula_1 and not to any other number.\n\n\n\nWe call formula_20 the limit of the sequence formula_2 if the following condition holds:\nIn other words, for every measure of closeness formula_26, the sequence's terms are eventually that close to the limit. The sequence formula_2 is said to converge to or tend to the limit formula_20, written formula_29 or formula_30.\n\nSymbolically, this is:\n\nIf a sequence converges to some limit, then it is convergent; otherwise it is divergent.\n\nLimits of sequences behave well with respect to the usual arithmetic operations. If formula_32 and formula_33, then formula_34, formula_35 and, if neither \"b\" nor any formula_36 is zero, formula_37.\n\nFor any continuous function \"f\", if formula_29 then formula_39. In fact, any real-valued function \"f\" is continuous if and only if it preserves the limits of sequences (though this is not necessarily true when using more general notions of continuity).\n\nSome other important properties of limits of real sequences include the following (provided, in each equation below, that the limits on the right exist).\n\n\nThese properties are extensively used to prove limits without the need to directly use the cumbersome formal definition. Once proven that \nformula_54 it becomes easy to show that formula_55, (formula_56), using the properties above.\n\nA sequence formula_2 is said to tend to infinity, written formula_58 or formula_59 if, for every \"K\", there is an \"N\" such that, for every formula_24, formula_61; that is, the sequence terms are eventually larger than any fixed \"K\". Similarly, formula_62 if, for every \"K\", there is an \"N\" such that, for every formula_24, formula_64. If a sequence tends to infinity, or to minus infinity, then it is divergent (however, a divergent sequence need not tend to plus or minus infinity: take for example formula_65).\n\nA point formula_20 of the metric space formula_67 is the limit of the sequence formula_2 if, for all formula_22, there is an formula_23 such that, for every formula_24, formula_72. This coincides with the definition given for real numbers when formula_73 and formula_74.\n\nFor any continuous function \"f\", if formula_29 then formula_39. In fact, a function \"f\" is continuous if and only if it preserves the limits of sequences.\n\nLimits of sequences are unique when they exist, as distinct points are separated by some positive distance, so for formula_26 less than half this distance, sequence terms cannot be within a distance formula_26 of both points.\n\nA point \"x\" of the topological space (\"X\", τ) is a limit of the sequence (\"x\") if, for every neighbourhood \"U\" of \"x\", there is an \"N\" such that, for every formula_24, formula_80. This coincides with the definition given for metric spaces if (\"X\",\"d\") is a metric space and formula_81 is the topology generated by \"d\".\n\nA limit of a sequence of points formula_82 in a topological space \"T\" is a special case of a limit of a function: the domain is formula_83 in the space formula_84 with the induced topology of the affinely extended real number system, the range is \"T\", and the function argument \"n\" tends to +∞, which in this space is a limit point of formula_83.\n\nIf \"X\" is a Hausdorff space then limits of sequences are unique where they exist. Note that this need not be the case in general; in particular, if two points \"x\" and \"y\" are topologically indistinguishable, any sequence that converges to \"x\" must converge to \"y\" and vice versa.\n\nA Cauchy sequence is a sequence whose terms ultimately become arbitrarily close together, after sufficiently many initial terms have been discarded. The notion of a Cauchy sequence is important in the study of sequences in metric spaces, and, in particular, in real analysis. One particularly important result in real analysis is the \"Cauchy criterion for convergence of sequences\": A sequence of real numbers is convergent if and only if it is a Cauchy sequence. This remains true in other complete metric spaces.\n\nThe definition of the limit using the hyperreal numbers formalizes the intuition that for a \"very large\" value of the index, the corresponding term is \"very close\" to the limit. More precisely, a real sequence formula_2 tends to \"L\" if for every infinite hypernatural \"H\", the term \"x\" is infinitely close to \"L\", i.e., the difference \"x\" − \"L\" is infinitesimal. Equivalently, \"L\" is the standard part of \"x\"\n\nThus, the limit can be defined by the formula\n\nwhere the limit exists if and only if the righthand side is independent of the choice of an infinite \"H\".\n\n\n\n"}
{"id": "47738065", "url": "https://en.wikipedia.org/wiki?curid=47738065", "title": "List of International Congresses of Mathematicians Plenary and Invited Speakers", "text": "List of International Congresses of Mathematicians Plenary and Invited Speakers\n\nThis is a list of International Congresses of Mathematicians Plenary and Invited Speakers. Being invited to talk at an ICM has been called \"the equivalent, in this community, of an induction to a hall of fame.\" (The current list of Plenary and Invited Speakers presented here is based on the ICM's post-WW II terminology, in which the one-hour speakers in the morning sessions are called \"Plenary Speakers\" and the other speakers (in the afternoon sessions) whose talks are included in the ICM published proceedings are called \"Invited Speakers\". In the pre-WW II congresses the Plenary Speakers were called \"Invited Speakers\". \n\nDuring the 1900 Congress in Paris, France, David Hilbert \"(pictured)\" announced his famous list of Hilbert's problems.\n\n\nIn 1904, in Heidelberg, the 69 invited speakers included Borel, Hadamard, Hilbert, Klein, Levi-Civita, Minkowski, Mittag-Leffler, and Sommerfeld.\n\n\nThe 1908 ICM in Rome had 121 invited speakers included Bernstein, Borel, Brückner, Brouwer, Darboux, Dickson, Fubini, Hadamard, Levi-Civita, Lorenz, Macfarlane, Mittag-Leffler, E.H. Moore, M. Noether, Picard, Poincaré, F. Rietz, Severi, Sommerfeld, and Zermelo. Robert Genese spoke again, this time on \"The Method of Reciprocal Polars Applied to Forces in Space\" (page 145 of the proceedings). \n\nThe 1912 ICM in Cambridge had 103 invited speakers, among them Bateman, Bernstein, Borel, Brouwer, Fehr, Fields, Grossman, Hadamard, Hardy, von Koch, Landau, Littlewood, Love, Macfarlane, E.H. Moore, Morley, Peano, Runge, Thomon, Volterra, Whitehead, and Zermelo.\n\n\nThe 1920 congress in Strasbourg had only 56 invited speakers, among them Cartan, Dickson, Grossman, Hadamard, Jordan, Lefschetz, Takagi, de la Vallée Poussin, Volterra, and Wiener.\n\n\nThe 1924 ICM in Toronto had 180 invited speakers, including Bell, Besicovitch, Cartan, Coats, Coker, Dickson, Eddington, Fehr, Fisher, Fréchet, Fubini, Hedrick, Hille, Morley, Ore, Peano, Plancherel, Ricci-Curbastro, H. Rietz, Severi, Sierpiński, Uspensky, and Zaremba.\n\n\nThe 1928 Bologna ICM had 265 invited speakers , including Banach, Bernstein, G.D. Birkhoff, Bompiana, Borel, Cartan, Čech, Courant, Fano, Fields, Fisher, Fréchet, Fubini, Haar, Hadamard, Hilbert, Julia, Lévy, Levi-Civita, Menger, Milne-Thomson, Mordell, Nevanlinna, Neyman, Nikodym, E. Noether, Ore, Plancherel, Pólya, Rademacher, Reidemeister, F. Rietz, Segre, Severi, Sierpiński, Steinhaus, Tarski, Veblen, Vitali, Volterra, Weyl, Whittaker, Zariski, and Zygmund.\n\nThe 1932 ICM in Zürich had 258 invited speakers, including Ahlfors, Alexandroff, Bernays, Bernstein, Bieberbach, Borsuk, Carathéodory, both Cartans, Čech, Cesari, de Rham, Delsarte, Fehr, Fraenkel, Hadamard, Hardy, Hasse, Hille, Hopf, Hurewicz, Julia, Krull, Kuratowski, Lévy, Littlewood, Menger, Milne-Thomson, Mordell, Morse, Nevanlinna, E. Noether, Ore, Pauli, Pontryagin, F. Rietz, Seifert, Severi, Sierpiński, Ulam, Volterra, Whitehead, Wiener, Zaremba, and Zygmund.\nThere were 191 invited speakers at the 1936 congress in Oslo, among them Ahlfors, Banach, Bateman, both Birkhoffs, Borel, Borsuk, Cartan, Cartwright, Courant, Cramér, Eilenberg, Erdős, Feller, Fréchet, Gelfond, Hesse, Hecke, Hurewicz, Lemaître, McShane, Menger, Mordell, Morley, Morse, both Newmans, Ore, Pólya, Rado, M. Riesz, Selberg, Siegel, Sierpinski, Skolem, Stone, Taussky, Veblen, Whitehead, and Wiener.\n\nAt the 1954 Congress of Mathematicians in Amsterdam, Richard Brauer announced his program for the classification of finite simple groups.\nAlexander Grothendieck \"(pictured)\" in his plenary lecture at the 1958 Congress outlined his programme \"to create arithmetic geometry via a (new) reformulation of algebraic geometry, seeking maximal generality.\"\n\nAt the 1962 Congress in Stockholm Kiyosi Ito (pictured) lectured on how to combine differential geometry and stochastic analysis, and this led to major advances in the 60s and 70s.\n\nThere were thirty-one Invited Addresses (eight in Abstract) at the 1966 congress.\n\nThe 1006 ICM in Madrid attracted several thousand mathematicians.\n\nThis list inventories the mathematicians who were the most invited to speak to an ICM.\n\nThis list inventories the mathematicians who were the most invited to speak to an ICM after 1950.\n\n"}
{"id": "49593538", "url": "https://en.wikipedia.org/wiki?curid=49593538", "title": "List of things named after Stefan Banach", "text": "List of things named after Stefan Banach\n\nStefan Banach was a Polish mathematician who made key contributions to mathematics. This article contains some of the things named in his memory.\n\n\n"}
{"id": "26149716", "url": "https://en.wikipedia.org/wiki?curid=26149716", "title": "Madhava series", "text": "Madhava series\n\nIn mathematics, a Leibniz or Madhava series is any one of the series in a collection of infinite series expressions all of which are believed to have been discovered by Madhava of Sangamagrama (c. 1350 – c. 1425), the founder of the Kerala school of astronomy and mathematics and later by Gottfried Wilhelm Leibniz, among others. These expressions are the Maclaurin series expansions of the trigonometric sine, cosine and arctangent functions, and the special case of the power series expansion of the arctangent function yielding a formula for computing π. The power series expansions of sine and cosine functions are respectively called \"Madhava's sine series\" and \"Madhava's cosine series\". The power series expansion of the arctangent function is sometimes called \"Madhava–Gregory series\" or \"Gregory–Madhava series\". These power series are also collectively called \"Taylor–Madhava series\". The formula for π is referred to as \"Madhava–Newton series\" or \"Madhava–Leibniz series\" or Leibniz formula for pi or Leibnitz–Gregory–Madhava series. These further names for the various series are reflective of the names of the Western discoverers or popularizers of the respective series.\n\nThe derivations use many calculus related concepts such as summation, rate of change, and interpolation, which suggests that Indian mathematicians had a solid understanding of the basics of calculus long before it was developed in Europe. There is no evidence, however, that any of his ideas went out of Kerala. The calculus was still in its primitive stages and it would remain so until Newton and Leibniz entered the scene. Even though they had very basic ideas on the infinite series, Indian Mathematicians were not able to convert calculus to the modern problem solving tool that it is today.\nNo surviving works of Madhava contain explicit statements regarding the expressions which are now referred to as Madhava series. However, in the writing of later members of the Kerala school of astronomy and mathematics like Nilakantha Somayaji and Jyeshthadeva one can find unambiguous attributions of these series to Madhava. It is also in the works of these later astronomers and mathematicians one can trace the Indian proofs of these series expansions. These proofs provide enough indications about the approach Madhava had adopted to arrive at his series expansions.\n\nUnlike most previous cultures, which had been rather nervous about the concept of infinity, Madhava was more than happy to play around with infinity, particularly infinite series. He showed how, although one can be approximated by adding a half plus a quarter plus an eighth plus a sixteenth, etc., (as even the ancient Egyptians and Greeks had known), the exact total of one can only be achieved by adding up infinitely many fractions. But Madhava went further and linked the idea of an infinite series with geometry and trigonometry. He realized that, by successively adding and subtracting different odd number fractions to infinity, he could home in on an exact formula for π (this was two centuries before Leibniz was to come to the same conclusion in Europe).\n\nIn the writings of the mathematicians and astronomers of the Kerala school, Madhava's series are described couched in the terminology and concepts fashionable at that time. When we translate these ideas into the notations and concepts of modern-day mathematics, we obtain the current equivalents of Madhava's series. These present-day counterparts of the infinite series expressions discovered by Madhava are the following:\n\nNone of Madhava's works, containing any of the series expressions attributed to him, have survived. These series expressions are found in the writings of the followers of Madhava in the Kerala school. At many places these authors have clearly stated that these are \"as told by Madhava\". Thus the enunciations of the various series found in Tantrasamgraha and its commentaries can be safely assumed to be in \"Madhava's own words\". The translations of the relevant verses as given in the \"Yuktidipika\" commentary of Tantrasamgraha (also known as \"Tantrasamgraha-vyakhya\") by Sankara Variar (circa. 1500 - 1560 CE) are reproduced below. These are then rendered in current mathematical notations.\n\nMadhava's sine series is stated in verses 2.440 and 2.441 in \"Yukti-dipika\" commentary (\"Tantrasamgraha-vyakhya\") by Sankara Variar. A translation of the verses follows.\n\n\"Multiply the arc by the square of the arc, and take the result of repeating that (any number of times). Divide (each of the above numerators) by the squares of the successive even numbers increased by that number and multiplied by the square of the radius. Place the arc and the successive results so obtained one below the other, and subtract each from the one above. These together give the jiva, as collected together in the verse beginning with \"vidvan\" etc. \"\n\nLet \"r\" denote the radius of the circle and \"s\" the arc-length.\n\n\nLet θ be the angle subtended by the arc \"s\" at the centre of the circle. Then \"s\" = \"r θ\" and \"jiva\" = \"r\" sin \"θ\". Substituting these in the last expression and simplifying we get\nwhich is the infinite power series expansion of the sine function.\n\nThe last line in the verse ′\"as collected together in the verse beginning with \"vidvan\" etc.\"′ is a reference to a reformulation of the series introduced by Madhava himself to make it convenient for easy computations for specified values of the arc and the radius.\nFor such a reformulation, Madhava considers a circle one-quarter of which measures 5400 minutes (say \"C\" minutes) and develops a scheme for the easy computations of the \"jiva\"′s of the various arcs of such a circle. Let \"R\" be the radius of a circle one-quarter of which measures C.\nMadhava had already computed the value of π using his series formula for π. Using this value of π, namely 3.1415926535922, the radius \"R\" is computed as follows:\nThen\n\nMadhava's expression for \"jiva\" corresponding to any arc \"s\" of a circle of radius \"R\" is equivalent to the following:\n\nMadhava now computes the following values:\n\nThe \"jiva\" can now be computed using the following scheme:\n\nThis gives an approximation of \"jiva\" by its Taylor polynomial of the 11'th order. It involves one division, six multiplications and five subtractions only. Madhava prescribes this numerically efficient computational scheme in the following words (translation of verse 2.437 in \"Yukti-dipika\"):\n\n\"vi-dvān, tu-nna-ba-la, ka-vī-śa-ni-ca-ya, sa-rvā-rtha-śī-la-sthi-ro, ni-rvi-ddhā-nga-na-rē-ndra-rung . Successively multiply these five numbers in order by the square of the arc divided by the quarter of the circumference (5400′), and subtract from the next number. (Continue this process with the result so obtained and the next number.) Multiply the final result by the cube of the arc divided by quarter of the circumference and subtract from the arc.\"\n\nMadhava's cosine series is stated in verses 2.442 and 2.443 in \"Yukti-dipika\" commentary (\"Tantrasamgraha-vyakhya\") by Sankara Variar. A translation of the verses follows.\n\n\"Multiply the square of the arc by the unit (i.e. the radius) and take the result of repeating that (any number of times). Divide (each of the above numerators) by the square of the successive even numbers decreased by that number and multiplied by the square of the radius. But the first term is (now)(the one which is) divided by twice the radius. Place the successive results so obtained one below the other and subtract each from the one above. These together give the śara as collected together in the verse beginning with stena, stri, etc. \"\n\nLet \"r\" denote the radius of the circle and \"s\" the arc-length.\n\n\nLet \"θ\" be the angle subtended by the arc \"s\" at the centre of the circle. Then \"s\" = \"rθ\" and \"śara\" = \"r\"(1 − cos \"θ\"). Substituting these in the last expression and simplifying we get\nwhich gives the infinite power series expansion of the cosine function.\n\nThe last line in the verse ′\"as collected together in the verse beginning with stena, stri, etc.\"′ is a reference to a reformulation introduced by Madhava himself to make the series convenient for easy computations for specified values of the arc and the radius.\nAs in the case of the sine series, Madhava considers a circle one quarter of which measures 5400 minutes (say \"C\" minutes) and develops a scheme for the easy computations of the \"śara\"′s of the various arcs of such a circle. Let \"R\" be the radius of a circle one quarter of which measures C. Then, as in the case of the sine series, Madhava gets\n\"R\" = 3437′ 44′′ 48′′′.\n\nMadhava's expression for \"śara\" corresponding to any arc \"s\" of a circle of radius \"R\" is equivalent to the following:\n\nMadhava now computes the following values:\n\nThe \"śara\" can now be computed using the following scheme:\n\nThis gives an approximation of \"śara\" by its Taylor polynomial of the 12'th order. This also involves one division, six multiplications and five subtractions only. Madhava prescribes this numerically efficient computational scheme in the following words (translation of verse 2.438 in \"Yukti-dipika\"):\n\n\"The six stena, strīpiśuna, sugandhinaganud, bhadrāngabhavyāsana, mīnāngonarasimha, unadhanakrtbhureva. Multiply by the square of the arc divided by the quarter of the circumference and subtract from the next number. (Continue with the result and the next number.) Final result will be utkrama-jya (R versed sign).\"\n\nMadhava's arctangent series is stated in verses 2.206 – 2.209 in \"Yukti-dipika\" commentary (\"Tantrasamgraha-vyakhya\") by Sankara Variar. A translation of the verses is given below.\nJyesthadeva has also given a description of this series in Yuktibhasa.\n\n\"Now, by just the same argument, the determination of the arc of a desired sine can be (made). That is as follows: The first result is the product of the desired sine and the radius divided by the cosine of the arc. When one has made the square of the sine the multiplier and the square of the cosine the divisor, now a group of results is to be determined from the (previous) results beginning from the first. When these are divided in order by the odd numbers 1, 3, and so forth, and when one has subtracted the sum of the even(-numbered) results from the sum of the odd (ones), that should be the arc. Here the smaller of the sine and cosine is required to be considered as the desired (sine). Otherwise, there would be no termination of results even if repeatedly (computed).\"\n\n\"By means of the same argument, the circumference can be computed in another way too. That is as (follows): The first result should by the square root of the square of the diameter multiplied by twelve. From then on, the result should be divided by three (in) each successive (case). When these are divided in order by the odd numbers, beginning with 1, and when one has subtracted the (even) results from the sum of the odd, (that) should be the circumference.\"\n\nLet \"s\" be the arc of the desired sine (\"jya\" or \"jiva\") \"y\". Let \"r\" be the radius and \"x\" be the cosine (\"kotijya\").\n\n\nLet θ be the angle subtended by the arc \"s\" at the centre of the circle. Then \"s\" = \"r\"θ, \"x\" = \"kotijya\" = \"r\" cos θ and \"y\" = \"jya\" = \"r\" sin θ.\nThen \"y\" / \"x\" = tan θ. Substituting these in the last expression and simplifying we get\nLetting tan θ = \"q\" we finally have\n\n\nThe second part of the quoted text specifies another formula for the computation of the circumference \"c\" of a circle having diameter \"d\". This is as follows.\n\nSince \"c\" = π \"d\" this can be reformulated as a formula to compute π as follows.\n\nThis is obtained by substituting \"q\" = formula_22 (therefore \"θ\" = π / 6) in the power series expansion for tan \"q\" above.\n\n\n"}
{"id": "9057460", "url": "https://en.wikipedia.org/wiki?curid=9057460", "title": "National Centre for Excellence in the Teaching of Mathematics", "text": "National Centre for Excellence in the Teaching of Mathematics\n\nThe National Centre for Excellence in the Teaching of Mathematics (NCETM) is an institution set up in the wake of the Smith Report to improve mathematics teaching in England.\n\nIt provides strategic leadership for mathematics-specific CPD and aims to raise the professional status of all those engaged in the teaching of mathematics so that the mathematical potential of learners will be fully realised.\n\nPlease note: some of the content on this page is now out of date. For an up-to-date view of the NCETM's work, please go to the Centre's website.\n\nIts Director until March 2013 was Dame Celia Hoyles, Professor of Mathematics Education at the Institute of Education, University of London and former chief adviser on mathematics education for the government. She was succeeded by the current Director, Charlie Stripp.\n\nAn innovative NCETM development is the MatheMaPedia project, masterminded by John Mason, which is a \"maths teaching wiki\".\n\nInitially headquartered in London, it is headquartered in the south of Sheffield city centre opposite St Mary's Church, Bramall Lane on part of the A61 dual-carriageway (Sheffield Inner Ring Road), east of the Velocity Tower, near the Bramall Lane Roundabout; it is the headquarters of Tribal Education.\n\nIt is run by Mathematics in Education and Industry (MEI) and Tribal Education.\n\nThe NCETM's Evidence Bulletin , available only to those logged into the site, asks \"How can you use research evidence to enhance your mathematics teaching?\" It covers themes such as the following:\n\n\nEmphases of the NCETM include:\n\nThe website, which anyone can join, offers special areas dedicated to early years, primary secondary, post-16 and new approaches to teaching and learning. Members can create their own personalised learning space within a social networking site, where they can share ideas with others and ask for inspiration.\n\nThe NCETM hosts online courses as well as real-world and workshops\n\nSpecial online events have included the world’s first online discussion of proof, the launch of ground-breaking report Mathematics Matters, led by Malcolm Swan at the University of Nottingham, and videos of Teachers Talking Theory: in Action, a new professional development resource created by and featuring primary and secondary teachers in the South West of England.\nDiscussion forums track ICT in mathematics teaching and the Bowland case studies, newly in schools from September 2008 to enliven the teaching of key \"stage 3 mathematics.\"\n\nAt the NCETM annual conference 2008, Sir Peter Williams launched the Review of Primary Mathematics, which called for a mathematics specialist in every primary school by 2015, amounting to improved and ongoing training for 13,000 primary teachers. Lord Adonis, representing the government, welcomed the report and agreed to its implementation.\n\n\n\n"}
{"id": "7330660", "url": "https://en.wikipedia.org/wiki?curid=7330660", "title": "Numerical linear algebra", "text": "Numerical linear algebra\n\nNumerical linear algebra is the study of algorithms for performing linear algebra computations, most notably matrix operations, on computers. It is often a fundamental part of engineering and computational science problems, such as image and signal processing, telecommunication, computational finance, materials science simulations, structural biology, data mining, bioinformatics, fluid dynamics, and many other areas. Such software relies heavily on the development, analysis, and implementation of state-of-the-art algorithms for solving various numerical linear algebra problems, in large part because of the role of matrices in finite difference and finite element methods.\n\nCommon problems in numerical linear algebra include computing the following: LU decomposition, QR decomposition, singular value decomposition, eigenvalues.\n\n\n\n"}
{"id": "31434142", "url": "https://en.wikipedia.org/wiki?curid=31434142", "title": "Numerical semigroup", "text": "Numerical semigroup\n\nIn mathematics, a numerical semigroup is a special kind of a semigroup. Its underlying set is the set of all nonnegative integers except a finite number and the binary operation is the operation of addition of integers. Also, the integer 0 must be an element of the semigroup. For example, while the set {0, 2, 3, 4, 5, 6, ...} is a numerical semigroup, the set {0, 1, 3, 5, 6, ...} is not because 1 is in the set and 1 + 1 = 2 is not in the set. Numerical semigroups are commutative monoids and are also known as numerical monoids. \n\nThe definition of numerical semigroup is intimately related to the problem of determining nonnegative integers that can be expressed in the form \"x\"\"n\" + \"x\" \"n\" + ... + \"x\" \"n\" for a given set {\"n\", \"n\", ..., \"n\"} of positive integers and for arbitrary nonnegative integers \"x\", \"x\", ..., \"x\". This problem had been considered by several mathematicians like Frobenius (1849 – 1917) and Sylvester (1814 – 1897) at the end of the 19th century. During the second half of the twentieth century, interest in the study of numerical semigroups resurfaced because of their applications in algebraic geometry.\n\nLet \"N\" be the set of nonnegative integers. A subset \"S\" of \"N\" is called a numerical semigroup if the following conditions are satisfied.\n\n\nThere is a simple method to construct numerical semigroups. Let \"A\" = {\"n\", \"n\", ..., \"n\"} be a nonempty set of positive integers. The set of all integers of the form \"x\" \"n\" + \"x\" \"n\" + ... + \"x\" \"n\" is the subset of \"N\" generated by \"A\" and is denoted by 〈 \"A\" 〉. The following theorem fully characterizes numerical semigroups.\n\nLet \"S\" be the subsemigroup of \"N\" generated by \"A\". Then \"S\" is a numerical semigroup if and only if gcd (\"A\") = 1. Moreover, every numerical semigroup arises in this way.\n\nThe following subsets of \"N\" are numerical semigroups.\n\nThe set \"A\" is a set of generators of the numerical semigroup 〈 \"A\" 〉. A set of generators of a numerical semigroup is a minimal system\nof generators if none of its proper subsets generates the numerical semigroup. It is known that \nevery numerical semigroup \"S\" has a unique minimal system of generators and also that this minimal system of generators is finite. The cardinality of the minimal set of generators is called the \"embedding dimension\" of the numerical semigroup \"S\" and is denoted by \"e\"(\"S\"). The smallest member in the minimal system of generators is called the \"multiplicity\" of the numerical semigroup \"S\" and is denoted by \"m\"(\"S\").\n\nThere are several notable numbers associated with a numerical semigroup \"S\".\n\nLet \"S\" = 〈 5, 7, 9 〉. Then we have:\nNumerical semigroups with small Frobenius number or genus\n\nThe following general results were known to Sylvester. Let \"a\" and \"b\" be positive integers such that gcd (\"a\", \"b\") = 1. Then \n\nThere is no known general formula to compute the Frobenius number of numerical semigroups having embedding dimension three or more. No polynomial formula can be found to compute the Frobenius number or genus of a numerical semigroup with embedding dimension three. Every positive integer is the Frobenius number of some numerical semigroup with embedding dimension three.\n\nThe following algorithm, known as Rödseth's algorithm,\ncan be used to compute the Frobenius number of a numerical semigroup \"S\" generated by {\"a\", \"a\", \"a\"} where \"a\" < \"a\" < \"a\" and gcd ( \"a\", \"a\", \"a\") = 1. Its worst-case complexity is not as good as Greenberg's algorithm\n\nbut it is much simpler to describe.\n\nAn \"irreducible numerical semigroup\" is a numerical semigroup such that it cannot be written as the intersection of two numerical semigroups properly containing it. A numerical semigroup \"S\" is irreducible if and only if \"S\" is maximal, with respect to set inclusion, in the collection of all numerical semigroups with Frobenius number \"F\"(\"S\"). \n\nA numerical semigroup \"S \" is \"symmetric\" if it is irreducible and its Frobenius number \"F\"(\"S\") is odd. We say that \"S\" is \"pseudo-symmetric\" provided that \"S\" is irreducible and F(S) is even. Such numerical semigroups have simple characterizations in terms of Frobenius number and genus:\n\n"}
{"id": "1063976", "url": "https://en.wikipedia.org/wiki?curid=1063976", "title": "OBJ (programming language)", "text": "OBJ (programming language)\n\nOBJ is a programming language family introduced by Joseph Goguen in 1976.\n\nIt is a family of declarative \"ultra high-level\" languages. It features abstract types, generic modules, subsorts (subtypes with multiple inheritance), pattern-matching modulo equations, E-strategies (user control over laziness), module expressions (for combining modules), theories and views (for describing module interfaces) for the massively parallel RRM (rewrite rule machine).\nMembers of the OBJ family of languages include CafeOBJ, Eqlog, FOOPS, Kumo, Maude and OBJ3.\n\nOBJ3 is a version of OBJ based on order-sorted rewriting. OBJ3 is agent-oriented and runs on Kyoto Common Lisp AKCL.\n\n\n\n"}
{"id": "19634230", "url": "https://en.wikipedia.org/wiki?curid=19634230", "title": "Philosophy of statistics", "text": "Philosophy of statistics\n\nThe philosophy of statistics involves the meaning, justification, utility, use and abuse of statistics and its methodology, and ethical and epistemological issues involved in the consideration of choice and interpretation of data and methods of statistics.\n\n\n"}
{"id": "33013769", "url": "https://en.wikipedia.org/wiki?curid=33013769", "title": "Pincherle polynomials", "text": "Pincherle polynomials\n\nIn mathematics, the Pincherle polynomials P(\"x\") are polynomials introduced by given by the generating function\n\nHumbert polynomials are a generalization of Pincherle polynomials\n\n"}
{"id": "41321254", "url": "https://en.wikipedia.org/wiki?curid=41321254", "title": "Polar point group", "text": "Polar point group\n\nIn geometry, a polar point group is a point group in which there is more than one point that every symmetry operation leaves unmoved. Therefore, a point group with more than one axis of rotation or a mirror plane perpendicular to the axis of rotation cannot be polar.\n\nA straight line joining unmoved points defines a unique axis of rotation, unless symmetry operations do not allow any rotation at all, such as mirror symmetry, in which case, the polar direction must be parallel to any mirror planes.\n\nOf the 32 crystallographic point groups, 10 are polar:\nThe space groups associated with a polar point group do not have their origins uniquely determined by symmetry elements.\nWhen materials having a polar point group crystal structure are heated or cooled, they may temporarily generate a voltage called pyroelectricity.\n\nMolecular crystals that occupy polar space groups will exhibit triboluminescence. A common example of this is sucrose, demonstrated by smashing a wintergreen lifesaver in a darkened room.\n"}
{"id": "5445552", "url": "https://en.wikipedia.org/wiki?curid=5445552", "title": "Poussin proof", "text": "Poussin proof\n\nIn number theory, the Poussin proof is the proof of an identity related to the fractional part of a ratio.\n\nIn 1838, Peter Gustav Lejeune Dirichlet proved an approximate formula for the average number of divisors of all the numbers from 1 to η:\n\nwhere \"d\" represents the divisor function, and γ represents the Euler-Mascheroni constant.\n\nIn 1898, Charles Jean de la Vallée-Poussin proved that if a large number η is divided by all the primes up to η, then the average fraction by which the quotient falls short of the next whole number is γ:\nwhere {\"x\"} represents the fractional part of \"x\", and π represents the prime-counting function.\nFor example, if we divide 29 by 2, we get 14.5, which falls short of 15 by 0.5.\n\n\n"}
{"id": "1691461", "url": "https://en.wikipedia.org/wiki?curid=1691461", "title": "Ramism", "text": "Ramism\n\nRamism was a collection of theories on rhetoric, logic, and pedagogy based on the teachings of Petrus Ramus, a French academic, philosopher, and Huguenot convert, who was murdered during the St. Bartholomew's Day massacre in August 1572.\n\nAccording to British historian Jonathan Israel:\n\n\"[Ramism], despite its crudity, enjoyed vast popularity in late sixteenth-century Europe, and at the outset of the seventeenth, providing as it did a method of systematizing all branches of knowledge, emphasizing the relevance of theory to practical applications [...]\"\n\nAudomarus Talaeus (Omer Talon) was an early French disciple and writer on Ramism. The work of Ramus gained early international attention, with Roger Ascham corresponding about him with Johann Sturm, teacher of Ramus and collaborator with Ascham; Ascham supported his stance on Joachim Perion, one early opponent, but also expressed some reservations. Later Ascham found Ramus's lack of respect for Cicero, rather than extreme proponents, just unacceptable.\n\nAfter Ramus died, his ideas had influence in some (but not all) parts of Protestant Europe. His influence was strong in Germany and the Netherlands, and on Puritan and Calvinist theologians of England, Scotland, and New England. He had little effect on mainstream Swiss Calvinists, and was largely ignored in Catholic countries. The progress of Ramism in the half-century roughly 1575 to 1625 was closely related to, and mediated by, university education: the religious factor came in through the different reception in Protestant and Catholic universities, all over Europe. The works of Ramus reached New England on the \"Mayflower\".\n\nRamus was killed in 1572, and a biography by Banosius (Théophile de Banos) appeared by 1576. His status as Huguenot martyr certainly had something to do with the early dissemination of his ideas. Outside France, for example, there was the 1574 English translation by the Scot Roland MacIlmaine of the University of St Andrews. Ramus's works and influence then appeared in the logical textbooks of the Scottish universities, and equally he had followers in England.\n\nAs late as 1626, Francis Burgersdyk divides the logicians of his day into the Aristotelians, the Ramists and the Semi-Ramists. These last endeavoured, like Rudolph Goclenius of Marburg and Amandus Polanus of Basel, to mediate between the contending parties. Ramism was closely linked to systematic Calvinism, but the hybrid Philippo-Ramism (which is where the Semi-Ramists fit in) arose as a blend of Ramus with the logic of Philipp Melanchthon.\n\nRamism, while in fashion, met with considerable hostility. The Jesuits were completely opposed. The Calvinist Aristotelian Theodore Beza was also a strong opponent of Ramism. Similarly the leading Lutheran Aristotelian philosopher Jakob Schegk resolutely rejected Ramus and opposed his visit to Tübingen. In Heidelberg the efforts of Giulio Pace to teach Ramist dialectic to Polish private students were forbidden.\n\nWhere universities were open to Ramist teaching, there still could be dislike and negative reactions, stemming from the perceived personality of Ramus (arrogant, a natural polemicist), or of that of his supporters (young men in a hurry). There was tacit adoption of some of the techniques such as the epitome, without acceptance of the whole package of reform including junking Aristotle in favour of the new textbooks, and making Ramus an authoritative figure. John Rainolds at Oxford was an example of an older academic torn by the issue; his follower Richard Hooker was firmly against \"Ramystry\".\n\nGerhard Johann Vossius at Leiden wrote massive works on classical rhetoric and opposed Ramism. He defended and enriched the Aristotelian tradition for the seventeenth century. He was a representative Dutch opponent; Ramism did not take permanent hold in the universities of the Netherlands, and once William Ames had died, it declined.\n\nMid-century, Ramism was still under attack, from Cartesians such as Johannes Clauberg, who defended Aristotle against Ramus.\n\nFrances Yates proposed a subtle relationship of Ramism to the legacy of Lullism, the art of memory, and Renaissance hermetism. She considers that Ramism drew on Lullism, but is more superficial; was opposed to the classical art of memory; and moved in an opposite direction to the occult (reducing rather than increasing the role of images). He \"abandoned imagery and the creative imagination\". Mary Carruthers referred back to Albertus Magnus and Thomas Aquinas:\n\n\"It is one of those ironies of history that Peter Ramus, who, in the sixteenth century, thought he was reacting against Aristotelianism by taking \"memoria\" from rhetoric and making it part of dialectic, was essentially remaking a move made 300 years before by two Dominican professors who were attempting to reshape memorial study in conformity with Aristotle.\"\n\nAn alternative to this aspect of Ramism, as belated and diminishing, is the discussion initiated by Walter Ong of Ramus in relation to several evolutionary steps. Ong's position, on the importance of Ramus as historical figure and humanist, has been summed up as \"the center of controversies about method (both in teaching and in scientific discovery) and about rhetoric and logic and their role in communication\".\n\nThe best known of Ong's theses is Ramus the post-Gutenberg writer, in other words the calibration of the indexing and schematics involved in Ramism to the transition away from written manuscripts, and the spoken word. Extensive charts were instead used, drawing on the resources of typography, to organise material, from left to right across a printed page, particularly in theological treatises. The cultural impact of Ramism depended on the nexus of printing (trees regularly laid out with braces) and rhetoric, forceful and persuasive at least to some Protestants; and it had partly been anticipated in cataloguing and indexing knowledge and its encyclopedism by Conrad Gesner. The term \"Ramean tree\" became standard in logic books, applying to the classical Porphyrian tree, or any binary tree, without clear distinction between the underlying structure and the way of displaying it; now scholars use the clearer term \"Ramist epitome\" to signify the structure. Ong argued that, a chart being a visual aid and logic having come down to charts, the role of voice and dialogue is placed squarely and rigidly in the domain of rhetoric, and in a lower position.\n\nTwo other theses of Ong on Ramism are: the end of \"copia\" or profuseness for its own sake in writing, making Ramus an opponent of the Erasmus of \"\"; and the beginning of the later Cartesian emphasis on clarity. Ong, though, consistently argues that Ramus is thin, insubstantial as a scholar, a beneficiary of fashion supported by the new medium of printing, as well as a transitional figure.\n\nThese ideas, from the 1950s and 1960s onwards, have been reconsidered. Brian Vickers summed up the view a generation or so later: dismissive of Yates, he notes that bracketed tables existed in older manuscripts, and states that Ong's emphases are found unconvincing. Further, \"methodus\", the Ramists' major slogan, was specific to figures of speech, deriving from Hermogenes of Tarsus via George of Trebizond. And the particular moves used by Ramus in the reconfiguration of rhetoric were in no sense innovative by themselves. Lisa Jardine agrees with Ong that he was not a first-rank innovator, more of a successful textbook writer adapting earlier insights centred on topics-logic, but insists on his importance and influence in \"humanistic logic\". She takes the Ramean tree to be a \"voguish\" pedagogic advance.\n\nIt has been said that:\n\nPuritans believed the maps proved well suited to rationalize and order the Christian view of revealed truth and the language and knowledge of the new learning, specifically the scientific and philosophical paradigms arising out of the Renaissance.\n\nDonald R. Kelley writes of the \"new learning\" (\"nova doctrina\") or opposition in Paris to traditional scholasticism as a \"trivial revolution\", i.e. growing out of specialist teachers of the \"trivium\". He argues that:\n\n\"The aim was a fundamental change of priorities, the transformation of hierarchy of disciplines into a 'circle' of learning, an 'encyclopedia' embracing human culture in all of its richness and concreteness and organized for persuasive transmission to society as a whole. This was the rationale of the Ramist method, which accordingly emphasized mnemonics and pedagogical technique at the expense of discovery and the advancement of learning.\n\nThe need for demarcation was seen in \"redundancies and overlapping categories\".\n\nThis was taken to the lengths where it could be mocked in the \"Port-Royal Logic\" (1662). There the authors claimed that \"everything that is useful to logic belongs to it\", with a swipe at the \"torments\" the Ramists put themselves through.\n\nThe method of demarcation was applied within the \"trivium\", made up of grammar, logic (for which Ramists usually preferred a traditional name, \"dialectic\"), and rhetoric. Logic falls, according to Ramus, into two parts: invention (treating of the notion and definition) and judgment (comprising the judgment proper, syllogism and method). In this he was influenced by Rodolphus Agricola. What Ramus does here in fact redefines rhetoric. There is a new configuration, with logic and rhetoric each having two parts: rhetoric was to cover \"elocutio\" (mainly figures of speech) and \"pronuntiatio\" (oratorical delivery). In general, Ramism liked to deal with binary trees as method for organising knowledge.\n\nRhetoric, traditionally, had had five parts, of which \"inventio\" (invention) was the first. Two others were \"dispositio\" (arrangement) and \"memoria\" (memory). Ramus proposed transferring those back to the realm of \"dialectic\" (logic); and merging them under a new heading, renaming them as \"iudicium\" (judgment). This was the final effect: as an intermediate \"memoria\" was left with rhetoric.\n\nIn the end the art of memory was diminished in Ramism, displaced by an idea of \"method\": better mental organisation would be more methodical, and mnemonic techniques drop away. This was a step in the direction of Descartes. The construction of disciplines, for Ramus, was subject to some laws, his \"methodus\". There were three, with clear origins in Aristotle, and his \"Posterior Analytics\".\n\nThey comprised the \"lex veritatis\" (French \"du tout\", law of truth), \"lex justitiae\" (\"par soi\", law of justice), and \"lex sapientiae\" (\"universalité\", or law of wisdom). The third was in the terms of Ramus \"universel premièrement\", or to make the universal the first instance. The \"wisdom\" is therefore to start with the universal, and set up a ramifying binary tree by subdivision.\n\nAs Ramism evolved, these characteristic binary trees, set up rigidly, were treated differently in various fields. In theology, for example, this procedure was turned on its head, since the search for God, the universal, would appear as the goal rather than the starting point.\n\nÉmile Bréhier wrote that after Ramus, \"order\" as a criterion of the methodical had become commonplace; Descartes needed only to supply to method the idea of relation, exemplified by the idea of a mathematical sequence based on a functional relationship of an element to its successor. Therefore, for Cartesians, the Ramist insights were quite easily absorbed.\n\nFor the Baconian method, on the other hand, the rigidity of Ramist distinctions was a serious criticism. Francis Bacon, a Cambridge graduate, was early aware of Ramism, but the near-equation of \"dispositio\" with method was unsatisfactory, for Baconians, because arrangement of material was seen to be inadequate for research. The \"Novum Organum\" implied in its title a further reform of Aristotle, and its aphorism viii of Book I made this exact point.\n\nA Ramist tradition took root in Christ's College, Cambridge in the 1570s, when Laurence Chaderton became the leading Ramist, and Gabriel Harvey lectured on the rhetoric of Ramus. Marshall McLuhan's dissertation on Thomas Nashe (via the classical trivium), who was involved in a high-profile literary quarrel with Harvey, was shaped by his interest in aligning Harvey with dialectic and the plain style (logic in the sense of Ramus), and Nashe with the full resources of Elizabethan rhetoric. After Chaderton, there was a succession of important theologians using Ramist logic, including William Perkins, and William Ames (Amesius), who made Ramist dialectic integral to his approach.\n\nWilliam Temple annotated a 1584 reprint of the \"Dialectics\" in Cambridge. Known as an advocate of Ramism, and involved in controversy with Everard Digby of Oxford, he became secretary to Sir Philip Sidney about a year later, in 1585. Temple was with Sidney when he died in 1586, and wrote a Latin Ramist commentary on \"An Apology for Poetry\". Sidney himself is supposed to have learned Ramist theory from John Dee, and was the dedicatee of the biography by Banosius, but was not in any strict sense a Ramist.\n\nThis Ramist school was influential:\nChristopher Marlowe encountered Ramist thought as a student at Cambridge (B.A. in 1584), and made Peter Ramus a character in \"The Massacre at Paris\". He also cited Ramus in \"Dr. Faustus\": \"Bene disserere est finis logices\" is a line given to Faustus, who states it is from Aristotle, when it is from the \"Dialecticae\" of Ramus.\n\nThere is a short treatise by John Milton, who was a student at Christ's from 1625, published two years before his death, called \"Artis Logicae Plenior Institutio ad Petri Rami Methodum concinnata\". It was one of the last commentaries on Ramist logic. Although composed in the 1640s, it was not published until 1672. Milton, whose first tutor at Christ's William Chappell used Ramist method, can take little enough credit for the content. Most of the text proper is adapted from the 1572 edition of Ramus's logic; most of the commentary is adapted from George Downham's \"Commentarii in P. Rami Dialecticam\" (1601)—Downham, also affiliated with Christ's, was a professor of logic at Cambridge. The biography of Ramus is a cut-down version of that of Johann Thomas Freigius (1543–83).\n\nHerborn Academy in Germany was founded in 1584, as a Protestant university, and initially was associated with a group of Reformed theologians who developed covenant theology. It was also a centre of Ramism, and in particular of its encyclopedic form. In turn, it was the birthplace of pansophism. Heinrich Alsted taught there, and John Amos Comenius studied with him.\n\nRamism was built into the curriculum, with the professors required to give Ramist treatments of the \"trivium\". Johannes Piscator anticipated the foundation in writing introductory Ramist texts, Johannes Althusius and Lazarus Schöner likewise wrote respectively on social science topics and mathematics, and Piscator later produced a Ramist theology text.\n\nBrian Vickers argues that the Ramist influence did add something to rhetoric: it concentrated more on the remaining aspect of \"elocutio\" or effective use of language, and emphasised the role of vernacular European languages (rather than Latin). The outcome was that rhetoric was applied in literature.\n\nIn 1588 Abraham Fraunce, a protégé of Philip Sidney, published \"Arcadian Rhetorike\", a Ramist-style rhetoric book cut down largely to a discussion of figures of speech (in prose and verse), and referring by its title to Sidney's \"Arcadia\". It was based on a translation of Talon's \"Rhetoricae\", and was a companion to \"The Lawiers Logike\" of 1585, an adapted translation of the \"Dialecticae\" of Ramus. Through it, Sidney's usage of figures was disseminated as the Ramist \"Arcadian rhetoric\" of standard English literary components and ornaments, before the source \"Arcadia\" had been published. It quickly lent itself to floridity of style. William Wimsatt and Cleanth Brooks consider that the Ramist reform at least created a tension between the ornamented and the plain style (of preachers and scientific scholars), into the seventeenth century, and contributed to the emergence of the latter. With the previous work of Dudley Fenner (1584), and the later book of Charles Butler (1598), Ramist rhetoric in Elizabethan England accepts the reduction to \"elocutio\" and \"pronuntiatio\", puts all the emphasis on the former, and reduces its scope to the trope.\n\nGeoffrey Hill classified Robert Burton's \"Anatomy of Melancholy\" (1621) as a \"post-Ramist anatomy\". It is a work (he says against Ong) of a rooted scholar with a \"method\" but turning Ramism back on itself.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "30727109", "url": "https://en.wikipedia.org/wiki?curid=30727109", "title": "Rickart space", "text": "Rickart space\n\nIn mathematics, a Rickart space (after Charles Earl Rickart), also called a basically disconnected space, is a topological space in which open σ-compact subsets have compact open closures. named them after , who showed that Rickart spaces are related to monotone σ-complete C*-algebras in the same way that Stonean spaces are related to AW*-algebras.\n\nRickart spaces are totally disconnected and sub-Stonean spaces.\n\n"}
{"id": "15175696", "url": "https://en.wikipedia.org/wiki?curid=15175696", "title": "Signal-flow graph", "text": "Signal-flow graph\n\nA signal-flow graph or signal-flowgraph (SFG), invented by Claude Shannon, but often called a Mason graph after Samuel Jefferson Mason who coined the term, is a specialized flow graph, a directed graph in which nodes represent system variables, and branches (edges, arcs, or arrows) represent functional connections between pairs of nodes. Thus, signal-flow graph theory builds on that of directed graphs (also called digraphs), which includes as well that of oriented graphs. This mathematical theory of digraphs exists, of course, quite apart from its applications.\n\nSFGs are most commonly used to represent signal flow in a physical system and its controller(s), forming a cyber-physical system. Among their other uses are the representation of signal flow in various electronic networks and amplifiers, digital filters, state-variable filters and some other types of analog filters. In nearly all literature, a signal-flow graph is associated with a set of linear equations.\n\nWai-Kai Chen wrote: \"The concept of a signal-flow graph was originally worked out by Shannon [1942] \nin dealing with analog computers. The greatest credit for the formulation of signal-flow graphs is normally extended to Mason [1953], [1956]. He showed how to use the signal-flow graph technique to solve some difficult electronic problems in a relatively simple manner. The term signal flow graph was used because of its original application to electronic problems and the association with electronic signals and flowcharts of the systems under study.\"\n\nLorens wrote: \"Previous to Mason's work, C. E. Shannon worked out a number of the properties of what are now known as flow graphs. Unfortunately, the paper originally had a restricted classification and very few people had access to the material.\"\n\n\"The rules for the evaluation of the graph determinant of a Mason Graph were first given and proven by Shannon [1942] using mathematical induction. His work remained essentially unknown even after Mason published his classical work in 1953. Three years later, Mason [1956] rediscovered the rules and proved them by considering the value of a determinant and how it changes as variables are added to the graph. [...]\"\n\nRobichaud \"et al.\" identify the domain of application of SFGs as follows:\n\nThe following illustration and its meaning were introduced by Mason to illustrate basic concepts:\n\nIn the simple flow graphs of the figure, a functional dependence of a node is indicated by an incoming arrow, the node originating this influence is the beginning of this arrow, and in its most general form the signal flow graph indicates by incoming arrows only those nodes that influence the processing at the receiving node, and at each node, \"i\", the incoming variables are processed according to a function associated with that node, say \"F\". The flowgraph in (a) represents a set of explicit relationships:\nNode \"x\" is an isolated node because no arrow is incoming; the equations for \"x\" and \"x\" have the graphs shown in parts (b) and (c) of the figure.\n\nThese relationships define for every node a function that processes the input signals it receives. Each non-source node combines the input signals in some manner, and broadcasts a resulting signal along each outgoing branch. \"A flow graph, as defined originally by Mason, implies a set of functional relations, linear or not.\"\n\nHowever, the commonly used Mason graph is more restricted, assuming that each node simply sums its incoming arrows, and that each branch involves only the initiating node involved. Thus, in this more restrictive approach, the node \"x\" is unaffected while:\n\nand now the functions \"f\" can be associated with the signal-flow branches \"ij\" joining the pair of nodes \"x, x\", rather than having general relationships associated with each node. A contribution by a node to itself like \"f\" for \"x\" is called a \"self-loop\". Frequently these functions are simply multiplicative factors (often called \"transmittances\" or \"gains\"), for example, \"f(x)=cx\", where \"c\" is a scalar, but possibly a function of some parameter like the Laplace transform variable \"s\". Signal-flow graphs are very often used with Laplace-transformed signals, and in this case the transmittance, \"c(s)\", often is called a transfer function.\n\nRobichaud et al. wrote: \"The signal flow graph contains the same information as the equations from which it is derived; but there does not exist a one-to-one correspondence between the graph and the system of equations. One system will give different graphs according to the order in which the equations are used to define the variable written on the left-hand side.\" If all equations relate all dependent variables, then there are \"n!\" possible SFGs to choose from.\n\nLinear signal-flow graph methods only apply to linear time-invariant systems, as studied by their associated theory. When modeling a system of interest, the first step is often to determine the equations representing the system's operation without assigning causes and effects (this is called acausal modeling). A SFG is then derived from this system of equations.\n\nA linear SFG consists of nodes indicated by dots and weighted directional branches indicated by arrows. The nodes are the variables of the equations and the branch weights are the coefficients. Signals may only traverse a branch in the direction indicated by its arrow. The elements of a SFG can only represent the operations of multiplication by a coefficient and addition, which are sufficient to represent the constrained equations. When a signal traverses a branch in its indicated direction, the signal is multiplied the weight of the branch. When two or more branches direct into the same node, their outputs are added.\n\nFor systems described by linear algebraic or differential equations, the signal-flow graph is mathematically equivalent to the system of equations describing the system, and the equations governing the nodes are discovered for each node by summing incoming branches to that node. These incoming branches convey the contributions of the other nodes, expressed as the connected node value multiplied by the weight of the connecting branch, usually a real number or function of some parameter (for example a Laplace transform variable \"s\").\n\nFor linear active networks, Choma writes: \"By a 'signal flow representation' [or 'graph', as it is commonly referred to] we mean a diagram that, by displaying the algebraic relationships among relevant branch variables of network, paints an unambiguous picture of the way an applied input signal ‘flows’ from input-to-output ... ports.\"\n\nA motivation for a SFG analysis is described by Chen:\n\nA linear signal flow graph is related to a system of linear equations of the following form:\n\nThe figure to the right depicts various elements and constructs of a signal flow graph (SFG).\n\nTerms used in linear SFG theory also include:\n\n\nA signal-flow graph may be simplified by graph transformation rules. These simplification rules are also referred to as \"signal-flow graph algebra\".\nThe purpose of this reduction is to relate the dependent variables of interest (residual nodes, sinks) to its independent variables (sources).\n\nThe systematic reduction of a linear signal-flow graph is a graphical method equivalent to the Gauss-Jordan elimination method for solving linear equations.\n\nThe rules presented below may be applied over and over until the signal flow graph is reduced to its \"minimal residual form\". Further reduction can require loop elimination or the use of a \"reduction formula\" with the goal to directly connect sink nodes representing the dependent variables to the source nodes representing the independent variables. By these means, any signal-flow graph can be simplified by successively removing internal nodes until only the input and output and index nodes remain. Robichaud described this process of systematic flow-graph reduction:\n\nFor digitally reducing a flow graph using an algorithm, Robichaud extends the notion of a simple flow graph to a \"generalized\" flow graph:\nThe definition of an elementary transformation varies from author to author:\n\nParallel edges. Replace parallel edges with a single edge having a gain equal to the sum of original gains.\n\nThe graph on the left has parallel edges between nodes. On the right, these parallel edges have been replaced with a single edge having a gain equal to the sum of the gains on each original edge.\nThe equations corresponding to the reduction between N and node I are:\n\nOutflowing edges. Replace outflowing edges with edges directly flowing from the node's sources.\n\nThe graph on the left has an intermediate node N between nodes from which it has inflows, and nodes to which it flows out.\nThe graph on the right shows direct flows between these node sets, without transiting via N.\nFor the sake of simplicity, N and its inflows are not represented. The outflows from N are eliminated.\nThe equations corresponding to the reduction directly relating N's input signals to its output signals are:\n\nZero-signal nodes.\nEliminate outflowing edges from a node determined to have a value of zero.\n\nIf the value of a node is zero, its outflowing edges can be eliminated.\n\nNodes without outflows.\nEliminate a node without outflows.\n\nIn this case, N is not a variable of interest, and it has no outgoing edges; therefore, N, and its inflowing edges, can be eliminated.\n\nSelf-looping edge. Replace looping edges by adjusting the gains on the incoming edges.\nThe graph on the left has a looping edge at node N, with a gain of g. On the right, the looping edge has been eliminated, and all inflowing edges have their gain divided by (1-g).The equations corresponding to the reduction between N and all its input signals are:\n\nThe above procedure for building the SFG from an acausal system of equations and for solving the SFG's gains have been implemented as an add-on to MATHLAB 68, an on-line system providing machine aid for the mechanical symbolic processes encountered in analysis.\n\nSignal flow graphs can be used to solve sets of simultaneous linear equations. The set of equations must be consistent and all equations must be linearly independent.\n\nFor M equations with N unknowns where each y is a known value and each x is an unknown value, there is equation for each known of the following form.\n\nAlthough it is feasible, particularly for simple cases, to establish a signal flow graph using the equations in this form, some rearrangement allows a general procedure that works easily for any set of equations, as now is presented. To proceed, first the equations are rewritten as\n\nand further rewritten as\n\nand finally rewritten as \n\nThe signal-flow graph is now arranged by selecting one of these equations and addressing the node on the right-hand side. This is the node for which the node connects to itself with the branch of weight including a '+1', making a \"self-loop\" in the flow graph. The other terms in that equation connect this node first to the source in this equation and then to all the other branches incident on this node. Every equation is treated this way, and then each incident branch is joined to its respective emanating node. For example, the case of three variables is shown in the figure, and the first equation is:\n\nwhere the right side of this equation is the sum of the weighted arrows incident on node \"x\".\n\nAs there is a basic symmetry in the treatment of every node, a simple starting point is an arrangement of nodes with each node at one vertex of a regular polygon. When expressed using the general coefficients {\"c\"}, the environment of each node is then just like all the rest apart from a permutation of indices. Such an implementation for a set of three simultaneous equations is seen in the figure.\n\nOften the known values, y are taken as the primary causes and the unknowns values, x to be effects, but regardless of this interpretation, the last form for the set of equations can be represented as a signal-flow graph. This point is discussed further in the subsection Interpreting 'causality'.\n\nIn the most general case, the values for all the x variables can be calculated by computing Mason's gain formula for the path from each y to each x and using superposition.\n\nIn general, there are N-1 paths from y to variable x so the computational effort to calculated G is proportional to N-1.\nSince there are M values of y, G must be computed M times for a single value of x. The computational effort to calculate a single x variable is proportional to (N-1)(M). The effort to compute all the x variables is proportional to (N)(N-1)(M). If there are N equations and N unknowns, then the computation effort is on the order of N.\n\nFor some authors, a linear signal-flow graph is more constrained than a block diagram, in that the SFG rigorously describes linear algebraic equations represented by a directed graph.\n\nFor other authors, linear block diagrams and linear signal-flow graphs are equivalent ways of depicting a system, and either can be used to solve the gain.\n\nA tabulation of the comparison between block diagrams and signal-flow graphs is provided by Bakshi & Bakshi, and another tabulation by Kumar. According to Barker \"et al.\":\n\nIn the figure, a simple block diagram for a feedback system is shown with two possible interpretations as a signal-flow graph. The input \"R(s)\" is the Laplace-transformed input signal; it is shown as a source node in the signal-flow graph (a source node has no input edges). The output signal \"C(s)\" is the Laplace-transformed output variable. It is represented as a sink node in the flow diagram (a sink has no output edges). \"G(s)\" and \"H(s)\" are transfer functions, with \"H(s)\" serving to feed back a modified version of the output to the input, \"B(s)\". The two flow graph representations are equivalent.\n\nThe term \"cause and effect\" was applied by Mason to SFGs: \n\nand has been repeated by many later authors:\n\nHowever, Mason's paper is concerned to show in great detail how a \"set of equations\" is connected to an SFG, an emphasis unrelated to intuitive notions of \"cause and effect\". Intuitions can be helpful for arriving at an SFG or for gaining insight from an SFG, but are inessential to the SFG. The essential connection of the SFG is to its own set of equations, as described, for example, by Ogata:\n\nThere is no reference to \"cause and effect\" here, and as said by Barutsky:\n\nThe term \"cause and effect\" may be misinterpreted as it applies to the SFG, and taken incorrectly to suggest a system view of causality, rather than a \"computationally\" based meaning. To keep discussion clear, it may be advisable to use the term \"computational causality\", as is suggested for bond graphs:\n\nThe term \"computational causality\" is explained using the example of current and voltage in a resistor:\n\nA computer program or algorithm can be arranged to solve a set of equations using various strategies. They differ in how they prioritize finding some of the variables in terms of the others, and these algorithmic decisions, which are simply about solution strategy, then set up the variables expressed as dependent variables earlier in the solution to be \"effects\", determined by the remaining variables that now are \"causes\", in the sense of \"computational causality\".\n\nUsing this terminology, it is \"computational\" causality, not \"system\" causality, that is relevant to the SFG. There exists a wide-ranging philosophical debate, not concerned specifically with the SFG, over connections between computational causality and system causality.\n\nSignal-flow graphs can be used for analysis, that is for understanding a model of an existing system, or for synthesis, that is for determining the properties of a design alternative.\n\nWhen building a model of a dynamic system, a list of steps is provided by Dorf & Bishop:\nIn this workflow, equations of the physical system's mathematical model are used to derive the signal-flow graph equations.\n\nSignal-flow graphs have been used in Design Space Exploration (DSE), as an intermediate representation towards a physical implementation. The DSE process seeks a suitable solution among different alternatives. In contrast with the typical analysis workflow, where a system of interest is first modeled with the physical equations of its components, the specification for synthesizing a design could be a desired transfer function. For example, different strategies would create different signal-flow graphs, from which implementations are derived.\nAnother example uses an annotated SFG as an expression of the continuous-time behavior, as input to an architecture generator\n\nShannon's formula is an analytic expression for calculating the gain of an interconnected set of amplifiers in an analog computer. During World War II, while investigating the functional operation of an analog computer, Claude Shannon developed his formula. Because of wartime restrictions, Shannon's work was not published at that time, and, in 1952, Mason rediscovered the same formula.\n\nHapp generalized the Shannon formula for topologically closed systems. The Shannon-Happ formula can be used for deriving transfer functions, sensitivities, and error functions.\n\nFor a consistent set of linear unilateral relations, the Shannon-Happ formula expresses the solution using direct substitution (non-iterative).\n\nNASA's electrical circuit software NASAP is based on the Shannon-Happ formula.\n\nThe amplification of a signal \"V\" by an amplifier with gain \"a\" is described mathematically by\n\nThis relationship represented by the signal-flow graph of Figure 1. is that V is dependent on V but it implies no dependency of V on V. See Kou page 57.\n\nA possible SFG for the asymptotic gain model for a negative feedback amplifier is shown in Figure 3, and leads to the equation for the gain of this amplifier as\n\nThe interpretation of the parameters is as follows: \"T\" = return ratio, \"G\" = direct amplifier gain, \"G\" = feedforward (indicating the possible bilateral nature of the feedback, possibly deliberate as in the case of feedforward compensation). Figure 3 has the interesting aspect that it resembles Figure 2 for the two-port network with the addition of the extra \"feedback relation\" \"x = T y\".\n\nFrom this gain expression an interpretation of the parameters \"G\" and \"G\" is evident, namely:\n\nThere are many possible SFG's associated with any particular gain relation. Figure 4 shows another SFG for the asymptotic gain model that can be easier to interpret in terms of a circuit. In this graph, parameter β is interpreted as a feedback factor and \"A\" as a \"control parameter\", possibly related to a dependent source in the circuit. Using this graph, the gain is\n\nTo connect to the asymptotic gain model, parameters \"A\" and β cannot be arbitrary circuit parameters, but must relate to the return ratio \"T\" by:\n\nand to the asymptotic gain as:\n\nSubstituting these results into the gain expression,\n\nwhich is the formula of the asymptotic gain model.\n\nThe figure to the right depicts a circuit that contains a \"y\"-parameter two-port network. V is the input of the circuit and V is the output. The two-port equations impose a set of linear constraints between its port voltages and currents. The terminal equations impose other constraints. All these constraints are represented in the SFG (Signal Flow Graph) below the circuit. There is only one path from input to output which is shown in a different color and has a (voltage) gain of -Ry. There are also three loops: -Ry, -Ry, RyRy. Sometimes a loop indicates intentional feedback but it can also indicate a constraint on the relationship of two variables. For example, the equation that describes a resistor says that the ratio of the voltage across the resistor to the current through the resistor is a constant which is called the resistance. This can be interpreted as the voltage is the input and the current is the output, or the current is the input and the voltage is the output, or merely that the voltage and current have a linear relationship. Virtually all passive two terminal devices in a circuit will show up in the SFG as a loop.\n\nThe SFG and the schematic depict the same circuit, but the schematic also suggests the circuit's purpose. Compared to the schematic, the SFG is awkward but it does have the advantage that the input to output gain can be written down by inspection using Mason's rule.\n\nThis example is representative of a SFG (signal-flow graph) used to represent a servo control system and illustrates several features of SFGs. Some of the loops (loop 3, loop 4 and loop 5) are extrinsic intentionally designed feedback loops. These are shown with dotted lines. There are also intrinsic loops (loop 0, loop1, loop2) that are not intentional feedback loops, although they can be analyzed as though they were. These loops are shown with solid lines. Loop 3 and loop 4 are also known as minor loops because they are inside a larger loop.\n\n\nSee Mason's rule for development of Mason's Gain Formula for this example.\n\nThere is some confusion in literature about what a signal-flow graph is; Henry Paynter, inventor of bond graphs, writes: \"But much of the decline of signal-flow graphs [...] is due in part to the mistaken notion that the branches must be linear and the nodes must be summative. Neither assumption was embraced by Mason, himself !\"\n\n\nA state transition SFG or state diagram is a simulation diagram for a system of equations, including the initial conditions of the states.\n\n Closed flowgraphs describe closed systems and have been utilized to provide a rigorous theoretical basis for topological techniques of circuit analysis.\n\nMason introduced both nonlinear and linear flow graphs. To clarify this point, Mason wrote : \"A linear flow graph is one whose associated equations are linear.\"\n\nIt we denote by x the signal at node j, the following are examples of node functions that do not pertain to a linear time-invariant system:\n\n\n\n\n\n\n"}
{"id": "2628786", "url": "https://en.wikipedia.org/wiki?curid=2628786", "title": "South (disambiguation)", "text": "South (disambiguation)\n\nSouth is a cardinal direction or compass point.\nSouth or The South may also refer to:\n\n\n\n\n\n\n\n"}
{"id": "26173821", "url": "https://en.wikipedia.org/wiki?curid=26173821", "title": "Space-filling tree", "text": "Space-filling tree\n\nSpace-filling trees are geometric constructions that are analogous to space-filling curves, but have a branching, tree-like structure and are rooted. A space-filling tree is defined by an incremental process that results in a tree for which every point in the space has a finite-length path that converges to it. In contrast to space-filling curves, individual paths in the tree are short, allowing any part of the space to be quickly reached from the root.\n\nA space-filling tree is defined by an iterative process whereby a single point in a continuous space is connected via a continuous path to every other point in the space by a path of length, and for every point in the space, there is at least one path that converges to it.\n\nThe term \"space-filling tree\" in this sense was created in a 2009 tech report that defines \"space-filling\" and \"tree\" differently than their traditional definitions in mathematics. As explained in the space-filling curve article, in 1890, Peano found the first space-filling curve, and by Jordan's 1887 definition, which is now standard, a curve is a single function, not a sequence of functions. The curve is \"space filling\" because it is \"a curve whose range contains the entire 2-dimensional unit square\" (as explained in the first sentence of space-filling curve).\n\nIn contrast, a space-filling tree, as defined in the tech report, is not a single tree. It is only a sequence of trees. The paper says \"A space-filling tree is actually defined as an infinite sequence of trees\". It defines formula_1 as a \"sequence of trees\", then states \"formula_1 is a space-filling tree\". It is not space-filling in the standard sense of including the entire 2-dimensional unit square. Instead, the paper defines it as having trees in the sequence coming arbitrarily close to every point. It states \"A tree sequence T is called 'space filling' in a space \"X\" if for every \"x\" ∈ \"X\", there exists a path in the tree that starts at the root and converges to \"x\".\". The standard term for this concept is that it includes a set of points that is dense everywhere in the unit square.\n\nThe simplest example of a space-filling tree is one that fills a square planar region. The images illustrate the construction for the planar region formula_3. At each iteration, additional branches are added to the existing trees.\n\nSpace-filling trees can also be defined for a variety of other shapes and volumes.\nBelow is the subdivision scheme used to define a space-filling for a triangular region.\nAt each iteration, additional branches are added to the existing trees connecting the center of each triangle to the centers of the four subtriangles.\n\nThe first six iterations of the triangle space-filling tree are illustrated below:\n\nSpace-filling trees can also be constructed in higher dimensions. The simplest examples are cubes in formula_4 and hypercubes in formula_5.\nA similar sequence of iterations used for the square space-filling tree can be used for hypercubes. The third iteration of such a space-filling tree in formula_4 is illustrated below:\n"}
{"id": "202672", "url": "https://en.wikipedia.org/wiki?curid=202672", "title": "Spectral density", "text": "Spectral density\n\nThe power spectrum formula_1 of a time series formula_2 describes \nthe distribution of power into frequency components composing that signal. \nAccording to Fourier analysis, any physical signal can be decomposed into a number of discrete frequencies, or a spectrum of frequencies over a continuous range. The statistical average of a certain signal or sort of signal (including noise) as analyzed in terms of its frequency content, is called its spectrum.\n\nWhen the energy of the signal is concentrated around a finite time interval, especially if its total energy is finite, one may compute the energy spectral density. More commonly used is the power spectral density (or simply power spectrum), which applies to signals existing over \"all\" time, or over a time period large enough (especially in relation to the duration of a measurement) that it could as well have been over an infinite time interval. The power spectral density (PSD) then refers to the spectral energy distribution that would be found per unit time, since the total energy of such a signal over all time would generally be infinite. Summation or integration of the spectral components yields the total power (for a physical process) or variance (in a statistical process), identical to what would be obtained by integrating formula_3 over the time domain, as dictated by Parseval's theorem.\n\nThe spectrum of a physical process formula_2 often contains essential information about the nature of formula_5. For instance, the pitch and timbre of a musical instrument are immediately determined from a spectral analysis. The color of a light source is determined by the spectrum of the electromagnetic wave's electric field formula_6 as it fluctuates at an extremely high frequency. Obtaining a spectrum from time series such as these involves the Fourier transform, and generalizations based on Fourier analysis. In many cases the time domain is not specifically employed in practice, such as when a dispersive prism is used to obtain a spectrum of light in a spectrograph, or when a sound is perceived through its effect on the auditory receptors of the inner ear, each of which is sensitive to a particular frequency.\nHowever this article concentrates on situations in which the time series is known (at least in a statistical sense) or directly measured (such as by a microphone sampled by a computer). The power spectrum is important in statistical signal processing and in the statistical study of stochastic processes, as well as in many other branches of physics and engineering. Typically the process is a function of time, but one can similarly discuss data in the spatial domain being decomposed in terms of spatial frequency.\n\nAny signal that can be represented as a variable that varies in time has a corresponding frequency spectrum. This includes familiar entities such as visible light (perceived as color), musical notes (perceived as pitch), radio/TV (specified by their frequency, or sometimes wavelength) and even the regular rotation of the earth. When these signals are viewed in the form of a frequency spectrum, certain aspects of the received signals or the underlying processes producing them are revealed. In some cases the frequency spectrum may include a distinct peak corresponding to a sine wave component. And additionally there may be peaks corresponding to harmonics of a fundamental peak, indicating a periodic signal which is \"not\" simply sinusoidal. Or a continuous spectrum may show narrow frequency intervals which are strongly enhanced corresponding to resonances, or frequency intervals containing almost zero power as would be produced by a notch filter.\n\nIn physics, the signal might be a wave, such as an electromagnetic wave, an acoustic wave, or the vibration of a mechanism. The \"power spectral density\" (PSD) of the signal describes the power present in the signal as a function of frequency, per unit frequency. Power spectral density is commonly expressed in watts per hertz (W/Hz).\n\nWhen a signal is defined in terms only of a voltage, for instance, there is no unique power associated with the stated amplitude. In this case \"power\" is simply reckoned in terms of the square of the signal, as this would always be \"proportional\" to the actual power delivered by that signal into a given impedance. So one might use units of V Hz for the PSD and V s Hz for the ESD (\"energy spectral density\") even though no actual \"power\" or \"energy\" is specified.\n\nSometimes one encounters an \"amplitude spectral density\" (ASD), which is the square root of the PSD; the ASD of a voltage signal has units of V Hz. This is useful when the \"shape\" of the spectrum is rather constant, since variations in the ASD will then be proportional to variations in the signal's voltage level itself. But it is mathematically preferred to use the PSD, since only in that case is the area under the curve meaningful in terms of actual power over all frequency or over a specified bandwidth.\n\nFor random vibration analysis, units of \"g\" Hz are frequently used for the PSD of acceleration. Here \"g\" denotes the g-force.\n\nMathematically, it is not necessary to assign physical dimensions to the signal or to the independent variable. In the following discussion the meaning of \"x(t)\" will remain unspecified, but the independent variable will be assumed to be that of time.\n\nEnergy spectral density describes how the energy of a signal or a time series is distributed with frequency. Here, the term energy is used in the generalized sense of signal processing; that is, the energy formula_7 of a signal formula_2 is\n\nThe energy spectral density is most suitable for transients—that is, pulse-like signals—having a finite total energy. In this case, Parseval's theorem gives us an alternate expression for the energy of the signal:\n\nwhere\n\nis the Fourier transform of the signal and formula_12 is the frequency in Hz, i.e., cycles per second. Often used is the angular frequency formula_13. Since the integral on the right-hand side is the energy of the signal, the integrand formula_14 can be interpreted as a density function describing the energy per unit frequency contained in the signal at the frequency formula_12. In light of this, the energy spectral density of a signal formula_2 is defined as\n\nAs a physical example of how one might measure the energy spectral density of a signal, suppose formula_18 represents the potential (in volts) of an electrical pulse propagating along a transmission line of impedance formula_19, and suppose the line is terminated with a matched resistor (so that all of the pulse energy is delivered to the resistor and none is reflected back). By Ohm's law, the power delivered to the resistor at time formula_20 is equal to formula_21, so the total energy is found by integrating formula_21 with respect to time over the duration of the pulse. To find the value of the energy spectral density formula_1 at frequency formula_12, one could insert between the transmission line and the resistor a bandpass filter which passes only a narrow range of frequencies (formula_25, say) near the frequency of interest and then measure the total energy formula_26 dissipated across the resistor. The value of the energy spectral density at formula_12 is then estimated to be formula_28. In this example, since the power formula_21 has units of V Ω, the energy formula_26 has units of V s Ω = J, and hence the estimate formula_31 of the energy spectral density has units of J Hz, as required. In many situations, it is common to forgo the step of dividing by formula_19 so that the energy spectral density instead has units of V s Hz.\n\nThis definition generalizes in a straightforward manner to a discrete signal with an infinite number of values formula_33 such as a signal sampled at discrete times formula_34:\n\nwhere formula_36 is the discrete Fourier transform of formula_37 and formula_38 is the complex conjugate of formula_39 The sampling interval formula_40 is needed to keep the correct physical units and to ensure that we recover the continuous case in the limit formula_41; however, in the mathematical sciences, the interval is often set to 1.\n\nThe above definition of energy spectral density is suitable for transients (pulse-like signals) whose energy is concentrated around one time window; then the Fourier transforms of the signals generally exist. For continuous signals over all time, such as stationary processes, one must rather define the \"power spectral density\" (PSD); this describes how power of a signal or time series is distributed over frequency, as in the simple example given previously. Here, power can be the actual physical power, or more often, for convenience with abstract signals, is simply identified with the squared value of the signal. For example, statisticians study the variance of a function over time formula_2 (or over another independent variable), and using an analogy with electrical signals (among other physical processes), it is customary to refer to it as the \"power spectrum\" even when there is no physical power involved. If one were to create a physical voltage source which followed formula_2 and applied it to the terminals of a 1 ohm resistor, then indeed the instantaneous power dissipated in that resistor would be given by \"x\" watts.\n\nThe average power \"P\" of a signal formula_2 over all time is therefore given by the following time average:\n\nNote that a stationary process, for instance, may have a finite power but an infinite energy. After all, energy is the integral of power, and the stationary signal continues over an infinite time. That is the reason that we cannot use the energy spectral density as defined above in such cases.\n\nIn analyzing the frequency content of the signal formula_2, one might like to compute the ordinary Fourier transform formula_47; however, for many signals of interest the Fourier transform does not formally exist. Because of this complication one can as well work with a truncated Fourier transform where the signal is integrated only over a finite interval [0, \"T\"]:\n\nThis is the amplitude spectral density. Then the power spectral density can be defined as\n\nHere formula_50 denotes the expected value; explicitly, we have\n\nIn the latter form (for a stationary random process), one can make the change of variables formula_52 and with the limits of integration (rather than [0,T]) approaching infinity, the resulting power spectral density formula_53 and the autocorrelation function of this signal are seen to be Fourier transform pairs (Wiener–Khinchin theorem). The autocorrelation function is a statistic defined as\n\nor more generally as\n\nin the case that \"X(t)\" is complex-valued. Provided that formula_56 is absolutely integrable (which is not always true),\n\nMany authors use this equality to actually \"define\" the power spectral density.\n\nThe power of the signal in a given frequency band formula_58 (or formula_59) can be calculated by integrating over frequency. Since formula_60, an equal amount of power can be attributed to positive and negative frequencies, which accounts for the factor of 2 in the following form (such trivial factors dependent on conventions used):\n\nMore generally, similar techniques may be used to estimate a time-varying spectral density. In this case the truncated Fourier transform defined above over the finite time interval \"(0, T)\" is \"not\" evaluated in the limit of \"T\" approaching infinity. This results in decreased spectral coverage and resolution since frequencies of less than \"1/T\" are not sampled, and results at frequencies which are not an integer multiple of \"1/T\" are not independent. Just using a single such time series, the estimated power spectrum will be very \"noisy\", however this can be alleviated if it is possible to evaluate the expected value (in the above equation) using a large (or infinite) number of short-term spectra corresponding to statistical ensembles of realizations of \"x(t)\" evaluated over the specified time window.\n\nThis definition of the power spectral density can be generalized to discrete time variables formula_33. As above we can consider a \"finite\" window of formula_63 with the signal sampled at discrete times formula_34 for a total measurement period formula_65. Then a single estimate of the PSD can be obtained through summation rather than integration:\n\nAs before, the actual PSD is achieved when \"N\" (and thus \"T\") approach infinity and the expected value is formally applied. In a real-world application, one would typically average this single-measurement PSD over many trials to obtain a more accurate estimate of the theoretical PSD of the physical process underlying the individual measurements. This computed PSD is sometimes called a periodogram. This periodogram converges to the true PSD as the number of estimates as well as the averaging time interval \"T\" approach infinity (Brown & Hwang).\n\nIf two signals both possess power spectral densities, then the #Cross-spectral density can similarly be calculated; as the PSD is related to the autocorrelation, so is the cross-spectral density related to the cross-correlation.\n\nSome properties of the PSD include:\n\n\nThe \"integrated spectrum\" or \"power spectral distribution\" formula_72 is defined as\n\nGiven two signals formula_2 and formula_75, each of which possess power spectral densities formula_53 and formula_77, it is possible to define a \"cross-spectral density\" (CSD) given by\n\nThe cross-spectral density (or 'cross power spectrum') is thus the Fourier transform of the cross-correlation function.\n\nwhere formula_80 is the cross-correlation of formula_2 and formula_75.\n\nBy an extension of the Wiener–Khinchin theorem, the Fourier transform of the cross-spectral density formula_83 is the cross-covariance function. In light of this, the PSD is seen to be a special case of the CSD for formula_84.\n\nFor discrete signals \"x\" and \"y\", the relationship between the cross-spectral density and the cross-covariance is\n\nThe goal of spectral density estimation is to estimate the spectral density of a random signal from a sequence of time samples. Depending on what is known about the signal, estimation techniques can involve parametric or non-parametric approaches, and may be based on time-domain or frequency-domain analysis. For example, a common parametric technique involves fitting the observations to an autoregressive model. A common non-parametric technique is the periodogram.\n\nThe spectral density is usually estimated using Fourier transform methods (such as the Welch method), but other techniques such as the maximum entropy method can also be used.\n\n\n\nThe concept and use of the power spectrum of a signal is fundamental in electrical engineering, especially in electronic communication systems, including radio communications, radars, and related systems, plus passive remote sensing technology. Electronic instruments called spectrum analyzers are used to observe and measure the power spectra of signals.\n\nThe spectrum analyzer measures the magnitude of the short-time Fourier transform (STFT) of an input signal. If the signal being analyzed can be considered a stationary process, the STFT is a good smoothed estimate of its power spectral density.\n\nPrimordial fluctuations, density variations in the early universe, are quantified by a power spectrum which gives the power of the variations as a function of spatial scale.\n\n\n"}
{"id": "18866777", "url": "https://en.wikipedia.org/wiki?curid=18866777", "title": "Steiner chain", "text": "Steiner chain\n\nIn geometry, a Steiner chain is a set of \"n\" circles, all of which are tangent to two given non-intersecting circles (blue and red in Figure 1), where \"n\" is finite and each circle in the chain is tangent to the previous and next circles in the chain. In the usual \"closed\" Steiner chains, the first and last (\"n\") circles are also tangent to each other; by contrast, in \"open\" Steiner chains, they need not be. The given circles \"α\" and \"β\" do not intersect, but otherwise are unconstrained; the smaller circle may lie completely inside or outside of the larger circle. In these cases, the centers of Steiner-chain circles lie on an ellipse or a hyperbola, respectively.\n\nSteiner chains are named after Jakob Steiner, who defined them in the 19th century and discovered many of their properties. A fundamental result is \"Steiner's porism\", which states:\n\n\"Tangent in the same way\" means that the arbitrary circle is internally or externally tangent in the same way as a circle of the original Steiner chain. A porism is a type of theorem relating to the number of solutions and the conditions on it. Porisms often describe a geometrical figure that cannot exist unless a condition is met, but otherwise may exist in infinite number; another example is Poncelet's porism.\n\nThe method of circle inversion is helpful in treating Steiner chains. Since it preserves tangencies, angles and circles, inversion transforms one Steiner chain into another of the same number of circles. One particular choice of inversion transforms the given circles \"α\" and \"β\" into concentric circles; in this case, all the circles of the Steiner chain have the same size and can \"roll\" around in the annulus between the circles similar to ball bearings. This standard configuration allows several properties of Steiner chains to be derived, e.g., its points of tangencies always lie on a circle. Several generalizations of Steiner chains exist, most notably Soddy's hexlet and Pappus chains.\n\n<gallery caption=\"Steiner chains with different internal/external tangencies\" perrow=5>\nImage:Steiner_chain_7mer.svg|The 7 circles of this Steiner chain (black) are externally tangent to the inner given circle (red) but internally tangent to the outer given circle (blue).\nImage:Steiner_chain_7mer_all_external.svg|The 7 circles of this Steiner chain (black) are externally tangent to both given circles (red and blue), which lie outside one another.\nImage:Steiner_chain_8mer_all_but_one_external.svg|Seven of the 8 circles of this Steiner chain (black) are externally tangent to both given circles (red and blue); the 8th circle is internally tangent to both.\n</gallery>\nThe two given circles \"α\" and \"β\" cannot intersect; hence, the smaller given circle must lie inside or outside the larger. The circles are usually shown as an annulus, i.e., with the smaller given circle inside the larger one. In this configuration, the Steiner-chain circles are externally tangent to the inner given circle and internally tangent to the outer circle. However, the smaller circle may also lie completely outside the larger one (Figure 2). The black circles of Figure 2 satisfy the conditions for a closed Steiner chain: they are all tangent to the two given circles and each is tangent to its neighbors in the chain. In this configuration, the Steiner-chain circles have the same type of tangency to both given circles, either externally or internally tangent to both. If the two given circles are tangent at a point, the Steiner chain becomes an infinite Pappus chain, which is often discussed in the context of the arbelos (\"shoemaker's knife\"), a geometric figure made from three circles. There is no general name for a sequence of circles tangent to two given circles that intersect at two points.\n\nThe two given circles \"α\" and \"β\" touch the \"n\" circles of the Steiner chain, but each circle \"C\" of a Steiner chain touches only four circles: \"α\", \"β\", and its two neighbors, \"C\" and \"C\". By default, Steiner chains are assumed to be \"closed\", i.e., the first and last circles are tangent to one another. By contrast, an \"open\" Steiner chain is one in which the first and last circles, \"C\" and \"C\", are not tangent to one another; these circles are tangent only to \"three\" circles. Multicyclic Steiner chains wrap around the inner circle several times before closing, i.e., before being tangent to the initial circle.\n\nThe simplest type of Steiner chain is a closed chain of \"n\" circles of equal size surrounding an inscribed circle of radius \"r\"; the chain of circles is itself surrounded by a circumscribed circle of radius \"R\". The inscribed and circumscribed given circles are concentric, and the Steiner-chain circles lie in the annulus between them. By symmetry, the angle 2θ between the centers of the Steiner-chain circles is 360°/\"n\". Because Steiner chain circles are tangent to one another, the distance between their centers equals the sum of their radii, here twice their radius \"ρ\". The bisector (green in Figure) creates two right triangles, with a central angle of . The sine of this angle can be written as the length of its opposite segment, divided by the hypotenuse of the right triangle\n\nSince \"θ\" is known from \"n\", this provides an equation for the unknown radius \"ρ\" of the Steiner-chain circles\n\nThe tangent points of a Steiner chain circle with the inner and outer given circles lie on a line that pass through their common center; hence, the outer radius .\n\nThese equations provide a criterion for the feasibility of a Steiner chain for two given concentric circles. A closed Steiner chain of \"n\" circles requires that the ratio of radii \"R\"/\"r\" of the given circles equal exactly\n\nAs shown below, this ratio-of-radii criterion for concentric given circles can be extended to all types of given circles by the inversive distance \"δ\" of the two given circles. For concentric circles, this distance is defined as a logarithm of their ratio of radii\n\nUsing the solution for concentric circles, the general criterion for a Steiner chain of \"n\" circles can be written\n\nIf a multicyclic annular Steiner chain has \"n\" total circles and wraps around \"m\" times before closing, the angle between Steiner-chain circles equals\n\nIn other respects, the feasibility criterion is unchanged.\n\nCircle inversion transforms one Steiner chain into another with the same number of circles.\n\nIn the transformed chain, the tangent points between adjacent circles of the Steiner chain all lie on a circle, namely the concentric circle midway between the two fixed concentric circles. Since tangencies and circles are preserved under inversion, this property of all tangencies lying on a circle is also true in the original chain. This property is also shared with the Pappus chain of circles, which can be construed as a special limiting case of the Steiner chain.\n\nIn the transformed chain, the tangent lines from O to the Steiner chain circles are separated by equal angles. In the original chain, this corresponds to equal angles between the tangent circles that pass through the center of inversion used to transform the original circles into a concentric pair.\n\nIn the transformed chain, the \"n\" lines connecting the pairs of tangent points of the Steiner circles with the concentric circles all pass through O, the common center. Similarly, the \"n\" lines tangent to each pair of adjacent circles in the Steiner chain also pass through O. Since lines through the center of inversion are invariant under inversion, and since tangency and concurrence are preserved under inversion, the 2\"n\" lines connecting the corresponding points in the original chain also pass through a single point, O.\n\nA Steiner chain between two non-intersecting circles can always be transformed into another Steiner chain of equally sized circles sandwiched between two concentric circles. Therefore, any such Steiner chain belongs to an infinite family of Steiner chains related by rotation of the transformed chain about O, the common center of the transformed bounding circles.\n\nThe centers of the circles of a Steiner chain lie on a conic section. For example, if the smaller given circle lies within the larger, the centers lie on an ellipse. This is true for any set of circles that are internally tangent to one given circle and externally tangent to the other; such systems of circles appear in the Pappus chain, the problem of Apollonius, and the three-dimensional Soddy's hexlet. Similarly, if some circles of the Steiner chain are externally tangent to both given circles, their centers must lie on a hyperbola, whereas those that are internally tangent to both lie on a different hyperbola.\n\nThe circles of the Steiner chain are tangent to two fixed circles, denoted here as α and β, where β is enclosed by α. Let the radii of these two circles be denoted as \"r\" and \"r\", respectively, and let their respective centers be the points A and B. Let the radius, diameter and center point of the \"k\" circle of the Steiner chain be denoted as \"r\", \"d\" and P, respectively.\n\nAll the centers of the circles in the Steiner chain are located on a common ellipse, for the following reason. The sum of the distances from the center point of the \"k\" circle of the Steiner chain to the two centers A and B of the fixed circles equals a constant\n\nThus, for all the centers of the circles of the Steiner chain, the sum of distances to A and B equals the same constant, \"r\"+\"r\". This defines an ellipse, whose two foci are the points A and B, the centers of the circles, α and β, that sandwich the Steiner chain of circles.\n\nThe sum of distances to the foci equals twice the semi-major axis \"a\" of an ellipse; hence,\n\nLet \"p\" equal the distance between the foci, A and B. Then, the eccentricity \"e\" is defined by 2 \"ae\" = \"p\", or\n\nFrom these parameters, the semi-minor axis \"b\" and the semi-latus rectum \"L\" can be determined\n\nTherefore, the ellipse can be described by an equation in terms of its distance \"d\" to one focus\n\nwhere θ is the angle with the line joining the two foci.\n\nIf a Steiner chain has an even number of circles, then any two diametrically opposite circles in the chain can be taken as the two given circles of a new Steiner chain to which the original circles belong. If the original Steiner chain has \"n\" circles in \"m\" wraps, and the new chain has \"p\" circles in \"q\" wraps, then the equation holds\n\nA simple example occurs for Steiner chains of four circles (\"n\" = 4) and one wrap (\"m\" = 1). In this case, the given circles and the Steiner-chain circles are equivalent in that both types of circles are tangent to four others; more generally, Steiner-chain circles are tangent to four circles, but the two given circles are tangent to \"n\" circles. In this case, any pair of opposite members of the Steiner chain may be selected as the given circles of another Steiner chain that involves the original given circles. Since \"m\" = \"p\" = 1 and \"n\" = \"q\" = 4, Steiner's equation is satisfied:\n\nThe simplest generalization of a Steiner chain is to allow the given circles to touch or intersect one another. In the former case, this corresponds to a Pappus chain, which has an infinite number of circles.\n\nSoddy's hexlet is a three-dimensional generalization of a Steiner chain of six circles. The centers of the six spheres (the \"hexlet\") travel along the same ellipse as do the centers of the corresponding Steiner chain. The envelope of the hexlet spheres is a Dupin cyclide, the inversion of a torus. The six spheres are not only tangent to the inner and outer sphere, but also to two other spheres, centered above and below the plane of the hexlet centers.\n\nMultiple rings of Steiner chains are another generalization. An ordinary Steiner chain is obtained by inverting an annular chain of tangent circles bounded by two concentric circles. This may be generalized to inverting three or more concentric circles that sandwich annular chains of tangent circles.\n\nHierarchical Steiner chains are yet another generalization. If the two given circles of an ordinary Steiner chain are nested, i.e., if one lies entirely within the other, then the larger given circle circumscribes the Steiner-chain circles. In a hierarchical Steiner chain, each circle of a Steiner chain is itself the circumscribing given circle of another Steiner chain within it; this process may be repeated indefinitely, forming a fractal.\n\n\n\n\n"}
{"id": "8453671", "url": "https://en.wikipedia.org/wiki?curid=8453671", "title": "Subtractor", "text": "Subtractor\n\nIn electronics, a subtractor can be designed using the same approach as that of an adder. The binary subtraction process is summarized below. As with an adder, in the general case of calculations on multi-bit numbers, three bits are involved in performing the subtraction for each bit of the difference: the minuend (formula_1), subtrahend (formula_2), and a borrow in from the previous (less significant) bit order position (formula_3). The outputs are the difference bit (formula_4) and borrow bit formula_5. The subtractor is best understood by considering that the subtrahend and both borrow bits have negative weights, whereas the X and D bits are positive. The operation performed by the subtractor is to rewrite formula_6 (which can take the values -2, -1, 0, or 1) as the sum formula_7.\n\nSubtractors are usually implemented within a binary adder for only a small cost when using the standard two's complement notation, by providing an addition/subtraction selector to the carry-in and to invert the second operand.\n\nThe half subtractor is a combinational circuit which is used to perform subtraction of two bits. It has two inputs, the minuend formula_12 and subtrahend formula_13 and two outputs the difference formula_14 and borrow out formula_15. The borrow out signal is set when the subtractor needs to borrow from the next digit in a multi-digit subtraction. That is, formula_16 when formula_17. Since formula_12 and formula_13 are bits, formula_20 if and only if formula_21 and formula_22. An important point worth mentioning is that the half subtractor diagram aside implements formula_23 and not formula_24 since formula_15 on the diagram is given by\nThis is an important distinction to make since subtraction itself is not commutative, but the difference bit formula_14 is calculated using an XOR gate which is commutative.\n\nThe truth table for the half subtractor is:\n\nUsing the table above and a Karnaugh map, we find the following logic equations for formula_14 and formula_15:\n\nConsequently, a simplified half-subtract circuit, advantageously avoiding crossed traces in particular as well as a negate gate is:\nwhere lines to the right are outputs and others (from the top, bottom or left) are inputs.\n\nThe full subtractor is a combinational circuit which is used to perform subtraction of three input bits: the minuend formula_12, subtrahend formula_13, and borrow in formula_34. The full subtractor generates two output bits: the difference formula_14 and borrow out formula_15. formula_34 is set when the previous digit is borrowed from formula_12. Thus, formula_34 is also subtracted from formula_12 as well as the subtrahend formula_13. Or in symbols: formula_42. Like the half subtractor, the full subtractor generates a borrow out when it needs to borrow from the next digit. Since we are subtracting formula_12 by formula_13 and formula_34, a borrow out needs to be generated when formula_46. When a borrow out is generated, 2 is added in the current digit. (This is similar to the subtraction algorithm in decimal. Instead of adding 2, we add 10 when we borrow.) Therefore, formula_47.\n\nThe truth table for the full subtractor is:\n\nTherefore the equation is D=X⊕Y⊕B and \"B\"=X'B+X'Y+YB\n\n\n\n"}
{"id": "5575498", "url": "https://en.wikipedia.org/wiki?curid=5575498", "title": "Symbolic simulation", "text": "Symbolic simulation\n\nIn computer science, a simulation is a computation of the execution of some appropriately modelled state-transition system. Typically this process models the complete state of the system at individual points in a discrete linear time frame, computing each state sequentially from its predecessor. Models for computer programs or VLSI logic designs can be very easily simulated, as they often have an operational semantics which can be used directly for simulation.\n\nSymbolic simulation is a form of simulation where many possible executions of a system are considered simultaneously. This is typically achieved by augmenting the domain over which the simulation takes place. A symbolic variable can be used in the simulation state representation in order to index multiple executions of the system. For each possible valuation of these variables, there is a concrete system state that is being indirectly simulated.\n\nBecause symbolic simulation can cover many system executions in a single simulation, it can greatly reduce the size of verification problems. Techniques such as symbolic trajectory evaluation (STE) and generalized symbolic trajectory evaluation (GSTE) are based on this idea of symbolic simulation.\n\n"}
{"id": "47962742", "url": "https://en.wikipedia.org/wiki?curid=47962742", "title": "Tally marks", "text": "Tally marks\n\nTally marks, also called hash marks, are a unary numeral system. They are a form of numeral used for counting. They are most useful in counting or tallying ongoing results, such as the score in a game or sport, as no intermediate results need to be erased or discarded.\n\nHowever, because of the length of large numbers, tallies are not commonly used for static text. Notched sticks, known as tally sticks, were also historically used for this purpose.\n\nCounting aids other than body parts appear in the Upper Paleolithic. The oldest tally sticks date to between 35,000 and 25,000 years ago, in the form of notched bones found in the context of the European Aurignacian to Gravettian and in Africa's Late Stone Age.\n\nThe so-called \"Wolf bone\" is a prehistoric artifact discovered in 1937 in Czechoslovakia during excavations at Vestonice, Moravia, led by Karl Absolon. Dated to the Aurignacian, approximately 30,000 years ago, the bone is marked with 55 marks which may be tally marks. The head of an ivory Venus figurine was excavated close to the bone.\n\nThe Ishango bone, found in the Ishango region of the present-day Democratic Republic of Congo, is dated to over 20,000 years old. Upon discovery, it was thought to portray a series of prime numbers. In the book \"How Mathematics Happened: The First 50,000 Years\", Peter Rudman argues that the development of the concept of prime numbers could only have come about after the concept of division, which he dates to after 10,000 BC, with prime numbers probably not being understood until about 500 BC. He also writes that \"no attempt has been made to explain why a tally of something should exhibit multiples of two, prime numbers between 10 and 20, and some numbers that are almost multiples of 10.\" Alexander Marshack examined the Ishango bone microscopically, and concluded that it may represent a six-month lunar calendar.\n\nTally marks are typically clustered in groups of five for legibility. The cluster size 5 has the advantages of (a) easy conversion into decimal for higher arithmetic operations and (b) avoiding error, as humans can far more easily correctly identify a cluster of 5 than one of 10.\n\nRoman numerals, the Chinese numerals for one through three (一 二 三), and rod numerals were derived from tally marks, as possibly was the ogham script.\n\nBase 1 arithmetic notation system is an unary positional system similar to tally marks. It is rarely used as a practical base for counting due to its difficult readability. It is made by the concatenation of zero.\n\nThe numbers 1, 2, 3, 4, 5, ... would be represented in this system as\n\nBase 1 notation is widely used in type numbers of flour, the higher number represents a higher grind.\n"}
{"id": "583785", "url": "https://en.wikipedia.org/wiki?curid=583785", "title": "Tarski's undefinability theorem", "text": "Tarski's undefinability theorem\n\nTarski's undefinability theorem, stated and proved by Alfred Tarski in 1936, is an important limitative result in mathematical logic, the foundations of mathematics, and in formal semantics. Informally, the theorem states that \"arithmetical truth cannot be defined in arithmetic\".\n\nThe theorem applies more generally to any sufficiently strong formal system, showing that truth in the standard model of the system cannot be defined within the system.\n\nIn 1931, Kurt Gödel published the incompleteness theorems, which he proved in part by showing how to represent the syntax of formal logic within first-order arithmetic. Each expression of the formal language of arithmetic is assigned a distinct number. This procedure is known variously as Gödel numbering, \"coding\" and, more generally, as arithmetization. In particular, various \"sets\" of expressions are coded as sets of numbers. It turns out that for various syntactic properties (such as \"being a formula\", \"being a sentence\", etc.), these sets are computable. Moreover, any computable set of numbers can be defined by some arithmetical formula. For example, there are formulas in the language of arithmetic defining the set of codes for arithmetic sentences, and for provable arithmetic sentences.\n\nThe undefinability theorem shows that this encoding cannot be done for semantic concepts such as truth. It shows that no sufficiently rich interpreted language can represent its own semantics. A corollary is that any metalanguage capable of expressing the semantics of some object language must have expressive power exceeding that of the object language. The metalanguage includes primitive notions, axioms, and rules absent from the object language, so that there are theorems provable in the metalanguage not provable in the object language.\n\nThe undefinability theorem is conventionally attributed to Alfred Tarski. Gödel also discovered the undefinability theorem in 1930, while proving his incompleteness theorems published in 1931, and well before the 1936 publication of Tarski's work (Murawski 1998). While Gödel never published anything bearing on his independent discovery of undefinability, he did describe it in a 1931 letter to John von Neumann. Tarski had obtained almost all results of his 1936 paper \"Der Wahrheitsbegriff in den formalisierten Sprachen\" between 1929 and 1931, and spoke about them to Polish audiences. However, as he emphasized in the paper, the undefinability theorem was the only result he did not obtain earlier. According to the footnote of the undefinability theorem (Satz I) of the 1936 paper, the theorem and the sketch of the proof were added to the paper only after the paper was sent to print. When he presented the paper to the Warsaw Academy of Science on March 21, 1931, he wrote only some conjectures instead of the results after his own investigations and partly after Gödel's short report on the incompleteness theorems \"Einige metamathematische Resultate über Entscheidungsdefinitheit und Widerspruchsfreiheit\", Akd. der Wiss. in Wien, 1930.\n\nWe will first state a simplified version of Tarski's theorem, then state and prove in the next section the theorem Tarski actually proved in 1936.\nLet \"L\" be the language of first-order arithmetic, and let \"N\" be the standard structure for \"L\". Thus (\"L\", \"N\") is the \"interpreted first-order language of arithmetic.\" Each sentence \"x\" in \"L\" has a Gödel number \"g\"(\"x\"). Let \"T\" denote the set of \"L\"-sentences true in \"N\", and \"T\"* the set of Gödel numbers of the sentences in \"T\". The following theorem answers the question: Can \"T\"* be defined by a formula of first-order arithmetic?\n\n\"Tarski's undefinability theorem\": There is no \"L\"-formula \"True\"(\"n\") that defines \"T\"*.\nThat is, there is no \"L\"-formula \"True\"(\"n\") such that for every \"L\"-formula \"A\", \"True\"(\"g\"(\"A\")) ↔ \"A\" holds.\n\nInformally, the theorem says that given some formal arithmetic, the concept of truth in that arithmetic is not definable using the expressive means that that arithmetic affords. This implies a major limitation on the scope of \"self-representation.\" It \"is\" possible to define a formula \"True\"(\"n\") whose extension is \"T\"*, but only by drawing on a metalanguage whose expressive power goes beyond that of \"L\". For example, a truth predicate for first-order arithmetic can be defined in second-order arithmetic. However, this formula would only be able to define a truth predicate for sentences in the original language \"L\". To define a truth predicate for the metalanguage would require a still higher \"metametalanguage\", and so on.\n\nThe theorem just stated is a corollary of Post's theorem about the arithmetical hierarchy, proved some years after Tarski (1936). A semantic proof of Tarski's theorem from Post's theorem is obtained by reductio ad absurdum as follows. Assuming \"T\"* is arithmetically definable, there is a natural number \"n\" such that \"T\"* is definable by a formula at level formula_1 of the arithmetical hierarchy. However, \"T\"* is formula_2-hard for all \"k\". Thus the arithmetical hierarchy collapses at level \"n\", contradicting Post's theorem.\n\nTarski proved a stronger theorem than the one stated above, using an entirely syntactical method. The resulting theorem applies to any formal language with negation, and with sufficient capability for self-reference that the diagonal lemma holds. First-order arithmetic satisfies these preconditions, but the theorem applies to much more general formal systems.\n\n\"Tarski's undefinability theorem (general form)\": Let (\"L\",\"N\") be any interpreted formal language which includes negation and has a Gödel numbering \"g\"(\"x\") such that for every \"L\"-formula \"A\"(\"x\") there is a formula \"B\" such that \"B\" ↔ \"A\"(\"g\"(\"B\")) holds in \"N\". Let \"T\"* be the set of Gödel numbers of \"L\"-sentences true in \"N\". Then there is no \"L\"-formula \"True\"(\"n\") which defines \"T\"*. That is, there is no \"L\"-formula \"True\"(\"n\") such that for every \"L\"-formula \"A\", \"True\"(\"g\"(\"A\")) ↔ \"A\" is itself true in \"N\".\n\nThe proof of Tarski's undefinability theorem in this form is again by reductio ad absurdum. Suppose that an \"L\"- formula \"True\"(\"n\") defines \"T\"*. In particular, if \"A\" is a sentence of arithmetic then \"True\"(\"g\"(\"A\")) holds in \"N\" if and only if \"A\" is true in \"N\". Hence for all \"A\", the Tarski \"T\"-sentence \"True\"(\"g\"(\"A\")) ↔ \"A\" is true in \"N\". But the diagonal lemma yields a counterexample to this equivalence, by giving a \"Liar\" sentence \"S\" such that \"S\" ↔ ¬\"True\"(\"g\"(\"S\")) holds in \"N\". Thus no \"L\"-formula \"True\"(\"n\") can define \"T\"*. QED.\n\nThe formal machinery of this proof is wholly elementary except for the diagonalization that the diagonal lemma requires. The proof of the diagonal lemma is likewise surprisingly simple; for example, it does not invoke recursive functions in any way. The proof does assume that every \"L\"-formula has a Gödel number, but the specifics of a coding method are not required. Hence Tarski's theorem is much easier to motivate and prove than the more celebrated theorems of Gödel about the metamathematical properties of first-order arithmetic.\n\nSmullyan (1991, 2001) has argued forcefully that Tarski's undefinability theorem deserves much of the attention garnered by Gödel's incompleteness theorems. That the latter theorems have much to say about all of mathematics and more controversially, about a range of philosophical issues (e.g., Lucas 1961) is less than evident. Tarski's theorem, on the other hand, is not directly about mathematics but about the inherent limitations of any formal language sufficiently expressive to be of real interest. Such languages are necessarily capable of enough self-reference for the diagonal lemma to apply to them. The broader philosophical import of Tarski's theorem is more strikingly evident.\n\nAn interpreted language is \"strongly-semantically-self-representational\" exactly when the language contains predicates and function symbols defining all the semantic concepts specific to the language. Hence the required functions include the \"semantic valuation function\" mapping a formula \"A\" to its truth value ||\"A\"||, and the \"semantic denotation function\" mapping a term \"t\" to the object it denotes. Tarski's theorem then generalizes as follows: \"No sufficiently powerful language is strongly-semantically-self-representational\".\n\nThe undefinability theorem does not prevent truth in one theory from being defined in a stronger theory. For example, the set of (codes for) formulas of first-order Peano arithmetic that are true in \"N\" is definable by a formula in second order arithmetic. Similarly, the set of true formulas of the standard model of second order arithmetic (or \"n\"-th order arithmetic for any \"n\") can be defined by a formula in first-order ZFC.\n\n"}
{"id": "8776733", "url": "https://en.wikipedia.org/wiki?curid=8776733", "title": "Tetracoordinate", "text": "Tetracoordinate\n\nTetracoordinate in coordination chemistry generally refers to four ligands or atomic attachments to a single metal centre. Tetracoordinate species can form tetrahedral, square planar or pyramidal geometries. This is usually dictated by lone electron pairs on the metal centre.\n"}
{"id": "360581", "url": "https://en.wikipedia.org/wiki?curid=360581", "title": "Turán graph", "text": "Turán graph\n\nThe Turán graph \"T\"(\"n\",\"r\") is a complete multipartite graph formed by partitioning a set of \"n\" vertices into \"r\" subsets, with sizes as equal as possible, and connecting two vertices by an edge if and only if they belong to different subsets. The graph will have formula_1 subsets of size formula_2, and formula_3 subsets of size formula_4. That is, it is a complete \"r\"-partite graph\nEach vertex has degree either formula_6 or formula_7. The number of edges is\n\nIt is a regular graph, if \"n\" is divisible by \"r\".\n\nTurán graphs are named after Pál Turán, who used them to prove Turán's theorem, an important result in extremal graph theory.\n\nBy the pigeonhole principle, every set of \"r\" + 1 vertices in the Turán graph includes two vertices in the same partition subset; therefore, the Turán graph does not contain a clique of size \"r\" + 1. According to Turán's theorem, the Turán graph has the maximum possible number of edges among all (\"r\" + 1)-clique-free graphs with \"n\" vertices. Keevash and Sudakov (2003) show that the Turán graph is also the only (\"r\" + 1)-clique-free graph of order \"n\" in which every subset of α\"n\" vertices spans at least formula_9 edges, if α is sufficiently close to 1. The Erdős–Stone theorem extends Turán's theorem by bounding the number of edges in a graph that does not have a fixed Turán graph as a subgraph. Via this theorem, similar bounds in extremal graph theory can be proven for any excluded subgraph, depending on the chromatic number of the subgraph.\n\nSeveral choices of the parameter \"r\" in a Turán graph lead to notable graphs that have been independently studied.\n\nThe Turán graph \"T\"(2\"n\",\"n\") can be formed by removing a perfect matching from a complete graph \"K\". As showed, this graph has boxicity exactly \"n\"; it is sometimes known as the \"Roberts graph\". This graph is also the 1-skeleton of an \"n\"-dimensional cross-polytope; for instance, the graph \"T\"(6,3) = \"K\" is the octahedral graph, the graph of the regular octahedron. If \"n\" couples go to a party, and each person shakes hands with every person except his or her partner, then this graph describes the set of handshakes that take place; for this reason it is also called the cocktail party graph.\n\nThe Turán graph \"T\"(\"n\",2) is a complete bipartite graph and, when \"n\" is even, a Moore graph. When \"r\" is a divisor of \"n\", the Turán graph is symmetric and strongly regular, although some authors consider Turán graphs to be a trivial case of strong regularity and therefore exclude them from the definition of a strongly regular graph.\n\nThe Turán graph formula_10 has 32 maximal cliques, where\n3\"a\" + 2\"b\" = \"n\" and \"b\" ≤ 2; each maximal clique is formed by choosing one vertex from each partition subset. This is the largest number of maximal cliques possible among all \"n\"-vertex graphs regardless of the number of edges in the graph (Moon and Moser 1965); these graphs are sometimes called Moon–Moser graphs.\n\nEvery Turán graph is a cograph; that is, it can be formed from individual vertices by a sequence of disjoint union and complement operations. Specifically, such a sequence can begin by forming each of the independent sets of the Turán graph as a disjoint union of isolated vertices. Then, the overall graph is the complement of the disjoint union of the complements of these independent sets.\n\nChao and Novacky (1982) show that the Turán graphs are \"chromatically unique\": no other graphs have the same chromatic polynomials. Nikiforov (2005) uses Turán graphs to supply a lower bound for the sum of the \"k\"th eigenvalues of a graph and its complement.\n\nFalls, Powell, and Snoeyink develop an efficient algorithm for finding clusters of orthologous groups of genes in genome data, by representing the data as a graph and searching for large Turán subgraphs.\n\nTurán graphs also have some interesting properties related to geometric graph theory. Pór and Wood (2005) give a lower bound of Ω((\"rn\")) on the volume of any three-dimensional grid embedding of the Turán graph. Witsenhausen (1974) conjectures that the maximum sum of squared distances, among \"n\" points with unit diameter in R, is attained for a configuration formed by embedding a Turán graph onto the vertices of a regular simplex.\n\nAn \"n\"-vertex graph \"G\" is a subgraph of a Turán graph \"T\"(\"n\",\"r\") if and only if \"G\" admits an equitable coloring with \"r\" colors. The partition of the Turán graph into independent sets corresponds to the partition of \"G\" into color classes. In particular, the Turán graph is the unique maximal \"n\"-vertex graph with an \"r\"-color equitable coloring.\n\n\n"}
{"id": "45391102", "url": "https://en.wikipedia.org/wiki?curid=45391102", "title": "Viral dynamics", "text": "Viral dynamics\n\nViral dynamics is a field of applied mathematics concerned with describing the progression of viral infections within a host organism. It employs a family of mathematical models that describe changes over time in the populations of cells targeted by the virus and the viral load. These equations may also track competition between different viral strains and the influence of immune responses. The original viral dynamics models were inspired by compartmental epidemic models (e.g. the SI model), with which they continue to share many common mathematical features, such as the concept of the basic reproductive ratio (\"R\"). The major distinction between these fields is in the scale at which the models operate: while epidemiological models track the spread of infection between individuals within a population (i.e. \"between host\"), viral dynamics models track the spread of infection between cells within an individual (i.e. \"within host\"). Analyses employing viral dynamic models have been used extensively to study HIV, hepatitis B virus, and hepatitis C virus, among other infections\n\n"}
