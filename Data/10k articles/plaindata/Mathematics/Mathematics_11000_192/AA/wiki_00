{"id": "630022", "url": "https://en.wikipedia.org/wiki?curid=630022", "title": "Abstract nonsense", "text": "Abstract nonsense\n\nIn mathematics, abstract nonsense, general abstract nonsense, generalized abstract nonsense, and general nonsense are terms used by mathematicians to describe abstract methods related to category theory and homological algebra. More generally, “abstract nonsense” may refer to a proof that relies on category-theoretic methods, or even to the study of category theory itself.\n\nRoughly speaking, category theory is the study of the general form, that is, categories of mathematical theories, without regard to their content. As a result, mathematical proofs that rely on category-theoretic ideas often seem out of context, somewhat akin to a non sequitur. Authors sometimes dub these proofs “abstract nonsense” as a light-hearted way of alerting readers to their abstract nature. Labeling an argument \"abstract nonsense\" is usually \"not\" intended to be derogatory, and is instead used jokingly, in a self-deprecating way, affectionately, or even as a compliment to the generality of the argument.\n\nCertain ideas and constructions in mathematics share a uniformity throughout many domains, unified by category theory. Typical methods include the use of classifying spaces and universal properties, use of the Yoneda lemma, natural transformations between functors, and diagram chasing.\n\nWhen an audience can be assumed to be familiar with the general form of such arguments, mathematicians will use the expression \"Such and such is true by abstract nonsense\" rather than provide an elaborate explanation of particulars. For example, one might say \"By abstract nonsense, products are unique up to isomorphism when they exist\", instead of arguing about how these isomorphisms can be derived from the universal property that defines the product. This allows one to skip proof details that can be considered trivial or not providing much insight, focusing instead on genuinely innovative parts of a larger proof.\n\nThe term predates the foundation of category theory as a subject itself. Referring to a joint paper with Samuel Eilenberg that introduced the notion of a \"category\" in 1942, Saunders Mac Lane wrote the subject was 'then called \"general abstract nonsense\"'. The term is often used to describe the application of category theory and its techniques to less abstract domains.\n\nThe term is believed to have been coined by the mathematician Norman Steenrod, himself one of the developers of the categorical point of view.\n\nConsider the example of showing that a 3-manifold \"M\" admits a map to the 2-sphere that is non-trivial (i.e. non-homotopic to a constant map), when the 2nd Betti number of \"M\" is positive. This means the 2nd cohomology group has positive rank (by the universal coefficient theorem for cohomology), so it has a non-zero element. The properties of Eilenberg–MacLane spaces then give a corresponding non-trivial map \"f\" from \"M\" to the infinite-dimensional complex projective space CP, since it is a \"K\"(Z,2) Eilenberg–MacLane space. The space CP can be realized as a CW complex with exactly one cell in each even dimension and no cells in odd dimension, while \"M\" can be realized with no cells in dimensions above 3, so by the cellular approximation theorem there is a map homotopic to \"f\" that maps \"M\" into the 3-skeleton of CP, which is the 2-sphere.\n\nThough this proof establishes the truth of the statement in question, the proof technique has little to do with the topology or geometry of the 2-sphere, let alone 3-manifolds, as it relies on more general categorical principles. Because of the reliance on these abstract principles, the result is independent of subtler geometric details, so offers little geometric insight into the nature of such a map. On the other hand, the proof is surprisingly short and clean, and a “hands-on” approach involving the explicit construction of such a map would be potentially laborious.\n\n"}
{"id": "39479572", "url": "https://en.wikipedia.org/wiki?curid=39479572", "title": "Adjacent-vertex-distinguishing-total coloring", "text": "Adjacent-vertex-distinguishing-total coloring\n\nIn graph theory, a total coloring is a coloring on the vertices and edges of a graph such that:\n\n(1). no adjacent vertices have the same color;\n\n(2). no adjacent edges have the same color; and\n\n(3). no edge and its endvertices are assigned the same color.\n\nIn 2005, Zhang et al. added a restriction to the definition of total coloring and proposed a new type of coloring defined as follows.\n\nLet \"G\" = (\"V\",\"E\") be a simple graph endowed with a total coloring φ, and let \"u\" be a vertex of \"G\". The set of colors that occurs in the vertex \"u\" is defined as \"C\"(\"u\") = {\"φ\"(\"u\")} ∪ {\"φ\"(\"uv\") | \"uv\" ∈ \"E\"(\"G\")}. Two vertices \"u\",\"v\" ∈ \"V\"(\"G\") are distinguishable if their color-sets are distinct, i.e., \"C\"(\"u\") ≠ \"C\"(\"v\").\n\nIn graph theory, a total coloring is an adjacent-vertex-distinguishing-total-coloring (AVD-total-coloring) if it has the following additional property:\n\n(4). for every two adjacent vertices \"u\",\"v\" of a graph \"G\", their colors-sets are distinct from each other, i.e., \"C\"(\"u\") ≠ \"C\"(\"v\").\n\nThe adjacent-vertex-distinguishing-total-chromatic number \"χ\"(\"G\") of a graph \"G\" is the least number of colors needed in an AVD-total-coloring of \"G\".\n\nThe following lower bound for the AVD-total chromatic number can be obtained from the definition of AVD-total-coloring: If a simple graph \"G\" has two adjacent vertices of maximum degree, then \"χ\"(\"G\") ≥ Δ(\"G\") + 2. Otherwise, if a simple graph \"G\" does not have two adjacent vertices of maximum degree, then \"χ\"(\"G\") ≥ Δ(\"G\") + 1.\n\nIn 2005, Zhang et al. determined the AVD-total-chromatic number for some classes of graphs, and based in their results they conjectured the following.\n\nAVD-Total-Coloring Conjecture. (Zhang et al.)\n\nThe AVD-Total-Coloring Conjecture is known to hold for some classes of graphs, such as complete graphs, graphs with Δ=3, and all bipartite graphs.\n\nIn 2012, Huang et al. showed that \"χ\"(\"G\") ≤ 2Δ(\"G\")\nfor any simple graph \"G\" with maximum degree Δ(\"G\") > 2.\nIn 2014, Papaioannou and Raftopoulou described an algorithmic procedure that gives a \n7-AVD-total-colouring for any 4-regular graph.\n\n"}
{"id": "16838651", "url": "https://en.wikipedia.org/wiki?curid=16838651", "title": "Arithmetic derivative", "text": "Arithmetic derivative\n\nIn number theory, the Lagarias arithmetic derivative, or number derivative, is a function defined for integers, based on prime factorization, by analogy with the product rule for the derivative of a function that is used in mathematical analysis.\n\nThere are many versions of \"arithmetic derivatives\", including the one discussed in this article (the Lagarias arithmetic derivative), such as Ihara's arithmetic derivative and Buium's arithmetic derivatives.\n\nFor natural numbers the arithmetic derivative is defined as follows:\n\n\nE. J. Barbeau was most likely the first person to formalize this definition. He also extended it to all integers by proving that formula_5 uniquely defines the derivative over the integers. Barbeau also further extended it to rational numbers, showing that the familiar quotient rule gives a well-defined derivative on Q:\n\nVictor Ufnarovski and Bo Åhlander expanded it to certain irrationals. In these extensions, the formula above still applies, but the exponents formula_7 are allowed to be arbitrary rational numbers.\n\nThe Leibniz rule implies that formula_8 (take formula_9) and formula_10 (take formula_11).\n\nThe \"power rule\" is also valid for the arithmetic derivative. For any integers and :\n\nThis allows one to compute the derivative from the prime factorisation of an integer, formula_13:\n\nFor example: \n\nor\n\nThe sequence of number derivatives for begins :\n\nThe \"logarithmic derivative\" formula_18 is a totally additive function: formula_19\n\nE. J. Barbeau examined bounds of the arithmetic derivative. He found that the arithmetic derivative of natural numbers is bounded by\nwhere p is the least prime in \"n\" and \n\nwhere s is the number of prime factors in \"n\".\nIn both bounds above, equality always occurs when \"n\" is a perfect power of 2, that is formula_22 for some \"m\".\n\nAlexander Loiko, Jonas Olsson and Niklas Dahl found that it is impossible to find similar bounds for the arithmetic derivative extended to rational numbers by proving that between any two rational numbers there are other rationals with arbitrary large or small derivatives.\n\nWe have \n\nand\n\nfor any δ>0, where \n\nVictor Ufnarovski and Bo Åhlander have detailed the function's connection to famous number-theoretic conjectures like the twin prime conjecture, the prime triples conjecture, and Goldbach's conjecture. For example, Goldbach's conjecture would imply, for each \"k\" > 1 the existence of an \"n\" so that \"n\"<nowiki>'</nowiki> = 2\"k\". The twin prime conjecture would imply that there are infinitely many \"k\" for which \"k\"<nowiki>\"</nowiki> = 1.\n\n"}
{"id": "40725", "url": "https://en.wikipedia.org/wiki?curid=40725", "title": "Arithmetic shift", "text": "Arithmetic shift\n\nIn computer programming, an arithmetic shift is a shift operator, sometimes termed a signed shift (though it is not restricted to signed operands). The two basic types are the arithmetic left shift and the arithmetic right shift. For binary numbers it is a bitwise operation that shifts all of the bits of its operand; every bit in the operand is simply moved a given number of bit positions, and the vacant bit-positions are filled in. Instead of being filled with all 0s, as in logical shift, when shifting to the right, the leftmost bit (usually the sign bit in signed integer representations) is replicated to fill in all the vacant positions (this is a kind of sign extension).\n\nSome authors prefer the terms \"sticky right-shift\" and \"zero-fill right-shift\" for arithmetic and logical shifts respectively.\n\nArithmetic shifts can be useful as efficient ways to perform multiplication or division of signed integers by powers of two. Shifting left by \"n\" bits on a signed or unsigned binary number has the effect of multiplying it by 2. Shifting right by \"n\" bits on a two's complement \"signed\" binary number has the effect of dividing it by 2, but it always rounds down (towards negative infinity). This is different from the way rounding is usually done in signed integer division (which rounds towards 0). This discrepancy has led to bugs in more than one compiler.\n\nFor example, in the x86 instruction set, the SAR instruction (arithmetic right shift) divides a signed number by a power of two, rounding towards negative infinity. However, the IDIV instruction (signed divide) divides a signed number, rounding towards zero. So a SAR instruction cannot be substituted for an IDIV by power of two instruction nor vice versa.\n\nThe formal definition of an arithmetic shift, from Federal Standard 1037C is that it is:\n\nAn important word in the FS 1073C definition is \"usually\".\n\nArithmetic \"left\" shifts are equivalent to multiplication by a (positive, integral) power of the radix (e.g., a multiplication by a power of 2 for binary numbers). Arithmetic left shifts are, with two exceptions, identical in effect to logical left shifts. Exception one is the minor trap that arithmetic shifts may trigger arithmetic overflow whereas logical shifts do not. Obviously, that exception occurs in real world use cases only if a trigger signal for such an overflow is needed by the design it is used for. Exception two is the MSB is preserved. Processors usually do not offer logical and arithmetic left shift operations with a significant difference, if any.\n\nHowever, arithmetic \"right\" shifts are major traps for the unwary, specifically in treating rounding of negative integers. For example, in the usual two's complement representation of negative integers, −1 is represented as all 1's. For an 8-bit signed integer this is 1111 1111. An arithmetic right-shift by 1 (or 2, 3, …, 7) yields 1111 1111 again, which is still −1. This corresponds to rounding down (towards negative infinity), but is not the usual convention for division.\n\nIt is frequently stated that arithmetic right shifts are equivalent to division by a (positive, integral) power of the radix (e.g., a division by a power of 2 for binary numbers), and hence that division by a power of the radix can be optimized by implementing it as an arithmetic right shift. (A shifter is much simpler than a divider. On most processors, shift instructions will execute faster than division instructions.) Large number of 1960s and 1970s programming handbooks, manuals, and other specifications from companies and institutions such as DEC, IBM, Data General, and ANSI make such incorrect statements .\n\nLogical right shifts are equivalent to division by a power of the radix (usually 2) only for positive or unsigned numbers. Arithmetic right shifts are equivalent to logical right shifts for positive signed numbers. Arithmetic right shifts for negative numbers in N−1's complement (usually two's complement) is roughly equivalent to division by a power of the radix (usually 2), where for odd numbers rounding downwards is applied (not towards 0 as usually expected).\n\nArithmetic right shifts for negative numbers are equivalent to division using rounding towards 0 in one's complement representation of signed numbers as was used by some historic computers, but this is no longer in general use.\n\nThe (1999) ISO standard for the programming language C defines the right shift operator in terms of divisions by powers of 2. Because of the above-stated non-equivalence, the standard explicitly excludes from that definition the right shifts of signed numbers that have negative values. It does not specify the behaviour of the right shift operator in such circumstances, but instead requires each individual C compiler to define the behaviour of shifting negative values right.\n\nIn applications where consistent rounding down is desired, arithmetic right shifts for signed values are useful. An example is in downscaling raster coordinates by a power of two, which maintains even spacing. For example, right shift by 1 sends 0, 1, 2, 3, 4, 5, … to 0, 0, 1, 1, 2, 2, …, and −1, −2, −3, −4, … to −1, −1, −2, −2, …, maintaining even spacing as −2, −2, −1, −1, 0, 0, 1, 1, 2, 2, … In contrast, integer division with rounding towards zero sends −1, 0, and 1 all to 0 (3 points instead of 2), yielding −2, −1, −1, 0, 0, 0, 1, 1, 2, 2, … instead, which is irregular at 0.\n\n"}
{"id": "3770438", "url": "https://en.wikipedia.org/wiki?curid=3770438", "title": "Association for Symbolic Logic", "text": "Association for Symbolic Logic\n\nThe Association for Symbolic Logic (ASL) is an international organization of specialists in mathematical logic and philosophical logic. The ASL was founded in 1936, and its first president was Alonzo Church. The current president of the ASL is Ulrich Kohlenbach.\n\nThe ASL publishes books and academic journals. Its three official journals are:\n\nIn addition, the ASL has a sponsored journal: \n\nThe organization played a part in publishing the collected writings of Kurt Gödel.\n\nThe ASL holds two main meetings every year, one in North America and one in Europe (the latter known as the \"Logic Colloquium\"). In addition, the ASL regularly holds joint meetings with both the American Mathematical Society (\"AMS\") and the American Philosophical Association (\"APA\"), and sponsors meetings in many different countries every year.\n\nThe association periodically presents a number of prizes and awards.\n\nThe Karp Prize is awarded by the association every five years for an outstanding paper or book in the field of symbolic logic. It consists of a cash award and was established in 1973 in memory of Professor Carol Karp. \n\nRecipients comprise:<br>\nSource: ASL\n\nThe Sacks Prize is awarded for the most outstanding doctoral dissertation in mathematical logic. It consists of a cash award and was established in 1999 to honor Professor Gerald Sacks of MIT and Harvard.\nThe recipients comprise:\n\nThe Shoenfield Prize is awarded for outstanding expository writing in the field of logic and honors the name of Joseph R. Shoenfield.\n\n"}
{"id": "5653826", "url": "https://en.wikipedia.org/wiki?curid=5653826", "title": "BUN-to-creatinine ratio", "text": "BUN-to-creatinine ratio\n\nIn medicine, the BUN-to-creatinine ratio is the ratio of two serum laboratory values, the blood urea nitrogen (BUN) (mg/dL) and serum creatinine (Cr) (mg/dL). Outside the United States, particularly in Canada and Europe, the truncated term urea is used (though it is still the same blood chemical) and the units are different (mmol/L). The units of creatinine are also different (μmol/L), and this value is termed the urea-to-creatinine ratio. The ratio may be used to determine the cause of acute kidney injury or dehydration.\n\nThe principle behind this ratio is the fact that both urea (BUN) and creatinine are freely filtered by the glomerulus; however, urea reabsorbed by the tubules can be regulated (increased or decreased) whereas creatinine reabsorption remains the same (minimal reabsorption).\n\nUrea and creatinine are nitrogenous end products of metabolism. Urea is the primary metabolite derived from dietary protein and tissue protein turnover. Creatinine is the product of muscle creatine catabolism. Both are relatively small molecules (60 and 113 daltons, respectively) that distribute throughout total body water. In Europe, the whole urea molecule is assayed, whereas in the United States only the nitrogen component of urea (the blood or serum urea nitrogen, i.e., BUN or SUN) is measured. The BUN, then, is roughly one-half (7/15 or 0.466) of the blood urea.\n\nThe normal range of urea nitrogen in blood or serum is 5 to 20 mg/dl, or 1.8 to 7.1 mmol urea per liter. The range is wide because of normal variations due to protein intake, endogenous protein catabolism, state of hydration, hepatic urea synthesis, and renal urea excretion. A BUN of 15 mg/dl would represent significantly impaired function for a woman in the thirtieth week of gestation. Her higher glomerular filtration rate (GFR), expanded extracellular fluid volume, and anabolism in the developing fetus contribute to her relatively low BUN of 5 to 7 mg/dl. In contrast, the rugged rancher who eats in excess of 125 g protein each day may have a normal BUN of 20 mg/dl.\n\nThe normal serum creatinine (sCr) varies with the subject's body muscle mass and with the technique used to measure it. For the adult male, the normal range is 0.6 to 1.2 mg/dl, or 53 to 106 μmol/L by the kinetic or enzymatic method, and 0.8 to 1.5 mg/dl, or 70 to 133 μmol/L by the older manual Jaffé reaction. For the adult female, with her generally lower muscle mass, the normal range is 0.5 to 1.1 mg/dl, or 44 to 97 μmol/L by the enzymatic method.\n\nMultiple methods for analysis of BUN and creatinine have evolved over the years. Most of those in current use are automated and give clinically reliable and reproducible results.\n\nThere are two general methods for the measurement of urea nitrogen. The diacetyl, or Fearon, reaction develops a yellow chromogen with urea, and this is quantified by photometry. It has been modified for use in autoanalyzers and generally gives relatively accurate results. It still has limited specificity, however, as illustrated by spurious elevations with sulfonylurea compounds, and by colorimetric interference from hemoglobin when whole blood is used.\n\nIn the more specific enzymatic methods, the enzyme urease converts urea to ammonia and carbonic acid. These products, which are proportional to the concentration of urea in the sample, are assayed in a variety of systems, some of which are automated. One system checks the decrease in absorbance at 340 mm when the ammonia reacts with alpha-ketoglutaric acid. The Astra system measures the rate of increase in conductivity of the solution in which urea is hydrolyzed.\n\nEven though the test is now performed mostly on serum, the term BUN is still retained by convention. The specimen should not be collected in tubes containing sodium fluoride because the fluoride inhibits urease. Also chloral hydrate and guanethidine have been observed to increase BUN values.\n\nThe 1886 Jaffé reaction, in which creatinine is treated with an alkaline picrate solution to yield a red complex, is still the basis of most commonly used methods for measuring creatinine. This reaction is nonspecific and subject to interference from many noncreatinine chromogens, including acetone, acetoacetate, pyruvate, ascorbic acid, glucose, cephalosporins, barbiturates, and protein. It is also sensitive to pH and temperature changes. One or another of the many modifications designed to nullify these sources of error is used in most clinical laboratories today. For example, the recent kinetic-rate modification, which isolates the brief time interval during which only true creatinine contributes to total color formation, is the basis of the Astra modular system.\n\nMore specific, non-Jaffé assays have also been developed. One of these, an automated dry-slide enzymatic method, measures ammonia generated when creatinine is hydrolyzed by creatinine iminohydrolase. Its simplicity, precision, and speed highly recommend it for routine use in the clinical laboratory. Only 5-fluorocytosine interferes significantly with the test.\n\nCreatinine must be determined in plasma or serum and not whole blood because erythrocytes contain considerable amounts of noncreatinine chromogens. To minimize the conversion of creatine to creatinine, specimens must be as fresh as possible and maintained at pH 7 during storage.\n\nThe amount of urea produced varies with substrate delivery to the liver and the adequacy of liver function. It is increased by a high-protein diet, by gastrointestinal bleeding (based on plasma protein level of 7.5 g/dl and a hemoglobin of 15 g/dl, 500 ml of whole blood is equivalent to 100 g protein), by catabolic processes such as fever or infection, and by antianabolic drugs such as tetracyclines (except doxycycline) or glucocorticoids. It is decreased by low-protein diet, malnutrition or starvation, and by impaired metabolic activity in the liver due to parenchymal liver disease or, rarely, to congenital deficiency of urea cycle enzymes. The normal subject on a 70 g protein diet produces about 12 g of urea each day.\n\nThis newly synthesized urea distributes throughout total body water. Some of it is recycled through the enterohepatic circulation. Usually, a small amount (less than 0.5 g/day) is lost through the gastrointestinal tract, lungs, and skin; during exercise, a substantial fraction may be excreted in sweat. The bulk of the urea, about 10 g each day, is excreted by the kidney in a process that begins with glomerular filtration. At high urine flow rates (greater than 2 ml/min), 40% of the filtered load is reabsorbed, and at flow rates lower than 2 ml/min, reabsorption may increase to 60%. Low flow, as in urinary tract obstruction, allows more time for reabsorption and is often associated with increases in antidiuretic hormone (ADH), which increases the permeability of the terminal collecting tubule to urea. During ADH-induced antidiuresis, urea secretion contributes to the intratubular concentration of urea. The subsequent buildup of urea in the inner medulla is critical to the process of urinary concentration. Reabsorption is also increased by volume contraction, reduced renal plasma flow as in congestive heart failure, and decreased glomerular filtration.\n\nCreatinine formation begins with the transamidination from arginine to glycine to form glycocyamine or guanidoacetic acid (GAA). This reaction occurs primarily in the kidneys, but also in the mucosa of the small intestine and the pancreas. The GAA is transported to the liver where it is methylated by S-adenosyl methionine (SAM) to form creatine. Creatine enters the circulation, and 90% of it is taken up and stored by muscle tissue.\n\nNormal serum values\nSerum Ratios\nAn elevated BUN:Cr due to a low or low-normal creatinine and a BUN within the reference range is unlikely to be of clinical significance.\n\nThe ratio is predictive of prerenal injury when BUN:Cr exceeds 20 or when urea:Cr exceeds 100. In prerenal injury, urea increases disproportionately to creatinine due to enhanced proximal tubular reabsorption that follows the enhanced transport of sodium and water.\n\nThe ratio is useful for the diagnosis of bleeding from the gastrointestinal (GI) tract in patients who do not present with overt vomiting of blood. In children, a BUN:Cr ratio of 30 or greater has a sensitivity of 68.8% and a specificity of 98% for upper gastrointestinal bleeding.\n\nA common assumption is that the ratio is elevated because of amino acid digestion, since blood (excluding water) consists largely of the protein hemoglobin and is broken down by digestive enzymes of the upper GI tract into amino acids, which are then reabsorbed in the GI tract and broken down into urea. However, elevated BUN:Cr ratios are not observed when other high protein loads (e.g., steak) are consumed. Renal hypoperfusion secondary to the blood lost from the GI bleed has been postulated to explain the elevated BUN:Cr ratio. However, other research has found that renal hypoperfusion cannot fully explain the elevation.\n\nBecause of decreased muscle mass, elderly patients may have an elevated BUN:Cr at baseline.\n\nHypercatabolic states, high-dose glucocorticoids, and resorption of large hematomas have all been cited as causes of a disproportionate rise in BUN relative to the creatinine.\n"}
{"id": "53605449", "url": "https://en.wikipedia.org/wiki?curid=53605449", "title": "Basis theorem (computability)", "text": "Basis theorem (computability)\n\nIn computability theory, there are a number of basis theorems. These theorems show that particular kinds of sets always must have some members that are, in terms of Turing degree, not too complicated. One family of basis theorems concern nonempty effectively closed sets (that is, nonempty formula_1 sets in the arithmetical hierarchy); these theorems are studied as part of classical computability theory. Another family of basis theorems concern nonempty lightface analytic sets (that is, formula_2 in the analytical hierarchy); these theorems are studied as part of hyperarithmetical theory.\n\nEffectively closed sets are a topic of study in classical computability theory. An effectively closed set is the set of all paths through some computable subtree of the binary tree formula_3. These sets are closed, in the topological sense, as subsets of the Cantor space formula_4, and the complement of an effective closed set is an effective open set in the sense of effective Polish spaces. Stephen Cole Kleene proved in 1952 that there is a nonempty, effectively closed set with no computable point (Cooper 1999, p. 134). There are basis theorems that show there must be points that are not \"too far\" from being computable, in an informal sense.\n\nA class formula_5 is a basis for effectively closed sets if every nonempty effectively closed set includes a member of \"X\" (Cooper 2003, p. 329). Basis theorems show that particular classes are bases in this sense. These theorems include (Cooper 1999, p. 134): \n\nIn the second bullet, a set \"X\" has hyperimmune-free degree if every total function from the natural numbers to the natural numbers is dominated by a total computable function.\n\nThere are also basis theorems for lightface formula_2 sets. These basis theorems are studied as part of hyperarithmetical theory. One theorem is the Gandy basis theorem, which is analogous to the low basis theorem. The Gandy basis theorem shows that each nonempty formula_2. set has an element that is hyperarithmetically low, that is, which has the same hyperdegree as Kleene's set formula_11.\n\n\n"}
{"id": "11219603", "url": "https://en.wikipedia.org/wiki?curid=11219603", "title": "Bracket (mathematics)", "text": "Bracket (mathematics)\n\nIn mathematics, various typographical forms of brackets are frequently used in mathematical notation such as parentheses ( ), square brackets [ ], braces { }, and angle brackets ⟨ ⟩. Generally such bracketing denotes some form of grouping: in evaluating an expression containing a bracketed sub-expression, the operators in the sub-expression take precedence over those surrounding it. Additionally, there are several uses and meanings for the various brackets.\n\nHistorically, other notations, such as the vinculum, were similarly used for grouping; in present-day use, these notations all have specific meanings. The earliest use of brackets to indicate aggregation (i.e. grouping) was suggested in 1608 by Christopher Clavius and in 1629 by Albert Girard.\n\nA variety of different symbols are used to represent angle brackets. In e-mail and other ASCII text it is common to use the less-than (codice_1) and greater-than (codice_2) signs to represent angle brackets, because ASCII does not include angle brackets. Unicode has three pairs of dedicated characters:\n\nIn LaTeX the markup is codice_3 and codice_4: formula_1.\n\nIn elementary algebra parentheses, ( ), are used to specify the order of operations. Terms inside the bracket are evaluated first; hence 2×(3 + 4) is 14 and 10 ÷ 5(1 + 0) is 2 and 8 ÷ 4(2 + 0) is 1 and (2×3) + 4 is 10. This notation is extended to cover more general algebra involving variables: for example formula_2. Square brackets are also often used in place of a second set of parentheses when they are nested, to provide a visual distinction.\n\nAlso in mathematical expressions in general, parentheses are used to indicate grouping (that is, which parts belong together) when necessary to avoid ambiguities, or for the sake of clarity. For example, in the formula (εη) = εη, used in the definition of composition of two natural transformations, the parentheses around εη serve to indicate that the indexing by \"X\" is applied to the composition εη, and not just its last component η.\n\nThe arguments to a function are frequently surrounded by brackets: formula_3. It is common to omit the parentheses around the argument when there is little chance of ambiguity, thus: formula_4.\n\nIn the cartesian coordinate system brackets are used to specify the coordinates of a point: (2,3) denotes the point with \"x\"-coordinate 2 and \"y\"-coordinate 3. \n\nThe inner product of two vectors is commonly written as formula_5, but the notation (\"a\", \"b\") is also used.\n\nBoth parentheses, ( ), and square brackets, [ ], can also be used to denote an interval. The notation formula_6 is used to indicate an interval from a to c that is inclusive of formula_7 but exclusive of formula_8. That is, formula_9 would be the set of all real numbers between 5 and 12, including 5 but not 12. The numbers may come as close as they like to 12, including 11.999 and so forth (with any finite number of 9s), but 12.0 is not included. In some European countries, the notation formula_10 is also used for this.\n\nThe endpoint adjoining the square bracket is known as \"closed\", while the endpoint adjoining the parenthesis is known as \"open\". If both types of brackets are the same, the entire interval may be referred to as \"closed\" or \"open\" as appropriate. Whenever infinity or negative infinity is used as an endpoint in the case of intervals on the real number line, it is always considered \"open\" and adjoined to a parenthesis. The endpoint can be closed when considering intervals on the extended real number line.\n\nBraces { } are used to identify the elements of a set: {\"a\",\"b\",\"c\"} denotes a set of three elements. \n\nAngle brackets are used in group theory to write group presentations, and to denote the subgroup generated by a collection of elements.\n\nAn explicitly given matrix is commonly written between large round or square brackets:\n\nThe notation\nstands for the \"n\"-th derivative of function \"f\", applied to argument \"x\". So, for example, if formula_13, then formula_14. This is to be contrasted with formula_15, the \"n\"-fold application of \"f\" to argument \"x\".\n\nThe notation (\"x\") is used to denote the \"falling factorial\", an \"n\"-th degree polynomial defined by\n\nConfusingly, the same notation may be encountered as representing the \"rising factorial\", also called \"Pochhammer symbol\". Another notation for the same is \"x\". It can be defined by\n\nIn quantum mechanics, angle brackets are also used as part of Dirac's formalism, bra–ket notation, to note vectors from the dual spaces of the bra formula_18 and the ket formula_19. \n\nIn statistical mechanics, angle brackets denote ensemble or time average.\n\nSquare brackets are used to denote the variable in polynomial rings. For example, formula_20 is the polynomial ring with the formula_21 variable and real number coefficients.\n\nIn group theory and ring theory, square brackets are used to denote the commutator. In group theory, the commutator <nowiki>[</nowiki>\"g\",\"h\"<nowiki>]</nowiki> is commonly defined as \"g\"\"h\"\"gh\". In ring theory, the commutator <nowiki>[</nowiki>\"a\",\"b\"<nowiki>]</nowiki> is defined as \"ab\" − \"ba\". Furthermore, in theory, braces are used to denote the anticommutator where {\"a\",\"b\"} is defined as \"ab\" + \"ba\". \n\nThe Lie bracket of a Lie algebra is a binary operation denoted by formula_22. By using the commutator as a Lie bracket, every associative algebra can be turned into a Lie algebra. There are many different forms of Lie bracket, in particular the Lie derivative and the Jacobi–Lie bracket.\n\nSquare brackets, as in , are sometimes used to denote the floor function, which rounds a real number down to the next integer. However the floor and ceiling functions are usually typeset with left and right square brackets where only the lower (for floor function) or upper (for ceiling function) horizontal bars are displayed, as in or .\n\nBraces, as in , may denote the fractional part of a real number.\n\n"}
{"id": "43215632", "url": "https://en.wikipedia.org/wiki?curid=43215632", "title": "Cantor algebra", "text": "Cantor algebra\n\nIn mathematics, a Cantor algebra, named after Georg Cantor, is one of two closely related Boolean algebras, one countable and one complete.\n\nThe countable Cantor algebra is the Boolean algebra of all clopen subsets of the Cantor set. This is the free Boolean algebra on a countable number of generators. Up to isomorphism, this is the only nontrivial Boolean algebra that is both countable and atomless.\n\nThe complete Cantor algebra is the complete Boolean algebra of Borel subsets of the reals modulo meager sets . It is isomorphic to the completion of the countable Cantor algebra. (The complete Cantor algebra is sometimes called the Cohen algebra, though \"Cohen algebra\" usually refers to a different type of Boolean algebra.) The complete Cantor algebra was studied by von Neumann in 1935 (later published as ), who showed that it is not isomorphic to the random algebra of Borel subsets modulo measure zero sets.\n\n"}
{"id": "535617", "url": "https://en.wikipedia.org/wiki?curid=535617", "title": "Category of metric spaces", "text": "Category of metric spaces\n\nIn category-theoretic mathematics, Met is a category that has metric spaces as its objects and metric maps (continuous functions between metric spaces that do not increase any pairwise distance) as its morphisms. This is a category because the composition of two metric maps is again a metric map. It was first considered by .\n\nThe monomorphisms in Met are the injective metric maps, maps that do not map two points into a single point. The epimorphisms are the metric maps in which the domain of the map has a dense image in the range. The isomorphisms are the isometries, metric maps that are one-to-one, onto, and distance-preserving.\n\nAs an example, the inclusion of the rational numbers into the real numbers is a monomorphism and an epimorphism, but it is clearly not an isomorphism; this example shows that Met is not a balanced category.\n\nThe empty metric space is the initial object of Met; any singleton metric space is a terminal object. Because the initial object and the terminal objects differ, there are no zero objects in Met.\n\nThe injective objects in Met are called injective metric spaces. Injective metric spaces were introduced and studied first by , prior to the study of Met as a category; they may also be defined intrinsically in terms of a Helly property of their metric balls, and because of this alternative definition Aronszajn and Panitchpakdi named these spaces \"hyperconvex spaces\". Any metric space has a smallest injective metric space into which it can be isometrically embedded, called its metric envelope or tight span.\n\nThe product of a finite set of metric spaces in Met is a metric space that has the cartesian product of the spaces as its points; the distance in the product space is given by the supremum of the distances in the base spaces. That is, it is the product metric with the sup norm. However, the product of an infinite set of metric spaces may not exist, because the distances in the base spaces may not have a supremum. That is, Met is not a complete category, but it is finitely complete. There is no coproduct in Met.\n\nThe \"forgetful\" functor Met → Set assigns to each metric space the underlying set of its points, and assigns to each metric map the underlying set-theoretic function. This functor is faithful, and therefore Met is a concrete category.\n\nMet is not the only category whose objects are metric spaces; others include the category of uniformly continuous functions, the category of Lipschitz functions and the category of quasi-Lipschitz mappings. The metric maps are both uniformly continuous and Lipschitz, with Lipschitz constant at most one.\n\n"}
{"id": "44638802", "url": "https://en.wikipedia.org/wiki?curid=44638802", "title": "Center for Data-Driven Discovery", "text": "Center for Data-Driven Discovery\n\nThe Center for Data-Driven Discovery is a multi-division research group at the California Institute of Technology, focusing on the methodologies for handling and analysis of large and complex data sets, facilitating the data-to-discovery process. It supports all applications of data-driven computing in various scientific domains, such as biology, physics, astronomy, geophysics, etc. It also functions as a catalyst for new collaborations and projects between different scientific disciplines, and between the campus and JPL, with especial interest in the sharing and transfer of methodologies, where the solutions from one field can be reapplied in another one.\n\nThe Center for Data-Driven Discovery is a part of a joint initiative with the Center for Data Science and Technology at JPL. It became operational Fall 2014. Also known as \"CD(cube)\".\n\n"}
{"id": "41766625", "url": "https://en.wikipedia.org/wiki?curid=41766625", "title": "Clock Constraints Specification Language", "text": "Clock Constraints Specification Language\n\nThe Clock Constraint Specification Language or CCSL, is a software language for modeling relations among so-called clocks. It is part of the time model defined in the UML Profile for MARTE.\n\nCCSL provides a concrete syntax to handle logical clocks. The term logical clock refers to Leslie Lamport's logical clocks and its usage in CCSL is directly inspired from Synchronous programming languages (like Esterel or Signal).\n\nA solver of CCSL constraints is implemented in the TimeSquare tool.\n"}
{"id": "50264", "url": "https://en.wikipedia.org/wiki?curid=50264", "title": "Codomain", "text": "Codomain\n\nIn mathematics, the codomain or target set of a function is the set into which all of the output of the function is constrained to fall. It is the set in the notation . The codomain is also sometimes referred to as the range but that term is ambiguous as it may also refer to the image.\n\nThe codomain is part of a function if it is defined as described in 1954 by Nicolas Bourbaki, namely a triple , with a functional subset of the Cartesian product and is the set of first components of the pairs in (the \"domain\"). The set is called the \"graph\" of the function. The set of all elements of the form , where ranges over the elements of the domain , is called the image of . In general, the image of a function is a subset of its codomain. Thus, it may not coincide with its codomain. Namely, a function that is not surjective has elements in its codomain for which the equation does not have a solution.\n\nAn alternative definition of \"function\" by Bourbaki [Bourbaki, \"op. cit.\", p. 77], namely as just a functional graph, does not include a codomain and is also widely used. For example in set theory it is desirable to permit the domain of a function to be a proper class , in which case there is formally no such thing as a triple . With such a definition functions do not have a codomain, although some authors still use it informally after introducing a function in the form .\n\nFor a function\n\ndefined by\n\nthe codomain of is formula_3, but does not map to any negative number. \nThus the image of is the set formula_4; i.e., the interval .\n\nAn alternative function is defined thus: \n\nWhile and map a given to the same number, they are not, in this view, the same function because they have different codomains. A third function can be defined to demonstrate why:\n\nThe domain of must be defined to be formula_4:\n\nThe compositions are denoted\n\nOn inspection, is not useful. It is true, unless defined otherwise, that the image of is not known; it is only known that it is a subset of formula_3. For this reason, it is possible that , when composed with , might receive an argument for which no output is defined – negative numbers are not elements of the domain of , which is the square root function.\n\nFunction composition therefore is a useful notion only when the \"codomain\" of the function on the right side of a composition (not its \"image\", which is a consequence of the function and could be unknown at the level of the composition) is the same as the domain of the function on the left side.\n\nThe codomain affects whether a function is a surjection, in that the function is surjective if and only if its codomain equals its image. In the example, is a surjection while is not. The codomain does not affect whether a function is an injection.\n\nA second example of the difference between codomain and image is demonstrated by the linear transformations between two vector spaces – in particular, all the linear transformations from formula_13 to itself, which can be represented by the matrices with real coefficients. Each matrix represents a map with the domain formula_13 and codomain formula_13. However, the image is uncertain. Some transformations may have image equal to the whole codomain (in this case the matrices with rank ) but many do not, instead mapping into some smaller subspace (the matrices with rank or ). Take for example the matrix given by\nwhich represents a linear transformation that maps the point to . The point is not in the image of , but is still in the codomain since linear transformations from formula_13 to formula_13 are of explicit relevance. Just like all matrices, represents a member of that set. Examining the differences between the image and codomain can often be useful for discovering properties of the function in question. For example, it can be concluded that does not have full rank since its image is smaller than the whole codomain.\n\n\n"}
{"id": "7975294", "url": "https://en.wikipedia.org/wiki?curid=7975294", "title": "Complex line", "text": "Complex line\n\nIn mathematics, a complex line is a one-dimensional affine subspace of a vector space over the complex numbers. A common point of confusion is that while a complex line has dimension one over C (hence the term \"line\"), it has dimension two over the real numbers R, and is topologically equivalent to a real plane, not a real line.\n\n"}
{"id": "4181062", "url": "https://en.wikipedia.org/wiki?curid=4181062", "title": "Dadda multiplier", "text": "Dadda multiplier\n\nThe Dadda multiplier is a hardware multiplier design invented by computer scientist Luigi Dadda in 1965. It is similar to the Wallace multiplier, but it is slightly faster (for all operand sizes) and requires fewer gates (for all but the smallest operand sizes).\n\nIn fact, Dadda and Wallace multipliers have the same three steps for two bit strings formula_1 and formula_2 of lengths formula_3 and formula_4 respectively:\n\n\nAs with the Wallace multiplier, the multiplication products of the first step carry different weights reflecting the magnitude of the original bit values in the multiplication. For example, the product of bits formula_8 has weight formula_9.\n\nUnlike Wallace multipliers that reduce as much as possible on each layer, Dadda multipliers attempt to minimize the number of gates used, as well as input/output delay. Because of this, Dadda multipliers have a less expensive reduction phase, but the final numbers may be a few bits longer, thus requiring slightly bigger adders.\n\nTo achieve a more optimal final product, the structure of the reduction process is governed by slightly more complex rules than in Wallace multipliers. \n\nThe progression of the reduction is controlled by a maximum-height sequence formula_10, defined by:\n\nThis yields a sequence like so:\n\nThe initial value of formula_13 is chosen as the largest value such that formula_14, where formula_15 and formula_16 are the number of bits in the input multiplicand and multiplier. The lesser of the two bit lengths will be the maximum height of each column of weights after the first stage of multiplication. For each stage formula_13 of the reduction, the goal of the algorithm is the reduce the height of each column so that it is less than or equal to the value of formula_10. \n\nFor each stage from formula_19, reduce each column starting at the lowest-weight column, formula_20 according to these rules:\n\nThe example in the adjacent image illustrates the reduction of an 8 × 8 multiplier, explained here.\n\nThe initial state formula_28 is chosen as formula_29, the largest value less than 8.\n\nStage formula_30, formula_29\nStage formula_42, formula_43\nStage formula_52, formula_53\nStage formula_59, formula_60\nAddition\n\nThe output of the last stage leaves 14 columns of height two or less which can be passed into a standard adder.\n\n"}
{"id": "54748286", "url": "https://en.wikipedia.org/wiki?curid=54748286", "title": "Dan Archdeacon", "text": "Dan Archdeacon\n\nDan Steven Archdeacon (1954–2015) was an American graph theorist specializing in topological graph theory, who served for many years as a professor of mathematics and statistics at the University of Vermont.\n\nArchdeacon was born on May 11, 1954 in Dayton, Ohio, and grew up in Centerville, Ohio.\nHe did his undergraduate studies at Earlham College, graduating in 1975.\nHe completed his Ph.D. in 1980 from Ohio State University, under the supervision of Henry Hatfield Glover, with a dissertation proving an analogue of Kuratowski's theorem for the projective plane. He took a position at the University of Vermont in 1982, joining fellow graph theorist and Ohio State graduate Jeff Dinitz, after previously working as an instructor at the University of Kansas.\nHe died of cancer on February 18, 2015, in Burlington, Vermont.\n\nIn 2003–2004, the University of Vermont named him as University Scholar.\nA special issue of the \"Australasian Journal of Combinatorics\" was published in his honor in 2017.\n"}
{"id": "864364", "url": "https://en.wikipedia.org/wiki?curid=864364", "title": "Dataflow", "text": "Dataflow\n\nDataflow is a term used in computing which has various meanings depending on application and the context in which the term is used. In the context of software architecture, data flow relates to stream processing or reactive programming.\n\nDataflow is a software paradigm based on the idea of disconnecting computational actors into stages (pipelines) that can execute concurrently. Dataflow can also be called stream processing or reactive programming.\n\nThere have been multiple data-flow/stream processing languages of various forms (see Stream processing). Data-flow hardware (see Dataflow architecture) is an alternative to the classic Von Neumann architecture. The most obvious example of data-flow programming is the subset known as reactive programming with spreadsheets. As a user enters new values, they are instantly transmitted to the next logical \"actor\" or formula for calculation.\n\nDistributed data flows have also been proposed as a programming abstraction that captures the dynamics of distributed multi-protocols. The data-centric perspective characteristic of data flow programming promotes high-level functional specifications and simplifies formal reasoning about system components.\n\nHardware architectures for dataflow was a major topic in Computer architecture research in the 1970s and early 1980s. Jack Dennis of MIT pioneered the field of static dataflow architectures. Designs that use conventional memory addresses as data dependency tags are called static dataflow machines. These machines did not allow multiple instances of the same routines to be executed simultaneously because the simple tags could not differentiate between them. Designs that use Content-addressable memory are called dynamic dataflow machines by Arvind. They use tags in memory to facilitate parallelism.\nData flows around the computer through the components of the computer. It gets entered from the input devices and can leave through output devices (printer etc.).\n\nA dataflow network is a network of concurrently executing processes or automata that can communicate by sending data over \"channels\" (see message passing.)\n\nIn Kahn process networks, named after Gilles Kahn, the processes are \"determinate\". This implies that each determinate process computes a continuous function from input streams to output streams, and that a network of determinate processes is itself determinate, thus computing a continuous function. This implies that the behavior of such networks can be described by a set of recursive equations, which can be solved using fixed point theory. The movement and transformation of the data is represented by a series of shapes and lines.\n\n\n"}
{"id": "6845737", "url": "https://en.wikipedia.org/wiki?curid=6845737", "title": "Disintegration theorem", "text": "Disintegration theorem\n\nIn mathematics, the disintegration theorem is a result in measure theory and probability theory. It rigorously defines the idea of a non-trivial \"restriction\" of a measure to a measure zero subset of the measure space in question. It is related to the existence of conditional probability measures. In a sense, \"disintegration\" is the opposite process to the construction of a product measure.\n\nConsider the unit square in the Euclidean plane R, \"S\" = [0, 1] × [0, 1]. Consider the probability measure μ defined on \"S\" by the restriction of two-dimensional Lebesgue measure λ to \"S\". That is, the probability of an event \"E\" ⊆ \"S\" is simply the area of \"E\". We assume \"E\" is a measurable subset of \"S\".\n\nConsider a one-dimensional subset of \"S\" such as the line segment \"L\" = {\"x\"} × [0, 1]. \"L\" has μ-measure zero; every subset of \"L\" is a μ-null set; since the Lebesgue measure space is a complete measure space,\n\nWhile true, this is somewhat unsatisfying. It would be nice to say that μ \"restricted to\" \"L\" is the one-dimensional Lebesgue measure λ, rather than the zero measure. The probability of a \"two-dimensional\" event \"E\" could then be obtained as an integral of the one-dimensional probabilities of the vertical \"slices\" \"E\" ∩ \"L\": more formally, if μ denotes one-dimensional Lebesgue measure on \"L\", then\n\nfor any \"nice\" \"E\" ⊆ \"S\". The disintegration theorem makes this argument rigorous in the context of measures on metric spaces.\n\nLet \"Y\" and \"X\" be two Radon spaces (i.e. separable metric spaces on which every probability measure is a Radon measure). Let μ ∈ P(\"Y\"), let π : \"Y\" → \"X\" be a Borel-measurable function, and let formula_3 ∈ P(\"X\") be the pushforward measure formula_3 = π(μ) = μ ∘ π. Then there exists a formula_3-almost everywhere uniquely determined family of probability measures {μ} ⊆ P(\"Y\") such that\n\n\nThe original example was a special case of the problem of product spaces, to which the disintegration theorem applies.\n\nWhen \"Y\" is written as a Cartesian product \"Y\" = \"X\" × \"X\" and π : \"Y\" → \"X\" is the natural projection, then each fibre \"π\"(\"x\") can be canonically identified with \"X\" and there exists a Borel family of probability measures formula_12 in P(\"X\") (which is (π)(μ)-almost everywhere uniquely determined) such that\n\nwhich is in particular\nand\n\nThe relation to conditional expectation is given by the identities\n\nThe disintegration theorem can also be seen as justifying the use of a \"restricted\" measure in vector calculus. For instance, in Stokes' theorem as applied to a vector field flowing through a compact surface Σ ⊂ R, it is implicit that the \"correct\" measure on Σ is the disintegration of three-dimensional Lebesgue measure λ on Σ, and that the disintegration of this measure on ∂Σ is the same as the disintegration of λ on ∂Σ.\n\nThe disintegration theorem can be applied to give a rigorous treatment of conditional probability distributions in statistics, while avoiding purely abstract formulations of conditional probability.\n\n"}
{"id": "11671", "url": "https://en.wikipedia.org/wiki?curid=11671", "title": "Fick's laws of diffusion", "text": "Fick's laws of diffusion\n\nFick's laws of diffusion describe diffusion and were derived by Adolf Fick in 1855. They can be used to solve for the diffusion coefficient, . Fick's first law can be used to derive his second law which in turn is identical to the diffusion equation.\n\nFick's first law relates the diffusive flux to the concentration under the assumption of steady state. It postulates that the flux goes from regions of high concentration to regions of low concentration, with a magnitude that is proportional to the concentration gradient (spatial derivative), or in simplistic terms the concept that a solute will move from a region of high concentration to a region of low concentration across a concentration gradient. In one (spatial) dimension, the law is:\n\nwhere\n\nIn two or more dimensions we must use , the del or gradient operator, which generalises the first derivative, obtaining\n\nwhere denotes the diffusion flux vector.\n\nThe driving force for the one-dimensional diffusion is the quantity , which for ideal mixtures is the concentration gradient. In chemical systems other than ideal solutions or mixtures, the driving force for diffusion of each species is the gradient of chemical potential of this species. Then Fick's first law (one-dimensional case) can be written as:\n\nwhere the index denotes the th species, is the concentration (mol/m), is the universal gas constant (J/K/mol), is the absolute temperature (K), and is the chemical potential (J/mol).\n\nIf the primary variable is mass fraction (, given, for example, in kg/kg), then the equation changes to:\n\nwhere is the fluid density (for example, in kg/m). Note that the density is outside the gradient operator.\n\nFick's second law predicts how diffusion causes the concentration to change with time. It is a partial differential equation which in one dimension reads:\n\nwhere\n\nIn two or more dimensions we must use the Laplacian , which generalises the second derivative, obtaining the equation\n\nIn one dimension, the following derivation is based on a similar argument made in Berg 1977 (see references).\n\nConsider a collection of particles performing a random walk in one dimension with length scale and time scale . Let be the number of particles at position at time .\n\nAt a given time step, half of the particles would move left and half would move right. Since half of the particles at point move right and half of the particles at point move left, the net movement to the right is:\n\nThe flux, , is this net movement of particles across some area element of area , normal to the random walk during a time interval . Hence we may write:\n\nMultiplying the top and bottom of the right hand side by and rewriting, one obtains:\n\nConcentration is defined as particles per unit volume, and hence\n\nIn addition, is the definition of the one-dimensional diffusion constant, . Thus our expression simplifies to:\n\nIn the limit where is infinitesimal, the right-hand side becomes a space derivative:\n\nFick's second law can be derived from Fick's first law and the mass conservation in absence of any chemical reactions:\n\nAssuming the diffusion coefficient to be a constant, one can exchange the orders of the differentiation and multiply by the constant:\n\nand, thus, receive the form of the Fick's equations as was stated above.\n\nFor the case of diffusion in two or more dimensions Fick's second law becomes\n\nwhich is analogous to the heat equation.\n\nIf the diffusion coefficient is not a constant, but depends upon the coordinate or concentration, Fick's second law yields\n\nAn important example is the case where is at a steady state, i.e. the concentration does not change by time, so that the left part of the above equation is identically zero. In one dimension with constant , the solution for the concentration will be a linear change of concentrations along . In two or more dimensions we obtain\n\nwhich is Laplace's equation, the solutions to which are referred to by mathematicians as harmonic functions.\n\nFick's second law is a special case of the convection–diffusion equation in which there is no advective flux and no net volumetric source. It can be derived from the continuity equation:\n\nwhere is the total flux and is a net volumetric source for . The only source of flux in this situation is assumed to be diffusive flux:\n\nPlugging the definition of diffusive flux to the continuity equation and assuming there is no source (), we arrive at Fick's second law:\n\nIf flux were the result of both diffusive flux and advective flux, the convection–diffusion equation is the result.\n\nA simple case of diffusion with time in one dimension (taken as the -axis) from a boundary located at position , where the concentration is maintained at a value is\n\nwhere is the complementary error function. This is the case when corrosive gases diffuse through the oxidative layer towards the metal surface (if we assume that concentration of gases in the environment is constant and the diffusion space – that is, the corrosion product layer – is \"semi-infinite\", starting at 0 at the surface and spreading infinitely deep in the material). If, in its turn, the diffusion space is \"infinite\" (lasting both through the layer with , and that with , ), then the solution is amended only with coefficient in front of (as the diffusion now occurs in both directions). This case is valid when some solution with concentration is put in contact with a layer of pure solvent. (Bokstein, 2005) The length is called the \"diffusion length\" and provides a measure of how far the concentration has propagated in the -direction by diffusion in time (Bird, 1976).\n\nAs a quick approximation of the error function, the first 2 terms of the Taylor series can be used:\n\nIf is time-dependent, the diffusion length becomes\nThis idea is useful for estimating a diffusion length over a heating and cooling cycle, where varies with temperature.\n\n\n\n\n\nThe Chapman–Enskog formulae for diffusion in gases include exactly the same terms. These physical models of diffusion are different from the test models which are valid for very small deviations from the uniform equilibrium. Earlier, such terms were introduced in the Maxwell–Stefan diffusion equation.\n\nFor anisotropic multicomponent diffusion coefficients one needs a rank-four tensor, for example , where refer to the components and correspond to the space coordinates.\n\nEquations based on Fick's law have been commonly used to model transport processes in foods, neurons, biopolymers, pharmaceuticals, porous soils, population dynamics, nuclear materials, plasma physics, and semiconductor doping processes. Theory of all voltammetric methods is based on solutions of Fick's equation. Much experimental research in polymer science and food science has shown that a more general approach is required to describe transport of components in materials undergoing glass transition. In the vicinity of glass transition the flow behavior becomes \"non-Fickian\". It can be shown that the Fick's law can be obtained from the Maxwell–Stefan equations\nof multi-component mass transfer. The Fick's law is limiting case of the Maxwell–Stefan equations, when the mixture is extremely dilute and every chemical species is interacting only with the bulk mixture and not with other species. To account for the presence of multiple species in a non-dilute mixture, several variations of the Maxwell–Stefan equations are used. See also non-diagonal coupled transport processes (Onsager relationship). \n\nThe first law gives rise to the following formula:\n\nin which,\n\nFick's first law is also important in radiation transfer equations. However, in this context it becomes inaccurate when the diffusion constant is low and the radiation becomes limited by the speed of light rather than by the resistance of the material the radiation is flowing through. In this situation, one can use a flux limiter.\n\nThe exchange rate of a gas across a fluid membrane can be determined by using this law together with Graham's law.\n\nWhen two miscible liquids are brought into contact, and diffusion takes place, the macroscopic (or average) concentration evolves following Fick's law. On a mesoscopic scale, that is, between the macroscopic scale described by Fick's law and molecular scale, where molecular random walks take place, fluctuations cannot be neglected. Such situations can be successfully modeled with Landau-Lifshitz fluctuating hydrodynamics. In this theoretical framework, diffusion is due to fluctuations whose dimensions range from the molecular scale to the macroscopic scale.\n\nIn particular, fluctuating hydrodynamic equations include a Fick's flow term, with a given diffusion coefficient, along with hydrodynamics equations and stochastic terms describing fluctuations. When calculating the fluctuations with a perturbative approach, the zero order approximation is Fick's law. The first order gives the fluctuations, and it comes out that fluctuations contribute to diffusion. This represents somehow a tautology, since the phenomena described by a lower order approximation is the result of a higher approximation: this problem is solved only by renormalizing the fluctuating hydrodynamics equations.\n\nIntegrated circuit fabrication technologies, model processes like CVD, thermal oxidation, wet oxidation, doping, etc. use diffusion equations obtained from Fick's law.\n\nIn certain cases, the solutions are obtained for boundary conditions such as constant source concentration diffusion, limited source concentration, or moving boundary diffusion (where junction depth keeps moving into the substrate).\n\nIn 1855, physiologist Adolf Fick first reported his now well-known laws governing the transport of mass through diffusive means. Fick's work was inspired by the earlier experiments of Thomas Graham, which fell short of proposing the fundamental laws for which Fick would become famous. The Fick's law is analogous to the relationships discovered at the same epoch by other eminent scientists: Darcy's law (hydraulic flow), Ohm's law (charge transport), and Fourier's Law (heat transport).\n\nFick's experiments (modeled on Graham's) dealt with measuring the concentrations and fluxes of salt, diffusing between two reservoirs through tubes of water. It is notable that Fick's work primarily concerned diffusion in fluids, because at the time, diffusion in solids was not considered generally possible. Today, Fick's Laws form the core of our understanding of diffusion in solids, liquids, and gases (in the absence of bulk fluid motion in the latter two cases). When a diffusion process does \"not\" follow Fick's laws (which happens in cases of diffusion through porous media and diffusion of swelling penetrants, among others), it is referred to as \"non-Fickian\".\n\n\n\n"}
{"id": "666987", "url": "https://en.wikipedia.org/wiki?curid=666987", "title": "Frenet–Serret formulas", "text": "Frenet–Serret formulas\n\nIn differential geometry, the Frenet–Serret formulas describe the kinematic properties of a particle moving along a continuous, differentiable curve in three-dimensional Euclidean space ℝ, or the geometric properties of the curve itself irrespective of any motion. More specifically, the formulas describe the derivatives of the so-called tangent, normal, and binormal unit vectors in terms of each other. The formulas are named after the two French mathematicians who independently discovered them: Jean Frédéric Frenet, in his thesis of 1847, and Joseph Alfred Serret in 1851. Vector notation and linear algebra currently used to write these formulas were not yet in use at the time of their discovery.\n\nThe tangent, normal, and binormal unit vectors, often called T, N, and B, or collectively the Frenet–Serret frame or TNB frame, together form an orthonormal basis spanning ℝ and are defined as follows:\nThe Frenet–Serret formulas are:\nwhere \"d\"/\"ds\" is the derivative with respect to arclength, \"κ\" is the curvature, and \"τ\" is the torsion of the curve. The two scalars \"κ\" and \"τ\" effectively define the curvature and torsion of a space curve. The associated collection, T, N, B, \"κ\", and \"τ\", is called the Frenet–Serret apparatus. Intuitively, curvature measures the failure of a curve to be a straight line, while torsion measures the failure of a curve to be planar.\n\nLet r(t) be a curve in Euclidean space, representing the position vector of the particle as a function of time. The Frenet–Serret formulas apply to curves which are \"non-degenerate\", which roughly means that they have nonzero curvature. More formally, in this situation the velocity vector r′(t) and the acceleration vector r′′(t) are required not to be proportional.\n\nLet \"s(t)\" represent the arc length which the particle has moved along the curve in time t. The quantity \"s\" is used to give the curve traced out by the trajectory of the particle a natural parametrization by arc length, since many different particle paths may trace out the same geometrical curve by traversing it at different rates. In detail, \"s\" is given by\nMoreover, since we have assumed that r′ ≠ 0, it follows that \"s\"(\"t\") is a strictly monotonically increasing function. Therefore, it is possible to solve for \"t\" as a function of \"s\", and thus to write r(\"s\") = r(\"t\"(\"s\")). The curve is thus parametrized in a preferred manner by its arc length.\n\nWith a non-degenerate curve r(\"s\"), parameterized by its arc length, it is now possible to define the Frenet–Serret frame (or TNB frame):\n\nFrom equation (2) it follows, since T always has unit magnitude, that N (the change of T) is always perpendicular to T, since there is no change in direction of T. From equation (3) it follows that B is always perpendicular to both T and N. Thus, the three unit vectors T, N, and B are all perpendicular to each other.\n\nThe Frenet–Serret formulas are:\n\nwhere formula_7 is the curvature and formula_8 is the torsion.\n\nThe Frenet–Serret formulas are also known as \"Frenet–Serret theorem\", and can be stated more concisely using matrix notation:\n\nThis matrix is skew-symmetric.\n\nThe Frenet–Serret formulas were generalized to higher-dimensional Euclidean spaces by Camille Jordan in 1874.\n\nSuppose that r(\"s\") is a smooth curve in R, parametrized by arc length, and that the first \"n\" derivatives of r are linearly independent. The vectors in the Frenet–Serret frame are an orthonormal basis constructed by applying the Gram-Schmidt process to the vectors (r′(\"s\"), r′′(\"s\"), ..., r(\"s\")).\n\nIn detail, the unit tangent vector is the first Frenet vector \"e\"(\"s\") and is defined as\n\nThe normal vector, sometimes called the curvature vector, indicates the deviance of the curve from being a straight line. It is defined as\n\nIts normalized form, the unit normal vector, is the second Frenet vector e(\"s\") and defined as\n\nThe tangent and the normal vector at point \"s\" define the osculating plane at point r(\"s\").\n\nThe remaining vectors in the frame (the binormal, trinormal, etc.) are defined similarly by\n\nThe real valued functions χ(\"s\") are called generalized curvature and are defined as\n\nThe Frenet–Serret formulas, stated in matrix language, are\n\nConsider the matrix\n\nThe rows of this matrix are mutually perpendicular unit vectors: an orthonormal basis of ℝ. As a result, the transpose of \"Q\" is equal to the inverse of \"Q\": \"Q\" is an orthogonal matrix. It suffices to show that\n\nNote the first row of this equation already holds, by definition of the normal N and curvature κ. So it suffices to show that (d\"Q\"/d\"s\")\"Q\" is a skew-symmetric matrix. Since \"I\" = \"QQ\", taking a derivative and applying the product rule yields\n\nwhich establishes the required skew-symmetry.\n\nThe Frenet–Serret frame consisting of the tangent T, normal N, and binormal B collectively forms an orthonormal basis of 3-space. At each point of the curve, this \"attaches\" a frame of reference or rectilinear coordinate system (see image).\n\nThe Frenet–Serret formulas admit a kinematic interpretation. Imagine that an observer moves along the curve in time, using the attached frame at each point as her coordinate system. The Frenet–Serret formulas mean that this coordinate system is constantly rotating as an observer moves along the curve. Hence, this coordinate system is always non-inertial. The angular momentum of the observer's coordinate system is proportional to the Darboux vector of the frame.\nConcretely, suppose that the observer carries an (inertial) top (or gyroscope) with her along the curve. If the axis of the top points along the tangent to the curve, then it will be observed to rotate about its axis with angular velocity -τ relative to the observer's non-inertial coordinate system. If, on the other hand, the axis of the top points in the binormal direction, then it is observed to rotate with angular velocity -κ. This is easily visualized in the case when the curvature is a positive constant and the torsion vanishes. The observer is then in uniform circular motion. If the top points in the direction of the binormal, then by conservation of angular momentum it must rotate in the \"opposite\" direction of the circular motion. In the limiting case when the curvature vanishes, the observer's normal precesses about the tangent vector, and similarly the top will rotate in the opposite direction of this precession.\n\nThe general case is illustrated below. There are further on Wikimedia.\n\nApplications. The kinematics of the frame have many applications in the sciences.\n\n\nThe Frenet–Serret formulas are frequently introduced in courses on multivariable calculus as a companion to the study of space curves such as the helix. A helix can be characterized by the height 2π\"h\" and radius \"r\" of a single turn. The curvature and torsion of a helix (with constant radius) are given by the formulas\n\nThe sign of the torsion is determined by the right-handed or left-handed sense in which the helix twists around its central axis. Explicitly, the parametrization of a single turn of a right-handed helix with height 2π\"h\" and radius \"r\" is\nand, for a left-handed helix,\nNote that these are not the arc length parametrizations (in which case, each of \"x\", \"y\", and \"z\" would need to be divided by formula_23.)\n\nIn his expository writings on the geometry of curves, Rudy Rucker employs the model of a slinky to explain the meaning of the torsion and curvature. The slinky, he says, is characterized by the property that the quantity\nremains constant if the slinky is vertically stretched out along its central axis. (Here 2π\"h\" is the height of a single twist of the slinky, and \"r\" the radius.) In particular, curvature and torsion are complementary in the sense that the torsion can be increased at the expense of curvature by stretching out the slinky.\n\nRepeatedly differentiating the curve and applying the Frenet–Serret formulas gives the following Taylor approximation to the curve near \"s\" = 0:\n\nFor a generic curve with nonvanishing torsion, the projection of the curve onto various coordinate planes in the T, N, B coordinate system at have the following interpretations:\n\n\nThe Frenet–Serret apparatus allows one to define certain optimal \"ribbons\" and \"tubes\" centered around a curve. These have diverse applications in materials science and elasticity theory, as well as to computer graphics.\n\nA Frenet ribbon along a curve \"C\" is the surface traced out by sweeping the line segment [−N,N] generated by the unit normal along the curve. Geometrically, a ribbon is a piece of the envelope of the osculating planes of the curve. Symbolically, the ribbon \"R\" has the following parametrization:\nIn particular, the binormal B is a unit vector normal to the ribbon. Moreover, the ribbon is a ruled surface whose reguli are the line segments spanned by N. Thus each of the frame vectors T, N, and B can be visualized entirely in terms of the Frenet ribbon.\n\nThe Gauss curvature of a Frenet ribbon vanishes, and so it is a developable surface. Geometrically, it is possible to \"roll\" a plane along the ribbon without slipping or twisting so that the regulus always remains within the plane. The ribbon then traces out a ribbon in the plane (possibly with multiple sheets). The curve \"C\" also traces out a curve \"C\" in the plane, whose curvature is given in terms of the curvature and torsion of \"C\" by\nThis fact gives a general procedure for constructing any Frenet ribbon. Intuitively, one can cut out a curved ribbon from a flat piece of paper. Then by bending the ribbon out into space without tearing it, one produces a Frenet ribbon. In the simple case of the slinky, the ribbon is several turns of an annulus in the plane, and bending it up into space corresponds to stretching out the slinky.\n\nIn classical Euclidean geometry, one is interested in studying the properties of figures in the plane which are \"invariant\" under congruence, so that if two figures are congruent then they must have the same properties. The Frenet-Serret apparatus presents the curvature and torsion as numerical invariants of a space curve.\n\nRoughly speaking, two curves \"C\" and \"C\"′ in space are \"congruent\" if one can be rigidly moved to the other. A rigid motion consists of a combination of a translation and a rotation. A translation moves one point of \"C\" to a point of \"C\"′. The rotation then adjusts the orientation of the curve \"C\" to line up with that of \"C\"′. Such a combination of translation and rotation is called a Euclidean motion. In terms of the parametrization r(t) defining the first curve \"C\", a general Euclidean motion of \"C\" is a composite of the following operations:\n\nThe Frenet–Serret frame is particularly well-behaved with regard to Euclidean motions. First, since T, N, and B can all be given as successive derivatives of the parametrization of the curve, each of them is insensitive to the addition of a constant vector to r(t). Intuitively, the TNB frame attached to r(t) is the same as the TNB frame attached to the new curve r(t) + v.\n\nThis leaves only the rotations to consider. Intuitively, if we apply a rotation \"M\" to the curve, then the TNB frame also rotates. More precisely, the matrix \"Q\" whose rows are the TNB vectors of the Frenet-Serret frame changes by the matrix of a rotation\n\n\"A fortiori\", the matrix (d\"Q\"/d\"s\")\"Q\" is unaffected by a rotation:\n\nsince \"MM\" = \"I\" for the matrix of a rotation.\n\nHence the entries κ and τ of (d\"Q\"/d\"s\")\"Q\" are \"invariants\" of the curve under Euclidean motions: if a Euclidean motion is applied to a curve, then the resulting curve has \"the same\" curvature and torsion.\n\nMoreover, using the Frenet–Serret frame, one can also prove the converse: any two curves having the same curvature and torsion functions must be congruent by a Euclidean motion. Roughly speaking, the Frenet–Serret formulas express the Darboux derivative of the TNB frame. If the Darboux derivatives of two frames are equal, then a version of the fundamental theorem of calculus asserts that the curves are congruent. In particular, the curvature and torsion are a \"complete\" set of invariants for a curve in three-dimensions.\n\nThe formulas given above for T, N, and B depend on the curve being given in terms of the arclength parameter. This is a natural assumption in Euclidean geometry, because the arclength is a Euclidean invariant of the curve. In the terminology of physics, the arclength parametrization is a natural choice of gauge. However, it may be awkward to work with in practice. A number of other equivalent expressions are available.\n\nSuppose that the curve is given by r(\"t\"), where the parameter \"t\" need no longer be arclength. Then the unit tangent vector T may be written as\n\nThe normal vector N takes the form\n\nThe binormal B is then\n\nAn alternative way to arrive at the same expressions is to take the first three derivatives of the curve r′(\"t\"), r′′(\"t\"), r′′′(\"t\"), and to apply the Gram-Schmidt process. The resulting ordered orthonormal basis is precisely the TNB frame. This procedure also generalizes to produce Frenet frames in higher dimensions.\n\nIn terms of the parameter \"t\", the Frenet–Serret formulas pick up an additional factor of ||r′(\"t\")|| because of the chain rule:\n\nExplicit expressions for the curvature and torsion may be computed. For example,\n\nThe torsion may be expressed using a scalar triple product as follows,\n\nIf the curvature is always zero then the curve will be a straight line. Here the vectors N, B and the torsion are not well defined.\n\nIf the torsion is always zero then the curve will lie in a plane.\n\nA curve may have nonzero curvature and zero torsion. For example, the circle of radius \"R\" given by r(\"t\")=(\"R\" cos \"t\", \"R\" sin \"t\", 0) in the \"z\"=0 plane has zero torsion and curvature equal to 1/\"R\". The converse, however, is false. That is, a regular curve with nonzero torsion must have nonzero curvature. (This is just the contrapositive of the fact that zero curvature implies zero torsion.)\n\nA helix has constant curvature and constant torsion.\n\nGiven a curve contained on the \"x\"-\"y\" plane, its tangent vector T is also contained on that plane. Its binormal vector B can be naturally postulated to coincide with the normal \"to the plane\" (along the \"z\" axis). Finally, the curve normal can be found completing the right-handed system, N = B × T. This form is well-defined even when the curvature is zero; for example, the normal to a straight line on a plane will be perpendicular to the tangent, all co-planar.\n\n\n\n"}
{"id": "4660507", "url": "https://en.wikipedia.org/wiki?curid=4660507", "title": "Graph state", "text": "Graph state\n\nIn quantum computing, a graph state is a special type of multi-qubit state that can be represented by a graph. Each qubit is represented by a vertex of the graph, and there is an edge between every interacting pair of qubits. In particular, they are a convenient way of representing certain types of entangled states.\n\nGraph states are useful in quantum error-correcting codes, entanglement measurement and purification and for characterization of computational resources in measurement based quantum computing models.\n\nGiven a graph \"G\" = (\"V\", \"E\"), with the set of vertices \"V\" and the set of edges \"E\", the corresponding graph state is defined as\n\nwhere formula_2 and the operator formula_3 is the controlled-\"Z\" interaction between the two vertices (qubits) \"a\", \"b\"\n\nAn alternative and equivalent definition is the following.\n\nDefine an operator formula_5 for each vertex \"v\" of \"G\":\n\nwhere formula_7 are the Pauli matrices and \"N\"(\"v\") is the set of vertices adjacent to \"v\". The formula_5 operators commute. The graph state formula_9 is defined as the simultaneous formula_10-eigenvalue eigenstate of the formula_11 operators formula_12: \n\n\nThe corresponding quantum state is \n\n\nThe corresponding quantum state is \n\nObserve that formula_22 and formula_23 are locally equivalent to each other, i.e., can be mapped to each other by applying one-qubit unitaries. Indeed, switching formula_24 and formula_25 on the first and last qubits, while switching formula_25 and formula_27 on the middle\nqubit, maps the stabilizer group of one into that of the other. \n\nMore generally, two graph states are locally equivalent if and only if the corresponding graphs are related by a sequence of so-called ``local complementation\" steps, as shown by Van den Nest et al. (2005). \n\n\n"}
{"id": "1039260", "url": "https://en.wikipedia.org/wiki?curid=1039260", "title": "HOMFLY polynomial", "text": "HOMFLY polynomial\n\nIn the mathematical field of knot theory, the HOMFLY polynomial, sometimes called the HOMFLY-PT polynomial or the generalized Jones polynomial, is a 2-variable knot polynomial, i.e. a knot invariant in the form of a polynomial of variables \"m\" and \"l\". \n\nA central question in the mathematical theory of knots is whether two knot diagrams represent the same knot. One tool used to answer such questions is a knot polynomial, which is computed from a diagram of the knot and can be shown to be an invariant of the knot, i.e. diagrams representing the same knot have the same polynomial. The converse may not be true. The HOMFLY polynomial is one such invariant and it generalizes two polynomials previously discovered, the Alexander polynomial and the Jones polynomial, both of which can be obtained by appropriate substitutions from HOMFLY. The HOMFLY polynomial is also a quantum invariant.\n\nThe name \"HOMFLY\" combines the initials of its co-discoverers: Jim Hoste, Adrian Ocneanu, Kenneth Millett, Peter J. Freyd, W. B. R. Lickorish, and David N. Yetter. The addition of \"PT\" recognizes independent work carried out by Józef H. Przytycki and Paweł Traczyk.\n\nThe polynomial is defined using skein relations:\n\nwhere formula_3 are links formed by crossing and smoothing changes on a local region of a link diagram, as indicated in the figure. \n\nThe HOMFLY polynomial of a link \"L\" that is a split union of two links formula_4 and formula_5 is given by\n\nSee the page on skein relation for an example of a computation using such relations.\n\nThis polynomial can be obtained also using other skein relations:\n\nThe Jones polynomial, \"V\"(\"t\"), and the Alexander polynomial, formula_11 can be computed in terms of the HOMFLY polynomial (the version in formula_12 and formula_13 variables) as follows:\n\n\n"}
{"id": "1665366", "url": "https://en.wikipedia.org/wiki?curid=1665366", "title": "Hadwiger number", "text": "Hadwiger number\n\nIn graph theory, the Hadwiger number of an undirected graph \"G\" is the size of the largest complete graph that can be obtained by contracting edges of \"G\".\nEquivalently, the Hadwiger number \"h\"(\"G\") is the largest number \"k\" for which the complete graph \"K\" is a minor of \"G\", a smaller graph obtained from \"G\" by edge contractions and vertex and edge deletions. The Hadwiger number is also known as the contraction clique number of \"G\" or the homomorphism degree of \"G\". It is named after Hugo Hadwiger, who introduced it in 1943 in conjunction with the Hadwiger conjecture, which states that the Hadwiger number is always at least as large as the chromatic number of \"G\".\n\nThe graphs that have Hadwiger number at most four have been characterized by . The graphs with any finite bound on the Hadwiger number are sparse, and have small chromatic number. Determining the Hadwiger number of a graph is NP-hard but fixed-parameter tractable.\n\nA graph \"G\" has Hadwiger number at most two if and only if it is a forest, for a three-vertex complete minor can only be formed by contracting a cycle in \"G\".\n\nA graph has Hadwiger number at most three if and only if its treewidth is at most two, which is true if and only if each of its biconnected components is a series-parallel graph.\nWagner's theorem, which characterizes the planar graphs by their forbidden minors, implies that the planar graphs have Hadwiger number at most four. In the same paper that proved this theorem, also characterized the graphs with Hadwiger number at most four more precisely: they are graphs that can be formed by clique-sum operations that combine planar graphs with the eight-vertex Wagner graph.\n\nThe graphs with Hadwiger number at most five include the apex graphs and the linklessly embeddable graphs, both of which have the complete graph \"K\" among their forbidden minors.\n\nEvery graph with \"n\" vertices and Hadwiger number \"k\" has \nO(\"nk\" ) edges. This bound is tight: for every \"k\", there exist graphs with Hadwiger number \"k\" that have Ω(\"nk\" ) edges. If a graph \"G\" has Hadwiger number \"k\", then all of its subgraphs also have Hadwiger number at most \"k\", and it follows that \"G\" must have degeneracy O(\"k\" ). Therefore, the graphs with bounded Hadwiger number are sparse graphs.\n\nThe Hadwiger conjecture states that the Hadwiger number is always at least as large as the chromatic number of \"G\". That is, every graph with Hadwiger number \"k\" should have a graph coloring with at most \"k\" colors. The case \"k\" = 4 is equivalent (by Wagner's characterization of the graphs with this Hadwiger number) to the four color theorem on colorings of planar graphs, and the conjecture has also been proven for \"k\" ≤ 5, but remains unproven for larger values of \"k\".\n\nBecause of their low degeneracy, the graphs with Hadwiger number at most \"k\" can be colored by a greedy coloring algorithm using O(\"k\" ) colors.\n\nTesting whether the Hadwiger number of a given graph is at least a given value \"k\" is NP-complete, from which it follows that determining the Hadwiger number is NP-hard. However, the problem is fixed-parameter tractable: there is an algorithm for finding the largest clique minor in an amount of time that depends only polynomially on the size of the graph, but exponentially in \"h\"(\"G\"). Additionally, polynomial time algorithms can approximate the Hadwiger number significantly more accurately than the best polynomial-time approximation (assuming P ≠ NP) to the size of the largest complete subgraph.\n\nThe achromatic number of a graph \"G\" is the size of the largest clique that can be formed by contracting a family of independent sets in \"G\".\n\nUncountable clique minors in infinite graphs may be characterized in terms of havens, which formalize the evasion strategies for certain pursuit-evasion games: if the Hadwiger number is uncountable, then it equals the largest order of a haven in the graph.\n\nEvery graph with Hadwiger number \"k\" has at most \"n\"2 cliques (complete subgraphs).\n\n"}
{"id": "35953535", "url": "https://en.wikipedia.org/wiki?curid=35953535", "title": "Hausdorff completion", "text": "Hausdorff completion\n\nIn algebra, the Hausdorff completion formula_1 of a group \"G\" with filtration formula_2 is the inverse limit formula_3 of the discrete group formula_4. A basic example is a profinite completion. The image of the canonical map formula_5 is a Hausdorff topological group and its kernel is the intersection of all formula_2: i.e., the closure of the identity element. The canonical homomorphism formula_7 is an isomorphism, where formula_8 is a graded module associated to the filtration.\n\nThe concept is named after Felix Hausdorff.\n\n"}
{"id": "58278355", "url": "https://en.wikipedia.org/wiki?curid=58278355", "title": "József Solymosi", "text": "József Solymosi\n\nJózsef Solymosi is a Hungarian-Canadian mathematician and a professor of mathematics at the University of British Columbia. His main research interests are arithmetic combinatorics, discrete geometry, graph theory, and combinatorial number theory.\n\nSolymosi earned his master’s degree in 1999 under the supervision of László Székely from the Eötvös Loránd University and his Ph.D. in 2001 at ETH Zürich under the supervision of Emo Welzl. His doctoral dissertation was \"Ramsey-Type Results on Planar Geometric Objects\".\n\nFrom 2001 to 2003 he was S. E. Warschawski Assistant Professor of Mathematics at the University of California, San Diego. He joined the faculty of the University of British Columbia in 2002.\n\nHe was editor in chief of the \"Electronic Journal of Combinatorics\" from 2013 to 2015.\n\nSolymosi was the first online contributor to the first Polymath Project, set by Timothy Gowers to find improvements to the Hales–Jewett theorem.\n\nOne of his theorems states that if a finite set of points in the Euclidean plane has every pair of points at an integer distance from each other, then\nthe set must have a diameter (largest distance) that is linear in the number of points. This result is connected to the Erdős–Anning theorem, according to which an infinite set of points with integer distances must lie on one line. In connection with the related Erdős–Ulam problem, on the existence of dense subsets of the plane for which all distances are rational numbers, Solymosi and de Zeeuw proved that every infinite rational-distance set must either be dense in the Zariski topology or it must have all but finitely many of its points on a single line or circle.\n\nWith Terence Tao, Solymosi proved a bound of formula_1 on the number of incidences between formula_2 points and formula_3 affine subspaces of any finite-dimensional Euclidean space, whenever each pair of subspaces has at most one point of intersection. This generalizes the Szemerédi–Trotter theorem on points and lines in the Euclidean plane, and because of this the exponent of formula_4 cannot be improved. Their theorem solves (up to the formula_5 in the exponent) a conjecture of Toth, and was inspired by an analogue of the Szemerédi–Trotter theorem for lines in the complex plane.\n\nHe has also contributed improved bounds for the Erdős–Szemerédi theorem, showing that every set of real numbers has either a large set of pairwise sums or a large set of pairwise products, and for the Erdős distinct distances problem, showing that every set of points in the plane has many different pairwise distances.\n\nIn 2006, Solymosi received a Sloan Research Fellowship and in 2008 he was awarded the André Aisenstadt Mathematics Prize. In 2012 he was named a doctor of the Hungarian Academy of Science.\n\n"}
{"id": "15314230", "url": "https://en.wikipedia.org/wiki?curid=15314230", "title": "Key ceremony", "text": "Key ceremony\n\nIn public-key cryptography and computer security, a root key ceremony is a procedure where a unique pair of public and private root keys is generated. Depending on the certificate policy, the generation of the root keys may require notarization, legal representation, witnesses and \"key holders\" to be present, as the information on the system is a responsibility of the parties. A commonly recognized best practice is to follow the standard for root key ceremonies.\n\nAt the heart of every certificate authority (CA) is at least one root key or root certificate and usually at least one intermediate root certificate. A root key is a term for a unique passcode that must be generated for secure server interaction with a protective network, usually called the root zone. Prompts for information from this zone can be done through a server. The keys and certificates mentioned are the credentials and safe guards for the system. These digital certificates are made from a public and a private key.\n\nExample A: These passcodes are used for Strong identification and non-repudiation for email and web access\n\nUnless the information being accessed or transmitted is valued in terms of millions of dollars, it is probably sufficient that the root key ceremony be conducted within the security of the vendor's laboratory. The customer may opt to have the root key stored in a hardware security module, but in most cases, the safe storage of the root Key on a CD or hard disk is sufficient. The root key is never stored on the CA server.\n\nExample B: Machine Readable Travel Document [MRTD] ID Card or e Passport\n\nThis type of environment requires much higher security. When conducting the root key ceremony, the government or organization will require rigorous security checks to be conducted on all personnel in attendance. Those that are normally required to attend the key ceremony will include a minimum of two administrators from the organization, two signatories from the organization, one lawyer, a notary, and two video camera operators, in addition to the CA software vendor's own technical team.\n\nExample A and B are at opposite ends of the security spectrum and no two environments are the same. Depending on the level of protection required, different levels of security will be used.\n\nThe actual root key-pair generation is normally conducted in a secure vault that has no communication or contact with the outside world other than a single telephone line or intercom. Once the vault is secured, all personnel present must prove their identity using at least two legally recognized forms of identification. Every person present, every transaction and every event is logged by the lawyer in a root key ceremony log book and each page is notarized by the notary. From the moment the vault door is closed until it is re-opened, everything is also video recorded. The lawyer and the organization's two signatories must sign the recording and it too is then notarized.\n\nFinally, as part of the above process, the root key is broken into as many as twenty-one parts and each individual part is secured in its own safe for which there is a key and a numerical lock. The keys are distributed to as many as twenty-one people and the numerical code is distributed to another twenty-one people.\n\nThe CA vendors and organisations that would implement projects of this nature where conducting a root key ceremony would be a central component of their service would be organisations like, for example; RSA, VeriSign and Digi-Sign.\n\n\n"}
{"id": "8788855", "url": "https://en.wikipedia.org/wiki?curid=8788855", "title": "Kharitonov region", "text": "Kharitonov region\n\nA Kharitonov region is a concept in mathematics. It arises in the study of the stability of polynomials.\n\nLet formula_1 be a simply-connected set in the complex plane and let formula_2 be the polynomial family.\n\nformula_1 is said to be a Kharitonov region if\n\nis a subset of formula_5 Here, formula_6 denotes the set of all vertex polynomials of complex interval polynomials formula_7 and formula_8 denotes the set of all vertex polynomials of real interval polynomials formula_9\n\n\n"}
{"id": "632487", "url": "https://en.wikipedia.org/wiki?curid=632487", "title": "List of algorithm general topics", "text": "List of algorithm general topics\n\nThis is a list of algorithm general topics. \n\n\n"}
{"id": "11641180", "url": "https://en.wikipedia.org/wiki?curid=11641180", "title": "Logic alphabet", "text": "Logic alphabet\n\nThe logic alphabet, also called the X-stem Logic Alphabet (XLA), constitutes an iconic set of symbols that systematically represents the sixteen possible binary truth functions of logic. The logic alphabet was developed by Shea Zellweger. The major emphasis of his iconic \"logic alphabet\" is to provide a more cognitively ergonomic notation for logic. Zellweger's visually iconic system more readily reveals, to the novice and expert alike, the underlying symmetry relationships and geometric properties of the sixteen binary connectives within Boolean algebra.\n\nTruth functions are functions from sequences of truth values to truth values. A unary truth function, for example, takes a single truth value and maps it onto another truth value. Similarly, a binary truth function maps ordered pairs of truth values onto truth values, while a ternary truth function maps ordered triples of truth values onto truth values, and so on.\n\nIn the unary case, there are two possible inputs, viz. T and F, and thus four possible unary truth functions: one mapping T to T and F to F, one mapping T to F and F to F, one mapping T to T and F to T, and finally one mapping T to F and F to T, this last one corresponding to the familiar operation of logical negation. In the form of a table, the four unary truth functions may be represented as follows.\n\nIn the binary case, there are four possible inputs, viz. (T,T), (T,F), (F,T), and (F,F), thus yielding sixteen possible binary truth functions. Quite generally, for any number \"n\", there are formula_1 possible \"n\"-ary truth functions. The sixteen possible binary truth functions are listed in the table below.\n\nZellweger's logic alphabet offers a visually systematic way of representing each of the sixteen binary truth functions. The idea behind the logic alphabet is to first represent the sixteen binary truth functions in the form of a square matrix rather than the more familiar tabular format seen in the table above, and then to assign a letter shape to each of these matrices. Letter shapes are derived from the distribution of Ts in the matrix. When drawing a logic symbol, one passes through each square with assigned F values while stopping in a square with assigned T values. In the extreme examples, the symbol for tautology is a X (stops in all four squares), while the symbol for contradiction is an O (passing through all squares without stopping). The square matrix corresponding to each binary truth function, as well as its corresponding letter shape, are displayed in the table below.\n\nThe interest of the logic alphabet lies in its aesthetic, symmetric, and geometric qualities. These qualities combine to allow an individual to more easily, rapidly and visually manipulate the relationships between entire truth tables. A logic operation performed on a two dimensional logic alphabet connective, with its geometric qualities, produces a symmetry transformation. When a symmetry transformation occurs, each input symbol, without any further thought, immediately changes into the correct output symbol. For example, by reflecting the symbol for NAND (viz. 'h') across the vertical axis we produce the symbol for ←, whereas by reflecting it across the horizontal axis we produce the symbol for →, and by reflecting it across both the horizontal and vertical axes we produce the symbol for ∨. Similar symmetry transformations can be obtained by operating upon the other symbols.\n\nIn effect, the X-stem Logic Alphabet is derived from three disciplines that have been stacked and combined: (1) mathematics, (2) logic, and (3) semiotics. This happens because, in keeping with the mathelogical semiotics, the connectives have been custom designed in the form of geometric letter shapes that serve as iconic replicas of their corresponding square-framed truth tables. Logic cannot do it alone. Logic is sandwiched between mathematics and semiotics. Indeed, Zellweger has constructed intriguing structures involving the symbols of the logic alphabet on the basis of these symmetries ( ). The considerable aesthetic appeal of the logic alphabet has led to exhibitions of Zellweger's work at the Museum of Jurassic Technology in Los Angeles, among other places.\n\nThe value of the logic alphabet lies in its use as a visually simpler pedagogical tool than the traditional system for logic notation. The logic alphabet eases the introduction to the fundamentals of logic, especially for children, at much earlier stages of cognitive development. Because the logic notation system, in current use today, is so deeply embedded in our computer culture, the \"logic alphabets\" adoption and value by the field of logic itself, at this juncture, is questionable. Additionally, systems of natural deduction, for example, generally require introduction and elimination rules for each connective, meaning that the use of all sixteen binary connectives would result in a highly complex proof system. Various subsets of the sixteen binary connectives (e.g., {∨,&,→,~}, {∨,~}, {&, ~}, {→,~}) are themselves functionally complete in that they suffice to define the remaining connectives. In fact, both NAND and NOR are sole sufficient operators, meaning that the remaining connectives can all be defined solely in terms of either of them. Nonetheless, the logic alphabet’s 2-dimensional geometric letter shapes along with its group symmetry properties can help ease the learning curve for children and adult students alike, as they become familiar with the interrelations and operations on all 16 binary connectives. Giving children and students this advantage is a decided gain.\n\n\n"}
{"id": "3957360", "url": "https://en.wikipedia.org/wiki?curid=3957360", "title": "Mass-to-charge ratio", "text": "Mass-to-charge ratio\n\nThe mass-to-charge ratio (\"m\"/\"Q\") is a physical quantity that is most widely used in the electrodynamics of charged particles, e.g. in electron optics and ion optics. It appears in the scientific fields of electron microscopy, cathode ray tubes, accelerator physics, nuclear physics, Auger electron spectroscopy, cosmology and mass spectrometry. The importance of the mass-to-charge ratio, according to classical electrodynamics, is that two particles with the same mass-to-charge ratio move in the same path in a vacuum , when subjected to the same electric and magnetic fields. Its SI units are kg/C. In rare occasions the thomson has been used as its unit in the field of mass spectrometry.\n\nSome fields use the charge-to-mass ratio (\"Q\"/\"m\") instead, which is the multiplicative inverse of the mass-to-charge ratio. The 2014 CODATA recommended value for an electron is = .\n\nWhen charged particles move in electric and magnetic fields the following two laws apply:\n\nwhere F is the force applied to the ion, \"m\" is the mass of the particle, a is the acceleration, \"Q\" is the electric charge, E is the electric field, and v × B is the cross product of the ion's velocity and the magnetic flux density.\n\nThis differential equation is the classic equation of motion for charged particles. Together with the particle's initial conditions, it completely determines the particle's motion in space and time in terms of \"m\"/\"Q\". Thus mass spectrometers could be thought of as \"mass-to-charge spectrometers\". When presenting data in a mass spectrum, it is common to use the dimensionless \"m\"/\"z\", which denotes the dimensionless quantity formed by dividing the mass number of the ion by its charge number.\n\nCombining the two previous equations yields:\n\nThis differential equation is the classic equation of motion of a charged particle in vacuum. Together with the particle's initial conditions it determines the particle's motion in space and time. It immediately reveals that two particles with the same \"m\"/\"Q\" ratio behave in the same way. This is why the mass-to-charge ratio is an important physical quantity in those scientific fields where charged particles interact with magnetic or electric fields.\n\nThere are non-classical effects that derive from quantum mechanics, such as the Stern–Gerlach effect that can diverge the path of ions of identical \"m\"/\"Q\".\n\nThe IUPAC recommended symbol for mass and charge are \"m\" and \"Q\", respectively, however using a lowercase \"q\" for charge is also very common. Charge is a scalar property, meaning that it can be either positive (+) or negative (−). The Coulomb (C) is the SI unit of charge; however, other units can be used, such as expressing charge in terms of the elementary charge (e). The SI unit of the physical quantity \"m\"/\"Q\" is kilogram per coulomb.\n\nThe units and notation above are used when dealing with the physics of mass spectrometry; however, the \"m\"/\"z\" notation is used for the independent variable in a mass spectrum. This notation eases data interpretation since it is numerically more related to the unified atomic mass unit. The \"m\" refers to the molecular or atomic mass number and \"z\" to the charge number of the ion; however, the quantity of \"m\"/\"z\" is dimensionless by definition. An ion of 100 atomic mass units (\"m\" = 100) carrying two charges (\"z\" = 2) will be observed at \"m\"/\"z\" = 50.\n\nIn the 19th century, the mass-to-charge ratios of some ions were measured by electrochemical methods. In 1897, the mass-to-charge ratio of the electron was first measured by J. J. Thomson. By doing this, he showed that the electron was in fact a particle with a mass and a charge, and that its mass-to-charge ratio was much smaller than that of the hydrogen ion H. In 1898, Wilhelm Wien separated ions (canal rays) according to their mass-to-charge ratio with an ion optical device with superimposed electric and magnetic fields (Wien filter). In 1901 Walter Kaufman measured the increase of electromagnetic mass of fast electrons (Kaufmann–Bucherer–Neumann experiments), or relativistic mass increase in modern terms. In 1913, Thomson measured the mass-to-charge ratio of ions with an instrument he called a parabola spectrograph. Today, an instrument that measures the mass-to-charge ratio of charged particles is called a mass spectrometer.\n\nThe charge-to-mass ratio (\"Q\"/\"m\") of an object is, as its name implies, the charge of an object divided by the mass of the same object. This quantity is generally useful only for objects that may be treated as particles. For extended objects, total charge, charge density, total mass, and mass density are often more useful.\n\nDerivation:\n\nformula_2 or formula_3 (1)\n\nSince formula_4,\n\nformula_5 or formula_6 (2)\n\nEquations (1) and (2) yield\n\nformula_7\n\nIn some experiments, the charge-to-mass ratio is the only quantity that can be measured directly. Often, the charge can be inferred from theoretical considerations, so that the charge-to-mass ratio provides a way to calculate the mass of a particle.\n\nOften, the charge-to-mass ratio can be determined from observing the deflection of a charged particle in an external magnetic field. The cyclotron equation, combined with other information such as the kinetic energy of the particle, will give the charge-to-mass ratio. One application of this principle is the mass spectrometer. The same principle can be used to extract information in experiments involving the cloud chamber.\n\nThe ratio of electrostatic to gravitational forces between two particles will be proportional to the product of their charge-to-mass ratios. It turns out that gravitational forces are negligible on the subatomic level, due to the extremely small masses of subatomic particles.\n\nThe elementary charge-to-electron mass quotient, formula_8, is a quantity in experimental physics. It bears significance because the electron mass \"m\" is difficult to measure directly, and is instead derived from measurements of the elementary charge e and formula_8. It also has historical significance; the \"Q\"/\"m\" ratio of the electron was successfully calculated by J. J. Thomson in 1897—and more successfully by Dunnington, which involves the angular momentum and deflection due to a perpendicular magnetic field. Thomson's measurement convinced him that cathode rays were particles, which were later identified as electrons, and he is generally credited with their discovery.\n\nThe 2014 CODATA recommended value is formula_8 = . CODATA refers to this as the electron charge-to-mass quotient, but ratio is still commonly used.\n\nThere are two other common ways of measuring the charge-to-mass ratio of an electron, apart from Thomson and Dunnington's methods.\n\n\nThe charge-to-mass ratio of an electron may also be measured with the Zeeman effect, which gives rise to energy splittings in the presence of a magnetic field \"B\":\n\nHere \"m\" are quantum integer values ranging from -\"j\" to \"j\", with \"j\" as the eigenvalue of the total angular momentum operator J, with\nwhere S is the spin operator with eigenvalue \"s\" and L is the angular momentum operator with eigenvalue \"l\". \"g\" is the Landé g-factor, calculated as\n\nThe shift in energy is also given in terms of frequency \"ν\" and wavelength \"λ\" as\n\nMeasurements of the Zeeman effect commonly involve the use of a Fabry–Pérot interferometer, with light from a source (placed in a magnetic field) being passed between two mirrors of the interferometer. If \"δD\" is the change in mirror separation required to bring the \"m\"-order ring of wavelength \"λ\" + \"Δλ\" into coincidence with that of wavelength \"λ\", and \"ΔD\" brings the (\"m\" + 1) ring of wavelength \"λ\" into coincidence with the \"m\"-order ring, then\n\nIt follows then that\n\nRearranging, it is possible to solve for the charge-to-mass ratio of an electron as\n\n\n\n"}
{"id": "37496709", "url": "https://en.wikipedia.org/wiki?curid=37496709", "title": "Mean-periodic function", "text": "Mean-periodic function\n\nIn mathematical analysis, the concept of a mean-periodic function is a generalization of the concept of a periodic function introduced in 1935 by Jean Delsarte. Further results were made by Laurent Schwartz. \n\nConsider a complex-valued function of a real variable. The function is periodic with period precisely if for all real , we have . This can be written as\n\nwhere formula_2 is the difference between the Dirac measures at 0 and \"a\". The function is mean-periodic if it satisfies the same equation (1), but where formula_2 is some arbitrary nonzero measure with compact (hence bounded) support.\n\nEquation (1) can be interpreted as a convolution, so that a mean-periodic function is a function for which there exists a compactly supported (signed) Borel measure formula_2 for which formula_5.\n\nThere are several well-known equivalent definitions.\n\nMean-periodic functions are a separate generalization of periodic functions from the almost periodic functions. For instance, exponential functions are mean-periodic since , but they are not almost periodic as they are unbounded. Still, there is a theorem which states that any uniformly continuous bounded mean-periodic function is almost periodic (in the sense of Bohr). In the other direction, there exist almost periodic functions which are not mean-periodic.\n\nIn work related to the Langlands correspondence, the mean-periodicity of certain (functions related to) zeta functions associated to an arithmetic scheme have been suggested to correspond to automorphicity of the related L-function. There is a certain class of mean-periodic functions arising from number theory. \n\n"}
{"id": "4265892", "url": "https://en.wikipedia.org/wiki?curid=4265892", "title": "Method of matched asymptotic expansions", "text": "Method of matched asymptotic expansions\n\nIn mathematics, the method of matched asymptotic expansions is a common approach to finding an accurate approximation to the solution to an equation, or system of equations. It is particularly used when solving singularly perturbed differential equations. It involves finding several different approximate solutions, each of which is valid (i.e. accurate) for part of the range of the independent variable, and then combining these different solutions together to give a single approximate solution that is valid for the whole range of values of the independent variable.\n\nIn a large class of singularly perturbed problems, the domain may be divided into two or more subdomains. In one of these, often the largest, the solution is accurately approximated by an asymptotic series found by treating the problem as a regular perturbation (i.e. by setting a relatively small parameter to zero). The other subdomains consist of one or more small areas in which that approximation is inaccurate, generally because the perturbation terms in the problem are not negligible there. These areas are referred to as transition layers, and as boundary or interior layers depending on whether they occur at the domain boundary (as is the usual case in applications) or inside the domain.\n\nAn approximation in the form of an asymptotic series is obtained in the transition layer(s) by treating that part of the domain as a separate perturbation problem. This approximation is called the \"inner solution,\" and the other is the \"outer solution,\" named for their relationship to the transition layer(s). The outer and inner solutions are then combined through a process called \"matching\" in such a way that an approximate solution for the whole domain is obtained.\n\nConsider the boundary value problem\n\nwhere formula_2 is a function of independent time variable formula_3, which ranges from 0 to 1, the boundary conditions are formula_4 and formula_5, and formula_6 is a small parameter, such that formula_7.\n\nSince formula_6 is very small, our first approach is to treat the equation as a regular perturbation problem, i.e. make the approximation formula_9, and hence find the solution to the problem\n\nAlternatively, consider that when formula_2 and formula_3 are both of size \"O\"(1), the four terms on the left hand side of the original equation are respectively of sizes \"O\"(formula_6), \"O\"(1), \"O\"(formula_6) and \"O\"(1). The leading-order balance on this timescale, valid in the distinguished limit formula_15, is therefore given by the second and fourth terms, i.e. formula_10\n\nThis has solution\n\nfor some constant formula_18. Applying the boundary condition formula_19, we would have formula_20; applying the boundary condition formula_21, we would have formula_22. It is therefore impossible to satisfy both boundary conditions, so formula_9 is not a valid approximation to make across the whole of the domain (i.e. this is a singular perturbation problem). From this we infer that there must be a boundary layer at one of the endpoints of the domain where formula_6 needs to be included. This region will be where formula_6 is no longer negligible compared to the independent variable formula_3, i.e. formula_3 and formula_6 are of comparable size, i.e. the boundary layer is adjacent to formula_29. Therefore, the other boundary condition formula_21 applies in this outer region, so formula_22, i.e. formula_32 is an accurate approximate solution to the original boundary value problem in this outer region. It is the leading-order solution.\n\nIn the inner region, formula_3 and formula_6 are both tiny, but of comparable size, so define the new \"O\"(1) time variable formula_35. Rescale the original boundary value problem by replacing formula_3 with formula_37, and the problem becomes\n\nwhich, after multiplying by formula_6 and taking formula_40, is\n\nAlternatively, consider that when formula_3 has reduced to size \"O\"(formula_6), then formula_2 is still of size \"O\"(1) (using the expression for formula_45), and so the four terms on the left hand side of the original equation are respectively of sizes \"O\"(formula_6), \"O\"(formula_6), \"O\"(1) and \"O\"(1). The leading-order balance on this timescale, valid in the distinguished limit formula_15, is therefore given by the first and second terms, i.e. formula_49\n\nThis has solution\n\nfor some constants formula_51 and formula_52. Since formula_4 applies in this inner region, this gives formula_54, so an accurate approximate solution to the original boundary value problem in this inner region (it is the leading-order solution) is\n\nWe use matching to find the value of the constant formula_51. The idea of matching is that the inner and outer solutions should agree for values of formula_3 in an intermediate (or overlap) region, i.e. where formula_58. We need the outer limit of the inner solution to match the inner limit of the outer solution, i.e.\nformula_59\nwhich gives formula_60.\n\nTo obtain our final, matched, composite solution, valid on the whole domain, one popular method is the uniform method. In this method, we add the inner and outer approximations and subtract their overlapping value, formula_61, which would otherwise be counted twice. The overlapping value is the outer limit of the inner boundary layer solution, and the inner limit of the outer solution; these limits were above found to equal formula_62. Therefore, the final approximate solution to this boundary value problem is,\n\nNote that this expression correctly reduces to the expressions for formula_64 and formula_45 when formula_3 is \"O\"(formula_6) and \"O\"(1), respectively.\n\nThis final solution satisfies the problem's original differential equation (shown by substituting it and its derivatives into the original equation). Also, the boundary conditions produced by this final solution match the values given in the problem, up to a constant multiple. This implies, due to the uniqueness of the solution, that the matched asymptotic solution is identical to the exact solution up to a constant multiple. This is not necessarily always the case, any remaining terms should go to zero uniformly as formula_68.\n\nNot only does our solution successfully approximately solve the problem at hand, it closely approximates the problem's exact solution. It happens that this particular problem is easily found to have exact solution\n\nwhich has the same form as the approximate solution, by the multiplying constant. Note also that the approximate solution is the first term in a binomial expansion of the exact solution in powers of formula_70.\n\nConveniently, we can see that the boundary layer, where formula_71 and formula_72 are large, is near formula_29, as we supposed earlier. If we had supposed it to be at the other endpoint and proceeded by making the rescaling formula_74, we would have found it impossible to satisfy the resulting matching condition. For many problems, this kind of trial and error is the only way to determine the true location of the boundary layer.\n\nThe problem above is a simple example because it is a single equation with only one dependent variable, and there is one boundary layer in the solution. Harder problems may contain several co-dependent variables in a system of several equations, and/or with several boundary and/or interior layers in the solution.\n\nIt is often desirable to find more terms in the asymptotic expansions of both the outer and the inner solutions. The appropriate form of these expansions is not always clear: while a power-series expansion in formula_6 may work, sometimes the appropriate form involves fractional powers of formula_6, functions such as formula_77, et cetera. As in the above example, we will obtain outer and inner expansions with some coefficients which must be determined by matching.\n\nA method of matched asymptotic expansions - with matching of solutions in the common domain of validity - has been developed and used extensively by Dingle and Müller-Kirsten for the derivation of asymptotic expansions of the solutions and characteristic numbers (band boundaries) of Schrödinger-like second-order differential equations with periodic potentials - in particular for the Mathieu equation (best example), Lamé and ellipsoidal wave equations, oblate and prolate spheroidal wave equations, and equations with anharmonic potentials.\n\n"}
{"id": "1401941", "url": "https://en.wikipedia.org/wiki?curid=1401941", "title": "Morley rank", "text": "Morley rank\n\nIn mathematical logic, Morley rank, introduced by , is a means of measuring the size of a subset of a model of a theory, generalizing the notion of dimension in algebraic geometry.\n\nFix a theory \"T\" with a model \"M\". The Morley rank of a formula φ defining a definable subset \"S\" of \"M\" \nis an ordinal or −1 or ∞, defined by first recursively defining what it means for a formula to have Morley rank at least α for some ordinal α. \nThe Morley rank is then defined to be α if it is at least α but not at least \"α\" + 1, and is defined to be ∞ if it is at least α for all ordinals α, and is defined to be −1 if \"S\" is empty.\n\nFor a subset of a model \"M\" defined by a formula φ the Morley rank is defined to be the Morley rank of φ in any ℵ-saturated elementary extension of \"M\". In particular for ℵ-saturated models the Morley rank of a subset is the Morley rank of any formula defining the subset.\n\nIf φ defining \"S\" has rank α, and \"S\" breaks up into no more than \"n\" < ω subsets of rank α, then φ is said to have Morley degree \"n\". A formula defining a finite set has Morley rank 0. A formula with Morley rank 1 and Morley degree 1 is called strongly minimal. A strongly minimal structure is one where the trivial formula \"x\" = \"x\" is strongly minimal. Morley rank and strongly minimal structures are key tools in the proof of Morley's categoricity theorem and in the larger area of stability theory (model theory).\n\n\n\n"}
{"id": "23868797", "url": "https://en.wikipedia.org/wiki?curid=23868797", "title": "Network Description Language", "text": "Network Description Language\n\nNetwork Description Language (NDL) is a tool to reduce the complexity as networks evolve into the future. NDL enables both humans and machines to have a better grasp on today’s highly evolved networks to ease time consuming and tedious tasks being performed by humans. Through the use of Resource Description Framework (RDF), researchers have been able to create an ontology for complex networks, thus creating a clear view of any network.\n\nNDL has proven itself useful in solving many issues as it pertains to the operation of hybrid networks, allowing the creation of network maps and facilitating path finding algorithms. SURFnet6, a Dutch national research and education network was one such network that has utilized NDL for lightpath and IP service planning.\n\n"}
{"id": "8105253", "url": "https://en.wikipedia.org/wiki?curid=8105253", "title": "Paul R. Halmos – Lester R. Ford Award", "text": "Paul R. Halmos – Lester R. Ford Award\n\nThe Paul R. Halmos – Lester R. Ford Award (formerly known as the Lester R. Ford Award) is a $1,000 prize given annually by the Mathematical Association of America for authors of articles of expository excellence published in \"The American Mathematical Monthly\" or \"Mathematics Magazine\". It is awarded to at most four authors each year. \nThe prize was established in 1964 as the Lester R. Ford Award to honor the contributions of mathematician and former MAA president Lester R. Ford. In 2012 the award was renamed the Paul R. Halmos – Lester R. Ford Award to honor the contributions of former \"The American Mathematical Monthly\" editor Paul R. Halmos and the support of the Halmos family for the awards. Halmos himself received the award in 1971 and 1977.\n\nThe recipients of the Paul R. Halmos – Lester R. Ford Award are:\n\n\n"}
{"id": "45164071", "url": "https://en.wikipedia.org/wiki?curid=45164071", "title": "Rectangular mask short-time Fourier transform", "text": "Rectangular mask short-time Fourier transform\n\nIn mathematics, a rectangular mask short-time Fourier transform has the simple form of short-time Fourier transform. Other types of the STFT may require more computation time than the rec-STFT.\nDefine its mask function\n\nWe can change \"B\" for different signal.\n\nRec-STFT\n\nInverse form\n\nRec-STFT has similar properties with Fourier transform\n(a)\n\n\n\n\nIf formula_10,formula_11and formula_12are their rec-STFTs, then\n\n\n\nFrom the image, when \"B\" is smaller, the time resolution is better. Otherwise, when \"B\" is larger, the frequency resolution is better.\n\nWe can choose specified \"B\" to decide time resolution and frequency resolution.\n\n\nAdvantage\nThe instantaneous frequency can be observed.\n\nDisadvantage\nHigher complexity of computation.\n\nThe rec-STFT has an advantage of the least computation time for digital implementation,\nbut its performance is worse than other types of time-frequency analysis.\n\n\n"}
{"id": "21091721", "url": "https://en.wikipedia.org/wiki?curid=21091721", "title": "Rota–Baxter algebra", "text": "Rota–Baxter algebra\n\nIn mathematics, a Rota–Baxter algebra is an associative algebra, together with a particular linear map \"R\" which satisfies the Rota–Baxter identity. It appeared first in the work of the American mathematician Glen E. Baxter in the realm of probability theory. Baxter's work was further explored from different angles by Gian-Carlo Rota, Pierre Cartier, and Frederic V. Atkinson, among others. Baxter’s derivation of this identity that later bore his name emanated from some of the fundamental results of the famous probabilist Frank Spitzer in random walk theory.\n\nIn the 1980s, the Rota-Baxter operator of weight 0 in the context of Lie algebras was rediscovered as the operator form of the classical Yang-Baxter equation, named after the well-known physicists Chen-Ning Yang and Rodney Baxter.\n\nThe study of Rota–Baxter algebras experienced a renaissance this century, beginning with several developments, in the algebraic approach to renormalization of perturbative quantum field theory, dendriform algebras, associative analogue of the classical Yang-Baxter equation and mixable shuffle product constructions.\n\nLet \"k\" be a commutative ring and let formula_1 be given. A linear operator \"R\" on a \"k\"-algebra \"A\" is called a Rota-Baxter operator of weight formula_1 if it satisfies the Rota-Baxter relation of weight formula_1:\n\nfor all formula_5. Then the pair formula_6 or simply \"A\" is called a Rota–Baxter algebra of weight formula_1. In some literature, formula_8 is used in which case the above equation becomes\n\ncalled the Rota-Baxter equation of weight formula_10. The terms Baxter operator algebra and Baxter algebra are also used.\n\nLet formula_11 be a Rota-Baxter of weight formula_1. Then formula_13 is also a Rota-Baxter operator of weight formula_1. Further, for formula_15 in \"k\", formula_16 is a Rota-Baxter operator of weight formula_17.\n\nIntegration by Parts\n\nIntegration by parts is an example of a Rota–Baxter algebra of weight 0. Let formula_18 be the algebra of continuous functions from the real line to the real line. Let :formula_19 be a continuous function. Define integration as the Rota–Baxter operator\n\nLet \"G(x)\" = \"I(g)(x)\" and \"F(x)\" = \"I(f)(x)\". Then the formula for integration for parts can be written in terms of these variables as\n\nIn other words\n\nwhich shows that \"I\" is a Rota–Baxter algebra of weight 0.\n\nThe Spitzer identity appeared is named after the American mathematician Frank Spitzer. It is regarded as a remarkable \nstepping stone in the theory of sums of independent random variables in fluctuation theory of probability. It can naturally be understood in terms of Rota–Baxter operators.\n\n"}
{"id": "4134907", "url": "https://en.wikipedia.org/wiki?curid=4134907", "title": "Salo Finkelstein", "text": "Salo Finkelstein\n\nSalo Finkelstein (born 1896 or 1897, date of death unknown) was a mental calculator, ranked eighth in the \"100 Greatest Mental Calculators\". He was born in Łódź (then within the Russian Empire, now Poland) to a Jewish family. \n\nWhile at school he was above average in mathematics, and discovered his calculating abilities as well as his faculty in memorizing numbers. At the age of 23, he began demonstrating this in public but lost interest for some time. He found employment with the Polish government in State Statistical office. \n\nIn 1928 he performed before Professor Hans Henning in the Free City of Danzig. Henning previously tested other calculators, Dr. Ferrol and Gottfried Ruckle, and found Finkelstein to be superior. In 1931 Finkelstein went on an international tour demonstrating his abilities and submitting himself for tests.\n\nIn 1932 he arrived in the United States and tried without success to find employment in a bank as a checker of calculations. In 1937 an article was published that described and analyzed his abilities, with the general conclusion that although he could perform calculations much more rapidly than most people, his thinking processes seem to obey the same laws and are not indicative of any unnatural powers. In particular, during multiplication, the time for performing operations was proportional not to the numbers of digits in multiplied numbers, but to the number of separate \"acts of attention\" necessary to perform multiplication by ordinary rules. Also, the correctness of the results was not always 100 percent, decreased rapidly with the growth of the number of \"acts of attention\", and apparently depended on concentration.\n\nAfter failing to secure himself a job that matched his abilities and unwilling to become a stage calculator, he attempted a career playing chess between 1941–1949. After that his further fate is unknown.\n\n\n"}
{"id": "57702051", "url": "https://en.wikipedia.org/wiki?curid=57702051", "title": "Space of directions", "text": "Space of directions\n\nIn metric geometry, the space of directions at a point describes the directions of curves that start at the point. It generalizes the tangent space in a differentiable manifold.\n\nLet (\"M\", \"d\") be a metric space. First we define the upper angle for two curves starting at the same point in \"M\". So let \nformula_1 be two curves with formula_2. The upper angle between them at \"p\" is\n\nThe upper angle satisfies the triangle inequality: For three curves formula_4 starting at \"p\",\n\nA curve is said to have a direction if the upper angle of two copies of itself at the starting point is zero. For curves which have directions at a point, we define an equivalence relation on them by saying that two curves are equivalent if the upper angle between them at the point is zero. Two equivalent curves are said to have the same direction at the point.\n\nThe set of equivalence classes of curves with directions at the point \"p\" equipped with the upper angle is a metric space, called the space of directions at the point, denoted as formula_6. The metric completion of the space of directions is called the completed space of directions, denoted as formula_7.\n\nFor an Alexandrov space with curvature bounded either above or below, there is also a similar definition in which shortest paths, which always have directions, are used. The space of directions at a point is then defined as the metric completion of the set of equivalence classes of shortest paths starting at the point.\n"}
{"id": "10854000", "url": "https://en.wikipedia.org/wiki?curid=10854000", "title": "Statistical study of energy data", "text": "Statistical study of energy data\n\nEnergy statistics refers to collecting, compiling, analyzing and disseminating data on commodities such as coal, crude oil, natural gas, electricity, or renewable energy sources (biomass, geothermal, wind or solar energy), when they are used for the energy they contain. Energy is the capability of some substances, resulting from their physico-chemical properties, to do work or produce heat. Some energy commodities, called fuels, release their energy content as heat when they burn. This heat could be used to run an internal or external combustion engine.\n\nThe need to have statistics on energy commodities became obvious during the 1973 oil crisis that brought tenfold increase in petroleum prices. Before the crisis, to have accurate data on global energy supply and demand was not deemed critical. Another concern of energy statistics today is a huge gap in energy use between developed and developing countries. As the gap narrows (\"see picture\"), the pressure on energy supply increases tremendously. \n\nThe data on energy and electricity come from three principal sources:\nThe flows of and trade in energy commodities are measured both in physical units (e.g., metric tons), and, when energy balances are calculated, in energy units (e.g., terajoules or tons of oil equivalent). What makes energy statistics specific and different from other fields of economic statistics is the fact that energy commodities undergo greater number of transformations (flows) than other commodities. In these transformations energy is conserved, as defined by and within the limitations of the first and second laws of thermodynamics. \n\n\n\n"}
{"id": "379619", "url": "https://en.wikipedia.org/wiki?curid=379619", "title": "Stone duality", "text": "Stone duality\n\nIn mathematics, there is an ample supply of categorical dualities between certain categories of topological spaces and categories of partially ordered sets. Today, these dualities are usually collected under the label Stone duality, since they form a natural generalization of Stone's representation theorem for Boolean algebras. These concepts are named in honor of Marshall Stone. Stone-type dualities also provide the foundation for pointless topology and are exploited in theoretical computer science for the study of formal semantics.\n\nThis article gives pointers to special cases of Stone duality and explains a very general instance thereof in detail.\n\nProbably the most general duality that is classically referred to as \"Stone duality\" is the duality between the category Sob of sober spaces with continuous functions and the category SFrm of spatial frames with appropriate frame homomorphisms. The dual category of SFrm is the category of locales denoted by SLoc. The categorical equivalence of Sob and SLoc is the basis for the mathematical area of pointless topology, that is devoted to the study of Loc – the category of all locales of which SLoc is a full subcategory. The involved constructions are characteristic for this kind of duality, and are detailed below.\n\nNow one can easily obtain a number of other dualities by restricting to certain special classes of sober spaces:\n\n\nMany other Stone-type dualities could be added to these basic dualities.\n\nThe starting point for the theory is the fact that every topological space is characterized by a set of points \"X\" and a system Ω(\"X\") of open sets of elements from \"X\", i.e. a subset of the powerset of \"X\". It is known that Ω(\"X\") has certain special properties: it is a complete lattice within which suprema and finite infima are given by set unions and finite set intersections, respectively. Furthermore, it contains both \"X\" and the empty set. Since the embedding of Ω(\"X\") into the powerset lattice of \"X\" preserves finite infima and arbitrary suprema, Ω(\"X\") inherits the following distributivity law:\n\nfor every element (open set) \"x\" and every subset \"S\" of Ω(\"X\"). Hence Ω(\"X\") is not an arbitrary complete lattice but a \"complete Heyting algebra\" (also called \"frame\" or \"locale\" – the various names are primarily used to distinguish several categories that have the same class of objects but different morphisms: frame morphisms, locale morphisms and homomorphisms of complete Heyting algebras). Now an obvious question is: To what extent is a topological space characterized by its locale of open sets?\n\nAs already hinted at above, one can go even further. The category Top of topological spaces has as morphisms the continuous functions, where a function \"f\" is continuous if the inverse image \"f\"(\"O\") of any open set in the codomain of \"f\" is open in the domain of \"f\". Thus any continuous function \"f\" from a space \"X\" to a space \"Y\" defines an inverse mapping \"f\" from Ω(\"Y\") to Ω(\"X\"). Furthermore, it is easy to check that \"f\" (like any inverse image map) preserves finite intersections and arbitrary unions and therefore is a \"morphism of frames\". If we define Ω(\"f\") = \"f\" then Ω becomes a contravariant functor from the category Top to the category Frm of frames and frame morphisms. Using the tools of category theory, the task of finding a characterization of topological spaces in terms of their open set lattices is equivalent to finding a functor from Frm to Top which is adjoint to Ω.\n\nThe goal of this section is to define a functor pt from Frm to Top that in a certain sense \"inverts\" the operation of Ω by assigning to each locale \"L\" a set of points pt(\"L\") (hence the notation pt) with a suitable topology. But how can we recover the set of points just from the locale, though it is not given as a lattice of sets? It is certain that one cannot expect in general that pt can reproduce all of the original elements of a topological space just from its lattice of open sets – for example all sets with the indiscrete topology yield (up to isomorphism) the same locale, such that the information on the specific set is no longer present. However, there is still a reasonable technique for obtaining \"points\" from a locale, which indeed gives an example of a central construction for Stone-type duality theorems.\n\nLet us first look at the points of a topological space \"X\". One is usually tempted to consider a point of \"X\" as an element \"x\" of the set \"X\", but there is in fact a more useful description for our current investigation. Any point \"x\" gives rise to a continuous function \"p\" from the one element topological space 1 (all subsets of which are open) to the space \"X\" by defining \"p\"(1) = \"x\". Conversely, any function from 1 to \"X\" clearly determines one point: the element that it \"points\" to. Therefore, the set of points of a topological space is equivalently characterized as the set of functions from 1 to \"X\".\n\nWhen using the functor Ω to pass from Top to Frm, all set-theoretic elements of a space are lost, but – using a fundamental idea of category theory – one can as well work on the function spaces. Indeed, any \"point\" \"p\": 1 → \"X\" in Top is mapped to a morphism Ω(\"p\"): Ω(\"X\") → Ω(1). The open set lattice of the one-element topological space Ω(1) is just (isomorphic to) the two-element locale 2 = { 0, 1 } with 0 < 1. After these observations it appears reasonable to define the set of points of a locale \"L\" to be the set of frame morphisms from \"L\" to 2. Yet, there is no guarantee that every point of the locale Ω(\"X\") is in one-to-one correspondence to a point of the topological space \"X\" (consider again the indiscrete topology, for which the open set lattice has only one \"point\").\n\nBefore defining the required topology on pt(\"X\"), it is worthwhile to clarify the concept of a point of a locale further. The perspective motivated above suggests to consider a point of a locale \"L\" as a frame morphism \"p\" from \"L\" to 2. But these morphisms are characterized equivalently by the inverse images of the two elements of 2. From the properties of frame morphisms, one can derive that \"p\"(0) is a lower set (since \"p\" is monotone), which contains a greatest element \"a\" = V \"p\"(0) (since \"p\" preserves arbitrary suprema). In addition, the principal ideal \"p\"(0) is a prime ideal since \"p\" preserves finite infima and thus the principal \"a\" is a meet-prime element. Now the set-inverse of \"p\"(0) given by \"p\"(1) is a completely prime filter because \"p\"(0) is a principal prime ideal. It turns out that all of these descriptions uniquely determine the initial frame morphism. We sum up:\n\nA point of a locale \"L\" is equivalently described as:\n\nAll of these descriptions have their place within the theory and it is convenient to switch between them as needed.\n\nNow that a set of points is available for any locale, it remains to equip this set with an appropriate topology in order to define the object part of the functor pt. This is done by defining the open sets of pt(\"L\") as\n\nfor every element \"a\" of \"L\". Here we viewed the points of \"L\" as morphisms, but one can of course state a similar definition for all of the other equivalent characterizations. It can be shown that setting Ω(pt(\"L\")) = {φ(\"a\") | \"a\" ∈ \"L\"} does really yield a topological space (pt(\"L\"), Ω(pt(\"L\"))). It is common to abbreviate this space as pt(\"L\").\n\nFinally pt can be defined on morphisms of Frm rather canonically by defining, for a frame morphism \"g\" from \"L\" to \"M\", pt(\"g\"): pt(\"M\") → pt(\"L\") as pt(\"g\")(\"p\") = \"p\" o \"g\". In words, we obtain a morphism from \"L\" to 2 (a point of \"L\") by applying the morphism \"g\" to get from \"L\" to \"M\" before applying the morphism \"p\" that maps from \"M\" to 2. Again, this can be formalized using the other descriptions of points of a locale as well – for example just calculate (\"p\" o \"g\")(0).\n\nAs noted several times before, pt and Ω usually are not inverses. In general neither is \"X\" homeomorphic to pt(Ω(\"X\")) nor is \"L\" order-isomorphic to Ω(pt(\"L\")). However, when introducing the topology of pt(\"L\") above, a mapping φ from \"L\" to Ω(pt(\"L\")) was applied. This mapping is indeed a frame morphism. Conversely, we can define a continuous function ψ from \"X\" to pt(Ω(\"X\")) by setting ψ(\"x\") = Ω(\"p\"), where \"p\" is just the characteristic function for the point \"x\" from 1 to \"X\" as described above. Another convenient description is given by viewing points of a locale as meet-prime elements. In this case we have ψ(\"x\") = \"X\" \\ Cl{\"x\"}, where Cl{\"x\"} denotes the topological closure of the set {\"x\"} and \\ is just set-difference.\n\nAt this point we already have more than enough data to obtain the desired result: the functors Ω and pt define an adjunction between the categories Top and Loc = Frm, where pt is right adjoint to Ω and the natural transformations ψ and φ provide the required unit and counit, respectively.\n\nThe above adjunction is not an equivalence of the categories Top and Loc (or, equivalently, a duality of Top and Frm). For this it is necessary that both ψ and φ are isomorphisms in their respective categories.\n\nFor a space \"X\", ψ: \"X\" → pt(Ω(\"X\")) is a homeomorphism if and only if it is bijective. Using the characterization via meet-prime elements of the open set lattice, one sees that this is the case if and only if every meet-prime open set is of the form \"X\" \\ Cl{\"x\"} for a unique \"x\". Alternatively, every join-prime closed set is the closure of a unique point, where \"join-prime\" can be replaced by (join-) irreducible since we are in a distributive lattice. Spaces with this property are called sober.\n\nConversely, for a locale \"L\", φ: \"L\" → Ω(pt(\"L\")) is always surjective. It is additionally injective if and only if any two elements \"a\" and \"b\" of \"L\" for which \"a\" is not less-or-equal to \"b\" can be separated by points of the locale, formally:\n\nIf this condition is satisfied for all elements of the locale, then the locale is spatial, or said to have enough points. (See also well-pointed category for a similar condition in more general categories.)\n\nFinally, one can verify that for every space \"X\", Ω(\"X\") is spatial and for every locale \"L\", pt(\"L\") is sober. Hence, it follows that the above adjunction of Top and Loc restricts to an equivalence of the full subcategories Sob of sober spaces and SLoc of spatial locales. This main result is completed by the observation that for the functor pt o Ω, sending each space to the points of its open set lattice is left adjoint to the inclusion functor from Sob to Top. For a space \"X\", pt(Ω(\"X\")) is called its soberification. The case of the functor Ω o pt is symmetric but a special name for this operation is not commonly used.\n\n"}
{"id": "26556669", "url": "https://en.wikipedia.org/wiki?curid=26556669", "title": "Theory of sonics", "text": "Theory of sonics\n\nThe theory of sonics is a branch of continuum mechanics which describes the transmission of mechanical energy through vibrations. The birth of the theory of sonics can be considered the publication of the book \"A treatise on transmission of power by vibrations\" in 1918 by the Romanian scientist Gogu Constantinescu.\nONE of the fundamental problems of mechanical engineering is that of transmitting energy found in nature, after suitable transformation, to some point at which can be made available for performing useful work. The methods of transmitting power known and practised by engineers are broadly included in two classes: mechanical including hydraulic, pneumatic and wire rope methods; and electrical methods...According to the new system, energy is transmitted from one point to another, which may be at a considerable distance, by means of impressed variations of pressure or tension producing longitudinal vibrations in solid, liquid or gaseous columns. The energy is transmitted by periodic changes of pressure and volume in the longitudinal direction and may be described as wave transmission of power, or mechanical wave transmission. – Gogu Constantinescu\n\nLater on the theory was expanded in electro-sonic, hydro-sonic, sonostereo-sonic and thermo-sonic.\nThe theory was the first chapter of compressible flow applications and has stated for the first time the mathematical theory of compressible fluid, and was considered a branch of continuum mechanics.The laws discovered by Constantinescu, used in sonicity are the same with the laws used in electricity.\n\nThe book \"A treatise on transmission of power by vibrations\" has the following chapters:\n\n\nGeorge Constantinescu defined his work as follow.\n\n\nIf v is the velocity of which waves travel along the pipe, and n the number of the revolutions of the crank a<br>\nThe wavelength λ is =v\"/\"n<br>\nAssuming that the pipe is finite and closed at the point r situated at a distance which is multiple of λ, and considering that the piston is smaller than wavelength, at r the wave compression is stopped and reflected, the reflected wave traveling back along the pipe.\n\nConsidering any flow or pipes, if:\n\nand\n\nthen we have:\n\nAssuming that the fluid current is produced by a piston having a simple harmonic movement, in a piston cylinder having a section Ω.\nIf we have:\n\nThen:\n\nWhere:\n\nIf T= period of a complete alternation (one revolution of the crank) then:\n\nThe effective current can be defined by the equation:\nThe stoke volume δ will be given by the relation:\n\nThe alternating pressures are very similar with alternating currents in electricity.\nIn a pipe were the currents are flowing, we will have:\nConsidering the above formulas:\nIf p is the pressure at an arbitrary point and p pressure in another arbitrary point:\nThe effective hydromotive force will be: formula_10\n\nIn alternating current flowing a pipe the friction appear at the surface of the pipe and also in liquid itself. Therefore, the relation between the hydromotive and current can be written:\n\nUsing experiments R may be calculated from formula:\n\nWhere:\n\nIf we introduce formula_17 in the formula, we get:\n\nFor pipes with greater diameter greater velocity can be achieve for same value of k.\nThe loss of power due to friction is calculated with:\n\nDefinition: Hydraulic condensers are appliances for making alterations in value of fluid currents, pressures or phases of alternating fluid currents. The apparatus usually consists of a mobile solid body, which is dividing the liquid column, and fixed elastically in a middle position, in such way that it follows the movements of the liquid column.\n\nThe principal function of hydraulic condensers is to counteract inertia effects due to moving masses.\n\n"}
{"id": "52378516", "url": "https://en.wikipedia.org/wiki?curid=52378516", "title": "Three spheres inequality", "text": "Three spheres inequality\n\nIn mathematics, the three spheres inequality bounds the formula_1 norm of an harmonic function on a given sphere in terms of the formula_1 norm of this function on two spheres, one with bigger radius and one with smaller radius.\n\nLet formula_3 be an harmonic function on formula_4. Then for all formula_5 one has\nwhere formula_7 for formula_8 is the sphere of radius formula_9 centred at the origin and where\nHere we use the following normalisation for the formula_1 norm:\n\n \n"}
{"id": "1528346", "url": "https://en.wikipedia.org/wiki?curid=1528346", "title": "Totally bounded space", "text": "Totally bounded space\n\nIn topology and related branches of mathematics, a totally bounded space is a space that can be covered by finitely many subsets of every fixed \"size\" (where the meaning of \"size\" depends on the given context). The smaller the size fixed, the more subsets may be needed, but any specific size should require only finitely many subsets. A related notion is a totally bounded set, in which only a subset of the space needs to be covered. Every subset of a totally bounded space is a totally bounded set; but even if a space is not totally bounded, some of its subsets still will be.\n\nThe term precompact (or pre-compact) is sometimes used with the same meaning, but \"pre-compact\" is also used to mean relatively compact. For subsets of a complete metric space \nthese meanings coincide but in general they do not. See also use of the axiom of choice below.\n\nA metric space formula_1 is totally bounded \nif and only if for every real number formula_2, there exists\na finite collection of open balls in \"M\" of radius formula_3 whose union contains \"M\". Equivalently, the metric space \"M\" is totally bounded if and only if for every formula_2, there exists a finite cover such that the radius of each element of the cover is at most formula_5. This is equivalent to the existence of a finite ε-net.\n\nEach totally bounded space is bounded (as the union of finitely many bounded sets is bounded), but the converse is not true in general.\nFor example, an infinite set equipped with the discrete metric is bounded but not totally bounded.\n\nIf \"M\" is Euclidean space and d is the Euclidean distance, then \na subset (with the subspace topology) is totally bounded if and only if it is bounded.\n\nA metric space is said to be cauchy-precompact if every sequence admits a Cauchy subsequence. Note that cauchy-precompact is not the same as precompact (relative compact), because cauchy-precompact is an intrinsic property of the space, while precompact depends on the ambient space. Thus for metric spaces we have: compactness = cauchy-precompactness + completeness. It turns out that the space is cauchy-precompact if and only if it is totally bounded. Therefore, both names (cauchy-precompact and totally bounded) can be used interchangeably.\n\nThe general logical form of the definition is: a subset \"S\" of a space \"X\" is a totally bounded set if and only if, given any size \"E\", there exist a natural number \"n\" and a family \"A\", \"A\", ..., \"A\" of subsets of \"X\", such that \"S\" is contained in the union of the family (in other words, the family is a \"finite cover\" of \"S\"), and such that each set \"A\" in the family is of size \"E\" (or less). In mathematical symbols:\n\nThe space \"X\" is a totally bounded space if and only if it is a totally bounded set when considered as a subset of itself.\n\nThe terms \"space\" and \"size\" here are vague, and they may be made precise in various ways:\n\nA subset \"S\" of a metric space \"X\" is totally bounded if and only if, given any positive real number \"E\", there exists a finite cover of \"S\" by subsets of \"X\" whose diameters are all less than \"E\". (In other words, a \"size\" here is a positive real number, and a subset is of size \"E\" if its diameter is less than \"E\".) Equivalently, \"S\" is totally bounded if and only if, given any \"E\" as before, there exist elements \"a\", \"a\", ..., \"a\" of \"X\" such that \"S\" is contained in the union of the \"n\" open balls of radius \"E\" around the points \"a\".\n\nA subset \"S\" of a topological vector space, or more generally topological abelian group, \"X\" is totally bounded if and only if, given any neighbourhood \"E\" of the identity (zero) element of \"X\", there exists a finite cover of \"S\" by subsets of \"X\" each of which is a translate of a subset of \"E\". (In other words, a \"size\" here is a neighbourhood of the identity element, and a subset is of size \"E\" if it is translate of a subset of \"E\".) Equivalently, \"S\" is totally bounded if and only if, given any \"E\" as before, there exist elements \"a\", \"a\", ..., \"a\" of \"X\" such that \"S\" is contained in the union of the \"n\" translates of \"E\" by the points \"a\".\n\nA topological group \"X\" is \"left\"-totally bounded if and only if it satisfies the definition for topological abelian groups above, using \"left\" translates. That is, use \"a\"\"E\" in place of \"E\" + \"a\". Alternatively, \"X\" is \"right\"-totally bounded if and only if it satisfies the definition for topological abelian groups above, using \"right\" translates. That is, use \"Ea\" in place of \"E\" + \"a\". (In other words, a \"size\" here is unambiguously a neighbourhood of the identity element, but there are two notions of \"whether\" a set is of a given size: a left notion based on left translation and a right notion based on right translation.)\n\nGeneralising the above definitions, a subset \"S\" of a uniform space \"X\" is totally bounded if and only if, given any entourage \"E\" in \"X\", there exists a finite cover of \"S\" by subsets of \"X\" each of whose Cartesian squares is a subset of \"E\". (In other words, a \"size\" here is an entourage, and a subset is of size \"E\" if its Cartesian square is a subset of \"E\".) Equivalently, \"S\" is totally bounded if and only if, given any \"E\" as before, there exist subsets \"A\", \"A\", ..., \"A\" of \"X\" such that \"S\" is contained in the union of the \"A\" and, whenever the elements \"x\" and \"y\" of \"X\" both belong to the same set \"A\", then (\"x\",\"y\") belongs to \"E\" (so that \"x\" and \"y\" are close as measured by \"E\").\n\nThe definition can be extended still further, to any category of spaces with a notion of compactness and Cauchy completion: a space is totally bounded if and only if its completion is compact.\n\n\nThere is a nice relationship between total boundedness and compactness:\n\nEvery compact metric space is totally bounded.\n\nEvery metric space that is complete (i.e. every Cauchy sequence of points in the space converges to a point within the space) and totally bounded is compact.\n\nA uniform space is compact if and only if it is both totally bounded and Cauchy complete. This can be seen as a generalisation of the Heine–Borel theorem from Euclidean spaces to arbitrary spaces: we must replace boundedness with total boundedness (and also replace closedness with completeness).\n\nThere is a complementary relationship between total boundedness and the process of Cauchy completion: A uniform space is totally bounded if and only if its Cauchy completion is totally bounded. (This corresponds to the fact that, in Euclidean spaces, a set is bounded if and only if its closure is bounded.)\n\nCombining these theorems, a uniform space is totally bounded if and only if its completion is compact. This may be taken as an alternative definition of total boundedness. Alternatively, this may be taken as a definition of \"precompactness\", while still using a separate definition of total boundedness. Then it becomes a theorem that a space is totally bounded if and only if it is precompact. (Separating the definitions in this way is useful in the absence of the axiom of choice; see the next section.)\n\nThe properties of total boundedness mentioned above rely in part on the axiom of choice. In the absence of the axiom of choice, total boundedness and precompactness must be distinguished. That is, we define total boundedness in elementary terms but define precompactness in terms of compactness and Cauchy completion. It remains true (that is, the proof does not require choice) that every precompact space is totally bounded; in other words, if the completion of a space is compact, then that space is totally bounded. But it is no longer true (that is, the proof requires choice) that every totally bounded space is precompact; in other words, the completion of a totally bounded space might not be compact in the absence of choice.\n\n\n"}
{"id": "6144888", "url": "https://en.wikipedia.org/wiki?curid=6144888", "title": "Tunnell's theorem", "text": "Tunnell's theorem\n\nIn number theory, Tunnell's theorem gives a partial resolution to the congruent number problem, and under the Birch and Swinnerton-Dyer conjecture, a full resolution.\n\nThe congruent number problem asks which positive integers can be the area of a right triangle with all three sides rational. Tunnell's theorem relates this to the number of integral solutions of a few fairly simple Diophantine equations.\n\nFor a given square-free integer \"n\", define\n\nTunnell's theorem states that supposing \"n\" is a congruent number, if \"n\" is odd then 2\"A\" = \"B\" and if \"n\" is even then 2\"C\" = \"D\". Conversely, if the Birch and Swinnerton-Dyer conjecture holds true for elliptic curves of the form formula_2, these equalities are sufficient to conclude that \"n\" is a congruent number.\n\nThe theorem is named for Jerrold B. Tunnell, a number theorist at Rutgers University, who proved it in .\n\nThe importance of Tunnell's theorem is that the criterion it gives is testable by a finite calculation. For instance, for a given \"n\", the numbers \"A\",\"B\",\"C\",\"D\" can be calculated by exhaustively searching through \"x\",\"y\",\"z\" in the range formula_3.\n\n"}
{"id": "9233359", "url": "https://en.wikipedia.org/wiki?curid=9233359", "title": "Vibrational circular dichroism", "text": "Vibrational circular dichroism\n\nVibrational circular dichroism (VCD) is a spectroscopic technique which detects differences in attenuation of left and right circularly polarized light passing through a sample. It is the extension of circular dichroism spectroscopy into the infrared and near infrared ranges.\n\nBecause VCD is sensitive to the mutual orientation of distinct groups in a molecule, it provides three-dimensional structural information. Thus, it is a powerful technique as VCD spectra of enantiomers can be simulated using \"ab initio\" calculations, thereby allowing the identification of absolute configurations of small molecules in solution from VCD spectra. Among such quantum computations of VCD spectra resulting from the chiral properties of small organic molecules are those based on density functional theory (DFT) and gauge-invariant atomic orbitals (GIAO). As a simple example of the experimental results that were obtained by VCD are the spectral data obtained within the carbon-hydrogen (C-H) stretching region of 21 amino acids in heavy water solutions. Measurements of vibrational optical activity (VOA) have thus numerous applications, not only for small molecules, but also for large and complex biopolymers such as muscle proteins (myosin, for example) and DNA.\n\nWhile the fundamental quantity associated with the infrared absorption is the dipole strength, the differential absorption is also proportional to the rotational strength, a quantity which depends on both the electric and magnetic dipole transition moments. Sensitivity of the handedness of a molecule toward circularly polarized light results from the form of the rotational strength. A rigorous theoretical development of VCD was developed concurrently by the late Professor P.J. Stephens, FRS, at the University of Southern California, and the group of Professor A.D. Buckingham, FRS, at Cambridge University in the UK, and first implemented analytically in the Cambridge Analytical Derivative Package (CADPAC) by R.D. Amos. Previous developments by D.P. Craig and T. Thirmachandiman at the Australian National University and Larry A. Nafie and Teresa B. Freedman at Syracuse University though theoretically correct, were not able to be straightfowardly implemented, which prevented their use. Only with the development of the Stephens formalism as implemented in CADPAC did a fast efficient and theoretically rigorous theoretical calculation of the VCD spectra of chiral molecules become feasible. This also stimulated the commercialization of VCD instruments by Biotools, Bruker, Jasco and Thermo-Nicolet (now Thermo-Fisher).\n\nExtensive VCD studies have been reported for both polypeptides and several proteins in solution; several recent reviews were also compiled. An extensive but not comprehensive VCD publications list is also provided in the \"References\" section. The published reports over the last 22 years have established VCD as a powerful technique with improved results over those previously obtained by visible/UV circular dichroism (CD) or optical rotatory dispersion (ORD) for proteins and nucleic acids.\n\nThe effects due to solvent on stabilizing the structures (conformers and zwitterionic species) of amino acids and peptides and the corresponding effects seen in the vibrational circular dichroism (VCD) and Raman optical activity spectra (ROA) have been recently documented by a combined theoretical and experimental work on L-alanine and N-acetyl L-alanine N'-methylamide. Similar effects have also been seen in the nuclear magnetic resonance (NMR) spectra by the Weise and Weisshaar NMR groups at the University of Wisconsin-Madison.\n\nVCD spectra of nucleotides, synthetic polynucleotides and several nucleic acids, including DNA, have been reported and assigned in terms of the type and number of helices present in A-, B-, and Z-DNA.\n\nVCD can be regarded as a relatively recent technique. Although Vibrational Optical Activity and in particular Vibrational Circular Dichroism, has been known for a long time, the first VCD instrument was developed in 1973 and commercial instruments were available only since 1997.\n\nFor biopolymers such as proteins and nucleic acids, the difference in absorbance between the levo- and dextro- configurations is five orders of magnitude smaller than the\ncorresponding (unpolarized) absorbance. Therefore, VCD of biopolymers requires the use of very sensitive, specially built instrumentation as well as time-averaging over relatively long intervals of time even with such sensitive VCD spectrometers.\nMost CD instruments produce left- and right- circularly polarized light which is then either sine-wave or square-wave modulated, with subsequent phase-sensitive detection and lock-in amplification of the detected signal. In the case of FT-VCD,\na photo-elastic modulator (PEM) is employed in conjunction with an FTIR interferometer set-up. An example is that of a Bomem model MB-100 FTIR interferometer equipped with additional polarizing optics/ accessories needed for recording VCD spectra.\nA parallel beam emerges through a side port of the interferometer which passes first through a wire grid linear polarizer and then through an octagonal-shaped ZnSe crystal PEM which modulates the polarized beam at a fixed, lower frequency such as 37.5 kHz. A mechanically stressed crystal such as ZnSe exhibits birefringence when stressed by an adjacent piezoelectric transducer. The linear polarizer is positioned close to, and at 45 degrees, with respect to the ZnSe crystal axis. The polarized radiation focused onto the detector is doubly modulated, both by the PEM and by the interferometer setup. A very low noise detector, such as MCT (HgCdTe), is also selected for the VCD signal phase-sensitive detection. The first dedicated VCD spectrometer brought to market was the ChiralIR from Bomem/BioTools, Inc. in 1997. Today, Thermo-Electron, Bruker, Jasco and BioTools offer either VCD accessories or stand-alone instrumentation. To prevent detector saturation an appropriate, long wave pass filter is placed before the very low noise MCT detector, which allows only radiation below 1750 cm to reach the MCT detector; the latter however measures radiation only down to 750 cm. FT-VCD spectra accumulation of the selected sample solution is then carried out, digitized and stored by an in-line computer. Published reviews that compare various VCD methods are also available.\n\nIn 1994, researchers at the University of Southern California (USC), U.S. Army Research Laboratory (USARL), and Lorentzian Inc., reported an accuracy ranking of quantum mechanical analytical techniques to theoretically determine vibrational frequencies, dipole strengths, and rotational strengths of an organic molecule. This ranking claimed density functional theory (DFT) at the B3LYP/6-31G* level of theory was the most accurate and effective computation used to model and vibrational circular dichroism (VCD)  spectra. Electronic structure computations, by solving either the Schrödinger equation or the Kohn-Sham equation, can be used to obtain information about ground state energy, bond vibrational frequency, and electron density (Ψ), and other characteristics. \n\nTheoretical calculations of vibrational energy often involve the Schrödinger’s equation with the Hamiltonian operator. The computers that process this massive calculation can incorporate the molecule’s kinetic energy as well as the vast number of repulsions and columbic attractions between subatomic particles. The calculations are said to be very costly, as they are difficult and take a long time to accomplish. This is partially because integrating the electron-electron interactions into the equation involves determining electron exchange interactions. Methods like DFT and the Hartree Fock look at a group of atomic orbitals referred to as a basis set to estimate wave function. The wave function can be used to determine frequency, wavelength, energy etc. Hartree Fock operates with a feedback loop called a self-consistent field that continuously refines the wave function estimates until the value falls within a satisfactory change in energy threshold that they converge the calculation and determine an approximated wavefunction. \n\nThe study conducted by USC, USARL, and Lorentzian Inc. analyzed the infrared (FTIR) and VCD spectra of the chiral molecule 4-methyl-2-oxetanone. Lorentzian bands were fit to FTIR and VCD spectra to obtain their peak intensity, line width, and frequency, which may be used to infer properties such as dipole strengths and rotational strengths. These experimental values were then compared to theoretical results. The scientists reported that DFT computations evaluated with the B3LYP functional best modelled FTIR and VCD spectra. To achieve a better cost to benefit ratio the researchers recommended pairing this method with the 6-31* basis set. The second best method reported was the second order Møller–Plesset perturbation theory (MP2). The third and fourth best calculation methods were DFT with the BLYP and LSDA functionals respectively. The researchers stated that \"Ab initio\" Hartree Fock Self Consistent Field (HF-SCF) computations modelled FTIR and VCD spectra with the lowest accuracy compared to other methodologies investigated.\n\nThe significance in claimed improvement in accuracy of DFT computations over \"ab\" \"initio\" techniques was that DFT computations were reported to quicken computational speed. By evaluating an effective potential using electron density, which can by specified across three degrees of freedom, DFT sidesteps the evaluation of coulombic potentials between every single electron, which is specified over 3N degrees of freedom (where N is the number of electrons). The B3LYP basis set is a hybrid between direct Hartree-Fock exchange terms as well as local and gradient corrections for exchange and correlation interactions. Therefore, the B3LYP functional is claimed to efficiently model FTIR and VCD of some molecules \"via\" DFT at a fraction of the cost. \n\nVCD spectra have also been reported in the presence of an applied external magnetic field. This method can enhance the VCD spectral resolution for small molecules.\n\nROA is a technique complementary to VCD especially useful in the 50–1600 cm spectral region; it is considered as the technique of choice for determining optical activity for photon energies less than 600 cm.\n"}
{"id": "15629024", "url": "https://en.wikipedia.org/wiki?curid=15629024", "title": "Warren Goldfarb", "text": "Warren Goldfarb\n\nWarren David Goldfarb (born 1949) is a philosopher and mathematician with a specialization in the history of analytic philosophy and in logic, most notably his work on the classical decision problem (see his book on the subject, \"The decision problem: Solvable classes of quantificational formulas\", with Burton Dreben).\n\nHe received his Ph.D. from Harvard University, where he is now Walter Beverly Pearson Professor of Modern Mathematics and Mathematical Logic in the Department of Philosophy. He has been on the Harvard faculty since 1975, and was tenured in 1982, the only philosopher to be promoted to tenure at Harvard between 1962 and 1999.\n\nProf. Goldfarb is also one of the founders of the Harvard Gay & Lesbian Caucus and was one of the first openly gay Harvard faculty members.\n\nGoldfarb was an editor of volumes III–V of Kurt Gödel's \"Collected Works\". He has also published articles on important analytic philosophers, including Frege, Russell, Wittgenstein's early and later work, Carnap and Quine.\n\n"}
{"id": "3036289", "url": "https://en.wikipedia.org/wiki?curid=3036289", "title": "Weil's conjecture on Tamagawa numbers", "text": "Weil's conjecture on Tamagawa numbers\n\nIn mathematics, the Weil conjecture on Tamagawa numbers is the statement that the Tamagawa number formula_1 of a simply connected simple algebraic group defined over a number field is 1. In this case, \"simply connected\" means \"not having a proper \"algebraic\" covering\" in the algebraic group theory sense, which is not always the topologists' meaning.\n\n calculated the Tamagawa number in many cases of classical groups and observed that it is an integer in all considered cases and that it was equal to 1 in the cases when the group is simply connected. The first observation does not hold for all groups: found examples where the Tamagawa numbers are not integers. The second observation, that the Tamagawa numbers of simply connected semisimple groups seem to be 1, became known as the Weil conjecture.\n\nRobert Langlands (1966) introduced harmonic analysis methods to show it for Chevalley groups. K. F. Lai (1980) extended the class of known cases to quasisplit reductive groups. proved it for all groups satisfying the Hasse principle, which at the time was known for all groups without \"E\" factors. V. I. Chernousov (1989) removed this restriction, by proving the Hasse principle for the resistant \"E\" case (see strong approximation in algebraic groups), thus completing the proof of Weil's conjecture. In 2011, Jacob Lurie and Dennis Gaitsgory announced a proof of the conjecture for algebraic groups over function fields over finite fields.\n\n used the Weil conjecture to calculate the Tamagawa numbers of all semisimple algebraic groups.\n\nFor spin groups, the conjecture implies the known Smith–Minkowski–Siegel mass formula.\n\n"}
