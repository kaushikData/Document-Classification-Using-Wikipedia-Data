{"id": "2959451", "url": "https://en.wikipedia.org/wiki?curid=2959451", "title": "233 (number)", "text": "233 (number)\n\n233 (two hundred [and] thirty-three) is the natural number following 232 and preceding 234.\n\n233 is a prime number,\n233 is a Sophie Germain prime,\na Pillai prime,\nand a Ramanujan prime.\nIt is a Fibonacci number,\none of the Fibonacci primes.\n\nThere are exactly 233 maximal planar graphs with ten vertices,\nand 233 connected topological spaces with four points.\n"}
{"id": "50761824", "url": "https://en.wikipedia.org/wiki?curid=50761824", "title": "4D vector", "text": "4D vector\n\nIn computer science, a 4D vector is a 4-component vector data type. Uses include homogeneous coordinates for 3-dimensional space in computer graphics, and \"red green blue alpha\" (RGBA) values for bitmap images with a color and alpha channel (as such they are widely used in computer graphics). They may also represent quaternions (useful for rotations) although the algebra they define is different.\n\nSome microprocessors have hardware support for 4D vectors with instructions dealing with 4 lane \"single instruction, multiple data\" (SIMD) instructions, usually with a 128-bit data path and 32-bit floating point fields.\n\nSpecific instructions (e.g., 4 element dot product) may facilitate the use of one 128-bit register to represent a 4D vector. For example, in chronological order: Hitachi SH4, PowerPC VMX128 extension, and Intel x86 SSE4.\n\nSome 4-element vector engines (e.g., the PS2 vector units) went further with the ability to broadcast components as multiply sources, and cross product support. Earlier generations of graphics processing unit (GPU) shader pipelines used \"very long instruction word\" (VLIW) instruction sets tailored for similar operations.\n\nSIMD use for 4D vectors can be conveniently wrapped in a \"vector maths library\" (commonly implemented in C or C++) \ncommonly used in video game development, along with 4×4 matrix support. These are distinct from more general linear algebra libraries in other domains focussing on matrices of arbitrary size. Such libraries sometimes support 3D vectors padded to 4D or loading 3D data into 4D registers, with arithmetic mapped efficiently to SIMD operations by per platform intrinsic function implementations. There is choice between AOS and SOA approaches given the availability of 4 element registers, versus SIMD instructions that are usually tailored toward homogenous data.\n\nShading languages for graphics processing unit (GPU) programming usually have a 4D datatypes (along with 2D, 3D) with x-y-z-w accessors including \"permutes\" or \"swizzle\" access, e.g., allowing easy swapping of RGBA or ARGB formats, accessing two 2D vectors packed into one 4D vector, etc. Modern GPUs have since moved to scalar single instruction, multiple threads (SIMT) pipelines (for more efficiency in \"general-purpose computing on graphics processing units\" (GPGPU)) but still support this programming model.\n\n"}
{"id": "19216612", "url": "https://en.wikipedia.org/wiki?curid=19216612", "title": "Asymptotology", "text": "Asymptotology\n\nAsymptotology has been defined as “the art of dealing with applied mathematical systems in limiting cases” as well as “the science about the synthesis of simplicity and exactness by means of localization.\n\nThe field of asymptotics is normally first encountered in school geometry with the introduction of the asymptote, a line to which a curve tends at infinity. The word Ασύμπτωτος (asymptotos) in Greek means non-coincident and puts strong emphasis on the point that approximation does not turn into coincidence. It is a salient feature of asymptotics, but this property alone does not entirely cover the idea of asymptotics and, etymologically, the term seems to be quite insufficient.\n\nIn physics and other fields of science, one frequently comes across problems of an asymptotic nature, such as damping, orbiting, stabilization of a perturbed motion, etc. Their solutions lend themselves to asymptotic analysis (perturbation theory), which is widely used in modern applied mathematics, mechanics and physics. But asymptotic methods put a claim on being more than a part of classical mathematics. K. Friedrichs said: “Asymptotic description is not only a convenient tool in the mathematical analysis of nature, it has some more fundamental significance”. M. Kruskal introduced the special term asymptotology, defined above, and called for a formalization of the accumulated experience to convert the art of asymptotology to a science.\nIn addition, “the success of ‘cybernetics’, ‘attractors’ and ‘catastrophe theory’ illustrates the fruitfulness of word creation as scientific research”.\n\nAlmost every physical theory, formulated in the most general manner, is rather difficult from a mathematical point of view. Therefore, both at the genesis of the theory and its further development, the simplest limiting cases, which allow analytical solutions, are of particular importance. In those limits, the number of equations usually decreases, their order reduces, nonlinear equations can be replaced by linear ones, the initial system becomes averaged in a certain sense, and so on.\n\nAll these idealizations, different as they may seem, increase the degree of symmetry of the mathematical model of the phenomenon under consideration.\n\nIn essence, the asymptotic approach to a complex problem consists in treating the insufficiently symmetrical governing system as close to a certain symmetrical one as possible.\n\nIn attempting to obtain a better approximation of the exact solution to the given problem, it is crucial that the determination of corrective solutions, which depart from the limit case, be much simpler than directly investigating the governing system. At first sight, the possibilities of such an approach seem restricted to varying the parameters determining the system only within a narrow range. However, experience in the investigation of different physical problems shows that if the system’s parameters have changed sufficiently and the system has deviated far from the symmetrical limit case, another limit system, often with less obvious symmetries can be found, to which an asymptotic analysis is also applicable. This allows one to describe the system’s behavior on the basis of a small number of limit cases over the whole range of parameter variations. Such an approach corresponds to the maximum level of intuition, promotes further insights, and eventually leads to the formulation of new physical concepts. It is also important that asymptotic methods help to establish the connection between different physical theories.\nThe aim of the asymptotic approach is to simplify the object. This simplification is attained by decreasing the vicinity of the singularity under consideration. It is typical that the accuracy of asymptotic expansions grows with localization. Exactness and simplicity are commonly regarded as mutually exclusive notions. When tending to simplicity, we sacrifice exactness, and trying to achieve exactness, we expect no simplicity. Under localization, however, the antipodes converge; the contradiction is resolved in a synthesis called asymptotics. In other words, simplicity and exactness are coupled by an “uncertainty principle” relation while the domain size serves as a small parameter – a measure of uncertainty.\n\nLet us illustrate the “asymptotic uncertainty principle”. Take the expansion of the function formula_1 in an asymptotic sequence formula_2: \nformula_3, formula_4 → formula_5.\n\nA partial sum of the series is designated by formula_6, and the exactness of approximation at a given formula_7 is estimated by formula_8. Simplicity is characterized here by the number formula_7 and the locality by the length of interval formula_4.\n\nBased on known properties of the asymptotic expansion, we consider the pair wise interrelation of values formula_4, formula_7, and formula_13. At a fixed formula_4 the expansion initially converges, i.e., the exactness increases at the cost of simplicity. If we fix formula_7, the exactness and the interval size begin to compete. The smaller the interval, the given value of formula_13 is reached more simply.\n\nWe illustrate these regularities using a simple example. Consider the exponential integral function: \nformula_17.\n\nIntegrating by parts, we obtain the following asymptotic expansion \nformula_18 → formula_19.\n\nPut formula_20, formula_21. Calculating the partial sums of this series and the values formula_22 and formula_1 for different formula_4 yields:\n\nThus, at a given formula_4, the exactness first increases with the growth of formula_7 and then decreases (so one has an asymptotic expansion). For a given formula_7, one may observe an improvement of exactness with diminishing formula_4.\n\nFinally, is it worth using asymptotic analysis if computers and numerical methods have reached such an advanced state? As D. G. Crighton has mentioned,\n\n"}
{"id": "40779", "url": "https://en.wikipedia.org/wiki?curid=40779", "title": "BCH code", "text": "BCH code\n\nIn coding theory, the BCH codes or Bose–Chaudhuri–Hocquenghem codes form a class of cyclic error-correcting codes that are constructed using polynomials over a finite field (also called \"Galois field\"). BCH codes were invented in 1959 by French mathematician Alexis Hocquenghem, and independently in 1960 by Raj Bose and D. K. Ray-Chaudhuri. The name \"Bose–Chaudhuri–Hocquenghem\" (and the acronym \"BCH\") arises from the initials of the inventors' surnames (mistakenly, in the case of Ray-Chaudhuri).\n\nOne of the key features of BCH codes is that during code design, there is a precise control over the number of symbol errors correctable by the code. In particular, it is possible to design binary BCH codes that can correct multiple bit errors. Another advantage of BCH codes is the ease with which they can be decoded, namely, via an algebraic method known as syndrome decoding. This simplifies the design of the decoder for these codes, using small low-power electronic hardware.\n\nBCH codes are used in applications such as satellite communications, compact disc players, DVDs, disk drives, solid-state drives and two-dimensional bar codes.\n\nGiven a prime number and prime power with positive integers and such that , a primitive narrow-sense BCH code over the finite field (or Galois field) with code length and distance at least is constructed by the following method.\n\nLet be a primitive element of .\nFor any positive integer , let be the minimal polynomial with coefficients in of .\nThe generator polynomial of the BCH code is defined as the least common multiple .\nIt can be seen that is a polynomial with coefficients in and divides .\nTherefore, the polynomial code defined by is a cyclic code.\n\nLet and (therefore ). We will consider different values of . For based on the polynomial with primitive root there are minimum polynomials with coefficients in satisfying\nThe minimal polynomials of the fourteen powers of are\n\nThe BCH code with formula_6 has generator polynomial\n\nformula_7\n\nIt has minimal Hamming distance at least 3 and corrects up to one error. Since the generator polynomial is of degree 4, this code has 11 data bits and 4 checksum bits.\n\nThe BCH code with formula_8 has generator polynomial\n\nformula_9\n\nIt has minimal Hamming distance at least 5 and corrects up to two errors. Since the generator polynomial is of degree 8, this code has 7 data bits and 8 checksum bits.\n\nThe BCH code with formula_10 has generator polynomial\n\nformula_11\n\nIt has minimal Hamming distance at least 7 and corrects up to three errors. Since the generator polynomial is of degree 10, this code has 5 data bits and 10 checksum bits. (This particular generator polynomial has a real-world application, in the format patterns of the QR code.)\n\nThe BCH code with formula_12 and higher has generator polynomial\n\nformula_13\n\nThis code has minimal Hamming distance 15 and corrects 7 errors. It has 1 data bit and 14 checksum bits. In fact, this code has only two codewords: 000000000000000 and 111111111111111.\n\nGeneral BCH codes differ from primitive narrow-sense BCH codes in two respects.\n\nFirst, the requirement that formula_14 be a primitive element of formula_15 can be relaxed. By relaxing this requirement, the code length changes from formula_16 to formula_17 the order of the element formula_18\n\nSecond, the consecutive roots of the generator polynomial may run from formula_19 instead of formula_20\n\nDefinition. Fix a finite field formula_21 where formula_22 is a prime power. Choose positive integers formula_23 such that formula_24 formula_25 and formula_26 is the multiplicative order of formula_22 modulo formula_28\n\nAs before, let formula_14 be a primitive formula_30th root of unity in formula_31 and let formula_32 be the minimal polynomial over formula_33 of formula_34 for all formula_35\nThe generator polynomial of the BCH code is defined as the least common multiple formula_36\n\nNote: if formula_37 as in the simplified definition, then formula_38 is 1, and the order of formula_22 modulo formula_30 is formula_41\nTherefore, the simplified definition is indeed a special case of the general one.\n\n\nThe generator polynomial formula_44 of a BCH code has coefficients from formula_45\nIn general, a cyclic code over formula_46 with formula_44 as the generator polynomial is called a BCH code over formula_48\nThe BCH code over formula_15 and generator polynomial formula_44 with successive powers of formula_14 as roots is one type of Reed–Solomon code where the decoder (syndromes) alphabet is the same as the channel (data and generator polynomial) alphabet, all elements of formula_15 . The other type of Reed Solomon code is an which is not a BCH code.\n\nThe generator polynomial of a BCH code has degree at most formula_53. Moreover, if formula_54 and formula_42, the generator polynomial has degree at most formula_56.\n\nEach minimal polynomial formula_32 has degree at most formula_26.\nTherefore, the least common multiple of formula_59 of them has degree at most formula_53.\nMoreover, if formula_61 then formula_62 for all formula_63.\nTherefore, formula_44 is the least common multiple of at most formula_65 minimal polynomials formula_32 for odd indices formula_67 each of degree at most formula_26.\nA BCH code has minimal Hamming distance at least formula_69.\n\nSuppose that formula_70 is a code word with fewer than formula_69 non-zero terms. Then\n\nRecall that formula_19 are roots of formula_74 hence of formula_70.\nThis implies that formula_76 satisfy the following equations, for each formula_77:\n\nIn matrix form, we have\n\nThe determinant of this matrix equals\n\nThe matrix formula_81 is seen to be a Vandermonde matrix, and its determinant is\nwhich is non-zero. It therefore follows that formula_83 hence formula_84\nA BCH code is cyclic. \nA polynomial code of length formula_30 is cyclic if and only if its generator polynomial divides formula_86\nSince formula_44 is the minimal polynomial with roots formula_88 it suffices to check that each of formula_19 is a root of formula_86\nThis follows immediately from the fact that formula_14 is, by definition, an formula_30th root of unity.\nThere are many algorithms for decoding BCH codes. The most common ones follow this general outline:\n\nDuring some of these steps, the decoding algorithm may determine that the received vector has too many errors and cannot be corrected. For example, if an appropriate value of \"t\" is not found, then the correction would fail. In a truncated (not primitive) code, an error location may be out of range. If the received vector has more errors than the code can correct, the decoder may unknowingly produce an apparently valid message that is not the one that was sent.\n\nThe received vector formula_93 is the sum of the correct codeword formula_94 and an unknown error vector formula_95\nThe syndrome values are formed by considering formula_93 as a polynomial and evaluating it at formula_97\nThus the syndromes are\nfor formula_99 to formula_100\nSince formula_101 are the zeros of formula_74 of which\nformula_103 is a multiple, formula_104\nExamining the syndrome values thus isolates the error vector so one can begin to solve for it.\n\nIf there is no error, formula_105 for all formula_106\nIf the syndromes are all zero, then the decoding is done.\n\nIf there are nonzero syndromes, then there are errors. The decoder needs to figure out how many errors and the location of those errors.\n\nIf there is a single error, write this as formula_107\nwhere formula_63 is the location of the error and formula_109 is its magnitude. Then the first two syndromes are\nso together they allow us to calculate formula_109 and provide some information about formula_63 (completely determining it in the case of Reed–Solomon codes).\n\nIf there are two or more errors,\nIt is not immediately obvious how to begin solving the resulting syndromes for the unknowns formula_115 and formula_116\nFirst step is finding locator polynomial\n\nTwo popular algorithms for this task are:\n\nPeterson's algorithm is the step 2 of the generalized BCH decoding procedure. Peterson's algorithm is used to calculate the error locator polynomial coefficients formula_119 of a polynomial\n\nNow the procedure of the Peterson–Gorenstein–Zierler algorithm. Expect we have at least 2\"t\" syndromes \"s\"...,\"s\".\nLet \"v\" = \"t\".\n if formula_131\n\nNow that you have the formula_135 polynomial, its roots can be found in the form formula_136 by brute force for example using the Chien search algorithm. The exponential\npowers of the primitive element formula_14 will yield the positions where errors occur in the received word; hence the name 'error locator' polynomial.\n\nThe zeros of Λ(\"x\") are \"α\", ..., \"α\".\n\nOnce the error locations are known, the next step is to determine the error values at those locations. The error values are then used to correct the received values at those locations to recover the original codeword.\n\nFor the case of binary BCH, (with all characters readable) this is trivial; just flip the bits for the received word at these positions, and we have the corrected code word. In the more general case, the error weights formula_138 can be determined by solving the linear system\n\nHowever, there is a more efficient method known as the Forney algorithm.\n\nLet\n\nAnd the error evaluator polynomial\n\nFinally:\n\nwhere\n\nThan if syndromes could be explained by an error word, which could be nonzero only on positions formula_145, then error values are\n\nFor narrow-sense BCH codes, \"c\" = 1, so the expression simplifies to:\n\nIt is based on Lagrange interpolation and techniques of generating functions.\n\nConsider formula_148 and for the sake of simplicity suppose formula_149 for formula_150 and formula_151 for formula_152 Then\n\nWe want to compute unknowns formula_155 and we could simplify the context by removing the formula_156 terms. This leads to the error evaluator polynomial\n\nThanks to formula_158 we have\n\nThanks to formula_125 (the Lagrange interpolation trick) the sum degenerates to only one summand for formula_161\n\nTo get formula_115 we just should get rid of the product. We could compute the product directly from already computed roots formula_164 of formula_165 but we could use simpler form.\n\nAs formal derivative\n\nwe get again only one summand in\n\nSo finally\n\nThis formula is advantageous when one computes the formal derivative of formula_125 form\n\nyielding:\n\nwhere\n\nAn alternate process of finding both the polynomial Λ and the error locator polynomial is based on Yasuo Sugiyama's adaptation of the Extended Euclidean algorithm. Correction of unreadable characters could be incorporated to the algorithm easily as well.\n\nLet formula_173 be positions of unreadable characters. One creates polynomial localising these positions formula_174\nSet values on unreadable positions to 0 and compute the syndromes.\n\nAs we have already defined for the Forney formula let formula_175\n\nLet us run extended Euclidean algorithm for locating least common divisor of polynomials formula_176 and formula_177\nThe goal is not to find the least common divisor, but a polynomial formula_178 of degree at most formula_179 and polynomials formula_180 such that formula_181\nLow degree of formula_178 guarantees, that formula_183 would satisfy extended (by formula_184) defining conditions for formula_185\n\nDefining formula_186 and using formula_187 on the place of formula_135 in the Fourney formula will give us error values.\n\nThe main advantage of the algorithm is that it meanwhile computes formula_189 required in the Forney formula.\n\nThe goal is to find a codeword which differs from the received word minimally as possible on readable positions. When expressing the received word as a sum of nearest codeword and error word, we are trying to find error word with minimal number of non-zeros on readable positions. Syndrom formula_190 restricts error word by condition\n\nWe could write these conditions separately or we could create polynomial\n\nand compare coefficients near powers formula_193 to formula_194\n\nSuppose there is unreadable letter on position formula_196 we could replace set of syndromes formula_197 by set of syndromes formula_198 defined by equation formula_199 Suppose for an error word all restrictions by original set formula_197 of syndromes hold,\nthan\n\nNew set of syndromes restricts error vector\n\nthe same way the original set of syndromes restricted the error vector formula_203 Note, that except the coordinate formula_196 where we have formula_205 an formula_206 is zero, if formula_207 For the goal of locating error positions we could change the set of syndromes in the similar way to reflect all unreadable characters. This shortens the set of syndromes by formula_208\n\nIn polynomial formulation, the replacement of syndromes set formula_197 by syndromes set formula_198 leads to\n\nTherefore,\n\nAfter replacement of formula_213 by formula_176, one would require equation for coefficients near powers formula_215\n\nOne could consider looking for error positions from the point of view of eliminating influence of given positions similarly as for unreadable characters. If we found formula_216 positions such that eliminating their influence leads to obtaining set of syndromes consisting of all zeros, than there exists error vector with errors only on these coordinates.\nIf formula_135 denotes the polynomial eliminating the influence of these coordinates, we obtain\n\nIn Euclidean algorithm, we try to correct at most formula_219 errors (on readable positions), because with bigger error count there could be more codewords in the same distance from the received word. Therefore, for formula_135 we are looking for, the equation must hold for coefficients near powers starting from\n\nIn Forney formula, formula_135 could be multiplied by a scalar giving the same result.\n\nIt could happen that the Euclidean algorithm finds formula_135 of degree higher than formula_219 having number of different roots equal to its degree, where the Fourney formula would be able to correct errors in all its roots, anyway correcting such many errors could be risky (especially with no other restrictions on received word). Usually after getting formula_135 of higher degree, we decide not to correct the errors. Correction could fail in the case formula_135 has roots with higher multiplicity or the number of roots is smaller than its degree. Fail could be detected as well by Forney formula returning error outside the transmitted alphabet.\n\nUsing the error values and error location, correct the errors and form a corrected code vector by subtracting error values at error locations.\n\nConsider a BCH code in GF(2) with formula_227 and formula_228. (This is used in QR codes.) Let the message to be transmitted be <nowiki>[1 1 0 1 1]</nowiki>, or in polynomial notation, formula_229\nThe \"checksum\" symbols are calculated by dividing formula_230 by formula_44 and taking the remainder, resulting in formula_232 or <nowiki>[ 1 0 0 0 0 1 0 1 0 0 ]</nowiki>. These are appended to the message, so the transmitted codeword is <nowiki>[ 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 ]</nowiki>.\n\nNow, imagine that there are two bit-errors in the transmission, so the received codeword is [ 1 0 1 1 1 0 0 0 1 0 1 0 0 ]. In polynomial notation:\nIn order to correct the errors, first calculate the syndromes. Taking formula_234 we have formula_235 formula_236 formula_237 formula_238 formula_239 and formula_240\nNext, apply the Peterson procedure by row-reducing the following augmented matrix.\nDue to the zero row, is singular, which is no surprise since only two errors were introduced into the codeword.\nHowever, the upper-left corner of the matrix is identical to , which gives rise to the solution formula_242 formula_243\nThe resulting error locator polynomial is formula_244 which has zeros at formula_245 and formula_246\nThe exponents of formula_14 correspond to the error locations.\nThere is no need to calculate the error values in this example, as the only possible value is 1.\n\nSuppose the same scenario, but the received word has two unreadable characters [ 1 0 ? 1 1 ? 0 0 1 0 1 0 0 ]. We replace the unreadable characters by zeros while creating the polynom reflecting their positions formula_248 We compute the syndromes formula_249 and formula_250 (Using log notation which is independent on GF(2) isomorphisms. For computation checking we can use the same representation for addition as was used in previous example. Hexadecimal description of the powers of formula_14 are consecutively 1,2,4,8,3,6,C,B,5,A,7,E,F,D,9 with the addition based on bitwise xor.)\n\nLet us make syndrome polynomial\n\ncompute\n\nRun the extended Euclidean algorithm:\n\nWe have reached polynomial of degree at most 3, and as\n\nwe get\n\nTherefore,\n\nLet formula_258 Don't worry that formula_259 Find by brute force a root of formula_185 The roots are formula_261 and formula_262 (after finding for example formula_263 we can divide formula_125 by corresponding monom formula_265 and the root of resulting monom could be found easily).\n\nLet\n\nLet us look for error values using formula\n\nwhere formula_164 are roots of formula_270 formula_271 We get\n\nFact, that formula_273 should not be surprising.\n\nCorrected code is therefore [ 1 0 1 1 0 0 1 0 1 0 0].\n\nLet us show the algorithm behaviour for the case with small number of errors. Let the received word is [ 1 0 ? 1 1 ? 0 0 0 1 0 1 0 0 ].\n\nAgain, replace the unreadable characters by zeros while creating the polynom reflecting their positions formula_248\nCompute the syndromes formula_275 and formula_276\nCreate syndrome polynomial\n\nLet us run the extended Euclidean algorithm:\n\nformula_279\n\nWe have reached polynomial of degree at most 3, and as\nwe get\n\nTherefore,\n\nLet formula_283 Don't worry that formula_259 The root of formula_135 is formula_286\n\nLet \n\nLet us look for error values using formula formula_289 where formula_164 are roots of polynomial formula_270\nformula_292\nWe get\n\nThe fact that formula_294 should not be surprising.\n\nCorrected code is therefore [ 1 0 1 1 0 0 0 1 0 1 0 0].\n\n\n\n"}
{"id": "6837693", "url": "https://en.wikipedia.org/wiki?curid=6837693", "title": "Bass diffusion model", "text": "Bass diffusion model\n\nThe Bass Model or Bass Diffusion Model was developed by Frank Bass. It consists of a simple differential equation that describes the process of how new products get adopted in a population. The model presents a rationale of how current adopters and potential adopters of a new product interact. The basic premise of the model is that adopters can be classified as innovators or as imitators and the speed and timing of adoption depends on their degree of innovativeness and the degree of imitation among adopters. The Bass model has been widely used in forecasting, especially new products' sales forecasting and technology forecasting. Mathematically, the basic Bass diffusion is a Riccati equation with constant coefficients.\n\nIn 1969, Frank Bass published his paper on a new product growth model for consumer durables. Prior to this, Everett Rogers published \"Diffusion of Innovations\", a highly influential work that described the different stages of product adoption. Bass contributed some mathematical ideas to the concept.\n\nWhere:\n\nSales formula_6 is the rate of change of installed base (i.e. adoption) formula_2 multiplied by the ultimate market potential formula_8:\n\nThe time of peak sales formula_11\n\nThe coefficient \"p\" is called the coefficient of innovation, external influence or advertising effect. The coefficient q is called the coefficient of imitation, internal influence or word-of-mouth effect.\n\nTypical values of \"p\" and \"q\" when time \"t\" is measured in years:\n\nBass found that his model fit the data for almost all product introductions, despite a wide range of managerial decision variables, e.g. pricing and advertising. This means that decision variables can shift the Bass curve in time, but that the shape of the curve is always similar.\n\nAlthough many extensions of the model have been proposed, only one of these reduces to the Bass model under ordinary circumstances. This model was developed in 1994 by Frank Bass, Trichy Krishnan and Dipak Jain:\n\nwhere formula_14 is a function of percentage change in price and other variables\n\nTechnology products succeed one another in generations. Norton and Bass extended the model in 1987 for sales of products with continuous repeat purchasing. The formulation for three generations is as follows: \n\nwhere \n\nIt has been found that the p and q terms are generally the same between successive generations.\n\nThere are two special cases of the Bass diffusion model.\n\n\nThe Bass model is a special case of the Gamma/shifted Gompertz distribution (G/SG): Bemmaor (1994) \n\nUse in online social networks\nThe rapid, recent (as of early 2007) growth in online social networks (and other virtual communities) has led to an increased use of the Bass diffusion model. The Bass diffusion model is used to estimate the size and growth rate of these social networks. The work by Christian Bauchkage and co-authors shows that the Bass model provides a more pessimistic picture of the future than alternative model(s) such as the Weibull distribution and the shifted Gompertz distribution.\n\nThe model is one of the most cited empirical generalizations in marketing; as of October 2018 the paper \"A New Product Growth for Model Consumer Durables\" published in Management Science had (approximately) 8499 citations in Google Scholar. \n\nThis model has been widely influential in marketing and management science. In 2004 it was selected as one of the ten most frequently cited papers in the 50-year history of \"Management Science\". It was ranked number five, and the only marketing paper in the list. It was subsequently reprinted in the December 2004 issue of Management Science.\n\n\n"}
{"id": "58580000", "url": "https://en.wikipedia.org/wiki?curid=58580000", "title": "Bernoulli polynomials of the second kind", "text": "Bernoulli polynomials of the second kind\n\nThe Bernoulli polynomials of the second kind , also known as the Fontana-Bessel polynomials, are the polynomials defined by the following generating function:\n\nThe first five polynomials are:\n\nSome authors define these polynomials slightly differently\n\nso that \n\nand may also use a different notation for them (the most used alternative notation is ).\n\nThe Bernoulli polynomials of the second kind were largely studied by the Hungarian mathematician Charles Jordan, but their history may also be traced back to the much earlier works.\n\nThe Bernoulli polynomials of the second kind may be represented via these integrals\n\nas well as \n\nThese polynomials are, therefore, up to a constant, the antiderivative of the binomial coefficient and also that of the falling factorial.\n\nFor an arbitrary , these polynomials may be computed explicitly via the following summation formula\n\nwhere where are the signed Stirling numbers of the first kind and are the Gregory coefficients.\n\nThe Bernoulli polynomials of the second kind satisfy the recurrence relation\n\nor equivalently \n\nThe repeated difference produces\n\nThe main property of the symmetry reads\n\nSome properties and particular values of these polynomials include \n\nwhere are the \"Cauchy numbers of the second kind\" and are the \"central difference coefficients\".\n\nThe expansion of the Bernoulli polynomials of the second kind into a Newton series reads\n\nThe digamma function may be expanded into a series with the Bernoulli polynomials of the second kind\nin the following way\n\nand hence \n\nformula_15\n\nand \n\nwhere is Euler's constant. Furthermore, we also have\n\nwhere is the gamma function. The Hurwitz and Riemann zeta functions may be expanded into these\npolynomials as follows\n\nand\n\nand also\n\nThe Bernoulli polynomials of the second kind are also involved in the following relationship\n\nbetween the zeta functions, as well as in various formulas for the Stieltjes constants, e.g.\n\nand \n\nwhich are both valid for formula_24 and formula_25.\n\n"}
{"id": "21241712", "url": "https://en.wikipedia.org/wiki?curid=21241712", "title": "Bipartite double cover", "text": "Bipartite double cover\n\nIn graph theory, the bipartite double cover of an undirected graph \"G\" is a bipartite covering graph of \"G\", with twice as many vertices as \"G\". It can be constructed as the tensor product of graphs, \"G\" × \"K\". It is also called the Kronecker double cover, canonical double cover or simply the bipartite double of \"G\".\n\nIt should not be confused with a cycle double cover of a graph, a family of cycles that includes each edge twice.\n\nThe bipartite double cover of \"G\" has two vertices \"u\" and \"w\" for each vertex \"v\" of \"G\". Two vertices \"u\" and \"w\" are connected by an edge in the double cover if and only if \"v\" and \"v\" are connected by an edge in \"G\". For instance, below is an illustration of a bipartite double cover of a non-bipartite graph \"G\". In the illustration, each vertex in the tensor product is shown using a color from the first term of the product (\"G\") and a shape from the second term of the product (\"K\"); therefore, the vertices \"u\" in the double cover are shown as circles while the vertices \"w\" are shown as squares.\n\nThe bipartite double cover may also be constructed using adjacency matrices (as described below) or as the derived graph of a voltage graph in which each edge of \"G\" is labeled by the nonzero element of the two-element group.\n\nThe bipartite double cover of the Petersen graph is the Desargues graph: \"K\" × \"G\"(5,2) = \"G\"(10,3).\n\nThe bipartite double cover of a complete graph \"K\" is a crown graph (a complete bipartite graph \"K\" minus a perfect matching). In particular, the bipartite double cover of the graph of a tetrahedron, \"K\", is the graph of a cube.\n\nThe bipartite double cover of an odd-length cycle graph is a cycle of twice the length, while the bipartite double of any bipartite graph (such as an even length cycle, shown in the following example) is formed by two disjoint copies of the original graph.\n\nIf an undirected graph \"G\" has a matrix \"A\" as its adjacency matrix, then the adjacency matrix of the double cover of \"G\" is\nand the biadjacency matrix of the double cover of \"G\" is just \"A\" itself. That is, the conversion from a graph to its double cover can be performed simply by reinterpreting \"A\" as a biadjacency matrix instead of as an adjacency matrix. More generally, the reinterpretation the adjacency matrices of directed graphs as biadjacency matrices provides a combinatorial equivalence between directed graphs and balanced bipartite graphs.\n\nThe bipartite double cover of any graph \"G\" is a bipartite graph; both parts of the bipartite graph have one vertex for each vertex of \"G\". A bipartite double cover is connected if and only if \"G\" is connected and non-bipartite.\n\nThe bipartite double cover is a special case of a \"double cover\" (a 2-fold covering graph). A double cover in graph theory can be viewed as a special case of a topological double cover.\n\nIf \"G\" is a non-bipartite symmetric graph, the double cover of \"G\" is also a symmetric graph; several known cubic symmetric graphs may be obtained in this way. For instance, the double cover of \"K\" is the graph of a cube; the double cover of the Petersen graph is the Desargues graph; and the double cover of the graph of the dodecahedron is a 40-vertex symmetric cubic graph.\n\nIt is possible for two different graphs to have isomorphic bipartite double covers. For instance, the Desargues graph is not only the bipartite double cover of the Petersen graph, but is also the bipartite double cover of a different graph that is not isomorphic to the Petersen graph. Not every bipartite graph is a bipartite double cover of another graph; for a bipartite graph \"G\" to be the bipartite cover of another graph, it is necessary and sufficient that the automorphisms of \"G\" include an involution that maps each vertex to a distinct and non-adjacent vertex. For instance, the graph with two vertices and one edge is bipartite but is not a bipartite double cover, because it has no non-adjacent pairs of vertices to be mapped to each other by such an involution; on the other hand, the graph of the cube is a bipartite double cover, and has an involution that maps each vertex to the diametrally opposite vertex. An alternative characterization of the bipartite graphs that may be formed by the bipartite double cover construction was obtained by .\n\nIn general, a graph may have multiple double covers that are different from the bipartite double cover. In the following figure, the graph \"C\" is a double cover of the graph \"H\":\nHowever, \"C\" is not a \"bipartite\" double cover of \"H\" or any other graph; it is not a bipartite graph. \n\nIf we replace one triangle by a square in \"H \"the resulting graph has four distinct double covers. Two of them are bipartite but only one of them is the Kronecker cover.\n\nAs another example, the graph of the icosahedron is a double cover of the complete graph \"K\"; to obtain a covering map from the icosahedron to \"K\", map each pair of opposite vertices of the icosahedron to a single vertex of \"K\". However, the icosahedron is not bipartite, so it is not the bipartite double cover of \"K\". Instead, it can be obtained as the orientable double cover of an embedding of \"K\" on the projective plane.\n\n\n"}
{"id": "20489385", "url": "https://en.wikipedia.org/wiki?curid=20489385", "title": "Birkhoff's representation theorem", "text": "Birkhoff's representation theorem\n\nIn mathematics, Birkhoff's representation theorem for distributive lattices states that the elements of any finite distributive lattice can be represented as finite sets, in such a way that the lattice operations correspond to unions and intersections of sets. The theorem can be interpreted as providing a one-to-one correspondence between distributive lattices and partial orders, between quasi-ordinal knowledge spaces and preorders, or between finite topological spaces and preorders. It is named after Garrett Birkhoff, who published a proof of it in 1937.\n\nThe name “Birkhoff's representation theorem” has also been applied to two other results of Birkhoff, one from 1935 on the representation of Boolean algebras as families of sets closed under union, intersection, and complement (so-called \"fields of sets\", closely related to the \"rings of sets\" used by Birkhoff to represent distributive lattices), and Birkhoff's HSP theorem representing algebras as products of irreducible algebras. Birkhoff's representation theorem has also been called the fundamental theorem for finite distributive lattices.\n\nMany lattices can be defined in such a way that the elements of the lattice are represented by sets, the join operation of the lattice is represented by set union, and the meet operation of the lattice is represented by set intersection. For instance, the Boolean lattice defined from the family of all subsets of a finite set has this property. More generally any finite topological space has a lattice of sets as its family of open sets. Because set unions and intersections obey the distributive law, any lattice defined in this way is a distributive lattice. Birkhoff's theorem states that in fact \"all\" finite distributive lattices can be obtained this way, and later generalizations of Birkhoff's theorem state a similar thing for infinite distributive lattices.\n\nConsider the divisors of some composite number, such as (in the figure) 120, partially ordered by divisibility. Any two divisors of 120, such as 12 and 20, have a unique greatest common factor 12 ∧ 20 = 4, the largest number that divides both of them, and a unique least common multiple 12 ∨ 20 = 60; both of these numbers are also divisors of 120. These two operations ∨ and ∧ satisfy the distributive law, in either of two equivalent forms: (\"x\" ∧ \"y\") ∨ \"z\" = (\"x\" ∨ \"z\") ∧ (\"y\" ∨ \"z\") and (\"x\" ∨ \"y\") ∧ \"z\" = (\"x\" ∧ \"z\") ∨ (\"y\" ∧ \"z\"), for all \"x\", \"y\", and \"z\". Therefore, the divisors form a finite distributive lattice.\n\nOne may associate each divisor with the set of prime powers that divide it: thus, 12 is associated with the set {2,3,4}, while 20 is associated with the set {2,4,5}. Then 12 ∧ 20 = 4 is associated with the set {2,3,4} ∩ {2,4,5} = {2,4}, while 12 ∨ 20 = 60 is associated with the set {2,3,4} ∪ {2,4,5} = {2,3,4,5}, so the join and meet operations of the lattice correspond to union and intersection of sets. \n\nThe prime powers 2, 3, 4, 5, and 8 appearing as elements in these sets may themselves be partially ordered by divisibility; in this smaller partial order, 2 ≤ 4 ≤ 8 and there are no order relations between other pairs. The 16 sets that are associated with divisors of 120 are the lower sets of this smaller partial order, subsets of elements such that if \"x\" ≤ \"y\" and \"y\" belongs to the subset, then \"x\" must also belong to the subset. From any lower set \"L\", one can recover the associated divisor by computing the least common multiple of the prime powers in \"L\". Thus, the partial order on the five prime powers 2, 3, 4, 5, and 8 carries enough information to recover the entire original 16-element divisibility lattice.\n\nBirkhoff's theorem states that this relation between the operations ∧ and ∨ of the lattice of divisors and the operations ∩ and ∪ of the associated sets of prime powers is not coincidental, and not dependent on the specific properties of prime numbers and divisibility: the elements of any finite distributive lattice may be associated with lower sets of a partial order in the same way.\n\nAs another example, the application of Birkhoff's theorem to the family of subsets of an \"n\"-element set, partially ordered by inclusion, produces the free distributive lattice with \"n\" generators. The number of elements in this lattice is given by the Dedekind numbers.\n\nIn a lattice, an element \"x\" is \"join-irreducible\" if \"x\" is not the join of a finite set of other elements. Equivalently, \"x\" is join-irreducible if it is neither the bottom element of the lattice (the join of zero elements) nor the join of any two smaller elements. For instance, in the lattice of divisors of 120, there is no pair of elements whose join is 4, so 4 is join-irreducible. An element \"x\" is \"join-prime\" if, whenever \"x\" ≤ \"y\" ∨ \"z\", either \"x\" ≤ \"y\" or \"x\" ≤ \"z\". In the same lattice, 4 is join-prime: whenever lcm(\"y\",\"z\") is divisible by 4, at least one of \"y\" and \"z\" must itself be divisible by 4.\n\nIn any lattice, a join-prime element must be join-irreducible. Equivalently, an element that is not join-irreducible is not join-prime. For, if an element \"x\" is not join-irreducible, there exist smaller \"y\" and \"z\" such that \"x\" = \"y\" ∨ \"z\". But then \"x\" ≤ \"y\" ∨ \"z\", and \"x\" is not less than or equal to either \"y\" or \"z\", showing that it is not join-prime.\n\nThere exist lattices in which the join-prime elements form a proper subset of the join-irreducible elements, but in a distributive lattice the two types of elements coincide. For, suppose that \"x\" is join-irreducible, and that \"x\" ≤ \"y\" ∨ \"z\". This inequality is equivalent to the statement that \"x\" = \"x\" ∧ (\"y\" ∨ \"z\"), and by the distributive law \"x\" = (\"x\" ∧ \"y\") ∨ (\"x\" ∧ \"z\"). But since \"x\" is join-irreducible, at least one of the two terms in this join must be \"x\" itself, showing that either \"x\" = \"x\" ∧ \"y\" (equivalently \"x\" ≤ \"y\") or \"x\" = \"x\" ∧ \"z\" (equivalently \"x\" ≤ \"z\").\n\nThe lattice ordering on the subset of join-irreducible elements forms a partial order; Birkhoff's theorem states that the lattice itself can be recovered from the lower sets of this partial order.\n\nIn any partial order, the lower sets form a lattice in which the lattice's partial ordering is given by set inclusion, the join operation corresponds to set union, and the meet operation corresponds to set intersection, because unions and intersections preserve the property of being a lower set. Because set unions and intersections obey the distributive law, this is a distributive lattice. Birkhoff's theorem states that any finite distributive lattice can be constructed in this way.\n\nThat is, there is a one-to-one order-preserving correspondence between elements of \"L\" and lower sets of the partial order. The lower set corresponding to an element \"x\" of \"L\" is simply the set of join-irreducible elements of \"L\" that are less than or equal to \"x\", and the element of \"L\" corresponding to a lower set \"S\" of join-irreducible elements is the join of \"S\".\n\nFor any lower set \"S\" of join-irreducible elements, let \"x\" be the join of \"S\", and let \"T\" be the lower set of the join-irreducible elements less than or equal to \"x\". Then \"S\" = \"T\". For, every element of \"S\" clearly belongs to \"T\", and any join-irreducible element less than or equal to \"x\" must (by join-primality) be less than or equal to one of the members of \"S\", and therefore must (by the assumption that \"S\" is a lower set) belong to \"S\" itself. Conversely, for any element \"x\" of \"L\", let \"S\" be the join-irreducible elements less than or equal to \"x\", and let \"y\" be the join of \"S\". Then \"x\" = \"y\". For, as a join of elements less than or equal to \"x\", \"y\" can be no greater than \"x\" itself, but if \"x\" is join-irreducible then \"x\" belongs to \"S\" while if \"x\" is the join of two or more join-irreducible items then they must again belong to \"S\", so \"y\" ≥ \"x\". Therefore, the correspondence is one-to-one and the theorem is proved.\n\n defined a \"ring of sets\" to be a family of sets that is closed under the operations of set unions and set intersections; later, motivated by applications in mathematical psychology, called the same structure a \"quasi-ordinal knowledge space\". If the sets in a ring of sets are ordered by inclusion, they form a distributive lattice. The elements of the sets may be given a preorder in which \"x\" ≤ \"y\" whenever some set in the ring contains \"x\" but not \"y\". The ring of sets itself is then the family of lower sets of this preorder, and any preorder gives rise to a ring of sets in this way.\n\nBirkhoff's theorem, as stated above, is a correspondence between individual partial orders and distributive lattices. However, it can also be extended to a correspondence between order-preserving functions of partial orders and bounded homomorphisms of the corresponding distributive lattices. The direction of these maps is reversed in this correspondence.\n\nLet 2 denote the partial order on the two-element set {0, 1}, with the order relation 0 < 1, and (following Stanley) let \"J(P)\" denote the distributive lattice of lower sets of a finite partial order \"P\". Then the elements of \"J(P)\" correspond one-for-one to the order-preserving functions from \"P\" to 2. For, if ƒ is such a function, ƒ(0) forms a lower set, and conversely if \"L\" is a lower set one may define an order-preserving function ƒ that maps \"L\" to 0 and that maps the remaining elements of \"P\" to 1. If \"g\" is any order-preserving function from \"Q\" to \"P\", one may define a function \"g\"* from \"J(P)\" to \"J(Q)\" that uses the composition of functions to map any element \"L\" of \"J(P)\" to ƒ ∘ \"g\". This composite function maps \"Q\" to 2 and therefore corresponds to an element \"g\"*(\"L\") = (ƒ ∘ \"g\")(0) of \"J(Q)\". Further, for any \"x\" and \"y\" in \"J(P)\", \"g\"*(\"x\" ∧ \"y\") = \"g\"*(\"x\") ∧ \"g\"*(\"y\") (an element of \"Q\" is mapped by \"g\" to the lower set \"x\" ∩ \"y\" if and only if belongs both to the set of elements mapped to \"x\" and the set of elements mapped to \"y\") and symmetrically \"g\"*(\"x\" ∨ \"y\") = \"g\"*(\"x\") ∨ \"g\"*(\"y\"). Additionally, the bottom element of \"J(P)\" (the function that maps all elements of \"P\" to 0) is mapped by \"g\"* to the bottom element of \"J(Q)\", and the top element of \"J(P)\" is mapped by \"g\"* to the top element of \"J(Q)\". That is, \"g\"* is a homomorphism of bounded lattices.\n\nHowever, the elements of \"P\" themselves correspond one-for-one with bounded lattice homomorphisms from \"J(P)\" to 2. For, if \"x\" is any element of \"P\", one may define a bounded lattice homomorphism \"j\" that maps all lower sets containing \"x\" to 1 and all other lower sets to 0. And, for any lattice homomorphism from \"J(P)\" to 2, the elements of \"J(P)\" that are mapped to 1 must have a unique minimal element \"x\" (the meet of all elements mapped to 1), which must be join-irreducible (it cannot be the join of any set of elements mapped to 0), so every lattice homomorphism has the form \"j\" for some \"x\". Again, from any bounded lattice homomorphism \"h\" from \"J(P)\" to \"J(Q)\" one may use composition of functions to define an order-preserving map \"h\"* from \"Q\" to \"P\". It may be verified that \"g\"** = \"g\" for any order-preserving map \"g\" from \"Q\" to \"P\" and that and \"h\"** = \"h\" for any bounded lattice homomorphism \"h\" from \"J(P)\" to \"J(Q)\".\n\nIn category theoretic terminology, \"J\" is a contravariant hom-functor \"J\" = Hom(—,2) that defines a duality of categories between, on the one hand, the category of finite partial orders and order-preserving maps, and on the other hand the category of finite distributive lattices and bounded lattice homomorphisms.\n\nIn an infinite distributive lattice, it may not be the case that the lower sets of the join-irreducible elements are in one-to-one correspondence with lattice elements. Indeed, there may be no join-irreducibles at all. This happens, for instance, in the lattice of all natural numbers, ordered with the reverse of the usual divisibility ordering (so \"x\" ≤ \"y\" when \"y\" divides \"x\"): any number \"x\" can be expressed as the join of numbers \"xp\" and \"xq\" where \"p\" and \"q\" are distinct prime numbers. However, elements in infinite distributive lattices may still be represented as sets via Stone's representation theorem for distributive lattices, a form of Stone duality in which each lattice element corresponds to a compact open set in a certain topological space. This generalized representation theorem can be expressed as a category-theoretic duality between distributive lattices and spectral spaces (sometimes called coherent spaces, but not the same as the coherent spaces in linear logic), topological spaces in which the compact open sets are closed under intersection and form a base for the topology. Hilary Priestley showed that Stone's representation theorem could be interpreted as an extension of the idea of representing lattice elements by lower sets of a partial order, using Nachbin's idea of ordered topological spaces. Stone spaces with an additional partial order linked with the topology via Priestley separation axiom can also be used to represent bounded distributive lattices. Such spaces are known as Priestley spaces. Further, certain bitopological spaces, namely pairwise Stone spaces, generalize Stone's original approach by utilizing \"two\" topologies on a set to represent an abstract distributive lattice. Thus, Birkhoff's representation theorem extends to the case of infinite (bounded) distributive lattices in at least three different ways, summed up in duality theory for distributive lattices.\n\nBirkhoff's representation theorem may also be generalized to finite structures other than distributive lattices. In a distributive lattice, the self-dual median operation\ngives rise to a median algebra, and the covering relation of the lattice forms a median graph. Finite median algebras and median graphs have a dual structure\nas the set of solutions of a 2-satisfiability instance; formulate this structure equivalently as the family of initial stable sets in a mixed graph. For a distributive lattice, the corresponding mixed graph has no undirected edges, and the initial stable sets are just the lower sets of the transitive closure of the graph. Equivalently, for a distributive lattice, the implication graph of the 2-satisfiability instance can be partitioned into two connected components, one on the positive variables of the instance and the other on the negative variables; the transitive closure of the positive component is the underlying partial order of the distributive lattice.\n\nAnother result analogous to Birkhoff's representation theorem, but applying to a broader class of lattices, is the theorem of that any finite join-distributive lattice may be represented as an antimatroid, a family of sets closed under unions but in which closure under intersections has been replaced by the property that each nonempty set has a removable element.\n\n"}
{"id": "5584076", "url": "https://en.wikipedia.org/wiki?curid=5584076", "title": "Cantamath", "text": "Cantamath\n\nCantamath is a mathematics competition competed in Christchurch, Canterbury, New Zealand by years 6 to 10 students. \n\nThere are two sections, the Competition section and the Project section. \n\nThe sponsors of Cantamath are Casio, Trimble Navigation, Every Educaid, Mathletics and University of Canterbury. \n\nIn the Team Competition section, each participating school sends in four selected student mathematicians per year level. The participants compete against other schools in the Christchurch Horncastle Arena. It's a speed competition and takes 30 minutes. There are 20 questions for each team to complete, the aim being for each team to answer all questions the fastest. One of the four team members is a runner who runs to a judge to check if the answer to their current question is right. Each question is worth 5 points, allowing a maximum score of 100. A team can only attempt one question at a time and have to keep working on it until they get it right. Passing is allowed, but no points will be received for that question, as well as preventing the team from returning to that question. \n\nThe winning team gets a badge and a prize from Casio. \n\nIn the Project section, the student submits a project on a certain topic. Projects can be awarded with an Excellence or Highly Commended award, depending on their quality. There is also an Outstanding award for the best few projects in the display section. \n\nThe categories include:\nVaries other exist, and may be found at the Cantamath site. \n\n"}
{"id": "38128", "url": "https://en.wikipedia.org/wiki?curid=38128", "title": "Cauchy–Schwarz inequality", "text": "Cauchy–Schwarz inequality\n\nIn mathematics, the Cauchy–Schwarz inequality, also known as the Cauchy–Bunyakovsky–Schwarz inequality, is a useful inequality encountered in many different settings, such as linear algebra, analysis, probability theory, vector algebra and other areas. It is considered to be one of the most important inequalities in all of mathematics.\n\nThe inequality for sums was published by , while the corresponding inequality for integrals was first proved by\n. The modern proof of the integral inequality was given by .\n\nThe Cauchy–Schwarz inequality states that for all vectors formula_1 and formula_2 of an inner product space it is true that\nwhere formula_4 is the inner product. Examples of inner products include the real and complex dot product, see the examples in inner product. Equivalently, by taking the square root of both sides, and referring to the norms of the vectors, the inequality is written as\nMoreover, the two sides are equal if and only if formula_6 and formula_7 are linearly dependent (meaning they are parallel: one of the vector's magnitudes is zero, or one is a scalar multiple of the other).\n\nIf formula_8 and formula_9, and the inner product is the standard complex inner product, then the inequality may be restated more explicitly as follows (where the bar notation is used for complex conjugation): \nor \n\nLet formula_1 and formula_2 be arbitrary vectors in a vector space over formula_14 with an inner product, where formula_14 is the field of real or complex numbers. We prove the inequality\nand that equality holds if and only if either formula_1 or formula_2 is a multiple of the other (which includes the special case that either is the zero vector).\n\nIf formula_19, it is clear that we have equality, and in this case formula_1 and formula_2 are also linearly dependent, regardless of formula_1, so the theorem is true. Similarly if formula_23. We henceforth assume that formula_2 is nonzero.\n\nLet\nThen, by linearity of the inner product in its first argument, one has\nTherefore, formula_27 is a vector orthogonal to the vector formula_2 (Indeed, formula_27 is the projection of formula_1 onto the plane orthogonal to formula_2 .) We can thus apply the Pythagorean theorem to\nwhich gives\nThe Cauchy–Schwarz inequality proves that this definition is sensible, by showing that the right-hand side lies in the interval [−1, 1] and justifies the notion that (real) Hilbert spaces are simply generalizations of the Euclidean space. It can also be used to define an angle in complex inner-product spaces, by taking the absolute value or the real part of the right-hand side, as is done when extracting a metric from quantum fidelity.\n\nLet \"X\", \"Y\" be random variables, then the covariance inequality is given by\nAfter defining an inner product on the set of random variables using the expectation of their product,\nthen the Cauchy–Schwarz inequality becomes\nTo prove the covariance inequality using the Cauchy–Schwarz inequality, let formula_37 and formula_38, then\n\nwhere formula_40 denotes variance, and formula_41 denotes covariance.\n\nVarious generalizations of the Cauchy–Schwarz inequality exist in the context of operator theory, e.g. for operator-convex functions and operator algebras, where the domain and/or range are replaced by a C*-algebra or W*-algebra.\n\nAn inner product can be used to define a positive linear functional. For example, given a Hilbert space formula_42 being a finite measure, the standard inner product gives rise to a positive functional formula_43 by formula_44. Conversely, every positive linear functional formula_43 on formula_46 can be used to define an inner product formula_47, where formula_48 is the pointwise complex conjugate of formula_49. In this language, the Cauchy–Schwarz inequality becomes\nwhich extends verbatim to positive functionals on C*-algebras:\n\nTheorem (Cauchy–Schwarz inequality for positive functionals on C*-algebras): If formula_43 is a positive linear functional on a C*-algebra formula_52 then for all formula_53, formula_54.\n\nThe next two theorems are further examples in operator algebra.\n\nTheorem (Kadison–Schwarz inequality, named after Richard Kadison): If formula_43 is a unital positive map, then for every normal element formula_56 in its domain, we have formula_57 and formula_58.\n\nThis extends the fact formula_59, when formula_43 is a linear functional. The case when formula_56 is self-adjoint, i.e. formula_62 is sometimes known as Kadison's inequality.\n\nTheorem (Modified Schwarz inequality for 2-positive maps): For a 2-positive map formula_43 between C*-algebras, for all formula_64 in its domain,\nAnother generalization is a refinement obtained by interpolating between both sides the Cauchy-Schwarz inequality: \n\nTheorem (Callebaut's Inequality) \nFor reals formula_67, \n\nIt can be easily proven by Hölder's inequality. There are also non commutative versions for operators and tensor products of matrices.\n\n\n"}
{"id": "1251559", "url": "https://en.wikipedia.org/wiki?curid=1251559", "title": "Club set", "text": "Club set\n\nIn mathematics, particularly in mathematical logic and set theory, a club set is a subset of a limit ordinal which is closed under the order topology, and is unbounded (see below) relative to the limit ordinal. The name \"club\" is a contraction of \"closed and unbounded\".\n\nFormally, if formula_1 is a limit ordinal, then a set formula_2 is \"closed\" in formula_1 if and only if for every formula_4, if formula_5, then formula_6. Thus, if the limit of some sequence from formula_7 is less than formula_1, then the limit is also in formula_7.\n\nIf formula_1 is a limit ordinal and formula_2 then formula_7 is unbounded in formula_1 if for any formula_4, there is some formula_15 such that formula_16.\n\nIf a set is both closed and unbounded, then it is a club set. Closed proper classes are also of interest (every proper class of ordinals is unbounded in the class of all ordinals).\n\nFor example, the set of all countable limit ordinals is a club set with respect to the first uncountable ordinal; but it is not a club set with respect to any higher limit ordinal, since it is neither closed nor unbounded.\nThe set of all limit ordinals formula_4 is closed unbounded in formula_18 (formula_18 regular). In fact a club set is nothing else but the range of a normal function (i.e. increasing and continuous).\n\nMore generally, if formula_20 is a nonempty set and formula_21 is a cardinal, then formula_22 is \"club\" if every union of a subset of formula_7 is in formula_7 and every subset of formula_20 of cardinality less than formula_21 is contained in some element of formula_7 (see stationary set).\n\nLet formula_28 be a limit ordinal of uncountable cofinality formula_29 For some formula_30, let formula_31 be a sequence of closed unbounded subsets of formula_32 Then formula_33 is also closed unbounded. To see this, one can note that an intersection of closed sets is always closed, so we just need to show that this intersection is unbounded. So fix any formula_34 and for each \"n\"<ω choose from each formula_35 an element formula_36 which is possible because each is unbounded. Since this is a collection of fewer than formula_37 ordinals, all less than formula_38 their least upper bound must also be less than formula_38 so we can call it formula_40 This process generates a countable sequence formula_41 The limit of this sequence must in fact also be the limit of the sequence formula_42 and since each formula_35 is closed and formula_37 is uncountable, this limit must be in each formula_45 and therefore this limit is an element of the intersection that is above formula_46 which shows that the intersection is unbounded. QED.\n\nFrom this, it can be seen that if formula_28 is a regular cardinal, then formula_48 is a non-principal formula_28-complete filter on formula_32\n\nIf formula_28 is a regular cardinal then club sets are also closed under diagonal intersection.\n\nIn fact, if formula_28 is regular and formula_53 is any filter on formula_38 closed under diagonal intersection, containing all sets of the form formula_55 for formula_56 then formula_53 must include all club sets.\n\n\n"}
{"id": "1338683", "url": "https://en.wikipedia.org/wiki?curid=1338683", "title": "Corecursion", "text": "Corecursion\n\nIn computer science, corecursion is a type of operation that is dual to recursion. Whereas recursion works analytically, starting on data further from a base case and breaking it down into smaller data and repeating until one reaches a base case, corecursion works synthetically, starting from a base case and building it up, iteratively producing data further removed from a base case. Put simply, corecursive algorithms use the data that they themselves produce, bit by bit, as they become available, and needed, to produce further bits of data. A similar but distinct concept is \"generative recursion\" which may lack a definite \"direction\" inherent in corecursion and recursion.\n\nWhere recursion allows programs to operate on arbitrarily complex data, so long as they can be reduced to simple data (base cases), corecursion allows programs to produce arbitrarily complex and potentially infinite data structures, such as streams, so long as it can be produced from simple data (base cases) in a sequence of \"finite\" steps. Where recursion may not terminate, never reaching a base state, corecursion starts from a base state, and thus produces subsequent steps deterministically, though it may proceed indefinitely (and thus not terminate under strict evaluation), or it may consume more than it produces and thus become non-\"productive\". Many functions that are traditionally analyzed as recursive can alternatively, and arguably more naturally, be interpreted as corecursive functions that are terminated at a given stage, for example recurrence relations such as the factorial.\n\nCorecursion can produce both finite and infinite data structures as results, and may employ self-referential data structures. Corecursion is often used in conjunction with lazy evaluation, to produce only a finite subset of a potentially infinite structure (rather than trying to produce an entire infinite structure at once). Corecursion is a particularly important concept in functional programming, where corecursion and codata allow total languages to work with infinite data structures.\n\nCorecursion can be understood by contrast with recursion, which is more familiar. While corecursion is primarily of interest in functional programming, it can be illustrated using imperative programming, which is done below using the generator facility in Python. In these examples local variables are used, and assigned values imperatively (destructively), though these are not necessary in corecursion in pure functional programming. In pure functional programming, rather than assigning to local variables, these computed values form an invariable sequence, and prior values are accessed by self-reference (later values in the sequence reference earlier values in the sequence to be computed). The assignments simply express this in the imperative paradigm and explicitly specify where the computations happen, which serves to clarify the exposition.\n\nA classic example of recursion is computing the factorial, which is defined recursively by \"0! := 1\" and \"n! := n × (n - 1)!\".\n\nTo \"recursively\" compute its result on a given input, a recursive function calls (a copy of) \"itself\" with a different (\"smaller\" in some way) input and uses the result of this call to construct its result. The recursive call does the same, unless the \"base case\" has been reached. Thus a call stack develops in the process. For example, to compute \"fac(3)\", this recursively calls in turn \"fac(2)\", \"fac(1)\", \"fac(0)\" (\"winding up\" the stack), at which point recursion terminates with \"fac(0) = 1\", and then the stack unwinds in reverse order and the results are calculated on the way back along the call stack to the initial call frame \"fac(3)\" that uses the result of \"fac(2) = 2\" to calculate the final result as \"3 × 2 = 3 × fac(2) =: fac(3)\" and finally return \"fac(3) = 6\". In this example a function returns a single value.\n\nThis stack unwinding can be explicated, defining the factorial \"corecursively\", as an iterator, where one \"starts\" with the case of formula_1, then from this starting value constructs factorial values for increasing numbers \"1, 2, 3...\" as in the above recursive definition with \"time arrow\" reversed, as it were, by reading it \"backwards\" as The corecursive algorithm thus defined produces a \"stream\" of \"all\" factorials. This may be concretely implemented as a generator. Symbolically, noting that computing next factorial value requires keeping track of both \"n\" and \"f\" (a previous factorial value), this can be represented as:\nor in Haskell, \n\nmeaning, \"starting from formula_3, on each step the next values are calculated as formula_4\". This is mathematically equivalent and almost identical to the recursive definition, but the formula_5 emphasizes that the factorial values are being built \"up\", going forwards from the starting case, rather than being computed after first going backwards, \"down\" to the base case, with a formula_6 decrement. Note also that the direct output of the corecursive function does not simply contain the factorial formula_7 values, but also includes for each value the auxiliary data of its index \"n\" in the sequence, so that any one specific result can be selected among them all, as and when needed.\n\nNote the connection with denotational semantics, where the denotations of recursive programs is built up corecursively in this way.\n\nIn Python, a recursive factorial function can be defined as:\n\nThis could then be called for example as codice_1 to compute \"5!\".\n\nA corresponding corecursive generator can be defined as:\n\nThis generates an infinite stream of factorials in order; a finite portion of it can be produced by:\n\nThis could then be called to produce the factorials up to \"5!\" via:\n\nIf we're only interested in a certain factorial, just the last value can be taken, or we can fuse the production and the access into one function,\n\nAs can be readily seen here, this is practically equivalent (just by substituting codice_2 for the only codice_3 there) to the accumulator argument technique for tail recursion, unwound into an explicit loop. Thus it can be said that the concept of corecursion is an explication of the embodiment of iterative computation processes by recursive definitions, where applicable.\n\nIn the same way, the Fibonacci sequence can be represented as:\nNote that because the Fibonacci sequence is a recurrence relation of order 2, the corecursive relation must track two successive terms, with the formula_9 corresponding to shift forward by one step, and the formula_10 corresponding to computing the next term. This can then be implemented as follows (using parallel assignment):\n\nIn Haskell, \n\nTree traversal via a depth-first approach is a classic example of recursion. Dually, breadth-first traversal can very naturally be implemented via corecursion.\n\nWithout using recursion or corecursion specifically, one may traverse a tree by starting at the root node, placing its child nodes in a data structure, then iterating by removing node after node from the data structure while placing each removed node's children back into that data structure. If the data structure is a stack (LIFO), this yields depth-first traversal, and if the data structure is a queue (FIFO), this yields breadth-first traversal.\n\nUsing recursion, a (post-order) depth-first traversal can be implemented by starting at the root node and recursively traversing each child subtree in turn (the subtree based at each child node) – the second child subtree does not start processing until the first child subtree is finished. Once a leaf node is reached or the children of a branch node have been exhausted, the node itself is visited (e.g., the value of the node itself is outputted). In this case, the call stack (of the recursive functions) acts as the stack that is iterated over.\n\nUsing corecursion, a breadth-first traversal can be implemented by starting at the root node, outputting its value, then breadth-first traversing the subtrees – i.e., passing on the \"whole list\" of subtrees to the next step (not a single subtree, as in the recursive approach) – at the next step outputting the value of all of their root nodes, then passing on their child subtrees, etc. In this case the generator function, indeed the output sequence itself, acts as the queue. As in the factorial example (above), where the auxiliary information of the index (which step one was at, \"n\") was pushed forward, in addition to the actual output of \"n\"!, in this case the auxiliary information of the remaining subtrees is pushed forward, in addition to the actual output. Symbolically:\nmeaning that at each step, one outputs the list of values of root nodes, then proceeds to the child subtrees. Generating just the node values from this sequence simply requires discarding the auxiliary child tree data, then flattening the list of lists (values are initially grouped by level (depth); flattening (ungrouping) yields a flat linear list). In Haskell, \nThese can be compared as follows. The recursive traversal handles a \"leaf node\" (at the \"bottom\") as the base case (when there are no children, just output the value), and \"analyzes\" a tree into subtrees, traversing each in turn, eventually resulting in just leaf nodes – actual leaf nodes, and branch nodes whose children have already been dealt with (cut off \"below\"). By contrast, the corecursive traversal handles a \"root node\" (at the \"top\") as the base case (given a node, first output the value), treats a tree as being \"synthesized\" of a root node and its children, then produces as auxiliary output a list of subtrees at each step, which are then the input for the next step – the child nodes of the original root are the root nodes at the next step, as their parents have already been dealt with (cut off \"above\"). Note also that in the recursive traversal there is a distinction between leaf nodes and branch nodes, while in the corecursive traversal there is no distinction, as each node is treated as the root node of the subtree it defines.\n\nNotably, given an infinite tree, the corecursive breadth-first traversal will traverse all nodes, just as for a finite tree, while the recursive depth-first traversal will go down one branch and not traverse all nodes, and indeed if traversing post-order, as in this example (or in-order), it will visit no nodes at all, because it never reaches a leaf. This shows the usefulness of corecursion rather than recursion for dealing with infinite data structures.\n\nIn Python, this can be implemented as follows.\nThe usual post-order depth-first traversal can be defined as:\n\nThis can then be called by codice_4 to print the values of the nodes of the tree in post-order depth-first order.\n\nThe breadth-first corecursive generator can be defined as:\n\nThis can then be called to print the values of the nodes of the tree in breadth-first order:\n\nInitial data types can be defined as being the least fixpoint (up to isomorphism) of some type equation; the isomorphism is then given by an initial algebra. Dually, final (or terminal) data types can be defined as being the greatest fixpoint of a type equation; the isomorphism is then given by a final coalgebra.\n\nIf the domain of discourse is the category of sets and total functions, then final data types may contain infinite, non-wellfounded values, whereas initial types do not. On the other hand, if the domain of discourse is the category of complete partial orders and continuous functions, which corresponds roughly to the Haskell programming language, then final types coincide with initial types, and the corresponding final coalgebra and initial algebra form an isomorphism.\n\nCorecursion is then a technique for recursively defining functions whose range (codomain) is a final data type, dual to the way that ordinary recursion recursively defines functions whose domain is an initial data type.\n\nThe discussion below provides several examples in Haskell that distinguish corecursion. Roughly speaking, if one were to port these definitions to the category of sets, they would still be corecursive. This informal usage is consistent with existing textbooks about Haskell. Also note that the examples used in this article predate the attempts to define corecursion and explain what it is.\n\nThe rule for \"primitive corecursion\" on codata is the dual to that for primitive recursion on data. Instead of descending on the argument by pattern-matching on its constructors (that \"were called up before\", somewhere, so we receive a ready-made datum and get at its constituent sub-parts, i.e. \"fields\"), we ascend on the result by filling-in its \"destructors\" (or \"observers\", that \"will be called afterwards\", somewhere - so we're actually calling a constructor, creating another bit of the result to be observed later on). Thus corecursion \"creates\" (potentially infinite) codata, whereas ordinary recursion \"analyses\" (necessarily finite) data. Ordinary recursion might not be applicable to the codata because it might not terminate. Conversely, corecursion is not strictly necessary if the result type is data, because data must be finite.\n\nIn \"Programming with streams in Coq: a case study: the Sieve of Eratosthenes\" we find\n\nwhere primes \"are obtained by applying the primes operation to the stream (Enu 2)\". Following the above notation, the sequence of primes (with a throwaway 0 prefixed to it) and numbers streams being progressively sieved, can be represented as \nor in Haskell, \n\nThe authors discuss how the definition of codice_5 is not guaranteed always to be \"productive\", and could become stuck e.g. if called with codice_6 as the initial stream.\n\nHere is another example in Haskell. The following definition produces the list of Fibonacci numbers in linear time:\nThis infinite list depends on lazy evaluation; elements are computed on an as-needed basis, and only finite prefixes are ever explicitly represented in memory. This feature allows algorithms on parts of codata to terminate; such techniques are an important part of Haskell programming.\n\nThis can be done in Python as well:\nThe definition of codice_7 can be inlined, leading to this:\n\nThis example employs a self-referential \"data structure\". Ordinary recursion makes use of self-referential \"functions\", but does not accommodate self-referential data. However, this is not essential to the Fibonacci example. It can be rewritten as follows:\n\nThis employs only self-referential \"function\" to construct the result. If it were used with strict list constructor it would be an example of runaway recursion, but with non-strict list constructor this guarded recursion gradually produces an indefinitely defined list.\n\nCorecursion need not produce an infinite object; a corecursive queue is a particularly good example of this phenomenon. The following definition produces a breadth-first traversal of a binary tree in linear time:\n\nThis definition takes an initial tree and produces a list of subtrees. This list serves dual purpose as both the queue and the result ( produces its output notches after its input back-pointer, , along the ). It is finite if and only if the initial tree is finite. The length of the queue must be explicitly tracked in order to ensure termination; this can safely be elided if this definition is applied only to infinite trees. \n\nAnother particularly good example gives a solution to the problem of breadth-first labeling. The function codice_8 visits every node in a binary tree in a breadth first fashion, and replaces each label with an integer, each subsequent integer is bigger than the last by one. This solution employs a self-referential data structure, and the binary tree can be finite or infinite.\n\nAn apomorphism (such as an anamorphism, such as unfold) is a form of corecursion in the same way that a paramorphism (such as a catamorphism, such as fold) is a form of recursion.\n\nThe Coq proof assistant supports corecursion and coinduction using the CoFixpoint command.\n\nCorecursion, referred to as \"circular programming,\" dates at least to , who credits John Hughes and Philip Wadler; more general forms were developed in . The original motivations included producing more efficient algorithms (allowing 1 pass over data in some cases, instead of requiring multiple passes) and implementing classical data structures, such as doubly linked lists and queues, in functional languages.\n\n\n"}
{"id": "3633706", "url": "https://en.wikipedia.org/wiki?curid=3633706", "title": "Curve orientation", "text": "Curve orientation\n\nIn mathematics, a positively oriented curve is a planar simple closed curve (that is, a curve in the plane whose starting point is also the end point and which has no other self-intersections) such that when traveling on it one always has the curve interior to the left (and consequently, the curve exterior to the right). If in the above definition one interchanges left and right, one obtains a negatively oriented curve.\n\nCrucial to this definition is the fact that every simple closed curve admits a well-defined interior; that follows from the Jordan curve theorem.\n\nAll simple closed curves can be classified as negatively oriented (clockwise), positively oriented (counterclockwise), or non-orientable. The inner loop of a beltway road in the United States (or other countries where people drive on the right side of the road) would be an example of a negatively oriented (clockwise) curve. A circle oriented counterclockwise is an example of a positively oriented curve. The same circle oriented clockwise would be a negatively oriented curve.\n\nThe concept of \"orientation\" of a curve is just a particular case of the notion of orientation of a manifold (that is, besides orientation of a curve one may also speak of orientation of a surface, hypersurface, etc.). Here, the interior and the exterior of a curve both inherit the usual orientation of the plane. The positive orientation on the curve is then the orientation it inherits as the boundary of its interior; the negative orientation is inherited from the exterior.\n\nIn two dimensions, given an ordered set of three or more connected vertices (points) (such as in connect-the-dots) which forms a simple polygon, the orientation of the resulting polygon is directly related to the sign of the angle at any vertex of the convex hull of the polygon, for example, of the angle ABC in the picture. In computations, the sign of the smaller angle formed by a pair of vectors is typically determined by the sign of the cross product of the vectors. The latter one may be calculated as the sign of the determinant of their orientation matrix. In the particular case when the two vectors are defined by two line segments with common endpoint, such as the sides BA and BC of the angle ABC in our example, the orientation matrix may be defined as follows: \n\nA formula for its determinant may be obtained, e.g., using the method of cofactor expansion:\n\nIf the determinant is negative, then the polygon is oriented clockwise. If the determinant is positive, the polygon is oriented counterclockwise. The determinant is non-zero if points A, B, and C are non-collinear. In the above example, with points ordered A, B, C, etc., the determinant is negative, and therefore the polygon is clockwise.\n\nIn practical applications, the following considerations are commonly taken into an account.\n\nOne does not need to construct the convex hull of a polygon to find a suitable vertex. A common choice is the vertex of the polygon with the smallest X-coordinate. If there are several of them, the one with the smallest Y-coordinate is picked. It is guaranteed to be the vertex of the convex hull of the polygon. Alternatively, the vertex with the smallest Y-coordinate among the ones with the largest X-coordinates or the vertex with the smallest X-coordinate among the ones with the largest Y-coordinates (or any other of 8 \"smallest, largest\" X/Y combinations) will do as well. \n\nIf the orientation of a convex polygon is sought, then, of course, any vertex may be picked.\n\nFor numerical reasons, the following equivalent formula for the determinant is commonly used:\n\nThe latter formula has four multiplications less. What is more important in computer computations involved in most practical applications, such as computer graphics or CAD, the absolute values of the multipliers are usually smaller (e.g., when A, B, C are within the same quadrant), thus giving a smaller numerical error or, in the extreme cases, avoiding the arithmetic overflow.\n\nWhen it is not known in advance that the sequence of points defines a simple polygon, the following things must be kept in mind. \n\nFor a self-intersecting polygon (complex polygon) (or for any self-intersecting curve) there is no natural notion of the \"interior\", hence the orientation is not defined. At the same time, in geometry and computer graphics there are a number of concepts to replace the notion of the \"interior\" for closed non-simple curves; see, e.g., \"flood fill\" and \"winding number\".\n\nIn \"mild\" cases of self-intersection, with degenerate vertices when three consecutive points are allowed be on the same straight line and form a zero-degree angle, the concept of \"interior\" still makes sense, but an extra care must be taken in selection of the tested angle. In the given example, imagine point A to lie on segment BC. In this situation the angle ABC and its determinant will be 0, hence useless. A solution is to test consecutive corners along the polygon (BCD, DEF...) until a non-zero determinant is found (unless all points lie on the same straight line). (Notice that the points C, D, E are on the same line and form a 180-degree angle with zero determinant.)\n\nOnce the orientation of a polygon formed from an ordered set of vertices is known, the concavity of a local region of the polygon can be determined using a second orientation matrix. This matrix is composed of three consecutive vertices which are being examined for concavity. For example, in the polygon pictured above, if we wanted to know whether the sequence of points F-G-H is concave, convex, or collinear (flat), we construct the matrix\n\nIf the determinant of this matrix is 0, then the sequence is collinear - neither concave nor convex. If the determinant has the same sign as that of the orientation matrix for the entire polygon, then the sequence is convex. If the signs differ, then the sequence is concave. In this example, the polygon is negatively oriented, but the determinant for the points F-G-H is positive, and so the sequence F-G-H is concave.\n\nThe following table illustrates rules for determining whether a sequence of points is convex, concave, or flat:\n\n\n"}
{"id": "241863", "url": "https://en.wikipedia.org/wiki?curid=241863", "title": "Difference quotient", "text": "Difference quotient\n\nIn single-variable calculus, the difference quotient is usually the name for the expression\n\nwhich when taken to the limit as \"h\" approaches 0 gives the derivative of the function \"f\". The name of the expression stems from the fact that it is the quotient of the difference of values of the function by the difference of the corresponding values of its argument (the latter is (\"x\"+\"h\")-\"x\"=\"h\" in this case). The difference quotient is a measure of the average rate of change of the function over an interval (in this case, an interval of length \"h\"). The limit of the difference quotient (i.e., the derivative) is thus the instantaneous rate of change.\n\nBy a slight change in notation (and viewpoint), for an interval [\"a\", \"b\"], the difference quotient\n\nis called the mean (or average) value of the derivative of \"f\" over the interval [\"a\", \"b\"]. This name is justified by the mean value theorem, which states that for a differentiable function \"f\", its derivative \"f′\" reaches its mean value at some point in the interval. Geometrically, this difference quotient measures the slope of the secant line passing through the points with coordinates (\"a\", \"f\"(\"a\")) and (\"b\", \"f\"(\"b\")).\n\nDifference quotients are used as approximations in numerical differentiation, but they have also been subject of criticism in this application.\n\nThe difference quotient is sometimes also called the Newton quotient (after Isaac Newton) or Fermat's difference quotient (after Pierre de Fermat).\n\nThe typical notion of the difference quotient discussed above is a particular case of a more general concept. The primary vehicle of calculus and other higher mathematics is the function. Its \"input value\" is its \"argument\", usually a point (\"P\") expressible on a graph. The difference between two points, themselves, is known as their Delta (Δ\"P\"), as is the difference in their function result, the particular notation being determined by the direction of formation:\nThe general preference is the forward orientation, as F(P) is the base, to which differences (i.e., \"ΔP\"s) are added to it. Furthermore,\n\n\nThe function difference divided by the point difference is known as \"difference quotient\":\n\nIf ΔP is infinitesimal, then the difference quotient is a \"derivative\", otherwise it is a \"divided difference\":\n\nRegardless if ΔP is infinitesimal or finite, there is (at least—in the case of the derivative—theoretically) a point range, where the boundaries are P ± (0.5) ΔP (depending on the orientation—ΔF(P), δF(P) or ∇F(P)):\nDerivatives can be regarded as functions themselves, harboring their own derivatives. Thus each function is home to sequential degrees (\"higher orders\") of derivation, or \"differentiation\". This property can be generalized to all difference quotients.<br>\nAs this sequencing requires a corresponding boundary splintering, it is practical to break up the point range into smaller, equi-sized sections, with each section being marked by an intermediary point (\"P\"), where LB = \"P\" and UB = \"P\", the \"n\"th point, equaling the degree/order:\n\nThere are other derivative notations, but these are the most recognized, standard designations.\n\nThe quintessential application of the divided difference is in the presentation of the definite integral, which is nothing more than a finite difference:\n\nGiven that the mean value, derivative expression form provides all of the same information as the classical integral notation, the mean value form may be the preferable expression, such as in writing venues that only support/accept standard ASCII text, or in cases that only require the average derivative (such as when finding the average radius in an elliptic integral).\nThis is especially true for definite integrals that technically have (e.g.) 0 and either formula_24 or formula_25 as boundaries, with the same divided difference found as that with boundaries of 0 and formula_26 (thus requiring less averaging effort):\n\nThis also becomes particularly useful when dealing with \"iterated\" and \"multiple integral\"s (ΔA = AU − AL, ΔB = BU − BL, ΔC = CU − CL):\n\nHence,\n\nand\n\n\n"}
{"id": "18991816", "url": "https://en.wikipedia.org/wiki?curid=18991816", "title": "Dynamic equation", "text": "Dynamic equation\n\nIn mathematics, dynamic equation can refer to:\n\n"}
{"id": "19575137", "url": "https://en.wikipedia.org/wiki?curid=19575137", "title": "Esquisse d'un Programme", "text": "Esquisse d'un Programme\n\n\"Esquisse d'un Programme\" (Sketch of a Programme) is a famous proposal for long-term mathematical research made by the German-born, French mathematician Alexander Grothendieck in 1984. He pursued the sequence of logically linked ideas in his important project proposal from 1984 until 1988, but his proposed research continues to date to be of major interest in several branches of advanced mathematics. Grothendieck's vision provides inspiration today for several developments in mathematics such as the extension and generalization of Galois theory, which is currently being extended based on his original proposal.\n\nSubmitted in 1984, the \"Esquisse d'un Programme\" was a proposal submitted by Alexander Grothendieck for a position at the Centre National de la Recherche Scientifique. The proposal was not successful, but Grothendieck obtained a special position where, while keeping his affiliation at the university of Montpellier, he was paid by the CNRS and released of his teaching obligations. Grothendieck held this position from 1984 till 1988. This proposal was not formally published until 1997, because the author \"could not be found, much less his permission requested\". The outlines of \"dessins d'enfants\", or \"children's drawings\", and \"Anabelian geometry\", that are contained in this manuscript continue to inspire research; thus, \"Anabelian geometry is a proposed theory in mathematics, describing the way the algebraic fundamental group \"G\" of an algebraic variety \"V\", or some related geometric object, determines how \"V\" can be mapped into another geometric object \"W\", under the assumption that \"G\" is not an abelian group, in the sense of being strongly noncommutative. The word \"anabelian\" (an alpha privative \"an-\" before \"abelian\") was introduced in \"Esquisse d'un Programme\". While the work of Grothendieck was for many years unpublished, and unavailable through the traditional formal scholarly channels, the formulation and predictions of the proposed theory received much attention, and some alterations, at the hands of a number of mathematicians. Those who have researched in this area have obtained some expected and related results, and in the 21st century the beginnings of such a theory started to be available.\"\"\n\n\nSuggested further reading for the interested mathematical reader is provided\nin the \"References\" section.\n\nGalois has developed a powerful, fundamental algebraic theory in mathematics that provides very efficient computations for certain algebraic problems by utilizing the algebraic concept of groups, which is now known as the theory of Galois groups; such computations were not possible before, and also in many cases are much more effective than the 'direct' calculations without using groups. To begin with, Alexander Grothendieck stated in his proposal:\" \"Thus, the group of Galois is realized as the automorphism group of a concrete, pro-finite group which respects certain structures that are essential to this group.\"\" This fundamental, Galois group theory in mathematics has been considerably expanded, at first to groupoids- as proposed in Alexander Grothendieck's \"Esquisse d' un Programme\" (\"EdP\")- and now already partially carried out for groupoids; the latter are now further developed beyond groupoids to categories by several groups of mathematicians. Here, we shall focus only on the well-established and fully validated extensions of Galois' theory. Thus, EdP also proposed and anticipated, along previous Alexander Grothendieck's \"IHÉS\" seminars (SGA1 to SGA4) held in the 1960s, the development of even more powerful extensions of the original Galois's theory for groups by utilizing categories, functors and natural transformations, as well as further expansion of the manifold of ideas presented in Alexander Grothendieck's \"Descent Theory\". The notion of motive has also been pursued actively. This was developed into the motivic Galois group, Grothendieck topology and Grothendieck category\n. Such developments were recently extended in algebraic topology \"via\" representable functors and the fundamental groupoid functor.\n\n\n\n\n\n\n"}
{"id": "51380802", "url": "https://en.wikipedia.org/wiki?curid=51380802", "title": "Eulerian coherent structure", "text": "Eulerian coherent structure\n\nIn applied mathematics, objective Eulerian coherent structures (OECSs) are the instantaneously most influential surfaces or curves that exert a major influence on nearby trajectories in a dynamical system over short time-scales, and are the short-time limit of Lagrangian coherent structures (LCSs). Such influence can be of different types, but OECSs invariably create a short-term coherent trajectory pattern for which they serve as a theoretical centerpiece. While LCSs are intrinsically tied to a specific finite time interval, OECSs can be computed at any time instant regardless of the multiple and generally unknown time scales of the system. \n\nIn observations of tracer patterns in nature, one readily identifies short-term variability in material structures such as emerging and dissolving coherent features. However, it is often the underlying structure creating these features that is of interest. While individual tracer trajectories forming coherent patterns are generally sensitive with respect to changes in their initial conditions and the system parameters, OECSs are robust and reveal the instantaneous time-varying skeleton of complex dynamical systems. Despite OECSs are defined for general dynamical systems, their role in creating coherent patterns is perhaps most readily observable in fluid flows. Therefore, OECSs are suitable in a number of applications ranging from flow control to environmental assessment such as now-casting or short-term forecasting of pattern evolution, where quick operational decisions need to be made. Examples include floating debris, oil spills, surface drifters, and control of unsteady flow separation.\n"}
{"id": "592151", "url": "https://en.wikipedia.org/wiki?curid=592151", "title": "Even and odd functions", "text": "Even and odd functions\n\nIn mathematics, even functions and odd functions are functions which satisfy particular symmetry relations, with respect to taking additive inverses. They are important in many areas of mathematical analysis, especially the theory of power series and Fourier series. They are named for the parity of the powers of the power functions which satisfy each condition: the function formula_1 is an even function if formula_2 is an even integer, and it is an odd function if formula_2 is an odd integer.\n\nEvenness or oddness are generally considered for real functions, that is real-valued functions of a real variable. However the concepts may be, more generally defined for functions whose domain and codomain both have an additive inverse. This includes additive groups, all rings, all fields, and all vector spaces. Thus, for example, a real function, as could a complex-valued function of a vector variable, and so on.\n\nThe given examples are real functions, to illustrate the symmetry of their graphs.\n\nLet formula_4 be a real-valued function of a real variable. Then formula_5 is even if the following equation holds for all formula_6 and formula_7 in the domain of formula_5:\n\nor\n\nGeometrically speaking, the graph face of an even function is symmetric with respect to the \"y\"-axis, meaning that its graph remains unchanged after reflection about the \"y\"-axis.\n\nExamples of even functions are:\n\nAgain, let formula_4 be a real-valued function of a real variable. Then formula_5 is odd if the following equation holds for all formula_6 and formula_7 in the domain of formula_5:\n\nor\n\nGeometrically, the graph of an odd function has rotational symmetry with respect to the origin, meaning that its graph remains unchanged after rotation of 180 degrees about the origin.\n\nExamples of odd functions are:\n\n\n\n\n\nEvery function may be uniquely decomposed as the sum of an even and an odd function, which are called respectively the even part and the odd part of the function. In fact, if one defines\nthen formula_29 is even, formula_30 is odd, and\n\nConversely, if\nwhere is even and is odd, then formula_33 and formula_34 since\n\nFor example, the hyperbolic cosine and the hyperbolic sine may be defined as the even and odd parts of the exponential function, as the first one is an even function, the second one is odd, and \n\n\n\nA function's being odd or even does not imply differentiability, or even continuity. For example, the Dirichlet function is even, but is nowhere continuous. \n\nIn the following, properties involving derivatives, Fourier series, Taylor series, and so on suppose that these concepts are defined of the functions that are considered.\n\n\nformula_39.\n\n\nIn signal processing, harmonic distortion occurs when a sine wave signal is sent through a memoryless nonlinear system, that is, a system whose output at time formula_40 only depends on the input at time formula_40 and does not depend on the input at any previous times. Such a system is described by a response function formula_42. The type of harmonics produced depend on the response function formula_5:\n\nNote that this does not hold true for more complex waveforms. A sawtooth wave contains both even and odd harmonics, for instance. After even-symmetric full-wave rectification, it becomes a triangle wave, which, other than the DC offset, contains only odd harmonics.\n\nThe definitions for even and odd symmetry for complex-valued functions of a real argument are similar to the real case but involve complex conjugation.\n\nA complex-valued function of a real argument formula_48 is called even symmetric if:\n\nA complex-valued function of a real argument formula_48 is called odd symmetric if:\n\n"}
{"id": "220782", "url": "https://en.wikipedia.org/wiki?curid=220782", "title": "Exterior derivative", "text": "Exterior derivative\n\nOn a differentiable manifold, the exterior derivative extends the concept of the differential of a function to differential forms of higher degree. The exterior derivative was first described in its current form by Élie Cartan in 1899; it allows for a natural, metric-independent generalization of Stokes' theorem, Gauss's theorem, and Green's theorem from vector calculus.\n\nIf a -form is thought of as measuring the flux through an infinitesimal -parallelotope, then its exterior derivative can be thought of as measuring the net flux through the boundary of a -parallelotope.\n\nThe exterior derivative of a differential form of degree is a differential form of degree \n\nIf is a smooth function (a -form), then the exterior derivative of is the differential of . That is, is the unique -form such that for every smooth vector field , , where is the directional derivative of in the direction of .\n\nThere are a variety of equivalent definitions of the exterior derivative of a general -form.\n\nThe exterior derivative is defined to be the unique -linear mapping from -forms to -forms satisfying the following properties:\n\n\nThe second defining property holds in more generality: in fact, for any -form ; more succinctly, . The third defining property implies as a special case that if is a function and a -form, then because functions are -forms, and scalar multiplication and the exterior product are equivalent when one of the arguments is a scalar.\n\nAlternatively, one can work entirely in a local coordinate system . The coordinate differentials form a basis of the space of one-forms, each associated with a coordinate. Given a multi-index with for (and denoting with an abuse of notation ), the exterior derivative of a (simple) -form\n\nover is defined as\n\n(using Einstein notation). The definition of the exterior derivative is extended linearly to a general -form\n\nwhere each of the components of the multi-index run over all the values in . Note that whenever equals one of the components of the multi-index then (see Exterior product).\n\nThe definition of the exterior derivative in local coordinates follows from the preceding definition in terms of axioms. Indeed, with the -form as defined above,\n\nHere, we have interpreted as a -form, and then applied the properties of the exterior derivative.\n\nThis result extends directly to the general -form as\n\nIn particular, for a -form , the components of in local coordinates are\n\nAlternatively, an explicit formula can be given for the exterior derivative of a -form , when paired with arbitrary smooth vector fields :\n\nwhere denotes the Lie bracket and a hat denotes the omission of that element:\n\nIn particular, for -forms we have: , where and are vector fields, is the scalar field defined by the vector field applied as a differential operator (\"directional derivative along \"X\"\") to the scalar field defined by applying as a covector field to the vector field and likewise for .\n\nNote: Some authors (e.g., Kobayashi–Nomizu and Helgason) use a formula that differs by a factor of :\n\nExample 1. Consider over a -form basis . The exterior derivative is:\n\nThe last formula follows easily from the properties of the exterior product. Namely, .\n\nExample 2. Let be a -form defined over . By applying the above formula to each term (consider and ) we have the following sum,\n\nIf is a compact smooth orientable -dimensional manifold with boundary, and is an -form on , then the generalized form of Stokes' theorem states that:\n\nIntuitively, if one thinks of as being divided into infinitesimal regions, and one adds the flux through the boundaries of all the regions, the interior boundaries all cancel out, leaving the total flux through the boundary of .\n\nA -form is called \"closed\" if ; closed forms are the kernel of . is called \"exact\" if for some -form ; exact forms are the image of . Because , every exact form is closed. The Poincaré lemma states that in a contractible region, the converse is true.\n\nBecause the exterior derivative has the property that , it can be used as the differential (coboundary) to define de Rham cohomology on a manifold. The -th de Rham cohomology (group) is the vector space of closed -forms modulo the exact -forms; as noted in the previous section, the Poincaré lemma states that these vector spaces are trivial for a contractible region, for . For smooth manifolds, integration of forms gives a natural homomorphism from the de Rham cohomology to the singular cohomology over . The theorem of de Rham shows that this map is actually an isomorphism, a far-reaching generalization of the Poincaré lemma. As suggested by the generalized Stokes' theorem, the exterior derivative is the \"dual\" of the boundary map on singular simplices.\n\nThe exterior derivative is natural in the technical sense: if is a smooth map and is the contravariant smooth functor that assigns to each manifold the space of -forms on the manifold, then the following diagram commutes\n\nso , where denotes the pullback of . This follows from that , by definition, is , being the pushforward of . Thus is a natural transformation from to .\n\nMost vector calculus operators are special cases of, or have close relationships to, the notion of exterior differentiation.\n\nA smooth function on a real differentiable manifold is a -form. The exterior derivative of this -form is the -form .\n\nWhen an inner product is defined, the gradient of a function is defined as the unique vector in such that its inner product with any element of is the directional derivative of along the vector, that is such that\n\nThat is,\nwhere denotes the musical isomorphism mentioned earlier that is induced by the inner product.\n\nThe -form is a section of the cotangent bundle, that gives a local linear approximation to in the cotangent space at each point.\n\nA vector field on has a corresponding -form\n\nwhere formula_16 denotes the omission of that element.\n\n(For instance, when , i.e. in three-dimensional space, the -form is locally the scalar triple product with .) The integral of over a hypersurface is the flux of over that hypersurface.\n\nThe exterior derivative of this -form is the -form\n\nA vector field on also has a corresponding -form\n\nLocally, is the dot product with . The integral of along a path is the work done against along that path.\n\nWhen , in three-dimensional space, the exterior derivative of the -form is the -form\n\nThe standard vector calculus operators can be generalized for any pseudo-Riemannian manifold, and written in coordinate-free notation as follows:\n\nwhere is the Hodge star operator, and are the musical isomorphisms, is a scalar field and is a vector field.\n\nNote that the expression for formula_21 makes sense only in three dimensions, since it requires formula_22 to act on formula_23, which is a form of degree formula_24.\n\n\n"}
{"id": "56835885", "url": "https://en.wikipedia.org/wiki?curid=56835885", "title": "Fairfield Experiment", "text": "Fairfield Experiment\n\nThe Fairfield experiment was an experiment in industrial relations carried out at the Fairfield Shipbuilding and Engineering Company, Glasgow during the 1960s. The experiment was agreed by George Brown, the First Secretary in Harold Wilson's cabinet, in 1966. The company was facing closure, but Brown agreed to provide £1M of state cash to enable the Trade Unions, the management and the shareholders to try out new ways of industrial management.\n\n\"The Bowler and the Bunnet\" was a film directed by Sean Connery and written by Cliff Hanley about the Fairfield Experiment.\n"}
{"id": "1018676", "url": "https://en.wikipedia.org/wiki?curid=1018676", "title": "Field of sets", "text": "Field of sets\n\nIn mathematics a field of sets is a pair formula_1 where formula_2 is a set and formula_3 is an algebra over formula_4 i.e., a non-empty subset of the power set of formula_2 closed under the intersection and union of pairs of sets and under complements of individual sets. In other words, formula_3 forms a subalgebra of the power set Boolean algebra of formula_2. (Many authors refer to formula_3 itself as a field of sets.) Elements of formula_4 are called points and those of formula_3 are called complexes and are said to be the admissible sets of formula_2. \n\nFields of sets should not be confused with fields in ring theory nor with fields in physics. Similarly the term \"algebra over formula_4\" is used in the sense of a Boolean algebra and should not be confused with algebras over fields or rings in ring theory.\n\nFields of sets play an essential role in the representation theory of Boolean algebras. Every Boolean algebra can be represented as a field of sets.\n\nEvery finite Boolean algebra can be represented as a whole power set - the power set of its set of atoms; each element of the Boolean algebra corresponds to the set of atoms below it (the join of which is the element). This power set representation can be constructed more generally for any complete atomic Boolean algebra.\n\nIn the case of Boolean algebras which are not complete and atomic we can still generalize the power set representation by considering fields of sets instead of whole power sets. To do this we first observe that the atoms of a finite Boolean algebra correspond to its ultrafilters and that an atom is below an element of a finite Boolean algebra if and only if that element is contained in the ultrafilter corresponding to the atom. This leads us to construct a representation of a Boolean algebra by taking its set of ultrafilters and forming complexes by associating with each element of the Boolean algebra the set of ultrafilters containing that element. This construction does indeed produce a representation of the Boolean algebra as a field of sets and is known as the Stone representation. It is the basis of Stone's representation theorem for Boolean algebras and an example of a completion procedure in order theory based on ideals or filters, similar to Dedekind cuts.\n\nAlternatively one can consider the set of homomorphisms onto the two element Boolean algebra and form complexes by associating each element of the Boolean algebra with the set of such homomorphisms that map it to the top element. (The approach is equivalent as the ultrafilters of a Boolean algebra are precisely the pre-images of the top elements under these homomorphisms.) With this approach one sees that Stone representation can also be regarded as a generalization of the representation of finite Boolean algebras by truth tables.\n\n\nThese definitions arise from considering the topology generated by the complexes of a field of sets. Given a field of sets formula_14 the complexes form a base for a topology. We denote by formula_15 the corresponding topological space, formula_16 where formula_17 is the topology formed by taking arbitrary unions of complexes. Then\n\n\nThe Stone representation of a Boolean algebra is always separative and compact; the corresponding Boolean space is known as the Stone space of the Boolean algebra. The clopen sets of the Stone space are then precisely the complexes of the Stone representation. The area of mathematics known as Stone duality is founded on the fact that the Stone representation of a Boolean algebra can be recovered purely from the corresponding Stone space whence a duality exists between Boolean algebras and Boolean spaces.\n\nIf an algebra over a set is closed under countable intersections and countable unions, it is called a sigma algebra and the corresponding field of sets is called a measurable space. The complexes of a measurable space are called measurable sets. The Loomis-Sikorski theorem provides a Stone-type duality between countably complete Boolean algebras (which may be called abstract sigma algebras) and measurable spaces.\n\nA measure space is a triple formula_27 where formula_1 is a measurable space and formula_29 is a measure defined on it. If formula_29 is in fact a probability measure we speak of a probability space and call its underlying measurable space a sample space. The points of a sample space are called samples and represent potential outcomes while the measurable sets (complexes) are called events and represent properties of outcomes for which we wish to assign probabilities. (Many use the term sample space simply for the underlying set of a probability space, particularly in the case where every subset is an event.) Measure spaces and probability spaces play a foundational role in measure theory and probability theory respectively.\n\nIn applications to Physics we often deal with measure spaces and probability spaces derived from rich mathematical structures such as inner product spaces or topological groups which already have a topology associated with them - this should not be confused with the topology generated by taking arbitrary unions of complexes.\n\nA topological field of sets is a triple formula_31 where formula_16 is a topological space and formula_1 is a field of sets which is closed under the closure operator of formula_17 or equivalently under the interior operator i.e. the closure and interior of every complex is also a complex. In other words, formula_3 forms a subalgebra of the power set interior algebra on formula_16.\n\nTopological fields of sets play a fundamental role in the representation theory of interior algebras and Heyting algebras. These two classes of algebraic structures provide the algebraic semantics for the modal logic \"S4\" (a formal mathematical abstraction of epistemic logic) and intuitionistic logic respectively. Topological fields of sets representing these algebraic structures provide a related topological semantics for these logics.\n\nEvery interior algebra can be represented as a topological field of sets with the underlying Boolean algebra of the interior algebra corresponding to the complexes of the topological field of sets and the interior and closure operators of the interior algebra corresponding to those of the topology. Every Heyting algebra can be represented by a topological field of sets with the underlying lattice of the Heyting algebra corresponding to the lattice of complexes of the topological field of sets that are open in the topology. Moreover the topological field of sets representing a Heyting algebra may be chosen so that the open complexes generate all the complexes as a Boolean algebra. These related representations provide a well defined mathematical apparatus for studying the relationship between truth modalities (possibly true vs necessarily true, studied in modal logic) and notions of provability and refutability (studied in intuitionistic logic) and is thus deeply connected to the theory of modal companions of intermediate logics.\n\nGiven a topological space the clopen sets trivially form a topological field of sets as each clopen set is its own interior and closure. The Stone representation of a Boolean algebra can be regarded as such a topological field of sets, however in general the topology of a topological field of sets can differ from the topology generated by taking arbitrary unions of complexes and in general the complexes of a topological field of sets need not be open or closed in the topology.\n\nA topological field of sets is called algebraic if and only if there is a base for its topology consisting of complexes.\n\nIf a topological field of sets is both compact and algebraic then its topology is compact and its compact open sets are precisely the open complexes. Moreover, the open complexes form a base for the topology.\n\nTopological fields of sets that are separative, compact and algebraic are called Stone fields and provide a generalization of the Stone representation of Boolean algebras. Given an interior algebra we can form the Stone representation of its underlying Boolean algebra and then extend this to a topological field of sets by taking the topology generated by the complexes corresponding to the open elements of the interior algebra (which form a base for a topology). These complexes are then precisely the open complexes and the construction produces a Stone field representing the interior algebra - the Stone representation. (The topology of the Stone representation is also known as the McKinsey-Tarski Stone topology after the mathematicians who first generalized Stone's result for Boolean algebras to interior algebras and should not be confused with the Stone topology of the underlying Boolean algebra of the interior algebra which will be a finer topology).\n\nA preorder field is a triple formula_37 where formula_38 is a preordered set and formula_39 is a field of sets.\n\nLike the topological fields of sets, preorder fields play an important role in the representation theory of interior algebras. Every interior algebra can be represented as a preorder field with its interior and closure operators corresponding to those of the Alexandrov topology induced by the preorder. In other words,\n\nSimilarly to topological fields of sets, preorder fields arise naturally in modal logic where the points represent the \"possible worlds\" in the Kripke semantics of a theory in the modal logic \"S4\", the preorder represents the accessibility relation on these possible worlds in this semantics, and the complexes represent sets of possible worlds in which individual sentences in the theory hold, providing a representation of the Lindenbaum–Tarski algebra of the theory. They are a special case of the general modal frames which are fields of sets with an additional accessibility relation providing representations of modal algebras.\n\nA preorder field is called algebraic (or tight) if and only if it has a set of complexes formula_47 which determines the preorder in the following manner: formula_48 if and only if for every complex formula_49, formula_50 implies formula_51. The preorder fields obtained from \"S4\" theories are always algebraic, the complexes determining the preorder being the sets of possible worlds in which the sentences of the theory closed under necessity hold.\n\nA separative compact algebraic preorder field is said to be canonical. Given an interior algebra, by replacing the topology of its Stone representation with the corresponding canonical preorder (specialization preorder) we obtain a representation of the interior algebra as a canonical preorder field. By replacing the preorder by its corresponding Alexandrov topology we obtain an alternative representation of the interior algebra as a topological field of sets. (The topology of this \"Alexandrov representation\" is just the Alexandrov bi-coreflection of the topology of the Stone representation.) While representation of modal algebras by general modal frames is possible for any normal modal algebra, it is only in the case of interior algebras (which correspond to the modal logic \"S4\") that the general modal frame corresponds to topological field of sets in this manner.\n\nThe representation of interior algebras by preorder fields can be generalized to a representation theorem for arbitrary (normal) Boolean algebras with operators. For this we consider structures formula_52 where formula_53 is a relational structure i.e. a set with an indexed family of relations defined on it, and formula_54 is a field of sets. The complex algebra (or algebra of complexes) determined by a field of sets formula_55 on a relational structure, is the Boolean algebra with operators\n\nwhere for all formula_57, if formula_58 is a relation of arity formula_59, then formula_60 is an operator of arity formula_61 and for all formula_62\n\nThis construction can be generalized to fields of sets on arbitrary algebraic structures having both operators and relations as operators can be viewed as a special case of relations. If formula_3 is the whole power set of formula_13 then formula_68 is called a full complex algebra or power algebra.\n\nEvery (normal) Boolean algebra with operators can be represented as a field of sets on a relational structure in the sense that it is isomorphic to the complex algebra corresponding to the field.\n\n\n"}
{"id": "52188012", "url": "https://en.wikipedia.org/wiki?curid=52188012", "title": "Fly algorithm", "text": "Fly algorithm\n\nThe Fly Algorithm is a type of cooperative coevolution based on the Parisian approach. The Fly Algorithm has first been developed in 1999 in the scope of the application of Evolutionary algorithms to computer stereo vision. Unlike the classical image-based approach to stereovision, which extracts image primitives then matches them in order to obtain 3-D information, the Fly Agorithm is based on the direct exploration of the 3-D space of the scene. A fly is defined as a 3-D point described by its coordinates (\"x\", \"y\", \"z\"). Once a random population of flies has been created in a search space corresponding to the field of view of the cameras, its evolution (based on the Evolutionary Strategy paradigm) used a fitness function that evaluates how likely the fly is lying on the visible surface of an object, based on the consistency of its image projections. To this end, the fitness function uses the grey levels, colours and/or textures of the calculated fly's projections.\n\nThe first application field of the Fly Algorithm has been stereovision. While classical `image priority' approaches use matching features from the stereo images in order to build a 3-D model, the Fly Algorithm directly explores the 3-D space and uses image data to evaluate the validity of 3-D hypotheses. A variant called the \"Dynamic Flies\" defines the fly as a 6-uple (\"x\", \"y\", \"z\", \"x’\", \"y’\", \"z’\") involving the fly's velocity. The velocity components are not explicitly taken into account in the fitness calculation but are used in the flies' positions updating and are subject to similar genetic operators (mutation, crossover).\n\nThe application of Flies to obstacle avoidance in vehicles exploits the fact that the population of flies is a time compliant, quasi-continuously evolving representation of the scene to directly generate vehicle control signals from the flies. The use of the Fly Algorithm is not strictly restricted to stereo images, as other sensors may be added (e.g. acoustic proximity sensors, etc.) as additional terms to the fitness function being optimised. Odometry information can also be used to speed up the updating of flies' positions, and conversely the flies positions can be used to provide localisation and mapping information.\n\nAnother application field of the Fly Algorithm is reconstruction for emission Tomography in nuclear medicine. The Fly Algorithm has been successfully applied in single-photon emission computed tomography and positron emission tomography\n. Here, each fly is considered a photon emitter and its fitness is based on the conformity of the simulated illumination of the sensors with the actual pattern observed on the sensors. Within this application, the fitness function has been re-defined to use the new concept of 'marginal evaluation'. Here, the fitness of one individual is calculated as its (positive or negative) contribution to the quality of the global population. It is based on the leave-one-out cross-validation principle. A \"global fitness function\" evaluates the quality of the population as a whole; only then the fitness of an individual (a fly) is calculated as the difference between the global fitness values of the population with and without the particular fly whose \"individual fitness function\" has to be evaluated. In the fitness of each fly is considered as a `level of confidence'. It is used during the voxelisation process to tweak the fly's individual footprint using implicit modelling (such as metaballs). It produces smooth results that are more accurate.\n\nMore recently it has been used in digital art to generate mosaic-like images or spray paint. Examples of images can be found on YouTube\n\nHere, the population of individuals is considered as a \"society\" where the individuals collaborate toward a common goal. \nThis is implemented using an evolutionary algorithm that includes all the common genetic operators (e.g. mutation, cross-over, selection). \nThe main difference is in the fitness function. \nHere two levels of fitness function are used:\nIn addition, a diversity mechanism is required to avoid individuals gathering in only a few areas of the search space. \nAnother difference is in the extraction of the problem solution once the evolutionary loop terminates. In classical evolutionary approaches, the best individual corresponds to the solution and the rest of the population is discarded. \nHere, all the individuals (or individuals of a sub-group of the population) are collated to build the problem solution.\nThe way the fitness functions are constructed and the way the solution extraction is made are of course problem-dependent.\n\nExamples of Parisian Evolution applications include:\n\nCooperative coevolution is a broad class of evolutionary algorithms where a complex problem is solved by decomposing it into subcomponents that are solved independently. \nThe Parisian approach shares many similarities with the cooperative coevolutionary algorithm. The Parisian approach makes use of a single-population whereas multi-species may be used in cooperative coevolutionary algorithm. \nSimilar internal evolutionary engines are considered in classical evolutionary algorithm, cooperative coevolutionary algorithm and Parisian evolution. \nThe difference between cooperative coevolutionary algorithm and Parisian evolution resides in the population's semantics. \nCooperative coevolutionary algorithm divides a big problem into sub-problems (groups of individuals) and solves them separately toward the big problem. There is no interaction/breeding between individuals of the different sub-populations, only with individuals of the same sub-population. \nHowever, Parisian evolutionary algorithms solve a whole problem as a big component. \nAll population's individuals cooperate together to drive the whole population toward attractive areas of the search space.\n\nCooperative coevolution and particle swarm optimisation (PSO) share many similarities. PSO is inspired by the social behaviour of bird flocking or fish schooling. \nIt was initially introduced as a tool for realistic animation in computer graphics. \nIt uses complex individuals that interact with each other in order to build visually realistic collective behaviours through adjusting the individuals' behavioural rules (which may use random generators). \nIn mathematical optimisation, every particle of the swarm somehow follows its own random path biased toward the best particle of the swarm. \nIn the Fly Algorithm, the flies aim at building spatial representations of a scene from actual sensor data; flies do not communicate or explicitly cooperate, and do not use any behavioural model.\n\nBoth algorithms are search methods that start with a set of random solutions, which are iteratively corrected toward a global optimum. \nHowever, the solution of the optimisation problem in the Fly Algorithm is the population (or a subset of the population): The flies implicitly collaborate to build the solution. In PSO the solution is a single particle, the one with the best fitness. Another main difference between the Fly Algorithm and with PSO is that the Fly Algorithm is not based on any behavioural model but only builds a geometrical representation.\n\n\nTomography reconstruction is an inverse problem that is often ill-posed due to missing data and/or noise. The answer to the inverse problem is not unique, and in case of extreme noise level it may not even exist. The input data of a reconstruction algorithm may be given as the Radon transform or sinogram formula_1 of the data to reconstruct formula_2. formula_3 is unknown; formula_4 is known. \nThe data acquisition in tomography can be modelled as:\n\nformula_5\n\nwhere formula_6 is the system matrix or projection operator and formula_7 corresponds to some Poisson noise. \nIn this case the reconstruction corresponds to the inversion of the Radon transform:\n\nformula_8\n\nNote that formula_9 can account for noise, acquisition geometry, etc. \nThe Fly Algorithm is an example of iterative reconstruction. Iterative methods in tomographic reconstruction are relatively easy to model:\n\nformula_10\n\nwhere formula_11 is an estimate of formula_3, that minimises an error metrics (here -norm, but other error metrics could be used) between formula_4 and formula_14. Note that a regularisation term can be introduced to prevent overfitting and to smooth noise whilst preserving edges. \nIterative methods can be implemented as follows:\n\nThe pseudocode below is a step-by-step description of the Fly Algorithm for tomographic reconstruction. The algorithm follows the steady-state paradigm. For illustrative purposes, advanced genetic operators, such as mitosis, dual mutation, etc. are ignored. A JavaScript implementation can be found on Fly4PET.\n\nIn this example, an input image is to be approximated by a set of tiles (for example as in an ancient mosaic). A tile has an orientation (angle θ), a three colour components (R, G, B), a size (w, h) and a position (x, y, z). If there are \"N\" tiles, there are 9\"N\" unknown floating point numbers to guess. In other words for 5,000 tiles, there are 45,000 numbers to find. Using a classical evolutionary algorithm where the answer of the optimisation problem is the best individual, the genome of an individual would made of 45,000 genes. This approach would be extremely costly in term of complexity and computing time. The same applies for any classical optimisation algorithm. Using the Fly Algorithm, every individual mimics a tile and can be individually evaluated using its local fitness to assess its contribution to the population's performance (the global fitness). Here an individual has 9 genes instead of 9\"N\", and there are \"N\" individuals. It can be solved as a reconstruction problem as follows:\n\nformula_15\n\nwhere formula_16 is the input image, formula_17 and formula_18 are the pixel coordinates along the horizontal and vertical axis respectively, formula_19 and formula_20 are the image width and height in number of pixels respectively, formula_21 is the fly population, and formula_6 is a projection operator that creates an image from flies. This projection operator formula_6 can take many forms. In her work, Z. Ali Aboodd uses OpenGL to generate different effects (e.g. mosaics, or spray paint). For speeding up the evaluation of the fitness functions, OpenCL is used too.\nThe algorithm starts with a population formula_21 that is randomly generated (see Line 3 in the algorithm above). formula_21 is then assessed using the global fitness to compute formula_26 (see Line 10). formula_27 is an error metrics, it has to be minimised.\n\n"}
{"id": "6765164", "url": "https://en.wikipedia.org/wiki?curid=6765164", "title": "Gabor atom", "text": "Gabor atom\n\nIn applied mathematics, Gabor atoms, or Gabor functions, are functions used in the analysis proposed by Dennis Gabor in 1946 in which a family of functions is built from translations and modulations of a generating function.\n\nIn 1946, Dennis Gabor suggested the idea of using a granular system to produce sound. In his work, Gabor discussed the problems with Fourier analysis. Although he found the mathematics to be correct, it did not reflect the behaviour of sound in the world, because sounds, such as the sound of a siren, have variable frequencies over time. Another problem was the underlying supposition, as we use sine waves analysis, that the signal under concern has infinite duration even though sounds in real life have limited duration – see time–frequency analysis. Gabor applied ideas from quantum physics to sound, allowing an analogy between sound and quanta. He proposed a mathematical method to reduce Fourier analysis into cells. His research aimed at the information transmission through communication channels. Gabor saw in his atoms a possibility to transmit the same information but using less data. Instead of transmitting the signal itself it would be possible to transmit only the coefficients which represent the same signal using his atoms.\n\nThe Gabor function is defined by\n\nwhere \"a\" and \"b\" are constants and \"g\" is a fixed function in \"L\"(R), such that ||\"g\"|| = 1. Depending on formula_2, formula_3, and formula_4, a Gabor system may be a basis for \"L\"(R), which is defined by translations and modulations. This is similar to a wavelet system, which may form a basis through dilating and translating a mother wavelet.\n\n\n\n"}
{"id": "51516730", "url": "https://en.wikipedia.org/wiki?curid=51516730", "title": "Gould's sequence", "text": "Gould's sequence\n\nGould's sequence is an integer sequence named after Henry W. Gould that counts the odd numbers in each row of Pascal's triangle. It consists only of powers of two, and begins:\nFor instance, the sixth number in the sequence is 4, because there are four odd numbers in the sixth row of Pascal's triangle (the four bold numbers in the sequence 1, 5, 10, 10, 5, 1).\n\nThe th value in the sequence (starting from ) gives the highest power of 2 that divides the central binomial coefficient formula_1, and it gives the numerator of formula_2 (expressed as a fraction in lowest terms).\nGould's sequence also gives the number of live cells in the th generation of the Rule 90 cellular automaton starting from a single live cell.\nIt has a characteristic growing sawtooth shape that can be used to recognize physical processes that behave similarly to Rule 90.\n\nThe binary logarithms (exponents in the powers of two) of Gould's sequence themselves form an integer sequence,\nin which the th value gives the number of nonzero bits in the binary representation of the number , sometimes written in mathematical notation as formula_3. Equivalently, the th value in Gould's sequence is\nTaking the sequence of exponents modulo two gives the Thue–Morse sequence.\n\nThe partial sums of Gould's sequence,\ncount all odd numbers in the first rows of Pascal's triangle. These numbers grow proportionally to formula_5,\nbut with a constant of proportionality that oscillates between 0.812556... and 1, periodically as a function of .\n\nThe first values in Gould's sequence may be constructed by recursively constructing the first values, and then concatenating the doubles of the first values. For instance, concatenating the first four values 1, 2, 2, 4 with their doubles 2, 4, 4, 8 produces the first eight values. Because of this doubling construction, the first occurrence of each power of two in this sequence is at position .\n\nGould's sequence, the sequence of its exponents, and the Thue–Morse sequence are all self-similar: they have the property that the subsequence of values at even positions in the whole sequence equals the original sequence, a property they also share with some other sequences such as Stern's diatomic sequence. In Gould's sequence, the values at odd positions are double their predecessors, while in the sequence of exponents, the values at odd positions are one plus their predecessors.\n\nThe sequence is named after Henry W. Gould, who studied it in the early 1960s. However, the fact that these numbers are powers of two, with the exponent of the th number equal to the number of ones in the binary representation of , was already known to J. W. L. Glaisher in 1899.\n\nProving that the numbers in Gould's sequence are powers of two was given as a problem in the 1956 William Lowell Putnam Mathematical Competition.\n"}
{"id": "45717431", "url": "https://en.wikipedia.org/wiki?curid=45717431", "title": "Hamid Naderi Yeganeh", "text": "Hamid Naderi Yeganeh\n\nHamid Naderi Yeganeh (; born July 26, 1990 in Iran) is an Iranian mathematical artist. He is known for using mathematical formulas to create drawings of real-life objects, intricate illustrations, animations, fractals and tessellations. His artwork \"9,000 Ellipses\" was used as the background cover image of \"The American Mathematical Monthly – November 2017\".\n\nNaderi Yeganeh has introduced two methods to draw real-life objects with mathematical formulas. In the first method, he creates tens of thousands of computer-generated mathematical figures to find a few interesting shapes accidentally. For example, by using this method, he found some shapes that resemble birds, fishes and sailing boats. In the second method, he draws a real life object with a step-by-step process. In each step, he tries to find out which mathematical formulas will produce the drawing. For example, by using this method, he drew birds in flight, butterflies, human faces and plants using trigonometric functions.\n\nHe has designed some fractals and tessellations inspired by the continents. For example, in 2015, he described the fractal Africa with an Africa-like octagon and its lateral inversion.\n\nNaderi Yeganeh believes that there are an infinite number of ways of using mathematical tools in art. He says, \"I don’t think computer-made art clashes with human creativity, but it can change the role of artists.”\n\nNaderi Yeganeh received his bachelor's degree in mathematics from the University of Qom. He won a gold medal at the 38th Iranian Mathematical Society’s Mathematics Competition in May 2014 and a silver medal at the 39th IMS’s Mathematics Competition in May 2015.\n\n\nBelow are some examples of Yeganeh's mathematical figures:\n"}
{"id": "9427744", "url": "https://en.wikipedia.org/wiki?curid=9427744", "title": "Isogonal", "text": "Isogonal\n\nIsogonal is a mathematical term which means \"having similar angles\". It occurs in several contexts:\n\n\nAn Isogonal is also the name for a line connecting points at which the magnetic declination is the same.\n"}
{"id": "296472", "url": "https://en.wikipedia.org/wiki?curid=296472", "title": "Lov Grover", "text": "Lov Grover\n\nLov Kumar Grover (born 1961) is an Indian-American computer scientist. He is the originator of the Grover database search algorithm used in quantum computing. Grover's 1996 algorithm won renown as the second major algorithm proposed for quantum computing (after Shor's 1994 algorithm), and in 2017 was finally implemented in a scalable physical quantum system. Grover's algorithm has been the subject of numerous popular science articles. Grover has been ranked as the 9th most prominent computer scientist from India.\n\nGrover received his bachelor's degree from the Indian Institute of Technology, Delhi in 1981 and his PhD in Electrical engineering from Stanford University in 1985. He then went to Bell Laboratories, where he worked for an assistant professor at Cornell University from 1987 to 1995.\n\n"}
{"id": "24971513", "url": "https://en.wikipedia.org/wiki?curid=24971513", "title": "Maximal common divisor", "text": "Maximal common divisor\n\nIn abstract algebra, particularly ring theory, maximal common divisors are an abstraction of the number theory concept of greatest common divisor (GCD). This definition is slightly more general than GCDs, and may exist in rings in which GCDs do not. Halter-Koch (1998) provides the following definition.\n\n\"d\" ∈ \"H\" is a maximal common divisor of a subset, \"B\" ⊂ \"H\", if the following criteria are met:\n"}
{"id": "44926697", "url": "https://en.wikipedia.org/wiki?curid=44926697", "title": "Michelle L. Wachs", "text": "Michelle L. Wachs\n\nMichelle Lynn Wachs is an American mathematician who specializes in algebraic combinatorics and works as a professor of mathematics at the University of Miami.\n\nWachs and her advisor Adriano Garsia are the namesakes of the Garsia–Wachs algorithm for optimal binary search trees, which they published in 1977.\nShe is also known for her research on shellings for simplicial complexes, partially ordered sets, and Coxeter groups, and on random permutation statistics and set partition statistics.\n\nWachs earned her doctorate in 1977 from the University of California, San Diego, under the supervision of Adriano Garsia. Her dissertation was \"Discrete Variational Techniques in Finite Mathematics\".\n\nIn 2012 Wachs became one of the inaugural fellows of the American Mathematical Society. In 2013 she and her husband, mathematician Gregory Galloway (the chair of the mathematics department at Miami) were recognized as Simons Fellows. A conference in her honor was held in January 2015 at the University of Miami.\n"}
{"id": "16520361", "url": "https://en.wikipedia.org/wiki?curid=16520361", "title": "Microsoft Mathematics", "text": "Microsoft Mathematics\n\nMicrosoft Mathematics (formerly \"Microsoft Math\") is a freely downloadable educational program, designed for Microsoft Windows, that allows users to solve math and science problems. Developed and maintained by Microsoft, it is primarily targeted at students as a learning tool.\n\nA related freeware add-in, called \"Microsoft Mathematics Add-In for Word and OneNote\", is also available from Microsoft and offers comparable functionality (Word 2007 or higher is required).\n\nMicrosoft Math has received 2008 Award of Excellence from Tech & Learning Magazine.\n\nMicrosoft Math contains features that are designed to assist in solving mathematics, science, and tech-related problems, as well as to educate the user. The application features such tools as a graphing calculator and a unit converter. It also includes a triangle solver, and an equation solver that provides step-by-step solutions to each problem.\n\nThe standalone version of Microsoft Math 3.0 also has support for calculus and Ink Handwriting, allowing the user to write out problems by hand and have them recognized by Microsoft Math.\n\n\nSystem requirements for Microsoft Math are:\nIn 2015 Microsoft released a similar branded mobile application for Windows Phone named Microsoft Math (alternatively called \"Nokia Mobile-Mathematics\" or \"Nokia Momaths\") specifically for South African and Tanzanian students which has no relation with the earlier Microsoft Mathematics product.\n\n\n"}
{"id": "56087242", "url": "https://en.wikipedia.org/wiki?curid=56087242", "title": "Moritz Epple", "text": "Moritz Epple\n\nMoritz Epple (7 May 1960, Stuttgart) is a German mathematician and historian of science.\n\nEpple studied mathematics, philosophy and physics at the University of Tübingen, where he received in 1987 his bachelor's degree (\"Diplom\") in physics and in 1991 his Ph.D. (\"Promotion)\" in mathematical physics. He then became an assistant in the history of mathematics and natural sciences at the University of Mainz, where he received in 1998 his \"Habilitation\". From 2001 to 2003 he was the head of the department of history of the natural sciences and technology at the University of Stuttgart. Since 2003 he has been a professor at the Goethe University of Frankfurt and head of the working group for the modern history of science at the historic seminary there. He was a visiting professor at several academic institutions including the Dibner Institute for the History of Science and Technology of Massachusetts Institute of Technology (MIT) and at the Max-Planck-Gesellschaft in Berlin.\n\nHis habilitation thesis on the history of knot theory was published in 1999 under the title \"Die Entstehung der Knotentheorie – Kontexte und Konstruktionen einer modernen mathematischen Theorie\" (with 2nd edition in 2013). He also wrote the article on knot theory in the book \"History of Topology\" edited by Ioan James. He has done research on the history of mathematical analysis, for example the article \"Geschichte der Grundlagen der Analysis 1860 – 1930\" in the 1999 book \"Geschichte der Analysis\" edited by Jahnke; Epple wrote on, among other topics, Luitzen Egbertus Jan Brouwer and applied mathematical research in Germany during WW II. His historical research has also dealt with the epistemological works of Felix Hausdorff and Jewish mathematicians in German-speaking academic culture.\n\nFrom 2000 to 2001 he was a Heisenberg Fellow. He was a board member of the \"Deutschen Gesellschaft für Geschichte der Medizin, Naturwissenschaft und Technik\" and a co-editor of \"NTM Zeitschrift für Geschichte der Wissenschaften, Technik und Medizin\". Since 2013 he has been a co-editor of the journal \"Science in Context\". In 2002 at the ICM in Beijing he was an Invited Speaker with talk \"From Quaternions to cosmology – spaces of constant curvature 1873–1925\". In 2015 Epple with his Frankfurt team received the media prize of the Deutsche Mathematiker-Vereinigung for the exhibition \"Transcending Tradition\". On 26 November 2016, Epple was elected a member of the Deutsche Akademie der Naturforscher Leopoldina.\n\n\n\n"}
{"id": "25976893", "url": "https://en.wikipedia.org/wiki?curid=25976893", "title": "Multiplicity-one theorem", "text": "Multiplicity-one theorem\n\nIn the mathematical theory of automorphic representations, a multiplicity-one theorem is a result about the representation theory of an adelic reductive algebraic group. The multiplicity in question is the number of times a given abstract group representation is realised in a certain space, of square-integrable functions, given in a concrete way.\n\nA multiplicity one theorem may also refer to a result about the restriction of a representation of a group G to a subgroup H. In that context, the pair (G, H) is called a strong Gelfand pair. \n\nLet \"G\" be a reductive algebraic group over a number field \"K\" and let A denote the adeles of \"K\". Let \"Z\" denote the centre of \"G\" and let ω be a continuous unitary character from \"Z\"(\"K\")\\Z(A) to C. Let \"L\"(\"G\"(\"K\")/\"G\"(A), ω) denote the space of cusp forms with central character ω on \"G\"(A). This space decomposes into a direct sum of Hilbert spaces\nwhere the sum is over irreducible subrepresentations and \"m\" are non-negative integers.\n\nThe group of adelic points of \"G\", \"G\"(A), is said to satisfy the multiplicity-one property if any smooth irreducible admissible representation of \"G\"(A) occurs with multiplicity at most one in the space of cusp forms of central character ω, i.e. \"m\" is 0 or 1 for all such π.\n\nThe fact that the general linear group, \"GL\"(\"n\"), has the multiplicity-one property was proved by for \"n\" = 2 and independently by and for \"n\" > 2 using the uniqueness of the Whittaker model. Multiplicity-one also holds for \"SL\"(2), but not for \"SL\"(\"n\") for \"n\" > 2 .\n\nThe strong multiplicity one theorem of and states that two cuspidal automorphic representations of the general linear group are isomorphic if their local components are isomorphic for all but a finite number of places.\n\n"}
{"id": "995746", "url": "https://en.wikipedia.org/wiki?curid=995746", "title": "Multiplier (Fourier analysis)", "text": "Multiplier (Fourier analysis)\n\nIn Fourier analysis, a multiplier operator is a type of linear operator, or transformation of functions. These operators act on a function by altering its Fourier transform. Specifically they multiply the Fourier transform of a function by a specified function known as the multiplier or symbol. Occasionally, the term \"multiplier operator\" itself is shortened simply to \"multiplier\". In simple terms, the multiplier reshapes the frequencies involved in any function. This class of operators turns out to be broad: general theory shows that a translation-invariant operator on a group which obeys some (very mild) regularity conditions can be expressed as a multiplier operator, and conversely. Many familiar operators, such as translations and differentiation, are multiplier operators, although there are many more complicated examples such as the Hilbert transform.\nIn signal processing, a multiplier operator is called a \"filter\", and the multiplier is the filter's frequency response (or transfer function).\n\nIn the wider context, multiplier operators are special cases of spectral multiplier operators, which arise from the functional calculus of an operator (or family of commuting operators). They are also special cases of pseudo-differential operators, and more generally Fourier integral operators. There are natural questions in this field that are still open, such as characterizing the \"L\" bounded multiplier operators (see below).\n\nMultiplier operators are unrelated to Lagrange multipliers, except that they both involve the multiplication operation.\n\n\"For the necessary background on the Fourier transform, see that page. Additional important background may be found on the pages operator norm and \"L\" space.\"\n\nIn the setting of periodic functions defined on the unit circle, the Fourier transform of a function is simply the sequence of its Fourier coefficients. To see that differentiation can be realized as multiplier, consider the Fourier series for the derivative of a periodic function formula_1 After using integration by parts in the definition of the Fourier coefficient we have that\n\nSo, formally, it follows that the Fourier series for the derivative is simply the Fourier series for formula_3 multiplied by a factor formula_4. This is the same as saying that differentiation is a multiplier operator with multiplier \"in\".\n\nAn example of a multiplier operator acting on functions on the real line is the Hilbert transform. It can be shown that the Hilbert transform is a multiplier operator whose multiplier is given by the m(\"ξ\") = −\"i\" sgn(\"ξ\"), where sgn is the signum function.\n\nFinally another important example of a multiplier is the characteristic function of the unit cube in formula_5 which arises in the study of \"partial sums\" for the Fourier transform (see Convergence of Fourier series).\n\nMultiplier operators can be defined on any group \"G\" for which the Fourier transform is also defined (in particular, on any locally compact abelian group). The general definition is as follows. If formula_6 is a sufficiently regular function, let formula_7 denote its Fourier transform (where formula_8 is the Pontryagin dual of \"G\"). Let formula_9 denote another function, which we shall call the \"multiplier\". Then the multiplier operator formula_10 associated to this symbol \"m\" is defined via the formula\n\nIn other words, the Fourier transform of \"Tf\" at a frequency ξ is given by the Fourier transform of \"f\" at that frequency, multiplied by the value of the multiplier at that frequency. This explains the terminology \"multiplier\".\n\nNote that the above definition only defines Tf implicitly; in order to recover \"Tf\" explicitly one needs to invert the Fourier transform. This can be easily done if both \"f\" and \"m\" are sufficiently smooth and integrable. One of the major problems in the subject is to determine, for any specified multiplier \"m\", whether the corresponding Fourier multiplier operator continues to be well-defined when \"f\" has very low regularity, for instance if it is only assumed to lie in an \"L\" space. See the discussion on the \"boundedness problem\" below. As a bare minimum, one usually requires the multiplier \"m\" to be bounded and measurable; this is sufficient to establish boundedness on formula_12 but is in general not strong enough to give boundedness on other spaces.\n\nOne can view the multiplier operator \"T\" as the composition of three operators, namely the Fourier transform, the operation of pointwise multiplication by \"m\", and then the inverse Fourier transform. Equivalently, \"T\" is the conjugation of the pointwise multiplication operator by the Fourier transform. Thus one can think of multiplier operators as operators which are diagonalized by the Fourier transform.\n\nWe now specialize the above general definition to specific groups \"G\". First consider the unit circle formula_13 functions on \"G\" can thus be thought of as 2π-periodic functions on the real line. In this group, the Pontryagin dual is the group of integers, formula_14 The Fourier transform (for sufficiently regular functions \"f\") is given by\n\nand the inverse Fourier transform is given by\n\nA multiplier in this setting is simply a sequence formula_17 of numbers, and the operator formula_10 associated to this multiplier is then given by the formula\n\nat least for sufficiently well-behaved choices of the multiplier formula_17 and the function \"f\".\n\nNow let \"G\" be a Euclidean space formula_21. Here the dual group is also Euclidean, formula_22 and the Fourier and inverse Fourier transforms are given by the formulae\n\nA multiplier in this setting is a function formula_25 and the associated multiplier operator formula_10 is defined by\n\nagain assuming sufficiently strong regularity and boundedness assumptions on the multiplier and function.\n\nIn the sense of distributions, there is no difference between multiplier operators and convolution operators; every multiplier \"T\" can also be expressed in the form \"Tf\" = \"f*K\" for some distribution \"K\", known as the \"convolution kernel\" of \"T\". In this view, translation by an amount \"x\" is convolution with a Dirac delta function δ(· − \"x\"), differentiation is convolution with δ'. Further examples are given in the table below.\n\nThe following table shows some common examples of multiplier operators on the unit circle formula_28\n\nThe following table shows some common examples of multiplier operators on Euclidean space formula_21.\n\nThe map formula_30 is a homomorphism of C*-algebras. This follows because the sum of two multiplier operators formula_31 and formula_32 is a multiplier operators with multiplier formula_33, the composition of these two multiplier operators is a multiplier operator with multiplier formula_34 and the adjoint of a multiplier operator formula_31 is another multiplier operator with multiplier formula_36.\n\nIn particular, we see that any two multiplier operators commute with each other. It is known that multiplier operators are translation-invariant. Conversely, one can show that any translation-invariant linear operator which is bounded on \"L\"(\"G\") is a multiplier operator.\n\nThe \"L\" boundedness problem (for any particular \"p\") for a given group \"G\" is, stated simply, to identify the multipliers \"m\" such that the corresponding multiplier operator is bounded from \"L\"(\"G\") to \"L\"(\"G\"). Such multipliers are usually simply referred to as \"\"L\" multipliers\". Note that as multiplier operators are always linear, such operators are bounded if and only if they are continuous. This problem is considered to be extremely difficult in general, but many special cases can be treated. The problem depends greatly on \"p\", although there is a duality relationship: if formula_37 and 1 ≤ \"p\", \"q\" ≤ ∞, then a multiplier operator is bounded on \"L\" if and only if it is bounded on \"L\".\n\nThe Riesz-Thorin theorem shows that if a multiplier operator is bounded on two different \"L\" spaces, then it is also bounded on all intermediate spaces. Hence we get that the space of multipliers is smallest for \"L\" and \"L\" and grows as one approaches \"L\", which has the largest multiplier space.\n\nThis is the easiest case. Parseval's theorem allows to solve this problem completely and obtain that a function \"m\" is an \"L\"(\"G\") multiplier if and only if it is bounded and measurable.\n\nThis case is more complicated than the Hilbertian (\"L\") case, but is fully resolved. The following is true:\n\nTheorem: \"In the euclidean space formula_5 a function formula_39 is an\" \"L\" \"multiplier (equivalently an \"L\" multiplier) if and only if there exists a finite Borel measure μ such that\" \"m\" \"is the Fourier transform of μ.\"\n\nIn this general case, necessary and sufficient conditions for boundedness have not been established, even for Euclidean space or the unit circle. However, several necessary conditions and several sufficient conditions are known. For instance it is known that in order for a multiplier operator to be bounded on even a single \"L\" space, the multiplier must be bounded and measurable (this follows from the characterisation of \"L\" multipliers above and the inclusion property). However, this is not sufficient except when \"p\" = 2.\n\nResults that give sufficient conditions for boundedness are known as multiplier theorems. Two such results are given below.\n\nLet formula_40 be a bounded function that is continuously differentiable on every set of the form formula_41 for formula_42 and has derivative such that\n\nThen \"m\" is an \"L\" multiplier for all 1 < \"p\" < ∞.\n\nLet \"m\" be a bounded function on formula_5 which is smooth except possibly at the origin, and such that the function formula_45 is bounded for all integers formula_46: then \"m\" is an \"L\" multiplier for all 1 < \"p\" < ∞.\n\nThis is a special case of the Hörmander-Mikhlin multiplier theorem.\n\nThe proofs of these two theorems are fairly tricky, involving techniques from Calderón–Zygmund theory and the Marcinkiewicz interpolation theorem: for the original proof, see or .\n\nTranslations are bounded operators on any \"L\". Differentiation is not bounded on any \"L\". The Hilbert transform is bounded only for \"p\" strictly between 1 and ∞. The fact that it is unbounded on \"L\" is easy, since it is well known that the Hilbert transform of a step function is unbounded. Duality gives the same for \"p\" = 1. However, both the Marcinkiewicz and Mikhlin multiplier theorems show that the Hilbert transform is bounded in \"L\" for all 1 < \"p\" < ∞.\n\nAnother interesting case on the unit circle is when the sequence formula_47 that is being proposed as a multiplier is constant for \"n\" in each of the sets formula_48 and formula_49 From the Marcinkiewicz multiplier theorem (adapted to the context of the unit circle) we see that any such sequence (also assumed to be bounded, of course) is a multiplier for every 1 < \"p\" < ∞.\n\nIn one dimension, the disk multiplier operator formula_50(see table above) is bounded on \"L\" for every 1 < \"p\" < ∞. However, in 1972, Charles Fefferman showed the surprising result that in two and higher dimensions the disk multiplier operator formula_50 is unbounded on \"L\" for every \"p\" ≠ 2. The corresponding problem for Bochner–Riesz multipliers is only partially solved; see also Bochner–Riesz operator and Bochner–Riesz conjecture.\n\n\n"}
{"id": "9190726", "url": "https://en.wikipedia.org/wiki?curid=9190726", "title": "Néron model", "text": "Néron model\n\nIn algebraic geometry, the Néron model (or Néron minimal model, or minimal model)\nfor an abelian variety \"A\" defined over the field of fractions \"K\" of a Dedekind domain \"R\" is the \"push-forward\" of \"A\" from Spec(\"K\") to Spec(\"R\"), in other words the \"best possible\" group scheme \"A\" defined over \"R\" corresponding to \"A\".\n\nThey were introduced by for abelian varieties over the quotient field of a Dedekind domain \"R\" with perfect residue fields, and extended this construction to semiabelian varieties over all Dedekind domains.\n\nSuppose that \"R\" is a Dedekind domain with field of fractions \"K\", and suppose that \"A\" is a smooth separated scheme over \"K\" (such as an abelian variety). Then a Néron model of \"A\" is defined to be a smooth separated scheme \"A\" over \"R\" with fiber \"A\" that is universal in the following sense. \nIn particular, the canonical map formula_1 is an isomorphism. If a Néron model exists then it is unique up to unique isomorphism.\n\nIn terms of sheaves, any scheme \"A\" over Spec(\"K\") represents a sheaf on the category of schemes smooth over Spec(\"K\") with the smooth Grothendieck topology, and this has a pushforward by the injection map from Spec(\"K\") to Spec(\"R\"), which is a sheaf over Spec(\"R\"). If this pushforward is representable by a scheme, then this scheme is the Néron model of \"A\".\n\nIn general the scheme \"A\" need not have any Néron model. \nFor abelian varieties \"A\" Néron models exist and are unique (up to unique isomorphism) and are commutative quasi-projective group schemes over \"R\". The fiber of a Néron model over a closed point of Spec(\"R\") is a smooth commutative algebraic group, but need not be an abelian variety: for example, it may be disconnected or a torus. Néron models exist as well for certain commutative groups other than abelian varieties such as tori, but these are only locally of finite type. Néron models do not exist for the additive group.\n\n\nThe Néron model of an elliptic curve \"A\" over \"K\" can be constructed as follows. First form the minimal model over \"R\" in the sense of algebraic (or arithmetic) surfaces. This is a regular proper surface over \"R\" but is not in general smooth over \"R\" or a group scheme over \"R\". Its subscheme of smooth points over \"R\" is the Néron model, which is a smooth group scheme over \"R\" but not necessarily proper over \"R\". The fibers in general may have several irreducible components, and to form the Néron model one discards all multiple components, all points where two components intersect, and all singular points of the components.\n\nTate's algorithm calculates the special fiber of the Néron model of an elliptic curve, or more precisely the fibers of the minimal surface containing the Néron model.\n\n"}
{"id": "44075827", "url": "https://en.wikipedia.org/wiki?curid=44075827", "title": "Pathway Systems", "text": "Pathway Systems\n\nPathway Systems International Inc. is an American corporation headquartered in Orlando, Florida, \nthat designs, develops and sells relationship modeling software.\n\nAlthough capable of modeling any system of inter-related objects, the software sold under the name Blueprints™, is focused primarily on the Information Technology (IT) segment of the market. Any physical or virtual part of an IT system can be visualized with relationship dependencies exposed in a graphical context.\n\nThe philosophy of Blueprints™ is to effect assemblage around a common model to transform \"tribal knowledge\" into shared awareness. In doing so, Blueprints™ provides both local and remote individuals the ability to share the same perspective, seeing each other's actions, resulting in collaboration in real-time.\n\nThe company was incorporated by Daniel Evenson in 2008. The genesis of Pathway Systems was the need within the IT industry for a system of visual documentation that was fast, easy to use, and easy to maintain. \nThe IT Dependency Mapping functionality of Blueprints™ is delivered as a SaaS or virtual appliance.\n\nAlthough Dependency Modeling and Dependency Mapping are used interchangeably within the IT industry, modeling in the case of Blueprints™ connotes a more dynamic approach to representing business process as effected by software and hardware systems. An article describing one small part of the advantages of good dependency modeling is covered within the Fall 2009 Disaster Recovery Journal.\n"}
{"id": "54594963", "url": "https://en.wikipedia.org/wiki?curid=54594963", "title": "Perfect obstruction theory", "text": "Perfect obstruction theory\n\nIn algebraic geometry, given a Deligne–Mumford stack \"X\", a perfect obstruction theory for \"X\" consists of:\n\nThe notion was introduced by for an application to the intersection theory on moduli stacks; in particular, to define a virtual fundamental class.\n\nConsider a regular embedding formula_7 fitting into a cartesian square\nwhere formula_9 are smooth. Then, the complex\nforms a perfect obstruction theory for \"X\". The map comes from the composition\nThis is a perfect obstruction theory because the complex comes equipped with a map to formula_13 coming from the maps formula_14 and formula_15. Note that the associated virtual fundamental class is formula_16\nConsider a smooth projective variety formula_17. If we set formula_18, then the perfect obstruction theory in formula_19 is\nand the associated virtual fundamental class is\nIn particular, if formula_22 is a smooth local complete intersection then the perfect obstruction theory is the cotangent complex (which is the same as the truncated cotangent complex).\n\nThe previous construction works too with Deligne–Mumford stacks.\n\nBy definition, a symmetric obstruction theory is a perfect obstruction theory together with nondegenerate symmetric bilinear form.\n\nExample: Let \"f\" be a regular function on a smooth variety (or stack). Then the set of critical points of \"f\" carries a symmetric obstruction theory in a canonical way.\n\nExample: Let \"M\" be a complex symplectic manifold. Then the (scheme-theoretic) intersection of Lagrangian submanifolds of \"M\" carries a canonical symmetric obstruction theory.\n\n\n"}
{"id": "23032386", "url": "https://en.wikipedia.org/wiki?curid=23032386", "title": "Plumbing drawing", "text": "Plumbing drawing\n\nA plumbing drawing, a type of technical drawing, shows the system of piping for fresh water going into the building and waste going out, both solid and liquid.\nIt also includes fuel gas drawings. Mainly plumbing drawing consist of Water supply system drawings, Drainage system drawings, Irrigation system drawings, Storm water system drawings. In water supply system drawing there will be hot water piping and cold water piping and hot water return piping also. \nIn drainage system drawings there will be waste piping , Soil piping and vent piping. \nThe set of drawing of each system like water supply , drainage etc is consist of Plans, Riser diagram, Installation details, Legends, Notes. Every pipes should me marked with pipe sizes. If the drawing is detailed , fixture units also should be marked along with the pipe. If it is shop drawing, sections also should be shown where there pipes are crossing. In shop drawings pipe sizes should be marked with the text and size should be shown with double line. Each pipes with different purposes will be displayed with different colors for ease of understanding. Drainage pipes should be shown with slope. For water supply , pump capacity and number of pumps will be attached as drawing file. For drainage, manhole schedule which consist of each manhole name, Invert level, Cover level , Depth are also attached as drawing file.\n"}
{"id": "20542241", "url": "https://en.wikipedia.org/wiki?curid=20542241", "title": "Projection method (fluid dynamics)", "text": "Projection method (fluid dynamics)\n\nThe projection method is an effective means of numerically solving time-dependent incompressible fluid-flow problems. It was originally introduced by Alexandre Chorin in 1967\nas an efficient means of solving the incompressible Navier-Stokes equations. The key advantage of the projection method is that the computations of the velocity and the pressure fields are decoupled.\n\nThe algorithm of the projection method is based on the Helmholtz decomposition (sometimes called Helmholtz-Hodge decomposition) of any vector field into a solenoidal part and an irrotational part. Typically, the algorithm consists of two stages. In the first stage, an intermediate velocity that does not satisfy the incompressibility constraint is computed at each time step. In the second, the pressure is used to project the intermediate velocity onto a space of divergence-free velocity field to get the next update of velocity and pressure.\n\nThe theoretical background of projection type method is the decomposition theorem of Ladyzhenskaya sometimes referred to as Helmholtz–Hodge Decomposition or simply as Hodge decomposition. It states that the vector field formula_1 defined on a simply connected domain can be uniquely decomposed into a divergence-free (solenoidal) part formula_2 and an irrotational part formula_3.\n\nThus, \n\nsince formula_5 for some scalar function, formula_6. Taking the\ndivergence of equation yields\n\nThis is a Poisson equation for the scalar function formula_6. If the vector field formula_1 is known, the above equation can be solved for the scalar function formula_6 and the divergence-free part of formula_1 can be extracted using the relation\n\nThis is the essence of solenoidal projection method for solving incompressible\nNavier–Stokes equations.\n\nThe incompressible Navier-Stokes equation (differential form of momentum equation) may be written as\n\nIn Chorin's original version of the projection method, one first computes an intermediate velocity, formula_14, explicitly using the momentum equation by ignoring the pressure gradient term:\n\nwhere formula_16 is the velocity at formula_17 time step. In the second half of the algorithm, the \"projection\" step, we correct the intermediate velocity to obtain the final solution of the time step formula_18:\n\nOne can rewrite this equation in the form of a time step as\n\nto make clear that the algorithm is really just an operator splitting approach in which one considers the viscous forces (in the first half step) and the pressure forces (in the second half step) separately.\n\nComputing the right-hand side of the second half step requires knowledge of the pressure, formula_21, at theformula_22 time level. This is obtained by taking the divergence and requiring that formula_23, which is the divergence (continuity) condition, thereby deriving the following Poisson equation for formula_24,\nIt is instructive to note that the equation written as\nis the standard Hodge decomposition if boundary condition for formula_21 on the domain boundary, formula_28 are formula_29. In practice, this condition is responsible for the errors this method shows close to the boundary of the domain since the real pressure (i.e., the pressure in the exact solution of the Navier-Stokes equations) does not satisfy such boundary conditions.\n\nFor the explicit method, the boundary condition for formula_14 in equation (1) is natural. If formula_31 on formula_28, is prescribed, then the space of divergence-free vector fields will be orthogonal to the space of irrotational vector fields, and from equation (2) one has\nThe explicit treatment of the boundary condition may be circumvented by using a staggered grid and requiring that formula_34 vanish at the pressure nodes that are adjacent to the boundaries.\n\nA distinguishing feature of Chorin's projection method is that the velocity field is forced to satisfy a discrete continuity constraint at the end of each time step.\n\nTypically the projection method operates as a two-stage fractional step scheme, a method which uses multiple calculation steps for each numerical time-step. In many projection algorithms, the steps are split as follows:\n\n"}
{"id": "159735", "url": "https://en.wikipedia.org/wiki?curid=159735", "title": "Quasi-empirical method", "text": "Quasi-empirical method\n\nQuasi-empirical methods are methods applied in science and mathematics to achieve epistemology similar to that of empiricism (thus \"quasi- + empirical\") when experience cannot falsify the ideas involved. Empirical research relies on empirical evidence, and its empirical methods involve experimentation and disclosure of apparatus for reproducibility, by which scientific findings are validated by other scientists. Empirical methods are studied extensively in the philosophy of science, but they cannot be used directly in fields whose hypotheses cannot be falsified by real experiment (for example, mathematics, philosophy, theology, and ideology). Because of such empirical limits in science, the scientific method must rely not only on empirical methods but sometimes also on quasi-empirical ones. The prefix \"quasi-\" came to denote methods that are \"almost\" or \"socially approximate\" an ideal of truly empirical methods.\n\nIt is unnecessary to find all counterexamples to a theory; all that is required to disprove a theory logically is one counterexample. The converse does not prove a theory; Bayesian inference simply makes a theory more likely, by weight of evidence.\n\nOne can argue that no science is capable of finding all counter-examples to a theory, therefore, no science is strictly empirical, it's all quasi-empirical. But usually, the term \"quasi-empirical\" refers to the means of choosing problems to focus on (or ignore), selecting prior work on which to build an argument or proof, notations for informal claims, peer review and acceptance, and incentives to discover, ignore, or correct errors. These are common to both science and mathematics, and do not include experimental method.\n\nAlbert Einstein's discovery of the general relativity theory relied upon thought experiments and mathematics. Empirical methods only became relevant when confirmation was sought. Furthermore, some empirical confirmation was found only some time after the general acceptance of the theory.\n\nThought experiments are almost standard procedure in philosophy, where a conjecture is tested out in the imagination for possible effects on experience; when these are thought to be implausible, unlikely to occur, or not actually occurring, then the conjecture may be either rejected or amended. Logical positivism was a perhaps extreme version of this practice, though this claim is open to debate.\n\nPost-20th-century philosophy of mathematics is mostly concerned with quasi-empirical mathematical methods, especially as reflected in the actual mathematical practice of working mathematicians. \n\n"}
{"id": "18422596", "url": "https://en.wikipedia.org/wiki?curid=18422596", "title": "Second moment method", "text": "Second moment method\n\nIn mathematics, the second moment method is a technique used in probability theory and analysis to show that a random variable has positive probability of being positive. More generally, the \"moment method\" consists of bounding the probability that a random variable fluctuates far from its mean, by using its moments.\n\nThe method is often quantitative, in that one can often deduce a lower bound on the probability that the random variable is larger than some constant times its expectation. The method involves comparing the second moment of random variables to the square of the first moment.\n\nThe first moment method is a simple application of Markov's inequality for integer-valued variables. For a non-negative, integer-valued random variable \"X\", we may want to prove that \"X\" = 0 with high probability. To obtain an upper bound for P(\"X\" > 0), and thus a lower bound for P(\"X\" = 0), we first note that since \"X\" takes only integer values, P(\"X\" > 0) = P(\"X\" ≥ 1). Since \"X\" is non-negative we can now apply Markov's inequality to obtain P(\"X\" ≥ 1) ≤ E[\"X\"]. Combining these we have P(\"X\" > 0) ≤ E[\"X\"]; the first moment method is simply the use of this inequality.\n\nIn the other direction, E[\"X\"] being \"large\" does not directly imply that P(\"X\" = 0) is small. However, we can often use the second moment to derive such a conclusion, using Cauchy–Schwarz inequality.\n\nTheorem: If \"X\" ≥ 0 is a random variable with\nfinite variance, then\n\nProof: Using the Cauchy–Schwarz inequality, we have\nSolving for formula_3, the desired inequality then follows. ∎\n\nThe method can also be used on distributional limits of random variables. Furthermore, the estimate of the previous theorem can be refined by means of the so-called Paley–Zygmund inequality. Suppose that \"X\" is a sequence of non-negative real-valued random variables which converge in law to a random variable \"X\". If there are finite positive constants \"c\", \"c\" such that\n\nhold for every \"n\", then it follows from the Paley–Zygmund inequality that for every \"n\" and θ in (0, 1)\n\nConsequently, the same inequality is satisfied by \"X\".\n\nThe Bernoulli bond percolation subgraph of a graph \"G\" at parameter \"p\" is a random subgraph obtained from \"G\" by deleting every edge of \"G\" with probability 1−\"p\", independently. The infinite complete binary tree \"T\" is an infinite tree where one vertex (called the root) has two neighbors and every other vertex has three neighbors. The second moment method can be used to show that at every parameter \"p\" ∈ (1/2, 1] with positive probability the connected component of the root in the percolation subgraph of \"T\" is infinite.\n\nLet \"K\" be the percolation component of the root, and let \"T\" be the set of vertices of \"T\" that are at distance \"n\" from the root. Let \"X\" be the number of vertices in \"T\" ∩ \"K\". To prove that \"K\" is infinite with positive probability, it is enough to show that formula_7 with positive probability. By the reverse Fatou lemma, it suffices to show that formula_8. The Cauchy–Schwarz inequality gives\nTherefore, it is sufficient to show that\nthat is, that the second moment is bounded from above by a constant times the first moment squared (and both are nonzero). In many applications of the second moment method, one is not able to calculate the moments precisely, but can nevertheless establish this inequality.\n\nIn this particular application, these moments can be calculated. For every specific \"v\" in \"T\",\n\nSince formula_12, it follows that\n\nwhich is the first moment. Now comes the second moment calculation.\n\nFor each pair \"v\", \"u\" in \"T\" let \"w(v, u)\" denote the vertex in \"T\" that is farthest away from the root and lies on the simple path in \"T\" to each of the two vertices \"v\" and \"u\", and let \"k(v, u)\" denote the distance from \"w\" to the root. In order for \"v\", \"u\" to both be in \"K\", it is necessary and sufficient for the three simple paths from \"w(v, u)\" to \"v\", \"u\" and the root to be in \"K\". Since the number of edges contained in the union of these three paths is 2\"n\" − \"k(v, u)\", we obtain\n\nThe number of pairs \"(v, u)\" such that \"k(v, u)\" = \"s\" is equal to formula_16, for \"s\" = 0, 1, ..., \"n\". Hence,\n\nwhich completes the proof.\n\n\n"}
{"id": "1187410", "url": "https://en.wikipedia.org/wiki?curid=1187410", "title": "Semidefinite embedding", "text": "Semidefinite embedding\n\nSemidefinite embedding (SDE) or maximum variance unfolding (MVU) is an algorithm in computer science that uses semidefinite programming to perform non-linear dimensionality reduction of high-dimensional vectorial input data. MVU can be viewed as a non-linear generalization of Principal component analysis.\n\nNon-linear dimensionality reduction algorithms attempt to map high-dimensional data onto a low-dimensional Euclidean vector space. Maximum variance Unfolding is a member of the manifold learning family, which also include algorithms such as isomap and locally linear embedding. In manifold learning, the input data is assumed to be sampled from a low dimensional manifold that is embedded inside of a higher-dimensional vector space. The main intuition behind MVU is to exploit the local linearity of manifolds and create a mapping that preserves local neighbourhoods at every point of the underlying manifold. \n\nMVU creates a mapping from the high dimensional input vectors to some low dimensional Euclidean vector space in the following steps:\n\nA neighbourhood graph is created. Each input is connected with its k-nearest input vectors (according to Euclidean distance metric) and all k-nearest neighbors are connected with each other. If the data is sampled well enough, the resulting graph is a discrete approximation of the underlying manifold. \n\nThe neighbourhood graph is \"unfolded\" with the help of semidefinite programming. Instead of learning the output vectors directly, the semidefinite programming aims to find an inner product matrix that maximizes the pairwise distances between any two inputs that are not connected in the neighbourhood graph while preserving the nearest neighbors distances. \n\nThe low-dimensional embedding is finally obtained by application of multidimensional scaling on the learned inner product matrix.\n\nThe steps of applying semidefinite programming followed by a linear dimensionality reduction step to recover a low-dimensional embedding into a Euclidean space were first proposed by Linial, London, and Rabinovich.\n\nLet formula_1 be the original input and formula_2 be the embedding. If formula_3 are two neighbors, then the local isometry constraint that needs to be satisfied is:\n\nLet formula_5 be the Gram matrices of formula_6 and formula_7 (i.e.: formula_8). We can express the above constraint for every neighbor points formula_3 in term of formula_5:\n\nIn addition, we also want to constrain the embedding formula_7 to center at the origin:\n\nformula_13\n\nAs described above, except the distances of neighbor points are preserved, the algorithm aims to maximize the pairwise distance of every pair of points. The objective function to be maximized is:\n\nformula_14\n\nIntuitively, maximizing the function above is equivalent to pulling the points as far away from each other as possible and therefore \"unfold\" the manifold. The local isometry constraint prevents the objective function from going to infinity. Proof:\n\nLet formula_15 where formula_16 if i and j are neighbors and formula_17 otherwise.\n\nSince the graph has N points, the distance between any two points formula_18. We can then bound the objective function as follow:\n\nThe objective function can be rewritten purely in the form of the Gram matrix:\n\nFinally, the optimization can be formulated as:\n\nMaximize formula_21\n\nSubject to formula_22 and\nformula_23 where formula_24 \n\nAfter the Gram matrix formula_25 is learned by semidefinite programming, the output formula_26 can be obtained via Cholesky decomposition. In particular, the Gram matrix can be written as formula_27 where formula_28 is the i-th element of eigenvector formula_29 of the eigenvalue formula_30.\n\nIt follows that the formula_31-th element of the output formula_32 is formula_33.\n\n\n\n"}
{"id": "372393", "url": "https://en.wikipedia.org/wiki?curid=372393", "title": "Semiperfect magic cube", "text": "Semiperfect magic cube\n\nIn mathematics, a semiperfect magic cube is a magic cube that is not a perfect magic cube, i.e., a magic cube for which the cross section diagonals do not necessarily sum up to the cube's magic constant.\n"}
{"id": "1609200", "url": "https://en.wikipedia.org/wiki?curid=1609200", "title": "Signedness", "text": "Signedness\n\nIn computing, signedness is a property of data types representing numbers in computer programs. A numeric variable is \"signed\" if it can represent both positive and negative numbers, and \"unsigned\" if it can only represent non-negative numbers (zero or positive numbers).\n\nAs \"signed\" numbers can represent negative numbers, they lose a range of positive numbers that can only be represented with \"unsigned\" numbers of the same size (in bits) because half the possible values are non-positive values (so if an 8-bit is signed, positive unsigned values 128 to 255 are gone while -128 to 127 are present). Unsigned variables can dedicate all the possible values to the positive number range.\n\nFor example, a two's complement signed 16-bit integer can hold the values −32768 to 32767 inclusively, while an unsigned 16 bit integer can hold the values 0 to 65535. For this sign representation method, the leftmost bit (most significant bit) denotes whether the value is positive or negative (0 for positive, 1 for negative).\n\nFor most architectures, there is no signed–unsigned type distinction in the machine language. Nevertheless, arithmetic instructions usually set different CPU flags such as the carry flag for unsigned arithmetic and the overflow flag for signed. Those values can be taken into account by subsequent branch or arithmetic commands.\n\nThe C programming language, along with its derivatives, implements a signedness for all integer data types, as well as for \"character\". The unsigned modifier defines the type to be unsigned. The default integer signedness is signed, but can be set explicitly with signed modifier. Integer literals can be made unsigned with U suffix. For example, 0xFFFFFFFF gives −1, but 0xFFFFFFFFU gives 4,294,967,295 for 32-bit code.\n\nCompilers often issue a warning when comparisons are made between signed and unsigned numbers or when one is cast to the other. These are potentially dangerous operations as the ranges of the signed and unsigned types are different.\n\n\n"}
{"id": "14355284", "url": "https://en.wikipedia.org/wiki?curid=14355284", "title": "Smallest-circle problem", "text": "Smallest-circle problem\n\nThe smallest-circle problem or minimum covering circle problem is a mathematical problem of computing the smallest circle that contains all of a given set of points in the Euclidean plane. The corresponding problem in \"n\"-dimensional space, the smallest bounding-sphere problem, is to compute the smallest \"n\"-sphere that contains all of a given set of points. The smallest-circle problem was initially proposed by the English mathematician James Joseph Sylvester in 1857.\n\nThe smallest-circle problem in the plane is an example of a facility location problem (the 1-center problem) in which the location of a new facility must be chosen to provide service to a number of customers, minimizing the farthest distance that any customer must travel to reach the new facility. Both the smallest circle problem in the plane, and the smallest bounding sphere problem in any higher-dimensional space of bounded dimension are solvable in linear time.\n\nMost of the geometric approaches for the problem look for points that lie on the boundary of the minimum circle and are based on the following simple facts:\nLet be any set of points in the plane, and suppose that there are two smallest enclosing disks of , with centers at formula_1 and formula_2. Let be their shared radius, and let formula_3 be the distance between their centers. Then since is a subset of both disks it is a subset of their intersection. However, their intersection is contained within the disk with center formula_4 and radius formula_5, as shown in the following image:\n\nSince is minimal, we must have formula_6, meaning formula_7, so the disks are identical.\nAs Nimrod Megiddo showed, the minimum enclosing circle can be found in linear time, and the same linear time bound also applies to the smallest enclosing sphere in Euclidean spaces of any constant dimension.\n\nEmo Welzl proposed a simple randomized algorithm for the\nminimum covering circle problem that runs in expected formula_8 time, based on a linear programming algorithm of Raimund Seidel. This algorithm is presented below.\n\nSubsequently, the smallest-circle problem was included in a general class of LP-type problems that can be solved by algorithms like Welzl's based on linear programming. As a consequence of membership in this class, it was shown that the dependence on the dimension of the constant factor in the formula_8 time bound, which was factorial for Seidel's method, could be reduced to subexponential, while still maintaining only linear dependence on \"N\".\n\nThe algorithm is recursive, and takes as arguments two (finite) sets of points and ; it computes the smallest enclosing circle of the union of and , as long as every point of is one of the boundary points of the eventual smallest enclosing circle. Thus, the original smallest enclosing circle problem can be solved by calling the algorithm with equal to the set of points to be enclosed and equal to the empty set; as the algorithm calls itself recursively, it will enlarge the set passed into the recursive calls until it includes all the boundary points of the circle.\n\nThe algorithm processes the points of in a random order, maintaining as it does the set of processed points and the smallest circle that encloses the union ∪ . At each step, it tests whether the next point to be processed is in ; if not, the algorithm replaces the by the result of a recursive call of the algorithm on the sets and ∪ . Whether the circle was replaced or not, is then included in the set . Processing each point, therefore, consists of testing in constant time whether the point belongs to a single circle and possibly performing a recursive call to the algorithm. It can be shown that the overall time is linear.\n\nPrior to Megiddo's result showing that the smallest-circle problem may be solved in linear time, several algorithms of higher complexity appeared in the literature. A naive algorithm solves the problem in time O(\"n\") by testing the circles determined by all pairs and triples of points.\n\nThe weighted version of the minimum covering circle problem takes as input a set of points in a Euclidean space, each with weights; the goal is to find a single point that minimizes the maximum weighted distance to any point. The original minimum covering circle problem can be recovered by setting all weights to the same number. As with the unweighted problem, the weighted problem may be solved in linear time in any space of bounded dimension, using approaches closely related to bounded dimension linear programming algorithms, although slower algorithms are again frequent in the literature.\n\n\n"}
{"id": "277125", "url": "https://en.wikipedia.org/wiki?curid=277125", "title": "Superrationality", "text": "Superrationality\n\nIn economics and game theory, a participant is considered to have superrationality (or renormalized rationality) if they have perfect rationality (and thus maximize their own utility) but assume that all other players are superrational too and that a superrational individual will always come up with the same strategy as any other superrational thinker when facing the same problem. Applying this definition, a superrational player playing against a superrational opponent in a prisoner's dilemma will cooperate while a rationally self-interested player would defect. \n\nThis decision rule is not a mainstream model within game theory and was suggested by Douglas Hofstadter in his article, series, and book \"Metamagical Themas\" as an alternative type of rational decision making different from the widely accepted game-theoretic one. Superrationality is a form of Immanuel Kant's categorical imperative. Hofstadter provided this definition: \"Superrational thinkers, by recursive definition, include in their calculations the fact that they are in a group of superrational thinkers.\"\n\nNote that contrary to the Homo reciprocans, the superrational thinker will not always play the equilibrium that maximizes the total social utility, and is thus not a philanthropist.\n\nThe idea of superrationality is that two logical thinkers analyzing the same problem will think of the same correct answer. For example, if two people are both good at math and both have been given the same complicated problem to do, both will get the same right answer. In math, knowing that the two answers are going to be the same doesn't change the value of the problem, but in game theory, knowing that the answer will be the same might change the answer itself.\n\nThe prisoner's dilemma is usually framed in terms of jail sentences for criminals, but it can be stated equally well with cash prizes instead. Two players are each given the choice to cooperate (C) or to defect (D). The players choose without knowing what the other is going to do. If both cooperate, each will get $100. If they both defect, they each get $1. If one cooperates and the other defects, then the defecting player gets $200, while the cooperating player gets nothing.\n\nThe four outcomes and the payoff to each player are listed below\n\nOne valid way for the players to reason is as follows:\n\nThe conclusion is that the rational thing to do is to defect. This type of reasoning defines game-theoretic rationality, and two game-theoretic rational players playing this game both defect and receive a dollar each.\n\nSuperrationality is an alternative method of reasoning. First, it is assumed that the answer to a symmetric problem will be the same for all the superrational players. Thus the sameness is taken into account \"before\" knowing what the strategy will be. The strategy is found by maximizing the payoff to each player, assuming that they all use the same strategy. Since the superrational player knows that the other superrational player will do the same thing, whatever that might be, there are only two choices for two superrational players. Both will cooperate or both will defect depending on the value of the superrational answer. Thus the two superrational players will both cooperate, since this answer maximizes their payoff. Two superrational players playing this game will each walk away with $100.\n\nNote that a superrational player playing against a game-theoretic rational player will defect, since the strategy only assumes that the superrational players will agree. \n\nAlthough standard game theory assumes common knowledge of rationality, it does so in a different way. The game theoretic analysis maximizes payoffs by allowing each player to change strategies independently of the others, even though in the end, it assumes that the answer in a symmetric game will be the same for all. This is the definition of a game theoretic Nash equilibrium, which defines a stable strategy as one where no player can improve the payoffs by unilaterally changing course. The superrational equilibrium in a symmetric game is one where all the players' strategies are forced to be the same before the maximization step. (There is no agreed upon extension of the concept of superrationality to asymmetric games.)\n\nSome argue that superrationality implies a kind of magical thinking in which each player supposes that their decision to cooperate will cause the other player to cooperate, despite the fact that there is no communication. Hofstadter points out that the concept of \"choice\" doesn't apply when the player's goal is to figure something out, and that the decision does not cause the other player to cooperate, but rather same logic leads to same answer independent of communication or cause and effect. This debate is over whether it is reasonable for human beings to act in a superrational manner, not over what superrationality means, and is similar to arguments about whether it is reasonable for humans to act in a 'rational' manner, as described by game theory (wherein they can figure out what other players will or have done by asking themselves, what would I do if I was them, and applying backwards induction and iterated elimination of dominated strategies).\n\nFor simplicity, the foregoing account of superrationality ignored mixed strategies: the possibility that the best choice could be to flip a coin, or more generally to choose different outcomes with some probability. In the prisoner's dilemma, it is superrational to cooperate with probability 1 even when mixed strategies are admitted, because the average payoff when one player cooperates and the other defects is the same as when both cooperate, and so defecting increases the risk of both defecting, which decreases the expected payout. But in some cases, the superrational strategy is mixed.\n\nFor example, if the payoffs in are as follows:\n\nSo that defecting has a huge reward, the superrational strategy is defecting with a probability of 499,900/999,899 or a little over 49.995%. As the reward increases to infinity, the probability only approaches 1/2 further, and the losses for adopting the simpler strategy of 1/2 (which are already minimal) approach 0. In a less extreme example, if the payoff for one cooperator and one defector was $400 and $0, respectively, the superrational mixed strategy world be defecting with probability 100/299 or about 1/3.\n\nIn similar situations with more players, using a randomising device can be essential. One example discussed by Hofstadter is the platonia dilemma: an eccentric trillionaire contacts 20 people, and tells them that if one and only one of them sends him or her a telegram (assumed to cost nothing) by noon the next day, that person will receive a billion dollars. If they receive more than one telegram, or none at all, no one will get any money, and communication between players is forbidden. In this situation, the superrational thing to do (if it is known that all 20 are superrational) is to send a telegram with probability p=1/20 — that is, each recipient essentially rolls a 20-sided die and only sends a telegram if it comes up \"1\". This maximizes the probability that exactly one telegram is received.\n\nNotice though that this is not the solution in a conventional game-theoretical analysis. Twenty game-theoretically rational players would each send in telegrams and therefore receive nothing. This is because sending telegrams is the dominant strategy; if an individual player sends telegrams they have a chance of receiving money, but if they send no telegrams they cannot get anything. (If all telegrams were guaranteed to arrive, they would only send one, and no one would expect to get any money.)\n\n"}
{"id": "22260858", "url": "https://en.wikipedia.org/wiki?curid=22260858", "title": "Symmetric set", "text": "Symmetric set\n\nIn mathematics, a nonempty subset \"S\" of a group \"G\" is said to be symmetric if\nwhere formula_2. In other words, \"S\" is symmetric if formula_3 whenever formula_4.\n\nIf \"S\" is a subset of a vector space, then \"S\" is said to be symmetric if it is symmetric with respect to the additive group structure of the vector space; that is, if formula_5.\n\n\n"}
{"id": "34421039", "url": "https://en.wikipedia.org/wiki?curid=34421039", "title": "Vladimir Berkovich", "text": "Vladimir Berkovich\n\nVladimir Berkovich is a mathematician at the Weizmann Institute of Science who introduced Berkovich spaces. His Ph.D. advisor was Yuri I. Manin. Berkovich was a visiting scholar at the Institute for Advanced Study in 1991-92 and again in the summer of 2000.\n\nIn 2012 he became a fellow of the American Mathematical Society.\n\n"}
{"id": "37083491", "url": "https://en.wikipedia.org/wiki?curid=37083491", "title": "West Coast Number Theory", "text": "West Coast Number Theory\n\nWest Coast Number Theory (WCNT), a meeting that has also been known variously as the Western Number Theory Conference and the Asilomar Number Theory meeting, is an annual gathering of number theorists first organized by D. H. and Emma Lehmer at the Asilomar Conference Grounds in 1969. In his tribute to D. H. Lehmer, John Brillhart stated that \"There is little doubt that one of [Dick and Emma's] most enduring contributions to the world of mathematicians is their founding of the West Coast Number Theory Meeting [an annual event] in 1969\". To date, the conference remains an active meeting of young and experienced number theorists alike.\n\nWest Coast Number Theory has been held at a variety of locations throughout western North America. Typically, odd years are held in Pacific Grove, CA. Until 2013, this was always at the Asilomar Conference Grounds, though more recent meetings have moved to the Lighthouse Lodge, just up the road.\n\n\n"}
{"id": "21903944", "url": "https://en.wikipedia.org/wiki?curid=21903944", "title": "Wolfram Alpha", "text": "Wolfram Alpha\n\nWolfram Alpha (also styled WolframAlpha, and Wolfram|Alpha) is a computational knowledge engine or answer engine developed by Wolfram Alpha LLC, a subsidiary of Wolfram Research. It is an online service that answers factual queries directly by computing the answer from externally sourced \"curated data\", rather than providing a list of documents or web pages that might contain the answer as a search engine might.\n\nWolfram Alpha, which was released on May 18, 2009, is based on Wolfram's earlier flagship product Wolfram Mathematica, a computational platform or toolkit that encompasses computer algebra, symbolic and numerical computation, visualization, and statistics capabilities. Additional data is gathered from both academic and commercial websites such as the CIA's \"The World Factbook\", the United States Geological Survey, a Cornell University Library publication called \"All About Birds\", \"Chambers Biographical Dictionary\", Dow Jones, the \"Catalogue of Life\", CrunchBase, Best Buy, the FAA and optionally a user's Facebook account.\n\nUsers submit queries and computation requests via a text field. Wolfram Alpha then computes answers and relevant visualizations from a knowledge base of curated, structured data that come from other sites and books. The site \"use[s] a portfolio of automated and manual methods, including statistics, visualization, source cross-checking, and expert review.\" The curated data makes Alpha different from semantic search engines, which index a large number of answers and then try to match the question to one.\n\nWolfram Alpha can only provide robust query results based on computational facts, not queries on the social sciences, cultural studies or even many questions about history where responses require more subtlety and complexity. It is able to respond to particularly-phrased natural language fact-based questions such as \"Where was Mary Robinson born?\" or more complex questions such as \"How old was Queen Elizabeth II in 1974?\" It displays its \"Input interpretation\" of such a question, using standardized phrases such as \"age | of Queen Elizabeth II (royalty) | in 1974\", the answer of which is \"Age at start of 1974: 47 years\", and a biography link. Wolfram Alpha does not answer queries which require a narrative response such as \"What is the difference between the Julian and the Gregorian calendars?\" but will answer factual or computational questions such as \"June 1 in Julian calendar\".\n\nMathematical symbolism can be parsed by the engine, which typically responds with more than the numerical results. For example, \"lim(x->0) (sin x)/x\" yields the correct limiting value of 1, as well as a plot, up to 235 terms () of the Taylor series, and (for registered users) a possible derivation using L'Hôpital's rule. It is also able to perform calculations on data using more than one source. For example, \"What is the fifty-second smallest country by GDP per capita?\" yields Nicaragua, $1160 per year.\n\nWolfram Alpha is written in 15 million lines of Wolfram Language code and runs on more than 10,000 CPUs. The database currently includes hundreds of datasets, such as \"All Current and Historical Weather.\" The datasets have been accumulated over several years. The curated (as distinct from auto-generated) datasets are checked for quality either by a scientist or other expert in a relevant field, or someone acting in a clerical capacity who simply verifies that the datasets are \"acceptable\".\n\nOne example of a live dataset that Wolfram Alpha can use is the profile of a Facebook user, through inputting the \"facebook report\" query. If the user authorizes Facebook to share his or her account details with the Wolfram site, Alpha can generate a \"personal analytics\" report containing the age distribution of friends, the frequency of words used in status updates and other detailed information. Within two weeks of launching the Facebook analytics service, 400,000 users had used it. Downloadable query results are behind a pay wall but summaries are accessible to free accounts.\n\nWolfram Alpha has been used to power some searches in the Microsoft Bing and DuckDuckGo search engines. With the first release on July 21, 2017, Brave web browser features Wolfram Alpha as one of its default search engines. For factual question answering, it is also queried by Apple's Siri, Samsung's S Voice, as well as Dexetra's speech recognition software for the Android platform, Iris, and the voice control software on BlackBerry 10.\n\nLaunch preparations began on May 15, 2009 at 7 pm CDT and were broadcast live on Justin.tv. The plan was to publicly launch the service a few hours later, with expected issues due to extreme load. The service was officially launched on May 18, 2009.\n\nWolfram Alpha has received mixed reviews. Wolfram Alpha advocates point to its potential, some even stating that how it determines results is more important than current usefulness.\n\nOn December 3, 2009, an iPhone app was introduced. Some users considered the initial $50 price of the iOS app unnecessarily high, since the same features could be freely accessed by using a web browser instead. They also complained about the simultaneous removal of the mobile formatting option for the site. Wolfram responded by lowering the price to $2, offering a refund to existing customers and re-instating the mobile site.\n\nOn October 6, 2010, an Android version of the app was released and it is now available for Kindle Fire and Nook. (The Nook version is not available outside the US). A further 71 apps are available which use the Wolfram Alpha engine for specialized tasks.\n\nOn June 18, 2018, the Japanese version of Wolfram Alpha was released.\n\nOn February 8, 2012, Wolfram Alpha Pro was released, offering users additional features for a monthly subscription fee. A key feature is the ability to upload many common file types and data—including raw tabular data, images, audio, XML, and dozens of specialized scientific, medical, and mathematical formats—for automatic analysis. Other features include an extended keyboard, interactivity with CDF, data downloads, in-depth step by step solution, the ability to customize and save graphical and tabular results and extra computation time.\n\nAlong with new premium features, Wolfram Alpha Pro led to some changes in the free version of the site:\n\n\"InfoWorld\" published an article warning readers of the potential implications of giving an automated website proprietary rights to the data it generates. Free software advocate Richard Stallman also opposes the idea of recognizing the site as a copyright holder and suspects that Wolfram would not be able to make this case under existing copyright law.\n\n\n"}
