{"id": "26675250", "url": "https://en.wikipedia.org/wiki?curid=26675250", "title": "Aegean numerals", "text": "Aegean numerals\n\nAegean numbers was the numeral system used by the Minoan and Mycenaean civilizations. They are attested in Linear A and Linear B scripts. They may have survived in the Cypro-Minoan script, where a single sign with \"100\" value is attested so far on a large clay tablet from Enkomi. \n\n\n"}
{"id": "14222703", "url": "https://en.wikipedia.org/wiki?curid=14222703", "title": "Affine differential geometry", "text": "Affine differential geometry\n\nAffine differential geometry, is a type of differential geometry in which the differential invariants are invariant under volume-preserving affine transformations. The name \"affine differential geometry\" follows from Klein's Erlangen program. The basic difference between affine and Riemannian differential geometry is that in the affine case we introduce volume forms over a manifold instead of metrics.\n\nHere we consider the simplest case, i.e. manifolds of codimension one. Let be an \"n\"-dimensional manifold, and let ξ be a vector field on transverse to such that for all where ⊕ denotes the direct sum and Span the linear span.\n\nFor a smooth manifold, say \"N\", let Ψ(\"N\") denote the module of smooth vector fields over \"N\". Let be the standard covariant derivative on R where \nWe can decompose \"DY\" into a component tangent to \"M\" and a transverse component, parallel to ξ. This gives the equation of Gauss: where is the induced connexion on \"M\" and is a bilinear form. Notice that ∇ and \"h\" depend upon the choice of transverse vector field ξ. We consider only those hypersurfaces for which \"h\" is non-degenerate. This is a property of the hypersurface \"M\" and does not depend upon the choice of transverse vector field ξ. If \"h\" is non-degenerate then we say that \"M\" is non-degenerate. In the case of curves in the plane, the non-degenerate curves are those without inflexions. In the case of surfaces in 3-space, the non-degenerate surfaces are those without parabolic points.\n\nWe may also consider the derivative of ξ in some tangent direction, say \"X\". This quantity, \"D\"ξ, can be decomposed into a component tangent to \"M\" and a transverse component, parallel to ξ. This gives the Weingarten equation: The type-(1,1)-tensor is called the affine shape operator, the differential one-form is called the transverse connexion form. Again, both \"S\" and τ depend upon the choice of transverse vector field ξ.\n\nLet be a volume form defined on R. We can induce a volume form on \"M\" given by given by This is a natural definition: in Euclidean differential geometry where ξ is the Euclidean unit normal then the standard Euclidean volume spanned by \"X\",…,\"X\" is always equal to ω(\"X\",…,\"X\"). Notice that ω depends on the choice of transverse vector field ξ.\n\nFor tangent vectors \"X\",…,\"X\" let be the given by We define a second volume form on \"M\" given by where Again, this is a natural definition to make. If \"M\" = R and \"h\" is the Euclidean scalar product then ν(\"X\",…,\"X\") is always the standard Euclidean volume spanned by the vectors \"X\",…,\"X\".\nSince \"h\" depends on the choice of transverse vector field ξ it follows that ν does too.\n\nWe impose two natural conditions. The first is that the induced connexion ∇ and the induced volume form ω be compatible, i.e. ∇ω ≡ 0. This means that for all In other words, if we parallel transport the vectors \"X\",…,\"X\" along some curve in \"M\", with respect to the connexion ∇, then the volume spanned by \"X\",…,\"X\", with respect to the volume form ω, does not change. A direct calculation shows that and so for all if, and only if, τ ≡ 0, i.e. for all This means that the derivative of ξ, in a tangent direction \"X\", with respect to \"D\" always yields a, possibly zero, tangent vector to \"M\". The second condition is that the two volume forms ω and ν coincide, i.e. \n\nIt can be shown that there is, up to sign, a unique choice of transverse vector field ξ for which the two conditions that and are both satisfied. These two special transverse vector fields are called affine normal vector fields, or sometimes called Blaschke normal fields. From its dependence on volume forms for its definition we see that the affine normal vector field is invariant under volume preserving affine transformations. These transformations are given by where SL(\"n\"+1,R) denotes the special linear group of matrices with real entries and determinant 1, and ⋉ denotes the semi-direct product. forms a Lie group.\n\nThe affine normal line at a point is the line passing through \"p\" and parallel to ξ.\n\nThe affine normal vector field for a curve in the plane has a nice geometrical interpretation. Let be an open interval and let be a smooth parametrisation of a plane curve. We assume that γ(\"I\") is a non-degenerate curve (in the sense of Nomizu and Sasaki), i.e. is without inflexion points. Consider a point on the plane curve. Since γ(\"I\") is without inflexion points it follows that γ(\"t\") is not an inflexion point and so the curve will be locally convex, i.e. all of the points γ(\"t\") with for sufficiently small ε, will lie on the same side of the tangent line to γ(\"I\") at γ(\"t\").\n\nConsider the tangent line to γ(\"I\") at γ(\"t\"), and consider near-by parallel lines on the side of the tangent line containing the piece of curve For parallel lines sufficiently close to the tangent line they will intersect \"P\" in exactly two points. On each parallel line we mark the midpoint of the line segment joining these two intersection points. For each parallel line we get a midpoint, and so the locus of midpoints traces out a curve starting at \"p\". The limiting tangent line to the locus of midpoints as we approach \"p\" is exactly the affine normal line, i.e. the line containing the affine normal vector to γ(\"I\") at γ(\"t\"). Notice that this is an affine invariant construction since parallelism and midpoints are invariant under affine transformations.\n\nConsider the parabola given by the parametrisation . This has the equation The tangent line at γ(0) has the equation and so the parallel lines are given by for sufficiently small The line intersects the curve at The locus of midpoints is given by These form a line segment, and so the limiting tangent line to this line segment as we tend to γ(0) is just the line containing this line segment, i.e. the line In that case the affine normal line to the curve at γ(0) has the equation In fact, direct calculation shows that the affine normal vector at γ(0), namely ξ(0), is given by In the figure the red curve is the curve γ, the black lines are the tangent line and some near-by tangent lines, the black dots are the midpoints on the displayed lines, and the blue line is the locus of midpoints.\n\nA similar analogue exists for finding the affine normal line at elliptic points of smooth surfaces in 3-space. This time one takes planes parallel to the tangent plane. These, for planes sufficiently close to the tangent plane, intersect the surface to make convex plane curves. Each convex plane curve has a centre of mass. The locus of centres of mass trace out a curve in 3-space. The limiting tangent line to this locus as one tends to the original surface point is the affine normal line, i.e. the line containing the affine normal vector.\n\n"}
{"id": "24497678", "url": "https://en.wikipedia.org/wiki?curid=24497678", "title": "Algebraic sentence", "text": "Algebraic sentence\n\nIn mathematical logic, an algebraic sentence is one that can be stated using only equations between terms with free variables. Inequalities and quantifiers are specifically disallowed. Sentential logic is the subset of first-order logic involving only algebraic sentences.\n\nSaying that a sentence is algebraic is a stronger condition than saying it is elementary.\n\n"}
{"id": "31042895", "url": "https://en.wikipedia.org/wiki?curid=31042895", "title": "Average high cost multiple", "text": "Average high cost multiple\n\nIn unemployment insurance (UI) in the United States, the average high-cost multiple (AHCM) is a commonly used actuarial measure of Unemployment Trust Fund adequacy. Technically, AHCM is defined as reserve ratio (i.e., the balance of UI trust fund expressed as % of total wages paid in covered employment) divided by average cost rate of three high-cost years in the state's recent history (typically 20 years or a period covering three recessions, whichever is longer). In this definition, cost rate for any duration of time is defined as benefit cost divided by total wages paid in covered employment for the same duration, usually expressed as a percentage.\n\nIntuitively, the AHCM provides an estimate of the length of time (measured in number of years) current reserve in the trust fund (without taking into account future revenue income) can pay out benefits at historically high payout rate. For example, if a state's AHCM is 1.0 immediately prior to a recession, and if the incoming recession is of the average magnitude of the past three recessions, then the state is expected to be able to pay one year of UI benefits using the money already in its trust fund alone. If the AHCM is 0.5, then the state is expected to be able to pay out six months of benefits when the a similar recession hits.\n\nAs of December 31, 2009, a state has a balance of $500 million in its UI trust fund. The total wages of its covered employment is $40 billion. The reserve ratio for this state on this day is $500/$40000 = 1.25%. Historically, the state experienced three highest-cost years in 1991, 2002, and 2009, when the cost rates were 1.50, 1.80, and 3.00, respectively. The average high-cost rate for this state is therefore 2.10. Thus, the average high-cost multiple is 1.25/2.10 = 0.595.\n\nThe following chart shows US average high-cost multiple from 1957 to 2009. \n\n"}
{"id": "42478101", "url": "https://en.wikipedia.org/wiki?curid=42478101", "title": "Axiality (geometry)", "text": "Axiality (geometry)\n\nIn the geometry of the Euclidean plane, axiality is a measure of how much axial symmetry a shape has. It is defined as the ratio of areas of the largest axially symmetric subset of the shape to the whole shape. Equivalently it is the largest fraction of the area of the shape that can be covered by a mirror reflection of the shape (with any orientation).\n\nA shape that is itself axially symmetric, such as an isosceles triangle, will have an axiality of exactly one, whereas an asymmetric shape, such as a scalene triangle, will have axiality less than one.\n\n showed that every convex set has axiality at least 2/3. This result improved a previous lower bound of 5/8 by . The best upper bound known is given by a particular convex quadrilateral, found through a computer search, whose axiality is less than 0.816.\n\nFor triangles and for centrally symmetric convex bodies, the axiality is always somewhat higher: every triangle, and every centrally symmetric convex body, has axiality at least formula_1. In the set of obtuse triangles whose vertices have formula_2-coordinates formula_3, formula_4, and formula_5, the axiality approaches formula_6 in the limit as the formula_7-coordinates approach zero, showing that the lower bound is as large as possible. It is also possible to construct a sequence of centrally symmetric parallelograms whose axiality has the same limit, again showing that the lower bound is tight.\n\nThe axiality of a given convex shape can be approximated arbitrarily closely in sublinear time, given access to the shape by oracles for finding an extreme point in a given direction and for finding the intersection of the shape with a line.\n\n lists 11 different measures of axial symmetry, of which the one described here is number three. He requires each such measure to be invariant under similarity transformations of the given shape, to take the value one for symmetric shapes, and to take a value between zero and one for other shapes. Other symmetry measures with these properties include the ratio of the area of the shape to its smallest enclosing symmetric superset, and the analogous ratios of perimeters.\n\n, as well as studying axiality, studies a restricted version of axiality in which the goal is to find a halfspace whose intersection with a convex shape has large area lies entirely within the reflection of the shape across the halfspace boundary. He shows that such an intersection can always be found to have area at least 1/8 that of the whole shape.\n\nIn the study of computer vision, proposed to measure the symmetry of a digital image (viewed as a function formula_13 from points in the plane to grayscale intensity values in the interval formula_14) by finding a reflection formula_15 that maximizes the area integral\nWhen formula_13 is the indicator function of a given shape, this is the same as the axiality.\n"}
{"id": "42482358", "url": "https://en.wikipedia.org/wiki?curid=42482358", "title": "Basic element", "text": "Basic element\n\nIn algebra, a basic element \"x\" with respect to an element \"y\" is an element of a cochain complex formula_1 (e.g., complex of differential forms on a manifold) that is closed: formula_2 and the contraction of \"x\" by \"y\" is zero.\n"}
{"id": "1451250", "url": "https://en.wikipedia.org/wiki?curid=1451250", "title": "Bidiagonal matrix", "text": "Bidiagonal matrix\n\nIn mathematics, a bidiagonal matrix is a banded matrix with non-zero entries along the main diagonal and \"either\" the diagonal above or the diagonal below. This means there are exactly two non zero diagonals in the matrix.\n\nWhen the diagonal above the main diagonal has the non-zero entries the matrix is upper bidiagonal. When the diagonal below the main diagonal has the non-zero entries the matrix is lower bidiagonal.\n\nFor example, the following matrix is upper bidiagonal:\n\nand the following matrix is lower bidiagonal:\n\nOne variant of the QR algorithm starts with reducing a general matrix into a bidiagonal one,\nand the Singular value decomposition uses this method as well.\n\n\n"}
{"id": "4282051", "url": "https://en.wikipedia.org/wiki?curid=4282051", "title": "Chow group", "text": "Chow group\n\nIn algebraic geometry, the Chow groups (named after Wei-Liang Chow by ) of an algebraic variety over any field are algebro-geometric analogs of the homology of a topological space. The elements of the Chow group are formed out of subvarieties (so-called algebraic cycles) in a similar way to how simplicial or cellular homology groups are formed out of subcomplexes. When the variety is smooth, the Chow groups can be interpreted as cohomology groups (compare Poincaré duality) and have a multiplication called the intersection product. The Chow groups carry rich information about an algebraic variety, and they are correspondingly hard to compute in general.\n\nFor what follows, define a variety over a field \"k\" to be an integral scheme of finite type over \"k\". For any scheme \"X\" of finite type over \"k\", an algebraic cycle on \"X\" means a finite linear combination of subvarieties of \"X\" with integer coefficients. (Here and below, subvarieties are understood to be closed in \"X\", unless stated otherwise.) For a natural number \"i\", the group \"Z\"(\"X\") of \"i\"-dimensional cycles (or \"i\"-cycles, for short) on \"X\" is the free abelian group on the set of \"i\"-dimensional subvarieties of \"X\".\n\nFor a variety \"W\" of dimension \"i\" + 1 and any rational function \"f\" on \"W\" which is not identically zero, the divisor of \"f\" is the \"i\"-cycle\nwhere the sum runs over all \"i\"-dimensional subvarieties \"Z\" of \"W\" and the integer ord(\"f\") denotes the order of vanishing of \"f\" along \"Z\". (Thus ord(\"f\") is negative if \"f\" has a pole along \"Z\".) The definition of the order of vanishing requires some care for \"W\" singular.\n\nFor a scheme \"X\" of finite type over \"k\", the group of \"i\"-cycles rationally equivalent to zero is the subgroup of \"Z\"(\"X\") generated by the cycles (\"f\") for all (\"i\"+1)-dimensional subvarieties \"W\" of \"X\" and all nonzero rational functions \"f\" on \"W\". The Chow group \"CH\"(X) of \"i\"-dimensional cycles on \"X\" is the quotient group of \"Z\"(\"X\") by the subgroup of cycles rationally equivalent to zero. Sometimes one writes [\"Z\"] for the class of a subvariety \"Z\" in the Chow group, and if two subvarieties \"Z\" and \"W\" have [\"Z\"] = [\"W\"], then \"Z\" and \"W\" are said to be rationally equivalent.\n\nFor example, when \"X\" is a variety of dimension \"n\", the Chow group \"CH\"(X) is the divisor class group of \"X\". When \"X\" is smooth over \"k\", this is isomorphic to the Picard group of line bundles on \"X\".\n\nWhen the scheme \"X\" is smooth over a field \"k\", the Chow groups form a ring, not just a graded abelian group. Namely, when \"X\" is smooth over \"k\", define \"CH\"(\"X\") to be the Chow group of codimension-\"i\" cycles on \"X\". (When \"X\" is a variety of dimension \"n\", this just means that \"CH\"(\"X\") = \"CH\"(\"X\").) Then the groups \"CH\"(\"X\") form a commutative graded ring, with a product:\n\nThe product arises from intersecting algebraic cycles. For example, if \"Y\" and \"Z\" are smooth subvarieties of \"X\" of codimension \"i\" and \"j\" respectively, and if \"Y\" and \"Z\" intersect transversely, then the product [\"Y\"][\"Z\"] in \"CH\"(\"X\") is the sum of the irreducible components of the intersection \"Y\" ∩ \"Z\", which all have codimension .\n\nMore generally, in various cases, intersection theory constructs an explicit cycle that represents the product [\"Y\"][\"Z\"] in the Chow ring. For example, if \"Y\" and \"Z\" are subvarieties of complementary dimension (meaning that their dimensions sum to the dimension of \"X\") whose intersection has dimension zero, then [\"Y\"][\"Z\"] is equal to the sum of the points of the intersection with coefficients called intersection numbers. For any subvarieties \"Y\" and \"Z\" of a smooth scheme \"X\" over \"k\", with no assumption on the dimension of the intersection, William Fulton and Robert MacPherson's intersection theory constructs a canonical element of the Chow groups of \"Y\" ∩ \"Z\" whose image in the Chow groups of \"X\" is the product [\"Y\"][\"Z\"].\n\nThe Chow ring of projective space P over any field \"k\" is the ring\n\nwhere \"H\" is the class of any hyperplane (the zero locus of a single linear function). Furthermore, any subvariety \"Y\" of degree \"a\" and codimension \"r\" in projective space is rationally equivalent to \"aH\". It follows that for any two subvarieties \"Y\" and \"Z\" of complementary dimension in P and degrees \"a\", \"b\" respectively, their product in the Chow ring is simply\n\nwhere \"H\" is the class of a \"k\"-rational point in P. For example, if \"Y\" and \"Z\" intersect transversely, it follows that \"Y\" ∩ \"Z\" is a zero-cycle of degree \"ab\". If the base field \"k\" is algebraically closed, this means that there are exactly \"ab\" points of intersection; this is a version of Bézout's theorem, a classic result of enumerative geometry.\n\nGiven a vector bundle formula_5 of rank formula_6 over a smooth proper scheme \"X\" over a field, the Chow ring of the associated projective bundle formula_7 can be computed using the chow ring of formula_8 and the Chern classes of formula_9. If we let formula_10 and formula_11 the Chern classes of formula_9, then there is an isomorphism of rings\n\nFor example, the Chow ring of a Hirzebruch surface can be readily computed using the projective bundle formula. Recall that it is constructed as formula_14 over formula_15. Then, the only non-trivial Chern class of this vector bundle is formula_16. This implies that the Chow ring is isomorphic to\n\nFor other algebraic varieties, Chow groups can have richer behavior. For example, let \"X\" be an elliptic curve over a field \"k\". Then the Chow group of zero-cycles on \"X\" fits into an exact sequence\nThus the Chow group of an elliptic curve \"X\" is closely related to the group \"X\"(\"k\") of \"k\"-rational points of \"X\". When \"k\" is a number field, \"X\"(\"k\") is called the Mordell–Weil group of \"X\", and some of the deepest problems in number theory are attempts to understand this group. When \"k\" is the complex numbers, the example of an elliptic curve shows that Chow groups can be uncountable abelian groups.\n\nFor a proper morphism \"f\": \"X\" → \"Y\" of schemes over \"k\", there is a pushforward homomorphism \"f\": \"CH\"(\"X\") → \"CH\"(\"Y\") for each integer \"i\". For example, for a proper scheme \"X\" over \"k\", this gives a homomorphism \"CH\"(\"X\") → Z, which takes a closed point in \"X\" to its degree over \"k\". (A closed point in \"X\" has the form Spec(\"E\") for a finite extension field \"E\" of \"k\", and its degree means the degree of the field \"E\" over \"k\".)\n\nFor a flat morphism \"f\": \"X\" → \"Y\" of schemes over \"k\" with fibers of dimension \"r\" (possibly empty), there is a homomorphism \"f\": \"CH\"(\"Y\") → \"CH\"(\"X\").\n\nA key computational tool for Chow groups is the localization sequence, as follows. For a scheme \"X\" over a field \"k\" and a closed subscheme \"Z\" of \"k\", there is an exact sequence\nwhere the first homomorphism is the pushforward associated to the proper morphism \"Z\" → \"X\", and the second homomorphism is pullback with respect to the flat morphism \"X\" − \"Z\" → \"X\". The localization sequence can be extended to the left using a generalization of Chow groups, (Borel–Moore) motivic homology groups, also known as higher Chow groups.\n\nFor any morphism \"f\": \"X\" → \"Y\" of smooth schemes over \"k\", there is a pullback homomorphism \"f\": \"CH\"(\"Y\") → \"CH\"(\"X\"), which is in fact a ring homomorphism \"CH\"(\"Y\") → \"CH\"(\"X\").\n\nNote that non-examples can be constructed using blowups; for example, if we take the blowup of the origin in formula_20 then the fiber over the origin is isomorphic to formula_15.\nConsider the branched covering of curves\nSince the morphism ramifies whenever formula_23 we get a factorization\nwhere one of the formula_25. This implies that the points formula_26 have multiplicities formula_27 respectively. The flat pullback of the point formula_28 is then\nConsider a flat family of varieties\nand a subvariety formula_31. Then, using the cartesian square\nwe see that the image of formula_33 is a subvariety of formula_8. Therefore we have\n\nThere are several homomorphisms (known as cycle maps) from Chow groups to more computable theories.\n\nFirst, for a scheme \"X\" over the complex numbers, there is a homomorphism from Chow groups to Borel–Moore homology:\nThe factor of 2 appears because an \"i\"-dimensional subvariety of \"X\" has real dimension 2\"i\". When \"X\" is smooth over the complex numbers, this cycle map can be rewritten using Poincaré duality as a homomorphism\nIn this case (\"X\" smooth over C), these homomorphisms form a ring homomorphism from the Chow ring to the cohomology ring. Intuitively, this is because the products in both the Chow ring and the cohomology ring describe the intersection of cycles.\n\nFor a smooth complex projective variety, the cycle map from the Chow ring to ordinary cohomology factors through a richer theory, Deligne cohomology. This incorporates the Abel–Jacobi map from cycles homologically equivalent to zero to the intermediate Jacobian. The exponential sequence shows that \"CH\"(\"X\") maps isomorphically to Deligne cohomology, but that fails for \"CH\"(\"X\") with \"j\" > 1.\n\nFor a scheme \"X\" over an arbitrary field \"k\", there is an analogous cycle map from Chow groups to (Borel–Moore) etale homology. When \"X\" is smooth over \"k\", this homomorphism can be identified with a ring homomorphism from the Chow ring to etale cohomology.\n\nAn (algebraic) vector bundle \"E\" on a smooth scheme \"X\" over a field has Chern classes \"c\"(\"E\") in \"CH\"(\"X\"), with the same formal properties as in topology. The Chern classes give a close connection between vector bundles and Chow groups. Namely, let \"K\"(\"X\") be the Grothendieck group of vector bundles on \"X\". As part of the Grothendieck–Riemann–Roch theorem, Grothendieck showed that the Chern character gives an isomorphism\nThis isomorphism shows the importance of rational equivalence, compared to any other adequate equivalence relation on algebraic cycles.\n\nSome of the deepest conjectures in algebraic geometry and number theory are attempts to understand Chow groups. For example:\n\n\nFulton and MacPherson extended the Chow ring to singular varieties by defining the \"operational Chow ring\" and more generally a bivariant theory associated to any morphism of schemes. A bivariant theory is a pair of covariant and contravariant functors that assign to a map a group and a ring respectively. It generalizes a cohomology theory, which is a contravariant functor that assigns to a space a ring, namely a cohomology ring. The name \"bivariant\" refers to the fact that the theory contains both covariant and contravariant functors.\n\nThis is in a sense the most elementary extension of the Chow ring to singular varieties; other theories such as motivic cohomology map to the operational Chow ring.\n\nArithmetic Chow groups are an amalgamation of Chow groups of varieties over Q together with a component encoding Arakelov-theoretical information, that is, differential forms on the associated complex manifold.\n\nThe theory of Chow groups of schemes of finite type over a field extends easily to that of algebraic spaces. The key advantage of this extension is that it is easier to form quotients in the latter category and thus it is more natural to consider equivariant Chow groups of algebraic spaces. A much more formidable extension is that of Chow group of a stack, which has been constructed only in some special case and which is needed in particular to make sense of a virtual fundamental class.\n\nRational equivalence of divisors (known as linear equivalence) was studied in various forms during the 19th century, leading to the ideal class group in number theory and the Jacobian variety in the theory of algebraic curves. For higher-codimension cycles, rational equivalence was introduced by Severi in the 1930s. In 1956, Wei-Liang Chow gave an influential proof that the intersection product is well-defined on cycles modulo rational equivalence for a smooth quasi-projective variety, using Chow's moving lemma. Starting in the 1970s, Fulton and MacPherson gave the current standard foundation for Chow groups, working with singular varieties wherever possible. In their theory, the intersection product for smooth varieties is constructed by deformation to the normal cone.\n\n\n"}
{"id": "53865181", "url": "https://en.wikipedia.org/wiki?curid=53865181", "title": "Cloud-based quantum computing", "text": "Cloud-based quantum computing\n\nCloud-based quantum computing is the invocation of quantum emulators, simulators or processors through the cloud. Increasingly, cloud services are being looked on as the method for providing access to quantum processing.\n\nCloud-based quantum computing is used in several contexts:\n\n\n"}
{"id": "1113115", "url": "https://en.wikipedia.org/wiki?curid=1113115", "title": "Combinatorial number system", "text": "Combinatorial number system\n\nIn mathematics, and in particular in combinatorics, the combinatorial number system of degree \"k\" (for some positive integer \"k\"), also referred to as combinadics, is a correspondence between natural numbers (taken to include 0) \"N\" and \"k\"-combinations, represented as strictly decreasing sequences \"c\" > ... > \"c\" > \"c\" ≥ 0. Since the latter are strings of numbers, one can view this as a kind of numeral system for representing \"N\", although the main utility is representing a \"k\"-combination by \"N\" rather than the other way around. Distinct numbers correspond to distinct \"k\"-combinations, and produce them in lexicographic order; moreover the numbers less than formula_1 correspond to all \"k\"-combinations of }. The correspondence does not depend on the size \"n\" of the set that the \"k\"-combinations are taken from, so it can be interpreted as a map from N to the \"k\"-combinations taken from N; in this view the correspondence is a bijection.\n\nThe number \"N\" corresponding to (\"c\"...,\"c\",\"c\") is given by\n\nThe fact that a unique sequence so corresponds to any number \"N\" was observed by D. H. Lehmer. Indeed a greedy algorithm finds the \"k\"-combination corresponding to \"N\": take \"c\" maximal with formula_3, then take \"c\" maximal with formula_4, and so forth. Finding the number \"N\", using the formula above, from the \"k\"-combination (\"c\"...,\"c\",\"c\") is also known as \"ranking\", and the opposite operation (given by the greedy algorithm) as \"unranking\"; these operations are known by those names in most computer algebra systems, and in computational mathematics.\n\nThe originally used term \"combinatorial representation of integers\" is shortened to \"combinatorial number system\" by Knuth,\nwho also gives a much older reference;\nthe term \"combinadic\" is introduced by James McCaffrey (without reference to previous terminology or work).\n\nUnlike the factorial number system, the combinatorial number system of degree \"k\" is not a mixed radix system: the part formula_5 of the number \"N\" represented by a \"digit\" \"c\" is not obtained from it by simply multiplying by a place value.\n\nThe main application of the combinatorial number system is that it allows rapid computation of the \"k\"-combination that is at a given position in the lexicographic ordering, without having to explicitly list the \"k\"-combinations preceding it; this allows for instance random generation of \"k\"-combinations of a given set. Enumeration of \"k\"-combinations has many applications, among which software testing, sampling, quality control, and the analysis of lottery games.\n\nA \"k\"-combination of a set \"S\" is a subset of \"S\" with \"k\" (distinct) elements. The main purpose of the combinatorial number system is to provide a representation, each by a single number, of all formula_1 possible \"k\"-combinations of a set \"S\" of \"n\" elements. Choosing, for any \"n\", } as such a set, it can be arranged that the representation of a given \"k\"-combination \"C\" is independent of the value of \"n\" (although \"n\" must of course be sufficiently large); in other words considering \"C\" as a subset of a larger set by increasing \"n\" will not change the number that represents \"C\". Thus for the combinatorial number system one just considers \"C\" as a \"k\"-combination of the set N of all natural numbers, without explicitly mentioning \"n\".\n\nIn order to ensure that the numbers representing the \"k\"-combinations of } are less than those representing \"k\"-combinations not contained in }, the \"k\"-combinations must be ordered in such a way that their largest elements are compared first. The most natural ordering that has this property is lexicographic ordering of the \"decreasing\" sequence of their elements. So comparing the 5-combinations \"C\" = {0,3,4,6,9} and \"C\"<nowiki>'</nowiki> = {0,1,3,7,9}, one has that \"C\" comes before \"C\"<nowiki>'</nowiki>, since they have the same largest part 9, but the next largest part 6 of \"C\" is less than the next largest part 7 of \"C\"<nowiki>'</nowiki>; the sequences compared lexicographically are (9,6,4,3,0) and (9,7,3,1,0). Another way to describe this ordering is view combinations as describing the \"k\" raised bits in the binary representation of a number, so that \"C\" = {\"c\"...,\"c\"} describes the number\n(this associates distinct numbers to \"all\" finite sets of natural numbers); then comparison of \"k\"-combinations can be done by comparing the associated binary numbers. In the example \"C\" and \"C\"<nowiki>'</nowiki> correspond to numbers 1001011001 = 601 and 1010001011 = 651, which again shows that \"C\" comes before \"C\"<nowiki>'</nowiki>. This number is not however the one one wants to represent the \"k\"-combination with, since many binary numbers have a number of raised bits different from \"k\"; one wants to find the relative position of \"C\" in the ordered list of (only) \"k\"-combinations.\n\nThe number associated in the combinatorial number system of degree \"k\" to a \"k\"-combination \"C\" is the number of \"k\"-combinations strictly less than \"C\" in the given ordering. This number can be computed from \"C\" = { \"c\", ..., \"c\", \"c\" } with \"c\" > ... > \"c\" > \"c\" as follows. From the definition of the ordering it follows that for each \"k\"-combination \"S\" strictly less than \"C\", there is a unique index \"i\" such that \"c\" is absent from \"S\", while \"c\", ..., \"c\" are present in \"S\", and no other value larger than \"c\" is. One can therefore group those \"k\"-combinations \"S\" according to the possible values 1, 2, ..., \"k\" of \"i\", and count each group separately. For a given value of \"i\" one must include\n\"c\", ..., \"c\" in \"S\", and the remaining \"i\" elements of \"S\" must be chosen from the \"c\" non-negative integers strictly less than \"c\"; moreover any such choice will result in a \"k\"-combinations \"S\" strictly less than \"C\". The number of possible choices is formula_5, which is therefore the number of combinations in group \"i\"; the total number of \"k\"-combinations strictly less than \"C\" then is\nand this is the index (starting from 0) of \"C\" in the ordered list of \"k\"-combinations. Obviously there is for every \"N\" ∈ N exactly one \"k\"-combination at index \"N\" in the list (supposing \"k\" ≥ 1, since the list is then infinite), so the above argument proves that every \"N\" can be written in exactly one way as a sum of \"k\" binomial coefficients of the given form.\n\nThe given formula allows finding the place in the lexicographic ordering of a given \"k\"-combination immediately. The reverse process of finding the \"k\"-combination at a given place \"N\" requires somewhat more work, but is straightforward nonetheless. By the definition of the lexicographic ordering, two \"k\"-combinations that differ in their largest element \"c\" will be ordered according to the comparison of those largest elements, from which it follows that all combinations with a fixed value of their largest element are contiguous in the list. Moreover the smallest combination with \"c\" as largest element is formula_10, and it has \"c\" = \"i\" − 1 for all \"i\" < \"k\" (for this combination all terms in the expression except formula_10 are zero). Therefore \"c\" is the largest number such that formula_3. If \"k\" > 1 the remaining elements of the \"k\"-combination form the -combination corresponding to the number formula_13 in the combinatorial number system of degree , and can therefore be found by continuing in the same way for formula_13 and instead of \"N\" and \"k\".\n\nSuppose one wants to determine the 5-combination at position 72. The successive values of formula_15 for \"n\" = 4, 5, 6, ... are 0, 1, 6, 21, 56, 126, 252, ..., of which the largest one not exceeding 72 is 56, for \"n\" = 8. Therefore \"c\" = 8, and the remaining elements form the 4-combination at position . The successive values of formula_16 for \"n\" = 3, 4, 5, ... are 0, 1, 5, 15, 35, ..., of which the largest one not exceeding 16 is 15, for \"n\" = 6, so \"c\" = 6. Continuing similarly to search for a 3-combination at position one finds \"c\" = 3, which uses up the final unit; this establishes formula_17, and the remaining values \"c\" will be the maximal ones with formula_18, namely . Thus we have found the 5-combination }.\n\nFor each of the formula_19 lottery combinations \"c\" < \"c\" < \"c\" < \"c\" < \"c\" < \"c\" < , there is a list number \"N\" between 0 and formula_20 which can be found by adding\n\nSuppose you want to find a position of a British National Lottery result 3,6,15,17,18,35 in a list of possible results. Excel function COMBIN(49,6) would show that number of results is 13983816. Now if you put numbers 3,6,15,17,18,35 each in its cell and formulas COMBIN(49-3,6), COMBIN(49-6,5), COMBIN(49-15,4), COMBIN(49-17,3), COMBIN(49-18,2), 49−35 under each of them. Use cell references instead of actual values, the actual values are provided for readability. You would get a row of numbers of 9366819,962598,46376,4960,465,14. Adding these would show particular position 10381232. Note that you do not need use formula COMBIN(49-35,1) to get 14. You can have it by subtraction 49-35 alone. Also the COMBIN function does not return 0 in case 49-X becomes less than 6. You'd need to use IF with ISNUMBER function to convert #NUM! into 0.\n\nNow the reverse engineering is a bit trickier. You could use 49 IF statements in one cell or use solver to find maximum argument for COMBIN result to be less or equal than position number. Instead let's use a table of 6 by 49 and use MATCH function where resulting row number would be the argument and a ball number. If you make column headings from 6 to 1 (B1:G1) in descending order and row labels of 1 to 49 (A2:A50) in ascending order (vertically ascending in Excel means numbers growing from top to bottom). Then if you fill up the table with formula COMBIN($A2,B$1) starting from left top corner. $ signs will make sure that index values are always taken from heading row and label column. Replace #NUM! with zeros. You should get something like this:\n\nNow if you create a new row of six cells and fill them with formulas which would find the largest values in each column which are less than or equal to position number in question:\nThe first cell with =INDEX(B2:B50,MATCH(10381232,B2:B50)), the rest of the cells with\n\nThis would present you with a row of values you have already seen 9366819,962598,46376,4960,465,14\nNow in a next row first cell write =49-MATCH(10381232,B2:B50) and similarly\n\nAgain use the references to cells instead of actual values. You should be presented with original ball numbers of 3,6,15,17,18,35.\n\nNow you can generate a fresh Lottery number combination from single =randbetween(1,combin(49,6)) or you could look at the list position numbers of earlier results to see if there is a trend.\n\nOne could use the combinatorial number system to list or traverse all \"k\"-combinations of a given finite set, but this is a very inefficient way to do that. Indeed, given some \"k\"-combination it is much easier to find the next combination in lexicographic ordering directly than to convert a number to a \"k\"-combination by the method indicated above. To find the next combination, find the smallest \"i\" ≥ 2 for which \"c\" ≥ \"c\"+2 (if there is no such \"i\", take \"i\" = \"k\"+1); then increase \"c\" by one and set all \"c\" with to their minimal value . If the \"k\"-combination is represented by its characteristic vector, that is as a binary value with \"k\" bits 1, then the next such value can be computed without any loop using bitwise arithmetic: the following C++ function will advance x to that value or return false:\n\n// find next k-combination\nbool next_combination(unsigned long& x) // assume x has form x'01^a10^b in binary\n\nThis is called Gosper's hack;\ncorresponding assembly code was described as item 175 in HAKMEM.\n\nOn the other hand the possibility to directly generate the \"k\"-combination at index \"N\" has useful applications. Notably, it allows generating a random \"k\"-combination of an \"n\"-element set using a random integer \"N\" with formula_22, simply by converting that number to the corresponding \"k\"-combination. If a computer program needs to maintain a table with information about every \"k\"-combination of a given finite set, the computation of the index \"N\" associated with a combination will allow the table to be accessed without searching.\n\n"}
{"id": "2606126", "url": "https://en.wikipedia.org/wiki?curid=2606126", "title": "Complete numbering", "text": "Complete numbering\n\nIn computability theory complete numberings are generalizations of Gödel numbering first introduced by A.I. Mal'tsev in 1963. They are studied because several important results like the Kleene's recursion theorem and Rice's theorem, which were originally proven for the Gödel-numbered set of computable functions, still hold for arbitrary sets with complete numberings.\n\nA numbering formula_1 of a set formula_2 is called complete (with respect to an element formula_3) if for every partial computable function formula_4 there exists a total computable function formula_5 so that (Ershov 1999:482):\n\nErshov refers to the element \"a\" as a \"special\" element for the numbering. A numbering formula_1 is called precomplete if the weaker property holds:\n\n\n"}
{"id": "42074131", "url": "https://en.wikipedia.org/wiki?curid=42074131", "title": "Derived stack", "text": "Derived stack\n\nIn algebraic geometry, a derived stack is, roughly, a stack together with a sheaf of commutative ring spectra. It generalizes a derived scheme. Derived stacks are the \"spaces\" studied in derived algebraic geometry.\n\n"}
{"id": "10384312", "url": "https://en.wikipedia.org/wiki?curid=10384312", "title": "Empirical probability", "text": "Empirical probability\n\nThe empirical probability, relative frequency, or experimental probability of an event is the ratio of the number of outcomes in which a specified event occurs to the total number of trials, not in a theoretical sample space but in an actual experiment. In a more general sense, empirical probability estimates probabilities from experience and observation.\n\nGiven an event \"A\" in a sample space, the relative frequency of \"A\" is the ratio \"m/n\", \"m\" being the number of outcomes in which the event \"A\" occurs, and \"n\" being the total number of outcomes of the experiment.\n\nIn statistical terms, the empirical probability is an \"estimate\" or estimator of a probability. In simple cases, where the result of a trial only determines whether or not the specified event has occurred, modelling using a binomial distribution might be appropriate and then the empirical estimate is the maximum likelihood estimate. It is the Bayesian estimate for the same case if certain assumptions are made for the prior distribution of the probability. If a trial yields more information, the empirical probability can be improved on by adopting further assumptions in the form of a statistical model: if such a model is fitted, it can be used to derive an estimate of the probability of the specified event.\n\nAn advantage of estimating probabilities using empirical probabilities is that this procedure is relatively free of assumptions.\n\nFor example, consider estimating the probability among a population of men that they satisfy two conditions:\n\nA direct estimate could be found by counting the number of men who satisfy both conditions to give the empirical probability of the combined condition. An alternative estimate could be found by multiplying the proportion of men who are over 6 feet in height with the proportion of men who prefer strawberry jam to raspberry jam, but this estimate relies on the assumption that the two conditions are statistically independent.\n\nA disadvantage in using empirical probabilities arises in estimating probabilities which are either very close to zero, or very close to one. In these cases very large sample sizes would be needed in order to estimate such probabilities to a good standard of relative accuracy. Here statistical models can help, depending on the context, and in general one can hope that such models would provide improvements in accuracy compared to empirical probabilities, provided that the assumptions involved actually do hold.\n\nFor example, consider estimating the probability that the lowest of the daily-maximum temperatures at a site in February in any one year is less than zero degrees Celsius. A record of such temperatures in past years could be used to estimate this probability. A model-based alternative would be to select a family of probability distributions and fit it to the dataset containing past years′ values. The fitted distribution would provide an alternative estimate of the desired probability. This alternative method can provide an estimate of the probability even if all values in the record are greater than zero.\n\nThe phrase \"a-posteriori probability\" is also used as an alternative to empirical probability or relative frequency. The use of the phrase \"a-posteriori\" is reminiscent of terms in Bayesian statistics, but is not directly related to Bayesian inference, where \"a-posteriori probability\" is occasionally used to refer to posterior probability, which is different even though it has a confusingly similar name.\n\nThe term \"a-posteriori probability\", in its meaning as equivalent to empirical probability, may be used in conjunction with \"a priori probability\" which represents an estimate of a probability not based on any observations, but based an deductive reasoning.\n\n"}
{"id": "9417", "url": "https://en.wikipedia.org/wiki?curid=9417", "title": "Euclidean geometry", "text": "Euclidean geometry\n\nEuclidean geometry is a mathematical system attributed to Alexandrian Greek mathematician Euclid, which he described in his textbook on geometry: the \"Elements\". Euclid's method consists in assuming a small set of intuitively appealing axioms, and deducing many other propositions (theorems) from these. Although many of Euclid's results had been stated by earlier mathematicians, Euclid was the first to show how these propositions could fit into a comprehensive deductive and logical system. The \"Elements\" begins with plane geometry, still taught in secondary school (High School) as the first axiomatic system and the first examples of formal proof. It goes on to the solid geometry of three dimensions. Much of the \"Elements\" states results of what are now called algebra and number theory, explained in geometrical language.\n\nFor more than two thousand years, the adjective \"Euclidean\" was unnecessary because no other sort of geometry had been conceived. Euclid's axioms seemed so intuitively obvious (with the possible exception of the parallel postulate) that any theorem proved from them was deemed true in an absolute, often metaphysical, sense. Today, however, many other self-consistent non-Euclidean geometries are known, the first ones having been discovered in the early 19th century. An implication of Albert Einstein's theory of general relativity is that physical space itself is not Euclidean, and Euclidean space is a good approximation for it only over short distances (relative to the strength of the gravitational field).\n\nEuclidean geometry is an example of synthetic geometry, in that it proceeds logically from axioms describing basic properties of geometric objects such as points and lines, to propositions about those objects, all without the use of coordinates to specify those objects. This is in contrast to analytic geometry, which uses coordinates to translate geometric propositions into algebraic formulas.\n\nThe \"Elements\" is mainly a systematization of earlier knowledge of geometry. Its improvement over earlier treatments was rapidly recognized, with the result that there was little interest in preserving the earlier ones, and they are now nearly all lost.\n\nThere are 13 books in the \"Elements\":\n\nBooks I–IV and VI discuss plane geometry. Many results about plane figures are proved, for example \"In any triangle two angles taken together in any manner are less than two right angles.\" (Book 1 proposition 17 ) and the Pythagorean theorem \"In right angled triangles the square on the side subtending the right angle is equal to the squares on the sides containing the right angle.\" (Book I, proposition 47)\n\nBooks V and VII–X deal with number theory, with numbers treated geometrically as lengths of line segments or areas of regions. Notions such as prime numbers and rational and irrational numbers are introduced. It is proved that there are infinitely many prime numbers.\n\nBooks XI–XIII concern solid geometry. A typical result is the 1:3 ratio between the volume of a cone and a cylinder with the same height and base. The platonic solids are constructed.\n\nEuclidean geometry is an axiomatic system, in which all theorems (\"true statements\") are derived from a small number of simple axioms. Until the advent of non-Euclidean geometry, these axioms were considered to be obviously true in the physical world, so that all the theorems would be equally true. However, Euclid's reasoning from assumptions to conclusions remains valid independent of their physical reality. \n\nNear the beginning of the first book of the \"Elements\", Euclid gives five postulates (axioms) for plane geometry, stated in terms of constructions (as translated by Thomas Heath):\n\n\nAlthough Euclid only explicitly asserts the existence of the constructed objects, in his reasoning they are implicitly assumed to be unique.\n\nThe \"Elements\" also include the following five \"common notions\":\n\n\nTo the ancients, the parallel postulate seemed less obvious than the others. They aspired to create a system of absolutely certain propositions, and to them it seemed as if the parallel line postulate required proof from simpler statements. It is now known that such a proof is impossible, since one can construct consistent systems of geometry (obeying the other axioms) in which the parallel postulate is true, and others in which it is false. Euclid himself seems to have considered it as being qualitatively different from the others, as evidenced by the organization of the \"Elements\": his first 28 propositions are those that can be proved without it.\n\nMany alternative axioms can be formulated which are logically equivalent to the parallel postulate (in the context of the other axioms). For example, Playfair's axiom states:\n\nThe \"at most\" clause is all that is needed since it can be proved from the remaining axioms that at least one parallel line exists. \n\nEuclidean Geometry is \"constructive\". Postulates 1, 2, 3, and 5 assert the existence and uniqueness of certain geometric figures, and these assertions are of a constructive nature: that is, we are not only told that certain things exist, but are also given methods for creating them with no more than a compass and an unmarked straightedge. In this sense, Euclidean geometry is more concrete than many modern axiomatic systems such as set theory, which often assert the existence of objects without saying how to construct them, or even assert the existence of objects that cannot be constructed within the theory. Strictly speaking, the lines on paper are \"models\" of the objects defined within the formal system, rather than instances of those objects. For example, a Euclidean straight line has no width, but any real drawn line will. Though nearly all modern mathematicians consider nonconstructive methods just as sound as constructive ones, Euclid's constructive proofs often supplanted fallacious nonconstructive ones—e.g., some of the Pythagoreans' proofs that involved irrational numbers, which usually required a statement such as \"Find the greatest common measure of ...\"\n\nEuclid often used proof by contradiction. Euclidean geometry also allows the method of superposition, in which a figure is transferred to another point in space. For example, proposition I.4, side-angle-side congruence of triangles, is proved by moving one of the two triangles so that one of its sides coincides with the other triangle's equal side, and then proving that the other sides coincide as well. Some modern treatments add a sixth postulate, the rigidity of the triangle, which can be used as an alternative to superposition.\n\nEuclidean geometry has two fundamental types of measurements: angle and distance. The angle scale is absolute, and Euclid uses the right angle as his basic unit, so that, e.g., a 45-degree angle would be referred to as half of a right angle. The distance scale is relative; one arbitrarily picks a line segment with a certain nonzero length as the unit, and other distances are expressed in relation to it. Addition of distances is represented by a construction in which one line segment is copied onto the end of another line segment to extend its length, and similarly for subtraction.\n\nMeasurements of area and volume are derived from distances. For example, a rectangle with a width of 3 and a length of 4 has an area that represents the product, 12. Because this geometrical interpretation of multiplication was limited to three dimensions, there was no direct way of interpreting the product of four or more numbers, and Euclid avoided such products, although they are implied, e.g., in the proof of book IX, proposition 20.\nEuclid refers to a pair of lines, or a pair of planar or solid figures, as \"equal\" (ἴσος) if their lengths, areas, or volumes are equal, and similarly for angles. The stronger term \"congruent\" refers to the idea that an entire figure is the same size and shape as another figure. Alternatively, two figures are congruent if one can be moved on top of the other so that it matches up with it exactly. (Flipping it over is allowed.) Thus, for example, a 2x6 rectangle and a 3x4 rectangle are equal but not congruent, and the letter R is congruent to its mirror image. Figures that would be congruent except for their differing sizes are referred to as similar. Corresponding angles in a pair of similar shapes are congruent and corresponding sides are in proportion to each other.\n\nPoints are customarily named using capital letters of the alphabet. Other figures, such as lines, triangles, or circles, are named by listing a sufficient number of points to pick them out unambiguously from the relevant figure, e.g., triangle ABC would typically be a triangle with vertices at points A, B, and C.\n\nAngles whose sum is a right angle are called complementary. Complementary angles are formed when a ray shares the same vertex and is pointed in a direction that is in between the two original rays that form the right angle. The number of rays in between the two original rays is infinite.\n\nAngles whose sum is a straight angle are supplementary. Supplementary angles are formed when a ray shares the same vertex and is pointed in a direction that is in between the two original rays that form the straight angle (180 degree angle). The number of rays in between the two original rays is infinite.\n\nIn modern terminology, angles would normally be measured in degrees or radians.\n\nModern school textbooks often define separate figures called lines (infinite), rays (semi-infinite), and line segments (of finite length). Euclid, rather than discussing a ray as an object that extends to infinity in one direction, would normally use locutions such as \"if the line is extended to a sufficient length,\" although he occasionally referred to \"infinite lines\". A \"line\" in Euclid could be either straight or curved, and he used the more specific term \"straight line\" when necessary.\n\nThe Bridge of Asses (\"Pons Asinorum\") states that \"in isosceles triangles the angles at the base equal one another, and, if the equal straight lines are produced further, then the angles under the base equal one another.\" Its name may be attributed to its frequent role as the first real test in the \"Elements\" of the intelligence of the reader and as a bridge to the harder propositions that followed. It might also be so named because of the geometrical figure's resemblance to a steep bridge that only a sure-footed donkey could cross.\n\nTriangles are congruent if they have all three sides equal (SSS), two sides and the angle between them equal (SAS), or two angles and a side equal (ASA) (Book I, propositions 4, 8, and 26). Triangles with three equal angles (AAA) are similar, but not necessarily congruent. Also, triangles with two equal sides and an adjacent angle are not necessarily equal or congruent.\n\nThe sum of the angles of a triangle is equal to a straight angle (180 degrees). This causes an equilateral triangle to have three interior angles of 60 degrees. Also, it causes every triangle to have at least two acute angles and up to one obtuse or right angle.\n\nThe celebrated Pythagorean theorem (book I, proposition 47) states that in any right triangle, the area of the square whose side is the hypotenuse (the side opposite the right angle) is equal to the sum of the areas of the squares whose sides are the two legs (the two sides that meet at a right angle).\n\nThales' theorem, named after Thales of Miletus states that if A, B, and C are points on a circle where the line AC is a diameter of the circle, then the angle ABC is a right angle. Cantor supposed that Thales proved his theorem by means of Euclid Book I, Prop. 32 after the manner of Euclid Book III, Prop. 31.\n\nIn modern terminology, the area of a plane figure is proportional to the square of any of its linear dimensions, formula_1, and the volume of a solid to the cube, formula_2. Euclid proved these results in various special cases such as the area of a circle and the volume of a parallelepipedal solid. Euclid determined some, but not all, of the relevant constants of proportionality. E.g., it was his successor Archimedes who proved that a sphere has 2/3 the volume of the circumscribing cylinder.\n\nBecause of Euclidean geometry's fundamental status in mathematics, it is impractical to give more than a representative sampling of applications here.\n\nAs suggested by the etymology of the word, one of the earliest reasons for interest in geometry was surveying, and certain practical results from Euclidean geometry, such as the right-angle property of the 3-4-5 triangle, were used long before they were proved formally. The fundamental types of measurements in Euclidean geometry are distances and angles, both of which can be measured directly by a surveyor. Historically, distances were often measured by chains, such as Gunter's chain, and angles using graduated circles and, later, the theodolite.\n\nAn application of Euclidean solid geometry is the determination of packing arrangements, such as the problem of finding the most efficient packing of spheres in n dimensions. This problem has applications in error detection and correction.\n\nGeometric optics uses Euclidean geometry to analyze the focusing of light by lenses and mirrors.\n\nGeometry is used extensively in architecture.\n\nGeometry can be used to design origami. Some classical construction problems of geometry are impossible using compass and straightedge, but can be solved using origami.\n\nQuite a lot of CAD (computer-aided design) and CAM (computer-aided manufacturing) is based on Euclidean geometry. Design geometry typically consists of shapes bounded by planes, cylinders, cones, tori, etc. CAD/CAM is essential in the design of almost everything, nowadays, including cars, airplanes, ships, and the iPhone. A few decades ago, sophisticated draftsmen learned some fairly advanced Euclidean geometry, including things like Pascal's theorem and Brianchon's theorem. But now they don't have to, because the geometric constructions are all done by CAD programs.\n\nEuclid believed that his axioms were self-evident statements about physical reality. Euclid's proofs depend upon assumptions perhaps not obvious in Euclid's fundamental axioms, in particular that certain movements of figures do not change their geometrical properties such as the lengths of sides and interior angles, the so-called \"Euclidean motions\", which include translations, reflections and rotations of figures. Taken as a physical description of space, postulate 2 (extending a line) asserts that space does not have holes or boundaries (in other words, space is homogeneous and unbounded); postulate 4 (equality of right angles) says that space is isotropic and figures may be moved to any location while maintaining congruence; and postulate 5 (the parallel postulate) that space is flat (has no intrinsic curvature).\n\nAs discussed in more detail below, Einstein's theory of relativity significantly modifies this view.\n\nThe ambiguous character of the axioms as originally formulated by Euclid makes it possible for different commentators to disagree about some of their other implications for the structure of space, such as whether or not it is infinite (see below) and what its topology is. Modern, more rigorous reformulations of the system typically aim for a cleaner separation of these issues. Interpreting Euclid's axioms in the spirit of this more modern approach, axioms 1-4 are consistent with either infinite or finite space (as in elliptic geometry), and all five axioms are consistent with a variety of topologies (e.g., a plane, a cylinder, or a torus for two-dimensional Euclidean geometry).\n\nArchimedes (c. 287 BCE – c. 212 BCE), a colorful figure about whom many historical anecdotes are recorded, is remembered along with Euclid as one of the greatest of ancient mathematicians. Although the foundations of his work were put in place by Euclid, his work, unlike Euclid's, is believed to have been entirely original. He proved equations for the volumes and areas of various figures in two and three dimensions, and enunciated the Archimedean property of finite numbers.\n\nApollonius of Perga (c. 262 BCE – c. 190 BCE) is mainly known for his investigation of conic sections.\n\nRené Descartes (1596–1650) developed analytic geometry, an alternative method for formalizing geometry which focused on turning geometry into algebra.\n\nIn this approach, a point on a plane is represented by its Cartesian (\"x\", \"y\") coordinates, a line is represented by its equation, and so on.\n\nIn Euclid's original approach, the Pythagorean theorem follows from Euclid's axioms. In the Cartesian approach, the axioms are the axioms of algebra, and the equation expressing the Pythagorean theorem is then a definition of one of the terms in Euclid's axioms, which are now considered theorems.\n\nThe equation\ndefining the distance between two points \"P\" = (\"p\", \"p\") and \"Q\" = (\"q\", \"q\") is then known as the \"Euclidean metric\", and other metrics define non-Euclidean geometries.\n\nIn terms of analytic geometry, the restriction of classical geometry to compass and straightedge constructions means a restriction to first- and second-order equations, e.g., \"y\" = 2\"x\" + 1 (a line), or \"x\" + \"y\" = 7 (a circle).\n\nAlso in the 17th century, Girard Desargues, motivated by the theory of perspective, introduced the concept of idealized points, lines, and planes at infinity. The result can be considered as a type of generalized geometry, projective geometry, but it can also be used to produce proofs in ordinary Euclidean geometry in which the number of special cases is reduced.\n\nGeometers of the 18th century struggled to define the boundaries of the Euclidean system. Many tried in vain to prove the fifth postulate from the first four. By 1763, at least 28 different proofs had been published, but all were found incorrect.\n\nLeading up to this period, geometers also tried to determine what constructions could be accomplished in Euclidean geometry. For example, the problem of trisecting an angle with a compass and straightedge is one that naturally occurs within the theory, since the axioms refer to constructive operations that can be carried out with those tools. However, centuries of efforts failed to find a solution to this problem, until Pierre Wantzel published a proof in 1837 that such a construction was impossible. Other constructions that were proved impossible include doubling the cube and squaring the circle. In the case of doubling the cube, the impossibility of the construction originates from the fact that the compass and straightedge method involve equations whose order is an integral power of two, while doubling a cube requires the solution of a third-order equation.\n\nEuler discussed a generalization of Euclidean geometry called affine geometry, which retains the fifth postulate unmodified while weakening postulates three and four in a way that eliminates the notions of angle (whence right triangles become meaningless) and of equality of length of line segments in general (whence circles become meaningless) while retaining the notions of parallelism as an equivalence relation between lines, and equality of length of parallel line segments (so line segments continue to have a midpoint).\n\nIn the early 19th century, Carnot and Möbius systematically developed the use of signed angles and line segments as a way of simplifying and unifying results.\n\nThe century's most significant development in geometry occurred when, around 1830, János Bolyai and Nikolai Ivanovich Lobachevsky separately published work on non-Euclidean geometry, in which the parallel postulate is not valid. Since non-Euclidean geometry is provably relatively consistent with Euclidean geometry, the parallel postulate cannot be proved from the other postulates.\n\nIn the 19th century, it was also realized that Euclid's ten axioms and common notions do not suffice to prove all of the theorems stated in the \"Elements\". For example, Euclid assumed implicitly that any line contains at least two points, but this assumption cannot be proved from the other axioms, and therefore must be an axiom itself. The very first geometric proof in the \"Elements,\" shown in the figure above, is that any line segment is part of a triangle; Euclid constructs this in the usual way, by drawing circles around both endpoints and taking their intersection as the third vertex. His axioms, however, do not guarantee that the circles actually intersect, because they do not assert the geometrical property of continuity, which in Cartesian terms is equivalent to the completeness property of the real numbers. Starting with Moritz Pasch in 1882, many improved axiomatic systems for geometry have been proposed, the best known being those of Hilbert, George Birkhoff, and Tarski.\n\nEinstein's theory of general relativity shows that the true geometry of spacetime is not Euclidean geometry. For example, if a triangle is constructed out of three rays of light, then in general the interior angles do not add up to 180 degrees due to gravity. A relatively weak gravitational field, such as the Earth's or the sun's, is represented by a metric that is approximately, but not exactly, Euclidean. Until the 20th century, there was no technology capable of detecting the deviations from Euclidean geometry, but Einstein predicted that such deviations would exist. They were later verified by observations such as the slight bending of starlight by the Sun during a solar eclipse in 1919, and such considerations are now an integral part of the software that runs the GPS system. It is possible to object to this interpretation of general relativity on the grounds that light rays might be improper physical models of Euclid's lines, or that relativity could be rephrased so as to avoid the geometrical interpretations. However, one of the consequences of Einstein's theory is that there is no possible physical test that can distinguish between a beam of light as a model of a geometrical line and any other physical model. Thus, the only logical possibilities are to accept non-Euclidean geometry as physically real, or to reject the entire notion of physical tests of the axioms of geometry, which can then be imagined as a formal system without any intrinsic real-world meaning.\n\nEuclid sometimes distinguished explicitly between \"finite lines\" (e.g., Postulate 2) and \"infinite lines\" (book I, proposition 12). However, he typically did not make such distinctions unless they were necessary. The postulates do not explicitly refer to infinite lines, although for example some commentators interpret postulate 3, existence of a circle with any radius, as implying that space is infinite.\n\nThe notion of infinitesimal quantities had previously been discussed extensively by the Eleatic School, but nobody had been able to put them on a firm logical basis, with paradoxes such as Zeno's paradox occurring that had not been resolved to universal satisfaction. Euclid used the method of exhaustion rather than infinitesimals.\n\nLater ancient commentators, such as Proclus (410–485 CE), treated many questions about infinity as issues demanding proof and, e.g., Proclus claimed to prove the infinite divisibility of a line, based on a proof by contradiction in which he considered the cases of even and odd numbers of points constituting it.\n\nAt the turn of the 20th century, Otto Stolz, Paul du Bois-Reymond, Giuseppe Veronese, and others produced controversial work on non-Archimedean models of Euclidean geometry, in which the distance between two points may be infinite or infinitesimal, in the Newton–Leibniz sense. Fifty years later, Abraham Robinson provided a rigorous logical foundation for Veronese's work.\n\nOne reason that the ancients treated the parallel postulate as less certain than the others is that verifying it physically would require us to inspect two lines to check that they never intersected, even at some very distant point, and this inspection could potentially take an infinite amount of time.\n\nThe modern formulation of proof by induction was not developed until the 17th century, but some later commentators consider it implicit in some of Euclid's proofs, e.g., the proof of the infinitude of primes.\n\nSupposed paradoxes involving infinite series, such as Zeno's paradox, predated Euclid. Euclid avoided such discussions, giving, for example, the expression for the partial sums of the geometric series in IX.35 without commenting on the possibility of letting the number of terms become infinite.\n\nEuclid frequently used the method of proof by contradiction, and therefore the traditional presentation of Euclidean geometry assumes classical logic, in which every proposition is either true or false, i.e., for any proposition P, the proposition \"P or not P\" is automatically true.\n\nPlacing Euclidean geometry on a solid axiomatic basis was a preoccupation of mathematicians for centuries. The role of primitive notions, or undefined concepts, was clearly put forward by Alessandro Padoa of the Peano delegation at the 1900 Paris conference: \n\nThat is, mathematics is context-independent knowledge within a hierarchical framework. As said by Bertrand Russell:\nSuch foundational approaches range between foundationalism and formalism.\n\n\nThe process of abstract axiomatization as exemplified by Hilbert's axioms reduces geometry to theorem proving or predicate logic. In contrast, the Greeks used construction postulates, and emphasized problem solving. For the Greeks, constructions are more primitive than existence propositions, and can be used to prove existence propositions, but not \"vice versa\". To describe problem solving adequately requires a richer system of logical concepts. The contrast in approach may be summarized:\n\nAndrei Nicholaevich Kolmogorov proposed a problem solving basis for geometry. This work was a precursor of a modern formulation in terms of constructive type theory. This development has implications for pedagogy as well.\n\n\n\n"}
{"id": "5619296", "url": "https://en.wikipedia.org/wiki?curid=5619296", "title": "Expensive Desk Calculator", "text": "Expensive Desk Calculator\n\nExpensive Desk Calculator by Robert A. Wagner is thought to be computing's first interactive calculation program.\n\nThe software first ran on the TX-0 computer loaned to the Massachusetts Institute of Technology (MIT) by Lincoln Laboratory. It was ported to the PDP-1 donated to MIT in 1961 by Digital Equipment Corporation.\n\nFriends from the MIT Tech Model Railroad Club, Wagner and a group of fellow students had access to these room-sized machines outside classes, signing up for time during off hours. Overseen by Jack Dennis, John McKenzie and faculty advisors, they were personal computer users as early as the late 1950s.\n\nThe calculators Wagner needed to complete his numerical analysis homework were across campus and in short supply so he wrote one himself. Although the program has about three thousand lines of code and took months to write, Wagner received a grade of zero on his homework. His professor's reaction was, \"You used a computer! This \"can't\" be right.\" Steven Levy wrote, \"The professor would learn in time, as would everyone, that the world opened up by the computer was a limitless one.\"\n\n"}
{"id": "13040642", "url": "https://en.wikipedia.org/wiki?curid=13040642", "title": "Fortunate number", "text": "Fortunate number\n\nA Fortunate number, named after Reo Fortune, is the smallest integer \"m\" > 1 such that, for a given positive integer \"n\", \"p\"# + \"m\" is a prime number, where the primorial \"p\"# is the product of the first \"n\" prime numbers.\n\nFor example, to find the seventh Fortunate number, one would first calculate the product of the first seven primes (2, 3, 5, 7, 11, 13 and 17), which is 510510. Adding 2 to that gives another even number, while adding 3 would give another multiple of 3. One would similarly rule out the integers up to 18. Adding 19, however, gives 510529, which is prime. Hence 19 is a Fortunate number. The Fortunate number for \"p\"# is always above \"p\" and all its divisors are larger than \"p\". This is because \"p\"#, and thus \"p\"# + \"m\", is divisible by the prime factors of \"m\" not larger than \"p\".\n\nThe Fortunate numbers for the first primorials are:\n\nThe Fortunate numbers sorted in numerical order with duplicates removed:\n\nReo Fortune conjectured that no Fortunate number is composite (\"Fortune's conjecture\"). A Fortunate prime is a Fortunate number which is also a prime number. , all the known Fortunate numbers are prime.\n\n"}
{"id": "10324687", "url": "https://en.wikipedia.org/wiki?curid=10324687", "title": "Geometric median", "text": "Geometric median\n\nThe geometric median of a discrete set of sample points in a Euclidean space is the point minimizing the sum of distances to the sample points. This generalizes the median, which has the property of minimizing the sum of distances for one-dimensional data, and provides a central tendency in higher dimensions. It is also known as the 1-median, spatial median, Euclidean minisum point, or Torricelli point.\n\nThe geometric median is an important estimator of location in statistics, where it is also known as the \"L\" estimator. It is also a standard problem in facility location, where it models the problem of locating a facility to minimize the cost of transportation.\n\nThe special case of the problem for three points in the plane (that is, \"m\" = 3 and \"n\" = 2 in the definition below) is sometimes also known as Fermat's problem; it arises in the construction of minimal Steiner trees, and was originally posed as a problem by Pierre de Fermat and solved by Evangelista Torricelli. Its solution is now known as the Fermat point of the triangle formed by the three sample points. The geometric median may in turn be generalized to the problem of minimizing the sum of \"weighted\" distances, known as the Weber problem after Alfred Weber's discussion of the problem in his 1909 book on facility location. Some sources instead call Weber's problem the Fermat–Weber problem, but others use this name for the unweighted geometric median problem.\n\nFormally, for a given set of \"m\" points formula_1 with each formula_2, the geometric median is defined as\n\nHere, arg min means the value of the argument formula_4 which minimizes the sum. In this case, it is the point formula_4 from where the sum of all Euclidean distances to the formula_6's is minimum.\n\n\n\nDespite the geometric median's being an easy-to-understand concept, computing it poses a challenge. The centroid or center of mass, defined similarly to the geometric median as minimizing the sum of the \"squares\" of the distances to each point, can be found by a simple formula — its coordinates are the averages of the coordinates of the points — but it has been shown that no explicit formula, nor an exact algorithm involving only arithmetic operations and \"k\"th roots, can exist in general for the geometric median. Therefore, only numerical or symbolic approximations to the solution of this problem are possible under this model of computation.\n\nHowever, it is straightforward to calculate an approximation to the geometric median using an iterative procedure in which each step produces a more accurate approximation. Procedures of this type can be derived from the fact that the sum of distances to the sample points is a convex function, since the distance to each sample point is convex and the sum of convex functions remains convex. Therefore, procedures that decrease the sum of distances at each step cannot get trapped in a local optimum.\n\nOne common approach of this type, called Weiszfeld's algorithm after the work of Endre Weiszfeld, is a form of iteratively re-weighted least squares. This algorithm defines a set of weights that are inversely proportional to the distances from the current estimate to the samples, and creates a new estimate that is the weighted average of the samples according to these weights. That is,\nThis method converges for almost all initial positions, but may fail to converge when one of its estimates falls on one of the given points. It can be modified to handle these cases so that it converges for all initial points.\n\nIf \"y\" is distinct from all the given points, \"x\", then \"y\" is the geometric median if and only if it satisfies:\n\nThis is equivalent to:\n\nwhich is closely related to Weiszfeld's algorithm.\n\nIn general, \"y\" is the geometric median if and only if there are vectors \"u\" such that:\nwhere for \"x\" ≠ \"y\", \nand for \"x\" = \"y\", \nAn equivalent formulation of this condition is\n\nIt can be seen as a generalization of the median property, in the sense that any partition of the points, in particular as induced by any hyperplane through \"y\", has the same and opposite sum of positive \"directions\" from \"y\" on each side. In the one dimensional case, the hyperplane is the point \"y\" itself, and the sum of directions simplifies to the (directed) counting measure.\n\nThe geometric median can be generalized from Euclidean spaces to general Riemannian manifolds (and even metric spaces) using the same idea which is used to define the Fréchet mean on a Riemannian manifold. Let formula_14 be a Riemannian manifold with corresponding distance function formula_15, let formula_16 be formula_17 weights summing to 1, and let formula_18\nbe formula_17 observations from formula_14. Then we define the weighted geometric median formula_21 (or weighted Fréchet median) of the data points as\nIf all the weights are equal, we say simply that formula_21 is the geometric median.\n\n"}
{"id": "32182729", "url": "https://en.wikipedia.org/wiki?curid=32182729", "title": "Gradient-like vector field", "text": "Gradient-like vector field\n\nIn differential topology, a mathematical discipline, and more specifically in Morse theory, a gradient-like vector field is a generalization of gradient vector field.\n\nThe primary motivation is as a technical tool in the construction of Morse functions, to show that one can construct a function whose critical points are at distinct levels. One first constructs a Morse function, then uses gradient-like vector fields to move around the critical points, yielding a different Morse function.\n\nGiven a Morse function \"f\" on a manifold \"M,\" a gradient-like vector field \"X\" for the function \"f\" is, informally:\nFormally:\nand on which \"X\" equals the gradient of \"f.\"\n\nThe associated dynamical system of a gradient-like vector field, a gradient-like dynamical system, is a special case of a Morse–Smale system.\n\n"}
{"id": "22437506", "url": "https://en.wikipedia.org/wiki?curid=22437506", "title": "Group family", "text": "Group family\n\nIn probability theory, especially as that field is used in statistics, a group family of probability distributions is a family obtained by subjecting a random variable with a fixed distribution to a suitable family of transformations such as a location-scale family, or otherwise a family of probability distributions acted upon by a group.\n\nConsideration of a particular family of distributions as a group family can, in statistical theory, lead to the identification of an ancillary statistic.\n\nA group family can be generated by subjecting a random variable with a fixed distribution to some suitable transformations. Different types of group families are as follows :\n\nThis family is obtained by adding a constant to a random variable. Let formula_1 be a random variable and formula_2 be a constant. Let formula_3 . Then formula_4For a fixed distribution , as formula_5 varies from formula_6 to formula_7 , the distributions that we obtain constitute the location family.\n\nThis family is obtained by multiplying a random variable with a constant. Let formula_1 be a random variable and formula_9 be a constant. Let formula_10 . Thenformula_11\n\nThis family is obtained by multiplying a random variable with a constant and then adding some other constant to it. Let formula_12 be a random variable , formula_2 and formula_9be constants. Let formula_15. Then \n\nformula_16\n\nNote that it is important that formula_17 and formula_18 in order to satisfy the properties mentioned in the following section.\n\nThe transformation applied to the random variable must satisfy the following properties.\n"}
{"id": "3133115", "url": "https://en.wikipedia.org/wiki?curid=3133115", "title": "Hadwiger–Nelson problem", "text": "Hadwiger–Nelson problem\n\nIn geometric graph theory, the Hadwiger–Nelson problem, named after Hugo Hadwiger and Edward Nelson, asks for the minimum number of colors required to color the plane such that no two points at distance 1 from each other have the same color. The answer is unknown, but has been narrowed down to one of the numbers 5, 6 or 7. The correct value may depend on the choice of axioms for set theory.\n\nThe question can be phrased in graph theoretic terms as follows. Let \"G\" be the unit distance graph of the plane: an infinite graph with all points of the plane as vertices and with an edge between two vertices if and only if the distance between the two points is 1. The Hadwiger–Nelson problem is to find the chromatic number of \"G\". As a consequence, the problem is often called \"finding the chromatic number of the plane\". By the de Bruijn–Erdős theorem, a result of , the problem is equivalent (under the assumption of the axiom of choice) to that of finding the largest possible chromatic number of a finite unit distance graph.\n\nAccording to , the problem was first formulated by E. Nelson in 1950, and first published by . had earlier published a related result, showing that any cover of the plane by five congruent closed sets contains a unit distance in one of the sets, and he also mentioned the problem in a later paper . discusses the problem and its history extensively.\n\nThe fact that the chromatic number of the plane must be at least four follows from the existence of a seven-vertex unit distance graph with chromatic number four, named the Moser spindle after its discovery in 1961 by the brothers William and Leo Moser. This graph consists of two unit equilateral triangles joined at a common vertex–\"x\". Each of these triangles is joined along another edge to another equilateral triangle; the vertices \"y\" and \"z\" of these joined triangles are at unit distance from each other. If the plane could be three-colored, the coloring within the triangles would force \"y\" and \"z\" to both have the same color as \"x\", but then, since \"y\" and \"z\" are at unit distance from each other, we would not have a proper coloring of the unit distance graph of the plane. Therefore, at least four colors are needed to color this graph and the plane containing it. An alternative lower bound in the form of a ten-vertex four-chromatic unit distance graph, the Golomb graph, was discovered at around the same time by Solomon W. Golomb.\n\nIn 2018, biologist Aubrey de Grey found a 1581-vertex, non-4-colourable unit-distance graph. The proof is computer assisted. Mathematician Gil Kalai and computer scientist Scott Aaronson posted discussion of de Grey's finding, with Aaronson reporting independent verifications of de Grey's result using SAT solvers. Kalai linked additional posts by Jordan Ellenberg and Noam Elkies, with Elkies and (separately) de Grey proposing a Polymath project to find non-4-colorable unit distance graphs with fewer vertices than the one in de Grey's construction. The smallest known graph with chromatic number 5 has 553 vertices . The page of the Polymath project, , contains further research and media citations and verification data.\n\nThe upper bound of seven on the chromatic number follows from the existence of a tessellation of the plane by regular hexagons, with diameter slightly less than one, that can be assigned seven colors in a repeating pattern to form a 7-coloring of the plane. According to , this upper bound was first observed by John R. Isbell.\n\nThe problem can easily be extended to higher dimensions. In particular, finding the chromatic number of space usually refers to the 3-dimensional version. As with the version on the plane, the answer is not known, but has been shown to be at least 6 and at most 15.\n\nIn the n-dimensional case of the problem, an easy upper bound on the number of required colorings found from tiling n-dimensional cubes is formula_1. A lower bound from simplexes is formula_2. For formula_3, a lower bound of formula_4 is available using a generalization of the Moser spindle: a pair of the objects (each 2 simplexes glued together on a facet) which are joined on 1 side by a point and the other side by a line.\n\nOne can also consider colorings of the plane in which the sets of points of each color are restricted to sets of some particular type. Such restrictions may cause the required number of colors to increase, as they prevent certain colorings from being considered acceptable. For instance, if a coloring of the plane consists of regions bounded by Jordan curves, then at least six colors are required.\n\n\n\n"}
{"id": "12383591", "url": "https://en.wikipedia.org/wiki?curid=12383591", "title": "Hyperarithmetical theory", "text": "Hyperarithmetical theory\n\nIn recursion theory, hyperarithmetic theory is a generalization of Turing computability. It has close connections with definability in second-order arithmetic and with weak systems of set theory such as Kripke–Platek set theory. It is an important tool in effective descriptive set theory.\n\nThe central focus of hyperarithmetic theory is the sets of natural numbers known as hyperarithmetic sets. There are three equivalent ways of defining this class of sets; the study of the relationships between these different definitions is one motivation for the study of hyperarithmetical theory.\n\nThe first definition of the hyperarithmetic sets uses the analytical hierarchy. \nA set of natural numbers is classified at level formula_1 of this hierarchy if it is definable by a formula of second-order arithmetic with only existential set quantifiers and no other set quantifiers. A set is classified at level formula_2 of the analytical hierarchy if it is definable by a formula of second-order arithmetic with only universal set quantifiers and no other set quantifiers. A set is formula_3 if it is both formula_1 and formula_2. The hyperarithmetical sets are exactly the formula_3 sets.\n\nThe definition of hyperarithmetical sets as formula_3 does not directly depend on computability results. A second, equivalent, definition shows that the hyperarithmetical sets can be defined using infinitely iterated Turing jumps. This second definition also shows that the hyperarithmetical sets can be classified into a hierarchy extending the arithmetical hierarchy; the hyperarithmetical sets are exactly the sets that are assigned a rank in this hierarchy.\n\nEach level of the hyperarithmetical hierarchy corresponds to a countable ordinal number (ordinal), but not all countable ordinals correspond to a level of the hierarchy. The ordinals used by the hierarchy are those with an ordinal notation, which is a concrete, effective description of the ordinal.\n\nAn ordinal notation is an effective description of a countable ordinal by a natural number. A system of ordinal notations is required in order to define the hyperarithmetic hierarchy. The fundamental property an ordinal notation must have is that it describes the ordinal in terms of small ordinals in an effective way. The following inductive definition is typical; it uses a pairing function formula_8.\n\nThere are only countably many ordinal notations, since each notation is a natural number; thus there is a countable ordinal which is the supremum of all ordinals that have a notation. This ordinal is known as the Church-Kleene ordinal and is denoted formula_14. Note that this ordinal is still countable, the symbol being only an analogy with the first uncountable ordinal, formula_15. The set of all natural numbers that are ordinal notations is denoted formula_16 and called \"Kleene's formula_16\".\n\nOrdinal notations are used to define iterated Turing jumps. These are sets of natural numbers denoted formula_18 for each formula_19. Suppose that δ has notation \"e\". The set formula_18 is defined using \"e\" as follows.\nAlthough the construction of formula_18 depends on having a fixed notation for δ, and each infinite ordinal has many notations, a theorem of Spector shows that the Turing degree of formula_18 depends only on δ, not on the particular notation used, and thus formula_18 is well defined up to Turing degree.\n\nThe hyperarithmetical hierarchy is defined from these iterated Turing jumps. A set \"X\" of natural numbers is classified at level δ of the hyperarithmetical hierarchy, for formula_19, if \"X\" is Turing reducible to formula_18. There will always be a least such δ if there is any; it is this least δ that measures the level of uncomputability of \"X\".\n\nA third characterization of the hyperarithmetical sets, due to Kleene, uses higher-type computable functionals. The type-2 functional formula_37 is defined by the following rules:\nUsing a precise definition of computability relative to a type-2 functional, Kleene showed that a set of natural numbers is hyperarithmetical if and only if it is computable relative to formula_40.\n\nEvery arithmetical set is hyperarithmetical, but there are many other hyperarithmetical sets. One example of a hyperarithmetical, nonarithmetical set is the set \"T\" of Gödel numbers of formulas of Peano arithmetic that are true in the standard natural numbers formula_41. The set \"T\" is Turing equivalent to the set formula_42, and so is not high in the hyperarithmetical hierarchy, although it is not arithmetically definable by Tarski's indefinability theorem.\n\nThe fundamental results of hyperarithmetic theory show that the three definitions above define the same collection of sets of natural numbers. These equivalences are due to Kleene.\n\nCompleteness results are also fundamental to the theory. A set of natural numbers is formula_2 complete if it is at level formula_2 of the analytical hierarchy and every formula_2 set of natural numbers is many-one reducible to it. The definition of a formula_2 complete subset of Baire space (formula_47) is similar. Several sets associated with hyperarithmetic theory are formula_2 complete:\n\nResults known as formula_1 bounding follow from these completeness results. For any formula_1 set \"S\" of ordinal notations, there is an formula_54 such that every element of \"S\" is a notation for an ordinal less than formula_55. For any subset \"T\" of Baire space consisting only of characteristic functions of well orderings, there is an formula_54 such that each ordinal represented in \"T\" is less than formula_55.\n\nThe definition of formula_16 can be relativized to a set \"X\" of natural numbers: in the definition of an ordinal notation, the clause for limit ordinals is changed so that the computable enumeration of a sequence of ordinal notations is allowed to use \"X\" as an oracle. The set of numbers that are ordinal notations relative to \"X\" is denoted formula_59. The supremum of ordinals represented in formula_59 is denoted formula_61; this is a countable ordinal no smaller than formula_14.\n\nThe definition of formula_18 can also be relativized to an arbitrary set formula_64 of natural numbers. The only change in the definition is that formula_65 is defined to be \"X\" rather than the empty set, so that formula_66 is the Turing jump of \"X\", and so on. Rather than terminating at formula_14 the hierarchy relative to \"X\" runs through all ordinals less than formula_61.\n\nThe relativized hyperarithmetical hierarchy is used to define hyperarithmetical reducibility. Given sets \"X\" and \"Y\", we say formula_69 if and only if there is a formula_70 such that \"X\" is Turing reducible to formula_71. If formula_69 and formula_73 then the notation formula_74 is used to indicate \"X\" and \"Y\" are hyperarithmetically equivalent. This is a coarser equivalence relation than Turing equivalence; for example, every set of natural numbers is hyperarithmetically equivalent to its Turing jump but not Turing equivalent to its Turing jump. The equivalence classes of hyperarithmetical equivalence are known as hyperdegrees.\n\nThe function that takes a set \"X\" to formula_59 is known as the hyperjump by analogy with the Turing jump. Many properties of the hyperjump and hyperdegrees have been established. In particular, it is known that Post's problem for hyperdegrees has a positive answer: for every set \"X\" of natural numbers there is a set \"Y\" of natural numbers such that formula_76.\n\nHyperarithmetical theory is generalized by α-recursion theory, which is the study of definable subsets of admissible ordinals. Hyperarithmetical theory is the special case in which α is formula_14.\n\n\n"}
{"id": "6892618", "url": "https://en.wikipedia.org/wiki?curid=6892618", "title": "Induced subgraph isomorphism problem", "text": "Induced subgraph isomorphism problem\n\nIn complexity theory and graph theory, induced subgraph isomorphism is an NP-complete decision problem that involves finding a given graph as an induced subgraph of a larger graph.\n\nFormally, the problem takes as input two graphs \"G\"=(\"V\", \"E\") and \"G\"=(\"V\", \"E\"), where the number of vertices in \"V\" can be assumed to be less than or equal to the number of vertices in \"V\". \"G\" is isomorphic to an induced subgraph of \"G\" if there is an injective function \"f\" which maps the vertices of \"G\" to vertices of \"G\" such that for all pairs of vertices \"x\", \"y\" in \"V\", edge (\"x\", \"y\") is in \"E\" if and only if the edge (\"f\"(\"x\"), \"f\"(\"y\")) is in \"E\". The answer to the decision problem is yes if this function \"f\" exists, and no otherwise.\n\nThis is different from the subgraph isomorphism problem in that the absence of an edge in \"G\" implies that the corresponding edge in \"G\" must also be absent. In subgraph isomorphism, these \"extra\" edges in \"G\" may be present.\n\nThe complexity of induced subgraph isomorphism separates outerplanar graphs from their generalization series-parallel graphs: it may be solved in polynomial time for 2-connected outerplanar graphs, but is NP-complete for 2-connected series-parallel graphs.\n\nThe special case of finding a long path as an induced subgraph of a hypercube has been particularly well-studied, and is called the snake-in-the-box problem. The maximum independent set problem is also an induced subgraph isomorphism problem in which one seeks to find a large independent set as an induced subgraph of a larger graph, and the maximum clique problem is an induced subgraph isomorphism problem in which one seeks to find a large clique graph as an induced subgraph of a larger graph.\n\nAlthough the induced subgraph isomorphism problem seems only slightly different from the subgraph isomorphism problem, the \"induced\" restriction introduces changes large enough that we can witness differences from a computational complexity point of view. \n\nFor example, the subgraph isomorphism problem is NP-complete on connected proper interval graphs and on connected bipartite permutation graphs, but the induced subgraph isomorphism problem can be solved in polynomial time on these two classes.\n\nMoreover, the induced subtree isomorphism problem (i.e. the induced subgraph isomorphism problem where \"G\" is restricted to be a tree) can be solved in polynomial time on interval graphs, while the subtree isomorphism problem is NP-complete on proper interval graphs.\n"}
{"id": "32144185", "url": "https://en.wikipedia.org/wiki?curid=32144185", "title": "International Society for Computational Biology Student Council", "text": "International Society for Computational Biology Student Council\n\nThe International Society for Computational Biology Student Council (ISCB-SC) is a dedicated section of the International Society for Computational Biology created in 2004. It is composed by students from all levels (undergraduates, postgraduates, and postdoctoral researchers) in the fields of bioinformatics and computational biology. The organisation promotes the development of the students' community worldwide by organizing different events including symposia, workshops, webinars, internship coordination and hackathons. A special focus is made on the development of soft skills in order to develop potential in bioinformatics and computational biology students around the world.\n\nThe ISCB-SC is composed of Regional Student Groups (RSGs) which are located in various regions across the globe. Since its foundation, the ISCB-SC has gained representation in most continents through over thirty RSGs which collectively represent more than 1,000 students and researchers around the world.\n\nThe ISCB Student Council was officially established in 2004 by a vote from the ISCB Board of Directors at the ISMB/ECCB meeting. The concept of the student council was first proposed by Manuel Corpas in late 2003. The establishment of the student council was the first move of the ISCB to include the new generation of computational biologists into the, then newly emerging, field.\n\nThe first meeting had about 30 students and eight members of the ISCB leadership at the Intelligent Systems for Molecular Biology (ISMB) 2005 in Detroit, Michigan\nThe first official Student Council Symposium (SCS) was held later that year at the European Conference on Computational Biology (ECCB) meeting in Madrid, Spain. The first SCS had approximately 100 attendees and was the beginning of a long-running series of meetings organized and highlighting students. Other than the first instance of SCS, all later editions have as a satellite meeting of ISMB. In 2012, the SC started a second set of symposia that takes place at ECCB when it is not co-located with ISMB; the European Student Council Symposium (ESCS) regularly has more attendees than SCS when both events are held in the same year. Recently, the SC symposia have also been held along with other ISCB conferences like the case for the Latin American Student Council Symposium (LA-SCS) which is held every two years along with the Latin American ISCB Conference since 2014 or the ISCB Student Council Symposium in Africa (SCS-Africa) which is held along the ISCB Africa ASBCB conference since 2015.\n\nIn 2006, the student council created the Regional Student Group (RSG) initiative to tailor the efforts of the council for the local community. RSGs are locally managed groups established at various regions. Each RSG operates either under the aegis of a higher education institution or as an independent organisation sub-affiliated to local computational biology or bioinformatics organisation. RSGs are provided support from both the student council and other local groups in their region and often organize multi-RSG events such as BeNeLuxFra. , the global network of the student council includes over 25 active RSGs operating across five continents.\n\nThe RSG committee was set up in 2008 to oversee and coordinates the activities of RSGs across the world. The team comprises an RSG-Chair, a co-chair and five vice-chairs corresponding to geographical regions (Asia-Pacific, Europe, Africa, Latin America and USA-Canada). The RSG chair is required to have previous experience as a president or secretary of an RSG. Currently, around 2,000 students and researchers are active members of RSG-related events.\n\nThe ISCB-SC continuously looks for PhD students or postdocs who may be interested in starting a new RSG in a region where the student council has no presence yet. The student council also supports existing RSGs by advising them on how to grow, in terms of the events and projects they face as well as economically. There are 3 funding calls every year where RSGs can submit proposals containing different types of events which will be evaluated by the continental RSG chairs and the Executive Committee. Since the creation of the Student Council \"F1000Research\" channel in 2016, achievements by RSGs throughout each year are discussed in editorial articles.\n\nThe ISCB-SC runs an internship program in which researchers throughout the world can offer a place for a student from a developing nation to join his/her group for a period of time. This program aims to give students from developing nations the opportunity to have an experience in a working environment from first class research groups and interact with expert Principal Investigators around the world.\n\nAs a worldwide organization, the SC organizes symposia, workshops and different events across the globe. After more than ten years of existence, many new experiences have been gained on how to deal with different obstacles that arise when trying to tackle complex objectives. Learning experiences are published as articles that aim to help new RSGs, as well as other student organizations, including bioinformatics communities, to organize their own events and strengthen their growth.\n\nThe Student Council is well recognized by members of the scientific community\nand its members are also recognized as outstanding members of their community\n\nThe Student Council also assists in efforts to improve the quality of public reference material\n\nFinally, highlights from all major symposia from both the Student Council and from the Regional Student Groups have been periodically published over the years (2007, 2008, 2009, 2010, 2011,\n2012,\n2014, 2015, 2016 and 2017\n\nSince its conception, the ISCB Student Council has not only advocated for the students in the ISCB and beyond but, also, the SC has made lots of efforts to help educate new students.\n\nIn 2014, the ISCB Student Council began publishing a collection of articles in the \"PLOS\" journals that covers details about the development of the SC and how to advance in the field. As a collective effort because of reaching a decade of story, different Student Council leaders from different regions in the world, came together and published 12 articles as part of the \"Stories from the road: ISCB Student Council Collection\" that summarize the experience and lessons that were learned through those years. These articles range in topic from starting and expanding your scientific network, dealing with the frustration of failure, to disseminating science to the public. In particular, the article \"Explain Bioinformatics to Your Grandmother!\" that aims to simplify the answer to the question \"What is bioinformatics?\" to non-scientists, has accumulated more than 17,000 views (as of 2017) and constitutes one of the most read articles in \"PLOS Computational Biology\".\n\nThe Student Council also has its own article channel on \"F1000Research\" which features publications highlighting the activities of the council.\n\nIn addition to the collections in F1000 and PLOS, the Student Council has publications that have appeared in \"BMC Bioinformatics\" and other journals.\n\nThe student council organizes several symposia each year to coincide with the major ISCB Conferences\n\nIn addition to the main ISCB-SC events that accompany the main ISCB conference(s), RSGs also hold events on a regular basis; these include annual events in Argentina, Germany and the UK.\n\nEvery year, ISCB and SC members elect new leadership positions. The positions are filled by students and/or postdoctoral members of the SC. The key mission of the SC's Executive Team is to lead the sustainable development of the organisation and its RSGs, and managing the coordination of all activities.\n\nIn addition, the SC aims to coordinate and integrate efforts of all members and volunteers who contribute their time. For the purpose of better organizational structure, the SC has established several committees that are chaired by SC members and managed by the executive team. Below is the list of the SC's committee's with description of each committee's responsibilities:\n"}
{"id": "24585634", "url": "https://en.wikipedia.org/wiki?curid=24585634", "title": "Lagrangian system", "text": "Lagrangian system\n\nIn mathematics, a Lagrangian system is a pair , consisting of a smooth fiber bundle and a Lagrangian density , which yields the Euler–Lagrange differential operator acting on sections of .\n\nIn classical mechanics, many dynamical systems are Lagrangian systems. The configuration space of such a Lagrangian system is a fiber bundle over the time axis . In particular, if a reference frame is fixed. In classical field theory, all field systems are the Lagrangian ones.\n\nA Lagrangian density (or, simply, a Lagrangian) of order is defined as an -form, , on the -order jet manifold of .\n\nA Lagrangian can be introduced as an element of the variational bicomplex of the differential graded algebra of exterior forms on jet manifolds of . The coboundary operator of this bicomplex contains the variational operator which, acting on , defines the associated Euler–Lagrange operator .\n\nGiven bundle coordinates on a fiber bundle and the adapted coordinates , , ) on jet manifolds , a Lagrangian and its Euler–Lagrange operator read\n\nwhere\n\ndenote the total derivatives.\n\nFor instance, a first-order Lagrangian and its second-order Euler–Lagrange operator take the form\n\nThe kernel of an Euler–Lagrange operator provides the Euler–Lagrange equations .\n\nCohomology of the variational bicomplex leads to the so-called\nvariational formula\n\nwhere\n\nis the total differential and is a Lepage equivalent of . Noether's first theorem and Noether's second theorem are corollaries of this variational formula.\n\nExtended to graded manifolds, the variational bicomplex provides description of graded Lagrangian systems of even and odd variables.\n\nIn a different way, Lagrangians, Euler–Lagrange operators and Euler–Lagrange equations are introduced in the framework of the calculus of variations.\n\nIn classical mechanics equations of motion are first and second order differential equations on a manifold or various fiber bundles over . A solution of the equations of motion is called a \"motion\".\n\n"}
{"id": "660651", "url": "https://en.wikipedia.org/wiki?curid=660651", "title": "Laws of Form", "text": "Laws of Form\n\nLaws of Form (hereinafter LoF) is a book by G. Spencer-Brown, published in 1969, that straddles the boundary between mathematics and philosophy. \"LoF\" describes three distinct logical systems:\n\n\"Boundary algebra\" is Meguire's (2011) term for the union of the primary algebra and the primary arithmetic. \"Laws of Form\" sometimes loosely refers to the \"primary algebra\" as well as to \"LoF\".\n\n\"LoF\" emerged from work in electronic engineering its author did around 1960, and from subsequent lectures on mathematical logic he gave under the auspices of the University of London's extension program. \"LoF\" has appeared in several editions, the most recent being a 1997 German translation, and has never gone out of print.\n\nThe mathematics fills only about 55pp and is rather elementary. But \"LoF\"'s mystical and declamatory prose, and its love of paradox, make it a challenging read for all. Spencer-Brown was influenced by Wittgenstein and R. D. Laing. \"LoF\" also echoes a number of themes from the writings of Charles Sanders Peirce, Bertrand Russell, and Alfred North Whitehead.\n\nThe entire book is written in an operational way, giving instructions to the reader instead of telling him what \"is\". In accordance with G. Spencer-Brown's interest in paradoxes, the only sentence that makes a statement that something is, is the statement, which says no such statements are used in this book. Except for this one sentence the book can be seen as an example of E-Prime.\n\nOstensibly a work of formal mathematics and philosophy, \"LoF\" became something of a cult classic, praised in the \"Whole Earth Catalog\". Those who agree point to \"LoF\" as embodying an enigmatic \"mathematics of consciousness,\" its algebraic symbolism capturing an (perhaps even \"the\") implicit root of cognition: the ability to \"distinguish\". \"LoF\" argues that primary algebra reveals striking connections among logic, Boolean algebra, and arithmetic, and the philosophy of language and mind.\n\nBanaschewski (1977) argues that the primary algebra is nothing but new notation for Boolean algebra. Indeed, the two-element Boolean algebra 2 can be seen as the intended interpretation of the primary algebra. Yet the notation of the primary algebra:\nMoreover, the syntax of the primary algebra can be extended to formal systems other than 2 and sentential logic, resulting in boundary mathematics (see Related Work below).\n\n\"LoF\" has influenced, among others, Heinz von Foerster, Louis Kauffman, Niklas Luhmann, Humberto Maturana, Francisco Varela and William Bricken. Some of these authors have modified the primary algebra in a variety of interesting ways.\n\n\"LoF\" claimed that certain well-known mathematical conjectures of very long standing, such as the Four Color Theorem, Fermat's Last Theorem, and the Goldbach conjecture, are provable using extensions of the primary algebra. Spencer-Brown eventually circulated a purported proof of the Four Color Theorem, but it met with skepticism.\n\nThe symbol:\n\nalso called the \"mark\" or \"cross\", is the essential feature of the Laws of Form. In Spencer-Brown's inimitable and enigmatic fashion, the Mark symbolizes the root of cognition, i.e., the dualistic Mark indicates the capability of differentiating a \"this\" from \"everything else \"but\" this.\"\n\nIn \"LoF\", a Cross denotes the drawing of a \"distinction\", and can be thought of as signifying the following, all at once:\n\nAll three ways imply an action on the part of the cognitive entity (e.g., person) making the distinction. As \"LoF\" puts it:\n\n\"The first command:\ncan well be expressed in such ways as:\nOr:\nThe counterpoint to the Marked state is the Unmarked state, which is simply nothing, the void, or the un-expressable infinite represented by a blank space. It is simply the absence of a Cross. No distinction has been made and nothing has been crossed. The Marked state and the void are the two primitive values of the Laws of Form.\n\nThe Cross can be seen as denoting the distinction between two states, one \"considered as a symbol\" and another not so considered. From this fact arises a curious resonance with some theories of consciousness and language. Paradoxically, the Form is at once Observer and Observed, and is also the creative act of making an observation. \"LoF\" (excluding back matter) closes with the words:\n\n\"...the first distinction, the Mark and the observer are not only interchangeable, but, in the form, identical.\"\n\nC. S. Peirce came to a related insight in the 1890s; see Related Work.\n\nThe syntax of the primary arithmetic goes as follows. There are just two atomic expressions:\nThere are two inductive rules:\nThe semantics of the primary arithmetic are perhaps nothing more than the sole explicit definition in \"LoF\": \"Distinction is perfect continence\".\n\nLet the \"unmarked state\" be a synonym for the void. Let an empty Cross denote the \"marked state\". To cross is to move from one value, the unmarked or marked state, to the other. We can now state the \"arithmetical\" axioms A1 and A2, which ground the primary arithmetic (and hence all of the Laws of Form):\n\n\"A1. The law of Calling\". Calling twice from a state is indistinguishable from calling once. To make a distinction twice has the same effect as making it once. For example, saying \"Let there be light\" and then saying \"Let there be light\" again, is the same as saying it once. Formally:\n\n\"A2. The law of Crossing.\" After crossing from the unmarked to the marked state, crossing again (\"recrossing\") starting from the marked state returns one to the unmarked state. Hence recrossing annuls crossing. Formally:\n\nIn both A1 and A2, the expression to the right of '=' has fewer symbols than the expression to the left of '='. This suggests that every primary arithmetic expression can, by repeated application of A1 and A2, be \"simplified\" to one of two states: the marked or the unmarked state. This is indeed the case, and the result is the expression's \"simplification\". The two fundamental metatheorems of the primary arithmetic state that:\nThus the relation of logical equivalence partitions all primary arithmetic expressions into two equivalence classes: those that simplify to the Cross, and those that simplify to the void.\n\nA1 and A2 have loose analogs in the properties of series and parallel electrical circuits, and in other ways of diagramming processes, including flowcharting. A1 corresponds to a parallel connection and A2 to a series connection, with the understanding that making a distinction corresponds to changing how two points in a circuit are connected, and not simply to adding wiring.\n\nThe primary arithmetic is analogous to the following formal languages from mathematics and computer science:\n\nThe phrase \"calculus of indications\" in \"LoF\" is a synonym for \"primary arithmetic\".\n\nA concept peculiar to \"LoF\" is that of \"canon\". While \"LoF\" does not define canon, the following two excerpts from the Notes to chpt. 2 are apt:\n\n\"The more important structures of command are sometimes called \"canons\". They are the ways in which the guiding injunctions appear to group themselves in constellations, and are thus by no means independent of each other. A canon bears the distinction of being outside (i.e., describing) the system under construction, but a command to construct (e.g., 'draw a distinction'), even though it may be of central importance, is not a canon. A canon is an order, or set of orders, to permit or allow, but not to construct or create.\"\n\n\"...the primary form of mathematical communication is not description but injunction... Music is a similar art form, the composer does not even attempt to describe the set of sounds he has in mind, much less the set of feelings occasioned through them, but writes down a set of commands which, if they are obeyed by the performer, can result in a reproduction, to the listener, of the composer's original experience.\"\n\nThese excerpts relate to the distinction in metalogic between the object language, the formal language of the logical system under discussion, and the metalanguage, a language (often a natural language) distinct from the object language, employed to exposit and discuss the object language. The first quote seems to assert that the \"canons\" are part of the metalanguage. The second quote seems to assert that statements in the object language are essentially commands addressed to the reader by the author. Neither assertion holds in standard metalogic.\n\nGiven any valid primary arithmetic expression, insert into one or more locations any number of Latin letters bearing optional numerical subscripts; the result is a primary algebra formula. Letters so employed in mathematics and logic are called variables. A primary algebra variable indicates a location where one can write the primitive value or its complement . Multiple instances of the same variable denote multiple locations of the same primitive value.\n\nThe sign '=' may link two logically equivalent expressions; the result is an equation. By \"logically equivalent\" is meant that the two expressions have the same simplification. Logical equivalence is an equivalence relation over the set of primary algebra formulas, governed by the rules R1 and R2. Let \"C\" and \"D\" be formulae each containing at least one instance of the subformula \"A\":\nR2 is employed very frequently in \"primary algebra\" demonstrations (see below), almost always silently. These rules are routinely invoked in logic and most of mathematics, nearly always unconsciously.\n\nThe \"primary algebra\" consists of equations, i.e., pairs of formulae linked by an infix '='. R1 and R2 enable transforming one equation into another. Hence the \"primary algebra\" is an \"equational\" formal system, like the many algebraic structures, including Boolean algebra, that are varieties. Equational logic was common before \"Principia Mathematica\" (e.g., Peirce, Johnson 1892), and has present-day advocates (Gries and Schneider 1993).\n\nConventional mathematical logic consists of tautological formulae, signalled by a prefixed turnstile. To denote that the \"primary algebra\" formula \"A\" is a tautology, simply write \"\"A\" = \". If one replaces '=' in R1 and R2 with the biconditional, the resulting rules hold in conventional logic. However, conventional logic relies mainly on the rule modus ponens; thus conventional logic is \"ponential\". The equational-ponential dichotomy distills much of what distinguishes mathematical logic from the rest of mathematics.\n\nAn \"initial\" is a \"primary algebra\" equation verifiable by a decision procedure and as such is \"not\" an axiom. \"LoF\" lays down the initials:\nThe absence of anything to the right of the \"=\" above, is deliberate.\n\nJ2 is the familiar distributive law of sentential logic and Boolean algebra.\n\nAnother set of initials, friendlier to calculations, is:\n\nIt is thanks to C2 that the \"primary algebra\" is a lattice. By virtue of J1a, it is a complemented lattice whose upper bound is . By J0, is the corresponding lower bound and identity element. J0 is also an algebraic version of A2 and makes clear the sense in which aliases with the blank page.\n\nT13 in \"LoF\" generalizes C2 as follows. Any \"primary algebra\" (or sentential logic) formula \"B\" can be viewed as an ordered tree with \"branches\". Then:\n\nT13: A subformula \"A\" can be copied at will into any depth of \"B\" greater than that of \"A\", as long as \"A\" and its copy are in the same branch of \"B\". Also, given multiple instances of \"A\" in the same branch of \"B\", all instances but the shallowest are redundant.\n\nWhile a proof of T13 would require induction, the intuition underlying it should be clear.\n\nC2 or its equivalent is named:\nPerhaps the first instance of an axiom or rule with the power of C2 was the \"Rule of (De)Iteration,\" combining T13 and \"AA=A\", of C. S. Peirce's existential graphs.\n\n\"LoF\" asserts that concatenation can be read as commuting and associating by default and hence need not be explicitly assumed or demonstrated. (Peirce made a similar assertion about his existential graphs.) Let a period be a temporary notation to establish grouping. That concatenation commutes and associates may then be demonstrated from the:\nHaving demonstrated associativity, the period can be discarded.\n\nThe initials in Meguire (2011) are \"AC.D\"=\"CD.A\", called B1; B2, J0 above; B3, J1a above; and B4, C2. By design, these initials are very similar to the axioms for an abelian group, G1-G3 below.\n\nThe \"primary algebra\" contains three kinds of proved assertions:\n\nThe distinction between consequence and theorem holds for all formal systems, including mathematics and logic, but is usually not made explicit. A demonstration or decision procedure can be carried out and verified by computer. The proof of a theorem cannot be.\n\nLet \"A\" and \"B\" be \"primary algebra\" formulas. A demonstration of \"A\"=\"B\" may proceed in either of two ways:\nOnce \"A\"=\"B\" has been demonstrated, \"A\"=\"B\" can be invoked to justify steps in subsequent demonstrations. \"primary algebra\" demonstrations and calculations often require no more than J1a, J2, C2, and the consequences ()\"A\"=() (C3 in \"LoF\"), ((\"A\"))=\"A\" (C1), and \"AA\"=\"A\" (C5).\n\nThe consequence (((\"A\")\"B\")\"C\") = (\"AC\")((\"B\")\"C\"), C7 in \"LoF\", enables an algorithm, sketched in \"LoF\"s proof of T14, that transforms an arbitrary \"primary algebra\" formula to an equivalent formula whose depth does not exceed two. The result is a \"normal form\", the \"primary algebra\" analog of the conjunctive normal form. \"LoF\" (T14-15) proves the \"primary algebra\" analog of the well-known Boolean algebra theorem that every formula has a normal form.\n\nLet \"A\" be a subformula of some formula \"B\". When paired with C3, J1a can be viewed as the closure condition for calculations: \"B\" is a tautology if and only if \"A\" and (\"A\") both appear in depth 0 of \"B\". A related condition appears in some versions of natural deduction. A demonstration by calculation is often little more than:\nThe last step of a calculation always invokes J1a.\n\n\"LoF\" includes elegant new proofs of the following standard metatheory:\nThat sentential logic is complete is taught in every first university course in mathematical logic. But university courses in Boolean algebra seldom mention the completeness of 2.\n\nIf the Marked and Unmarked states are read as the Boolean values 1 and 0 (or True and False), the \"primary algebra\" interprets 2 (or sentential logic). \"LoF\" shows how the \"primary algebra\" can interpret the syllogism. Each of these interpretations is discussed in a subsection below. Extending the \"primary algebra\" so that it could interpret standard first-order logic has yet to be done, but Peirce's \"beta\" existential graphs suggest that this extension is feasible.\n\nThe \"primary algebra\" is an elegant minimalist notation for the two-element Boolean algebra 2. Let:\nIf join (meet) interprets \"AC\", then meet (join) interprets ((\"A\")(\"C\")). Hence the \"primary algebra\" and 2 are isomorphic but for one detail: \"primary algebra\" complementation can be nullary, in which case it denotes a primitive value. Modulo this detail, 2 is a model of the primary algebra. The primary arithmetic suggests the following arithmetic axiomatization of 2: 1+1=1+0=0+1=1=~0, and 0+0=0=~1.\n\nThe set formula_3 formula_4 formula_5 is the Boolean domain or \"carrier\". In the language of universal algebra, the \"primary algebra\" is the algebraic structure formula_6 of type formula_7. The expressive adequacy of the Sheffer stroke points to the \"primary algebra\" also being a formula_8 algebra of type formula_9. In both cases, the identities are J1a, J0, C2, and \"ACD=CDA\". Since the \"primary algebra\" and 2 are isomorphic, 2 can be seen as a formula_10 algebra of type formula_7. This description of 2 is simpler than the conventional one, namely an formula_12 algebra of type formula_13.\n\nThe two possible interpretations are dual to each other in the Boolean sense. (In Boolean algebra, exchanging AND ↔ OR and 1 ↔ 0 throughout an equation yields an equally valid equation.) The identities remain invariant regardless of which interpretation is chosen, so the transformations or modes of calculation remain the same; only the interpretation of each form would be different. Example: J1a is (\"A\") \"A\" = (). Interpreting juxtaposition as OR and () as 1, this translates to formula_14 which is true. Interpreting juxtaposition as AND and () as 0, this translates to formula_15 which is true as well (and the dual of formula_14).\n\nLet the blank page denote True or False, and let a Cross be read as Not. Then the primary arithmetic has the following sentential reading:\n\nThe \"primary algebra\" interprets sentential logic as follows. A letter represents any given sentential expression. Thus:\n\nThus any expression in sentential logic has a \"primary algebra\" translation. Equivalently, the \"primary algebra\" interprets sentential logic. Given an assignment of every variable to the Marked or Unmarked states, this \"primary algebra\" translation reduces to a primary arithmetic expression, which can be simplified. Repeating this exercise for all possible assignments of the two primitive values to each variable, reveals whether the original expression is tautological or satisfiable. This is an example of a decision procedure, one more or less in the spirit of conventional truth tables. Given some \"primary algebra\" formula containing \"N\" variables, this decision procedure requires simplifying 2 primary arithmetic formulae. For a less tedious decision procedure more in the spirit of Quine's \"truth value analysis,\" see Meguire (2003).\n\nSchwartz (1981) proved that the \"primary algebra\" is equivalent -- syntactically, semantically, and proof theoretically—with the classical propositional calculus. Likewise, it can be shown that the \"primary algebra\" is syntactically equivalent with expressions built up in the usual way from the classical truth values true and false, the logical connectives NOT, OR, and AND, and parentheses.\n\nInterpreting the Unmarked State as False is wholly arbitrary; that state can equally well be read as True. All that is required is that the interpretation of concatenation change from OR to AND. IF A THEN B now translates as (\"A\"(\"B\")) instead of (\"A\")\"B\". More generally, the \"primary algebra\" is \"self-dual,\" meaning that any \"primary algebra\" formula has two sentential or Boolean readings, each the dual of the other. Another consequence of self-duality is the irrelevance of De Morgan's laws; those laws are built into the syntax of the \"primary algebra\" from the outset.\n\nThe true nature of the distinction between the \"primary algebra\" on the one hand, and 2 and sentential logic on the other, now emerges. In the latter formalisms, complementation/negation operating on \"nothing\" is not well-formed. But an empty Cross is a well-formed \"primary algebra\" expression, denoting the Marked state, a primitive value. Hence a nonempty Cross is an operator, while an empty Cross is an operand because it denotes a primitive value. Thus the \"primary algebra\" reveals that the heretofore distinct mathematical concepts of operator and operand are in fact merely different facets of a single fundamental action, the making of a distinction.\n\nAppendix 2 of \"LoF\" shows how to translate traditional syllogisms and sorites into the \"primary algebra\". A valid syllogism is simply one whose \"primary algebra\" translation simplifies to an empty Cross. Let \"A\"* denote a \"literal\", i.e., either \"A\" or (\"A\"), indifferently. Then every syllogism that does not require that one or more terms be assumed nonempty is one of 24 possible permutations of a generalization of Barbara whose \"primary algebra\" equivalent is (\"A\"*\"B\")((\"B\")\"C\"*)\"A\"*\"C\"*. These 24 possible permutations include the 19 syllogistic forms deemed valid in Aristotelian and medieval logic. This \"primary algebra\" translation of syllogistic logic also suggests that the \"primary algebra\" can interpret monadic and term logic, and that the \"primary algebra\" has affinities to the Boolean term schemata of Quine (1982: Part II).\n\nThe following calculation of Leibniz's nontrivial \"Praeclarum Theorema\" exemplifies the demonstrative power of the \"primary algebra\". Let C1 be ((\"A\"))=\"A\", C2 be \"A\"(\"A\" \"B\")=\"A\"(\"B\"), C3 be ()\"A\"=(), J1a be (\"A\")\"A\"=(), and let OI mean that variables and subformulae have been reordered in a way that commutativity and associativity permit.\n\nThe \"primary algebra\" embodies a point noted by Huntington in 1933: Boolean algebra requires, in addition to one unary operation, one, and not two, binary operations. Hence the seldom-noted fact that Boolean algebras are magmas. (Magmas were called groupoids until the latter term was appropriated by category theory.) To see this, note that the \"primary algebra\" is a commutative:\n\nGroups also require a unary operation, called inverse, the group counterpart of Boolean complementation. Let (\"a\") denote the inverse of \"a\". Let () denote the group identity element. Then groups and the \"primary algebra\" have the same signatures, namely they are both 〈--,(-),()〉 algebras of type 〈2,1,0〉. Hence the \"primary algebra\" is a boundary algebra. The axioms for an abelian group, in boundary notation, are:\nFrom G1 and G2, the commutativity and associativity of concatenation may be derived, as above. Note that G3 and J1a are identical. G2 and J0 would be identical if (())=() replaced A2. This is the defining arithmetical identity of group theory, in boundary notation.\n\nThe \"primary algebra\" differs from an abelian group in two ways:\nBoth A2 and C2 follow from \"B\" 's being an ordered set.\n\nChapter 11 of \"LoF\" introduces \"equations of the second degree\", composed of recursive formulae that can be seen as having \"infinite\" depth. Some recursive formulae simplify to the marked or unmarked state. Others \"oscillate\" indefinitely between the two states depending on whether a given depth is even or odd. Specifically, certain recursive formulae can be interpreted as oscillating between true and false over successive intervals of time, in which case a formula is deemed to have an \"imaginary\" truth value. Thus the flow of time may be introduced into the \"primary algebra\".\n\nTurney (1986) shows how these recursive formulae can be interpreted via Alonzo Church's Restricted Recursive Arithmetic (RRA). Church introduced RRA in 1955 as an axiomatic formalization of finite automata. Turney (1986) presents a general method for translating equations of the second degree into Church's RRA, illustrating his method using the formulae E1, E2, and E4 in chapter 11 of \"LoF\". This translation into RRA sheds light on the names Spencer-Brown gave to E1 and E4, namely \"memory\" and \"counter\". RRA thus formalizes and clarifies \"LoF\" 's notion of an imaginary truth value.\n\nGottfried Leibniz, in memoranda not published before the late 19th and early 20th centuries, invented Boolean logic. His notation was isomorphic to that of \"LoF\": concatenation read as conjunction, and \"non-(\"X\")\" read as the complement of \"X\". Leibniz's pioneering role in algebraic logic was foreshadowed by Lewis (1918) and Rescher (1954). But a full appreciation of Leibniz's accomplishments had to await the work of Wolfgang Lenzen, published in the 1980s and reviewed in Lenzen (2004).\n\nCharles Sanders Peirce (1839–1914) anticipated the \"primary algebra\" in three veins of work:\nIronically, \"LoF\" cites vol. 4 of Peirce's \"Collected Papers,\" the source for the formalisms in (2) and (3) above.\n(1)-(3) were virtually unknown at the time when (1960s) and in the place where (UK) \"LoF\" was written. Peirce's semiotics, about which \"LoF\" is silent, may yet shed light on the philosophical aspects of \"LoF\".\n\nKauffman (2001) discusses another notation similar to that of \"LoF\", that of a 1917 article by Jean Nicod, who was a disciple of Bertrand Russell's.\n\nThe above formalisms are, like the \"primary algebra\", all instances of \"boundary mathematics\", i.e., mathematics whose syntax is limited to letters and brackets (enclosing devices). A minimalist syntax of this nature is a \"boundary notation.\" Boundary notation is free of infix, prefix, or postfix operator symbols. The very well known curly braces ('{', '}') of set theory can be seen as a boundary notation.\n\nThe work of Leibniz, Peirce, and Nicod is innocent of metatheory, as they wrote before Emil Post's landmark 1920 paper (which \"LoF\" cites), proving that sentential logic is complete, and before Hilbert and Łukasiewicz showed how to prove axiom independence using models.\n\nCraig (1979) argued that the world, and how humans perceive and interact with that world, has a rich Boolean structure. Craig was an orthodox logician and an authority on algebraic logic.\n\nSecond-generation cognitive science emerged in the 1970s, after \"LoF\" was written. On cognitive science and its relevance to Boolean algebra, logic, and set theory, see Lakoff (1987) (see index entries under \"Image schema examples: container\") and Lakoff and Núñez (2001). Neither book cites \"LoF\".\n\nThe biologists and cognitive scientists Humberto Maturana and his student Francisco Varela both discuss \"LoF\" in their writings, which identify \"distinction\" as the fundamental cognitive act. The Berkeley psychologist and cognitive scientist Eleanor Rosch has written extensively on the closely related notion of categorization.\n\nOther formal systems with possible affinities to the primary algebra include:\n\nThe primary arithmetic and algebra are a minimalist formalism for sentential logic and Boolean algebra. Other minimalist formalisms having the power of set theory include:\n\n\n\n\n"}
{"id": "17354269", "url": "https://en.wikipedia.org/wiki?curid=17354269", "title": "LePUS3", "text": "LePUS3\n\nLePUS3 is a language for modelling and visualizing object-oriented (Java, C++, C#) programs and design patterns. It is defined as a formal specification language, formulated as an axiomatized subset of First-order predicate logic. A diagram in LePUS3 is also called a Codechart. LePUS, the name of the first version of the language, is an abbreviation for \"Language for Pattern Uniform Specification\".\n\nLePUS3 is tailored for the following purposes:\n\n\nLePUS3 belongs to the following families of languages:\n\nLePUS3 was designed to accommodate for parsimony and for economy of expression. Its vocabulary consists of only 15 visual tokens.\n\nThe Two-Tier Programming Toolkit\ncan be used to\n\nLePUS3 was specifically designed to model, among others, the 'Gang of Four' design patterns, including abstract factory, factory method, adapter, decorator, composite, proxy, iterator, state, strategy, template method, and visitor. (See \"The 'Gang of Four' Companion\") The abbreviation LePUS for \"Language for Pattern Uniform Specification\" is used because the precursor of this language was primarily concerned with design patterns. The implementation of design patterns specified in LePUS3 can be automatically verified by the TTP Toolkit.\n\nLePUS3 is particularly suitable for modelling large programs, design patterns, and object-oriented application frameworks. It is unsuitable for modelling non object-oriented programs, architectural styles, and undecidable and semi-decidable properties.\n\n"}
{"id": "1804899", "url": "https://en.wikipedia.org/wiki?curid=1804899", "title": "Mackey space", "text": "Mackey space\n\nIn mathematics, particularly in functional analysis, a Mackey space is a locally convex topological vector space \"X\" such that the topology of \"X\" coincides with the Mackey topology τ(\"X\",\"X′\"), the finest topology which still preserves the continuous dual.\n\nExamples of Mackey spaces include:\n\n\n"}
{"id": "2383040", "url": "https://en.wikipedia.org/wiki?curid=2383040", "title": "Matrix congruence", "text": "Matrix congruence\n\nIn mathematics, two square matrices A and B over a field are called congruent if there exists an invertible matrix P over the same field such that \n\nwhere \"T\" denotes the matrix transpose. Matrix congruence is an equivalence relation.\n\nMatrix congruence arises when considering the effect of change of basis on the Gram matrix attached to a bilinear form or quadratic form on a finite-dimensional vector space: two matrices are congruent if and only if they represent the same bilinear form with respect to different bases.\n\nNote that Halmos defines congruence in terms of conjugate transpose (with respect to a complex inner product space) rather than transpose, but this definition has not been adopted by most other authors.\n\nSylvester's law of inertia states that two congruent symmetric matrices with real entries have the same numbers of positive, negative, and zero eigenvalues. That is, the number of eigenvalues of each sign is an invariant of the associated quadratic form.\n\n\n"}
{"id": "1793590", "url": "https://en.wikipedia.org/wiki?curid=1793590", "title": "Maximal independent set", "text": "Maximal independent set\n\nIn graph theory, a maximal independent set (MIS) or maximal stable set is an independent set that is not a subset of any other independent set. In other words, there is no vertex outside the independent set that may join it because it is maximal with respect to the independent set property.\n\nFor example, in the graph formula_1, a path with three vertices formula_2, formula_3, and formula_4, and two edges formula_5 and formula_6, the sets formula_7 and formula_8 are both maximally independent. The set formula_9 is independent, but is not maximal independent, because it is a subset of the larger independent set formula_8. In this same graph, the maximal cliques are the sets formula_11 and formula_12.\n\nA MIS is also a dominating set in the graph, and every dominating set that is independent must be maximal independent, so MISs are also called independent dominating sets. \n\nA graph may have many MISs of widely varying sizes; the largest, or possibly several equally large, MISs of a graph is called a maximum independent set. The graphs in which all maximal independent sets have the same size are called well-covered graphs.\n\nThe phrase \"maximal independent set\" is also used to describe maximal subsets of independent elements in mathematical structures other than graphs, and in particular in vector spaces and matroids.\n\nTwo algorithmic problems are associated with MISs: finding a \"single\" MIS in a given graph and listing \"all\" MISs in a given graph.\n\nFor a graph formula_13, an independent set formula_14 is a maximal independent set if for formula_15, one of the following is true:\nThe above can be restated as a vertex either belongs to the independent set or has at least one neighbor vertex that belongs to the independent set. As a result, every edge of the graph has at least one endpoint not in formula_14. However, it is not true that every edge of the graph has at least one, or even one endpoint in formula_14\n\nNotice that any neighbor to a vertex in the independent set formula_14 cannot be in formula_14 because these vertices are disjoint by the independent set definition.\n\nIf \"S\" is a maximal independent set in some graph, it is a maximal clique or maximal complete subgraph in the complementary graph. A maximal clique is a set of vertices that induces a complete subgraph, and that is not a subset of the vertices of any larger complete subgraph. That is, it is a set \"S\" such that every pair of vertices in \"S\" is connected by an edge and every vertex not in \"S\" is missing an edge to at least one vertex in \"S\". A graph may have many maximal cliques, of varying sizes; finding the largest of these is the maximum clique problem.\n\nSome authors include maximality as part of the definition of a clique, and refer to maximal cliques simply as cliques.\n\nThe complement of a maximal independent set, that is, the set of vertices not belonging to the independent set, forms a minimal vertex cover. That is, the complement is a vertex cover, a set of vertices that includes at least one endpoint of each edge, and is minimal in the sense that none of its vertices can be removed while preserving the property that it is a cover. Minimal vertex covers have been studied in statistical mechanics in connection with the hard-sphere lattice gas model, a mathematical abstraction of fluid-solid state transitions.\n\nEvery maximal independent set is a dominating set, a set of vertices such that every vertex in the graph either belongs to the set or is adjacent to the set. A set of vertices is a maximal independent set if and only if it is an independent dominating set.\n\nCertain graph families have also been characterized in terms of their maximal cliques or maximal independent sets. Examples include the maximal-clique irreducible and hereditary maximal-clique irreducible graphs. A graph is said to be \"maximal-clique irreducible\" if every maximal clique has an edge that belongs to no other maximal clique, and \"hereditary maximal-clique irreducible\" if the same property is true for every induced subgraph. Hereditary maximal-clique irreducible graphs include triangle-free graphs, bipartite graphs, and interval graphs.\n\nCographs can be characterized as graphs in which every maximal clique intersects every maximal independent set, and in which the same property is true in all induced subgraphs.\n\n showed that any graph with \"n\" vertices has at most 3 maximal cliques. Complementarily, any graph with \"n\" vertices also has at most 3 maximal independent sets. A graph with exactly 3 maximal independent sets is easy to construct: simply take the disjoint union of \"n\"/3 triangle graphs. Any maximal independent set in this graph is formed by choosing one vertex from each triangle. The complementary graph, with exactly 3 maximal cliques, is a special type of Turán graph; because of their connection with Moon and Moser's bound, these graphs are also sometimes called Moon-Moser graphs. Tighter bounds are possible if one limits the size of the maximal independent sets: the number of maximal independent sets of size \"k\" in any \"n\"-vertex graph is at most\nThe graphs achieving this bound are again Turán graphs.\n\nCertain families of graphs may, however, have much more restrictive bounds on the numbers of maximal independent sets or maximal cliques. If all \"n\"-vertex graphs in a family of graphs have O(\"n\") edges, and if every subgraph of a graph in the family also belongs to the family, then each graph in the family can have at most O(\"n\") maximal cliques, all of which have size O(1). For instance, these conditions are true for the planar graphs: every \"n\"-vertex planar graph has at most 3\"n\" − 6 edges, and a subgraph of a planar graph is always planar, from which it follows that each planar graph has O(\"n\") maximal cliques (of size at most four). Interval graphs and chordal graphs also have at most \"n\" maximal cliques, even though they are not always sparse graphs.\n\nThe number of maximal independent sets in \"n\"-vertex cycle graphs is given by the Perrin numbers, and the number of maximal independent sets in \"n\"-vertex path graphs is given by the Padovan sequence. Therefore, both numbers are proportional to powers of 1.324718, the plastic number.\n\nGiven a Graph G(V,E), it is easy to find a single MIS using the following algorithm:\n\n\nThe runtime is O(\"m\") since in the worst case as we have to check all edges.\n\nO(m) is obviously the best possible runtime for a serial algorithm. But a parallel algorithm can solve the problem much faster.\n\nThe following algorithm finds a MIS in time O(log \"n\").\n\n\nANALYSIS: For each node v, divide its neighbours to \"lower neighbours\" (whose degree is lower than the degree of v) and \"higher neighbours\" (whose degree is higher than the degree of v), breaking ties as in the algorithm.\n\nCall a node v \"bad\" if more than 2/3 of its neighbors are higher neighbours. Call an edge \"bad\" if both its endpoints are bad; otherwise the edge is \"good\".\n\nA worst-case graph, in which the average number of steps is formula_26, is a graph made of \"n\"/2 connected components, each with 2 nodes. The degree of all nodes is 1, so each node is selected with probability 1/2, and with probability 1/4 both nodes in a component are not chosen. Hence, the number of nodes drops by a factor of 4 each step, and the expected number of steps is formula_27.\n\nThe following algorithm is better than the previous one in that at least one new node is always added in each connected component:\n\n\nNote that in every step, the node with the smallest number in each connected component always enters I, so there is always some progress. In particular, in the worst-case of the previous algorithm (\"n\"/2 connected components with 2 nodes each), a MIS will be found in a single step.\n\nANALYSIS: \n"}
{"id": "9667107", "url": "https://en.wikipedia.org/wiki?curid=9667107", "title": "Minimal polynomial (linear algebra)", "text": "Minimal polynomial (linear algebra)\n\nIn linear algebra, the minimal polynomial of an matrix over a field is the monic polynomial over of least degree such that . Any other polynomial with is a (polynomial) multiple of .\n\nThe following three statements are equivalent:\n\nThe multiplicity of a root of is the largest power such that \"strictly\" contains . In other words, increasing the exponent up to will give ever larger kernels, but further increasing the exponent beyond will just give the same kernel.\n\nIf the field is not algebraically closed, then the minimal and characteristic polynomials need not factor according to their roots (in ) alone, in other words they may have irreducible polynomial factors of degree greater than . For irreducible polynomials one has similar equivalences:\n\nLike the characteristic polynomial, the minimal polynomial does not depend on the base field, in other words considering the matrix as one with coefficients in a larger field does not change the minimal polynomial. The reason is somewhat different from for the characteristic polynomial (where it is immediate from the definition of determinants), namely the fact that the minimal polynomial is determined by the relations of linear dependence between the powers of : extending the base field will not introduce any new such relations (nor of course will it remove existing ones).\n\nThe minimal polynomial is often the same as the characteristic polynomial, but not always. For example, if is a multiple of the identity matrix, then its minimal polynomial is since the kernel of is already the entire space; on the other hand its characteristic polynomial is (the only eigenvalue is , and the degree of the characteristic polynomial is always equal to the dimension of the space). The minimal polynomial always divides the characteristic polynomial, which is one way of formulating the Cayley–Hamilton theorem (for the case of matrices over a field).\n\nGiven an endomorphism on a finite-dimensional vector space over a field , let be the set defined as \n\nwhere is the space of all polynomials over the field . is a proper ideal of . Since is a field, is a principal ideal domain, thus any ideal is generated by a single polynomial, which is unique up to units in . A particular choice among the generators can be made, since precisely one of the generators is monic. The minimal polynomial is thus defined to be the monic polynomial which generates . It is the monic polynomial of least degree in .\n\nAn endomorphism of a finite dimensional vector space over a field is diagonalizable if and only if its minimal polynomial factors completely over into \"distinct\" linear factors. The fact that there is only one factor for every eigenvalue means that the generalized eigenspace for is the same as the eigenspace for : every Jordan block has size . More generally, if satisfies a polynomial equation where factors into distinct linear factors over , then it will be diagonalizable: its minimal polynomial is a divisor of and therefore also factors into distinct linear factors. In particular one has:\n\n\nThese cases can also be proved directly, but the minimal polynomial gives a unified perspective and proof.\n\nFor a vector in define:\n\nThis definition satisfies the properties of a proper ideal. Let be the monic polynomial which generates it.\n\nDefine to be the endomorphism of with matrix, on the canonical basis,\n\nTaking the first canonical basis vector and its repeated images by one obtains\n\nof which the first three are easily seen to be linearly independent, and therefore span all of . The last one then necessarily is a linear combination of the first three, in fact\n\nso that: \n\nThis is in fact also the minimal polynomial and the characteristic polynomial : indeed divides which divides , and since the first and last are of degree and all are monic, they must all be the same. Another reason is that in general if any polynomial in annihilates a vector , then it also annihilates (just apply to the equation that says that it annihilates ), and therefore by iteration it annihilates the entire space generated by the iterated images by of ; in the current case we have seen that for that space is all of , so . Indeed one verifies for the full matrix that is the null matrix:\n"}
{"id": "4671403", "url": "https://en.wikipedia.org/wiki?curid=4671403", "title": "Model-driven engineering", "text": "Model-driven engineering\n\nModel-driven engineering (MDE) is a software development methodology that focuses on creating and exploiting domain models, which are conceptual models of all the topics related to a specific problem. Hence, it highlights and aims at abstract representations of the knowledge and activities that govern a particular application domain, rather than the computing (i.e. algorithmic) concepts.\n\nThe MDE approach is meant to increase productivity by maximizing compatibility between systems (via reuse of standardized models), simplifying the process of design (via models of recurring design patterns in the application domain), and promoting communication between individuals and teams working on the system (via a standardization of the terminology and the best practices used in the application domain).\n\nA modeling paradigm for MDE is considered effective if its models make sense from the point of view of a user that is familiar with the domain, and if they can serve as a basis for implementing systems. The models are developed through extensive communication among product managers, designers, developers and users of the application domain. As the models approach completion, they enable the development of software and systems.\n\nSome of the better known MDE initiatives are:\n\nThe first tools to support MDE were the Computer-Aided Software Engineering (CASE) tools developed in the 1980s. Companies like Integrated Development Environments (IDE - StP), Higher Order Software (now Hamilton Technologies, Inc., HTI), Cadre Technologies, Bachman Information Systems, and Logic Works (BP-Win and ER-Win) were pioneers in the field.\n\nThe US government got involved in the modeling definitions creating the IDEF specifications. With several variations of the modeling definitions (see Booch, Rumbaugh, Jacobson, Gane and Sarson, Harel, Shlaer and Mellor, and others) they were eventually joined creating the Unified Modeling Language (UML). Rational Rose, a product for UML implementation, was done by Rational Corporation (Booch) responding automation yield higher levels of abstraction in software development. This abstraction promotes simpler models with a greater focus on problem space. Combined with executable semantics this elevates the total level of automation possible. The Object Management Group (OMG) has developed a set of standards called model-driven architecture (MDA), building a foundation for this advanced architecture-focused approach.\n\nAccording to Douglas C. Schmidt, model-driven engineering technologies offer a promising approach to address the inability of third-generation languages to alleviate the complexity of platforms and express domain concepts effectively.\n\nNotable software tools for model-driven engineering include:\n\n\n\n"}
{"id": "11291250", "url": "https://en.wikipedia.org/wiki?curid=11291250", "title": "Monotonically normal space", "text": "Monotonically normal space\n\nIn mathematics, a monotonically normal space is a particular kind of normal space, with some special characteristics, and is such that it is hereditarily normal, and any two separated subsets are strongly separated. They are defined in terms of a monotone normality operator.\n\nA formula_1 topological space formula_2 is said to be \"monotonically normal\" if the following condition holds:\n\nFor every formula_3, where G is open, there is an open set formula_4 such that\n\nThere are some equivalent criteria of monotone normality.\n\nA space X is called monotonically normal if it is formula_1 and for each pair of disjoint closed subsets formula_10 there is an open set formula_11 with the properties\n\n\nThis operator formula_16 is called monotone normality operator. \n\nNote that if G is a monotone normality operator, then formula_17 defined by formula_18 is also a monotone normality operator; and formula_17 satisfies \n\nFor this reason we some time take the monotone normality operator so as to satisfy the above requirement; and that facilitates the proof of some theorems and of the equivalence of the definitions as well.\n\nA space X is called monotonically normal if it is formula_1,and to each pair (A, B) of subsets of X, with formula_22, one can assign an open subset G(A, B) of X such that\n\nA space X is called monotonically normal if it is formula_1 and there is a function H that assigns to each ordered pair (p,C) where C is closed and p is without C, an open set H(p,C) satisfying:\n\nAn important example of these spaces would be, assuming Axiom of Choice, the linearly ordered spaces; however, it really needs axiom of choice for an arbitrary linear order to be normal (see van Douwen's paper). Any generalised metric is monotonically normal even without choice. An important property of monotonically normal spaces is that any two separated subsets are strongly separated there. Monotone normality is hereditary property and a monotonically normal space is always normal by the first condition of the second equivalent definition.\n\nWe list up some of the properties :\n\n"}
{"id": "19074153", "url": "https://en.wikipedia.org/wiki?curid=19074153", "title": "PGF/TikZ", "text": "PGF/TikZ\n\nPGF/Ti\"k\"Z is a pair of languages for producing vector graphics from a geometric/algebraic description. PGF is a lower-level language, while Ti\"k\"Z is a set of higher-level macros that use PGF. The top-level PGF and Ti\"k\"Z commands are invoked as TeX macros, but in contrast with PSTricks, the PGF/Ti\"k\"Z graphics themselves are described in a language that resembles MetaPost. Till Tantau is the designer of these languages, and he is also the main developer of the only known interpreter for PGF and Ti\"k\"Z, which is written in TeX. PGF is an acronym for \"Portable Graphics Format\". Ti\"k\"Z was introduced in version 0.95 of PGF, and it is a recursive acronym for \"Ti\"k\"Z ist \"kein\" Zeichenprogramm\" (German for \"Ti\"k\"Z is \"not\" a drawing program\").\n\nThe PGF/Ti\"k\"Z interpreter can be used from the popular LaTeX and ConTeXt macro packages, and also directly from the original TeX. Since TeX itself is not concerned with graphics, the interpreter supports multiple TeX output backends: dvips, dvipdfm/dvipdfmx/xdvipdfmx, TeX4ht, and pdftex's internal PDF output driver. Unlike PSTricks, PGF can thus directly produce either PostScript or PDF output, but it cannot use some of the more advanced PostScript programming features that PSTricks can use due to the \"least common denominator\" effect. PGF/Ti\"k\"Z comes with extensive documentation. The version 3.0.0 manual has 1165 pages.\n\nThe standard LaTeX codice_1 environment can also be used as a front end for PGF merely by using the codice_2 package.\n\nSeveral graphical editors can produce output for PGF/Ti\"k\"Z like the KDE program Cirkuit, and the math drawing program GeoGebra. Export to Ti\"k\"Z is also available as extensions for Inkscape, Blender, MATLAB, matplotlib, Gnuplot and R.\n\nThe project has been under constant development since 2005. Most of the development is done by Till Tantau. Version 3.0.0 was released on 2013-12-20. One of the major new features is graph drawing using the codice_3 package, which however requires LuaTeX. This version also added a new data visualization method and support for direct SVG output via the new dvisvgm driver.\n\n\n"}
{"id": "1521971", "url": "https://en.wikipedia.org/wiki?curid=1521971", "title": "Ptolemy's theorem", "text": "Ptolemy's theorem\n\nIn Euclidean geometry, Ptolemy's theorem is a relation between the four sides and two diagonals of a cyclic quadrilateral (a quadrilateral whose vertices lie on a common circle). The theorem is named after the Greek astronomer and mathematician Ptolemy (Claudius Ptolemaeus). Ptolemy used the theorem as an aid to creating his table of chords, a trigonometric table that he applied to astronomy.\n\nIf the vertices of the cyclic quadrilateral are \"A\", \"B\", \"C\", and \"D\" in order, then the theorem states that:\n\nwhere the vertical lines denote the lengths of the line segments between the named vertices. In the context of geometry, the above equality is often simply written as\n\nThis relation may be verbally expressed as follows:\n\nMoreover, the converse of Ptolemy's theorem is also true:\n\nPtolemy's Theorem yields as a corollary a pretty theorem regarding an equilateral triangle inscribed in a circle.\n\nGiven An equilateral triangle inscribed on a circle and a point on the circle.\n\nThe distance from the point to the most distant vertex of the triangle is the sum of the distances from the point to the two nearer vertices.\n\nProof: Follows immediately from Ptolemy's theorem:\n\nAny square can be inscribed in a circle whose center is the center of the square. If the common length of its four sides is equal to formula_3 then the length of the diagonal is equal to formula_4 according to the Pythagorean theorem and the relation obviously holds.\n\nMore generally, if the quadrilateral is a rectangle with sides a and b and diagonal d then Ptolemy's theorem reduces to the Pythagorean theorem. In this case the center of the circle coincides with the point of intersection of the diagonals. The product of the diagonals is then d, the right hand side of Ptolemy's relation is the sum \"a\" + \"b\".\n\nCopernicus – who used Ptolemy's theorem extensively in his trigonometrical work – refers to this result as a 'Porism' or self-evident corollary:\n\nA more interesting example is the relation between the length \"a\" of the side and the (common) length \"b\" of the 5 chords in a regular pentagon. In this case the relation reads \"b\" = \"a\" + \"ab\" which yields the golden ratio\n\nIf now diameter AF is drawn bisecting DC so that DF and CF are sides c of an inscribed decagon, Ptolemy's Theorem can again be applied – this time to cyclic quadrilateral ADFC with diameter \"d\" as one of its diagonals:\n\nwhence the side of the inscribed decagon is obtained in terms of the circle diameter. Pythagoras's theorem applied to right triangle AFD then yields \"b\" in terms of the diameter and \"a\" the side of the pentagon is thereafter calculated as\n\nAs Copernicus (following Ptolemy) wrote,\n\nLet ABCD be a cyclic quadrilateral.\nOn the chord BC, the inscribed angles ∠BAC = ∠BDC, and on AB, ∠ADB = ∠ACB.\nConstruct K on AC such that ∠ABK = ∠CBD; since ∠ABK + ∠CBK = ∠ABC = ∠CBD + ∠ABD, ∠CBK = ∠ABD.\n\nNow, by common angles △ABK is similar to △DBC, and likewise △ABD is similar △KBC.\nThus AK/AB = CD/BD, and CK/BC = DA/BD;\nequivalently, AK·BD = AB·CD, and CK·BD = BC·DA.\nBy adding two equalities we have AK·BD + CK·BD = AB·CD + BC·DA, and factorizing this gives (AK+CK)·BD = AB·CD + BC·DA.\nBut AK+CK = AC, so AC·BD = AB·CD + BC·DA, Q.E.D.\n\nThe proof as written is only valid for simple cyclic quadrilaterals. If the quadrilateral is self-crossing then K will be located outside the line segment AC. But in this case, AK−CK=±AC, giving the expected result.\n\nLet the inscribed angles subtended by formula_11, formula_12 and formula_13 be, respectively, formula_14, formula_15 and formula_16, and the radius of the circle be formula_17, then we have formula_18, formula_19, formula_20, formula_21, formula_22 and formula_23, and the original equality to be proved is transformed to\n\nfrom which the factor formula_25 has disappeared by dividing both sides of the equation by it.\n\nNow by using the sum formulae, formula_26 and formula_27, it is trivial to show that both sides of the above equation are equal to\n\nQ.E.D.\n\nChoose an auxiliary circle formula_29 centered at D with respect to which the circumcircle of ABCD is inverted into a line (see figure).\nThen\nformula_30\nWithout loss of generality formula_29 has radius formula_32. Then formula_33 and formula_34 can be expressed as \nformula_35 respectively. Multiplying previous relation by formula_36 yields Ptolemy's equality.\n\nQ.E.D.\n\nIn the case of a circle of unit diameter the sides formula_37 of any cyclic quadrilateral ABCD are numerically equal to the sines of the angles formula_38 and formula_39 which they subtend. Similarly the diagonals are equal to the sine of the sum of whichever pair of angles they subtend. We may then write Ptolemy's Theorem in the following trigonometric form:\n\nApplying certain conditions to the subtended angles formula_38 and formula_39 it is possible to derive a number of important corollaries using the above as our starting point. In what follows it is important to bear in mind that the sum of angles formula_43.\n\nLet formula_44 and formula_45. Then formula_46\n(since opposite angles of a cyclic quadrilateral are supplementary). Then:\n\nLet formula_45. The rectangle of corollary 1 is now a symmetrical trapezium with equal diagonals and a pair of equal sides. The parallel sides differ in length by formula_51 units where:\n\nIt will be easier in this case to revert to the standard statement of Ptolemy's theorem:\n\nThe cosine rule for triangle ABC.\n\nLet\n\nformula_54\n\nThen\n\nTherefore,\n\nFormula for compound angle sine (+).\n\nLet formula_57. Then formula_58. Hence,\n\nFormula for compound angle sine (−).\n\nThis derivation corresponds to the Third Theorem\nas chronicled by Copernicus following Ptolemy in Almagest. In particular if the sides of a pentagon (subtending 36° at the circumference) and of a hexagon (subtending 30° at the circumference) are given, a chord subtending 6° may be calculated. This was a critical step in the ancient method of calculating tables of chords.\n\nThis corollary is the core of the Fifth Theorem as chronicled by Copernicus following Ptolemy in Almagest.\n\nLet formula_62. Then formula_63. Hence\n\nFormula for compound angle cosine (+)\n\nDespite lacking the dexterity of our modern trigonometric notation, it should be clear from the above corollaries that in Ptolemy's theorem (or more simply the Second Theorem) the ancient world had at its disposal an extremely flexible and powerful trigonometric tool which enabled the cognoscenti of those times to draw up accurate tables of chords (corresponding to tables of sines) and to use these in their attempts to understand and map the cosmos as they saw it. Since tables of chords were drawn up by Hipparchus three centuries before Ptolemy, we must assume he knew of the 'Second Theorem' and its derivatives. Following the trail of ancient astronomers, history records the star catalogue of Timocharis of Alexandria. If, as seems likely, the compilation of such catalogues required an understanding of the 'Second Theorem' then the true origins of the latter disappear thereafter into the mists of antiquity but it cannot be unreasonable to presume that the astronomers, architects and construction engineers of ancient Egypt may have had some knowledge of it.\n\nThe equation in Ptolemy's theorem is never true with non-cyclic quadrilaterals. Ptolemy's inequality is an extension of this fact, and it is a more general form of Ptolemy's theorem. It states that, given a quadrilateral \"ABCD\", then\n\nwhere equality holds if and only if the quadrilateral is cyclic. This special case is equivalent to Ptolemy's theorem.\n\n\n"}
{"id": "931978", "url": "https://en.wikipedia.org/wiki?curid=931978", "title": "Quasi-continuous function", "text": "Quasi-continuous function\n\nIn mathematics, the notion of a quasi-continuous function is similar to, but weaker than, the notion of a continuous function. All continuous functions are quasi-continuous but the converse is not true in general.\n\nLet formula_1 be a topological space. A real-valued function formula_2 is quasi-continuous at a point formula_3 if for any every formula_4 and any open neighborhood formula_5 of formula_6 there is a non-empty open set formula_7 such that\n\nNote that in the above definition, it is not necessary that formula_9.\n\n\nConsider the function formula_15 defined by formula_16 whenever formula_17 and formula_18 whenever formula_19. Clearly f is continuous everywhere except at x=0, thus quasi-continuous everywhere except at x=0. At x=0, take any open neighborhood U of x. Then there exists an open set formula_7 such that formula_21. Clearly this yields formula_22 thus f is quasi-continuous.\n"}
{"id": "5665228", "url": "https://en.wikipedia.org/wiki?curid=5665228", "title": "Quasi-open map", "text": "Quasi-open map\n\nIn topology a branch of mathematics, a quasi-open map or quasi-interior map is a function which has similar properties to continuous maps. However, continuous maps and quasi-open maps are not related.\n\nA function formula_1 between topological spaces formula_2 and formula_3 is quasi-open if, for any non-empty open set formula_4, the interior of formula_5 in formula_3 is non-empty.\n\nLet formula_7 be a function such that \"X\" and \"Y\" are topological spaces.\n"}
{"id": "15523181", "url": "https://en.wikipedia.org/wiki?curid=15523181", "title": "Riemannian Penrose inequality", "text": "Riemannian Penrose inequality\n\nIn mathematical general relativity, the Penrose inequality, first conjectured by Sir Roger Penrose, estimates the mass of a spacetime in terms of the total area of its black holes and is a generalization of the positive mass theorem. The Riemannian Penrose inequality is an important special case. Specifically, if (\"M\", \"g\") is an asymptotically flat Riemannian 3-manifold with nonnegative scalar curvature and ADM mass \"m\", and \"A\" is the area of the outermost minimal surface (possibly with multiple connected components), then the Riemannian Penrose inequality asserts\n\nThis is purely a geometrical fact, and it corresponds to the case of a complete three-dimensional, space-like, totally geodesic submanifold\nof a (3 + 1)-dimensional spacetime. Such a submanifold is often called a time-symmetric initial data set for a spacetime. The condition of (\"M\", \"g\") having nonnegative scalar curvature is equivalent to the spacetime obeying the dominant energy condition.\n\nThis inequality was first proved by Gerhard Huisken and Tom Ilmanen in 1997 in the case where \"A\" is the area of the largest component of the outermost minimal surface. Their proof relied on the machinery of weakly defined inverse mean curvature flow, which they developed. In 1999, Hubert Bray gave the first complete proof of the above inequality using a conformal flow of metrics. Both of the papers were published in 2001.\n\nThe original physical argument that led Penrose to conjecture such an inequality invoked the Hawking area theorem and the cosmic censorship hypothesis.\n\nBoth the Bray and Huisken–Ilmanen proofs of the Riemannian Penrose inequality state that under the hypotheses, if\n\nthen the manifold in question is isometric to a slice of the Schwarzschild spacetime outside of the outermost minimal surface.\n\nMore generally, Penrose conjectured that an inequality as above should hold for spacelike submanifolds of spacetimes that are not necessarily time-symmetric. In this case, nonnegative scalar curvature is replaced with the dominant energy condition, and one possibility is to replace the minimal surface condition with an apparent horizon condition. Proving such an inequality remains an open problem in general relativity, called the Penrose conjecture.\n\n\n"}
{"id": "25148935", "url": "https://en.wikipedia.org/wiki?curid=25148935", "title": "Schoenflies problem", "text": "Schoenflies problem\n\nIn mathematics, the Schoenflies problem or Schoenflies theorem, of geometric topology is a sharpening of the Jordan curve theorem by Arthur Schoenflies. For Jordan curves in the plane it is often referred to as the Jordan–Schoenflies theorem.\n\nThe original formulation of the Schoenflies problem states that not only does every simple closed curve in the plane separate the plane into two regions, one (the \"inside\") bounded and the other (the \"outside\") unbounded; but also that these two regions are homeomorphic to the inside and outside of a standard circle in the plane.\n\nAn alternative statement is that if formula_1 is a simple closed curve, then there is a homeomorphism formula_2 such that formula_3 is the unit circle in the plane. Elementary proofs can be found in , , and . The result can first be proved for polygons when the homeomorphism can be taken to be piecewise linear and the identity map off some compact set; the case of a continuous curve is then deduced by approximating by polygons. The theorem is also an immediate consequence of Carathéodory's extension theorem for conformal mappings, as discussed in .\n\nIf the curve is smooth then the homeomorphism can be chosen to be a diffeomorphism. Proofs in this case rely on techniques from differential topology. Although direct proofs are possible (starting for example from the polygonal case), existence of the diffeomorphism can also be deduced by using the smooth Riemann mapping theorem for the interior and exterior of the curve in combination with the Alexander trick for diffeomorphisms of the circle and a result on smooth isotopy from differential topology.\n\nSuch a theorem is valid only in two dimensions. In three dimensions there are counterexamples such as Alexander's horned sphere. Although they separate space into two regions, those regions are so twisted and knotted that they are not homeomorphic to the inside and outside of a normal sphere.\n\nFor smooth or polygonal curves, the Jordan curve theorem can be proved in a straightforward way. Indeed the curve has a tubular neighbourhood, defined in the smooth case by the field of unit normal vectors to the curve or in the polygonal case by points at a distance of less than ε from the curve.\nIn a neighbourhood of a differentiable point on the curve, there is a coordinate change in which the curve becomes the diameter of an open disk. Taking a point not on the curve, a straight line aimed at the curve starting at the point will eventually meet the tubular neighborhood; the path can be continued next to the curve until it meets the disk. It will meet it on one side or the other. This proves that the complement of the curve has at most two connected components. On the other hand using the Cauchy integral formula for the winding number, it can be seen that the winding number is constant on connected components of the complement of the curve, is zero near infinity and increases by 1 when crossing the curve. Hence the curve has exactly two components, its interior and the unbounded component. The same argument works for a piecewise differentiable Jordan curve.\n\nGiven a simple closed polygonal curve in the plane, the piecewise linear Jordan–Schoenflies theorem states that there is a piecewise linear homeomorphism of the plane, with compact support, carrying the polygon onto a triangle and taking the interior and exterior of one onto the interior and exterior of the other.\n\nThe interior of the polygon can be triangulated by small triangles, so that the edges of the polygon form edges of some of the small triangles. Piecewise linear homeomorphisms can be made up from special homeomorphisms obtained by removing a diamond from the plane and taking a piecewise affine map, fixing the edges of the diamond, but moving one diagonal into a V shape. Compositions of homeomorphisms of this kind give rise to piecewise linear homeomorphisms of compact support; they fix the outside of a polygon and act in an affine way on a triangulation of the interior. A simple inductive argument shows that it is always possible to remove a \"free\" triangle—one for which the intersection with the boundary is a connected set made up of one or two edges—leaving a simple closed Jordan polygon. The special homeomorphisms described above or their inverses provide piecewise linear homeomorphisms which carry the interior of the larger polygon onto the polygon with the free triangle removed. Iterating this process it follows that there is a piecewise linear homeomorphism of compact support carrying the original polygon onto a triangle.\n\nBecause the homormorphism is obtained by composing finite many homeomorphisms of the plane of compact support, it follows that the piecewise linear homeomorphism in the statement of the piecewise linear Jordan-Schoenflies theorem has compact support.\n\nAs a corollary, it follows that any homeomorphism between simple closed polygonal curves extends to a homeomorphism between their interiors. For each polygon there is a homeomorphism of a given triangle onto the closure of their interior. The three homeomorphisms yield a single homeomorphism of the boundary of the triangle. By the Alexander trick this homeomorphism can be extended to a homeomorphism of closure of interior of the triangle. Reversing this process this homeomorphism yields a homeomorphism between the closures of the interiors of the polygonal curves.\n\nThe Jordan-Schoenflies theorem for continuous curves can be proved using Carathéodory's theorem on conformal mapping. It states that the Riemann mapping between the interior of a simple Jordan curve and the open unit disk extends continuously to a homeomorphism between their closures, mapping the Jordan curve homeomorphically onto the unit circle. To prove the theorem, Carathéodory's theorem can be applied to the two regions on the Riemann sphere defined by the Jordan curve. This will result in homeomorphisms between their closures and the closed disks |\"z\"| ≤ 1 and |\"z\"| ≥ 1. The homeomorphisms from the Jordan curve to\nthe circle will differ by a homeomorphism of the circle which can be extended to the unit disk (or its complement) by the Alexander trick. Composition with this homeomorphism will yield a pair of homeomorphisms which match on the Jordan curve and therefore define a homeomorphism of the Riemann sphere carrying the Jordan curve onto the unit circle.\n\nThe continuous case can also be deduced from the polygonal case by approximating the continuous curve by a polygon. The Jordan curve theorem is first deduced by this method. The Jordan curve is given by a continuous function on the unit circle. It and the inverse function from its image back to the unit circle are uniformly continuous. So dividing the circle up into small enough intervals, there are points on the curve such that the line segments joining adjacent points lie close to the curve, say by ε. Together these line segments form a polygonal curve. If it has self-intersections, these must also create polygonal loops. Erasing these loops, results in a polygonal curve without self-intersections which still lies close to the curve; some its vertices might not lie on the curve, but they all lie within a neighbourhood of the curve. The polygonal curve divides the plane into two regions, one bounded region \"U\" and one unbounded region \"V\". Both \"U\" and \"V\" ∪ ∞ are continuous images of the closed unit disk. Since the original curve is contained within a small neighbourhood of the polygonal curve, the union of the images of slightly smaller concentric open disks entirely misses the original curve only excludes a small neighbourhood of it. One is a bounded open set consisting of points around which the curve has winding number one; the other is an unbounded open set consisting of points of winding number zero. Repeating for a sequence of values of ε tending to 0, leads to a union of open path-connected bounded sets of points of winding number one and a union of open path-connected unbounded sets of winding number zero. By construction these two disjoint open path-connected sets fill out the complement of the curve in the plane.\nGiven the Jordan curve theorem, the Jordan-Schoenflies theorem can be proved as follows.\n\n\nProofs in the smooth case depend on finding a diffeomorphism between the interior/exterior of the curve and the closed unit disk (or its complement in the extended plane). This can be solved for example by using the smooth Riemann mapping theorem, for which a number of direct methods are available, for example through the Dirichlet problem on the curve or Bergman kernels. (Such diffeomorphisms will be holomorphic on the interior and exterior of the curve; more general diffeomorphisms can be constructed more easily using vector fields and flows.) Regarding the smooth curve as lying inside the extended plane or 2-sphere, these analytic methods produce smooth maps up to the boundary between the closure of the interior/exterior of the smooth curve and those of the unit circle. The two identifications of the smooth curve and the unit circle will \ndiffer by a diffeomorphism of the unit circle. On the other hand a diffeomorphism of the unit circle can be extended to a diffeomorphism of the unit disk by the Alexander extension:\n\nwhere is a smooth function with values in [0,1], equal to 0 near 0 and 1 near 1, and , with . Composing one of the diffeomorphisms with the Alexander extension allows the two diffeomorphisms to be patched together to give a homeomorphism of the 2-sphere which restricts to a diffeomorphism on the closed unit disk and the closures of its complement which it carries onto the interior and exterior of the original smooth curve. By the \"isotopy theorem\" in differential topology, the homeomorphism can be adjusted to a diffeomorphism on the whole 2-sphere without changing it on the unit circle. This diffeomorphism then provides the smooth solution to the Schoenflies problem.\n\nThe Jordan-Schoenflies theorem can be deduced using differential topology. In fact it is an immediate consequence of the classification up to diffeomorphism of smooth oriented 2-manifolds with boundary, as described in . Indeed the smooth curve divides the 2-sphere into two parts. By the classification each is diffeomorphic to the unit disk and—taking into account the isotopy theorem—they are glued together by a diffeomorphism of the boundary. By the Alexander trick, such a diffeomorphism extends to the disk itself. Thus there is a diffeomorphism of the 2-sphere carrying the smooth curve onto the unit circle.\n\nOn the other hand the diffeomorphism can also be constructed directly using the Jordan-Schoenflies theorem for polygons and elementary methods from differential topology, namely flows defined by vector fields. When the Jordan curve is smooth (parametrized by arc length) the unit normal vectors give a non-vanishing vector field \"X\" in a tubular neighbourhood \"U\" of the curve. Take a polygonal curve in the interior of the curve close to the boundary and transverse to the curve (at the vertices the vector field should be strictly within the angle formed by the edges). By the piecewise linear Jordan–Schoenflies theorem, there is a piecewise linear homeomorphism, affine on an appropriate triangulation of the interior of the polygon, taking the polygon onto a triangle. Take an interior point \"P\" in one of the small triangles of the triangulation. It corresponds to a point \"Q\" in the image triangle. There is a radial vector field on the image triangle, formed of straight lines pointing towards \"Q\". This gives a series of lines in the small triangles making up the polygon. Each defines a vector field \"X\" on a neighbourhood \"U\" of the closure of the triangle. Each vector field is transverse to the sides, provided that \"Q\" is chosen in \"general position\" so that it is not collinear with any of the finitely many edges in the triangulation. Translating if necessary, it can be assumed that \"P\" and \"Q\" are at the origin 0. On the triangle containing \"P\" the vector field can be taken to be the standard radial vector field. Similarly the same procedure can be applied to the outside of the smooth curve, after applying Möbius transformation to map it into the finite part of the plane and ∞ to 0. In this case the neighbourhoods \"U\" of the triangles have negative indices. Take the vector fields \"X\" with a negative sign, pointing away from the point at infinity. Together \"U\" and the \"U\"'s with \"i\" ≠ 0 form an open cover of the 2-sphere. Take a smooth partition of unity ψ subordinate to the cover \"U\" and set\n\n\"X\" is a smooth vector field on the two sphere vanishing only at 0 and ∞. It has index 1 at 0 and -1 at ∞. Near 0 the vector field equals the radial vector field pointing towards 0. If α is the smooth flow defined by \"X\", the point 0 is an attracting point and ∞ a repelling point. As \"t\" tends to +∞, the flow send points to 0; while as \"t\" tends to –∞ points are sent to ∞. Replacing \"X\" by \"f\"⋅\"X\" with \"f\" a smooth positive function, changes the parametrization of the integral curves of \"X\", but not the integral curves themselves. For an appropriate choice of \"f\" equal to 1 outside a small annulus near 0, the integral curves starting at points of the smooth curve will all reach smaller circle bounding the annulus at the same time \"s\". The diffeomorphism α therefore carries the smooth curve onto this small circle. A scaling transformation, fixing 0 and ∞, then carries the small circle onto the unit circle. Composing these diffeomorphisms gives a diffeomorphism carrying the smooth curve onto the unit circle.\n\nThere does exist a higher-dimensional generalization due to and independently with , which is also called the generalized Schoenflies theorem. It states that, if an (\"n\" − 1)-dimensional sphere \"S\" is embedded into the \"n\"-dimensional sphere \"S\" in a locally flat way (that is, the embedding extends to that of a thickened sphere), then the pair (\"S\", \"S\") is homeomorphic to the pair (\"S\", \"S\"), where \"S\" is the equator of the \"n\"-sphere. Brown and Mazur received the Veblen Prize for their contributions.\n\nThe Schoenflies problem can be posed in categories other than the topologically locally flat category, i.e. does a smoothly (piecewise-linearly) embedded (\"n\" − 1)-sphere in the \"n\"-sphere bound a smooth (piecewise-linear) \"n\"-ball? For \"n\" = 4, the problem is still open for both categories. See Mazur manifold. For \"n\" ≥ 5 the question has an affirmative answer, and follows from the h-cobordism theorem.\n\n"}
{"id": "3213223", "url": "https://en.wikipedia.org/wiki?curid=3213223", "title": "Schwarz reflection principle", "text": "Schwarz reflection principle\n\nIn mathematics, applying the Schwarz reflection principle is a way to extend the domain of definition of an analytic function of a complex variable, \"F\", which is defined on the upper half-plane and has well-defined and real number boundary values on the real axis. In that case, the putative extension of \"F\" to the rest of the complex plane is\n\nor\n\nThat is, we make the definition that agrees along the real axis.\n\nThe result proved by Hermann Schwarz is as follows. Suppose that \"F\" is a continuous function on the closed upper half plane formula_3, holomorphic on the upper half plane formula_4, which takes real values on the real axis. Then the extension formula given above is an analytic continuation to the whole complex plane.\n\nIn practice it would be better to have a theorem that allows \"F\" certain singularities, for example \"F\" a meromorphic function. To understand such extensions, one needs a proof method that can be weakened. In fact Morera's theorem is well adapted to proving such statements. Contour integrals involving the extension of \"F\" clearly split into two, using part of the real axis. So, given that the principle is rather easy to prove in the special case from Morera's theorem, understanding the proof is enough to generate other results.\n\nThe principle also adapts to apply to harmonic functions.\n\n"}
{"id": "1348798", "url": "https://en.wikipedia.org/wiki?curid=1348798", "title": "Skolem's paradox", "text": "Skolem's paradox\n\nIn mathematical logic and philosophy, Skolem's paradox is a seeming contradiction that arises from the downward Löwenheim–Skolem theorem. Thoralf Skolem (1922) was the first to discuss the seemingly contradictory aspects of the theorem, and to discover the relativity of set-theoretic notions now known as non-absoluteness. Although it is not an actual antinomy like Russell's paradox, the result is typically called a paradox, and was described as a \"paradoxical state of affairs\" by Skolem (1922: p. 295). \n\nSkolem's paradox is that every countable axiomatisation of set theory in first-order logic, if it is consistent, has a model that is countable. This appears contradictory because it is possible to prove, from those same axioms, a sentence that intuitively says (or that precisely says in the standard model of the theory) that there exist sets that are not countable. Thus the seeming contradiction is that a model that is itself countable, and which therefore contains only countable sets, satisfies the first order sentence that intuitively states \"there are uncountable sets\". \n\nA mathematical explanation of the paradox, showing that it is not a contradiction in mathematics, was given by Skolem (1922). Skolem's work was harshly received by Ernst Zermelo, who argued against the limitations of first-order logic, but the result quickly came to be accepted by the mathematical community. \n\nThe philosophical implications of Skolem's paradox have received much study. One line of inquiry questions whether it is accurate to claim that any first-order sentence actually states \"there are uncountable sets\". This line of thought can be extended to question whether any set is uncountable in an absolute sense. More recently, the paper \"Models and Reality\" by Hilary Putnam, and responses to it, led to renewed interest in the philosophical aspects of Skolem's result.\n\nOne of the earliest results in set theory, published by Georg Cantor in 1874, was the existence of uncountable sets, such as the powerset of the natural numbers, the set of real numbers, and the Cantor set. An infinite set \"X\" is countable if there is a function that gives a one-to-one correspondence between \"X\" and the natural numbers, and is uncountable if there is no such correspondence function. When Zermelo proposed his axioms for set theory in 1908, he proved Cantor's theorem from them to demonstrate their strength. \n\nLöwenheim (1915) and Skolem (1920, 1923) proved the Löwenheim–Skolem theorem. The downward form of this theorem shows that if a countable first-order axiomatisation is satisfied by any infinite structure, then the same axioms are satisfied by some countable structure. In particular, this implies that if the first order versions of Zermelo's axioms of set theory are satisfiable, they are satisfiable in some countable model. The same is true of any consistent first order axiomatisation of set theory.\n\nSkolem (1922) pointed out the seeming contradiction between the Löwenheim–Skolem theorem on the one hand, which implies that there is a countable model of Zermelo's axioms, and Cantor's theorem on the other hand, which states that uncountable sets exist, and which is provable from Zermelo's axioms. \"So far as I know,\" Skolem writes, \"no one has called attention to this peculiar and apparently paradoxical state of affairs. By virtue of the axioms we can prove the existence of higher cardinalities... How can it be, then, that the entire domain \"B\" [a countable model of Zermelo's axioms] can already be enumerated by means of the finite positive integers?\" (Skolem 1922, p. 295, translation by Bauer-Mengelberg) \n\nMore specifically, let \"B\" be a countable model of Zermelo's axioms. Then there is some set \"u\" in \"B\" such that \"B\" satisfies the first-order formula saying that \"u\" is uncountable. For example, \"u\" could be taken as the set of real numbers in \"B\". Now, because \"B\" is countable, there are only countably many elements \"c\" such that \"c\" ∈ \"u\" according to \"B\", because there are only countably many elements \"c\" in \"B\" to begin with. Thus it appears that \"u\" should be countable. This is Skolem's paradox. \n\nSkolem went on to explain why there was no contradiction. In the context of a specific model of set theory, the term \"set\" does not refer to an arbitrary set, but only to a set that is actually included in the model. The definition of countability requires that a certain one-to-one correspondence, which is itself a set, must exist. Thus it is possible to recognise that a particular set \"u\" is countable, but not countable in a particular model of set theory, because there is no set in the model that gives a one-to-one correspondence between \"u\" and the natural numbers in that model. \n\nFrom an interpretation of the model into our conventional notions of these sets, this means that although \"u\" maps to an uncountable set, there are many elements in our intuitive notion of \"u\" that don't have a corresponding element in the model. The model, however, is consistent, because the absence of these elements cannot be observed through first-order logic. With \"u\" as the reals, these missing elements would correspond to undefinable numbers. \n\nSkolem used the term \"relative\" to describe this state of affairs, where the same set is included in two models of set theory, is countable in one model, and is not countable in the other model. He described this as the \"most important\" result in his paper. Contemporary set theorists describe concepts that do not depend on the choice of a transitive model as absolute. From their point of view, Skolem's paradox simply shows that countability is not an absolute property in first order logic. (Kunen 1980 p. 141; Enderton 2001 p. 152; Burgess 1977 p. 406).\n\nSkolem described his work as a critique of (first-order) set theory, intended to illustrate its weakness as a foundational system:\n\nA central goal of early research into set theory was to find a first-order axiomatisation for set theory which was categorical, meaning that the axioms would have exactly one model, consisting of all sets. Skolem's result showed this is not possible, creating doubts about the use of set theory as a foundation of mathematics. It took some time for the theory of first-order logic to be developed enough for mathematicians to understand the cause of Skolem's result; no resolution of the paradox was widely accepted during the 1920s. Fraenkel (1928) still described the result as an antinomy:\n\nIn 1925, von Neumann presented a novel axiomatisation of set theory, which developed into NBG set theory. Very much aware of Skolem's 1922 paper, von Neumann investigated countable models of his axioms in detail. In his concluding remarks, Von Neumann comments that there is no categorical axiomatisation of set theory, or any other theory with an infinite model. Speaking of the impact of Skolem's paradox, he wrote,\n\nZermelo at first considered the Skolem paradox a hoax (van Dalen and Ebbinghaus, 2000, p. 148 ff.), and spoke against it starting in 1929. Skolem's result applies only to what is now called first-order logic, but Zermelo argued against the finitary metamathematics that underlie first-order logic (Kanamori 2004, p. 519 ff.). Zermelo argued that his axioms should instead be studied in second-order logic, a setting in which Skolem's result does not apply. Zermelo published a second-order axiomatisation in 1930 and proved several categoricity results in that context. Zermelo's further work on the foundations of set theory after Skolem's paper led to his discovery of the cumulative hierarchy and formalisation of infinitary logic (van Dalen and Ebbinghaus, 2000, note 11). \n\nFraenkel \"et al.\" (1973, pp. 303–304) explain why Skolem's result was so surprising to set theorists in the 1920s. Gödel's completeness theorem and the compactness theorem were not proved until 1929. These theorems illuminated the way that first-order logic behaves and established its finitary nature, although Gödel's original proof of the completeness theorem was complicated. Leon Henkin's alternative proof of the completeness theorem, which is now a standard technique for constructing countable models of a consistent first-order theory, was not presented until 1947. Thus, in 1922, the particular properties of first-order logic that permit Skolem's paradox to go through were not yet understood. It is now known that Skolem's paradox is unique to first-order logic; if set theory is studied using higher-order logic with full semantics then it does not have any countable models, due to the semantics being used.\n\nCurrent mathematical logicians do not view Skolem's paradox as any sort of fatal flaw in set theory. Kleene (1967, p. 324) describes the result as \"not a paradox in the sense of outright contradiction, but rather a kind of anomaly\". After surveying Skolem's argument that the result is not contradictory, Kleene concludes \"there is no absolute notion of countability.\" Hunter (1971, p. 208) describes the contradiction as \"hardly even a paradox\". Fraenkel \"et al.\" (1973, p. 304) explain that contemporary mathematicians are no more bothered by the lack of categoricity of first-order theories than they are bothered by the conclusion of Gödel's incompleteness theorem that no consistent, effective, and sufficiently strong set of\nfirst-order axioms is complete. \n\nCountable models of ZF have become common tools in the study of set theory. Forcing, for example, is often explained in terms of countable models. The fact that these countable models of ZF still satisfy the theorem that there are uncountable sets is not considered a pathology; van Heijenoort (1967) describes it as \"a novel and unexpected feature of formal systems.\" (van Heijenoort 1967, p. 290) \n\nAlthough most mathematicians no longer consider Skolem's result paradoxical, the result is often discussed by philosophers, who may not be satisfied with a merely mathematical resolution of the paradox.\n\n\n"}
{"id": "542347", "url": "https://en.wikipedia.org/wiki?curid=542347", "title": "Specialization (pre)order", "text": "Specialization (pre)order\n\nIn the branch of mathematics known as topology, the specialization (or canonical) preorder is a natural preorder on the set of the points of a topological space. For most spaces that are considered in practice, namely for all those that satisfy the T separation axiom, this preorder is even a partial order (called the specialization order). On the other hand, for T spaces the order becomes trivial and is of little interest.\n\nThe specialization order is often considered in applications in computer science, where T spaces occur in denotational semantics. The specialization order is also important for identifying suitable topologies on partially ordered sets, as is done in order theory.\n\nConsider any topological space \"X\". The specialization preorder ≤ on \"X\" relates two points of \"X\" when one lies in the closure of the other. However, various authors disagree on which 'direction' the order should go. What is agreed is that if\n\n(where cl{\"y\"} denotes the closure of the singleton set {\"y\"}, i.e. the intersection of all closed sets containing {\"y\"}), we say that \"x\" is a specialization of \"y\" and that \"y\" is a generization of \"x\"; this is commonly written using a wavy arrow (such as the one given by the command \"\\leadsto\" in the \"amssymb\" LaTeX package) leading from \"y\" to \"x\".\n\nUnfortunately, the property \"\"x\" is a specialization of \"y\" is alternatively written as \"x\" ≤ \"y\" and as \"y\" ≤ \"x\" by various authors (see, respectively, and ).\n\nBoth definitions have intuitive justifications: in the case of the former, we have\n\nHowever in the case where our space \"X\" is the prime spectrum \"Spec R\" of a commutative ring \"R\" (which is the motivational situation in applications related to algebraic geometry), then under our second definition of the order, we have\n\nFor the sake of consistency, for the remainder of this article we will take the first definition, that \"x\" is a specialization of \"y\"\" be written as \"x\" ≤ \"y\". We then see,\n\nThese restatements help to explain why one speaks of a \"specialization\": \"y\" is more general than \"x\", since it is contained in more open sets. This is particularly intuitive if one views closed sets as properties that a point \"x\" may or may not have. The more closed sets contain a point, the more properties the point has, and the more special it is. The usage is consistent with the classical logical notions of genus and species; and also with the traditional use of generic points in algebraic geometry, in which closed points are the most specific, while a generic point of a space is one contained in every nonempty open subset. Specialization as an idea is applied also in valuation theory.\n\nThe intuition of upper elements being more specific is typically found in domain theory, a branch of order theory that has ample applications in computer science.\n\nLet \"X\" be a topological space and let ≤ be the specialization preorder on \"X\". Every open set is an upper set with respect to ≤ and every closed set is a lower set. The converses are not generally true. In fact, a topological space is an Alexandrov-discrete space if and only if every upper set is also open (or equivalently every lower set is also closed).\n\nLet \"A\" be a subset of \"X\". The smallest upper set containing \"A\" is denoted ↑\"A\"\nand the smallest lower set containing \"A\" is denoted ↓\"A\". In case \"A\" = {\"x\"} is a singleton one uses the notation ↑\"x\" and ↓\"x\". For \"x\" ∈ \"X\" one has:\n\n\nThe lower set ↓\"x\" is always closed; however, the upper set ↑\"x\" need not be open or closed. The closed points of a topological space \"X\" are precisely the minimal elements of \"X\" with respect to ≤.\n\n\nAs suggested by the name, the specialization preorder is a preorder, i.e. it is reflexive and transitive.\n\nThe equivalence relation determined by the specialization preorder is just that of topological indistinguishability. That is, \"x\" and \"y\" are topologically indistinguishable if and only if \"x\" ≤ \"y\" and \"y\" ≤ \"x\". Therefore, the antisymmetry of ≤ is precisely the T separation axiom: if \"x\" and \"y\" are indistinguishable then \"x\" = \"y\". In this case it is justified to speak of the specialization order.\n\nOn the other hand, the symmetry of specialization preorder is equivalent to the R separation axiom: \"x\" ≤ \"y\" if and only if \"x\" and \"y\" are topologically indistinguishable. It follows that if the underlying topology is T, then the specialization order is discrete, i.e. one has \"x\" ≤ \"y\" if and only if \"x\" = \"y\". Hence, the specialization order is of little interest for T topologies, especially for all Hausdorff spaces.\n\nAny continuous function between two topological spaces is monotone with respect to the specialization preorders of these spaces. The converse, however, is not true in general. In the language of category theory, we then have a functor from the category of topological spaces to the category of preordered sets which assigns a topological space its specialization preorder. This functor has a left adjoint which places the Alexandrov topology on a preordered set.\n\nThere are spaces that are more specific than T spaces for which this order is interesting: the sober spaces. Their relationship to the specialization order is more subtle:\n\nFor any sober space \"X\" with specialization order ≤, we have\n\nOne may describe the second property by saying that open sets are \"inaccessible by directed suprema\". A topology is order consistent with respect to a certain order ≤ if it induces ≤ as its specialization order and it has the above property of inaccessibility with respect to (existing) suprema of directed sets in ≤.\n\nThe specialization order yields a tool to obtain a preorder from every topology. It is natural to ask for the converse too: Is every preorder obtained as a specialization preorder of some topology?\n\nIndeed, the answer to this question is positive and there are in general many topologies on a set \"X\" which induce a given order ≤ as their specialization order. The Alexandroff topology of the order ≤ plays a special role: it is the finest topology that induces ≤. The other extreme, the coarsest topology that induces ≤, is the upper topology, the least topology within which all complements of sets {\"y\" in \"X\" | \"y\" ≤ \"x\"} (for some \"x\" in \"X\") are open.\n\nThere are also interesting topologies in between these two extremes. The finest sober topology that is order consistent in the above sense for a given order ≤ is the Scott topology. The upper topology however is still the coarsest sober order consistent topology. In fact, its open sets are even inaccessible by \"any\" suprema. Hence any sober space with specialization order ≤ is finer than the upper topology and coarser than the Scott topology. Yet, such a space may fail to exist, that is, there exist partial orders for which there is no sober order consistent topology. Especially, the Scott topology is not necessarily sober.\n\n"}
{"id": "466277", "url": "https://en.wikipedia.org/wiki?curid=466277", "title": "St. Petersburg paradox", "text": "St. Petersburg paradox\n\nThe St. Petersburg paradox or St. Petersburg lottery is a paradox related to probability and decision theory in economics. It is based on a particular (theoretical) lottery game that leads to a random variable with infinite expected value (i.e., infinite expected payoff) but nevertheless seems to be worth only a very small amount to the participants. The St. Petersburg paradox is a situation where a naive decision criterion which takes only the expected value into account predicts a course of action that presumably no actual person would be willing to take. Several resolutions are possible.\n\nThe paradox takes its name from its resolution by Daniel Bernoulli, one-time resident of the eponymous Russian city, who published his arguments in the \"Commentaries of the Imperial Academy of Science of Saint Petersburg\" . However, the problem was invented by Daniel's cousin, Nicolas Bernoulli, who first stated it in a letter to Pierre Raymond de Montmort on September 9, 1713 .\n\nA casino offers a game of chance for a single player in which a fair coin is tossed at each stage. The initial stake starts at 2 dollars and is doubled every time heads appears. The first time tails appears, the game ends and the player wins whatever is in the pot. Thus the player wins 2 dollars if tails appears on the first toss, 4 dollars if heads appears on the first toss and tails on the second, 8 dollars if heads appears on the first two tosses and tails on the third, and so on. Mathematically, the player wins 2 dollars, where \"k\" equals number of tosses (k must be a whole number and greater than zero). What would be a fair price to pay the casino for entering the game?\n\nTo answer this, one needs to consider what would be the average payout: with probability , the player wins 2 dollars; with probability the player wins 4 dollars; with probability the player wins 8 dollars, and so on. The expected value is thus\n\nAssuming the game can continue as long as the coin toss results in heads and in particular that the casino has unlimited resources, this sum grows without bound and so the expected win for repeated play is an infinite amount of money. Considering nothing but the expected value of the net change in one's monetary wealth, one should therefore play the game at any price if offered the opportunity. Yet, in published descriptions of the game, many people expressed disbelief in the result. Martin Robert quotes Ian Hacking as saying \"few of us would pay even $25 to enter such a game\" and says most commentators would agree. The paradox is the discrepancy between what people seem willing to pay to enter the game and the infinite expected value.\n\nSeveral approaches have been proposed for solving the paradox.\n\nThe classical resolution of the paradox involved the explicit introduction of a utility function, an expected utility hypothesis, and the presumption of diminishing marginal utility of money.\n\nIn Daniel Bernoulli's own words:\nThe determination of the value of an item must not be based on the price, but rather on the utility it yields…. There is no doubt that a gain of one thousand ducats is more significant to the pauper than to a rich man though both gain the same amount.\n\nA common utility model, suggested by Bernoulli himself, is the logarithmic function \"U\"(\"w\") = ln(\"w\") (known as “log utility”). It is a function of the gambler’s total wealth \"w\", and the concept of diminishing marginal utility of money is built into it. The expected utility hypothesis posits that a utility function exists the sign of whose expected net change from accepting the gamble is a good criterion for real people's behavior. For each possible event, the change in utility will be weighted by the probability of that event occurring. Let \"c\" be the cost charged to enter the game. The expected incremental utility of the lottery now converges to a finite value:\n\nThis formula gives an implicit relationship between the gambler's wealth and how much he should be willing to pay to play (specifically, any \"c\" that gives a positive change in expected utility). For example, with natural log utility, a millionaire ($1,000,000) should be willing to pay up to $20.88, a person with $1,000 should pay up to $10.95, a person with $2 should borrow $1.35 and pay up to $3.35.\n\nBefore Daniel Bernoulli published, in 1728, a mathematician from Geneva, Gabriel Cramer, had already found parts of this idea (also motivated by the St. Petersburg Paradox) in stating that\nthe mathematicians estimate money in proportion to its quantity, and men of good sense in proportion to the usage that they may make of it.\n\nHe demonstrated in a letter to Nicolas Bernoulli that a square root function describing the diminishing marginal benefit of gains can resolve the problem. However, unlike Daniel Bernoulli, he did not consider the total wealth of a person, but only the gain by the lottery.\n\nThis solution by Cramer and Bernoulli, however, is not completely satisfying, since the lottery can easily be changed in a way such that the paradox reappears. To this aim, we just need to change the game so that it gives even more rapidly increasing payoffs. For any unbounded utility function, one can find a lottery that allows for a variant of the St. Petersburg paradox, as was first pointed out by Menger .\n\nRecently, expected utility theory has been extended to arrive at more behavioral decision models. In some of these new theories, as in cumulative prospect theory, the St. Petersburg paradox again appears in certain cases, even when the utility function is concave, but not if it is bounded .\n\nNicolas Bernoulli himself proposed an alternative idea for solving the paradox. He conjectured that people will neglect unlikely events . Since in the St. Petersburg lottery only unlikely events yield the high prizes that lead to an infinite expected value, this could resolve the paradox. The idea of probability weighting resurfaced much later in the work on prospect theory by Daniel Kahneman and Amos Tversky.\n\nCumulative prospect theory is one popular generalization of expected utility theory that can predict many behavioral regularities . However, the overweighting of small probability events introduced in cumulative prospect theory may restore the St. Petersburg paradox. Cumulative prospect theory avoids the St. Petersburg paradox only when the power coefficient of the utility function is lower than the power coefficient of the probability weighting function . Intuitively, the utility function must not simply be concave, but it must be concave relative to the probability weighting function to avoid the St. Petersburg paradox.\nOne can argue that the formulas for the prospect theory are obtained in the region of less than $400 . This is not applicable for infinitely increasing sums in the St. Petersburg paradox.\n\nVarious authors, including Jean le Rond d'Alembert and John Maynard Keynes, have rejected maximization of expectation (even of utility) as a proper rule of conduct. Keynes, in particular, insisted that the \"relative risk\" of an alternative could be sufficiently high to reject it even if its expectation were enormous.\n\nThe classical St. Petersburg lottery assumes that the casino has infinite resources. This assumption is unrealistic, particularly in connection with the paradox, which involves the reactions of ordinary people to the lottery. Of course, the resources of an actual casino (or any other potential backer of the lottery) are finite. More importantly, the expected value of the lottery only grows logarithmically with the resources of the casino. As a result, the expected value of the lottery, even when played against a casino with the largest resources realistically conceivable, is quite modest. If the total resources (or total maximum jackpot) of the casino are \"W\" dollars, then \"L\" = floor(log(\"W\")) is the maximum number of times the casino can play before it no longer fully covers the next bet. The expected value \"E\" of the lottery then becomes:\n\nThe following table shows the expected value \"E\" of the game with various potential bankers and their bankroll \"W\" (with the assumption that if you win more than the bankroll you will be paid what the bank has):\n\nA rational person might not find the lottery worth even the modest amounts in the above table, suggesting that the naive decision model of the expected return causes essentially the same problems as for the infinite lottery. Even so, the possible discrepancy between theory and reality is far less dramatic.\n\nThe premise of infinite resources produces a variety of paradoxes in economics. In the martingale betting system, a gambler betting on a tossed coin doubles his bet after every loss, so that an eventual win would cover all losses; this system fails with any finite bankroll. The gambler's ruin concept shows a persistent gambler will go broke, even if the game provides a positive expected value, and no betting system can avoid this inevitability.\n\nAlthough this paradox is three centuries old, new arguments are still being introduced.\n\nA mathematically correct solution is sampling by William Feller. In order to understand Feller's answer correctly, sufficient knowledge about probability theory and statistics is necessary, but it can be understood intuitively \"to perform this game with a large number of people and calculate the expected value from the sample extraction\". In this method, when the games of infinite number of times are possible, the expected value will be infinity, and in the case of finite, the expected value will be a small value.\n\nSamuelson resolves the paradox by arguing that, even if an entity had infinite resources, the game would never be offered. If the lottery represents an infinite expected gain to the player, then it also represents an infinite expected loss to the host. No one could be observed paying to play the game because it would never be offered. As Paul Samuelson describes the argument:\n\"Paul will never be willing to give as much as Peter will demand for such a contract; and hence the indicated activity will take place at the equilibrium level of zero intensity.\" \n\nOle Peters thinks that the St. Petersburg paradox can be solved by using concepts and ideas from ergodic theory . In statistical mechanics it is a central problem to understand whether time averages resulting from a long observation of a single system are equivalent to expectation values. This is the case only for a very limited class of systems that are called \"ergodic.\" For non-ergodic systems there is no general reason why expectation values should have any relevance.\n\nPeters points out that computing the naive expected payout is mathematically equivalent to considering multiple outcomes of the same lottery in parallel universes. This is irrelevant to the individual considering whether to buy a ticket since he exists in only one universe and is unable to exchange resources with the others. It is therefore unclear why expected wealth should be a quantity whose maximization should lead to a sound decision theory. Indeed, the St. Petersburg paradox is only a paradox if one accepts the premise that rational actors seek to maximize their expected wealth. The classical resolution is to apply a utility function to the wealth, which reflects the notion that the \"usefulness\" of an amount of money depends on how much of it one already has, and then to maximise the expectation of this. The choice of utility function is often framed in terms of the individual's risk preferences and may vary between individuals: it therefore provides a somewhat arbitrary framework for the treatment of the problem.\n\nAn alternative premise, which is less arbitrary and makes fewer assumptions, is that the performance \"over time\" of an investment better characterises an investor's prospects and, therefore, better informs his investment decision. In this case, the passage of time is incorporated by identifying as the quantity of interest the average \"rate\" of exponential growth of the player's wealth in a single round of the lottery,\n\nper round, where \"D\" is the \"k\"th (positive finite) payout, \"p\" is the (non-zero) probability of receiving it, \"w\" is the wealth of the player, and \"c\" is the cost of a ticket. In the standard St. Petersburg lottery, and .\n\nAlthough this is an expectation value of a growth rate, and may therefore be thought of in one sense as an average over parallel universes, it is in fact equivalent to the time average growth rate that would be obtained if repeated lotteries were played over time . While is identical to the rate of change of the expected logarithmic utility, it has been obtained without making any assumptions about the player's risk preferences or behaviour, other than that he is interested in the rate of growth of his wealth.\n\nUnder this paradigm, an individual with wealth \"w\" should buy a ticket at a price \"c\" provided\n\nThis strategy counsels against paying any amount of money for a ticket that admits the possibility of bankruptcy, i.e.\n\nfor any \"k\", since this generates a negatively divergent logarithm in the sum for which can be shown to dominate all other terms in the sum and guarantee that  < 0. If we assume the smallest payout is \"D\", then the individual will always be advised to decline the ticket at any price greater than\n\n\"regardless\" of the payout structure of the lottery. The ticket price for which the expected growth rate falls to zero will be less than \"c\" but may be greater than \"w\", indicating that borrowing money to purchase a ticket for more than one's wealth can be a sound decision. This would be the case, for example, where the smallest payout exceeds the player's current wealth, as it does in Menger's game.\n\nIt should also be noted in the above treatment that, contrary to Menger's analysis, no higher-paying lottery can generate a paradox which the time resolution – or, equivalently, Bernoulli's or Laplace's logarithmic resolutions – fail to resolve, since there is \"always\" a price at which the lottery should not be entered, even though for especially favourable lotteries this may be greater than one's worth.\n\nThe St. Petersburg paradox and the theory of marginal utility have been highly disputed in the past. For a discussion from the point of view of a philosopher, see .\n\n\n\n\n\n\n\n\n"}
{"id": "7892663", "url": "https://en.wikipedia.org/wiki?curid=7892663", "title": "Stanley–Wilf conjecture", "text": "Stanley–Wilf conjecture\n\nThe Stanley–Wilf conjecture, formulated independently by Richard P. Stanley and Herbert Wilf in the late 1980s, states that the growth rate of every proper permutation class is singly exponential. It was proved by and is no longer a conjecture. Marcus and Tardos actually proved a different conjecture, due to , which had been shown to imply the Stanley–Wilf conjecture by .\n\nThe Stanley–Wilf conjecture states that for every permutation \"β\", there is a constant \"C\" such that the number |\"S\"(\"β\")| of permutations of length \"n\" which avoid \"β\" as a permutation pattern is at most \"C\". As observed, this is equivalent to the convergence of the limit\n\nThe upper bound given by Marcus and Tardos for \"C\" is exponential in the length of \"β\". A stronger conjecture of had stated that one could take \"C\" to be , where \"k\" denotes the length of \"β\", but this conjecture was disproved for the permutation by . Indeed, has shown that \"C\" is, in fact, exponential in \"k\" for almost all permutations.\n\nThe growth rate (or Stanley–Wilf limit) of a permutation class is defined as\nwhere \"a\" denotes the number of permutations of length \"n\" in the class. Clearly not every positive real number can be a growth rate of a permutation class, regardless of whether it is defined by a single forbidden permutation pattern or a set of forbidden patterns. For example, numbers strictly between 0 and 1 cannot be growth rates of permutation classes.\n\nfor an integer \"k\" ≥ 2. This shows that 2 is the least accumulation point of growth rates of permutation classes.\n\n\n\n"}
{"id": "23874923", "url": "https://en.wikipedia.org/wiki?curid=23874923", "title": "Szpilrajn extension theorem", "text": "Szpilrajn extension theorem\n\nIn mathematics, the Szpilrajn extension theorem, due to (later called Edward Marczewski), is one of many examples of the use of the axiom of choice (in the form of Zorn's lemma) to find a maximal set with certain properties.\n\nThe theorem states that every strict partial order is contained into a total order, where:\n\n\nIntuitively, the theorem states that a comparison between elements that leaves some pairs incomparable can be extended in such a way every element is either less than or greater than another.\n\nA binary relation \"R\" over a set \"S\" is formally defined as a set of pairs of elements \"<x,y>\" where \"x, y ∈ S\". The condition \"<x,y> ∈ R\" is generally abbreviated \"xRy\".\n\nA relation is irreflexive if \"xRx\" holds for no element \"x ∈ S\". It is transitive if \"xRy\" and \"yRz\" imply \"xRz\". It is total if either \"xRy\" or \"yRx\" holds for every pair of elements \"x\" and \"y\" of \"S\".\n\nIf a relation \"R\" is contained in another \"T\" then every pair in the first is also in the second. As a result, \"xRy\" implies \"xTy\", which can be taken as an alternative definition of containment.\n\nThe extension theorem tells that every relation \"R\" that is non-reflexive and transitive (a strict partial order) is contained in another \"T\" that is still non-reflexive and transitive but also total (a total order).\n\nThe theorem is proved in two steps: first, if a strict partial order does not compare \"x\" and \"y\", it can be extended by first adding the pair \"〈 x,y 〉\" and then performing the transitive closure; second, since this operation generates an ordering that strictly contains the original one and can be applied to all pairs of incomparable elements, there exists a relation in which all pairs of elements have been made comparable.\n\nThe first step is proved as a preliminary lemma, in which a strict partial order where a pair of elements \"x\" and \"y\" are incomparable is changed to make them comparable. This is obtained by first adding the pair \"xRy\" to the relation, which may result in a non-transitive relation, and then restoring transitivity by adding all pairs \"qRy\" such that \"qRx\", all pairs \"xRp\" such that \"yRp\", as well as all pairs \"qRp\" such that \"qRx\" and \"yRp\". This is done on a single pair of incomparable elements \"x\" and \"y\", but produces a relation that is still irreflexive and transitive and that strictly contains the original one because at least the pair \"xRy\" has been added. Some other pairs of elements may still be incomparable, but the change can be performed on it again.\n\nThe extension theorem is then proved by showing that the set of strict partial orders containing \"R\" has some maximal element. If such an element had a pair of incomparable elements the transformation could be applied to it, resulting in a relation that strictly contains it. This would contradict maximality.\n\nThe existence of a maximal elements is proved via Zorn's lemma. In this context, it is applied to the set of all strict orders extending \"R\", compared by set inclusion. A chain is therefore a sequence of relations, each extending \"R\" and contained in the following one. The lemma can be applied if every chain of relations extending \"R\" has an upper bound which is also a strict partial order extending \"R\".\n\nIn other words, this upper bound should be a superset of the original relation, which is the case because every member of the chain contains it, and also a strict partial order: non-reflexive and transitive. The first holds because every element of the chain is a strict partial order, so none of the relations of the chain contains such a pair. Transitivity holds because the pairs \"〈 x,y 〉\" and \"〈 y,z 〉\" are in the union only if the first is in some relation of the chain and the second in some other. Being a chain, either the first relation is contained in the second or the second in the first. In the first case, \"〈 x,y 〉\" is also in the second relation. Since all relations produced by the transformation are transitive, \"〈 x,z 〉\" is in the second relation and therefore also in the union. A similar argument holds if the second relation is contained in the first.\n\nBy Zorn's lemma, the set of strict partial orders extending the original relation \"R\" has a maximal element \"Q\", which is also a strict partial order extending \"R\". This relation \"Q\" is proved to be total: if it had a pair of incomparable elements then the transformation could be applied to it, leading to another strict partial order that extends \"R\" and strictly contains \"Q\", therefore contradicting the assumption that the \"Q\" is maximal in the set of strict partial orders extending \"R\". This proves that \"Q\" has no pair of incomparable elements, in addition to being a strict partial order extending \"R\". As a result, it is a total order extending \"R\".\n\n\n"}
{"id": "15785733", "url": "https://en.wikipedia.org/wiki?curid=15785733", "title": "Tangential angle", "text": "Tangential angle\n\nIn geometry, the tangential angle of a curve in the Cartesian plane, at a specific point, is the angle between the tangent line to the curve at the given point and the -axis. (Note that some authors define the angle as the deviation from the direction of the curve at some fixed starting point. This is equivalent to the definition given here by the addition of a constant to the angle or by rotating the curve.)\n\nIf a curve is given parametrically by , then the tangential angle at is defined (up to a multiple of ) by\n\nHere, the prime symbol denotes the derivative with respect to . Thus, the tangential angle specifies the direction of the velocity vector , while the speed specifies its magnitude. The vector\n\nis called the unit tangent vector, so an equivalent definition is that the tangential angle at is the angle such that is the unit tangent vector at .\n\nIf the curve is parametrized by arc length , so , then the definition simplifies to\n\nIn this case, the curvature is given by , where is taken to be positive if the curve bends to the left and negative if the curve bends to the right.\n\nIf the curve is given by , then we may take as the parametrization, and we may assume is between and . This produces the explicit expression\n\nIn polar coordinates, the polar tangential angle is defined as the angle between the tangent line to the curve at the given point and ray from the origin to the point. If denotes the polar tangential angle, then , where is as above and is, as usual, the polar angle.\n\nIf the curve is defined in polar coordinates by , then the polar tangential angle at is defined (up to a multiple of ) by\n\nIf the curve is parametrized by arc length as , , so , then the definition becomes\n\nThe logarithmic spiral can be defined a curve whose polar tangential angle is constant.\n\n\n"}
{"id": "3500070", "url": "https://en.wikipedia.org/wiki?curid=3500070", "title": "The Fourth Dimension (book)", "text": "The Fourth Dimension (book)\n\nThe Fourth Dimension is a non-fiction work written by Rudy Rucker, the Silicon Valley professor of mathematics and computer science, and was published in 1984 by Houghton Mifflin. The book is subtitled as a guided tour of the higher universes. The foreword included is by Martin Gardner, and the 200+ illustrations are by David Povilaitis. Like other books by Rucker, \"The Fourth Dimension\" is dedicated to Edwin Abbott Abbott, author of the novella \"Flatland\".\n\n\"The Fourth Dimension\" teaches readers about the concept of a fourth spatial dimension. Several analogies are made to \"Flatland\"; in particular, Rucker compares how a square in Flatland would react to a cube in Spaceland to how a cube in Spaceland would react to a hypercube from the fourth dimension.\n\nThe book also includes multiple puzzles.\n\n\"Kirkus Reviews\" called it \"animated, often amusing\", and a \"rare treat\", but noted that the book eventually leaves mathematical topics behind to focus instead on \"mysticism of the all-is-one-one-is-all thinking of an Ouspensky.\" The \"Quarterly Review of Biology\" declared it to be \"nice\", and \"at times (...) enchanting\", comparing it to \"The Tao of Physics\".\n\n"}
{"id": "29954", "url": "https://en.wikipedia.org/wiki?curid=29954", "title": "Topology", "text": "Topology\n\nIn mathematics, topology (from the Greek τόπος, \"place\", and λόγος, \"study\") is concerned with the properties of space that are preserved under continuous deformations, such as stretching, twisting, crumpling and bending, but not tearing or gluing. \n\nAn n-dimensional topological space is a space (not necessarily Euclidean) with certain properties of connectedness and compactness. \n\nThe space may be continuous (like all points on a rubber sheet), or discrete (like the set of integers). It can be open (like the set of points inside a circle) or closed (like the set of points inside a circle, together with the points on the circle).\n\nTopology developed as a field of study out of geometry and set theory, through analysis of concepts such as space, dimension, and transformation. Such ideas go back to Gottfried Leibniz, who in the 17th century envisioned the \"geometria situs\" (Greek-Latin for \"geometry of place\") and \"analysis situs\" (Greek-Latin for \"picking apart of place\"). Leonhard Euler's Seven Bridges of Königsberg Problem and Polyhedron Formula are arguably the field's first theorems. The term \"topology\" was introduced by Johann Benedict Listing in the 19th century, although it was not until the first decades of the 20th century that the idea of a topological space was developed. By the middle of the 20th century, topology had become a major branch of mathematics.\nTopology, as a well-defined mathematical discipline, originates in the early part of the twentieth century, but some isolated results can be traced back several centuries. Among these are certain questions in geometry investigated by Leonhard Euler. His 1736 paper on the Seven Bridges of Königsberg is regarded as one of the first practical applications of topology. On 14 November 1750 Euler wrote to a friend that he had realised the importance of the \"edges\" of a polyhedron. This led to his polyhedron formula, \"V\" − \"E\" + \"F\" = 2 (where \"V\", \"E\" and \"F\" respectively indicate the number of vertices, edges and faces of the polyhedron). Some authorities regard this analysis as the first theorem, signalling the birth of topology.\n\nFurther contributions were made by Augustin-Louis Cauchy, Ludwig Schläfli, Johann Benedict Listing, Bernhard Riemann and Enrico Betti. Listing introduced the term \"Topologie\" in \"Vorstudien zur Topologie\", written in his native German, in 1847, having used the word for ten years in correspondence before its first appearance in print. The English form \"topology\" was used in 1883 in Listing's obituary in the journal \"Nature\" to distinguish \"qualitative geometry from the ordinary geometry in which quantitative relations chiefly are treated\". The term \"topologist\" in the sense of a specialist in topology was used in 1905 in the magazine \"Spectator\".\n\nTheir work was corrected, consolidated and greatly extended by Henri Poincaré. In 1895 he published his ground-breaking paper on \"Analysis Situs\", which introduced the concepts now known as homotopy and homology, which are now considered part of algebraic topology.\n\nUnifying the work on function spaces of Georg Cantor, Vito Volterra, Cesare Arzelà, Jacques Hadamard, Giulio Ascoli and others, Maurice Fréchet introduced the metric space in 1906. A metric space is now considered a special case of a general topological space, with any given topological space potentially giving rise to many distinct metric spaces. In 1914, Felix Hausdorff coined the term \"topological space\" and gave the definition for what is now called a Hausdorff space. Currently, a topological space is a slight generalization of Hausdorff spaces, given in 1922 by Kazimierz Kuratowski.\n\nModern topology depends strongly on the ideas of set theory, developed by Georg Cantor in the later part of the 19th century. In addition to establishing the basic ideas of set theory, Cantor considered point sets in Euclidean space as part of his study of Fourier series. For further developments, see point-set topology and algebraic topology.\n\nTopology can be formally defined as \"the study of qualitative properties of certain objects (called topological spaces) that are invariant under a certain kind of transformation (called a continuous map), especially those properties that are invariant under a certain kind of invertible transformation (called homeomorphisms).\"\n\nTopology is also used to refer to a structure imposed upon a set \"X\", a structure that essentially 'characterizes' the set \"X\" as a topological space by taking proper care of properties such as convergence, connectedness and continuity, upon transformation.\n\nTopological spaces show up naturally in almost every branch of mathematics. This has made topology one of the great unifying ideas of mathematics.\n\nThe motivating insight behind topology is that some geometric problems depend not on the exact shape of the objects involved, but rather on the way they are put together. For example, the square and the circle have many properties in common: they are both one dimensional objects (from a topological point of view) and both separate the plane into two parts, the part inside and the part outside.\n\nIn one of the first papers in topology, Leonhard Euler demonstrated that it was impossible to find a route through the town of Königsberg (now Kaliningrad) that would cross each of its seven bridges exactly once. This result did not depend on the lengths of the bridges, nor on their distance from one another, but only on connectivity properties: which bridges connect to which islands or riverbanks. This problem in introductory mathematics called \"Seven Bridges of Königsberg\" led to the branch of mathematics known as graph theory.\n\nSimilarly, the hairy ball theorem of algebraic topology says that \"one cannot comb the hair flat on a hairy ball without creating a cowlick.\" This fact is immediately convincing to most people, even though they might not recognize the more formal statement of the theorem, that there is no nonvanishing continuous tangent vector field on the sphere. As with the \"Bridges of Königsberg\", the result does not depend on the shape of the sphere; it applies to any kind of smooth blob, as long as it has no holes.\n\nTo deal with these problems that do not rely on the exact shape of the objects, one must be clear about just what properties these problems \"do\" rely on. From this need arises the notion of homeomorphism. The impossibility of crossing each bridge just once applies to any arrangement of bridges homeomorphic to those in Königsberg, and the hairy ball theorem applies to any space homeomorphic to a sphere.\n\nIntuitively, two spaces are homeomorphic if one can be deformed into the other without cutting or gluing. A traditional joke is that a topologist cannot distinguish a coffee mug from a doughnut, since a sufficiently pliable doughnut could be reshaped to a coffee cup by creating a dimple and progressively enlarging it, while shrinking the hole into a handle.\n\nHomeomorphism can be considered the most basic topological equivalence. Another is homotopy equivalence. This is harder to describe without getting technical, but the essential notion is that two objects are homotopy equivalent if they both result from \"squishing\" some larger object.\nAn introductory exercise is to classify the uppercase letters of the English alphabet according to homeomorphism and homotopy equivalence. The result depends partially on the font used. The figures use the sans-serif Myriad font. Homotopy equivalence is a rougher relationship than homeomorphism; a homotopy equivalence class can contain several homeomorphism classes. The simple case of homotopy equivalence described above can be used here to show two letters are homotopy equivalent. For example, O fits inside P and the tail of the P can be squished to the \"hole\" part.\n\nHomeomorphism classes are:\n\nHomotopy classes are larger, because the tails can be squished down to a point. They are:\n\nTo classify the letters correctly, we must show that two letters in the same class are equivalent and two letters in different classes are not equivalent. In the case of homeomorphism, this can be done by selecting points and showing their removal disconnects the letters differently. For example, X and Y are not homeomorphic because removing the center point of the X leaves four pieces; whatever point in Y corresponds to this point, its removal can leave at most three pieces. The case of homotopy equivalence is harder and requires a more elaborate argument showing an algebraic invariant, such as the fundamental group, is different on the supposedly differing classes.\n\nLetter topology has practical relevance in stencil typography. For instance, Braggadocio font stencils are made of one connected piece of material.\n\nThe term topology also refers to a specific mathematical idea central to the area of mathematics called topology. Informally, a topology tells how elements of a set relate spatially to each other. The same set can have different topologies. For instance, the real line, the complex plane (which is a 1-dimensional complex vector space), and the Cantor set can be thought of as the same set with different topologies.\n\nFormally, let \"X\" be a set and let \"τ\" be a family of subsets of \"X\". Then \"τ\" is called a \"topology on X\" if:\n\n\nIf \"τ\" is a topology on \"X\", then the pair (\"X\", \"τ\") is called a topological space. The notation \"X\" may be used to denote a set \"X\" endowed with the particular topology \"τ\".\n\nThe members of \"τ\" are called \"open sets\" in \"X\". A subset of \"X\" is said to be closed if its complement is in \"τ\" (i.e., its complement is open). A subset of \"X\" may be open, closed, both (clopen set), or neither. The empty set and \"X\" itself are always both closed and open. A subset of \"X\" including an open set containing a point \"x\" is called a 'neighborhood' of \"x\".\n\nA function or map from one topological space to another is called \"continuous\" if the inverse image of any open set is open. If the function maps the real numbers to the real numbers (both spaces with the standard topology), then this definition of continuous is equivalent to the definition of continuous in calculus. If a continuous function is one-to-one and onto, and if the inverse of the function is also continuous, then the function is called a homeomorphism and the domain of the function is said to be homeomorphic to the range. Another way of saying this is that the function has a natural extension to the topology. If two spaces are homeomorphic, they have identical topological properties, and are considered topologically the same. The cube and the sphere are homeomorphic, as are the coffee cup and the doughnut. But the circle is not homeomorphic to the doughnut.\n\nWhile topological spaces can be extremely varied and exotic, many areas of topology focus on the more familiar class of spaces known as manifolds. A manifold is a topological space that resembles Euclidean space near each point. More precisely, each point of an \"n\"-dimensional manifold has a neighbourhood that is homeomorphic to the Euclidean space of dimension \"n\". Lines and circles, but not figure eights, are one-dimensional manifolds. Two-dimensional manifolds are also called surfaces. Examples include the plane, the sphere, and the torus, which can all be realized without self-intersection in three dimensions, but also the Klein bottle and real projective plane, which cannot.\n\nGeneral topology is the branch of topology dealing with the basic set-theoretic definitions and constructions used in topology. It is the foundation of most other branches of topology, including differential topology, geometric topology, and algebraic topology. Another name for general topology is point-set topology.\n\nThe fundamental concepts in point-set topology are \"continuity\", \"compactness\", and \"connectedness\". Intuitively, continuous functions take nearby points to nearby points. Compact sets are those that can be covered by finitely many sets of arbitrarily small size. Connected sets are sets that cannot be divided into two pieces that are far apart. The words \"nearby\", \"arbitrarily small\", and \"far apart\" can all be made precise by using open sets. If we change the definition of \"open set\", we change what continuous functions, compact sets, and connected sets are. Each choice of definition for \"open set\" is called a \"topology\". A set with a topology is called a \"topological space\".\n\n\"Metric spaces\" are an important class of topological spaces where distances can be assigned a number called a \"metric\". Having a metric simplifies many proofs, and many of the most common topological spaces are metric spaces.\n\nAlgebraic topology is a branch of mathematics that uses tools from abstract algebra to study topological spaces. The basic goal is to find algebraic invariants that classify topological spaces up to homeomorphism, though usually most classify up to homotopy equivalence.\n\nThe most important of these invariants are homotopy groups, homology, and cohomology.\n\nAlthough algebraic topology primarily uses algebra to study topological problems, using topology to solve algebraic problems is sometimes also possible. Algebraic topology, for example, allows for a convenient proof that any subgroup of a free group is again a free group.\n\nDifferential topology is the field dealing with differentiable functions on differentiable manifolds. It is closely related to differential geometry and together they make up the geometric theory of differentiable manifolds.\n\nMore specifically, differential topology considers the properties and structures that require only a smooth structure on a manifold to be defined. Smooth manifolds are 'softer' than manifolds with extra geometric structures, which can act as obstructions to certain types of equivalences and deformations that exist in differential topology. For instance, volume and Riemannian curvature are invariants that can distinguish different geometric structures on the same smooth manifold—that is, one can smoothly \"flatten out\" certain manifolds, but it might require distorting the space and affecting the curvature or volume.\n\nGeometric topology is a branch of topology that primarily focuses on low-dimensional manifolds (i.e. spaces of dimensions 2,3 and 4) and their interaction with geometry, but it also includes some higher-dimensional topology.\n\nIn high-dimensional topology, characteristic classes are a basic invariant, and surgery theory is a key theory.\n\nLow-dimensional topology is strongly geometric, as reflected in the uniformization theorem in 2 dimensions – every surface admits a constant curvature metric; geometrically, it has one of 3 possible geometries: positive curvature/spherical, zero curvature/flat, negative curvature/hyperbolic – and the geometrization conjecture (now theorem) in 3 dimensions – every 3-manifold can be cut into pieces, each of which has one of eight possible geometries.\n\n2-dimensional topology can be studied as complex geometry in one variable (Riemann surfaces are complex curves) – by the uniformization theorem every conformal class of metrics is equivalent to a unique complex one, and 4-dimensional topology can be studied from the point of view of complex geometry in two variables (complex surfaces), though not every 4-manifold admits a complex structure.\n\nOccasionally, one needs to use the tools of topology but a \"set of points\" is not available. In pointless topology one considers instead the lattice of open sets as the basic notion of the theory, while Grothendieck topologies are structures defined on arbitrary categories that allow the definition of sheaves on those categories, and with that the definition of general cohomology theories.\n\nKnot theory, a branch of topology, is used in biology to study the effects of certain enzymes on DNA. These enzymes cut, twist, and reconnect the DNA, causing knotting with observable effects such as slower electrophoresis. Topology is also used in evolutionary biology to represent the relationship between phenotype and genotype. Phenotypic forms that appear quite different can be separated by only a few mutations depending on how genetic changes map to phenotypic changes during development. In neuroscience, topological quantities like the Euler characteristic and Betti number have been used to measure the complexity of patterns of activity in neural networks.\n\nTopological data analysis uses techniques from algebraic topology to determine the large scale structure of a set (for instance, determining if a cloud of points is spherical or toroidal). The main method used by topological data analysis is:\n\nTopology is relevant to physics in areas such as condensed matter physics, quantum field theory and physical cosmology.\n\nThe topological dependence of mechanical properties in solids is of interest in disciplines of mechanical engineering and materials science. Electrical and mechanical properties depend on the arrangement and network structures of molecules and elementary units in materials. The compressive strength of crumpled topologies is studied in attempts to understand the high strength to weight of such structures that are mostly empty space. Topology is of further significance in Contact mechanics where the dependence of stiffness and friction on the dimensionality of surface structures is the subject of interest with applications in multi-body physics.\n\nA topological quantum field theory (or topological field theory or TQFT) is a quantum field theory that computes topological invariants.\n\nAlthough TQFTs were invented by physicists, they are also of mathematical interest, being related to, among other things, knot theory, the theory of four-manifolds in algebraic topology, and to the theory of moduli spaces in algebraic geometry. Donaldson, Jones, Witten, and Kontsevich have all won Fields Medals for work related to topological field theory.\n\nThe topological classification of Calabi-Yau manifolds has important implications in string theory, as different manifolds can sustain different kinds of strings.\n\nIn cosmology, topology can be used to describe the overall shape of the universe. This area of research is commonly known as spacetime topology.\n\nTopological methods have been used to evaluate the complexity of geomorphological and ecological functions in a landscape and can be applied on geographic maps, or field observations (Papadimitriou, 2013), or even on models and programs representing such functions and their changes in time (Papadimitriou, 2012). \n\nThe possible positions of a robot can be described by a manifold called configuration space. In the area of motion planning, one finds paths between two points in configuration space. These paths represent a motion of the robot's joints and other parts into the desired pose.\n\nTanglement puzzles are based on topological aspects of the puzzle's shapes and components.\n\nIn order to create a continuous join of pieces in a modular construction, it is necessary to create an unbroken path in an order which surrounds each piece and traverses each edge only once. This process is an application of the Eulerian path.\n\n\n\n"}
{"id": "40282", "url": "https://en.wikipedia.org/wiki?curid=40282", "title": "Type theory", "text": "Type theory\n\nIn mathematics, logic, and computer science, type theory is a branch of computational logic that studies types, which informally, are attributes that objects can possess. In type theory, each object is a \"term\" of a definite type and operations on objects are restricted to those which are definitely terms of the relevant types. As a formal system, some type theories contend to be simultaneously an alternative foundation of mathematics to set theory, a programming language and a calculus for category theory. \n\nType theory is closely related to (and in some cases overlaps with) type systems, which are a programming language feature used to reduce bugs. Type theory was created to avoid paradoxes in a variety of formal logics and rewrite systems.\n\nTwo well-known type theories that can serve as mathematical foundations are Alonzo Church's typed λ-calculus and Per Martin-Löf's intuitionistic type theory.\n\nBetween 1902 and 1908 Bertrand Russell proposed various \"theories of type\" in response to his discovery that Gottlob Frege's version of naive set theory was afflicted with Russell's paradox. By 1908 Russell arrived at a \"ramified\" theory of types together with an \"axiom of reducibility\" both of which featured prominently in Whitehead and Russell's \"Principia Mathematica\" published between 1910 and 1913. They attempted to resolve Russell's paradox by first creating a hierarchy of types, then assigning each concrete mathematical (and possibly other) entity to a type. Entities of a given type are built exclusively from entities of those types that are lower in their hierarchy, thus preventing an entity from being assigned to itself. In the 1920s, Leon Chwistek and Frank P. Ramsey proposed an unramified type theory, now known as the \"theory of simple types\" or \"simple type theory\", that collapsed the hierarchy of the types in the earlier ramified theory and as such did not require the axiom of reducibility.\n\nThe common usage of \"type theory\" is when those types are used with a term rewrite system. The most famous early example is Alonzo Church's simply typed lambda calculus. Church's theory of types helped the formal system avoid the Kleene–Rosser paradox that afflicted the original untyped lambda calculus. Church demonstrated that it could serve as a foundation of mathematics and it was referred to as a higher-order logic.\n\nSome other type theories include Per Martin-Löf's intuitionistic type theory, which has been the foundation used in some areas of constructive mathematics and for the proof assistant Agda. Thierry Coquand's calculus of constructions and its derivatives are the foundation used by Coq and others. The field is an area of active research, as demonstrated by homotopy type theory.\n\nIn a system of type theory, each term has a type. For example, formula_1, formula_2, and formula_3 are all separate terms with the type formula_4 for natural numbers. Traditionally, the term is followed by a colon and its type, such as formula_5.\n\nType theories have explicit computation and it is encoded in rules for rewriting terms. These are called conversion rules or, if the rule only works in one direction, a reduction rule. For example, formula_6 and formula_1 are syntactically different terms, but the former reduces to the latter. This reduction is written formula_8.\n\nFunctions in type theory have a special reduction rule: the argument of the function call gets substituted for every occurrence of the parameter in the function definition. Let's say the function formula_9 is defined as formula_10 (using Church's lambda notation) or formula_11 (using a more modern notation). Then, the function call formula_12 would be reduced by substituting formula_13 for every copy of formula_14 in the body of the function definition. Thus, formula_15.\n\nThe type of a function is denoted with an arrow formula_16 from the parameter type to the function's resulting type. Thus, formula_17. Calling or \"applying\" a function to an argument may be written with or without parentheses, so formula_18 or formula_12. Not using parentheses is more common, because multiple argument functions can be defined using currying.\n\nThere are many different set theories and many different systems of type theory, so what follows are generalizations.\n\nThe term formula_20 reduces to formula_21. Since formula_21 cannot be reduced further, it is called a normal form. A system of type theory is said to be strongly normalizing if all terms have a normal form and any order of reductions reaches it. Weakly normalizing systems have a normal form but some orders of reductions may loop forever and never reach it.\n\nFor a normalizing system, some borrow the word element from set theory and use it to refer to all closed terms that can reduce to the same normal form. A closed term is one without parameters. (A term like formula_23 with its parameter formula_14 is called an open term.) Thus, formula_20 and formula_26 may be different terms but they are both from the element formula_21.\n\nA similar idea that works for open and closed terms is convertibility. Two terms are convertible if there exists a term that they both reduce to. For example, formula_20 and formula_29 are convertible. As are formula_30 and formula_31. However, formula_23 and formula_33 (where formula_14 is a free variable) are not because both are in normal form and they are not the same. Confluent and weakly normalizing systems can test if two terms are convertible by checking if they both reduce to the same normal form.\n\nA dependent type is a type that depends on a term or on another type. Thus, the type returned by a function may depend upon the argument to the function.\n\nFor example, a list of formula_4s of length 4 may be a different type than a list of formula_4s of length 5. In a type theory with dependent types, it is possible to define a function that takes a parameter \"n\" and returns a list containing \"n\" zeros. Calling the function with 4 would produce a term with a different type than if the function was called with 5.\n\nDependent types play a central role in intuitionistic type theory and in the design of functional programming languages like Idris, ATS, Agda and Epigram.\n\nMany systems of type theory have a type that represents equality of types and of terms. This type is different from convertibility, and is often denoted propositional equality.\n\nIn intuitionistic type theory, the equality type (also called the identity type) is known as formula_37 for identity. There is a type formula_38 when formula_39 is a type and formula_40 and formula_41 are both terms of type formula_39. A term of type formula_38 is interpreted as meaning that formula_40 is equal to formula_41.\n\nIn practice, it is possible to build a type formula_46 but there will not exist a term of that type. In intuitionistic type theory, new terms of equality start with reflexivity. If formula_21 is a term of type formula_4, then there exists a term of type formula_49. More complicated equalities can be created by creating a reflexive term and then doing a reduction on one side. So if formula_50 is a term of type formula_4, then there is a term of type formula_52 and, by reduction, generate a term of type formula_53. Thus, in this system, the equality type denotes that two values of the same type are convertible by reductions.\n\nHaving a type for equality is important because it can be manipulated inside the system. There is usually no judgement to say two terms are \"not\" equal; instead, as in the Brouwer–Heyting–Kolmogorov interpretation, we map formula_54 to formula_55, where formula_56 is the bottom type having no values. There exists a term with type but not one of type formula_57.\n\nHomotopy type theory differs from intuitionistic type theory mostly by its handling of the equality type.\n\nA system of type theory requires some basic terms and types to operate on. Some systems build them out of functions using Church encoding. Other systems have inductive types: a set of base types and a set of type constructors that generate types with well-behaved properties. For example, certain recursive functions called on inductive types are guaranteed to terminate.\n\nCoinductive type are infinite data types created by giving a function that generates the next element(s). See Coinduction and Corecursion.\n\nInduction-induction is a feature for declaring an inductive type and a family of types that depends on the inductive type.\n\nInduction recursion allows a wider range of well-behaved types, allowing the type and recursive functions operating on it to be defined at the same time.\n\nTypes were created to prevent paradoxes, such as Russell's paradox. However, the motives that lead to those paradoxes—being able to say things about all types—still exist. So, many type theories have a \"universe type\", which contains all \"other\" types (and not itself).\n\nIn systems where you might want to say something about universe types, there is a hierarchy of universe types, each containing the one below it in the hierarchy. The hierarchy is defined as being infinite, but statements must only refer to a finite number of universe levels.\n\nType universes are particularly tricky in type theory. The initial proposal of intuitionistic type theory suffered from Girard's paradox.\n\nMany systems of type theory, such as the simply-typed lambda calculus, intuitionistic type theory, and the calculus of constructions, are also programming languages. That is, they are said to have a \"computational component\". The computation is the reduction of terms of the language using rewriting rules.\n\nA system of type theory that has a well-behaved computational component also has a simple connection to constructive mathematics through the BHK interpretation.\n\nNon-constructive mathematics in these systems is possible by adding operators on continuations such as call with current continuation. However, these operators tend to break desirable properties such as canonicity and parametricity.\n\n\n\n\nThere is extensive overlap and interaction between the fields of type theory and type systems. Type systems are a programming language feature designed to identify bugs. Any static program analysis, such as the type checking algorithms in the semantic analysis phase of compiler, has a connection to type theory.\n\nA prime example is Agda, a programming language which uses intuitionistic type theory for its type system. The programming language ML was developed for manipulating type theories (see LCF) and its own type system was heavily influenced by them.\n\nThe first computer proof assistant, called Automath, used type theory to encode mathematics on a computer. Martin-Löf specifically developed intuitionistic type theory to encode \"all\" mathematics to serve as a new foundation for mathematics. There is current research into mathematical foundations using homotopy type theory.\n\nMathematicians working in category theory already had difficulty working with the widely accepted foundation of Zermelo–Fraenkel set theory. This led to proposals such as Lawvere's Elementary Theory of the Category of Sets (ETCS). Homotopy type theory continues in this line using type theory. Researchers are exploring connections between dependent types (especially the identity type) and algebraic topology (specifically homotopy).\n\nMuch of the current research into type theory is driven by proof checkers, interactive proof assistants, and automated theorem provers. Most of these systems use a type theory as the mathematical foundation for encoding proofs, which is not surprising, given the close connection between type theory and programming languages:\n\nMultiple type theories are supported by LEGO and Isabelle. Isabelle also supports foundations besides type theories, such as ZFC. Mizar is an example of a proof system that only supports set theory.\n\nType theory is also widely in use in formal theories of semantics of natural languages, especially Montague grammar and its descendants. In particular, categorial grammars and pregroup grammars make extensive use of type constructors to define the types (\"noun\", \"verb\", etc.) of words.\n\nThe most common construction takes the basic types formula_58 and formula_59 for individuals and truth-values, respectively, and defines the set of types recursively as follows:\n\nA complex type formula_62 is the type of functions from entities of type formula_40 to entities of type formula_41. Thus one has types like formula_66 which are interpreted as elements of the set of functions from entities to truth-values, i.e. indicator functions of sets of entities. An expression of type formula_67 is a function from sets of entities to truth-values, i.e. a (indicator function of a) set of sets. This latter type is standardly taken to be the type of natural language quantifiers, like \" everybody\" or \" nobody\" (Montague 1973, Barwise and Cooper 1981).\n\nGregory Bateson introduced a theory of logical types into the social sciences; his notions of double bind and logical levels are based on Russell's theory of types.\n\nAlthough the initial motivation for category theory was far removed from foundationalism, the two fields turned out to have deep connections. As John Lane Bell writes: \"In fact categories can \"themselves\" be viewed as type theories of a certain kind; this fact alone indicates that type theory is much more closely related to category theory than it is to set theory.\" In brief, a category can be viewed as a type theory by regarding its objects as types (or sorts), i.e. \"Roughly speaking, a category may be thought of as a type theory shorn of its syntax.\" A number of significant results follow in this way:\n\nThe interplay, known as categorical logic, has been a subject of active research since then; see the monograph of Jacobs (1999) for instance.\n\n\n\n"}
