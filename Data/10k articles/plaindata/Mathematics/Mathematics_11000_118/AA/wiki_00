{"id": "407347", "url": "https://en.wikipedia.org/wiki?curid=407347", "title": "96 (number)", "text": "96 (number)\n\n96 (ninety-six) is the natural number following 95 and preceding 97. It is a number that, when turned upside down, is still itself. Some other numbers that equal themselves include 69, 0, 88, and in some cases, 1 and 11\n\n96 is:\n\n\nEvery integer greater than 96 may be represented as a sum of distinct super-prime numbers.\n\n\n\n\n\n\n"}
{"id": "791314", "url": "https://en.wikipedia.org/wiki?curid=791314", "title": "Actualism", "text": "Actualism\n\nIn contemporary analytic philosophy, actualism is the view that everything there \"is\" (i.e., everything that has \"being\", in the broadest sense) is actual. Another phrasing of the thesis is that the domain of unrestricted quantification ranges over all and only actual existents.\n\nThe denial of actualism is possibilism, the thesis that there are some entities that are \"merely possible\": these entities have being but are not actual and, hence, enjoy a \"less robust\" sort of being than do actually existing things. An important, but significantly different notion of possibilism known as \"modal realism\" was developed by the philosopher David Lewis. On Lewis's account, the actual world is identified with the physical universe of which we are all a part. Other possible worlds exist in exactly the same sense as the actual world; they are simply spatio-temporally unrelated to our world, and to each other. Hence, for Lewis, \"merely possible\" entities—entities that exist in other possible worlds—exist in exactly the same sense as do we in the actual world; to be actual, from the perspective of any given individual \"x\" in any possible world, is simply to be part of the same world as \"x\".\n\nConsider the statement \"Sherlock Holmes exists.\" This is a false statement about the world, but is usually accepted as representing a possible truth. This contingency is usually described by the statement \"there is a possible world in which Sherlock Holmes exists\". The possibilist argues that apparent existential claims such as this (that \"there are\" possible worlds of various sorts) ought to be taken more or less at face value: as stating the \"existence\" of two or more worlds, only one of which (at the most) can be the actual one. Hence, they argue, there are innumerably many possible worlds other than our own, which exist just as much as ours does.\n\nMost actualists will be happy to grant the interpretation of \"Sherlock Holmes' existence is possible\" in terms of possible worlds. But they argue that the possibilist goes wrong in taking this as a sign that there exist other worlds that are just like ours, except for the fact that we are not actually in them. The actualist argues, instead, that when we claim \"possible worlds\" exist we are making claims that things exist \"in our own actual world\" which can serve as possible worlds for the interpretation of modal claims: that many \"ways the world could be\" (actually) exist, but not that any \"worlds which are those ways\" exist other than the actual world around us.\n\nFrom an actualist point of view, such as Adams', possible worlds are nothing more than fictions created within the actual world. Possible worlds are mere descriptions of ways this world (the actual one) might have been, and nothing else. Thus, as modal constructions, they come in as a handy heuristic device to use with modal logic; as it helps our modal reasoning to imagine ways the world might have been. Thus, the actualist interpretation of \"◊p\" sees the modality (i.e., \"the way\" in which it is true) as being de dicto and not entailing any ontological commitment.\n\nSo, from this point of view, what distinguishes the actual world from other possible worlds is what distinguishes reality from a description of a simulation of reality, this world from Sherlock Holmes': the former exists and is not a product of imagination and the latter does not exist and is a product of the imagination set in a modal construction.\n\nFrom a modal realist’s point of view, such as Lewis', the proposition \"◊p\" means that p obtains in at least one other, distinct world that is as real as the one we are in. If a state of affairs is possible, then it really obtains, it physically occurs in at least one world. Therefore, as Lewis is happy to admit, there is a world where someone named Sherlock Holmes lived at 221b Baker Street in Victorian times, there is another world where pigs fly, and there is even another world where both Sherlock Holmes exists and pigs fly.\n\nThis leaves open the question, of course, of what an actually existing \"way the world could be\" \"is\"; and on this question actualists are divided. One of the most popular solutions is to claim, as William Lycan and Robert Adams do, that \"possible worlds\" talk can be reduced to logical relations amongst consistent and maximally complete \"sets of propositions\". \"Consistent\" here means that none of its propositions contradict one another (if they did, it would not be a \"possible\" description of the world); \"maximally complete\" means that the set covers \"every\" feature of the world. (More precisely: a set of propositions is \"maximally complete\" if, for \"any\" meaningful proposition P, P is either an element of the set, or the \"negation\" of an element of the set, or entailed by the conjunction of one or more elements of the set, or the \"negation\" of a proposition entailed by the conjunction of one or more elements of the set). Here the \"possible world\" which is said to be actual is actual in virtue of \"all\" its elements being \"true\" of the world around us.\n\nAnother common actualist account, advanced in different forms by Alvin Plantinga and David Armstrong views \"possible worlds\" not as \"descriptions\" of how the world might be (through a very large set of statements) but rather as a maximally complete \"state of affairs\" that covers every \"state of affairs\" which might obtain or not obtain. Here, the \"possible world\" which is said to be actual is actual in virtue of that state of affairs obtaining in the world around us. (Since it is maximally complete, only \"one\" such state of affairs could actually obtain; all the others would differ from the actual world in various large or small ways.)\n\nAccording to the indexical conception of actuality, favoured by Lewis (1986), actuality is an attribute which our world has relative to itself, but which all the other worlds have relative to themselves too. Actuality is an intrinsic property of each world, so world \"w\" is actual just at world \"w\". \"Actual\" is seen as an indexical term, and its reference depends on its context. Therefore, there is no feature of this world (nor of any other) to be distinguished in order to infer that the world is actual, \"the actual world\" is actual simply in virtue of the definition of \"actual\": a world is actual \"simpliciter\".\n\n"}
{"id": "1416758", "url": "https://en.wikipedia.org/wiki?curid=1416758", "title": "Ad infinitum", "text": "Ad infinitum\n\nAd infinitum is a Latin phrase meaning \"to infinity\" or \"forevermore\".\n\nIn context, it usually means \"continue forever, without limit\" and this can be used to describe a non-terminating process, a non-terminating \"repeating\" process, or a set of instructions to be repeated \"forever,\" among other uses. It may also be used in a manner similar to the Latin phrase \"et cetera\" to denote written words or a concept that continues for a lengthy period beyond what is shown. Examples include:\n\n<poem>\nThe vermin only teaze and pinch\nTheir foes superior by an inch.\nSo, naturalists observe, a flea\nHas smaller fleas that on him prey;\nAnd these have smaller still to bite 'em,\nAnd so proceed \"ad infinitum\".\nThus every poet, in his kind,\nIs bit by him that comes behind</poem>\n\n\n__notoc__\n"}
{"id": "1620216", "url": "https://en.wikipedia.org/wiki?curid=1620216", "title": "Algebraic graph theory", "text": "Algebraic graph theory\n\nAlgebraic graph theory is a branch of mathematics in which algebraic methods are applied to problems about graphs. This is in contrast to geometric, combinatoric, or algorithmic approaches. There are three main branches of algebraic graph theory, involving the use of linear algebra, the use of group theory, and the study of graph invariants.\n\nThe first branch of algebraic graph theory involves the study of graphs in connection with linear algebra. Especially, it studies the spectrum of the adjacency matrix, or the Laplacian matrix of a graph (this part of algebraic graph theory is also called spectral graph theory). For the Petersen graph, for example, the spectrum of the adjacency matrix is (−2, −2, −2, −2, 1, 1, 1, 1, 1, 3). Several theorems relate properties of the spectrum to other graph properties. As a simple example, a connected graph with diameter \"D\" will have at least \"D\"+1 distinct values in its spectrum. Aspects of graph spectra have been used in analysing the synchronizability of networks.\n\nThe second branch of algebraic graph theory involves the study of graphs in connection to group theory, particularly automorphism groups and geometric group theory. The focus is placed on various families of graphs based on symmetry (such as symmetric graphs, vertex-transitive graphs, edge-transitive graphs, distance-transitive graphs, distance-regular graphs, and strongly regular graphs), and on the inclusion relationships between these families. Certain of such categories of graphs are sparse enough that lists of graphs can be drawn up. By Frucht's theorem, all groups can be represented as the automorphism group of a connected graph (indeed, of a cubic graph). Another connection with group theory is that, given any group, symmetrical graphs known as Cayley graphs can be generated, and these have properties related to the structure of the group.\n\nThis second branch of algebraic graph theory is related to the first, since the symmetry properties of a graph are reflected in its spectrum. In particular, the spectrum of a highly symmetrical graph, such as the Petersen graph, has few distinct values (the Petersen graph has 3, which is the minimum possible, given its diameter). For Cayley graphs, the spectrum can be related directly to the structure of the group, in particular to its irreducible characters.\n\nFinally, the third branch of algebraic graph theory concerns algebraic properties of invariants of graphs, and especially the chromatic polynomial, the Tutte polynomial and knot invariants. The chromatic polynomial of a graph, for example, counts the number of its proper vertex colorings. For the Petersen graph, this polynomial is formula_1. In particular, this means that the Petersen graph cannot be properly colored with one or two colors, but can be colored in 120 different ways with 3 colors. Much work in this area of algebraic graph theory was motivated by attempts to prove the four color theorem. However, there are still many open problems, such as characterizing graphs which have the same chromatic polynomial, and determining which polynomials are chromatic.\n\n"}
{"id": "2289219", "url": "https://en.wikipedia.org/wiki?curid=2289219", "title": "Analytic element method", "text": "Analytic element method\n\nThe analytic element method (AEM) is a numerical method used for the solution of partial differential equations. It was initially developed by O.D.L. Strack at the University of Minnesota. It is similar in nature to the boundary element method (BEM), as it does not rely upon discretization of volumes or areas in the modeled system; only internal and external boundaries are discretized. One of the primary distinctions between AEM and BEMs is that the boundary integrals are calculated analytically. \n\nThe analytic element method has been applied to problems of groundwater flow governed by a variety of linear partial differential equations including the Laplace, the Poisson equation, the modified Helmholtz equation, the heat equation, and the biharmonic equations. \n\nThe basic premise of the analytic element method is that, for linear differential equations, elementary solutions may be superimposed to obtain more complex solutions. A suite of 2D and 3D analytic solutions (\"elements\") are available for different governing equations. These elements typically correspond to a discontinuity in the dependent variable or its gradient along a geometric boundary (e.g., point, line, ellipse, circle, sphere, etc.). This discontinuity has a specific functional form (usually a polynomial in 2D) and may be manipulated to satisfy Dirichlet, Neumann, or Robin (mixed) boundary conditions. Each analytic solution is infinite in space and/or time. In addition, each analytic solution contains degrees of freedom (coefficients) that may be calculated to meet prescribed boundary conditions along the element's border. To obtain a global solution (i.e., the correct element coefficients), a system of equations is solved such that the boundary conditions are satisfied along all of the elements (using collocation, least-squares minimization, or a similar approach). Notably, the global solution provides a spatially continuous description of the dependent variable everywhere in the infinite domain, and the governing equation is satisfied everywhere exactly except along the border of the element, where the governing equation is not strictly applicable due to the discontinuity.\n\nThe ability to superpose numerous elements in a single solution means that analytical solutions can be realized for arbitrarily complex boundary conditions. That is, models that have complex geometries, straight or curved boundaries, multiple boundaries, transient boundary conditions, multiple aquifer layers, piecewise varying properties and continuously varying properties can be solved. Elements can be implemented using far-field expansions such that model containing many thousands of elements can be solved efficiently to high precision.\n\nA contemporary student of Strack's who is a proponent of the Analytic Element Method (AEM) in groundwater modeling applications is Dr. David Steward of Kansas State University.\n\n\n\n"}
{"id": "48834853", "url": "https://en.wikipedia.org/wiki?curid=48834853", "title": "Anti-information", "text": "Anti-information\n\nInformation is that which reduces uncertainty, wholly or in part.\nSimilarly, anti-information is that which increases uncertainty.\nIt is negative information.\n\nNoise on a noisy communication channel is an example of anti-information.\nAccording to Shannon's Channel Coding Theorem\nthe entropy of the noise must be subtracted to obtain the channel capacity\nthat remains available for reliable communication.\n\nThe gambling industry has made a business out of selling anti-information.\nPeople are willing to pay for anti-information.\nThe increase in uncertainty enables them to savor the information that they subsequently receive\nwhen the uncertainty is finally resolved.\n\nThe term anti-information was introduced by Koos Verhoeff in the 1970s\nwhile teaching informatics at the Erasmus University Rotterdam.\n"}
{"id": "6038197", "url": "https://en.wikipedia.org/wiki?curid=6038197", "title": "Antiderivative (complex analysis)", "text": "Antiderivative (complex analysis)\n\nIn complex analysis, a branch of mathematics, the antiderivative, or primitive, of a complex-valued function \"g\" is a function whose complex derivative is \"g\". More precisely, given an open set formula_1 in the complex plane and a function formula_2 the antiderivative of formula_3 is a function formula_4 that satisfies formula_5.\n\nAs such, this concept is the complex-variable version of the antiderivative of a real-valued function.\n\nThe derivative of a constant function is the zero function. Therefore, any constant function is an antiderivative of the zero function. If formula_1 is a connected set, then the constant functions are the only antiderivatives of the zero function. Otherwise, a function is an antiderivative of the zero function if and only if it is constant on each connected component of formula_1 (those constants need not be equal).\n\nThis observation implies that if a function formula_8 has an antiderivative, then that antiderivative is unique up to addition of a function which is constant on each connected component of formula_1.\n\nOne can characterize the existence of antiderivatives via path integrals in the complex plane, much like the case of functions of a real variable. Perhaps not surprisingly, \"g\" has an antiderivative \"f\" if and only if, for every γ path from \"a\" to \"b\", the path integral \n\nEquivalently, \n\nfor any closed path γ. \n\nHowever, this formal similarity notwithstanding, possessing a complex-antiderivative is a much more restrictive condition than its real counterpart. While it is possible for a discontinuous real function to have an anti-derivative, anti-derivatives can fail to exist even for \"holomorphic\" functions of a complex variable. For example, consider the reciprocal function, \"g\"(\"z\") = 1/\"z\" which is holomorphic on the punctured plane C\\{0}. A direct calculation shows that the integral of \"g\" along any circle enclosing the origin is non-zero. So \"g\" fails the condition cited above. This is similar to the existence of potential functions for conservative vector fields, in that Green's theorem is only able to guarantee path independence when the function in question is defined on a \"simply connected\" region, as in the case of the Cauchy integral theorem.\n\nIn fact, holomorphy is characterized by having an antiderivative \"locally\", that is, \"g\" is holomorphic if for every \"z\" in its domain, there is some neighborhood \"U\" of \"z\" such that \"g\" has an antiderivative on \"U\". Furthermore, holomorphy is a necessary condition for a function to have an antiderivative, since the derivative of any holomorphic function is holomorphic.\n\nVarious versions of Cauchy integral theorem, an underpinning result of Cauchy function theory, which makes heavy use of path integrals, gives sufficient conditions under which, for a holomorphic \"g\", \n\nvanishes for any closed path γ (which may be, for instance, that the domain of \"g\" be simply connected or star-convex).\n\nFirst we show that if \"f\" is an antiderivative of \"g\" on \"U\", then \"g\" has the path integral property given above. Given any piecewise \"C\" path γ : [\"a\", \"b\"] → \"U\", one can express the path integral of \"g\" over γ as \n\nBy the chain rule and the fundamental theorem of calculus one then has\n\nTherefore the integral of \"g\" over γ does \"not\" depend on the actual path γ, but only on its endpoints, which is what we wanted to show.\n\nNext we show that if \"g\" is holomorphic, and the integral of \"g\" over any path depends only on the endpoints, then \"g\" has an antiderivative. We will do so by finding an anti-derivative explicitly.\n\nWithout loss of generality, we can assume that the domain \"U\" of \"g\" is connected, as otherwise one can prove the existence of an antiderivative on each connected component. With this assumption, fix a point \"z\" in \"U\" and for any \"z\" in \"U\" define the function\n\nwhere γ is any path joining \"z\" to \"z\". Such a path exists since \"U\" is assumed to be an open connected set. The function \"f\" is well-defined because the integral depends only on the endpoints of γ. \n\nThat this \"f\" is an antiderivative of \"g\" can be argued in the same way as the real case. We have, for a given \"z\" in \"U\",\n\nwhere [\"z\", \"w\"] denotes the line segment between \"z\" and \"w\". By continuity of \"g\", the final expression goes to zero as \"w\" approaches \"z\". In other words, \"f′\" = \"g\".\n\n\n"}
{"id": "970447", "url": "https://en.wikipedia.org/wiki?curid=970447", "title": "Chern–Weil homomorphism", "text": "Chern–Weil homomorphism\n\nIn mathematics, the Chern–Weil homomorphism is a basic construction in Chern–Weil theory that computes topological invariants of vector bundles and principal bundles on a smooth manifold \"M\" in terms of connections and curvature representing classes in the de Rham cohomology rings of \"M\". That is, the theory forms a bridge between the areas of algebraic topology and differential geometry. It was developed in the late 1940s by Shiing-Shen Chern and André Weil, in the wake of proofs of the generalized Gauss–Bonnet theorem. This theory was an important step in the theory of characteristic classes.\n\nLet \"G\" be a real or complex Lie group with Lie algebra formula_1; and let formula_2 denote the algebra of formula_3-valued polynomials on formula_1 (exactly the same argument works if we used formula_5 instead of formula_3.) Let formula_7 be the subalgebra of fixed points in formula_8 under the adjoint action of \"G\"; that is, it consists of all polynomials \"f\" such that for any \"g\" in \"G\" and \"x\" in formula_9, formula_10\n\nGiven principal G-bundle \"P\" on \"M\", there is an associated \nhomomorphism of formula_11-algebras\ncalled the Chern–Weil homomorphism,\nwhere on the right cohomology is de Rham cohomology. This homomorphism is obtained by taking invariant polynomials in the curvature of any connection on the given bundle. If \"G\" is either compact or semi-simple, \nthen the cohomology ring of the classifying space for \"G\"-bundles \"BG\" is isomorphic to the algebra formula_13 of invariant polynomials:\n(The cohomology ring of \"BG\" can still be given in the de Rham sense:\nwhen formula_16 and formula_17 are manifolds.)\n\nChoose any connection form ω in \"P\", and let Ω be the associated curvature 2-form; i.e., Ω = \"D\"ω, the exterior covariant derivative of ω. If formula_18 is a homogeneous polynomial function of degree \"k\"; i.e., formula_19 for any complex number \"a\" and \"x\" in formula_1, then, viewing \"f\" as a symmetric multilinear functional on formula_21 (see the ring of polynomial functions), let\nbe the (scalar-valued) 2\"k\"-form on \"P\" given by\nwhere \"v\" are tangent vectors to \"P\", formula_24 is the sign of the permutation formula_25 in the symmetric group on 2\"k\" numbers formula_26 (see Lie algebra-valued forms#Operations as well as Pfaffian).\n\nIf, moreover, \"f\" is invariant; i.e., formula_27, then one can show that formula_22 is a closed form, it descends to a unique form on \"M\" and that the de Rham cohomology class of the form is independent of \"ω\". First, that formula_22 is a closed form follows from the next two lemmas:\n\nIndeed, Bianchi's second identity says formula_33 and, since \"D\" is a graded derivation, formula_34 Finally, Lemma 1 says formula_22 satisfies the hypothesis of Lemma 2.\n\nTo see Lemma 2, let formula_36 be the projection and \"h\" be the projection of formula_37 onto the horizontal subspace. Then Lemma 2 is a consequence of the fact that formula_38 (the kernel of formula_39 is precisely the vertical subspace.) As for Lemma 1, first note\nwhich is because formula_41 and \"f\" is invariant. Thus, one can define formula_31 by the formula:\nwhere formula_44 are any lifts of formula_45: formula_46.\n\nNext, we show that the de Rham cohomology class of formula_31 on \"M\" is independent of a choice of connection. Let formula_48 be arbitrary connection forms on \"P\" and let formula_49 be the projection. Put\nwhere \"t\" is a smooth function on formula_51 given by formula_52. Let formula_53 be the curvature forms of formula_54. Let formula_55 be the inclusions. Then formula_56 is homotopic to formula_57. Thus, formula_58 and formula_59 belong to the same de Rham cohomology class by the homotopy invariance of de Rham cohomology. Finally, by naturality and by uniqueness of descending,\nand the same for formula_61. Hence, formula_62 belong to the same cohomology class.\n\nThe construction thus gives the linear map: (cf. Lemma 1)\nIn fact, one can check that the map thus obtained:\nis an algebra homomorphism.\n\nLet formula_65 and formula_66 its Lie algebra. For each \"x\" in formula_9, we can consider its characteristic polynomial in \"t\":\nwhere \"i\" is the square root of -1. Then formula_69 are invariant polynomials on formula_9, since the left-hand side of the equation is. The \"k\"-th Chern class of a smooth complex-vector bundle \"E\" of rank \"n\" on a manifold \"M\":\nis given as the image of \"f\" under the Chern–Weil homomorphism defined by \"E\" (or more precisely the frame bundle of \"E\"). If \"t\" = 1, then formula_72 is an invariant polynomial. The total Chern class of \"E\" is the image of this polynomial; that is,\n\nDirectly from the definition, one can show \"c\", \"c\" given above satisfy the axioms of Chern classes. For example, for the Whitney sum formula, we consider\nwhere we wrote Ω for the curvature 2-form on \"M\" of the vector bundle \"E\" (so it is the descendent of the curvature form on the frame bundle of \"E\"). The Chern–Weil homomorphism is the same if one uses this Ω. Now, suppose \"E\" is a direct sum of vector bundles \"E\"'s and Ω the curvature form of \"E\" so that, in the matrix term, Ω is the block diagonal matrix with Ω's on the diagonal. Then, since formula_75, we have:\nwhere on the right the multiplication is that of a cohomology ring: cup product. For the normalization property, one computes the first Chern class of the complex projective line; see .\n\nSince formula_77, we also have:\n\nFinally, the Chern character of \"E\" is given by\nwhere Ω is the curvature form of some connection on \"E\" (since Ω is nilpotent, it is a polynomial in Ω.) Then ch is a ring homomorphism:\nNow suppose, in some ring \"R\" containing the cohomology ring \"H\"(\"M\", C), there is the factorization of the polynomial in \"t\":\nwhere λ are in \"R\" (they are sometimes called Chern roots.) Then formula_82.\n\nIf \"E\" is a smooth real vector bundle on a manifold \"M\", then the \"k\"-th Pontrjagin class of \"E\" is given as:\nwhere we wrote formula_84 for the complexification of \"E\". Equivalently, it is the image under the Chern–Weil homomorphism of the invariant polynomial formula_85 on formula_86 given by:\nLet \"E\" be a holomorphic (complex-)vector bundle on a complex manifold \"M\". The curvature form Ω of \"E\", with respect to some hermitian metric, is not just a 2-form, but is in fact a (1, 1)-form (see holomorphic vector bundle#Hermitian metrics on a holomorphic vector bundle). Hence, the Chern–Weil homomorphism assumes the form: with formula_65,\n\n"}
{"id": "733042", "url": "https://en.wikipedia.org/wiki?curid=733042", "title": "Classical information channel", "text": "Classical information channel\n\nIn quantum information science, a classical information channel (often called simply classical channel) is a communication channel that can be used to transmit classical information (as opposed to quantum channel which can transmit quantum information). An example would be a light travelling over fiber optics lines or electricity travelling over phone lines.\n\nAlthough classical channels cannot transmit quantum information by themselves, they can be useful in combination with quantum channels. Examples of their use are:\n\n\n"}
{"id": "32670698", "url": "https://en.wikipedia.org/wiki?curid=32670698", "title": "Continuous q-Hahn polynomials", "text": "Continuous q-Hahn polynomials\n\nIn mathematics, the continuous \"q\"-Hahn polynomials are a family of basic hypergeometric orthogonal polynomials in the basic Askey scheme. give a detailed list of their properties.\n\nThe polynomials are given in terms of basic hypergeometric functions and the Pochhammer symbol by \n\nformula_2\n\n"}
{"id": "18729970", "url": "https://en.wikipedia.org/wiki?curid=18729970", "title": "Correlation swap", "text": "Correlation swap\n\nA correlation swap is an over-the-counter financial derivative that allows one to speculate on or hedge risks associated with the observed average correlation, of a collection of underlying products, where each product has periodically observable prices, as with a commodity, exchange rate, interest rate, or stock index.\n\nThe fixed leg of a correlation swap pays the notional formula_1 times the agreed strike formula_2, while the floating leg pays the realized correlation formula_3. The contract value at expiration from the pay-fixed perspective is therefore \n\nGiven a set of nonnegative weights formula_5 on formula_6 securities, the realized correlation is defined as the weighted average of all pairwise correlation coefficients formula_7:\nTypically formula_7 would be calculated as the Pearson correlation coefficient between the daily log-returns of assets \"i\" and \"j\", possibly under zero-mean assumption.\n\nMost correlation swaps trade using equal weights, in which case the realized correlation formula simplifies to:\n\nNo industry-standard models yet exist that have stochastic correlation and are arbitrage-free.\n\n"}
{"id": "991210", "url": "https://en.wikipedia.org/wiki?curid=991210", "title": "Divisibility rule", "text": "Divisibility rule\n\nA divisibility rule is a shorthand way of determining whether a given integer is divisible by a fixed divisor without performing the division, usually by examining its digits. Although there are divisibility tests for numbers in any radix, or base, and they are all different, this article presents rules and examples only for decimal, or base 10, numbers. Martin Gardner explained and popularized these rules in his September 1962 \"Mathematical Games\" column in \"Scientific American\".\n\nThe rules given below transform a given number into a generally smaller number, while preserving divisibility by the divisor of interest. Therefore, unless otherwise noted, the resulting number should be evaluated for divisibility by the same divisor. In some cases the process can be iterated until the divisibility is obvious; for others (such as examining the last \"n\" digits) the result must be examined by other means.\n\nFor divisors with multiple rules, the rules are generally ordered first for those appropriate for numbers with many digits, then those useful for numbers with fewer digits.\n\nNote: To test divisibility by any number that can be expressed as 2 or 5, in which \"n\" is a positive integer, just examine the last \"n\" digits.\n\nNote: To test divisibility by any number expressed as the product of prime factors formula_1, we can separately test for divisibility by each prime to its appropriate power. For example, testing divisibility by 24 (24 = 8*3 = 2*3) is equivalent to testing divisibility by 8 (2) and 3 simultaneously, thus we need only show divisibility by 8 and by 3 to prove divisibility by 24.\n\nFirst, take any number (for this example it will be 376) and note the last digit in the number, discarding the other digits. Then take that digit (6) while ignoring the rest of the number and determine if it is divisible by 2. If it is divisible by 2, then the original number is divisible by 2.\n\nExample\n\nFirst, take any number (for this example it will be 492) and add together each digit in the number (4 + 9 + 2 = 15). Then take that sum (15) and determine if it is divisible by 3. The original number is divisible by 3 (or 9) if and only if the sum of its digits is divisible by 3 (or 9).\n\nIf a number is a multiplication of 3 consecutive numbers then that number is always divisible by 3. This is useful for when the number takes the form of (\"n\" × (\"n\" − 1) × (\"n\" + 1))\n\nEx.\n\nEx.\n\nThe basic rule for divisibility by 4 is that if the number formed by the last two digits in a number is divisible by 4, the original number is divisible by 4; this is because 100 is divisible by 4 and so adding hundreds, thousands, etc. is simply adding another number that is divisible by 4. If any number ends in a two digit number that you know is divisible by 4 (e.g. 24, 04, 08, etc.), then the whole number will be divisible by 4 regardless of what is before the last two digits.\n\nAlternatively, one can simply divide the number by 2, and then check the result to find if it is divisible by 2. If it is, the original number is divisible by 4. In addition, the result of this test is the same as the original number divided by 4.\n\nEx.<br>\nGeneral rule\n\nAlternative example\n\nDivisibility by 5 is easily determined by checking the last digit in the number (475), and seeing if it is either 0 or 5. If the last number is either 0 or 5, the entire number is divisible by 5.\n\nIf the last digit in the number is 0, then the result will be the remaining digits multiplied by 2. For example, the number 40 ends in a zero (0), so take the remaining digits (4) and multiply that by two (4 × 2 = 8). The result is the same as the result of 40 divided by 5(40/5 = 8).\n\nIf the last digit in the number is 5, then the result will be the remaining digits multiplied by two (2), plus one (1). For example, the number 125 ends in a 5, so take the remaining digits (12), multiply them by two (12 × 2 = 24), then add one (24 + 1 = 25). The result is the same as the result of 125 divided by 5 (125/5=25).\n\nEx.<br>\n\nIf the last digit is 5\n\nDivisibility by 6 is determined by checking the original number to see if it is both an even number (divisible by 2) and divisible by 3. This is the best test to use.\n\nIf the number is divisible by six, take the original number (246) and divide it by two (246 ÷ 2 = 123). Then, take that result and divide it by three (123 ÷ 3 = 41). This result is the same as the original number divided by six (246 ÷ 6 = 41).\n\nEx.\n\nExample: What is the remainder when 1036125837 is divided by 6?\n\nDivisibility by 7 can be tested by a recursive method. A number of the form 10\"x\" + \"y\" is divisible by 7 if and only if \"x\" − 2\"y\" is divisible by 7. In other words, subtract twice the last digit from the number formed by the remaining digits. Continue to do this until a number known to be divisible by 7 is obtained. The original number is divisible by 7 if and only if the number obtained using this procedure is divisible by 7. For example, the number 371: 37 − (2×1) = 37 − 2 = 35; 3 − (2 × 5) = 3 − 10 = −7; thus, since −7 is divisible by 7, 371 is divisible by 7.\n\nAnother method is multiplication by 3. A number of the form 10\"x\" + \"y\" has the same remainder when divided by 7 as 3\"x\" + \"y\". One must multiply the leftmost digit of the original number by 3, add the next digit, take the remainder when divided by 7, and continue from the beginning: multiply by 3, add the next digit, etc. For example, the number 371: 3×3 + 7 = 16 remainder 2, and 2×3 + 1 = 7. This method can be used to find the remainder of division by 7.\n\nA more complicated algorithm for testing divisibility by 7 uses the fact that 10 ≡ 1, 10 ≡ 3, 10 ≡ 2, 10 ≡ 6, 10 ≡ 4, 10 ≡ 5, 10 ≡ 1, ... (mod 7). Take each digit of the number (371) in reverse order (173), multiplying them successively by the digits 1, 3, 2, 6, 4, 5, repeating with this sequence of multipliers as long as necessary (1, 3, 2, 6, 4, 5, 1, 3, 2, 6, 4, 5, ...), and adding the products (1×1 + 7×3 + 3×2 = 1 + 21 + 6 = 28). The original number is divisible by 7 if and only if the number obtained using this procedure is divisible by 7 (hence 371 is divisible by 7 since 28 is).\n\nThis method can be simplified by removing the need to multiply. All it would take with this simplification is to memorize the sequence above (132645...), and to add and subtract, but always working with one-digit numbers.\n\nThe simplification goes as follows:\n\nIf through this procedure you obtain a 0 or any recognizable multiple of 7, then the original number is a multiple of 7. If you obtain any number from 1 to 6, that will indicate how much you should subtract from the original number to get a multiple of 7. In other words, you will find the remainder of dividing the number by 7. For example, take the number 186:\n\nNow we have a number lower than 7, and this number (4) is the remainder of dividing 186/7. So 186 minus 4, which is 182, must be a multiple of 7.\n\nNote: The reason why this works is that if we have: a+b=c and b is a multiple of any given number n, then a and c will necessarily produce the same remainder when divided by n. In other words, in 2 + 7 = 9, 7 is divisible by 7. So 2 and 9 must have the same reminder when divided by 7. The remainder is 2.\n\nTherefore, if a number \"n\" is a multiple of 7 (i.e.: the remainder of \"n\"/7 is 0), then adding (or subtracting) multiples of 7 cannot change that property.\n\nWhat this procedure does, as explained above for most divisibility rules, is simply subtract little by little multiples of 7 from the original number until reaching a number that is small enough for us to remember whether it is a multiple of 7. If 1 becomes a 3 in the following decimal position, that is just the same as converting 10×10 into a 3×10. And that is actually the same as subtracting 7×10 (clearly a multiple of 7) from 10×10.\n\nSimilarly, when you turn a 3 into a 2 in the following decimal position, you are turning 30×10 into 2×10, which is the same as subtracting 30×10−28×10, and this is again subtracting a multiple of 7. The same reason applies for all the remaining conversions:\n\nFirst method example<br>\n1050 → 105 − 0=105 → 10 − 10 = 0. ANSWER: 1050 is divisible by 7.\n\nSecond method example<br>\n1050 → 0501 (reverse) → 0×1 + 5×3 + 0×2 + 1×6 = 0 + 15 + 0 + 6 = 21 (multiply and add). ANSWER: 1050 is divisible by 7.\n\nVedic method of divisibility by osculation<br>\nDivisibility by seven can be tested by multiplication by the \"Ekhādika\". Convert the divisor seven to the nines family by multiplying by seven. 7×7=49. Add one, drop the units digit and, take the 5, the \"Ekhādika\", as the multiplier. Start on the right. Multiply by 5, add the product to the next digit to the left. Set down that result on a line below that digit. Repeat that method of multiplying the units digit by five and adding that product to the number of tens. Add the result to the next digit to the left. Write down that result below the digit. Continue to the end. If the end result is zero or a multiple of seven, then yes, the number is divisible by seven. Otherwise, it is not. This follows the Vedic ideal, one-line notation.\n\nVedic method example:\n\nPohlman–Mass method of divisibility by 7<br>\nThe Pohlman–Mass method provides a quick solution that can determine if most integers are divisible by seven in three steps or less. This method could be useful in a mathematics competition such as MATHCOUNTS, where time is a factor to determine the solution without a calculator in the Sprint Round.\n\nStep A:\nIf the integer is 1,000 or less, subtract twice the last digit from the number formed by the remaining digits. If the result is a multiple of seven, then so is the original number (and vice versa). For example:\n\nBecause 1,001 is divisible by seven, an interesting pattern develops for repeating sets of 1, 2, or 3 digits that form 6-digit numbers (leading zeros are allowed) in that all such numbers are divisible by seven. For example:\n\nFor all of the above examples, subtracting the first three digits from the last three results in a multiple of seven. Notice that leading zeros are permitted to form a 6-digit pattern.\n\nThis phenomenon forms the basis for Steps B and C.\n\nStep B:\nIf the integer is between 1,001 and one million, find a repeating pattern of 1, 2, or 3 digits that forms a 6-digit number that is close to the integer (leading zeros are allowed and can help you visualize the pattern). If the positive difference is less than 1,000, apply Step A. This can be done by subtracting the first three digits from the last three digits. For example:\n\nThe fact that 999,999 is a multiple of 7 can be used for determining divisibility of integers larger than one million by reducing the integer to a 6-digit number that can be determined using Step B. This can be done easily by adding the digits left of the first six to the last six and follow with Step A.\n\nStep C:\nIf the integer is larger than one million, subtract the nearest multiple of 999,999 and then apply Step B. For even larger numbers, use larger sets such as 12-digits (999,999,999,999) and so on. Then, break the integer into a smaller number that can be solved using Step B. For example:\n\nThis allows adding and subtracting alternating sets of three digits to determine divisibility by seven. Understanding these patterns allows you to quickly calculate divisibility of seven as seen in the following examples:\n\nPohlman–Mass method of divisibility by 7, examples:\n\nMultiplication by 3 method of divisibility by 7, examples:\n\nFinding remainder of a number when divided by 7\n\n7 − (1, 3, 2, −1, −3, −2, cycle repeats for the next six digits) Period: 6 digits.\nRecurring numbers: 1, 3, 2, −1, −3, −2\n<br>Minimum magnitude sequence <br>\n(1, 3, 2, 6, 4, 5, cycle repeats for the next six digits) Period: 6 digits.\nRecurring numbers: 1, 3, 2, 6, 4, 5\n<br>Positive sequence\n\nMultiply the right most digit by the left most digit in the sequence and multiply the second right most digit by the second left most digit in the sequence and so on and so for. Next, compute the sum of all the values and take the modulus of 7.\n<br>Example: What is the remainder when 1036125837 is divided by 7? <br>\n<br>Multiplication of the rightmost digit = 1 × 7 = 7 <br>\n<br>Multiplication of the second rightmost digit = 3 × 3 = 9 <br>\n<br>Third rightmost digit = 8 × 2 = 16 <br>\n<br>Fourth rightmost digit = 5 × −1 = −5 <br>\n<br>Fifth rightmost digit = 2 × −3 = −6 <br>\n<br>Sixth rightmost digit = 1 × −2 = −2 <br>\n<br>Seventh rightmost digit = 6 × 1 = 6 <br>\n<br>Eighth rightmost digit = 3 × 3 = 9 <br>\n<br>Ninth rightmost digit = 0 <br>\n<br>Tenth rightmost digit = 1 × −1 = −1 <br>\n<br>Sum = 33 <br>\n<br>33 modulus 7 = 5 <br>\n<br>Remainder = 5\n\nDigit pair method of divisibility by 7\n\nThis method uses 1, −3, 2 pattern on the \"digit pairs\". That is, the divisibility of any number by seven can be tested by first separating the number into digit pairs, and then applying the algorithm on three digit pairs (six digits). When the number is smaller than six digits, then fill zero’s to the right side until there are six digits. When the number is larger than six digits, then repeat the cycle on the next six digit group and then add the results. Repeat the algorithm until the result is a small number. The original number is divisible by seven if and only if the number obtained using this algorithm is divisible by seven. This method is especially suitable for large numbers.\n\n\"Example 1:\"<br>\nThe number to be tested is 157514.\nFirst we separate the number into three digit pairs: 15, 75 and 14.<br>\nThen we apply the algorithm: 1 × 15 − 3 × 75 + 2 × 14 = 182<br>\nBecause the resulting 182 is less than six digits, we add zero’s to the right side until it is six digits.<br>\nThen we apply our algorithm again: 1 × 18 − 3 × 20 + 2 × 0 = −42<br>\nThe result −42 is divisible by seven, thus the original number 157514 is divisible by seven.\n\n\"Example 2:\"<br>\nThe number to be tested is 15751537186.<br>\n(1 × 15 − 3 × 75 + 2 × 15) + (1 × 37 − 3 × 18 + 2 × 60) = −180 + 103 = −77<br>\nThe result −77 is divisible by seven, thus the original number 15751537186 is divisible by seven.\n\nRemainder Test\n13 (1, −3, −4, −1, 3, 4, cycle goes on.)\nIf you are not comfortable with negative numbers, then use this sequence. (1, 10, 9, 12, 3, 4)\nMultiply the right most digit of the number with the left most number in the sequence shown above and the second right most digit to the second left most digit of the number in the sequence. The cycle goes on.\n\nExample: What is the remainder when 321 is divided by 13?\nUsing the first sequence, <br>\nAns: 1 × 1 + 2 × −3 + 3 × −4 = −17\nRemainder = −17 mod 13 = 9\n\nExample: What is the remainder when 1234567 is divided by 13?\nUsing the second sequence, <br>\nAnswer: 7 × 1 + 6 × 10 + 5 × 9 + 4 × 12 + 3 × 3 + 2 × 4 + 1 × 1 = 178 mod 13 = 9\nRemainder = 9\n\nDivisibility properties can be determined in two ways, depending on the type of the divisor.\n\nA number is divisible by a given divisor if it is divisible by the highest power of each of its prime factors. For example, to determine divisibility by 36, check divisibility by 4 and by 9. Note that checking 3 and 12, or 2 and 18, would not be sufficient. A table of prime factors may be useful.\n\nA composite divisor may also have a rule formed using the same procedure as for a prime divisor, given below, with the caveat that the manipulations involved may not introduce any factor which is present in the divisor. For instance, one cannot make a rule for 14 that involves multiplying the equation by 7. This is not an issue for prime divisors because they have no smaller factors.\n\nThe goal is to find an inverse to 10 modulo the prime under consideration (does not work for 2 or 5) and use that as a multiplier to make the divisibility of the original number by that prime depend on the divisibility of the new (usually smaller) number by the same prime.\nUsing 31 as an example, since 10 × (−3) = −30 = 1 mod 31, we get the rule for using \"y\" − 3\"x\" in the table above. Likewise, since 10 × (28) = 280 = 1 mod 31 also, we obtain a complementary rule \"y\" + 28\"x\" of the same kind - our choice of addition or subtraction being dictated by arithmetic convenience of the smaller value. In fact, this rule for prime divisors besides 2 and 5 is \"really\" a rule for divisibility by any integer relatively prime to 10 (including 33 and 39; see the table below). This is why the last divisibility condition in the tables above and below for any number relatively prime to 10 has the same kind of form (add or subtract some multiple of the last digit from the rest of the number).\n\nThe following table provides rules for some more notable divisors:\n\nTo test for divisibility by \"D\", where \"D\" ends in 1, 3, 7, or 9, the following method can be used. Find any multiple of \"D\" ending in 9. (If \"D\" ends respectively in 1, 3, 7, or 9, then multiply by 9, 3, 7, or 1.) Then add 1 and divide by 10, denoting the result as \"m\". Then a number \"N\" = 10\"t\" + \"q\" is divisible by \"D\" if and only if \"mq + t\" is divisible by \"D\".\n\nFor example, to determine if 913 = 10×91 + 3 is divisible by 11, find that \"m\" = (11×9+1)÷10 = 10. Then \"mq+t\" = 10×3+91 = 121; this is divisible by 11 (with quotient 11), so 913 is also divisible by 11. As another example, to determine if 689 = 10×68 + 9 is divisible by 53, find that \"m\" = (53×3+1)÷10 = 16. Then \"mq+t\" = 16×9 + 68 = 212, which is divisible by 53 (with quotient 4); so 689 is also divisible by 53.\n\nMany of the simpler rules can be produced using only algebraic manipulation, creating binomials and rearranging them. By writing a number as the sum of each digit times a power of 10 each digit's power can be manipulated individually.\n\nCase where all digits are summed\n\nThis method works for divisors that are factors of 10 − 1 = 9.\n\nUsing 3 as an example, 3 divides 9 = 10 − 1. That means formula_2 (see modular arithmetic). The same for all the higher powers of 10: formula_3 They are all congruent to 1 modulo 3. Since two things that are congruent modulo 3 are either both divisible by 3 or both not, we can interchange values that are congruent modulo 3. So, in a number such as the following, we can replace all the powers of 10 by 1:\n\nwhich is exactly the sum of the digits.\n\nCase where the alternating sum of digits is used\n\nThis method works for divisors that are factors of 10 + 1 = 11.\n\nUsing 11 as an example, 11 divides 11 = 10 + 1. That means formula_5. For the higher powers of 10, they are congruent to 1 for even powers and congruent to −1 for odd powers:\n\nLike the previous case, we can substitute powers of 10 with congruent values:\n\nwhich is also the difference between the sum of digits at odd positions and the sum of digits at even positions.\n\nCase where only the last digit(s) matter\n\nThis applies to divisors that are a factor of a power of 10. This is because sufficiently high powers of the base are multiples of the divisor, and can be eliminated.\n\nFor example, in base 10, the factors of 10 include 2, 5, and 10. Therefore, divisibility by 2, 5, and 10 only depend on whether the last 1 digit is divisible by those divisors. The factors of 10 include 4 and 25, and divisibility by those only depend on the last 2 digits.\n\nCase where only the last digit(s) are removed\n\nMost numbers do not divide 9 or 10 evenly, but do divide a higher power of 10 or 10 − 1. In this case the number is still written in powers of 10, but not fully expanded.\n\nFor example, 7 does not divide 9 or 10, but does divide 98, which is close to 100. Thus, proceed from\n\nwhere in this case a is any integer, and b can range from 0 to 99. Next,\n\nand again expanding\n\nand after eliminating the known multiple of 7, the result is\n\nwhich is the rule \"double the number formed by all but the last two digits, then add the last two digits\".\n\nCase where the last digit(s) is multiplied by a factor\n\nThe representation of the number may also be multiplied by any number relatively prime to the divisor without changing its divisibility. After observing that 7 divides 21, we can perform the following:\n\nafter multiplying by 2, this becomes\n\nand then\n\nEliminating the 21 gives\n\nand multiplying by −1 gives\n\nEither of the last two rules may be used, depending on which is easier to perform. They correspond to the rule \"subtract twice the last digit from the rest\".\n\nThis section will illustrate the basic method; all the rules can be derived following the same procedure. The following requires a basic grounding in modular arithmetic; for divisibility other than by 2's and 5's the proofs rest on the basic fact that 10 mod \"m\" is invertible if 10 and \"m\" are relatively prime.\n\nFor 2 or 5:\n\nOnly the last \"n\" digits need to be checked.\n\nRepresenting \"x\" as formula_18\n\nand the divisibility of \"x\" is the same as that of \"z\".\n\nFor 7:\n\nSince 10 × 5  ≡  10 × (−2)  ≡ 1 (mod 7) we can do the following:\n\nRepresenting \"x\" as formula_20\n\nso \"x\" is divisible by 7 if and only if \"y\" − 2\"z\" is divisible by 7.\n\nParity (mathematics)\n\n\n"}
{"id": "5297278", "url": "https://en.wikipedia.org/wiki?curid=5297278", "title": "Dowling geometry", "text": "Dowling geometry\n\nIn combinatorial mathematics, a Dowling geometry, named after Thomas A. Dowling, is a matroid associated with a group. There is a Dowling geometry of each rank for each group. If the rank is at least 3, the Dowling geometry uniquely determines the group. Dowling geometries have a role in matroid theory as universal objects (Kahn and Kung, 1982); in that respect they are analogous to projective geometries, but based on groups instead of fields.\n\nA Dowling lattice is the geometric lattice of flats associated with a Dowling geometry. The lattice and the geometry are mathematically equivalent: knowing either one determines the other. Dowling lattices, and by implication Dowling geometries, were introduced by Dowling (1973a,b).\n\nA Dowling lattice or geometry of rank \"n\" of a group \"G\" is often denoted \"Q\"(\"G\").\n\nIn his first paper (1973a) Dowling defined the rank-\"n\" Dowling lattice of the multiplicative group of a finite field \"F\". It is the set of all those subspaces of the vector space \"F\" that are generated by subsets of the set \"E\" that consists of vectors with at most two nonzero coordinates. The corresponding Dowling geometry is the set of 1-dimensional vector subspaces generated by the elements of \"E\".\n\nIn his second paper (1973b) Dowling gave an intrinsic definition of the rank-\"n\" Dowling lattice of any finite group \"G\". Let \"S\" be the set {1...,\"n\"}. A \"G\"-labelled set (\"T\", α) is a set \"T\" together with a function α: \"T\" → \"G\". Two \"G\"-labelled sets, (\"T\", α) and (\"T\", β), are equivalent if there is a group element, \"g\", such that β = \"g\"α. \nAn equivalence class is denoted [\"T\", α].\nA partial \"G\"-partition of \"S\" is a set γ = {[\"B\",α], ..., [\"B\",α]} of equivalence classes of \"G\"-labelled sets such that \"B\", ..., \"B\" are nonempty subsets of \"S\" that are pairwise disjoint. (\"k\" may equal 0.) \nA partial \"G\"-partition γ is said to be ≤ another one, γ*, if \n\nThis gives a partial ordering of the set of all partial \"G\"-partitions of \"S\". The resulting partially ordered set is the Dowling lattice \"Q\"(\"G\").\n\nThe definitions are valid even if \"F\" or \"G\" is infinite, though Dowling mentioned only finite fields and groups.\n\nA graphical definition was then given by Doubilet, Rota, and Stanley (1972). We give the slightly simpler (but essentially equivalent) graphical definition of Zaslavsky (1991), expressed in terms of gain graphs.\n\nTake \"n\" vertices, and between each pair of vertices, \"v\" and \"w\", take a set of |\"G\"| parallel edges labelled by each of the elements of the group \"G\". The edges are oriented, in that, if the label in the direction from \"v\" to \"w\" is the group element \"g\", then the label of the same edge in the opposite direction, from \"w\" to \"v\", is \"g\". The label of an edge therefore depends on the direction of the edge; such labels are called gains. Also add to each vertex a loop whose gain is any value other than 1. (1 is the group identity element.) This gives a graph which is called \"GK\" (note the raised circle).\n\nA cycle in the graph then has a gain. The cycle is a sequence of edges, \"e\"\"e\"···\"e\". Suppose the gains of these edges, in a fixed direction around the cycle, are \"g\", \"g\", ..., \"g\". Then the gain of the cycle is the product, \"g\"\"g\"···\"g\". The value of this gain is not completely well defined, since it depends on the direction chosen for the cycle and on which is called the \"first\" edge of the cycle. What is independent of these choices is the answer to the following question: is the gain equal to 1 or not? If it equals 1 under one set of choices, then it is also equal to 1 under all sets of choices.\n\nTo define the Dowling geometry, we specify the circuits (minimal dependent sets). The circuits of the matroid are \n\nThus, the Dowling geometry \"Q\"(\"G\") is the frame matroid or (bias matroid) of the gain graph \"GK\" (the raised circle denotes the presence of loops). \nOther, equivalent definitions are described in the article on gain graphs.\n\nOne reason for interest in Dowling lattices is that the characteristic polynomial is very simple. If \"L\" is the Dowling lattice of rank \"n\" of a finite group \"G\" having \"m\" elements, then\n\nan exceptionally simple formula for any geometric lattice.\n\nThere is also a Dowling geometry, of rank 3 only, associated with each quasigroup; see Dowling (1973b). This does not generalize in a straightforward way to higher ranks. There is a generalization due to Zaslavsky (2012) that involves \"n\"-ary quasigroups.\n\n"}
{"id": "31631871", "url": "https://en.wikipedia.org/wiki?curid=31631871", "title": "Dynamic logic (modal logic)", "text": "Dynamic logic (modal logic)\n\nDynamic logic is an extension of modal logic originally intended for reasoning about computer programs and later applied to more general complex behaviors arising in linguistics, philosophy, AI, and other fields.\n\nModal logic is characterized by the modal operators formula_1 (box p) asserting that formula_2 is necessarily the case, and formula_3 (diamond p) asserting that formula_2 is possibly the case. Dynamic logic extends this by associating to every action formula_5 the modal operators formula_6 and formula_7, thereby making it a multimodal logic. The meaning of formula_8 is that after performing action formula_5 it is necessarily the case that formula_2 holds, that is, formula_5 must bring about formula_2. The meaning of formula_13 is that after performing formula_5 it is possible that formula_2 holds, that is, formula_5 might bring about formula_2. These operators are related by formula_18 and formula_19, analogously to the relationship between the universal (formula_20) and existential (formula_21) quantifiers.\n\nDynamic logic permits compound actions built up from smaller actions. While the basic control operators of any programming language could be used for this purpose, Kleene's regular expression operators are a good match to modal logic. Given actions formula_5 and formula_23, the compound action formula_24, \"choice\", also written formula_25 or formula_26, is performed by performing one of formula_5 or formula_23. The compound action formula_29, \"sequence\", is performed by performing first formula_5 and then formula_23. The compound action formula_32, \"iteration\", is performed by performing formula_5 zero or more times, sequentially. The constant action formula_34 or BLOCK does nothing and does not terminate, whereas the constant action formula_35 or SKIP or NOP, definable as formula_36, does nothing but does terminate.\n\nThese operators can be axiomatized in dynamic logic as follows, taking as already given a suitable axiomatization of modal logic including such axioms for modal operators as the above-mentioned axiom formula_18 and the two inference rules \"modus ponens\" (formula_38 and formula_39 implies formula_40) and \"necessitation\" (formula_38 implies formula_42).\n\nA1. formula_43\n\nA2. formula_44\n\nA3. formula_45\n\nA4. formula_46\n\nA5. formula_47\n\nA6. formula_48\n\nAxiom A1 makes the empty promise that when BLOCK terminates, formula_2 will hold, even if formula_2 is the proposition false. (Thus BLOCK abstracts the essence of the action of hell freezing over.)\nA2 says that NOP acts as the identity function on propositions, that is, it transforms formula_2 into itself.\nA3 says that if doing one of formula_5 or formula_23 must bring about formula_2, then formula_5 must bring about formula_2 and likewise for formula_23, and conversely.\nA4 says that if doing formula_5 and then formula_23 must bring about formula_2, then formula_5 must bring about a situation in which formula_23 must bring about formula_2.\nA5 is the evident result of applying A2, A3 and A4 to the equation formula_64 of Kleene algebra.\nA6 asserts that if formula_2 holds now, and no matter how often we perform formula_5 it remains the case that the truth of formula_2 after that performance entails its truth after one more performance of formula_5, then formula_2 must remain true no matter how often we perform formula_5. A6 is recognizable as mathematical induction with the action n := n+1 of incrementing n generalized to arbitrary actions formula_5.\n\nThe modal logic axiom formula_72 permits the derivation of the following six theorems corresponding to the above:\n\nT1. formula_73\n\nT2. formula_74\n\nT3. formula_75\n\nT4. formula_76\n\nT5. formula_77\n\nT6. formula_78\n\nT1 asserts the impossibility of bringing anything about by performing BLOCK.\nT2 notes again that NOP changes nothing, bearing in mind that NOP is both deterministic and terminating whence formula_79 and formula_80 have the same force.\nT3 says that if the choice of formula_5 or formula_23 could bring about formula_2, then either formula_5 or formula_23 alone could bring about formula_2.\nT4 is just like A4.\nT5 is explained as for A5.\nT6 asserts that if it is possible to bring about formula_2 by performing formula_5 sufficiently often, then either formula_2 is true now or it is possible to perform formula_5 repeatedly to bring about a situation where formula_2 is (still) false but one more performance of formula_5 could bring about formula_2.\n\nBox and diamond are entirely symmetric with regard to which one takes as primitive. An alternative axiomatization would have been to take the theorems T1-T6 as axioms, from which we could then have derived A1-A6 as theorems.\n\nThe difference between implication and inference is the same in dynamic logic as in any other logic: whereas the implication formula_94 asserts that if formula_2 is true then so is formula_96, the inference formula_97 asserts that if formula_2 is valid then so is formula_96. However the dynamic nature of dynamic logic moves this distinction out of the realm of abstract axiomatics into the common-sense experience of situations in flux. The inference rule formula_100, for example, is sound because its premise asserts that formula_2 holds at all times, whence no matter where formula_5 might take us, formula_2 will be true there. The implication formula_104 is not valid, however, because the truth of formula_2 at the present moment is no guarantee of its truth after performing formula_5. For example, formula_104 will be true in any situation where formula_2 is false, or in any situation where formula_8 is true, but the assertion formula_110 is false in any situation where formula_111 has value 1, and therefore is not valid.\n\nAs for modal logic, the inference rules \"modus ponens\" and \"necessitation\" suffice also for dynamic logic as the only primitive rules it needs, as noted above. However, as usual in logic, many more rules can be derived from these with the help of the axioms. An example instance of such a derived rule in dynamic logic is that if kicking a broken TV once can't possibly fix it, then repeatedly kicking it can't possibly fix it either. Writing formula_112 for the action of kicking the TV, and formula_23 for the proposition that the TV is broken, dynamic logic expresses this inference as formula_114, having as premise formula_115 and as conclusion formula_116. The meaning of formula_117 is that it is guaranteed that after kicking the TV, it is broken. Hence the premise formula_115 means that if the TV is broken, then after kicking it once it will still be broken. formula_119 denotes the action of kicking the TV zero or more times. Hence the conclusion formula_116 means that if the TV is broken, then after kicking it zero or more times it will still be broken. For if not, then after the second-to-last kick the TV would be in a state where kicking it once more would fix it, which the premise claims can never happen under any circumstances.\n\nThe inference formula_114 is sound. However the implication formula_122 is not valid because we can easily find situations in which formula_115 holds but formula_116 does not. In any such counterexample situation, formula_23 must hold but formula_126 must be false, while formula_117 however must be true. But this could happen in any situation where the TV is broken but can be revived with two kicks. The implication fails (is not valid) because it only requires that formula_115 hold now, whereas the inference succeeds (is sound) because it requires that formula_115 hold in all situations, not just the present one.\n\nAn example of a valid implication is the proposition formula_130. This says that if formula_111 is greater or equal to 3, then after incrementing formula_111, formula_111 must be greater or equal to 4. In the case of deterministic actions formula_5 that are guaranteed to terminate, such as formula_135, \"must\" and \"might\" have the same force, that is, formula_6 and formula_7 have the same meaning. Hence the above proposition is equivalent to formula_138 asserting that if formula_111 is greater or equal to 3 then after performing formula_135, formula_111 might be greater or equal to 4.\n\nThe general form of an assignment statement is formula_142 where formula_111 is a variable and formula_144 is an expression built from constants and variables with whatever operations are provided by the language, such as addition and multiplication. The Hoare axiom for assignment is not given as a single axiom but rather as an axiom schema.\n\nA7. formula_145\n\nThis is a schema in the sense that formula_146 can be instantiated with any formula formula_147 containing zero or more instances of a variable formula_111. The meaning of formula_149 is formula_147 with those occurrences of formula_111 that occur free in formula_147, i.e. not bound by some quantifier as in formula_153, replaced by formula_144. For example, we may instantiate A7 with formula_155, or with formula_156. Such an axiom schema allows infinitely many axioms having a common form to be written as a finite expression connoting that form.\n\nThe instance formula_157 of A7 allows us to calculate mechanically that the example formula_158 encountered a few paragraphs ago is equivalent to formula_159, which in turn is equivalent to formula_160 by elementary algebra.\n\nAn example illustrating assignment in combination with formula_161 is the proposition formula_162. This asserts that it is possible, by incrementing formula_111 sufficiently often, to make formula_111 equal to 7. This of course is not always true, e.g. if formula_111 is 8 to begin with, or 6.5, whence this proposition is not a theorem of dynamic logic. If formula_111 is of type integer however, then this proposition is true if and only if formula_111 is at most 7 to begin with, that is, it is just a roundabout way of saying formula_168.\n\nMathematical induction can be obtained as the instance of A6 in which the proposition formula_2 is instantiated as formula_170, the action formula_5 as formula_172, and formula_173 as formula_34. The first two of these three instantiations are straightforward, converting A6 to formula_175. However, the ostensibly simple substitution of formula_34 for formula_173 is not so simple as it brings out the so-called \"referential opacity\" of modal logic in the case when a modality can interfere with a substitution.\n\nWhen we substituted formula_170 for formula_2, we were thinking of the proposition symbol formula_2 as a rigid designator with respect to the modality formula_181, meaning that it is the same proposition after incrementing formula_173 as before, even though incrementing formula_173 may impact its truth. Likewise, the action formula_5 is still the same action after incrementing formula_173, even though incrementing formula_173 will result in its executing in a different environment. However, formula_173 itself is not a rigid designator with respect to the modality formula_181; if it denotes 3 before incrementing formula_173, it denotes 4 after. So we can't just substitute formula_34 for formula_173 everywhere in A6.\n\nOne way of dealing with the opacity of modalities is to eliminate them. To this end, expand formula_192 as the infinite conjunction formula_193, that is, the conjunction over all formula_194 of formula_195. Now apply A4 to turn formula_195 into formula_197, having formula_194 modalities. Then apply Hoare's axiom formula_194 times to this to produce formula_200, then simplify this infinite conjunction to formula_201. This whole reduction should be applied to both instances of formula_202 in A6, yielding formula_203. The remaining modality can now be eliminated with one more use of Hoare's axiom to give formula_204.\n\nWith the opaque modalities now out of the way, we can safely substitute formula_34 for formula_173 in the usual manner of first-order logic to obtain Peano's celebrated axiom formula_207, namely mathematical induction.\n\nOne subtlety we glossed over here is that formula_208 should be understood as ranging over the natural numbers, where formula_194 is the superscript in the expansion of formula_32 as the union of formula_211 over all natural numbers formula_194. The importance of keeping this typing information straight becomes apparent if formula_173 had been of type \"integer\", or even \"real\", for any of which A6 is perfectly valid as an axiom. As a case in point, if formula_173 is a real variable and formula_170 is the predicate formula_173 \"is a natural number\", then axiom A6 after the first two substitutions, that is, formula_217, is just as valid, that is, true in every state regardless of the value of formula_173 in that state, as when formula_173 is of type \"natural number\". If in a given state formula_173 is a natural number, then the antecedent of the main implication of A6 holds, but then formula_221 is also a natural number so the consequent also holds. If formula_173 is not a natural number, then the antecedent is false and so A6 remains true regardless of the truth of the consequent. We could strengthen A6 to an equivalence formula_223 without impacting any of this, the other direction being provable from A5, from which we see that if the antecedent of A6 does happen to be false somewhere, then the consequent \"must\" be false.\n\nDynamic logic associates to every proposition formula_2 an action formula_225 called a test. When formula_2 holds, the test formula_225 acts as a NOP, changing nothing while allowing the action to move on. When formula_2 is false, formula_225 acts as BLOCK. Tests can be axiomatized as follows.\n\nA8. formula_230\n\nThe corresponding theorem for formula_231 is:\n\nT8. formula_232\n\nThe construct if p then a else b is realized in dynamic logic as formula_233. This action expresses a guarded choice: if formula_2 holds then formula_235 is equivalent to formula_5, whereas formula_237 is equivalent to BLOCK, and formula_238 is equivalent to formula_5. Hence when formula_2 is true the performer of the action can only take the left branch, and when formula_2 is false the right.\n\nThe construct while p do a is realized as formula_242. This performs formula_235 zero or more times and then performs formula_244. As long as formula_2 remains true, the formula_244 at the end blocks the performer from terminating the iteration prematurely, but as soon as it becomes false, further iterations of the body formula_2 are blocked and the performer then has no choice but to exit via the test formula_244.\n\nThe random-assignment statement formula_249 denotes the nondeterministic action of setting formula_111 to an arbitrary value. formula_251 then says that formula_2 holds no matter what you set formula_111 to, while formula_254 says that it is possible to set formula_111 to a value that makes formula_2 true. formula_257 thus has the same meaning as the universal quantifier formula_153, while formula_259 similarly corresponds to the existential quantifier formula_260. That is, first-order logic can be understood as the dynamic logic of programs of the form formula_249.\n\nDijkstra claimed to show the impossibility of a program that sets the value of variable x to an arbitrary positive integer. However, in dynamic logic with assignment and the * operator, x can be set to an arbitrary positive integer with the dynamic logic program formula_262: hence we must either reject Dijkstra's argument or hold that the * operator is not effective.\n\nModal logic is most commonly interpreted in terms of possible world semantics or Kripke structures. This semantics carries over naturally to dynamic logic by interpreting worlds as states of a computer in the application to program verification, or states of our environment in applications to linguistics, AI, etc. One role for possible world semantics is to formalize the intuitive notions of truth and validity, which in turn permit the notions of soundness and completeness to be defined for axiom systems. An inference rule is sound when validity of its premises implies validity of its conclusion. An axiom system is sound when all its axioms are valid and its inference rules are sound. An axiom system is complete when every valid formula is derivable as a theorem of that system. These concepts apply to all systems of logic including dynamic logic.\n\nOrdinary or first-order logic has two types of terms, respectively assertions and data. As can be seen from the examples above, dynamic logic adds a third type of term denoting actions. The dynamic logic assertion formula_263 contains all three types: formula_111, formula_265, and formula_266 are data, formula_267 is an action, and formula_268 and formula_263 are assertions. Propositional logic is derived from first-order logic by omitting data terms and reasons only about abstract propositions, which may be simple propositional variables or atoms or compound propositions built with such logical connectives as \"and\", \"or\", and \"not\".\n\nPropositional dynamic logic, or PDL, was derived from dynamic logic in 1977 by Michael J. Fischer and Richard Ladner. PDL blends the ideas behind propositional logic and dynamic logic by adding actions while omitting data; hence the terms of PDL are actions and propositions. The TV example above is expressed in PDL whereas the next example involving formula_267 is in first-order DL. PDL is to (first-order) dynamic logic as propositional logic is to first-order logic.\n\nFischer and Ladner showed in their 1977 paper that PDL satisfiability was of computational complexity at most nondeterministic exponential time, and at least deterministic exponential time in the worst case. This gap was closed in 1978 by Vaughan Pratt who showed that PDL was decidable in deterministic exponential time. In 1977, Krister Segerberg proposed a complete axiomatization of PDL, namely any complete axiomatization of modal logic K together with axioms A1-A6 as given above. Completeness proofs for Segerberg's axioms were found by Gabbay (unpublished note), Parikh (1978), Pratt (1979), and Kozen and Parikh (1981).\n\nDynamic logic was developed by Vaughan Pratt in 1974 in notes for a class on program verification as an approach to assigning meaning to Hoare logic by expressing the Hoare formula formula_271 as formula_272. The approach was later published in 1976 as a logical system in its own right. The system parallels A. Salwicki's system of algorithmic logic and Edsger Dijkstra's notion of weakest-precondition predicate transformer formula_273, with formula_8 corresponding to Dijkstra's formula_275, weakest liberal precondition. Those logics however made no connection with either modal logic, Kripke semantics, regular expressions, or the calculus of binary relations; dynamic logic therefore can be viewed as a refinement of algorithmic logic and predicate transformers that connects them up to the axiomatics and Kripke semantics of modal logic as well as to the calculi of binary relations and regular expressions.\n\nHoare logic, algorithmic logic, weakest preconditions, and dynamic logic are all well suited to discourse and reasoning about sequential behavior. Extending these logics to concurrent behavior however has proved problematic. There are various approaches but all of them lack the elegance of the sequential case. In contrast Amir Pnueli's 1977 system of temporal logic, another variant of modal logic sharing many common features with dynamic logic, differs from all of the above-mentioned logics by being what Pnueli has characterized as an \"endogenous\" logic, the others being \"exogenous\" logics. By this Pnueli meant that temporal logic assertions are interpreted within a universal behavioral framework in which a single global situation changes with the passage of time, whereas the assertions of the other logics are made externally to the multiple actions about which they speak. The advantage of the endogenous approach is that it makes no fundamental assumptions about what causes what as the environment changes with time. Instead a temporal logic formula can talk about two unrelated parts of a system, which because they are unrelated tacitly evolve in parallel. In effect ordinary logical conjunction of temporal assertions is the concurrent composition operator of temporal logic. The simplicity of this approach to concurrency has resulted in temporal logic being the modal logic of choice for reasoning about concurrent systems with its aspects of synchronization, interference, independence, deadlock, livelock, fairness, etc.\n\nThese concerns of concurrency would appear to be less central to linguistics, philosophy, and artificial intelligence, the areas in which dynamic logic is most often encountered nowadays.\n\nFor a comprehensive treatment of dynamic logic see the book by David Harel et al. cited below.\n\n\n"}
{"id": "25548455", "url": "https://en.wikipedia.org/wiki?curid=25548455", "title": "Everything is a file", "text": "Everything is a file\n\n\"Everything is a file\" describes one of the defining features of Unix, and its derivatives — that a wide range of input/output resources such as documents, directories, hard-drives, modems, keyboards, printers and even some inter-process and network communications are simple streams of bytes exposed through the filesystem name space.\n\nThe advantage of this approach is that the same set of tools, utilities and APIs can be used on a wide range of resources. There are a number of file types. When a file is opened, a file descriptor is created. The file path becoming the addressing system and the file descriptor being the byte stream I/O interface. But file descriptors are also created for things like anonymous pipes and network sockets via different methods. So it is more accurate to say \"Everything is a file descriptor\".\n\nAdditionally, a range of pseudo and virtual filesystems exists which exposes information about processes and other system information in a hierarchical file-like structure. These are mounted into the single file hierarchy.\n\nAn example of this purely virtual filesystem is under /proc that exposes many system properties as files.\n\nAll of these \"files\" have standard Unix file attributes such as an owner and access permissions, and can be queried by the same classic Unix tools and filters. However, this is not universally considered a fast or portable approach. Some operating systems do not even mount /proc by default due to security or speed concerns. It is, though, used heavily by both the widely installed BusyBox on embedded systems and by procps, which is used on most Linux systems. In both cases it is used in implementations of process-related POSIX shell commands. It is similarly used on Android systems in the operating system's Toolbox program.\n\nUnix's successor Plan 9 took this concept into distributed computing with the 9P protocol.\n\n"}
{"id": "245552", "url": "https://en.wikipedia.org/wiki?curid=245552", "title": "Gaussian function", "text": "Gaussian function\n\nIn mathematics, a Gaussian function, often simply referred to as a Gaussian, is a function of the form:\n\nfor arbitrary real constants , and non zero . It is named after the mathematician Carl Friedrich Gauss. The graph of a Gaussian is a characteristic symmetric \"bell curve\" shape. The parameter is the height of the curve's peak, is the position of the center of the peak and (the standard deviation, sometimes called the Gaussian RMS width) controls the width of the \"bell\".\n\nGaussian functions are often used to represent the probability density function of a normally distributed random variable with expected value and variance . In this case, the Gaussian is of the form:\n\nGaussian functions are widely used in statistics to describe the normal distributions, in signal processing to define Gaussian filters, in image processing where two-dimensional Gaussians are used for Gaussian blurs, and in mathematics to solve heat equations and diffusion equations and to define the Weierstrass transform.\n\nGaussian functions arise by composing the exponential function with a concave quadratic function. The Gaussian functions are thus those functions whose logarithm is a concave quadratic function.\n\nThe parameter is related to the full width at half maximum (FWHM) of the peak according to\n\nThe function may then be expressed in terms of the FWHM, represented by :\n\nAlternatively, the parameter can be interpreted by saying that the two inflection points of the function occur at and .\n\nThe \"full width at tenth of maximum\" (FWTM) for a Gaussian could be of interest and is \n\nGaussian functions are analytic, and their limit as is 0 (for the above case of ).\n\nGaussian functions are among those functions that are elementary but lack elementary antiderivatives; the integral of the Gaussian function is the error function. Nonetheless their improper integrals over the whole real line can be evaluated exactly, using the Gaussian integral\n\nand one obtains\n\nThis integral is 1 if and only if formula_8, and in this case the Gaussian is the probability density function of a normally distributed random variable with expected value and variance :\n\nThese Gaussians are plotted in the accompanying figure.\nGaussian functions centered at zero minimize the Fourier uncertainty principle.\n\nThe product of two Gaussian functions is a Gaussian, and the convolution of two Gaussian functions is also a Gaussian, with variance being the sum of the original variances: formula_10. The product of two Gaussian probability density functions, though, is not in general a Gaussian PDF.\n\nTaking the Fourier transform (unitary, angular frequency convention) of a Gaussian function with parameters , and yields another Gaussian function, with parameters formula_11, and formula_12. So in particular the Gaussian functions with and formula_13 are kept fixed by the Fourier transform (they are eigenfunctions of the Fourier transform with eigenvalue 1).\n\nA physical realization is that of the diffraction pattern: for example, a photographic slide whose transmittance has a Gaussian variation is also a Gaussian function.\nThe fact that the Gaussian function is an eigenfunction of the continuous Fourier transform\nallows us to derive the following interesting identity from the Poisson summation formula:\n\nThe integral of an arbitrary Gaussian function is\n\nAn alternative form is\n\nwhere \"f\" must be strictly positive for the integral to converge.\n\nThe integral\n\nfor some real constants a, b, c > 0 can be calculated by putting it into the form of a Gaussian integral. First, the constant \"a\" can simply be factored out of the integral. Next, the variable of integration is changed from \"x\" to \"y\" = \"x\" - \"b\".\n\nand then to formula_19\n\nThen, using the Gaussian integral identity\n\nwe have\n\nIn two dimensions, the power to which \"e\" is raised in the Gaussian function is any negative-definite quadratic form. Consequently, the level sets of the Gaussian will always be ellipses.\n\nA particular example of a two-dimensional Gaussian function is\n\nHere the coefficient \"A\" is the amplitude, \"x\",y is the center and σ, σ are the \"x\" and \"y\" spreads of the blob. The figure on the right was created using \"A\" = 1, \"x\" = 0, \"y\" = 0, σ = σ = 1.\n\nThe volume under the Gaussian function is given by\n\nIn general, a two-dimensional elliptical Gaussian function is expressed as\n\nwhere the matrix\n\nis positive-definite.\n\nUsing this formulation, the figure on the right can be created using \"A\" = 1, (\"x\", \"y\") = (0, 0), \"a\" = \"c\" = 1/2, \"b\" = 0.\n\nFor the general form of the equation the coefficient \"A\" is the height of the peak and (\"x\", \"y\") is the center of the blob.\n\nIf we set\n\nthen we rotate the blob by a clockwise angle formula_30 (for counterclockwise rotation invert the signs in the b coefficient). This can be seen in the following examples:\n\nUsing the following Octave code, one can easily see the effect of changing the parameters\n\nSuch functions are often used in image processing and in computational models of visual system function—see the articles on scale space and affine shn.\n\nAlso see multivariate normal distribution.\n\nA more general formulation of a Gaussian function with a flat-top and Gaussian fall-off can be taken by raising the content of the exponent to a power, formula_31:\n\nformula_32\nThis function is known as a super-Gaussian function and is often used for Gaussian beam formulation. In a two-dimensional formulation, a Gaussian function along formula_33 and formula_34 can be combined with potentially different formula_35 and formula_36 to form an elliptical Gaussian distribution,formula_37 or a rectangular Gaussian distribution, formula_38.\n\nIn an formula_39-dimensional space a Gaussian function can be defined as\nwhere formula_41 is a column of formula_39 coordinates, formula_43 is a positive-definite formula_44 matrix, and formula_45 denotes matrix transposition.\n\nThe integral of this Gaussian function over the whole formula_39-dimensional space is given as\nIt can be easily calculated by diagonalizing the matrix formula_43 and changing the integration variables to the eigenvectors of formula_43.\n\nMore generally a shifted Gaussian function is defined as\nwhere formula_51 is the shift vector and the matrix formula_43 can be assumed to be symmetric, formula_53, and positive-definite. The following integrals with this function can be calculated with the same technique,\n\nA number of fields such as stellar photometry, Gaussian beam characterization, and emission/absorption line spectroscopy work with sampled Gaussian functions and need to accurately estimate the height, position, and width parameters of the function. These are formula_55, formula_56, and formula_11 for a 1D Gaussian function, formula_43, formula_59, and formula_60 for a 2D Gaussian function. The most common method for estimating the profile parameters is to take the logarithm of the data and fit a parabola to the resulting data set. While this provides a simple least squares fitting procedure, the resulting algorithm is biased by excessively weighting small data values, and this can produce large errors in the profile estimate. One can partially compensate for this through weighted least squares estimation, in which the small data values are given small weights, but this too can be biased by allowing the tail of the Gaussian to dominate the fit. In order to remove the bias, one can instead use an iterative procedure in which the weights are updated at each iteration (see Iteratively reweighted least squares).\n\nOnce one has an algorithm for estimating the Gaussian function parameters, it is also important to know how accurate those estimates are. While an estimation algorithm can provide numerical estimates for the variance of each parameter (i.e. the variance of the estimated height, position, and width of the function), one can use Cramér–Rao bound theory to obtain an analytical expression for the lower bound on the parameter variances, given some assumptions about the data.\nWhen these assumptions are satisfied, the following covariance matrix K applies for the 1D profile parameters formula_55, formula_56, and formula_11 under i.i.d. Gaussian noise and under Poisson noise:\n\nwhere formula_65 is the width of the pixels used to sample the function, formula_66 is the quantum efficiency of the detector, and formula_67 indicates the standard deviation of the measurement noise. Thus, the individual variances for the parameters are, in the Gaussian noise case,\n\nand in the Poisson noise case,\n\nFor the 2D profile parameters giving the amplitude formula_43, position formula_59, and width formula_60 of the profile, the following covariance matrices apply:\n\nwhere the individual parameter variances are given by the diagonal elements of the covariance matrix.\n\nOne may ask for a discrete analog to the Gaussian;\nthis is necessary in discrete applications, particularly digital signal processing. A simple answer is to sample the continuous Gaussian, yielding the sampled Gaussian kernel. However, this discrete function does not have the discrete analogs of the properties of the continuous function, and can lead to undesired effects, as described in the article scale space implementation.\n\nAn alternative approach is to use discrete Gaussian kernel:\nwhere formula_76 denotes the modified Bessel functions of integer order.\n\nThis is the discrete analog of the continuous Gaussian in that it is the solution to the discrete diffusion equation (discrete space, continuous time), just as the continuous Gaussian is the solution to the continuous diffusion equation.\n\nGaussian functions appear in many contexts in the natural sciences, the social sciences, mathematics, and engineering. Some examples include:\n\n\n"}
{"id": "11197892", "url": "https://en.wikipedia.org/wiki?curid=11197892", "title": "Gromov norm", "text": "Gromov norm\n\nIn mathematics, the Gromov norm (or simplicial volume) of a compact oriented \"n\"-manifold is a norm on the homology (with real coefficients) given by minimizing the sum of the absolute values of the coefficients over all singular chains representing a cycle. The Gromov norm of the manifold is the Gromov norm of the fundamental class.\n\nIt is named after Mikhail Gromov, who with William Thurston, proved that the Gromov norm of a finite volume hyperbolic \"n\"-manifold is proportional to the hyperbolic volume. Thurston also used the Gromov norm to prove that hyperbolic volume decreases under hyperbolic Dehn surgery.\n\n"}
{"id": "4546087", "url": "https://en.wikipedia.org/wiki?curid=4546087", "title": "HC-256", "text": "HC-256\n\nHC-256 is a stream cipher designed to provide bulk encryption in software at high speeds while permitting strong confidence in its security. A 128-bit variant was submitted as an eSTREAM cipher candidate and has been selected as one of the four final contestants in the software profile.\n\nThe algorithm is designed by Hongjun Wu, and was first published in 2004. It is not patented.\n\nHC-256 has a 256 bit key and an initialization vector (nonce) of 256 bits.\n\nInternally, it consists of two secret tables (P and Q). Each table contains 1024 32-bit words. For each state update one 32-bit word in each table is updated using a non-linear update function. After 2048 steps all elements of the tables have been updated.\n\nIt generates one 32-bit word for each update step using a 32-bit to 32-bit mapping function similar to the output function of the Blowfish cipher. Finally a linear bit-masking function is applied to generate an output word. It uses the two message schedule functions in the hash function SHA-256 internally, but with the tables P and Q as S-boxes.\n\nHC-128 is similar in function, and reduces each of key length, nonce, number of words in the tables P and Q, and number of table updating steps by half.\n\nThe performance of HC-256 is estimated by its author to be about 4 cycles per byte on a Pentium 4 processor. However the initialization phase of the cipher includes expanding the 256-bit key into the tables P, Q and then running the cipher for 4096 steps. The author of HC-256 estimates this process to take around 74,000 cycles.\n\nFor HC-128 an encryption speed of about 3 cycles per byte on a Pentium M processor are cited.\n\nThe implementation of HC-128 on various computing structures is studied in detail, with significant performance gains compared to naive SW implementation.\n\n"}
{"id": "24489100", "url": "https://en.wikipedia.org/wiki?curid=24489100", "title": "Henry Cotterill", "text": "Henry Cotterill\n\nHenry Cotterill (1812 – 16 April 1886) was an Anglican bishop serving in South Africa in the second half of the 19th century. From 1872 until death he was a bishop of the Scottish Episcopal Church in Edinburgh.\n\nCotterill was born in Ampton in 1812 into an ecclesiastical family of committed Church Evangelicals. His father, Joseph Cotterill (1780 – 1858), was Rector of Blakeney, Norfolk, and a prebendary of Norwich Cathedral. His mother, Anne Boak, was a close friend of Hannah More. Educated at his father's old college, St John's College, Cambridge, he was both Senior Wrangler and headed the list of Classicists in 1835, on the strength of which he was elected as a Fellow of his college. Influenced by Charles Simeon, he was ordained in 1836 and went to India as Chaplain to the Madras Presidency the following year. Forced by malaria to return to England in 1846, he became inaugural Vice Principal and then the second Principal of Brighton College. In post less than six years, he reinvigorated the languishing infant school. In a whirlwind of energetic reform, he overhauled the curriculum by introducing the teaching of the sciences and oriental languages, restored discipline, launched a fund to build a chapel, built the first on-site boarding house and connected the school to the town's gas supply.\n\nAt the suggestion of Anthony Ashley-Cooper, 7th Earl of Shaftesbury and John Sumner, Archbishop of Canterbury, he was nominated and consecrated in 1856 as the second colonial Bishop of Grahamstown in South Africa. As was then customary, he was simultaneously created a Doctor of Divinity (DD).\n\nCotterill was consecrated on 23 November 1856, and arrived in Grahamstown in May 1857. Bishop Cotterill's episcopate was occupied with the\ndevelopment and consolidation of his diocese, and with the institution of diocesan and provincial synods. The opening service of the first synod of the diocese was held in the Grahamstown Cathedral on 20 June 1860. It may be of interest to record\nthat H. Blaine and F. Carlisle were the representatives of the Cathedral congregation at the synod.\n\nAs one of the bishops of South Africa, he sat in judgement in December 1863 on John Colenso, Bishop of Natal, his college friend from Cambridge days.\n\nHe was translated to Edinburgh in 1871 as coadjutor bishop of the Scottish Episcopal Church, and was created a full diocesan bishop in 1872.\n\nDuring his time in Edinburgh he resided at 10 North Manor Place (later renumbered as 56 Manor Place), just north of his place of worship St Mary's Cathedral.\n\nHe died in post in Edinburgh in 1886 and was buried between the choir stalls in the cathedral. His grave is covered by a large memorial brass made by Francis Skidmore of Coventry.\n\nIn 1836 he married Anna Isabella Parnther (1812-1899) who had been born in Jamaica.\n\nThey had at least two daughters and four sons. The four boys all attended Brighton College. George Edward (1839 – 1913), a Cambridge cricket blue and Sussex cricketer, was briefly Headmaster of St. Andrew's College, Grahamstown (1863 – 65) before returning to teach at Brighton College (1865 – 81). Henry Bernard (1846 – 1924) was an African missionary explorer and writer. Joseph Montagu Cotterill (1851 – 1933) played cricket for Sussex and became President of the Royal College of Surgeons of Edinburgh and was knighted. Arthur John (1849 – 1915) was Engineer-in-Chief, Egyptian Railways.\n\nHis brother George was on the teaching staff of Brighton College 1849 – 51 before emigrating to New Zealand while, intriguingly, his youngest brother, James Henry, was a pupil at the school while he was the Principal. James Henry became Professor of Applied Mathematics at the Royal Naval College, Greenwich (1873 – 97) and was elected a Fellow of the Royal Society in 1878.\n\nHis published works include\n\n"}
{"id": "3782688", "url": "https://en.wikipedia.org/wiki?curid=3782688", "title": "Herbert Wilf", "text": "Herbert Wilf\n\nHerbert Saul Wilf (June 13, 1931 – January 7, 2012) was a mathematician, specializing in combinatorics and graph theory. He was the Thomas A. Scott Professor of Mathematics in Combinatorial Analysis and Computing at the University of Pennsylvania. He wrote numerous books and research papers. Together with Neil Calkin he founded \"The Electronic Journal of Combinatorics\" in 1994 and was its editor-in-chief until 2001.\n\nWilf was the author of numerous papers and books, and was adviser and mentor to many students and colleagues. His collaborators include Doron Zeilberger and Donald Knuth. One of Wilf's former students is Richard Garfield, the creator of the collectible card game \"\". He also served as a thesis advisor for E. Roy Weintraub in the late 1960s.\n\nWilf died of a progressive neuromuscular disease in 2012.\n\nIn 1998, Wilf and Zeilberger received the Leroy P. Steele Prize for Seminal Contribution to Research for their joint paper, \"Rational functions certify combinatorial identities\" (\"Journal of the American Mathematical Society\", 3 (1990) 147–158). The prize citation reads: \"New mathematical ideas can have an impact on experts in a field, on people outside the field, and on how the field develops after the idea has been introduced. The remarkably simple idea of the work of Wilf and Zeilberger has already changed a part of mathematics for the experts, for the high-level users outside the area, and the area itself.\" Their work has been translated into computer packages that have simplified hypergeometric summation.\n\nIn 2002, Wilf was awarded the Euler Medal by the Institute of Combinatorics and its Applications.\n\n\n\n\n\n"}
{"id": "9419642", "url": "https://en.wikipedia.org/wiki?curid=9419642", "title": "Implication graph", "text": "Implication graph\n\nIn mathematical logic, an implication graph is a skew-symmetric directed graph \"G\"(\"V\", \"E\") composed of vertex set \"V\" and directed edge set \"E\". Each vertex in \"V\" represents the truth status of a Boolean literal, and each directed edge from vertex \"u\" to vertex \"v\" represents the material implication \"If the literal \"u\" is true then the literal \"v\" is also true\". Implication graphs were originally used for analyzing complex Boolean expressions.\n\nA 2-satisfiability instance in conjunctive normal form can be transformed into an implication graph by replacing each of its disjunctions by a pair of implications. For example, the statement formula_1 can be rewritten as the pair formula_2. An instance is satisfiable if and only if no literal and its negation belong to the same strongly connected component of its implication graph; this characterization can be used to solve 2-satisfiability instances in linear time.\n\nIn CDCL SAT-solvers, unit propagation can be naturally associated with an implication graph that captures all possible ways of deriving all implied literals from decision literals, which is then used for clause learning.\n"}
{"id": "1805135", "url": "https://en.wikipedia.org/wiki?curid=1805135", "title": "Inequity aversion", "text": "Inequity aversion\n\nInequity aversion (IA) is the preference for fairness and resistance to incidental inequalities. The social sciences that study inequity aversion include sociology, economics, psychology, anthropology, and ethology.\n\nInequity aversion research on humans mostly occurs in the discipline of economics though it is also studied in sociology.\n\nResearch on inequity aversion began in 1978 when studies suggested that humans are sensitive to inequities in favor of as well as those against them, and that some people attempt overcompensation when they feel \"guilty\" or unhappy to have received an undeserved reward.\n\nA more recent definition of inequity aversion (resistance to inequitable outcomes) was developed in 1999 by Fehr and Schmidt. They postulated that people make decisions so as to minimize inequity in outcomes. Specifically, consider a setting with individuals {1,2...,\"n\"} who receive pecuniary outcomes \"x\". Then the utility to person \"i\" would be given by \n\nwhere α parametrizes the distaste of person \"i\" for disadvantageous inequality in the first nonstandard term, and β parametrizes the distaste of person \"i\" for advantageous inequality in the final term.\n\nFehr and Schmidt showed that disadvantageous inequity aversion manifests itself in humans as the \"willingness to sacrifice potential gain to block another individual from receiving a superior reward\". They argue that this apparently self-destructive response is essential in creating an environment in which bilateral bargaining can thrive. Without inequity aversion's rejection of injustice, stable cooperation would be harder to maintain (for instance, there would be more opportunities for successful free riders).\n\nJames H. Fowler and his colleagues also argue that inequity aversion is essential for cooperation in multilateral settings. In particular, they show that subjects in \"random income\" games (closely related to public goods games) are willing to spend their own money to reduce the income of wealthier group members and increase the income of poorer group members even when there is no cooperation at stake. Thus, individuals who free ride on the contributions of fellow group members are likely to be punished because they earn more, creating a decentralized incentive for the maintenance of cooperation.\n\nInequity aversion is broadly consistent with observations of behavior in three standard economics experiments:\n\nIn 2005, John List modified these experiments slightly to determine if something in the construction of the experiments was prompting specific behaviors. When given a choice to steal money from the other player, even a single dollar, the observed altruism all but disappeared. In another experiment, the two players were given a sum of money and the choice to give or take any amount from the other player. In this experiment, only 10% of the participants gave the other person any money at all, and fully 40% of the players opted to take all of the other player's money.\n\nThe last such experiment was identical to the former, where 40% were turned into a gang of robbers, with one catch: the two players were forced to earn the money by stuffing envelopes. In this last experiment, more than two thirds of the players neither took nor gave a cent, while just over 20% still took some of the other player's money.\n\nIn 2011, Ert Erev and Roth ran a model prediction competition on two datasets, each of which included 120 two-player games. In each game player 1 decides whether to \"opt out\" and determine the payoffs for both players, or to \"opt in\" and let player 2 decide about the payoff allocation by choosing between actions \"left\" or \"right\". The payoffs were randomly selected, so the dataset included games like the Ultimatum, Dictator, and Trust, as well as other games. The results suggested that inequity aversion could be described as one of many strategies that people might use in such games.\n\nOther research in experimental economics addresses risk aversion in decision making and the comparison of inequality measures to subjective judgments on perceived inequalities.\n\nSurveys of employee opinions within firms have shown modern labor economists that inequity aversion is very important to them. Employees compare not only relative salaries but also relative performance against that of co-workers. Where these comparisons lead to guilt or envy, inequity aversion may lower employee morale. According to Bewley (1999), the main reason that managers create formal pay structures is so that the inter-employee comparison is seen to be \"fair\", which they considered \"key\" for morale and job performance.\n\nIt is natural to think of inequity aversion leading to greater solidarity within the labor pool, to the benefit of the average employee. However, a 2008 paper by Pedro Rey-Biel shows that this assumption can be subverted, and that an employer can use inequity aversion to get higher performance for less pay than would be possible otherwise. This is done by moving away from formal pay structures and using off-equilibrium bonus payments as incentives for extra performance. He shows that the optimal contract for inequity aversion employees is less generous at the optimal production level than contracts for \"standard agents\" (who don't have inequity aversion) in an otherwise identical two-employee model.\n\nIn 2005 Avner Shaked distributed a \"pamphlet\" entitled \"The Rhetoric of Inequity Aversion\" that attacked the inequity aversion papers of Fehr & Schmidt. In 2010, Shaked has published an extended version of the criticism together with Ken Binmore in the \"Journal of Economic Behavior and Organization\" (the same issue also contains a reply by Fehr and Schmidt and a rejoinder by Binmore and Shaked). A problem of inequity aversion models is the fact that there are free parameters; standard theory is simply a special case of the inequity aversion model. Hence, by construction inequity aversion must always be at least as good as standard theory when the inequity aversion parameters can be chosen after seeing the data. Binmore and Shaked also point out that Fehr and Schmidt (1999) pick a distribution of alpha and beta without conducting a formal estimation. The perfect correlation between the alpha and beta parameters in Fehr and Schmidt (1999) is an assumption made in the appendix of their paper that is not justified by the data that they provide. \n\nMore recently, several papers have estimated Fehr-Schmidt inequity aversion parameters using estimation techniques such as maximum likelihood. The results are mixed. Some authors have found beta larger than alpha, which contradicts a central assumption made by Fehr and Schmidt (1999). Other authors have found that inequity aversion with Fehr and Schmidt's (1999) distribution of alphas and betas explains data of contract-theoretic experiments not better than standard theory; they also estimate average values of alpha that are much smaller than suggested by Fehr and Schmidt (1999). Moreover, Levitt and List (2007) have pointed out that laboratory experiments tend to exaggerate the importance of pro-social behaviors because the subjects in the laboratory know that they are being monitored. \n\nAn alternative to the concept of a general inequity aversion is the assumption, that the \"degree\" and the structure of inequality could lead either to acceptance or to aversion of inequality.\n\nAn experiment on capuchin monkeys (Brosnan, S and de Waal, F) showed that the subjects would prefer receiving nothing to receiving a reward awarded inequitably in favor of a second monkey, and appeared to target their anger at the researchers responsible for the inequitable distribution of food. Anthropologists suggest that this research indicates a biological and evolutionary sense of social \"fair play\" in primates, though others believe that this is learned behavior or explained by other mechanisms. There is also evidence for inequity aversion in chimpanzees (though see a recent study questioning this interpretation). The latest study shows that chimpanzees play the Ultimatum Game in the same way as children, preferring equitable outcomes. The authors claim that we now are near the point of no difference between humans and apes with regard to a sense of fairness. Recent studies suggest that animals in the canidae family also recognize a basic level of fairness, stemming from living in cooperative societies. Animal cognition studies in other biological orders have not found similar importance on \"relative\" \"equity\" and \"justice\" as opposed to \"absolute\" utility.\n\nFehr and Schmidt's model may partially explain the widespread opposition to economic inequality in democracies, but a distinction should be drawn between inequity aversion's \"guilt\" and egalitarianism's \"compassion\", which does not necessarily imply \"injustice\".\n\nInequity aversion should not be confused with the arguments against the \"consequences\" of inequality. For example, the pro-publicly funded health care slogan \"Hospitals for the poor become poor hospitals\" directly objects to a predicted decline in medical care, not the health-care apartheid that is supposed to cause it. The argument that average medical outcomes improve with reduction in healthcare inequality (at the same total spending) is separate from the case for public healthcare on the grounds of inequity aversion.\n\n\n"}
{"id": "19781408", "url": "https://en.wikipedia.org/wiki?curid=19781408", "title": "Institutiones calculi differentialis", "text": "Institutiones calculi differentialis\n\nInstitutiones calculi differentialis (Foundations of differential calculus) is a mathematical work written in 1748 by Leonhard Euler and published in 1755 that lays the groundwork for the differential calculus. It consists of a single volume containing two internal books; there are 9 chapters in book I, and 18 in book II.\n\n\n\n"}
{"id": "38053861", "url": "https://en.wikipedia.org/wiki?curid=38053861", "title": "Joan Feigenbaum", "text": "Joan Feigenbaum\n\nJoan Feigenbaum (born 1958 in Brooklyn, New York) is a theoretical computer scientist with a background in mathematics. She is the Grace Murray Hopper Professor of Computer Science at Yale University. At Yale she also holds a courtesy appointment in the Department of Economics. Feigenbaum co-invented the computer-security research area of trust management.\n\nFeigenbaum did her undergraduate work in Mathematics at Harvard University. She became interested in computers during the Summer Research Program at AT&T's Bell Labs between her junior and senior years. She then earned a PHD in computer science at Stanford University, under the supervision of Andrew Yao, while working summers at Bell Labs. After graduation she joined Bell Labs. She became the Hopper Professor at Yale in 2008.\n\nShe is married to Jeffrey Nussbaum. They have a son, Sam Baum. Baum was chosen as child's surname as the greatest common suffix of Feigenbaum and Nussbaum.\n\nIn 2001, Feigenbaum became a fellow of the Association for Computing Machinery for her \"foundational and highly influential contributions to cryptographic complexity theory, authorization and trust management, massive-data-stream computation, and algorithmic mechanism design.\" In 2012 she was named a fellow of the American Association for the Advancement of Science and, in 2013, a member of the Connecticut Academy of Science and Engineering. The Connecticut Technology Council chose her as a Woman of Innovation in 2012. She acts as one of the three award-committee members on ACM SIGecom test of time award. \n"}
{"id": "38616928", "url": "https://en.wikipedia.org/wiki?curid=38616928", "title": "Kellogg's theorem", "text": "Kellogg's theorem\n\nKellogg's theorem is a pair of related results in the mathematical study of the regularity of harmonic functions on sufficiently smooth domains by Oliver Dimon Kellogg.\n\nIn the first version, it states that, for formula_1, if the domain's boundary is of class formula_2 and the \"k\"-th derivatives of the boundary are Dini continuous, then the harmonic functions are uniformly formula_2 as well. The second, more common version of the theorem states that for domains which are formula_4, if the boundary data is of class formula_4, then so is the harmonic function itself.\n\nKellogg's method of proof analyzes the representation of harmonic functions provided by the Poisson kernel, applied to an interior tangent sphere.\n\nIn modern presentations, Kellogg's theorem is usually covered as a specific case of the boundary Schauder estimates for elliptic partial differential equations.\n\n\n"}
{"id": "32926406", "url": "https://en.wikipedia.org/wiki?curid=32926406", "title": "Knaster's condition", "text": "Knaster's condition\n\nIn mathematics, a partially ordered set \"P\" is said to have Knaster's condition upwards (sometimes property (K)) if any uncountable subset \"A\" of \"P\" has an upwards-linked uncountable subset. An analogous definition applies to Knaster's condition downwards.\n\nThe property is named after Polish mathematician Bronisław Knaster.\n\nKnaster's condition implies the countable chain condition (ccc), and it is sometimes used in conjunction with a weaker form of Martin's axiom, where the ccc requirement is replaced with Knaster's condition. Not unlike ccc, Knaster's condition is also sometimes used as a property of a topological space, in which case it means that the topology (as in, the family of all open sets) with inclusion satisfies the condition.\n\nFurthermore, assuming MA(formula_1), ccc implies Knaster's condition, making the two equivalent.\n"}
{"id": "3281385", "url": "https://en.wikipedia.org/wiki?curid=3281385", "title": "Lagrange's theorem (number theory)", "text": "Lagrange's theorem (number theory)\n\nIn number theory, Lagrange's theorem is a statement named after Joseph-Louis Lagrange about how frequently a polynomial over the integers may evaluate to a multiple of a fixed prime. More precisely, it states that if \"p\" is a prime number and formula_1 is a polynomial with integer coefficients, then either:\n\nSolutions are \"incongruent\" if they do not differ by a multiple of \"p\". If the modulus is not prime, then it is possible for there to be more than deg \"f(x)\" solutions.\n\nThe two key ideas are the following. Let formula_3 be the polynomial obtained from formula_4 by taking the coefficients formula_5. Now (i) formula_6 is divisible by formula_7 if and only if formula_8; (ii) formula_9 has no more roots than its degree.\n\nMore rigorously, start by noting that formula_10 if and only if each coefficient of formula_4 is divisible by formula_7. Assume formula_13 is not 0; its degree is thus well-defined. It's easy to see formula_14. To prove (i), first note that we can compute formula_9 either directly, i.e. by plugging in (the residue class of) formula_16 and performing arithmetic in formula_17, or by reducing formula_18. Hence formula_19 if and only if formula_20, i.e. if and only if formula_21 is divisible by formula_7. To prove (ii), note that formula_17 is a field, which is a standard fact; a quick proof is to note that since formula_7 is prime, formula_17 is a finite integral domain, hence is a field. Another standard fact is that a non-zero polynomial over a field has at most as many roots as its degree; this follows from the division algorithm.\n\nFinally, note that two solutions formula_26 are incongruent if and only if formula_27. Putting it all together: the number of incongruent solutions by (i) is the same as the number of roots of formula_13, which by (ii) is at most formula_29, which is at most formula_30.\n\n"}
{"id": "4376814", "url": "https://en.wikipedia.org/wiki?curid=4376814", "title": "Large countable ordinal", "text": "Large countable ordinal\n\nIn the mathematical discipline of set theory, there are many ways of describing specific countable ordinals. The smallest ones can be usefully and non-circularly expressed in terms of their Cantor normal forms. Beyond that, many ordinals of relevance to proof theory still have computable ordinal notations. However, it is not possible to decide effectively whether a given putative ordinal notation is a notation or not (for reasons somewhat analogous to the unsolvability of the halting problem); various more-concrete ways of defining ordinals that definitely have notations are available.\n\nSince there are only countably many notations, all ordinals with notations are exhausted well below the first uncountable ordinal ω; their supremum is called Church–Kleene ω or ω (not to be confused with the first uncountable ordinal, ω), described below. Ordinal numbers below ω are the recursive ordinals (see below). Countable ordinals larger than this may still be defined, but do not have notations.\n\nDue to the focus on countable ordinals, ordinal arithmetic is used throughout, except where otherwise noted. The ordinals described here are not as large as the ones described in large cardinals, but they are large among those that have constructive notations (descriptions). Larger and larger ordinals can be defined, but they become more and more difficult to describe.\n\nRecursive ordinals (or computable ordinals) are certain countable ordinals: loosely speaking those represented by a computable function. There are several equivalent definitions of this: the simplest is to say that a computable ordinal is the order-type of some recursive (i.e., computable) well-ordering of the natural numbers; so, essentially, an ordinal is recursive when we can present the set of smaller ordinals in such a way that a computer (Turing machine, say) can manipulate them (and, essentially, compare them).\nA different definition uses Kleene's system of ordinal notations. Briefly, an ordinal notation is either the name zero (describing the ordinal 0), or the successor of an ordinal notation (describing the successor of the ordinal described by that notation), or a Turing machine (computable function) that produces an increasing sequence of ordinal notations (that describe the ordinal that is the limit of the sequence), and ordinal notations are (partially) ordered so as to make the successor of \"o\" greater than \"o\" and to make the limit greater than any term of the sequence (this order is computable; however, the set O of ordinal notations itself is highly non-recursive, owing to the impossibility of deciding whether a given Turing machine does indeed produce a sequence of notations); a recursive ordinal is then an ordinal described by some ordinal notation.\n\nAny ordinal smaller than a recursive ordinal is itself recursive, so the set of all recursive ordinals forms a certain (countable) ordinal, the Church–Kleene ordinal (see below).\n\nIt is tempting to forget about ordinal notations, and only speak of the recursive ordinals themselves: and some statements are made about recursive ordinals which, in fact, concern the notations for these ordinals. This leads to difficulties, however, as even the smallest infinite ordinal, ω, has many notations, some of which cannot be proved to be equivalent to the obvious notation (the limit of the simplest program that enumerates all natural numbers).\n\nThere is a relation between computable ordinals and certain formal systems (containing arithmetic, that is, at least a reasonable fragment of Peano arithmetic).\n\nCertain computable ordinals are so large that while they can be given by a certain ordinal notation \"o\", a given formal system might not be sufficiently powerful to show that \"o\" is, indeed, an ordinal notation: the system does not show transfinite induction for such large ordinals.\n\nFor example, the usual first-order Peano axioms do not prove transfinite induction for (or beyond) ε: while the ordinal ε can easily be arithmetically described (it is countable), the Peano axioms are not strong enough to show that it is indeed an ordinal; in fact, transfinite induction on ε proves the consistency of Peano's axioms (a theorem by Gentzen), so by Gödel's second incompleteness theorem, Peano's axioms cannot formalize that reasoning. (This is at the basis of the Kirby–Paris theorem on Goodstein sequences.) We say that ε measures the proof-theoretic strength of Peano's axioms.\n\nBut we can do this for systems far beyond Peano's axioms. For example, the proof-theoretic strength of Kripke–Platek set theory is the Bachmann–Howard ordinal, and, in fact, merely adding to Peano's axioms the axioms that state the well-ordering of all ordinals below the Bachmann–Howard ordinal is sufficient to obtain all arithmetical consequences of Kripke–Platek set theory.\n\nWe have already mentioned (see Cantor normal form) the ordinal ε, which is the smallest satisfying the equation formula_1, so it is the limit of the sequence 0, 1, formula_2, formula_3, formula_4, etc. The next ordinal satisfying this equation is called ε: it is the limit of the sequence\n\nMore generally, the formula_6-th ordinal such that formula_1 is called formula_8. We could define formula_9 as the smallest ordinal such that formula_10, but since the Greek alphabet does not have transfinitely many letters it is better to use a more robust notation: define ordinals formula_11 by transfinite induction as follows: let formula_12 and let formula_13 be the formula_14-th fixed point of formula_15 (i.e., the formula_14-th ordinal such that formula_17; so for example, formula_18), and when formula_19 is a limit ordinal, define formula_20 as the formula_21-th common fixed point of the formula_15 for all formula_23. This family of functions is known as the Veblen hierarchy (there are inessential variations in the definition, such as letting, for formula_19 a limit ordinal, formula_20 be the limit of the formula_26 for formula_23: this essentially just shifts the indices by 1, which is harmless). formula_15 is called the formula_29 Veblen function (to the base formula_2).\n\nOrdering: formula_31 if and only if either (formula_32 and formula_33) or (formula_34 and formula_35) or (formula_36 and formula_37).\n\nThe smallest ordinal such that formula_38 is known as the Feferman–Schütte ordinal and generally written formula_39. It can be described as the set of all ordinals that can be written as finite expressions, starting from zero, using only the Veblen hierarchy and addition. The Feferman–Schütte ordinal is important because, in a sense that is complicated to make precise, it is the smallest (infinite) ordinal that cannot be (“predicatively”) described using smaller ordinals. It measures the strength of such systems as “arithmetical transfinite recursion”.\n\nMore generally, Γ enumerates the ordinals that cannot be obtained from smaller ordinals using addition and the Veblen functions.\n\nIt is, of course, possible to describe ordinals beyond the Feferman–Schütte ordinal. One could continue to seek fixed points in more and more complicated manner: enumerate the fixed points of formula_40, then enumerate the fixed points of \"that\", and so on, and then look for the first ordinal α such that α is obtained in α steps of this process, and continue diagonalizing in this \"ad hoc\" manner. This leads to the definition of the “small” and “large” Veblen ordinals.\n\nTo go far beyond the Feferman–Schütte ordinal, one needs to introduce new methods. Unfortunately there is not yet any standard way to do this: every author in the subject seems to have invented their own system of notation, and it is quite hard to translate between the different systems. The first such system was introduced by Bachmann in 1950 (in an \"ad hoc\" manner), and different extensions and variations of it were described by Buchholz, Takeuti (ordinal diagrams), Feferman (θ systems), Aczel, Bridge, Schütte, and Pohlers. However most systems use the same basic idea, of constructing new countable ordinals by using the existence of certain uncountable ordinals. Here is an example of such a definition, described in much greater detail in the article on ordinal collapsing function:\nHere Ω = ω is the first uncountable ordinal. It is put in because otherwise the function ψ gets \"stuck\" at the smallest ordinal σ such that ε=σ: in particular ψ(α)=σ for any ordinal α satisfying σ≤α≤Ω. However the fact that we included Ω allows us to get past this point: ψ(Ω+1) is greater than σ. The key property of Ω that we used is that it is greater than any ordinal produced by ψ.\n\nTo construct still larger ordinals, we can extend the definition of ψ by throwing in more ways of constructing uncountable ordinals. There are several ways to do this, described to some extent in the article on ordinal collapsing function.\n\nThe Bachmann–Howard ordinal (sometimes just called the Howard ordinal, ψ(ε) with the notation above) is an important one, because it describes the proof-theoretic strength of Kripke–Platek set theory. Indeed, the main importance of these large ordinals, and the reason to describe them, is their relation to certain formal systems as explained above. However, such powerful formal systems as full second-order arithmetic, let alone Zermelo–Fraenkel set theory, seem beyond reach for the moment.\n\nBy dropping the requirement of having a useful description, even larger recursive countable ordinals can be obtained as the ordinals measuring the strengths of various strong theories; roughly speaking, these ordinals are the smallest ordinals that the theories cannot prove are well ordered. By taking stronger and stronger theories such as second-order arithmetic, Zermelo set theory, Zermelo–Fraenkel set theory, or Zermelo–Fraenkel set theory with various large cardinal axioms, one gets some extremely large recursive ordinals. (Strictly speaking it is not known that all of these really are ordinals: by construction, the ordinal strength of a theory can only be proved to be an ordinal from an even stronger theory. So for the large cardinal axioms this becomes quite unclear.)\n\nThe set of recursive ordinals is an ordinal that is the smallest ordinal that \"cannot\" be described in a recursive way. (It is not the order type of any recursive well-ordering of the integers.) That ordinal is a countable ordinal called the Church–Kleene ordinal, formula_41. Thus, formula_41 is the smallest non-recursive ordinal, and there is no hope of precisely “describing” any ordinals from this point on—we can only \"define\" them. But it is still far less than the first uncountable ordinal, formula_43. However, as its symbol suggests, it behaves in many ways rather like formula_43. \n\nThe Church–Kleene ordinal is again related to Kripke–Platek set theory, but now in a different way: whereas the Bachmann–Howard ordinal (described above) was the smallest ordinal for which KP does not prove transfinite induction, the Church–Kleene ordinal is the smallest α such that the construction of the Gödel universe, \"L\", up to stage α, yields a model formula_45 of KP. Such ordinals are called admissible, thus formula_41 is the smallest admissible ordinal (beyond ω in case the axiom of infinity is not included in KP).\n\nBy a theorem of Sacks, the countable admissible ordinals are exactly those constructed in a manner similar to the Church–Kleene ordinal but for Turing machines with oracles. One sometimes writes formula_47 for the formula_21-th ordinal that is either admissible or a limit of admissible.\n\nAn ordinal that is both admissible and a limit of admissibles, or equivalently such that formula_21 is the formula_21-th admissible ordinal, is called \"recursively inaccessible\". There exists a theory of large ordinals in this manner that is highly parallel to that of (small) large cardinals. For example, we can define recursively \"Mahlo ordinals\": these are the formula_21 such that every formula_21-recursive closed unbounded subset of formula_21 contains an admissible ordinal (a recursive analog of the definition of a Mahlo cardinal). But note that we are still talking about possibly countable ordinals here. (While the existence of inaccessible or Mahlo cardinals cannot be proved in Zermelo–Fraenkel set theory, that of recursively inaccessible or recursively Mahlo ordinals is a theorem of ZFC: in fact, any regular cardinal is recursively Mahlo and more, but even if we limit ourselves to countable ordinals, ZFC proves the existence of recursively Mahlo ordinals. They are, however, beyond the reach of Kripke–Platek set theory.)\n\nAn admissible ordinal formula_21 is called \"nonprojectible\" if there is no total formula_21-recursive injective function mapping formula_21 into a smaller ordinal. (This is trivially true for regular cardinals; however, we are mainly interested in countable ordinals.) Being nonprojectible is a much stronger condition than being admissible, recursively inaccessible, or even recursively Mahlo. It is equivalent to the statement that the Gödel universe, \"L\", up to stage α, yields a model formula_45 of KP + formula_58-separation.\n\nWe can imagine even larger ordinals that are still countable. For example, if ZFC has a transitive model (a hypothesis stronger than the mere hypothesis of consistency, and implied by the existence of an inaccessible cardinal), then there exists a countable formula_21 such that formula_45 is a model of ZFC. Such ordinals are beyond the strength of ZFC in the sense that it cannot (by construction) prove their existence.\n\nEven larger countable ordinals, called the \"stable ordinals\", can be defined by indescribability conditions or as those formula_21 such that formula_45 is a 1-elementary submodel of \"L\"; the existence of these ordinals can be proved in ZFC, and they are closely related to the nonprojectible ordinals.\n\nWithin the scheme of notations of Kleene some represent ordinals and some do not. One can define a recursive total ordering that is a subset of the Kleene notations and has an initial segment which is well-ordered with order-type formula_41. Every recursively enumerable (or even hyperarithmetic) nonempty subset of this total ordering has a least element. So it resembles a well-ordering in some respects. For example, one can define the arithmetic operations on it. Yet it is not possible to effectively determine exactly where the initial well-ordered part ends and the part lacking a least element begins.\n\nFor an example of a recursive pseudowellordering, let S be ATR or another recursively axiomatizable theory that has an ω-model but no hyperarithmetical ω-models, and (if needed) conservatively extend S with Skolem functions. Let T be the tree of (essentially) finite partial ω-models of S: A sequence of natural numbers formula_64 is in T iff S plus ∃m φ(m) ⇒ φ(x) (for the first n formulas φ with one numeric free variable; ⌈φ⌉ is the Gödel number) has no inconsistency proof shorter than n. Then the Kleene–Brouwer order of T is a recursive pseudowellordering.\n\nMost books describing large countable ordinals are on proof theory, and unfortunately tend to be out of print.\n\n\n\n"}
{"id": "3152856", "url": "https://en.wikipedia.org/wiki?curid=3152856", "title": "Large set (combinatorics)", "text": "Large set (combinatorics)\n\nIn combinatorial mathematics, a large set of positive integers\n\nis one such that the infinite sum of the reciprocals\n\ndiverges. A small set is any subset of the positive integers that is not large; that is, one whose sum of reciprocals converges.\n\nLarge sets appear in the Müntz–Szász theorem and in the Erdős conjecture on arithmetic progressions.\n\n\n\n\nPaul Erdős famously asked the question of whether any set that does not contain arbitrarily long arithmetic progressions must necessarily be small. He offered a prize of $3000 for the solution to this problem, more than for any of his other conjectures, and joked that this prize offer violated the minimum wage law. This question is still open.\n\nIt is not known how to identify whether a given set is large or small in general. As a result, there are many sets which are not known to be either large or small.\n\n\n"}
{"id": "340617", "url": "https://en.wikipedia.org/wiki?curid=340617", "title": "List of complex analysis topics", "text": "List of complex analysis topics\n\nComplex analysis, traditionally known as the theory of functions of a complex variable, is the branch of mathematics that investigates functions of complex numbers. It is useful in many branches of mathematics, including number theory and applied mathematics; as well as in physics, including hydrodynamics, thermodynamics, and electrical engineering.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "36467210", "url": "https://en.wikipedia.org/wiki?curid=36467210", "title": "Mingarelli identity", "text": "Mingarelli identity\n\nIn the field of ordinary differential equations, the Mingarelli identity is a theorem that provides criteria for the oscillation and non-oscillation of solutions of some linear differential equations in the real domain. It extends the Picone identity from two to three or more differential equations of the second order.\n\nConsider the solutions of the following (uncoupled) system of second order linear differential equations over the –interval :\nLet formula_3 denote the forward difference operator, i.e.\nThe second order difference operator is found by iterating the first order operator as in \nwith a similar definition for the higher iterates. Leaving out the independent variable for convenience, and assuming the on , there holds the identity,\n\nwhere \nWhen this equality reduces to the Picone identity.\n\nThe above identity leads quickly to the following comparison theorem for three linear differential equations, which extends the classical Sturm–Picone comparison theorem.\n\nLet , , be real-valued continuous functions on the interval and let\nbe three homogeneous linear second order differential equations in self-adjoint form, where \n\nAssume that for all in we have,\nThen, if on and , then any solution has at least one zero in .\n\n"}
{"id": "10070974", "url": "https://en.wikipedia.org/wiki?curid=10070974", "title": "Minimal prime (recreational mathematics)", "text": "Minimal prime (recreational mathematics)\n\nIn recreational number theory, a minimal prime is a prime number for which there is no shorter subsequence of its digits in a given base that form a prime. In base 10 there are exactly 26 minimal primes:\n\nFor example, 409 is a minimal prime because there is no prime among the shorter subsequences of the digits: 4, 0, 9, 40, 49, 09. The subsequence does not have to consist of consecutive digits, so 109 is not a minimal prime (because 19 is prime). But it does have to be in the same order; so, for example, 991 is still a minimal prime even though a subset of the digits can form the shorter prime 19 by changing the order.\n\nSimilarly, there are exactly 32 composite numbers which have no shorter composite subsequence:\n\nThere are 146 primes congruent to 1 mod 4 which have no shorter prime congruent to 1 mod 4 subsequence:\n\nThere are 113 primes congruent to 3 mod 4 which have no shorter prime congruent to 3 mod 4 subsequence:\n\nMinimal primes can be generalized to other bases. It can be shown that there are only a finite number of minimal primes in every base. Equivalently, every sufficiently large prime contains a shorter subsequence that forms a prime. \n\nThe base 12 minimal primes written in base 10 are listed in .\n\nNumber of minimal (probable) primes in base \"n\" are\n\nThe length of the largest minimal (probable) prime in base \"n\" are\n\nLargest minimal (probable) prime in base \"n\" (written in base 10) are\n\nNumber of minimal composites in base \"n\" are\n\nThe length of the largest minimal composite in base \"n\" are\n\n"}
{"id": "37817", "url": "https://en.wikipedia.org/wiki?curid=37817", "title": "Möbius strip", "text": "Möbius strip\n\nThe Möbius strip or Möbius band (, ; ), also spelled Mobius or Moebius, is a surface with only one side (when embedded in three-dimensional Euclidean space) and only one boundary. The Möbius strip has the mathematical property of being unorientable. It can be realized as a ruled surface. Its discovery is attributed to the German mathematicians August Ferdinand Möbius and Johann Benedict Listing in 1858, though a structure similar to the Möbius strip can be seen in Roman mosaics dated circa 200–250 AD.\n\nAn example of a Möbius strip can be created by taking a paper strip and giving it a half-twist, and then joining the ends of the strip to form a loop. However, the Möbius strip is not a surface of only one exact size and shape, such as the half-twisted paper strip depicted in the illustration. Rather, mathematicians refer to the closed Möbius band as any surface that is homeomorphic to this strip. Its boundary is a simple closed curve, i.e., homeomorphic to a circle. This allows for a very wide variety of geometric versions of the Möbius band as surfaces each having a definite size and shape. For example, any rectangle can be glued to itself (by identifying one edge with the opposite edge after a reversal of orientation) to make a Möbius band. Some of these can be smoothly modeled in Euclidean space, and others cannot.\n\nA half-twist clockwise gives an embedding of the Möbius strip different from that of a half-twist counterclockwise – that is, as an embedded object in Euclidean space, the Möbius strip is a chiral object with right- or left-handedness. However, the underlying topological spaces within the Möbius strip are homeomorphic in each case. An infinite number of topologically different embeddings of the same topological space into three-dimensional space exist, as the Möbius strip can also be formed by twisting the strip an odd number of times greater than one, or by knotting and twisting the strip, before joining its ends. The complete open Möbius band is an example of a topological surface that is closely related to the standard Möbius strip, but that is not homeomorphic to it.\n\nFinding algebraic equations, the solutions of which have the topology of a Möbius strip, is straightforward, but, in general, these equations do not describe the same geometric shape that one gets from the twisted paper model described above. In particular, the twisted paper model is a developable surface, having zero Gaussian curvature. A system of differential-algebraic equations that describes models of this type was published in 2007 together with its numerical solution.\n\nThe Euler characteristic of the Möbius strip is zero.\nThe Möbius strip has several curious properties. A line drawn starting from the seam down the middle meets back at the seam, but at the other side. If continued, the line meets the starting point, and is double the length of the original strip. This single continuous curve demonstrates that the Möbius strip has only one boundary.\n\nCutting a Möbius strip along the center line with a pair of scissors yields one long strip with two full twists in it, rather than two separate strips; the result is not a Möbius strip. This happens because the original strip only has one edge that is twice as long as the original strip. Cutting creates a second independent edge, half of which was on each side of the scissors. Cutting this new, longer, strip down the middle creates two strips wound around each other, each with two full twists.\n\nIf the strip is cut along about a third of the way in from the edge, it creates two strips: One is a thinner Möbius strip – it is the center third of the original strip, comprising one-third of the width and the same length as the original strip. The other is a longer but thin strip with two full twists in it – this is a neighborhood of the edge of the original strip, and it comprises one-third of the width and twice the length of the original strip.\n\nOther analogous strips can be obtained by similarly joining strips with two or more half-twists in them instead of one. For example, a strip with three half-twists, when divided lengthwise, becomes a twisted strip tied in a trefoil knot. (If this knot is unravelled, the strip has eight half-twists.) A strip with \"N\" half-twists, when bisected, becomes a strip with \"N\" + 1 full twists. Giving it extra twists and reconnecting the ends produces figures called paradromic rings.\n\nOne way to represent the Möbius strip as a subset of three-dimensional Euclidean space is using the parametrization:\n\nwhere formula_4 and formula_5. This creates a Möbius strip of width 1 whose center circle has radius 1, lies in the formula_6-plane and is centered at formula_7. The parameter formula_8 runs around the strip while formula_9 moves from one edge to the other.\n\nIn cylindrical polar coordinates formula_10, an unbounded version of the Möbius strip can be represented by the equation:\n\nIf a smooth Möbius strip in three-space is a rectangular one – that is, created from identifying two opposite sides of a geometrical rectangle with bending but not stretching the surface – then such an embedding is known to be possible if the aspect ratio of the rectangle is greater than the square root of three. (Note the shorter sides of the rectangle are identified to obtain the Möbius strip.) For an aspect ratio less than or equal to the square root of three, however, a smooth embedding of a rectangular Möbius strip into three-space may be impossible.\n\nAs the aspect ratio approaches the limiting ratio of from above, any such rectangular Möbius strip in three-space seems to approach a shape that in the limit can be thought of as a strip of three equilateral triangles, folded on top of one another so that they occupy just one equilateral triangle in three-space.\n\nIf the Möbius strip in three-space is only once continuously differentiable (in symbols: C), however, then the theorem of Nash-Kuiper shows that no lower bound exists.\n\nA method of making a Möbius strip from a rectangular strip too wide to simply twist and join (e.g., a rectangle only one unit long and one unit wide) is to first fold the wide direction back and forth using an even number of folds—an \"accordion fold\"—so that the folded strip becomes narrow enough that it can be twisted and joined, much as a single long-enough strip can be joined. With two folds, for example, a strip would become a folded strip whose cross section is in the shape of an 'N' and would remain an 'N' after a half-twist. This folded strip, three times as long as it is wide, would be long enough to then join at the ends. This method works in principle, but becomes impractical after sufficiently many folds, if paper is used. Using normal paper, this construction can be folded flat, with all the layers of the paper in a single plane, but mathematically, whether this is possible without stretching the surface of the rectangle is not clear.\n\nTopologically, the Möbius strip can be defined as the square formula_12 with its top and bottom sides identified by the relation formula_13 for formula_14, as in the diagram on the right.\n\nA less used presentation of the Möbius strip is as the topological quotient of a torus. A torus can be constructed as the square formula_12 with the edges identified as formula_16 (glue left to right) and formula_17 (glue bottom to top). If one then also identified , then one obtains the Möbius strip. The diagonal of the square (the points where both coordinates agree) becomes the boundary of the Möbius strip, and carries an orbifold structure, which geometrically corresponds to \"reflection\" – geodesics (straight lines) in the Möbius strip reflect off the edge back into the strip. Notationally, this is written as T/S – the 2-torus quotiented by the group action of the symmetric group on two letters (switching coordinates), and it can be thought of as the configuration space of two unordered points on the circle, possibly the same (the edge corresponds to the points being the same), with the torus corresponding to two ordered points on the circle.\n\nThe Möbius strip is a two-dimensional compact manifold (i.e. a surface) with boundary. It is a standard example of a surface that is not orientable. In fact, the Möbius strip is the epitome of the topological phenomenon of nonorientability. This is because two-dimensional shapes (surfaces) are the lowest-dimensional shapes for which nonorientability is possible and the Möbius strip is the only surface that is topologically a subspace of every nonorientable surface. As a result, any surface is nonorientable if and only if it contains a Möbius band as a subspace.\n\nThe Möbius strip is also a standard example used to illustrate the mathematical concept of a fiber bundle. Specifically, it is a nontrivial bundle over the circle \"S\" with a fiber the unit interval, . Looking only at the edge of the Möbius strip gives a nontrivial two point (or Z) bundle over \"S\".\n\nA simple construction of the Möbius strip that can be used to portray it in computer graphics or modeling packages is:\n\nThe open Möbius band is formed by deleting the boundary of the standard Möbius band. It is constructed from the set by identifying (glueing) the points and for all .\n\nIt may be constructed as a surface of constant positive, negative, or zero (Gaussian) curvature. In the cases of negative and zero curvature, the Möbius band can be constructed as a (geodesically) complete surface, which means that all geodesics (\"straight lines\" on the surface) may be extended indefinitely in either direction.\n\nConstant negative curvature:\nLike the plane and the open cylinder, the open Möbius band admits not only a complete metric of constant curvature 0, but also a complete metric of constant negative curvature, say −1. One way to see this is to begin with the upper half plane (Poincaré) model of the hyperbolic plane ℍ, namely with the Riemannian metric given by . The orientation-preserving isometries of this metric are all the maps of the form , where \"a\", \"b\", \"c\", \"d\" are real numbers satisfying . Here \"z\" is a complex number with , and we have identified ℍ with endowed with the Riemannian metric that was mentioned. Then one orientation-reversing isometry \"g\" of ℍ given by , where conj(\"z\") denotes the complex conjugate of \"z\". These facts imply that the mapping given by is an orientation-reversing isometry of ℍ that generates an infinite cyclic group \"G\" of isometries. (Its square is the isometry , which can be expressed as .) The quotient of the action of this group can easily be seen to be topologically a Möbius band. But it is also easy to verify that it is complete and non-compact, with constant negative curvature −1.\n\nThe group of isometries of this Möbius band is 1-dimensional and is isomorphic to the special orthogonal group SO(2).\n\n(Constant) zero curvature:\nThis may also be constructed as a complete surface, by starting with portion of the plane R defined by and identifying with for all \"x\" in R (the reals). The resulting metric makes the open Möbius band into a (geodesically) complete flat surface (i.e., having Gaussian curvature equal to 0 everywhere). This is the only metric on the Möbius band, up to uniform scaling, that is both flat and complete.\n\nThe group of isometries of this Möbius band is 1-dimensional and is isomorphic to the orthogonal group SO(2).\n\nConstant positive curvature:\nA Möbius band of constant positive curvature cannot be complete, since it is known that the only complete surfaces of constant positive curvature are the sphere and the projective plane. The projective plane P of constant curvature +1 may be constructed as the quotient of the unit sphere \"S\" in R by the antipodal map \"A\": \"S\" → \"S\", defined by . The open Möbius band is homeomorphic to the once-punctured projective plane, that is, P with any one point removed. This may be thought of as the closest that a Möbius band of constant positive curvature can get to being a complete surface: just one point away.\n\nThe group of isometries of this Möbius band is also 1-dimensional and isomorphic to the orthogonal group O(2).\n\nThe space of unoriented lines in the plane is diffeomorphic to the open Möbius band. To see why, let \"L\"(\"θ\") denote the line through the origin at an angle \"θ\" to the positive x-axis. For each \"L\"(\"θ\") there is the family \"P\"(\"θ\") of all lines in the plane that are perpendicular to \"L\"(\"θ\"). Topologically, the family \"P\"(\"θ\") is just a line (because each line in \"P\"(\"θ\") intersects the line \"L\"(\"θ\") in just one point). In this way, as \"θ\" increases in the range , the line \"L\"(\"θ\") represents a line's worth of distinct lines in the plane. But when \"θ\" reaches 180°, \"L\"(180°) is identical to \"L\"(0), and so the families \"P\"(0°) and \"P\"(180°) of perpendicular lines are also identical families. The line \"L\"(0°), however, has returned to itself as \"L\"(180°) \"pointed in the opposite direction\". Every line in the plane corresponds to exactly one line in some family \"P\"(\"θ\"), for exactly one \"θ\", for , and \"P\"(180°) is identical to \"P\"(0°) but returns pointed in the opposite direction. This ensures that the space of all lines in the plane – the union of all the \"L\"(\"θ\") for – is an open Möbius band.\n\nThe group of bijective linear transformations of the plane to itself (real matrices with non-zero determinant) naturally induces bijections of the space of lines in the plane to itself, which form a group of self-homeomorphisms of the space of lines. Hence the same group forms a group of self-homeomorphisms of the Möbius band described in the previous paragraph. But there is no metric on the space of lines in the plane that is invariant under the action of this group of homeomorphisms. In this sense, the space of lines in the plane has no natural metric on it.\n\nThis means that the Möbius band possesses a natural 4-dimensional Lie group of self-homeomorphisms, given by , but this high degree of symmetry cannot be exhibited as the group of isometries of any metric.\n\nThe edge, or boundary, of a Möbius strip is homeomorphic (topologically equivalent) to a circle. Under the usual embeddings of the strip in Euclidean space, as above, the boundary is not a true circle. However, it is possible to embed a Möbius strip in three dimensions so that the boundary is a perfect circle lying in some plane. For example, see Figures 307, 308, and 309 of \"Geometry and the imagination\".\n\nA much more geometric embedding begins with a minimal Klein bottle immersed in the 3-sphere, as discovered by Blaine Lawson. We then take half of this Klein bottle to get a Möbius band embedded in the 3-sphere (the unit sphere in 4-space). The result is sometimes called the \"Sudanese Möbius Band\"\n, where \"sudanese\" refers not to the country Sudan but to the names of two topologists, Sue Goodman and Daniel Asimov. Applying stereographic projection to the Sudanese band places it in 3-dimensional space, as can be seen below – a version due to George Francis can be found here.\n\nFrom Lawson's minimal Klein bottle we derive an embedding of the band into the 3-sphere \"S\", regarded as a subset of C, which is geometrically the same as R. We map angles \"η\", \"φ\" to complex numbers \"z\", \"z\" via\nHere the parameter \"η\" runs from 0 to \"π\" and \"φ\" runs from 0 to 2\"π\". Since , the embedded surface lies entirely in \"S\". The boundary of the strip is given by (corresponding to ), which is clearly a circle on the 3-sphere.\n\nTo obtain an embedding of the Möbius strip in R one maps \"S\" to R via a stereographic projection. The projection point can be any point on \"S\" that does not lie on the embedded Möbius strip (this rules out all the usual projection points). One possible choice is formula_20. Stereographic projections map circles to circles and preserves the circular boundary of the strip. The result is a smooth embedding of the Möbius strip into R with a circular edge and no self-intersections.\n\nThe Sudanese Möbius band in the three-sphere \"S\" is geometrically a fibre bundle over a great circle, whose fibres are great semicircles. The most symmetrical image of a stereographic projection of this band into R is obtained by using a projection point that lies on that great circle that runs through the midpoint of each of the semicircles. Each choice of such a projection point results in an image that is congruent to any other. But because such a projection point lies on the Möbius band itself, two aspects of the image are significantly different from the case (illustrated above) where the point is not on the band: 1) the image in R is not the full Möbius band, but rather the band with one point removed (from its centerline); and 2) the image is unbounded – and as it gets increasingly far from the origin of R, it increasingly approximates a plane. Yet this version of the stereographic image has a group of 4 symmetries in R (it is isomorphic to the Klein 4-group), as compared with the bounded version illustrated above having its group of symmetries the unique group of order 2. (If all symmetries and not just orientation-preserving isometries of R are allowed, the numbers of symmetries in each case doubles.)\n\nBut the most geometrically symmetrical version of all is the original Sudanese Möbius band in the three-sphere \"S\", where its full group of symmetries is isomorphic to the Lie group O(2). Having an infinite cardinality (that of the continuum), this is far larger than the symmetry group of any possible embedding of the Möbius band in R.\n\nA closely related 'strange' geometrical object is the Klein bottle. A Klein bottle can be produced by gluing two Möbius strips together along their edges; this cannot be done in ordinary three-dimensional Euclidean space without creating self-intersections.\n\nAnother closely related manifold is the real projective plane. If a circular disk is cut out of the real projective plane, what is left is a Möbius strip. Going in the other direction, if one glues a disk to a Möbius strip by identifying their boundaries, the result is the projective plane. To visualize this, it is helpful to deform the Möbius strip so that its boundary is an ordinary circle (see above). The real projective plane, like the Klein bottle, cannot be embedded in three-dimensions without self-intersections.\n\nIn graph theory, the Möbius ladder is a cubic graph closely related to the Möbius strip.\n\nIn 1968, Gonzalo Vélez Jahn (UCV, Caracas, Venezuela) discovered three dimensional bodies with Möbian characteristics; these were later described by Martin Gardner as prismatic rings that became toroidal polyhedrons in his August 1978 Mathematical Games column in Scientific American.\n\nThere have been several technical applications for the Möbius strip. Giant Möbius strips have been used as conveyor belts that last longer because the entire surface area of the belt gets the same amount of wear, and as continuous-loop recording tapes (to double the playing time). Möbius strips are common in the manufacture of fabric computer printer and typewriter ribbons, as they let the ribbon be twice as wide as the print head while using both halves evenly.\n\nA Möbius resistor is an electronic circuit element that cancels its own inductive reactance. Nikola Tesla patented similar technology in 1894: \"Coil for Electro Magnets\" was intended for use with his system of global transmission of electricity without wires.\n\nThe Möbius strip is the configuration space of two unordered points on a circle. Consequently, in music theory, the space of all two-note chords, known as dyads, takes the shape of a Möbius strip; this and generalizations to more points is a significant application of orbifolds to music theory.\n\nIn physics/electro-technology as:\n\nIn chemistry/nano-technology as:\n\nThe Möbius strip principle has been used as a method of creating the illusion of magic. The trick, known as the Afghan bands, was very popular in the first half of the twentieth century. Many versions of this trick exist and have been performed by famous illusionists such as Harry Blackstone Sr. and Thomas Nelson Downs.\n\n\n"}
{"id": "30759480", "url": "https://en.wikipedia.org/wiki?curid=30759480", "title": "No free lunch with vanishing risk", "text": "No free lunch with vanishing risk\n\nNo free lunch with vanishing risk (NFLVR) is a no-arbitrage argument. We have \"free lunch with vanishing risk\" if by utilizing a sequence of time self-financing portfolios which converge to an arbitrage strategy, we can approximate a self-financing portfolio (called the \"free lunch with vanishing risk\").\n\nFor a semimartingale \"S\", let formula_1 where a strategy is admissible if it is permitted by the market. Then define formula_2. \"S\" is said to satisfy \"no free lunch with vanishing risk\" if formula_3 such that formula_4 is the closure of \"C\" in the norm topology of formula_5.\n\nIf formula_6 is a semimartingale with values in formula_7 then \"S\" does not allow for a free lunch with vanishing risk if and only if there exists an equivalent martingale measure formula_8 such that \"S\" is a sigma-martingale under formula_8.\n"}
{"id": "739601", "url": "https://en.wikipedia.org/wiki?curid=739601", "title": "Part III of the Mathematical Tripos", "text": "Part III of the Mathematical Tripos\n\nPart III of the Mathematical Tripos (officially Master of Mathematics/Master of Advanced Study) is a one-year Masters-level taught course in mathematics offered at the Faculty of Mathematics, University of Cambridge. It is regarded as one of the hardest and most intensive mathematics courses in the world and is taken by approximately 200 students each year. Roughly one third of the students take the course as a fourth year of mathematical study at Cambridge (after Parts IA, IB, and II), whilst the remaining two thirds take the course as a one-year course.\n\nThe Smith's Prize Examination was founded by bequest of Robert Smith upon his death in 1768 to encourage the study of more advanced mathematics than that found in the undergraduate course. T. W. Körner notes\n\nIn 1883 this was replaced by an exam called \"Part III\" and the Smith's Prize awarded for an essay rather than examination. In 1886 this exam was renamed \"Part II\", and later in 1909 \"Part II, Schedule B\". In 1934 it was again renamed \"Part III\". \n\nIn the 1980s the \"Certificate of Advanced Study in Mathematics\" was introduced; for those students successfully completing Part III of the Mathematical Tripos in Easter Term 2011 CASM was replaced by two new degrees, the \"Master of Mathematics\" (M.Math.) and \"Master of Advanced Study\" (M.A.St.). All who have passed the course since 1962 are entitled to these new degrees. The first retrospective M.Math and M.A.St. degrees were conferred as part of a celebration of the University's 800th anniversary. The course is often still referred to as Part III.\n\nStudents who have completed their undergraduate degree at Cambridge will be awarded both a Bachelor of Arts (B.A.) and the Master of Mathematics (M.Math.) degree for four years of study, provided they have not previously graduated with a B.A. This allows Cambridge graduates to remain eligible for government funding for the course. Progression from Part II of the Mathematical Tripos to Part III normally requires either a first in Part II or very good performances in Parts IB and Part II. \nStudents who complete Part III of the Mathematical Tripos, but did not complete undergraduate studies at Cambridge (or have previously graduated with a B.A.) will be awarded the Master of Advanced Study (M.A.St.) in Mathematics degree for the one-year course.\n\nThe program previously resulted in a Certificate of Advanced Study in Mathematics instead of a Master's degree.\n\nThe course lasts one year, divided into three eight-week terms. There are a wide variety of lectures on both pure and applied maths, mostly concentrated in the first two terms. The third term is primarily for examinations (and revision for said examinations) which, together with the option of writing a part III essay (introduced in the 1970s, a miniature thesis of sorts, often in the form of a literature review), determine one's final grade entirely.\n\nThe grades available are Fail, Pass (Honours), Merit, and Distinction (the Merit grade was introduced in 2000). Cambridge recognises that in Part III of the mathematical tripos a merit is equivalent to a First Class in the other parts of the Tripos. The level of achievement required for a distinction is yet higher.\nTraditionally, results are announced in the University's Senate House. Standing on the balcony, the examiner reads out the class results for each student, and printed copies of the results are then thrown to the audience below. The students' exact rankings are no longer announced, but highest-ranked student is still identified, nowadays by the tipping of the examiner's academic hat when the relevant name is read out.\n\nIn addition to the grades, there are five associated prizes. Four of these may be awarded at the discretion of the examiners: the Mayhew Prize for Applied Mathematics, the Tyson Medal for mathematics and astronomy, the Bartlett Prize for applied probability and the Wishart Prize for statistics. Several notable astronomers and astrophysicists have been awarded the Tyson Medal in the history of Part III maths, including Jayant Narlikar, Ray Lyttleton and Edmund Whittaker. In addition, the Thomas Bond Sprague Prize\nis awarded by the Rollo Davidson Trust for distinguished performance in actuarial science, \nfinance, insurance, mathematics of operational research, probability, risk\nand statistics.\n"}
{"id": "32968918", "url": "https://en.wikipedia.org/wiki?curid=32968918", "title": "Patrick C. Fischer", "text": "Patrick C. Fischer\n\nPatrick Carl Fischer (December 3, 1935 – August 26, 2011) was an American computer scientist, a noted researcher in computational complexity theory and database theory, and a target of the Unabomber.\n\nFischer was born December 3, 1935, in St. Louis, Missouri. His father, Carl H. Fischer, became a professor of actuarial mathematics at the University of Michigan in 1941, and the family moved to Ann Arbor, Michigan where he grew up. Fischer himself went to the University of Michigan, receiving a bachelor's degree in 1957 and an MBA in 1958. He went on to graduate studies at the Massachusetts Institute of Technology, earning a Ph.D. in 1962 under the supervision of Hartley Rogers, Jr., with a thesis on the subject of recursion theory.\n\nAfter receiving his Ph.D. in 1962, Fischer joined the faculty of Harvard University as an assistant professor of applied mathematics; his students at Harvard included Albert R. Meyer, through whom Fischer has over 250 academic descendants. as well as noted computer scientists Dennis Ritchie and Arnold L. Rosenberg. In 1965, he moved to a tenured position as associate professor of computer science at Cornell University. After teaching at the University of British Columbia from 1967 to 1968 (where he met his second wife Charlotte Froese) he moved to the University of Waterloo where he became a professor of applied analysis and computer science. At Waterloo, he was department chair from 1972 to 1974. He then moved to Pennsylvania State University in 1974, where he headed the computer science department, and moved again to Vanderbilt University as department chair in 1980. He taught at Vanderbilt for 18 years, and was chair for 15 years. He retired in 1998, and died of stomach cancer on August 26, 2011 in Rockville, Maryland.\n\nLike his father, Fischer became a fellow of the Society of Actuaries.\nFischer's second wife, Charlotte Froese Fischer, was also a computer science professor at Vanderbilt University and the University of British Columbia, and his brother, Michael J. Fischer, is a computer science professor at Yale University.\n\nFischer's thesis research concerned the effects of different models of computation on the efficiency of solving problems. For instance, he showed how to generate the sequence of prime numbers using a one-dimensional cellular automaton, based on earlier solutions to the firing squad synchronization problem, and his work in this area set the foundation for much later work on parallel algorithms. WIth Meyer and Rosenberg, Fischer performed influential early research on counter machines, showing that they obeyed time hierarchy and space hierarchy theorems analogous to those for Turing machines.\n\nFischer was an early leader in the field of computational complexity, and helped establish theoretical computer science as a discipline separate from mathematics and electrical engineering. He was the first chair of SIGACT, the Special Interest Group on Algorithms and Computation Theory of the Association for Computing Machinery, which he founded in 1968. He also founded the annual Symposium on Theory of Computing, which together with the Symposium on Foundations of Computer Science is one of the two flagship conferences in theoretical computer science, and he served five times as chair of the conference.\n\nIn the 1980s, Fischer's research interests shifted to database theory. His research in that area included the study of the semantics of databases, metadata, and incomplete information. Fischer did important work defining the nested relational model of databases, in which the values in the cells of a relational database may themselves be relations, and his work on the mathematical foundations of database query languages became central to the databases now used by major web servers worldwide.\n\nFischer was also an expert in information systems and their use by educational institutions.\n\nTed Kaczynski, known as the Unabomber, was a graduate student of mathematics at the University of Michigan, where Fischer's father was a professor. In 1982, Kaczynski sent the fifth of his mail bombs to Fischer, at his Penn State address; it was forwarded to Vanderbilt, where it was opened on May 5 by Fischer's secretary, Janet Smith, who was hospitalized for three weeks after the attack. Fischer claimed not to have ever met Kaczynski, and speculated that he was targeted because he \"went from pure math to theoretical computer science.\"\n\nKaczynski was not apprehended until 1996, by which time the statute of limitations on the 1982 bombing had expired, so he was never prosecuted for it.\n"}
{"id": "35077006", "url": "https://en.wikipedia.org/wiki?curid=35077006", "title": "Paul Baum (mathematician)", "text": "Paul Baum (mathematician)\n\nPaul Frank Baum (born 1936) is an American mathematician, the Evan Pugh Professor of Mathematics at Pennsylvania State University. He is known for formulating the Baum–Connes conjecture with Alain Connes in the early 1980s.\n\nBaum studied at Harvard University, earning a bachelor's degree \"summa cum laude\" in 1958. He went on to Princeton University for his graduate studies, completing his Ph.D. in 1963 under the supervision of John Coleman Moore and Norman Steenrod. He was several times a visiting scholar at the Institute for Advanced Study (1964–65, 1976–77, 2004) After several visiting positions and an assistant professorship at Princeton, he moved to Brown University in 1967, and remained there until 1987 when he moved to Penn State. He became a distinguished professor in 1991 and was given his named chair in 1996.\n\nIn 2007, a meeting in honor of his 70th birthday was held in Warsaw by the Polish Academy of Sciences. In 2011, the University of Colorado gave him an honorary doctorate. In 2012 he became a fellow of the American Mathematical Society.\n"}
{"id": "11649911", "url": "https://en.wikipedia.org/wiki?curid=11649911", "title": "Prevalent and shy sets", "text": "Prevalent and shy sets\n\nIn mathematics, the notions of prevalence and shyness are notions of \"almost everywhere\" and \"measure zero\" that are well-suited to the study of infinite-dimensional spaces and make use of the translation-invariant Lebesgue measure on finite-dimensional real spaces. The term \"shy\" was suggested by the American mathematician John Milnor.\n\nLet \"V\" be a real topological vector space and let \"S\" be a Borel-measurable subset of \"V\". \"S\" is said to be prevalent if there exists a finite-dimensional subspace \"P\" of \"V\", called the probe set, such that for all \"v\" ∈ \"V\" we have \"v\" + \"p\" ∈ \"S\" for \"λ\"-almost all \"p\" ∈ \"P\", where \"λ\" denotes the dim(\"P\")-dimensional Lebesgue measure on \"P\". Put another way, for every \"v\" ∈ \"V\", Lebesgue-almost every point of the hyperplane \"v\" + \"P\" lies in \"S\".\n\nA non-Borel subset of \"V\" is said to be prevalent if it contains a prevalent Borel subset.\n\nA Borel subset of \"V\" is said to be shy if its complement is prevalent; a non-Borel subset of \"V\" is said to be shy if it is contained within a shy Borel subset.\n\nAn alternative, and slightly more general, definition is to define a set \"S\" to be shy if there exists a transverse measure for \"S\" (other than the trivial measure).\n\nA subset \"S\" of \"V\" is said to be locally shy if every point \"v\" ∈ \"V\" has a neighbourhood \"N\" whose intersection with \"S\" is a shy set. \"S\" is said to be locally prevalent if its complement is locally shy.\n\n\nIn the following, \"almost every\" is taken to mean that the stated property holds of a prevalent subset of the space in question.\n\n\n\n\n"}
{"id": "491442", "url": "https://en.wikipedia.org/wiki?curid=491442", "title": "Prior Analytics", "text": "Prior Analytics\n\nThe Prior Analytics (; ) is Aristotle's work on deductive reasoning, which is known as his syllogistic. Being one of the six extant Aristotelian writings on logic and scientific method, it is part of what later Peripatetics called the \"Organon\". Modern work on Aristotle's logic builds on the tradition started in 1951 with the establishment by Jan Lukasiewicz of a revolutionary paradigm. The Jan Lukasiewicz approach was replaced in the early 1970s in a series of papers by John Corcoran and Timothy Smiley —which inform modern translations of \"Prior Analytics\" by Robin Smith in 1989 and Gisela Striker in 2009.\n\nThe term \"analytics\" comes from the Greek words ἀναλυτός (\"analytos\" \"solvable\") and ἀναλύω (\"analyo\" \"to solve\", literally \"to loose\"). However, in Aristotle's corpus, there are distinguishable differences in the meaning of ἀναλύω and its cognates. There is also the possibility that Aristotle may have borrowed his use of the word \"analysis\" from his teacher Plato. On the other hand, the meaning that best fits the \"Analytics\" is one derived from the study of Geometry and this meaning is very close to what Aristotle calls έπιστήμη \"episteme\", knowing the reasoned facts. Therefore, Analysis is the process of finding the reasoned facts.\n\nAristotle's \"Prior Analytics\" represents the first time in history when Logic is scientifically investigated. On those grounds alone, Aristotle could be considered the Father of Logic for as he himself says in \"Sophistical Refutations\", \"... When it comes to this subject, it is not the case that part had been worked out before in advance and part had not; instead, nothing existed at all.\"\n\nA problem in meaning arises in the study of \"Prior Analytics\" for the word \"syllogism\" as used by Aristotle in general does not carry the same narrow connotation as it does at present; Aristotle defines this term in a way that would apply to a wide range of valid arguments. Some scholars prefer to use the word \"deduction\" instead as the meaning given by Aristotle to the Greek word συλλογισμός \"syllogismos\". At present, \"syllogism\" is used exclusively as the method used to reach a conclusion which is really the narrow sense in which it is used in the Prior Analytics dealing as it does with a much narrower class of arguments closely resembling the \"syllogisms\" of traditional logic texts: two premises followed by a conclusion each of which is a categorial sentence containing all together three terms, two extremes which appear in the conclusion and one middle term which appears in both premises but not in the conclusion. In the \"Analytics\" then, \"Prior Analytics\" is the first theoretical part dealing with the science of deduction and the \"Posterior Analytics\" is the second demonstratively practical part. \"Prior Analytics\" gives an account of deductions in general narrowed down to three basic syllogisms while \"Posterior Analytics\" deals with demonstration.\n\nIn the \"Prior Analytics\", Aristotle defines syllogism as \"... A deduction in a discourse in which, certain things being supposed, something different from the things supposed results of necessity because these things are so.\" In modern times, this definition has led to a debate as to how the word \"syllogism\" should be interpreted. Scholars Jan Lukasiewicz, Józef Maria Bocheński and Günther Patzig have sided with the Protasis-Apodosis dichotomy while John Corcoran prefers to consider a syllogism as simply a deduction.\n\nIn the third century AD, Alexander of Aphrodisias's commentary on the \"Prior Analytics\" is the oldest extant and one of the best of the ancient tradition and is available in the English language.\n\nIn the sixth century, Boethius composed the first known Latin translation of the \"Prior Analytics\". No Westerner between Boethius and Bernard of Utrecht is known to have read the \"Prior Analytics\". The so-called \"Anonymus Aurelianensis III\" from the second half of the twelfth century is the first extant Latin commentary, or rather fragment of a commentary.\n\nThe \"Prior Analytics\" represents the first formal study of logic, where logic is understood as the study of arguments. An argument is a series of true or false statements which lead to a true or false conclusion. In the \"Prior Analytics\", Aristotle identifies valid and invalid forms of arguments called syllogisms. A syllogism is an argument that consists of at least three sentences: at least two premises and a conclusion. Although Aristotles does not call them \"categorical sentences,\" tradition does; he deals with them briefly in the \"Analytics\" and more extensively in \"On Interpretation\". Each proposition (statement that is a thought of the kind expressible by a declarative sentence) of a syllogism is a categorical sentence which has a subject and a predicate connected by a verb. The usual way of connecting the subject and predicate of a categorical sentence as Aristotle does in \"On Interpretation\" is by using a linking verb e.g. P is S. However, in the Prior Analytics Aristotle rejects the usual form in favor of three of his inventions: 1) P belongs to S, 2) P is predicated of S and 3) P is said of S. Aristotle does not explain why he introduces these innovative expressions but scholars conjecture that the reason may have been that it facilitates the use of letters instead of terms avoiding the ambiguity that results in Greek when letters are used with the linking verb. In his formulation of syllogistic propositions, instead of the copula (\"All/some... are/are not...\"), Aristotle uses the expression, \"... belongs to/does not belong to all/some...\" or \"... is said/is not said of all/some...\" There are four different types of categorical sentences: universal affirmative (A), particular affirmative (I), universal negative (E) and particular negative (O).\n\n\nA method of symbolization that originated and was used in the Middle Ages greatly simplifies the study of the Prior Analytics.\nFollowing this tradition then, let:\n\na = belongs to every\n\ne = belongs to no\n\ni = belongs to some\n\no = does not belong to some\n\nCategorical sentences may then be abbreviated as follows:\n\nAaB = A belongs to every B (Every B is A)\n\nAeB = A belongs to no B (No B is A)\n\nAiB = A belongs to some B (Some B is A)\n\nAoB = A does not belong to some B (Some B is not A)\n\nFrom the viewpoint of modern logic, only a few types of sentences can be represented in this way.\n\nDepending on the position of the middle term, Aristotle divides the syllogism into three kinds: Syllogism in the first, second and third figure. If the Middle Term is subject of one premise and predicate of the other, the premises are in the First Figure. If the Middle Term is predicate of both premises, the premises are in the Second Figure. If the Middle Term is subject of both premises, the premises are in the Third Figure.\n\nSymbolically, the Three Figures may be represented as follows:\n\nIn the \"Prior Analytics\" translated by A. J. Jenkins as it appears in volume 8 of the Great Books of the Western World, Aristotle says of the First Figure: \"... If A is predicated of all B, and B of all C, A must be predicated of all C.\" In the \"Prior Analytics\" translated by Robin Smith, Aristotle says of the first figure: \"... For if A is predicated of every B and B of every C, it is necessary for A to be predicated of every C.\"\n\nTaking a = is predicated of all = is predicated of every, and using the symbolical method used in the Middle Ages, then the first figure is simplified to:\n\nIf AaB\n\nand BaC\n\nthen AaC.\n\nOr what amounts to the same thing:\n\nAaB, BaC; therefore AaC\n\nWhen the four syllogistic propositions, a, e, i, o are placed in the first figure, Aristotle comes up with the following valid forms of deduction for the first figure:\n\nAaB, BaC; therefore, AaC\n\nAeB, BaC; therefore, AeC\n\nAaB, BiC; therefore, AiC\n\nAeB, BiC; therefore, AoC\n\nIn the Middle Ages, for mnemonic reasons they were called respectively \"Barbara\", \"Celarent\", \"Darii\" and \"Ferio\".\n\nThe difference between the first figure and the other two figures is that the syllogism of the first figure is complete while that of the second and fourth is not. [?? and the third?? something wrong here.] This is important in Aristotle's theory of the syllogism for the first figure is axiomatic while the second and third require proof. The proof of the second and third figure always leads back to the first figure.\n\nThis is what Robin Smith says in English that Aristotle said in Ancient Greek: \"... If M belongs to every N but to no X, then neither will N belong to any X. For if M belongs to no X, neither does X belong to any M; but M belonged to every N; therefore, X will belong to no N (for the first figure has again come about).\"\n\nThe above statement can be simplified by using the symbolical method used in the Middle Ages:\n\nIf MaN\n\nbut MeX\n\nthen NeX.\n\nFor if MeX\n\nthen XeM\n\nbut MaN\n\ntherefore XeN.\n\nWhen the four syllogistic propositions, a, e, i, o are placed in the second figure, Aristotle comes up with the following valid forms of deduction for the second figure:\n\nMaN, MeX; therefore NeX\n\nMeN, MaX; therefore NeX\n\nMeN, MiX; therefore NoX\n\nMaN, MoX; therefore NoX\n\nIn the Middle Ages, for mnemonic resons they were called respectively \"Camestres\", \"Cesare\", \"Festino\" and \"Baroco\".\n\nAristotle says in the Prior Analytics, \"... If one term belongs to all and another to none of the same thing, or if they both belong to all or none of it, I call such figure the third.\" Referring to universal terms, \"... then when both P and R belongs to every S, it results of necessity that P will belong to some R.\"\n\nSimplifying:\n\nIf PaS\n\nand RaS\n\nthen PiR.\n\nWhen the four syllogistic propositions, a, e, i, o are placed in the third figure, Aristotle develops six more valid forms of deduction:\n\nPaS, RaS; therefore PiR\n\nPeS, RaS; therefore PoR\n\nPiS, RaS; therefore PiR\n\nPaS, RiS; therefore PiR\n\nPoS, RaS; therefore PoR\n\nPeS, RiS; therefore PoR\n\nIn the Middle Ages, for mnemonic reasons, these six forms were called respectively: \"Darapti\", \"Felapton\", \"Disamis\", \"Datisi\", \"Bocardo\" and \"Ferison\".\n\n\"In Aristotelian syllogistic (\"Prior Analytics\", Bk I Caps 4-7), syllogisms are divided into three figures according to the position of the middle term in the two premises. The fourth figure, in which the middle term is the predicate in the major premise and the subject in the minor, was added by Aristotle's pupil Theophrastus and does not occur in Aristotle's work, although there is evidence that Aristotle knew of fourth-figure syllogisms.\"\n\nGeorge Boole's unwavering acceptance of Aristotle’s logic is emphasized by the historian of logic John Corcoran in an accessible introduction to \"Laws of Thought\" Corcoran also wrote a point-by-point comparison of \"Prior Analytics\" and \"Laws of Thought\". According to Corcoran, Boole fully accepted and endorsed Aristotle’s logic. Boole’s goals were “to go under, over, and beyond” Aristotle’s logic by 1) providing it with mathematical foundations involving equations, 2) extending the class of problems it could treat—from assessing validity to solving equations--, and 3) expanding the range of applications it could handle—e.g. from propositions having only two terms to those having arbitrarily many.\n\nMore specifically, Boole agreed with what Aristotle said; Boole’s ‘disagreements’, if they might be called that, concern what Aristotle did not say. \nFirst, in the realm of foundations, Boole reduced the four propositional forms of Aristotle's logic to formulas in the form of equations—-by itself a revolutionary idea. \nSecond, in the realm of logic’s problems, Boole’s addition of equation solving to logic—-another revolutionary idea—-involved Boole’s doctrine that Aristotle’s rules of inference (the “perfect syllogisms”) must be supplemented by rules for equation solving. \nThird, in the realm of applications, Boole’s system could handle multi-term propositions and arguments whereas Aristotle could handle only two-termed subject-predicate propositions and arguments. For example, Aristotle’s system could not deduce “No quadrangle that is a square is a rectangle that is a rhombus” from “No square that is a quadrangle is a rhombus that is a rectangle” or from “No rhombus that is a rectangle is a square that is a quadrangle”.\n\n\n\n"}
{"id": "6034957", "url": "https://en.wikipedia.org/wiki?curid=6034957", "title": "Ramanujan's lost notebook", "text": "Ramanujan's lost notebook\n\nRamanujan's lost notebook is the manuscript in which the Indian mathematician Srinivasa Ramanujan recorded the mathematical discoveries of the last year (1919–1920) of his life. Its whereabouts were unknown to all but a few mathematicians until it was rediscovered by George Andrews in 1976, in a box of effects of G. N. Watson stored at the Wren Library at Trinity College, Cambridge. The \"notebook\" is not a book, but consists of loose and unordered sheets of paper — \"more than one hundred pages written on 138 sides in Ramanujan's distinctive handwriting. The sheets contained over six hundred mathematical formulas listed consecutively without proofs.\"\nhave published several books in which they give proofs for Ramanujan's formulas included in the notebook. Berndt says of the notebooks' discovery: \"The discovery of this 'Lost Notebook' caused roughly as much stir in the mathematical world as the discovery of Beethoven’s tenth symphony would cause in the musical world.\" \n\nAfter Ramanujan died on April 26, 1920, at the age of 32, his wife gave his notebooks to the University of Madras. On August 30, 1923, the registrar Francis Drewsbury sent much of this material to G. H. Hardy, probably including the lost notebook.\n\nSome time between 1934 and 1947 Hardy probably passed the notebook on to G. N. Watson, who with B. M. Wilson started on the project of editing Ramanujan's notebooks. However, Wilson died in 1935 and Watson seems to have lost interest in the project in the late 1930s. After Watson's death in 1965, J. M. Whittaker examined Watson's papers (which were a complete mess, due to be incinerated in a few days) and found Ramanujan's notebook, which he and R. A. Rankin sent to Trinity College Wren library on December 26, 1968. , following a suggestion by Lucy Slater, found the lost notebook in the spring of 1976 while on a visit to Trinity College. It was published on December 22, 1987, by Narosa publishing house.\n\nGeorge Andrews, an American mathematician, wrote an account of the discovery for the 125th celebration of Ramanujan's birth. In his account, Andrews states that he was already an advanced researcher in fields, such as mock theta functions and hypergeometric series, related closely to works of Ramanujan. In 1970, anticipating a sabbatical, he wrote to British mathematician Lucy Slater. Slater \"intriguingly\" stated in her reply that she had inherited a \"great collection\" of papers from mathematicians such as Watson, Bailey, Jackson and Rogers, which were unsorted, including one of the last by Ramanujan. She also mentioned other papers were held by the Trinity College library. \n\nAlthough unable to travel to Europe in 1970, Andrews became able to do so in 1976, when he was due to attend a European conference in Strasbourg, near the France-Germany border. He obtained permission and support from Slater, from the Trinity College library, and from his professor, Ben Noble, to visit Cambridge after the conference, in order to investigate the \"invaluable\" unpublished writings of Watson \"et al\". Noble agreed, adding that if he could attempt to find a lost paper by James Clerk Maxwell at the same time, it would be appreciated. The library's documents included a list of matters held from Watson's estate. The list included the item: \"A 139 page manuscript by S. Ramanujan on q-series\", containing the work from Ramanujan's final year. \n\nAlthough not labelled as such, the identity of the papers was settled because Ramanujan's final letters to Hardy had referred to the discovery of what Ramanujan called mock theta functions, although without great detail, and the manuscript included what appeared to be his full notes on these.\n\n described the lost notebook in detail. The majority of the formulas are about \"q\"-series and mock theta functions, about a third are about modular equations and singular moduli, and the remaining formulas are mainly about integrals, Dirichlet series, congruences, and asymptotics. The mock theta functions in the notebook have been found to be useful for calculating the entropy of black holes.\n\n\n"}
{"id": "37478359", "url": "https://en.wikipedia.org/wiki?curid=37478359", "title": "Recurrent word", "text": "Recurrent word\n\nIn mathematics, a recurrent word or sequence is an infinite word over a finite alphabet in which every factor occurs infinitely often. An infinite word is recurrent if and only if it is a sesquipower.\n\nA uniformly recurrent word is a recurrent word in which for any given factor \"X\" in the sequence, there is some length \"n\" (often much longer than the length of \"X\") such that \"X\" appears in \"every\" block of length \"n\". The term minimal sequence or almost periodic sequence(Muchnik, Semenov, Ushakov 2003) is also used.\n\n\n"}
{"id": "9608295", "url": "https://en.wikipedia.org/wiki?curid=9608295", "title": "Reed–Muller expansion", "text": "Reed–Muller expansion\n\nIn Boolean logic, a Reed–Muller expansion (or Davio expansion) is a decomposition of a Boolean function.\n\nFor a Boolean function formula_1 we call\n\nthe positive and negative cofactors of formula_3 with respect to formula_4, and\nthe boolean derivation of formula_3 with respect to formula_4, where formula_8 denotes the XOR operator.\n\nThen we have for the Reed–Muller or positive Davio expansion:\n\nThis equation is written in a way that it resembles a Taylor expansion of formula_3 about formula_11. There is a similar decomposition corresponding to an expansion about formula_12 (negative Davio):\n\nRepeated application of the Reed–Muller expansion results in an XOR polynomial in formula_14:\n\nThis representation is unique and sometimes also called Reed–Muller expansion.\n\nE.g. for formula_16 the result would be\n\nwhere \n\nFor formula_19 the result would be\n\nwhere\n\nThis formula_19 case can be given a cubical geometric interpretation (or a graph-theoretic interpretation) as follows: when moving along the edge from formula_23 to formula_24, XOR up the functions of the two end-vertices of the edge in order to obtain the coefficient of formula_25. To move from formula_23 to formula_27 there are two shortest paths: one is a two-edge path passing through formula_24 and the other one a two-edge path passing through formula_29. These two paths encompass four vertices of a square, and XORing up the functions of these four vertices yields the coefficient of formula_30. Finally, to move from formula_23 to formula_32 there are six shortest paths which are three-edge paths, and these six paths encompass all the vertices of the cube, therefore the coefficient of formula_32 can be obtained by XORing up the functions of all eight of the vertices. (The other, unmentioned coefficients can be obtained by symmetry.)\n\nThe shortest paths all involve monotonic changes to the values of the variables, whereas non-shortest paths all involve non-monotonic changes of such variables; or, to put it another way, the shortest paths all have lengths equal to the Hamming distance between the starting and destination vertices. This means that it should be easy to generalize an algorithm for obtaining coefficients from a truth table by XORing up values of the function from appropriate rows of a truth table, even for hyperdimensional cases (formula_34 and above). Between the starting and destination rows of a truth table, some variables have their values remaining fixed: find all the rows of the truth table such that those variables likewise remain fixed at those given values, then XOR up their functions and the result should be the coefficient for the monomial corresponding to the destination row. (In such monomial, include any variable whose value is 1 (at that row) and exclude any variable whose value is 0 (at that row), instead of including the negation of the variable whose value is 0, as in the minterm style.)\n\nSimilar to binary decision diagrams (BDDs), where nodes represent Shannon expansion with respect to the according variable, we can define a \ndecision diagram based on the Reed–Muller expansion. These decision diagrams are called functional BDDs (FBDDs).\n\nThe Reed–Muller expansion can be derived from the XOR-form of the Shannon decomposition, using the identity formula_35:\n\nDerivation of the expansion for formula_37:\n\nDerivation of the second-order boolean derivative:\n\n\n"}
{"id": "23413137", "url": "https://en.wikipedia.org/wiki?curid=23413137", "title": "Saccheri–Legendre theorem", "text": "Saccheri–Legendre theorem\n\nIn absolute geometry, the Saccheri–Legendre theorem states that the sum of the angles in a triangle is at most 180°. Absolute geometry is the geometry obtained from assuming all the axioms that lead to Euclidean geometry with the exception of the axiom that is equivalent to the parallel postulate of Euclid.\n\nThe theorem is named after Giovanni Girolamo Saccheri and Adrien-Marie Legendre.\n\nThe existence of at least one triangle with angle sum of 180 degrees in absolute geometry implies Euclid's parallel postulate. Similarly, the existence of at least one triangle with angle sum of less than 180 degrees implies the characteristic postulate of hyperbolic geometry.\n\nMax Dehn gave an example of a non-Legendrian geometry where the angle sum of a triangle is greater than 180 degrees, and a semi-Euclidean geometry where there is a triangle with an angle sum of 180 degrees but Euclid's parallel postulate fails. In Dehn's geometries the Archimedean axiom does not hold.\n"}
{"id": "4233115", "url": "https://en.wikipedia.org/wiki?curid=4233115", "title": "Scottish Book", "text": "Scottish Book\n\nThe Scottish Book () was a thick notebook used by mathematicians of the Lwów School of Mathematics in Poland for jotting down problems meant to be solved. The notebook was named after the \"Scottish Café\" where it was kept.\n\nOriginally, the mathematicians who gathered at the cafe would write down the problems and equations directly on the cafe's marble table tops, but these would be erased at the end of each day, and so the record of the preceding discussions would be lost. The idea for the book was most likely originally suggested by Stefan Banach, or his wife, Łucja, who purchased a large notebook and left it with the proprietor of the cafe.\n\nThe Scottish Café () was the café in Lwów (now \"Lviv\") where, in the 1930s and 1940s, mathematicians from the Lwów School collaboratively discussed research problems, particularly in functional analysis and topology.\n\nStanislaw Ulam recounts that the tables of the café had marble tops, so they could write in pencil, directly on the table, during their discussions. To keep the results from being lost, and after becoming annoyed with their writing directly on the table tops, Stefan Banach's wife provided the mathematicians with a large notebook, which was used for writing the problems and answers and eventually became known as the \"Scottish Book\". The book—a collection of solved, unsolved, and even probably unsolvable problems—could be borrowed by any of the guests of the café. Solving any of the problems was rewarded with prizes, with the most difficult and challenging problems having expensive prizes (during the Great Depression and on the eve of World War II), such as a bottle of fine brandy.\n\nFor problem 153, which was later recognized as being closely related to Stefan Banach's \"basis problem\", Stanisław Mazur offered the prize of a live goose. This problem was solved only in 1972 by Per Enflo, who was presented with the live goose in a ceremony that was broadcast throughout Poland.\n\nThe café building used to house the at the street address of 27 Taras Shevchenko Prospekt. The original cafe was renovated in May 2014 and contains a copy of the Scottish Book.\n\nA total of 193 problems were written down in the book. Stanisław Mazur contributed a total of 43 problems, 24 of them as a single author and 19 together with Stefan Banach. Banach himself wrote 14, plus another 11 with Stanislaw Ulam and Mazur. Ulam wrote 40 problems and additional 15 ones with others.\n\nDuring the Soviet occupation of Lwów, several Russian mathematicians visited the city and also added problems to the book.\n\nHugo Steinhaus contributed the last one in May 1941 (other sources give March 1941), which involved a question about the likely distribution of matches within a matchbox—a problem motivated by Banach's habit of chain smoking cigarettes—shortly before the German attack on the Soviet Union.\n\nAfter World War II, an English translation annotated by Ulam was published by Los Alamos National Laboratory in 1957. After World War II, Steinhaus at the University of Wrocław revived the tradition of the Scottish book by initiating \"The New Scottish Book\".\n\nThe following mathematicians were associated with the Lwów School of Mathematics or contributed to \"The Scottish Book\":\n\n\n"}
{"id": "30872676", "url": "https://en.wikipedia.org/wiki?curid=30872676", "title": "Tally stick", "text": "Tally stick\n\nA tally stick (or simply tally) was an ancient memory aid device used to record and document numbers, quantities, or even messages. Tally sticks first appear as animal bones carved with notches during the Upper Paleolithic; a notable example is the Ishango Bone. Historical reference is made by Pliny the Elder (AD 23–79) about the best wood to use for tallies, and by Marco Polo (1254–1324) who mentions the use of the tally in China. Tallies have been used for numerous purposes such as messaging and scheduling, and especially in financial and legal transactions, \n\nPrincipally, there are two different kinds of tally sticks: the single tally and the split tally. A common form of the same kind of primitive counting device is seen in various kinds of prayer beads.\n\nA number of anthropological artefacts have been conjectured to be tally sticks:\n\n\nThe single tally stick was an elongated piece of bone, ivory, wood, or stone which is marked with a system of notches (see: Tally marks). The single tally stick serves predominantly mnemonic purposes. Related to the single tally concept are messenger sticks (e.g., Inuit tribes), the knotted cords, \"khipus\" or \"quipus\", as used by the Inca. Herodotus (c. 485–425 BC) reported the use of a knotted cord by Darius I of Persia (c. 521–486 BC).\n\nThe split tally was a technique which became common in medieval Europe, which was constantly short of money (coins) and predominantly illiterate, in order to record bilateral exchange and debts. A stick (squared hazelwood sticks were most common) was marked with a system of notches and then split lengthwise. This way the two halves both record the same notches and each party to the transaction received one half of the marked stick as proof. Later this technique was refined in various ways and became virtually tamper proof. One of the refinements was to make the two halves of the stick of different lengths. The longer part was called \"stock\" and was given to the party which had advanced money (or other items) to the receiver. The shorter portion of the stick was called \"foil\" and was given to the party which had received the funds or goods. Using this technique each of the parties had an identifiable record of the transaction. The natural irregularities in the surfaces of the tallies where they were split would mean that only the original two halves would fit back together perfectly, and so would verify that they were matching halves of the same transaction. If one party tried to unilaterally change the value of his half of the tally stick by adding more notches, the absence of those notches would be apparent on the other party's tally stick. The split tally was accepted as legal proof in medieval courts and the Napoleonic Code (1804) still makes reference to the tally stick in Article 1333. Along the Danube and in Switzerland the tally was still used in the 20th century in rural economies.\n\nThe most prominent and best recorded use of the split tally stick or \"nick-stick\" being used as a form of currency was when Henry I introduced the tally stick system in medieval England in around 1100. He would accept the tally stick only for taxes, and it was a tool of the Exchequer for the collection of taxes by local sheriffs (tax farmers \"farming the shire\") for seven centuries. The split tally of the Exchequer was in continuous use until 1826. In 1834 tally sticks representing six centuries worth of financial records were ordered to be burned in a stove in the Houses of Parliament. The resulting fire set the chimney ablaze and then spread until most of the building was destroyed. This event was described by Charles Dickens in an 1855 article on administrative reform.\n\nThe system of tally marks of the Exchequer is described in \"The Dialogue Concerning the Exchequer\" (see external links below) as follows:\n\nThe cuts were made the full width of the stick so that, after splitting, the portion kept by the issuer (the \"stock\") exactly matched the piece (the \"foil\") given as a receipt. Each stick had to have the details of the transaction written on it, in ink, to make it a valid record.\n\nRoyal tallies (debt of the Crown) also played a role in the formation of the Bank of England at the end of the 17th century. In 1697, the bank issued £1 million worth of stock in exchange for £800,000 worth of tallies at par and £200,000 in bank notes. This new stock was said to be \"engrafted\". The government promised not only to pay the Bank interest on the tallies subscribed but to redeem them over a period of years. The \"engrafted\" stock was then cancelled simultaneously with the redemption.\n\nTally sticks feature in the design of the entrance gates to The National Archives at Kew.\n\n\n\n\n"}
{"id": "3646268", "url": "https://en.wikipedia.org/wiki?curid=3646268", "title": "Unit type", "text": "Unit type\n\nIn the area of mathematical logic and computer science known as type theory, a unit type is a type that allows only one value (and thus can hold no information). The carrier (underlying set) associated with a unit type can be any singleton set. There is an isomorphism between any two such sets, so it is customary to talk about \"the\" unit type and ignore the details of its value. One may also regard the unit type as the type of 0-tuples, i.e. the product of no types.\n\nThe unit type is the terminal object in the category of types and typed functions. It should not be confused with the \"zero\" or bottom type, which allows \"no\" values and is the initial object in this category. Similarly, the Boolean is the type with \"two\" values.\n\nThe unit type is implemented in most functional programming languages. The void type that is used in some imperative programming languages serves some of its functions, but because its carrier set is empty, it has some limitations (as detailed below).\n\nSeveral computer programming languages provide a unit type to specify the result type of a function with the sole purpose of causing a side effect, and the argument type of a function that does not require arguments. \n\nIn C, C++, C#, D and Java, codice_19 is used to designate a function that does not return anything useful, or a function that accepts no arguments. The unit type in C is conceptually similar to an empty codice_20, but a struct without members is not allowed in the C language specification. Instead, 'codice_19' is used in a manner that simulates some, but not all, of the properties of the unit type, as detailed below. Like most imperative languages, C allows functions that do not return a value; these are specified as having the void return type. Such functions are called procedures in other imperative languages like Pascal, where a syntactic distinction, instead of type-system distinction, is made between functions and procedures.\n\nThe first notable difference between a true unit type and the void type is that the unit type may always be the type of the argument to a function, but the void type cannot be the type of an argument in C, despite the fact that it may appear as the sole argument in the list. This problem is best illustrated by the following program, which is a compile-time error in C:\n\nThis issue does not arise in most programming practice in C, because since the codice_19 type carries no information, it is useless to pass it anyway; but it may arise in generic programming, such as C++ templates, where codice_19 must be treated differently from other types. In C++ however, empty classes are allowed, so it is possible to implement a real unit type; the above example becomes compilable as:\n\nThe second notable difference is that the void type is special and can never be stored in a record type, i.e. in a struct or a class in C/C++. In contrast, the unit type can be stored in records in functional programming languages, i.e. it can appear as the type of a field; the above implementation of the unit type in C++ can also be stored. While this may seem a useless feature, it does allow one for instance to elegantly implement a set as a map to the unit type; in the absence of a unit type, one can still implement a set this way by storing some dummy value of another type for each key.\n\nIn Java Generics, type parameters must be reference types. The wrapper type codice_9 is often used when a unit type parameter is needed. Although the codice_9 type can never have any instances, it does have one value, codice_27 (like all other reference types), so it acts as a unit type. In practice, any other non-instantiable type, e.g. codice_28, can also be used for this purpose, since they also have exactly one value, codice_27.\n\n\n"}
{"id": "22863030", "url": "https://en.wikipedia.org/wiki?curid=22863030", "title": "Universal Systems Language", "text": "Universal Systems Language\n\nUniversal Systems Language (USL) is a modeling language and formal method for the specification and design of software and other complex systems. It was designed by Margaret Hamilton based on her experiences writing flight software for the Apollo program. The language is implemented through the 001 Tool Suite software by Hamilton Technologies, Inc. USL evolved from 001AXES which in turn evolved from AXES all of which are based on Hamilton's axioms of control. The 001 Tool Suite uses the preventative concept of Development Before the Fact (DBTF) for its life-cycle development process. DBTF eliminates errors as early as possible during the development process removing the need to look for errors after-the-fact.\n\nUSL was inspired by Hamilton's recognition of patterns or categories of errors occurring during Apollo software development. Errors at the interfaces between subsystem boundaries accounted for the majority of errors and were often the most subtle and most difficult to find. Each interface error was placed into a category identifying the means to prevent it by way of system definition. This process led to a set of six axioms, forming the basis for a mathematical constructive logical theory of control for designing systems that would eliminate entire classes of errors just by the way a system is defined.\n\nCertain correctness guarantees are embedded in the USL grammar. In contrast to reactive approaches to program verification, testing for errors late into the life cycle, USL's development-before-the-fact philosophy is preventive, not allowing errors in the first place. A USL definition models both its application (for example, an avionics or banking system) and properties of control into its own life cycle. Providing a mathematical framework within which objects, their interactions, and their relationships can be captured, USL – a metalanguage – has \"metamechanisms\" for defining systems. USL's philosophy is that all objects are recursively reusable and reliable; reliable systems are defined in terms of reliable systems; only reliable systems are used as building blocks; and only reliable systems are used as mechanisms to integrate these building blocks to form a new system. Designers can then use the new system, along with more primitive ones, to define (and build) more comprehensive reliable systems. If a system is reliable, all the objects in all its levels and layers are reliable.\n\nUSL is regarded by some users as more user-friendly than other formal systems. It is not only a formalism for software, but also defines ontologies for common elements of problem domains, such as physical space and event timing.\n\nA systems philosophy formalism for representing the logic of the control of systems, USL is based on a set of axioms of a general systems control theory with formal rules for its application. At the base of every USL system is a set of six axioms and the assumption of a universal set of objects. The axioms provide the formal foundation for a USL \"hierarchy\" – referred to as a map, which is a tree of control that spans networks of relations between objects. Explicit rules for defining a map have been derived from the axioms, where – among other things – structure, behavior, and their integration are captured. Each axiom defines a relation of immediate domination of a parent over its children. The union of these relations is control. Among other things, the axioms establish the relationships of an object for invocation in time and space, input and output (domain and codomain), input access rights and output access rights (domain access rights and codomain access rights), error detection and recovery, and ordering during its developmental and operational states. Every system can ultimately be defined in terms of three primitive control structures, each of which is derived from the six axioms – resulting in a universal semantics for defining systems.\n\nAll representations of a system are defined in terms of a function map (FMap) and a type map (TMap). With USL, all functions in a system and their relationships are defined with a set of FMaps. Similarly, all types in a system and their relationships are defined with a set of TMaps. FMaps represent the dynamic (doing) world of action by capturing functional and temporal (including priority) characteristics. TMaps represent the static (being) world of objects by capturing spatial characteristics – for example, containment of one object by another or relationships between locations of objects in space. FMaps are inherently integrated with TMaps. Three universal primitive structures derived from the set of axioms and non-primitive structures derived ultimately in terms of the primitive structures specify each map. Primitive structures are universal in that they are able to be used to derive new abstract universal structures, functions or types. The process of deriving new objects (i.e., structures, types and functions) is equivalent to the process of deriving new types in a constructive type theory. Primitive functions, corresponding to primitive operations on types defined in a TMap, reside at the bottom nodes of an FMap. Primitive types, each defined by its own set of axioms, reside at the bottom nodes of a TMap. Each primitive function (or type) can be realized as a top node of a map on a lower (more concrete) layer of the system. Resident at every node on a map is the same kind of object (for example, a function on every node of an FMap and a type on a TMap). The object at each node plays multiple roles; for example, the object can serve as a parent (in control of its children) or a child (being controlled by its parent). Whereas each function on an FMap has a \"mapping\" from its input to output (domain to codomain), each type on a TMap has a \"relation\" between its domain and codomain. A structure relates each parent and its children according to the set of rules derived from the axioms of control. A primitive structure provides a relationship of the most primitive form (finest grain) of control. All maps are defined ultimately in terms of the primitive structures and therefore abide by the rules associated with each structure: A parent controls its children to have a dependent (Join), independent (Include), or decision-making relationship (Or).\n\nAny system can be defined completely using only primitive structures, but less primitive structures defined by and derived from the primitive structures – and therefore governed by the control axioms – accelerate the definition and understanding of a system. The defined structure, a form of template-like reuse, provides a mechanism to define a map without explicitly defining some of its elements. An FMap structure has placeholders for variable functions; a TMap structure has placeholders for variable types; a universal structure has placeholders for functions or types. Async is an example of a real-time, distributed, communicating FMap structure with both asynchronous and synchronous behavior. An example of a TMap structure is TreeOf, a collection of the same type of objects ordered using a tree indexing system. Each TMap structure assumes its own set of possible relations for its parent and children types. Abstract types decomposed with the same TMap structure inherit the same primitive operations and therefore the same behavior (each of which is available to FMaps that have access to members of each of its TMap's types).\n\nThe process of developing a software system with USL together with its automation, the 001 Tool Suite (001), is as follows: define the system with USL, automatically analyze the definition with 001's analyzer to ensure that USL was used correctly, automatically generate much of the design and all of the implementation code with 001's generator. USL can be used to lend its formal support to other languages.\n\n\n\n"}
{"id": "56688455", "url": "https://en.wikipedia.org/wiki?curid=56688455", "title": "Uta Merzbach", "text": "Uta Merzbach\n\nUta Caecilia Merzbach (February 9, 1933 – June 27, 2017) was a German-American historian of mathematics who became the first curator of mathematical instruments at the Smithsonian Institution.\n\nMerzbach was born in Berlin, where her mother was a philologist and her father was an economist who worked for the Reich Association of Jews in Germany during World War II. The Nazi government closed the association in June 1943; they arrested the family, along with other leading members of the association, and sent them to the Theresienstadt concentration camp on August 4, 1943. The Merzbachs survived the war and the camp, and after living for a year in a refugee camp in Deggendorf they moved to Georgetown, Texas in 1946, where her father found a faculty position at Southwestern University.\n\nAfter high school in Brownwood, Texas, Merzbach entered Southwestern, but transferred after two years to the University of Texas at Austin, where she graduated in 1952 with a bachelor's degree in mathematics. In 1954, she earned a master's degree there, also in mathematics.\nMerzbach became a school teacher, but soon returned to graduate study at Harvard University. She completed her Ph.D. at Harvard in 1965. Her dissertation, \"Quantity of Structure: Development of Modern Algebraic Concepts from Leibniz to Dedekind\", combined mathematics and the history of mathematics; it was jointly supervised by mathematician Garrett Birkhoff and historian of science I. Bernard Cohen.\n\nMerzbach joined the Smithsonian as an associate curator in 1964, and served there until 1988 in the National Museum of American History. As well as collecting mathematical objects at the Smithsonian, she also collected interviews with many of the pioneers of computing. In 1991, she became the co-author of the second edition of \"A History of Mathematics\", originally published in 1968 by Carl Benjamin Boyer. After her retirement she returned to Georgetown, Texas, where she died in 2017.\n"}
{"id": "758386", "url": "https://en.wikipedia.org/wiki?curid=758386", "title": "Vector fields in cylindrical and spherical coordinates", "text": "Vector fields in cylindrical and spherical coordinates\n\nNOTE: This page uses common physics notation for spherical coordinates, in which formula_1 is the angle between the \"z\" axis and the radius vector connecting the origin to the point in question, while formula_2 is the angle between the projection of the radius vector onto the \"x-y\" plane and the \"x\" axis. Several other definitions are in use, and so care must be taken in comparing different sources.\n\nVectors are defined in cylindrical coordinates by (\"ρ\", φ, \"z\"), where\n\n(\"ρ\", φ, \"z\") is given in cartesian coordinates by:\n\nor inversely by:\n\nAny vector field can be written in terms of the unit vectors as:\nThe cylindrical unit vectors are related to the cartesian unit vectors by:\n\nTo find out how the vector field A changes in time we calculate the time derivatives.\nFor this purpose we use Newton's notation for the time derivative (formula_7).\nIn cartesian coordinates this is simply:\n\nHowever, in cylindrical coordinates this becomes:\n\nWe need the time derivatives of the unit vectors. \nThey are given by:\n\nSo the time derivative simplifies to:\n\nThe second time derivative is of interest in physics, as it is found in equations of motion for classical mechanical systems.\nThe second time derivative of a vector field in cylindrical coordinates is given by:\n\nTo understand this expression, we substitute A = P, where p is the vector (r, θ, z).\n\nThis means that formula_13.\n\nAfter substituting we get:\n\nIn mechanics, the terms of this expression are called:\n\nSee also: Centripetal force, Angular acceleration, Coriolis effect.\n\nVectors are defined in spherical coordinates by (ρ,θ,φ), where\n\n(ρ,θ,φ) is given in Cartesian coordinates by:\n\nor inversely by:\n\nAny vector field can be written in terms of the unit vectors as:\nThe spherical unit vectors are related to the cartesian unit vectors by:\n\nSo the cartesian unit vectors are related to the spherical unit vectors by:\n\nTo find out how the vector field A changes in time we calculate the time derivatives.\nIn cartesian coordinates this is simply:\n\nHowever, in spherical coordinates this becomes:\n\nWe need the time derivatives of the unit vectors. \nThey are given by:\n\nSo the time derivative becomes:\n\n"}
{"id": "4723511", "url": "https://en.wikipedia.org/wiki?curid=4723511", "title": "ΛProlog", "text": "ΛProlog\n\nλProlog, also written lambda Prolog, is a logic programming language featuring polymorphic typing, modular programming, and higher-order programming. These extensions to Prolog are derived from the higher-order hereditary Harrop formulas used to justify the foundations of λProlog. Higher-order quantification, simply typed λ-terms, and higher-order unification gives λProlog the basic supports needed to capture the λ-tree syntax approach to \"higher-order abstract syntax\", an approach to representing syntax that maps object-level bindings to programming language bindings. Programmers in λProlog need not deal with bound variable names: instead various declarative devices are available to deal with binder scopes and their instantiations. Since 1986, λProlog has received numerous implementations. As of 2013, the language and its implementations are still actively being developed.\n\nThe Abella theorem prover has been designed to provide an interactive environment for proving theorems about the declarative core of λProlog.\n\n\n\n\n"}
