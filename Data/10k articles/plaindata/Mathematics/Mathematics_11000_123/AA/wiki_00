{"id": "654760", "url": "https://en.wikipedia.org/wiki?curid=654760", "title": "132 (number)", "text": "132 (number)\n\n132 (one hundred [and] thirty-two) is the natural number following 131 and preceding 133.\n\n132 is the sixth Catalan number. It is a pronic number, the product of 11 and 12. As it has 12 divisors total, 132 is a refactorable number.\n\nIf you take the sum of all 2-digit numbers you can make from 132, you get 132: formula_1. 132 is the smallest number with this property, which is shared by 264, 396 and 35964 (see digit-reassembly number).\nBut there is no number that, when added to the sum of its own digits, sums to 132, therefore 132 is a self number. 132 is also a Harshad number, divisible by the sum of its base-ten digits.\n\n\n\n132 is also:\n\n"}
{"id": "208174", "url": "https://en.wikipedia.org/wiki?curid=208174", "title": "8", "text": "8\n\n8 (eight) is the natural number following 7 and preceding 9.\n\n8 is:\n\nA number is divisible by 8 if its last three digits, when written in decimal, are also divisible by 8, or its last three digits are 0 when written in binary.\n\nThere are a total of eight convex deltahedra.\n\nA polygon with eight sides is an octagon. Figurate numbers representing octagons (including eight) are called octagonal numbers.\n\nA polyhedron with eight faces is an octahedron. A cuboctahedron has as faces six equal squares and eight equal regular triangles.\n\nA cube has eight vertices.\n\nSphenic numbers always have exactly eight divisors.\n\nThe number 8 is involved with a number of interesting mathematical phenomena related to the notion of Bott periodicity. For example, if \"O\"(∞) is the direct limit of the inclusions of real orthogonal groups\nthen\n\nClifford algebras also display a periodicity of 8. For example, the algebra \"Cl\"(\"p\" + 8,\"q\") is isomorphic to the algebra of 16 by 16 matrices with entries in \"Cl\"(\"p\",\"q\"). We also see a period of 8 in the K-theory of spheres and in the representation theory of the rotation groups, the latter giving rise to the 8 by 8 spinorial chessboard. All of these properties are closely related to the properties of the octonions.\n\nThe spin group Spin(8) is the unique such group that exhibits the phenomenon of triality.\n\nThe lowest-dimensional even unimodular lattice is the 8-dimensional E lattice. Even positive definite unimodular lattices exist only in dimensions divisible by 8.\n\nA figure 8 is the common name of a geometric shape, often used in the context of sports, such as skating. Figure-eight turns of a rope or cable around a cleat, pin, or bitt are used to belay something.\n\nEnglish \"eight\", from Old English \"eahta, æhta\", Proto-Germanic \"*ahto\"\nis a direct continuation of Proto-Indo-European \"*oḱtṓ(w)-\", and as such cognate with Greek and Latin \"octo-\", both of which stems are reflected by the English prefix oct(o)-, as in the ordinal adjective \"octaval\" or \"octavary\", the distributive adjective is \"octonary\".\nThe adjective \"octuple\" (Latin \"octu-plus\") may also be used as a noun, meaning \"a set of eight items\"; the diminutive \"octuplet\" is mostly used to refer to eight sibling delivered in one birth.\n\nThe Semitic numeral is based on a root \"*θmn-\", whence Akkadian \"smn-\", Arabic \"ṯmn-\", Hebrew \"šmn-\" etc.\n\nThe Chinese numeral, written (Mandarin: \"bā\"; Cantonese: \"baat\"), is from Old Chinese \"*priāt-\", ultimately from Sino-Tibetan \"b-r-gyat\" or \"b-g-ryat\" which also yielded Tibetan \"brgyat\".\nIt has been argued that, as the cardinal number is the highest number of item that can universally be cognitively processed as a single set, the etymology of the numeral \"eight\" might be the first to be considered composite, either as \"twice four\" or as \"two short of ten\", or similar. \nThe Turkic words for \"eight\" are from a Proto-Turkic stem \"*sekiz\", which has been suggested as originating as a negation of \"eki\" \"two\", as in \"without two fingers\" (i.e., \"two short of ten; two fingers are not being held up\");\nthis same principle is found in Finnic \"*kakte-ksa\", which conveys a meaning of \"two before (ten)\". The Proto-Indo-European reconstruction \"*oḱtṓ(w)-\" itself has been argued as representing an old dual, which would correspond to an original meaning of \"twice four\". \nProponents of this \"quaternary hypothesis\" adduce the numeral \"\", which might be built on the stem \"new-\", meaning \"new\" (indicating the beginning of a \"new set of numerals\" after having counted to eight).\n\nThe modern 8 glyph, like all modern Arabic numerals (other than zero) originates with the Brahmi numerals. \nThe Brahmi numeral for \"eight\" by the 1st century was written in one stroke as a curve └┐ looking like an uppercase H with the bottom half of the left line and the upper half of the right line removed.\nHowever the \"eight\" glyph used in India in the early centuries of the Common Era developed considerable variation, and in some cases took the shape of a single wedge, which was adopted into the Perso-Arabic tradition as ٨ (and also gave rise to the later Devanagari numeral ८; the alternative curved glyph also existed as a variant in Perso-Arabic tradition, where it came to look similar to our glyph \"5\".\n\nThe numerals as used in Al-Andalus by the 10th century were a distinctive western variant of the glyphs used in the Arabic-speaking world, known as \"ghubār\" numerals (\"ghubār\" translating to \"sand table\"). In these numerals, the line of the \"5\"-like glyph used in Indian manuscripts for eight came to be formed in ghubār as a closed loop, which was the \"8\"-shape that became adopted into European use in the 10th century.\n\nJust as in most modern typefaces, in typefaces with text figures the 8 character usually has an ascender, as, for example, in .\n\nThe infinity symbol ∞, described as a \"sideways figure eight\" is unrelated to the \"8\" glyph in origin; it is first used (in the mathematical meaning \"infinity\") in the 17th century, and it may be derived from the Roman numeral for \"one thousand\" CIƆ, or alternatively from the final Greek letter, ω.\n\nThe numeral eight in Greek numerals, developed in Classical Greece by the 5th century BC, was written as Η, the eighth letter of the Greek alphabet.\n\nThe Chinese numeral eight is written in two strokes, ; the glyph is also the 12th Kangxi radical.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "30823252", "url": "https://en.wikipedia.org/wiki?curid=30823252", "title": "Algebraic interior", "text": "Algebraic interior\n\nIn functional analysis, a branch of mathematics, the algebraic interior or radial kernel of a subset of a vector space is a refinement of the concept of the interior. It is the subset of points contained in a given set with respect to which it is absorbing, i.e. the radial points of the set. The elements of the algebraic interior are often referred to as internal points.\n\nFormally, if formula_1 is a linear space then the algebraic interior of formula_2 is\n\nNote that in general formula_4, but if formula_5 is a convex set then formula_6. If formula_5 is a convex set then if formula_8 then formula_9.\n\nIf formula_10 then formula_11, but formula_12 and formula_13.\n\nLet formula_14 then:\n\nLet formula_1 be a topological vector space, formula_21 denote the interior operator, and formula_22 then: \n\n"}
{"id": "17655204", "url": "https://en.wikipedia.org/wiki?curid=17655204", "title": "Automatic semigroup", "text": "Automatic semigroup\n\nIn mathematics, an automatic semigroup is a finitely generated semigroup equipped with several regular languages over an alphabet representing a generating set. One of these languages determines \"canonical forms\" for the elements of the semigroup, the other languages determine if two canonical forms represent elements that differ by multiplication by a generator.\n\nFormally, let formula_1 be a semigroup and formula_2 be a finite set of generators. Then an \"automatic structure\" for formula_1 with respect to formula_2 consists of a regular language formula_5 over formula_2 such that every element of formula_1 has at least one representative in formula_5 and such that for each formula_9, the relation consisting of pairs formula_10 with formula_11 is regular.\n\nThe concept of an automatic semigroup was generalized from automatic groups by Campbell et al. (2001)\n\nUnlike automatic groups (see Epstein et al. 1992), a semigroup may have an automatic structure with respect to one generating set, but not with respect to another. However, if an automatic semigroup has an identity, then it has an automatic structure with respect to any generating set (Duncan et al. 1999).\n\nLike automatic groups, automatic semigroups have word problem solvable in quadratic time. Kambites & Otto (2006) showed that it is undecidable whether an element of an automatic monoid possesses a right inverse.\n\nCain (2006) proved that both cancellativity and left-cancellativity are undecidable for automatic semigroups. On the other hand, right-cancellativity is decidable for automatic semigroups (Silva & Steinberg 2004).\n\nAutomatic structures for groups have an elegant geometric characterization called the \"fellow traveller property\" (Epstein et al. 1992, ch. 2). Automatic structures for semigroups \"possess\" the fellow traveller property but are not in general characterized by it (Campbell et al. 2001). However, the characterization can be generalized to certain 'group-like' classes of semigroups, notably completely simple semigroups (Campbell et al. 2002) and group-embeddable semigroups (Cain et al. 2006).\n\n"}
{"id": "43640244", "url": "https://en.wikipedia.org/wiki?curid=43640244", "title": "Barrier resilience", "text": "Barrier resilience\n\nBarrier resilience is an algorithmic optimization problem in computational geometry motivated by the design of wireless sensor networks, in which one seeks a path through a collection of barriers (often modeled as unit disks) that passes through as few barriers as possible.\n\nThe barrier resilience problem was introduced by (using different terminology) to model the ability of wireless sensor networks to detect intruders robustly when some sensors may become faulty.\nIn this problem, the region under surveillance from each sensor is modeled as a unit disk in the Euclidean plane. An intruder can reach a target region of the plane without detection, if there exists a path in the plane connecting a given start region to the target region without crossing any of the sensor disks. The \"barrier resilience\" of a sensor network is defined to be the minimum, over all paths from the start region to the target region, of the number of sensor disks intersected by the path. The barrier resilience problem is the problem of computing this number by finding an optimal path through the barriers.\n\nA simplification of the problem, which encapsulates most of its essential features, makes the target region be the origin of the plane, and the start region be the set of points outside the convex hull of the sensor disks. In this version of the problem, the goal is to connect the origin to points arbitrarily far from the origin by a path through as few sensor disks as possible.\n\nAnother variation of the problem counts the number of times a path crosses the boundary of a sensor disk. If a path crosses the same disk multiple times, each crossing counts towards the total. The \"barrier thickness\" of a sensor network is the minimum number of crossings of a path from the start region to the target region.\n\nBarrier thickness may be computed in polynomial time by constructing the arrangement of the barriers (the subdivision of the plane formed by overlaying all barrier boundaries) and computing a shortest path in the dual graph of this subdivision.\n\nThe complexity of barrier resilience for unit disk barriers is an open problem. It may be solved by a fixed-parameter tractable algorithm whose time is cubic in the total number of barriers and exponential in the square of the resilience, but it is not known whether it has a fully polynomial time solution.\nThe corresponding problem for barriers of some other shapes, including unit-length line segments or axis-aligned rectangles of aspect ratio close to 1, is known to be NP-hard.\n\nA variation of the barrier resilience problem, studied by , restricts both the sensors and the escape path to a rectangle in the plane. In this variation, the goal is to find a path from the top side of the rectangle to the bottom side that passes through as few of the sensor disks as possible. By applying Menger's theorem to the unit disk graph defined from the barriers, this minimal number of disks can be shown to equal the maximum number of subsets into which all of the disks can be partitioned, such that each subset contains a chain of disks passing all the way from the left to the right side of the rectangle. As showed, this characterization allows the optimal resilience to be computed in polynomial time by transforming the problem into an instance of the maximum flow problem.\n\nFor unit disks with bounded \"ply\" (the maximum number of disks that have a common intersection) there exists a polynomial-time approximation scheme for the resilience, that can be generalized to barrier shapes of the same size as each other with bounded aspect ratios. For unit disks without assuming bounded ply, the problem of computing the resilience may be approximated to within a constant factor, using the fact that for this shape of barrier the optimal path can only cross each barrier a constant number of times, so the barrier thickness and barrier resilience are within a constant factor of each other. Similar methods can be generalized to non-uniform sensors of approximately equal size.\n\n"}
{"id": "31735347", "url": "https://en.wikipedia.org/wiki?curid=31735347", "title": "Beatrice Mabel Cave-Browne-Cave", "text": "Beatrice Mabel Cave-Browne-Cave\n\nBeatrice Mabel Cave-Browne-Cave, MBE (30 May 1874 – 9 July 1947) was an English mathematician who undertook pioneering work in the mathematics of aeronautics.\n\nBeatrice Cave-Browne-Cave was the daughter of Sir Thomas Cave-Browne-Cave (1835–1924; see Cave-Browne-Cave baronets for earlier history of the family) and Blanche Matilda Mary Ann (née Milton). One of six siblings, one of her brothers was Henry Cave-Browne-Cave, a Royal Air Force officer. She was educated at home in Streatham and entered Girton College, Cambridge with her younger sister Frances in 1895. Gaining a second-class degree in the mathematical tripos, part one (1898), she took part two a year later (1899), and was placed in the third class.\n\nAfter eleven years teaching mathematics to girls at a high school in Clapham in south London, in the years just before the First World War she worked under Professor Karl Pearson in the Galton Laboratory at University College, London.\n\nDuring World War I, she carried out original research for the government on the mathematics of aeronautics which remained classified under the Official Secrets Act for fifty years. Elected an associate fellow of the Royal Aeronautical Society in 1919 and awarded an MBE in 1920, she later worked as an assistant to Sir Leonard Bairstow, the Zaharoff Professor of Aviation at Imperial College, London. She retired in 1937, continuing to live in Streatham\n\nBeatrice Mabel Cave-Browne-Cave died on 9 July 1947 at age 73, unmarried.\n"}
{"id": "897558", "url": "https://en.wikipedia.org/wiki?curid=897558", "title": "Beta (finance)", "text": "Beta (finance)\n\nIn finance, the beta (β or beta coefficient) of an investment indicates whether the investment is more or less volatile than the market as a whole. \n\nBeta is a measure of the risk arising from exposure to general market movements as opposed to idiosyncratic factors. The market portfolio of all investable assets has a beta of exactly 1. A beta below 1 can indicate either an investment with lower volatility than the market, or a volatile investment whose price movements are not highly correlated with the market. An example of the first is a treasury bill: the price does not go up or down a lot, so it has a low beta. An example of the second is gold. The price of gold does go up and down a lot, but not in the same direction or at the same time as the market.\n\nA beta greater than 1 generally means that the asset both is volatile and tends to move up and down with the market. An example is a stock in a big technology company. Negative betas are possible for investments that tend to go down when the market goes up, and vice versa. There are few fundamental investments with consistent and significant negative betas, but some derivatives like put options can have large negative betas.\n\nBeta is important because it measures the risk of an investment that cannot be reduced by diversification. It does not measure the risk of an investment held on a stand-alone basis, but the amount of risk the investment adds to an already-diversified portfolio. In the Capital Asset Pricing Model (CAPM), beta risk is the only kind of risk for which investors should receive an expected return higher than the risk-free rate of interest.\n\nThe definition above covers only theoretical beta. The term is used in many related ways in finance. For example, the betas commonly quoted in mutual fund analyses generally measure the risk of the fund arising from exposure to a benchmark for the fund, rather than from exposure to the entire market portfolio. Thus they measure the amount of risk the fund adds to a diversified portfolio of funds of the same type, rather than to a portfolio diversified among all fund types.\n\nBeta decay refers to the tendency for a company with a high beta coefficient (β > 1) to have its beta coefficient decline to the market beta. It is an example of regression toward the mean.\n\nA statistical estimate of beta is calculated by a regression method. For a given asset and a benchmark, the goal is to find an approximate formula\n\nwhere \"r\" is the return of the asset, alpha (α) is the active return, and \"r\" is return of the benchmark.\n\nSince practical data are typically available as a discrete time series of samples, the statistical model is\n\nwhere ε is an error term (the unexplained return).\nThe best (in the sense of least squared error) estimates for α and β are those such that Σε is as small as possible.\n\nA common expression for beta is\n\nwhere Cov and Var are the covariance and variance operators.\n\nBy using the relationships between standard deviation, variance and correlation: formula_4, formula_5, formula_6, the above expression can also be written as\n\nwhere ρ is the correlation of the two returns, and σ and σ are the respective volatilities. If \"a\" refers to the investment and \"b\" refers to the market, it now becomes clear that the interpretation of beta as 'the volatility of an investment relative to the market volatility' is inconsistent with how beta is calculated; this is due to the presence of the correlation in the above formula.\n\nBeta can be computed for prices in the past, where the data is known, which is historical beta. However, what most people are interested in is \"future beta\", which relates to risks going forward. Estimating future beta is a difficult problem. One guess is that future beta equals historical beta.\n\nFrom this, we find that beta can be explained as \"correlated relative volatility\". This has three components:\n\nBeta is also referred to as financial elasticity or correlated relative volatility, and can be referred to as a measure of the sensitivity of the asset's returns to market returns, its non-diversifiable risk, its systematic risk, or market risk. On an individual asset level, measuring beta can give clues to volatility and liquidity in the marketplace. In fund management, measuring beta is thought to separate a manager's skill from his or her willingness to take risk.\n\nThe portfolio of interest in the CAPM formulation is the market portfolio that contains all risky assets, and so the \"r\" terms in the formula are replaced by \"r\", the rate of return of the market. The regression line is then called the security characteristic line (SCL).\n\nformula_9 is called the asset's alpha and formula_10 is called the asset's beta coefficient. Both coefficients have an important role in modern portfolio theory.\n\nFor example, in a year where the broad market or benchmark index returns 25% above the risk free rate, suppose two managers gain 50% above the risk free rate. Because this higher return is theoretically possible merely by taking a leveraged position in the broad market to double the beta so it is exactly 2.0, we would expect a skilled portfolio manager to have built the outperforming portfolio with a beta somewhat less than 2, such that the excess return not explained by the beta is positive. If one of the managers' portfolios has an average beta of 3.0, and the other's has a beta of only 1.5, then the CAPM simply states that the extra return of the first manager is not sufficient to compensate us for that manager's risk, whereas the second manager has done more than expected given the risk. Whether investors can expect the second manager to duplicate that performance in future periods is of course a different question.\n\nThe SML graphs the results from the capital asset pricing model (CAPM) formula. The \"x\"-axis represents the risk (beta), and the \"y\"-axis represents the expected return. The market risk premium is determined from the slope of the SML.\n\nThe relationship between β and required return is plotted on the \"security market line\" (SML) which shows expected return as a function of β. The intercept is the nominal risk-free rate \"R\" available for the market, while the slope is E(\"R\")− \"R\" (for market return \"R\"). The security market line can be regarded as representing a single-factor model of the asset price, where beta is exposure to changes in value of the market. The equation of the SML, giving the expected value of the return on asset \"i\", is thus:\n\nIt is a useful tool in determining if an asset being considered for a portfolio offers a reasonable expected return for risk. Individual securities are plotted on the SML graph. If the security's risk versus expected return is plotted above the SML, it is undervalued because the investor can expect a greater return for the inherent risk. A security plotted below the SML is overvalued because the investor would be accepting a lower return for the amount of risk assumed.\n\nIn the U.S., published betas typically use a stock market index such as the S&P 500 as a benchmark. The S&P 500 is a popular index of U.S. large-cap stocks. Other choices may be an international index such as the MSCI EAFE. The benchmark is often chosen to be similar to the assets chosen by the investor. For example, for a person who owns S&P 500 index funds and gold bars, the index would combine the S&P 500 and the price of gold. In practice a standard index is used.\n\nThe choice of the index need not reflect the portfolio under question; e.g., beta for gold bars compared to the S&P 500 may be low or negative carrying the information that gold does not track stocks and may provide a mechanism for reducing risk. The restriction to stocks as a benchmark is somewhat arbitrary. A model portfolio may be stocks plus bonds. Sometimes the market is defined as \"all investable assets\" (see Roll's critique); unfortunately, this includes lots of things for which returns may be hard to measure.\n\nBy definition, the market itself has a beta of 1, and individual stocks are ranked according to how much they deviate from the macro market (for simplicity purposes, the S&P 500 is sometimes used as a proxy for the market as a whole). A stock whose returns vary more than the market's returns over time can have a beta whose absolute value is greater than 1.0 (whether it is, in fact, greater than 1.0 will depend on the correlation of the stock's returns and the market's returns). A stock whose returns vary less than the market's returns has a beta with an absolute value less than 1.0.\n\nA stock with a beta of 2 has returns that change, on average, by twice the magnitude of the overall market; when the market's return falls or rises by 3%, the stock's return will fall or rise (respectively) by 6% on average. (However, because beta also depends on the correlation of returns, there can be considerable variance about that average; the higher the correlation, the less variance; the lower the correlation, the higher the variance.) Beta can also be negative, meaning the stock's returns tend to move in the opposite direction of the market's returns. A stock with a beta of −3 would see its return \"decline\" 9% (on average) when the market's return goes up 3%, and would see its return \"climb\" 9% (on average) if the market's return falls by 3%.\n\nHigher-beta stocks tend to be more volatile and therefore riskier, but provide the potential for higher returns. Lower-beta stocks pose less risk but generally offer lower returns. Some have challenged this idea, claiming that the data show little relation between beta and potential reward, or even that lower-beta stocks are both less risky and more profitable (contradicting CAPM). In the same way a stock's beta shows its relation to market shifts, it is also an indicator for required returns on investment (ROI). Given a risk-free rate of 2%, for example, if the market (with a beta of 1) has an expected return of 8%, a stock with a beta of 1.5 should return 11% (= 2% + 1.5(8% − 2%)) in accordance with the financial CAPM model.\n\nSuppose an investor has all his money in an asset class X and wishes to move a small amount to an asset class Y. For example, X could be U.S. stocks, while Y could be stocks of a different country, or bonds. Then the new portfolio, Z, can be expressed symbolically\n\nThe variance can be computed as\n\nwhich can be simplified by ignoring δ terms:\n\nThe first formula is exact, while the second one is only valid for small δ. Using the formula for β of Y relative to X,\n\nwe can compute\n\nThis suggests that an asset with β greater than 1 will increase variance, while an asset with β less than 1 will decrease variance, if added in the right amount. This assumes that variance is an accurate measure of risk, which is usually good. However, the beta does need to be computed with respect to what the investor currently owns.\n\nAcademic theory claims that higher-risk investments should have higher returns over the \"long-term\". Wall Street has a saying that \"higher return requires higher risk\", not that a risky investment will automatically do better. Some things may just be poor investments (e.g., playing roulette). Further, highly rational investors should consider correlated volatility (beta) instead of simple volatility (sigma). Theoretically, a negative beta equity is possible; for example, an inverse ETF should have negative beta to the relevant index. Also, a short position should have opposite beta.\n\nThis expected return on equity, or equivalently, a firm's cost of equity, can be estimated using the capital asset pricing model (CAPM). According to the model, the expected return on equity is a function of a firm's equity beta (β) which, in turn, is a function of both leverage and asset risk (β):\n\nwhere:\n\n\nbecause:\n\nand\n\nAn indication of the systematic riskiness attaching to the returns on ordinary shares. It equates to the asset Beta for an ungeared firm, or is adjusted upwards to reflect the extra riskiness of shares in a geared firm., i.e. the Geared Beta.\n\nThe arbitrage pricing theory (APT) has multiple betas in its model. In contrast to the CAPM that has only one risk factor, namely the overall market, APT has multiple risk factors. Each risk factor has a corresponding beta indicating the responsiveness of the asset being priced to that risk factor.\n\nMultiple-factor models contradict CAPM by claiming that some other factors can influence return, therefore one may find two stocks (or funds) with equal beta, but one may be a better investment.\n\nTo estimate beta, one needs a list of returns for the asset and returns for the index; these returns can be daily, weekly or any period. Then one uses standard formulas from linear regression. The slope of the fitted line from the linear least-squares calculation is the estimated Beta. The \"y\"-intercept is the alpha.\n\nMyron Scholes and Joseph Williams (1977) provided a model for estimating betas from nonsynchronous data.\n\nBeta specifically gives the volatility ratio multiplied by the correlation of the plotted data. To take an extreme example, something may have a beta of zero even though it is highly volatile, provided it is uncorrelated with the market. Tofallis (2008) provides a discussion of this, together with a real example involving AT&T Inc. The graph showing monthly returns from AT&T is visibly more volatile than the\nindex and yet the standard estimate of beta for this is less than one.\n\nThe relative volatility ratio described above is actually known as Total Beta (at least by appraisers who practice business valuation). Total beta is equal to the identity: beta/\"R\" or the standard deviation of the stock/standard deviation of the market (note: the relative volatility). Total beta captures the security's risk as a stand-alone asset (because the correlation coefficient, R, has been removed from beta), rather than part of a well-diversified portfolio. Because appraisers frequently value closely held companies as stand-alone assets, total beta is gaining acceptance in the business valuation industry. Appraisers can now use total beta in the following equation: total cost of equity (TCOE) = risk-free rate + total beta·equity risk premium. Once appraisers have a number of TCOE benchmarks, they can compare/contrast the risk factors present in these publicly traded benchmarks and the risks in their closely held company to better defend/support their valuations.\n\nSome interpretations of beta are explained in the following table:\nIt measures the part of the asset's statistical variance that cannot be removed by the diversification provided by the portfolio of many risky assets, because of the correlation of its returns with the returns of the other assets that are in the portfolio. Beta can be estimated for individual companies using regression analysis against a stock market index. An alternative to standard beta is downside beta.\n\nBeta is always measured in respect to some benchmark. Therefore, an asset may have different betas depending on which benchmark is used. Just a number is useless if the benchmark is not known.\n\n\nSeth Klarman of the Baupost group wrote in \"Margin of Safety\":\n\"I find it preposterous that a single number reflecting past price fluctuations could be thought to completely describe the risk in a security. Beta views risk solely from the perspective of market prices, failing to take into consideration specific business\nfundamentals or economic developments. The price level is also ignored, as if IBM selling at 50 dollars per share would\nnot be a lower-risk investment than the same IBM at 100 dollars per share. Beta fails to allow for the influence that investors\nthemselves can exert on the riskiness of their holdings through such efforts as proxy contests, shareholder resolutions, communications with management, or the ultimate purchase of sufficient stock to gain corporate control and with it direct access to underlying value. Beta also assumes that the upside potential and downside risk of any investment are essentially equal,\nbeing simply a function of that investment's volatility compared with that of the market as a whole. This too is inconsistent\nwith the world as we know it. The reality is that past security price volatility does not reliably predict future investment\nperformance (or even future volatility) and therefore is a poor measure of risk.\"\n\nAt the industry level, beta tends to underestimate downside beta two-thirds of the time (resulting in value overestimation) and overestimate upside beta one-third of the time resulting in value underestimation.\n\nAnother weakness of beta can be illustrated through an easy example by considering two hypothetical stocks, A and B. The returns on A, B and the market follow the probability distribution below:\nThe table shows that stock A goes down half as much as the market when the market goes down and up twice as much as the market when the market goes up. Stock B, on the other hand, goes down twice as much as the market when the market goes down and up half as much as the market when the market goes up. Most investors would label stock B as more risky. In fact, stock A has better return in every possible case. However, according to the capital asset pricing model, stock A and B would have the same beta, meaning that theoretically, investors would require the same rate of return for both stocks. Of course it is entirely expected that this example could break the CAPM as the CAPM relies on certain assumptions one of the most central being the nonexistence of arbitrage, However, in this example buying stock A and selling stock B is an example of an arbitrage as stock A is worth more in every scenario. This is an illustration of how using standard beta might mislead investors. The dual-beta model, in contrast, takes into account this issue and differentiates downside beta from upside beta, or downside risk from upside risk, and thus allows investors to make better informed investing decisions.\n\n"}
{"id": "397571", "url": "https://en.wikipedia.org/wiki?curid=397571", "title": "Bisimulation", "text": "Bisimulation\n\nIn theoretical computer science a bisimulation is a binary relation between state transition systems, associating systems that behave in the same way in the sense that one system simulates the other and vice versa.\n\nIntuitively two systems are bisimilar if they match each other's moves. In this sense, each of the systems cannot be distinguished from the other by an observer.\n\nGiven a labelled state transition system (formula_1, Λ, →), a \"bisimulation\" relation is a binary relation formula_2 over formula_1 (i.e., formula_2 ⊆ formula_1 × formula_1) such that both formula_2 and its converse formula_8 are simulations.\n\nEquivalently formula_2 is a bisimulation if for every pair of elements formula_10 in formula_1 with formula_12 in formula_2, for all α in Λ:\n\nfor all formula_14 in formula_1,\n\nand, symmetrically, for all formula_17 in formula_1\n\nGiven two states formula_28 and formula_29 in formula_1, formula_28 is bisimilar to formula_29, written formula_33, if there is a bisimulation formula_2 such that formula_35 is in formula_2.\n\nThe bisimilarity relation formula_37 is an equivalence relation. Furthermore, it is the largest bisimulation relation over a given transition system.\n\nNote that it is not always the case that if formula_28 simulates formula_29 and formula_29 simulates formula_28 then they are bisimilar. For formula_28 and formula_29 to be bisimilar, the simulation between formula_28 and formula_29 must be the converse of the simulation between formula_29 and formula_28. Counter-example (in CCS, describing a coffee machine) : formula_48 and formula_49 simulate each other but are not bisimilar.\n\nBisimulation can be defined in terms of composition of relations as follows.\n\nGiven a labelled state transition system formula_50, a \"bisimulation\" relation is a binary relation formula_2 over formula_1 (i.e., formula_2 ⊆ formula_1 × formula_1) such that formula_56\n\nFrom the monotonicity and continuity of relation composition, it follows immediately that the set of the bisimulations is closed under unions (joins in the poset of relations), and a simple algebraic calculation shows that the relation of bisimilarity—the join of all bisimulations—is an equivalence relation. This definition, and the associated treatment of bisimilarity, can be interpreted in any involutive quantale.\n\nBisimilarity can also be defined in order theoretical fashion, in terms of fixpoint theory, more precisely as the greatest fixed point of a certain function defined below.\n\nGiven a labelled state transition system (formula_1, Λ, →), define formula_60 to be a function from binary relations over formula_1 to binary relations over formula_1, as follows:\n\nLet formula_2 be any binary relation over formula_1. formula_65 is defined to be the set of all pairs formula_12 in formula_1 × formula_1 such that:\n\nand\n\nBisimilarity is then defined to be the greatest fixed point of formula_71.\n\nBisimulation can also be thought of in terms of a game between two players: attacker and defender.\n\n\"Attacker\" goes first and may choose any valid transition, formula_72, from formula_12. I.e.:\n\nformula_74\nor \nformula_75\n\nThe \"Defender\" must then attempt to match that transition, formula_72 from either formula_77 or formula_78 depending on the attacker's move.\nI.e., they must find an formula_72 such that:\n\nformula_80\nor \nformula_81\n\nAttacker and defender continue to take alternating turns until:\n\n\nBy the above definition the system is a bisimulation if and only if there exists a winning strategy for the defender.\n\nA bisimulation for state transition systems is a special case of coalgebraic bisimulation for the type of covariant powerset functor.\nNote that every state transition system formula_50 is bijectively a function formula_85 from formula_1 to the powerset of formula_1 indexed by formula_88 written as formula_89, defined by\n\nLet formula_91 be formula_92-th projection mapping\nformula_35 to formula_28 and formula_29 respectively for formula_96; and\nformula_97 the forward image of formula_98 defined by dropping the third component\nwhere formula_100 is a subset of formula_101. Similarly for formula_102.\n\nUsing the above notations, a relation formula_103 is a bisimulation on a transition system formula_50 if and only if there exists a transition system formula_105 on the relation formula_2 such that the diagram\n\ncommutes, i.e. for formula_96, the equations\nhold\nwhere formula_109 is the functional representation of formula_50.\n\nIn special contexts the notion of bisimulation is sometimes refined by adding additional requirements or constraints. For example, if the state transition system includes a notion of \"silent\" (or \"internal\") action, often denoted with formula_111, i.e. actions that are not visible by external observers, then bisimulation can be relaxed to be \"weak bisimulation\", in which if two states formula_28 and formula_29 are bisimilar and there is some number of internal actions leading from formula_28 to some state formula_14 then there must exist state formula_17 such that there is some number (possibly zero) of internal actions leading from formula_29 to formula_17. A relation formula_119 on processes is a weak bisimulation if the following holds (with formula_120, and formula_121 being an observable and mute transition respectively):\n\nformula_122\n\nformula_123\n\nThis is closely related to bisimulation up to a relation.\n\nTypically, if the state transition system gives the operational semantics of a programming language, then the precise definition of bisimulation will be specific to the restrictions of the programming language. Therefore, in general, there may be more than one kind of bisimulation, (bisimilarity resp.) relationship depending on the context.\n\nSince Kripke models are a special case of (labelled) state transition systems, bisimulation is also a topic in modal logic. In fact, modal logic is the fragment of first-order logic invariant under bisimulation (van Benthem's theorem).\n\n\n\n\n"}
{"id": "1595873", "url": "https://en.wikipedia.org/wiki?curid=1595873", "title": "Cantor–Dedekind axiom", "text": "Cantor–Dedekind axiom\n\nIn mathematical logic, the Cantor–Dedekind axiom is the thesis that the real numbers are order-isomorphic to the linear continuum of geometry. In other words, the axiom states that there is a one-to-one correspondence between real numbers and points on a line.\n\nThis axiom is the cornerstone of analytic geometry. The Cartesian coordinate system developed by René Descartes implicitly assumes this axiom by blending the distinct concepts of real number system with the geometric line or plane into a conceptual metaphor. This is sometimes referred to as the \"real number line\" blend.\n\nA consequence of this axiom is that Alfred Tarski's proof of the decidability of the ordered real field could be seen as an algorithm to solve any problem in Euclidean geometry.\n\n"}
{"id": "1824845", "url": "https://en.wikipedia.org/wiki?curid=1824845", "title": "Cartesian tensor", "text": "Cartesian tensor\n\nIn geometry and linear algebra, a Cartesian tensor uses an orthonormal basis to represent a tensor in a Euclidean space in the form of components. Converting a tensor's components from one such basis to another is through an orthogonal transformation.\n\nThe most familiar coordinate systems are the two-dimensional and three-dimensional Cartesian coordinate systems. Cartesian tensors may be used with any Euclidean space, or more technically, any finite-dimensional vector space over the field of real numbers that has an inner product.\n\nUse of Cartesian tensors occurs in physics and engineering, such as with the Cauchy stress tensor and the moment of inertia tensor in rigid body dynamics. Sometimes general curvilinear coordinates are convenient, as in high-deformation continuum mechanics, or even necessary, as in general relativity. While orthonormal bases may be found for some such coordinate systems (e.g. tangent to spherical coordinates), Cartesian tensors may provide considerable simplification for applications in which rotations of rectilinear coordinate axes suffice. The transformation is a passive transformation, since the coordinates are changed and not the physical system.\n\nIn 3d Euclidean space, ℝ, the standard basis is e, e, e. Each basis vector points along the x-, y-, and z-axes, and the vectors are all unit vectors (or normalized), so the basis is orthonormal.\n\nThroughout, when referring to Cartesian coordinates in three dimensions, a right-handed system is assumed and this is much more common than a left-handed system in practice, see orientation (vector space) for details.\nsimi\nFor Cartesian tensors of order 1, a Cartesian vector a can be written algebraically as a linear combination of the basis vectors e, e, e:\n\nwhere the coordinates of the vector with respect to the Cartesian basis are denoted \"a\", \"a\", \"a\". It is common and helpful to display the basis vectors as column vectors\n\nwhen we have a coordinate vector in a column vector representation:\n\nA row vector representation is also legitimate, although in the context of general curvilinear coordinate systems the row and column vector representations are used separately for specific reasons – see Einstein notation and covariance and contravariance of vectors for why.\n\nThe term \"component\" of a vector is ambiguous: it could refer to:\n\n\nA more general notation is tensor index notation, which has the flexibility of numerical values rather than fixed coordinate labels. The Cartesian labels are replaced by tensor indices in the basis vectors e ↦ e, e ↦ e, e ↦ e and coordinates \"A\" ↦ \"A\", \"A\" ↦ \"A\", \"A\" ↦ \"A\". In general, the notation e, e, e refers to \"any\" basis, and \"A\", \"A\", \"A\" refers to the corresponding coordinate system; although here they are restricted to the Cartesian system. Then:\n\nIt is standard to use the Einstein notation—the summation sign for summation over an index that is present exactly twice within a term may be suppressed for notational conciseness:\n\nAn advantage of the index notation over coordinate-specific notations is the independence of the dimension of the underlying vector space, i.e. the same expression on the right hand side takes the same form in higher dimensions (see below). Previously, the Cartesian labels x, y, z were just labels and \"not\" indices. (It is informal to say \"\"i\" = x, y, z\").\n\nA dyadic tensor T is an order 2 tensor formed by the tensor product ⊗ of two Cartesian vectors a and b, written T = a ⊗ b. Analogous to vectors, it can be written as a linear combination of the tensor basis , , ..., (the right hand side of each identity is only an abbreviation, nothing more):\n\nRepresenting each basis tensor as a matrix:\n\nthen T can be represented more systematically as a matrix:\n\nSee matrix multiplication for the notational correspondence between matrices and the dot and tensor products.\n\nMore generally, whether or not T is a tensor product of two vectors, it is always a linear combination of the basis tensors with coordinates \"T\", \"T\", ... \"T\":\n\nwhile in terms of tensor indices:\n\nand in matrix form:\n\nSecond order tensors occur naturally in physics and engineering when physical quantities have directional dependence in the system, often in a \"stimulus-response\" way. This can be mathematically seen through one aspect of tensors - they are multilinear functions. A second order tensor T which takes in a vector u of some magnitude and direction will return a vector v; of a different magnitude and in a different direction to u, in general. The notation used for functions in mathematical analysis leads us to write , while the same idea can be expressed in matrix and index notations (including the summation convention), respectively:\n\nBy \"linear\", if for two scalars \"ρ\" and \"σ\" and vectors r and s, then in function and index notations:\n\nand similarly for the matrix notation. The function, matrix, and index notations all mean the same thing. The matrix forms provide a clear display of the components, while the index form allows easier tensor-algebraic manipulation of the formulae in a compact manner. Both provide the physical interpretation of \"directions\"; vectors have one direction, while second order tensors connect two directions together. One can associate a tensor index or coordinate label with a basis vector direction.\n\nThe use of second order tensors are the minimum to describe changes in magnitudes and directions of vectors, as the dot product of two vectors is always a scalar, while the cross product of two vectors is always a pseudovector perpendicular to the plane defined by the vectors, so these products of vectors alone cannot obtain a new vector of any magnitude in any direction. (See also below for more on the dot and cross products). The tensor product of two vectors is a second order tensor, although this has no obvious directional interpretation by itself.\n\nThe previous idea can be continued: if T takes in two vectors p and q, it will return a scalar \"r\". In function notation we write \"r\" = T(p, q), while in matrix and index notations (including the summation convention) respectively:\n\nThe tensor T is linear in both input vectors. When vectors and tensors are written without reference to components, and indices are not used, sometimes a dot · is placed where summations over indices (known as tensor contractions) are taken. For the above cases:\n\nmotivated by the dot product notation:\n\nMore generally, a tensor of order \"m\" which takes in \"n\" vectors (where \"n\" is between 0 and \"m\" inclusive) will return a tensor of order , see Tensor: As multilinear maps for further generalizations and details. The concepts above also apply to pseudovectors in the same way as for vectors. The vectors and tensors themselves can vary within throughout space, in which case we have vector fields and tensor fields, and can also depend on time.\n\nFollowing are some examples:\n\nFor the electrical conduction example, the index and matrix notations would be:\n\nwhile for the rotational kinetic energy \"T\":\n\nSee also constitutive equation for more specialized examples.\n\nIn \"n\"-dimensional Euclidean space over the real numbers, ℝ, the standard basis is denoted e, e, e, ... e. Each basis vector e points along the positive \"x\" axis, with the basis being orthonormal. Component \"j\" of e is given by the Kronecker delta:\n\nA vector in ℝ takes the form:\n\nSimilarly for the order 2 tensor above, for each vector a and b in ℝ:\n\nor more generally:\n\nThe position vector x in ℝ is a simple and common example of a vector, and can be represented in \"any\" coordinate system. Consider the case of rectangular coordinate systems with orthonormal bases only. It is possible to have a coordinate system with rectangular geometry if the basis vectors are all mutually perpendicular and not normalized, in which case the basis is ortho\"gonal\" but not ortho\"normal\". However, orthonormal bases are easier to manipulate and are often used in practice. The following results are true for orthonormal bases, not orthogonal ones.\n\nIn one rectangular coordinate system, x as a contravector has coordinates \"x\" and basis vectors e, while as a covector it has coordinates \"x\" and basis covectors e, and we have:\n\nIn another rectangular coordinate system, x as a contravector has coordinates \"\" and bases , while as a covector it has coordinates \"\" and bases , and we have:\n\nEach new coordinate is a function of all the old ones, and vice versa for the inverse function:\n\nand similarly each new basis vector is a function of all the old ones, and vice versa for the inverse function:\n\nfor all \"i\", \"j\".\n\nA vector is invariant under any change of basis, so if coordinates transform according to a transformation matrix L, the bases transform according to the matrix inverse L, and conversely if the coordinates transform according to inverse L, the bases transform according to the matrix L. The difference between each of these transformations is shown conventionally through the indices as superscripts for contravariance and subscripts for covariance, and the coordinates and bases are linearly transformed according to the following rules:\n\nwhere L represents the entries of the transformation matrix (row number is \"i\" and column number is \"j\") and (L) denotes the entries of the inverse matrix of the matrix L.\n\nIf L is an orthogonal transformation (orthogonal matrix), the objects transforming by it are defined as Cartesian tensors. This geometrically has the interpretation that a rectangular coordinate system is mapped to another rectangular coordinate system, in which the norm of the vector x is preserved (and distances are preserved).\n\nThe determinant of L is det(L) = ±1, which corresponds to two types of orthogonal transformation: (+1) for rotations and (−1) for improper rotations (including reflections).\n\nThere are considerable algebraic simplifications, the matrix transpose is the inverse from the definition of an orthogonal transformation:\n\nFrom the previous table, orthogonal transformations of covectors and contravectors are identical. There is no need to differ between raising and lowering indices, and in this context and applications to physics and engineering the indices are usually all subscripted to remove confusion for exponents. All indices will be lowered in the remainder of this article. One can determine the actual raised and lowered indices by considering which quantities are covectors or contravectors, and the relevant transformation rules.\n\nExactly the same transformation rules apply to any vector a, not only the position vector. If its components \"a\" do not transform according to the rules, a is not a vector.\n\nDespite the similarity between the expressions above, for the change of coordinates such as , and the action of a tensor on a vector like , L is not a tensor, but T is. In the change of coordinates, L is a \"matrix\", used to relate two rectangular coordinate systems with orthonormal bases together. For the tensor relating a vector to a vector, the vectors and tensors throughout the equation all belong to the same coordinate system and basis.\n\nThe entries of L are partial derivatives of the new or old coordinates with respect to the old or new coordinates, respectively.\n\nDifferentiating \"\" with respect to \"x\":\n\nso\n\nis an element of the Jacobian matrix. There is a (partially mnemonical) correspondence between index positions attached to L and in the partial derivative: \"i\" at the top and \"j\" at the bottom, in each case, although for Cartesian tensors the indices can be lowered.\n\nConversely, differentiating \"x\" with respect to \"\":\n\nso\n\nis an element of the inverse Jacobian matrix, with a similar index correspondence.\nMany sources state transformations in terms of the partial derivatives:\n\nand the explicit matrix equations in 3d are:\n\nsimilarly for\n\nAs with all linear transformations, L depends on the basis chosen. For two orthonormal bases\n\n\nHence the components reduce to direction cosines between the \"\" and \"x\" axes:\n\nwhere \"θ\" and \"θ\" are the angles between the \"\" and \"x\" axes. In general, \"θ\" is not equal to \"θ\", because for example \"θ\" and \"θ\" are two different angles.\n\nThe transformation of coordinates can be written:\n\n_i\\cdot\\mathbf{e}_j \\right) = x_i\\cos\\theta_{ij}\\\\\n\\upharpoonleft\\downharpoonright\\\\\n</math>\n\nand the explicit matrix equations in 3d are:\n\nsimilarly for\n\nThe geometric interpretation is the \"\" components equal to the sum of projecting the \"x\" components onto the \"\" axes.\n\nThe numbers e⋅e arranged into a matrix would form a symmetric matrix (a matrix equal to its own transpose) due to the symmetry in the dot products, in fact it is the metric tensor g. By contrast e⋅ or ⋅e do \"not\" form symmetric matrices in general, as displayed above. Therefore, while the L matrices are still orthogonal, they are not symmetric.\n\nApart from a rotation about any one axis, in which the \"x\" and \"\" for some \"i\" coincide, the angles are not the same as Euler angles, and so the L matrices are not the same as the rotation matrices.\n\nThe dot product and cross product occur very frequently, in applications of vector analysis to physics and engineering, examples include:\n\n\nHow these products transform under orthogonal transformations is illustrated below.\n\nThe dot product ⋅ of each possible pairing of the basis vectors follows from the basis being orthonormal. For perpendicular pairs we have\n\nwhile for parallel pairs we have\n\nReplacing Cartesian labels by index notation as shown above, these results can be summarized by\n\nwhere \"δ\" are the components of the Kronecker delta. The Cartesian basis can be used to represent \"δ\" in this way.\n\nIn addition, each metric tensor component \"g\" with respect to any basis is the dot product of a pairing of basis vectors:\n\nFor the Cartesian basis the components arranged into a matrix are:\n\nso are the simplest possible for the metric tensor, namely the \"δ\":\n\nThis is \"not\" true for general bases: orthogonal coordinates have diagonal metrics containing various scale factors (i.e. not necessarily 1), while general curvilinear coordinates could also have nonzero entries for off-diagonal components.\n\nThe dot product of two vectors a and b transforms according to\n\nwhich is intuitive, since the dot product of two vectors is a single scalar independent of any coordinates. This also applies more generally to any coordinate systems, not just rectangular ones; the dot product in one coordinate system is the same in any other.\n\nFor the cross product × of two vectors, the results are (almost) the other way round. Again, assuming a right-handed 3d Cartesian coordinate system, cyclic permutations in perpendicular directions yield the next vector in the cyclic collection of vectors:\n\nwhile parallel vectors clearly vanish:\n\nand replacing Cartesian labels by index notation as above, these can be summarized by:\n\nwhere \"i\", \"j\", \"k\" are indices which take values 1, 2, 3. It follows that:\n\nThese permutation relations and their corresponding values are important, and there is an object coinciding with this property: the Levi-Civita symbol, denoted by \"ε\". The Levi-Civita symbol entries can be represented by the Cartesian basis:\n\nwhich geometrically corresponds to the volume of a cube spanned by the orthonormal basis vectors, with sign indicating orientation (and \"not\" a \"positive or negative volume\"). Here, the orientation is fixed by \"ε\" = +1, for a right-handed system. A left-handed system would fix \"ε\" = −1 or equivalently \"ε\" = +1.\n\nThe scalar triple product can now be written:\n\nwith the geometric interpretation of volume (of the parallelepiped spanned by a, b, c) and algebraically is a determinant:\n\nThis in turn can be used to rewrite the cross product of two vectors as follows:\n\nContrary to its appearance, the Levi-Civita symbol is \"not a tensor\", but a pseudotensor, the components transform according to:\n\nTherefore, the transformation of the cross product of a and b is:\n\nand so a × b transforms as a pseudovector, because of the determinant factor.\n\nThe tensor index notation applies to any object which has entities that form multidimensional arrays – not everything with indices is a tensor by default. Instead, tensors are defined by how their coordinates and basis elements change under a transformation from one coordinate system to another.\n\nNote the cross product of two vectors is a pseudovector, while the cross product of a pseudovector with a vector is another vector.\n\nOther identities can be formed from the \"δ\" tensor and \"ε\" pseudotensor, a notable and very useful identity is one that converts two Levi-Civita symbols adjacently contracted over two indices into an antisymmetrized combination of Kronecker deltas:\n\nThe index forms of the dot and cross products, together with this identity, greatly facilitate the manipulation and derivation of other identities in vector calculus and algebra, which in turn are used extensively in physics and engineering. For instance, it is clear the dot and cross products are distributive over vector addition:\n\nwithout resort to any geometric constructions - the derivation in each case is a quick line of algebra. Although the procedure is less obvious, the vector triple product can also be derived. Rewriting in index notation:\n\nand because cyclic permutations of indices in the \"ε\" symbol does not change its value, cyclically permuting indices in \"ε\" to obtain \"ε\" allows us to use the above \"δ\"-\"ε\" identity to convert the \"ε\" symbols into \"δ\" tensors:\n\nthusly:\n\nNote this is antisymmetric in b and c, as expected from the left hand side. Similarly, via index notation or even just cyclically relabelling a, b, and c in the previous result and taking the negative:\n\nand the difference in results show that the cross product is not associative. More complex identities, like quadruple products;\n\nand so on, can be derived in a similar manner.\n\nTensors are defined as quantities which transform in a certain way under linear transformations of coordinates.\n\nText below contradics introduced above contravariant and covariant vectors (tensors)\n\nLet a = \"ae and b = \"be be two vectors, so that they transform according to \"\" = \"aL\", \"\" = \"bL\".\n\nTaking the tensor product gives:\n\nthen applying the transformation to the components\n\nand to the bases\n\ngives the transformation law of an order-2 tensor. The tensor a⊗b is invariant under this transformation:\n\nMore generally, for any order-2 tensor\n\nthe components transform according to;\n\nand the basis transforms by:\n\nIf R does not transform according to this rule - whatever quantity R may be, it's not an order 2 tensor.\n\nMore generally, for any order \"p\" tensor\n\nthe components transform according to;\n\nand the basis transforms by:\n\nFor a pseudotensor S of order \"p\", the components transform according to;\n\nThe antisymmetric nature of the cross product can be recast into a tensorial form as follows. Let c be a vector, a be a pseudovector, b be another vector, and T be a second order tensor such that:\n\nAs the cross product is linear in a and b, the components of T can be found by inspection, and they are:\n\nso the pseudovector a can be written as an antisymmetric tensor. This transforms as a tensor, not a pseudotensor. For the mechanical example above for the tangential velocity of a rigid body, given by , this can be rewritten as where Ω is the tensor corresponding to the pseudovector ω:\n\nFor an example in electromagnetism, while the electric field E is a vector field, the magnetic field B is a pseudovector field. These fields are defined from the Lorentz force for a particle of electric charge \"q\" traveling at velocity v:\n\nand considering the second term containing the cross product of a pseudovector B and velocity vector v, it can be written in matrix form, with F, E, and v as column vectors and B as an antisymmetric matrix:\n\nIf a pseudovector is explicitly given by a cross product of two vectors (as opposed to entering the cross product with another vector), then such pseudovectors can also be written as antisymmetric tensors of second order, with each entry a component of the cross product. The angular momentum of a classical pointlike particle orbiting about an axis, defined by , is another example of a pseudovector, with corresponding antisymmetric tensor:\n\nAlthough Cartesian tensors do not occur in the theory of relativity; the tensor form of orbital angular momentum J enters the spacelike part of the relativistic angular momentum tensor, and the above tensor form of the magnetic field B enters the spacelike part of the electromagnetic tensor.\n\nIt should be emphasized the following formulae are only so simple in Cartesian coordinates - in general curvilinear coordinates there are factors of the metric and its determinant - see tensors in curvilinear coordinates for more general analysis.\n\nFollowing are the differential operators of vector calculus. Throughout, left Φ(r, \"t\") be a scalar field, and\n\nbe vector fields, in which all scalar and vector fields are functions of the position vector r and time \"t\".\n\nThe gradient operator in Cartesian coordinates is given by:\n\nand in index notation, this is usually abbreviated in various ways:\n\nThis operator acts on a scalar field Φ to obtain the vector field directed in the maximum rate of increase of Φ:\n\nThe index notation for the dot and cross products carries over to the differential operators of vector calculus.\n\nThe directional derivative of a scalar field Φ is the rate of change of Φ along some direction vector a (not necessarily a unit vector), formed out of the components of a and the gradient:\n\nThe divergence of a vector field A is:\n\nNote the interchange of the components of the gradient and vector field yields a different differential operator\n\nwhich could act on scalar or vector fields. In fact, if A is replaced by the velocity field u(r, \"t\") of a fluid, this is a term in the material derivative (with many other names) of continuum mechanics, with another term being the partial time derivative:\n\nwhich usually acts on the velocity field leading to the non-linearity in the Navier-Stokes equations.\n\nAs for the curl of a vector field A, this can be defined as a pseudovector field by means of the \"ε\" symbol:\n\nwhich is only valid in three dimensions, or an antisymmetric tensor field of second order via antisymmetrization of indices, indicated by delimiting the antisymmetrized indices by square brackets (see Ricci calculus):\n\nwhich is valid in any number of dimensions. In each case, the order of the gradient and vector field components should not be interchanged as this would result in a different differential operator:\n\nwhich could act on scalar or vector fields.\n\nFinally, the Laplacian operator is defined in two ways, the divergence of the gradient of a scalar field Φ:\n\nor the square of the gradient operator, which acts on a scalar field Φ or a vector field A:\n\nIn physics and engineering, the gradient, divergence, curl, and Laplacian operator arise inevitably in fluid mechanics, Newtonian gravitation, electromagnetism, heat conduction, and even quantum mechanics.\n\nVector calculus identities can be derived in a similar way to those of vector dot and cross products and combinations. For example, in three dimensions, the curl of a cross product of two vector fields A and B:\n\nwhere the product rule was used, and throughout the differential operator was not interchanged with A or B. Thus:\n\nOne can continue the operations on tensors of higher order. Let T = T(r, \"t\") denote a second order tensor field, again dependent on the position vector r and time \"t\".\n\nFor instance, the gradient of a vector field in two equivalent notations (\"dyadic\" and \"tensor\", respectively) is:\n\nwhich is a tensor field of second order.\n\nThe divergence of a tensor is:\n\nwhich is a vector field. This arises in continuum mechanics in Cauchy's laws of motion - the divergence of the Cauchy stress tensor σ is a vector field, related to body forces acting on the fluid.\n\nCartesian tensors are as in tensor algebra, but Euclidean structure of and restriction of the basis brings some simplifications compared to the general theory.\n\nThe general tensor algebra consists of general mixed tensors of type (\"p\", \"q\"):\n\nwith basis elements:\n\nthe components transform according to:\n\nas for the bases:\n\nFor Cartesian tensors, only the order of the tensor matters in a Euclidean space with an orthonormal basis, and all indices can be lowered. A Cartesian basis does not exist unless the vector space has a positive-definite metric, and thus cannot be used in relativistic contexts.\n\nDyadic tensors were historically the first approach to formulating second-order tensors, similarly triadic tensors for third-order tensors, and so on. Cartesian tensors use tensor index notation, in which the variance may be glossed over and is often ignored, since the components remain unchanged by raising and lowering indices.\n\n\n\n\n"}
{"id": "2568071", "url": "https://en.wikipedia.org/wiki?curid=2568071", "title": "Cathetus", "text": "Cathetus\n\nIn a right triangle, a cathetus (originally from the Greek word ; plural: catheti), commonly known as a leg, is either of the sides that are adjacent to the right angle. It is occasionally called a \"side about the right angle\". The side opposite the right angle is the hypotenuse. In the context of the hypotenuse, the catheti are sometimes referred to simply as \"the other two sides\". \n\nIf the catheti of a right triangle have equal lengths, the triangle is isosceles. If they have different lengths, a distinction can be made between the minor (shorter) and major (longer) cathetus. The ratio of the lengths of the catheti defines the trigonometric functions tangent and cotangent of the acute angles in the triangle: the ratio formula_1 is the tangent of the acute angle adjacent to formula_2 and is also the cotangent of the acute angle adjacent to formula_3. \n\nIn a right triangle, the length of a cathetus is the geometric mean of the length of the adjacent segment cut by the altitude to the hypotenuse and the length of the whole hypotenuse.\n\nBy the Pythagorean theorem, the sum of the squares of the lengths of the catheti is equal to the square of the length of the hypotenuse.\n\nThe term leg, in addition to referring to a cathetus of a right triangle, is also used to refer to either of the equal sides of an isosceles triangle or to either of the non-parallel sides of a trapezoid.\n\n"}
{"id": "1012798", "url": "https://en.wikipedia.org/wiki?curid=1012798", "title": "Chain-complete partial order", "text": "Chain-complete partial order\n\nIn order-theoretic mathematics, a partially ordered set is chain-complete if every chain in it has a least upper bound. It is ω-complete when every increasing sequence of elements (a type of countable chain) has a least upper bound; the same notion can be extended to other cardinalities of chains.\n\nEvery complete lattice is chain-complete. Unlike complete lattices, chain-complete posets are relatively common. Examples include:\n\n\nA poset is chain-complete if and only if it is a pointed dcpo. However, this equivalence requires the axiom of choice.\n\nZorn's lemma states that, if a poset has an upper bound for every chain, then it has a maximal element. Thus, it applies to chain-complete posets, but is more general in that it allows chains that have upper bounds but do not have least upper bounds.\n\nChain-complete posets also obey the Bourbaki–Witt theorem, a fixed point theorem stating that, if \"f\" is a function from a chain complete poset to itself with the property that, for all \"x\", \"f\"(\"x\") ≥ \"x\", then \"f\" has a fixed point. This theorem, in turn, can be used to prove that Zorn's lemma is a consequence of the axiom of choice.\n\nBy analogy with the Dedekind–MacNeille completion of a partially ordered set, every partially ordered set can be extended uniquely to a minimal chain-complete poset.\n\n"}
{"id": "217628", "url": "https://en.wikipedia.org/wiki?curid=217628", "title": "Complex plane", "text": "Complex plane\n\nIn mathematics, the complex plane or \"z\"-plane is a geometric representation of the complex numbers established by the real axis and the perpendicular imaginary axis. It can be thought of as a modified Cartesian plane, with the real part of a complex number represented by a displacement along the x-axis, and the imaginary part by a displacement along the y-axis.\n\nThe concept of the complex plane allows a geometric interpretation of complex numbers. Under addition, they add like vectors. The multiplication of two complex numbers can be expressed most easily in polar coordinates—the magnitude or \"modulus\" of the product is the product of the two absolute values, or moduli, and the angle or \"argument\" of the product is the sum of the two angles, or arguments. In particular, multiplication by a complex number of modulus 1 acts as a rotation.\n\nThe complex plane is sometimes known as the Argand plane. \n\nIn complex analysis, the complex numbers are customarily represented by the symbol \"z\", which can be separated into its real (\"x\") and imaginary (\"y\") parts:\nfor example: \"z\" = 4 + 5\"i\", where \"x\" and \"y\" are real numbers, and \"i\" is the imaginary unit. In this customary notation the complex number \"z\" corresponds to the point (\"x\", \"y\") in the Cartesian plane.\n\nIn the Cartesian plane the point (\"x\", \"y\") can also be represented in polar coordinates as\n\nIn the Cartesian plane it may be assumed that the arctangent takes values from −\"π/2\" to \"π/2\" (in radians), and some care must be taken to define the \"real\" arctangent function for points (\"x\", \"y\") when \"x\" ≤ 0. In the complex plane these polar coordinates take the form\nwhere\n\nHere |\"z\"| is the \"absolute value\" or \"modulus\" of the complex number \"z\"; \"θ\", the \"argument\" of \"z\", is usually taken on the interval 0 ≤ \"θ\" < 2\"π\"; and the last equality (to |\"z\"|\"e\") is taken from Euler's formula. Notice that without the constraint on the range of \"θ\", the argument of \"z\" is multi-valued, because the complex exponential function is periodic, with period 2\"π i\". Thus, if \"θ\" is one value of arg(\"z\"), the other values are given by arg(\"z\") = \"θ\" + 2\"nπ\", where \"n\" is any integer ≠ 0. \n\nWhile seldom used explicitly, the geometric view of the complex numbers is implicitly based on its structure of a Euclidean vector space of dimension 2, where the inner product of complex numbers and is given by formula_5; then for a complex number its absolute value || coincides with its Euclidean norm, and its argument with the angle turning from 1 to .\n\nThe theory of contour integration comprises a major part of complex analysis. In this context the direction of travel around a closed curve is important – reversing the direction in which the curve is traversed multiplies the value of the integral by −1. By convention the \"positive\" direction is counterclockwise. For example, the unit circle is traversed in the positive direction when we start at the point \"z\" = 1, then travel up and to the left through the point \"z\" = \"i\", then down and to the left through −1, then down and to the right through −\"i\", and finally up and to the right to \"z\" = 1, where we started.\n\nAlmost all of complex analysis is concerned with complex functions – that is, with functions that map some subset of the complex plane into some other (possibly overlapping, or even identical) subset of the complex plane. Here it is customary to speak of the domain of \"f\"(\"z\") as lying in the \"z\"-plane, while referring to the range or \"image\" of \"f\"(\"z\") as a set of points in the \"w\"-plane. In symbols we write\nand often think of the function \"f\" as a transformation from the \"z\"-plane (with coordinates (\"x\", \"y\")) into the \"w\"-plane (with coordinates (\"u\", \"v\")).\n\nArgand diagram refers to a geometric plot of complex numbers as points z=x+iy using the x-axis as the real axis and y-axis as the imaginary axis.. Such plots are named after Jean-Robert Argand (1768–1822), although they were first described by Norwegian–Danish land surveyor and mathematician Caspar Wessel (1745–1818). Argand diagrams are frequently used to plot the positions of the zeros and poles of a function in the complex plane. \n\nIt can be useful to think of the complex plane as if it occupied the surface of a sphere. Given a sphere of unit radius, place its center at the origin of the complex plane, oriented so that the equator on the sphere coincides with the unit circle in the plane, and the north pole is \"above\" the plane.\n\nWe can establish a one-to-one correspondence between the points on the surface of the sphere minus the north pole and the points in the complex plane as follows. Given a point in the plane, draw a straight line connecting it with the north pole on the sphere. That line will intersect the surface of the sphere in exactly one other point. The point will be projected onto the south pole of the sphere. Since the interior of the unit circle lies inside the sphere, that entire region () will be mapped onto the southern hemisphere. The unit circle itself () will be mapped onto the equator, and the exterior of the unit circle () will be mapped onto the northern hemisphere, minus the north pole. Clearly this procedure is reversible – given any point on the surface of the sphere that is not the north pole, we can draw a straight line connecting that point to the north pole and intersecting the flat plane in exactly one point.\n\nUnder this stereographic projection the north pole itself is not associated with any point in the complex plane. We perfect the one-to-one correspondence by adding one more point to the complex plane – the so-called \"point at infinity\" – and identifying it with the north pole on the sphere. This topological space, the complex plane plus the point at infinity, is known as the extended complex plane. We speak of a single \"point at infinity\" when discussing complex analysis. There are two points at infinity (positive, and negative) on the real number line, but there is only one point at infinity (the north pole) in the extended complex plane.\n\nImagine for a moment what will happen to the lines of latitude and longitude when they are projected from the sphere onto the flat plane. The lines of latitude are all parallel to the equator, so they will become perfect circles centered on the origin . And the lines of longitude will become straight lines passing through the origin (and also through the \"point at infinity\", since they pass through both the north and south poles on the sphere).\n\nThis is not the only possible yet plausible stereographic situation of the projection of a sphere onto a plane consisting of two or more values. For instance, the north pole of the sphere might be placed on top of the origin in a plane that is tangent to the circle. The details don't really matter. Any stereographic projection of a sphere onto a plane will produce one \"point at infinity\", and it will map the lines of latitude and longitude on the sphere into circles and straight lines, respectively, in the plane.\n\nWhen discussing functions of a complex variable it is often convenient to think of a cut in the complex plane. This idea arises naturally in several different contexts.\n\nConsider the simple two-valued relationship\n\nBefore we can treat this relationship as a single-valued function, the range of the resulting value must be restricted somehow. When dealing with the square roots of non-negative real numbers this is easily done. For instance, we can just define\nto be the non-negative real number \"y\" such that \"y\" = \"x\". This idea doesn't work so well in the two-dimensional complex plane. To see why, let's think about the way the value of \"f\"(\"z\") varies as the point \"z\" moves around the unit circle. We can write\n\nEvidently, as \"z\" moves all the way around the circle, \"w\" only traces out one-half of the circle. So one continuous motion in the complex plane has transformed the positive square root \"e\" = 1 into the negative square root \"e\" = −1.\n\nThis problem arises because the point \"z\" = 0 has just one square root, while every other complex number \"z\" ≠ 0 has exactly two square roots. On the real number line we could circumvent this problem by erecting a \"barrier\" at the single point \"x\" = 0. A bigger barrier is needed in the complex plane, to prevent any closed contour from completely encircling the branch point \"z\" = 0. This is commonly done by introducing a branch cut; in this case the \"cut\" might extend from the point \"z\" = 0 along the positive real axis to the point at infinity, so that the argument of the variable \"z\" in the cut plane is restricted to the range 0 ≤ arg(\"z\") < 2\"π\".\n\nWe can now give a complete description of \"w\" = \"z\". To do so we need two copies of the \"z\"-plane, each of them cut along the real axis. On one copy we define the square root of 1 to be e = 1, and on the other we define the square root of 1 to be \"e\" = −1. We call these two copies of the complete cut plane \"sheets\". By making a continuity argument we see that the (now single-valued) function \"w\" = \"z\" maps the first sheet into the upper half of the \"w\"-plane, where 0 ≤ arg(\"w\") < \"π\", while mapping the second sheet into the lower half of the \"w\"-plane (where \"π\" ≤ arg(\"w\") < 2\"π\").\n\nThe branch cut in this example doesn't have to lie along the real axis. It doesn't even have to be a straight line. Any continuous curve connecting the origin \"z\" = 0 with the point at infinity would work. In some cases the branch cut doesn't even have to pass through the point at infinity. For example, consider the relationship\n\nHere the polynomial \"z\" − 1 vanishes when \"z\" = ±1, so \"g\" evidently has two branch points. We can \"cut\" the plane along the real axis, from −1 to 1, and obtain a sheet on which \"g\"(\"z\") is a single-valued function. Alternatively, the cut can run from \"z\" = 1 along the positive real axis through the point at infinity, then continue \"up\" the negative real axis to the other branch point, \"z\" = −1.\n\nThis situation is most easily visualized by using the stereographic projection described above. On the sphere one of these cuts runs longitudinally through the southern hemisphere, connecting a point on the equator (\"z\" = −1) with another point on the equator (\"z\" = 1), and passing through the south pole (the origin, \"z\" = 0) on the way. The second version of the cut runs longitudinally through the northern hemisphere and connects the same two equatorial points by passing through the north pole (that is, the point at infinity).\n\nA meromorphic function is a complex function that is holomorphic and therefore analytic everywhere in its domain except at a finite, or countably infinite, number of points. The points at which such a function cannot be defined are called the poles of the meromorphic function. Sometimes all these poles lie in a straight line. In that case mathematicians may say that the function is \"holomorphic on the cut plane\". Here's a simple example.\n\nThe gamma function, defined by\nwhere \"γ\" is the Euler–Mascheroni constant, and has simple poles at 0, −1, −2, −3, ... because exactly one denominator in the infinite product vanishes when \"z\" is zero, or a negative integer. Since all its poles lie on the negative real axis, from \"z\" = 0 to the point at infinity, this function might be described as \"holomorphic on the cut plane, the cut extending along the negative real axis, from 0 (inclusive) to the point at infinity.\"\n\nAlternatively, Γ(\"z\") might be described as \"holomorphic in the cut plane with −\"π\" < arg(\"z\") < \"π\" and excluding the point \"z\" = 0.\"\n\nNotice that this cut is slightly different from the branch cut we've already encountered, because it actually \"excludes\" the negative real axis from the cut plane. The branch cut left the real axis connected with the cut plane on one side (0 ≤ \"θ\"), but severed it from the cut plane along the other side (\"θ\" < 2\"π\").\n\nOf course, it's not actually necessary to exclude the entire line segment from \"z\" = 0 to −∞ to construct a domain in which Γ(\"z\") is holomorphic. All we really have to do is puncture the plane at a countably infinite set of points {0, −1, −2, −3, ...}. But a closed contour in the punctured plane might encircle one or more of the poles of Γ(\"z\"), giving a contour integral that is not necessarily zero, by the residue theorem. By cutting the complex plane we ensure not only that Γ(\"z\") is holomorphic in this restricted domain – we also ensure that the contour integral of Γ over any closed curve lying in the cut plane is identically equal to zero.\n\nMany complex functions are defined by infinite series, or by continued fractions. A fundamental consideration in the analysis of these infinitely long expressions is identifying the portion of the complex plane in which they converge to a finite value. A cut in the plane may facilitate this process, as the following examples show.\n\nConsider the function defined by the infinite series\n\nSince \"z\" = (−\"z\") for every complex number \"z\", it's clear that \"f\"(\"z\") is an even function of \"z\", so the analysis can be restricted to one half of the complex plane. And since the series is undefined when\nit makes sense to cut the plane along the entire imaginary axis and establish the convergence of this series where the real part of \"z\" is not zero before undertaking the more arduous task of examining \"f\"(\"z\") when \"z\" is a pure imaginary number.\n\nIn this example the cut is a mere convenience, because the points at which the infinite sum is undefined are isolated, and the \"cut\" plane can be replaced with a suitably \"punctured\" plane. In some contexts the cut is necessary, and not just convenient. Consider the infinite periodic continued fraction\n\nIt can be shown that \"f\"(\"z\") converges to a finite value if and only if \"z\" is not a negative real number such that \"z\" < −¼. In other words, the convergence region for this continued fraction is the cut plane, where the cut runs along the negative real axis, from −¼ to the point at infinity.\n\nWe have already seen how the relationship\ncan be made into a single-valued function by splitting the domain of \"f\" into two disconnected sheets. It is also possible to \"glue\" those two sheets back together to form a single Riemann surface on which can be defined as a holomorphic function whose image is the entire \"w\"-plane (except for the point ). Here's how that works.\n\nImagine two copies of the cut complex plane, the cuts extending along the positive real axis from to the point at infinity. On one sheet define , so that , by definition. On the second sheet define , so that , again by definition. Now flip the second sheet upside down, so the imaginary axis points in the opposite direction of the imaginary axis on the first sheet, with both real axes pointing in the same direction, and \"glue\" the two sheets together (so that the edge on the first sheet labeled \" is connected to the edge labeled \" on the second sheet, and the edge on the second sheet labeled \" is connected to the edge labeled \" on the first sheet). The result is the Riemann surface domain on which is single-valued and holomorphic (except when ).\n\nTo understand why \"f\" is single-valued in this domain, imagine a circuit around the unit circle, starting with on the first sheet. When we are still on the first sheet. When we have crossed over onto the second sheet, and are obliged to make a second complete circuit around the branch point before returning to our starting point, where is equivalent to , because of the way we glued the two sheets together. In other words, as the variable \"z\" makes two complete turns around the branch point, the image of \"z\" in the \"w\"-plane traces out just one complete circle.\n\nFormal differentiation shows that\n\nfrom which we can conclude that the derivative of \"f\" exists and is finite everywhere on the Riemann surface, except when (that is, \"f\" is holomorphic, except when ).\n\nHow can the Riemann surface for the function\nalso discussed above, be constructed? Once again we begin with two copies of the \"z\"-plane, but this time each one is cut along the real line segment extending from to – these are the two branch points of \"g\"(\"z\"). We flip one of these upside down, so the two imaginary axes point in opposite directions, and glue the corresponding edges of the two cut sheets together. We can verify that \"g\" is a single-valued function on this surface by tracing a circuit around a circle of unit radius centered at . Commencing at the point on the first sheet we turn halfway around the circle before encountering the cut at . The cut forces us onto the second sheet, so that when \"z\" has traced out one full turn around the branch point , \"w\" has taken just one-half of a full turn, the sign of \"w\" has been reversed (since ), and our path has taken us to the point on the second sheet of the surface. Continuing on through another half turn we encounter the other side of the cut, where , and finally reach our starting point ( on the first sheet) after making two full turns around the branch point.\n\nThe natural way to label in this example is to set on the first sheet, with on the second. The imaginary axes on the two sheets point in opposite directions so that the counterclockwise sense of positive rotation is preserved as a closed contour moves from one sheet to the other (remember, the second sheet is \"upside down\"). Imagine this surface embedded in a three-dimensional space, with both sheets parallel to the \"xy\"-plane. Then there appears to be a vertical hole in the surface, where the two cuts are joined together. What if the cut is made from down the real axis to the point at infinity, and from , up the real axis until the cut meets itself? Again a Riemann surface can be constructed, but this time the \"hole\" is horizontal. Topologically speaking, both versions of this Riemann surface are equivalent – they are orientable two-dimensional surfaces of genus one.\n\nIn control theory, one use of the complex plane is known as the 's-plane'. It is used to visualise the roots of the equation describing a system's behaviour (the characteristic equation) graphically. The equation is normally expressed as a polynomial in the parameter 's' of the Laplace transform, hence the name 's' plane. Points in the s-plane take the form formula_18, where \"'j'\" is used instead of the usual \"'i\"' to represent the imaginary component.\n\nAnother related use of the complex plane is with the Nyquist stability criterion. This is a geometric principle which allows the stability of a closed-loop feedback system to be determined by inspecting a Nyquist plot of its open-loop magnitude and phase response as a function of frequency (or loop transfer function) in the complex plane.\n\nThe 'z-plane' is a discrete-time version of the s-plane, where z-transforms are used instead of the Laplace transformation.\n\nThe complex plane is associated with two distinct quadratic spaces. For a point \"z\" = \"x\" + \"iy\" in the complex plane, the squaring function \"z\" and the norm-squared formula_19 are both quadratic forms. The former is frequently neglected in the wake of the latter's use in setting a metric on the complex plane. These distinct faces of the complex plane as a quadratic space arise in the construction of algebras over a field with the Cayley–Dickson process. That procedure can be applied to any field, and different results occur for the fields ℝ and ℂ: when ℝ is the take-off field, then ℂ is constructed with the quadratic form formula_20 but the process can also begin with ℂ and \"z\", and that case generates algebras that differ from those derived from ℝ. In any case, the algebras generated are composition algebras; in this case the complex plane is the point set for two distinct composition algebras.\n\nThe preceding sections of this article deal with the complex plane in terms of a geometric representation of the complex numbers. Although this usage of the term \"complex plane\" has a long and mathematically rich history, it is by no means the only mathematical concept that can be characterized as \"the complex plane\". There are at least three additional possibilities.\n\nWhile the terminology \"complex plane\" is historically accepted, the object could be more appropriately named \"complex line\" as it is a 1-dimensional complex vector space.\n\n\n\n"}
{"id": "39431504", "url": "https://en.wikipedia.org/wiki?curid=39431504", "title": "Dold–Kan correspondence", "text": "Dold–Kan correspondence\n\nIn mathematics, more precisely, in the theory of simplicial sets, the Dold–Kan correspondence (named after Albrecht Dold and Daniel Kan) states that there is an equivalence between the category of (nonnegatively graded) chain complexes and the category of simplicial abelian groups. Moreover, under the equivalence, the formula_1th homology group of a chain complex is the formula_1th homotopy group of the corresponding simplicial abelian group, and a chain homotopy corresponds to a simplicial homotopy. (In fact, the correspondence preserves the respective standard model structures.) \n\nExample: Let \"C\" be a chain complex that has an abelian group \"A\" in degree \"n\" and zero in other degrees. Then the corresponding simplicial group is the Eilenberg–MacLane space formula_3.\n\nThere is also an ∞-category-version of a Dold–Kan correspondence.\n\nThe book \"Nonabelian Algebraic Topology\" cited below has a Section 14.8 on cubical versions of the Dold-Kan theorem, and relates them to a previous equivalence of categories \nbetween cubical omega-groupoids and crossed complexes, which is fundamental to the work of that book.\n\n\n"}
{"id": "677451", "url": "https://en.wikipedia.org/wiki?curid=677451", "title": "Effect system", "text": "Effect system\n\nIn computing, an effect system is a formal system which describes the computational effects of computer programs, such as side effects. An effect system can be used to provide a compile-time check of the possible effects of the program.\n\nThe effect system extends the notion of type to have an \"effect\" component, which comprises an effect kind and a region. The effect kind describes \"what\" is being done, and the region describes \"with what\" it is being done.\n\nAn effect system is typically an extension of a type system. The term \"type and effect system\" is sometimes used in this case. Often, a type of a value is denoted together with its effect as \"type ! effect\", where both the type component and the effect component mention certain regions (for example, a type of a mutable memory cell is parameterized by the label of the memory region in which the cell resides).\n\nSome examples of the behaviors that can be described by effect systems include:\n\nEffect systems may be used to prove the external purity of certain internally impure definitions: for example, if a function internally allocates and modifies a region of memory, but the function's type does not mention the region, then the corresponding effect may be erased from the function's effect.\n\n\n"}
{"id": "11354632", "url": "https://en.wikipedia.org/wiki?curid=11354632", "title": "Eleazar Chisma", "text": "Eleazar Chisma\n\n\"For other people named Eleazer. see: Eleazar (name)\"\n\nEleazar Chisma (Ḥisma; , \"\"Eleazar Chasma\", or אלעזר בן חסמא, \"Eleazar ben Chasma\"\") was a tanna (sage) of the second and third generations (2nd century); he was a disciple of Joshua ben Hananiah and Gamaliel II. (Ḥag. 3a; Hor. 10a).\n\nIn their use of the word \"ben\" in connection with his cognomen \"Ḥisma\" or \"Ḥasma\" (see Geiger, \"Schriften,\" iv. 343, and Strack, \"Einleitung in den Thalmud,\" 2d ed., p. 81), the sources are inconsistent; its insertion, however, seems justifiable. \"Ḥisma,\" is not an adjectival cognomen (see Eleazar I.), but a locative, the place probably being identical with Hizmeh (see Luncz, \"Jerusalem,\" vi. 67; ; hence \"ben Ḥisma\" means \"son of [= \"native of\"] Ḥisma\" (compare R. H. 17a; Meg. 19a; Ḳid. ii. 3).\n\nSeveral \"halakot\" are preserved under Eleazar's name in the \"Mishnah\" (Ter. iii. 5; B. M. vii. 5), and he is met with in halakic controversies with Eleazar ben Azariah and Rabbi Akiva (Neg. vii. 2; Sifra, Tazria', i. 2), and with Eliezer ben Jacob I (Pes. 32a; Yalḳ., Lev. 638); and to him is ascribed the economic rule that the employee is not entitled to a proportion of his employer's produce greater than the amount of his wages (B. M. vii. 5, 92a; Sifre, Deut. 266).\n\nSome \"haggadot\" also are ascribed to him (Mek., Beshallaḥ Wayassa', 4; ib., Amalek, 1; Yoma 19b). Conjointly with Rabbi Joshua, he gives an allegorical reason for Amalek's attack on Israel (Ex. xvii. 8 et seq.) just at the time it occurred. Citing Job viii. 11, \"Can a rush grow up without mire? Can the flag grow without water?\" he remarks, \"Even so is it impossible for Israel to flourish without the Law; and since they had neglected the Law [see Ex. xvii. 1-7], an enemy was ordered out to war against them\" (compare Yalḳ. to Ex. l.c., § 262; anonymous in Yalḳ. to Job l.c., § 904). Again, he cites Isa. xliii. 22, \"But thou hast not called on me, O Jacob,\" and applies it to those who are not devout in their prayers, but while reciting the \"Shema'\" communicate with their neighbors by sign language (compare Yalḳ. to Isa. l.c., § 318).\n\nNot only was Chizma possessed of wide rabbinic learning, but he was also an adept in the sciences. Joshua, introducing him and Johanan b. (Gudgada) Nuri to the notice of Patriarch Gamaliel II, remarked of them that they could approximately calculate the number of drops contained in the ocean (Hor. 10a). As they were very poor, Gamaliel appointed them to remunerative offices in the academy (Sifre, Deut. 14; Yalḳ., Deut. 902; Hor. l.c.). Probably it was here—because the academicians sought from him instruction in secular science—that Eleazar remarked, \"The laws concerning birds' nests and those concerning the incipient uncleanness of woman are elements of the Law, while astronomy and geometry are only condiments of wisdom\" (Ab. iii. 18; Ab. R. N. xxvii. 2).\n\n\n"}
{"id": "3521238", "url": "https://en.wikipedia.org/wiki?curid=3521238", "title": "Exhaustion by compact sets", "text": "Exhaustion by compact sets\n\nIn mathematics, especially analysis, exhaustion by compact sets of an open set \"E\" in the Euclidean space R (or a manifold with countable base) is an increasing sequence of compact sets formula_1, where by increasing we mean formula_1 is a subset of formula_3, with the limit (union) of the sequence being \"E\". \n\nSometimes one requires the sequence of compact sets to satisfy one more property—that formula_1 is contained in the interior of formula_3 for each formula_6. This, however, is dispensed in R or a manifold with countable base.\n\nFor example, consider a unit open disk and the concentric closed disk of each radius inside. That is let formula_7 and formula_8. Then taking the limit (union) of the sequence formula_1 gives \"E\". The example can be easily generalized in other dimensions.\n\n\n"}
{"id": "1069091", "url": "https://en.wikipedia.org/wiki?curid=1069091", "title": "Exponent bias", "text": "Exponent bias\n\nIn IEEE 754 floating point numbers, the exponent is biased in the engineering sense of the word – the value stored is offset from the actual value by the exponent bias. \nBiasing is done because exponents have to be signed values in order to be able to represent both tiny and huge values, but two's complement, the usual representation for signed values, would make comparison harder.\n\nTo solve this problem the exponent is stored as an unsigned value suitable for comparison, and when being interpreted it is converted into an exponent within a signed range by subtracting the bias.\n\nBy arranging the fields such that the sign bit takes the most significant bit position, the biased exponent takes the middle position, then the mantissa will be the least significant bits and the resulting value will be ordered properly. This is the case whether or not it is interpreted as a floating point or integer value. The purpose of this is to enable high speed comparisons between floating point numbers using fixed point hardware.\n\nTo calculate the bias for an arbitrarily sized floating point number apply the formula 2 − 1 where k is the number of bits in the exponent.\n\nWhen interpreting the floating-point number, the bias is subtracted to retrieve the actual exponent.\n\n\nThe floating point format of the IBM 704 introduced the use of a biased exponent in 1954.\n\n"}
{"id": "9309", "url": "https://en.wikipedia.org/wiki?curid=9309", "title": "Extractor (mathematics)", "text": "Extractor (mathematics)\n\nAn formula_1 -extractor is a bipartite graph with formula_2 nodes on the left and formula_3 nodes on the right such that each node on the left has formula_4 neighbors (on the right), which has the added property that\nfor any subset formula_5 of the left vertices of size at least formula_6, the distribution on right vertices obtained by choosing a random node in formula_5 and then following a random edge to get a node x on the right side is formula_8-close to the uniform distribution in terms of total variation distance.\n\nA disperser is a related graph. \n\nAn equivalent way to view an extractor is as a bivariate function \n\nin the natural way. With this view it turns out that the extractor property is equivalent to: for any source of randomness formula_10 that gives formula_11 bits with min-entropy formula_12, the distribution formula_13 is formula_8-close to formula_15, where formula_16 denotes the uniform distribution on formula_17.\n\nExtractors are interesting when they can be constructed with small formula_18 relative to formula_2 and formula_3 is as close to formula_21 (the total randomness in the input sources) as possible.\n\nExtractor functions were originally researched as a way to \"extract\" randomness from weakly random sources. \"See\" randomness extractor.\n\nUsing the probabilistic method it is easy to show that extractor graphs with really good parameters exist. The challenge is to find explicit or polynomial time computable examples of such graphs with good parameters. Algorithms that compute extractor (and disperser) graphs have found many applications in computer science.\n\n"}
{"id": "4142944", "url": "https://en.wikipedia.org/wiki?curid=4142944", "title": "Fermat's theorem (stationary points)", "text": "Fermat's theorem (stationary points)\n\nIn mathematics, Fermat's theorem (also known as interior extremum theorem) is a method to find local maxima and minima of differentiable functions on open sets by showing that every local extremum of the function is a stationary point (the function derivative is zero at that point). Fermat's theorem is a theorem in real analysis, named after Pierre de Fermat.\n\nBy using Fermat's theorem, the potential extrema of a function formula_1, with derivative formula_2, are found by solving an equation in formula_2. Fermat's theorem gives only a necessary condition for extreme function values, as some stationary points are inflection points (not a maximum or minimum). The function's second derivative, if it exists, can determine if any stationary point is a maximum, minimum, or inflection point.\n\nOne way to state Fermat's theorem is that, if a function has a local extremum at some point and is differentiable there, then the function's derivative at that point must be zero. In precise mathematical language:\n\nAnother way to understand the theorem is via the contrapositive statement: if the derivative of a function at any point is not zero, then there is not a local extremum at that point. Formally:\n\nThe global extrema of a function \"f\" on a domain \"A\" occur only at boundaries, non-differentiable points, and stationary points.\nIf formula_13 is a global extremum of \"f\", then one of the following is true:\n\nIn higher dimensions, exactly the same statement holds; however, the proof is slightly more complicated. The complication is that in 1 dimension, one can either move left or right from a point, while in higher dimensions, one can move in many directions. Thus, if the derivative does not vanish, one must argue that there is \"some\" direction in which the function increases – and thus in the opposite direction the function decreases. This is the only change to the proof or the analysis.\n\nThe statement can also be extended to differentiable manifolds. If formula_19 is a differentiable function on a manifold formula_20, then its local extrema must be critical points of formula_6, in particular points where the exterior derivative formula_22 is zero.\n\nFermat's theorem is central to the calculus method of determining maxima and minima: in one dimension, one can find extrema by simply computing the stationary points (by computing the zeros of the derivative), the non-differentiable points, and the boundary points, and then investigating this set to determine the extrema.\n\nOne can do this either by evaluating the function at each point and taking the maximum, or by analyzing the derivatives further, using the first derivative test, the second derivative test, or the higher-order derivative test.\n\nIntuitively, a differentiable function is approximated by its derivative – a differentiable function behaves infinitesimally like a linear function formula_23 or more precisely, formula_24 Thus, from the perspective that \"if \"f\" is differentiable and has non-vanishing derivative at formula_25 then it does not attain an extremum at formula_25\" the intuition is that if the derivative at formula_13 is positive, the function is \"increasing\" near formula_25 while if the derivative is negative, the function is \"decreasing\" near formula_29 In both cases, it cannot attain a maximum or minimum, because its value is changing. It can only attain a maximum or minimum if it \"stops\" – if the derivative vanishes (or if it is not differentiable, or if one runs into the boundary and cannot continue). However, making \"behaves like a linear function\" precise requires careful analytic proof.\n\nMore precisely, the intuition can be stated as: if the derivative is positive, there is \"some point\" to the right of formula_13 where \"f\" is greater, and \"some point\" to the left of formula_13 where \"f\" is less, and thus \"f\" attains neither a maximum nor a minimum at formula_29 Conversely, if the derivative is negative, there is a point to the right which is lesser, and a point to the left which is greater. Stated this way, the proof is just translating this into equations and verifying \"how much greater or less\".\n\nThe intuition is based on the behavior of polynomial functions. Assume that function \"f\" has a maximum at \"x\", the reasoning being similar for a function minimum. If formula_33 is a local maximum then, roughly, there is a (possibly small) neighborhood of formula_8 such as the function \"is increasing before\" and \"decreasing after\" formula_8. As the derivative is positive for an increasing function and negative for a decreasing function, formula_2 is positive before and negative after formula_8. formula_2 doesn't skip values (by Darboux's theorem), so it has to be zero at some point between the positive and negative values. The only point in the neighbourhood where it is possible to have formula_39 is formula_8.\n\nThe theorem (and its proof below) is more general than the intuition in that it doesn't require the function to be differentiable over a neighbourhood around formula_8. It is sufficient for the function to be differentiable only in the extreme point.\n\nSuppose that \"f\" is differentiable at formula_42 with derivative \"K,\" and assume without loss of generality that formula_43 so the tangent line at formula_13 has positive slope (is increasing). Then there is a neighborhood of formula_13 on which the secant lines through formula_13 all have positive slope, and thus to the right of formula_25 \"f\" is greater, and to the left of formula_25 \"f\" is lesser.\n\nThe schematic of the proof is:\n\nFormally, by the definition of derivative, formula_52 means that\nIn particular, for sufficiently small formula_54 (less than some formula_55), the fraction must be at least formula_56 by the definition of limit. Thus on the interval formula_57 one has:\none has replaced the \"equality\" in the limit (an infinitesimal statement) with an \"inequality\" on a neighborhood (a local statement). Thus, rearranging the equation, if formula_59 then:\nso on the interval to the right, \"f\" is greater than formula_61 and if formula_62 then:\nso on the interval to the left, \"f\" is less than formula_64\n\nThus formula_13 is not a local or global maximum or minimum of \"f.\"\n\nAlternatively, one can start by assuming that formula_8 is a local maximum, and then prove that the derivative is 0.\n\nSuppose that formula_8 is a local maximum (a similar proof applies if formula_8 is a local minimum). Then there formula_69 such that formula_70 and such that we have formula_71 with formula_72. Hence for any formula_73 we notice that it holds\n\nSince the limit of this ratio as formula_75 gets close to 0 from above exists and is equal to formula_76 we conclude that formula_77. On the other hand for formula_78 we notice that\n\nbut again the limit as formula_75 gets close to 0 from below exists and is equal to formula_76 so we also have formula_82.\n\nHence we conclude that formula_83\n\nA subtle misconception that is often held in the context of Fermat's theorem is to assume that it makes a stronger statement about local behavior than it does. Notably, Fermat's theorem does \"not\" say that functions (monotonically) \"increase up to\" or \"decrease down from\" a local maximum. This is very similar to the misconception that a limit means \"monotonically getting closer to a point\". For \"well-behaved functions\" (which here mean continuously differentiable), some intuitions hold, but in general functions may be ill-behaved, as illustrated below. The moral is that derivatives determine \"infinitesimal\" behavior, and that \"continuous\" derivatives determine \"local\" behavior.\n\nIf \"f\" is continuously differentiable formula_84 on an open neighborhood of the point formula_13, then formula_86 does mean that \"f\" is increasing on a neighborhood of formula_25 as follows.\n\nIf formula_88 and formula_89 then\nby continuity of the derivative, there is some formula_90 such that formula_91 formula_92. Then \"f\" is increasing on this interval, by the mean value theorem: the slope of any secant line is at least formula_56 as it equals the slope of some tangent line.\n\nHowever, in the general statement of Fermat's theorem, where one is only given that the derivative \"at\" formula_13 is positive, one can only conclude that secant lines \"through\" formula_13 will have positive slope, for secant lines between formula_13 and near enough points.\n\nConversely, if the derivative of \"f\" at a point is zero (formula_13 is a stationary point), one cannot in general conclude anything about the local behavior of \"f\" – it may increase to one side and decrease to the other (as in formula_98), increase to both sides (as in formula_99), decrease to both sides (as in formula_100), or behave in more complicated ways, such as oscillating (as in formula_101, as discussed below).\n\nOne can analyze the infinitesimal behavior via the second derivative test and higher-order derivative test, if the function is differentiable enough, and if the first non-vanishing derivative at formula_13 is a continuous function, one can then conclude local behavior (i.e., if formula_103 is the first non-vanishing derivative, and formula_104 is continuous, so formula_105), then one can treat \"f\" as locally close to a polynomial of degree \"k,\" since it behaves approximately as formula_106 but if the \"k\"th derivative is not continuous, one cannot draw such conclusions, and it may behave rather differently.\n\nConsider the function formula_107 – it oscillates increasingly rapidly between formula_108 and formula_109 as \"x\" approaches 0. Consider then formula_110 – this oscillates increasingly rapidly between 0 and formula_111 as \"x\" approaches 0. If one extends this function by formula_112 then the function is continuous and everywhere differentiable (it is differentiable at 0 with derivative 0), but has rather unexpected behavior near 0: in any neighborhood of 0 it attains 0 infinitely many times, but also equals formula_111 (a positive number) infinitely often.\n\nContinuing in this vein, formula_114 oscillates between formula_115 and formula_116 and formula_117 is a local and global minimum, but on no neighborhood of 0 is it decreasing down to or increasing up from 0 – it oscillates wildly near 0.\n\nThis pathology can be understood because, while the function is everywhere differentiable, it is not \"continuously\" differentiable: the limit of formula_118 as formula_119 does not exist, so the derivative is not continuous at 0. This reflects the oscillation between increasing and decreasing values as it approaches 0.\n\n\n"}
{"id": "3247635", "url": "https://en.wikipedia.org/wiki?curid=3247635", "title": "Fibered manifold", "text": "Fibered manifold\n\nIn differential geometry, in the category of differentiable manifolds, a fibered manifold is a surjective submersion\n\ni.e. a surjective differentiable mapping such that at each point the tangent mapping\n\nis surjective, or, equivalently, its rank equals dim .\n\nIn topology, the words fiber (Faser in German) and fiber space (gefaserter Raum) appeared for the first time in a paper by Seifert in 1932, but his definitions are limited to a very special case. The main difference from the present day conception of a fiber space, however, was that for Seifert what is now called the base space (topological space) of a fiber (topological) space \"E\" was not part of the structure, but derived from it as a quotient space of \"E\". The first definition of fiber space is given by Hassler Whitney in 1935 under the name sphere space, but in 1940 Whitney changed the name to sphere bundle.\n\nThe theory of fibered spaces, of which vector bundles, principal bundles, topological fibrations and fibered manifolds are a special case, is attributed to Seifert, Hopf, Feldbau, Whitney, Steenrod, Ehresmann, Serre, and others.\n\nA triple where and are differentiable manifolds and is a surjective submersion, is called a fibered manifold. \"E\" is called the total space, \"B\" is called the base.\n\n\n\nLet (resp. ) be an -dimensional (resp. -dimensional) manifold. A fibered manifold admits fiber charts. We say that a chart on is a fiber chart, or is adapted to the surjective submersion if there exists a chart on such that and\n\nwhere\n\nThe above fiber chart condition may be equivalently expressed by\n\nwhere\n\nis the projection onto the first coordinates. The chart is then obviously unique. In view of the above property, the fibered coordinates of a fiber chart are usually denoted by where , , the coordinates of the corresponding chart on are then denoted, with the obvious convention, by where .\n\nConversely, if a surjection admits a fibered atlas, then is a fibered manifold.\n\nLet be a fibered manifold and any manifold. Then an open covering of together with maps\n\ncalled trivialization maps, such that\n\nis a local trivialization with respect to .\n\nA fibered manifold together with a manifold is a fiber bundle with typical fiber (or just fiber) if it admits a local trivialization with respect to . The atlas is then called a bundle atlas.\n\n\n\n"}
{"id": "36099150", "url": "https://en.wikipedia.org/wiki?curid=36099150", "title": "Fundamental theorem of ideal theory in number fields", "text": "Fundamental theorem of ideal theory in number fields\n\nIn ideal theory, the fundamental theorem of ideal theory in number fields states that every nonzero proper ideal in the ring of integers of a number field admits unique factorization into a product of nonzero prime ideals.\n\n"}
{"id": "1277699", "url": "https://en.wikipedia.org/wiki?curid=1277699", "title": "G-structure on a manifold", "text": "G-structure on a manifold\n\nIn differential geometry, a \"G\"-structure on an \"n\"-manifold \"M\", for a given structure group \"G\", is a \"G\"-subbundle of the tangent frame bundle F\"M\" (or GL(\"M\")) of \"M\".\n\nThe notion of \"G\"-structures includes various classical structures that can be defined on manifolds, which in some cases are tensor fields. For example, for the orthogonal group, an O(\"n\")-structure defines a Riemannian metric, and for the special linear group an SL(\"n\",R)-structure is the same as a volume form. For the trivial group, an {\"e\"}-structure consists of an absolute parallelism of the manifold.\n\nGeneralising this idea to arbitrary principal bundles on topological spaces, one can ask if a principal formula_1-bundle over a group formula_1 \"comes from\" a subgroup formula_3 of formula_1. This is called reduction of the structure group (to formula_3).\n\nSeveral structures on manifolds, such as a complex structure, a symplectic structure, or a Kähler structure, are \"G\"-structures with an additional integrability condition.\n\nOne can ask if a principal formula_1-bundle over a group formula_1 \"comes from\" a subgroup formula_3 of formula_1. This is called reduction of the structure group (to formula_3), and makes sense for any map formula_11, which need not be an inclusion map (despite the terminology).\n\nIn the following, let formula_12 be a topological space, formula_13 topological groups and a group homomorphism formula_14.\n\nGiven a principal formula_1-bundle formula_16 over formula_12, a \"reduction of the structure group\" (from formula_1 to formula_3) is an \"formula_3\"-bundle formula_21 and an isomorphism formula_22 of the associated bundle to the original bundle.\n\nGiven a map formula_23, where formula_24 is the classifying space for formula_1-bundles, a \"reduction of the structure group\" is a map formula_26 and a homotopy formula_27.\n\nReductions of the structure group do not always exist. If they exist, they are usually not essentially unique, since the isomorphism formula_28 is an important part of the data.\n\nAs a concrete example, every even-dimensional real vector space is isomorphic to the underlying real space of a complex vector space: it admits a linear complex structure. A real vector bundle admits an almost complex structure if and only if it is isomorphic to the underlying real bundle of a complex vector bundle. This is then a reduction along the inclusion \"GL\"(\"n\",C) → \"GL\"(2\"n\",R)\n\nIn terms of transition maps, a \"G\"-bundle can be reduced if and only if the transition maps can be taken to have values in \"H\". Note that the term \"reduction\" is misleading: it suggests that \"H\" is a subgroup of \"G\", which is often the case, but need not be (for example for spin structures): it's properly called a lifting.\n\nMore abstractly, \"\"G\"-bundles over \"X\"\" is a functor in \"G\": given a map \"H\" → \"G\", one gets a map from \"H\"-bundles to \"G\"-bundles by inducing (as above). Reduction of the structure group of a \"G\"-bundle \"B\" is choosing an \"H\"-bundle whose image is \"B\".\n\nThe inducing map from \"H\"-bundles to \"G\"-bundles is in general neither onto nor one-to-one, so the structure group cannot always be reduced, and when it can, this reduction need not be unique. For example, not every manifold is orientable, and those that are orientable admit exactly two orientations.\n\nIf \"H\" is a closed subgroup of \"G\", then there is a natural one-to-one correspondence between reductions of a \"G\"-bundle \"B\" to \"H\" and global sections of the fiber bundle \"B\"/\"H\" obtained by quotienting \"B\" by the right action of \"H\". Specifically, the fibration \"B\" → \"B\"/\"H\" is a principal \"H\"-bundle over \"B\"/\"H\". If σ : \"X\" → \"B\"/\"H\" is a section, then the pullback bundle \"B\" = σ\"B\" is a reduction of \"B\".\n\nEvery vector bundle of dimension formula_29 has a canonical formula_30-bundle, the frame bundle. In particular, every smooth manifold has a canonical vector bundle, the tangent bundle. For a Lie group formula_1 and a group homomorphism formula_32, a formula_1-structure is a reduction of the structure group of the frame bundle to formula_1.\n\nThe following examples are defined for real vector bundles, particularly the tangent bundle of a smooth manifold.\n\nSome formula_1-structures are defined terms of others: Given a Riemannian metric on an oriented manifold, a formula_1-structure for the 2-fold cover formula_37 is a spin structure. (Note that the group homomorphism here is \"not\" an inclusion.)\n\nAlthough the theory of principal bundles plays an important role in the study of \"G\"-structures, the two notions are different. A \"G\"-structure is a principal subbundle of the tangent frame bundle, but the fact that the \"G\"-structure bundle \"consists of tangent frames\" is regarded as part of the data. For example, consider two Riemannian metrics on R. The associated O(\"n\")-structures are isomorphic if and only if the metrics are isometric. But, since R is contractible, the underlying O(\"n\")-bundles are always going to be isomorphic as principal bundles because the only bundles over contractible spaces are trivial bundles.\n\nThis fundamental difference between the two theories can be captured by giving an additional piece of data on the underlying \"G\"-bundle of a \"G\"-structure: the solder form. The solder form is what ties the underlying principal bundle of the \"G\"-structure to the local geometry of the manifold itself by specifying a canonical isomorphism of the tangent bundle of \"M\" to an associated vector bundle. Although the solder form is not a connection form, it can sometimes be regarded as a precursor to one.\n\nIn detail, suppose that \"Q\" is the principal bundle of a \"G\"-structure. If \"Q\" is realized as a reduction of the frame bundle of \"M\", then the solder form is given by the pullback of the tautological form of the frame bundle along the inclusion. Abstractly, if one regards \"Q\" as a principal bundle independently of its realization as a reduction of the frame bundle, then the solder form consists of a representation ρ of \"G\" on R and an isomorphism of bundles θ : \"TM\" → \"Q\" × R.\n\nSeveral structures on manifolds, such as a complex structure, a symplectic structure, or a Kähler structure, are \"G\"-structures (and thus can be obstructed), but need to satisfy an additional integrability condition. Without the corresponding integrability condition, the structure is instead called an \"almost\" structure, as in an almost complex structure, an almost symplectic structure, or an almost Kähler structure.\n\nSpecifically, a symplectic manifold structure is a stronger concept than a \"G\"-structure for the symplectic group. A symplectic structure on a manifold is a two-form \"ω\" on \"M\" that is non-degenerate (which is an formula_38-structure, or almost symplectic structure), \"together with\" the extra condition that d\"ω\" = 0; this latter is called an integrability condition.\n\nSimilarly, foliations correspond to \"G\"-structures coming from block matrices, together with integrability conditions so that the Frobenius theorem applies.\n\nA flat \"G\"-structure is a \"G\"-structure \"P\" having a global section (\"V\"...,\"V\") consisting of commuting vector fields. A \"G\"-structure is integrable (or \"locally flat\") if it is locally isomorphic to a flat \"G\"-structure.\n\nThe set of diffeomorphisms of \"M\" that preserve a \"G\"-structure is called the \"automorphism group\" of that structure. For an O(\"n\")-structure they are the group of isometries of the Riemannian metric and for an SL(\"n\",R)-structure volume preserving maps.\n\nLet \"P\" be a \"G\"-structure on a manifold \"M\", and \"Q\" a \"G\"-structure on a manifold \"N\". Then an isomorphism of the \"G\"-structures is a diffeomorphism \"f\" : \"M\" → \"N\" such that the pushforward of linear frames \"f\" : \"FM\" → \"FN\" restricts to give a mapping of \"P\" into \"Q\". (Note that it is sufficient that \"Q\" be contained within the image of \"f\".) The \"G\"-structures \"P\" and \"Q\" are locally isomorphic if \"M\" admits a covering by open sets \"U\" and a family of diffeomorphisms \"f\" : \"U\" → \"f\"(\"U\") ⊂ \"N\" such that \"f\" induces an isomorphism of \"P\"| → \"Q\"|.\n\nAn automorphism of a \"G\"-structure is an isomorphism of a \"G\"-structure \"P\" with itself. Automorphisms arise frequently in the study of transformation groups of geometric structures, since many of the important geometric structures on a manifold can be realized as \"G\"-structures.\n\nA wide class of equivalence problems can be formulated in the language of \"G\"-structures. For example, a pair of Riemannian manifolds are (locally) equivalent if and only if their bundles of orthonormal frames are (locally) isomorphic \"G\"-structures. In this view, the general procedure for solving an equivalence problem is to construct a system of invariants for the \"G\"-structure which are then sufficient to determine whether a pair of \"G\"-structures are locally isomorphic or not.\n\nLet \"Q\" be a \"G\"-structure on \"M\". A principal connection on the principal bundle \"Q\" induces a connection on any associated vector bundle: in particular on the tangent bundle. A linear connection ∇ on \"TM\" arising in this way is said to be compatible with \"Q\". Connections compatible with \"Q\" are also called adapted connections.\n\nConcretely speaking, adapted connections can be understood in terms of a moving frame. Suppose that \"V\" is a basis of local sections of \"TM\" (i.e., a frame on \"M\") which defines a section of \"Q\". Any connection ∇ determines a system of basis-dependent 1-forms ω via\n\nwhere, as a matrix of 1-forms, ω ∈ Ω(M)⊗gl(\"n\"). An adapted connection is one for which ω takes its values in the Lie algebra g of \"G\".\n\nAssociated to any \"G\"-structure is a notion of torsion, related to the torsion of a connection. Note that a given \"G\"-structure may admit many different compatible connections which in turn can have different torsions, but in spite of this it is possible to give an independent notion of torsion \"of the G-structure\" as follows.\n\nThe difference of two adapted connections is a 1-form on \"M\" with values in the adjoint bundle Ad. That is to say, the space \"A\" of adapted connections is an affine space\nfor Ω(Ad).\n\nThe torsion of an adapted connection defines a map\n\nto 2-forms with coefficients in \"TM\". This map is linear; its linearization \n\nis called the algebraic torsion map. Given two adapted connections ∇ and ∇′, their torsion tensors \"T\", \"T\" differ by τ(∇−∇′). Therefore, the image of \"T\" in coker(τ) is independent from the choice of ∇.\n\nThe image of \"T\" in coker(τ) for any adapted connection ∇ is called the torsion of the \"G\"-structure. A \"G\"-structure is said to be torsion-free if its torsion vanishes. This happens precisely when \"Q\" admits a torsion-free adapted connection.\n\nAn example of a \"G\"-structure is an almost complex structure, that is, a reduction\nof a structure group of an even-dimensional manifold to GL(\"n\",C). Such a reduction is uniquely determined by a \"C\"-linear endomorphism \"J\" ∈ End(\"TM\") such that \"J\" = −1. In this situation, the torsion can be computed explicitly as follows.\n\nAn easy dimension count shows that\n\nwhere Ω(\"TM\") is a space of forms \"B\" ∈ Ω(\"TM\") which satisfy\n\nTherefore, the torsion of an almost complex structure can be considered as an element in \nΩ(\"TM\"). It is easy to check that the torsion of an almost complex structure is equal to its Nijenhuis tensor.\n\nImposing integrability conditions on a particular \"G\"-structure (for instance, with the case of a symplectic form) can be dealt with via the process of prolongation. In such cases, the prolonged \"G\"-structure cannot be identified with a \"G\"-subbundle of the bundle of linear frames. In many cases, however, the prolongation is a principal bundle in its own right, and its structure group can be identified with a subgroup of a higher-order jet group. In which case, it is called a higher order \"G\"-structure [Kobayashi]. In general, Cartan's equivalence method applies to such cases.\n\n\n"}
{"id": "45667899", "url": "https://en.wikipedia.org/wiki?curid=45667899", "title": "Genevieve Grotjan Feinstein", "text": "Genevieve Grotjan Feinstein\n\nGenevieve Marie Grotjan Feinstein (April 30, 1913 – August 10, 2006) was an American mathematician and cryptanalyst. She worked for the Signals Intelligence Service throughout World War II, during which time she played an important role in deciphering the Japanese cryptography machine Purple, and later worked on the Cold War-era Venona project.\n\nFeinstein discovered a passion for mathematics at a young age and aspired to become a math teacher until the beginning of World War II, when U.S. President Franklin D. Roosevelt made it possible for women to fulfill non-combat roles in the military. She passed the necessary tests to become a government mathematician in 1939, and was hired by William F. Friedman to work as a cryptanalyst for the Army's Signals Intelligence Service (SIS). For eighteen months, she worked with other SIS codebreakers to decipher the code used by Purple, a Japanese cryptography machine, and ultimately played a key role in cracking the cipher in September 1940. This enabled the construction of an analog machine by the SIS which in turn enabled the interception of almost all messages exchanged between the Japanese government and its embassies in foreign countries.\n\nAfter the conclusion of World War II, Feinstein continued to work at the SIS throughout the Cold War, trying to decode encrypted messages sent by the Soviet KGB and Main Intelligence Directorate (GRU). She made a significant breakthrough in the early stages of the Venona project, which allowed American cryptographers to recognize when an individual cipher key was reused, but resigned from the SIS in 1947. After resigning from government cryptanalysis, she began working in the faculty of George Mason University, where she served as a professor of mathematics.\n\nGenevieve Grotjan married the chemist Hyman Feinstein in 1943, and they had a son named Ellis. She died in 2006.\n\nFeinstein's breakthrough in deciphering the Purple machine has been called, in the \"Encyclopedia of American Women at War\", \"one of the greatest achievements in the history of U.S. codebreaking\". She was posthumously inducted into the NSA Hall of Honor in 2010, and an award in cryptology was established at George Mason University in her honor.\n"}
{"id": "19447", "url": "https://en.wikipedia.org/wiki?curid=19447", "title": "Group (mathematics)", "text": "Group (mathematics)\n\nIn mathematics, a group is an algebraic structure consisting of a set of elements equipped with a binary operation which combines any two elements to form a third element. To be a group, this operation must satisfy four conditions called the group axioms, namely closure, associativity, identity and invertibility. One of the most familiar examples of a group is the set of integers together with the addition operation, but the abstract formalization of the group axioms, detached as it is from the concrete nature of any particular group and its operation, applies much more widely. It allows entities with highly diverse mathematical origins in abstract algebra and beyond to be handled in a flexible way while retaining their essential structural aspects. The ubiquity of groups in numerous areas within and outside mathematics makes them a central organizing principle of contemporary mathematics.\n\nGroups share a fundamental kinship with the notion of symmetry. For example, a symmetry group encodes symmetry features of a geometrical object: the group consists of the set of transformations that leave the object unchanged and the operation of combining two such transformations by performing one after the other. Lie groups are the symmetry groups used in the Standard Model of particle physics; Poincaré groups, which are also Lie groups, can express the physical symmetry underlying special relativity; and point groups are used to help understand symmetry phenomena in molecular chemistry.\n\nThe concept of a group arose from the study of polynomial equations, starting with Évariste Galois in the 1830s. After contributions from other fields such as number theory and geometry, the group notion was generalized and firmly established around 1870. Modern group theory—an active mathematical discipline—studies groups in their own right. To explore groups, mathematicians have devised various notions to break groups into smaller, better-understandable pieces, such as subgroups, quotient groups and simple groups. In addition to their abstract properties, group theorists also study the different ways in which a group can be expressed concretely, both from a point of view of representation theory (that is, through the representations of the group) and of computational group theory. A theory has been developed for finite groups, which culminated with the classification of finite simple groups, completed in 2004. Since the mid-1980s, geometric group theory, which studies finitely generated groups as geometric objects, has become a particularly active area in group theory.\n\nOne of the most familiar groups is the set of integers formula_1 which consists of the numbers\nThe following properties of integer addition serve as a model for the group axioms given in the definition below.\n\n\nThe integers, together with the operation +, form a mathematical object belonging to a broad class sharing similar structural aspects. To appropriately understand these structures as a collective, the following definition is developed.\n\nA group is a set, \"G\", together with an operation • (called the \"group law\" of \"G\") that combines any two elements \"a\" and \"b\" to form another element, denoted or \"ab\". To qualify as a group, the set and operation, , must satisfy four requirements known as the \"group axioms\":\n\n\nThe result of an operation may depend on the order of the operands. In other words, the result of combining element \"a\" with element \"b\" need not yield the same result as combining element \"b\" with element \"a\"; the equation\nmay not always be true. This equation always holds in the group of integers under addition, because for any two integers (commutativity of addition). Groups for which the commutativity equation always holds are called \"abelian groups\" (in honor of Niels Henrik Abel). The symmetry group described in the following section is an example of a group that is not abelian.\n\nThe identity element of a group \"G\" is often written as 1 or 1, a notation inherited from the multiplicative identity. If a group is abelian, then one may choose to denote the group operation by + and the identity element by 0; in that case, the group is called an additive group. The identity element can also be written as \"id\".\n\nThe set \"G\" is called the \"underlying set\" of the group . Often the group's underlying set \"G\" is used as a short name for the group . Along the same lines, shorthand expressions such as \"a subset of the group \"G\"\" or \"an element of group \"G\"\" are used when what is actually meant is \"a subset of the underlying set \"G\" of the group \" or \"an element of the underlying set \"G\" of the group \". Usually, it is clear from the context whether a symbol like \"G\" refers to a group or to an underlying set.\n\nAn alternate (but equivalent) definition is to expand the structure of a group to define a group as a set equipped with three operations satisfying the same axioms as above, with the \"there exists\" part removed in the two last axioms, these operations being\nthe group law, as above, which is a binary operation,\nthe \"inverse operation\", which is a unary operation and maps to formula_2\nand the identity element, which is viewed as a 0-ary operation.\n\nAs this formulation of the definition avoids existential quantifiers, it is generally preferred for computing with groups and for computer-aided proofs. This formulation exhibits groups as a variety of universal algebra. It is also useful for talking of properties of the inverse operation, as needed for defining topological groups and group objects.\n\nTwo figures in the plane are congruent if one can be changed into the other using a combination of rotations, reflections, and translations. Any figure is congruent to itself. However, some figures are congruent to themselves in more than one way, and these extra congruences are called symmetries. A square has eight symmetries. These are:\n\nThese symmetries are represented by functions. Each of these functions sends a point in the square to the corresponding point under the symmetry. For example, r sends a point to its rotation 90° clockwise around the square's center, and f sends a point to its reflection across the square's vertical middle line. Composing two of these symmetry functions gives another symmetry function. These symmetries determine a group called the dihedral group of degree 4 and denoted D. The underlying set of the group is the above set of symmetry functions, and the group operation is function composition. Two symmetries are combined by composing them as functions, that is, applying the first one to the square, and the second one to the result of the first application. The result of performing first \"a\" and then \"b\" is written symbolically \"from right to left\" as\nThe right-to-left notation is the same notation that is used for composition of functions.\n\nThe group table on the right lists the results of all such compositions possible. For example, rotating by 270° clockwise (r) and then reflecting horizontally (f) is the same as performing a reflection along the diagonal (f). Using the above symbols, highlighted in blue in the group table:\n\nGiven this set of symmetries and the described operation, the group axioms can be understood as follows:\nIn contrast to the group of integers above, where the order of the operation is irrelevant, it does matter in D: but In other words, D is not abelian, which makes the group structure more difficult than the integers introduced first.\n\nThe modern concept of an abstract group developed out of several fields of mathematics. The original motivation for group theory was the quest for solutions of polynomial equations of degree higher than 4. The 19th-century French mathematician Évariste Galois, extending prior work of Paolo Ruffini and Joseph-Louis Lagrange, gave a criterion for the solvability of a particular polynomial equation in terms of the symmetry group of its roots (solutions). The elements of such a Galois group correspond to certain permutations of the roots. At first, Galois' ideas were rejected by his contemporaries, and published only posthumously. More general permutation groups were investigated in particular by Augustin Louis Cauchy. Arthur Cayley's \"On the theory of groups, as depending on the symbolic equation θ = 1\" (1854) gives the first abstract definition of a finite group.\n\nGeometry was a second field in which groups were used systematically, especially symmetry groups as part of Felix Klein's 1872 Erlangen program. After novel geometries such as hyperbolic and projective geometry had emerged, Klein used group theory to organize them in a more coherent way. Further advancing these ideas, Sophus Lie founded the study of Lie groups in 1884.\n\nThe third field contributing to group theory was number theory. Certain abelian group structures had been used implicitly in Carl Friedrich Gauss' number-theoretical work \"Disquisitiones Arithmeticae\" (1798), and more explicitly by Leopold Kronecker. In 1847, Ernst Kummer made early attempts to prove Fermat's Last Theorem by developing groups describing factorization into prime numbers.\n\nThe convergence of these various sources into a uniform theory of groups started with Camille Jordan's \"Traité des substitutions et des équations algébriques\" (1870). Walther von Dyck (1882) introduced the idea of specifying a group by means of generators and relations, and was also the first to give an axiomatic definition of an \"abstract group\", in the terminology of the time. As of the 20th century, groups gained wide recognition by the pioneering work of Ferdinand Georg Frobenius and William Burnside, who worked on representation theory of finite groups, Richard Brauer's modular representation theory and Issai Schur's papers. The theory of Lie groups, and more generally locally compact groups was studied by Hermann Weyl, Élie Cartan and many others. Its algebraic counterpart, the theory of algebraic groups, was first shaped by Claude Chevalley (from the late 1930s) and later by the work of Armand Borel and Jacques Tits.\n\nThe University of Chicago's 1960–61 Group Theory Year brought together group theorists such as Daniel Gorenstein, John G. Thompson and Walter Feit, laying the foundation of a collaboration that, with input from numerous other mathematicians, led to the classification of finite simple groups, with the final step taken by Aschbacher and Smith in 2004. This project exceeded previous mathematical endeavours by its sheer size, in both length of proof and number of researchers. Research is ongoing to simplify the proof of this classification. These days, group theory is still a highly active mathematical branch, impacting many other fields.\n\nBasic facts about all groups that can be obtained directly from the group axioms are commonly subsumed under \"elementary group theory\". For example, repeated applications of the associativity axiom show that the unambiguity of\ngeneralizes to more than three factors. Because this implies that parentheses can be inserted anywhere within such a series of terms, parentheses are usually omitted.\n\nThe axioms may be weakened to assert only the existence of a left identity and left inverses. Both can be shown to be actually two-sided, so the resulting definition is equivalent to the one given above.\n\nTwo important consequences of the group axioms are the uniqueness of the identity element and the uniqueness of inverse elements. There can be only one identity element in a group, and each element in a group has exactly one inverse element. Thus, it is customary to speak of \"the\" identity, and \"the\" inverse of an element.\n\nTo prove the uniqueness of an inverse element of \"a\", suppose that \"a\" has two inverses, denoted \"b\" and \"c\", in a group (\"G\", •). Then\n\nThe term \"b\" on the first line above and the \"c\" on the last are equal, since they are connected by a chain of equalities. In other words, there is only one inverse element of \"a\". Similarly, to prove that the identity element of a group is unique, assume \"G\" is a group with two identity elements \"e\" and \"f\". Then \"e\" = \"e\" • \"f\" = \"f\", hence \"e\" and \"f\" are equal.\n\nIn groups, the existence of inverse elements implies that division is possible: given elements \"a\" and \"b\" of the group \"G\", there is exactly one solution \"x\" in \"G\" to the equation , namely . In fact, we have \nUniqueness results by multiplying the two sides of the equation by . The element , often denoted , is called the \"right quotient\" of \"b\" by \"a\", or the result of the \"right division\" of \"b\" by \"a\".\n\nSimilarly there is exactly one solution \"y\" in \"G\" to the equation , namely . This solution is the \"left quotient\" of \"b\" by \"a\", and is sometimes denoted .\n\nIn general and may be different, but, if the group operation is commutative (that is, if the group is abelian), they are equal. In this case, the group operation is often denoted as an addition, and one talks of \"subtraction\" and \"difference\" instead of division and quotient.\n\nA consequence of this is that multiplication by a group element \"g\" is a bijection. Specifically, if \"g\" is an element of the group \"G\", the function (mathematics) from \"G\" to itself that maps to is a bijection. This function is called the \"left translation\" by \"g\" . Similarly, the \"right translation\" by \"g\" is the bijection from \"G\" to itself, that maps \"h\" to . If \"G\" is abelian, the left and the right translation by a group element are the same.\n\nTo understand groups beyond the level of mere symbolic manipulations as above, more structural concepts have to be employed. There is a conceptual principle underlying all of the following notions: to take advantage of the structure offered by groups (which sets, being \"structureless\", do not have), constructions related to groups have to be \"compatible\" with the group operation. This compatibility manifests itself in the following notions in various ways. For example, groups can be related to each other via functions called group homomorphisms. By the mentioned principle, they are required to respect the group structures in a precise sense. The structure of groups can also be understood by breaking them into pieces called subgroups and quotient groups. The principle of \"preserving structures\"—a recurring topic in mathematics throughout—is an instance of working in a category, in this case the category of groups.\n\n\"Group homomorphisms\" are functions that preserve group structure. A function between two groups and is called a \"homomorphism\" if the equation\nholds for all elements \"g\", \"k\" in \"G\". In other words, the result is the same when performing the group operation after or before applying the map \"a\". This requirement ensures that , and also for all \"g\" in \"G\". Thus a group homomorphism respects all the structure of \"G\" provided by the group axioms.\n\nTwo groups \"G\" and \"H\" are called \"isomorphic\" if there exist group homomorphisms and , such that applying the two functions one after another in each of the two possible orders gives the identity functions of \"G\" and \"H\". That is, and for any \"g\" in \"G\" and \"h\" in \"H\". From an abstract point of view, isomorphic groups carry the same information. For example, proving that for some element \"g\" of \"G\" is equivalent to proving that , because applying \"a\" to the first equality yields the second, and applying \"b\" to the second gives back the first.\n\nInformally, a \"subgroup\" is a group \"H\" contained within a bigger one, \"G\". Concretely, the identity element of \"G\" is contained in \"H\", and whenever \"h\" and \"h\" are in \"H\", then so are and \"h\", so the elements of \"H\", equipped with the group operation on \"G\" restricted to \"H\", indeed form a group.\n\nIn the example above, the identity and the rotations constitute a subgroup highlighted in red in the group table above: any two rotations composed are still a rotation, and a rotation can be undone by (i.e., is inverse to) the complementary rotations 270° for 90°, 180° for 180°, and 90° for 270° (note that rotation in the opposite direction is not defined). The subgroup test is a necessary and sufficient condition for a nonempty subset \"H\" of a group \"G\" to be a subgroup: it is sufficient to check that for all elements . Knowing the subgroups is important in understanding the group as a whole.\n\nGiven any subset \"S\" of a group \"G\", the subgroup generated by \"S\" consists of products of elements of \"S\" and their inverses. It is the smallest subgroup of \"G\" containing \"S\". In the introductory example above, the subgroup generated by r and f consists of these two elements, the identity element id and . Again, this is a subgroup, because combining any two of these four elements or their inverses (which are, in this particular case, these same elements) yields an element of this subgroup.\n\nIn many situations it is desirable to consider two group elements the same if they differ by an element of a given subgroup. For example, in D above, once a reflection is performed, the square never gets back to the r configuration by just applying the rotation operations (and no further reflections), i.e., the rotation operations are irrelevant to the question whether a reflection has been performed. Cosets are used to formalize this insight: a subgroup \"H\" defines left and right cosets, which can be thought of as translations of \"H\" by arbitrary group elements \"g\". In symbolic terms, the \"left\" and \"right\" cosets of \"H\" containing \"g\" are\n\nThe left cosets of any subgroup \"H\" form a partition of \"G\"; that is, the union of all left cosets is equal to \"G\" and two left cosets are either equal or have an empty intersection. The first case happens precisely when , i.e., if the two elements differ by an element of \"H\". Similar considerations apply to the right cosets of \"H\". The left and right cosets of \"H\" may or may not be equal. If they are, i.e., for all \"g\" in \"G\", , then \"H\" is said to be a \"normal subgroup\".\n\nIn D, the introductory symmetry group, the left cosets \"gR\" of the subgroup \"R\" consisting of the rotations are either equal to \"R\", if \"g\" is an element of \"R\" itself, or otherwise equal to (highlighted in green). The subgroup \"R\" is also normal, because and similarly for any element other than f. (In fact, in the case of D, observe that all such cosets are equal, such that .)\n\nIn some situations the set of cosets of a subgroup can be endowed with a group law, giving a \"quotient group\" or \"factor group\". For this to be possible, the subgroup has to be normal. Given any normal subgroup \"N\", the quotient group is defined by\n\nThis set inherits a group operation (sometimes called coset multiplication, or coset addition) from the original group \"G\": for all \"g\" and \"h\" in \"G\". This definition is motivated by the idea (itself an instance of general structural considerations outlined above) that the map that associates to any element \"g\" its coset \"gN\" be a group homomorphism, or by general abstract considerations called universal properties. The coset serves as the identity in this group, and the inverse of \"gN\" in the quotient group is .\nThe elements of the quotient group are \"R\" itself, which represents the identity, and . The group operation on the quotient is shown at the right. For example, . Both the subgroup as well as the corresponding quotient are abelian, whereas D is not abelian. Building bigger groups by smaller ones, such as D from its subgroup \"R\" and the quotient is abstracted by a notion called semidirect product.\n\nQuotient groups and subgroups together form a way of describing every group by its \"presentation\": any group is the quotient of the free group over the \"generators\" of the group, quotiented by the subgroup of \"relations\". The dihedral group D, for example, can be generated by two elements \"r\" and \"f\" (for example, \"r\" = r, the right rotation and \"f\" = f the vertical (or any other) reflection), which means that every symmetry of the square is a finite composition of these two symmetries or their inverses. Together with the relations\nthe group is completely described. A presentation of a group can also be used to construct the Cayley graph, a device used to graphically capture discrete groups.\n\nSub- and quotient groups are related in the following way: a subset \"H\" of \"G\" can be seen as an injective map , i.e., any element of the target has at most one element that maps to it. The counterpart to injective maps are surjective maps (every element of the target is mapped onto), such as the canonical map . Interpreting subgroup and quotients in light of these homomorphisms emphasizes the structural concept inherent to these definitions alluded to in the introduction. In general, homomorphisms are neither injective nor surjective. Kernel and image of group homomorphisms and the first isomorphism theorem address this phenomenon.\n\nExamples and applications of groups abound. A starting point is the group Z of integers with addition as group operation, introduced above. If instead of addition multiplication is considered, one obtains multiplicative groups. These groups are predecessors of important constructions in abstract algebra.\n\nGroups are also applied in many other mathematical areas. Mathematical objects are often examined by associating groups to them and studying the properties of the corresponding groups. For example, Henri Poincaré founded what is now called algebraic topology by introducing the fundamental group. By means of this connection, topological properties such as proximity and continuity translate into properties of groups. For example, elements of the fundamental group are represented by loops. The second image at the right shows some loops in a plane minus a point. The blue loop is considered null-homotopic (and thus irrelevant), because it can be continuously shrunk to a point. The presence of the hole prevents the orange loop from being shrunk to a point. The fundamental group of the plane with a point deleted turns out to be infinite cyclic, generated by the orange loop (or any other loop winding once around the hole). This way, the fundamental group detects the hole.\n\nIn more recent applications, the influence has also been reversed to motivate geometric constructions by a group-theoretical background. In a similar vein, geometric group theory employs geometric concepts, for example in the study of hyperbolic groups. Further branches crucially applying groups include algebraic geometry and number theory.\n\nIn addition to the above theoretical applications, many practical applications of groups exist. Cryptography relies on the combination of the abstract group theory approach together with algorithmical knowledge obtained in computational group theory, in particular when implemented for finite groups. Applications of group theory are not restricted to mathematics; sciences such as physics, chemistry and computer science benefit from the concept.\n\nMany number systems, such as the integers and the rationals enjoy a naturally given group structure. In some cases, such as with the rationals, both addition and multiplication operations give rise to group structures. Such number systems are predecessors to more general algebraic structures known as rings and fields. Further abstract algebraic concepts such as modules, vector spaces and algebras also form groups.\n\nThe group of integers formula_1 under addition, denoted formula_4, has been described above. The integers, with the operation of multiplication instead of addition, formula_5 do \"not\" form a group. The closure, associativity and identity axioms are satisfied, but inverses do not exist: for example, is an integer, but the only solution to the equation in this case is , which is a rational number, but not an integer. Hence not every element of formula_1 has a (multiplicative) inverse.\n\nThe desire for the existence of multiplicative inverses suggests considering fractions\nFractions of integers (with \"b\" nonzero) are known as rational numbers. The set of all such irreducible fractions is commonly denoted formula_8. There is still a minor obstacle for formula_9, the rationals with multiplication, being a group: because the rational number 0 does not have a multiplicative inverse (i.e., there is no \"x\" such that ), formula_9 is still not a group.\n\nHowever, the set of all \"nonzero\" rational numbers formula_11 does form an abelian group under multiplication, generally denoted formula_12. Associativity and identity element axioms follow from the properties of integers. The closure requirement still holds true after removing zero, because the product of two nonzero rationals is never zero. Finally, the inverse of \"a\"/\"b\" is \"b\"/\"a\", therefore the axiom of the inverse element is satisfied.\n\nThe rational numbers (including 0) also form a group under addition. Intertwining addition and multiplication operations yields more complicated structures called rings and—if division is possible, such as in formula_8—fields, which occupy a central position in abstract algebra. Group theoretic arguments therefore underlie parts of the theory of those entities.\n\nIn modular arithmetic, two integers are added and then the sum is divided by a positive integer called the \"modulus.\" The result of modular addition is the remainder of that division. For any modulus, \"n\", the set of integers from 0 to forms a group under modular addition: the inverse of any element \"a\" is , and 0 is the identity element. This is familiar from the addition of hours on the face of a clock: if the hour hand is on 9 and is advanced 4 hours, it ends up on 1, as shown at the right. This is expressed by saying that 9 + 4 equals 1 \"modulo 12\" or, in symbols,\nThe group of integers modulo \"n\" is written formula_14 or formula_15.\n\nFor any prime number \"p\", there is also the multiplicative group of integers modulo \"p\". Its elements are the integers 1 to . The group operation is multiplication modulo \"p\". That is, the usual product is divided by \"p\" and the remainder of this division is the result of modular multiplication. For example, if , there are four group elements 1, 2, 3, 4. In this group, , because the usual product 16 is equivalent to 1, which divided by 5 yields a remainder of 1. for 5 divides , denoted\nThe primality of \"p\" ensures that the product of two integers neither of which is divisible by \"p\" is not divisible by \"p\" either, hence the indicated set of classes is closed under multiplication. The identity element is 1, as usual for a multiplicative group, and the associativity follows from the corresponding property of integers. Finally, the inverse element axiom requires that given an integer \"a\" not divisible by \"p\", there exists an integer \"b\" such that\nThe inverse \"b\" can be found by using Bézout's identity and the fact that the greatest common divisor equals 1. In the case above, the inverse of 4 is 4, and the inverse of 3 is 2, as . Hence all group axioms are fulfilled. Actually, this example is similar to formula_16 above: it consists of exactly those elements in formula_17 that have a multiplicative inverse. These groups are denoted F. They are crucial to public-key cryptography.\n\nA \"cyclic group\" is a group all of whose elements are powers of a particular element \"a\". In multiplicative notation, the elements of the group are:\nwhere \"a\" means \"a\" • \"a\", and \"a\" stands for \"a\" • \"a\" • \"a\" = (\"a\" • \"a\" • \"a\") etc. Such an element \"a\" is called a generator or a primitive element of the group. In additive notation, the requirement for an element to be primitive is that each element of the group can be written as\n\nIn the groups Z/\"n\"Z introduced above, the element 1 is primitive, so these groups are cyclic. Indeed, each element is expressible as a sum all of whose terms are 1. Any cyclic group with \"n\" elements is isomorphic to this group. A second example for cyclic groups is the group of \"n\"-th complex roots of unity, given by complex numbers \"z\" satisfying . These numbers can be visualized as the vertices on a regular \"n\"-gon, as shown in blue at the right for . The group operation is multiplication of complex numbers. In the picture, multiplying with \"z\" corresponds to a counter-clockwise rotation by 60°. Using some field theory, the group F can be shown to be cyclic: for example, if , 3 is a generator since , , , and .\n\nSome cyclic groups have an infinite number of elements. In these groups, for every non-zero element \"a\", all the powers of \"a\" are distinct; despite the name \"cyclic group\", the powers of the elements do not cycle. An infinite cyclic group is isomorphic to , the group of integers under addition introduced above. As these two prototypes are both abelian, so is any cyclic group.\n\nThe study of finitely generated abelian groups is quite mature, including the fundamental theorem of finitely generated abelian groups; and reflecting this state of affairs, many group-related notions, such as center and commutator, describe the extent to which a given group is not abelian.\n\n\"Symmetry groups\" are groups consisting of symmetries of given mathematical objects—be they of geometric nature, such as the introductory symmetry group of the square, or of algebraic nature, such as polynomial equations and their solutions. Conceptually, group theory can be thought of as the study of symmetry. Symmetries in mathematics greatly simplify the study of geometrical or analytical objects. A group is said to act on another mathematical object \"X\" if every group element performs some operation on \"X\" compatibly to the group law. In the rightmost example below, an element of order 7 of the (2,3,7) triangle group acts on the tiling by permuting the highlighted warped triangles (and the other ones, too). By a group action, the group pattern is connected to the structure of the object being acted on.\nIn chemical fields, such as crystallography, space groups and point groups describe molecular symmetries and crystal symmetries. These symmetries underlie the chemical and physical behavior of these systems, and group theory enables simplification of quantum mechanical analysis of these properties. For example, group theory is used to show that optical transitions between certain quantum levels cannot occur simply because of the symmetry of the states involved.\n\nNot only are groups useful to assess the implications of symmetries in molecules, but surprisingly they also predict that molecules sometimes can change symmetry. The Jahn-Teller effect is a distortion of a molecule of high symmetry when it adopts a particular ground state of lower symmetry from a set of possible ground states that are related to each other by the symmetry operations of the molecule.\n\nLikewise, group theory helps predict the changes in physical properties that occur when a material undergoes a phase transition, for example, from a cubic to a tetrahedral crystalline form. An example is ferroelectric materials, where the change from a paraelectric to a ferroelectric state occurs at the Curie temperature and is related to a change from the high-symmetry paraelectric state to the lower symmetry ferroelectric state, accompanied by a so-called soft phonon mode, a vibrational lattice mode that goes to zero frequency at the transition.\n\nSuch spontaneous symmetry breaking has found further application in elementary particle physics, where its occurrence is related to the appearance of Goldstone bosons.\n\nFinite symmetry groups such as the Mathieu groups are used in coding theory, which is in turn applied in error correction of transmitted data, and in CD players. Another application is differential Galois theory, which characterizes functions having antiderivatives of a prescribed form, giving group-theoretic criteria for when solutions of certain differential equations are well-behaved. Geometric properties that remain stable under group actions are investigated in (geometric) invariant theory.\n\nMatrix groups consist of matrices together with matrix multiplication. The \"general linear group\" consists of all invertible \"n\"-by-\"n\" matrices with real entries. Its subgroups are referred to as \"matrix groups\" or \"linear groups\". The dihedral group example mentioned above can be viewed as a (very small) matrix group. Another important matrix group is the special orthogonal group SO(\"n\"). It describes all possible rotations in \"n\" dimensions. Via Euler angles, rotation matrices are used in computer graphics.\n\n\"Representation theory\" is both an application of the group concept and important for a deeper understanding of groups. It studies the group by its group actions on other spaces. A broad class of group representations are linear representations, i.e., the group is acting on a vector space, such as the three-dimensional Euclidean space R. A representation of \"G\" on an \"n\"-dimensional real vector space is simply a group homomorphism\nfrom the group to the general linear group. This way, the group operation, which may be abstractly given, translates to the multiplication of matrices making it accessible to explicit computations.\n\nGiven a group action, this gives further means to study the object being acted on. On the other hand, it also yields information about the group. Group representations are an organizing principle in the theory of finite groups, Lie groups, algebraic groups and topological groups, especially (locally) compact groups.\n\n\"Galois groups\" were developed to help solve polynomial equations by capturing their symmetry features. For example, the solutions of the quadratic equation are given by\nExchanging \"+\" and \"−\" in the expression, i.e., permuting the two solutions of the equation can be viewed as a (very simple) group operation. Similar formulae are known for cubic and quartic equations, but do \"not\" exist in general for degree 5 and higher. Abstract properties of Galois groups associated with polynomials (in particular their solvability) give a criterion for polynomials that have all their solutions expressible by radicals, i.e., solutions expressible using solely addition, multiplication, and roots similar to the formula above.\n\nThe problem can be dealt with by shifting to field theory and considering the splitting field of a polynomial. Modern Galois theory generalizes the above type of Galois groups to field extensions and establishes—via the fundamental theorem of Galois theory—a precise relationship between fields and groups, underlining once again the ubiquity of groups in mathematics.\n\nA group is called \"finite\" if it has a finite number of elements. The number of elements is called the order of the group. An important class is the \"symmetric groups\" S, the groups of permutations of \"N\" letters. For example, the symmetric group on 3 letters S is the group consisting of all possible orderings of the three letters \"ABC\", i.e., contains the elements \"ABC\", \"ACB\", \"BAC\", \"BCA\", \"CAB\", \"CBA\", in total 6 (factorial of 3) elements. This class is fundamental insofar as any finite group can be expressed as a subgroup of a symmetric group S for a suitable integer \"N\", according to Cayley's theorem. Parallel to the group of symmetries of the square above, S can also be interpreted as the group of symmetries of an equilateral triangle.\n\nThe order of an element \"a\" in a group \"G\" is the least positive integer \"n\" such that \"a\" = \"e\", where \"a\" represents\n\ni.e., application of the operation • to \"n\" copies of \"a\". (If • represents multiplication, then \"a\" corresponds to the \"n\"th power of \"a\".) In infinite groups, such an \"n\" may not exist, in which case the order of \"a\" is said to be infinity. The order of an element equals the order of the cyclic subgroup generated by this element.\n\nMore sophisticated counting techniques, for example counting cosets, yield more precise statements about finite groups: Lagrange's Theorem states that for a finite group \"G\" the order of any finite subgroup \"H\" divides the order of \"G\". The Sylow theorems give a partial converse.\n\nThe dihedral group (discussed above) is a finite group of order 8. The order of r is 4, as is the order of the subgroup \"R\" it generates (see above). The order of the reflection elements f etc. is 2. Both orders divide 8, as predicted by Lagrange's theorem. The groups F above have order .\n\nMathematicians often strive for a complete classification (or list) of a mathematical notion. In the context of finite groups, this aim leads to difficult mathematics. According to Lagrange's theorem, finite groups of order \"p\", a prime number, are necessarily cyclic (abelian) groups Z. Groups of order \"p\" can also be shown to be abelian, a statement which does not generalize to order \"p\", as the non-abelian group D of order 8 = 2 above shows. Computer algebra systems can be used to list small groups, but there is no classification of all finite groups. An intermediate step is the classification of finite simple groups. A nontrivial group is called \"simple\" if its only normal subgroups are the trivial group and the group itself. The Jordan–Hölder theorem exhibits finite simple groups as the building blocks for all finite groups. Listing all finite simple groups was a major achievement in contemporary group theory. 1998 Fields Medal winner Richard Borcherds succeeded in proving the monstrous moonshine conjectures, a surprising and deep relation between the largest finite simple sporadic group—the \"monster group\"—and certain modular functions, a piece of classical complex analysis, and string theory, a theory supposed to unify the description of many physical phenomena.\n\nMany groups are simultaneously groups and examples of other mathematical structures. In the language of category theory, they are group objects in a category, meaning that they are objects (that is, examples of another mathematical structure) which come with transformations (called morphisms) that mimic the group axioms. For example, every group (as defined above) is also a set, so a group is a group object in the category of sets.\n\nSome topological spaces may be endowed with a group law. In order for the group law and the topology to interweave well, the group operations must be continuous functions, that is, , and \"g\" must not vary wildly if \"g\" and \"h\" vary only little. Such groups are called \"topological groups,\" and they are the group objects in the category of topological spaces. The most basic examples are the reals R under addition, , and similarly with any other topological field such as the complex numbers or \"p\"-adic numbers. All of these groups are locally compact, so they have Haar measures and can be studied via harmonic analysis. The former offer an abstract formalism of invariant integrals. Invariance means, in the case of real numbers for example:\nfor any constant \"c\". Matrix groups over these fields fall under this regime, as do adele rings and adelic algebraic groups, which are basic to number theory. Galois groups of infinite field extensions such as the absolute Galois group can also be equipped with a topology, the so-called Krull topology, which in turn is central to generalize the above sketched connection of fields and groups to infinite field extensions. An advanced generalization of this idea, adapted to the needs of algebraic geometry, is the étale fundamental group.\n\n\"Lie groups\" (in honor of Sophus Lie) are groups which also have a manifold structure, i.e., they are spaces looking locally like some Euclidean space of the appropriate dimension. Again, the additional structure, here the manifold structure, has to be compatible, i.e., the maps corresponding to multiplication and the inverse have to be smooth.\n\nA standard example is the general linear group introduced above: it is an open subset of the space of all \"n\"-by-\"n\" matrices, because it is given by the inequality\nwhere \"A\" denotes an \"n\"-by-\"n\" matrix.\n\nLie groups are of fundamental importance in modern physics: Noether's theorem links continuous symmetries to conserved quantities. Rotation, as well as translations in space and time are basic symmetries of the laws of mechanics. They can, for instance, be used to construct simple models—imposing, say, axial symmetry on a situation will typically lead to significant simplification in the equations one needs to solve to provide a physical description. Another example are the Lorentz transformations, which relate measurements of time and velocity of two observers in motion relative to each other. They can be deduced in a purely group-theoretical way, by expressing the transformations as a rotational symmetry of Minkowski space. The latter serves—in the absence of significant gravitation—as a model of space time in special relativity. The full symmetry group of Minkowski space, i.e., including translations, is known as the Poincaré group. By the above, it plays a pivotal role in special relativity and, by implication, for quantum field theories. Symmetries that vary with location are central to the modern description of physical interactions with the help of gauge theory.\n\nIn abstract algebra, more general structures are defined by relaxing some of the axioms defining a group. For example, if the requirement that every element has an inverse is eliminated, the resulting algebraic structure is called a monoid. The natural numbers N (including 0) under addition form a monoid, as do the nonzero integers under multiplication , see above. There is a general method to formally add inverses to elements to any (abelian) monoid, much the same way as is derived from , known as the Grothendieck group.\nGroupoids are similar to groups except that the composition need not be defined for all \"a\" and \"b\". They arise in the study of more complicated forms of symmetry, often in topological and analytical structures, such as the fundamental groupoid or stacks. Finally, it is possible to generalize any of these concepts by replacing the binary operation with an arbitrary \"n\"-ary one (i.e., an operation taking \"n\" arguments). With the proper generalization of the group axioms this gives rise to an \"n\"-ary group. The table gives a list of several structures generalizing groups.\n\n\n\n"}
{"id": "11014228", "url": "https://en.wikipedia.org/wiki?curid=11014228", "title": "Hausdorff density", "text": "Hausdorff density\n\nIn measure theory, a field of mathematics, the Hausdorff density measures how concentrated a Radon measure is at some point.\n\nLet formula_1 be a Radon measure and formula_2 some point in Euclidean space. The \"s\"-dimensional upper and lower Hausdorff densities are defined to be, respectively, \nand\nwhere formula_5 is the ball of radius \"r\" > 0 centered at \"a\". Clearly, formula_6 for all formula_2. In the event that the two are equal, we call their common value the s-density of formula_1 at \"a\" and denote it formula_9.\n\nThe following theorem states that the times when the \"s\"-density exists are rather seldom.\n\nIn 1987 David Preiss proved a stronger version of Marstrand's theorem. One consequence is that sets with positive and finite density are rectifiable sets.\n\n\n"}
{"id": "3394448", "url": "https://en.wikipedia.org/wiki?curid=3394448", "title": "Hotelling's lemma", "text": "Hotelling's lemma\n\nHotelling's lemma is a result in microeconomics that relates the supply of a good to the profit of the good's producer. It was first shown by Harold Hotelling, and is widely used in the theory of the firm.\n\nThe lemma can be stated as: \"The change in profits from a change in price is equal to the quantity produced.\"\n\n\"For formula_2 the profit function of the firm n terms of the good's price p and formula_3the production function in terms of the good's price p, assuming that formula_4 and that derivative exists.\"\n\nThe proof of the theorem stems from the fact that for a profit-maximizing firm an under duality, the maximum of the firm's profit at some output formula_5 is given by the minimum of formula_6 (cost) at some price, formula_7, namely where formula_8 holds. Thus, formula_9; QED.\n\nThe proof is also a corollary of the envelope theorem.\n\nThis is not an application of Hotelling's lemma, it's the definition of partial derivatives.\n\nLet the firm's profit function be:\n\nwhere:\nIf a firm produces 10 units of formula_3 using 5 units of input formula_12 which cost 1 dollar each and sells each output for 2 dollars. the profit the firm makes is:\n\nIf the firm increases the price of the output to 3 dollars and still sells the same amount of formula_3, the firm's profits are now:\n\nTaking the difference between formula_16 and formula_17\n\nThe change in profits from a change in price is 10, which is exactly the same as the output produced. thus the statement of formula_9 holds.\n\nA number of criticisms have been made with regards to the use and application of Hotelling's lemma in empirical work.\n\nC.Robert Taylor points out that the accuracy of Hotelling's lemma is dependent on the firm maximizing profits, meaning that it is producing profit maximizing output formula_20 and cost minimizing input formula_21. If a firm is not producing at these optimums, then Hotelling's lemma would not hold.\n\n\n"}
{"id": "177633", "url": "https://en.wikipedia.org/wiki?curid=177633", "title": "Hypothetical syllogism", "text": "Hypothetical syllogism\n\nIn classical logic, hypothetical syllogism is a valid argument form which is a syllogism having a conditional statement for one or both of its premises.\n\nAn example in English:\n\nThe term originated with Theophrastus.\n\nIn propositional logic, hypothetical syllogism is the name of a valid rule of inference (often abbreviated HS and sometimes also called the chain argument, chain rule, or the principle of transitivity of implication). Hypothetical syllogism is one of the rules in classical logic that is not always accepted in certain systems of non-classical logic. The rule may be stated:\n\nwhere the rule is that whenever instances of \"formula_2\", and \"formula_3\" appear on lines of a proof, \"formula_4\" can be placed on a subsequent line.\n\nHypothetical syllogism is closely related and similar to disjunctive syllogism, in that it is also type of syllogism, and also the name of a rule of inference.\n\nThe \"hypothetical syllogism\" inference rule may be written in sequent notation, which amounts to a specialization of the cut rule:\n\nwhere formula_6 is a metalogical symbol and formula_7 meaning that formula_8 is a syntactic consequence of formula_9 in some logical system;\n\nand expressed as a truth-functional tautology or theorem of propositional logic:\n\nwhere formula_11, formula_12, and formula_13 are propositions expressed in some formal system.\n\n\n"}
{"id": "18305757", "url": "https://en.wikipedia.org/wiki?curid=18305757", "title": "Intermediate Math League of Eastern Massachusetts", "text": "Intermediate Math League of Eastern Massachusetts\n\nThe Intermediate Math League of Eastern Massachusetts (or IMLEM) is a math league for middle schools across Eastern Massachusetts. A brief history of IMLEM is given in its By-Laws:\n\nAs of 2017, 86 different schools attend the competition. Each school is allowed to send more than 1 team and each team can consist of at most 10 people. Alternates, people who are not officially part of team, can be taken too. There are a total of 15 different geographic clusters of schools and there is even a cluster of schools from Pennsylvania. The schools are then separated into different divisions with the schools in each division be approximately the same level. Schools can then make their way up through divisions to try to get into the top division, which is the Lexington Division. In total there are 13 divisions. \nSchools may send more than one team, however no student can compete on more than one team in a year. Also, a school may send alternates to gain the experience of a meet.\n\nIMLEM has five meets every year, one in October, November, January, February, and March. For the first three meets, no calculators or external aids of any sort are allowed for any round. However, for the last two meets, calculators without programming or graphing capabilities, and without a QWERTY keyboard, are allowed for all rounds.\n\nMeets are held at distinct locations for each of the ten geographic clusters. Schools within the clusters generally hold at most one of the meets. Each of the meets are generally held on the same day by all clusters; however for scheduling conflicts, schools may host meets on other days.\n\nThere are five individual categories, and they are: Mystery, Geometry, Number Theory, Arithmetic, and Algebra. Each individual round contains three questions, varying in content but focusing on topics published by the test writer. Questions in individual rounds are worth 2 points each. (Before this was the case, questions would be worth the question number, i.e., one point for question one, two points for question two and three points for question three.) Students are given 10 minutes to complete the round, along with a 1-minute warning.\n\nThe sixth category is a 15-minute team round that consists of six or nine questions (The amount of questions is unknown before the round starts). The entire team collaborates to solve each of the questions. The questions are usually based on topics from the five individual rounds with some extra knowledge required to solve other questions.\n\nStudents take three individual categories, and no more than six students on a team may take a single category. In a round, the maximum score for an individual is 6, and the maximum score for a team is 36. It follows that the maximum individual score is 18, and the maximum team score is 216.\n\n18s are not uncommon, and each meet sees many individuals who get 18s. Coming into the 2004-05 school year, a score above 200 had been achieved only five times: twice by Diamond Middle School, twice by Clarke Middle School, and the long-standing record of 205 set by Marblehead in 1983. (This was when individual rounds were weighted 1-2-3 instead of 2-2-2.) The first meet of 2004-05 saw two teams score 200: Clarke scored 200, and Diamond shattered the record by scoring 212. Diamond scored perfect 36s in each of the rounds except for round 3 (number theory), in which 2 students each got a question wrong.\n\nThe 2007-08 season saw Clarke Middle School shatter many records, recent and not alike. Firstly, Clarke scored 1006 for the year, which is an \"average\" of 201.2 per meet. This broke the previous record set by Diamond back in the 2004-05 year. Secondly, they outscored second place Diamond by 122 points, also breaking a Diamond record, this one set in 2005-06. This is in part an effect of the test-writers' push to make problems easier so as to encourage more participation.\n\nEach of the meets follows the following basic format. Each of the teams arrive usually by 3:15, and after snacks, the five individual rounds are held in succession. After that, a fifteen-minute team round is conducted. For all rounds, alternates and regulars are split. Finally, awards are distributed and teams should depart around 5:30.\n\n† Year indicated is that of the \"end\" of that school year (i.e., 2007 represents the 2006-07 school year).\n\n"}
{"id": "1070084", "url": "https://en.wikipedia.org/wiki?curid=1070084", "title": "Jean Dieudonné", "text": "Jean Dieudonné\n\nJean Alexandre Eugène Dieudonné (; 1 July 1906 – 29 November 1992) was a French mathematician, notable for research in abstract algebra, algebraic geometry, and functional analysis, for close involvement with the Nicolas Bourbaki pseudonymous group and the \"Éléments de géométrie algébrique\" project of Alexander Grothendieck, and as a historian of mathematics, particularly in the fields of functional analysis and algebraic topology. His work on the classical groups (the book \"La Géométrie des groupes classiques\" was published in 1955), and on formal groups, introducing what now are called Dieudonné modules, had a major effect on those fields.\n\nHe was born and brought up in Lille, with a formative stay in England where he was introduced to algebra. In 1924 he was admitted to the École Normale Supérieure, where André Weil was a classmate. He began working, conventionally enough, in complex analysis. In 1934 he was one of the group of \"normaliens\" convened by Weil, which would become 'Bourbaki'.\n\nHe served in the French Army during World War II, and then taught in Clermont-Ferrand until the liberation of France. After holding professorships at the University of São Paulo (1946–47), the University of Nancy (1948–1952) and the University of Michigan (1952–53), he joined the Department of Mathematics at Northwestern University in 1953, before returning to France as a founding member of the Institut des Hautes Études Scientifiques. He moved to the University of Nice to found the Department of Mathematics in 1964, and retired in 1970. He was elected as a member of the Académie des Sciences in 1968.\n\nDieudonné drafted much of the Bourbaki series of texts, the many volumes of the EGA algebraic geometry series, and nine volumes of his own Éléments d'Analyse. The first volume of the \"Traité\" is a French version of the book \"Foundations of Modern Analysis\" (1960), which had become a graduate textbook on functional analysis.\n\nHe also wrote individual monographs on \"Infinitesimal Calculus\", \"Linear Algebra and Elementary Geometry\", invariant theory, commutative algebra, algebraic geometry, and formal groups.\n\nWith Laurent Schwartz he supervised the early research of Alexander Grothendieck. Later from 1959 to 1964 he was at the Institut des Hautes Études Scientifiques alongside Grothendieck, and collaborating on the expository work needed to support the project of refounding algebraic geometry on the new basis of schemes.\n\n\n\n\n"}
{"id": "41312173", "url": "https://en.wikipedia.org/wiki?curid=41312173", "title": "Linear optical quantum computing", "text": "Linear optical quantum computing\n\nLinear Optical Quantum Computing or Linear Optics Quantum Computation (LOQC) is a paradigm of quantum computation, allowing (under certain conditions, described below) universal quantum computation. LOQC uses photons as information carriers, mainly uses linear optical elements (including beam splitters, phase shifters, and mirrors) to process quantum information, and uses photon detectors and quantum memories to detect and store quantum information.\n\nAlthough there are many other implementations for quantum information processing (QIP) and quantum computation, optical quantum systems are prominent candidates, since they link quantum computation and quantum communication in the same framework. In optical systems for quantum information processing, the unit of light in a given mode—or photon—is used to represent a qubit. Superpositions of quantum states can be easily represented, encrypted, transmitted and detected using photons. Besides, linear optical elements of optical systems may be the simplest building blocks to realize quantum operations and quantum gates. Each linear optical element equivalently applies a unitary transformation on a finite number of qubits. The system of finite linear optical elements constructs a network of linear optics, which can realize any quantum circuit diagram or quantum network based on the quantum circuit model. Quantum computing with continuous variables is also possible under the linear optics scheme.\n\nThe universality of 1- and 2-bit gates to implement arbitrary quantum computation has been proven. Up to formula_1 unitary matrix operations (formula_2) can be realized by only using mirrors, beam splitters and phase shifters (this is also a starting point of boson sampling and of computational complexity analysis for LOQC). It points out that each formula_2 operator with formula_4 inputs and formula_4 outputs can be constructed via formula_6 linear optical elements. Based on the reason of universality and complexity, LOQC usually only uses mirrors, beam splitters, phase shifters and their combinations such as Mach-Zehnder interferometers with phase shifts to implement arbitrary quantum operators. If using a non-deterministic scheme, this fact also implies that LOQC could be resource-inefficient in terms of the number of optical elements and time steps needed to implement a certain quantum gate or circuit, which is a major drawback of LOQC.\n\nOperations via linear optical elements (beam splitters, mirrors and phase shifters, in this case) preserve the photon statistics of input light. For example, a coherent (classical) light input produces a coherent light output; a superposition of quantum states input yields a quantum light state output. Due to this reason, people usually use single photon source case to analyze the effect of linear optical elements and operators. Multi-photon cases can be implied through some statistical transformations.\n\nAn intrinsic problem in using photons as information carriers is that photons hardly interact with each other. This potentially causes a scalability problem for LOQC, since nonlinear operations are hard to implement, which can increase the complexity of operators and hence can increase the resources required to realize a given computational function. One way to solve this problem is to bring nonlinear devices into the quantum network. For instance, the Kerr effect can be applied into LOQC to make a single-photon controlled-NOT and other operations.\n\nIt was believed that adding nonlinearity to the linear optical network was sufficient to realize efficient quantum computation. However, to implement nonlinear optical effects is a difficult task. In 2000, Knill, Laflamme and Milburn proved that it is possible to create universal quantum computers solely with linear optical tools. Their work has become known as the \"KLM scheme\" or \"KLM protocol\", which uses linear optical elements, single photon sources and photon detectors as resources to construct a quantum computation scheme involving only ancilla resources, quantum teleportations and error corrections. It uses another way of efficient quantum computation with linear optical systems, and promotes nonlinear operations solely with linear optical elements.\n\nAt its root, the KLM scheme induces an effective interaction between photons by making projective measurements with photodetectors, which falls into the category of non-deterministic quantum computation. It is based on a non-linear sign shift between two qubits that uses two ancilla photons and post-selection. It is also based on the demonstrations that the probability of success of the quantum gates can be made close to one by using entangled states prepared non-deterministically and quantum teleportation with single-qubit operations Otherwise, without a high enough success rate of a single quantum gate unit, it may require an exponential amount of computing resources. Meanwhile, the KLM scheme is based on the fact that proper quantum coding can reduce the resources for obtaining accurately encoded qubits efficiently with respect to the accuracy achieved, and can make LOQC fault-tolerant for photon loss, detector inefficiency and phase decoherence. As a result, LOQC can be robustly implemented through the KLM scheme with a low enough resource requirement to suggest practical scalability, making it as promising a technology for QIP as other known implementations.\n\nThe more limited boson sampling model was suggested and analyzed by Aaronson and Arkhipov in 2013. It is not believed to be universal, but can still solve problems that are believed to be beyond the ability of classical computers, such as the boson sampling problem.\n\nDiVincenzo's criteria for quantum computation and QIP give that a universal system for QIP should satisfy at least the following requirements:\n\nAs a result of using photons and linear optical circuits, in general LOQC systems can easily satisfy conditions 3, 6 and 7. The following sections mainly focus on the implementations of quantum information preparation, readout, manipulation, scalability and error corrections, in order to discuss the advantages and disadvantages of LOQC as a candidate for QIP\n\nA qubit is one of the fundamental QIP units. A qubit state which can be represented by\nformula_8 is a superposition state which, if measured in the orthonormal basis <math>\\\n\nIn the optical realization of the CNOT gate, the polarization and location are the control and target qubit, respectively.\n"}
{"id": "49064417", "url": "https://en.wikipedia.org/wiki?curid=49064417", "title": "List of things named after Hermann Grassmann", "text": "List of things named after Hermann Grassmann\n\nThe following is a list of things named in the memory of the German scholar and polymath Hermann Grassmann:\n\n\n"}
{"id": "38040638", "url": "https://en.wikipedia.org/wiki?curid=38040638", "title": "List of things named after Johannes Kepler", "text": "List of things named after Johannes Kepler\n\nThis is a list of things named after German mathematician and astronomer Johannes Kepler (1571 – 1630).\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "17267863", "url": "https://en.wikipedia.org/wiki?curid=17267863", "title": "Locally finite space", "text": "Locally finite space\n\nIn the mathematical field of topology, a locally finite space is a topological space in which every point has a finite neighborhood.\n\nA locally finite space is Alexandrov.\n\nA T space is locally finite if and only if it is discrete.\n"}
{"id": "636427", "url": "https://en.wikipedia.org/wiki?curid=636427", "title": "Mahler's compactness theorem", "text": "Mahler's compactness theorem\n\nIn mathematics, Mahler's compactness theorem, proved by , is a foundational result on lattices in Euclidean space, characterising sets of lattices that are 'bounded' in a certain definite sense. Looked at another way, it explains the ways in which a lattice could degenerate (\"go off to infinity\") in a sequence of lattices. In intuitive terms it says that this is possible in just two ways: becoming \"coarse-grained\" with a fundamental domain that has ever larger volume; or containing shorter and shorter vectors. It is also called his selection theorem, following an older convention used in naming compactness theorems, because they were formulated in terms of sequential compactness (the possibility of selecting a convergent subsequence).\n\nLet \"X\" be the space \n\nthat parametrises lattices in formula_2, with its quotient topology. There is a well-defined function Δ on \"X\", which is the absolute value of the determinant of a matrix – this is constant on the cosets, since an invertible integer matrix has determinant 1 or −1.\n\nMahler's compactness theorem states that a subset \"Y\" of \"X\" is relatively compact if and only if Δ is bounded on \"Y\", and there is a neighbourhood \"N\" of {0} in formula_2 such that for all \"Λ\" in \"Y\", the only lattice point of Λ in \"N\" is 0 itself.\n\nThe assertion of Mahler's theorem is equivalent to the compactness of the space of unit-covolume lattices in formula_2 whose systole is larger or equal than any fixed formula_5.\n\nMahler's compactness theorem was generalized to semisimple Lie groups by David Mumford; see Mumford's compactness theorem.\n\n"}
{"id": "17185722", "url": "https://en.wikipedia.org/wiki?curid=17185722", "title": "Marden's theorem", "text": "Marden's theorem\n\nIn mathematics, Marden's theorem, named after Morris Marden but proven much earlier by Jörg Siebeck, gives a geometric relationship between the zeroes of a third-degree polynomial with complex coefficients and the zeroes of its derivative.\n\nA cubic polynomial has three zeroes in the complex number plane, which in general form a triangle, and the Gauss–Lucas theorem states that the roots of its derivative lie within this triangle. Marden's theorem states their location within this triangle more precisely:\n\nBy the Gauss–Lucas theorem, the root of the double derivative must be the average of the two foci, which is the center point of the ellipse and the centroid of the triangle.\nIn the special case that the triangle is equilateral (as happens, for instance, for the polynomial ) the inscribed ellipse degenerates to a circle, and the derivative of  has a double root at the center of the circle. Conversely, if the derivative has a double root, then the triangle must be equilateral .\n\nA more general version of the theorem, due to , applies to polynomials whose degree may be higher than three, but that have only three roots , , and . For such polynomials, the roots of the derivative may be found at the multiple roots of the given polynomial (the roots whose exponent is greater than one) and at the foci of an ellipse whose points of tangency to the triangle divide its sides in the ratios , , and .\n\nAnother generalization () is to \"n\"-gons: some \"n\"-gons have an interior ellipse that is tangent to each side at the side's midpoint. Marden's theorem still applies: the foci of this midpoint-tangent inellipse are zeroes of the derivative of the polynomial whose zeroes are the vertices of the \"n\"-gon.\n\nJörg Siebeck discovered this theorem 81 years before Marden wrote about it. However, Dan Kalman titled his American Mathematical Monthly paper \"Marden's theorem\" because, as he writes, \"I call this Marden’s Theorem because I first read it in M. Marden’s wonderful book\".\n\nA short and elementary proof of Marden’s theorem is explained in the solution of an exercise in Fritz Carlson’s book “Geometri” (in Swedish, 1943).\n\n\n"}
{"id": "35613685", "url": "https://en.wikipedia.org/wiki?curid=35613685", "title": "Mihai Pătrașcu", "text": "Mihai Pătrașcu\n\nMihai Pătraşcu (17 July 1982 – 5 June 2012) was a Romanian-American computer scientist at AT&T Labs in Florham Park, New Jersey, USA.\n\nPătraşcu attended Carol I National College in Craiova.\nAs a high school student, he won multiple medals at the International Olympiad in Informatics. He completed his undergraduate and graduate studies in Computer Science at Massachusetts Institute of Technology, completing his thesis under the supervision of Erik Demaine in 2008.\n\nPătraşcu’s work was concerned with fundamental questions about basic data structures.\nPătraşcu received the Machtey Award for the best student paper at the Symposium on Foundations of Computer Science in 2008, and the Presburger Award from the European Association for Theoretical Computer Science in 2012, for breaking \"many old barriers on fundamental data structure problems, not only revitalizing but also revolutionizing a field that was almost silent for over a decade.\"\n\nPătraşcu died in 2012 after suffering from brain cancer for a year and a half.\n\n\n"}
{"id": "27966551", "url": "https://en.wikipedia.org/wiki?curid=27966551", "title": "Minkowski problem", "text": "Minkowski problem\n\nIn differential geometry, the Minkowski problem, named after Hermann Minkowski, asks for the construction of a strictly convex compact surface \"S\" whose Gaussian curvature is specified. More precisely, the input to the problem is a strictly positive real function \"ƒ\" defined on a sphere, and the surface that is to be constructed should have Gaussian curvature \"ƒ\"(\"n\"(\"x\")) at the point \"x\", where \"n\"(\"x\") denotes the normal to \"S\" at \"x\". Eugenio Calabi stated: \"From the geometric view point it [the Minkowski problem] is the Rosetta Stone, from which several related problems can be solved.\"\n\nIn full generality, the Minkowski problem asks for necessary and sufficient conditions on a non-negative Borel measure on the unit sphere \"S\" to be the surface area measure of a convex body in formula_1. Here the surface area measure \"S\" of a convex body \"K\" is the pushforward of the \"(n-1)\"-dimensional Hausdorff measure restricted to the boundary of \"K\" via the Gauss map. The Minkowski problem was solved by Hermann Minkowski, Aleksandr Danilovich Aleksandrov, Werner Fenchel and Børge Jessen: a Borel measure \"μ\" on the unit sphere is the surface area measure of a convex body if and only if \"μ\" has centroid at the origin and is not concentrated on a great subsphere. The convex body is then uniquely determined by \"μ\" up to translations.\n\nThe problem of radiolocation is easily reduced to the Minkowski problem in Euclidean 3-space: restoration of convex shape over the given Gauss surface curvature. The inverse problem of the short-wave diffraction is reduced to the Minkowski problem. The Minkowski problem is the basis of the mathematical theory of diffraction as well as for the physical theory of diffraction. In the 1960s Petr Ufimtsev (P. Ya. Ufimtsev) began developing a high-frequency asymptotic theory for predicting the scattering of electromagnetic waves from two-dimensional and three-dimensional objects. Now this theory is well known as the physical theory of diffraction (PTD). This theory played the main role in the design of American stealth-aircraft F-117 and B-2.\n\nIn 1953 Louis Nirenberg published the solutions of two long standing open problems, the Weyl problem and the Minkowski problem in Euclidean 3-space. L. Nirenberg's solution of the Minkowski problem was a milestone in global geometry. He has been selected to be the first recipient of the Chern Medal (in 2010) for his role in the formulation of the modern theory of non-liner elliptic partial differential equations, particularly for solving the Weyl problem and the Minkowski problems in Euclidean 3-space.\n\nA. V. Pogorelov received Ukraine State Prize (1973) for resolving the multidimensional Minkowski problem in Euclidean spaces. Pogorelov resolved the Weyl problem in Riemannian space in 1969.\n\nShing-Tung Yau's joint work with Shiu-Yuen Cheng gives a complete proof of the higher-dimensional Minkowski problem in Euclidean spaces. Shing-Tung Yau received the Fields Medal at the International Congress of Mathematicians in Warsaw in 1982 for his work in global differential geometry and elliptic partial differential equations, particularly for solving such difficult problems as the Calabi conjecture of 1954, and a problem of Hermann Minkowski in Euclidean spaces concerning the Dirichlet problem for the real Monge–Ampère equation.\n\n"}
{"id": "18900", "url": "https://en.wikipedia.org/wiki?curid=18900", "title": "Modus ponens", "text": "Modus ponens\n\nIn propositional logic, modus ponens (; MP; also modus ponendo ponens (Latin for \"mode that affirms by affirming\") or implication elimination) is a rule of inference. It can be summarized as \"\"P implies Q\" and \"P\" is asserted to be true, therefore \"Q\" must be true.\"\n\n\"Modus ponens\" is closely related to another valid form of argument, \"modus tollens\". Both have apparently similar but invalid forms such as affirming the consequent, denying the antecedent, and evidence of absence. Constructive dilemma is the disjunctive version of \"modus ponens\". Hypothetical syllogism is closely related to \"modus ponens\" and sometimes thought of as \"double \"modus ponens\".\"\n\nThe history of \"modus ponens\" goes back to antiquity. The first to explicitly describe the argument form \"modus ponens\" was Theophrastus.\n\nThe \"modus ponens\" rule may be written in sequent notation as\n\nwhere \"P\", \"Q\" and \"P\" → \"Q\" are statements (or propositions) in a formal language and ⊢ is a metalogical symbol meaning that \"Q\" is a syntactic consequence of \"P\" and \"P\" → \"Q\" in some logical system.\n\nThe argument form has two premises (hypothesis). The first premise is the \"if–then\" or conditional claim, namely that \"P\" implies \"Q\". The second premise is that \"P\", the antecedent of the conditional claim, is true. From these two premises it can be logically concluded that \"Q\", the consequent of the conditional claim, must be true as well. In artificial intelligence, \"modus ponens\" is often called forward chaining.\n\nAn example of an argument that fits the form \"modus ponens\":\n\nThis argument is valid, but this has no bearing on whether any of the statements in the argument are true; for \"modus ponens\" to be a sound argument, the premises must be true for any true instances of the conclusion. An argument can be valid but nonetheless unsound if one or more premises are false; if an argument is valid \"and\" all the premises are true, then the argument is sound. For example, John might be going to work on Wednesday. In this case, the reasoning for John's going to work (because it is Wednesday) is unsound. The argument is not only sound on Tuesdays (when John goes to work), but valid on every day of the week. A propositional argument using \"modus ponens\" is said to be deductive.\n\nIn single-conclusion sequent calculi, \"modus ponens\" is the Cut rule. The cut-elimination theorem for a calculus says that every proof involving Cut can be transformed (generally, by a constructive method) into a proof without Cut, and hence that Cut is admissible.\n\nThe Curry–Howard correspondence between proofs and programs relates \"modus ponens\" to function application: if \"f\" is a function of type \"P\" → \"Q\" and \"x\" is of type \"P\", then \"f x\" is of type \"Q\".\n\nThe validity of \"modus ponens\" in classical two-valued logic can be clearly demonstrated by use of a truth table.\nIn instances of \"modus ponens\" we assume as premises that \"p\" → \"q\" is true and \"p\" is true. Only one line of the truth table—the first—satisfies these two conditions (\"p\" and \"p\" → \"q\"). On this line, \"q\" is also true. Therefore, whenever \"p\" → \"q\" is true and \"p\" is true, \"q\" must also be true.\n\nWhile \"modus ponens\" is one of the most commonly used argument forms in logic it must not be mistaken for a logical law; rather, it is one of the accepted mechanisms for the construction of deductive proofs that includes the \"rule of definition\" and the \"rule of substitution\". \"Modus ponens\" allows one to eliminate a conditional statement from a logical proof or argument (the antecedents) and thereby not carry these antecedents forward in an ever-lengthening string of symbols; for this reason modus ponens is sometimes called the rule of detachment or the law of detachment. Enderton, for example, observes that \"modus ponens can produce shorter formulas from longer ones\", and Russell observes that \"the process of the inference cannot be reduced to symbols. Its sole record is the occurrence of ⊦q [the consequent] . . . an inference is the dropping of a true premise; it is the dissolution of an implication\".\n\nA justification for the \"trust in inference is the belief that if the two former assertions [the antecedents] are not in error, the final assertion [the consequent] is not in error\". In other words: if one statement or proposition implies a second one, and the first statement or proposition is true, then the second one is also true. If \"P\" implies \"Q\" and \"P\" is true, then \"Q\" is true.\n\n\"Modus ponens\" represents an instance of the Law of total probability which for a binary variable is expressed as:\n\nformula_2,\n\nwhere e.g. formula_3 denotes the probability of formula_4 and the conditional probability formula_5 generalizes the logical implication formula_6. Assume that formula_7 is equivalent to formula_4 being TRUE, and that formula_9 is equivalent to formula_4 being FALSE. It is then easy to see that formula_7 when formula_12 and formula_13. Hence, the law of total probability represents a generalization of \"modus ponens\" .\n\n\"Modus ponens\" represents an instance of the binomial deduction operator in subjective logic expressed as:\n\nformula_14,\n\nwhere formula_15 denotes the subjective opinion about formula_16 as expressed by source formula_17, and the conditional opinion formula_18 generalizes the logical implication formula_6. The deduced marginal opinion about formula_4 is denoted by formula_21. The case where formula_15 is an absolute TRUE opinion about formula_16 is equivalent to source formula_17 saying that formula_16 is TRUE, and the case where formula_15 is an absolute FALSE opinion about formula_16 is equivalent to source formula_17 saying that formula_16 is FALSE. The deduction operator formula_30 of subjective logic produces an absolute TRUE deduced opinion formula_21 when the conditional opinion formula_18 is absolute TRUE and the antecedent opinion formula_15 is absolute TRUE. Hence, subjective logic deduction represents a generalization of both \"modus ponens\" and the Law of total probability .\n\nThe philosopher and logician Vann McGee has argued that \"modus ponens\" can fail to be valid when the consequent is itself a conditional sentence. Here is an example:\n\nThe first premise seems reasonable enough, because Shakespeare is generally credited with writing \"Hamlet\". The second premise seems reasonable, as well, because with the range of \"Hamlet\" 's possible authors limited to just Shakespeare and Hobbes, eliminating one leaves only the other. But the conclusion is dubious, because if Shakespeare is ruled out as \"Hamlet\"'s author, there are many more plausible alternatives than Hobbes.\n\nThe general form of McGee-type counterexamples to \"modus ponens\" is simply formula_34, therefore formula_35; it is not essential that formula_16 have the form of a disjunction, as in the example given. That these kinds of cases constitute failures of \"modus ponens\" remains a minority view among logicians, but there is no consensus on how the cases should be disposed of.\n\nThe fallacy of affirming the consequent is a common misinterpretation of the modus ponens.\n\n\n\n"}
{"id": "62047", "url": "https://en.wikipedia.org/wiki?curid=62047", "title": "Mrs. Miniver's problem", "text": "Mrs. Miniver's problem\n\nMrs. Miniver's problem is a geometry problem about circles. Given a circle \"A\", find a circle \"B\" such that the area of the lens formed by intersecting their two interiors is equal to the area of the symmetric difference of \"A\" and \"B\" (the sum of the areas contained in one but not both circles).\n\nThe problem derives from \"A Country House Visit\", one of Jan Struther's newspaper articles featuring her character Mrs. Miniver. According to the story:\n\nShe saw every relationship as a pair of intersecting circles. It would seem at first glance that the more they overlapped the better the relationship; but this is not so. Beyond a certain point the law of diminishing returns sets in, and there are not enough private resources left on either side to enrich the life that is shared. Probably perfection is reached when the area of the two outer crescents, added together, is exactly equal to that of the leaf-shaped piece in the middle. On paper there must be some neat mathematical formula for arriving at this; in life, none.\n\nAlan Wachtel writes of the problem:\n\nIt seems that certain mathematicians took this literary challenge literally, and Fadiman follows it with an excerpt from \"Ingenious Mathematical Problems and Methods,\" by L. A. Graham, who had evidently posed the problem in a mathematics journal. Graham gives a solution by William W. Johnson of Cleveland for the general case of unequal circles. The analysis isn't difficult, but the resulting transcendental equation is messy and can't be solved exactly. When the circles are of equal size, the equation is much simpler, but it still can be solved only approximately.\n\nIn the case of two circles of equal size, the ratio of the distance between their centers and their radius is often quoted as approximately 0.807946. However, that actually describes the case when the three areas each are of equal size. The solution for the problem as stated in the story (\"when the area of the two outer crescents, added together, is exactly equal to that of the leaf-shaped piece in the middle\") is approximately 0.529864.\n\n\n"}
{"id": "5494042", "url": "https://en.wikipedia.org/wiki?curid=5494042", "title": "Paul Epstein", "text": "Paul Epstein\n\nPaul Epstein (July 24, 1871 – August 11, 1939) was a German mathematician. He was known for his contributions to number theory, in particular the Epstein zeta function.\n\nEpstein was born and brought up in Frankfurt, where his father was a professor. He received his PhD in 1895 from the University of Strasbourg. From 1895 to 1918 he was a Privatdozent at the University in Strasbourg, which at that time was part of the German Empire. At the end of World War I the city of Strasbourg reverted to France, and Epstein, being German, had to return to Frankfurt.\n\nEpstein was appointed to a non-tenured post at the university and he lectured in Frankfurt from 1919. Later he was appointed professor at Frankfurt. However, after the Nazis came to power in Germany he lost his university position. Because of his age he was unable to find a new position abroad, and finally committed suicide by barbital overdose at Dornbusch, fearing Gestapo torture because he was a Jew.\n\n"}
{"id": "20983125", "url": "https://en.wikipedia.org/wiki?curid=20983125", "title": "Pettis integral", "text": "Pettis integral\n\nIn mathematics, the Pettis integral or Gelfand–Pettis integral, named after Israel M. Gelfand and Billy James Pettis, extends the definition of the Lebesgue integral to vector-valued functions on a measure space, by exploiting duality. The integral was introduced by Gelfand for the case when the measure space is an interval with Lebesgue measure. The integral is also called the weak integral in contrast to the Bochner integral, which is the strong integral.\n\nLet formula_1 where formula_2 is a measure space and formula_3 is a topological vector space. Suppose that formula_3 admits a dual space formula_5 that separates points, e.g. formula_3 is a Banach space or (more generally) a is locally-convex Hausdorff vector space. We write evaluation of a functional as duality pairing: formula_7.\n\nWe say that formula_8 is Pettis integrable if formula_9 for all formula_10 and there exists a vector formula_11 so that:\n\nIn this case, we call formula_13 the Pettis integral of formula_8. Common notations for the Pettis integral formula_13 include \n\n\n\nAn important property is that the Pettis integral with respect to a finite measure is contained in the closure of the convex hull of the values scaled by the measure of the integration domain:\n\nThis is a consequence of the Hahn-Banach theorem and generalises the mean value theorem for integrals of real-valued functions: If formula_34 then closed convex sets are simply intervals and for formula_35 the inequalities \n\nhold.\n\n\n\n\n\nLet formula_58 be a probability space, and let formula_3 be a topological vector space with a dual space that separates points. Let formula_60 be a sequence of Pettis-integrable random variables, and write formula_61 for the Pettis integral of formula_62 (over formula_42). Note that formula_61 is a (non-random) vector in formula_3, and is not a scalar value.\n\nLet \n\ndenote the sample average. By linearity, formula_67 is Pettis integrable, and \n\nSuppose that the partial sums \n\nconverge absolutely in the topology of formula_3, in the sense that all rearrangements of the sum converge to a single vector formula_71. The weak law of large numbers implies that formula_72 for every functional formula_73. Consequently, formula_74 in the weak topology on formula_42.\n\nWithout further assumptions, it is possible that formula_76 does not converge to formula_77. To get strong convergence, more assumptions are necessary.\n\n\n"}
{"id": "8635114", "url": "https://en.wikipedia.org/wiki?curid=8635114", "title": "Pompeiu's theorem", "text": "Pompeiu's theorem\n\nPompeiu's theorem is a result of plane geometry, discovered by the Romanian mathematician Dimitrie Pompeiu. The theorem is simple, but not classical. It states the following:\n\nThe proof is quick. Consider a rotation of 60° about the point \"C\". Assume \"A\" maps to \"B\", and \"P\" maps to \"P\" <nowiki>'</nowiki>. Then formula_1, and formula_2. Hence triangle \"PCP\" <nowiki>'</nowiki> is equilateral and formula_3. Then formula_4. Thus, triangle \"PBP\" <nowiki>'</nowiki> has sides equal to \"PA\", \"PB\", and \"PC\" and the proof by construction is complete.\n\nFurther investigations reveal that if \"P\" is not in the interior of the triangle, but rather on the circumcircle, then \"PA\", \"PB\", \"PC\" form a degenerate triangle, with the largest being equal to the sum of the others.\n\n"}
{"id": "566869", "url": "https://en.wikipedia.org/wiki?curid=566869", "title": "Quasi-arithmetic mean", "text": "Quasi-arithmetic mean\n\nIn mathematics and statistics, the quasi-arithmetic mean or generalised \"f\"-mean is one generalisation of the more familiar means such as the arithmetic mean and the geometric mean, using a function formula_1. It is also called Kolmogorov mean after Russian mathematician Andrey Kolmogorov. It is a broader generalization than the regular generalized mean.\n\nIf \"f\" is a function which maps an interval formula_2 of the real line to the real numbers, and is both continuous and injective, the \"f\"-mean of formula_3 numbers\nis defined as\n\nWe require \"f\" to be injective in order for the inverse function formula_6 to exist. Since formula_1 is defined over an interval, formula_8 lies within the domain of formula_6.\n\nSince \"f\" is injective and continuous, it follows that \"f\" is a strictly monotonic function, and therefore that the \"f\"-mean is neither larger than the largest number of the tuple formula_10 nor smaller than the smallest number in formula_10.\n\n\nMeans are usually homogeneous, but for most functions formula_1, the \"f\"-mean is not.\nIndeed, the only homogeneous quasi-arithmetic means are the power means and the geometric mean; see Hardy–Littlewood–Pólya, page 68.\n\nThe homogeneity property can be achieved by normalizing the input values by some (homogeneous) mean formula_42.\nHowever this modification may violate monotonicity and the partitioning property of the mean.\n\n\n"}
{"id": "321913", "url": "https://en.wikipedia.org/wiki?curid=321913", "title": "Quasiperfect number", "text": "Quasiperfect number\n\nIn mathematics, a quasiperfect number is a natural number \"n\" for which the sum of all its divisors (the divisor function \"σ\"(\"n\")) is equal to 2\"n\" + 1. Equivalently, \"n\" is the sum of its non-trivial divisors (that is, its divisors excluding 1 and \"n\"). No quasiperfect numbers have been found so far.\n\nThe quasiperfect numbers are the abundant numbers of minimal abundance 1.\n\nIf a quasiperfect number exists, it must be an odd square number greater than 10 and have at least seven distinct prime factors.\n\nNumbers do exist where the sum of all the divisors \"σ\"(\"n\") is equal to 2\"n\" + 2: 20, 104, 464, 650, 1952, 130304, 522752 ... . \nMany of these numbers are of the form 2(2 − 3) where 2 − 3 is prime (instead of 2 − 1 with perfect numbers). In addition, numbers exist where the sum of all the divisors \"σ\"(\"n\") is equal to 2n - 1, such as the powers of 2.\n\nBetrothed numbers relate to quasiperfect numbers like amicable numbers relate to perfect numbers.\n\n"}
{"id": "2009061", "url": "https://en.wikipedia.org/wiki?curid=2009061", "title": "Regularization (mathematics)", "text": "Regularization (mathematics)\n\nIn mathematics, statistics, and computer science, particularly in the fields of machine learning and inverse problems, regularization is a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting.\n\nIn general, regularization is a technique that applies to objective functions in ill-posed optimization problems.\n\nOne particular use of regularization is in the field of classification. Empirical learning of classifiers (learning from a finite data set) is always an underdetermined problem, because in general we are trying to infer a function of any formula_1 given only some examples formula_2.\n\nA regularization term (or regularizer) formula_3 is added to a loss function:\n\nwhere formula_5 is an underlying loss function that describes the cost of predicting formula_6 when the label is formula_7, such as the square loss or hinge loss; and formula_8 is a parameter which controls the importance of the regularization term. formula_3 is typically chosen to impose a penalty on the complexity of formula_10. Concrete notions of complexity used include restrictions for smoothness and bounds on the vector space norm.\n\nA theoretical justification for regularization is that it attempts to impose Occam's razor on the solution (as depicted in the figure above, where the green function, the simpler one, may be preferred). From a Bayesian point of view, many regularization techniques correspond to imposing certain prior distributions on model parameters.\n\nRegularization can be used to learn simpler models, induce models to be sparse, introduce group structure into the learning problem, and more.\n\nThe same idea arose in many fields of science. For example, the least-squares method can be viewed as a very simple form of regularization . A simple form of regularization applied to integral equations, generally termed Tikhonov regularization after Andrey Nikolayevich Tikhonov, is essentially a trade-off between fitting the data and reducing a norm of the solution. More recently, non-linear regularization methods, including total variation regularization, have become popular.\n\nRegularization can be motivated as a technique to improve the generalizability of a learned model.\n\nThe goal of this learning problem is to find a function that fits or predicts the outcome (label) that minimizes the expected error over all possible inputs and labels. The expected error of a function formula_11 is:\n\nTypically in learning problems, only a subset of input data and labels are available, measured with some noise. Therefore, the expected error is unmeasurable, and the best surrogate available is the empirical error over the formula_13 available samples:\n\nWithout bounds on the complexity of the function space (formally, the reproducing kernel Hilbert space) available, a model will be learned that incurs zero loss on the surrogate empirical error. If measurements (e.g. of formula_15) were made with noise, this model may suffer from overfitting and display poor expected error. Regularization introduces a penalty for exploring certain regions of the function space used to build the model, which can improve generalization.\n\nWhen learning a linear function formula_10, characterized by an unknown vector formula_17 such that formula_18, the formula_19-norm loss Tikhonov regularization. This is one of the most common forms of regularization, is also known as ridge regression, and is expressed as:\n\nIn the case of a general function, we take the norm of the function in its reproducing kernel Hilbert space:\n\nAs the formula_19 norm is differentiable, learning problems using Tikhonov regularization can be solved by gradient descent.\n\nThe learning problem with the least squares loss function and Tikhonov regularization can be solved analytically. Written in matrix form, the optimal formula_17 will be the one for which the gradient of the loss function with respect to formula_17 is 0.\n\nBy construction of the optimization problem, other values of formula_17 would give larger values for the loss function. This could be verified by examining the second derivative formula_31.\n\nDuring training, this algorithm takes formula_32 time. The terms correspond to the matrix inversion and calculating formula_33, respectively. Testing takes formula_34 time.\n\nEarly stopping can be viewed as regularization in time. Intuitively, a training procedure like gradient descent will tend to learn more and more complex functions as the number of iterations increases. By regularizing on time, the complexity of the model can be controlled, improving generalization.\n\nIn practice, early stopping is implemented by training on a training set and measuring accuracy on a statistically independent validation set. The model is trained until performance on the validation set no longer improves. The model is then tested on a testing set.\n\nConsider the finite approximation of Neumann series for an invertible matrix where formula_35:\n\nThis can be used to approximate the analytical solution of unregularized least squares, if is introduced to ensure the norm is less than one.\n\nThe exact solution to the unregularized least squares learning problem will minimize the empirical error, but may fail to generalize and minimize the expected error. By limiting , the only free parameter in the algorithm above, the problem is regularized on time which may improve its generalization.\n\nThe algorithm above is equivalent to restricting the number of gradient descent iterations for the empirical risk\n\nwith the gradient descent update:\n\nThe base case is trivial. The inductive case is proved as follows:\n\nAssume that a dictionary formula_41 with dimension formula_42 is given such that a function in the function space can be expressed as:\n\nEnforcing a sparsity constraint on formula_17 can lead to simpler and more interpretable models. This is useful in many real-life applications such as computational biology. An example is developing a simple predictive test for a disease in order to minimize the cost of performing medical tests while maximizing predictive power.\n\nA sensible sparsity constraint is the formula_45 norm formula_46, defined as the number of non-zero elements in formula_17. Solving a formula_45 regularized learning problem, however, has been demonstrated to be NP-hard.\n\nThe formula_49 norm can be used to approximate the optimal formula_45 norm via convex relaxation. It can be shown that the formula_49 norm induces sparsity. In the case of least squares, this problem is known as LASSO in statistics and basis pursuit in signal processing.\n\nformula_49 regularization can occasionally produce non-unique solutions. A simple example is provided in the figure when the space of possible solutions lies on a 45 degree line. This can be problematic for certain applications, and is overcome by combining formula_49 with formula_19 regularization in elastic net regularization, which takes the following form:\n\nElastic net regularization tends to have a grouping effect, where correlated input features are assigned equal weights.\n\nElastic net regularization is commonly used in practice and is implemented in many machine learning libraries.\n\nWhile the formula_49 norm does not result in an NP-hard problem, the formula_49 norm is convex but is not strictly diffentiable due to the kink at x = 0. Subgradient methods which rely on the subderivative can be used to solve formula_49 regularized learning problems. However, faster convergence can be achieved through proximal methods.\n\nFor a problem formula_60 such that formula_61 is convex, continuous, differentiable, with Lipschitz continuous gradient (such as the least squares loss function), and formula_62 is convex, continuous, and proper, then the proximal method to solve the problem is as follows. First define the proximal operator\n\nand then iterate\n\nThe proximal method iteratively performs gradient descent and then projects the result back into the space permitted by formula_62.\n\nWhen formula_62 is the formula_49 regularizer, the proximal operator is equivalent to the soft-thresholding operator,\n\nThis allows for efficient computation.\n\nGroups of features can be regularized by a sparsity constraint, which can be useful for expressing certain prior knowledge into an optimization problem.\n\nIn the case of a linear model with non-overlapping known groups, a regularizer can be defined:\n\nThis can be viewed as inducing a regularizer over the formula_19 norm over members of each group followed by an formula_49 norm over groups.\n\nThis can be solved by the proximal method, where the proximal operator is a block-wise soft-thresholding function:\n\nThe algorithm described for group sparsity without overlaps can be applied to the case where groups do overlap, in certain situations. This will likely result in some groups with all zero elements, and other groups with some non-zero and some zero elements.\n\nIf it is desired to preserve the group structure, a new regularizer can be defined:\n\nFor each formula_75, formula_76 is defined as the vector such that the restriction of formula_76 to the group formula_78 equals formula_75 and all other entries of formula_76 are zero. The regularizer finds the optimal disintegration of formula_17 into parts. It can be viewed as duplicating all elements that exist in multiple groups. Learning problems with this regularizer can also be solved with the proximal method with a complication. The proximal operator cannot be computed in closed form, but can be effectively solved iteratively, inducing an inner iteration within the proximal method iteration.\n\nWhen labels are more expensive to gather than input examples, semi-supervised learning can be useful. Regularizers have been designed to guide learning algorithms to learn models that respect the structure of unsupervised training samples. If a symmetric weight matrix formula_82 is given, a regularizer can be defined:\n\nIf formula_84 encodes the result of some distance metric for points formula_15 and formula_86, it is desirable that formula_87. This regularizer captures this intuition, and is equivalent to:\n\nThe optimization problem formula_91 can be solved analytically if the constraint formula_92 is applied for all supervised samples. The labeled part of the vector formula_10 is therefore obvious. The unlabeled part of formula_10 is solved for by:\n\nNote that the pseudo-inverse can be taken because formula_98 has the same range as formula_99.\n\nIn the case of multitask learning, formula_100 problems are considered simultaneously, each related in some way. The goal is to learn formula_100 functions, ideally borrowing strength from the relatedness of tasks, that have predictive power. This is equivalent to learning the matrix formula_102 .\n\nThis regularizer defines an L2 norm on each column and an L1 norm over all columns. It can be solved by proximal methods.\n\nThis regularizer constrains the functions learned for each task to be similar to the overall average of the functions across all tasks. This is useful for expressing prior information that each task is expected to share similarities with each other task. An example is predicting blood iron levels measured at different times of the day, where each task represents a different person.\n\nThis regularizer is similar to the mean-constrained regularizer, but instead enforces similarity between tasks within the same cluster. This can capture more complex prior information. This technique has been used to predict Netflix recommendations. A cluster would correspond to a group of people who share similar preferences in movies.\n\nMore general than above, similarity between tasks can be defined by a function. The regularizer encourages the model to learn similar functions for similar tasks.\n\nBayesian learning methods make use of a prior probability that (usually) gives lower probability to more complex models. Well-known model selection techniques include the Akaike information criterion (AIC), minimum description length (MDL), and the Bayesian information criterion (BIC). Alternative methods of controlling overfitting not involving regularization include cross-validation.\n\nExamples of applications of different methods of regularization to the linear model are:\n\n"}
{"id": "977922", "url": "https://en.wikipedia.org/wiki?curid=977922", "title": "Relativity (M. C. Escher)", "text": "Relativity (M. C. Escher)\n\nRelativity is a lithograph print by the Dutch artist M. C. Escher, first printed in December 1953. \n\nIt depicts a world in which the normal laws of gravity do not apply. The architectural structure seems to be the centre of an idyllic community, with most of its inhabitants casually going about their ordinary business, such as dining. There are windows and doorways leading to park-like outdoor settings. All of the figures are dressed in identical attire and have featureless bulb-shaped heads. Identical characters such as these can be found in many other Escher works. \n\nIn the world of \"Relativity\", there are three sources of gravity, each being orthogonal to the two others. Each inhabitant lives in one of the gravity wells, where normal physical laws apply. There are sixteen characters, spread between each gravity source, six in one and five each in the other two. The apparent confusion of the lithograph print comes from the fact that the three gravity sources are depicted in the same space.\n\nThe structure has seven stairways, and each stairway can be used by people who belong to two different gravity sources. This creates interesting phenomena, such as in the top stairway, where two inhabitants use the same stairway in the same direction and on the same side, but each using a different face of each step; thus, one descends the stairway as the other climbs it, even while moving in the same direction nearly side-by-side. In the other stairways, inhabitants are depicted as climbing the stairways upside-down, but based on their own gravity source, they are climbing normally.\n\nEach of the three parks belongs to one of the gravity wells. All but one of the doors seem to lead to basements below the parks. Though physically possible, such basements are certainly unusual and add to the surreal effect of the picture.\n\nThis is one of Escher’s most popular works and has been used in a variety of ways.\n"}
{"id": "27069553", "url": "https://en.wikipedia.org/wiki?curid=27069553", "title": "Robert Frucht", "text": "Robert Frucht\n\nRobert Wertheimer Frucht (later known as Roberto Frucht) (9 August 1906 – 26 June 1997) was a German-Chilean mathematician; his research specialty was graph theory and the symmetries of graphs.\nIn 1908, Frucht's family moved from Brünn, Austria-Hungary (now in the Czech Republic), where he was born, to Berlin. Frucht entered the University of Berlin in 1924 with an interest in differential geometry, but switched to group theory under the influence of his doctoral advisor, Issai Schur; he received his Ph.D. in 1931. Unable to find academic employment in Germany due to his Jewish descent, he became an actuary in Trieste, but left Italy in 1938 because of the racial laws that came into effect at that time. He moved to Argentina, where relatives of his wife lived, and attempted to move from there to the United States, but his employment outside academia prevented him from obtaining the necessary visa. At the same time Robert Breusch, another German mathematician who had been working in Chile for three years but was leaving for the U.S., invited Frucht to fill his position at Federico Santa María Technical University in Valparaiso, Chile, where Frucht found an academic home beginning in 1939. At Santa María, Frucht became dean of the faculty of mathematics and physics from 1948 to 1968, and retired to become an emeritus professor in 1970.\n\nFrucht is known for Frucht's theorem, the result that every group can be realized as the group of symmetries of an undirected graph, and for the Frucht graph, one of the two smallest cubic graphs without any nontrivial symmetries. LCF notation, a method for describing cubic Hamiltonian graphs, was named for the initials of Joshua Lederberg, H. S. M. Coxeter, and Frucht, its key developers.\n\nFrucht was elected to the Chilean Academy of Sciences as a corresponding member in 1979.\nA special issue of the \"Journal of Graph Theory\" was published in Frucht's honor in 1982, and another special issue of the journal \"Scientia, Series A\" (the journal of the mathematics department of Federico Santa María Technical University) was published in honor of his 80th birthday in 1986.\n"}
{"id": "2546747", "url": "https://en.wikipedia.org/wiki?curid=2546747", "title": "Static timing analysis", "text": "Static timing analysis\n\nStatic timing analysis (STA) is a simulation method of computing the expected timing of a digital circuit without requiring a simulation of the full circuit.\n\nHigh-performance integrated circuits have traditionally been characterized by the clock frequency at which they operate. Measuring the ability of a circuit to operate at the specified speed requires an ability to measure, during the design process, its delay at numerous steps. Moreover, delay calculation must be incorporated into the inner loop of timing optimizers at various phases of design, such as logic synthesis, layout (placement and routing), and in in-place optimizations performed late in the design cycle. While such timing measurements can theoretically be performed using a rigorous circuit simulation, such an approach is liable to be too slow to be practical. Static timing analysis plays a vital role in facilitating the fast and reasonably accurate measurement of circuit timing. The speedup comes from the use of simplified timing models and by mostly ignoring logical interactions in circuits. This has become a mainstay of design over the last few decades.\n\nOne of the earliest descriptions of a static timing approach was based on the Program Evaluation and Review Technique (PERT), in 1966. More modern versions and algorithms appeared in the early 1980s.\n\nIn a synchronous digital system, data is supposed to move in lockstep, advancing one stage on each tick of the clock signal. This is enforced by synchronizing elements such as flip-flops or latches, which copy their input to their output when instructed to do so by the clock. Only two kinds of timing errors are possible in such a system:\n\nThe time when a signal arrives can vary due to many reasons. The input data may vary, the circuit may perform different operations, the temperature and voltage may change, and there are manufacturing differences in the exact construction of each part. The main goal of static timing analysis is to verify that despite these possible variations, all signals will arrive neither too early nor too late, and hence proper circuit operation can be assured.\n\nSince STA is capable of verifying every path, it can detect other problems like glitches, slow paths and clock skew.\n\n\nQuite often, designers will want to qualify their design across many conditions. Behavior of an electronic circuit is often dependent on various factors in its environment like temperature or local voltage variations. In such a case either STA needs to be performed for more than one such set of conditions, or STA must be prepared to work with a range of possible delays for each component, as opposed to a single value. \n\nWith proper techniques, the patterns of condition variations are characterized and their extremes are recorded. Each extreme condition can be termed as a corner. Let us say, each extreme in cell characteristics as ‘PVT corner’ and net characteristics as ‘extraction corner’. Then each combination pattern of PVT extraction corners is referred to as a ‘timing corner’ as it represents a point where timing will be extreme. If the design works at each extreme condition, then under the assumption of monotonic behavior, the design is also qualified for all intermediate points.\n\nThe use of corners in static timing analysis has several limitations. It may be overly optimistic, since it assumes perfect tracking: if one gate is fast, all gates are assumed fast, or if the voltage is low for one gate, it is also low for all others. Corners may also be overly pessimistic, for the worst case corner may seldom occur. In an IC, for example, it may not be rare to have one metal layer at the thin or thick end of its allowed range, but it would be very rare for all 10 layers to be at the same limit, since they are manufactured independently. Statistical STA, which replaces delays with distributions, and tracking with correlation, offers a more sophisticated approach to the same problem.\n\nIn static timing analysis, the word \"static\" alludes to the fact that this timing analysis is carried out in an input-independent manner, and purports to find the worst-case delay of the circuit over all possible input combinations. The computational efficiency (linear in the number of edges in the graph) of such an approach has resulted in its widespread use, even though it has some limitations. A method that is commonly referred to as PERT is popularly used in STA. However, PERT is a misnomer, and the so-called PERT method discussed in most of the literature on timing analysis refers to the critical path method (CPM) that is widely used in project management. While the CPM-based methods are the dominant ones in use today, other methods for traversing circuit graphs, such as depth-first search, have been used by various timing analyzers.\n\nMany of the common problems in chip designing are related to interface timing between different components of the design. These can arise because of many factors including incomplete simulation models, lack of test cases to properly verify interface timing, requirements for synchronization, incorrect interface specifications, and lack of designer understanding of a component supplied as a 'black box'. There are specialized CAD tools designed explicitly to analyze interface timing, just as there are specific CAD tools to verify that an implementation of an interface conforms to the functional specification (using techniques such as model checking).\n\nStatistical static timing analysis (SSTA) is a procedure that is becoming increasingly necessary to handle the complexities of process and environmental variations in integrated circuits.\n\n\n"}
{"id": "34283581", "url": "https://en.wikipedia.org/wiki?curid=34283581", "title": "Τ-additivity", "text": "Τ-additivity\n\nIn mathematics, in the field of measure theory, τ-additivity is a certain property of measures on topological spaces.\n\nA measure \"µ\" on a space \"X\", defined on a sigma-algebra Σ is said to be τ-additive, if for any upward-directed family formula_1 of nonempty open sets, such that its union is in Σ, the measure of the union is the supremum of measures of elements of formula_2, i.e.:\n\n"}
