{"id": "208161", "url": "https://en.wikipedia.org/wiki?curid=208161", "title": "4", "text": "4\n\n4 (four) is a number, numeral, and glyph. It is the natural number following 3 and preceding 5.\n\nFour is the smallest composite number, its proper divisors being and .\n\n4 is the smallest squared prime (\"p\") and the only even number in this form. 4 is also the only square one more than a prime number.\n\nA number is a multiple of 4 if its last two digits are a multiple of 4. For example, 1092 is a multiple of 4 because .\n\nIn addition, 2 + 2 = 2 × 2 = 2 = 4. Continuing the pattern in Knuth's up-arrow notation, , and so on, for any number of up arrows. (That is, for every positive integer \"n\", where is the hyperoperation.)\n\nA four-sided plane figure is a quadrilateral (quadrangle) which include kites, rhombi, rectangles and squares, sometimes also called a \"tetragon\". A circle divided by 4 makes right angles and four quadrants. Because of it, four (4) is the base number of the plane (mathematics). Four cardinal directions, four seasons, the duodecimal system, and the vigesimal system are based on four.\n\nA solid figure with four faces as well as four vertices is a tetrahedron, and 4 is the smallest possible number of faces (as well as vertices) of a polyhedron. The regular tetrahedron is the simplest Platonic solid. A tetrahedron, which can also be called a 3-simplex, has four triangular faces and four vertices. It is the only self-dual regular polyhedron.\n\nFour-dimensional space is the highest-dimensional space featuring more than three convex regular figures:\n\nFour-dimensional differential manifolds have some unique properties. There is only one differential structure on ℝ except when , in which case there are uncountably many.\n\nThe smallest non-cyclic group has four elements; it is the Klein four-group. Four is also the order of the smallest non-trivial groups that are not simple.\n\nFour is the only integer \"n\" for which the (non trivial) alternating group \"A\" is not simple.\n\nFour is the maximum number of dimensions of a real associative division algebra (the quaternions), by a theorem of Ferdinand Georg Frobenius.\n\nThe four-color theorem states that a planar graph (or, equivalently, a flat map of two-dimensional regions such as countries) can be colored using four colors, so that adjacent vertices (or regions) are always different colors. Three colors are not, in general, sufficient to guarantee this. The largest planar complete graph has four vertices.\n\nLagrange's four-square theorem states that every positive integer can be written as the sum of at most four square numbers. Three are not always sufficient; for instance cannot be written as the sum of three squares.\n\nEach natural number divisible by 4 is a difference of squares of two natural numbers, i.e. .\n\nFour is the highest degree general polynomial equation for which there is a solution in radicals.\n\nThe four fours game, there are known solutions for all integers from 0 to 880 (but not 881).\n\nRepresenting 1, 2 and 3 in as many lines as the number represented worked well. The Brahmin Indians simplified 4 by joining its four lines into a cross that looks like the modern plus sign. The Shunga would add a horizontal line on top of the numeral, and the Kshatrapa and Pallava evolved the numeral to a point where the speed of writing was a secondary concern. The Arabs' 4 still had the early concept of the cross, but for the sake of efficiency, was made in one stroke by connecting the \"western\" end to the \"northern\" end; the \"eastern\" end was finished off with a curve. The Europeans dropped the finishing curve and gradually made the numeral less cursive, ending up with a glyph very close to the original Brahmin cross.\n\nWhile the shape of the 4 character has an ascender in most modern typefaces, in typefaces with text figures the character usually has a descender, as, for example, in .\nOn the seven-segment displays of pocket calculators and digital watches, as well as certain optical character recognition fonts, 4 is seen with an open top.\n\nTelevision stations that operate on channel 4 have occasionally made use of another variation of the \"open 4\", with the open portion being on the side, rather than the top. This version resembles the Canadian Aboriginal syllabics letter ᔦ or the Coptic letter Ϥ. The magnetic ink character recognition \"CMC-7\" font also uses this variety of \"4\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "417534", "url": "https://en.wikipedia.org/wiki?curid=417534", "title": "Algorism", "text": "Algorism\n\nAlgorism is the technique of performing basic arithmetic by writing numbers in place value form and applying a set of memorized rules and facts to the digits. One who practices algorism is known as an algorist. This positional notation system largely superseded earlier calculation systems that used a different set of symbols for each numerical magnitude, such as Roman numerals, and in some cases required a device such as an abacus.\n\nThe word \"algorism\" comes from the name Al-Khwārizmī (c. 780–850), a Persian mathematician, astronomer, geographer and scholar in the House of Wisdom in Baghdad, whose name means \"the native of Khwarezm\", a city that was part of the Greater Iran during his era and now is in modern-day Uzbekistan He wrote a treatise in Arabic language in the 9th century, which was translated into Latin in the 12th century under the title \"Algoritmi de numero Indorum\". This title means \"Algoritmi on the numbers of the Indians\", where \"Algoritmi\" was the translator's Latinization of Al-Khwarizmi's name. Al-Khwarizmi was the most widely read mathematician in Europe in the late Middle Ages, primarily through his other book, the Algebra. In late medieval Latin, \"algorismus\", the corruption of his name, simply meant the \"decimal number system\" that is still the meaning of modern English algorism. During the 17th century, the French form for the word –but not its meaning– was changed to \"algorithm\", following the model of the word \"logarithm\", this form alluding to the ancient Greek . English adopted the French very soon afterwards, but it wasn't until the late 19th century that \"algorithm\" took on the meaning that it has in modern English. In English, it was first used about 1230 and then by Chaucer in 1391. Another early use of the word is from 1240, in a manual titled \"Carmen de Algorismo\" composed by Alexandre de Villedieu. It begins thus:\n\nwhich translates as:\nThe word \"algorithm\" also derives from \"algorism\", a generalization of the meaning to any set of rules specifying a computational procedure. Occasionally \"algorism\" is also used in this generalized meaning, especially in older texts.\n\nStarting with the integer arithmetic developed in India using base 10 notation, Al-Khwārizmī along with other mathematicians in medieval Islam, both Iranian and Arabic, documented new arithmetic methods and made many other contributions to decimal arithmetic (see the articles linked below). These included the concept of the decimal fractions as an extension of the notation, which in turn led to the notion of the decimal point. This system was popularized in Europe by Leonardo of Pisa, now known as Fibonacci.\n\n"}
{"id": "2838129", "url": "https://en.wikipedia.org/wiki?curid=2838129", "title": "Cauchy's theorem (group theory)", "text": "Cauchy's theorem (group theory)\n\nCauchy's theorem is a theorem in the mathematics of group theory, named after Augustin Louis Cauchy. It states that if \"G\" is a finite group and \"p\" is a prime number dividing the order of \"G\" (the number of elements in \"G\"), then \"G\" contains an element of order \"p\". That is, there is \"x\" in \"G\" so that \"p\" is the lowest non-zero number with \"x\" = \"e\", where \"e\" is the identity element.\n\nThe theorem is related to Lagrange's theorem, which states that the order of any subgroup of a finite group \"G\" divides the order of \"G\". Cauchy's theorem implies that for any prime divisor \"p\" of the order of \"G\", there is a subgroup of \"G\" whose order is \"p\"—the cyclic group generated by the element in Cauchy's theorem.\n\nCauchy's theorem is generalised by Sylow's first theorem, which implies that if \"p\" is the maximal power of \"p\" dividing the order of \"G\", then \"G\" has a subgroup of order \"p\" (and using the fact that a \"p\"-group is solvable, one can show that \"G\" has subgroups of order \"p\" for any \"r\" less than or equal to \"n\").\n\nMany texts appear to prove the theorem with the use of strong induction and the class equation, though considerably less machinery is required to prove the theorem in the abelian case. One can also invoke group actions for the proof.\n\nTheorem: Let \"G\" be a finite group and \"p\" be a prime. If \"p\" divides the order of \"G\", then \"G\" has an element of order \"p\".\n\nWe first prove the special case that where \"G\" is abelian, and then the general case; both proofs are by induction on \"n\" = |\"G\"|, and have as starting case \"n\" = \"p\" which is trivial because any non-identity element now has order \"p\". Suppose first that \"G\" is abelian. Take any non-identity element \"a\", and let \"H\" be the cyclic group it generates. If \"p\" divides |\"H\"|, then \"a\" is an element of order \"p\". If \"p\" does not divide |\"H\"|, then it divides the order [\"G\":\"H\"] of the quotient group \"G\"/\"H\", which therefore contains an element of order \"p\" by the inductive hypothesis. That element is a class \"xH\" for some \"x\" in \"G\", and if \"m\" is the order of \"x\" in \"G\", then \"x\" = \"e\" in \"G\" gives (\"xH\") = \"eH\" in \"G\"/\"H\", so \"p\" divides \"m\"; as before \"x\" is now an element of order \"p\" in \"G\", completing the proof for the abelian case.\n\nIn the general case, let \"Z\" be the center of \"G\", which is an abelian subgroup. If \"p\" divides |\"Z\"|, then \"Z\" contains an element of order \"p\" by the case of abelian groups, and this element works for \"G\" as well. So we may assume that \"p\" does not divide the order of \"Z\"; since it does divide |\"G\"|, the class equation shows that there is at least one conjugacy class of a non-central element \"a\" whose size is not divisible by \"p\". But that size is [\"G\" : \"C\"(\"a\")], so \"p\" divides the order of the centralizer \"C\"(\"a\") of \"a\" in \"G\", which is a proper subgroup because \"a\" is not central. This subgroup contains an element of order \"p\" by the inductive hypothesis, and we are done.\n\nThis proof uses the fact that for any action of a (cyclic) group of prime order \"p\", the only possible orbit sizes are 1 and \"p\", which is immediate from the orbit stabilizer theorem.\n\nThe set that our cyclic group shall act on is the set formula_1 of \"p\"-tuples of elements of \"G\" whose product (in order) gives the identity. Such a \"p\"-tuple is uniquely determined by all its components except the last one, as the last element must be the inverse of the product of those preceding elements. One also sees that those elements can be chosen freely, so \"X\" has |\"G\"| elements, which is divisible by \"p\".\n\nNow from the fact that in a group if \"ab\" = \"e\" then also \"ba\" = \"e\", it follows that any cyclic permutation of the components of an element of \"X\" again gives an element of \"X\". Therefore one can define an action of the cyclic group \"C\" of order \"p\" on \"X\" by cyclic permutations of components, in other words in which a chosen generator of \"C\" sends formula_2.\n\nAs remarked, orbits in \"X\" under this action either have size 1 or size \"p\". The former happens precisely for those tuples (\"x\",\"x\"...,\"x\") for which \"x\" = \"e\". Counting the elements of \"X\" by orbits, and reducing modulo \"p\", one sees that the number of elements satisfying \"x\" = \"e\" is divisible by \"p\". But \"x\" = \"e\" is one such element, so there must be at least other solutions for \"x\", and these solutions are elements of order \"p\". This completes the proof.\n\nA practically immediate consequence of Cauchy's Theorem is a useful characterization of finite \"p\"-groups, where \"p\" is a prime. In particular, a finite group \"G\" is a \"p\"-group (i.e. all of its elements have order \"p\" for some natural number \"k\") if and only if \"G\" has order \"p\" for some natural number \"n\". One may use the abelian case of Cauchy's Theorem in an inductive proof of the first of Sylow's Theorems, similar to the first proof above, although there also exist proofs that avoid doing this special case separately.\n\n\n"}
{"id": "14336399", "url": "https://en.wikipedia.org/wiki?curid=14336399", "title": "Chromatic spectral sequence", "text": "Chromatic spectral sequence\n\nIn mathematics, the chromatic spectral sequence is a spectral sequence, introduced by , used for calculating the initial term of the Adams spectral sequence for Brown–Peterson cohomology, which is in turn used for calculating the stable homotopy groups of spheres.\n\n"}
{"id": "4777875", "url": "https://en.wikipedia.org/wiki?curid=4777875", "title": "Clifford theory", "text": "Clifford theory\n\nIn mathematics, Clifford theory, introduced by , describes the relation between representations of a group and those of a normal subgroup.\n\nAlfred H. Clifford proved the following result on the restriction of finite-dimensional irreducible representations from a group \"G\" to a normal subgroup \"N\" of finite index:\n\nTheorem. Let π: \"G\" → GL(\"n\",\"K\") be an irreducible representation with \"K\" a field. Then the restriction of π to \"N\" breaks up into a direct sum of irreducible representations of \"N\" of equal dimensions. These irreducible representations of \"N\" lie in one orbit for the action of \"G\" by conjugation on the equivalence classes of irreducible representations of \"N\". In particular the number of pairwise nonisomorphic summands is no greater than the index of \"N\" in \"G\".\n\nClifford's theorem yields information about the restriction of a complex irreducible character of a finite group \"G\" to a normal subgroup \"N.\" If μ is a complex character of \"N\", then for a fixed element \"g\" of \"G\", another character, μ, of \"N\" may be constructed by setting\n\nfor all \"n\" in \"N\". The character μ is irreducible if and only if μ is. Clifford's theorem states that if χ is a complex irreducible character of \"G,\" and μ is an irreducible character of \"N\" with\n\nwhere \"e\" and \"t\" are positive integers, and each \"g\" is an element of \"G.\" The integers \"e\" and \"t\" both divide the index [\"G\":\"N\"]. The integer \"t\" is the index of a subgroup of \"G\", containing \"N\", known as the inertial subgroup of μ. This is\n\nand is often denoted by\n\nThe elements \"g\" may be taken to be representatives of all the right cosets of the subgroup \"I\"(μ) in \"G\".\n\nIn fact, the integer \"e\" divides the index\n\nthough the proof of this fact requires some use of Schur's theory of projective representations.\n\nThe proof of Clifford's theorem is best explained in terms of modules (and the module-theoretic version works for irreducible modular representations). Let \"F\" be a field, \"V\" be an irreducible \"F\"[\"G\"]-module, \"V\" be its restriction to \"N\" and \"U\" be an irreducible \"F\"[N]-submodule of \"V\". For each \"g\" in \"G\", \"U\".\"g\" is an irreducible \"F\"[\"N\"]-submodule of \"V\", and formula_7 is an \"F\"[\"G\"]-submodule of \"V\", so must be all of \"V\" by irreducibility. Now \"V\" is expressed as a sum of irreducible submodules, and this expression may be refined to a direct sum. The proof of the character-theoretic statement of the theorem may now be completed in the case \"F\" = C. Let χ be the character of \"G\" afforded by \"V\" and μ be the character of \"N\" afforded by \"U\". For each \"g\" in \"G\", the C[\"N\"]-submodule \"U\".\"g\" affords the character μ and formula_8. The respective equalities follow because χ is a class-function of \"G\" and \"N\" is a normal subgroup. The integer \"e\" appearing in the statement of the theorem is this common multiplicity.\n\nA corollary of Clifford's theorem, which is often exploited, is that the irreducible character χ appearing in the theorem is induced from an irreducible character of the inertial subgroup \"I\"(μ). If, for example, the irreducible character χ is primitive (that is, χ is not induced from any proper subgroup of \"G\"), then \"G\" = \"I\"(μ) and χ = \"e\"μ. A case where this property of primitive characters is used particularly frequently is when \"N\" is Abelian and χ is faithful (that is, its kernel contains just the identity element). In that case, μ is linear, \"N\" is represented by scalar matrices in any representation affording character χ and \"N\" is thus contained in the center of \"G\" (that is, the subgroup of \"G\" consisting of those elements which themselves commute with every element of \"G\"). For example, if \"G\" is the symmetric group \"S\", then \"G\" has a faithful complex irreducible character χ of degree \"3.\" There is an Abelian normal subgroup \"N\" of order \"4\" (a Klein \"4\"-subgroup) which is not contained in the center of \"G\". Hence χ is induced from a character of a proper subgroup of \"G\" containing \"N.\" The only possibility is that χ is induced from a linear character of a Sylow \"2\"-subgroup of \"G\".\n\nClifford's theorem has led to a branch of representation theory in its own right, now known as Clifford theory. This is particularly relevant to the representation theory of finite solvable groups, where normal subgroups usually abound. For more general finite groups, Clifford theory often allows representation-theoretic questions to be reduced to questions about groups that are close (in a sense which can be made precise) to being simple.\n\n"}
{"id": "16706727", "url": "https://en.wikipedia.org/wiki?curid=16706727", "title": "Cocker's Arithmetick", "text": "Cocker's Arithmetick\n\nCocker's Arithmetick, also known by its full title \"Cocker's Arithmetick: Being a Plain and Familiar Method Suitable to the Meanest Capacity for the Full Understanding of That Incomparable Art, As It Is Now Taught by the Ablest School-Masters in City and Country\", is a grammar school mathematics textbook written by Edward Cocker (1631–1676) and published posthumously by John Hawkins in 1677. \"Arithmetick\" along with companion volume, \"Decimal Arithmetick\" published in 1684, were used to teach mathematics in schools in the United Kingdom for more than 150 years.\n\nSome controversy exists over the authorship of the book. Augustus De Morgan claimed the work was written by Hawkins, who merely used Cocker's name to lend the authority of his reputation to the book. Ruth Wallis, in 1997, wrote an article in \"Annals of Science\", claiming De Morgan's analysis was flawed and Cocker was the real author.\n\nThe popularity of \"Arithmetick\" is unquestioned by its more than 130 editions, and that its place was woven in the fabric of the popular culture of the time is evidenced by its references in the phrase, \"according to Cocker\", meaning \"absolutely correct\" or \"according to the rules\". Such noted figures of history as Benjamin Franklin and Thomas Simpson are documented as having used the book. Over 100 years after its publication, Samuel Johnson carried a copy of \"Arithmetick\" on his tour of Scotland, and mentions it in his letters:\n\nThough popular, like most texts of its time, \"Arithmetick\" style is formal, stiff and difficult to follow as illustrated in its explanation of the \"rule of three\".\nAs well as the rule of three, \"Arithmetick\" contains instructions on alligation and the rule of false position. Following the common practice of textbooks at the time, each rule is illustrated with numerous examples of commercial transactions involving the exchange of wheat, rye and other seeds; calculation of costs for the erection of houses and other structures; and the rotation of gears on a shaft. The text contains the earliest known use of the term \"lowest terms\".\n\n"}
{"id": "296666", "url": "https://en.wikipedia.org/wiki?curid=296666", "title": "Construction of the real numbers", "text": "Construction of the real numbers\n\nIn mathematics, there are several ways of defining the real number system as an ordered field. The \"synthetic\" approach gives a list of axioms for the real numbers as a \"complete ordered field\". Under the usual axioms of set theory, one can show that these axioms are categorical, in the sense that there is a model for the axioms, and any two such models are isomorphic. Any one of these models must be explicitly constructed, and most of these models are built using the basic properties of the rational number system as an ordered field.\n\nThe synthetic approach axiomatically defines the real number system as a complete ordered field. Precisely, this means the following. A \"model for the real number system\" consists of a set R, two distinct elements 0 and 1 of R, two binary operations + and × on R (called \"addition\" and \"multiplication\", respectively), and a binary relation ≤ on R, satisfying the following properties.\n\n\nThe rational numbers Q satisfy the first three axioms (i.e. Q is totally ordered field) but Q does not satisfy axiom 4. So axiom 4, which requires the order to be Dedekind-complete, is crucial. Axiom 4 implies the Archimedean property. Several models for axioms 1-4 are given below. Any two models for axioms 1-4 are isomorphic, and so up to isomorphism, there is only one complete ordered Archimedean field.\n\nWhen we say that any two models of the above axioms are isomorphic, we mean that for any two models (\"R\", 0, 1, +, ×, ≤) and (\"S\", 0, 1, +, ×, ≤), there is a bijection \"f\" : \"R\" → \"S\" preserving both the field operations and the order. Explicitly,\n\nAn alternative synthetic axiomatization of the real numbers and their arithmetic was given by Alfred Tarski, consisting of only the 8 axioms shown below and a mere four primitive notions: a set called \"the real numbers\", denoted R, a binary relation over R called \"order\", denoted by infix <, a binary operation over R called \"addition\", denoted by infix +, and the constant 1.\n\n\"Axioms of order\" (primitives: R, <):\n\nAxiom 1. If \"x\" < \"y\", then not \"y\" < \"x\". That is, \"<\" is an asymmetric relation.\n\nAxiom 2. If \"x\" < \"z\", there exists a \"y\" such that \"x\" < \"y\" and \"y\" < \"z\". In other words, \"<\" is dense in R.\n\nAxiom 3. \"<\" is Dedekind-complete. More formally, for all \"X\", \"Y\" ⊆ R, if for all \"x\" ∈ \"X\" and \"y\" ∈ \"Y\", \"x\" < \"y\", then there exists a \"z\" such that for all \"x\" ∈ \"X\" and \"y\" ∈ \"Y\", if \"z\" ≠ \"x\" and \"z\" ≠ \"y\", then \"x\" < \"z\" and \"z\" < \"y\".\n\nTo clarify the above statement somewhat, let \"X\" ⊆ R and \"Y\" ⊆ R. We now define two common English verbs in a particular way that suits our purpose:\n\nAxiom 3 can then be stated as:\n\n\"Axioms of addition\" (primitives: R, <, +):\n\nAxiom 4. \"x\" + (\"y\" + \"z\") = (\"x\" + \"z\") + \"y\".\n\nAxiom 5. For all \"x\", \"y\", there exists a \"z\" such that \"x\" + \"z\" = \"y\".\n\nAxiom 6. If \"x\" + \"y\" < \"z\" + \"w\", then \"x\" < \"z\" or \"y\" < \"w\".\n\n\"Axioms for one\" (primitives: R, <, +, 1):\n\nAxiom 7. 1 ∈ R.\n\nAxiom 8. 1 < 1 + 1.\n\nThese axioms imply that R is a linearly ordered abelian group under addition with distinguished element 1. R is also Dedekind-complete and divisible.\n\nWe shall not prove that any models of the axioms are isomorphic. Such a proof can be found in any number of modern analysis or set theory textbooks. We will sketch the basic definitions and properties of a number of constructions, however, because each of these is important for both mathematical and historical reasons. The first three, due to Georg Cantor/Charles Méray, Richard Dedekind and Karl Weierstrass/Otto Stolz all occurred within a few years of each other. Each has advantages and disadvantages. A major motivation in all three cases was the instruction of mathematics students.\n\nA standard procedure to force all Cauchy sequences in a metric space to converge is adding new points to the metric space in a process called completion.\n\nR is defined as the completion of Q with respect to the metric |\"x\"-\"y\"|, as will be detailed below (for completions of Q with respect to other metrics, see \"p\"-adic numbers.)\n\nLet \"R\" be the set of Cauchy sequences of rational numbers. That is, sequences\nof rational numbers such that for every rational , there exists an integer \"N\" such that for all natural numbers , . Here the vertical bars denote the absolute value.\n\nCauchy sequences (\"x\") and (\"y\") can be added and multiplied as follows:\n\nTwo Cauchy sequences are called \"equivalent\" if and only if the difference between them tends to zero.\nThis defines an equivalence relation that is compatible with the operations defined above, and the set R of all equivalence classes can be shown to satisfy all axioms of the real numbers. We can embed Q into R by identifying the rational number \"r\" with the equivalence class of the sequence .\n\nComparison between real numbers is obtained by defining the following comparison between Cauchy sequences: if and only if \n\"x\" is equivalent to \"y\" or there exists an integer \"N\" such that for all \n\nBy construction, every real number \"x\" is represented by a Cauchy sequence of rational numbers. This representation is far from unique; every rational sequence that converges to \"x\" is a representation of \"x\". This reflects the observation that one can often use different sequences to approximate the same real number.\n\nThe only real number axiom that does not follow easily from the definitions is the completeness of ≤, i.e. the least upper bound property. It can be proved as follows: Let \"S\" be a non-empty subset of R and \"U\" be an upper bound for \"S\". Substituting a larger value if necessary, we may assume \"U\" is rational. Since \"S\" is non-empty, we can choose a rational number \"L\" such that for some \"s\" in \"S\". Now define sequences of rationals (\"u\") and (\"l\") as follows:\n\nFor each \"n\" consider the number:\n\nIf \"m\" is an upper bound for \"S\" set:\n\nOtherwise set:\n\nThis defines two Cauchy sequences of rationals, and so we have real numbers and . It is easy to prove, by induction on \"n\" that:\n\nand:\n\nThus \"u\" is an upper bound for \"S\". To see that it is a least upper bound, notice that the limit of (\"u\" − \"l\") is 0, and so \"l\" = \"u\". Now suppose is a smaller upper bound for \"S\". Since (\"l\") is monotonic increasing it is easy to see that for some \"n\". But \"l\" is not an upper bound for S and so neither is \"b\". Hence \"u\" is a least upper bound for \"S\" and ≤ is complete.\n\nThe usual decimal notation can be translated to Cauchy sequences in a natural way. For example, the notation π = 3.1415... means that π is the equivalence class of the Cauchy sequence (3, 3.1, 3.14, 3.141, 3.1415, ...). The equation 0.999... = 1 states that the sequences (0, 0.9, 0.99, 0.999...) and (1, 1, 1, 1...) are equivalent, i.e., their difference converges to 0.\n\nAn advantage of constructing R as the completion of Q is that this construction is not specific to one example; it is used for other metric spaces as well.\n\nA Dedekind cut in an ordered field is a partition of it, (\"A\", \"B\"), such that \"A\" is nonempty and closed downwards, \"B\" is nonempty and closed upwards, and \"A\" contains no greatest element. Real numbers can be constructed as Dedekind cuts of rational numbers.\n\nFor convenience we may take the lower set formula_1 as the representative of any given Dedekind cut formula_2, since formula_3 completely determines formula_4. By doing this we may think intuitively of a real number as being represented by the set of all smaller rational numbers. In more detail, a real number formula_5 is any subset of the set formula_6 of rational numbers that fulfills the following conditions:\n\n\nAs an example of a Dedekind cut representing an irrational number, we may take the positive square root of 2. This can be defined by the set formula_49. It can be seen from the definitions above that formula_3 is a real number, and that formula_51. However, neither claim is immediate. Showing that formula_1 is real requires showing that formula_3 has no greatest element, i.e. that for any positive rational formula_54 with formula_55, there is a rational formula_56 with formula_57 and formula_58 The choice formula_59 works. Then formula_60 but to show equality requires showing that if formula_61 is any rational number less than 2, then there is positive formula_54 in formula_3 with formula_64.\n\nAn advantage of this construction is that each real number corresponds to a unique cut.\n\nAs in the hyperreal numbers, one constructs the hyperrationals Q from the rational numbers by means of an ultrafilter. Here a hyperrational is by definition a ratio of two hyperintegers. Consider the ring \"B\" of all limited (i.e. finite) elements in Q. Then \"B\" has a unique maximal ideal \"I\", the infinitesimal numbers. The quotient ring \"B/I\" gives the field R of real numbers . Note that \"B\" is not an internal set in Q.\nNote that this construction uses a non-principal ultrafilter over the set of natural numbers, the existence of which is guaranteed by the axiom of choice.\n\nIt turns out that the maximal ideal respects the order on Q. Hence the resulting field is an ordered field. Completeness can be proved in a similar way to the construction from the Cauchy sequences.\n\nEvery ordered field can be embedded in the surreal numbers. The real numbers form a maximal subfield that is Archimedean (meaning that no real number is infinitely large). This embedding is not unique, though it can be chosen in a canonical way.\n\nA relatively less known construction allows to define real numbers using only the additive group of integers formula_65 with different versions. The construction has been formally verified by the IsarMathLib project. Shenitzer and Arthan refer to this construction as the \"Eudoxus reals\".\n\nLet an almost homomorphism be a map formula_66 such that the set formula_67 is finite. (Note that formula_68 is an almost homomorphism for every formula_69.) Almost homomorphisms form an abelian group under pointwise addition. We say that two almost homomorphisms formula_70 are almost equal if the set formula_71 is finite. This defines an equivalence relation on the set of almost homomorphisms. Real numbers are defined as the equivalence classes of this relation. Alternatively, the almost homomorphisms taking only finitely many values form a subgroup, and the underlying additive group of the real number is the quotient group. To add real numbers defined this way we add the almost homomorphisms that represent them. Multiplication of real numbers corresponds to functional composition of almost homomorphisms. If formula_72 denotes the real number represented by an almost homomorphism formula_73 we say that formula_74 if formula_73 is bounded or formula_73 takes an infinite number of positive values on formula_77. This defines the linear order relation on the set of real numbers constructed this way.\n\nA number of other constructions have been given, by:\n\nAs a reviewer of one noted: \"The details are all included, but as usual they are tedious and not too instructive.\"\n\n"}
{"id": "38838", "url": "https://en.wikipedia.org/wiki?curid=38838", "title": "Cyclic redundancy check", "text": "Cyclic redundancy check\n\nA cyclic redundancy check (CRC) is an error-detecting code commonly used in digital networks and storage devices to detect accidental changes to raw data. Blocks of data entering these systems get a short \"check value\" attached, based on the remainder of a polynomial division of their contents. On retrieval, the calculation is repeated and, in the event the check values do not match, corrective action can be taken against data corruption. CRCs can be used for error correction (see bitfilters).\n\nCRCs are so called because the \"check\" (data verification) value is a \"redundancy\" (it expands the message without adding information) and the algorithm is based on \"cyclic\" codes. CRCs are popular because they are simple to implement in binary hardware, easy to analyze mathematically, and particularly good at detecting common errors caused by noise in transmission channels. Because the check value has a fixed length, the function that generates it is occasionally used as a hash function.\n\nThe CRC was invented by W. Wesley Peterson in 1961; the 32-bit CRC function, used in Ethernet and many other standards, is the work of several researchers and was published in 1975.\n\nCRCs are based on the theory of cyclic error-correcting codes. The use of systematic cyclic codes, which encode messages by adding a fixed-length check value, for the purpose of error detection in communication networks, was first proposed by W. Wesley Peterson in 1961.\nCyclic codes are not only simple to implement but have the benefit of being particularly well suited for the detection of burst errors: contiguous sequences of erroneous data symbols in messages. This is important because burst errors are common transmission errors in many communication channels, including magnetic and optical storage devices. Typically an \"n\"-bit CRC applied to a data block of arbitrary length will detect any single error burst not longer than \"n\" bits and the fraction of all longer error bursts that it will detect is .\n\nSpecification of a CRC code requires definition of a so-called generator polynomial. This polynomial becomes the divisor in a polynomial long division, which takes the message as the dividend and in which the quotient is discarded and the remainder becomes the result. The important caveat is that the polynomial coefficients are calculated according to the arithmetic of a finite field, so the addition operation can always be performed bitwise-parallel (there is no carry between digits).\n\nIn practice, all commonly used CRCs employ the Galois field of two elements, GF(2). The two elements are usually called 0 and 1, comfortably matching computer architecture.\n\nA CRC is called an \"n\"-bit CRC when its check value is \"n\" bits long. For a given \"n\", multiple CRCs are possible, each with a different polynomial. Such a polynomial has highest degree \"n\", which means it has terms. In other words, the polynomial has a length of ; its encoding requires bits. Note that most polynomial specifications either drop the MSB or LSB, since they are always 1. The CRC and associated polynomial typically have a name of the form CRC-\"n\"-XXX as in the table below.\n\nThe simplest error-detection system, the parity bit, is in fact a 1-bit CRC: it uses the generator polynomial  (two terms), and has the name CRC-1.\n\nA CRC-enabled device calculates a short, fixed-length binary sequence, known as the \"check value\" or \"CRC\", for each block of data to be sent or stored and appends it to the data, forming a \"codeword\".\n\nWhen a codeword is received or read, the device either compares its check value with one freshly calculated from the data block, or equivalently, performs a CRC on the whole codeword and compares the resulting check value with an expected \"residue\" constant.\n\nIf the CRC values do not match, then the block contains a data error.\n\nThe device may take corrective action, such as rereading the block or requesting that it be sent again. Otherwise, the data is assumed to be error-free (though, with some small probability, it may contain undetected errors; this is inherent in the nature of error-checking).\n\nCRCs are specifically designed to protect against common types of errors on communication channels, where they can provide quick and reasonable assurance of the integrity of messages delivered. However, they are not suitable for protecting against intentional alteration of data.\n\nFirstly, as there is no authentication, an attacker can edit a message and recompute the CRC without the substitution being detected. When stored alongside the data, CRCs and cryptographic hash functions by themselves do not protect against \"intentional\" modification of data. Any application that requires protection against such attacks must use cryptographic authentication mechanisms, such as message authentication codes or digital signatures (which are commonly based on cryptographic hash functions).\n\nSecondly, unlike cryptographic hash functions, CRC is an easily reversible function, which makes it unsuitable for use in digital signatures.\n\nThirdly, CRC is a linear function with a property that\n\nas a result, even if the CRC is encrypted with a stream cipher that uses XOR as its combining operation (or mode of block cipher which effectively turns it into a stream cipher, such as OFB or CFB), both the message and the associated CRC can be manipulated without knowledge of the encryption key; this was one of the well-known design flaws of the Wired Equivalent Privacy (WEP) protocol.\n\nThis is the algorithm for the CRC-32 variant of CRC. The CRCTable is a memoization of a calculation that would have to be repeated for each byte of the message.\n\nTo compute an \"n\"-bit binary CRC, line the bits representing the input in a row, and position the ()-bit pattern representing the CRC's divisor (called a \"polynomial\") underneath the left-hand end of the row.\n\nIn this example, we shall encode 14 bits of message with a 3-bit CRC, with a polynomial . The polynomial is written in binary as the coefficients; a 3rd-order polynomial has 4 coefficients (). In this case, the coefficients are 1, 0, 1 and 1. The result of the calculation is 3 bits long.\n\nStart with the message to be encoded:\n\nThis is first padded with zeros corresponding to the bit length \"n\" of the CRC. Here is the first calculation for computing a 3-bit CRC:\n\nThe algorithm acts on the bits directly above the divisor in each step. The result for that iteration is the bitwise XOR of the polynomial divisor with the bits above it. The bits not above the divisor are simply copied directly below for that step. The divisor is then shifted one bit to the right, and the process is repeated until the divisor reaches the right-hand end of the input row. Here is the entire calculation:\n\nSince the leftmost divisor bit zeroed every input bit it touched, when this process ends the only bits in the input row that can be nonzero are the n bits at the right-hand end of the row. These \"n\" bits are the remainder of the division step, and will also be the value of the CRC function (unless the chosen CRC specification calls for some postprocessing).\n\nThe validity of a received message can easily be verified by performing the above calculation again, this time with the check value added instead of zeroes. The remainder should equal zero if there are no detectable errors.\n\nThe following Python code outlines a function which will return the initial CRC remainder for a chosen input and polynomial, with either 1 or 0 as the initial padding. Note that this code works with string inputs rather than raw numbers:\n\ndef crc_remainder(input_bitstring, polynomial_bitstring, initial_filler):\n\ndef crc_check(input_bitstring, polynomial_bitstring, check_value):\n»> crc_check('11010011101100','1011','100')\nTrue\n»> crc_remainder('11010011101100','1011','0')\n'100'\n\nMathematical analysis of this division-like process reveals how to select a divisor that guarantees good error-detection properties. In this analysis, the digits of the bit strings are taken as the coefficients of a polynomial in some variable \"x\"—coefficients that are elements of the finite field GF(2), instead of more familiar numbers. The set of binary polynomials is a mathematical ring.\n\nThe selection of the generator polynomial is the most important part of implementing the CRC algorithm. The polynomial must be chosen to maximize the error-detecting capabilities while minimizing overall collision probabilities.\n\nThe most important attribute of the polynomial is its length (largest degree(exponent) +1 of any one term in the polynomial), because of its direct influence on the length of the computed check value.\n\nThe most commonly used polynomial lengths are:\n\nA CRC is called an \"n\"-bit CRC when its check value is \"n\"-bits. For a given \"n\", multiple CRCs are possible, each with a different polynomial. Such a polynomial has highest degree \"n\", and hence terms (the polynomial has a length of ). The remainder has length \"n\". The CRC has a name of the form CRC-\"n\"-XXX.\n\nThe design of the CRC polynomial depends on the maximum total length of the block to be protected (data + CRC bits), the desired error protection features, and the type of resources for implementing the CRC, as well as the desired performance. A common misconception is that the \"best\" CRC polynomials are derived from either irreducible polynomials or irreducible polynomials times the factor , which adds to the code the ability to detect all errors affecting an odd number of bits. In reality, all the factors described above should enter into the selection of the polynomial and may lead to a reducible polynomial. However, choosing a reducible polynomial will result in a certain proportion of missed errors, due to the quotient ring having zero divisors.\n\nThe advantage of choosing a primitive polynomial as the generator for a CRC code is that the resulting code has maximal total block length in the sense that all 1-bit errors within that block length have different remainders (also called syndromes) and therefore, since the remainder is a linear function of the block, the code can detect all 2-bit errors within that block length. If formula_2 is the degree of the primitive generator polynomial, then the maximal total block length is formula_3, and the associated code is able to detect any single-bit or double-bit errors. We can improve this situation. If we use the generator polynomial formula_4, where formula_5 is a primitive polynomial of degree formula_6, then the maximal total block length is formula_7, and the code is able to detect single, double, triple and any odd number of errors.\n\nA polynomial formula_8 that admits other factorizations may be chosen then so as to balance the maximal total blocklength with a desired error detection power. The BCH codes are a powerful class of such polynomials. They subsume the two examples above. Regardless of the reducibility properties of a generator polynomial of degree \"r\", if it includes the \"+1\" term, the code will be able to detect error patterns that are confined to a window of \"r\" contiguous bits. These patterns are called \"error bursts\".\n\nThe concept of the CRC as an error-detecting code gets complicated when an implementer or standards committee uses it to design a practical system. Here are some of the complications:\n\nThese complications mean that there are three common ways to express a polynomial as an integer: the first two, which are mirror images in binary, are the constants found in code; the third is the number found in Koopman's papers. \"In each case, one term is omitted.\" So the polynomial formula_10 may be transcribed as:\nIn the table below they are shown as:\nCRCs in proprietary protocols might be obfuscated by using a non-trivial initial value and a final XOR, but these techniques do not add cryptographic strength to the algorithm and can be reverse engineered using straightforward methods.\n\nNumerous varieties of cyclic redundancy checks have been incorporated into technical standards. By no means does one algorithm, or one of each degree, suit every purpose; Koopman and Chakravarty recommend selecting a polynomial according to the application requirements and the expected distribution of message lengths. The number of distinct CRCs in use has confused developers, a situation which authors have sought to address. There are three polynomials reported for CRC-12, nineteen conflicting definitions of CRC-16, and seven of CRC-32.\n\nThe polynomials commonly applied are not the most efficient ones possible. Since 1993, Koopman, Castagnoli and others have surveyed the space of polynomials between 3 and 64 bits in size, finding examples that have much better performance (in terms of Hamming distance for a given message size) than the polynomials of earlier protocols, and publishing the best of these with the aim of improving the error detection capacity of future standards. In particular, iSCSI and SCTP have adopted one of the findings of this research, the CRC-32C (Castagnoli) polynomial.\n\nThe design of the 32-bit polynomial most commonly used by standards bodies, CRC-32-IEEE, was the result of a joint effort for the Rome Laboratory and the Air Force Electronic Systems Division by Joseph Hammond, James Brown and Shyan-Shiang Liu of the Georgia Institute of Technology and Kenneth Brayer of the Mitre Corporation. The earliest known appearances of the 32-bit polynomial were in their 1975 publications: Technical Report 2956 by Brayer for Mitre, published in January and released for public dissemination through DTIC in August, and Hammond, Brown and Liu's report for the Rome Laboratory, published in May. Both reports contained contributions from the other team. During December 1975, Brayer and Hammond presented their work in a paper at the IEEE National Telecommunications Conference: the IEEE CRC-32 polynomial is the generating polynomial of a Hamming code and was selected for its error detection performance. Even so, the Castagnoli CRC-32C polynomial used in iSCSI or SCTP matches its performance on messages from 58 bits to 131 kbits, and outperforms it in several size ranges including the two most common sizes of Internet packet. The ITU-T G.hn standard also uses CRC-32C to detect errors in the payload (although it uses CRC-16-CCITT for PHY headers).\n\nCRC32 computation is implemented in hardware as an operation of SSE4.2 instruction set, first introduced in Intel processors' Nehalem microarchitecture.\n\nThe table below lists only the polynomials of the various algorithms in use. Variations of a particular protocol can impose pre-inversion, post-inversion and reversed bit ordering as described above. For example, the CRC32 used in Gzip and Bzip2 use the same polynomial, but Gzip employs reversed bit ordering, while Bzip2 does not.\n\n\n\n\n\n"}
{"id": "30152339", "url": "https://en.wikipedia.org/wiki?curid=30152339", "title": "Doubling space", "text": "Doubling space\n\nIn mathematics, a metric space with metric is said to be doubling if there is some doubling constant such that for any and , it is possible to cover the ball with the union of at most balls of radius . The base-2 logarithm of is often referred to as the doubling dimension of . Euclidean spaces equipped with the usual Euclidean metric are examples of doubling spaces where the doubling constant depends on the dimension . For example, in one dimension, ; and in two dimensions, .\n\nAn important question in metric space geometry is to characterize those metric spaces that can be embedded in some Euclidean space by a bi-Lipschitz function. This means that one can essentially think of the metric space as a subset of Euclidean space. Not all metric spaces may be embedded in Euclidean space. Doubling metric spaces, on the other hand, would seem like they have more of a chance, since the doubling condition says, in a way, that the metric space is not infinite dimensional. However, this is still not the case in general. The Heisenberg group with its Carnot metric is an example of a doubling metric space which cannot be embedded in any Euclidean space.\n\nAssouad's Theorem states that, for a \"M\"-doubling metric space \"X\", if we give it the metric \"d\"(\"x\", \"y\") for some 0 < \"ε\" < 1, then there is a \"L\"-bi-Lipschitz map \"f\":\"X\" → ℝ, where \"d\" and \"L\" depend on \"M\" and \"ε\".\n\nA nontrivial measure on a metric space \"X\" is said to be doubling if the measure of any ball is finite and approximately the measure of its double, or more precisely, if there is a constant \"C\" > 0 such that\n\nfor all \"x\" in \"X\" and \"r\" > 0. In this case, we say \"μ\" is C-doubling.\n\nA metric measure space that supports a doubling measure is necessarily a doubling metric space, where the doubling constant depends on the constant \"C\". Conversely, any complete doubling metric space supports a doubling measure.\n\nA simple example of a doubling measure is Lebesgue measure on a Euclidean space. One can, however, have doubling measures on Euclidean space that are singular with respect to Lebesgue measure. One example on the real line is the weak limit of the following sequence of measures:\n\nOne can construct another singular doubling measure \"μ\" on the interval [0, 1] as follows: for each \"k\" ≥ 0, partition the unit interval [0,1] into 3 intervals of length 3. Let Δ be the collection of all such intervals in [0,1] obtained for each \"k\" (these are the \"triadic intervals\"), and for each such interval \"I\", let \"m\"(\"I\") denote its \"middle third\" interval. Fix 0 < \"δ\" < 1 and let \"μ\" be the measure such that \"μ\"([0, 1]) = 1 and for each triadic interval \"I\", \"μ\"(\"m\"(\"I\")) = \"δμ\"(\"I\"). Then this gives a doubling measure on [0, 1] singular to Lebesgue measure.\n\nThe definition of a doubling measure may seem arbitrary, or purely of geometric interest. However, many results from classical harmonic analysis and computational geometry extend to the setting of metric spaces with doubling measures.\n"}
{"id": "18167327", "url": "https://en.wikipedia.org/wiki?curid=18167327", "title": "Eben Matlis", "text": "Eben Matlis\n\nEben Matlis (born 1923) is a mathematician known for his contributions to the theory of rings and modules, especially for his work with injective modules over commutative Noetherian rings, and his introduction of Matlis duality.\n\nMatlis earned his Ph.D. at the University of Chicago in 1958, with Irving Kaplansky as advisor. He is an emeritus professor at Northwestern University and was a member of the Institute for Advanced Study from August 1962 to June 1963.\n\n"}
{"id": "37561810", "url": "https://en.wikipedia.org/wiki?curid=37561810", "title": "Elliott Ward Cheney Jr.", "text": "Elliott Ward Cheney Jr.\n\nElliott Ward Cheney Jr. (June 28, 1929 – July 13, 2016) was an American mathematician and an Emeritus Professor at the University of Texas at Austin. Known to his friends and colleagues as Ward Cheney, he was one of the pioneers in the fields of approximation theory and numerical analysis. His 1966 book, \"An Introduction to Approximation Theory\", remains in print and is \"highly respected and well known\", \"a small book almost encyclopedic in character\", and \"is a classic with few competitors\".\n\nThe second of two children of E. W. Cheney, Sr., and Carleton (Pratt) Cheney, Elliott Ward Cheney Jr., was born in Gettysburg, Pennsylvania, and grew up in Washington, D.C., New Jersey, and Bethlehem, Pennsylvania. Ward began clarinet studies at age ten and would play in chamber music groups throughout his life.\n\nWard Cheney was a 1947 graduate of Fountain Hill High School, Bethlehem, Pennsylvania. In 1951, he earned his bachelor's degree in mathematics from Lehigh University, where his father was a physics professor. During undergraduate summers, Ward worked for the United States Forest Service, where he met Elizabeth Jean \"Beth\", whom he married in 1952. The young couple resided in Lawrence, Kansas, while Ward studied and served as a mathematics instructor at The University of Kansas, earning his Ph.D. in 1957. Ward had three children with his first wife, Beth, all of whom earned doctorates: daughter Margaret is a professor of mathematics, son David is a manager of international research projects, and son Elliott is a professional cellist. \nTheir mother Beth remarried in 1975 and died in 1991. Ward and his wife Victoria had been together since 1983.\n\nFollowing the launch of Sputnik 1 by the Soviet Union in 1957, the United States intensified its focus on their aerospace program. Cheney became a research scientist at Convair Astronautics in San Diego, California, where his mathematical team worked on calculations for the Atlas rocket—which would take John Glenn into space.\n\nCheney also worked for Space Technology, near Los Angeles, and taught at UCLA, with visiting positions at Michigan State University and Iowa State University. In addition, Ward was a consultant and/or a guest worker at Boeing Scientific Research Laboratories, The Aerospace Corporation, and IBM Research Laboratory. In the summers of 1961–63, he was director of the NSF Summer Institute in Numerical Analysis at the University of California, Los Angeles.\n\nIn 1964, Ward joined the mathematics faculty of The University of Texas at Austin, where he taught for the next 41 years, until his retirement at age 76.\n\nCheney served continuously on the editorial board of the Journal of Approximation Theory from its inception in 1968 until sometime after the start of 2015, and published 14 papers there.\n\nProfessor Cheney supervised 17 PhD students, 35 Masters students, and worked with three post-doctoral students. He was Associate Editor for ten mathematical journals as well as referee and reviewer for many other journals. Cheney had over a 100 published papers and was the author of two dozen mathematical textbooks, with several having multiple editions. A reviewer wrote of his 1986 monograph: \"Cheney's book scores highly on all ... points\". Ward Cheney and Will Light wrote two graduate level books. Ward Cheney and David Kincaid co-authored two undergraduate textbooks, and a graduate textbook.\n\nDuring his career, Ward frequently spent his summers and a sabbatical semester in England at Lancaster University and at Leicester University. Also, he held a visiting professorship at Lund University, Sweden. Ward was a world-wide traveler and was frequently invited to give lecturers on approximation theory at universities wherever he went.\n\nWard was an inspirational teacher and a superb lecturer who presented over 165 invited lectures and colloquium talks at universities and conferences around the world. Special honors include an invited lectures at the \"1963 National Meeting of the Society for Industrial and Applied Mathematics (SIAM)\", in Denver, Colorado, and at the \"1974 International Congress of Mathematicians\", in Vancouver, Canada. Moreover, Professor Cheney was the honoree at the \"1995 International Conference on Approximation Theory\", College Station, Texas. For over 40 years, this has been the main general conference on approximation theory with presentations by international mathematician from academia, industry, and government.\n\nCheney was awarded grants for his research on approximation theory from the National Science Foundation, United States Air Force and United States Army as well as the UK Research Councils and the Italian Scientific Research Council, among others.\n\nIn 2012, Ward became a Fellow of the American Mathematical Society, which was the first year the honor was awarded.\n\nCheney died in July 2016, after having had Alzheimer's disease for several years.\n\n\n\n"}
{"id": "52119", "url": "https://en.wikipedia.org/wiki?curid=52119", "title": "Embedding", "text": "Embedding\n\nIn mathematics, an embedding (or imbedding) is one instance of some mathematical structure contained within another instance, such as a group that is a subgroup.\n\nWhen some object \"X\" is said to be embedded in another object \"Y\", the embedding is given by some injective and structure-preserving map . The precise meaning of \"structure-preserving\" depends on the kind of mathematical structure of which \"X\" and \"Y\" are instances. In the terminology of category theory, a structure-preserving map is called a morphism.\n\nThe fact that a map is an embedding is often indicated by the use of a \"hooked arrow\" (); thus: formula_1 (On the other hand, this notation is sometimes reserved for inclusion maps.)\n\nGiven \"X\" and \"Y\", several different embeddings of \"X\" in \"Y\" may be possible. In many cases of interest there is a standard (or \"canonical\") embedding, like those of the natural numbers in the integers, the integers in the rational numbers, the rational numbers in the real numbers, and the real numbers in the complex numbers. In such cases it is common to identify the domain \"X\" with its image \"f\"(\"X\") contained in \"Y\", so that .\n\nIn general topology, an embedding is a homeomorphism onto its image. More explicitly, an injective continuous map formula_2 between topological spaces formula_3 and formula_4 is a topological embedding if formula_5 yields a homeomorphism between formula_3 and formula_7 (where formula_7 carries the subspace topology inherited from formula_4). Intuitively then, the embedding formula_2 lets us treat formula_3 as a subspace of formula_4. Every embedding is injective and continuous. Every map that is injective, continuous and either open or closed is an embedding; however there are also embeddings which are neither open nor closed. The latter happens if the image formula_7 is neither an open set nor a closed set in formula_4.\n\nFor a given space formula_4, the existence of an embedding formula_16 is a topological invariant of formula_3. This allows two spaces to be distinguished if one is able to be embedded in a space while the other is not.\n\nIn differential topology:\nLet formula_18 and formula_19 be smooth manifolds and formula_20 be a smooth map. Then formula_5 is called an immersion if its derivative is everywhere injective. An embedding, or a smooth embedding, is defined to be an injective immersion which is an embedding in the topological sense mentioned above (i.e. homeomorphism onto its image). \n\nIn other words, an embedding is diffeomorphic to its image, and in particular the image of an embedding must be a submanifold. An immersion is a local embedding (i.e. for any point formula_22 there is a neighborhood formula_23 such that formula_24 is an embedding.)\n\nWhen the domain manifold is compact, the notion of a smooth embedding is equivalent to that of an injective immersion.\n\nAn important case is formula_25. The interest here is in how large formula_26 must be for an embedding, in terms of the dimension formula_27 of formula_18. The Whitney embedding theorem states that formula_29 is enough, and is the best possible linear bound. For example the real projective space RP of dimension formula_27, where formula_27 is a power of two, requires formula_29 for an embedding. However, this does not apply to immersions; for instance, RP can be immersed in formula_33 as is explicitly shown by Boy's surface—which has self-intersections. The Roman surface fails to be an immersion as it contains cross-caps.\n\nAn embedding is proper if it behaves well with respect to boundaries: one requires the map formula_34 to be such that\n\n\nThe first condition is equivalent to having formula_39 and formula_40. The second condition, roughly speaking, says that \"f\"(\"X\") is not tangent to the boundary of \"Y\".\n\nIn Riemannian geometry:\nLet (\"M,g\") and (\"N,h\") be Riemannian manifolds.\nAn isometric embedding is a smooth embedding \"f\" : \"M\" → \"N\" which preserves the metric in the sense that \"g\" is equal to the pullback of \"h\" by \"f\", i.e. \"g\" = \"f\"*\"h\". Explicitly, for any two tangent vectors \n\nwe have \n\nAnalogously, isometric immersion is an immersion between Riemannian manifolds which preserves the Riemannian metrics.\n\nEquivalently, an isometric embedding (immersion) is a smooth embedding (immersion) which preserves length of curves (cf. Nash embedding theorem).\n\nIn general, for an algebraic category \"C\", an embedding between two \"C\"-algebraic structures \"X\" and \"Y\" is a \"C\"-morphism that is injective.\n\nIn field theory, an embedding of a field \"E\" in a field \"F\" is a ring homomorphism .\n\nThe kernel of \"σ\" is an ideal of \"E\" which cannot be the whole field \"E\", because of the condition . Furthermore, it is a well-known property of fields that their only ideals are the zero ideal and the whole field itself. Therefore, the kernel is 0, so any embedding of fields is a monomorphism. Hence, \"E\" is isomorphic to the subfield \"σ\"(\"E\") of \"F\". This justifies the name \"embedding\" for an arbitrary homomorphism of fields.\n\nIf σ is a signature and formula_43 are σ-structures (also called σ-algebras in universal algebra or models in model theory), then a map formula_44 is a σ-embedding iff all of the following hold:\n\nHere formula_55 is a model theoretical notation equivalent to formula_56. In model theory there is also a stronger notion of elementary embedding.\n\nIn order theory, an embedding of partial orders is a function F from X to Y such that:\n\nIn domain theory, an additional requirement is: \n\nA mapping formula_59 of metric spaces is called an \"embedding\"\n(with distortion formula_60) if \nfor some constant formula_62.\n\nAn important special case is that of normed spaces; in this case it is natural to consider linear embeddings. \n\nOne of the basic questions that can be asked about a finite-dimensional normed space formula_63 is, \"what is the maximal dimension formula_64 such that the Hilbert space formula_65 can be linearly embedded into formula_3 with constant distortion?\"\n\nThe answer is given by Dvoretzky's theorem.\n\nIn category theory, there is no satisfactory and generally accepted definition of embeddings that is applicable in all categories. One would expect that all isomorphisms and all compositions of embeddings are embeddings, and that all embeddings are monomorphisms. Other typical requirements are: any extremal monomorphism is an embedding and embeddings are stable under pullbacks.\n\nIdeally the class of all embedded subobjects of a given object, up to isomorphism, should also be small, and thus an ordered set. In this case, the category is said to be well powered with respect to the class of embeddings. This allows defining new local structures in the category (such as a closure operator). \n\nIn a concrete category, an embedding is a morphism \"ƒ\": \"A\" → \"B\" which is an injective function from the underlying set of \"A\" to the underlying set of \"B\" and is also an initial morphism in the following sense:\nIf \"g\" is a function from the underlying set of an object \"C\" to the underlying set of \"A\", and if its composition with \"ƒ\" is a morphism \"ƒg\": \"C\" → \"B\", then \"g\" itself is a morphism.\n\nA factorization system for a category also gives rise to a notion of embedding. If (\"E\", \"M\") is a factorization system, then the morphisms in \"M\" may be regarded as the embeddings, especially when the category is well powered with respect to \"M\". Concrete theories often have a factorization system in which \"M\" consists of the embeddings in the previous sense. This is the case of the majority of the examples given in this article.\n\nAs usual in category theory, there is a dual concept, known as quotient. All the preceding properties can be dualized.\n\nAn embedding can also refer to an embedding functor.\n\n\n\n"}
{"id": "56621861", "url": "https://en.wikipedia.org/wiki?curid=56621861", "title": "Floating point error mitigation", "text": "Floating point error mitigation\n\nFloating-point error arises because real numbers cannot, in general, be accurately represented in a fixed space. By definition, floating-point error cannot be eliminated, and, at best, can only be managed.\n\nH. M. Sierra noted in his 1956 patent \"Floating Decimal Point Arithmetic Control Means for Calculator\":\n\"Thus under some conditions, the major portion of the significant data digits may lie beyond the capacity of the registers. Therefore, the result obtained may have little meaning if not totally erroneous.\"\nThe first computer (relays) developed by Zuse in 1936 with floating point arithmetic and was thus susceptible to floating point error. Early computers, however, with operation times measured in milliseconds, were incapable of solving large, complex problems and thus were seldom plagued with floating point error. Today, however, with super computer system performance measured in petaflops, (10) floating-point operations per second, floating point error is a major concern for computational problem solvers. Further, there are two types of floating point error, cancellation and rounding. Cancellation occurs when subtracting two similar numbers and rounding occurs with significant bits cannot be saved and are rounded or truncated. Cancellation error is exponential relative to rounding error.\n\nThe following sections describe the strengths and weaknesses of various means of mitigating floating point error.\n\nThough not the primary focus of numerical analysis, numerical error analysis for the analysis and minimization of floating point rounding error. Numerical error analysis generally does not account for cancellation error.\n\nError analysis by Monte Carlo arithmetic is accomplished by repeatedly injecting small errors into an algorithms data values and determining the relative effect on the results.\n\nExtension of precision is the use of larger representations of real values. The ISO standard define precision as the number of digits available to represent real numbers, and typically including single precision (32-bits), double precision (64-bits), and quad precision (128-bits). While extension of precision makes the effects of error less likely or less important, the true accuracy of the results are still unknown.\n\nVariable length arithmetic represents numbers as a string of digits of variable length limited only by the memory available. Variable length arithmetic operations are considerably slower than fixed length format floating point instructions. When high performance is not a requirement, but high precision is, variable length arithmetic can prove useful, thought the actual accuracy of the result may not be known.\n\nInterval arithmetic is an algorithm for bounding rounding and measurement errors. The algorithm results in two floating point numbers representing the minimum and maximum limits for the real value represented. \"Instead of using a single floating-point number as approximation for the value of a real variable in the mathematical model under investigation, interval arithmetic acknowledges limited precision by associating with the variable\na set of reals as possible values. For ease of storage and computation, these sets are restricted to intervals.\"The evaluation of interval arithmetic expression may provide a large range of values, and may seriously overestimate the true error boundaries.\n\nUnums (\"Universal Numbers\") are an extension of variable length arithmetic proposed by John Gustafson. Unums have variable length fields for the exponent and significand lengths and error information is carried in a single bit, the ubit, representing possible error in the least significant bit of the significand (ULP).\n\nThe efficacy of unums is questioned by William Kahan.\n"}
{"id": "27324597", "url": "https://en.wikipedia.org/wiki?curid=27324597", "title": "Gaston Milhaud", "text": "Gaston Milhaud\n\nGaston Milhaud (10 August 1858 in Nîmes – 1 October 1918) was a French philosopher and historian of science.\n\nGaston Milhaud studied mathematics with Gaston Darboux at the École Normale Supérieure. In 1881 he took a teaching post at the University of Le Havre. In 1891 he became professor of mathematics at Montpellier University, and in 1895 became professor of philosophy there. In 1909 a chair in the history of philosophy in its relationship to the sciences was created for him at the Sorbonne. Milhaud's successor in the chair was Abel Rey.\n\n"}
{"id": "205497", "url": "https://en.wikipedia.org/wiki?curid=205497", "title": "George Biddell Airy", "text": "George Biddell Airy\n\nSir George Biddell Airy (; 27 July 18012 January 1892) was an English mathematician and astronomer, Astronomer Royal from 1835 to 1881. His many achievements include work on planetary orbits, measuring the mean density of the Earth, a method of solution of two-dimensional problems in solid mechanics and, in his role as Astronomer Royal, establishing Greenwich as the location of the prime meridian. His reputation has been tarnished by allegations that, through his inaction, Britain lost the opportunity of priority in the discovery of Neptune.\n\nAiry was born at Alnwick, one of a long line of Airys who traced their descent back to a family of the same name residing at Kentmere, in Westmorland, in the 14th century. The branch to which he belonged, having suffered in the English Civil War, moved to Lincolnshire and became farmers. Airy was educated first at elementary schools in Hereford, and afterwards at Colchester Royal Grammar School. An introverted child, Airy gained popularity with his schoolmates through his great skill in the construction of peashooters.\n\nFrom the age of 13, Airy stayed frequently with his uncle, Arthur Biddell at Playford, Suffolk. Biddell introduced Airy to his friend Thomas Clarkson, the slave trade abolitionist who lived at Playford Hall. Clarkson had an MA in mathematics from Cambridge, and examined Airy in classics and then subsequently arranged for him to be examined by a Fellow from Trinity College, Cambridge on his knowledge of mathematics. As a result, he entered Trinity in 1819, as a sizar, meaning that he paid a reduced fee but essentially worked as a servant to make good the fee reduction. Here he had a brilliant career, and seems to have been almost immediately recognised as the leading man of his year. In 1822 he was elected scholar of Trinity, and in the following year he graduated as senior wrangler and obtained first Smith's Prize. On 1 October 1824 he was elected fellow of Trinity, and in December 1826 was appointed Lucasian professor of mathematics in succession to Thomas Turton. This chair he held for little more than a year, being elected in February 1828 Plumian professor of astronomy and director of the new Cambridge Observatory. In 1836 he was elected a Fellow of the Royal Society and in 1840, a foreign member of the Royal Swedish Academy of Sciences. In 1859 he became foreign member of the Royal Netherlands Academy of Arts and Sciences.\n\nSome idea of his activity as a writer on mathematical and physical subjects during these early years may be gathered from the fact that previous to this appointment he had contributed no less than three important memoirs to the \"Philosophical Transactions\" of the Royal Society, and eight to the Cambridge Philosophical Society. At the Cambridge Observatory Airy soon showed his power of organisation. The only telescope in the establishment when he took charge was the transit instrument, and to this he vigorously devoted himself. By the adoption of a regular system of work, and a careful plan of reduction, he was able to keep his observations up to date, and published them annually with a punctuality which astonished his contemporaries. Before long a mural circle was installed, and regular observations were instituted with it in 1833. In the same year the Duke of Northumberland presented the Cambridge observatory with a fine object-glass of 12-inch aperture, which was mounted according to Airy's designs and under his superintendence, although construction was not completed until after he moved to Greenwich in 1835.\n\nAiry's writings during this time are divided between mathematical physics and astronomy. The former are for the most part concerned with questions relating to the theory of light arising out of his professorial lectures, among which may be specially mentioned his paper \"On the Diffraction of an Object-Glass with Circular Aperture,\" and his enunciation of the complete theory of the rainbow. In 1831 the Copley Medal of the Royal Society was awarded to him for these researches. Of his astronomical writings during this period the most important are his investigation of the mass of Jupiter, his report to the British Association on the progress of astronomy during the 19th century, and his work \"On an Inequality of Long Period in the Motions of the Earth and Venus\".\n\nOne of the sections of his able and instructive report was devoted to \"A Comparison of the Progress of Astronomy in England with that in other Countries,\" very much to the disadvantage of England. This reproach was subsequently to a great extent removed by his own labours.\n\nOne of the most remarkable of Airy's researches was his determination of the mean density of the Earth. In 1826, the idea occurred to him of attacking this problem by means of pendulum experiments at the top and bottom of a deep mine. His first attempt, made in the same year, at the Dolcoath mine in Cornwall, failed in consequence of an accident to one of the pendulums. A second attempt in 1828 was defeated by a flooding of the mine, and many years elapsed before another opportunity presented itself. The experiments eventually took place at the Harton pit near South Shields in 1854. Their immediate result was to show that gravity at the bottom of the mine exceeded that at the top by 1/19286 of its amount, the depth being 383 m (1,256 ft) From this he was led to the final value of Earth's specific density of 6.566. This value, although considerably in excess of that previously found by different methods, was held by Airy, from the care and completeness with which the observations were carried out and discussed, to be \"entitled to compete with the others on, at least, equal terms.\" The currently accepted value for Earth's density is 5.5153 g/cm³.\n\nIn 1830, Airy calculated the lengths of the polar radius and equatorial radius of the earth using measurements taken in the UK. Although his measurements were superseded by more accurate radius figures (such as those used for GRS 80 and WGS84) his \"Airy geoid\" (strictly a reference ellipsoid, OSGB36) is still used by Great Britain's Ordnance Survey for mapping of England, Scotland and Wales because it better fits the local sea level (about 80 cm below world average).\n\nAiry's discovery of a new inequality in the motions of Venus and the Earth is in some respects his most remarkable achievement. In correcting the elements of Delambre's solar tables he had been led to suspect an inequality overlooked by their constructor. The cause of this he did not long seek in vain; thirteen times the mean motion of Venus is so nearly equal to eight times that of Earth that the difference amounts to only a small fraction of Earth's mean motion, and from the fact that the term depending on this difference, although very small in itself, receives in the integration of the differential equations a multiplier of about 2,200,000, Airy was led to infer the existence of a sensible inequality extending over 240 years (\"Phil. Trans.\" cxxii. 67). The investigation was probably the most laborious that had been made up to Airy's time in planetary theory, and represented the first specific improvement in the solar tables effected in England since the establishment of the theory of gravity. In recognition of this work the Gold Medal of the Royal Astronomical Society was awarded to him in 1833 (he would win it again in 1846).\n\nThe resolution of optical devices is limited by diffraction. So even the most perfect lens can't quite generate a point image at its focus, but instead there is a bright central pattern now called the Airy disk, surrounded by concentric rings comprising an Airy pattern. The size of the Airy disk depends on the light wavelength and the size of the aperture. John Herschel had previously described the phenomenon, but Airy was the first to explain it theoretically.\n\nThis was a key argument in refuting one of the last remaining arguments for absolute geocentrism: the giant star argument. Tycho Brahe and Giovanni Battista Riccioli pointed out that the lack of stellar parallax detectable at the time entailed that stars were a huge distance away. But the naked eye and the early telescopes with small apertures seemed to show that stars were disks of a certain size. This would imply that the stars were many times larger than our sun (they were not aware of supergiant or hypergiant stars, but some were calculated to be even larger than the size of the whole universe estimated at the time). However, the disk appearances of the stars were spurious: they were not actually seeing stellar images, but Airy disks. In reality, the actual images of almost all stars, even using the best telescopes with the largest apertures, look like mere points of light.\n\nIn June 1835 Airy was appointed Astronomer Royal in succession to John Pond, and began his long career at the national observatory which constitutes his chief title to fame. The condition of the observatory at the time of his appointment was such that Lord Auckland, the first Lord of the Admiralty, considered that \"it ought to be cleared out,\" while Airy admitted that \"it was in a queer state.\" With his usual energy he set to work at once to reorganise the whole management. He remodelled the volumes of observations, put the library on a proper footing, mounted the new (Sheepshanks) equatorial and organised a new magnetic observatory. In 1847 an altazimuth was erected, designed by Airy to enable observations of the moon to be made not only on the meridian, but whenever it might be visible. In 1848 Airy invented the reflex zenith tube to replace the zenith sector previously employed. At the end of 1850 the great transit circle of 203 mm (8 inch) aperture and 3.5 m (11 ft 6 in) focal length was erected, and is still the principal instrument of its class at the observatory. The mounting in 1859 of an equatorial of 330 mm (13 inch) aperture evoked the comment in his journal for that year, \"There is not now a single person employed or instrument used in the observatory which was there in Mr Pond's time\"; and the transformation was completed by the inauguration of spectroscopic work in 1868 and of the photographic registration of sunspots in 1873.\nThe formidable undertaking of reducing the accumulated planetary observations made at Greenwich from 1750 to 1830 was already in progress under Airy's supervision when he became Astronomer Royal. Shortly afterwards he undertook the further laborious task of reducing the enormous mass of observations of the moon made at Greenwich during the same period under the direction, successively, of James Bradley, Nathaniel Bliss, Nevil Maskelyne and John Pond, to defray the expense of which a large sum of money was allotted by the Treasury. As a result, no fewer than 8,000 lunar observations were rescued from oblivion, and were, in 1846, placed at the disposal of astronomers in such a form that they could be used directly for comparison with the theory and for the improvement of the tables of the moon's motion.\n\nFor this work Airy received in 1848 a testimonial from the Royal Astronomical Society, and it at once led to the discovery by Peter Andreas Hansen of two new inequalities in the moon's motion. After completing these reductions, Airy made inquiries, before engaging in any theoretical investigation in connection with them, whether any other mathematician was pursuing the subject, and learning that Hansen had taken it in hand under the patronage of the king of Denmark, but that, owing to the death of the king and the consequent lack of funds, there was danger of his being compelled to abandon it, he applied to the admiralty on Hansen's behalf for the necessary sum. His request was immediately granted, and thus it came about that Hansen's famous \"Tables de la Lune\" were dedicated to \"La Haute Amirauté de sa Majesté la Reine de la Grande Bretagne et d'Irlande\".\n\nIn 1851 Airy established a new Prime Meridian at Greenwich. This line, the fourth \"Greenwich Meridian,\" became the definitive internationally recognised line in 1884.\n\nIn June 1846, Airy started corresponding with French astronomer Urbain Le Verrier over the latter's prediction that irregularities in the motion of Uranus were due to a so-far unobserved body. Aware that Cambridge Astronomer John Couch Adams had suggested that he had made similar predictions, on 9 July Airy entreated James Challis to undertake a systematic search in the hope of securing the triumph of discovery for Britain. Ultimately, a rival search in Berlin by Johann Gottfried Galle, instigated by Le Verrier, won the race for priority. Though Airy was \"abused most savagely both by English and French\" for his failure to act on Adams's suggestions more promptly, there have also been claims that Adams's communications had been vague and dilatory and further that the search for a new planet was not the responsibility of the Astronomer Royal.\n\nBy means of a water-filled telescope, Airy in 1871 looked for a change in stellar aberration through the refracting water due to an ether drag. Like in all other aether drift experiments, he obtained a negative result.\n\nIn 1872 Airy conceived the idea of treating the lunar theory in a new way, and at the age of seventy-one he embarked on the prodigious toil which this scheme entailed. A general description of his method will be found in the \"Monthly Notices of the Royal Astronomical Society\", vol. xxxiv, No. 3. It consisted essentially in the adoption of Charles-Eugène Delaunay's final numerical expressions for longitude, latitude, and parallax, with a symbolic term attached to each number, the value of which was to be determined by substitution in the equations of motion.\n\nIn this mode of treating the question the order of the terms is numerical, and though the amount of labour is such as might well have deterred a younger man, yet the details were easy, and a great part of it might be entrusted to \"a mere computer\".\n\nThe work was published in 1886, when its author was eighty-five years of age. For some little time previously he had been harassed by a suspicion that certain errors had crept into the computations, and accordingly he addressed himself to the task of revision. But his powers were no longer what they had been, and he was never able to examine sufficiently into the matter. In 1890 he tells us how a grievous error had been committed in one of the first steps, and pathetically adds, \"My spirit in the work was broken, and I have never heartily proceeded with it since.\"\n\nIn 1862, Airy presented a new technique to determine the strain and stress field within a beam. This technique, sometimes called the , can be used to find solutions to many two-dimensional problems in solid mechanics (see ). For example, it was used by H. M. Westergaard to determine the stress and strain field around a crack tip and thereby this method contributed to the development of fracture mechanics.\n\nAiry was consulted about wind speeds and pressures likely to be encountered on the proposed Forth suspension bridge being designed by Thomas Bouch for the North British Railway in the late 1870s. He thought that pressures no greater than about 10 pounds per square foot could be expected, a comment Bouch took to mean also applied to the first Tay railway bridge then being built. Much greater pressures, however, can be expected in severe storms. Airy was called to give evidence before the Official Inquiry into the Tay Bridge disaster, and was criticised for his advice. However, little was known about the problems of wind resistance of large structures, and a Royal Commission on Wind Pressure was asked to conduct research into the problem.\n\nAiry was described in his obituary published by the Royal Society as being \"a tough adversary\" and stories of various disagreements and conflicts with other scientists survive. Francis Ronalds discovered Airy to be his foe while he was inaugural Honorary Director of the Kew Observatory, which Airy considered to be a competitor to Greenwich. Other well documented conflicts were with Charles Babbage and Sir James South.\n\nIn July 1824, Airy met Richarda Smith (1804–1875), \"a great beauty\", on a walking tour of Derbyshire. He later wrote, \"Our eyes met ... and my fate was sealed ... I felt irresistibly that we must be united,\" and Airy proposed two days later. Richarda's father, the Revd Richard Smith, felt that Airy lacked the financial resources to marry his daughter. Only in 1830, with Airy established in his Cambridge position, was permission for the marriage granted.\n\nThe Airys had nine children, the first three died young. \n\nElizabeth Airy (born 1833) died of consumption (tuberculosis) in 1852. \n\nThe eldest to survive to adulthood was Wilfrid (1836-1925), who designed and engineered \"Colonel\" George Tomline's Orwell Park Observatory. Wilfrid's daughter was the artist Anna Airy. Anna’s mother died shortly after she was born and she was raised by her maiden aunts Christabel and Annot (see below).\n\nTheir son Hubert Airy (1838–1903) was a doctor, and a pioneer in the study of migraine. Airy himself suffered from this condition.\n\nTheir eldest daughter, Hilda (1840–1916), married the mathematician Edward Routh in 1864.\n\nNeither of their next two daughters married, they were Christabel (1842-1917) and Annot (1843-1924).\n\nThe Airys youngest child was Osmund (1845-1929).\n\nAiry was knighted on 17 June 1872.\n\nAiry retired in 1881, living with his two un-married daughters at Croom's Hill near Greenwich. In 1891, he suffered a fall and an internal injury. He survived the consequential surgery only a few days. His wealth at death was £27,713. Airy and his wife and three pre-deceased children are buried at St. Mary's Church in Playford, Suffolk. A cottage owned by Airy still stands, adjacent to the church and now in private hands.\n\nPatrick Moore the late and widely known astronomer and TV presenter asserts in his autobiography that the ghost of Airy has been seen haunting the Royal Greenwich Observatory after nightfall. (page 178)\n\n\n\nA complete list of Airy's 518 printed papers is in Airy (1896). Among the most important are:\n\n\n\n"}
{"id": "23754121", "url": "https://en.wikipedia.org/wiki?curid=23754121", "title": "Georges Giraud", "text": "Georges Giraud\n\nGeorges Julien Giraud (22 July 1889 – 16 March 1943) was a French mathematician, working in potential theory, partial differential equations, singular integrals and singular integral equations: he is mainly known for his solution of the regular oblique derivative problem and also for his extension to –dimensional () singular integral equations of the concept of symbol of a singular integral, previously introduced by Solomon Mikhlin.\n\nThe scientific work of Georges Giraud was widely acknowledged and earned him several prizes, mainly, but not exclusively, awarded him by the French Academy of Sciences: he was seven times recipient of academy prizes.\n\nIn 1919, he was awarded the \"Prix Francœur\" for his work on the theory of automorphic functions: the members of the commission who examined his work and nominated him were Camille Jordan, Paul Appell, Marie Georges Humbert, Jacques Hadamard, Édouard Goursat, Joseph Boussinesq, Léon Lecornu and Emile Picard (the relator). For the same motivation, On 17 December 1923 he was awarded the \"Gustave Roux\" prize.\n\nIn 1924 he won the Hirn Foundation Prize, for his whole scientific work: he won again the same prize in 1935, for his work on singularities of boundary value problems in the theory of partial differential equations.\n\nIn 1928 Giraud won the \"Grand Prix des sciences Mathématiques\" for his work in the theory of partial differential equations: for the same motivation, in 1930 he was also awarded the \"Prix Houllevigue\". In the same year, he was also awarded the prize of the Lasserre foundation.\n\nIn 1933 he was recipient of the Prix Saintour, for his work on partial differential and integral equations.\n\nFinally, in 1935, apart from winning the Hirn foundation prize for a second time, he was awarded the prize of the Annali della Reale Scuola Normale Superiore di Pisa, equally divided between him, Guido Ascoli and Pietro Buzano: the members of the jury who awarded ex-aequo the prize were Guido Fubini, Mauro Picone and Giovanni Sansone.\n\nOn 14 December 1936, following up a proposal Jacques Hadamard made since 1931, he was elected corresponding member of the French Academy of Sciences.\n\nHe was also a member of the Société Mathématique de France from 1913 to his death.\n\n\n\n\n\n\n"}
{"id": "35520327", "url": "https://en.wikipedia.org/wiki?curid=35520327", "title": "Gibbons–Hawking space", "text": "Gibbons–Hawking space\n\nIn mathematical physics, a Gibbons–Hawking space, named after Gary Gibbons and Stephen Hawking, is essentially a hyperkähler manifold with an extra U(1) symmetry. (In general, Gibbons–Hawking metrics are a subclass of hyperkähler metrics.) Gibbons–Hawking spaces, especially ambipolar ones, find an application in the study of black hole microstate geometries.\n\n"}
{"id": "9214056", "url": "https://en.wikipedia.org/wiki?curid=9214056", "title": "Gyula Y. Katona", "text": "Gyula Y. Katona\n\nGyula Y. Katona (born December 4, 1965) is a Hungarian mathematician, the son of mathematician Gyula O. H. Katona. He received his Ph.D. in 1997 from Hungarian Academy of Sciences, with a dissertation entitled \"Paths and Cycles in Graphs and Hypergraphs\" under the advisement of László Lovász and András Recski, and is on the faculty of the Budapest University of Technology and Economics.\n\nKatona is the coauthor of three textbooks, \"Introduction to Computer Science\" (Typotex, Budapest, 2002), \"Introduction to Finite Mathematics\", (Eötvös L. University, Budapest, 1993), and \"Combinatorics, Graph Theory and Algorithms\" (Technical University of Budapest, 1993). In addition his research publications include several works on Hamiltonian cycles and related properties of graphs.\n\n"}
{"id": "58527033", "url": "https://en.wikipedia.org/wiki?curid=58527033", "title": "Hans Schneider Prize in Linear Algebra", "text": "Hans Schneider Prize in Linear Algebra\n\nThe Hans Schneider Prize in Linear Algebra is awarded every three years by the International Linear Algebra Society. It recognizes research, contributions, and achievements at the highest level of linear algebra and was first awarded in 1993. It may be awarded for an outstanding scientific achievement or for lifetime contributions and may be awarded to more than one recipient. The award honors Hans Schneider, \"one of the most influential mathematicians of the 20th Century in the field of linear algebra and matrix analysis.” The prize includes a plaque, certificate and/or a monetary award.\n\nThe recipients of the Hans Schneider Prize in Linear Algebra are:\n\n"}
{"id": "38934769", "url": "https://en.wikipedia.org/wiki?curid=38934769", "title": "Henri Berestycki", "text": "Henri Berestycki\n\nHenri Berestycki (born 25 March 1951, in Paris) is a French mathematician who obtained his PhD from Université Paris VI – Université Pierre et Marie Curie in 1975. His Dissertation was titled \"Contributions à l'étude des problèmes elliptiques non linéaires\", and his doctoral advisor was Haim Brezis He was an L.E. Dickson Instructor in Mathematics at the University of Chicago from 1975–77, after which he returned to France to continue his research. He has made many contributions in nonlinear analysis, ranging from nonlinear elliptic equations, hamiltonian systems, spectral theory of elliptic operators, and with applications to the description of mathematical modelling of fluid mechanics and combustion. His current research interests include the mathematical modelling of financial markets, mathematical models in biology and especially in ecology, and modelling in social sciences (in particular, urban planning and criminology). For these latter topics, he obtained an ERC Advanced grant in 2012.\n\nHe worked at the French National Center of Scientific Research (CNRS), then moved to an appointment as Professor at Univ. Paris XIII (1983–1985). He became a Professor of Mathematics in 1988 at Université Pierre et Marie Curie, Paris VI (1988–2001 of “exceptional class” since 1993), and became Professor at Ecole normale supérieure, Paris (1994–1999), and part-time professor Ecole Polytechnique (1987–1999). He is also a visiting Professor in the Department of Mathematics at the University of Chicago, and was also co-director of the Stevanovich Center of Financial Mathematics in Chicago. He is currently the Directeur d'études (Research Professor) at École des hautes études en sciences sociales (EHESS), since 2001.\n\n\n\n"}
{"id": "5586409", "url": "https://en.wikipedia.org/wiki?curid=5586409", "title": "History of variational principles in physics", "text": "History of variational principles in physics\n\nA variational principle in physics is an alternative method for determining the state or dynamics of a physical system, by identifying it as an extremum (minimum, maximum or saddle point) of a function or functional. This article describes the historical development of such principles.\n\nVariational principles are found among earlier ideas in surveying and optics. The rope stretchers of ancient Egypt stretched corded ropes between two points to measure the path which minimized the distance of separation, and Claudius Ptolemy, in his Geographia (Bk 1, Ch 2), emphasized that one must correct for \"deviations from a straight course\"; in ancient Greece Euclid states in his \"Catoptrica\" that, for the path of light reflecting from a mirror, the angle of incidence equals the angle of reflection; and Hero of Alexandria later showed that this path was the shortest length and least time.\n\nThis was generalized to refraction by Pierre de Fermat, who, in the 17th century, refined the principle to \"light travels between two given points along the path of shortest \"time\"\"; now known as the principle of least time or Fermat's principle.\n\nCredit for the formulation of the principle of least action is commonly given to Pierre Louis Maupertuis, who wrote about it in 1744 and 1746, although the true priority is less clear, as discussed below.\n\nMaupertuis felt that \"Nature is thrifty in all its actions\", and applied the principle broadly: \"The laws of movement and of rest deduced from this principle being precisely the same as those observed in nature, we can admire the application of it to all phenomena. The movement of animals, the vegetative growth of plants ... are only its consequences; and the spectacle of the universe becomes so much the grander, so much more beautiful, the worthier of its Author, when one knows that a small number of laws, most wisely established, suffice for all movements.\" \n\nIn application to physics, Maupertuis suggested that the quantity to be minimized was the product of the duration (time) of movement within a system by the \"vis viva\", twice what we now call the kinetic energy of the system.\n\nLeonhard Euler gave a formulation of the action principle in 1744, in very recognizable terms, in the \"Additamentum 2\" to his \"Methodus Inveniendi Lineas Curvas Maximi Minive Proprietate Gaudentes\". He begins the second paragraph:\n\nA translation of this passage reads:\n\nAs Euler states, formula_3 is the integral of the momentum over distance traveled (note that here formula_5 contrary to usual notation denotes the \"squared\" velocity) which, in modern notation, equals the reduced action formula_12. Thus, Euler made an equivalent and (apparently) independent statement of the variational principle in the same year as Maupertuis, albeit slightly later. In rather general terms he wrote that \"Since the fabric of the Universe is most perfect and is the work of a most wise Creator, nothing whatsoever takes place in the Universe in which some relation of maximum and minimum does not appear.\"\nHowever, Euler did not claim any priority, as the following episode shows.\n\nMaupertuis' priority was disputed in 1751 by the mathematician Samuel König, who claimed that it had been invented by Gottfried Leibniz in 1707. Although similar to many of Leibniz's arguments, the principle itself has not been documented in Leibniz's works. König himself showed a \"copy\" of a 1707 letter from Leibniz to Jacob Hermann with the principle, but the \"original\" letter has been lost. In contentious proceedings, König was accused of forgery, and even the King of Prussia entered the debate, defending Maupertuis, while Voltaire defended König. Euler, rather than claiming priority, was a staunch defender of Maupertuis, and Euler himself prosecuted König for forgery before the Berlin Academy on 13 April 1752. The claims of forgery were re-examined 150 years later, and archival work by C.I. Gerhardt in 1898 and W. Kabitz in 1913 uncovered other copies of the letter, and three others cited by König, in the Bernoulli archives.\n\nEuler continued to write on the topic; in his \"Reflexions sur quelques loix generales de la nature\" (1748), he called the quantity \"effort\". His expression corresponds to what we would now call potential energy, so that his statement of least action in statics is equivalent to the principle that a system of bodies at rest will adopt a configuration that minimizes total potential energy.\n\nThe full importance of the principle to mechanics was stated by Joseph Louis Lagrange in 1760, although the variational principle was not used to derive the equations of motion until almost 75 years later, when William Rowan Hamilton in 1834 and 1835 applied the variational principle to the function formula_13 to obtain what are now called the Lagrangian equations of motion.\n\nIn 1842, Carl Gustav Jacobi tackled the problem of whether the variational principle found minima or other extrema (e.g. a saddle point); most of his work focused on geodesics on two-dimensional surfaces. The first clear general statements were given by Marston Morse in the 1920s and 1930s, leading to what is now known as Morse theory. For example, Morse showed that the number of conjugate points in a trajectory equaled the number of negative eigenvalues in the second variation of the Lagrangian.\n\nOther extremal principles of classical mechanics have been formulated, such as Gauss' principle of least constraint and its corollary, Hertz's principle of least curvature.\n\nThe action for electromagnetism is:\n\nThe Einstein–Hilbert action which gives rise to the vacuum Einstein field equations is\nwhere formula_16 is the determinant of a spacetime Lorentz metric and formula_17 is the scalar curvature.\n\n\nAlthough equivalent mathematically, there is an important \"philosophical\" difference between the differential equations of motion and their integral counterpart. The differential equations are statements about quantities localized to a single point in space or single moment of time. For example, Newton's second law formula_18 states that the \"instantaneous\" force formula_19 applied to a mass formula_20 produces an acceleration formula_21 at the same \"instant\". By contrast, the action principle is not localized to a point; rather, it involves integrals over an interval of time and (for fields) extended region of space. Moreover, in the usual formulation of classical action principles, the initial and final states of the system are fixed, e.g.,\n\nIn particular, the fixing of the \"final\" state appears to give the action principle a teleological character which has been controversial historically. This apparent teleology is eliminated in the quantum mechanical version of the action principle.\n\n"}
{"id": "25140914", "url": "https://en.wikipedia.org/wiki?curid=25140914", "title": "Holmgren's uniqueness theorem", "text": "Holmgren's uniqueness theorem\n\nIn the theory of partial differential equations, Holmgren's uniqueness theorem, or simply Holmgren's theorem, named after the Swedish mathematician Erik Albert Holmgren (1873–1943), is a uniqueness result for linear partial differential equations with real analytic coefficients.\n\nWe will use the multi-index notation:\nLet formula_1,\nwith formula_2 standing for the nonnegative integers;\ndenote formula_3 and\n\nHolmgren's theorem in its simpler form could be stated as follows:\n\nThis statement, with \"analytic\" replaced by \"smooth\", is Hermann Weyl's classical lemma on elliptic regularity:\n\nThis statement can be proved using Sobolev spaces.\n\nLet formula_5 be a connected open neighborhood in formula_6, and let formula_7 be an analytic hypersurface in formula_5, such that there are two open subsets formula_9 and formula_10 in formula_5, nonempty and connected, not intersecting formula_7 nor each other, such that formula_13.\n\nLet formula_14\n\nthe principal symbol of formula_15.\nformula_16 is a conormal bundle to formula_7, defined as\nformula_18.\n\nThe classical formulation of Holmgren's theorem is as follows:\n\nConsider the problem\n\nwith the Cauchy data\n\nAssume that formula_28 is real-analytic with respect to all its arguments in the neighborhood of formula_29\nand that formula_30 are real-analytic in the neighborhood of formula_31.\n\nNote that the Cauchy–Kowalevski theorem does not exclude the existence of solutions which are not real-analytic.\n\nOn the other hand, in the case when formula_28 is polynomial of order one in formula_35, so that\n\nHolmgren's theorem states that the solution formula_19 is real-analytic and hence, by the Cauchy–Kowalevski theorem, is unique.\n\n"}
{"id": "9943124", "url": "https://en.wikipedia.org/wiki?curid=9943124", "title": "Intelligent Systems for Molecular Biology", "text": "Intelligent Systems for Molecular Biology\n\nIntelligent Systems for Molecular Biology (ISMB) is an annual academic conference on the subjects of bioinformatics and computational biology organised by the International Society for Computational Biology (ISCB). The principal focus of the conference is on the development and application of advanced computational methods for biological problems. The conference has been held every year since 1993 and has grown to become one of the largest and most prestigious meetings in these fields, hosting over 2,000 delegates in 2004. From the first meeting, ISMB has been held in locations worldwide; since 2007, meetings have been located in Europe and North America in alternating years. Since 2004, European meetings have been held jointly with the European Conference on Computational Biology (ECCB).\n\nThe main ISMB conference is usually held over three days and consists of presentations, poster sessions and keynote talks. Most presentations are given in multiple parallel tracks; however, keynote talks are presented in a single track and are chosen to reflect outstanding research in bioinformatics. Notable ISMB keynote speakers have included eight Nobel laureates. The recipients of the ISCB Overton Prize and ISCB Accomplishment by a Senior Scientist Award are invited to give keynote talks as part of the programme. The proceedings of the conference are currently published by the journal \"Bioinformatics\".\n\nThe origins of the ISMB conference lie in a workshop for artificial intelligence researchers with an interest in molecular biology held in November 1991. The workshop was organised by American researcher Lawrence Hunter, then director of the Machine Learning Project at the United States National Institutes of Health's National Library of Medicine (NLM) in Bethesda, Maryland. A subsequent workshop on the same topic held in 1992, hosted by the NLM and the National Science Foundation, made it clear that a regular international conference for the field was required. Such a conference would be dedicated to molecular biology as a rapidly emerging application of artificial intelligence. Having successfully applied for grants from AAAI, NIH and the Department of Energy Office of Health and Environmental Research, the first ISMB conference was held in July 1993, at the NLM. The conference was chaired by Hunter, David Searls (research associate professor at University of Pennsylvania School of Medicine) and Jude Shavlik (assistant professor of computer science at University of Wisconsin–Madison) and attracted over 200 attendees from 13 countries, submitting 69 scientific papers.\n\nThe success of the first conference prompted the announcement of a second ISMB conference at the end of the meeting. ISMB 1994 was initially planned to be held in Seattle. However, a competing meeting forced ISMB to change venues at short notice. The conference was held at Stanford University in August 1994 and was organised by Russ Altman, a research scientist at Stanford University School of Medicine. To emphasise the international aspect of the conference, ISMB 1995 was held at Robinson College, Cambridge. ISMB 1995 also marked a shift in the focus of the conference. ISCB Board member and Director of the Spanish National Bioinformatics Institute Alfonso Valencia has stated that, in 1995, \"the conference changed from a computer science-based conference to a point where everyone realized that, if you want to make progress, there has to be more focus in biology.\"\n\nISMB 1997 was held in Halkidiki, Greece and marked the foundation of the International Society for Computational Biology (ISCB). ISCB was formed with a focus on managing all scientific, organizational and financial aspects of the ISMB conference and to provide a forum for scientists to address the emerging role of computers in the biological sciences. ISCB has assisted in organising the ISMB conference series since 1998. The period following the formation of ISCB also marked an expansion in the number of ISMB attendees: ISMB 2000 (held at the University of California, San Diego) was attended by over 1,000 delegates, submitting 141 scientific papers. This meeting was also the last time ISMB would be held at a university, due to size limitations.\n\nIn 2004, ISMB was jointly held with the European Conference on Computational Biology for the first time. The conference was also co-located with the Genes, Proteins and Computers conference. This meeting, held in Glasgow, UK, was the largest bioinformatics conference ever held, attended by 2,136 delegates, submitting 496 scientific papers. Alfonso Valencia considers ISMB/ECCB 2004 to be an important milestone in the history of ISMB: \"it was the first one where the balance between Europe and the States became an important part of the conference. It was here that we established the rules and the ways and the spirit of collaboration between the Americans and the Europeans.\" The success of the joint conference paved the way for future European ISMB meetings to be held jointly with ECCB.\n\nBy the end of 2006, ISCB was in financial difficulty. Two conferences (ISMB 2003, in Brisbane and ISMB 2006, in Fortaleza) had drastically reduced numbers of participants due to their location, with a corresponding reduction in income. To allow more delegates to attend, it was decided to limit conference locations to North America and Europe. In January 2007, ISMB and ECCB agreed to hold joint conferences in Europe every other year, beginning with ISMB/ECCB 2007. ISMB would be held in North America in the years between joint meetings. , this pattern has been confirmed to continue until at least 2019. ISMB/ECCB 2007 (held in Vienna, Austria) marked the first conference for which ISCB took full responsibility for organising. Vienna became the first city to host ISMB twice with ISMB/ECCB 2011. This 'return visit' was an experiment intended to reduce the increasing effort required to find suitable conference venues. Although the return to Vienna was only deemed partially successful due to price increases, Boston (which hosted ISMB 2010 and 2014) is predicted to become a 'safe' site which ISMB can periodically return to.\n\nISMB celebrated its 20th meeting with ISMB 2012, held in Long Beach, California. This event attracted around 1,600 delegates, submitting 268 scientific papers. Richard H. Lathrop and Lawrence Hunter presented a special keynote presentation, looking back at previous ISMB meetings and attempting to predict where the field of bioinformatics may head in the future. ISMB/ECCB 2013 was held in Berlin, Germany and was attended by around 2,000 delegates, submitting 233 scientific papers.\n\nThe main ISMB conference is usually held over three days and consists of presentations, poster sessions and keynote talks.\n\nAcademic papers at ISMB were traditionally presented in a single track. Presentations at ISMB 1994 were split further into three themed days, focusing on protein secondary structure prediction, sequence analysis and AI techniques and biochemical applications, respectively. As attendance at ISMB increased, the single track approach became increasingly unsustainable and two parallel tracks were introduced at ISMB/ECCB 2004. Further expansion meant that, by ISMB 2012, over 200 talks were presented in nine parallel tracks including multiple proceedings tracks, a highlights track and a technology track. The introduction of parallel tracks to ISMB was controversial. Christopher Rawlings (head of Computational and Systems Biology at Rothamsted Research and organiser of ISMB 1995) has said: \"There were a lot of people who wanted to keep it more strongly in the AI intelligent systems model and have a meeting where everybody would go to everything. But it just grew too big. We just couldn’t.\" As the number of submitted proceedings papers has increased, the acceptance rate has decreased dramatically, from 75% in 1994 to 13% in 2012. ISMB proceedings from 1993-2000 were published by AAAI Press. Since ISMB 2001, proceedings have been published in the journal \"Bioinformatics\". The number of posters presented at ISMB has also increased dramatically. 25 posters were presented at ISMB 1994; at recent ISMB meetings, 500-1,000 posters have been presented in multiple poster sessions.\nKeynote talks are presented in a single track and generally attract the largest audience. These presentations are chosen to highlight outstanding research in the field of bioinformatics. Notable ISMB keynote speakers have included eight Nobel laureates: Richard J. Roberts (keynote speaker in 1994, 2006), John Sulston (1995), Manfred Eigen (1999), Gerald Edelman (2000), Sydney Brenner (2003), Kurt Wüthrich (2006), Robert Huber (2006) and Michael Levitt (2015).\n\nAs of 2012, ISMB runs on a budget in excess of $1.5M and, in terms of proceeds, brings in four times that of the other ISCB conferences (ISCB-Latin America, ISCB-Africa, ISCB-Asia, Rocky Mountain Bioinformatics Conference, CSHALS and the Great Lakes Bioinformatics Conference) combined. Standard registration fees (as of 2013) are around $1,000 for academics who are ISCB members ($1,350 for non-members), with lower rates for students and higher rates for corporate delegates respectively. Discounts are provided for early registration.\n\nPre-conference tutorials have played an important role in ISMB since the first conference. Tutorials at ISMB 1994 included introductions to genetic algorithms, neural networks, AI for molecular biologists and molecular biology for computer scientists. Tutorials on computational mass spectrometry-based proteomics and ENCODE data access were presented at ISMB/ECCB 2013.\n\nAs attendance at ISMB grew in the late 1990s, several satellite meetings and special interest group (SIG) meetings formed alongside the main conference. SIG meetings are held over one or two days before the main conference and focus on a specific topic, allowing more detailed discussion than there would be time for in the main conference. Notable SIG meetings include the Bioinformatics Open Source Conference (BOSC), which has been held annually since 2000 and Bio-Ontologies, which has been held annually since 1998. Satellite meetings are usually two days long and are held in conjunction with ISMB. The 12th CAMDA conference and the 9th 3DSIG meeting were held as satellite meetings of ISMB/ECCB 2013.\n\n"}
{"id": "44832995", "url": "https://en.wikipedia.org/wiki?curid=44832995", "title": "Ioana Dumitriu", "text": "Ioana Dumitriu\n\nIoana Dumitriu (born July 6, 1976) is a Romanian-American mathematician who works as a professor of mathematics at the University of Washington. Her research interests include the theory of random matrices, numerical analysis, scientific computing, and game theory.\n\nDumitriu is the daughter of two Romanian electrical engineering professors from Bucharest. Early in her life she was identified as having mathematical talent, and at age 11 won a national mathematics contest. She entered mathematics training camps in preparation for participation on the Romanian team at the International Mathematical Olympiad, although her highest level of participation in the olympiad was the national semifinal.\nAs a 19-year-old freshman at NYU, Dumitriu already was taking graduate-level classes in mathematics. She graduated summa cum laude from NYU in 1999 with a B.A. in mathematics and a minor in computer science. She earned her Ph.D. in 2003 from the Massachusetts Institute of Technology under the supervision of Alan Edelman, with a thesis on \"Eigenvalue statistics for beta-ensembles\". After postdoctoral research as a Miller Research Fellow at the University of California, Berkeley, she joined the faculty of the University of Washington in 2006.\n\nDumitriu won the Alice T. Schafer prize for excellence in mathematics by an undergraduate woman in 1996, and the Leslie Fox Prize for Numerical Analysis (given to a young numerical analysis researcher who excels both mathematically and in presentation skills) in 2007. In 2009 she received a CAREER Award from the National Science Foundation. In 2012, she became one of the inaugural fellows of the American Mathematical Society.\n\nIn 1996, as a sophomore at New York University, Dumitriu became the first woman to become a Putnam Fellow, meaning that she earned one of the top five scores at the William Lowell Putnam Mathematical Competition. In 1995, 1996, and 1997 she won the Elizabeth Lowell Putnam Award that is given to the top woman in the contest, a record that was not matched until ten years later when Alison Miller also won the same award in three consecutive years.\n\n\n"}
{"id": "54328391", "url": "https://en.wikipedia.org/wiki?curid=54328391", "title": "Jacobi transform", "text": "Jacobi transform\n\nIn mathematics, Jacobi transform is an integral transform named after the mathematician Carl Gustav Jacob Jacobi, which uses Jacobi polynomials formula_1 as kernels of the transform.\n\nThe Jacobi transform of a function formula_2 is\n\nThe inverse Jacobi transform is given by\n"}
{"id": "195351", "url": "https://en.wikipedia.org/wiki?curid=195351", "title": "Jacobian matrix and determinant", "text": "Jacobian matrix and determinant\n\nIn vector calculus, the Jacobian matrix (, ) is the matrix of all first-order partial derivatives of a vector-valued function. When the matrix is a square matrix, both the matrix and its determinant are referred to as the Jacobian in literature.\n\nSuppose is a function which takes as input the vector and produces as output the vector . Then the Jacobian matrix of is an matrix, usually defined and arranged as follows:\n\nor, component-wise:\n\nThis matrix, whose entries are functions of , is also denoted by , , and . (Note that some literature defines the Jacobian as the transpose of the matrix given above.)\n\nThe Jacobian matrix is important because if the function is differentiable at a point (this is a slightly stronger condition than merely requiring that all partial derivatives exist at ), then the Jacobian matrix defines a linear map , which is the best (pointwise) linear approximation of the function near the point . This linear map is thus the generalization of the usual notion of derivative, and is called the \"derivative\" or the \"differential\" of at .\n\nIf = , the Jacobian matrix is a square matrix, and its determinant, a function of , is the Jacobian determinant of . It carries important information about the local behavior of . In particular, the function has locally in the neighborhood of a point an inverse function that is differentiable if and only if the Jacobian determinant is nonzero at (see Jacobian conjecture). The Jacobian determinant also appears when changing the variables in multiple integrals (see substitution rule for multiple variables).\n\nIf = 1, is a scalar field and the Jacobian matrix is reduced to a row vector of partial derivatives of —i.e. the transpose of the gradient of , when denoted as column vector with respect to the ordered basis formula_3.\n\nThese concepts are named after the mathematician Carl Gustav Jacob Jacobi (1804–1851).\n\nThe Jacobian generalizes the gradient of a scalar-valued function of multiple variables, which itself generalizes the derivative of a scalar-valued function of a single variable. In other words, the Jacobian for a scalar-valued multivariate function is the gradient and that of a scalar-valued function of single variable is simply its derivative. The Jacobian can also be thought of as describing the amount of \"stretching\", \"rotating\" or \"transforming\" that a transformation imposes locally. For example, if is used to transform an image, the Jacobian , describes how the image in the neighborhood of is transformed.\n\nIf a function is differentiable at a point, its derivative is given in coordinates by the Jacobian, but a function does not need to be differentiable for the Jacobian to be defined, since only the partial derivatives are required to exist.\n\nIf is a point in and is differentiable at , then its derivative is given by . In this case, the linear map described by is the best linear approximation of near the point , in the sense that\n\nfor close to and where is the little o-notation (for ) and is the distance between and . (See Total derivative#The total derivative as a linear map.)\n\nCompare this to a Taylor series for a scalar function of a scalar argument, truncated to first order:\n\nIn a sense, both the gradient and Jacobian are \"first derivatives\"—the former the first derivative of a \"scalar function\" of several variables, the latter the first derivative of a \"vector function\" of several variables.\n\nThe Jacobian of the gradient of a scalar function of several variables has a special name: the Hessian matrix, which in a sense is the \"second derivative\" of the function in question.\n\nIf , then is a function from to itself and the Jacobian matrix is a square matrix. We can then form its determinant, known as the Jacobian determinant. The Jacobian determinant is sometimes referred to as \"the Jacobian\".\n\nThe Jacobian determinant at a given point gives important information about the behavior of near that point. For instance, the continuously differentiable function is invertible near a point if the Jacobian determinant at is non-zero. This is the inverse function theorem. Furthermore, if the Jacobian determinant at is positive, then preserves orientation near ; if it is negative, reverses orientation. The absolute value of the Jacobian determinant at gives us the factor by which the function expands or shrinks volumes near ; this is why it occurs in the general substitution rule.\n\nThe Jacobian determinant is used when making a change of variables when evaluating a multiple integral of a function over a region within its domain. To accommodate for the change of coordinates the magnitude of the Jacobian determinant arises as a multiplicative factor within the integral. This is because the -dimensional element is in general a parallelepiped in the new coordinate system, and the -volume of a parallelepiped is the determinant of its edge vectors.\n\nThe Jacobian can also be used to solve systems of differential equations at an equilibrium point or approximate solutions near an equilibrium point.\n\nAccording to the inverse function theorem, the matrix inverse of the Jacobian matrix of an invertible function is the Jacobian matrix of the \"inverse\" function. That is, if the Jacobian of the function is continuous and nonsingular at the point in , then is invertible when restricted to some neighborhood of and\n\nConversely, if the Jacobian determinant is not zero at a point, then the function is \"locally invertible\" near this point, that is, there is a neighbourhood of this point in which the function is invertible.\n\nThe (unproved) Jacobian conjecture is related to global invertibility in the case of a polynomial function, that is a function defined by \"n\" polynomials in \"n\" variables. It asserts that, if the Jacobian determinant is a non-zero constant (or, equivalently, that it does not have any complex zero), then the function is invertible and its inverse is a polynomial function.\n\nIf is a differentiable function, a \"critical point\" of is a point where the rank of the Jacobian matrix is not maximal. This means that the rank at the critical point is lower than the rank at some neighbour point. In other words, let be the maximal dimension of the open balls contained in the image of ; then a point is critical if all minors of rank of are zero.\n\nIn the case where , a point is critical if the Jacobian determinant is zero.\n\nConsider the function given by\nThen we have\nand\nand the Jacobian matrix of is\nand the Jacobian determinant is\n\nThe transformation from polar coordinates to Cartesian coordinates (\"x\", \"y\"), is given by the function with components:\n\nThe Jacobian determinant is equal to . This can be used to transform integrals between the two coordinate systems:\n\nThe transformation from spherical coordinates to Cartesian coordinates (\"x\", \"y\", \"z\"), is given by the function with components:\n\nThe Jacobian matrix for this coordinate change is\n\nThe determinant is . As an example, since this determinant implies that the differential volume element . Unlike for a change of Cartesian coordinates, this determinant is not a constant, and varies with coordinates ( and ).\n\nThe Jacobian matrix of the function with components\n\nis\n\nThis example shows that the Jacobian need not be a square matrix.\n\nThe Jacobian determinant of the function with components\n\nis\n\nFrom this we see that reverses orientation near those points where and have the same sign; the function is locally invertible everywhere except near points where or . Intuitively, if one starts with a tiny object around the point and apply to that object, one will get a resulting object with approximately times the volume of the original one, with orientation reversed.\n\nThe Jacobian serves as a linearized design matrix in statistical regression and curve fitting; see non-linear least squares.\n\nConsider a dynamical system of the form formula_21, where formula_22 is the (component-wise) derivative of formula_23 with respect to the evolution parameter formula_24 (time), and formula_25 is differentiable. If formula_26, then formula_27 is a stationary point (also called a steady state). By the Hartman–Grobman theorem, the behavior of the system near a stationary point is related to the eigenvalues of formula_28, the Jacobian of formula_29 at the stationary point. Specifically, if the eigenvalues all have real parts that are negative, then the system is stable near the stationary point, if any eigenvalue has a real part that is positive, then the point is unstable. If the largest real part of the eigenvalues is zero, the Jacobian matrix does not allow for an evaluation of the stability.\n\nA square system of coupled nonlinear equations can be solved iteratively by Newton's method. This method uses the Jacobian matrix of the system of equations.\n\nLet \"n\" = 2 so the Jacobian is a 2 × 2 real matrix. Suppose a surface diffeomorphism f: \"U\" → \"V\" in the neighborhood of \"p\" in \"U\" is written formula_30 The matrix formula_31 can be interpreted as a complex number: ordinary, split, or dual. Furthermore, since formula_31 is invertible, the complex number has a polar decomposition or an alternative planar decomposition.\n\nAnd again, each such complex number represents a group action on the tangent plane at \"p\". The action is dilation by the norm of the complex number, and rotation respecting angle, hyperbolic angle, or slope, according to the case of formula_33 Such action corresponds to a conformal mapping.\n\n\n"}
{"id": "53605032", "url": "https://en.wikipedia.org/wiki?curid=53605032", "title": "Jean Gallier", "text": "Jean Gallier\n\nJean Henri Gallier (born 1949) is a researcher in computational logic at the University of Pennsylvania, where he holds appointments in the Computer and Information Science Department and the Department of Mathematics.\n\nGallier was born January 5, 1949 in Nancy, France, and holds dual French and American citizenship. He earned his baccalauréat at the Lycée de Sèvres in 1966, and a degree in civil engineering at the École Nationale des Ponts et Chaussées in 1972.\nHe then moved to the University of California, Los Angeles for his graduate studies, earning a Ph.D. in computer science in 1978 under the joint supervision of Sheila Greibach and Emily Perlinski Friedman. His dissertation was entitled \"Semantics and Correctness of Classes of Deterministic and Nondeterministic Recursive Programs\".\nAfter postdoctoral study at the University of California, Santa Barbara, he joined the University of Pennsylvania Department of Computer and Information Science in 1978. At Pennsylvania, he was promoted to full professor in 1990, gained a secondary appointment to the Department of Mathematics in 1994, and directed the French Institute of Culture and Technology from 2001 to 2004.\n\nGallier's most heavily cited research paper, with his student William F. Dowling, gives a linear time algorithm for Horn-satisfiability.\nThis is a variant of the Boolean satisfiability problem: its input is a Boolean formula in conjunctive normal form with at most one positive literal per clause, and the goal is to assign truth values to the variables of the formula to make the whole formula true. Solving Horn-satisfiability problems is the central computational paradigm in the Prolog programming language.\n\nGallier is also the author of five books in computational logic,\ncomputational geometry,\nlow-dimensional topology,\nand discrete mathematics.\n\n"}
{"id": "8625257", "url": "https://en.wikipedia.org/wiki?curid=8625257", "title": "Joan Birman", "text": "Joan Birman\n\nJoan Sylvia Lyttle Birman (born May 30, 1927 in New York City) is an American mathematician, specialising in low-dimensional topology. She has given contributions to the study of knots, 3-manifolds, mapping class groups of surfaces, geometric group theory, contact structures and dynamical systems. Birman is currently Research Professor Emerita at Barnard College, Columbia University, where she has been since 1973.\n\nHer parents were George and Lillian Lyttle, both Jewish immigrants. Her father was from Russia but grew up in Liverpool, England. Her mother was born in New York and her parents were Russian-Polish immigrants. At age 17, George emigrated to the US and became a successful dress manufacturer. He appreciated the opportunities from having a business but he wanted his daughters to focus on education. She has three children. Her late husband, Joseph Birman, was a physicist and a leading advocate for human rights for scientists.\n\nAfter high school, Birman entered Swarthmore College, a coeducational institution in Swarthmore, Pennsylvania, and majored in mathematics. However, she disliked living in the dorms so she transferred to Barnard College, a women's only college affiliated to Columbia University, to live at home.\n\nBirman received her B.A. (1948) in mathematics from Barnard College and an M.A. (1950) in physics from Columbia University.\nAfter working in the industry from 1950 to 1960, she did a PhD in mathematics at the Courant Institute (NYU) under the supervision of Wilhelm Magnus, graduating in 1968. Her dissertation was titled \"Braid groups and their relationship to mapping class groups\".\n\nBirman's first position was at the Stevens Institute of Technology (1968–1973). She also was a visiting professor at Princeton University during part of this period. In 1973, she joined the faculty at Barnard College.\nIn 1987 she was selected by the Association for Women in Mathematics to be a Noether Lecturer; this lecture honors women who have made fundamental and sustained contributions to the mathematical sciences. She was a visiting scholar at the Institute for Advanced Study in the summer of 1988. She has also been a Sloan Foundation Fellow (1974–76) and a Guggenheim Foundation Fellow (1994–95). In 1996, she won the Chauvenet Prize. Then in 2005, she won the New York City Mayor's Award for Excellence in Science and Technology.\n\nIn 2012, she became a fellow of the American Mathematical Society. She supervised 21 doctoral students, and has a total of 50 academic descendants. Her doctoral students include Józef Przytycki.\n\nIn 2017, she endowed the Joan and Joseph Birman Fellowship for Women Scholars at the American Mathematical Society to support mathematical research by mid-career women.\n\nBirman's scientific work includes 77 research publications and 16 expository articles or reviews. She is the author of the research monograph \"Braids, Links, and Mapping Class Groups.\" \n\n\n\n\n"}
{"id": "55633571", "url": "https://en.wikipedia.org/wiki?curid=55633571", "title": "L. Gustave du Pasquier", "text": "L. Gustave du Pasquier\n\nLouis-Gustave du Pasquier (18 August 1876, Auvernier – 31 January 1957, Cornaux) was a Swiss mathematician and historian of mathematics and mathematical sciences.\n\nDu Pasquier studied at l'École Polytechnique, the University of Zurich, La Sorbonne, the Collège de France, and the Collège Libre des Sciences Sociales. He received his doctorate in 1906 from the University of Zurich with dissertation \"Zahlentheorie der Tettarionen\" under the supervision of Adolf Hurwitz. Du Pasquier then taught at La Chaux-de-Fonds, Kusnacht, Frauenfeld, Winterthur, and Zurich before he became in 1911 a professor at the University of Neuchâtel. Du Pasquier wrote more than 60 articles published in scientific journals. He did research on number theory, probability theory, relativity theory, astronomy, and actuarial science. He edited the 7th volume of the collected works of Leonhard Euler.\n\nDu Pasquier was an Invited Speaker of the ICM in 1920 at Strasbourg, in 1924 at Toronto, in 1928 at Bologna, and in 1932 at Zurich.\n\n"}
{"id": "25077398", "url": "https://en.wikipedia.org/wiki?curid=25077398", "title": "Lagrangian coherent structure", "text": "Lagrangian coherent structure\n\nLagrangian coherent structures (LCSs) are distinguished surfaces of trajectories in a dynamical system that exert a major influence on nearby trajectories over a time interval of interest. The type of this influence may vary, but it invariably creates a coherent trajectory pattern for which the underlying LCS serves as a theoretical centerpiece. In observations of tracer patterns in nature, one readily identifies coherent features, but it is often the underlying structure creating these features that is of interest.\n\nAs illustrated on the right, individual tracer trajectories forming coherent patterns are generally sensitive with respect to changes in their initial conditions and the system parameters. In contrast, the LCSs creating these trajectory patterns turn out to be robust and provide a simplified skeleton of the overall dynamics of the system. The robustness of this skeleton makes LCSs ideal tools for model validation, model comparison and benchmarking. LCSs can also be used for now-casting and even short-term forecasting of pattern evolution in complex dynamical systems.\n\nPhysical phenomena governed by LCSs include floating debris, oil spills, surface drifters and chlorophyll patterns in the ocean; clouds of volcanic ash and spores in the atmosphere; and coherent crowd patterns formed by humans and animals.\n\nWhile LCSs generally exist in any dynamical system, their role in creating coherent patterns is perhaps most readily observable in fluid flows. The images below are examples of how different types of LCSs hidden in geophysical flows shape tracer patterns.\n\nOn a phase space formula_1 and over a time interval formula_2 , consider a non-autonomous dynamical system defined through the flow map formula_3, mapping initial conditions formula_4 into their position formula_5 for any time formula_6. If the flow map formula_7 is a diffeomorphism for any choice of formula_8, then for any smooth set formula_9 of initial conditions in formula_1, the set\n\nis an invariant manifold in the extended phase space formula_12. Borrowing terminology from fluid dynamics, we refer to the evolving time slice formula_13 of the manifold formula_14 as a material surface (see Fig. 1). Since any choice of the initial condition set formula_9 yields an invariant manifold formula_16, invariant manifolds and their associated material surfaces are abundant and generally undistinguished in the extended phase space. Only few of them will act as cores of coherent trajectory patterns.\n\nIn order to create a coherent pattern, a material surface formula_17 should exert a sustained and consistent action on nearby trajectories throughout the time interval formula_18. Examples of such action are attraction, repulsion, or shear. In principle, any well-defined mathematical property qualifies that creates coherent patterns out of randomly selected nearby initial conditions.\n\nMost such properties can be expressed by strict inequalities. For instance, we call a material surface formula_17 \"attracting\" over the interval formula_20 if all small enough initial perturbations to formula_9 are carried by the flow into even smaller final perturbations to formula_22. In classical dynamical systems theory, invariant manifolds satisfying such an attraction property over infinite times are called attractors. They are not only special, but even locally unique in the phase space: no continuous family of attractors may exist.\n\nIn contrast, in dynamical systems defined over a finite time interval formula_18, strict inequalities do not define \"exceptional\" (i.e., locally unique) material surfaces. This follows from the continuity of the flow map formula_7 over formula_18 . For instance, if a material surface formula_17 attracts all nearby trajectories over the time interval formula_18, then so will any sufficiently close other material surface.\n\nThus, attracting, repelling and shearing material surfaces are necessarily stacked on each other, i.e., occur in continuous families. This leads to the idea of seeking LCSs in finite-time dynamical systems as \"exceptional\" material surfaces that exhibit a coherence-inducing property \"more strongly\" than any of the neighboring material surfaces. Such LCSs, defined as extrema (or more generally, stationary surfaces) for a finite-time coherence property, will indeed serve as observed centerpieces of trajectory patterns. Examples of attracting, repelling and shearing LCSs are in a direct numerical simulation of 2D turbulence are shown in Fig.2a.\n\nClassical invariant manifolds are invariant sets in the phase space formula_1 of an autonomous dynamical system. In contrast,\nLCSs are only required to be invariant in the extended phase space. This means that even if the underlying dynamical system is autonomous, the LCSs of the system over the interval formula_29 will generally be time-dependent, acting as the evolving skeletons of observed coherent trajectory patterns. Figure 2b shows the difference between an attracting LCS and a classic unstable manifold of a saddle point, for evolving times, in an autonomous dynamical system.\n\nAssume that the phase space of the underlying dynamical system is the material configuration space of a continuum, such as a fluid or a deformable body. For instance, for a dynamical system generated by an unsteady velocity field\n\nthe open set formula_31 of possible particle positions is a material configuration space. In this space, LCSs are material surfaces, formed by trajectories. Whether or not a material trajectory is contained in an LCS is a property that is independent of the choice of coordinates, and hence cannot depend of the observer. As a consequence, LCSs are subject to the basic objectivity (material frame-indifference) requirement of continuum mechanics. The objectivity of LCSs requires them to be invariant with respect to all possible observer changes, i.e., linear coordinate changes of the form\n\nwhere formula_33 is the vector of the transformed coordinates; formula_34 is an arbitrary formula_35 proper orthogonal matrix representing time-dependent rotations; and formula_36 is an arbitrary formula_37-dimensional vector representing time-dependent translations. As a consequence, any self-consistent LCS definition or criterion should be expressible in terms of quantities that are frame-invariant. For instance, the strain rate formula_38 and the spin tensor formula_39 defined as\n\ntransform under Euclidean changes of frame into the quantities\n\nA Euclidean frame change is, therefore, equivalent to a similarity transform for formula_38, and hence an LCS approach depending only on the eigenvalues and eigenvectors of formula_38 is automatically frame-invariant. In contrast, an LCS approach depending on the eigenvalues of formula_39 is generally not frame-invariant.\n\nA number of frame-independent quantities, such as formula_45, formula_46, formula_47, as well as the averages or eigenvalues of these quantities, are routinely used in heuristic LCS detection. While such quantities may effectively mark features of the instantaneous velocity field formula_48, the ability of these quantities to capture material mixing, transport, and coherence is limited and a priori unknown in any given frame. As an example, consider the linear unsteady fluid particle motion\n\nwhich is an exact solution of the two-dimensional Navier–Stokes equations. The (frame-dependent) Okubo-Weiss criterion classifies the whole domain in this flow as elliptic (vortical) because formula_50 holds, with formula_51 referring to the Euclidean matrix norm. As seen in Fig. 3, however, trajectories grow exponentially along a rotating line and shrink exponentially along another rotating line. In material terms, therefore, the flow is hyperbolic (saddle-type) in any frame.\n\nSince Newton’s equation for particle motion and the Navier–Stokes equations for fluid motion are well known to be frame-dependent, it might first seem counterintuitive to require frame-invariance for LCSs, which are composed of solutions of these frame-dependent equations. Recall, however, that the Newton and Navier–Stokes equations represent objective physical principles for material particle trajectories. As long as correctly transformed from one frame to the other, these equations generate physically the same material trajectories in the new frame. In fact, we decide how to transform the equations of motion from an formula_52-frame to a formula_53-frame through a coordinate change formula_54 precisely by upholding that trajectories are mapped into trajectories, i.e., by requiring formula_55 to hold for all times. Temporal differentiation of this identity and substitution into the original equation in the formula_52-frame then yields the transformed equation in the formula_53-frame. While this process adds new terms (inertial forces) to the equations of motion, these inertial terms arise precisely to ensure the invariance of material trajectories. Fully composed of material trajectories, LCSs remain invariant in the transformed equation of motion defined in the formula_53-frame of reference. Consequently, any self-consistent LCS definition or detection method must also be frame-invariant.\n\nMotivated by the above discussion, the simplest way to define an attracting LCS is by requiring it to be a locally strongest attracting material surface in the extended phase space formula_12 (see. Fig. 4) . Similarly, a repelling LCS can be defined as a locally strongest repelling material surface. Attracting and repelling LCSs together are usually referred to as hyperbolic LCSs, as they provide a finite-time genearalization of the classic concept of normally hyperbolic invariant manifolds in dynamical systems.\n\nHeuristically, one may seek initial positions formula_60 of repelling LCSs as set of initial conditions at which infinitesimal perturbations to trajectories starting from formula_60 grow locally at the highest rate relative to trajectories starting off of formula_62. The heuristic element here is that instead of constructing a highly repelling material surface, one simply seeks points of large particle separation. Such a separation may well be due to strong shear along the set of points so identified; this set is not at all guaranteed to exert any normal repulsion on nearby trajectories.\n\nThe growth of an infinitesimal perturbation formula_63 along a trajectory formula_64 is governed by the flow map gradient formula_47. Let formula_66 be a small perturbation to the initial condition formula_67, with formula_68, and with formula_69 denoting an arbitrary unit vector in formula_70. This perturbation generally grows along the trajectory formula_64 into the perturbation vector formula_72. Then the maximum relative stretching of infinitesimal perturbations at the point formula_67 can be computed as\n\nwhere formula_75 denotes the right Cauchy–Green strain tensor. One then concludes that the maximum relative stretching experienced along a trajectory starting from formula_67 is just formula_77. As this relative stretching tends to grow rapidly, it is more convenient to work with its growth exponent formula_78, which is then precisely the finite-time Lyapunov exponent (FTLE)\n\nTherefore, one expects hyperbolic LCSs to appear as codimension-one local maximizing surfaces (or \"ridges\") of the FTLE field.\nThis expectation turns out to be justified in the majority of cases: time formula_80 positions of repelling LCSs are marked by ridges of formula_81. By applying the same argument in backward time,\nwe obtain that time formula_82 positions of attracting LCSs are marked by ridges of the backward FTLE field formula_83.\n\nThe classic way of computing Lyapunov exponents is solving a linear differential equation for the linearized flow map formula_84. A more expedient approach is to compute the FTLE field from a simple finite-difference approximation to the deformation gradient.\nFor example, in a three-dimenisonal flow, we launch a trajectory formula_85 from any element formula_67 of a grid of initial conditions. Using the coordinate representation formula_87 for the evolving trajectory formula_85, we approximate the gradient of the flow map as\n\nwith a small vector formula_90 pointing in the formula_91 coordinate direction. For two-dimensional flows, only the first formula_92 minor matrix of the above matrix is relevant.\n\nFTLE ridges have proven to be a simple and efficient tool for the visualize hyperbolic LCSs in a number of physical problems, yielding intriguing images of initial positions of hyperbolic LCSs in different applications (see, e.g., Figs. 5a-b). However, FTLE ridges obtained over sliding time windows formula_93 do not form material surfaces. Thus, ridges of formula_94 under varying formula_95 cannot be used to \"define\" Lagrangian objects, such as hyperbolic LCSs. Indeed, a locally strongest repelling material surface over formula_96 will generally not play the same role over formula_97 and hence its evolving position at time formula_98 will not be a ridge for formula_99. Nonetheless, evolving second-derivative FTLE ridges computed over sliding intervals of the form formula_97 have been identified by some authors broadly with LCSs. In support of this identification, it is also often argued that the material flux over such sliding-window FTLE ridges should necessarily be small.\n\nThe \"FTLE ridge=LCS\" identification, however, suffers form the following conceptual and mathematical problems:\n\nThe local variational theory of hyperbolic LCSs builds on their original definition as strongest repelling or repelling material surfaces in the flow over the time interval formula_103. At an initial point formula_104 , let formula_105\ndenote a unit normal to an initial material surface formula_106 (cf. Fig. 6). By the invariance of material lines, the tangent space formula_107 is mapped into the tangent space of formula_108\nby the linearized flow map formula_109. At the same time, the image of the normal formula_110 normal under formula_109 generally does not remain normal to formula_112.\nTherefore, in addition to a normal component of length formula_113, the advected normal also develops a tangential component of length formula_114 (cf. Fig. 7).\n\nIf formula_115, then the evolving material surface formula_116 strictly repels nearby trajectories by the end of the time interval formula_103. Similarly, formula_118\nsignals that formula_116 strictly attracts nearby trajectories along its normal directions. A repelling (attracting) LCS over the interval formula_103 can be defined as a material surface formula_116 whose net repulsion formula_122 is pointwise maximal (minimal) with respect to perturbations\nof the initial normal vector field formula_123. As earlier, we refer to repelling and attracting LCSs collectively as hyperbolic LCSs.\n\nSolving these local extremum principles for hyperbolic LCSs in two and three dimensions yields unit normal vector fields to which hyperbolic LCSs should everywhere be tangent. The existence of such normal surfaces also requires a Frobenius-type integrability condition in the three-dimensional case. All these results can be summarized as follows:\n\nRepelling LCSs are obtained as most repelling shrink lines, starting from local maxima of formula_124. Attracting LCSs are obtained as most attracting stretch lines, starting from local minima of formula_125. These starting points serve are initial positions of exceptional saddle-type trajectories in the flow. An example of the local variational computation of a repelling LCS is shown in FIg. 8. The computational algorithm is available in LCS Tool.\nIn 3D flows, instead of solving the Frobenius PDE (see table above) for hyperbolic LCSs, an easier approach is to construct intersections of hyperbolic LCSs with select 2D planes, and fit a surface numerically to a large number of such intersection curves. Let us denote the unit normal of a 2D plane formula_126 by formula_127. The intersection curve of a 2D repelling LCS surface with the plane formula_126 is normal to both formula_127 and to the unit normal formula_130 of the LCS. As a consequence, an intersection curve formula_131 satisfies the ODE\n\nwhose trajectories we refer to as reduced shrink lines. (Strictly speaking, this equation is not an ordinary differential equation, given that its right-hand side is not a vector field, but a direction field, which is generally not globally orientable). Intersections of hyperbolic LCSs with formula_126 are fastest contracting reduced shrink lines. Determining such shrink lines in a smooth family of nearby formula_126 planes, then fitting a surface to the curve family so obtained yields a numerical approximation of a 2D repelling LCS.\n\nA general material surface experiences shear and strain in its deformation, both of which depend continuously on initial conditions by the continuity of the map formula_135.\nThe averaged strain and shear within a strip of formula_136-close material lines, therefore, typically show formula_136 variation within such a strip.\nThe two-dimensional geodesic theory of LCSs seeks exceptionally coherent locations where this general trend fails, resulting in an order of magnitude smaller variability in shear or strain than what is normally expected across an formula_136 strip. Specifically, the geodesic theory searches for LCSs as special material lines around which formula_136 material strips show no formula_136 variability either in the material-line\naveraged shear (Shearless LCSs) or in the material-line averaged strain (Strainless or Elliptic LCSs). Such LCSs turn out to be null-geodesics of appropriate metric tensors defined by the deformation field—hence the name of this theory.\n\nShearless LCSs are found to be null-geodesics of a Lorentzian metric tensor formula_141 defined as\n\nSuch null-geodesics can be proven to be tensorlines of the Cauchy–Green strain tensor, i.e., are tangent to the direction field formed by the strain eigenvector fields formula_143. Specifically, repelling LCSs are trajectories of formula_144 starting from local maxima of the formula_145 eigenvalue field. Similarly, attracting LCSs are trajectories of formula_146 starting from local minims of the formula_147 eigenvalue field. This agrees with the conclusion of the local variational theory of LCSs. The geodesic approach, however, also sheds more light on the robustness of hyperbolic LCSs: hyperbolic LCSs only prevail as stationary curves of the averaged shear functional under variations that leave their endpoints fixed. This is to be contrasted with parabolic LCSs (see below), which are also shearless LCSs but prevail as stationary curves to the shear functional even under arbitrary variations. As a consequence, individual trajectories are objective, and statements about the coherent structures they form should also be objective.\n\nA sample application is shown in Fig. 9, where the sudden appearance of a hyperbolic core (strongest attracting part of a stretchline) within the oil spill caused the \"notable Tiger-Tail instability\" in the shape of the oil spill.\n\nElliptc LCSs are closed and nested material surfaces that act as building blocks of the Lagrangian equivalents of vortices, i.e., rotation-dominated regions of trajectories that generally traverse the phase space without substantial stretching or folding. They mimic the behavior of Kolmogorov–Arnold–Moser (KAM) tori that form elliptic regions is Hamiltonian systems. There coherence can be approached either through their homogeneous material rotation or through their homogeneous stretching properties.\n\nAs a simplest approach to rotational coherence, one may define an elliptic LCS as a tubular material surface along which small material volumes complete the same net rotation over the time intervall formula_148 of interest.\nA challenge in that in each material volume element, all individual material fibers (tangent vectors to trajectories) perform different rotations.\n\nTo obtain a well-defined bulk rotation for each material element, one may employ the unique left and right polar decompositions of the flow gradient in the form\n\nwhere the proper orthogonal tensor formula_150 is called the rotation tensor and the symmetric, positive definite tensors formula_151 are called the left stretch tensor and right stretch tensor, respectively.\n\nSince the Cauchy–Green strain tensor can be written as\nthe local material straining described by the eigenvalues and eigenvectors of formula_153 are fully captured by the singular values and singular vectors of the stretch tensors. The remaining factor in the deformation gradient is represented by formula_150, interpreted as the bulk solid-body rotation component of volume elements. In planar motions, this rotation is defined relative to the normal of the plane. In three dimensions, the rotation is defined relative to the axis defined by the eigenvector of formula_150 corresponding to its unit eigenvalue. In higher-dimensional flows, the rotation tensor cannot be viewed as a rotation about a single axis.\nIn two and three dimensions, therefore, there exists a polar rotation angle (PRA) formula_156 that characterises the material rotation generated by formula_150 for a volume element centered at the initial condition formula_67. This PRA is well-defined up to multiples of formula_159. For two-dimensional flows, the PRA can be computed from the invariants of formula_153 using the formulas\n\nwhich yield a four-quadrant version of the PRA via the formula\n\nFor three-dimensional flows, the PRA can again be computed from the invariants of formula_153 from the formulas\n\nwhere formula_165 is the Levi-Civita symbol, formula_166 is the eigenvector corresponding to the unit eigenvector of the matrix formula_167.\n\nThe time formula_80 positions of elliptic LCSs are visualized as tubular level sets of the PRA distribution formula_169. In two-dimensions, therefore, (polar) elliptic LCSs are simply closed level curves of the PRA, which turn out to be objective. In three dimensions, (polar) elliptic LCSs are toroidal or cylindrical level surfaces of the PRA, which are, however, not objective and hence will generally change in rotating frames. Coherent Lagrangian vortex boundaries can be visualized as outermost members of nested families of elliptic LCSs. Two- and three-dimensional examples of elliptic LCS revealed by tubular level surfaces of the PRA are shown in Fig. 10a-b.\n\nThe level sets of the PRA are objective in two dimensions but not in three dimensions. An additional shortcoming of the polar rotation tensor is its dynamical inconsistency: polar rotations computed over adjacent sub-intervals of a total deformation do not sum up to the rotation computed for the full-time interval of the same deformation. Therefore, while formula_170 is the closest rotation tensor to formula_171 in the formula_172 norm over a fixed time interval formula_173, these piecewise best fits do not form a family of rigid-body rotations as formula_174 and formula_175 are varied. For this reason, rotations predicted by the polar rotation tensor over varying time intervals divert from the experimentally observed mean material rotation of fluid elements.\n\nAn alternative to the classic polar decomposition provides a resolution to both the non-objectivity and the dynamic inconsistency issue. Specifically, the Dynamic Polar Decomposition (DPD) of the deformation gradient is also of the form\n\nwhere the proper orthogonal tensor formula_177 is the dynamic rotation tensor and the non-singular tensors formula_178 are the left dynamic stretch tensor and right dynamic stretch tensor, respectively. Just as the classic polar decomposition, the DPD is valid in any finite dimension. Unlike the classic polar decomposition, however, the dynamic rotation and stretch tensors are obtained from solving linear differential equations, rather than from matrix manipulations. In particular, formula_179 is the deformation gradient of the purely rotational flow\n\nand formula_181 is the deformation gradient of the purely straining flow\n\nThe dynamic rotation tensor formula_183 can further be factorized into two deformation gradients: one for a spatially uniform (rigid-body) rotation, and one that deviates from this uniform rotation:\n\nAs a spatially independent rigid-body rotation, the proper orthogonal relative rotation tensor formula_185 is dynamically consistent, serving as the deformation gradient of the relative rotation flow\n\nIn contrast, the proper orthogonal mean rotation tensor formula_187 is the deformation gradient of the mean-rotation flow\n\nThe dynamic consistency of formula_189 implies that the total angle swept by formula_189 around its own axis of rotation is dynamically consistent. This intrinsic rotation angle formula_191 is also objective, and turns out to equal to one half of the Lagrangian-averaged vorticity deviation (LAVD). The LAVD is defined as the trajectory-averaged magnitude of the deviation of the vorticity from its spatial mean. With the vorticity formula_192 and its spatial mean\n\nthe LAVD over a time interval formula_103 therefore takes the form\n\nwith formula_196 denoting the (possibly time-varying) domain of definition of the velocity field formula_197. This result applies both in two- and three dimensions, and enables the computation of a well-defined, objective and dynamically consistent material rotation angle along any trajectory.\nOutermost complex tubular level curves of the LAVD define initial positions of rotationally coherent material vortex boundaries in two-dimensional unsteady flows (see Fig. 11a). By construction, these boundaries may exhibit transverse filamentation, but any developing filament keeps rotating with the boundary, without global transverse departure form the material vortex. (Exceptions are inviscid flows where such a global departure of LAVD level surfaces from a vortex is possible as fluid elements preserve their material rotation rate for all times). Remarkably, centers of rotationally coherent vortices (defined by local maxima of the LAVD field) can be proven to be the observed centers of attraction or repulsion for finite-size (inertial) particle motion in geophysical flows (see Fig. 11b). In three-dimensional flows, tubular level surfaces of the LAVD define initial positions of two-dimensional eddy boundary surfaces (see Fig. 11c) that remain rotationally coherent over a time intcenter|erval formula_198 (see Fig. 11d).\n\nThe local variational theory of elliptic LCSs targets material surfaces that locally maximize material shear over the finite time interval formula_103 of interest. This means that at initial point each point formula_200 of an elliptic LCS formula_17, the tangent space formula_202 is the plane along which the local Lagrangian shear formula_203 is maximal (cf. Fig 7).\n\nIntroducing the two-dimensional shear vector field\n\nand the three-dimensional shear normal vector field\n\nthe criteria for two- and three-dimensional elliptic LCSs can be summarized as follows:\n\nFor 3D flows, as in the case of hyperbolic LCSs, solving the Frobenius PDE can be avoided. Instead, one can construct intersections of a tubular elliptic LCS with select 2D planes, and fit a surface numerically to a large number of these intersection curves. As for hyperbolic LCSs above, let us denote the unit normal of a 2D plane formula_126 by formula_127. Again, the intersection curves of elliptic LCSs with the plane formula_126 are normal to both formula_127 and to the unit normal formula_210 of the LCS. As a consequence, an intersection curve formula_131 satisfies the reduced shear ODE\n\nwhose trajectories we refer to as reduced shear lines. (Strictly speaking, the reduced shear ODE is not an ordinary differential equation, given that its right-hand side is not a vector field, but a direction field, which is generally not globally orientable). Intersections of tubular elliptic LCSs with formula_126 are limit cycles of the reduced shear ODE. Determining such limit cycles in a smooth family of nearby formula_126 planes, then fitting a surface to the limit cycle family yields a numerical approximation for 2D shear surface. A three-dimensional example of this local variational computation of an elliptic LCS is shown in Fig. 11.\nAs noted above under hyperbolic LCSs, a global variational approach has been developed in two dimensions to capture elliptic LCSs as closed stationary curves of the material-line-averaged Lagrangian strain functional. Such curves turn out to be closed null-geodesics of the generalized Green–Lagrange strain tensor family formula_215, where formula_216 is a positive parameter (Lagrange multiplier). The closed null-geodesics can be shown to coincide with limit cycles of the family of direction fields\n\nformula_217\n\nNote that for formula_218, the direction field formula_219 coincides with the direction field formula_220 for shearlines obtained above from the local variational theory of LCSs.\n\nTrajectories of formula_221 are referred to as formula_222-lines. Remarkably, they are initial positions of material lines that are \"infinitesimally uniformly stretching\" under the flow map formula_223. Specifically, any subset of a formula_222-line is stretched by a factor of formula_222 between the times formula_80 and formula_82. As an example, Fig. 13 shows elliptic LCSs identified as closed formula_222-lines within the Great Red Spot of Jupiter.\n\nParabolic LCSs are shearless material surfaces that delineate cores of jet-type sets of trajectories. Such LCSs are characterized by both low stretching (because they are inside a non-stretching structure), but also by low shearing (because material shearing is minimal in jet cores).\n\nSince both shearing and stretching are as low as possible along a parabolic LCS, one may seek initial positions of such material surfaces as trenches of the FTLE field formula_229. A geophysical example of a parabolic LCS (generalized jet core) revealed as a trench of the FTLE field is shown in Fig. 14a.\n\nIn two dimensions, parabolic LCSs are also solutions of the global shearless variational principle described above for hyperbolic LCSs. As such, parabolic LCSs are composed of shrink lines and stretch lines that represent geodesics of the Lorentzian metric tensor formula_230. In contrast to hyperbolic LCSs, however, parabolic LCSs satisfy more robust boundary conditions: they remain stationary curves of the material-line-averaged shear functional even under variations to their endpoints. This explains the high degree of robustness and observability that jet cores exhibit in mixing. This is to be contrasted with the highly sensitive and fading footprint of hyperbolic LCSs away from strongly hyperbolic regions in diffusive tracer patterns.\n\nUnder variable endpoint boundary conditions, initial positions of parabolic LCSs turn out to be alternating chains of shrink lines and stretch lines that connect singularities of these line fields. These singularities occur at points where formula_231, and hence no infinitesimal deformation takes place between the two time instances formula_80 and formula_82. Fig. 14b shows an example of parabolic LCSs in Jupiter's atmosphere, located using this variational theory. The chevron-type shapes forming out of circular material blobs positioned along the jet core is characteristic of tracer deformation near parabolic LCSs. \n\nGeodesic computation of 2D hyperbolic and elliptic LCS:\n\n\nAutomated geodesic computation of 2D elliptic LCS:\n\n\nComputation of 2D and 3D rotational elliptic LCS:\n\n\nParticle advection and calculation:\n\n\n\n"}
{"id": "19635706", "url": "https://en.wikipedia.org/wiki?curid=19635706", "title": "Lattice graph", "text": "Lattice graph\n\nA lattice graph, mesh graph, or grid graph, is a graph whose drawing, embedded in some Euclidean space R, forms a regular tiling. This implies that the group of bijective transformations that send the graph to itself is a lattice in the group-theoretical sense.\n\nTypically, no clear distinction is made between such a graph in the more abstract sense of graph theory, and its drawing in space (often the plane or 3D space). This type of graph may more shortly be called just a lattice, mesh, or grid. Moreover, these terms are also commonly used for a finite section of the infinite graph, as in \"an 8×8 square grid\".\n\nThe term lattice graph has also been given in the literature to various other kinds of graphs with some regular structure, such as the Cartesian product of a number of complete graphs.\n\nA common type of a lattice graph (known under different names, such as square grid graph) is the graph whose vertices correspond to the points in the plane with integer coordinates, x-coordinates being in the range 1..., n, y-coordinates being in the range 1..., m, and two vertices are connected by an edge whenever the corresponding points are at distance 1. In other words, it is a unit distance graph for the described point set.\n\nA square grid graph is a Cartesian product of graphs, namely, of two path graphs with and edges. Since a path graph is a median graph, the latter fact implies that the square grid graph is also a median graph. All grid graphs are bipartite, which is easily verified by the fact that one can color the vertices in a checkerboard fashion.\n\nA path graph may also be considered to be a grid graph on the grid \"n\" times 1. A 2x2 grid graph is a 4-cycle.\n\nEvery planar graph \"H\" is a minor of the \"h\"×\"h\"-grid, where formula_1.\n\nA triangular grid graph is a graph that corresponds to a triangular grid.\n\nA Hanan grid graph for a finite set of points in the plane is produced by the grid obtained by intersections of all vertical and horizontal lines through each point of the set.\n\nThe rook's graph (the graph that represents all legal moves of the rook chess piece on a chessboard) is also sometimes called the lattice graph.\n\n"}
{"id": "1868210", "url": "https://en.wikipedia.org/wiki?curid=1868210", "title": "Leading zero", "text": "Leading zero\n\nA leading zero is any 0 digit that comes before the first nonzero digit in a number string in positional notation. For example, James Bond's famous identifier, 007, has two leading zeros. When leading zeros occupy the most significant digits of an integer, they could be left blank or omitted for the same numeric value. Therefore, the usual decimal notation of integers does not use leading zeros except for the zero itself, which would be denoted as an empty string otherwise. However, in decimal fractions strictly between −1 and 1, the leading zeros digits between the decimal point and the first nonzero digit are necessary for conveying the magnitude of a number and cannot be omitted, while trailing zeroes – zeroes occurring after the decimal point and after the last nonzero digit – can be omitted without changing the meaning.\n\nOften, leading zeros are found on non-electronic digital displays or on such electronic ones as seven-segment displays, that contain fixed sets of digits. These devices include manual counters, stopwatches, odometers, and digital clocks. Leading zeros are also generated by many older computer programs when creating values to assign to new records, accounts and other files, and as such are likely to be used by utility billing systems, human resources information systems and government databases. Many digital cameras and other electronic media recording devices use leading zeros when creating and saving new files to make names of the equal length.\n\nLeading zeros also present whenever the number of digits is technically fixed (such as in a memory register), but the stored value is not large enough to result in a non-zero most significant digit. The count leading zeros operation efficiently determines the number of leading zero bits in a machine word.\n\nA leading zero appears in roulette in the United States, where \"00\" is distinct from \"0\" (a wager on \"0\" will not win if the ball lands in \"00\", and vice versa). Sports where competitors are numbered follow this as well; a stock car numbered \"07\" would be considered distinct from one numbered \"7\". Benito Santiago, a Major League Baseball catcher who wore the number 09 for several years, is the only major professional sports league player to use a jersey number with a leading zero, not counting several who have worn the number 00. This is most common with single-digit numbers.\n\nLeading zeros are used to make ascending order of numbers correspond with alphabetical order: e.g., 11 comes alphabetically before 2, but after 02. (See, e.g., ISO 8601.) This does not work with negative numbers, though, whether leading zeroes are used or not: −23 comes alphabetically after −01, −1, and −22, although it is less than all of them. \n\nLeading zeros in a sentence also make it less likely that a careless reader will overlook the decimal point. For example, in modern pharmacy there is a widely followed convention that leading zeros must not be omitted from any dose or dosage value in drug prescribing. Likewise, just as leading zeros are mandatory, trailing zeros are forbidden. In both cases, the reason is the same: to prevent misreading and the resultant misdose by one or several orders of magnitude.\n\nLeading zeros can also be used to prevent fraud by filling in character positions that might normally be empty. For example, adding leading zeros to the amount of a check (or similar financial document) makes it more difficult for fraudsters to alter the amount of the check before presenting it for payment.\n\nA prefix codice_1 is used in C to specify string representations of octal numbers, as required by the ANSI C standard for the \"strtol\" function (the string to long integer converter) in the \"stdlib.h\" library. Many other programming languages, such as Python 2, Perl, Ruby, PHP, and the Unix shell bash also follow this specification for converting strings to numbers. As an example, \"codice_2\" does not represent 20 (2×10 + 0×10), but rather 20 = 16 (2×8 + 0×8 = 1×10 + 6×10). Decimal numbers written with leading zeros will be interpreted as octal by languages that follow this convention and will generate errors (not just unexpected results) if they contain \"8\" or \"9\", since these digits do not exist in octal. This behavior can be quite a nuisance when working with sequences of strings with embedded, zero-padded decimal numbers (typically file names) to facilitate alphabetical sorting (see above) or when validating inputs from users who would not know that adding a leading zero triggers this base conversion.\n\n"}
{"id": "53920590", "url": "https://en.wikipedia.org/wiki?curid=53920590", "title": "Mark Colyvan", "text": "Mark Colyvan\n\nMark Colyvan is an Australian philosopher and Professor of Philosophy at the University of Sydney. He is a former president of the Australasian Association of Philosophy. Colyvan is known for his research on philosophy of mathematics.\n\n\n"}
{"id": "11300890", "url": "https://en.wikipedia.org/wiki?curid=11300890", "title": "Meusnier's theorem", "text": "Meusnier's theorem\n\nIn differential geometry, Meusnier's theorem states that all curves on a surface passing through a given point \"p\" and having the same tangent line at \"p\" also have the same normal curvature at \"p\" and their osculating circles form a sphere. The theorem was first announced by Jean Baptiste Meusnier in 1776, but not published until 1785. At least prior to 1912, several writers in English were in the habit of calling the result \"Meunier's theorem\", although there is no evidence that Meusnier himself ever spelt his name in this way.\nThis alternative spelling of Meusnier's name also appears on the Arc de Triomphe in Paris.\n\n"}
{"id": "24392919", "url": "https://en.wikipedia.org/wiki?curid=24392919", "title": "Model-based specification", "text": "Model-based specification\n\nModel-based specification is an approach to formal specification where the system specification is expressed as a system state model. This state model is constructed using well-understood mathematical entities such as sets and functions. System operations are specified by defining how they affect the state of the system model.\n\nThe most widely used notations for developing model-based specifications are VDM and Z (pronounced Zed, not Zee). These notations are based on typed set theory. Systems are therefore modelled using sets and relations between sets.\n\nAnother well-known approach to formal specification is algebraic specification.\n\n"}
{"id": "25824647", "url": "https://en.wikipedia.org/wiki?curid=25824647", "title": "Moise's theorem", "text": "Moise's theorem\n\nIn geometric topology, a branch of mathematics, Moise's theorem, proved by Edwin E. Moise in , states that any topological 3-manifold has an essentially unique piecewise-linear structure and smooth structure.\n\nThe analogue of Moise's theorem in dimension 4 (and above) is false: there are topological 4-manifolds with no piecewise linear structures, and others with an infinite number of inequivalent ones.\n\n\n"}
{"id": "31096145", "url": "https://en.wikipedia.org/wiki?curid=31096145", "title": "NPSOL", "text": "NPSOL\n\nNPSOL is a software package that performs numerical optimization. It solves nonlinear constrained problems using the sequential quadratic programming algorithm. It was written in Fortran by Philip Gill of UCSD and Walter Murray, Michael Saunders and Margaret Wright of Stanford University. The name derives from a combination of NP for nonlinear programming and SOL, the Systems Optimization Laboratory at Stanford.\n\n"}
{"id": "46472280", "url": "https://en.wikipedia.org/wiki?curid=46472280", "title": "Natascha Artin Brunswick", "text": "Natascha Artin Brunswick\n\nNatascha Artin Brunswick, née Jasny (June 11, 1909 – February 3, 2003) was a German-American mathematician and photographer.\n\nNatascha Artin Brunswick was the daughter of , a Russian Jewish economist from Kharkiv. Her mother was a Russian orthodox aristocrat and dentist. Since at the time Russian orthodox Christians were prohibited from marrying Jews, she converted to Protestantism. They were married in Finland.\n\nNaum Jasny was an adherent of the Mensheviks and fled to Tbilisi after the October Revolution in 1917. Natascha, her sister, and her mother followed in 1920. After the Bolsheviks took control of Georgia, the family lived in Austria from 1922 to 1924, for a brief period in 1924 in Berlin, and finally moved to Langenhorn, Hamburg, where they remained until 1937. Natascha Jasny attended the progressive Lichtwark school. While still in school, she photographed with a simple box camera and processed her own pictures in the bathroom at home, which served as a makeshift darkroom.\n\nNatascha graduated in 1928. She hoped to study architecture at the Bauhaus Dessau, but the family's financial situation made this impossible. She instead studied mathematics at the University of Hamburg, where she also took courses in art history from Aby Warburg and Erwin Panofsky. She graduated from the university in 1930 with a Magister degree.\n\nOn August 29, 1929 she married her mathematics professor Emil Artin, who had been teaching in Hamburg since 1923. In 1933, the Artins had a daughter, Karin, and in 1934 a son, Michael.\n\nBecause his wife was half Jewish, Emil Artin was forced into early retirement from his teaching position under the Nazi Party Law for the Restoration of the Professional Civil Service. On September 27, 1934, Artin already had to sign a declaration that his wife was not \"Aryan\". The Artin family managed to leave Germany for the United States on October 21, 1937. Since they were prohibited from taking larger sums of money with them, the Artins shipped their entire household, which reflected their modernist sensibilities.\n\nNatascha's husband first obtained a teaching position at the University of Notre Dame, and in 1938 moved to Indiana University in Bloomington, Indiana. The Artins had their third child, son Thomas (Tom), in 1938. During World War II, Natascha Artin was classified an enemy alien. The United States Army nevertheless hired her in 1942 to teach Russian to soldiers under Army Specialized Training Program at Indiana University.\n\nIn 1946, Emil was hired by Princeton University, and the Artins moved to Princeton, New Jersey. They divorced in 1958, after which Emil Artin returned to Hamburg. Natasha Artin remarried in 1960. Her second husband was composer Mark Brunswick.\n\nArtin Brunswick returned to Hamburg as an official guest of the City of Hamburg in 1998, on the occasion of Emil Artin's 100th birthday. She lived in Princeton until her death in 2003.\n\nAfter her move to Princeton, Natascha Artin joined the group around Richard Courant at the mathematics department of New York University. She became the technical editor of the journal \"Communications on Pure and Applied Mathematics,\" founded at the Courant Institute of Mathematical Sciences in 1948, and in 1956 became the primary translation editor for the journal \"Theory of Probability and Its Applications,\" a position she held until 1989. In recognition of her long-standing membership of over 50 years, she was made an Honorary Member of the American Mathematical Society.\n\nArtin Brunswick never saw herself as a professional photographer. She considered it a \"private passion, nevertheless, it was a bit more than just taking snapshots.\"\n\nAfter they married in 1929, Emil Artin, who shared her passion for photography, gave her a Leica compact camera. She was encouraged in her photography by the painter Heinrich Stegemann, a family friend. She first took pictures of family members, friends, and landscapes, but later explored Hamburg and photographed scenes such as the Port of Hamburg, the Jungfernstieg, and the main railway station. She was particularly interested in architecture, and, influenced by the ideas of the Bauhaus, preferred clear, bright lines in her photographs.\n\nAs she was classified an enemy alien during World War II, her camera was provisionally confiscated by police in 1942. By the time it was returned to her, she had lost her passion for photography. Her prints from the time in Hamburg, however, survived through her emigration. Her son Tom rediscovered them about forty years later in a cabinet. He recognized their importance and contacted galleries in Hamburg. Artin Brunswick's photographs were first shown at the \"Kunstgenuss\" gallery in Hamburg-Eppendorf in 1999. In 2001, the Museum für Kunst und Gewerbe organized an exhibition of 227 original prints under the title \"Hamburg, As I Saw It. Photographs from the 1920s and 30s.\" Despite her advanced age of 91 years, Natascha Brunswick took the trip from New York to attend the opening. The museum now holds 230 original prints; the negatives are in the possession of the Artin family.\n"}
{"id": "32131629", "url": "https://en.wikipedia.org/wiki?curid=32131629", "title": "Partial cyclic order", "text": "Partial cyclic order\n\nIn mathematics, a partial cyclic order is a ternary relation that generalizes a cyclic order in the same way that a partial order generalizes a linear order.\n\nDirect sum\n\nDirect product\n\nPower\n\nDedekind–MacNeille completion\n\nlinear extension, Szpilrajn extension theorem\n\nstandard example\n\nThe relationship between partial and total cyclic orders is more complex than the relationship between partial and total linear orders. To begin with, not every partial cyclic order can be extended to a total cyclic order. An example is the following relation on the first thirteen letters of the alphabet: {\"acd, bde, cef, dfg, egh, fha, gac, hcb\"} ∪ {\"abi, cij, bjk, ikl, jlm, kma, lab, mbc\"} ∪ {\"hcm, bhm\"}. This relation is a partial cyclic order, but it cannot be extended with either \"abc\" or \"cba\"; either attempt would result in a contradiction.\n\nThe above was a relatively mild example. One can also construct partial cyclic orders with higher-order obstructions such that, for example, any 15 triples can be added but the 16th cannot. In fact, cyclic ordering is NP-complete, since it solves 3SAT. This is in stark contrast with the recognition problem for linear orders, which can be solved in linear time.\n\n\n\n\n"}
{"id": "27311850", "url": "https://en.wikipedia.org/wiki?curid=27311850", "title": "Philosophia Mathematica", "text": "Philosophia Mathematica\n\nPhilosophia Mathematica is a philosophical journal devoted to the philosophy of mathematics, published by Oxford University Press. The journal publishes three issues per year.\n\n"}
{"id": "34151069", "url": "https://en.wikipedia.org/wiki?curid=34151069", "title": "Positive harmonic function", "text": "Positive harmonic function\n\nIn mathematics, a positive harmonic function on the unit disc in the complex numbers is characterized as the Poisson integral of a finite positive measure on the circle. This result, the \"Herglotz representation theorem\", was proved by Gustav Herglotz in 1911. It can be used to give a related formula and characterization for any holomorphic function on the unit disc with positive real part. Such functions had already been characterized in 1907 by Constantin Carathéodory in terms of the positive definiteness of their Taylor coefficients.\n\nA positive function \"f\" on the unit disk with \"f\"(0) = 1 is harmonic if and only if there is a probability measure μ on the unit circle such that\n\nThe formula clearly defines a positive harmonic function with \"f\"(0) = 1.\n\nConversely if \"f\" is positive and harmonic and \"r\" increases to 1, define\n\nThen\n\nwhere\n\nis a probability measure.\n\nBy a compactness argument (or equivalently in this case\nHelly's selection theorem for Stieltjes integrals), a subsequence of these probability measures has a weak limit which is also a probability measure μ.\n\nSince \"r\" increases to 1, so that \"f\"(\"z\") tends to \"f\"(\"z\"), the Herglotz formula follows.\n\nA holomorphic function \"f\" on the unit disk with \"f\"(0) = 1 has positive real part if and only if there is a probability measure μ on the unit circle such that\n\nThis follows from the previous theorem because:\n\n\nLet\n\nbe a holomorphic function on the unit disk. Then \"f\"(\"z\") has positive real part on the disk\nif and only if\n\nfor any complex numbers λ, λ, ..., λ, where\n\nfor \"m\" > 0.\n\nIn fact from the Herglotz representation for \"n\" > 0\n\nHence\n\nConversely, setting λ = \"z\",\n\n\n"}
{"id": "15187614", "url": "https://en.wikipedia.org/wiki?curid=15187614", "title": "Procrustes transformation", "text": "Procrustes transformation\n\nA Procrustes transformation is a geometric transformation that involves only translation, rotation, uniform scaling, or a combination of these transformations. Hence, it may change the size, but not the shape of a geometric object.\n\nNamed after the mythical Greek robber, Procrustes, who made his victims fit his bed either by stretching their limbs or cutting them off.\n\n\n"}
{"id": "26513034", "url": "https://en.wikipedia.org/wiki?curid=26513034", "title": "Pythagorean theorem", "text": "Pythagorean theorem\n\nIn mathematics, the Pythagorean theorem, also known as Pythagoras' theorem, is a fundamental relation in Euclidean geometry among the three sides of a right triangle. It states that the square of the hypotenuse (the side opposite the right angle) is equal to the sum of the squares of the other two sides. The theorem can be written as an equation relating the lengths of the sides \"a\", \"b\" and \"c\", often called the \"Pythagorean equation\":\nwhere \"c\" represents the length of the hypotenuse and \"a\" and \"b\" the lengths of the triangle's other two sides.\n\nAlthough it is often argued that knowledge of the theorem predates him, the theorem is named after the ancient Greek mathematician Pythagoras ( 570–495 BC) as it is he who, by tradition, is credited with its first proof, although no evidence of it exists. There is some evidence that Babylonian mathematicians understood the formula, although little of it indicates an application within a mathematical framework. Mesopotamian, Indian and Chinese mathematicians all discovered the theorem independently and, in some cases, provided proofs for special cases.\n\nThe theorem has been given numerous proofspossibly the most for any mathematical theorem. They are very diverse, including both geometric proofs and algebraic proofs, with some dating back thousands of years. The theorem can be generalized in various ways, including higher-dimensional spaces, to spaces that are not Euclidean, to objects that are not right triangles, and indeed, to objects that are not triangles at all, but \"n\"-dimensional solids. The Pythagorean theorem has attracted interest outside mathematics as a symbol of mathematical abstruseness, mystique, or intellectual power; popular references in literature, plays, musicals, songs, stamps and cartoons abound.\n\nThe Pythagorean theorem was known long before Pythagoras, but he may well have been the first to prove it. In any event, the proof attributed to him is very simple, and is called a proof by rearrangement.\n\nThe two large squares shown in the figure each contain four identical triangles, and the only difference between the two large squares is that the triangles are arranged differently. Therefore, the white space within each of the two large squares must have equal area. Equating the area of the white space yields the Pythagorean theorem, Q.E.D.\n\nThat Pythagoras originated this very simple proof is sometimes inferred from the writings of the later Greek philosopher and mathematician Proclus. Several other proofs of this theorem are described below, but this is known as the Pythagorean one.\n\nIf \"c\" denotes the length of the hypotenuse and \"a\" and \"b\" denote the lengths of the other two sides, the Pythagorean theorem can be expressed as the Pythagorean equation:\nIf the length of both \"a\" and \"b\" are known, then \"c\" can be calculated as\nIf the length of the hypotenuse \"c\" and of one side (\"a\" or \"b\") are known, then the length of the other side can be calculated as\nor\nThe Pythagorean equation relates the sides of a right triangle in a simple way, so that if the lengths of any two sides are known the length of the third side can be found. Another corollary of the theorem is that in any right triangle, the hypotenuse is greater than any one of the other sides, but less than their sum.\n\nA generalization of this theorem is the law of cosines, which allows the computation of the length of any side of any triangle, given the lengths of the other two sides and the angle between them. If the angle between the other sides is a right angle, the law of cosines reduces to the Pythagorean equation.\n\nThis theorem may have more known proofs than any other (the law of quadratic reciprocity being another contender for that distinction); the book \"The Pythagorean Proposition\" contains 370 proofs.\n\nThis proof is based on the proportionality of the sides of two similar triangles, that is, upon the fact that the ratio of any two corresponding sides of similar triangles is the same regardless of the size of the triangles.\n\nLet \"ABC\" represent a right triangle, with the right angle located at \"C\", as shown on the figure. Draw the altitude from point \"C\", and call \"H\" its intersection with the side \"AB\". Point \"H\" divides the length of the hypotenuse \"c\" into parts \"d\" and \"e\". The new triangle \"ACH\" is similar to triangle \"ABC\", because they both have a right angle (by definition of the altitude), and they share the angle at \"A\", meaning that the third angle will be the same in both triangles as well, marked as \"θ\" in the figure. By a similar reasoning, the triangle \"CBH\" is also similar to \"ABC\". The proof of similarity of the triangles requires the triangle postulate: the sum of the angles in a triangle is two right angles, and is equivalent to the parallel postulate. Similarity of the triangles leads to the equality of ratios of corresponding sides:\nThe first result equates the cosines of the angles \"θ\", whereas the second result equates their sines.\n\nThese ratios can be written as\nSumming these two equalities results in\nwhich, after simplification, expresses the Pythagorean theorem:\n\nThe role of this proof in history is the subject of much speculation. The underlying question is why Euclid did not use this proof, but invented another. One conjecture is that the proof by similar triangles involved a theory of proportions, a topic not discussed until later in the \"Elements\", and that the theory of proportions needed further development at that time.\n\nIn outline, here is how the proof in Euclid's \"Elements\" proceeds. The large square is divided into a left and right rectangle. A triangle is constructed that has half the area of the left rectangle. Then another triangle is constructed that has half the area of the square on the left-most side. These two triangles are shown to be congruent, proving this square has the same area as the left rectangle. This argument is followed by a similar version for the right rectangle and the remaining square. Putting the two rectangles together to reform the square on the hypotenuse, its area is the same as the sum of the area of the other two squares. The details follow.\n\nLet \"A\", \"B\", \"C\" be the vertices of a right triangle, with a right angle at \"A\". Drop a perpendicular from \"A\" to the side opposite the hypotenuse in the square on the hypotenuse. That line divides the square on the hypotenuse into two rectangles, each having the same area as one of the two squares on the legs.\n\nFor the formal proof, we require four elementary lemmata:\n\nNext, each top square is related to a triangle congruent with another triangle related in turn to one of two rectangles making up the lower square.\n\nThe proof is as follows:\n\nThis proof, which appears in Euclid's \"Elements\" as that of Proposition 47 in Book 1, demonstrates that the area of the square on the hypotenuse is the sum of the areas of the other two squares. This is quite distinct from the proof by similarity of triangles, which is conjectured to be the proof that Pythagoras used.\n\nWe have already discussed the Pythagorean proof, which was a proof by rearrangement. The same idea is conveyed by the leftmost animation below, which consists of a large square, side , containing four identical right triangles. The triangles are shown in two arrangements, the first of which leaves two squares \"a\" and \"b\" uncovered, the second of which leaves square \"c\" uncovered. The area encompassed by the outer square never changes, and the area of the four triangles is the same at the beginning and the end, so the black square areas must be equal, therefore \n\nA second proof by rearrangement is given by the middle animation. A large square is formed with area \"c\", from four identical right triangles with sides \"a\", \"b\" and \"c\", fitted around a small central square. Then two rectangles are formed with sides \"a\" and \"b\" by moving the triangles. Combining the smaller square with these rectangles produces two squares of areas \"a\" and \"b\", which must have the same area as the initial large square.\n\nThe third, rightmost image also gives a proof. The upper two squares are divided as shown by the blue and green shading, into pieces that when rearranged can be made to fit in the lower square on the hypotenuse – or conversely the large square can be divided as shown into pieces that fill the other two. This way of cutting one figure into pieces and rearranging them to get another figure is called dissection. This shows the area of the large square equals that of the two smaller ones.\n\nAlbert Einstein gave a proof by dissection in which the pieces need not get moved. Instead of using a square on the hypotenuse and two squares on the legs, one can use any other shape that includes the hypotenuse, and two similar shapes that each include one of two legs instead of the hypotenuse (see #Similar figures on the three sides). In Einstein's proof, the shape that includes the hypotenuse is the right triangle itself. The dissection consists of dropping a perpendicular from the vertex of the right angle of the triangle to the hypotenuse, thus splitting the whole triangle into two parts. Those two parts have the same shape as the original right triangle, and have the legs of the original triangle as their hypotenuses, and the sum of their areas is that of the original triangle. Because the ratio of the area of a right triangle to the square of its hypotenuse is the same for similar triangles, the relationship between the areas of the three triangles holds for the squares of the sides of the large triangle as well.\n\nThe theorem can be proved algebraically using four copies of a right triangle with sides \"a\", \"b\" and \"c\", arranged inside a square with side \"c\" as in the top half of the diagram. The triangles are similar with area formula_10, while the small square has side and area . The area of the large square is therefore\n\nBut this is a square with side \"c\" and area \"c\", so\n\nA similar proof uses four copies of the same triangle arranged symmetrically around a square with side \"c\", as shown in the lower part of the diagram. This results in a larger square, with side and area . The four triangles and the square side \"c\" must have the same area as the larger square,\n\ngiving\n\nA related proof was published by future U.S. President James A. Garfield (then a U.S. Representative). Instead of a square it uses a trapezoid, which can be constructed from the square in the second of the above proofs by bisecting along a diagonal of the inner square, to give the trapezoid as shown in the diagram. The area of the trapezoid can be calculated to be half the area of the square, that is\n\nThe inner square is similarly halved, and there are only two triangles so the proof proceeds as above except for a factor of formula_16, which is removed by multiplying by two to give the result.\n\nOne can arrive at the Pythagorean theorem by studying how changes in a side produce a change in the hypotenuse and employing calculus.\n\nThe triangle \"ABC\" is a right triangle, as shown in the upper part of the diagram, with \"BC\" the hypotenuse. At the same time the triangle lengths are measured as shown, with the hypotenuse of length \"y\", the side \"AC\" of length \"x\" and the side \"AB\" of length \"a\", as seen in the lower diagram part.\nIf \"x\" is increased by a small amount \"dx\" by extending the side \"AC\" slightly to \"D\", then \"y\" also increases by \"dy\". These form two sides of a triangle, \"CDE\", which (with \"E\" chosen so \"CE\" is perpendicular to the hypotenuse) is a right triangle approximately similar to \"ABC\". Therefore, the ratios of their sides must be the same, that is:\n\nThis can be rewritten as formula_18 , which is a differential equation that can be solved by direct integration:\ngiving\nThe constant can be deduced from \"x\" = 0, \"y\" = \"a\" to give the equation\nThis is more of an intuitive proof than a formal one: it can be made more rigorous if proper limits are used in place of \"dx\" and \"dy\".\n\nThe converse of the theorem is also true:\nFor any three positive numbers \"a\", \"b\", and \"c\" such that , there exists a triangle with sides \"a\", \"b\" and \"c\", and every such triangle has a right angle between the sides of lengths \"a\" and \"b\".\n\nAn alternative statement is:\n\nFor any triangle with sides \"a\", \"b\", \"c\", if then the angle between \"a\" and \"b\" measures 90°.\n\nThis converse also appears in Euclid's \"Elements\" (Book I, Proposition 48): \n\nIt can be proven using the law of cosines or as follows:\n\nLet \"ABC\" be a triangle with side lengths \"a\", \"b\", and \"c\", with Construct a second triangle with sides of length \"a\" and \"b\" containing a right angle. By the Pythagorean theorem, it follows that the hypotenuse of this triangle has length \"c\" = , the same as the hypotenuse of the first triangle. Since both triangles' sides are the same lengths \"a\", \"b\" and \"c\", the triangles are congruent and must have the same angles. Therefore, the angle between the side of lengths \"a\" and \"b\" in the original triangle is a right angle.\n\nThe above proof of the converse makes use of the Pythagorean theorem itself. The converse can also be proven without assuming the Pythagorean theorem.\n\nA corollary of the Pythagorean theorem's converse is a simple means of determining whether a triangle is right, obtuse, or acute, as follows. Let \"c\" be chosen to be the longest of the three sides and (otherwise there is no triangle according to the triangle inequality). The following statements apply:\n\nEdsger W. Dijkstra has stated this proposition about acute, right, and obtuse triangles in this language:\n\nwhere \"α\" is the angle opposite to side \"a\", \"β\" is the angle opposite to side \"b\", \"γ\" is the angle opposite to side \"c\", and sgn is the sign function.\n\nA Pythagorean triple has three positive integers \"a\", \"b\", and \"c\", such that In other words, a Pythagorean triple represents the lengths of the sides of a right triangle where all three sides have integer lengths. Such a triple is commonly written Some well-known examples are and \n\nA primitive Pythagorean triple is one in which \"a\", \"b\" and \"c\" are coprime (the greatest common divisor of \"a\", \"b\" and \"c\" is 1).\n\nThe following is a list of primitive Pythagorean triples with values less than 100:\n\nOne of the consequences of the Pythagorean theorem is that line segments whose lengths are incommensurable (so the ratio of which is not a rational number) can be constructed using a straightedge and compass. Pythagoras's theorem enables construction of incommensurable lengths because the hypotenuse of a triangle is related to the sides by the square root operation.\n\nThe figure on the right shows how to construct line segments whose lengths are in the ratio of the square root of any positive integer. Each triangle has a side (labeled \"1\") that is the chosen unit for measurement. In each right triangle, Pythagoras's theorem establishes the length of the hypotenuse in terms of this unit. If a hypotenuse is related to the unit by the square root of a positive integer that is not a perfect square, it is a realization of a length incommensurable with the unit, such as , ,  . For more detail, see Quadratic irrational.\n\nIncommensurable lengths conflicted with the Pythagorean school's concept of numbers as only whole numbers. The Pythagorean school dealt with proportions by comparison of integer multiples of a common subunit. According to one legend, Hippasus of Metapontum (\"ca.\" 470 B.C.) was drowned at sea for making known the existence of the irrational or incommensurable.\n\nFor any complex number\nthe absolute value or modulus is given by\nSo the three quantities, \"r\", \"x\" and \"y\" are related by the Pythagorean equation,\nNote that \"r\" is defined to be a positive number or zero but \"x\" and \"y\" can be negative as well as positive. Geometrically \"r\" is the distance of the \"z\" from zero or the origin \"O\" in the complex plane.\n\nThis can be generalised to find the distance between two points, \"z\" and \"z\" say. The required distance is given by\nso again they are related by a version of the Pythagorean equation,\n\nThe distance formula in Cartesian coordinates is derived from the Pythagorean theorem. If and are points in the plane, then the distance between them, also called the Euclidean distance, is given by\n\nMore generally, in Euclidean \"n\"-space, the Euclidean distance between two points, formula_28 and formula_29, is defined, by generalization of the Pythagorean theorem, as:\n\nIf Cartesian coordinates are not used, for example, if polar coordinates are used in two dimensions or, in more general terms, if curvilinear coordinates are used, the formulas expressing the Euclidean distance are more complicated than the Pythagorean theorem, but can be derived from it. A typical example where the straight-line distance between two points is converted to curvilinear coordinates can be found in the applications of Legendre polynomials in physics. The formulas can be discovered by using Pythagoras's theorem with the equations relating the curvilinear coordinates to Cartesian coordinates. For example, the polar coordinates can be introduced as:\n\nThen two points with locations and are separated by a distance \"s\":\n\nPerforming the squares and combining terms, the Pythagorean formula for distance in Cartesian coordinates produces the separation in polar coordinates as:\nusing the trigonometric product-to-sum formulas. This formula is the law of cosines, sometimes called the generalized Pythagorean theorem. From this result, for the case where the radii to the two locations are at right angles, the enclosed angle and the form corresponding to Pythagoras's theorem is regained: formula_34 The Pythagorean theorem, valid for right triangles, therefore is a special case of the more general law of cosines, valid for arbitrary triangles.\n\nIn a right triangle with sides \"a\", \"b\" and hypotenuse \"c\", trigonometry determines the sine and cosine of the angle \"θ\" between side \"a\" and the hypotenuse as:\n\nFrom that it follows:\n\nwhere the last step applies Pythagoras's theorem. This relation between sine and cosine is sometimes called the fundamental Pythagorean trigonometric identity. In similar triangles, the ratios of the sides are the same regardless of the size of the triangles, and depend upon the angles. Consequently, in the figure, the triangle with hypotenuse of unit size has opposite side of size sin \"θ\" and adjacent side of size cos \"θ\" in units of the hypotenuse.\n\nThe Pythagorean theorem relates the cross product and dot product in a similar way:\n\nThis can be seen from the definitions of the cross product and dot product, as\n\nwith n a unit vector normal to both a and b. The relationship follows from these definitions and the Pythagorean trigonometric identity.\n\nThis can also be used to define the cross product. By rearranging the following equation is obtained\n\nThis can be considered as a condition on the cross product and so part of its definition, for example in seven dimensions.\n\nA generalization of the Pythagorean theorem extending beyond the areas of squares on the three sides to similar figures was known by Hippocrates of Chios in the 5th century BC, and was included by Euclid in his \"Elements\":\n\nIf one erects similar figures (see Euclidean geometry) with corresponding sides on the sides of a right triangle, then the sum of the areas of the ones on the two smaller sides equals the area of the one on the larger side.\n\nThis extension assumes that the sides of the original triangle are the corresponding sides of the three congruent figures (so the common ratios of sides between the similar figures are \"a:b:c\"). While Euclid's proof only applied to convex polygons, the theorem also applies to concave polygons and even to similar figures that have curved boundaries (but still with part of a figure's boundary being the side of the original triangle).\n\nThe basic idea behind this generalization is that the area of a plane figure is proportional to the square of any linear dimension, and in particular is proportional to the square of the length of any side. Thus, if similar figures with areas \"A\", \"B\" and \"C\" are erected on sides with corresponding lengths \"a\", \"b\" and \"c\" then:\n\nBut, by the Pythagorean theorem, \"a\" + \"b\" = \"c\", so \"A\" + \"B\" = \"C\".\n\nConversely, if we can prove that \"A\" + \"B\" = \"C\" for three similar figures without using the Pythagorean theorem, then we can work backwards to construct a proof of the theorem. For example, the starting center triangle can be replicated and used as a triangle \"C\" on its hypotenuse, and two similar right triangles (\"A\" and \"B\" ) constructed on the other two sides, formed by dividing the central triangle by its altitude. The sum of the areas of the two smaller triangles therefore is that of the third, thus \"A\" + \"B\" = \"C\" and reversing the above logic leads to the Pythagorean theorem a + b = c. (\"See also Einstein's proof by dissection without rearrangement\")\nThe Pythagorean theorem is a special case of the more general theorem relating the lengths of sides in any triangle, the law of cosines:\nwhere θ is the angle between sides \"a\" and \"b\".\n\nWhen θ is 90 degrees (/2 radians), then cos\"θ\" = 0, and the formula reduces to the usual Pythagorean theorem.\n\nAt any selected angle of a general triangle of sides \"a, b, c\", inscribe an isosceles triangle such that the equal angles at its base θ are the same as the selected angle. Suppose the selected angle θ is opposite the side labeled \"c\". Inscribing the isosceles triangle forms triangle \"ABD\" with angle θ opposite side \"a\" and with side \"r\" along \"c\". A second triangle is formed with angle θ opposite side \"b\" and a side with length \"s\" along \"c\", as shown in the figure. Thābit ibn Qurra stated that the sides of the three triangles were related as:\nAs the angle θ approaches /2, the base of the isosceles triangle narrows, and lengths \"r\" and \"s\" overlap less and less. When θ = /2, \"ADB\" becomes a right triangle, \"r\" + \"s\" = \"c\", and the original Pythagorean theorem is regained.\n\nOne proof observes that triangle \"ABC\" has the same angles as triangle \"ABD\", but in opposite order. (The two triangles share the angle at vertex B, both contain the angle θ, and so also have the same third angle by the triangle postulate.) Consequently, \"ABC\" is similar to the reflection of \"ABD\", the triangle \"DBA\" in the lower panel. Taking the ratio of sides opposite and adjacent to θ,\nLikewise, for the reflection of the other triangle,\nClearing fractions and adding these two relations:\nthe required result.\n\nThe theorem remains valid if the angle formula_47 is obtuse so the lengths \"r\" and \"s\" are non-overlapping.\n\nPappus's area theorem is a further generalization, that applies to triangles that are not right triangles, using parallelograms on the three sides in place of squares (squares are a special case, of course). The upper figure shows that for a scalene triangle, the area of the parallelogram on the longest side is the sum of the areas of the parallelograms on the other two sides, provided the parallelogram on the long side is constructed as indicated (the dimensions labeled with arrows are the same, and determine the sides of the bottom parallelogram). This replacement of squares with parallelograms bears a clear resemblance to the original Pythagoras's theorem, and was considered a generalization by Pappus of Alexandria in 4 AD\n\nThe lower figure shows the elements of the proof. Focus on the left side of the figure. The left green parallelogram has the same area as the left, blue portion of the bottom parallelogram because both have the same base \"b\" and height \"h\". However, the left green parallelogram also has the same area as the left green parallelogram of the upper figure, because they have the same base (the upper left side of the triangle) and the same height normal to that side of the triangle. Repeating the argument for the right side of the figure, the bottom parallelogram has the same area as the sum of the two green parallelograms.\n\nIn terms of solid geometry, Pythagoras's theorem can be applied to three dimensions as follows. Consider a rectangular solid as shown in the figure. The length of diagonal \"BD\" is found from Pythagoras's theorem as:\n\nwhere these three sides form a right triangle. Using horizontal diagonal \"BD\" and the vertical edge \"AB\", the length of diagonal \"AD\" then is found by a second application of Pythagoras's theorem as:\n\nor, doing it all in one step:\n\nThis result is the three-dimensional expression for the magnitude of a vector v (the diagonal AD) in terms of its orthogonal components {v} (the three mutually perpendicular sides):\n\nThis one-step formulation may be viewed as a generalization of Pythagoras's theorem to higher dimensions. However, this result is really just the repeated application of the original Pythagoras's theorem to a succession of right triangles in a sequence of orthogonal planes.\n\nA substantial generalization of the Pythagorean theorem to three dimensions is de Gua's theorem, named for Jean Paul de Gua de Malves: If a tetrahedron has a right angle corner (like a corner of a cube), then the square of the area of the face opposite the right angle corner is the sum of the squares of the areas of the other three faces. This result can be generalized as in the \"\"n\"-dimensional Pythagorean theorem\":\nThis statement is illustrated in three dimensions by the tetrahedron in the figure. The \"hypotenuse\" is the base of the tetrahedron at the back of the figure, and the \"legs\" are the three sides emanating from the vertex in the foreground. As the depth of the base from the vertex increases, the area of the \"legs\" increases, while that of the base is fixed. The theorem suggests that when this depth is at the value creating a right vertex, the generalization of Pythagoras's theorem applies. In a different wording:\nThe Pythagorean theorem can be generalized to inner product spaces, which are generalizations of the familiar 2-dimensional and 3-dimensional Euclidean spaces. For example, a function may be considered as a vector with infinitely many components in an inner product space, as in functional analysis.\n\nIn an inner product space, the concept of perpendicularity is replaced by the concept of orthogonality: two vectors v and w are orthogonal if their inner product formula_52 is zero. The inner product is a generalization of the dot product of vectors. The dot product is called the \"standard\" inner product or the \"Euclidean\" inner product. However, other inner products are possible.\n\nThe concept of length is replaced by the concept of the norm ||v|| of a vector v, defined as:\n\nIn an inner-product space, the Pythagorean theorem states that for any two orthogonal vectors v and w we have\nHere the vectors v and w are akin to the sides of a right triangle with hypotenuse given by the vector sum v + w. This form of the Pythagorean theorem is a consequence of the properties of the inner product:\nwhere the inner products of the cross terms are zero, because of orthogonality.\n\nA further generalization of the Pythagorean theorem in an inner product space to non-orthogonal vectors is the \"parallelogram law\" :\n\nwhich says that twice the sum of the squares of the lengths of the sides of a parallelogram is the sum of the squares of the lengths of the diagonals. Any norm that satisfies this equality is \"ipso facto\" a norm corresponding to an inner product.\nThe Pythagorean identity can be extended to sums of more than two orthogonal vectors. If v, v, ..., v are pairwise-orthogonal vectors in an inner-product space, then application of the Pythagorean theorem to successive pairs of these vectors (as described for 3-dimensions in the section on solid geometry) results in the equation\n\nAnother generalization of the Pythagorean theorem applies to Lebesgue-measurable sets of objects in any number of dimensions. Specifically, the square of the measure of an \"m\"-dimensional set of objects in one or more parallel \"m\"-dimensional flats in \"n\"-dimensional Euclidean space is equal to the sum of the squares of the measures of the orthogonal projections of the object(s) onto all \"m\"-dimensional coordinate subspaces.\n\nIn mathematical terms:\nwhere:\n\nThe Pythagorean theorem is derived from the axioms of Euclidean geometry, and in fact, the Pythagorean theorem given above does not hold in a non-Euclidean geometry. (The Pythagorean theorem has been shown, in fact, to be equivalent to Euclid's Parallel (Fifth) Postulate.)\nIn other words, in non-Euclidean geometry, the relation between the sides of a triangle must necessarily take a non-Pythagorean form. For example, in spherical geometry, all three sides of the right triangle (say \"a\", \"b\", and \"c\") bounding an octant of the unit sphere have length equal to /2, and all its angles are right angles, which violates the Pythagorean theorem because \n\nHere two cases of non-Euclidean geometry are considered—spherical geometry and hyperbolic plane geometry; in each case, as in the Euclidean case for non-right triangles, the result replacing the Pythagorean theorem follows from the appropriate law of cosines.\n\nHowever, the Pythagorean theorem remains true in hyperbolic geometry and elliptic geometry if the condition that the triangle be right is replaced with the condition that two of the angles sum to the third, say \"A\"+\"B\" = \"C\". The sides are then related as follows: the sum of the areas of the circles with diameters \"a\" and \"b\" equals the area of the circle with diameter \"c\".\n\nFor any right triangle on a sphere of radius \"R\" (for example, if γ in the figure is a right angle), with sides \"a\", \"b\", \"c\", the relation between the sides takes the form:\n\nThis equation can be derived as a special case of the spherical law of cosines that applies to all spherical triangles:\n\nBy expressing the Maclaurin series for the cosine function as an asymptotic expansion with the remainder term in big O notation,\n\nit can be shown that as the radius \"R\" approaches infinity and the arguments \"a/R\", \"b/R\", and \"c/R\" tend to zero, the spherical relation between the sides of a right triangle approaches the Euclidean form of the Pythagorean theorem. Substituting the asymptotic expansion for each of the cosines into the spherical relation for a right triangle yields\n\nThe constants \"a\", \"b\", and \"c\" have been absorbed into the big \"O\" remainder terms since they are independent of the radius \"R\". This asymptotic relationship can be further simplified by multiplying out the bracketed quantities, cancelling the ones, multiplying through by −2, and collecting all the error terms together:\n\nAfter multiplying through by \"R\", the Euclidean Pythagorean relationship \"c\" = \"a\" + \"b\" is recovered in the limit as the radius \"R\" approaches infinity (since the remainder term tends to zero):\n\nFor small right triangles (\"a\", \"b\" « \"R\"), the cosines can be eliminated to avoid loss of significance, giving\n\nIn a hyperbolic space with uniform curvature −1/\"R\", for a right triangle with legs \"a\", \"b\", and hypotenuse \"c\", the relation between the sides takes the form:\n\nwhere cosh is the hyperbolic cosine. This formula is a special form of the hyperbolic law of cosines that applies to all hyperbolic triangles:\n\nwith γ the angle at the vertex opposite the side \"c\".\n\nBy using the Maclaurin series for the hyperbolic cosine, , it can be shown that as a hyperbolic triangle becomes very small (that is, as \"a\", \"b\", and \"c\" all approach zero), the hyperbolic relation for a right triangle approaches the form of Pythagoras's theorem.\n\nFor small right triangles (\"a\", \"b\" « \"R\"), the hyperbolic cosines can be eliminated to avoid loss of significance, giving\n\nFor any uniform curvature \"K\" (positive, zero, or negative), in very small right triangles (|\"K\"|\"a\", |\"K\"|\"b\" « 1) with hypotenuse \"c\", it can be shown that\n\nOn an infinitesimal level, in three dimensional space, Pythagoras's theorem describes the distance between two infinitesimally separated points as:\n\nwith \"ds\" the element of distance and (\"dx\", \"dy\", \"dz\") the components of the vector separating the two points. Such a space is called a Euclidean space. However, in Riemannian geometry, a generalization of this expression useful for general coordinates (not just Cartesian) and general spaces (not just Euclidean) takes the form:\n\nwhich is called the metric tensor. (Sometimes, by abuse of language, the same term is applied to the set of coefficients .) It may be a function of position, and often describes curved space. A simple example is Euclidean (flat) space expressed in curvilinear coordinates. For example, in polar coordinates:\n\nThere is debate whether the Pythagorean theorem was discovered once, or many times in many places, and the date of first discovery is uncertain, as is the date of the first proof. According to Joran Friberg, a historian of mathematics, evidence indicates that the Pythagorean theorem was well-known to the mathematicians of the First Babylonian Dynasty (20th to 16th centuries BC), which would have been over a thousand years before Pythagoras was born, thus an example of Stigler's law of eponymy. (Yale's Institute for the Preservation of Cultural Heritage's 3-D scan of a cuneiform tablet depicting the proof is one of their mostly widely used images.) Other sources, such as a book by Leon Lederman and Dick Teresi, mention that Pythagoras discovered the theorem, although Teresi subsequently stated that the Babylonians developed the theorem \"at least fifteen hundred years before Pythagoras was\nborn.\" The history of the theorem can be divided into four parts: knowledge of Pythagorean triples, knowledge of the relationship among the sides of a right triangle, knowledge of the relationships among adjacent angles, and proofs of the theorem within some deductive system.\n\nBartel Leendert van der Waerden (1903–1996) conjectured that Pythagorean triples were discovered algebraically by the Babylonians. Written between 2000 and 1786 BC, the Middle Kingdom Egyptian \"Berlin Papyrus 6619\" includes a problem whose solution is the Pythagorean triple 6:8:10, but the problem does not mention a triangle. The Mesopotamian tablet \"Plimpton 322\", written between 1790 and 1750 BC during the reign of Hammurabi the Great, contains many entries closely related to Pythagorean triples.\n\nIn India, the \"Baudhayana Sulba Sutra\", the dates of which are given variously as between the 8th and 5th century BC, contains a list of Pythagorean triples discovered algebraically, a statement of the Pythagorean theorem, and a geometrical proof of the Pythagorean theorem for an isosceles right triangle.\nThe \"Apastamba Sulba Sutra\" (c. 600 BC) contains a numerical proof of the general Pythagorean theorem, using an area computation. Van der Waerden believed that \"it was certainly based on earlier traditions\". Carl Boyer states that the Pythagorean theorem in \"Śulba-sũtram\" may have been influenced by ancient Mesopotamian math, but there is no conclusive evidence in favor or opposition of this possibility.\nWith contents known much earlier, but in surviving texts dating from roughly the 1st century BC, the Chinese text \"Zhoubi Suanjing\" (周髀算经), (\"The Arithmetical Classic of the Gnomon and the Circular Paths of Heaven\") gives a reasoning for the Pythagorean theorem for the (3, 4, 5) triangle—in China it is called the \"Gougu theorem\" (勾股定理). During the Han Dynasty (202 BC to 220 AD), Pythagorean triples appear in \"The Nine Chapters on the Mathematical Art\", together with a mention of right triangles. Some believe the theorem arose first in China, where it is alternatively known as the \"Shang Gao theorem\" (商高定理), named after the Duke of Zhou's astronomer and mathematician, whose reasoning composed most of what was in the \"Zhoubi Suanjing\".\n\nPythagoras, whose dates are commonly given as 569–475 BC, used algebraic methods to construct Pythagorean triples, according to Proclus's commentary on Euclid. Proclus, however, wrote between 410 and 485 AD. According to Thomas L. Heath (1861–1940), no specific attribution of the theorem to Pythagoras exists in the surviving Greek literature from the five centuries after Pythagoras lived. However, when authors such as Plutarch and Cicero attributed the theorem to Pythagoras, they did so in a way which suggests that the attribution was widely known and undoubted. \"Whether this formula is rightly attributed to Pythagoras personally, [...] one can safely assume that it belongs to the very oldest period of Pythagorean mathematics.\"\n\nAround 400 BC, according to Proclus, Plato gave a method for finding Pythagorean triples that combined algebra and geometry. Around 300 BC, in Euclid's \"Elements\", the oldest extant axiomatic proof of the theorem is presented.\n\n\n"}
{"id": "44074687", "url": "https://en.wikipedia.org/wiki?curid=44074687", "title": "Rare events", "text": "Rare events\n\nRare events are events that occur with low frequency, and the term is often used in particular reference to infrequent or hypothetical events that have potentially widespread impact and which might destabilize society. Rare events encompass natural phenomena (major earthquakes, tsunamis, hurricanes, floods, asteroid impacts, solar flares, etc.), anthropogenic hazards (warfare and related forms of violent conflict, acts of terrorism, industrial accidents, financial and commodity market crashes, etc.), as well as phenomena for which natural and anthropogenic factors interact in complex ways (epidemic disease spread, global warming-related changes in climate and weather, etc.).\n\nRare events are discrete occurrences that are statistically “improbable” in that they are very infrequently observed. Despite being statistically improbable, such events are plausible insofar as historical instances of the event (or a similar event) have been documented.\nScholarly and popular analyses of rare events often focus on those events that could be reasonably expected to have a substantial negative impact on a society—either economically or in terms of human casualties (typically, both). Examples of such events might include an 8.0+ Richter magnitude earthquake, a nuclear incident that kills thousands of people, or a 10%+ single-day change in the value of a stock market index.\n\nRare event modeling (REM) refers to efforts to characterize the statistical distribution parameters, generative processes, or dynamics that govern the occurrence of statistically rare events, including but not limited to high-impact natural or human-made catastrophes. Such “modeling” may include a wide range of approaches, including, most notably, statistical models derived from historical event data and computational software models that attempt to simulate rare event processes and dynamics. REM also encompasses efforts to forecast the occurrence of similar events over some future time horizon, which may be of interest for both scholarly and applied purposes (e.g., risk mitigation and planning).\n\nIn many cases, rare and catastrophic events can be regarded as extreme-magnitude instances of more mundane phenomena. For example, seismic activity, stock market fluctuations, and acts of organized violence all occur along a continuum of extremity, with more extreme-magnitude cases being statistically infrequent. Therefore, rather than viewing rare event data as its own class of information, data concerning “rare” events often exists as a subset of data within a broader parent event class (e.g., a seismic activity data set would include instances of extreme earthquakes, as well as data on much lower-intensity seismic events).\n\nThe following is a list of data sets focusing on domains that are of broad scholarly and policy interest, and where “rare” (extreme-magnitude) cases may be of particularly keen interest due to their potentially devastating consequences.\nDescriptions of the data sets are extracted from the source websites or providers.\n\n"}
{"id": "54727043", "url": "https://en.wikipedia.org/wiki?curid=54727043", "title": "Simplicial depth", "text": "Simplicial depth\n\nIn robust statistics and computational geometry, simplicial depth is a measure of central tendency determined by the simplices that contain a given point. For the Euclidean plane, it counts the number of triangles of sample points that contain a given point.\n\nThe simplicial depth of a point formula_1 in formula_2-dimensional Euclidean space, with respect to a set of sample points in that space, is the number of formula_2-dimensional simplices (the convex hulls of sets of formula_4 sample points) that contain formula_1.\nThe same notion can be generalized to any probability distribution on points of the plane, not just the empirical distribution given by a set of sample points, by defining the depth to be the probability that a randomly chosen formula_6-tuple of points has a convex hull that This probability can be calculated, from the number of simplices that by dividing by formula_7 where formula_8 is the number of sample points.\n\nUnder the standard definition of simplicial depth, the simplices that have formula_1 on their boundaries count equally much as the simplices with formula_1 in their interiors. In order to avoid some problematic behavior of this definition, proposed a modified definition of simplicial depth, in which the simplices with formula_1 on their boundaries count only half as much. Equivalently, their definition is the average of the number of open simplices and the number of closed simplices that \n\nSimplicial depth is robust against outliers: if a set of sample points is represented by the point of maximum depth, then up to a constant fraction of the sample points can be arbitrarily corrupted without significantly changing the location of the representative point. It is also invariant under affine transformations of the plane.\n\nHowever, simplicial depth fails to have some other desirable properties for robust measures of central tendency. When applied to centrally symmetric distributions, it is not necessarily the case that there is a unique point of maximum depth in the center of the distribution. And, along a ray from the point of maximum depth, it is not necessarily the case that the simplicial depth decreases monotonically.\n\nFor sets of formula_8 sample points in the Euclidean plane \nthe simplicial depth of any other point formula_1 can be computed in time \noptimal in some models of computation.\nIn three dimensions, the same problem can be solved in time \n\nIt possible to construct a data structure using ε-nets that can approximate the simplicial depth of a query point (given either a fixed set of samples, or a set of samples undergoing point insertions) in near-constant time per query, in any dimension, with an approximation whose error is a small fraction of the total number of triangles determined by the samples. In two dimensions, a more accurate approximation algorithm is known, for which the approximation error is a small multiple of the simplicial depth itself. The same methods also lead to fast approximation algorithms in higher dimensions.\n\nSpherical depth, formula_14 is defined to be the probability that a point formula_15 is contained inside a random closed hyperball obtained from a pair of points from formula_16. While the time complexity of most other data depths grows exponentially, the spherical depth grows only linearly in the dimension formula_2 – the straightforward algorithm for computing the spherical depth takes formula_18. Simplicial depth (SD) is linearly bounded by spherical depth (formula_19).\n"}
{"id": "36528826", "url": "https://en.wikipedia.org/wiki?curid=36528826", "title": "Sonkin enterprise multiple", "text": "Sonkin enterprise multiple\n\nThe Sonkin enterprise multiple (Sonkin ratio) was named after by Paul D. Sonkin, a graduate of Columbia Business School. This ratio can be used when Value investing, and can be calculated using the following formula:\n\nSonkin ratio = (market capitalization + debt – cash) / (earnings before interest and taxes – tax)\n\nThe Sonkin ratio is an alternative to the P/E ratio (price to earnings ratio) and represents the multiple of operating earnings an investor would pay if using the company's cash. A lower multiple means that an investor will pay less to own the after-tax operating earnings of the business.\n"}
{"id": "2588624", "url": "https://en.wikipedia.org/wiki?curid=2588624", "title": "Tannaka–Krein duality", "text": "Tannaka–Krein duality\n\nIn mathematics, Tannaka–Krein duality theory concerns the interaction of a compact topological group and its category of linear representations. It is a natural extension of Pontryagin duality, between compact and discrete commutative topological groups, to groups that are compact but noncommutative. The theory is named for two men, the Soviet mathematician Mark Grigorievich Krein, and the Japanese Tadao Tannaka. In contrast to the case of commutative groups considered by Lev Pontryagin, the notion dual to a noncommutative compact group is not a group, but a category of representations Π(\"G\") with some additional structure, formed by the finite-dimensional representations of \"G\". \n\nDuality theorems of Tannaka and Krein describe the converse passage from the category Π(\"G\") back to the group \"G\", allowing one to recover the group from its category of representations. Moreover, they in effect completely characterize all categories that can arise from a group in this fashion. Alexander Grothendieck later showed that by a similar process, Tannaka duality can be extended to the case of algebraic groups: see tannakian category. Meanwhile, the original theory of Tannaka and Krein continued to be developed and refined by mathematical physicists. A generalization of Tannaka–Krein theory provides the natural framework for studying representations of quantum groups, and is currently being extended to quantum supergroups, quantum groupoids and their dual Hopf algebroids.\n\nIn Pontryagin duality theory for locally compact commutative groups, the dual object to a group \"G\" is its character group formula_1 which consists of its one-dimensional unitary representations. If we allow the group \"G\" to be noncommutative, the most direct analogue of the character group is the set of equivalence classes of irreducible unitary representations of \"G\". The analogue of the product of characters is the tensor product of representations. However, irreducible representations of \"G\" in general fail to form a group, or even a monoid, because a tensor product of irreducible representations is not necessarily irreducible. It turns out that one needs to consider the set Π(\"G\") of all finite-dimensional representations, and treat it as a monoidal category, where the product is the usual tensor product of representations, and the dual object is given by the operation of the contragredient representation.\n\nA representation of the category Π(\"G\") is a monoidal natural transformation from the identity functor formula_2 to itself. In other words, it is a non-zero function φ that associates with any formula_3 an endomorphism of the space of \"T\" and satisfies the conditions of compatibility with tensor products, formula_4, and with arbitrary intertwining operators \"f:\" \"T\" → \"U\", namely, formula_5. The collection Γ(Π(\"G\")) of all representations of the category Π(\"G\") can be endowed with multiplication φψ(\"T\") = φ(\"T\") ψ(\"T\") and topology, in which convergence is defined pointwise, i.e. a sequence formula_6 converges to some formula_7 if and only if formula_8 converges to formula_9 for all formula_10. It can be shown that the set Γ(Π(\"G\")) thus becomes a compact (topological) group.\n\nTannaka's theorem provides a way to reconstruct the compact group \"G\" from its category of representations Π(\"G\"). \n\nLet \"G\" be a compact group and let \"F:\" Π(\"G\") → Vect be the forgetful functor from finite-dimensional complex representations of \"G\" to complex finite-dimensional vector spaces. One puts a topology on the natural transformations \"τ:\" \"F\" → \"F\" by setting it to be the coarsest topology possible such that each of the projections End(\"F\") → End(\"V\") given by formula_11 (taking a natural transformation formula_12 to its component formula_13 at formula_14) is a continuous function. We say that a natural transformation is tensor-preserving if it is the identity map on the trivial representation of \"G\", and if it preserves tensor products in the sense that formula_15. We also say that τ is self-conjugate if formula_16 where the bar denotes complex conjugation. Then the set formula_17 of all tensor-preserving, self-conjugate natural transformations of \"F\" is a closed subset of End(\"F\"), which is in fact a (compact) group whenever \"G\" is a (compact) group. Every element \"x\" of \"G\" gives rise to a tensor-preserving self-conjugate natural transformation via multiplication by \"x\" on each representation, and hence one has a map formula_18. Tannaka's theorem then says that this map is an isomorphism.\n\nKrein's theorem answers the following question: which categories can arise as a dual object to a compact group? \n\nLet Π be a category of finite-dimensional vector spaces, endowed with operations of tensor product and involution. The following conditions are necessary and sufficient in order for Π to be a dual object to a compact group \"G\".\n\nIf all these conditions are satisfied then the category Π = Π(\"G\"), where \"G\" is the group of the representations of Π.\n\nInterest in Tannaka–Krein duality theory was reawakened in the 1980s with the discovery of quantum groups in the work of Drinfeld and Jimbo. One of the main approaches to the study of a quantum group proceeds through its finite-dimensional representations, which form a category akin to the symmetric monoidal categories Π(\"G\"), but of more general type, braided monoidal category. It turned out that a good duality theory of Tannaka–Krein type also exists in this case and plays an important role in the theory of quantum groups by providing a natural setting in which both the quantum groups and their representations can be studied. Shortly afterwards different examples of braided monoidal categories were found in rational conformal field theory. Tannaka–Krein philosophy suggests that braided monoidal categories arising from conformal field theory can also be obtained from quantum groups, and in a series of papers, Kazhdan and Lusztig proved that it was indeed so. On the other hand, braided monoidal categories arising from certain quantum groups were applied by Reshetikhin and Turaev to construction of new invariants of knots.\n\nThe Doplicher–Roberts theorem (due to Sergio Doplicher and John E. Roberts) characterises Rep(\"G\") in terms of category theory, as a type of subcategory of the category of Hilbert spaces. Such subcategories of compact group unitary representations on Hilbert spaces are:\n\n\n"}
{"id": "62950", "url": "https://en.wikipedia.org/wiki?curid=62950", "title": "Ternary numeral system", "text": "Ternary numeral system\n\nThe ternary numeral system (also called base 3) has three as its base. Analogous to a bit, a ternary digit is a trit (trinary digit). One trit is equivalent to log₂3 (about 1.58496) bits of information.\n\nAlthough \"ternary\" most often refers to a system in which the three digits are all non–negative numbers, specifically , , and , the adjective also lends its name to the balanced ternary system, comprising the digits −1, 0 and +1, used in comparison logic and ternary computers.\n\nRepresentations of integer numbers in ternary do not get uncomfortably lengthy as quickly as in binary. For example, decimal 365 corresponds to binary 101101101 (nine digits) and to ternary 111112 (six digits). However, they are still far less compact than the corresponding representations in bases such as decimalsee below for a compact way to codify ternary using nonary and septemvigesimal.\n\nAs for rational numbers, ternary offers a convenient way to represent 1÷3 (as opposed to its cumbersome representation as an infinite string of recurring digits in decimal); but a major drawback is that, in turn, ternary does not offer a finite representation for 1÷2 (neither for 1÷4, 1÷8, etc.), because 2 is not a prime factor of the base; as with base 2, 1÷10 is not representable exactly (that would need e.g. base 10); nor is 1÷6.\n\nThe value of a binary number with \"n\" bits that are all 1 is 2\"ⁿ\" - 1.\n\nSimilarly, for a number \"N\"(\"b\", \"d\") with base \"b\" and \"d\" digits, all of which are the maximal digit value \"b\" - 1, we can write:\n\nThen\n\nFor a three-digit ternary number, \"N\"(3, 3) = 3³ - 1 = 26 = 2 × 3² + 2 × 3¹ + 2 × 3⁰ = 18 + 6 + 2.\n\nNonary (base 9, each digit is two ternary digits) or septemvigesimal (base 27, each digit is three ternary digits) can be used for compact representation of ternary, similar to how octal and hexadecimal systems are used in place of binary.\n\nIn certain analog logic, the state of the circuit is often expressed ternary. This is most commonly seen in CMOS circuits, and also in Transistor–transistor logic with totem-pole output. The output is said to either be low (grounded), high, or open (high–Z). In this configuration the output of the circuit is actually not connected to any voltage reference at all. Where the signal is usually grounded to a certain reference, or at a certain voltage level, the state is said to be high impedance because it is open and serves its own reference. Thus, the actual voltage level is sometimes unpredictable.\n\nA rare \"ternary point\" is used to denote fractional parts of an inning in baseball. Since each inning consists of three outs, each out is considered one third of an inning and is denoted as .1. For example, if a player pitched all of the 4th, 5th and 6th innings, plus 2 outs of the 7th inning, his Innings pitched column for that game would be listed as 3.2, meaning 3⅔. In this usage, only the fractional part of the number is written in ternary form.\n\nTernary numbers can be used to convey self–similar structures like the Sierpinski triangle or the Cantor set conveniently. Additionally, it turns out that the ternary representation is useful for defining the Cantor set and related point sets, because of the way the Cantor set is constructed. The Cantor set consists of the points from 0 to 1 that have a ternary expression that does not contain any instance of the digit 1. Any terminating expansion in the ternary system is equivalent to the expression that is identical up to the term preceding the last non-zero term followed by the term one less than the last nonzero term of the first expression, followed by an infinite tail of twos. For example: .1020 is equivalent to .1012222... because the expansions are the same until the \"two\" of the first expression, the two was decremented in the second expansion, and trailing zeros were replaced with trailing twos in the second expression.\n\nTernary is the integer base with the lowest radix economy, followed closely by binary and quaternary. It has been used for some computing systems because of this efficiency. It is also used to represent three-option \"trees\", such as phone menu systems, which allow a simple path to any branch.\n\nA form of redundant binary representation called balanced ternary or signed-digit representation is sometimes used in low-level software and hardware to accomplish fast addition of integers because it can eliminate carries.\n\nSimulation of ternary computers using binary computers, or interfacing between ternary and binary computers, can involve use of binary-coded ternary (BCT) numbers, with two bits used to encode each trit. BCT encoding is analogous to binary-coded decimal (BCD) encoding. If the trit values 0, 1 and 2 are encoded 00, 01 and 10, conversion in either direction between binary-coded ternary and binary can be done in logarithmic time. A library of C code supporting BCT arithmetic is available.\n\nSome ternary computers such as the Setun defined a tryte to be six trits or approximately 9.5 bit (holding more information than the \"de facto\" binary byte).\n\n\n"}
{"id": "374851", "url": "https://en.wikipedia.org/wiki?curid=374851", "title": "Type rule", "text": "Type rule\n\nIn type theory, a type rule is an inference rule that describes how a type system assigns a type to a syntactic construction. These rules may be applied by the type system to determine if a program is well typed and what type expressions have. A prototypical example of the use of type rules is in defining type inference in the simply typed lambda calculus, which is the internal language of Cartesian closed categories.\n\nAn expression formula_1 of type formula_2 is written as formula_3. The typing environment is written as formula_4. The notation for inference is the usual one for sequents and inference rules, and has the following general form\n\nThe sequents above the line are the premises that must be fulfilled for the rule to be applied, yielding the conclusion: the sequents below the line. This can be read as: \"if expression formula_6 has type formula_7 in environment formula_8, for all formula_9, then the expression formula_1 will have an environment formula_4 and type formula_2\".\n\nFor example, a simple language to perform arithmetic calculations on real numbers may have the following rules\n\nA type rule may have no premises, and usually the line is omitted in these cases. A type rule may also change an environment by adding new variables to a previous environment; for example, a declaration may have the following type rule, where a new variable formula_14,\nwith type formula_15, is added to formula_4:\n\nHere the syntax of the let expression is that of Standard ML. Thus type rules can be used to derive the types of composed expressions, much like in natural deduction.\n\n\n"}
{"id": "11609450", "url": "https://en.wikipedia.org/wiki?curid=11609450", "title": "Union of two regular languages", "text": "Union of two regular languages\n\nIn formal language theory, and in particular the theory of nondeterministic finite automata, it is known that the union of two regular languages is a regular language. This article provides a proof of that statement.\n\nFor any regular languages formula_1 and formula_2, language formula_3 is regular.\"\n\n\"Proof\" \n\nSince formula_1 and formula_2 are regular, there exist NFAs formula_6 that recognize formula_1 and formula_2.\n\nLet \n\nConstruct\n\nwhere\n\nIn the following, we shall use formula_14 to denote formula_15\n\nLet formula_16 be a string from formula_3. Without loss of generality assume formula_18.\n\nLet formula_19 where formula_20\n\nSince formula_21 accepts formula_22, there exist formula_23 such that\n\nSince formula_25\n\nWe can therefore substitute formula_30 for formula_31 and rewrite the above path as\nFurthermore,\n\nand \n\nThe above path can be rewritten as\nTherefore, formula_36 accepts formula_22 and the proof is complete.\nNote: The idea drawn from this mathematical proof for constructing a machine to recognize formula_3 is to create an initial state and connect it to the initial states of formula_1 and formula_2 using formula_41 arrows.\n\n"}
