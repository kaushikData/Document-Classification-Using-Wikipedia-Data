{"id": "32040747", "url": "https://en.wikipedia.org/wiki?curid=32040747", "title": "Affine braid group", "text": "Affine braid group\n\nIn mathematics, an affine braid group is a braid group associated to an affine Coxeter system. Their group rings have quotients called affine Hecke algebras. They are subgroups of double affine braid groups.\n\n"}
{"id": "253447", "url": "https://en.wikipedia.org/wiki?curid=253447", "title": "Almost", "text": "Almost\n\nIn set theory, when dealing with sets of infinite size, the term almost or nearly is used to mean \"all the elements except for finitely many\".\n\nIn other words, an infinite set \"S\" that is a subset of another infinite set \"L\", is almost \"L\" if the subtracted set \"L\"\\\"S\" is of finite size.\n\nExamples:\n\nThis is conceptually similar to the \"almost everywhere\" concept of measure theory, but is not the same. For example, the Cantor set is uncountably infinite, but has Lebesgue measure zero. So a real number in (0, 1) is a member of the complement of the Cantor set \"almost everywhere\", but it is not true that the complement of the Cantor set is \"almost\" the real numbers in (0, 1).\n\n"}
{"id": "9537636", "url": "https://en.wikipedia.org/wiki?curid=9537636", "title": "Alternative beta", "text": "Alternative beta\n\nAlternative beta is the concept of managing volatile \"alternative investments\", often through the use of hedge funds. Alternative beta is often also referred to as \"alternative risk premia\".\n\nResearcher Lars Jaeger says that the return from an investment mainly results from exposure to systematic risk factors. These exposures can take two basic forms: long only \"buy and hold\" exposures and exposures through the use of alternative investment techniques such as long/short investing, the use of derivatives (non-linear payout profiles), or the employment of leverage)\n\nAlthough alternative investment is a general term, (commonly defined as any investment other than stocks, bonds or cash), alternative beta relates to the use of hedge funds. At its most basic, a hedge fund is an investment vehicle that pools capital from a number of investors and invests in securities and other instruments. It is administered by a professional management firm, and often structured as a limited partnership, limited liability company, or similar vehicle.\n\nFor an investment that involves risk to be worthwhile, its returns must be higher than a risk-free investment. The risk is related to volatility.\n\nA measure of the factors influencing an investment's volatility is the beta. The beta is a measure of the risk arising from exposure to general market movements as opposed to idiosyncratic factors.\n\nA beta below 1 can indicate either an investment with lower volatility than the market, or a volatile investment whose price movements are not highly correlated with the market. An example of the first is a treasury bill: the price does not go up or down a lot, so it has a low beta. An example of the second is gold. The price of gold does go up and down a lot, but not in the same direction or at the same time as the market.\n\nA beta above 1 generally means that the asset both is volatile and tends to move up and down with the market. An example is a stock in a big technology company. Negative betas are possible for investments that tend to go down when the market goes up, and vice versa. There are few fundamental investments with consistent and significant negative betas, but some derivatives like equity put options can have large negative beta values.\n\nInvestments with a high beta value are often called \"beta investments\", as opposed to \"alpha investments\" which typically have lower volatility and lower returns.\n\nSeparating returns into alpha and beta can also be applied to determine the amount and type of fees to charge. The consensus is to charge higher fees for alpha (incl. performance fee), since it is mostly viewed as skill based. The topic has received increasing levels of attention due to the very rapid growth of the hedge fund industry, where investment companies typically charge fees higher than those of mutual funds, based on the assumption that hedge funds are alpha investments. Investors have started to question whether hedge funds are actually alpha investments, or just some “new” form of beta (i.e. \"alternative beta\").\n\nThis issue was raised in the 1997 paper \"Empirical Characteristics of Dynamic Trading Strategies: The Case of Hedge Funds\" by William Fung and David Hsieh. Following this paper, several groups of academics (such as Thomas Schneeweis et al.) started to explain past hedge fund returns using various systematic risk factors (i.e. Alternative Betas). Following this, a paper has discussed whether investable strategies based on such factors can not only explain past returns, but also replicate future ones.\n\nTraditional betas can be seen as those related to investments the common investor would already be experienced with (examples include stocks and most bonds). They are typically represented through indexation, and the techniques employed here are what is called “long only”. The definition of alternative beta in contrast requires the consideration of other investment techniques such as short selling, use of derivatives and leverage - techniques which are often associated with the activities of hedge funds. The underlying non-traditional investment risks are often seen as being riskier, as investors are less familiar with them.\n\nViewed from the implementation side, investment techniques and strategies are the means to either capture risk premia (beta) or to obtain excess returns (alpha). Whereas returns from beta are a result of exposing the portfolio to systematic risks (traditional or alternative), alpha is an exceptional return that an investor or portfolio manager earns due to his unique skill, i.e. exploiting market inefficiencies. Academic studies as well as their performance in recent years strongly support the idea that the return from hedge funds mostly consists of (alternative) risk premia. This is the basis of the various approaches to replicate the return profile of hedge funds by direct exposures to alternative beta (hedge fund replication).\n\nThere are currently two main approaches to replicate the return profile of hedge funds based on the idea of Alternative Betas:\n\n"}
{"id": "30667901", "url": "https://en.wikipedia.org/wiki?curid=30667901", "title": "Astérisque", "text": "Astérisque\n\n"}
{"id": "2504397", "url": "https://en.wikipedia.org/wiki?curid=2504397", "title": "Banach manifold", "text": "Banach manifold\n\nIn mathematics, a Banach manifold is a manifold modeled on Banach spaces. Thus it is a topological space in which each point has a neighbourhood homeomorphic to an open set in a Banach space (a more involved and formal definition is given below). Banach manifolds are one possibility of extending manifolds to infinite dimensions.\n\nA further generalisation is to Fréchet manifolds, replacing Banach spaces by Fréchet spaces. On the other hand, a Hilbert manifold is a special case of a Banach manifold in which the manifold is locally modelled on Hilbert spaces.\n\nLet \"X\" be a set. An atlas of class \"C\", \"r\" ≥ 0, on \"X\" is a collection of pairs (called charts) (\"U\", \"φ\"), \"i\" ∈ \"I\", such that\n\nOne can then show that there is a unique topology on \"X\" such that each \"U\" is open and each \"φ\" is a homeomorphism. Very often, this topological space is assumed to be a Hausdorff space, but this is not necessary from the point of view of the formal definition.\n\nIf all the Banach spaces \"E\" are equal to the same space \"E\", the atlas is called an \"E\"-atlas. However, it is not \"\" necessary that the Banach spaces \"E\" be the same space, or even isomorphic as topological vector spaces. However, if two charts (\"U\", \"φ\") and (\"U\", \"φ\") are such that \"U\" and \"U\" have a non-empty intersection, a quick examination of the derivative of the crossover map\n\nshows that \"E\" and \"E\" must indeed be isomorphic as topological vector spaces. Furthermore, the set of points \"x\" ∈ \"X\" for which there is a chart (\"U\", \"φ\") with \"x\" in \"U\" and \"E\" isomorphic to a given Banach space \"E\" is both open and closed. Hence, one can without loss of generality assume that, on each connected component of \"X\", the atlas is an \"E\"-atlas for some fixed \"E\".\n\nA new chart (\"U\", \"φ\") is called compatible with a given atlas { (\"U\", \"φ\") | \"i\" ∈ \"I\" } if the crossover map\n\nis an \"r\"-times continuously differentiable function for every \"i\" ∈ \"I\". Two atlases are called compatible if every chart in one is compatible with the other atlas. Compatibility defines an equivalence relation on the class of all possible atlases on \"X\".\n\nA \"C\"-manifold structure on \"X\" is then defined to be a choice of equivalence class of atlases on \"X\" of class \"C\". If all the Banach spaces \"E\" are isomorphic as topological vector spaces (which is guaranteed to be the case if \"X\" is connected), then an equivalent atlas can be found for which they are all equal to some Banach space \"E\". \"X\" is then called an \"E\"-manifold, or one says that \"X\" is modeled on \"E\".\n\n\nIt is by no means true that a finite-dimensional manifold of dimension \"n\" is \"globally\" homeomorphic to R, or even an open subset of R. However, in an infinite-dimensional setting, it is possible to classify “well-behaved” Banach manifolds up to homeomorphism quite nicely. A 1969 theorem of David Henderson states that every infinite-dimensional, separable, metric Banach manifold \"X\" can be embedded as an open subset of the infinite-dimensional, separable Hilbert space, \"H\" (up to linear isomorphism, there is only one such space). In fact, Henderson's result is stronger: the same conclusion holds for any metric manifold modeled on a separable infinite-dimensional Fréchet space.\n\nThe embedding homeomorphism can be used as a global chart for \"X\". Thus, in the infinite-dimensional, separable, metric case, the “only” Banach manifolds are the open subsets of Hilbert space.\n\n"}
{"id": "22696380", "url": "https://en.wikipedia.org/wiki?curid=22696380", "title": "Bogomolov–Miyaoka–Yau inequality", "text": "Bogomolov–Miyaoka–Yau inequality\n\nIn mathematics, the Bogomolov–Miyaoka–Yau inequality is the inequality\n\nbetween Chern numbers of compact complex surfaces of general type. Its major interest is the way it restricts the possible topological types of the underlying real 4-manifold. It was proved independently by and , after and proved weaker versions with the constant 3 replaced by 8 and 4.\n\nArmand Borel and Friedrich Hirzebruch showed that the inequality is best possible by finding infinitely many cases where equality holds. The inequality is false in positive characteristic: and gave examples of surfaces in characteristic \"p\", such as generalized Raynaud surfaces, for which it fails.\n\nThe conventional formulation of the Bogomolov–Miyaoka–Yau inequality is as follows. Let \"X\" be a compact complex surface of general type, and let \"c\" = \"c\"(\"X\") and \"c\" = \"c\"(\"X\") be the first and second Chern class of the complex tangent bundle of the surface. Then\n\nMoreover if equality holds then \"X\" is a quotient of a ball. The latter statement is a consequence of Yau's differential geometric approach which is based on his resolution of the Calabi conjecture.\nSince formula_3 is the topological Euler characteristic and by the Thom–Hirzebruch signature theorem formula_4 where formula_5 is the signature of the intersection form on the second cohomology, the Bogomolov–Miyaoka–Yau inequality can also be written as a restriction on the topological type of the surface of general type:\n\nmoreover if formula_7 then the universal covering is a ball.\n\nTogether with the Noether inequality the Bogomolov–Miyaoka–Yau inequality sets boundaries in the search for complex surfaces. Mapping out the topological types that are realized as complex surfaces is called geography of surfaces. see surfaces of general type.\n\nIf \"X\" is a surface of general type with formula_8, so that equality holds in the Bogomolov–Miyaoka–Yau inequality, then proved that \"X\" is isomorphic to a quotient of the unit ball in formula_9 by an infinite discrete group. Examples of surfaces satisfying this equality are hard to find. showed that there are infinitely many values of \"c\" = 3\"c\" for which a surface exists. found a fake projective plane with \"c\" = 3\"c\" = 9, which is the minimum possible value because \"c\" + \"c\" is always divisible by 12, and , , showed that there are exactly 50 fake projective planes.\n\n"}
{"id": "38183665", "url": "https://en.wikipedia.org/wiki?curid=38183665", "title": "Booleo", "text": "Booleo\n\nBooleo (stylized \"bOOleO\") is a strategy card game using boolean logic gates. It was developed by Jonathan Brandt and Chris Kampf with Sean P. Dennis in 2008, and it was first published by Tessera Games LLC in 2009.\n\nThe deck consists of 64 cards:\n\nStarting with a line of Initial Binary cards laid perpendicular to two facing players, the object of the game is to be the first to complete a logical pyramid whose final output equals that of the rightmost Initial Binary card facing that player.\n\nThe game is played in “draw one play one” format. The pyramid consists of decreasing rows of gate cards, where the outputs of any contiguous pair of cards comprise the input values to a single card in the following row. The pyramid, therefore, has Initial Binary values as its base and tapers to a single card closest to the player. By tracing the “flow” of values through any series of gate, every card placed in the pyramid must make “logical sense”, i.e. the inputs and output value of every gate card must conform to the rule of that gate card.\n\nThe NOT cards are played against any of the Initial Binary cards in play, causing that card to be rotated 180 degrees, literally “flipping” the value of that card from 0 to 1 or vice versa.\n\nBy changing the value of any Initial Binary, any and all gate cards which “flow” from it must be re-evaluated to ensure its placement makes “logical sense”. If it does not, that gate card is removed from the player's pyramid.\n\nSince both players' pyramids share the Initial Binary cards as a base, “flipping” an Initial Binary has an effect on both players' pyramids. A principal strategy during game play is to invalidate gate cards in the opponent's logic pyramid while rendering as little damage to one’s own pyramid in the process.\n\nSome logic gates are more robust than others to a change to their inputs. Therefore, not all logic gate cards have the same strategic value. \n\nThe standard edition of the game does not contain NAND, NOR, or XNOR gates. It is possible, therefore, for a player to arrive at an unresolvable pair of inputs.\n\nThe number of cards in Booleo will comfortably support a match between two players whose logic pyramids are six cards wide at their base. By combining decks, it is possible to construct larger pyramids or to have matches among more than two players. For example:\n\nTessera Games also published \"bOOleO-N Edition\", which is identical to Booleo with the exception that it uses the inverse set of logic gates: NAND, NOR, and XNOR. bOOleO-N Edition may be played on its own, or it may be combined with Booleo.\n"}
{"id": "1341563", "url": "https://en.wikipedia.org/wiki?curid=1341563", "title": "Broken space diagonal", "text": "Broken space diagonal\n\nIn a magic cube, a broken space diagonal is a sequence of cells of the cube that follows a line parallel to a space diagonal of the cube, and continues on the corresponding point of an opposite face whenever it reaches a face of the cube. Broken space diagonals are also known as broken triagonals. The corresponding concept in two-dimensional magic squares is a broken diagonal.\n"}
{"id": "37253906", "url": "https://en.wikipedia.org/wiki?curid=37253906", "title": "Bruce Reed (mathematician)", "text": "Bruce Reed (mathematician)\n\nBruce Alan Reed is a Canadian mathematician and computer scientist, the Canada Research Chair in Graph Theory and a professor of computer science at McGill University. His research is primarily in graph theory.\n\nReed earned his Ph.D. in 1986 from McGill, under the supervision of Vašek Chvátal. Before returning to McGill as a Canada Research Chair, Reed held positions at the University of Waterloo, Carnegie Mellon University, and the French National Centre for Scientific Research.\n\nReed was elected as a fellow of the Royal Society of Canada in 2009, and is the recipient of the 2013 CRM-Fields-PIMS Prize.\n\nReed's thesis research concerned perfect graphs.\nWith Michael Molloy, he is the author of a book on graph coloring and the probabilistic method. Reed has also published highly cited papers on the giant component in random graphs with a given degree sequence, random satisfiability problems, acyclic coloring, tree decomposition, and constructive versions of the Lovász local lemma.\n\nHe was an invited speaker at the International Congress of Mathematicians in 2002. His talk there concerned a proof by Reed and Benny Sudakov, using the probabilistic method, of a conjecture by Kyoji Ohba that graphs whose number of vertices and chromatic number are (asymptotically) within a factor of two of each other have equal chromatic number and list chromatic number.\n\n"}
{"id": "23030584", "url": "https://en.wikipedia.org/wiki?curid=23030584", "title": "Civil drawing", "text": "Civil drawing\n\nA civil drawing, or site drawing, is a type of technical drawing that shows information about grading, landscaping, or other site details. These drawings are intended to give a clear picture of all things in a construction site to a civil engineer.\n\nCivil drafters prepare drawings and topographical and relief maps used in major construction or civil engineering projects, such as highways, bridges, pipelines, flood control projects, and water and sewage systems.\n\n"}
{"id": "46845197", "url": "https://en.wikipedia.org/wiki?curid=46845197", "title": "ClearVolume", "text": "ClearVolume\n\nClearVolume is an open source real-time live 3D visualization library designed for high-end volumetric light sheet microscopes. ClearVolume enables the live visualization of microscope data - allowing the biologists to immediately decide whether a sample is worth imaging. ClearVolume can easily be integrated into existing Java, C/C++, Python, or LabVIEW based microscope software. It has a dedicated interface to MicroManager/OpenSpim/OpenSpin control software. ClearVolume supports multi-channels, live 3D data streaming from remote microscopes, and uses a multi-pass Fibonacci rendering algorithm that can handle large volumes. Moreover,ClearVolume is integrated into the FiJi/ImageJ2/KNIME ecosystem.\n\n\n"}
{"id": "24104134", "url": "https://en.wikipedia.org/wiki?curid=24104134", "title": "Conditional probability", "text": "Conditional probability\n\nIn probability theory, conditional probability is a measure of the probability of an event (some particular situation occurring) given that (by assumption, presumption, assertion or evidence) another event has occurred. If the event of interest is \"A\" and the event \"B\" is known or assumed to have occurred, \"the conditional probability of \"A\" given \"B\"\", or \"the probability of \"A\" under the condition \"B\"\", is usually written as \"P\"(\"A\"|\"B\"), or sometimes \"P\"(\"A\") or \"P\"(\"A\"/\"B\"). For example, the probability that any given person has a cough on any given day may be only 5%. But if we know or assume that the person has a cold, then they are much more likely to be coughing. The conditional probability of coughing given that you have a cold might be a much higher 75%.\n\nThe concept of conditional probability is one of the most fundamental and one of the most important concepts in probability theory. But conditional probabilities can be quite slippery and require careful interpretation. For example, there need not be a causal relationship between \"A\" and \"B\", and they don’t have to occur simultaneously.\n\n\"P\"(\"A\"|\"B\") may or may not be equal to \"P\"(\"A\") (the unconditional probability of \"A\"). If \"P\"(\"A\"|\"B\") = \"P\"(\"A\"), then events \"A\" and \"B\" are said to be independent: in such a case, having knowledge about either event does not change our knowledge about the other event. Also, in general, \"P\"(\"A\"|\"B\") (the conditional probability of A given B) is not equal to \"P\"(\"B\"|\"A\"). For example, if a person has dengue they might have a 90% chance of testing positive for dengue. In this case what is being measured is that if event \"B\" (\"having dengue\") has occurred, the probability of \"A\" (\"test is positive\") given that \"B\" (\"having dengue\") occurred is 90%: that is, \"P\"(\"A\"|\"B\") = 90%. Alternatively, if a person tests positive for dengue they may have only a 15% chance of actually having dengue because most people do not have dengue and the false positive rate for the test may be high. In this case what is being measured is the probability of the event \"B\" (\"having dengue\") given that the event \"A\" (\"test is positive\") has occurred: \"P\"(\"B\"|\"A\") = 15%. Falsely equating the two probabilities causes various errors of reasoning such as the base rate fallacy. Conditional probabilities can be correctly reversed using Bayes' theorem.\n\nConditional probabilities can be displayed in a conditional probability table.\n\nGiven two events \"A\" and \"B\", from the sigma-field of a probability space, with the unconditional probability of B (that is, of the event \"B\" occurring ) being greater than zero – \"P\"(\"B\") > 0 – the conditional probability of \"A\" given \"B\" is defined as the quotient of the probability of the joint of events \"A\" and \"B\", and the probability of \"B\":\n\nwhere formula_2 is the probability that both events \"A\" and \"B\" occur. This may be visualized as restricting the sample space to situations in which \"B\" occurs. The logic behind this equation is that if the possible outcomes for \"A\" and \"B\" are restricted to those in which \"B\" occurs, this set serves as the new sample space.\n\nNote that this is a definition but not a theoretical result. We just denote the quantity formula_3 as formula_4 and call it the conditional probability of \"A\" given \"B\".\n\nSome authors, such as de Finetti, prefer to introduce conditional probability as an axiom of probability:\n\nAlthough mathematically equivalent, this may be preferred philosophically; under major probability interpretations such as the subjective theory, conditional probability is considered a primitive entity. Further, this \"multiplication axiom\" introduces a symmetry with the summation axiom for mutually exclusive events:\n\nConditional probability can be defined as the probability of a conditional event \nformula_7. Assuming that the experiment underlying the events formula_8 and formula_9 is repeated, the \nGoodman-Nguyen-van Fraassen\nconditional event can be defined as \n\nIt can be shown that \n\nwhich meets the Kolmogorov definition of conditional probability. Note that the equation \nformula_12 is a theoretical result and not a definition. \nThe definition via conditional events can be understood directly in terms of the Kolmogorov axioms and is particularly close to the Kolmogorov interpretation of probability in terms of experimental data. For example, conditional events can be repeated themselves leading to a generalized notion of conditional event \nformula_13. It can be shown that the sequence \nformula_14 is i.i.d., which yields a strong law of large numbers for conditional probability:\n\nIf \"P\"(\"B\") = 0, then according to the simple definition, \"P\"(\"A\"|\"B\") is undefined. However, it is possible to define a conditional probability with respect to a σ-algebra of such events (such as those arising from a continuous random variable).\n\nFor example, if \"X\" and \"Y\" are non-degenerate and jointly continuous random variables with density \"ƒ\"(\"x\", \"y\") then, if \"B\" has positive measure,\n\nThe case where \"B\" has zero measure is problematic. For the case that \"B\" = {\"y\"}, representing a single point, the conditional probability could be defined as\n\nhowever this approach leads to the Borel–Kolmogorov paradox. The more general case of zero measure is even more problematic, as can be seen by noting that the limit, as all \"δy\" approach zero, of\ndepends on their relationship as they approach zero. See conditional expectation for more information.\n\nConditioning on an event may be generalized to conditioning on a random variable. Let \"X\" be a random variable; we assume for the sake of presentation that \"X\" is discrete, that is, \"X\" takes on only finitely many values \"x\". Let \"A\" be an event. The conditional probability of \"A\" given \"X\" is defined as the random variable, written \"P\"(\"A\"|\"X\")\", that takes on the value\n\nwhenever\n\nMore formally,\n\nThe conditional probability \"P\"(\"A\"|\"X\") is a function of \"X\": e.g., if the function \"g\" is defined as\n\nthen\n\nNote that \"P\"(\"A\"|\"X\") and \"X\" are now both random variables. From the law of total probability, the expected value of \"P\"(\"A\"|\"X\") is equal to the unconditional probability of \"A\".\n\nThe partial conditional probability \nformula_24\nis about the probability of event \nformula_8 given that each of the condition events\nformula_26 has occurred to a degree\nformula_27 (degree of belief, degree of experience) that might be different from 100%. Frequentistically, partial conditional probability makes sense, if the conditions are tested in experiment repetitions of appropriate length \nformula_28\n. Such \nformula_28-bounded partial conditional probability can be defined as the conditionally expected average occurrence of event\nformula_8 in testbeds of length \nformula_28 that adhere to all of the probability specifications\nformula_32, i.e.:\n\nBased on that, partial conditional probability can be defined as\nwhere formula_35\nJeffrey conditionalization\nis a special case of partial conditional probability in which the condition events must form a partition:\n\nSuppose that somebody secretly rolls two fair six-sided dice, and we wish to compute the probability that the face-up value of the first one is 2, given the information that their sum is no greater than 5.\n\nProbability that \"D1\" = 2\n\nTable 1 shows the sample space of 36 combinations of rolled values of the two dice, each of which occurs probability 1/36, with the numbers displayed in the red and dark gray cells being \"D\"1 + \"D\"2.\n\n\"D1\" = 2 in exactly 6 of the 36 outcomes; thus \"P\"(\"D1\"=2) =  = :\n\nProbability that \"D1\"+\"D2\" ≤ 5 \n\nTable 2 shows that \"D1\"+\"D2\" ≤ 5 for exactly 10 of the 36 outcomes, thus \"P\"(\"D1\"+\"D2\" ≤ 5) = :\n\nProbability that \"D1\" = 2 \"given that\" \"D1\"+\"D2\" ≤ 5 \n\nTable 3 shows that for 3 of these 10 outcomes, \"D1\" = 2.\n\nThus, the conditional probability \"P\"(\"D1\"=2 | \"D1\"+\"D2\" ≤ 5) =  = 0.3:\n\nHere, in the earlier notation for the definition of conditional probability, the conditioning event \"B\" is that \"D1+D2\" ≤ 5, and the event \"A\" is \"D1\" = 2. We have formula_37 as seen in the table.\n\nIn statistical inference, the conditional probability is an update of the probability of an event based on new information. Incorporating the new information can be done as follows:\n\n\nThis approach results in a probability measure that is consistent with the original probability measure and satisfies all the Kolmogorov axioms. This conditional probability measure also could have resulted by assuming that the relative magnitude of the probability of \"A\" with respect to \"X\" will be preserved with respect to \"B\" (cf. a Formal Derivation below).\n\nThe wording \"evidence\" or \"information\" is generally used in the Bayesian interpretation of probability. The conditioning event is interpreted as evidence for the conditioned event. That is, \"P\"(\"A\") is the probability of \"A\" before accounting for evidence \"E\", and \"P\"(\"A\"|\"E\") is the probability of \"A\" after having accounted for evidence \"E\" or after having updated \"P\"(\"A\"). This is consistent with the frequentist interpretation, which is the first definition given above.\n\nEvents \"A\" and \"B\" are defined to be statistically independent if\n\nIf \"P\"(\"B\") is not zero, then this is equivalent to the statement that\n\nSimilarly, if \"P\"(\"A\") is not zero, then\n\nis also equivalent. Although the derived forms may seem more intuitive, they are not the preferred definition as the conditional probabilities may be undefined, and the preferred definition is symmetrical in \"A\" and \"B\".\n\nIndependent events vs. mutually exclusive events\n\nThe concepts of mutually independent events and mutually exclusive events are separate and distinct.\n\nAs noted, statistical independence means\n\nprovided that the probability for the conditioning event is not zero. However, events being mutually exclusive means that\n\nIn fact, mutually exclusive events cannot be statistically independent, since knowing that one occurs gives information about the other (specifically, that it certainly does not occur).\n\nIn general, it cannot be assumed that \"P\"(\"A\"|\"B\") ≈ \"P\"(\"B\"|\"A\"). This can be an insidious error, even for those who are highly conversant with statistics. The relationship between \"P\"(\"A\"|\"B\") and \"P\"(\"B\"|\"A\") is given by Bayes' theorem:\n\nThat is, \"P\"(\"A\"|\"B\") ≈ \"P\"(\"B\"|\"A\") only if \"P\"(\"B\")/\"P\"(\"A\") ≈ 1, or equivalently, \"P\"(\"A\") ≈ \"P\"(\"B\").\n\nIn general, it cannot be assumed that \"P\"(\"A\") ≈ \"P\"(\"A\"|\"B\"). These probabilities are linked through the law of total probability:\n\nwhere the events formula_48 form a countable partition of formula_49.\n\nThis fallacy may arise through selection bias. For example, in the context of a medical claim, let \"S\" be the event that a sequela (chronic disease) \"S\" occurs as a consequence of circumstance (acute condition) \"C\". Let \"H\" be the event that an individual seeks medical help. Suppose that in most cases, \"C\" does not cause \"S\" so \"P\"(\"S\") is low. Suppose also that medical attention is only sought if \"S\" has occurred due to \"C\". From experience of patients, a doctor may therefore erroneously conclude that \"P\"(\"S\") is high. The actual probability observed by the doctor is \"P\"(\"S\"|\"H\").\n\nNot taking prior probability into account partially or completely is called \"base rate neglect\". The reverse, insufficient adjustment from the prior probability is \"conservatism.\n\nFormally, \"P\"(\"A\"|\"B\") is defined as the probability of \"A\" according to a new probability function on the sample space, such that outcomes not in \"B\" have probability 0 and that it is consistent with all original probability measures.\n\nLet Ω be a sample space with elementary events {ω}. Suppose we are told the event \"B\" ⊆ Ω has occurred. A new probability distribution (denoted by the conditional notation) is to be assigned on {ω} to reflect this. For events in \"B\", it is reasonable to assume that the relative magnitudes of the probabilities will be preserved. For some constant scale factor α, the new distribution will therefore satisfy:\n\nSubstituting 1 and 2 into 3 to select α:\n\nSo the new probability distribution is\n\nNow for a general event \"A\",\n\n\n"}
{"id": "346992", "url": "https://en.wikipedia.org/wiki?curid=346992", "title": "Dimension theorem for vector spaces", "text": "Dimension theorem for vector spaces\n\nIn mathematics, the dimension theorem for vector spaces states that all bases of a vector space have equally many elements. This number of elements may be finite or infinite (in the latter case, it is a cardinal number), and defines the dimension of the vector space.\n\nFormally, the dimension theorem for vector spaces states that\n\nAs a basis is a generating set that is linearly independent, the theorem is a consequence of the following theorem, which is also useful:\n\nIn particular if is finitely generated, then all its bases are finite and have the same number of elements.\n\nWhile the proof of the existence of a basis for any vector space in the general case requires Zorn's lemma and is in fact equivalent to the axiom of choice, the uniqueness of the cardinality of the basis requires only the ultrafilter lemma, which is strictly weaker (the proof given below, however, assumes trichotomy, i.e., that all cardinal numbers are comparable, a statement which is also equivalent to the axiom of choice). The theorem can be generalized to arbitrary -modules for rings having invariant basis number.\n\nIn the finitely generated case the proof uses only elementary arguments of algebra, and does not require the axiom of choice nor its weaker variants.\n\nLet be a vector space, } be a linearly independent set of elements of , and } be a generating set. One has to prove that the cardinality of is not larger than that of .\n\nIf is finite, this results from the Steinitz exchange lemma. (Indeed, the Steinitz exchange lemma implies every finite subset of has cardinality not larger than that of , hence is finite with cardinality not larger than that of .) If is finite, a proof based on matrix theory is also possible. \n\nAssume that is infinite. If is finite, there is nothing to prove. Thus, we may assume that is also infinite. Let us suppose that the cardinality of is larger than that of . We have to prove that this leads to a contradiction. \n\nBy Zorn's lemma, every linearly independent set is contained in a maximal linearly independent set . This maximality implies that spans and is therefore a basis (the maximality implies that every element of is linearly dependent from the elements of , and therefore is a linear combination of elements of . As the cardinality of is greater or equal with the cardinality of , one may replace with , that is, one may suppose, without loss of generality, that is a basis. \n\nThus, every can be written as a finite sum\nwhere formula_2 is a finite subset of formula_3.\nAs is infinite, formula_4 has the same cardinality as . Therefore formula_4 has cardinality smaller than that of . \nSo there is some formula_6 which does not appear\nin any formula_2. The corresponding formula_8 can be expressed as a finite linear combination of formula_9's, which in turn can be expressed as finite linear combination of formula_10's, not involving formula_8. Hence formula_12 is linearly dependent on the other formula_13's, which provides the desired contradiction.\n\nThis application of the dimension theorem is sometimes itself called the \"dimension theorem\". Let\n\nbe a linear transformation. Then\n\nthat is, the dimension of \"U\" is equal to the dimension of the transformation's range plus the dimension of the kernel. See rank–nullity theorem for a fuller discussion.\n"}
{"id": "27776666", "url": "https://en.wikipedia.org/wiki?curid=27776666", "title": "Elliott Mendelson", "text": "Elliott Mendelson\n\nElliott Mendelson (born 1931) is an American logician. He was a professor of mathematics at Queens College of the City University of New York, and the Graduate Center, CUNY. He was Jr. Fellow, Society of Fellows, Harvard University, 1956-58. \n\nMendelson taught mathematics at the college level for more than 30 years, and is the author of books on logic, philosophy of mathematics, calculus, game theory and mathematical analysis.\n\nHis \"Introduction to Mathematical Logic\" was reviewed by Dirk van Dalen who noted that it included \"a large variety of subjects that should be part of the education of any mathematics student with an interest in foundational matters.\"\n\n\n\n"}
{"id": "7684634", "url": "https://en.wikipedia.org/wiki?curid=7684634", "title": "Factor-critical graph", "text": "Factor-critical graph\n\nIn graph theory, a mathematical discipline, a factor-critical graph (or hypomatchable graph) is a graph with vertices in which every subgraph of vertices has a perfect matching. (A perfect matching in a graph is a subset of its edges with the property that each of its vertices is the endpoint of exactly one of the edges in the subset.)\n\nA matching that covers all but one vertex of a graph is called a near-perfect matching. So equivalently, a factor-critical graph is a graph in which there are near-perfect matchings that avoid every possible vertex.\n\nAny odd-length cycle graph is factor-critical, as is any complete graph with an odd number of vertices. More generally, every Hamiltonian graph with an odd number of vertices is factor-critical. The friendship graphs (graphs formed by connecting a collection of triangles at a single common vertex) provide examples of graphs that are factor-critical but not Hamiltonian.\n\nIf a graph is factor-critical, then so is the Mycielskian of . For instance, the Grötzsch graph, the Mycielskian of a five-vertex cycle-graph, is factor-critical.\n\nEvery 2-vertex-connected claw-free graph with an odd number of vertices is factor-critical. For instance, the 11-vertex graph formed by removing a vertex from the regular icosahedron (the graph of the gyroelongated pentagonal pyramid) is both 2-connected and claw-free, so it is factor-critical. This result follows directly from the more fundamental theorem that every connected claw-free graph with an even number of vertices has a perfect matching.\n\nFactor-critical graphs may be characterized in several different ways, other than their definition as graphs in which each vertex deletion allows for a perfect matching:\n\nFactor-critical graphs must always have an odd number of vertices, and must be 2-edge-connected (that is, they cannot have any bridges). However, they are not necessarily 2-vertex-connected; the friendship graphs provide a counterexample. It is not possible for a factor-critical graph to be bipartite, because in a bipartite graph with a near-perfect matching, the only vertices that can be deleted to produce a perfectly matchable graph are the ones on the larger side of the bipartition.\n\nEvery 2-vertex-connected factor-critical graph with edges has at least different near-perfect matchings, and more generally every factor-critical graph with edges and blocks (2-vertex-connected components) has at least different near-perfect matchings. The graphs for which these bounds are tight may be characterized by having odd ear decompositions of a specific form.\n\nAny connected graph may be transformed into a factor-critical graph by contracting sufficiently many of its edges. The minimal sets of edges that need to be contracted to make a given graph factor-critical form the bases of a matroid, a fact that implies that a greedy algorithm may be used to find the minimum weight set of edges to contract to make a graph factor-critical, in polynomial time.\n\nA blossom is a factor-critical subgraph of a larger graph. Blossoms play a key role in Jack Edmonds' algorithms for maximum matching and minimum weight perfect matching in non-bipartite graphs.\n\nIn polyhedral combinatorics, factor-critical graphs play an important role in describing facets of the matching polytope of a given graph.\n\nA graph is said to be -factor-critical if every subset of vertices has a perfect matching. Under this definition, a hypomatchable graph is 1-factor-critical. Even more generally, a graph is -factor-critical if every subset of vertices has an -factor, that is, it is the vertex set of an -regular subgraph of the given graph.\n\nA critical graph (without qualification) is usually assumed to mean a graph for which removing each of its vertices reduces the number of colors it needs in a graph coloring. The concept of criticality has been used much more generally in graph theory to refer to graphs for which removing each possible vertex changes or does not change some relevant property of the graph. A matching-critical graph is a graph for which the removal of any vertex does not change the size of a maximum matching; by Gallai's characterization, the matching-critical graphs are exactly the graphs in which every connected component is factor-critical. The complement graph of a critical graph is necessarily matching-critical, a fact that was used by Gallai to prove lower bounds on the number of vertices in a critical graph.\n\nBeyond graph theory, the concept of factor-criticality has been extended to matroids by defining a type of ear decomposition on matroids and defining a matroid to be factor-critical if it has an ear decomposition in which all ears are odd.\n"}
{"id": "56120789", "url": "https://en.wikipedia.org/wiki?curid=56120789", "title": "Friedrich Hultsch", "text": "Friedrich Hultsch\n\nFriedrich Otto Hultsch (22 July 1833, Dresden – 6 April 1906, Dresden) was a German classical philologist and historian of mathematics in antiquity.\n\nAfter graduating from the Dresden \"Kreuzschule\", Friedrich Hultsch studied classical philology at the University of Leipzig from 1851 to 1855. After a probationary year at the \"Kreuzschule\", he was employed in 1857 as a second \"Adjunkt\" at the \"Alte Nikolaischule\" in Leipzig. In 1858 he became a teacher at the Zwickau \"Gymnasium\". In 1861 Hultsch was again employed at the \"Kreuzschule\", where he was the rector from 1868 until his retirement in 1889. From 1879 to 1882 he also headed the newly founded \"Wettiner Gymnasium\".\n\nHultsch specialized in historical metrology and textual criticism concerning mathematical antiquity.\n\nHis most important works are:\n\nHe wrote many articles on Greek mathematics in Pauly-Wissowa (\"e.g.\" Archimedes and Euclid).\n\nHultsch died in 1906 in Dresden and was buried in the \"Trinitatisfriedhof\". He was elected a member of the Saxon Academy of Sciences of Leipzig (1885) and a corresponding member of the Göttingen Academy of Sciences and Humanities.\n\n"}
{"id": "53762899", "url": "https://en.wikipedia.org/wiki?curid=53762899", "title": "Fusion category", "text": "Fusion category\n\nIn mathematics, a fusion category is a category that is rigid, semisimple, formula_1-linear, monoidal and has only finitely many isomorphism classes of simple objects, such that the monoidal unit is simple. If the ground field formula_1 is algebraically closed, then the latter is equivalent to formula_3 by Schur's lemma.\n\n\nUnder Tannaka-Krein duality, every fusion category arises as the representations of a weak Hopf algebra.\n"}
{"id": "11118285", "url": "https://en.wikipedia.org/wiki?curid=11118285", "title": "Harry Lindgren", "text": "Harry Lindgren\n\nHarry Lindgren (1912–1992) was a British/Australian engineer, linguist and amateur mathematician. He was born in Newcastle-on-Tyne in England.\n\nIn 1935 he emigrated to Australia. He received a BSc degree from the University of Western Australia and later became a Commonwealth Patent Officer.\n\nLindgren published articles in several mathematical journals which culminated in his famous book 'Geometric Dissections' (Van Nostrand 1964), which explores techniques for devising and solving dissection puzzles. When published, his work was the only complete treatment of this subject in any language. This remained true for a third of a century.\n\nIn 1969 Lindgren published \"Spelling Reform: A New Approach\" (Alpha Books, 104 Bathurst St, Sydney 2000, Australia). This work outlines a proposal for introducing phoneme-by-phoneme adjustment of English spelling, in order that spellings may more accurately represent the sounds of the speech they denote. The book features several cartoons illustrating the absurdities of existing spellings.\n\nOn 1 September 1971 Lindgren launched the Spelling Action Society to promote his suggested reforms. He chose the name to share its initials with Scandinavia's SAS airline to acknowledge his Nordic ancestry. He published the newsletter Spelling Action under the society to promote use of his Spelling Reform step One (SR1). As Lindgren's health deteriorated in later life the newsletter was taken over by Gary Jimmieson (and later Doug Everingham).\n"}
{"id": "200374", "url": "https://en.wikipedia.org/wiki?curid=200374", "title": "Henry Thomas Colebrooke", "text": "Henry Thomas Colebrooke\n\nHenry Thomas Colebrooke FRS FRSE (15 June 1765 – 10 March 1837) was an English orientalist and mathematician. He has been described as \"the first great Sanskrit scholar in Europe\".\n\nHenry Thomas Colebrooke was born on 15 June 1765. His parents were Sir George Colebrooke, 2nd Baronet, MP for Arundel and Chairman of the East India Company from 1769, and Mary Gaynor, daughter and heir of Patrick Gaynor of Antigua. He was educated at home.\n\nIn 1782 Colebrooke was appointed through his father's influence to a writership with the East a India Company in Calcutta. In 1786 and three years later he was appointed assistant collector in the revenue department at Tirhut. He wrote \"Remarks on the Husbandry and Commerce of Bengal\", which was privately publisher in 1795, by which time he had transferred to Purnia. This opposed the East India Company's monopoly on Indian trade, advocating instead for free trade between Britain and India, which caused offence to the East India Company's governors.\n\nHe was appointed to the magistracy of Mirzapur in 1795 and was sent to Nagpur in 1799 to negotiate am allowance with the Raja of Berar. He was unsuccessful in this, due to events elsewhere, and returned in 1801.\nOn his return was made a judge of the new court of appeal in Calcutta, of which he became president of the bench in 1805. Also in 1805, Lord Wellesley appointed him honorary professor of Hindu law and Sanskrit at the college of Fort William. In 1807 he became a member of council, serving for five years, and was elected President of the Asiatic Society of Calcutta. He returned to England in 1815.\n\nIn 1816 he was elected to the fellowship of both the Royal Society and the Royal Society of Edinburgh In 1820 he was a founder of the Royal Astronomical Society. He often chaired the society's meetings in the absence of the first president, William Herschel, and was elected as its second president on Herschel's death, serving 1823–1825. In 1823 he was also a founder of the Royal Asiatic Society, chairing its first meeting although he declined to become its president.\n\nAfter eleven years' residence in India, Colebrooke began the study of the Sanskrit language; and to him was entrusted the translation of the major \"Digest of Hindu Laws\", a monumental study of Hindu law which had been left unfinished by Sir William Jones. He translated the two treatises, the \"Mitacshara\" of Vijnaneshwara and the \"Dayabhaga\" of Jimutavahana, under the title \"Law of Inheritance\". During his residence at Calcutta he wrote his \"Sanskrit Grammar\" (1805), some papers on the religious ceremonies of the Hindus, and his \"Essay on the Vedas\" (1805), for a long time the standard work in English on the subject.\n\n\nA posthumous essay on his father's life was published by Sir T. E. Colebrooke in 1873 as part of a reprinting of \"Miscellaneous Essays\".\n\nColebrooke married Elizabeth Wilkinson in 1810. The marriage was short-lived and she died in 1814. Colebrooke had several illegitimate children from Indian women.\n\n"}
{"id": "5747450", "url": "https://en.wikipedia.org/wiki?curid=5747450", "title": "Heronian mean", "text": "Heronian mean\n\nIn mathematics, the Heronian mean \"H\" of two non-negative real numbers \"A\" and \"B\" is given by the formula:\n\nIt is named after Hero of Alexandria.\n\nThe Heronian mean may be used in finding the volume of a frustum of a pyramid or cone. The volume is equal to the product of the height of the frustum and the Heronian mean of the areas of the opposing parallel faces.\n\nThe Heronian mean of the numbers \"A\" and \"B\" is a weighted mean of their arithmetic and geometric means:\n\n\n"}
{"id": "23652892", "url": "https://en.wikipedia.org/wiki?curid=23652892", "title": "Hitchin system", "text": "Hitchin system\n\nIn mathematics, the Hitchin integrable system is an integrable system depending on the choice of a complex reductive group and a compact Riemann surface, introduced by Nigel Hitchin in 1987. \nIt lies on the crossroads\nof algebraic geometry, the theory of Lie algebras and integrable system theory. \nIt also plays an important role in geometric Langlands correspondence over the field of complex numbers; related to conformal field theory. A genus zero analogue of the Hitchin system arises as a certain limit of the Knizhnik–Zamolodchikov equations. Almost all integrable systems of classical mechanics can be obtained as particular cases of the Hitchin system (or its meromorphic generalization or in a singular limit).\n\nThe Hitchin fibration is the map from the moduli space of Hitchin pairs to characteristic polynomials. \n\nUsing the language of algebraic geometry, the phase space of the system is a partial compactification of the cotangent bundle to the moduli space of stable \"G\"-bundles for some reductive group \"G\", on some compact algebraic curve. This space is endowed with a canonical symplectic form. Suppose for simplicity that \"G\"=GL(\"n\"), the general linear group; then the hamiltonians can be described as follows: the tangent space to \"G\"-bundles at the bundle \"F\" is \n\nwhich by Serre duality is dual to \n\nso a pair \n\ncalled a Hitchin pair or Higgs bundle, defines a point in the cotangent bundle. Taking \n\none obtains elements in \n\nwhich is a vector space which does not depend on formula_3. So taking any basis in these vector spaces we obtain functions \"H\", which are Hitchin's hamiltonians. The construction for general reductive group is similar and uses invariant polynomials on the Lie algebra of \"G\".\n\nFor trivial reasons these functions are algebraically independent, and some calculations show that their number is exactly half of the dimension of the phase space. The nontrivial part is a proof of Poisson commutativity of these functions.\n\n"}
{"id": "5957182", "url": "https://en.wikipedia.org/wiki?curid=5957182", "title": "Independent equation", "text": "Independent equation\n\nAn independent equation is an equation in a system of simultaneous equations which cannot be derived algebraically from the other equations. The concept typically arises in the context of linear equations. If it is possible to duplicate one of the equations in a system by multiplying each of the other equations by some number (potentially a different number for each equation) and summing the resulting equations, then that equation is dependent on the others. But if this is not possible, then that equation is independent of the others.\n\nIf an equation is independent of the other equations in its system, then it provides information beyond that which is provided by the other equations. In contrast, if an equation is dependent on the others, then it provides no information not contained in the others collectively, and the equation can be dropped from the system without any information loss.\n\nThe number of independent equations in a system equals the rank of the augmented matrix of the system—the system's coefficient matrix with one additional column appended, that column being the column vector of constants.\n\nThe number of independent equations in a system of consistent equations (a system that has at least one solution) can never be greater than the number of unknowns. Equivalently, if a system has more independent equations than unknowns, it is inconsistent and has no solutions.\n\n"}
{"id": "1142530", "url": "https://en.wikipedia.org/wiki?curid=1142530", "title": "Jean Gaston Darboux", "text": "Jean Gaston Darboux\n\nJean-Gaston Darboux FAS MIF FRS FRSE (14 August 1842 – 23 February 1917) was a French mathematician.\n\nAccording to his birth certificate, he was born in Nîmes in France on 14 August 1842, at 1 am. However, probably due to the midnight birth, Darboux himself usually reported his own birthday as 13 August, \"e.g.\" in his filled form for Légion d'Honneur.\n\nHis parents were François Darboux, businessman of mercery, and Alix Gourdoux. The father died when Gaston was 7. His mother undertook the mercery business with great courage, and insisted that her children receive good education. Gaston had a younger brother, Louis, who taught mathematics at the Lycée Nîmes for almost his entire life. \n\nHe studied at the Nîmes Lycée and the Montpellier Lycée before being accepted at the École normale supérieure in 1861, and received his Ph.D. there in 1866. His thesis, written under the direction of Michel Chasles, was titled \"Sur les surfaces orthogonales\". During his studies at the ENS, he also took lectures in Sorbonne University and Collège de France.\n\nIn 1870, he co-founded the journal \"Bulletin des sciences mathématiques et astronomiques\", called \"Darboux's Journal\" by his contemporary mathematicians. \n\nIn 1872, he married the Beauvaisian milliner Amélie \"Célina\" Carbonnier (1848-1911), daughter of Charles Louis Carbonnier, tailor, and Marie Victorine Anastase Hènocq. He and Célina had two children, Jean-\"Gaston\" (1870-1921), who was born at the time of the Siege of Paris and later became a marine zoologist at the Faculty of Science in Marseille, and Anaïs Berthe \"Lucie\" (1873-1970). \n\nHe participated in the foundation of the École normale supérieure de jeunes filles in 1880, an institute that aimed at training female educators and ran parallel to the École normale supérieure on rue d'Ulm. Its first director was Julie Favre.\n\nIn 1884, Darboux was elected to the Académie des Sciences. \n\nDarboux made several important contributions to geometry and mathematical analysis (see, for example, linear PDEs). He was a biographer of Henri Poincaré and he edited the Selected Works of Joseph Fourier.\n\nAmong his students were Émile Borel, Élie Cartan, Émile Picard, Gheorghe Țițeica and Stanisław Zaremba.\n\nDarboux's contribution to the differential geometry of surfaces appears in the four-volume collection of studies he published between 1887 and 1896; see links below for access to these texts.\n\nIn 1900, he was appointed the Academy's permanent secretary of its Mathematics section.\n\nIn 1902, he was elected to the Royal Society; in 1916, he received the Sylvester Medal from the Society. In 1908, he was a plenary speaker at the International Congress of Mathematicians in Rome.\n\nThere are many things named after him:\n\n1887–96. \"Leçons sur la théorie générale des surfaces et les applications géométriques du calcul infinitésimal\". Gauthier-Villars:\n1898. \"Leçons sur les systèmes orthogonaux et les coordonnées curvilignes\". Tome I. Gauthier-Villars.\n\n\n\n"}
{"id": "13816037", "url": "https://en.wikipedia.org/wiki?curid=13816037", "title": "Kervaire invariant", "text": "Kervaire invariant\n\nThe Kervaire invariant is an invariant of a framed formula_1-dimensional manifold that measures whether the manifold could be surgically converted into a sphere. This invariant evaluates to 0 if the manifold can be converted to a sphere, and 1 otherwise. This invariant was named after Michel Kervaire who built on work of Cahit Arf.\n\nThe Kervaire invariant is defined as the Arf invariant of the skew-quadratic form on the middle dimensional homology group. It can be thought of as the simply-connected \"quadratic\" L-group formula_2, and thus analogous to the other invariants from L-theory: the signature, a formula_3-dimensional invariant (either symmetric or quadratic, formula_4), and the De Rham invariant, a formula_5-dimensional \"symmetric\" invariant formula_6\n\nIn any given dimension, there are only two possibilities: either all manifolds have Arf–Kervaire invariant equal to 0, or half have Arf–Kervaire invariant 0 and the other half have Arf–Kervaire invariant 1.\n\nThe Kervaire invariant problem is the problem of determining in which dimensions the Kervaire invariant can be nonzero. For differentiable manifolds, this can happen in dimensions 2, 6, 14, 30, 62, and possibly 126, and in no other dimensions. The final case of dimension 126 remains open.\n\nThe Kervaire invariant is the Arf invariant of the quadratic form determined by the framing on the middle-dimensional formula_7-coefficient homology group\n\nand is thus sometimes called the Arf–Kervaire invariant. The quadratic form (properly, skew-quadratic form) is a quadratic refinement of the usual ε-symmetric form on the middle dimensional homology of an (unframed) even-dimensional manifold; the framing yields the quadratic refinement.\n\nThe quadratic form \"q\" can be defined by algebraic topology using functional Steenrod squares, and geometrically via the self-intersections \nof immersions formula_9formula_10 formula_11 determined by the framing, or by the triviality/non-triviality of the normal bundles of embeddings formula_9formula_10 formula_11 (for formula_15) and the mod 2 Hopf invariant of maps formula_16\n(for formula_17).\n\nThe Kervaire invariant is a generalization of the Arf invariant of a framed surface (= 2-dimensional manifold with stably trivialized tangent bundle) which was used by Lev Pontryagin in 1950 to compute the homotopy group formula_18 of maps formula_19 formula_10 formula_21 (for formula_22), which is the cobordism group of surfaces embedded in formula_19 with trivialized normal bundle.\n\nwhere formula_26 is the cyclic subgroup of \"n\"-spheres that bound a parallelizable manifold of dimension formula_27, formula_28 is the \"n\"th stable homotopy group of spheres, and \"J\" is the image of the J-homomorphism, which is also a cyclic group. The groups formula_26 and formula_30 have easily understood cyclic factors, which are trivial or order two except in dimension formula_31, in which case they are large, with order related to the Bernoulli numbers. The quotients are the difficult parts of the groups. The map between these quotient groups is either an isomorphism or is injective and has an image of index 2. It is the latter if and only if there is an \"n\"-dimensional framed manifold of nonzero Kervaire invariant, and thus the classification of exotic spheres depends up to a factor of 2 on the Kervaire invariant problem.\n\nFor the standard embedded torus, the skew-symmetric form is given by formula_32 (with respect to the standard symplectic basis), and the skew-quadratic refinement is given by formula_33 with respect to this basis: formula_34: the basis curves don't self-link; and formula_35: a (1,1) self-links, as in the Hopf fibration. This form thus has Arf invariant 0 (most of its elements have norm 0; it has isotropy index 1), and thus the standard embedded torus has Kervaire invariant 0.\n\nThe question of in which dimensions \"n\" there are \"n\"-dimensional framed manifolds of nonzero Kervaire invariant is called the Kervaire invariant problem. This is only possible if \"n\" is 2 mod 4, and indeed one must have \"n\" is 2 − \"2\" (two less than a power of two). The question is almost completely resolved; only the case of dimension 126 is open: there are manifolds with nonzero Kervaire invariant in dimension 2, 6, 14, 30, 62, and none in all other dimensions other than possibly 126.\n\nThe main results are those of , who reduced the problem from differential topology to stable homotopy theory and showed that the only possible dimensions are formula_36, and those of , who showed that there were no such manifolds for formula_37 (formula_38). Together with explicit constructions for lower dimensions (through 62), this leaves open only dimension 126.\n\nIt is conjectured by Michael Atiyah that there is such a manifold in dimension 126, and that the higher-dimensional manifolds with nonzero Kervaire invariant are related to well-known exotic manifolds two dimension higher, in dimensions 16, 32, 64, and 128, namely the Cayley projective plane formula_39 (dimension 16, octonionic projective plane) and the analogous Rosenfeld projective planes (the bi-octonionic projective plane in dimension 32, the quateroctonionic projective plane in dimension 64, and the octo-octonionic projective plane in dimension 128), specifically that there is a construction that takes these projective planes and produces a manifold with nonzero Kervaire invariant in two dimensions lower.\n\n\nThe Kervaire–Milnor invariant is a closely related invariant of framed surgery of a 2, 6 or 14-dimensional framed manifold, that gives isomorphisms from the 2nd and 6th stable homotopy group of spheres to Z/2Z, \nand a homomorphism from the 14th stable homotopy group of spheres onto Z/2Z. For \"n\" = 2, 6, 14 there is an\nexotic framing on \"S\" x \"S\" with Kervaire-Milnor invariant 1.\n\n\n\n"}
{"id": "47161006", "url": "https://en.wikipedia.org/wiki?curid=47161006", "title": "L(h, k)-coloring", "text": "L(h, k)-coloring\n\nL(\"h\", \"k\") coloring in graph theory, is a (proper) vertex coloring in which every pair of adjacent vertices has color numbers that differ by at least \"h\", and any pair of vertices at distance 2 have their colors differ by at least \"k\". When \"h\"=1 and \"k\"=0, it is the usual (proper) vertex coloring.\n"}
{"id": "18931487", "url": "https://en.wikipedia.org/wiki?curid=18931487", "title": "Lacunarity", "text": "Lacunarity\n\nLacunarity, from the Latin lacuna meaning \"gap\" or \"lake\", is a specialized term in geometry referring to a measure of how patterns, especially fractals, fill space, where patterns having more or larger gaps generally have higher lacunarity. Beyond being an intuitive measure of gappiness, lacunarity can quantify additional features of patterns such as \"rotational invariance\" and more generally, heterogeneity. This is illustrated in Figure 1 showing three fractal patterns. When rotated 90°, the first two fairly homogeneous patterns do not appear to change, but the third more heterogeneous figure does change and has correspondingly higher lacunarity. The earliest reference to the term in geometry is usually attributed to Mandelbrot, who, in 1983 or perhaps as early as 1977, introduced it as, in essence, an adjunct to fractal analysis. Lacunarity analysis is now used to characterize patterns in a wide variety of fields and has application in multifractal analysis in particular (see Applications).\n\nIn many patterns or data sets, lacunarity is not readily perceivable or quantifiable, so computer-aided methods have been developed to calculate it. As a measurable quantity, lacunarity is often denoted in scientific literature by the Greek letters formula_1 or formula_2 but it is important to note that there is no single standard and several different methods exist to assess and interpret lacunarity.\n\nOne well-known method of determining lacunarity for patterns extracted from digital images uses box counting, the same essential algorithm typically used for some types of fractal analysis. Similar to looking at a slide through a microscope with changing levels of magnification, box counting algorithms look at a digital image from many levels of resolution to examine how certain features change with the size of the element used to inspect the image. Basically, the arrangement of pixels is measured using traditionally square (i.e., box-shaped) elements from an arbitrary set of formula_3 sizes, conventionally denoted formula_4s. For each formula_4, the box is placed successively over the entire image, and each time it is laid down, the number of pixels that fall within the box is recorded. In standard box counting, the box for each formula_4 in formula_3 is placed as though it were part of a grid overlaid on the image so that the box does not overlap itself, but in sliding box algorithms the box is slid over the image so that it overlaps itself and the \"Sliding Box Lacunarity\" or SLac is calculated. Figure 2 illustrates both types of box counting.\n\nThe data gathered for each formula_4 are manipulated to calculate lacunarity. One measure, denoted here as formula_9, is found from the coefficient of variation (formula_10), calculated as the standard deviation (formula_11) divided by the mean (formula_12), for pixels per box. Because the way an image is sampled will depend on the arbitrary starting location, for any image sampled at any formula_4 there will be some number (formula_14) of possible orientations, each denoted here by formula_15, that the data can be gathered over, which can have varying effects on the measured distribution of pixels. Equation shows the basic method of calculating formula_16:\n\nAlternatively, some methods sort the numbers of pixels counted into a probability distribution having formula_17 bins, and use the bin sizes (masses, formula_18) and their corresponding probabilities (formula_19) to calculate formula_16 according to Equations through : \n\nLacunarity based on formula_16 has been assessed in several ways including by using the variation in or the average value of formula_16 for each formula_4 (see Equation ) and by using the variation in or average over all grids (see Equation ).\n\nLacunarity analyses using the types of values discussed above have shown that data sets extracted from dense fractals, from patterns that change little when rotated, or from patterns that are homogeneous, have low lacunarity, but as these features increase, so generally does lacunarity. In some instances, it has been demonstrated that fractal dimensions and values of lacunarity were correlated, but more recent research has shown that this relationship does not hold for all types of patterns and measures of lacunarity. Indeed, as Mandelbrot originally proposed, lacunarity has been shown to be useful in discerning amongst patterns (e.g., fractals, textures, etc.) that share or have similar fractal dimensions in a variety of scientific fields including neuroscience.\n\nOther methods of assessing lacunarity from box counting data use the relationship between values of lacunarity (e.g., formula_16) and formula_4 in different ways from the ones noted above. One such method looks at the formula_27 vs formula_27 plot of these values. According to this method, the curve itself can be analyzed visually, or the slope at formula_15 can be calculated from the formula_27 vs formula_27 regression line. Because they tend to behave in certain ways for respectively mono-, multi-, and non-fractal patterns, formula_27 vs formula_27 lacunarity plots have been used to supplement methods of classifying such patterns.\n\nTo make the plots for this type of analysis, the data from box counting first have to be transformed as in Equation :\n\nThis transformation avoids undefined values, which is important because homogeneous images will have formula_11 at some formula_4 equal to 0 so that the slope of the formula_27 vs formula_27 regression line would be impossible to find. With formula_38, homogeneous images have a slope of 0, corresponding intuitively to the idea of no rotational or translational invariance and no gaps.\n\nOne box counting technique using a \"gliding\" box calculates lacunarity according to: \n\nformula_39 is the number of filled data points in the box and formula_40 the normalized frequency distribution of formula_39 for different box sizes.\n\nAnother proposed way of assessing lacunarity using box counting, the \"Prefactor\" method, is based on the value obtained from box counting for the fractal dimension (formula_42). This statistic uses the variable formula_43 from the scaling rule formula_44, where formula_43 is calculated from the y-intercept (formula_46) of the ln-ln regression line for formula_4 and either the count (formula_48) of boxes that had any pixels at all in them or else formula_18 at formula_50. formula_43 is particularly affected by image size and the way data are gathered, especially by the lower limit of formula_4s used. The final measure is calculated as shown in Equations through :\n\nBelow is a list of some fields where lacunarity plays an important role, along with links to relevant research illustrating practical uses of lacunarity.\n\n"}
{"id": "2070188", "url": "https://en.wikipedia.org/wiki?curid=2070188", "title": "Lebesgue constant (interpolation)", "text": "Lebesgue constant (interpolation)\n\nIn mathematics, the Lebesgue constants (depending on a set of nodes and of its size) give an idea of how good the interpolant of a function (at the given nodes) is in comparison with the best polynomial approximation of the function (the degree of the polynomials are obviously fixed). The Lebesgue constant for polynomials of degree at most and for the set of nodes is generally denoted by . These constants are named after Henri Lebesgue.\n\nWe fix the interpolation nodes \"x\", ..., \"x\" and an interval [\"a\", \"b\"] containing all the interpolation nodes. The process of interpolation maps the function \"f\" to a polynomial \"p\". This defines a mapping \"X\" from the space \"C\"([\"a\", \"b\"]) of all continuous functions on [\"a\", \"b\"] to itself. The map \"X\" is linear and it is a projection on the subspace of polynomials of degree or less.\n\nThe Lebesgue constant is defined as the operator norm of \"X\". This definition requires us to specify a norm on \"C\"([\"a\", \"b\"]). The maximum norm is usually the most convenient.\n\nThe Lebesgue constant bounds the interpolation error: let denote the best approximation of \"f\" among the polynomials of degree or less. In other words, minimizes among all \"p\" in Π. Then\n\nWe will here prove this statement with the maximum norm.\n\nby the triangle inequality. But \"X\" is a projection on Π, so\n\nThis finishes the proof since formula_3. Note that this relation comes also as a special case of Lebesgue's lemma.\n\nIn other words, the interpolation polynomial is at most a factor worse than the best possible approximation. This suggests that we look for a set of interpolation nodes with a small Lebesgue constant.\n\nThe Lebesgue constant can be expressed in terms of the Lagrange basis polynomials:\n\nIn fact, we have the Lebesgue function\n\nand the Lebesgue constant (or Lebesgue number) for the grid is its maximum value\n\nNevertheless, it is not easy to find an explicit expression for .\n\nIn the case of equidistant nodes, the Lebesgue constant grows exponentially. More precisely, we have the following asymptotic estimate\n\nOn the other hand, the Lebesgue constant grows only logarithmically if Chebyshev nodes are used, since we have\n\nWe conclude again that Chebyshev nodes are a very good choice for polynomial interpolation. However, there is an easy (linear) transformation of Chebyshev nodes that gives a better Lebesgue constant. Let denote the -th Chebyshev node. Then, define\n\nFor such nodes:\n\nThose nodes are, however, not optimal (i.e. they do not minimize the Lebesgue constants) and the search for an optimal set of nodes (which has already been proved to be unique under some assumptions) is still an intriguing topic in mathematics today. However, this set of nodes is optimal for interpolation over formula_11 the set of times differentiable functions whose -th derivatives are bounded in absolute values by a constant as shown by N. S. Hoang. \nUsing a computer, one can approximate the values of the minimal Lebesgue constants, here for the canonical interval :\n\nThere are uncountable infinitely many sets of nodes in [-1,1] that minimize, for fixed > 1, the Lebesgue constant. Though if we assume that we always take −1 and 1 as nodes for interpolation (which is called a \"canonical\" node configuration), then such a set is unique and zero-symmetric. To illustrate this property, we shall see what happens when \"n\" = 2 (i.e. we consider 3 interpolation nodes in which case the property is not trivial). One can check that each set of (zero-symmetric) nodes of type is optimal when (we consider only nodes in [−1, 1]). If we force the set of nodes to be of the type , then \"b\" must equal 0 (look at the Lebesgue function, whose maximum is the Lebesgue constant). All \"arbitrary\" (i.e. zero-symmetric or zero-asymmetric) optimal sets of nodes in [-1,1] when \"n\" = 2 have been determined by F. Schurer, and in an alternative fashion by H.-J. Rack and R. Vajda (2014).\n\nIf we assume that we take −1 and 1 as nodes for interpolation, then as shown by H.-J. Rack (1984 and 2013), for the case \"n\" = 3, the explicit values of the optimal (unique and zero-symmetric) 4 interpolation nodes and the explicit value of the minimal Lebesgue constant are known. All \"arbitrary\" optimal sets of 4 interpolation nodes in [-1,1] when \"n\" = 3 have been explicitly determined, in two different but equivalent fashions, by H.-J. Rack and R. Vajda (2015).\n\nThe Padua points provide another set of nodes with slow growth (although not as slow as the Chebyshev nodes) and with the additional property of being a unisolvent point set.\n\nThe Lebesgue constants also arise in another problem. Let \"p\"(\"x\") be a polynomial of degree expressed in the Lagrangian form associated with the points in the vector \"t\" (i.e. the vector \"u\" of its coefficients is the vector containing the values formula_12). Let formula_13 be a polynomial obtained by slightly changing the coefficients \"u\" of the original polynomial \"p\"(\"x\") to formula_14. Let us consider the inequality:\n\nThis means that the (relative) error in the values of formula_13 will not be higher than the appropriate Lebesgue constant times the relative error in the coefficients. In this sense, the Lebesgue constant can be viewed as the relative condition number of the operator mapping each coefficient vector \"u\" to the set of the values of the polynomial with coefficients \"u\" in the Lagrange form. We can actually define such an operator for each polynomial basis but its condition number is greater than the optimal Lebesgue constant for most convenient bases.\n\n"}
{"id": "7473871", "url": "https://en.wikipedia.org/wiki?curid=7473871", "title": "Levinson's inequality", "text": "Levinson's inequality\n\nIn mathematics, Levinson's inequality is the following inequality, due to Norman Levinson, involving positive numbers. Let formula_1 and let formula_2 be a given function having a third derivative on the range formula_3, and such that \n\nfor all formula_5. Suppose formula_6 and formula_7 for formula_8. Then \n\nThe Ky Fan inequality is the special case of Levinson's inequality, where \n\nand \n\n"}
{"id": "234985", "url": "https://en.wikipedia.org/wiki?curid=234985", "title": "List of integrals of logarithmic functions", "text": "List of integrals of logarithmic functions\n\nThe following is a list of integrals (antiderivative functions) of logarithmic functions. For a complete list of integral functions, see list of integrals.\n\n\"Note:\" \"x\" > 0 is assumed throughout this article, and the constant of integration is omitted for simplicity.\n\nFor formula_28 consecutive integrations, the formula \n\ngeneralizes to\n\n"}
{"id": "57766123", "url": "https://en.wikipedia.org/wiki?curid=57766123", "title": "List of irreducible Tits indices", "text": "List of irreducible Tits indices\n\nIn the mathematical theory of linear algebraic groups, a Tits index (or index) is an object used to classify semisimple algebraic groups defined over a base field \"k\", not assumed to be algebraically closed. The possible irreducible indices were classified by Jacques Tits, and this classification is reproduced below. (Because every index is a direct sum of irreducible indices, classifying \"all\" indices amounts to classifying irreducible indices.)\n\nAn index can be represented as a Dynkin diagram with certain vertices drawn close to each other (the orbit of the vertices under the *-action of the Galois group of \"k\") and with certain sets of vertices circled (the orbits of the non-distinguished vertices under the *-action). This representation captures the full information of the index except when the underlying Dynkin diagram is D, in which case one must distinguish between an action by the cyclic group \"C\" or the permutation group \"S\".\n\nAlternatively, an index can be represented using the name of the underlying Dykin diagram together with additional superscripts and subscripts, to be explained momentarily. This representation, together with the labeled Dynkin diagram described in the previous paragraph, captures the full information of the index.\n\nThe notation for an index is of the form \"X\", where\n\nImage: \n\nFull name: A\n\nConditions: \"d\" · (\"r\" + 1) = \"n\" + 1, \"d\" ≥ 1.\n\nAlgebraic group: The special linear group SL(\"D\") where \"D\" is a central division algebra over \"k\".\n\nSpecial fields: Over a finite field, \"d\" = 1; over the reals, \"d\" = 1 or 2; over a \"p\"-adic field or a number field, \"d\" is arbitrary.\n\nImage: \n\nFull name: A\n\nConditions: \"d\" | \"n\" + 1, \"d\" ≥ 1, 2\"rd\" ≤ \"n\" + 1.\n\nAlgebraic group: The special unitary group SU(\"D\",\"h\"), where \"D\" is a central division algebra of degree \"d\" over a separable quadratic extension \"k' \" of \"k\", and where \"h\" is a nondegenerate hermitian form of index \"r\" relative to the unique non-trivial \"k\"-automorphism of \"k' \".\n\nSpecial fields: Over a finite field, \"d\" = 1 and \"r\" = ⌊(\"n\"+1)/2⌋; over the reals, \"d\" = 1; over a \"p\"-adic field, \"d\" = 1 and \"n\" = 2\"r\" − 1; over a number field, \"d\" and \"r\" are arbitrary.\n\nImage: \n\nFull name: B\n\nConditions: None.\n\nAlgebraic group: The special orthogonal group SO(\"k\",\"q\"), where \"q\" is a quadratic form of index \"r\", and defect 1 if \"k\" has characteristic 2.\n\nSpecial fields: Over a finite field, \"r\" = \"n\"; over a \"p\"-adic field, \"r\" = \"n\" or \"n\" − 1; over the reals or a number field, \"r\" is arbitrary.\n\nImage: \n\nFull name: C\n\nConditions: 2 | 2\"n\", \"d\" ≥ 1; \"n\" = \"r\" if \"d\" = 1.\n\nAlgebraic group: The special unitary group SU(\"D\",\"h\"), where \"D\" is a division algebra of degree \"d\" over \"k\" and \"h\" is a nondegenerate antihermitian form relative to a \"k\"-linear involution σ of \"D\" (also called an \"involution of the first kind\") such that the fixed-point subring \"D\" has dimension 1/2 \"d\"(\"d\" + 1); or equivalently, when \"d\" > 1 and char \"k\" ≠ 2, the group SU where \"D\" and \"h\" are as above except that \"h\" is hermitian and D has dimension 1/2 \"d\"(\"d\" − 1). When \"d\" = 1, this group is the symplectic group Sp(\"k\").\n\nSpecial fields: Over a finite field, \"d\" = 1; over the reals or a number field, \"d\" = 1 (and \"r\" = \"n\") or \"d\" = 2; over a \"p\"-adic field, \"d\" = 1 (and \"r\" = \"n\") or \"d\" = 2, and \"n\" = 2\"r\" or 2\"r\" − 1.\n\nImage: \n\nFull name: D\n\nConditions: \"d\" is a power of 2, \"d\" | 2\"n\", \"d\" ≥ 1, \"rd\" ≤ \"n\", n ≠ \"rd\" + 1.\n\nAlgebraic group: If \"k\" has characteristic 2, the same as for C except that \"h\" is a hermitian form of discriminant 1 and index \"r\".\n\nSpecial fields: Over a finite field, \"d\" = 1 and \"n\" = \"r\"; over the reals, \"d\" = 1 and \"n\" − \"r\" = 2\"m\", or \"d\" = 2 and \"n\" = 2\"r\"; over a \"p\"-adic field, \"d\" = 1 and \"r\" = \"n\" or \"n\" − 2, or \"d\" = 2 and \"n\" = 2\"r\" or 2\"r\" + 3; over a number field, \"d\" = 1 and \"n\" − \"r\" = 2\"m\", or \"d\" = 2 and \"n\" − 2\"r\" = 2\"m\" or 3.\n\nFull name: D\n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nImage: \n\nAlgebraic Group: The automorphism group of an exceptional simple Jordan algebra \"J\" that does not contain nonzero nilpotent elements.\n\nImage: \n\nAlgebraic Group: The automorphism group of an exceptional simple Jordan algebra \"J\" containing nonzero nilpotent elements, no two of which are nonproportional and orthogonal.\n\nImage: \n\nAlgebraic Group: The automorphism group of an exceptional simple Jordan algebra \"J\" containing nonproportional orthogonal nilpotent elements.\n\nA group of type G is always the automorphism group of an octonion algebra.\n\nImage: \n\nAlgebraic group: the automorphism group of a division octonion algebra.\n\nSpecial fields: Exists over the reals and number fields; does not exist over finite fields or a \"p\"-adic field.\n\nImage: \n\nAlgebraic group: the automorphism group of a split octonion algebra.\n\nSpecial fields: Exists over a finite field, the reals, a \"p\"-adic field, and a number field.\n\n"}
{"id": "5971803", "url": "https://en.wikipedia.org/wiki?curid=5971803", "title": "List of mathematicians (E)", "text": "List of mathematicians (E)\n\n\n\n\n\n\n\n"}
{"id": "12149363", "url": "https://en.wikipedia.org/wiki?curid=12149363", "title": "Marcel Berger", "text": "Marcel Berger\n\nMarcel Berger (14 April 1927 – 15 October 2016) was a French mathematician, doyen of French differential geometry, and a former director of the Institut des Hautes Études Scientifiques (IHÉS), France. Formerly residing in Le Castera in Lasseube, Berger was instrumental in Mikhail Gromov's accepting positions both at the University of Paris and at the IHÉS.\n\n\n\n\n\n"}
{"id": "5954264", "url": "https://en.wikipedia.org/wiki?curid=5954264", "title": "Mock modular form", "text": "Mock modular form\n\nIn mathematics, a mock modular form is the holomorphic part of a harmonic weak Maass form, and a mock theta function is essentially a mock modular form of weight 1/2. The first examples of mock theta functions were described by Srinivasa Ramanujan in his last 1920 letter to G. H. Hardy and in his lost notebook. discovered that adding certain non-holomorphic functions to them turns them into harmonic weak Maass forms.\n\nRamanujan's 12 January 1920 letter to Hardy, reprinted in , listed 17 examples of functions that he called mock theta functions, and his lost notebook contained several more examples. (Ramanujan used the term \"theta function\" for what today would be called a modular form.) Ramanujan pointed out that they have an asymptotic expansion at the cusps, similar to that of modular forms of weight 1/2, possibly with poles at cusps, but cannot be expressed in terms of \"ordinary\" theta functions. He called functions with similar properties \"mock theta functions\". Zwegers later discovered the connection of the mock theta function with weak Maass forms.\n\nRamanujan associated an order to his mock theta functions, which was not clearly defined. Before the work of Zwegers, the orders of known mock theta functions included\n\nRamanujan's notion of order later turned out to correspond to the conductor of the Nebentypus character of the weight harmonic Maass forms which admit Ramanujan's mock theta functions as their holomorphic projections.\n\nIn the next few decades, Ramanujan's mock theta functions were studied by Watson, Andrews, Selberg, Hickerson, Choi, McIntosh, and others, who proved Ramanujan's statements about them and found several more examples and identities. (Most of the \"new\" identities and examples were already known to Ramanujan and reappeared in his lost notebook.) found that under the action of elements of the modular group, the order 3 mock theta functions almost transform like modular forms of weight 1/2 (multiplied by suitable powers of \"q\"), except that there are \"error terms\" in the functional equations, usually given as explicit integrals. However for many years there was no good definition of a mock theta function. This changed in 2001 when Zwegers discovered the relation with non-holomorphic modular forms, Lerch sums, and indefinite theta series. showed, using the previous work of Watson and Andrews, that the mock theta functions of orders 3, 5, and 7 can be written as the sum of a weak Maass form of weight and a function that is bounded along geodesics ending at cusps. The weak Maass form has eigenvalue 3/16 under the hyperbolic Laplacian (the same value as holomorphic modular forms of weight ); however, it increases exponentially fast near cusps, so it does not satisfy the usual growth condition for Maass wave forms. Zwegers proved this result in three different ways, by relating the mock theta functions to Hecke's theta functions of indefinite lattices of dimension 2, and to Appell–Lerch sums, and to meromorphic Jacobi forms.\n\nZwegers's fundamental result shows that mock theta functions are the \"holomorphic parts\" of real analytic modular forms of weight 1/2. This allows one to extend many results about modular forms to mock theta functions. In particular, like modular forms, mock theta functions all lie in certain explicit finite-dimensional spaces, which reduces the long and hard proofs of many identities between them to routine linear algebra. For the first time it became possible to produce infinite number of examples of mock theta functions; before this work there were only about 50 examples known (most of which were first found by Ramanujan). As further applications of Zwegers's ideas, Kathrin Bringmann and Ken Ono showed that certain q-series arising from the Rogers–Fine basic hypergeometric series are related to holomorphic parts of weight 3/2 harmonic weak Maass forms and showed that the asymptotic series for coefficients of the order 3 mock theta function \"f\"(\"q\") studied by and converges to the coefficients . In particular Mock theta functions have asymptotic expansions at cusps of the modular group, acting on the upper half-plane, that resemble those of modular forms of weight 1/2 with poles at the cusps.\n\nA mock modular form will be defined as the \"holomorphic part\" of a harmonic weak Maass form.\n\nFix a weight \"k\", usually with 2\"k\" integral. \nFix a subgroup Γ of SL(\"Z\") (or of the metaplectic group if \"k\" is half-integral) and a character ρ of Γ. A modular form \"f\" for this character and this group Γ transforms under elements of Γ by\n\nA weak Maass form of weight \"k\" is a continuous function on the upper half plane that transforms like a modular form of weight 2 − \"k\" and is an eigenfunction of the weight \"k\" Laplacian operator, and is called harmonic if its eigenvalue is (1 − \"k\"/2)\"k\"/2 . This is the eigenvalue of holomorphic weight \"k\" modular forms, so these are all examples of harmonic weak Maass forms. (A Maass form is a weak Maass form that decreases rapidly at cusps.)\nSo a harmonic weak Maass form is annihilated by the differential operator\n\nIf \"F\" is any harmonic weak Maass form then the function \"g\" given by\n\nis holomorphic and transforms like a modular form of weight \"k\", though it may not be holomorphic at cusps. If we can find any other function \"g\" with the same image \"g\", then \"F\" − \"g\" will be holomorphic. Such a function is given by inverting the differential operator by integration; for example we can define\n\nwhere \n\nis essentially the incomplete gamma function.\nThe integral converges whenever \"g\" has a zero at the cusp \"i\"∞, and the incomplete gamma function can be extended by analytic continuation, so this formula can be used to define the holomorphic part \"g\" of \"F\" even in the case when \"g\" is meromorphic at \"i\"∞, though this requires some care if \"k\" is 1 or not integral or if \"n\" = 0. The inverse of the differential operator is far from unique as we can add any homomorphic function to \"g\" without affecting its image, and as a result the function \"g\" need not be invariant under the group Γ. The function \"h\" = \"F\" − \"g\" is called the holomorphic part of \"F\".\n\nA mock modular form is defined to be the holomorphic part \"h\" of some harmonic weak Maass form \"F\". So there is an isomorphism from the space of mock modular forms \"h\" to a subspace of the harmonic weak Maass forms.\n\nThe mock modular form \"h\" is holomorphic but not quite modular, while \"h\" + \"g\" is modular but not quite holomorphic. The space of mock modular forms of weight \"k\" contains the space of nearly modular forms (\"modular forms that may be meromorphic at cusps\") of weight \"k\" as a subspace. The quotient is (antilinearly) isomorphic to the space of holomorphic modular forms of weight 2 − \"k\". The weight-(2 − \"k\") modular form \"g\" corresponding to a mock modular form \"h\" is called its shadow. It is quite common for different mock theta functions to have the same shadow. For example, the 10 mock theta functions of order 5 found by Ramanujan fall into two groups of 5, where all the functions in each group have the same shadow (up to multiplication by a constant).\n\na theta series of the form\n\nfor a positive rational κ and an odd periodic function \"ε\". (Any such theta series is a modular form of weight 3/2). The rational power of \"q\" is a historical accident.\n\nMost mock modular forms and weak Maass forms have rapid growth at cusps. It is common to impose the condition that they grow at most exponentially fast at cusps (which for mock modular forms means they are \"meromorphic\" at cusps). The space of mock modular forms (of given weight and group) whose growth is bounded by some fixed exponential function at cusps is finite-dimensional.\n\nAppell–Lerch sums, a generalization of Lambert series, were first studied by and . Watson studied the order 3 mock theta functions by expressing them in terms of Appell–Lerch sums, and Zwegers used them to show that mock theta functions are essentially mock modular forms.\n\nThe Appell–Lerch series is\n\nwhere \n\nand\n\nThe modified series\n\nwhere \n\nand \"y\" = Im(τ) and\n\nsatisfies the following transformation properties\n\nIn other words the modified Appell–Lerch series transforms like a modular form with respect to τ. Since mock theta functions can be expressed in terms of Appell–Lerch series this means that mock theta functions transform like modular forms if they have a certain non-analytic series added to them.\n\n showed that several of Ramanujan’s fifth order mock theta functions are equal to quotients Θ(τ)/θ(τ) where θ(τ) is a modular form of weight 1/2 and Θ(τ) is a theta function of an indefinite binary quadratic form, and proved similar results for seventh order mock theta functions. Zwegers showed how to complete the indefinite theta functions to produce real analytic modular forms, and used this to give another proof of the relation between mock theta functions and weak Maass wave forms.\n\n observed that some of Ramanujan's fifth order mock theta functions could be expressed in terms of quotients of Jacobi's theta functions. Zwegers used this idea to express mock theta functions as Fourier coefficients of meromorphic Jacobi forms.\n\n\n\n\nMock theta functions are mock modular forms of weight 1/2 whose shadow is a unary theta function, multiplied by a rational power of \"q\" (for historical reasons). Before the work of Zwegers led to a general method for constructing them, most examples were given as basic hypergeometric functions, but this is largely a historical accident, and most mock theta functions have no known simple expression in terms of such functions.\n\nThe \"trivial\" mock theta functions are the (holomorphic) modular forms of weight 1/2, which were classified by , who showed that they could all be written in terms of theta functions of 1-dimensional lattices.\n\nThe following examples use the q-Pochhammer symbols formula_18 which are defined as:\n\nSome order 2 mock theta functions were studied by .\n\nThe function μ was found by Ramanujan in his lost notebook.\n\nThese are related to the functions listed in the section on order 8 functions by\n\nRamanujan mentioned four order-3 mock theta functions in his letter to Hardy, and listed a further three in his lost notebook, which were rediscovered by G. N. Watson. proved the relations between them stated by Ramanujan and also found their transformations under elements of the modular group by expressing them as Appell–Lerch sums. described the asymptotic expansion of their coefficients. related them to harmonic weak Maass forms. See also \n\nThe seven order-3 mock theta functions given by Ramanujan are\n\nThe first four of these form a group with the same shadow (up to a constant), and so do the last three. More precisely, the functions satisfy the following relations (found by Ramanujan and proved by Watson):\n\nRamanujan wrote down ten mock theta functions of order 5 in his 1920 letter to Hardy, and stated some relations between them that were proved by . In his lost notebook he stated some further identities relating these functions, equivalent to the mock theta conjectures , that were proved by . found representations of many of these functions as the quotient of an indefinite theta series by modular forms of weight 1/2.\n\n wrote down seven mock theta functions of order 6 in his lost notebook, and stated 11 identities between them, which were proved in . Two of Ramanujan's identities relate φ and ψ at various arguments, four of them express φ and ψ in terms of Appell–Lerch series, and the last five identities express \nthe remaining five sixth-order mock theta functions in terms of φ and ψ. discovered two more sixth order functions.\nThe order 6 mock theta functions are:\n\nRamanujan gave three mock theta functions of order 7 in his 1920 letter to Hardy. They were studied by , who found asymptotic expansion for their coefficients, and in . found representations of many of these functions as the quotients of indefinite theta series by modular forms of weight 1/2. described their modular transformation properties. \nThese three mock theta functions have different shadows, so unlike the case of Ramanujan's order 3 and order 5 functions, there are no linear relations between them and ordinary modular forms.\nThe corresponding weak Maass forms are\n\nwhere\n\nand\n\nis more or less the complementary error function.\nUnder the metaplectic group, these three functions transform according to a certain 3-dimensional representation of the metaplectic group as follows\n\nIn other words, they are the components of a level 1 vector-valued harmonic weak Maass form of weight 1/2.\n\n found eight mock theta functions of order 8. They found five linear relations involving them, and expressed four of the functions as Appell–Lerch sums, and described their transformations under the modular group. \nThe two functions \"V\" and \"U\" were found earlier by in his lost notebook.\n\n listed four order-10 mock theta functions in his lost notebook, and stated some relations between them, which were proved by .\n\n\n"}
{"id": "5347359", "url": "https://en.wikipedia.org/wiki?curid=5347359", "title": "Observability Gramian", "text": "Observability Gramian\n\nIn control theory, we may need to find out whether or not a system such as \n\nformula_1\n\nis observable or not, where formula_2, formula_3, formula_4 and formula_5 are, respectively, formula_6, formula_7,formula_8 and formula_9 matrices. \n\nOne of the many ways one can achieve such goal is by the use of the Observability Gramian.\nLinear Time Invariant (LTI) Systems are those systems in which the parameters formula_2, formula_3, formula_4 and formula_5 are invariant with respect to time.\n\nOne can determine if the LTI system is or is not observable simply by looking at the pair formula_14. Then, we can say that the following statements are equivalent:\n\n1. The pair formula_14 is observable.\n\n2. The formula_6 matrix \n\nformula_17\n\nis nonsingular for any formula_18.\n\n3. The formula_19 observability matrix\n\nformula_20\n\nhas rank n.\n\n4. The formula_21 matrix \n\nformula_22\n\nhas full column rank at every eigenvalue formula_23 of formula_2.\n\n5. If, in addition, all eigenvalues of formula_2 have negative real parts (formula_2 is stable), then the unique solution of\n\nformula_27\n\nis positive definite. The solution is called the Observability Gramian and can be expressed as\n\nformula_28\n\nIn the following section we are going to take a closer look at the Observability Gramian.\n\nThe Observability Gramian can be found as the solution of the Lyapunov equation given by\n\nformula_27\n\nIn fact, we can see that if we take\n\nformula_30\n\nas a solution, we are going to find that:\n\nformula_31\n\nWhere we used the fact that formula_32 at formula_33 for stable formula_2 (all its eigenvalues have negative real part). This shows us that formula_35 is indeed the solution for the Lyapunov equation under analysis.\n\nWe can see that formula_36 is a symmetric matrix, therefore, so is formula_35. \n\nWe can use again the fact that, if formula_2 is stable (all its eigenvalues have negative real part) to show that formula_35 is unique. In order to prove so, suppose we have two different solutions for \n\nformula_27\n\nand they are given by formula_41 and formula_42. Then we have:\n\nformula_43\n\nMultiplying by formula_44 by the left and by formula_45 by the right, would lead us to\n\nformula_46\n\nIntegrating from formula_47 to formula_48:\n\nformula_49\n\nusing the fact that formula_50 as formula_51:\n\nformula_52\n\nIn other words, formula_35 has to be unique.\n\nAlso, we can see that\n\nformula_54\n\nis positive for any t, and that makes formula_35 a positive definite matrix. \n\nMore properties of observable systems can be found in , as well as the proof for the other equivalent statements of “The pair formula_14 is observable” presented in section Observability in LTI Systems.\n\nFor discrete time systems as\n\nformula_57\n\nOne can check that there are equivalences for the statement “The pair formula_14 is observable” (the equivalences are much alike for the continuous time case). \n\nWe are interested in the equivalence that claims that, if “The pair formula_14 is observable” and all the eigenvalues of formula_2 have magnitude less than formula_61 (formula_2 is stable), then the unique solution of\n\nformula_63\n\nis positive definite and given by\n\nformula_64\n\nThat is called the discrete Observability Gramian. We can easily see the correspondence between discrete time and the continuous time case, that is, if we can check that formula_65 is positive definite, and all eigenvalues of formula_2 have magnitude less than formula_61, the system formula_68 is observable. More properties and proofs can be found in.\n\nLinear time variant (LTV) systems are those in the form:\n\nformula_69\n\nThat is, the matrices formula_2, formula_3 and formula_4 have entries that varies with time. Again, as well as in the continuous time case and in the discrete time case, one may be interested in discovering if the system given by the pair formula_73 is observable or not. This can be done in a very similar way of the preceding cases.\n\nThe system formula_73 is observable at time formula_75 if and only if there exists a finite formula_76 such that the formula_6 matrix also called the Observability Gramian is given by\n\nformula_78\n\nwhere formula_79 is the state transition matrix of formula_80 is nonsingular.\n\nAgain, we have a similar method to determine if a system is or not a observable system.\n\nWe have that the Observability Gramian formula_82 have the following property:\n\nformula_83\n\nthat can easily be seen by the definition of formula_82 and by the property of the state transition matrix that claims that:\n\nformula_85\n\nMore about the Observability Gramian can be found in. \n\n\n"}
{"id": "30666756", "url": "https://en.wikipedia.org/wiki?curid=30666756", "title": "Operations Research (journal)", "text": "Operations Research (journal)\n"}
{"id": "37786960", "url": "https://en.wikipedia.org/wiki?curid=37786960", "title": "Philip J. Hanlon", "text": "Philip J. Hanlon\n\nPhilip J. Hanlon (born April 10, 1955) is an American mathematician, computer scientist, and educator who serves as the 18th President of his \"alma mater\", Dartmouth College, his tenure beginning on June 10, 2013.\n\nHanlon was born and raised in Gouverneur, New York. He attended Dartmouth College, graduating Phi Beta Kappa with a Bachelor of Arts in 1977. While an undergraduate, he was a member of Alpha Delta, the fraternity that was a partial inspiration for the 1978 film \"Animal House\". He earned a doctorate at the California Institute of Technology in 1981.\n\nAfter completing his postdoctoral work at the Massachusetts Institute of Technology, Hanlon joined the faculty of the University of Michigan in 1986. He moved from associate professor to full professor in 1990. He was the Donald J. Lewis Professor of Mathematics. He was the associate dean for planning and finance for the University of Michigan College of Literature, Science, and the Arts from 2001 to 2004 and the vice provost from 2004 to 2010. In 2010, he was appointed as the provost of the University of Michigan. In June 2013 he became the 18th president of Dartmouth College.\n\nHe serves on the editorial boards of the \"Journal of Algebraic Combinatorics\" and the \"Electronic Journal of Combinatorics\".\n\n\nHanlon is married to Gail Gentes. They have three children.\n\n"}
{"id": "7160712", "url": "https://en.wikipedia.org/wiki?curid=7160712", "title": "Proclus of Laodicea", "text": "Proclus of Laodicea\n\nProclus () or Proculeius, son of the physician Themison. Was hierophant at Laodiceia in Syria. He wrote, according to the Suda, the following works:\n\n"}
{"id": "6703729", "url": "https://en.wikipedia.org/wiki?curid=6703729", "title": "Ramanujan's congruences", "text": "Ramanujan's congruences\n\nIn mathematics, Ramanujan's congruences are some remarkable congruences for the partition function \"p\"(\"n\"). The mathematician Srinivasa Ramanujan discovered the congruences\n\nThis means that:\n\n\n\n\nIn his 1919 paper, he gave proof for the first two congruences using the following identities (using q-Pochhammer symbol notation):\n\nthen stated that \"It appears there are no equally simple properties for any moduli involving primes other than these\".\n\nAfter Ramanujan died in 1920, G. H. Hardy extracted proofs of all three congruences from an unpublished manuscript of Ramanujan on \"p\"(\"n\") (Ramanujan, 1921). The proof in this manuscript employs Eisenstein series.\n\nIn 1944, Freeman Dyson defined the rank function and conjectured the existence of a crank function for partitions that would provide a combinatorial proof of Ramanujan's congruences modulo 11. Forty years later, George Andrews and Frank Garvan successfully found such a function, and proved the celebrated result that the crank simultaneously \"explains\" the three Ramanujan congruences modulo 5, 7 and 11.\n\nIn the 1960s, A. O. L. Atkin of the University of Illinois at Chicago discovered additional congruences for small prime moduli. For example:\n\nExtending the results of A. Atkin, Ken Ono in 2000 proved that there are such Ramanujan congruences modulo every integer coprime to 6. For example, his results give\nLater Ken Ono conjectured that the elusive crank also satisfies exactly the same types of general congruences. This was proved by his Ph.D. student Karl Mahlburg in his 2005 paper \"Partition Congruences and the Andrews–Garvan–Dyson Crank\", linked below. This paper won the first Proceedings of the National Academy of Sciences Paper of the Year prize.\n\nA conceptual explanation for Ramanujan's observation was finally discovered in January 2011 by considering the Hausdorff dimension of the following formula_5 function in the l-adic topology:\n\nIt is seen to have dimension 0 only in the cases where \"ℓ\" = 5, 7 or 11 and since the partition function can be written as a linear combination of these functions this can be considered a formalization and proof of Ramanujan's observation.\n\nIn 2001, R.L. Weaver gave an effective algorithm for finding congruences of the partition function, and tabulated 76,065 congruences. This was extended in 2012 by F. Johansson to 22,474,608,014 congruences, one large example being\n\n\n\n"}
{"id": "34781952", "url": "https://en.wikipedia.org/wiki?curid=34781952", "title": "Sarason interpolation theorem", "text": "Sarason interpolation theorem\n\nIn mathematics, the Sarason interpolation theorem, introduced by , is a generalization of the Caratheodory interpolation theorem and Nevanlinna–Pick interpolation.\n"}
{"id": "231920", "url": "https://en.wikipedia.org/wiki?curid=231920", "title": "Scheduling (computing)", "text": "Scheduling (computing)\n\nIn computing, scheduling is the method by which work specified by some means is assigned to resources that complete the work. The work may be virtual computation elements such as threads, processes or data flows, which are in turn scheduled onto hardware resources such as processors, network links or expansion cards.\n\nA scheduler is what carries out the scheduling activity. Schedulers are often implemented so they keep all computer resources busy (as in load balancing), allow multiple users to share system resources effectively, or to achieve a target quality of service. Scheduling is fundamental to computation itself, and an intrinsic part of the execution model of a computer system; the concept of scheduling makes it possible to have computer multitasking with a single central processing unit (CPU).\n\nA scheduler may aim at one or more of many goals, for example: \n\nmaximizing \"throughput\" (the total amount of work completed per time unit); \n\nminimizing \"wait time\" \n(time from work becoming enabled until the first point it begins execution on resources); \n\nminimizing \"latency\" or \"response time\" (time from work becoming enabled until it is finished in case of batch activity,\n\nor until the system responds and hands the first output to the user in case of interactive activity);\n\nor maximizing \"fairness\" (equal CPU time to each process, or more generally appropriate times according to the priority and workload of each process). In practice, these goals often conflict (e.g. throughput versus latency), thus a scheduler will implement a suitable compromise. Preference is measured by any one of the concerns mentioned above, depending upon the user's needs and objectives.\n\nIn real-time environments, such as embedded systems for automatic control in industry (for example robotics), the scheduler also must ensure that processes can meet deadlines; this is crucial for keeping the system stable. Scheduled tasks can also be distributed to remote devices across a network and managed through an administrative back end.\n\nThe scheduler is an operating system module that selects the next jobs to be admitted into the system and the next process to run. Operating systems may feature up to three distinct scheduler types: a \"long-term scheduler\" (also known as an admission scheduler or high-level scheduler), a \"mid-term or medium-term scheduler\", and a \"short-term scheduler\". The names suggest the relative frequency with which their functions are performed.\n\nThe process scheduler is a part of the operating system that decides which process runs at a certain point in time. It usually has the ability to pause a running process, move it to the back of the running queue and start a new process; such a scheduler is known as \"preemptive scheduler\", otherwise it is a \"cooperative scheduler\".\n\nThe \"long-term scheduler\", or \"admission scheduler\", decides which jobs or processes are to be admitted to the ready queue (in main memory); that is, when an attempt is made to execute a program, its admission to the set of currently executing processes is either authorized or delayed by the long-term scheduler. Thus, this scheduler dictates what processes are to run on a system, and the degree of concurrency to be supported at any one time whether many or few processes are to be executed concurrently, and how the split between I/O-intensive and CPU-intensive processes is to be handled. The long-term scheduler is responsible for controlling the degree of multiprogramming.\n\nIn general, most processes can be described as either I/O-bound or CPU-bound. An I/O-bound process is one that spends more of its time doing I/O than it spends doing computations. A CPU-bound process, in contrast, generates I/O requests infrequently, using more of its time doing computations. It is important that a long-term scheduler selects a good process mix of I/O-bound and CPU-bound processes. If all processes are I/O-bound, the ready queue will almost always be empty, and the short-term scheduler will have little to do. On the other hand, if all processes are CPU-bound, the I/O waiting queue will almost always be empty, devices will go unused, and again the system will be unbalanced. The system with the best performance will thus have a combination of CPU-bound and I/O-bound processes. In modern operating systems, this is used to make sure that real-time processes get enough CPU time to finish their tasks.\n\nLong-term scheduling is also important in large-scale systems such as batch processing systems, computer clusters, supercomputers, and render farms. For example, in concurrent systems, coscheduling of interacting processes is often required to prevent them from blocking due to waiting on each other. In these cases, special-purpose job scheduler software is typically used to assist these functions, in addition to any underlying admission scheduling support in the operating system.\n\nThe \"medium-term scheduler\" temporarily removes processes from main memory and places them in secondary memory (such as a hard disk drive) or vice versa, which is commonly referred to as \"swapping out\" or \"swapping in\" (also incorrectly as \"paging out\" or \"paging in\"). The medium-term scheduler may decide to swap out a process which has not been active for some time, or a process which has a low priority, or a process which is page faulting frequently, or a process which is taking up a large amount of memory in order to free up main memory for other processes, swapping the process back in later when more memory is available, or when the process has been unblocked and is no longer waiting for a resource. [Stallings, 396] [Stallings, 370]\n\nIn many systems today (those that support mapping virtual address space to secondary storage other than the swap file), the medium-term scheduler may actually perform the role of the long-term scheduler, by treating binaries as \"swapped out processes\" upon their execution. In this way, when a segment of the binary is required it can be swapped in on demand, or \"lazy loaded\". [Stallings, 394]\n\nThe \"short-term scheduler\" (also known as the \"CPU scheduler\") decides which of the ready, in-memory processes is to be executed (allocated a CPU) after a clock interrupt, an I/O interrupt, an operating system call or another form of signal. Thus the short-term scheduler makes scheduling decisions much more frequently than the long-term or mid-term schedulersa scheduling decision will at a minimum have to be made after every time slice, and these are very short. This scheduler can be preemptive, implying that it is capable of forcibly removing processes from a CPU when it decides to allocate that CPU to another process, or non-preemptive (also known as \"voluntary\" or \"co-operative\"), in which case the scheduler is unable to \"force\" processes off the CPU.\n\nA preemptive scheduler relies upon a programmable interval timer which invokes an interrupt handler that runs in kernel mode and implements the scheduling function.\n\nAnother component that is involved in the CPU-scheduling function is the dispatcher, which is the module that gives control of the CPU to the process selected by the short-term scheduler. It receives control in kernel mode as the result of an interrupt or system call. The functions of a dispatcher involve the following:\n\n\nThe dispatcher should be as fast as possible, since it is invoked during every process switch. During the context switches, the processor is virtually idle for a fraction of time, thus unnecessary context switches should be avoided. The time it takes for the dispatcher to stop one process and start another is known as the \"dispatch latency\".\n\nScheduling disciplines are algorithms used for distributing resources among parties which simultaneously and asynchronously request them. Scheduling disciplines are used in routers (to handle packet traffic) as well as in operating systems (to share CPU time among both threads and processes), disk drives (I/O scheduling), printers (print spooler), most embedded systems, etc.\n\nThe main purposes of scheduling algorithms are to minimize resource starvation and to ensure fairness amongst the parties utilizing the resources. Scheduling deals with the problem of deciding which of the outstanding requests is to be allocated resources. There are many different scheduling algorithms. In this section, we introduce several of them.\n\nIn packet-switched computer networks and other statistical multiplexing, the notion of a scheduling algorithm is used as an alternative to first-come first-served queuing of data packets.\n\nThe simplest best-effort scheduling algorithms are round-robin, fair queuing (a max-min fair scheduling algorithm), proportionally fair scheduling and maximum throughput. If differentiated or guaranteed quality of service is offered, as opposed to best-effort communication, weighted fair queuing may be utilized.\n\nIn advanced packet radio wireless networks such as HSDPA (High-Speed Downlink Packet Access) 3.5G cellular system, channel-dependent scheduling may be used to take advantage of channel state information. If the channel conditions are favourable, the throughput and system spectral efficiency may be increased. In even more advanced systems such as LTE, the scheduling is combined by channel-dependent packet-by-packet dynamic channel allocation, or by assigning OFDMA multi-carriers or other frequency-domain equalization components to the users that best can utilize them.\n\n\"First in, first out\" (FIFO), also known as \"first come, first served\" (FCFS), is the simplest scheduling algorithm. FIFO simply queues processes in the order that they arrive in the ready queue. This is commonly used for a , for example as illustrated in this section.\n\n\nEarliest deadline first (EDF) or \"least time to go\" is a dynamic scheduling algorithm used in real-time operating systems to place processes in a priority queue. Whenever a scheduling event occurs (a task finishes, new task is released, etc.), the queue will be searched for the process closest to its deadline, which will be the next to be scheduled for execution.\n\nSimilar to shortest job first (SJF). With this strategy the scheduler arranges processes with the least estimated processing time remaining to be next in the queue. This requires advanced knowledge or estimations about the time required for a process to complete.\n\n\nThe operating system assigns a fixed priority rank to every process, and the scheduler arranges the processes in the ready queue in order of their priority. Lower-priority processes get interrupted by incoming higher-priority processes.\n\nThe scheduler assigns a fixed time unit per process, and cycles through them. If process completes within that time-slice it gets terminated otherwise it is rescheduled after giving a chance to all other processes.\n\n\nThis is used for situations in which processes are easily divided into different groups. For example, a common division is made between foreground (interactive) processes and background (batch) processes. These two types of processes have different response-time requirements and so may have different scheduling needs. It is very useful for shared memory problems.\n\nA work-conserving scheduler is a scheduler that always tries to keep the scheduled resources busy, if there are submitted jobs ready to be scheduled. In contrast, a non-work conserving scheduler is a scheduler that, in some cases, may leave the scheduled resources idle despite the presence of jobs ready to be scheduled.\n\nThere are several scheduling problems in which the goal is to decide which job goes to which station at what time, such that the total makespan is minimized:\n\nA very common method in embedded systems is to schedule jobs manually. This can for example be done in a time-multiplexed fashion. Sometimes the kernel is divided in three or more parts: Manual scheduling, preemptive and interrupt level. Exact methods for scheduling jobs are often proprietary.\n\n\nWhen designing an operating system, a programmer must consider which scheduling algorithm will perform best for the use the system is going to see. There is no universal “best” scheduling algorithm, and many operating systems use extended or combinations of the scheduling algorithms above.\n\nFor example, Windows NT/XP/Vista uses a multilevel feedback queue, a combination of fixed-priority preemptive scheduling, round-robin, and first in, first out algorithms. In this system, threads can dynamically increase or decrease in priority depending on if it has been serviced already, or if it has been waiting extensively. Every priority level is represented by its own queue, with round-robin scheduling among the high-priority threads and FIFO among the lower-priority ones. In this sense, response time is short for most threads, and short but critical system threads get completed very quickly. Since threads can only use one time unit of the round-robin in the highest-priority queue, starvation can be a problem for longer high-priority threads.\n\nThe algorithm used may be as simple as round-robin in which each process is given equal time (for instance 1 ms, usually between 1 ms and 100 ms) in a cycling list. So, process A executes for 1 ms, then process B, then process C, then back to process A.\n\nMore advanced algorithms take into account process priority, or the importance of the process. This allows some processes to use more time than other processes. The kernel always uses whatever resources it needs to ensure proper functioning of the system, and so can be said to have infinite priority. In SMP(symmetric multiprocessing) systems, processor affinity is considered to increase overall system performance, even if it may cause a process itself to run more slowly. This generally improves performance by reducing cache thrashing.\n\nIBM OS/360 was available with three different schedulers. The differences were such that the variants were often considered three different operating systems:\n\n\nLater virtual storage versions of MVS added a \"Workload Manager\" feature to the scheduler, which schedules processor resources according to an elaborate scheme defined by the installation.\n\nVery early MS-DOS and Microsoft Windows systems were non-multitasking, and as such did not feature a scheduler. Windows 3.1x used a non-preemptive scheduler, meaning that it did not interrupt programs. It relied on the program to end or tell the OS that it didn't need the processor so that it could move on to another process. This is usually called cooperative multitasking. Windows 95 introduced a rudimentary preemptive scheduler; however, for legacy support opted to let 16 bit applications run without preemption.\n\nWindows NT-based operating systems use a multilevel feedback queue. 32 priority levels are defined, 0 through to 31, with priorities 0 through 15 being \"normal\" priorities and priorities 16 through 31 being soft real-time priorities, requiring privileges to assign. 0 is reserved for the Operating System. Users can select 5 of these priorities to assign to a running application from the Task Manager application, or through thread management APIs. The kernel may change the priority level of a thread depending on its I/O and CPU usage and whether it is interactive (i.e. accepts and responds to input from humans), raising the priority of interactive and I/O bounded processes and lowering that of CPU bound processes, to increase the responsiveness of interactive applications. The scheduler was modified in Windows Vista to use the cycle counter register of modern processors to keep track of exactly how many CPU cycles a thread has executed, rather than just using an interval-timer interrupt routine. Vista also uses a priority scheduler for the I/O queue so that disk defragmenters and other such programs do not interfere with foreground operations.\n\nMac OS 9 uses cooperative scheduling for threads, where one process controls multiple cooperative threads, and also provides preemptive scheduling for multiprocessing tasks. The kernel schedules multiprocessing tasks using a preemptive scheduling algorithm. All Process Manager processes run within a special multiprocessing task, called the \"blue task\". Those processes are scheduled cooperatively, using a round-robin scheduling algorithm; a process yields control of the processor to another process by explicitly calling a such as codice_1. Each process has its own copy of the Thread Manager that schedules that process's threads cooperatively; a thread yields control of the processor to another thread by calling codice_2 or codice_3.\n\nmacOS uses a multilevel feedback queue, with four priority bands for threadsnormal, system high priority, kernel mode only, and real-time. Threads are scheduled preemptively; macOS also supports cooperatively scheduled threads in its implementation of the Thread Manager in Carbon.\n\nIn AIX Version 4 there are three possible values for thread scheduling policy:\n\n\nThreads are primarily of interest for applications that currently consist of several asynchronous processes. These applications might impose a lighter load on the system if converted to a multithreaded structure.\n\nAIX 5 implements the following scheduling policies: FIFO, round robin, and a fair round robin. The FIFO policy has three different implementations: FIFO, FIFO2, and FIFO3. The round robin policy is named SCHED_RR in AIX, and the fair round robin is called SCHED_OTHER.\n\nIn Linux 2.4, an O(n) scheduler with a multilevel feedback queue with priority levels ranging from 0 to 140 was used; 0–99 are reserved for real-time tasks and 100–140 are considered nice task levels. For real-time tasks, the time quantum for switching processes was approximately 200 ms, and for nice tasks approximately 10 ms. The scheduler ran through the run queue of all ready processes, letting the highest priority processes go first and run through their time slices, after which they will be placed in an expired queue. When the active queue is empty the expired queue will become the active queue and vice versa.\n\nHowever, some enterprise Linux distributions such as SUSE Linux Enterprise Server replaced this scheduler with a backport of the O(1) scheduler (which was maintained by Alan Cox in his Linux 2.4-ac Kernel series) to the Linux 2.4 kernel used by the distribution.\n\nIn versions 2.6.0 to 2.6.22, the kernel used an O(1) scheduler developed by Ingo Molnar and many other kernel developers during the Linux 2.5 development. For many kernel in time frame, Con Kolivas developed patch sets which improved interactivity with this scheduler or even replaced it with his own schedulers.\n\nCon Kolivas's work, most significantly his implementation of \"fair scheduling\" named \"Rotating Staircase Deadline\", inspired Ingo Molnár to develop the Completely Fair Scheduler as a replacement for the earlier O(1) scheduler, crediting Kolivas in his announcement. CFS is the first implementation of a fair queuing process scheduler widely used in a general-purpose operating system.\n\nThe Completely Fair Scheduler (CFS) uses a well-studied, classic scheduling algorithm called fair queuing originally invented for packet networks. Fair queuing had been previously applied to CPU scheduling under the name stride scheduling. The fair queuing CFS scheduler has a scheduling complexity of O(log N), where N is the number of tasks in the runqueue. Choosing a task can be done in constant time, but reinserting a task after it has run requires O(log N) operations, because the run queue is implemented as a red-black tree.\n\nThe Brain Fuck Scheduler (BFS), also created by Con Kolivas, is an alternative to the CFS.\n\nFreeBSD uses a multilevel feedback queue with priorities ranging from 0–255. 0–63 are reserved for interrupts, 64–127 for the top half of the kernel, 128–159 for real-time user threads, 160–223 for time-shared user threads, and 224–255 for idle user threads. Also, like Linux, it uses the active queue setup, but it also has an idle queue.\n\nNetBSD uses a multilevel feedback queue with priorities ranging from 0–223. 0–63 are reserved for time-shared threads (default, SCHED_OTHER policy), 64–95 for user threads which entered kernel space, 96-128 for kernel threads, 128–191 for user real-time threads (SCHED_FIFO and SCHED_RR policies), and 192–223 for software interrupts.\n\nSolaris uses a multilevel feedback queue with priorities ranging between 0 and 169. Priorities 0–59 are reserved for time-shared threads, 60–99 for system threads, 100–159 for real-time threads, and 160–169 for low priority interrupts. Unlike Linux, when a process is done using its time quantum, it is given a new priority and put back in the queue. Solaris 9 introduced two new scheduling classes, namely fixed priority class and fair share class. The threads with fixed priority have the same priority range as that of the time-sharing class, but their priorities are not dynamically adjusted. The fair scheduling class uses CPU \"shares\" to prioritize threads for scheduling decisions. CPU shares indicate the entitlement to CPU resources. They are allocated to a set of processes, which are collectively known as a project.\n\n"}
{"id": "10002631", "url": "https://en.wikipedia.org/wiki?curid=10002631", "title": "Simplexity", "text": "Simplexity\n\nSimplexity is an emerging theory that proposes a possible complementary relationship between complexity and simplicity. The term draws from General Systems Theory, Dialectics (philosophy) and Design. Jeffrey Kluger wrote a book about this phenomenon that describes how house plants can be more complicated than industrial plants, how a truck driver's job can be as difficult as a CEO's and why 90% of the money donated to help cure diseases are given only to the research of 10% of them (and vice versa).\n\nThe term has been adopted in advertising, marketing and other industries.\n\n\nLike most terms, it has been shaped through dialogues and discussion. Several individuals have participated in the development of the meaning of this term in both formal and informal venues. Most notable is Anuraj Gambhir who is attributed with the creation and popularisation of this term through various presentations and conferences around the world.\n\nOne of the first formally published instances of the word was in the journal 'Childhood Education' (1924), in the article it appears to be used to discuss education and psychology related issues.\n\nJack Cohen and Ian Stewart authors of the book \"The Collapse of Chaos\" (1995), a non-fiction book that attempts to explain chaos theory and complex systems to the general public.\n\nThe complexity of algorithms and of mathematical problems is one of the central subjects of theoretical computer science. Simplexity was whimsically defined by computer scientists Broder and Stolfi as a concept worthy of as much attention as complexity:\n\n\"The simplexity of a problem is the maximum inefficiency among the reluctant algorithms that solve P . An algorithm is said to be pessimal for a problem P if the best-cast inefficiency of A is asymptotically equal to the simplexity of P\"\n\nThe concept of Simplexity was amplified in the mid 1970s by Bruce Schiff (self-described theorist) and restated as \"The process by which nature strives towards simple ends by complex means. The result of a simple random act can only be predicted by complicated means. The spatial placement of each atom in a transplanted plant root ball is difficult to predict, but, the plant will adapt its root ball to the hole, and continue to thrive. This conceptual expression harkens back to original statements of Newton, \"Nature is pleased with simplicity\", Dalton, \"rule of greatest simplicity,\" and Einstein, \"Nature is the realization of the simplest conceivable mathematical ideas.\"\n\nIn 1974 Rustum Roy and Olaf Müller noted simplexity in the structure of ternary compounds:\n\nIn 2003, simplexity has been defined by Philippe Compain in the context of an article on the future of synthetic chemistry as follows:\n\n\"Simplexity may be defined as the combination of simplicity and complexity within the context of a dynamic relationship between means and ends.\";\n\nThe quest for simplicity constitutes indeed the basis for all future challenges of chemical synthesis including diversity, selectivity and green chemistry. Due to the almost infinity of molecules that could in theory be synthesized, the focus of synthetic research has been shifted gradually from target-oriented synthesis to diversity-oriented synthesis (divergent synthesis). A scientific approach based on the power of the molecular construction game and guided by an ideal for simplicity, has a high potential for discovery. The creative combination of synthetic methodologies, mixing simplicity and maximization of structural complexity, is indeed expected to be a powerful tool to produce unprecedented molecular structures with beneficial properties for mankind.\n\nSimplexity became the topic of formal discussion at the Hamburg Trend Day in Germany. Among the formal contributors were Peter Wippermann, Dr. Norbert Bolz, David Bosshart, Designer Ora Ito, Markus Shepherd, Susane Tide Frater.\n\n\"Simplexity: Why Simple Things Become Complex (and How Complex Things Can Be Made Simple)\" by Jeffrey Kluger details many ways in which simplexity theory can be applied to multiple disciplines. Kluger offers a look at simplexity at work in economics, sports, linguistics, technology, medicine, and dozens of human behaviors. \"Simplexity\" also provides insight into how readers can improve their lives by understanding the interplay of complexity and simplicity.\n\nSimplexity has been used by Jens Nordvig to describe the particular aim of his analytics firm Exante Data.\"Maybe the best word for what we are trying to achieve is \"simplexity\". A research product that draws on a very complex analytical foundation, but is presented in a very simple and easy to digest manner. ... This concept of \"simplexity\" may be important in a much broader sense. The general idea of simplexity could be key to effectively service a growing community of so-called “quantmental” fund managers that use large amounts of data and modeling in their processes, while still applying a human touch in the final investment decisions. We think \"simplexity\" will be increasingly important to how we develop our overall research at Exante.\"\n\nProfessor Peter Wipperman proposed a social definition \"We long for simplicity and satisfaction. Simplexity therefore stands for a balance between the growing complexity of daily life and our own personal satisfaction. In order to attain this state, we have to stop always striving to make optimal decisions. In the future, it will be more important to make judgments that are just good enough.\"\n\n\nDan Geesin first used the term 'Simplexity' in his essay 'The melancholy of the set square', 2002, when describing how technology creates more distance through complex interfaces whilst performing a simple task. For example, getting money from a bank machine. He describes how in between the chain of interfaces there is more room for error. More interfaces, more potential problems.\n\n\n\n\n"}
{"id": "252686", "url": "https://en.wikipedia.org/wiki?curid=252686", "title": "Simplicial complex", "text": "Simplicial complex\n\nIn mathematics, a simplicial complex is a set composed of points, line segments, triangles, and their \"n\"-dimensional counterparts (see illustration). Simplicial complexes should not be confused with the more abstract notion of a simplicial set appearing in modern simplicial homotopy theory. The purely combinatorial counterpart to a simplicial complex is an abstract simplicial complex.\n\nA simplicial complex formula_1 is a set of simplices that satisfies the following conditions:\n\nNote that the empty set is a face of every simplex. See also the definition of an abstract simplicial complex, which loosely speaking is a simplicial complex without an associated geometry.\n\nA simplicial \"k\"-complex formula_1 is a simplicial complex where the largest dimension of any simplex in formula_1 equals \"k\". For instance, a simplicial 2-complex must contain at least one triangle, and must not contain any tetrahedra or higher-dimensional simplices.\n\nA pure or homogeneous simplicial \"k\"-complex formula_1 is a simplicial complex where every simplex of dimension less than \"k\" is a face of some simplex formula_10 of dimension exactly \"k\". Informally, a pure 1-complex \"looks\" like it's made of a bunch of lines, a 2-complex \"looks\" like it's made of a bunch of triangles, etc. An example of a \"non\"-homogeneous complex is a triangle with a line segment attached to one of its vertices.\n\nA facet is any simplex in a complex that is \"not\" a face of any larger simplex. (Note the difference from a \"face\" of a simplex). A pure simplicial complex can be thought of as a complex where all facets have the same dimension.\n\nSometimes the term \"face\" is used to refer to a simplex of a complex, not to be confused with a face of a simplex.\n\nFor a simplicial complex embedded in a \"k\"-dimensional space, the \"k\"-faces are sometimes referred to as its cells. The term \"cell\" is sometimes used in a broader sense to denote a set homeomorphic to a simplex, leading to the definition of cell complex.\n\nThe underlying space, sometimes called the carrier of a simplicial complex is the union of its simplices.\n\nLet \"K\" be a simplicial complex and let \"S\" be a collection of simplices in \"K\".\n\nThe closure of \"S\" (denoted Cl \"S\") is the smallest simplicial subcomplex of \"K\" that contains\neach simplex in \"S\". Cl \"S\" is obtained by repeatedly adding to \"S\" each face of every simplex in \"S\".\n\nThe star of \"S\" (denoted St \"S\") is the union of the stars of each simplex in \"S\". For a single simplex \"s\", the star of \"s\" is the set of simplices having \"s\" as a face. (Note that the star of \"S\" is generally not a simplicial complex itself).\n\nThe link of \"S\" (denoted Lk \"S\") equals Cl St \"S\" − St Cl \"S\".\nIt is the closed star of \"S\" minus the stars of all faces of \"S\".\n\nIn algebraic topology, simplicial complexes are often useful for concrete calculations. For the definition of homology groups of a simplicial complex, one can read the corresponding chain complex directly, provided that consistent orientations are made of all simplices. The requirements of homotopy theory lead to the use of more general spaces, the CW complexes. Infinite complexes are a technical tool basic in algebraic topology. See also the discussion at polytope of simplicial complexes as subspaces of Euclidean space, made up of subsets each of which is a simplex. That somewhat more concrete concept is there attributed to Alexandrov. Any finite simplicial complex in the sense talked about here can be embedded as a polytope in that sense, in some large number of dimensions. In algebraic topology a compact topological space which is homeomorphic to the geometric realization of a finite simplicial complex is usually called a polyhedron (see , , ).\n\nCombinatorialists often study the \"f\"-vector of a simplicial d-complex Δ, which is the integral sequence formula_11, where \"f\" is the number of (\"i\" − 1)-dimensional faces of Δ (by convention, \"f\" = 1 unless Δ is the empty complex). For instance, if Δ is the boundary of the octahedron, then its \"f\"-vector is (1, 6, 12, 8), and if Δ is the first simplicial complex pictured above, its \"f\"-vector is (1, 18, 23, 8, 1). A complete characterization of the possible \"f\"-vectors of simplicial complexes is given by the Kruskal–Katona theorem.\n\nBy using the \"f\"-vector of a simplicial \"d\"-complex Δ as coefficients of a polynomial (written in decreasing order of exponents), we obtain the f-polynomial of Δ. In our two examples above, the \"f\"-polynomials would be formula_12 and formula_13, respectively.\n\nCombinatorists are often quite interested in the h-vector of a simplicial complex Δ, which is the sequence of coefficients of the polynomial that results from plugging \"x\" − 1 into the \"f\"-polynomial of Δ. Formally, if we write \"F\"(\"x\") to mean the \"f\"-polynomial of Δ, then the h-polynomial of Δ is\n\nand the \"h\"-vector of Δ is\n\nWe calculate the h-vector of the octahedron boundary (our first example) as follows:\n\nSo the \"h\"-vector of the boundary of the octahedron is (1, 3, 3, 1). It is not an accident this \"h\"-vector is symmetric. In fact, this happens whenever Δ is the boundary of a simplicial polytope (these are the Dehn–Sommerville equations). In general, however, the \"h\"-vector of a simplicial complex is not even necessarily positive. For instance, if we take Δ to be the 2-complex given by two triangles intersecting only at a common vertex, the resulting \"h\"-vector is (1, 3, −2).\n\nA complete characterization of all simplicial polytope \"h\"-vectors is given by the celebrated g-theorem of Stanley, Billera, and Lee.\n\nSimplicial complexes can be seen to have the same geometric structure as the contact graph of a sphere packing (a graph where vertices are the centers of spheres and edges exist if the corresponding packing elements touch each other) and as such can be used to determine the combinatorics of sphere packings, such as the number of touching pairs (1-simplices), touching triplets (2-simplices), and touching quadruples (3-simplices) in a sphere packing.\n\n\n"}
{"id": "16801419", "url": "https://en.wikipedia.org/wiki?curid=16801419", "title": "Spheroidal wave function", "text": "Spheroidal wave function\n\nSpheroidal wave functions are solutions of the Helmholtz equation that are found by writing the equation in spheroidal coordinates and applying the technique of separation of variables, just like the use of spherical coordinates lead to spherical harmonics. They are called \"oblate spheroidal wave functions\" if oblate spheroidal coordinates are used and \"prolate spheroidal wave functions\" if prolate spheroidal coordinates are used.\nIf instead of the Helmholtz equation, the Laplace equation is solved in spheroidal coordinates using the method of separation of variables, the spheroidal wave functions reduce to the spheroidal harmonics. With oblate spheroidal coordinates, the solutions \nare called \"oblate harmonics\" and with prolate spheroidal coordinates, \"prolate harmonics\". Both type of spheroidal harmonics\nare expressible in terms of Legendre functions. \n\n\n\n"}
{"id": "35839140", "url": "https://en.wikipedia.org/wiki?curid=35839140", "title": "Statistics and Computing", "text": "Statistics and Computing\n\nStatistics and Computing is a peer-reviewed academic journal that deals with statistics and computing. It was established in 1991 and is published by Springer. \n"}
{"id": "2648614", "url": "https://en.wikipedia.org/wiki?curid=2648614", "title": "Superposition calculus", "text": "Superposition calculus\n\nThe superposition calculus is a calculus for reasoning in equational first-order logic. It was developed in the early 1990s and combines concepts from first-order resolution with ordering-based equality handling as developed in the context of (unfailing) Knuth–Bendix completion. It can be seen as a generalization of either resolution (to equational logic) or unfailing completion (to full clausal logic). As most first-order calculi, superposition tries to show the \"unsatisfiability\" of a set of first-order clauses, i.e. it performs proofs by refutation. Superposition is refutation-complete—given unlimited resources and a \"fair\" derivation strategy, from any unsatisfiable clause set a contradiction will eventually be derived.\n\nAs of 2007, most of the (state-of-the-art) theorem provers for first-order logic are based on superposition (e.g. the E equational theorem prover), although only a few implement the pure calculus.\n\n\n"}
{"id": "47937215", "url": "https://en.wikipedia.org/wiki?curid=47937215", "title": "The Master Algorithm", "text": "The Master Algorithm\n\nThe Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World is a book by Pedro Domingos released in 2015. Domingos wrote the book in order to generate interest from people outside the field.\n\nThe book outlines five tribes of machine learning: inductive reasoning, connectionism, evolutionary computation, bayes theorem and analogical modelling. The author explains these tribes to the reader by referring to more understandable processes of logic, connections made in the brain, natural selection, probability and similarity judgements. Throughout the book, it is suggested that each different tribe has the potential to contribute to a unifying \"master algorithm\".\n\nTowards the end of the book the author pictures a \"master algorithm\" in the near future, where machine learning algorithms asymptotically grow to a perfect understanding of how the world and people in it work. Although the algorithm doesn't yet exist, he briefly reviews his own invention of the Markov logic network.\n\nIn 2016 Bill Gates recommended the book, alongside Nick Bostrom's \"Superintelligence\", as one of two books everyone should read to understand AI. In 2018 the book was noted to be on Chinese President Xi Jinping's bookshelf.\n\nA computer science educator stated in \"Times Higher Education\" that the examples are clear and accessible. In contrast, \"The Economist\" agreed Domingo \"does a good job\" but complained that he \"constantly invents metaphors that grate or confuse\". \"Kirkus Reviews\" praised the book, stating \"Readers unfamiliar with logic and computer theory will have a difficult time, but those who persist will discover fascinating insights.\"\n\nA \"New Scientist\" review called it \"compelling but rather unquestioning\".\n\n"}
{"id": "1901903", "url": "https://en.wikipedia.org/wiki?curid=1901903", "title": "Trigonal bipyramidal molecular geometry", "text": "Trigonal bipyramidal molecular geometry\n\nIn chemistry a trigonal bipyramid formation is a molecular geometry with one atom at the center and 5 more atoms at the corners of a triangular bipyramid. This is one geometry for which the bond angles surrounding the central atom are not identical (see also pentagonal bipyramid), because there is no geometrical arrangement with five terminal atoms in equivalent positions. Examples of this molecular geometry are phosphorus pentafluoride (PF), and phosphorus pentachloride (PCl) in the gas phase.\n\nThe five atoms bonded to the central atom are not all equivalent, and two different types of position are defined. For phosphorus pentachloride as an example, the phosphorus atom shares a plane with three chlorine atoms at 120° angles to each other in \"equatorial\" positions, and two more chlorine atoms above and below the plane (\"axial\" or \"apical\" positions).\n\nAccording to the VSEPR theory of molecular geometry, an axial position is more crowded because an axial atom has three neighboring equatorial atoms (on the same central atom) at a 90° bond angle, whereas an equatorial atom has only two neighboring axial atoms at a 90° bond angle. For molecules with five identical ligands, the axial bond lengths tend to be longer because the ligand atom cannot approach the central atom as closely. As examples, in PF the axial P−F bond length is 158 pm and the equatorial is 152 pm, and in PCl the axial and equatorial are 214 and 202 pm respectively.\n\nIn the mixed halide PFCl the chlorines occupy two of the equatorial positions, indicating that fluorine has a greater apicophilicity or tendency to occupy an axial position. In general ligand apicophilicity increases with electronegativity and also with pi-electron withdrawing ability, as in the sequence Cl < F < CN. Both factors decrease electron density in the bonding region near the central atom so that crowding in the axial position is less important.\n\nThe VSEPR theory also predicts that substitution of a ligand at a central atom by a lone pair of valence electrons leaves the general form of the electron arrangement unchanged with the lone pair now occupying one position. For molecules with five pairs of valence electrons including both bonding pairs and lone pairs, the electron pairs are still arranged in a trigonal bipyramid but one or more equatorial positions is not attached to a ligand atom so that the molecular geometry (for the nuclei only) is different.\n\nThe seesaw molecular geometry is found in sulfur tetrafluoride (SF) with a central sulfur atom surrounded by four fluorines occupying two axial and two equatorial positions, as well as one equatorial lone pair, corresponding to an AXE molecule in the AXE notation. A T-shaped molecular geometry is found in chlorine trifluoride (ClF), an AXE molecule with fluorines in two axial and one equatorial position, as well as two equatorial lone pairs. Finally, the triiodide ion () is also based upon a trigonal bipyramid, but the actual molecular geometry is linear with terminal iodines in the two axial positions only and the three equatorial positions filled with lone pairs of electrons (AXE).\n\nIsomers with a trigonal bipyramidal geometry are able to interconvert through a process known as Berry pseudorotation. Pseudorotation is similar in concept to the movement of a conformational diastereomer, though no full revolutions are completed. In the process of pseudorotation, two equatorial ligands (both of which have a shorter bond length than the third) \"shift\" toward the molecule's axis, while the axial ligands simultaneously \"shift\" toward the equator, creating a constant cyclical movement. Pseudorotation is particularly notable in simple molecules such as phosphorus pentafluoride (PF).\n\n\n"}
{"id": "5882353", "url": "https://en.wikipedia.org/wiki?curid=5882353", "title": "Uwe Storch", "text": "Uwe Storch\n\nUwe Storch (born 12 July 1940, Leopoldshall– Lanzarote, 17 September 2017) was a German mathematician. His field of research was\ncommutative algebra and analytic and algebraic geometry, in particular derivations, divisor class group, resultants.\n\nStorch studied mathematics, physics and mathematical logic in\nMünster and in\nHeidelberg. He got his PhD 1966 under the supervision of Heinrich Behnke with a thesis on almost (or Q) factorial rings.\n1972 Habilitation in Bochum, 1974 professor in Osnabrück and since 1981 professor for algebra and geometry in Bochum. 2005 Emeritation. Uwe Storch is married and has four sons.\n\nThe Theorem of Eisenbud-Evans-Storch states that\nevery algebraic variety in n-dimensional affine space\nis given geometrically (i.e. up to radical) by n polynomials.\n\nGünther Scheja and Uwe Storch, \"Lehrbuch der Algebra\", 2 volumes, Stuttgart 1980, 1988.\n\nUwe Storch and Hartmut Wiebe, \"Lehrbuch der Mathematik\", 4 volumes.\n"}
{"id": "673457", "url": "https://en.wikipedia.org/wiki?curid=673457", "title": "Vinculum (symbol)", "text": "Vinculum (symbol)\n\nA vinculum is a horizontal line used in mathematical notation for a specific purpose. It may be placed as an overline (or underline) over (or under) a mathematical expression to indicate that the expression is to be considered grouped together. Historically, vincula were extensively used to group items together, especially in written mathematics, but in modern mathematics this function has almost entirely been replaced by the use of parentheses. Today, however, the common usage of a vinculum to indicate the repetend of a repeating decimal is a significant exception and reflects the original usage.\n\n\"Vinculum\" is Latin for \"bond\", \"fetter\", \"chain\", or \"tie\", which is suggestive of some of the uses of the symbol.\n\nA vinculum can indicate a line segment where \"A\" and \"B\" are the endpoints:\n\nA vinculum can indicate the repetend of a repeating decimal value:\n\nSimilarly, it is used to show the repeating terms in a periodic continued fraction. Quadratic irrational numbers are the only numbers that have these.\n\nIts main use was as a notation to indicate a group (a bracketing device serving the same function as parentheses):\n\nmeaning to add \"b\" and \"c\" first and then subtract the result from \"a\", which would be written more commonly today as . Parentheses, used for grouping, are only rarely found in the mathematical literature before the eighteenth century. The vinculum was used extensively, usually as an overline, but Chuquet in 1484 used the underline version.\n\nThe vinculum is used as part of the notation of a radical to indicate the radicand whose root is being indicated. In the following, the quantity formula_3 is the whole radicand, and thus has a vinculum over it:\n\nIn 1637 Descartes was the first to unite the German radical sign √ with the vinculum to create the radical symbol in common use today.\n\nThe symbol used to indicate a vinculum need not be a line segment (overline or underline); sometimes braces can be used (pointing either up or down).\n\nThere are several mathematical notations which use an overbar that can easily be mistaken for a vinculum. Among these are:\n\nIt can be used in signed-digit representation to represent negative digits, such as the following example in balanced ternary:\n\nor the \"bar\" notation in common logarithms, such as\n\nThe overbar is sometimes used in Boolean algebra, where it serves to indicate a group of expressions whose logical result is to be negated, as in:\n\nIn electronics, the overbar is used to notate complementary binary signals. For example, pronounced \"not ready\", would be the same signal as READY but with the opposite polarity. This usage is closely related to the usage in Boolean algebra.\n\nIt is also used to refer to the conjugate of a complex number:\n\nIn statistics the overbar can be used to indicate the mean of series of values.\n\nIn particle physics, the overline is used to indicate antiparticles. For example, p and are the symbols for proton and antiproton, respectively.\n\nThough similar, a horizontal fractional line is not considered a vinculum. The vinculum should also not be confused with a similar-looking vector notation, e.g. formula_9 \"vector from A to B\", or formula_10 \"vector named \"a\"\", though an overline or underline without the arrowhead is sometimes used instead (e.g., formula_11 or formula_12).\n\nIt has been stated that in Roman numeral notation, a vinculum may indicate that the numerals under the line represented a thousand times the unmodified value. Mathematical historian David Eugene Smith disputes this. The notation was certainly in use in the Middle Ages.\n\nThe vinculum can be formed in Unicode by using the combining overline (U+0305) after the character that one wishes to add it to. For example, typing \"33.333...\" with combining overlines over the final three \"3\"s produces: \"33.3̅3̅3̅...\".\n\nIn HTML code, the vinculum can be generated over any given character or run of characters by using the CSS rule codice_1. However, this does not carry over when pasting into a plain text editor, because CSS affects format, not content.\n\nWord processors frequently have an overbar option. In Microsoft Word, this can be achieved through the Equation Editor or, in the 2007 and later versions, the built-in equation editing feature. On a Mac using Pages, the Unicode Hex keyboard may have to be installed first, in System Preferences|Keyboard. Open the Word Processor and make sure the Unicode keyboard is active. Type the first number to be overbarred, then ALT 0305, then the second number, then ALT 0305, and so on. \n\nIn LaTeX, codice_2 gives formula_13.\n\n"}
