{"id": "454992", "url": "https://en.wikipedia.org/wiki?curid=454992", "title": "190 (number)", "text": "190 (number)\n\n190 (one hundred [and] ninety) is the natural number following 189 and preceding 191.\n\n\n 190 is also:\n\n\n"}
{"id": "26855522", "url": "https://en.wikipedia.org/wiki?curid=26855522", "title": "Alhazen's problem", "text": "Alhazen's problem\n\nAlhazen's problem is a problem in geometrical optics first formulated by Ptolemy in 150 AD.\nIt is named for the 11th-century Arab mathematician Alhazen (\"Ibn al-Haytham\") who presented a geometric solution in his \"Book of Optics\". The algebraic solution involves quartic equations and was found only as late as 1965, by Jack M. Elkin.\n\nThe problem comprises drawing lines from two points in a circle meeting at a third point on its circumference and making equal angles with the normal at that point. \nThus, its main application in optics is to solve the problem, \"Given a light source and a spherical mirror, find the point on the mirror where the light will be reflected to the eye of an observer.\" This leads to an equation of the fourth degree.\n\nIbn al-Haytham solved the problem using conic sections and a geometric proof.\nHe derived a formula for the sum of fourth powers, where previously only the formulas for the sums of squares and cubes had been stated.\n\nHis method can be readily generalized to find the formula for the sum of any integral powers, although he did not himself do this (perhaps because he only needed the fourth power to calculate the volume of the paraboloid he was interested in). \nHe used his result on sums of integral powers to perform what would now be called an integration, where the formulas for the sums of integral squares and fourth powers allowed him to calculate the volume of a paraboloid.\n\nLater mathematicians such as Christiaan Huygens, James Gregory, Guillaume de l'Hôpital, Isaac Barrow, and many others, attempted to find an algebraic solution to the problem, using various methods, including analytic methods of geometry and derivation by complex numbers.\n\nAn algebraic solution to the problem was finally found in 1965 by Jack M. Elkin, an actuarian. Other solutions were discovered in 1989, by Harald Riede and in 1997 by the Oxford mathematician Peter M. Neumann.\n\nRecently, Mitsubishi Electric Research Labs researchers solved the extension of Alhazen's problem to general rotationally symmetric quadric mirrors including hyperbolic, parabolic and elliptical mirrors. They showed that the mirror reflection point can be computed by solving an eighth degree equation in the most general case. If the camera (eye) is placed on the axis of the mirror, the degree of the equation reduces to six. Alhazen's problem can also be extended to multiple refractions from a spherical ball. Given a light source and a spherical ball of certain refractive index, the closest point on the spherical ball where the light is refracted to the eye of the observer can be obtained by solving a tenth degree equation.\n\n"}
{"id": "54234131", "url": "https://en.wikipedia.org/wiki?curid=54234131", "title": "Andreas Wallraff", "text": "Andreas Wallraff\n\nAndreas Wallraff is a German physicist who conducts research in quantum information processing and quantum optics. He has taught as a professor at ETH Zürich in Zürich, Switzerland since 2006. He worked as a research scientist with Robert J. Schoelkopf at Yale University from 2002 to 2005, during which time he performed experiments in which the coherent interaction of a single photon with a single quantum electronic circuit was observed for the first time. His current work at ETH Zürich focuses on hybrid quantum systems combining superconducting electronic circuits with semiconductor quantum dots and individual Rydberg atoms as well as quantum error correction with superconducting qubits. \n\nHe has contributed primarily to the field of quantum information science, particularly in superconducting quantum computing and hybrid quantum systems.\n\nAndreas Wallraff obtained his undergraduate degrees in physics from Imperial College London and RWTH Aachen University and conducted research on soliton dynamics in stacked Josephson tunnel junctions for his master's degree at the Forschungszentrum Jülich and RWTH Aachen, which he earned in 1997. During his doctoral research on soliton and vortex dynamics in superconductors at the University of Erlangen-Nuremberg, he observed for the first time the tunneling and energy level quantization of an individual quantum vortex for which he obtained a PhD degree in physics in 2000.\n\nFollowing his doctoral research, Wallraff continued to work as a research scientist and later as an assistant professor at the University of Erlangen-Nuremberg. In 2002, he left Europe to work as a postdoctoral researcher with Robert J. Schoelkopf and Michel Devoret within the Department of Applied Physics at Yale University in New Haven, Connecticut. During this time, he was an author on papers regarding the coupling of superconducting qubits via a cavity bus and the coherent interaction of a single photon to a Cooper-pair box, among others. In 2004 he was appointed as an associate research scientist in the Department of Applied Physics at Yale and in June 2005 he was elected as a tenure-track assistant professor at ETH Zürich. Following the professorship appointment, he departed from Yale and started the Quantum Device Lab at ETH Zürich in January 2006, encouraged by the freedom of research afforded to him by the university. In 2011, he was chosen from among 380 ETH Zürich professors to be awarded the Prize for his research activities. In 2012, he worked as a visiting professor at the Kastler-Brossel Laboratory within the École Normale Supérieure in Paris, France.\n\nWallraff has in recent years been studying a variety of topics related to quantum information science. Among them include interactions between distant artificial atoms, quantum many-body systems, digital quantum simulation, quantum nonlocality, the implementation of the Toffoli gate in quantum computation, deterministic quantum teleportation, and the Hong-Ou-Mandel effect. \n\nIn general, his research is primarily focused on investigating circuit QED (cQED) systems and their applications in superconducting quantum computing. These include implementing quantum gates, identifying and eliminating sources of quantum decoherence to extend qubit lifetimes, and creating solid-state architectures in which quantum error correction is possible. In addition, he conducts research into \"hybrid quantum systems\"; cQED systems interacting with Rydberg atoms and semiconductor quantum dots to combine \"the long coherence times available in microscopic quantum systems with the strong interactions and integration available in solid state systems... [to allow] for strong interactions with control fields and thus fast manipulation of the quantum state of a system.\" \n\nSince January 2006, Wallraff has held a professorship position at ETH Zürich where he is the head of the Quantum Device Lab within the Laboratory for Solid State Physics. He has been awarded several grants, including two from the European Research Council (one in 2009 for hybrid cavity quantum electrodynamics and one in 2013 for superconducting quantum networks) and holds positions as the President of the Strategy Commission for the Department of Physics and Deputy Head of the Laboratory for Solid State Physics at ETH Zürich. He is also a member of the Scientific Committee for the Swiss National Science Foundation National Center of Competence in Research (NCCR) in Quantum Science and Technology (QSIT), a member of the Global Future Council for the Future of Computing at the World Economic Forum, and an Associate Fellow of the Canadian Institute for Advanced Research.\n\nWallraff has been an invited speaker at several talks and conferences, including at the inaugural celebration for the Center for Quantum Coherence Science at the University of California, Berkeley, The Optical Society \"Quantum Information and Measurement - Quantum Technologies\" meeting at the Pierre and Marie Curie University, the IWQD 2017 workshop at the National Institute of Informatics, and the 635th WE Heraeus Seminar in 2017. In 2016 he spoke at the Institute of Physics Silicon Quantum Information Processing meeting at Murray Edwards College, Cambridge, the University of Science and Technology of China, and the SCALEQIT International Conference at the Delft University of Technology. \n\nIn previous years he also spoke at the University of Oxford, the Max Planck Institute for the Science of Light, the Institute for Theoretical Atomic Molecular and Optical Physics (ITAMP) 2015 workshop at Harvard University, and the Kavli Institute for Theoretical Physics at the University of California, Santa Barbara.\n\n"}
{"id": "257242", "url": "https://en.wikipedia.org/wiki?curid=257242", "title": "Apollonius of Perga", "text": "Apollonius of Perga\n\nApollonius of Perga (; ; late 3rdearly 2nd centuries BC) was a Greek geometer and astronomer known for his theories on the topic of conic sections. Beginning from the theories of Euclid and Archimedes on the topic, he brought them to the state they were in just before the invention of analytic geometry. His definitions of the terms ellipse, parabola, and hyperbola are the ones in use today.\n\nApollonius worked on many other topics, including astronomy. Most of the work has not survived except in fragmentary references in other authors. His hypothesis of eccentric orbits to explain the apparently aberrant motion of the planets, commonly believed until the Middle Ages, was superseded during the Renaissance.\n\nFor such an important contributor to the field of mathematics, scant biographical information remains. The 6th century Palestinian commentator, Eutocius of Ascalon, on Apollonius’ major work, \"Conics\", states: \n“Apollonius, the geometrician, ... came from Perga in Pamphylia in the times of Ptolemy Euergetes, so records Herakleios the biographer of Archimedes ...”\n\nPerga at the time was a Hellenized city of Pamphylia in Anatolia. The ruins of the city yet stand. It was a center of Hellenistic culture. Euergetes, “benefactor,” identifies Ptolemy III Euergetes, third Greek dynast of Egypt in the diadochi succession. Presumably, his “times” are his regnum, 246-222/221 BC. Times are always recorded by ruler or officiating magistrate, so that if Apollonius was born earlier than 246, it would have been the “times” of Euergetes’ father. The identity of Herakleios is uncertain. The approximate times of Apollonius are thus certain, but no exact dates can be given. The figure Specific birth and death years stated by the various scholars are only speculative.\n\nEutocius appears to associate Perga with the Ptolemaic dynasty of Egypt. Never under Egypt, Perga in 246 BC belonged to the Seleucid Empire, an independent diadochi state ruled by the Seleucid dynasty. During the last half of the 3rd century BC, Perga changed hands a number of times, being alternatively under the Seleucids and under the Kingdom of Pergamon to the north, ruled by the Attalid dynasty. Someone designated “of Perga” might well be expected to have lived and worked there. To the contrary, if Apollonius was later identified with Perga, it was not on the basis of his residence. The remaining autobiographical material implies that he lived, studied and wrote in Alexandria.\n\nA letter by the Greek mathematician and astronomer Hypsicles was originally part of the supplement taken from Euclid's Book XIV, part of the thirteen books of Euclid's Elements.\nApollonius lived toward the end of a historical period now termed the Hellenistic Period, characterized by the superposition of Hellenic culture over extensive non-Hellenic regions to various depths, radical in some places, hardly at all in others. The change was initiated by Philip II of Macedon and his son, Alexander the Great, who, subjecting all of Greece is a series of stunning victories, went on to conquer the Persian Empire, which ruled territories from Egypt to Pakistan. Philip was assassinated in 336 BC. Alexander went on to fulfill his plan by conquering the vast Persian empire.\n\nThe material is located in the surviving false “Prefaces” of the books of his \"Conics.\" These are letters delivered to influential friends of Apollonius asking them to review the book enclosed with the letter. The Preface to Book I, addressed to one Eudemus, reminds him that \"Conics\" was initially requested by a house guest at Alexandria, the geometer, Naucrates, otherwise unknown to history. Naucrates had the first draft of all eight books in his hands by the end of the visit. Apollonius refers to them as being “without a thorough purgation” (\"ou diakatharantes\" in Greek, \"ea non perpurgaremus\" in Latin). He intended to verify and emend the books, releasing each one as it was completed.\n\nHearing of this plan from Apollonius himself on a subsequent visit of the latter to Pergamon, Eudemus had insisted Apollonius send him each book before release. The circumstances imply that at this stage Apollonius was a young geometer seeking the company and advice of established professionals. Pappus states that he was with the students of Euclid at Alexandria. Euclid was long gone. This stay had been, perhaps, the final stage of Apollonius’ education. Eudemus was perhaps a senior figure in his earlier education at Pergamon; in any case, there is reason to believe that he was or became the head of the Library and Research Center (Museum) of Pergamon. Apollonius goes on to state that the first four books were concerned with the development of elements while the last four were concerned with special topics.\n\nThere is something of a gap between Prefaces I and II. Apollonius has sent his son, also Apollonius, to deliver II. He speaks with more confidence, suggesting that Eudemus use the book in special study groups, which implies that Eudemus was a senior figure, if not the headmaster, in the research center. Research in such institutions, which followed the model of the Lycaeum of Aristotle at Athens, due to the residency of Alexander the Great and his companions in its northern branch, was part of the educational effort, to which the library and museum were adjunct. There was only one such school in the state. Owned by the king, it was under royal patronage, which was typically jealous, enthusiastic, and participatory. The kings bought, begged, borrowed and stole the precious books whenever and wherever they could. Books were of the highest value, affordable only to wealthy patrons. Collecting them was a royal obligation. Pergamon was known for its parchment industry, whence “parchment” is derived from “Pergamon.”\n\nApollonius brings to mind Philonides of Laodicea, a geometer whom he introduced to Eudemus in Ephesus. Philonides became Eudemus' student. He lived mainly in Syria during the 1st half of the 2nd century BC. Whether the meeting indicates that Apollonius now lived in Ephesus is unresolved. The intellectual community of the Mediterranean was international in culture. Scholars were mobile in seeking employment. They all communicated via some sort of postal service, public or private. Surviving letters are abundant. They visited each other, read each other's works, made suggestions to each other, recommended students and accumulated a tradition termed by some “the golden age of mathematics.”\n\nPreface III is missing. During the interval Eudemus passed away, says Apollonius in IV, again supporting a view that Eudemus was senior over Apollonius. Prefaces IV–VII are more formal, omitting personal information and concentrating on summarizing the books. They are all addressed to a mysterious Attalus, a choice made “because”, as Apollonius writes to Attalus, “of your earnest desire to possess my works.” By that time a good many people at Pergamum had such a desire. Presumably, this Attalus was someone special, receiving copies of Apollonius’ masterpiece fresh from the author's hand. One strong theory is that Attalus is Attalus II Philadelphus, 220-138 BC, general and defender of his brother's kingdom (Eumenes II), co-regent on the latter's illness in 160 BC, and heir to his throne and his widow in 158 BC. He and his brother were great patrons of the arts, expanding the library into international magnificence. The dates are consonant with those of Philonides, while Apollonius’ motive is consonant with Attalus' book-collecting initiative.\n\nApollonius sent to Attalus Prefaces V–VII. In Preface VII he describes Book VIII as “an appendix” ... “which I will take care to send you as speedily as possible.” There is no record that it was ever sent or ever completed. It may be missing from history because it was never in history, Apollonius having died before its completion.\n\nApollonius was a prolific geometer, turning out a large number of works. Only one survives, \"Conics\". It is a dense and extensive reference work on the topic, even by today's standards, serving as a repository of now little known geometric propositions as well as a vehicle for some new ones devised by Apollonius. Its audience was not the general population, which could not read or write. It was always intended for savants of mathematics and their small number of educated readers associated with the state schools and their associated libraries. It always was, in other words, a library reference work. Its basic definitions have become an important mathematical heritage. For the most part its methods and conclusions have been superseded by Analytic Geometry.\n\nOf its eight books, only the first four have a credible claim to descent from the original texts of Apollonius. Books 5-7 have been translated from the Arabic into Latin. The original Greek has been lost. The status of Book VIII is unknown. A first draft existed. Whether the final draft was ever produced is not known. A \"reconstruction\" of it by Edmond Halley exists in Latin. There is no way to know how much of it, if any, is verisimilar to Apollonius. Halley also reconstructed \"De Rationis Sectione\" and \"De Spatii Sectione\". Beyond these works, except for a handful of fragments, documentation that might in any way be interpreted as descending from Apollonius ends.\n\nMany of the lost works are described or mentioned by commentators. In addition are ideas attributed to Apollonius by other authors without documentation. Credible or not, they are hearsay. Some authors identify Apollonius as the author of certain ideas, consequently named after him. Others attempt to express Apollonius in modern notation or phraseology with indeterminate degrees of fidelity.\n\nThe Greek text of \"Conics\" uses the Euclidean arrangement of definitions, figures and their parts; i.e., the “givens,” followed by propositions “to be proved.” Books I-VII present 387 propositions. This type of arrangement can be seen in any modern geometry textbook of the traditional subject matter. As in any course of mathematics, the material is very dense and consideration of it, necessarily slow. Apollonius had a plan for each book, which is partly described in the \"Prefaces\". The headings, or pointers to the plan, are somewhat in deficit, Apollonius having depended more on the logical flow of the topics.\n\nAn intellectual niche is thus created for the commentators of the ages. Each must present Apollonius in the most lucid and relevant way for his own times. They use a variety of methods: annotation, extensive prefatory material, different formats, additional drawings, superficial reorganization by the addition of capita, and so on. There are subtle variations in interpretation. The modern English speaker encounters a lack of material in English due to the preference for New Latin by English scholars. Such intellectual English giants as Edmund Halley and Isaac Newton, the proper descendants of the Hellenistic tradition of mathematics and astronomy, can only be read and interpreted in translation by populations of English speakers unacquainted with the classical languages; that is, most of them.\n\nPresentations written entirely in native English begin in the late 19th century. Of special note is Heath's \"Treatise on Conic Sections\". His extensive prefatory commentary includes such items as a lexicon of Apollonian geometric terms giving the Greek, the meanings, and usage. Commenting that “the apparently portentious bulk of the treatise has deterred many from attempting to make its acquaintance,” he promises to add headings, changing the organization superficially, and to clarify the text with modern notation. His work thus references two systems of organization, his own and Apollonius’, to which concordances are given in parentheses.\n\nHeath's work is indispensable. He taught throughout the early 20th century, passing away in 1940, but meanwhile another point of view was developing. St. John's College (Annapolis/Santa Fe), which had been a military school since colonial times, preceding the United States Naval Academy at Annapolis, Maryland, to which it is adjacent, in 1936 lost its accreditation and was on the brink of bankruptcy. In desperation the board summoned Stringfellow Barr and Scott Buchanan from the University of Chicago, where they had been developing a new theoretical program for instruction of the Classics. Leaping at the opportunity, in 1937 they instituted the “new program” at St. John's, later dubbed the Great Books program, a fixed curriculum that would teach the works of select key contributors to the culture of western civilization. At St. John's, Apollonius came to be taught as himself, not as some adjunct to analytic geometry.\n\nThe “tutor” of Apollonius was R. Catesby Taliaferro, a new PhD in 1937 from the University of Virginia. He tutored until 1942 and then later for one year in 1948, supplying the English translations by himself, translating Ptolemy's \"Almagest\" and Apollonius’ \"Conics\". These translations became part of the Encyclopædia Britannica's Great Books of the Western World series. Only Books I-III are included, with an appendix for special topics. Unlike Heath, Taliaferro did not attempt to reorganize Apollonius, even superficially, or to rewrite him. His translation into modern English follows the Greek fairly closely. He does use modern geometric notation to some degree.\n\nContemporaneously with Taliaferro's work, Ivor Thomas an Oxford don of the World War II era, was taking an intense interest in Greek mathematics. He planned a compendium of selections, which came to fruition during his military service as an officer in the Royal Norfolk Regiment. After the war it found a home in the Loeb Classical Library, where it occupies two volumes, all translated by Thomas, with the Greek on one side of the page and the English on the other, as is customary for the Loeb series. Thomas' work has served as a handbook for the golden age of Greek mathematics. For Apollonius he only includes mainly those portions of Book I that define the sections.\n\nHeath, Taliaferro, and Thomas satisfied the public demand for Apollonius in translation for most of the 20th century. The subject moves on. More recent translations and studies incorporate new information and points of view as well as examine the old.\n\nBook I presents 58 propositions. Its most salient content is all the basic definitions concerning cones and conic sections. These definitions are not exactly the same as the modern ones of the same words. Etymologically the modern words derive from the ancient, but the etymon often differs in meaning from its reflex.\n\nA conical surface is generated by a line segment rotated about a bisector point such that the end points trace circles, each in its own plane. A cone, one branch of the double conical surface, is the surface with the point (apex or vertex), the circle (base), and the axis, a line joining vertex and center of base.\n\nA “section” (Latin sectio, Greek tome) is an imaginary “cutting” of a cone by a plane.\n\nThe Greek geometers were interested in laying out select figures from their inventory in various applications of engineering and architecture, as the great inventors, such as Archimedes, were accustomed to doing. A demand for conic sections existed then and exists now. The development of mathematical characterization had moved geometry in the direction of Greek geometric algebra, which visually features such algebraic fundamentals as assigning values to line segments as variables. They used a coordinate system intermediate between a grid of measurements and the Cartesian coordinate system. The theories of proportion and application of areas allowed the development of visual equations. (See below under Methods of Apollonius).\nThe “application of areas” implicitly asks, given an area and a line segment, does this area apply; that is, is it equal to, the square on the segment? If yes, an applicability (parabole) has been established. Apollonius followed Euclid in asking if a rectangle on the abscissa of any point on the section applies to the square of the ordinate. If it does, his word-equation is the equivalent of\n\nwhich is one modern form of the equation for a parabola. The rectangle has sides k and x. It was he who accordingly named the figure, parabola, “application.”\n\nThe “no applicability” case is further divided into two possibilities. Given a function, f(x), such that, in the applicability case, , in the no applicability case, either or . In the former, g(x) falls short of y by a quantity termed the ellipsis, ”deficit.” In the latter, g(x) overshoots by a quantity termed the hyperbole, “surfit.”\n\nApplicability could be achieved by adding the deficit, , or subtracting the surfit, g(x) - s. The figure compensating for a deficit was named an ellipse; for a surfit, a hyperbola. The terms of the modern equation depend on the translation and rotation of the figure from the origin, but the general equation for an ellipse,\n\ncan be placed in the form\n\nformula_1\n\nwhere C/B is the d, while an equation for the hyperbola,\n\nbecomes\n\nformula_2\n\nwhere C/B is the s.\n\nBook II contains 53 propositions. Apollonius says that he intended to cover \"the properties having to do with the diameters and axes and also the asymptotes and other things ... for limits of possibility.\" His definition of \"diameter\" is different from the traditional, as he finds it necessary to refer the intended recipient of the letter to his work for a definition. The elements mentioned are those that specify the shape and generation of the figures. Tangents are covered at the end of the book.\n\nBook III contains 56 propositions. Apollonius claims original discovery for theorems \"of use for the construction of solid loci ... the three-line and four-line locus ...\" The locus of a conic section is the section. The three-line locus problem (as stated by Taliafero's appendix to Book III) finds \"the locus of points whose distances from three given fixed straight lines ... are such that the square of one of the distances is always in a constant ratio to the rectangle contained by the other two distances.\" This is the proof of the application of areas resulting in the parabola. The four-line problem results in the ellipse and hyperbola. Analytic geometry derives the same loci from simpler criteria supported by algebra, rather than geometry, for which Descartes was highly praised. He supersedes Apollonius in his methods.\n\nBook IV contains 57 propositions. The first sent to Attalus, rather than to Eudemus, it thus represents his more mature geometric thought. The topic is rather specialized: \"the greatest number of points at which sections of a cone can meet one another, or meet a circumference of a circle, ...\" Nevertheless, he speaks with enthusiasm, labeling them \"of considerable use\" in solving problems (Preface 4).\n\nBook V, known only through translation from the Arabic, contains 77 propositions, the most of any book. They cover the ellipse (50 propositions), the parabola (22), and the hyperbola (28). These are not explicitly the topic, which in Prefaces I and V Apollonius states to be maximum and minimum lines. These terms are not explained. In contrast to Book I, Book V contains no definitions and no explanation.\n\nThe ambiguity has served as a magnet to exegetes of Apollonius, who must interpret without sure knowledge of the meaning of the book's major terms. Until recently Heath's view prevailed: the lines are to be treated as normals to the sections. A normal in this case is the perpendicular to a curve at a tangent point sometimes called the foot. If a section is plotted according to Apollonius’ coordinate system (see below under Methods of Apollonius), with the diameter (translated by Heath as the axis) on the x-axis and the vertex at the origin on the left, the phraseology of the propositions indicates that the minima/maxima are to be found between the section and the axis. Heath is led into his view by consideration of a fixed point p on the section serving both as tangent point and as one end of the line. The minimum distance between p and some point g on the axis must then be the normal from p.\n\nIn modern mathematics, normals to curves are known for being the location of the center of curvature of that small part of the curve located around the foot. The distance from the foot to the center is the radius of curvature. The latter is the radius of a circle, but for other than circular curves, the small arc can be approximated by a circular arc. The curvature of non-circular curves; e.g., the conic sections, must change over the section. A map of the center of curvature; i.e., its locus, as the foot moves over the section, is termed the evolute of the section. Such a figure, the edge of the successive positions of a line, is termed an envelope today. Heath believed that in Book V we are seeing Apollonius establish the logical foundation of a theory of normals, evolutes, and envelopes.\n\nHeath's was accepted as the authoritative interpretation of Book V for the entire 20th century, but the changing of the century brought with it a change of view. In 2001, Apollonius scholars Fried & Unguru, granting all due respect to other Heath chapters, balked at the historicity of Heath's analysis of Book V, asserting that he “reworks the original to make it more congenial to a modern mathematician ... this is the kind of thing that makes Heath’s work of dubious value for the historian, revealing more of Heath’s mind than that of Apollonius.” Some of his arguments are in summary as follows. There is no mention of maxima/minima being per se normals in either the prefaces or the books proper. Out of Heath's selection of 50 propositions said to cover normals, only 7, Book V: 27-33, state or imply maximum/minimum lines being perpendicular to the tangents. These 7 Fried classifies as isolated, unrelated to the main propositions of the book. They do not in any way imply that maxima/minima in general are normals. In his extensive investigation of the other 43 propositions, Fried proves that many cannot be.\n\nFried and Unguru counter by portraying Apollonius as a continuation of the past rather than a foreshadowing of the future. First is a complete philological study of all references to minimum and maximum lines, which uncovers a standard phraseology. There are three groups of 20-25 propositions each. The first group contains the phrase “from a point on the axis to the section,” which is exactly the opposite of a hypothetical “from a point on the section to the axis.” The former does not have to be normal to anything, although it might be. Given a fixed point on the axis, of all the lines connecting it to all the points of the section, one will be longest (maximum) and one shortest (minimum). Other phrases are “in a section,” “drawn from a section,” “cut off between the section and its axis,” cut off by the axis,” all referring to the same image.\n\nIn the view of Fried and Unguru, the topic of Book V is exactly what Apollonius says it is, maximum and minimum lines. These are not code words for future concepts, but refer to ancient concepts then in use. The authors cite Euclid, Elements, Book III, which concerns itself with circles, and maximum and minimum distances from interior points to the circumference. Without admitting to any specific generality they use terms such as “like” or “the analog of.” They are known for innovating the term “neusis-like.” A neusis construction was a method of fitting a given segment between two given curves. Given a point P, and a ruler with the segment marked off on it. one rotates the ruler around P cutting the two curves until the segment is fitted between them. In Book V, P is the point on the axis. Rotating a ruler around it, one discovers the distances to the section, from which the minimum and maximum can be discerned. The technique is not applied to the situation, so it is not neusis. The authors use neusis-like, seeing an archetypal similarity to the ancient method.\n\nBook VI, known only through translation from the Arabic, contains 33 propositions, the least of any book. It also has large lacunae, or gaps in the text, due to damage or corruption in the previous texts.\n\nThe topic is relatively clear and uncontroversial. Preface 1 states that it is “equal and similar sections of cones.” Apollonius extends the concepts of congruence and similarity presented by Euclid for more elementary figures, such as triangles, quadrilaterals, to conic sections. Preface 6 mentions “sections and segments” that are “equal and unequal” as well as “similar and dissimilar,” and adds some constructional information.\n\nBook VI features a return to the basic definitions at the front of the book. “Equality” is determined by an application of areas. If one figure; that is, a section or a segment, is “applied” to another (Halley's \"si applicari possit altera super alteram\"), they are “equal” (Halley's \"aequales\") if they coincide and no line of one crosses any line of the other. This is obviously a standard of congruence following Euclid, Book I, Common Notions, 4: “and things coinciding (\"epharmazanta\") with one another are equal (\"isa\").” Coincidence and equality overlap, but they are not the same: the application of areas used to define the sections depends on quantitative equality of areas but they can belong to different figures.\n\nBetween instances that are the same (homos), being equal to each other, and those that are different, or unequal, are figures that are “same-ish” (hom-oios), or similar. They are neither entirely the same nor different, but share aspects that are the same and do not share aspects that are different. Intuitively the geometricians had scale in mind; e.g., a map is similar to a topographic region. Thus figures could have larger or smaller versions of themselves.\n\nThe aspects that are the same in similar figures depend on the figure. Book 6 of Euclid's Elements presents similar triangles as those that have the same corresponding angles. A triangle can thus have miniatures as small as you please, or giant versions, and still be “the same” triangle as the original.\n\nIn Apollonius' definitions at the beginning of Book VI, similar right cones have similar axial triangles. Similar sections and segments of sections are first of all in similar cones. In addition, for every abscissa of one must exist an abscissa in the other at the desired scale. Finally, abscissa and ordinate of one must be matched by coordinates of the same ratio of ordinate to abscissa as the other. The total effect is as though the section or segment were moved up and down the cone to achieve a different scale.\n\nBook VII, also a translation from the Arabic, contains 51 Propositions. These are the last that Heath considers in his 1896 edition. In Preface I, Apollonius does not mention them, implying that, at the time of the first draft, they may not have existed in sufficiently coherent form to describe. Apollonius uses obscure language, that they are “peri dioristikon theorematon”, which Halley translated as “de theorematis ad determinationem pertinentibus,” and Heath as “theorems involving determinations of limits.” This is the language of definition, but no definitions are forthcoming. Whether the reference might be to a specific kind of definition is a consideration but to date nothing credible has been proposed. The topic of Book VII, completed toward the end of Apollonius’ life and career, is stated in Preface VII to be diameters and “the figures described upon them,” which must include conjugate diameters, as he relies heavily on them. In what way the term “limits” or “determinations” might apply is not mentioned.\n\nDiameters and their conjugates are defined in Book I (Definitions 4-6). Not every diameter has a conjugate. The topography of a diameter (Greek diametros) requires a regular curved figure. Irregularly-shaped areas, addressed in modern times, are not in the ancient game plan. Apollonius has in mind, of course, the conic sections, which he describes in often convolute language: “a curve in the same plane” is a circle, ellipse or parabola, while “two curves in the same plane” is a hyperbola. A chord is a straight line whose two end points are on the figure; i.e., it cuts the figure in two places. If a grid of parallel chords is imposed on the figure, then the diameter is defined as the line bisecting all the chords, reaching the curve itself at a point called the vertex. There is no requirement for a closed figure; e.g., a parabola has a diameter.\n\nA parabola has symmetry in one dimension. If you imagine it folded on its one diameter, the two halves are congruent, or fit over each other. The same may be said of one branch of a hyperbola. Conjugate diameters (Greek suzugeis diametroi, where suzugeis is “yoked together”), however, are symmetric in two dimensions. The figures to which they apply require also an areal center (Greek kentron), today called a centroid, serving as a center of symmetry in two directions. These figures are the circle, ellipse, and two-branched hyperbola. There is only one centroid, which must not be confused with the foci. A diameter is a chord passing through the centroid, which always bisects it.\n\nFor the circle and ellipse, let a grid of parallel chords be superimposed over the figure such that the longest is a diameter and the others are successively shorter until the last is not a chord, but is a tangent point. The tangent must be parallel to the diameter. A conjugate diameter bisects the chords, being placed between the centroid and the tangent point. Moreover, both diameters are conjugate to each other, being called a conjugate pair. It is obvious that any conjugate pair of a circle are perpendicular to each other, but in an ellipse, only the major and minor axes are, the elongation destroying the perpendicularity in all other cases.\n\nConjugates are defined for the two branches of a hyperbola resulting from the cutting of a double cone by a single plane. They are called conjugate branches. They have the same diameter. Its centroid bisects the segment between vertices. There is room for one more diameter-like line: let a grid of lines parallel to the diameter cut both branches of the hyperbola. These lines are chord-like except that they do not terminate on the same continuous curve. A conjugate diameter can be drawn from the centroid to bisect the chord-like lines.\n\nThese concepts mainly from Book I get us started on the 51 propositions of Book VII defining in detail the relationships between sections, diameters, and conjugate diameters. As with some of Apollonius other specialized topics, their utility today compared to Analytic Geometry remains to be seen, although he affirms in Preface VII that they are both useful and innovative; i.e., he takes the credit for them.\n\nPappus mentions other treatises of Apollonius:\nEach of these was divided into two books, and—with the \"Data\", the \"Porisms\", and \"Surface-Loci\" of Euclid and the \"Conics\" of Apollonius—were, according to Pappus, included in the body of the ancient analysis. Descriptions follow of the six works mentioned above.\n\n\"De Rationis Sectione\" sought to resolve a simple problem: Given two straight lines and a point in each, draw through a third given point a straight line cutting the two fixed lines such that the parts intercepted between the given points in them and the points of intersection with this third line may have a given ratio.\n\n\"De Spatii Sectione\" discussed a similar problem requiring the rectangle contained by the two intercepts to be equal to a given rectangle.\n\nIn the late 17th century, Edward Bernard discovered a version of \"De Rationis Sectione\" in the Bodleian Library. Although he began a translation, it was Halley who finished it and included it in a 1706 volume with his restoration of \"De Spatii Sectione\".\n\n\"De Sectione Determinata\" deals with problems in a manner that may be called an analytic geometry of one dimension; with the question of finding points on a line that were in a ratio to the others. The specific problems are: Given two, three or four points on a straight line, find another point on it such that its distances from the given points satisfy the condition that the square on one or the rectangle contained by two has a given ratio either (1) to the square on the remaining one or the rectangle contained by the remaining two or (2) to the rectangle contained by the remaining one and another given straight line. Several have tried to restore the text to discover Apollonius's solution, among them Snellius (Willebrord Snell, Leiden, 1698); Alexander Anderson of Aberdeen, in the supplement to his \"Apollonius Redivivus\" (Paris, 1612); and Robert Simson in his \"Opera quaedam reliqua\" (Glasgow, 1776), by far the best attempt.\n\n\"De Tactionibus\" embraced the following general problem: Given three things (points, straight lines, or circles) in position, describe a circle passing through the given points and touching the given straight lines or circles. The most difficult and historically interesting case arises when the three given things are circles. In the 16th century, Vieta presented this problem (sometimes known as the Apollonian Problem) to Adrianus Romanus, who solved it with a hyperbola. Vieta thereupon proposed a simpler solution, eventually leading him to restore the whole of Apollonius's treatise in the small work \"Apollonius Gallus\" (Paris, 1600). The history of the problem is explored in fascinating detail in the preface to J. W. Camerer's brief \"Apollonii Pergaei quae supersunt, ac maxime Lemmata Pappi in hos Libras, cum Observationibus, &c\" (Gothae, 1795, 8vo).\n\nThe object of \"De Inclinationibus\" was to demonstrate how a straight line of a given length, tending towards a given point, could be inserted between two given (straight or circular) lines. Though Marin Getaldić and Hugo d'Omerique (\"Geometrical Analysis\", Cadiz, 1698) attempted restorations, the best is by Samuel Horsley (1770).\n\n\"De Locis Planis\" is a collection of propositions relating to loci that are either straight lines or circles. Since Pappus gives somewhat full particulars of its propositions, this text has also seen efforts to restore it, not only by P. Fermat (\"Oeuvres\", i., 1891, pp. 3–51) and F. Schooten (Leiden, 1656) but also, most successfully of all, by R. Simson (Glasgow, 1749).\n\nAncient writers refer to other works of Apollonius that are no longer extant:\n\nThe early printed editions began for the most part in the 16th century. At that time, scholarly books were expected to be in Latin, today's New Latin. As almost no manuscripts were in Latin, the editors of the early printed works translated from the Greek or Arabic to Latin. The Greek and Latin were typically juxtaposed, but only the Greek is original, or else was restored by the editor to what he thought was original. Critical apparatuses were in Latin. The ancient commentaries, however, were in ancient or medieval Greek. Only in the 18th and 19th centuries did modern languages begin to appear. A representative list of early printed editions is given below. The originals of these printings are rare and expensive. For modern editions in modern languages see the references.\n\n\nThe hypothesis of eccentric orbits, or equivalently, deferent and epicycles, to explain the apparent motion of the planets and the varying speed of the Moon, is also attributed to him. Ptolemy describes Apollonius' theorem in the \"Almagest\" XII.1.\n\nAccording to Heath, “The Methods of Apollonius” were not his and were not personal. Whatever influence he had on later theorists was that of geometry, not of his own innovation of technique. Heath says, \n“As a preliminary to the consideration in detail of the methods employed in the Conics, it may be stated generally that they follow steadily the accepted principles of geometrical investigation which found their definitive expression in the Elements of Euclid.”\n\nWith regard to moderns speaking of golden age geometers, the term \"method\" means specifically the visual, reconstructive way in which the geometer unknowingly produces the same result as an algebraic method used today. As a simple example, algebra finds the area of a square by squaring its side. The geometric method of accomplishing the same result is to construct a visual square. Geometric methods in the golden age could produce most of the results of elementary algebra.\n\nHeath goes on to use the term geometrical algebra for the methods of the entire golden age. The term is “not inappropriately” called that, he says. Today the term has been resurrected for use in other senses (see under geometric algebra). Heath was using it as it had been defined by Henry Burchard Fine in 1890 or before. Fine applies it to La Géométrie of René Descartes, the first full-blown work of analytic geometry. Establishing as a precondition that “two algebras are formally identical whose fundamental operations are formally the same,” Fine says that Descartes’ work “is not ... mere numerical algebra, but what may for want of a better name be called the algebra of line segments. Its symbolism is the same as that of numerical algebra; ...”\n\nFor example, in Apollonius a line segment AB (the line between Point A and Point B) is also the numerical length of the segment. It can have any length. AB therefore becomes the same as an algebraic variable, such as x (the unknown), to which any value might be assigned; e.g., x=3.\n\nVariables are defined in Apollonius by such word statements as “let AB be the distance from any point on the section to the diameter,” a practice that continues in algebra today. Every student of basic algebra must learn to convert “word problems” to algebraic variables and equations, to which the rules of algebra apply in solving for x. Apollonius had no such rules. His solutions are geometric.\n\nRelationships not readily amenable to pictorial solutions were beyond his grasp; however, his repertory of pictorial solutions came from a pool of complex geometric solutions generally not known (or required) today. One well-known exception is the indispensable Pythagorean Theorem, even now represented by a right triangle with squares on its sides illustrating an expression such as a + b = c. The Greek geometers called those terms “the square on AB,” etc. Similarly, the area of a rectangle formed by AB and CD was \"the rectangle on AB and CD.\"\n\nThese concepts gave the Greek geometers algebraic access to linear functions and quadratic functions, which latter the conic sections are. They contain powers of 1 or 2 respectively. Apollonius had not much use for cubes (featured in solid geometry), even though a cone is a solid. His interest was in conic sections, which are plane figures. Powers of 4 and up were beyond visualization, requiring a degree of abstraction not available in geometry, but ready at hand in algebra.\n\nAll ordinary measurement of length in public units, such as inches, using standard public devices, such as a ruler, implies public recognition of a Cartesian grid; that is, a surface divided into unit squares, such as one square inch, and a space divided into unit cubes, such as one cubic inch. The ancient Greek units of measurement had provided such a grid to Greek mathematicians since the Bronze Age. \nPrior to Apollonius, Menaechmus and Archimedes had already started locating their figures on an implied window of the common grid by referring to distances conceived to be measured from a left-hand vertical line marking a low measure and a bottom horizontal line marking a low measure, the directions being rectilinear, or perpendicular to one another. These edges of the window become, in the Cartesian coordinate system, the axes. One specifies the rectilinear distances of any point from the axes as the coordinates. The ancient Greeks did not have that convention. They simply referred to distances.\n\nApollonius does have a standard window in which he places his figures. Vertical measurement is from a horizontal line he calls the “diameter.” The word is the same in Greek as it is in English, but the Greek is somewhat wider in its comprehension. If the figure of the conic section is cut by a grid of parallel lines, the diameter bisects all the line segments included between the branches of the figure. It must pass through the vertex (koruphe, \"crown\"). A diameter thus comprises open figures such as a parabola as well as closed, such as a circle. There is no specification that the diameter must be perpendicular to the parallel lines, but Apollonius uses only rectilinear ones.\n\nThe rectilinear distance from a point on the section to the diameter is termed tetagmenos in Greek, etymologically simply “extended.” As it is only ever extended “down” (kata-) or “up” (ana-), the translators interpret it as ordinate. In that case the diameter becomes the x-axis and the vertex the origin. The y-axis then becomes a tangent to the curve at the vertex. The abscissa is then defined as the segment of the diameter between the ordinate and the vertex.\n\nUsing his version of a coordinate system, Apollonius manages to develop in pictorial form the geometric equivalents of the equations for the conic sections, which raises the question of whether his coordinate system can be considered Cartesian. There are some differences. The Cartesian system is to be regarded as universal, covering all figures in all space applied before any calculation is done. It has four quadrants divided by the two crossed axes. Three of the quadrants include negative coordinates meaning directions opposite the reference axes of zero.\n\nApollonius has no negative numbers, does not explicitly have a number for zero, and does not develop the coordinate system independently of the conic sections. He works essentially only in Quadrant 1, all positive coordinates. Carl Boyer, a modern historian of mathematics, therefore says:\n\n”However, Greek geometric algebra did not provide for negative magnitudes; moreover, the coordinate system was in every case superimposed \"a posteriori\" upon a given curve in order to study its properties ... Apollonius, the greatest geometer of antiquity, failed to develop analytic geometry...’’\n\nNo one denies, however, that Apollonius occupies some sort of intermediate niche between the grid system of conventional measurement and the fully developed Cartesian Coordinate System of Analytic Geometry. In reading Apollonius, one must take care not to assume modern meanings for his terms.\n\nApollonius uses the \"Theory of Proportions\" as expressed in Euclid’s \"Elements\", Books 5 and 6. Devised by Eudoxus of Cnidus, the theory is intermediate between purely graphic methods and modern number theory. A standard decimal number system is lacking, as is a standard treatment of fractions. The propositions, however, express in words rules for manipulating fractions in arithmetic. Heath proposes that they stand in place of multiplication and division.\n\nBy the term “magnitude” Eudoxus hoped to go beyond numbers to a general sense of size, a meaning it still retains. With regard to the figures of Euclid, it most often means numbers, which was the Pythagorean approach. Pythagoras believed the universe could be characterized by quantities, which belief has become the current scientific dogma. Book V of Euclid begins by insisting that a magnitude (megethos, “size”) must be divisible evenly into units (meros, “part”). A magnitude is thus a multiple of units. They do not have to be standard measurement units, such as meters or feet. One unit can be any designated line segment.\n\nThere follows perhaps the most useful fundamental definition ever devised in science: the ratio (Greek logos, meaning roughly “explanation.”) is a statement of relative magnitude. Given two magnitudes, say of segments AB and CD. the ratio of AB to CD, where CD is considered unit, is the number of CD in AB; for example, 3 parts of 4, or 60 parts per million, where ppm still uses the “parts” terminology. The ratio is the basis of the modern fraction, which also still means “part,” or “fragment”, from the same Latin root as fracture.\nThe ratio is the basis of mathematical prediction in the logical structure called a “proportion” (Greek analogos). The proportion states that if two segments, AB and CD, have the same ratio as two others, EF and GH, then AB and CD are proportional to EF and GH, or, as would be said in Euclid, AB is to CD as EF is to GH.\n\nAlgebra reduces this general concept to the expression AB/CD = EF/GH. Given any three of the terms, one can calculate the fourth as an unknown. Rearranging the above equation, one obtains AB = (CD/GH)•EF, in which, expressed as y = kx, the CD/GH is known as the “constant of proportionality.” The Greeks had little difficulty with taking multiples (Greek pollaplasiein), probably by successive addition.\n\nApollonius uses ratios almost exclusively of line segments and areas, which are designated by squares and rectangles. The translators have undertaken to use the colon notation introduced by Gottfried Wilhelm Leibniz in \"Acta Eruditorum\", 1684. Here is an example from \"Conics\", Book I, on Proposition 11:\n\nThe crater Apollonius on the Moon is named in his honor.\n\n\nMany of the popular sites in the history of mathematics linked below reference or analyze concepts attributed to Apollonius in modern notations and concepts. Since much of Apollonius is subject to interpretation, and he does not per se use modern vocabulary or concepts, the analyses below may not be optimal or accurate. They represent the historical theories of their authors.\n"}
{"id": "12428320", "url": "https://en.wikipedia.org/wiki?curid=12428320", "title": "Arthur T. Benjamin", "text": "Arthur T. Benjamin\n\nArthur T. Benjamin (born March 19, 1961) is an American mathematician who specializes in combinatorics. Since 1989 he has been a professor of mathematics at Harvey Mudd College, where he is the Smallwood Family Professor of Mathematics.\n\nHe is known for mental math capabilities and \"Mathemagics\" performances in front of live audiences. His mathematical abilities have been highlighted in newspaper and magazine articles, at TED Talks and on the Colbert Report.\n\nBenjamin earned a Bachelor of Science degree with highest honors in applied mathematics at Carnegie Mellon University in 1983. He then went on to receive a Master of Science degree in 1985 and a Doctorate of Philosophy in engineering in mathematical sciences at Johns Hopkins University in 1989. His PhD dissertation was titled \"Turnpike Structures for Optimal Maneuvers\", and was supervised by Alan J. Goldman.\n\nDuring his freshman year at CMU he wrote the lyrics and created the magic effects for the musical comedy, \"Kije!\", in collaboration with author Scott McGregor and composer Arthur Darrell Turner. This musical was the winner of an annual competition and was first performed as the CMU's Spring Musical in 1980.\n\nBenjamin held several mathematics positions while attending university, including stints with the National Bureau of Standards, the National Security Agency, and the Institute for Defense Analyses. Upon receipt of his PhD he was hired as an assistant professor of mathematics at Harvey Mudd College. He is currently a full professor at Harvey Mudd and was chair of the mathematics department from 2002 to 2004. He has published over 90 academic papers and five books. He has also filmed several sets of lectures on mathematical topics for The Great Courses series from The Teaching Company, including a course on Discrete Mathematics, Mental Math, and The Mathematics of Games and Puzzles: From Cards to Sudoku. He served as co-editor of \"Math Horizons\" magazine for five years.\n\nBenjamin has long had an interest in magic. While in college he honed his skills as a magician and attended magic conferences. At one of these conferences he met well-known magician and skeptic James Randi, who greatly influenced Benjamin's decision to perform Mathemagics shows for live audiences. Randi invited him to perform his mathematical tricks on a television program called \"Exploring Psychic Powers Live\", co-hosted by Uri Geller. Randi also encouraged Benjamin to become involved in the growing skeptical movement. He attended early meetings of the Southern California Skeptics in the 1990s, which later evolved into the Skeptics Society. It was at these meetings that he met Skeptics Society President Michael Shermer, who would later become a co-author on three of Benjamin's books.\n\nBenjamin regularly performs his Mathemagics program for live audiences at schools, colleges, conferences, and even at The Magic Castle in Hollywood, California. These shows feature Benjamin performing mathematical feats like rapidly squaring numbers with up to five digits and correctly identifying the day of the week on which audience members were born based on their birth dates.\n\nHe was also featured in \"Mathemagics\", a multimedia disc released for the 3DO Interactive Multiplayer in 1994, which consists largely of short demonstrations and lessons by Benjamin in mental math and mathemagics.\n\nBenjamin has appeared in three TED Talks. The first, in 2005, was a demonstration of his Mathemagics show. The second, in 2009, was a plea for improved math education in schools. The third, in 2013, was about the way the Fibonacci series of numbers provides an excellent example of the three most important reasons for studying mathematics: Calculation, Application, and Inspiration.\n\nHe has appeared on numerous television programs throughout the years, including a notable performance on the Colbert Report in 2010. He has been profiled in over 100 articles in periodicals such as \"The New York Times\", People Magazine, USA Today, and Scientific American.\n\n\n"}
{"id": "15565288", "url": "https://en.wikipedia.org/wiki?curid=15565288", "title": "Basic dimension", "text": "Basic dimension\n\nIn a technical drawing, a basic dimension is a theoretically exact dimension, given from a datum to a feature of interest. In Geometric dimensioning and tolerancing, basic dimensions are defined as a numerical value used to describe the theoretically exact size, profile, orientation or location of a feature or datum target. \n\nAllowable variations from the theoretically exact geometry are indicated by feature control frames, notes, and tolerances on other non-basic dimensions.\n\nBasic dimensions are currently denoted by enclosing the number of the dimension in a rectangle.\n\nIn earlier times, they were denoted by appending \"BASIC\" or \"BSC\" to the dimension\n\nWhen features are located using BASIC dimensions by chain dimensioning, there is no accumulation of tolerance between features, because the dimensions refer to the theoretically perfect position of the feature, not the actual location of the feature within the range permitted by tolerances.\n"}
{"id": "3053039", "url": "https://en.wikipedia.org/wiki?curid=3053039", "title": "Bateman Manuscript Project", "text": "Bateman Manuscript Project\n\nThe Bateman Manuscript Project was a major effort at collation and encyclopedic compilation of the mathematical theory of special functions. It resulted in the eventual publication of five important reference volumes, under the editorship of Arthur Erdélyi.\n\nThe theory of special functions was a core activity of the field of applied mathematics, from the middle of the nineteenth century to the advent of high-speed electronic computing. The intricate properties of spherical harmonics, elliptic functions and other staples of problem-solving in mathematical physics, astronomy and right across the physical sciences, are not easy to document completely, absent a theory explaining the inter-relationships. Mathematical tables to perform actual calculations needed to mesh with an adequate theory of how functions could be transformed into those already tabulated.\n\nHarry Bateman, a distinguished applied mathematician, undertook the somewhat quixotic task of trying to collate the content of the very large literature. On his death in 1946, his papers on this project were still in a uniformly rough state. The publication of the edited version provided special functions texts more up-to-date than, for example, the classic Whittaker & Watson.\n\nThe volumes were out of print for many years, and copyright in the works reverted to the California Institute of Technology, who renewed them in the early 1980s. Dover planned to reprint them for publication in 2007 but this never occurred . In 2011, the California Institute of Technology gave permission for scans of the volumes to be made publicly available (see References).\n\nOther mathematicians involved in the project include Wilhelm Magnus.\n\n\n"}
{"id": "26266763", "url": "https://en.wikipedia.org/wiki?curid=26266763", "title": "Beta skeleton", "text": "Beta skeleton\n\nIn computational geometry and geometric graph theory, a \"β\"-skeleton or beta skeleton is an undirected graph defined from a set of points in the Euclidean plane. Two points \"p\" and \"q\" are connected by an edge whenever all the angles \"prq\" are sharper than a threshold determined from the numerical parameter \"β\".\n\nLet \"β\" be a positive real number, and calculate an angle \"θ\" using the formulas\n\nFor any two points \"p\" and \"q\" in the plane, let \"R\" be the set of points for which angle \"prq\" is greater than \"θ\". Then \"R\" takes the form of a union of two open disks with diameter \"βd\"(\"p\",\"q\") for \"β\" ≥ 1 and \"θ\" ≤ π/2, and it takes the form of the intersection of two open disks with diameter \"d\"(\"p\",\"q\")/\"β\" for \"β\" ≤ 1 and \"θ\" ≥ π/2. When \"β\" = 1 the two formulas give the same value \"θ\" = π/2, and \"R\" takes the form of a single open disk with \"pq\" as its diameter.\n\nThe \"β\"-skeleton of a discrete set \"S\" of points in the plane is the undirected graph that connects two points \"p\" and \"q\" with an edge \"pq\" whenever \"R\" contains no points of \"S\". That is, the \"β\"-skeleton is the empty region graph defined by the regions \"R\". When \"S\" contains a point \"r\" for which angle \"prq\" is greater than \"θ\", then \"pq\" is not an edge of the \"β\"-skeleton; the \"β\"-skeleton consists of those pairs \"pq\" for which no such point \"r\" exists.\n\nSome authors use an alternative definition in which the empty regions \"R\" for \"β\" > 1 are not unions of two disks but rather lenses (more often called in this context \"lunes\"), intersections of two congruent disks with diameter \"βd\"(\"pq\"), such that line segment \"pq\" lies on a radius of both disks and such that the points \"p\" and \"q\" both lie on the boundary of the intersection. As with the circle-based \"β\"-skeleton, the lune-based \"β\"-skeleton has an edge \"pq\" whenever region \"R\" is empty of other input points. For this alternative definition, the relative neighborhood graph is a special case of a \"β\"-skeleton with \"β\" = 2. The two definitions coincide for \"β\" ≤ 1, and for larger values of \"β\" the circle-based skeleton is a subgraph of the lune-based skeleton.\n\nOne important difference between the circle-based and lune-based \"β\"-skeletons is that, for any point set that does not lie on a single line, there always exists a sufficiently large value of \"β\" such that the circle-based \"β\"-skeleton is the empty graph. In contrast, if a pair of points \"p\" and \"q\" has the property that, for all other points \"r\", one of the two angles \"pqr\" and \"qpr\" is obtuse, then the lune-based \"β\"-skeleton will contain edge \"pq\" no matter how large \"β\" is.\n\n\"β\"-skeletons were first defined by as a scale-invariant variation of the alpha shapes of . The name, \"\"β\"-skeleton\", reflects the fact that in some sense the \"β\"-skeleton describes the shape of a set of points in the same way that a topological skeleton describes the shape of a two-dimensional region. Several generalizations of the \"β\"-skeleton to graphs defined by other empty regions have also been considered.\n\nIf \"β\" varies continuously from 0 to ∞, the circle-based \"β\"-skeletons form a sequence of graphs extending from the complete graph to the empty graph. The special case \"β\" = 1 leads to the Gabriel graph, which is known to contain the Euclidean minimum spanning tree; therefore, the \"β\"-skeleton also contains the Gabriel graph and the minimum spanning tree whenever \"β\" ≤ 1.\n\nFor any constant \"β\", a fractal construction resembling a flattened version of the Koch snowflake can be used to define a sequence of point sets whose \"β\"-skeletons are paths of arbitrarily large length within a unit square. Therefore, unlike the closely related Delaunay triangulation, \"β\"-skeletons have unbounded stretch factor and are not geometric spanners.\n\nA naïve algorithm that tests each triple \"p\", \"q\", and \"r\" for membership of \"r\" in the region \"R\" can construct the \"β\"-skeleton of any set of \"n\" points in time O(\"n\").\n\nWhen \"β\" ≥ 1, the \"β\"-skeleton (with either definition) is a subgraph of the Gabriel graph, which is a subgraph of the Delaunay triangulation. If \"pq\" is an edge of the Delaunay triangulation that is not an edge of the \"β\"-skeleton, then a point \"r\" that forms a large angle \"prq\" can be found as one of the at most two points forming a triangle with \"p\" and \"q\" in the Delaunay triangulation. Therefore, for these values of \"β\", the circle-based \"β\"-skeleton for a set of \"n\" points can be constructed in time O(\"n\" log \"n\") by computing the Delaunay triangulation and using this test to filter its edges.\n\nFor \"β\" < 1, a different algorithm of allows the construction of the \"β\"-skeleton in time O(\"n\"). No better worst-case time bound is possible because, for any fixed value of \"β\" smaller than one, there exist point sets in general position (small perturbations of a regular polygon) for which the \"β\"-skeleton is a complete graph with a quadratic number of edges. In the same quadratic time bound, the entire \"β\"-spectrum (the sequence of circle-based \"β\"-skeletons formed by varying \"β\") may also be calculated.\n\nThe circle-based \"β\"-skeleton may be used in image analysis to reconstruct the shape of a two-dimensional object, given a set of sample points on the boundary of the object (a computational form of the connect the dots puzzle where the sequence in which the dots are to be connected must be deduced by an algorithm rather than being given as part of the puzzle). Although, in general, this requires a choice of the value of the parameter \"β\", it is possible to prove that the choice \"β\" = 1.7 will correctly reconstruct the entire boundary of any smooth surface, and not generate any edges that do not belong to the boundary, as long as the samples are generated sufficiently densely relative to the local curvature of the surface. However in experimental testing a lower value, \"β\" = 1.2, was more effective for reconstructing street maps from sets of points marking the center lines of streets in a geographic information system. For generalizations of the \"β\"-skeleton technique to reconstruction of surfaces in three dimensions, see .\n\nCircle-based \"β\"-skeletons have been used to find subgraphs of the minimum weight triangulation of a point set: for sufficiently large values of \"β\", every \"β\"-skeleton edge can be guaranteed to belong to the minimum weight triangulation. If the edges found in this way form a connected graph on all of the input points, then the remaining minimum weight triangulation edges may be found in polynomial time by dynamic programming. However, in general the minimum weight triangulation problem is NP-hard, and the subset of its edges found in this way may not be connected.\n\n\"β\"-skeletons have also been applied in machine learning to solve geometric classification problems and in wireless ad hoc networks as a mechanism for controlling communication complexity by choosing a subset of the pairs of wireless stations that can communicate with each other.\n\nParametrized proximity graphs based \"β\"-skeletons are to be applied in visual perception and pattern recognition also used to find family of proximity graphs, geometric graphs in which two vertices are connected with an edge if and only if they satisfy particular geometric requirements. For β-skeletons such edge between points v1 and v2 exists if no point from a given set lies inside a set called β-neighborhood (β-lune, respectively) of v1 and v2. \n\n"}
{"id": "48496573", "url": "https://en.wikipedia.org/wiki?curid=48496573", "title": "Bhargava factorial", "text": "Bhargava factorial\n\nIn mathematics, Bhargava's factorial function, or simply Bhargava factorial, is a certain generalization of the factorial function developed by the Fields Medal winning mathematician Manjul Bhargava as part of his thesis in Harvard University in 1996. The Bhargava factorial has the property that many number-theoretic results involving the ordinary factorials remain true even when the factorials are replaced by the Bhargava factorials. Using an arbitrary infinite subset \"S\" of the set \"Z\" of integers, Bhargava associated a positive integer with every positive integer \"k\", which he denoted by \"k\" !, with the property that if we take \"S\" = \"Z\" itself, then the integer associated with \"k\", that is \"k\" !, would turn out to be the ordinary factorial of \"k\".\nThe factorial of a non-negative integer \"n\", denoted by \"n\"!, is the product of all positive integers less than or equal to \"n\". For example, 5! = 5×4×3×2×1 = 120. By convention, the value of 0! is defined as 1. This classical factorial function appears prominently in many theorems in number theory. The following are a few of these theorems.\n\n\nBhargava posed to himself the following problem and obtained an affirmative answer: In the above theorems, can one replace the set of integers by some other set \"S\" (a subset of \"Z\", or a subset of some ring) and define a function depending on \"S\" which assigns a value to each non-negative integer \"k\", denoted by \"k\"!, such that the statements obtained from the theorems given earlier by replacing \"k\"! by \"k\"! remain true?\n\n\nLet \"S\" be the set of all prime numbers \"P\" = {2, 3, 5, 7, 11, . . . }.\n\nThe first few factorials associated with the set of prime numbers are obtained as follows .\nTable of values of \"v\"(\"P\", p) and \"k\"!\n\nLet \"S\" be the set of natural numbers \"Z\".\n\nThus the first few factorials using the natural numbers are\n\nThe following table contains the general expressions for \"k\"! for some special cases of \"S\".\n\nLet \"S\" be an infinite subset of the set \"Z\" of integers. For any integer \"k\", let \"k\"! be the Bhargava factorial of \"k\" associated with the set \"S\". Manjul Bhargava proved the following results which are generalisations of corresponding results for ordinary factorials.\n\n"}
{"id": "20696", "url": "https://en.wikipedia.org/wiki?curid=20696", "title": "Class (set theory)", "text": "Class (set theory)\n\nIn set theory and its applications throughout mathematics, a class is a collection of sets (or sometimes other mathematical objects) that can be unambiguously defined by a property that all its members share. The precise definition of \"class\" depends on foundational context. In work on Zermelo–Fraenkel set theory, the notion of class is informal, whereas other set theories, such as von Neumann–Bernays–Gödel set theory, axiomatize the notion of \"proper class\", e.g., as entities that are not members of another entity.\n\nA class that is not a set (informally in Zermelo–Fraenkel) is called a proper class, and a class that is a set is sometimes called a small class. For instance, the class of all ordinal numbers, and the class of all sets, are proper classes in many formal systems.\n\nOutside set theory, the word \"class\" is sometimes used synonymously with \"set\". This usage dates from a historical period where classes and sets were not distinguished as they are in modern set-theoretic terminology. Many discussions of \"classes\" in the 19th century and earlier are really referring to sets, or perhaps to a more ambiguous concept.\n\nThe collection of all algebraic objects of a given type will usually be a proper class. Examples include the class of all groups, the class of all vector spaces, and many others. In category theory, a category whose collection of objects forms a proper class (or whose collection of morphisms forms a proper class) is called a large category.\n\nThe surreal numbers are a proper class of objects that have the properties of a field.\n\nWithin set theory, many collections of sets turn out to be proper classes. Examples include the class of all sets, the class of all ordinal numbers, and the class of all cardinal numbers.\n\nOne way to prove that a class is proper is to place it in bijection with the class of all ordinal numbers. This method is used, for example, in the proof that there is no free complete lattice on three or more generators.\n\nThe paradoxes of naive set theory can be explained in terms of the inconsistent assumption that \"all classes are sets\". With a rigorous foundation, these paradoxes instead suggest proofs that certain classes are proper (i.e., that they are not sets). For example, Russell's paradox suggests a proof that the class of all sets which do not contain themselves is proper, and the Burali-Forti paradox suggests that the class of all ordinal numbers is proper. The paradoxes do not arise with classes because there is no notion of classes containing classes. Otherwise, one could, for example, define a class of all classes that do not contain themselves, which would lead to a Russell paradox for classes. A conglomerate, on the other hand, can have proper classes as members.\n\nZF set theory does not formalize the notion of classes, so each formula with classes must be reduced syntactically to a formula without classes. For example, one can reduce the formula formula_1 to formula_2. Semantically, in a metalanguage, the classes can be described as equivalence classes of logical formulas: If formula_3 is a structure interpreting ZF, then the object language class builder expression formula_4 is interpreted in formula_3 by the collection of all the elements from the domain of formula_3 on which formula_7 holds; thus, the class can be described as the set of all predicates equivalent to formula_8 (including formula_8 itself). In particular, one can identify the \"class of all sets\" with the set of all predicates equivalent to formula_10.\n\nBecause classes do not have any formal status in the theory of ZF, the axioms of ZF do not immediately apply to classes. However, if an inaccessible cardinal formula_11 is assumed, then the sets of smaller rank form a model of ZF (a Grothendieck universe), and its subsets can be thought of as \"classes\".\n\nIn ZF, the concept of a function can also be generalised to classes. A class function is not a function in the usual sense, since it is not a set; it is rather a formula formula_12 with the property that for any set formula_13 there is no more than one set formula_14 such that the pair formula_15 satisfies formula_16. For example, the class function mapping each set to its successor may be expressed as the formula formula_17. The fact that the ordered pair formula_15 satisfies formula_16 may be expressed with the shorthand notation formula_20.\n\nAnother approach is taken by the von Neumann–Bernays–Gödel axioms (NBG); classes are the basic objects in this theory, and a set is then defined to be a class that is an element of some other class. However, the class existence axioms of NBG are restricted so that they only quantify over sets, rather than over all classes. This causes NBG to be a conservative extension of ZF.\n\nMorse–Kelley set theory admits proper classes as basic objects, like NBG, but also allows quantification over all proper classes in its class existence axioms. This causes MK to be strictly stronger than both NBG and ZF.\n\nIn other set theories, such as New Foundations or the theory of semisets, the concept of \"proper class\" still makes sense (not all classes are sets) but the criterion of sethood is not closed under subsets. For example, any set theory with a universal set has proper classes which are subclasses of sets.\n\nEvery element of a class is a set. A conglomerate, on the other hand, can have proper classes as members.\n\n"}
{"id": "44901757", "url": "https://en.wikipedia.org/wiki?curid=44901757", "title": "Cotangent sheaf", "text": "Cotangent sheaf\n\nIn algebraic geometry, given a morphism \"f\": \"X\" → \"S\" of schemes, the cotangent sheaf on \"X\" is the sheaf of formula_1-modules that represents (or classifies) \"S\"-derivations in the sense: for any formula_1-modules \"F\", there is an isomorphism\nthat depends naturally on \"F\". In other words, the cotangent sheaf is characterized by the universal property: there is the differential formula_4 such that any \"S\"-derivation formula_5 factors as formula_6 with some formula_7.\n\nIn the case \"X\" and \"S\" are affine schemes, the above definition means that formula_8 is the module of Kähler differentials. The standard way to construct a cotangent sheaf (e.g., Hartshorne, Ch II. § 8) is through a diagonal morphism (which amounts to gluing modules of Kähler differentials on affine charts to get the globally-defined cotangent sheaf.) The dual module of the cotangent sheaf on a scheme \"X\" is called the tangent sheaf on \"X\" and is sometimes denoted by formula_9.\n\nThere are two important exact sequences:\n\nThe cotangent sheaf is closely related to smoothness of a variety or scheme. For example, an algebraic variety is smooth of dimension \"n\" if and only if Ω is a locally free sheaf of rank \"n\".\n\nLet formula_12 be a morphism of schemes as in the introduction and Δ: \"X\" → \"X\" × \"X\" the diagonal morphism. Then the image of Δ is locally closed; i.e., closed in some open subset \"W\" of \"X\" × \"X\" (the image is closed if and only if \"f\" is separated). Let \"I\" be the ideal sheaf of Δ(\"X\") in \"W\". One then puts:\nand checks this sheaf of modules satisfies the required universal property of a cotangent sheaf (Hartshorne, Ch II. Remark 8.9.2). The construction shows in particular that the cotangent sheaf is quasi-coherent. It is coherent if \"S\" is Noetherian and \"f\" is of finite type.\n\nThe above definition means that the cotangent sheaf on \"X\" is the restriction to \"X\" of the conormal sheaf to the diagonal embedding of \"X\" over \"S\".\n\nSee also: bundle of principal parts.\n\nThe cotangent sheaf on a projective space is related to the tautological line bundle \"O\"(-1) by the following exact sequence: writing formula_14 for the projective space over a ring \"R\",\n\nFor this notion, see § 1 of\nThere, the cotangent stack on an algebraic stack \"X\" is defined as the relative Spec of the symmetric algebra of the tangent sheaf on \"X\". (Note: in general, if \"E\" is a locally free sheaf of finite rank, formula_16 is the algebraic vector bundle corresponding to \"E\".)\n\nSee also: Hitchin fibration (the cotangent stack of formula_17 is the total space of the Hitchin fibration.)\n\n\n\n"}
{"id": "33447667", "url": "https://en.wikipedia.org/wiki?curid=33447667", "title": "Covariance operator", "text": "Covariance operator\n\nIn probability theory, for a probability measure P on a Hilbert space \"H\" with inner product formula_1, the covariance of P is the bilinear form Cov: \"H\" × \"H\" → R given by\n\nfor all \"x\" and \"y\" in \"H\". The covariance operator \"C\" is then defined by\n\n(from the Riesz representation theorem, such operator exists if Cov is bounded). Since Cov is symmetric in its arguments, the covariance operator is\nself-adjoint (the infinite-dimensional analogy of the transposition symmetry in the finite-dimensional case). When P is a centred Gaussian measure, \"C\" is also a nuclear operator. In particular, it is a compact operator of trace class, that is, it has finite trace.\n\nEven more generally, for a probability measure P on a Banach space \"B\", the covariance of P is the bilinear form on the algebraic dual \"B\", defined by\n\nwhere formula_5 is now the value of the linear functional \"x\" on the element \"z\".\n\nQuite similarly, the covariance function of a function-valued random element (in special cases called random process or random field) \"z\" is\n\nwhere \"z\"(\"x\") is now the value of the function \"z\" at the point \"x\", i.e., the value of the linear functional formula_7 evaluated at \"z\".\n"}
{"id": "40384846", "url": "https://en.wikipedia.org/wiki?curid=40384846", "title": "Cubic cupola", "text": "Cubic cupola\n\nIn 4-dimensional geometry, the cubic cupola is a 4-polytope bounded by a rhombicuboctahedron, a parallel cube, connected by 6 square prisms, 12 triangular prisms, 8 triangular pyramids.\n\nThe \"cubic cupola\" can be sliced off from a runcinated tesseract, on a hyperplane parallel to cubic cell. The cupola can be seen in an edge-centered (B) orthogonal projection of the runcinated tesseract:\n\n\n"}
{"id": "43734728", "url": "https://en.wikipedia.org/wiki?curid=43734728", "title": "Displaced Poisson distribution", "text": "Displaced Poisson distribution\n\nIn statistics, the displaced Poisson, also known as the hyper-Poisson distribution, is a generalization of the Poisson distribution.\nThe probability mass function is\n\nwhere formula_2 and \"r\" is a new parameter; the Poisson distribution is recovered at \"r\" = 0. Here formula_3 is the incomplete gamma function and \"s\" is the integral part of \"r\". The motivation given by Staff is that the ratio of successive probabilities in the Poisson distribution (that is formula_4) is given by formula_5 for formula_6 and the displaced Poisson generalizes this ratio to formula_7.\n"}
{"id": "399730", "url": "https://en.wikipedia.org/wiki?curid=399730", "title": "Dividing a circle into areas", "text": "Dividing a circle into areas\n\nIn geometry, the problem of dividing a circle into areas by means of an inscribed polygon with \"n\" sides, in such a way as to \"maximise\" the number of areas created by the edges and diagonals, has a solution by an inductive method.\n\nIf we already have \"n\" points on the circle and add one more point, we draw \"n\" lines from the new point to previously existing points. Two cases are possible. In the first case (a), the new line passes through a point where two or more old lines (between previously existing points) cross. In the second case (b), the new line crosses each of the old lines in a different point. It will be useful to know the following fact.\n\nLemma. We can choose the new point \"A\" so that case \"b\" occurs for each of the new lines.\n\nProof. Notice that, for the case \"a\", three points must be on one line: the new point \"A\", the old point \"O\" to which we draw the line, and the point \"I\" where two of the old lines intersect. Notice that there are \"n\" old points \"O\", and hence finitely many points \"I\" where two of the old lines intersect. For each \"O\" and \"I\", the line \"OI\" crosses the circle in one point other than \"O\". Since the circle has infinitely many points, it has a point \"A\" which will be on none of the lines \"OI\". Then, for this point \"A\" and all of the old points \"O\", case \"b\" will be true.\n\nThis lemma means that, if there are \"k\" lines crossing \"AO\", then each of them crosses \"AO\" at a different point and \"k+1\" new areas are created by the line \"AO\".\n\nThe lemma establishes an important property for solving the problem. By employing an inductive proof, one can arrive at a formula for \"f\"(\"n\") in terms of \"f\"(\"n\" − 1).\n\nIn the figure you can see the dark lines connecting\npoints 1 through 4 dividing the circle into 8 total\nregions (i.e., \"f\"(4) = 8). This figure illustrates the inductive\nstep from \"n\" = 4 to \"n\" = 5 with the dashed lines. When the fifth point is added (i.e., when\ncomputing \"f\"(5) using \"f\"(4)), this results\nin four new lines (the dashed lines in the diagram) being added, numbered 1 through 4, one for each point that they connect\nto. The number of new regions introduced by the fifth point can\ntherefore be determined by considering the number of regions added by\neach of the 4 lines. Set \"i\" to count the lines we are adding.\nEach new line can cross a number of existing lines, depending on which\npoint it is to (the value of \"i\"). The new lines will never cross\neach other, except at the new point.\n\nThe number of lines that each new line intersects can be determined by\nconsidering the number of points on the \"left\" of the line and the\nnumber of points on the \"right\" of the line. Since all existing\npoints already have lines between them, the number of points on the\nleft multiplied by the number of points on the right is the number of\nlines that will be crossing the new line. For the line to point \"i\",\nthere are\n\npoints on the left and\n\non the right, so\na total of\n\nlines must be crossed.\n\nIn this example, the lines to \"i\" = 1 and \"i\" = 4 each cross zero lines,\nwhile the lines to \"i\" = 2 and \"i\" = 3 each cross two lines (there are two\npoints on one side and one on the other).\n\nSo the recurrence can be expressed as\n\nwhich can be easily reduced to\n\nusing the sums of the first formula_3 natural numbers and the first formula_3 squares, this combines to\n\nFinally\n\nwhich yields\n\nThe lemma asserts that the number of regions is maximal if all \"inner\" intersections of chords are simple (exactly two chords pass through each point of intersection in the interior). This will be the case if the points on the circle are chosen \"in general position\". Under this assumption of \"generic intersection\", the number of regions can also be determined in a non-inductive way, using the formula for the Euler characteristic of a connected planar graph (viewed here as a graph embedded in the 2-sphere S).\n\nA planar graph determines a cell decomposition of the plane with \"F\" faces (2-dimensional cells), \"E\" edges (1-dimensional cells) and \"V\" vertices (0-dimensional cells). As the graph is connected, the Euler relation for the 2-dimensional sphere S \nholds. View the diagram (the circle together with all the chords) above as a planar graph. If the general formulas for \"V\" and \"E\" can both be found, the formula for \"F\" can also be derived, which will solve the problem.\n\nIts vertices include the \"n\" points on the circle, referred to as the exterior vertices, as well as the interior vertices, the intersections of distinct chords in the interior of the circle. The \"generic intersection\" assumption made above guarantees that each interior vertex is the intersection of no more than two chords.\n\nThus the main task in determining \"V\" is finding the number of interior vertices. As a consequence of the lemma, any two intersecting chords will uniquely determine an interior vertex. These chords are in turn uniquely determined by the four corresponding endpoints of the chords, which are all exterior vertices. Any four exterior vertices determine a cyclic quadrilateral, and all cyclic quadrilaterals are convex quadrilaterals, so each set of four exterior vertices have exactly one point of intersection formed by their diagonals(chords). Further, by definition \"all\" interior vertices are formed by intersecting chords.\n\nTherefore each interior vertex is uniquely determined by a combination of four exterior vertices, where the number of interior vertices is given by\nand so\n\nThe edges include the \"n\" circular arcs connecting pairs of adjacent exterior vertices, as well as the chordal line segments (described below) created inside the circle by the collection of chords. Since there are two groups of vertices: exterior and interior, the chordal line segments can be further categorized into three groups:\n\nTo find the number of edges in groups 2 and 3, consider each interior vertex, which is connected to exactly four edges. This yields\nedges. Since each edge is defined by two endpoint vertices, and we only enumerated the interior vertices, group 2 edges are counted twice while group 3 edges are counted once only.\n\nNotice that every chord that is cut by another (i.e., chords not in group 1) must contain two group 3 edges, its beginning and ending chordal segments. As chords are uniquely determined by two exterior vertices, there are altogether\ngroup 3 edges. This is twice the total number of chords that are not themselves members of group 1.\n\nThe sum of these results divided by two gives the combined number of edges in groups 2 and 3. Adding the \"n\" edges from group 1, and the \"n\" circular arc edges brings the total to\n\nSubstituting \"V\" and \"E\" into the Euler relation solved for \"F\", formula_15 one then obtains\n\nSince one of these faces is the exterior of the circle, the number of regions \"r\" inside the circle is \"F\" − 1, or\n\nwhich resolves to\n\nwhich yields the same quartic polynomial obtained by using the inductive method\n\n\n"}
{"id": "1821467", "url": "https://en.wikipedia.org/wiki?curid=1821467", "title": "Edge-graceful labeling", "text": "Edge-graceful labeling\n\nIn graph theory, an edge-graceful graph labeling is a type of graph labeling. This is a labeling for simple graphs in which no two distinct edges connect the same two distinct vertices, no edge connects a vertex to itself, and the graph is connected. Edge-graceful labelings were first introduced by S. Lo in his seminal paper.\n\nGiven a graph \"G\", we denote the set of edges by \"E\"(\"G\") and the vertices by \"V\"(\"G\"). Let q be the cardinality of \"E\"(\"G\") and \"p\" be that of \"V\"(\"G\"). Once a labeling of the edges is given, a vertex \"u\" of the graph is labeled by the sum of the labels of the edges incident to it, modulo \"p\". Or, in symbols, the induced labeling on the vertex \"u\" is given by\n\nwhere \"V\"(\"u\") is the label for the vertex and \"E\"(\"e\") is the assigned value of an edge incident to \"u\".\n\nThe problem is to find a labeling for the edges such that all the labels from 1 to \"q\" are used once and the induced labels on the vertices run from 0 to \"p\" − 1. In other words, the resulting set for labels of the edges should be formula_2 and formula_3 for the vertices.\n\nA graph \"G\" is said to be edge-graceful if it admits an edge-graceful labeling.\n\nConsider a path with two vertices, \"P\". Here the only possibility is to label the only edge in the graph 1. The induced labeling on the two vertices are both 1. So \"P\" is not edge-graceful.\n\nAppending an edge and a vertex to \"P\" gives \"P\", the path with three vertices. Denote the vertices by \"v\", \"v\", and \"v\". Label the two edges in the following way: the edge (\"v\", \"v\") is labeled 1 and (\"v\", \"v\") labeled 2. The induced labelings on \"v\", \"v\", and \"v\" are then 1, 0, and 2 respectively. This is an edge-graceful labeling and so \"P\" is edge-graceful.\n\nSimilarly, one can check that \"P\" is not edge-graceful.\n\nIn general, \"P\" is edge-graceful when \"m\" is odd and not edge-graceful when it is even. This follows from a necessary condition for edge-gracefulness (see below).\n\nConsider the cycle with three vertices, \"C\". This is simply a triangle. One can label the edges 1, 2, and 3, and check directly that, along with the induced labeling on the vertices, this gives an edge-graceful labeling.\n\nSimilar to paths, formula_4 is edge-graceful when \"m\" is odd and not when \"m\" is even.\n\nAn edge-graceful labeling of formula_5 is shown in the following figure:\n\nLo gave a necessary condition for a graph to be edge-graceful. It is that a graph with \"q\" edges and \"p\" vertices is edge graceful only if\n\nor, in symbols,\n\nThis is referred to as Lo's condition in the literature. This follows from the fact that the sum of the labels of the vertices is twice the sum of the edges, modulo \"p\". This is useful for disproving a graph is edge-graceful. For instance, one can apply this directly to the path and cycle examples given above.\n\n\n"}
{"id": "12979890", "url": "https://en.wikipedia.org/wiki?curid=12979890", "title": "Effective method", "text": "Effective method\n\nIn logic, mathematics and computer science, especially metalogic and computability theory, an effective method or effective procedure is a procedure for solving a problem from a specific class. An effective method is sometimes also called mechanical method or procedure.\n\nThe definition of an effective method involves more than the method itself. In order for a method to be called effective, it must be considered with respect to a class of problems. Because of this, one method may be effective with respect to one class of problems and \"not\" be effective with respect to a different class.\n\nA method is formally called effective for a class of problems when it satisfies these criteria:\n\nOptionally, it may also be required that the method never returns a result as if it were an answer when the method is applied to a problem from \"outside\" its class. Adding this requirement reduces the set of classes for which there is an effective method.\n\nAn effective method for calculating the values of a function is an algorithm. Functions for which an effective method exists are sometimes called effectively calculable.\n\nSeveral independent efforts to give a formal characterization of effective calculability led to a variety of proposed definitions (general recursion, Turing machines, λ-calculus) that later were shown to be equivalent. The notion captured by these definitions is known as recursive or effective computability.\n\nThe Church-Turing thesis states that the two notions coincide: any number-theoretic function that is effectively calculable is recursively computable. As this is not a mathematical statement, it cannot be proven by a mathematical proof.\n\n\n"}
{"id": "645208", "url": "https://en.wikipedia.org/wiki?curid=645208", "title": "Equant", "text": "Equant\n\nEquant (or punctum aequans) is a mathematical concept developed by Claudius Ptolemy in the 2nd century AD to account for the observed motion of the planets. The equant is used to explain the observed speed change in planetary orbit during different stages of the orbit. This planetary concept allowed Ptolemy to keep the theory of uniform circular motion alive by stating that the path of heavenly bodies was uniform around one point and circular around another point.\n\nThe equant point (shown in the diagram by the large • ), is placed so that it is directly opposite to Earth from the deferent's center, known as the \"eccentric\" (represented by the × ). A planet or the center of an epicycle (a smaller circle carrying the planet) was conceived to move at a constant angular speed with respect to the equant. In other words, to a hypothetical observer placed at the equant point, the epicycle's center (indicated by the small · ) would appear to move at a steady angular speed. However, the epicycle's center will not move at a constant speed along its deferent.\n\nThe reason for the implementation of the equant was to maintain a semblance of constant circular motion of celestial bodies, a long-standing article of faith originated by Aristotle for philosophical reasons, while also allowing for the best match of the computations of the observed movements of the bodies, particularly in the size of the apparent retrograde motion of all Solar System bodies except the Sun and the Moon.\n\nThe angle α whose vertex is at the center of the deferent and whose sides intersect the planet and the equant respectively is a function of time \"t\":\n\nwhere Ω is the constant angular speed seen from the equant which is situated at a distance \"E\" when the radius of the deferent is \"R\".\n\nThe equant model has a body in motion on a circular path that does not share a center with Earth. The moving object's speed will actually vary during its orbit around the outer circle (dashed line), faster in the bottom half and slower in the top half. The motion is considered uniform only because the planet sweeps around equal angles in equal times from the equant point. The speed of the object is non-uniform when viewed from any other point within the orbit.\n\nPtolemy introduced the equant in \"Almagest\". The evidence that the equant was a required adjustment to Aristotelian physics relied on observations made by himself and a certain \"Theon\" (perhaps, Theon of Smyrna).\n\nIn models of the universe that precede Ptolemy, generally attributed to Hipparchus, the eccentric and epicycles were already a feature. The Roman Pliny in the 1st century CE, who apparently had access to writings of late Greek astronomers, and not being an astronomer himself, still correctly identified the lines of apsides for the five known planets and where they pointed in the zodiac. Such data requires the concept of eccentric centers of motion. Most of what we know about Hipparchus comes to us through mentions of his works by Ptolemy in the \"Almagest\". Hipparchus' models' features explained differences in the length of the seasons on Earth (known as the \"first anomaly\"), and the appearance of retrograde motion in the planets (known as the \"second anomaly\"). But Hipparchus was unable to make the predictions about the location and duration of retrograde motions of the planets match observations; he could match location, or he could match duration, but not both simultaneously. Ptolemy's introduction of the equant resolved that contradiction: the location was determined by the deferent and epicycle, while the duration was determined by uniform motion around the equant.\n\nPtolemy's model of astronomy was used as a technical method that could answer questions regarding astrology and predicting planets positions for almost 1500 years, even though the equant and eccentric were violations of pure Aristotelian physics which required all motion to be centered on the Earth. For many centuries rectifying these violations was a preoccupation among scholars, culminating in the solutions of Ibn al-Shatir and Copernicus. Ptolemy's predictions, which required constant oversight and corrections by concerned scholars over those centuries, culminated in the observations of Tycho Brahe at Uraniborg.\n\nIt wasn't until Johannes Kepler published his \"Astronomia Nova\", based on the data he and Tycho collected at Uraniborg, that Ptolemy's model of the heavens was entirely supplanted by a new geometrical model.\n\nThe equant solved the last major problem of accounting for the anomalistic motion of the planets but was believed by some to compromise the principles of the ancient Greek philosopher/astronomers, namely uniform circular motion about the Earth. The uniformity was generally assumed to be observed from the center of the deferent, and since that happens at only one point, only non-uniform motion is observed from any other point. Ptolemy moved the observation point explicitly off the center of the deferent to the equant. This can be seen as breaking part of the uniform circular motion rules. Noted critics of the equant include the Persian astronomer Nasir al-Din Tusi who developed the Tusi-couple as an alternative explanation, and Nicolaus Copernicus, whose alternative was a new pair of epicycles for each deferent. Dislike of the equant was a major motivation for Copernicus to construct his heliocentric system. This violation of perfect circular motion around the center of the deferent bothered many thinkers, especially Copernicus who mentions the equant as a monstrous construction in \"De Revolutionibus\". Copernicus' movement of the Earth away from the center of the universe obviated the primary need for Ptolemy's epicycles by explaining retrograde movement as an optical illusion, but he re-instituted two smaller epicycles into each planet's motion in order to replace the equant.\n\n\n"}
{"id": "449736", "url": "https://en.wikipedia.org/wiki?curid=449736", "title": "Fixed-point arithmetic", "text": "Fixed-point arithmetic\n\nIn computing, a fixed-point number representation is a real data type for a number that has a fixed number of digits after (and sometimes also before) the radix point (after the decimal point '.' in English decimal notation). Fixed-point number representation can be compared to the more complicated (and more computationally demanding) floating-point number representation.\n\nFixed-point numbers are useful for representing fractional values, usually in base 2 or base 10, when the executing processor has no floating point unit (FPU) or if fixed-point provides improved performance or accuracy for the application at hand. Older or low-cost embedded microprocessors and microcontrollers do not have an FPU.\n\nA value of a fixed-point data type is essentially an integer that is scaled by an implicit specific factor determined by the type. For example, the value 1.23 can be represented as 1230 in a fixed-point data type with scaling factor of 1/1000, and the value 1,230,000 can be represented as 1230 with a scaling factor of 1000. Unlike floating-point data types, the scaling factor is the same for all values of the same type, and does not change during the entire computation.\n\nThe scaling factor is usually a power of 10 (for human convenience) or a power of 2 (for computational efficiency). However, other scaling factors may be used occasionally, e.g. a time value in hours may be represented as a fixed-point type with a scale factor of 1/3600 to obtain values with one-second accuracy.\n\nThe maximum value of a fixed-point type is simply the largest value that can be represented in the underlying integer type multiplied by the scaling factor; and similarly for the minimum value.\n\nTo convert a number from a fixed point type with scaling factor \"R\" to another type with scaling factor \"S\", the underlying integer must be multiplied by \"R\" and divided by \"S\"; that is, multiplied by the ratio \"R\"/\"S\". Thus, for example, to convert the value 1.23 = 123/100 from a type with scaling factor \"R\"=1/100 to one with scaling factor \"S\"=1/1000, the underlying integer 123 must be multiplied by (1/100)/(1/1000) = 10, yielding the representation 1230/1000. If \"S\" does not divide \"R\" (in particular, if the new scaling factor \"S\" is greater than the original \"R\"), the new integer will have to be rounded. The rounding rules and methods are usually part of the language's specification.\n\nTo add or subtract two values of the same fixed-point type, it is sufficient to add or subtract the underlying integers, and keep their common scaling factor. The result can be exactly represented in the same type, as long as no overflow occurs (i.e. provided that the sum of the two integers fits in the underlying integer type). If the numbers have different fixed-point types, with different scaling factors, then one of them must be converted to the other before the sum.\n\nTo multiply two fixed-point numbers, it suffices to multiply the two underlying integers, and assume that the scaling factor of the result is the product of their scaling factors. This operation involves no rounding. For example, multiplying the numbers 123 scaled by 1/1000 (0.123) and 25 scaled by 1/10 (2.5) yields the integer 123×25 = 3075 scaled by (1/1000)×(1/10) = 1/10000, that is 3075/10000 = 0.3075. If the two operands belong to the same fixed-point type, and the result is also to be represented in that type, then the product of the two integers must be explicitly multiplied by the common scaling factor; in this case the result may have to be rounded, and overflow may occur. For example, if the common scaling factor is 1/100, multiplying 1.23 by 0.25 entails multiplying 123 by 25 to yield 3075 with an intermediate scaling factor of 1/10000. This then must be multiplied by 1/100 to yield either 31 (0.31) or 30 (0.30), depending on the rounding method used, to result in a final scale factor of 1/100.\n\nTo divide two fixed-point numbers, one takes the integer quotient of their underlying integers, and assumes that the scaling factor is the quotient of their scaling factors. The first division involves rounding in general. For example, division of 3456 scaled by 1/100 (34.56) and 1234 scaled by 1/1000 (1.234) yields the integer 3456÷1234 = 3 (rounded) with scale factor (1/100)/(1/1000) = 10, that is, 30. One can obtain a more accurate result by first converting the dividend to a more precise type: in the same example, converting 3456 scaled by 1/100 (34.56) to 3,456,000 scaled by 1/100000, before dividing by 1234 scaled by 1/1000 (1.234), would yield 3456000÷1234 = 2801 (rounded) with scaling factor (1/100000)/(1/1000) = 1/100, that is 28.01 (instead of 30). If both operands and the desired result are represented in the same fixed-point type, then the quotient of the two integers must be explicitly divided by the common scaling factor.\n\nThe two most common classes of fixed-point types are decimal and binary. Decimal fixed-point types have a scaling factor that is a power of ten; for binary fixed-point types it is a power of two.\n\nBinary fixed-point types are most commonly used, because the rescaling operations can be implemented as fast bit shifts. Binary fixed-point numbers can represent fractional powers of two exactly, but, like binary floating-point numbers, cannot exactly represent fractional powers of ten. If exact fractional powers of ten are desired, then a decimal format should be used. For example, one-tenth (0.1) and one-hundredth (0.01) can be represented only approximately by binary fixed-point or binary floating-point representations, while they can be represented exactly in decimal fixed-point or decimal floating-point representations. These representations may be encoded in many ways, including binary-coded decimal (BCD).\n\nThere are various notations used to represent word length and radix point in a binary fixed-point number. In the following list, \"f\" represents the number of fractional bits, \"m\" the number of magnitude or integer bits, \"s\" the number of sign bits, and \"b\" the total number of bits.\n\n\nBecause fixed point operations can produce results that have more bits than the operands, information loss is possible. For instance, the result of fixed point multiplication could potentially have as many bits as the sum of the number of bits in the two operands. In order to fit the result into the same number of bits as the operands, the answer must be rounded or truncated. If this is the case, the choice of which bits to keep is very important. When multiplying two fixed point numbers with the same format, for instance with formula_1 integer bits, and formula_2 fractional bits, the answer could have up to formula_3 integer bits, and formula_4 fractional bits.\n\nFor simplicity, many fixed-point multiply procedures use the same result format as the operands. This has the effect of keeping the middle bits; the I-number of least significant integer bits, and the Q-number of most significant fractional bits. Fractional bits lost below this value represent a precision loss which is common in fractional multiplication. If any integer bits are lost, however, the value will be radically inaccurate. Some model-based fixed-point packages allow you to specify a result format different from the input formats. This allows you to maximize precision and avoid overflow.\n\nSome operations, like divide, often have built-in result limiting so that any positive overflow results in the largest possible number that can be represented by the current format. Likewise, negative overflow results in the largest negative number represented by the current format. This built in limiting is often referred to as \"saturation\".\n\nSome processors support a hardware overflow flag that can generate an exception on the occurrence of an overflow, but it is usually too late to salvage the proper result at this point.\n\nVery few computer languages include built-in support for fixed point values, because for most applications, binary or decimal floating-point representations are usually simpler to use and accurate enough. Floating-point representations are easier to use than fixed-point representations, because they can handle a wider dynamic range and do not require programmers to specify the number of digits after the radix point. However, if they are needed, fixed-point numbers can be implemented even in programming languages like C and C++, which do not commonly include such support.\n\nA common use of fixed-point BCD numbers is for storing monetary values, where the inexact values of binary floating-point numbers are often a liability. Historically, fixed-point representations were the norm for decimal data types; for example, in PL/I or COBOL. The Ada programming language includes built-in support for both fixed-point (binary and decimal) and floating-point. JOVIAL and Coral 66 also provide both floating- and fixed-point types.\n\nISO/IEC TR 18037 specifies fixed-point data types for the C programming language; vendors are expected to implement the language extensions for fixed point arithmetic in coming years. Fixed-point support is implemented in GCC.\n\nC# includes a decimal type, which supports a precision of 28-29 significant digits.\n\nPython has included the decimal module in its standard libraries since version 2.4 (November, 2004).\n\nAlmost all relational databases, and the SQL, support fixed-point decimal arithmetic and storage of numbers. PostgreSQL has a special numeric type for exact storage of numbers with up to 1000 digits.\n\n\n\n"}
{"id": "42434747", "url": "https://en.wikipedia.org/wiki?curid=42434747", "title": "Game design", "text": "Game design\n\nGame design is the art of applying design and aesthetics to create a game for entertainment or for educational, exercise, or experimental purposes. Increasingly, elements and principles of game design are also applied to other interactions, particularly virtual ones (see gamification).\n\nGame design creates goals, rules and challenges to define a board game, card game, dice game, casino game, role-playing game, sport, video game, war game or simulation that produces desirable interactions among its participants and, possibly, spectators.\n\nAcademically, game design is part of game studies, while game theory studies strategic decision making (primarily in non-game situations). Games have historically inspired seminal research in the fields of probability, artificial intelligence, economics, and optimization theory. Applying game design to itself is a current research topic in metadesign.\n\nSports (see history of sports), gambling, and board games are known, respectively, to have existed for at least nine thousand, six thousand, and four thousand years.\n\nTabletop games played today whose descent can be traced from ancient times include chess, go, pachisi, backgammon, mahjong, mancala, and pick-up sticks. The rules of these games were not codified until early modern times and their features gradually evolved and changed over time, through the folk process. Given this, these games are not considered to have had a designer or been the result of a design process in the modern sense.\n\nAfter the rise of commercial game publishing in the late 19th century, many games which had formerly evolved via folk processes became commercial properties, often with custom scoring pads or preprepared material. For example, the similar public domain games Generala, Yacht, and Yatzy led to the commercial game Yahtzee in the mid-1950s.\n\nToday, many commercial games, such as Taboo, Balderdash, Pictionary, or Time's Up!, are descended from traditional parlour games. Adapting traditional games to become commercial properties is an example of game design.\n\nSimilarly, many sports, such as soccer and baseball, are the result of folk processes, while others were designed, such as basketball, invented in 1891 by James Naismith.\n\nTechnological advances have provided new media for games throughout history.\n\nThe printing press allowed packs of playing cards, adapted from Mahjong tiles, to be mass-produced, leading to many new card games. Accurate topographic maps produced as lithographs and provided free to Prussian officers helped popularize wargaming. Cheap bookbinding (printed labels wrapped around cardboard) led to mass-produced board games with custom boards. Inexpensive (hollow) lead figurine casting contributed to the development of miniature wargaming. Cheap custom dice led to poker dice. Flying discs led to disc golf and Ultimate. Personal computers contributed to the popularity of computer games, leading to the wide availability of video game consoles and video games. Smart phones have led to a proliferation of mobile games.\n\nThe first games in a new medium are frequently adaptations of older games. Pong, one of the first widely disseminated video games, adapted table tennis. Later games will often exploit distinctive properties of a new medium. Adapting older games and creating original games for new media are both examples of game design.\n\nGame studies or gaming theory is a discipline that deals with the critical study of games, game design, players, and their role in society and culture. Prior to the late-twentieth century, the academic study of games was rare and limited to fields such as history and anthropology. As the video game revolution took off in the early 1980s, so did academic interest in games, resulting in a field that draws on diverse methodologies and schools of thought. These influences may be characterized broadly in three ways: the social science approach, the humanities approach, and the industry and engineering approach.\n\nBroadly speaking, the social scientific approach has concerned itself with the question of \"What do games do to people?\" Using tools and methods such as surveys, controlled laboratory experiments, and ethnography researchers have investigated both the positive and negative impacts that playing games could have on people.\nMore sociologically informed research has sought to move away from simplistic ideas of gaming as either 'negative' or 'positive', but rather seeking to understand its role and location in the complexities of everyday life.\n\nIn general terms, the humanities approach has concerned itself with the question of \"What meanings are made through games?\" Using tools and methods such as interviews, ethnographies and participant observation, researchers have investigated the various roles that videogames play in people's lives and activities together with the meaning they assign to their experiences.\n\nFrom an industry perspective, a lot of game studies research can be seen as the academic response to the videogame industry's questions regarding the products it creates and sells. The main question this approach deals with can be summarized as \"How can we create better games?\" with the accompanying \"What makes a game good?\" \"Good\" can be taken to mean many different things, including providing an entertaining and an engaging experience, being easy to learn and play, and being innovative and having novel experiences. Different approaches to studying this problem have included looking at describing how to design games and extracting guidelines and rules of thumb for making better games\n\nGame theory is a study of strategic decision making. Specifically, it is \"the study of mathematical models of conflict and cooperation between intelligent rational decision-makers\". An alternative term suggested \"as a more descriptive name for the discipline\" is \"interactive decision theory\".\nThe subject first addressed zero-sum games, such that one person's gains exactly equal net losses of the other participant or participants. Today, however, game theory applies to a wide range of behavioral relations, and has developed into an umbrella term for the logical side of decision science.\n\nThe games studied in game theory are well-defined mathematical objects. To be fully defined, a game must specify the following elements: the \"players\" of the game, the \"information\" and \"actions\" available to each player at each decision point, and the \"payoffs\" for each outcome. (Rasmusen refers to these four \"essential elements\" by the acronym \"PAPI\".) A game theorist typically uses these elements, along with a solution concept of their choosing, to deduce a set of equilibrium strategies for each player such that, when these strategies are employed, no player can profit by unilaterally deviating from their strategy. These equilibrium strategies determine an equilibrium to the game—a stable state in which either one outcome occurs or a set of outcomes occur with known probability.\n\nGames can be characterized by \"what the player does\" and what the player experiences. This is often referred to as gameplay. Major key elements identified in this context are tools and rules that define the overall context of game.\n\nGames are often classified by the components required to play them (e.g. miniatures, a ball, cards, a board and pieces, or a computer). In places where the use of leather is well established, the ball has been a popular game piece throughout recorded history, resulting in a worldwide popularity of ball games such as rugby, basketball, football, cricket, tennis, and volleyball. Other tools are more idiosyncratic to a certain region. Many countries in Europe, for instance, have unique standard decks of playing cards. Other games such as chess may be traced primarily through the development and evolution of its game pieces.\n\nMany game tools are tokens, meant to represent other things. A token may be a pawn on a board, play money, or an intangible item such as a point scored.\n\nGames such as hide-and-seek or tag do not utilise any obvious tool; rather, their interactivity is defined by the environment. Games with the same or similar rules may have different gameplay if the environment is altered. For example, hide-and-seek in a school building differs from the same game in a park; an auto race can be radically different depending on the track or street course, even with the same cars.\n\nWhereas games are often characterized by their tools, they are often defined by their rules. While rules are subject to variations and changes, enough change in the rules usually results in a \"new\" game. There are exceptions to this in that some games deliberately involve the changing of their own rules, but even then there are often immutable meta-rules.\n\nRules generally determine turn order, the rights and responsibilities of the players, each player's goals, and how game components interact with each other in to produce changes in a game's state. Player rights may include when they may spend resources or move tokens.\n\nCommon win conditions are being first to amass a certain quota of points or tokens (as in Settlers of Catan), having the greatest number of tokens at the end of the game (as in Monopoly), some relationship of one's game tokens to those of one's opponent (as in chess's checkmate), or reaching a certain point in a storyline (as in most roleplay-games).\n\nMost games require multiple players. Single-player games are unique in respect to the type of challenges a player faces. Unlike a game with multiple players competing with or against each other to reach the game's goal, a single-player game is against an element of the environment, against one's own skills, against time, or against chance. This is also true of cooperative games, in which multiple players share a common goal and win or lose together.\n\nMany games described as \"single-player\" or \"cooperative\" could alternatively be described as puzzles or recreations, in that they do not involve strategic behavior (as defined by game theory), in which the expected reaction of an opponent to a possible move becomes a factor in choosing which move to make.\n\nGames against opponents simulated with artificial intelligence differ from other single-player games in that the algorithms used usually do incorporate strategic behavior.\n\nStories told in games may focus on narrative elements that can be communicated through the use of mechanics and player choice. Narrative plots in games generally have a clearly defined and simplistic structure. Mechanical choices on the part of the designer(s) often drastically effect narrative elements in the game. However, due to a lack of unified and standardized teaching and understanding of narrative elements in games, individual interpretations, methods, and terminology vary wildly. Because of this, most narrative elements in games are created unconsciously and intuitively. However, as a general rule, game narratives increase in complexity and scale as player choice or game mechanics increase in complexity and scale. One example of this is removing a players ability to directly affect the plot for a limited time. This lack of player choice necessitates an increase in mechanical complexity, and could be used as a metaphor to symbolize depression that is felt by a character in the narrative.\n\nA game's tools and rules will result in its requiring skill, strategy, luck, or a combination thereof, and are classified accordingly.\n\nGames of skill include games of physical skill, such as wrestling, tug of war, hopscotch, target shooting, and horseshoes, and games of mental skill such as checkers and chess. Games of strategy include checkers, chess, go, arimaa, and tic-tac-toe, and often require special equipment to play them. Games of chance include gambling games (blackjack, mah-jongg, roulette, etc.), as well as snakes and ladders and rock, paper, scissors; most require equipment such as cards or dice.\n\nMost games contain two or all three of these elements. For example, American football and baseball involve both physical skill and strategy while tiddlywinks, poker, and Monopoly combine strategy and chance. Many card and board games combine all three; most trick-taking games involve mental skill, strategy, and an element of chance, as do many strategic board games such as Risk, Settlers of Catan, and Carcassonne.\n\nBy learning through play children can develop social and cognitive skills, mature emotionally, and gain the self-confidence required to engage in new experiences and environments.\nKey ways that young children learn include playing, being with other people, being active, exploring and new experiences, talking to themselves, communication with others, meeting physical and mental challenges, being shown how to do new things, practicing and repeating skills and having fun.\n\nPlay develops children's content knowledge and provides children the opportunity to develop social skills, competences and disposition to learn. Play-based learning is based on a Vygotskian model of scaffolding where the teacher pays attention on specific elements of the play activity and provides encouragement and feedback on children's learning. When children engage in real-life and imaginary activities, play can be challenging in children's thinking. To extend the learning process, sensitive intervention can be provided with adult support when necessary during play-based learning.\n\nGame design is part of a game's development from concept to its final form. Typically, the development process is an iterative process, with repeated phases of testing and revision. During revision, additional design or re-design may be needed.\n\nA game designer (or inventor) is the person who invents a game's concept, its central mechanisms, and its rules.\n\nOften, the game designer also invents the game's title and, if the game isn't abstract, its theme. Sometimes these activities are done by the game publisher, not the designer, or may be dictated by a licensed property (such as when designing a game based on a film).\n\nA game developer is the person who fleshes out the details of a game's design, oversees its testing, and revises the game in response to player feedback.\n\nOften the game designer is also its developer, although some publishers do extensive development of games to suit their particular target audience after licensing a game from a designer. For larger games, such as collectible card games and most video games, a team is used and the designer and developer roles are usually split among multiple people.\n\nA game artist is an artist who creates art for one or more types of games.\n\nMany graphic elements of games are created by the designer when producing a prototype of the game, revised by the developer based on testing, and then further refined by the artist and combined with artwork as a game is prepared for publication or release.\n\nFor video games, game artists are responsible for all of the aspects of game development that call for visual art. Game artists are often vital to and credited in role-playing games, collectible card games and video games.\n\nA game concept is an idea for a game, briefly describing its core play mechanisms, who the players represent, and how they win or lose.\n\nA game concept may be \"pitched\" to a game publisher in a similar manner as film ideas are pitched to potential film producers. Alternatively, game publishers holding a game license to intellectual property in other media may solicit game concepts from several designers before picking one to design a game, typically paying the designer in advance against future royalties.\n\nDuring design, a game concept is fleshed out. Mechanisms are specified in terms of components (boards, cards, on-screen entities, etc.) and rules. The play sequence and possible player actions are defined, as well as how the game starts, ends, and what is its winning condition. In video games, storyboards and screen mockups may be created.\n\nA game prototype is a draft version of a game used for testing. Typically, creating a prototype marks the shift from game design to game development and testing. Although prototyping in regards to human-computer interaction and interaction design are both studied, the use of prototyping in game design has remained relatively unexplored. It's known that game design has clear benefits from prototyping, such as exploring new game design possibilities and technologies, the field of game design has different characteristics than other types of software industries that considers prototyping in game design in a different category and need a new perspective\n\nGame testing is a major part of game development. During testing, players play the game and provide feedback on its gameplay, the usability of its components or screen elements, the clarity of its goals and rules, ease of learning, and enjoyment to the game developer. The developer then revises the design, its components, presentation, and rules before testing it again. Later testing may take place with focus groups to test consumer reactions before publication.\n\nDuring testing, various balance issues may be identified, requiring changes to the game's design.\n\nVideo game testing is a software testing process for quality control of video games. The primary function of game testing is the discovery and documentation of software defects (aka bugs). Interactive entertainment software testing is a highly technical field requiring computing expertise, analytic competence, critical evaluation skills, and endurance.\n\nDifferent types of games pose different game design issues.\n\nBoard game design is the development of rules and presentational aspects of a board game. When a player takes part in a game, it is the player's self-subjection to the rules that creates a sense of purpose for the duration of the game. Maintaining the players' interest throughout the gameplay experience is the goal of board game design. To achieve this, board game designers emphasize different aspects such as social interaction, strategy, and competition, and target players of differing needs by providing for short versus long-play, and luck versus skill. Beyond this, board game design reflects the culture in which the board game is produced.\n\nThe most ancient board games known today are over 5000 years old. They are frequently abstract in character and their design is primarily focused on a core set of simple rules. Of those that are still played today, games like go (c.400BC), mancala (c.700AD), and chess (c.600AD) have gone through many presentational and/or rule variations. In the case of chess, for example, new variants are developed constantly, to focus on certain aspects of the game, or just for variation's sake.\n\nTraditional board games date from the nineteenth and early twentieth century. Whereas ancient board game design was primarily focused on rules alone, traditional board games were often influenced by Victorian mores. Academic (e.g. history and geography) and moral didacticism were important design features for traditional games, and Puritan associations between dice and the Devil meant that early American game designers eschewed their use in board games entirely. Even traditional games that did use dice, like \"Monopoly\" (based on the 1906 \"The Landlord's Game\"), were rooted in educational efforts to explain political concepts to the masses. By the 1930s and 1940s, board game design began to emphasize amusement over education, and characters from comic strips, radio programmes, and (in the 1950s) television shows began to be featured in board game adaptations.\n\nRecent developments in modern board game design can be traced to the 1980s in Germany, and have led to increased popularity of \"German-style board games\" (also known as \"Eurogames\" or \"designer games\"). The design emphasis of these board games is to give players meaningful choices. This is manifested by eliminating elements like randomness and luck to be replaced by skill, strategy, and resource competition, by removing the potential for players to fall irreversibly behind in the early stages of a game, and by reducing the number of rules and possible player options to produce what Alan R. Moon has described as \"elegant game design\". The concept of elegant game design has been identified by \"The Boston Globe\"'s Leon Neyfakh as related to Mihaly Csikszentmihalyi's concept of \"flow\" from his 1990 book, \"Flow: The Psychology of Optimal Experience\".\n\nModern technological advances have had a democratizing effect on board game production, with services like Kickstarter providing designers with essential startup capital and tools like 3D printers facilitating the production of game pieces and board game prototypes. A modern adaptation of figure games are miniature wargames like \"Warhammer 40,000\".\n\nCard games include games with cards that are custom-tailored to the game, as in many modern games, as well as those whose design is constricted by the type of the deck of cards, like Tarot or the four-suited Latin decks. Card games can be played for fun, such as Go Fish, or as gambling games, such as Poker.\n\nIn Asian cultures, special sets of tiles can serve the same function as cards, as in mahjong, a game similar to (and thought to be the distant ancestor of) the Western card game rummy. Western dominoes games are believed to have developed from Asian tile games in the 18th century.\n\n\"\" was the first collectible card game (or \"trading card game\") in 1993.\n\nThe line between card and board games is not clear-cut, as many card games, such as solitaire, involve playing cards to form a \"tableau\", a spatial layout or board. Many board games, in turn, uses specialized cards to provide random events, such as the Chance cards of Monopoly (game), or as the central mechanism driving play, as in many card-driven wargames.\n\nAs cards are typically shuffled and revealed gradually during play, most card games involve randomness, either initially or during play, and hidden information, such as the cards in a player's hand. This is in contrast to many board games, in which most of the game's current state is visible to all participants, even though players may also have a small amount of private information, such as the letter tiles on each player's rack during Scrabble.\n\nHow players play their cards, revealing information and interacting with previous plays as they do so, is central to card game design. In partnership card games, such as Bridge, rules limiting communication between players on the same team become an important part of the game design. This idea of limited communication has been extended to cooperative card games, such as Hanabi.\n\nDice games are among the oldest known games and have often been associated with gambling. Non-gambling dice games, such as Yatzy, Poker dice, or Yahtzee became popular in the mid-20th century.\n\nThe line between dice and board games is not clear-cut, as dice are often used as randomization devices in board games, such as Monopoly or Risk, while serving as the central drivers of play in games such as Backgammon or Pachisi.\n\nDice games differ from card games in that each throw of the dice is an independent event, whereas the odds of a given card being drawn is affected by all the previous cards drawn or revealed from a deck. Dice game design often centers around forming scoring combinations and managing re-rolls, either by limiting their number, as in Yahtzee, or by introducing a press-your-luck element, as in Can't Stop.\n\nCasino game design can entail the creation of an entirely new casino game, the creation of a variation on an existing casino game, or the creation of a new side bet on an existing casino game.\n\nCasino game mathematician, Michael Shackleford has noted that it is much more common for casino game designers today to make successful variations than entirely new casino games. Gambling columnist John Grochowski points to the emergence of community-style slot machines in the mid-1990s, for example, as a successful variation on an existing casino game type.\n\nUnlike the majority of other games which are designed primarily in the interest of the player, one of the central aims of casino game design is to optimize the house advantage and maximize revenue from gamblers. Successful casino game design works to provide entertainment for the player and revenue for the gambling house.\n\nTo maximise player entertainment, casino games are designed with simple easy-to-learn rules that emphasize winning (i.e. whose rules enumerate many victory conditions and few loss conditions), and that provide players with a variety of different gameplay postures (e.g. card hands). Player entertainment value is also enhanced by providing gamblers with familiar gaming elements (e.g. dice and cards) in new casino games.\n\nTo maximise success for the gambling house, casino games are designed to be easy for croupiers to operate and for pit managers to oversee.\n\nThe two most fundamental rules of casino game design is that the games must be non-fraudable (including being as nearly as possible immune from advantage gambling), and that they must mathematically favor the house winning. Shackleford suggests that the optimum casino game design should give the house an edge of smaller than 5%.\n\nThe design of role-playing games requires the establishment of setting, characters, and basic gameplay rules or mechanics. After a role-playing game is produced, additional design elements are often devised by the players themselves. In many instances, for example, character creation is left to the players. Likewise, the progression of a role-playing game is determined in large part by the gamemaster whose individual campaign design may be directed by one of several role-playing game theories.\n\nThere is no central core for tabletop role-playing game theory because different people want such different things out of the games. Probably the most famous category of RPG theory, GNS Theory assumes that people want one of three things out of the game – a better, more interestingly challenging game, to create a more interesting story, or a better simulation – in other words better rules to support worldbuilding. GNS Theory has been abandoned by its creator, partly because it neglects emotional investment, and partly because it just didn't work properly. There are techniques that people use (such as dice pools) to better create the game they want – but with no consistent goal or agreement for what makes for a good game there's no overarching theory generally agreed on.\n\nSports games are made with the same rules as the sport the game portrays.\n\nVideo game design is a process that takes place in the pre-production phase of video game development. In the video game industry, game design describes the creation of the content and rules of a video game. The goal of this process for the game designer is to provide players with the opportunity to make meaningful decisions in relation to playing the game. Elements of video game design such as the establishment of fundamental gameplay rules provide a framework within which players will operate, while the addition of narrative structures provide players with a reason to care about playing the game. To establish the rules and narrative, an internally consistent game world is created, requiring visual, audio, and programming development for world, character, and level design. The amount of work that is required to accomplish this often demands the use of a design team which may be divided into smaller game design disciplines. In order to maintain internal consistency between the teams, a specialized software design document known as a \"game design document\" (and sometimes an even broader scope \"game bible\" document) provides overall contextual guidance on ambient mood, appropriate tone, and other less tangible aspects of the game world.\n\nAn important aspect of video game design is human-computer interaction and game feel.\n\nThe first military war games, or Kriegsspiel, were designed in Prussia in the 19th century to train staff officers. They are also played as a hobby for entertainment.\n\nModern war games are designed to test doctrines, strategies and tactics in full scale exercises with opposing forces at venues like the NTC, JRTC and the JMRC, involving NATO countries.\n\n\n"}
{"id": "10994244", "url": "https://en.wikipedia.org/wiki?curid=10994244", "title": "Harry R. Lewis", "text": "Harry R. Lewis\n\nLewis has been honored for his \"particularly distinguished contributions to undergraduate teaching\"; his students have included future entrepreneurs Bill Gates and Mark Zuckerberg, and numerous future faculty members at Harvard and other schools.\nThe website \"Six Degrees to Harry Lewis\", created by Zuckerberg while at Harvard, was a precursor to Facebook.\n\nA new professorship in Engineering and Applied Sciences, endowed by a former student, will be named for Lewis and his wife upon their retirements.\nLewis was born in Boston and grew up in Wellesley. His parents were physicianshis father a hospital chief of anesthesiology and his mother the head of the Dever State School for disabled children. His father was a World War II veteran and the son of a German Lutheran father and a Russian Jewish mother. After graduating \"summa cum laude\" at the end of the eleventh grade at Boston's Roxbury Latin School he entered Harvard College, where he was for a time a third-string lacrosse goalie.\n\nLewis has said that he discovered \"I wasn't a real [once] I got out of the amateur leagues of high school mathematics\", but was \"tremendously excited\" by the computer-science research at Harvard.\nAs a senior he lectured a graduate class using a computer-graphics program, SHAPESHIFTER, which he had developed for displaying complex-plane on a cathode ray tube. SHAPESHIFTER automatically recognized formulas and commands hand-entered via a stylus on a RAND tablet, and could be \"trained\" to recognize the handwriting of individual users.\nThere being no degree program in computer science \"per se\" at Harvard at the time, in 1968 Lewis received his BA (\"summa\", Quincy House) in applied mathematics and was elected to Phi Beta Kappa.\n\nAfter two years as a mathematician and computer scientist for the National Institutes of Health in Bethesda, Maryland,\nhe spent a year in Europe as a Frederick Sheldon Traveling Fellow.\nHe then returned to Harvard, where he earned his M.A. in 1973 and PhD in 1974, after which he was immediately appointed Assistant Professor of Computer Science. He became an Associate Professor in 1978, and has been Gordon McKay Professor of Computer Science since 1981.\n\nLewis plans to retire in 2020, at which time a new professorship in Engineering and Applied Sciences, endowed by former student Larry Lebowitz, will be named for Lewis and for his wife Marlyn McGrath, who is Harvard's director of admissions.\n\nLewis has pointed out thatlargely because his career began when the field of computer science \"barely existed\", and Harvard offered almost no computer science courses at the undergraduate levelhe originated almost all the courses he has taught. It was his proposal, in the late 1970s, that Harvard create a major specifically for computer science\n(which until then had been a branch of Harvard's applied mathematics program).\n\nFrom 2003 to 2008 he was designated a Harvard College Professor in recognition of \"particularly distinguished contributions to undergraduate teaching\".\nSix of his teaching assistants are now members of the Harvard faculty and many others are professors of computer science (or related disciplines) elsewhere;\nmany have gone on to win teaching awards themselves, including Eric Roberts (Association for Computing Machinery Karlstrom Award), Nicholas Horton (Robert V. Hogg Award), Joseph A. Konstan (University of Minnesota Distinguished University Teaching Professor, Graduate/Professional Teaching Award), and Margo Seltzer (Herchel Smith Professor of Computer Science at Harvard, Phi Beta Kappa teaching award, Abramson Teaching Award).\n\nHis undergraduate students have included Mark Zuckerberg (whose website \"Six Degrees to Harry Lewis\" was a precursor to Facebook\"six degrees\" being a reference to the small world hypothesis),\nMicrosoft founder Bill Gates (who solved an open theoretical problem Lewis had described in class), and nine future Harvard professors.\n\nLewis is the author or coauthor of three undergraduate textbooks:\n\nLewis also teaches a course on amateur athletics and the social history of sports in America.\n\nIn 1994 Lewis coauthored the \"comprehensive\" \"Report on the Structure of Harvard College\", and in 1995 he was appointed dean of Harvard College, responsible for the nonacademic aspects of undergraduate life. In that capacity he oversaw a number of sometimes-controversial policy changes, including changes to the handling of allegations of sexual assault, reorganization of the college's public-service programs, a crackdown on underage alcohol use, and random assignment of students to upperclass houses (countering the social segregation found under the prior system of assignment according to student preference).\nHe also pressed improvements to advising and health care.\nA colleague has said that Lewis \"reshaped undergraduate life more powerfully than anyone else in recent memory.\"\n\nAfter the 2001 inauguration of Harvard University's twenty-seventh president, Lawrence Summers, Lewis and Summers came into conflict over the direction of the College and its educational philosophy. Lewis, for example, emphasized the importance of extracurricular pursuits, advising incoming freshmen that \"flexibility in your schedule, unstructured time in your day, and evenings spent with your friends rather than your books are all, in a larger sense, essential for your education\", while Summers complained of an insufficiently intellectual \"Camp Harvard\" and admonished students that \"You are here to work, and your business here is to learn.\" After Lewis issued what \"The Harvard Crimson\" called \"a scathing indictment of the view that increasing intellectual rigor ought to be the [College's] priority\"pointing out that prospective employers show less interest in grades than in personal qualities built outside the classroomhe was peremptorily removed as dean in March 2003.\n\nLewis continued to teach throughout his time as dean. In 2015 he served as interim Dean of the Harvard School of Engineering and Applied Sciences.\n\nLewis is a Faculty Associate of Harvard's Berkman Center for Internet & Society.\nIn addition to his research publications and textbooks, he has written a number of works on higher education and the impact of computers on society.\n\nDrawing heavily on his experience as dean of Harvard College, his \"Excellence Without A Soul: How a Great University Forgot Education\" (2006) critiques what he sees as the abandonment by American universities, including Harvard, of the\n\nIn \"Renewing the Civic Mission of American Higher Education\" (with Ellen Condliffe Lagemann, 2012) Lewis warns that \"a flourishing multiplicity of worthy but uncoordinated agendas has crowded out higher education's commitment to the common good\":\nDeveloped from a course taught by its authors,\n\"Blown to Bits: Your Life, Liberty, and Happiness After the Digital Explosion\" (2008, with Hal Abelson and Ken Ledeen) explores the origins and consequences of the 21st-century explosion in digital information, including its impact on culture and privacy:\n\"Baseball as a Second Language: Explaining the Game Americans Use to Explain Everything Else\" (self-published as an experiment in open access in 2011) discusses the many ways baseball concepts and imagery have made their way into American English. It was inspired by Lewis' experiences explaining baseball to international students.\n\nLewis' undergraduate thesis describing SHAPESHIFTER, \"Two applications of hand-printed two-dimensional computer input\", was written under computer graphics pioneer Ivan Sutherland\nand presented at the 23rd National Conference of the Association for Computing Machinery in 1968.\nIt was followed by several papers on related topics.\n\nMuch of Lewis' subsequent research concerned the computational complexity of problems in mathematical logic.\nHis doctoral thesis, \"Herbrand Expansions and Reductions of the Decision Problem\", was supervised by Burton Dreben and dealt with Herbrand's theorem.\nHis 1979 book, \"Unsolvable classes of quantificational formulas\"\ncomplemented \"The Decision Problem: Solvable classes of quantificational formula\" by Dreben and Warren Goldfarb.\n\nHis 1978 paper \"Renaming a set of clauses as a Horn set\" addressed the Boolean satisfiability problem, of determining whether a logic formula in conjunctive normal form can be made true by a suitable assignment of its variables. In general, these problems are hard, but there are two major subclasses of satisfiability for which polynomial time solutions are known: 2-satisfiability (where each clause of the formula has two literals) and Horn-satisfiability (where each clause has at most one positive literal). Lewis expanded the second of these subclasses, by showing that the problem can still be solved in polynomial time\nwhen the input is not already in Horn form, but can be put into Horn form by replacing some variables by their negations. The problem of choosing which variables to negate to make each clause get two positive literals, making the re-signed instance into a Horn set, turns out to be expressible as an instance of 2-satisfiability, the other solvable case of the satisfiability problem. By solving a 2-satisfiability instance to turn the given input into a Horn set, Lewis shows that the instances that can be turned into Horn sets can also be solved in polynomial time. The time for the sign reassignment in the original version of what Lindhorst and Shahrokhi called \"this elegant result\" was for an instance with clauses and variables, but it can be reduced to linear time by breaking long input clauses into smaller clauses and applying a faster 2-satisfiability algorithm.\n\nLewis' paper \"Complexity results for classes of quantificational formulas\" (1980) deals with the computational complexity of problems in first-order logic. Such problems are undecidable in general, but there are several special classes of these problems, defined by restricting the order in which their quantifiers appear, that were known to be decidable. One of these special classes, for instance, is the Bernays–Schönfinkel class. For each of these special classes, Lewis establishes tight\nexponential time bounds either for deterministic or nondeterministic time complexity. For instance, he shows that the Bernays–Schönfinkel class is NEXPTIME-complete, and more specifically that its nondeterministic time complexity is both upper- and lower-bounded by a singly exponential function of the input length.\nBörger, Grädel, and Gurevich write that \"this paper initiated the study of the complexity of decidable classes of the decision problem\".\n\n\"A logic of concrete time intervals\" (1990) concerned temporal logic. This paper accompanied an earlier Aiken Computation Laboratory technical report, \"Finite-state analysis of asynchronous circuits with bounded temporal uncertainty\", where he first proposed the representation of an asynchronous circuit, with bounded temporal uncertainty on gate transition events, as a finite-state machine. This paper was the earliest work on the verification of timing properties that modeled time both asynchronously and continuously, neither discretizing time nor imposing a global clock.\n\nSome of Lewis' other heavily cited research papers extend beyond logic.\nHis paper \"Symbolic evaluation and the global value graph\" (1977, with his student John Reif) concerned data-flow analysis and symbolic execution in compilers.\nAnd his paper \"Symmetric space-bounded computation\" (1982, with Christos Papadimitriou)\nwas the first to define symmetric Turing machines and symmetric space complexity classes such as SL (an undirected or reversible analogue of nondeterministic space complexity, later shown to coincide with deterministic logarithmic space).\nIn 1982, he chaired the program committee for the Symposium on Theory of Computing, one of the two top research conferences in theoretical computer science, considered broadly.\n\nLewis is a Visitor of Ralston College and a Life Trustee of the Roxbury Latin School. From 1995 to 2003 he was Trustee of the Charity of Edward Hopkins.\n\"Washington Post\" journalist David Fahrenthold is his son-in-law; while still a Harvard undergraduate, Fahrenthold wrote of his future father-in-law:\n\n"}
{"id": "1181131", "url": "https://en.wikipedia.org/wiki?curid=1181131", "title": "Herman Chernoff", "text": "Herman Chernoff\n\nHerman Chernoff (born July 1, 1923, New York) is an American applied mathematician, statistician and physicist formerly a professor at MIT and currently working at Harvard University.\n\nIn 2012 he became a fellow of the American Mathematical Society.\n\nHerman Chernoff's parents were Pauline and Max Chernoff, Jewish immigrants from Russia. He studied in Townsend Harris High School.\n\n\n\n"}
{"id": "2754080", "url": "https://en.wikipedia.org/wiki?curid=2754080", "title": "Homoeoid", "text": "Homoeoid\n\nA homoeoid is a shell (a bounded region) bounded by two concentric, similar ellipses (in 2D) or ellipsoids (in 3D).\nWhen the thickness of the shell becomes negligible, it is called a thin homoeoid. The name homoeoid was coined by Lord Kelvin and Peter Tait.\n\nIf the outer shell is given by\n\nwith semiaxes formula_2 the inner shell is given for formula_3 by\n\nThe thin homoeoid is then given by the limit\nformula_5\n\nA homoeoid can be used as a construction element of a matter or charge distribution. The gravitational or electromagnetic potential of a homoeoid homogeneously filled with matter or charge is constant inside the shell. This means that a test mass or charge will not feel any force inside the shell.\n\n"}
{"id": "366723", "url": "https://en.wikipedia.org/wiki?curid=366723", "title": "Indeterminate form", "text": "Indeterminate form\n\nIn calculus and other branches of mathematical analysis, limits involving an algebraic combination of functions in an independent variable may often be evaluated by replacing these functions by their limits; if the expression obtained after this substitution does not give enough information to determine the original limit, it is said to take on an indeterminate form. The term was originally introduced by Cauchy's student Moigno in the middle of the 19th century.\n\nThe indeterminate forms typically considered in the literature are denoted:\n\nThe most common example of an indeterminate form occurs as the ratio of two functions, in which both of these functions tend to zero in the limit, and is referred to as \"the indeterminate form 0/0\". As \"x\" approaches 0, the ratios \"x\"/\"x\", \"x\"/\"x\", and \"x\"/\"x\" go to ∞, 1, and 0 respectively. In each case, if the limits of the numerator and denominator are substituted, the resulting expression is 0/0, which is undefined. So, in a manner of speaking, 0/0 can take on the values 0, 1, or ∞, and it is possible to construct similar examples for which the limit is any particular value.\n\nMore formally, when given that the functions \"f\"(\"x\") and \"g\"(\"x\") both approach 0 as \"x\" approaches some limit point \"c\" one does not have enough information to evaluate the limit\n\nNot every undefined algebraic expression corresponds to an indeterminate form. For example, the expression 1/0 is undefined as a real number but does not correspond to an indeterminate form, because any limit that gives rise to this form will diverge to infinity.\n\nAn indeterminate form expression may have a value in some contexts.\nFor example, if \"κ\" is an infinite cardinal number, then expressions 0, 0, 1 and \"κ\" are well defined in the context of cardinal arithmetic. See also Zero to the power of zero. Note that zero to the power infinity is not an indeterminate form.\n\nThe indeterminate form 0/0 is particularly common in calculus because it often arises in the evaluation of derivatives using their limit definition.\n\nAs mentioned above,\nwhile\nThis is enough to show that 0/0 is an indeterminate form.\nOther examples with this indeterminate form include\nand\nDirect substitution of the number that \"x\" approaches into any of these expressions shows that these are examples of the indeterminate form 0/0, but these limits take many different values. Any desired value \"a\" can be obtained for this indeterminate form as follows:\nThe value infinity can also be obtained (in the sense of divergence to infinity):\n\nThe following limits illustrate that the expression 0 is an indeterminate form:\n\nThus, in general, knowing that formula_11 and formula_12 is not sufficient to calculate the limit\n\nIf the functions \"f\" and \"g\" are analytic at \"c\", and \"f\" is positive for \"x\" sufficiently close (but not equal) to \"c\", then the limit of \"f\"(\"x\") will be 1. Otherwise, use the transformation in the table below to evaluate the limit.\n\nThe expression 1/0 is not commonly regarded as an indeterminate form because there is not an infinite range of values that \"f\"/\"g\" could approach. Specifically, if \"f\" approaches 1 and \"g\" approaches 0, then \"f\" and \"g\" may be chosen so that (1) \"f\"/\"g\" approaches +∞, (2) \"f\"/\"g\" approaches −∞, or (3) the limit fails to exist. In each case the absolute value |\"f\"/\"g\"| approaches +∞, and so the quotient \"f\"/\"g\" must diverge, in the sense of the extended real numbers. (In the framework of the projectively extended real line, the limit is the unsigned infinity ∞ in all three cases.) Similarly, any expression of the form \"a\"/0, with (including and ), is not an indeterminate form since a quotient giving rise to such an expression will always diverge.\n\nThe expression 0 is not an indeterminate form. The expression 0 has the limiting value 0 for the given individual limits, and the expression 0 is equivalent to 1/0.\n\nThe adjective \"indeterminate\" does \"not\" imply that the limit does not exist, as many of the examples above show. In many cases, algebraic elimination, L'Hôpital's rule, or other methods can be used to manipulate the expression so that the limit can be evaluated.\n\nFor example, the expression \"x\"/\"x\" can be simplified to \"x\" at any point other than \"x\" = 0. Thus, the limit of this expression as \"x\" approaches 0 (which depends only on points \"near\" 0, not at \"x\" = 0 itself) is the limit of \"x\", which is 0. Most of the other examples above can also be evaluated using algebraic simplification.\n\nWhen two variables formula_14 and formula_15 converge to zero at the same point and formula_16, they are called \"equivalent infinitesimal\".\n\nFor the evaluation of the indeterminate form 0/0, we can use the following equivalent infinitesimals:\n\nformula_17\n\nformula_18\n\nformula_19\n\nformula_20\n\nformula_21\n\nformula_22\n\nformula_23\n\nformula_24\n\nformula_25\n\nformula_26\n\nformula_27\n\nFor example:\nHere is a brief proof:\n\nSuppose there are two equivalent infinitesimals formula_28 and formula_29.\n\nL'Hôpital's rule is a general method for evaluating the indeterminate forms 0/0 and ∞/∞. This rule states that (under appropriate conditions)\nwhere \"f<nowiki>'</nowiki>\" and \"g<nowiki>'</nowiki>\" are the derivatives of \"f\" and \"g\". (Note that this rule does \"not\" apply to expressions ∞/0, 1/0, and so on; these expressions are not indeterminate forms.) These derivatives will allow one to perform algebraic simplification and eventually evaluate the limit.\n\nL'Hôpital's rule can also be applied to other indeterminate forms, using first an appropriate algebraic transformation. For example, to evaluate the form 0:\nThe right-hand side is of the form ∞/∞, so L'Hôpital's rule applies to it. Note that this equation is valid (as long as the right-hand side is defined) because the natural logarithm (ln) is a continuous function; it is irrelevant how well-behaved \"f\" and \"g\" may (or may not) be as long as \"f\" is asymptotically positive.\n\nAlthough L'Hôpital's rule applies both to 0/0 and to ∞/∞, one of these forms may be more useful than the other in a particular case (because of the possibility of algebraic simplification afterwards). One can change between these forms, if necessary, by transforming \"f\"/\"g\" to (1/\"g\")/(1/\"f\").\n\nThe following table lists the most common indeterminate forms and the transformations for applying l'Hôpital's rule.\n\n"}
{"id": "2156522", "url": "https://en.wikipedia.org/wiki?curid=2156522", "title": "Institution (computer science)", "text": "Institution (computer science)\n\nThe notion of institution has been created by Joseph Goguen and Rod Burstall in the late 1970s. In order to deal with the \"population explosion among the logical systems used in computer science\". The notion tries to capture the essence of the concept of \"logical system\".\n\nThe use of institutions makes it possible to develop concepts of specification languages (like structuring of specifications, parameterization, implementation, refinement, development), proof calculi and even tools in a way completely independent of the underlying logical system. There are also morphisms that allow to relate and translate logical systems. Important applications of this are re-use of logical structure (also called borrowing), heterogeneous specification and combination of logics. \n\nThe spread of institutional model theory has generalized various notions and results of model theory and institutions themselves have impacted the progress of universal logic.\nThe theory of institutions does not assume anything about the nature of the logical system. That is, models and sentences may be arbitrary objects; the only assumption is that there is a satisfaction relation between models and sentences, telling whether a sentence holds in a model or not. Satisfaction is inspired by Tarski's truth definition, but can in fact be any binary relation.\nA crucial feature of institutions now is that models, sentences and their satisfaction are always being considered to live in some vocabulary or context (called signature) that defines the (non-logical) symbols that may be used in sentences and that need to be interpreted in models. Moreover, signature morphisms allow to extend signatures, change notation etc. Nothing is assumed about signatures and signature morphisms except that signature morphisms can be composed; this amounts to having a \ncategory of signatures and morphisms. Finally, it is assumed that signature morphisms lead to translations of sentences and models in a way that satisfaction is preserved. While sentences are translated along with signature morphisms (think of symbols being replaced along the morphism), models are translated (or better: reduced) against signature morphisms: for example, in case of a signature extension, a model of the (larger) target signature may be reduced to a model of the (smaller) source signature by just forgetting some components of the model.\n\nFormally, an institution consists of\n\n\nsuch that for each formula_5 in formula_1 the following satisfaction condition holds:\n\nformula_20 if and only if formula_21\n\nfor each formula_22 and formula_23.\n\nThe satisfaction condition expresses that truth is invariant under change of notation \n(and also under enlargement or quotienting of context).\n\nStrictly speaking, the model functor ends in the \"category\" of all large categories.\n\n\n\n\n"}
{"id": "26919476", "url": "https://en.wikipedia.org/wiki?curid=26919476", "title": "Jeff Kahn", "text": "Jeff Kahn\n\nJeffry Ned Kahn is a professor of mathematics at Rutgers University notable for his work in combinatorics. Kahn received his Ph.D from The Ohio State University in 1979 after completing his dissertation under his advisor Dijen K. Ray-Chaudhuri.\n\nIn 1980 he showed the importance of the bundle theorem for ovoidal Möbius planes. In 1993, together with Gil Kalai, he disproved Borsuk's conjecture. In 1996 he was awarded the Pólya Prize (SIAM). \nIn 2004, with David Galvin\nhe made seminal contributions to the combinatorial theory of phase transitions.\n\nHe was an invited speaker at the 1994 International Congress of Mathematicians in Zurich.\n\nIn 2012, he was awarded the Fulkerson Prize (jointly with Anders Johansson and Van H. Vu) for determining the threshold of edge density above which a random graph can be covered by disjoint copies of a given smaller graph. Also in 2012, he became a fellow of the American Mathematical Society.\n"}
{"id": "8207636", "url": "https://en.wikipedia.org/wiki?curid=8207636", "title": "John Shaw-Lefevre", "text": "John Shaw-Lefevre\n\nSir John George Shaw-Lefevre KCB (24 January 1797 – 20 August 1879) was a British barrister, Whig politician and civil servant.\n\nShaw-Lefevre was the son of Charles Shaw-Lefevre by his wife Helen, daughter of John Lefevre. Charles Shaw-Lefevre, 1st Viscount Eversley, was his elder brother. He was educated at Trinity College, Cambridge, where he was Senior Wrangler in 1818, and was called to the Bar, Inner Temple. He was elected a Fellow of the Royal Society in 1820. \n\nHe was returned to Parliament for Petersfield in December 1832, but was unseated on petition in March 1833. He served under Lord Grey as Under-Secretary of State for War and the Colonies in 1834. The latter year Shaw-Lefevre was appointed a Poor Law Commissioner after the passing of the Poor Law Amendment Act, which he remained until 1841. Between 1856 and 1875 he served as Clerk of the Parliaments. He also helped found the University of London and served as its Vice-Chancellor for many years. He was made a KCB in 1857 for his public services.\n\nShaw-Lefevre married Rachel Emily, daughter of Ichabod Wright, in 1824. They had one surviving son, George, who became a prominent politician and was ennobled as Baron Eversley, and five daughters. One daughter, Madeleine Shaw-Lefevre, was the first Principal of Somerville Hall; another daughter, Rachel, married Arthur Hamilton-Gordon, son of the Prime Minister the 4th Earl of Aberdeen.\n\nShaw-Lefevre died in August 1879, aged 82. His wife lived for six more years before dying in February 1885.\n\nThe Lefevre Peninsula in South Australia, was named by Governor John Hindmarsh on 3 June 1837 after Shaw-Lefevre, who was one of South Australia's Colonisation Commissioners.\n\n"}
{"id": "48504779", "url": "https://en.wikipedia.org/wiki?curid=48504779", "title": "José de Jesús Martínez", "text": "José de Jesús Martínez\n\nJosé de Jesús Martínez (also known as \"Chuchú Martínez\" or \"Sergeant Chuchú\") (June 8, 1929 in Managua – January 27, 1991 in Panama) was a poet, playwright, philosopher, pilot and mathematician and a former aide to General Omar Torrijos Herrera, ruler of Panama from 1968 to 1981. Through his association with Torrijos, Martínez became a major figure in Graham Greene's 1984 book \"Getting to Know the General: The Story of an Involvement\".\n\nMartínez was born in Managua in 1929.\n\nMartínez became a Panamanian citizen and an aide to General Omar Torrijos Herrera, ruler of Panama from 1968 to 1981. Martínez was known as \"Chuchú Martínez\" in the intellectual circles, and later as \"Sergeant Chuchu\" among the military.\n\nMartínez was professor of Marxist philosophy at Panama University and also professor of mathematics.\n\nIn literature, Martínez was awarded the National Theater Prize in Madrid in 1952 for his play, \"La Perrera\". In 1969 and 1971 he won Panama's national literary prize Premio Ricardo Miro for his plays and philosophical essays. For his book, \"Mi General Torrijos\" (1987) he won Cuba's Casa de Las Americas prize.\n\nThrough his association with Torrijos, Martínez became a major figure in Graham Greene's 1984 book \"Getting to Know the General: The Story of an Involvement\".\n\nMartínez died, aged 61, in 1991.\n\n\n"}
{"id": "10002357", "url": "https://en.wikipedia.org/wiki?curid=10002357", "title": "Khintchine inequality", "text": "Khintchine inequality\n\nIn mathematics, the Khintchine inequality, named after Aleksandr Khinchin and spelled in multiple ways in the Roman alphabet, is a theorem from probability, and is also frequently used in analysis. Heuristically, it says that if we pick formula_1 complex numbers formula_2, and add them together each multiplied by a random sign formula_3, then the expected value of the sum's modulus, or the modulus it will be closest to on average, will be not too far off from formula_4.\n\nLet formula_5 be i.i.d. random variables \nwith formula_6 for formula_7, \ni.e., a sequence with Rademacher distribution. Let formula_8. Then\n\nfor some constants formula_10 depending only on formula_11 (see Expected value for notation). The sharp values of the constants formula_12 were found by Haagerup (Ref. 2; see Ref. 3 for a simpler proof). It is a simple matter to see that formula_13 when formula_14, and formula_15 when formula_16.\n\nThe uses of this inequality are not limited to applications in probability theory. One example of its use in analysis is the following: if we let formula_17 be a linear operator between two L spaces formula_18 and formula_19, formula_20, with bounded norm formula_21, then one can use Khintchine's inequality to show that\n\nfor some constant formula_23 depending only on formula_11 and formula_25.\n\n\n"}
{"id": "3078180", "url": "https://en.wikipedia.org/wiki?curid=3078180", "title": "Kiss (cryptanalysis)", "text": "Kiss (cryptanalysis)\n\nIn cryptanalysis, a kiss is a pair of identical messages sent using different ciphers, one of which has been broken. The term was used at Bletchley Park during World War II. A deciphered message in the breakable system provided a \"crib\" (piece of known plaintext) which could then be used to read the unbroken messages. One example was where messages read in a German meteorological cipher could be used to provide cribs for reading the difficult 4-wheel Naval Enigma cipher.\n\n\n"}
{"id": "8170186", "url": "https://en.wikipedia.org/wiki?curid=8170186", "title": "Lawrence J. Fogel", "text": "Lawrence J. Fogel\n\nDr. Lawrence Jerome Fogel (March 2, 1928 – February 18, 2007) was a pioneer in evolutionary computation and human factors analysis. He is known as the inventor of active noise cancellation and the father of evolutionary programming. His scientific career spanned nearly six decades and included electrical engineering, aerospace engineering, communication theory, human factors research, information processing, cybernetics, biotechnology, artificial intelligence, and computer science.\n\nDuring 1948-1949, shortly after completing his bachelor's degree in electrical engineering from New York University, Lawrence Fogel worked at Watson Laboratories (USAF) computing radiation patterns for VHF and UHF radio direction finders for use in ground-to-air operations. He designed feedback amplifier filters to improve the signal-to-noise ratio for these radio systems. At Eglin Air Force Base, he controlled the final flight test program for the Diversity Antenna Array. Between 1950 and 1953, Fogel worked for Coles Signal Laboratory (U.S. Army Signal Corps) as an engineer in charge of the installation of electronic communication and navigation equipment in Army aircraft and helicopters. He completed his master's degree in electrical engineering at this same time from Rutgers University. During his time with Stavid Engineering, Inc. (New Jersey) between 1953 and 1956, he directed field operations of the Regulus Missile guidance system for submarines and also assisted with the design of flight instrumentation, communications, and electronics for aircraft and helicopters. As a part of this research, he formulated a solution for a mathematical model of the human operator as part of an aircraft flight control system that included such qualities as anticipation, development of a computer facility incorporating such a mode for use in the design of more effective human-machine relations. His efforts also led to five patents between 1958 and 1961 regarding active noise cancellation to reduce noise in helicopter cockpit environments for improved communication. These were the first patents in noise-cancelling headphone systems.\n\nFogel was also interested in information theory and communications, especially those associated with aircraft instrument displays. He published several articles intended to link communication theory and instrument design., These investigations led to other strategies to help with air traffic control, as this was similar to the information transfer of knowledge to humans that was experienced in the cockpit.\n\nIn 1956, Fogel moved to San Diego, California to work for Convair, a division of General Dynamics Corporation. He worked as head of the Reliability Group with the responsibility for maintaining and improving the overall reliability of systems such as the F-102 Delta Dagger, F-106 Delta Dart, Convair 880 transport, and other missile and electronics products. He was responsible for human engineering systems design and analysis for manned aircraft. Fogel initiated a program of investigation into the use of anticipatory displays that allow the pilot to \"fly ahead\" of the aircraft system being controlled. He invented and patented a new display called the \"Kinelog,\" which for the first time offered the pilot inter-sensory compatibility as a protection against the onset of vertigo.\n\nOn leave from Convair, Lawrence Fogel served as Special Assistant to the Associate Director (Research) at the National Science Foundation (NSF) from July 1960 to July 1961. While at the NSF, Fogel represented the Associate Director at technical and professional meetings related to the merit of individual research proposals or to the effect of national policies on future manpower and economic and military strength. He devised mathematical models for the projection of the economic value of science funding. These projections were coordinated with many agencies including the Department of Defense, National Aeronautics and Space Administration, Department of Health, Education, and Welfare, and the United States Atomic Energy Commission. This effort culminated in a report to the U.S. Congress on the worth of investing in scientific progress. While at NSF, Fogel's interest in cybernetics, biotechnology, and consciousness led to a hypothesis that a simulation of evolution on computers could be used to generate artificial intelligence without the need for expert systems. These theories were first tested successfully upon his return to Convair in 1961.\n\nIn 1960, Fogel served as a member of the founding editorial board for the journal \"IRE Transactions on Human Factors in Electronics.\" He remained as a part of that editorial board as that journal transitioned to the IEEE and became \"IEEE Transactions on Man-Machine Systems,\" a predecessor to \"IEEE Systems, Man, and Cybernetics.\"\n\nWhile at Convair during 1961-1965, Fogel explored evolutionary programming for time-series prediction. These experiments validated the merit of the approach and this became the basis of Fogel's Ph.D. dissertation \"On the Organization of Intellect\" at the University of California, Los Angeles where he received the Ph.D. in 1964 in biotechnology with minors in mathematics and communication theory. His was the first dissertation in the field of evolutionary computation. The dissertation became the basis of several papers as well as the first book in the field of evolutionary computation \"Artificial Intelligence Through Simulated Evolution\" co-authored with Alvin Owens and Michael Walsh, also from Convair. Fogel continued to publish on these concepts in the scientific literature.\n\nIn his role as Senior Staff Scientist in Astronautics for General Dynamics, he provided improved the reliability of complex missile systems and information processing systems. Fogel devised the COFEC Reliability Data System for the Atlas rocket. The Atlas was built at Convair in San Diego as America's first intercontinental ballistic missile. The Atlas D was used for Project Mercury to launch astronauts into low-Earth orbit in the 1960s. He also focused on how humans sense and process information.\n\nFogel also became associated with the burgeoning field of cybernetics and served as the third president of the American Society for Cybernetics in 1969, following Warren McCulloch. He also served as the founding editor-in-chief for the \"Journal of Cybernetics\" and helped organize the second and third annual symposia of the American Society for Cybernetics (1964, 1965).\n\nIn 1965, Fogel left General Dynamics to form a new company, Decision Science, Inc. in San Diego, specifically for applications of evolutionary programming. He served as President and directed research and real-world applications in the areas of information science, computer simulation, prediction, and systems control. Decision Science, Inc. was the first company specifically applying evolutionary computation to solve real-world problems. The methods were further developed through the efforts of Alvin Owens and George Burgin and formed the basis of a new generation of flight simulator first deployed at Langley Research Center for the purpose of air-to-air combat training. This approach was called the Adaptive Maneuvering Logic. While at Decision Science, Fogel and Burgin also experimented with simulations of co-evolutionary games. He also continued applying evolutionary computation in many ways including modeling of human operators and thinking about biological communication.\n\nIn 1982, Decision Science, Inc. was acquired by the Titan Corporation, a defense contractor in San Diego. Fogel continued working as a Vice President at Titan, and later in 1988 as a Vice President of ORINCON Corporation. In 1993, he was a co-founder in the formation of a new company, Natural Selection, Inc., which continues to apply methods of computational intelligence to real-world problems. Lawrence Fogel was the President of Natural Selection, Inc. until his death in 2007.\n\nFogel was also well known for his interest in radio-controlled sailboats and sailplanes. He was a passionate enthusiast and preservation advocate for the Torrey Pines Gliderport in San Diego. In the 1970s, he wrote a monthly column on RC soaring for \"Model Builder\" magazine. He was twice president of the Torrey Pines Gulls R/C Soaring Club, co-founded the Torrey Pines Scale Soaring Society, and was president of the National Soaring Society from 1975 to 1977. He was recognized with the highest membership grade of Fellow by the Academy of Model Aeronautics in 1996.\n\nFogel also enjoyed music and was proficient on piano, flute, saxophone, clarinet, and other instruments. He often enjoyed playing jazz at the Catamaran Hotel and other locations in San Diego and Washington, D.C.\n\n\n\n\n\n\n"}
{"id": "23992011", "url": "https://en.wikipedia.org/wiki?curid=23992011", "title": "List of derivatives and integrals in alternative calculi", "text": "List of derivatives and integrals in alternative calculi\n\nThere are many alternatives to the classical calculus of Newton and Leibniz; for example, each of the infinitely many non-Newtonian calculi. Occasionally an alternative calculus is more suited than the classical calculus for expressing a given scientific or mathematical idea.\n\nThe table below is intended to assist people working with the alternative calculus called the \"geometric calculus\" (or its discrete analog). Interested readers are encouraged to improve the table by inserting citations for verification, and by inserting more functions and more calculi.\n\nIn the following table formula_1 is the digamma function, formula_2 is the K-function, formula_3 is subfactorial, formula_4 are the generalized to real numbers Bernoulli polynomials.\n\n\n"}
{"id": "350167", "url": "https://en.wikipedia.org/wiki?curid=350167", "title": "List of graph theory topics", "text": "List of graph theory topics\n\nThis is a list of graph theory topics, by Wikipedia page.\n\nSee glossary of graph theory terms for basic terminology\n\n\n\n\n\n\n\"See list of network theory topics\"\n\n"}
{"id": "39479277", "url": "https://en.wikipedia.org/wiki?curid=39479277", "title": "List of things named after Élie Cartan", "text": "List of things named after Élie Cartan\n\nThese are things named after Élie Cartan (9 April 1869 – 6 May 1951), a French mathematician.\n\nNote some are after Henri Cartan, a son of É. Cartan; e.g.,\n"}
{"id": "27096954", "url": "https://en.wikipedia.org/wiki?curid=27096954", "title": "Lobb number", "text": "Lobb number\n\nIn combinatorial mathematics, the Lobb number \"L\" counts the number of ways that \"n\" + \"m\" open parentheses and \"n\" − \"m\" close parentheses can be arranged to form the start of a valid sequence of balanced parentheses.\n\nLobb numbers form a natural generalization of the Catalan numbers, which count the number of complete strings of balanced parentheses of a given length. Thus, the \"n\"th Catalan number equals the Lobb number \"L\". They are named after Andrew Lobb, who used them to give a simple inductive proof of the formula for the \"n\" Catalan number.\n\nThe Lobb numbers are parameterized by two non-negative integers \"m\" and \"n\" with \"n\" ≥ \"m\" ≥ 0. The (\"m\", \"n\") Lobb number \"L\" is given in terms of binomial coefficients by the formula\n\nAs well as counting sequences of parentheses, the Lobb numbers also count the number of ways in which \"n\" + \"m\" copies of the value +1 and \"n\" − \"m\" copies of the value −1 may be arranged into a sequence such that all of the partial sums of the sequence are non-negative.\n"}
{"id": "1721284", "url": "https://en.wikipedia.org/wiki?curid=1721284", "title": "Method of exhaustion", "text": "Method of exhaustion\n\nThe method of exhaustion (, or ) is a method of finding the area of a shape by inscribing inside it a sequence of polygons whose areas converge to the area of the containing shape. If the sequence is correctly constructed, the difference in area between the \"n\"-th polygon and the containing shape will become arbitrarily small as \"n\" becomes large. As this difference becomes arbitrarily small, the possible values for the area of the shape are systematically \"exhausted\" by the lower bound areas successively established by the sequence members.\n\nThe method of exhaustion typically required a form of proof by contradiction, known as \"reductio ad absurdum\". This amounts to finding an area of a region by first comparing it to the area of a second region (which can be “exhausted” so that its area becomes arbitrarily close to the true area). The proof involves assuming that the true area is greater than the second area, and then proving that assertion false, and then assuming that it is less than the second area, and proving that assertion false, too.\n\nThe idea originated in the late 5th century BC with Antiphon, although it is not entirely clear how well he understood it. The theory was made rigorous a few decades later by Eudoxus of Cnidus, who used it to calculate areas and volumes. It was later reinvented in China by Liu Hui in the 3rd century AD in order to find the area of a circle. The first use of the term was in 1647 by Gregory of Saint Vincent in \"Opus geometricum quadraturae circuli et sectionum\".\n\nThe method of exhaustion is seen as a precursor to the methods of calculus. The development of analytical geometry and rigorous integral calculus in the 17th-19th centuries subsumed the method of exhaustion so that it is no longer explicitly used to solve problems. An important alternative approach was Cavalieri's principle, also termed the \"method of indivisibles\", which eventually evolved into the infinitesimal calculus of Roberval, Torricelli, Wallis, Leibniz, and others.\n\nEuclid used the method of exhaustion to prove the following six propositions in the book 12 of his \"Elements\".\n\nArchimedes used the method of exhaustion as a way to compute the area inside a circle by filling the circle with a polygon of a greater area and greater number of sides. The quotient formed by the area of this polygon divided by the square of the circle radius can be made arbitrarily close to π as the number of polygon sides becomes large, proving that the area inside the circle of radius r is πr, π being defined as the ratio of the circumference to the diameter (C/d) or of the area of the circle to the square of its radius (A/r²).\n\nHe also provided the bounds 3 + \"/\" < \"π\" < 3 + \"/\", (giving a range of \"/\") by comparing the perimeters of the circle with the perimeters of the inscribed and circumscribed 96-sided regular polygons.\n\nOther results he obtained with the method of exhaustion included\n\n"}
{"id": "3126358", "url": "https://en.wikipedia.org/wiki?curid=3126358", "title": "Operad theory", "text": "Operad theory\n\nOperad theory is a field of mathematics concerned with prototypical algebras that model properties such as commutativity or anticommutativity as well as various amounts of associativity. Operads generalize the various associativity properties already observed in algebras and coalgebras such as Lie algebras or Poisson algebras by modeling computational trees within the algebra. Algebras are to operads as group representations are to groups. An operad can be seen as a set of operations, each one having a fixed finite number of inputs (arguments) and one output, which can be composed one with others. They form a category-theoretic analog of universal algebra.\n\nOperads originate in algebraic topology from the study of iterated loop spaces by J. Michael Boardman and Rainer M. Vogt, and J. Peter May. The word \"operad\" was created by May as a portmanteau of \"operations\" and \"monad\" (and also because his mother was an opera singer). Interest in operads was considerably renewed in the early 90s when, based on early insights of Maxim Kontsevich, Victor Ginzburg and Mikhail Kapranov discovered that some duality phenomena in rational homotopy theory could be explained using Koszul duality of operads. Operads have since found many applications, such as in deformation quantization of Poisson manifolds, the Deligne conjecture, or graph homology in the work of Maxim Kontsevich and Thomas Willwacher.\n\nA non-symmetric operad (sometimes called an operad without permutations, or a non-formula_1 or plain operad) consists of the following:\n\nsatisfying the following coherence axioms:\n\n(the number of arguments corresponds to the arities of the operations).\n\nAlternatively, a plain operad is a multicategory with one object.\n\nA symmetric operad (often just called operad) is a non-symmetric operad formula_11 as above, together with a right action of the symmetric group formula_12 on formula_13, satisfying the above associative and identity axioms, as well as\n\n(where by abuse of notation, formula_16 on the right hand side of the first equivariance relation is the element\nof formula_17 that acts on the set formula_18 by breaking it into formula_3 blocks, the first of size formula_20, the second of size formula_21, through the formula_3th block of size formula_23, and then permutes these formula_3 blocks by formula_16).\n\nThe permutation actions in this definition are vital to most applications, including the original application to loop spaces.\n\nA morphism of operads formula_26 consists of a sequence\nwhich:\n\nWe have so far considered only operads in the category of sets. It is actually possible to define operads in any symmetric monoidal category (or, for non-symmetric operads, any monoidal category).\n\nA common example would be given by the category of topological space, with the monoidal product given by the Cartesian product. In this case, a topological operad is given by a sequence of \"spaces\" (instead of sets) formula_33. The structure maps of the operad (the composition and the actions of the symmetric groups) must then be assumed to be continuous. The result is called a \"topological operad\". Similarly, in the definition of a morphism, it would be necessary to assume that the maps involved are continuous.\n\nOther common settings to define operads include, for example, module over a ring, chain complexes, groupoids (or even the category of categories itself), coalgebras, etc.\n\n\"Associativity\" means that \"composition\" of operations is associative\n(the function formula_34 is associative), analogous to the axiom in category theory that formula_35; it does \"not\" mean that the operations \"themselves\" are associative as operations.\nCompare with the associative operad, below.\n\nAssociativity in operad theory means that one can write expressions involving operations without ambiguity from the omitted compositions, just as associativity for operations allows one to write products without ambiguity from the omitted parentheses.\n\nFor instance, suppose that formula_29 is a binary operation, which is written as formula_37 or formula_38. Note that formula_29 may or may not be associative.\n\nThen what is commonly written formula_40 is unambiguously written operadically as formula_41 . This sends formula_42 to formula_43 (apply formula_29 on the first two, and the identity on the third), and then the formula_29 on the left \"multiplies\" formula_46 by formula_47.\nThis is clearer when depicted as a tree:\n\nwhich yields a 3-ary operation:\n\nHowever, the expression formula_48 is \"a priori\" ambiguous:\nit could mean formula_49, if the inner compositions are performed first, or it could mean formula_50,\nif the outer compositions are performed first (operations are read from right to left).\nWriting formula_51, this is formula_52 versus formula_53. That is, the tree is missing \"vertical parentheses\":\n\nIf the top two rows of operations are composed first (puts an upward parenthesis at the formula_54 line; does the inner composition first), the following results:\n\nwhich then evaluates unambiguously to yield a 4-ary operation.\nAs an annotated expression:\n\nIf the bottom two rows of operations are composed first (puts a downward parenthesis at the formula_56 line; does the outer composition first), following results:\n\nwhich then evaluates unambiguously to yield a 4-ary operation:\n\nThe operad axiom of associativity is that \"these yield the same result,\" and thus that the expression formula_48 is unambiguous.\n\nThe identity axiom (for a binary operation) can be visualized in a tree as:\n\nmeaning that the three operations obtained are equal: pre- or post- composing with the identity makes no difference. Note that, as for categories, formula_58 is a corollary of the identity axiom.\n\nA little discs operad or, little balls operad or, more specifically, the little \"n\"-discs operad is a topological operad defined in terms of configurations of disjoint \"n\"-dimensional discs inside a unit \"n\"-disc centered in the origin of R. The operadic composition for little 2-discs is illustrated in the figure.\n\nOriginally the little \"n\"-cubes operad or the little intervals operad (initially called little \"n\"-cubes PROPs) was defined by Michael Boardman and Rainer Vogt in a similar way, in terms of configurations of disjoint axis-aligned \"n\"-dimensional hypercubes (n-dimensional intervals) inside the unit hypercube. Later it was generalized by May to little convex bodies operad, and \"little discs\" is a case of \"folklore\" derived from the \"little convex bodies\".\n\nAnother class of examples of operads are those capturing the structures of algebraic structures, such as associative algebras, commutative algebras and Lie algebras. Each of these can be exhibited as a finitely presented operad, in each of these three generated by binary operations.\n\nThus, the associative operad is generated by a binary operation formula_59, subject to the condition that\n\nThis condition \"does\" correspond to associativity of the binary operation formula_59; writing formula_62 multiplicatively, the above condition is formula_63. This associativity of the \"operation\" should not be confused with associativity of \"composition\"; see the axiom of associativity, above.\n\nThis operad is terminal in the category of non-symmetric operads, as it has exactly one \"n\"-ary operation for each \"n,\" corresponding to the unambiguous product of \"n\" terms: formula_64. For this reason, it is sometimes written as 1 by category theorists (by analogy with the one-point set, which is terminal in the category of sets).\n\nThe terminal symmetric operad is the operad whose algebras are commutative monoids, which also has one \"n\"-ary operation for each \"n\", with each formula_65 acting trivially; this triviality corresponds to commutativity, and whose \"n\"-ary operation is the unambiguous product of \"n\"-terms, where order does not matter:\nfor any permutation formula_67.\n\nThere is an operad for which each formula_13 is given by the symmetric group formula_65. The composite formula_70 permutes its inputs in blocks according to formula_71, and within blocks according to the appropriate formula_72. Similarly, there is a non-formula_1 operad for which each formula_13 is given by the Artin braid group formula_75. Moreover, this non-formula_1 operad has the structure of a braided operad, which generalizes the notion of an operad from symmetric to braid groups.\n\nIn linear algebra, one can consider vector spaces to be algebras over the operad formula_77 (the infinite direct sum, so only finitely many terms are non-zero; this corresponds to only taking finite sums), which parametrizes linear combinations: the vector formula_78 for instance corresponds to the linear combination\n\nSimilarly, one can consider affine combinations, conical combinations, and convex combinations to correspond to the sub-operads where the terms sum to 1, the terms are all non-negative, or both, respectively. Graphically, these are the infinite affine hyperplane, the infinite hyper-octant, and the infinite simplex. This formalizes what is meant by formula_80 being or the standard simplex being model spaces, and such observations as that every bounded convex polytope is the image of a simplex. Here suboperads correspond to more restricted operations and thus more general theories.\n\nThis point of view formalizes the notion that linear combinations are the most general sort of operation on a vector space – saying that a vector space is an algebra over the operad of linear combinations is precisely the statement that \"all possible\" algebraic operations in a vector space are linear combinations. The basic operations of vector addition and scalar multiplication are a generating set for the operad of all linear combinations, while the linear combinations operad canonically encodes all possible operations on a vector space.\n\n\n"}
{"id": "34917449", "url": "https://en.wikipedia.org/wiki?curid=34917449", "title": "PRESENT", "text": "PRESENT\n\nPRESENT is a lightweight block cipher, developed by the Orange Labs (France), Ruhr University Bochum (Germany) and the Technical University of Denmark in 2007. PRESENT is designed by Andrey Bogdanov, Lars R. Knudsen, Gregor Leander, Christof Paar, Axel Poschmann, Matthew J. B. Robshaw, Yannick Seurin, and C. Vikkelsoe. The algorithm is notable for its compact size (about 2.5 times smaller than AES).\n\nThe block size is 64 bits and the key size can be 80 bit or 128 bit. The non-linear layer is based on a single 4-bit S-box which was designed with hardware optimizations in mind. \"PRESENT\" is intended to be used in situations where low-power consumption and high chip efficiency is desired. The International Organization for Standardization and the International Electrotechnical Commission included PRESENT in the new international standard for lightweight cryptographic methods.\n\nA truncated differential attack on 26 out of 31 rounds of \"PRESENT\" was suggested in 2014.\n\nSeveral full-round attacks using biclique cryptanalysis have been introduced on \"PRESENT\".\n\nBy design all block ciphers with a block size of 64 bit can have problems with block collisions if they are used with large amounts of data. Therefore, implementations need to make sure that the amount of data encrypted with the same key is limited and rekeying is properly implemented.\n\n"}
{"id": "47549053", "url": "https://en.wikipedia.org/wiki?curid=47549053", "title": "Peter W. Bates", "text": "Peter W. Bates\n\nPeter W. Bates is a professor of mathematics at Michigan State University.\n\nBates received his Ph.D. from the University of Utah in 1976.\n\nIn 2012, Bates became a fellow of the American Mathematical Society.\n"}
{"id": "41026219", "url": "https://en.wikipedia.org/wiki?curid=41026219", "title": "Pointer jumping", "text": "Pointer jumping\n\nPointer jumping or path doubling is a design technique for parallel algorithms that operate on pointer structures, such as linked lists and directed graphs. It can be used to find the roots of a forest of rooted trees, and can also be applied to parallelize many other graph algorithms including connected components, minimum spanning trees, and biconnected components.\n\nOne of the simpler tasks that can be solved by a pointer jumping algorithm is the \"list ranking\" problem. This problem is defined as follows: given a linked list of nodes, find the distance (measured in the number of nodes) of each node to the end of the list. The distance is defined as follows, for nodes that point to their successor by a pointer called :\n\n\nThis problem can easily be solved in linear time on a sequential machine, but a parallel algorithm can do better: given processors, the problem can be solved in logarithmic time, , by the following pointer jumping algorithm:\n\nThe pointer jumping occurs in the last line of the algorithm, where each node's pointer is reset to skip the node's direct successor. It is assumed, as in common in the PRAM model of computation, that memory access are performed in lock-step, so that each memory fetch is performed before each memory store; otherwise, processors may clobber each other's data, producing inconsistencies.\n\nAnalyzing the algorithm yields a logarithmic running time. The initialization loop takes constant time, because each of the processors performs a constant amount of work, all in parallel. The inner loop of the main loop also takes constant time, as does (by assumption) the termination check for the loop, so the running time is determined by how often this inner loop is executed. Since the pointer jumping in each iteration splits the list into two parts, one consisting of the \"odd\" elements and one of the \"even\" elements, the length of the list pointed to by each processor's is halved in each iteration, which can be done at most time before each list has a length of at most one.\n\nFollowing a path in a graph is an inherently serial operation, but pointer jumping reduces the total amount of work by following all paths simultaneously and sharing results among dependent operations. Pointer jumping iterates and finds a \"successor\" — a vertex closer to the tree root — each time. By following successors computed for other vertices, the traversal down each path can be doubled every iteration, which means that the tree roots can be found in logarithmic time.\n\nPointer doubling operates on an array successor with an entry for every vertex in the graph. Each successor[\"i\"] is initialized with the parent index of vertex \"i\" if that vertex is not a root or to \"i\" itself if that vertex is a root. At each iteration, each successor is updated to its successor's successor. The root is found when the successor's successor points to itself.\n\nThe following pseudocode demonstrates the algorithm.\n\nThe following image provides an example of using pointer jumping on a small forest. On each iteration the successor points to the vertex following one more successor. After two iterations, every vertex points to its root node.\n"}
{"id": "23944162", "url": "https://en.wikipedia.org/wiki?curid=23944162", "title": "Press–Schechter formalism", "text": "Press–Schechter formalism\n\nThe Press–Schechter formalism is a mathematical model for predicting the number of objects (such as galaxies or galaxy clusters) of a certain mass within a given volume of the Universe. It was described in a famous paper by William H. Press and Paul Schechter in 1974.\n\nIn the context of cold dark matter cosmological models,\nperturbations on all scales are imprinted on the universe at very early times,\nfor example by quantum fluctuations during an inflationary era.\nLater, as radiation redshifts away, these become mass perturbations, and they\nstart to grow linearly. Only long after that, starting with small mass scales\nand advancing over time to larger mass scales, do the perturbations actually\ncollapse to form (for example) galaxies or clusters of galaxies, in so-called\nhierarchical structure formation (see Physical cosmology).\n\nPress and Schechter observed that the fraction of mass in collapsed objects\nmore massive than some mass M is related to the fraction of volume samples\nin which the smoothed initial density fluctuations are above some\ndensity threshold. This yields a formula for the mass function (distribution\nof masses) of objects at any given time.\n\nThe Press–Schechter formalism predicts that the number of objects with mass between formula_1 and formula_2 is:\n\nwhere formula_4 is the mean (baryonic and dark) matter density of the universe, formula_5 is the index of the power spectrum of the fluctuations in the early universe formula_6, and formula_7 is a critical mass above which structures will form.\n\nQualitatively, the prediction is that the mass distribution is a power law for\nsmall masses, with an exponential cutoff above some characteristic mass that\nincreases with time. Such functions had previously been noted by Schechter\nas observed luminosity functions,\nand are now known as Schechter luminosity functions. The Press-Schechter\nformalism provided the first quantitative model for how such functions might\narise.\n"}
{"id": "39976047", "url": "https://en.wikipedia.org/wiki?curid=39976047", "title": "Rodica Simion", "text": "Rodica Simion\n\nRodica Eugenia Simion (January 18, 1955 – January 7, 2000) was a Romanian-American mathematician. She was the Columbian School Professor of Mathematics at George Washington University. Her research concerned combinatorics: she was a pioneer in the study of permutation patterns, and an expert on noncrossing partitions.\n\nSimion was one of the top competitors in the Romanian national mathematical olympiads. She graduated from the University of Bucharest in 1974, and immigrated to the United States in 1976. She did her graduate studies at the University of Pennsylvania, earning a Ph.D. in 1981 under the supervision of Herbert Wilf. After teaching at Southern Illinois University and Bryn Mawr College, she moved to George Washington University in 1987, and became Columbian School Professor in 1997.\n\nSimion's thesis research concerned the concavity and unimodality of certain combinatorially defined sequences, and included what Richard P. Stanley calls \"a very influential result\" that the zeros of certain polynomials are all real.\n\nNext, with Frank Schmidt, she was one of the first to study the combinatorics of sets of permutations defined by forbidden patterns; she found a bijective proof that the stack-sortable permutations and the permutations formed by interleaving two monotonic sequences are equinumerous, and found combinatorial enumerations of many permutation classes. The \"simsun permutations\" were named after her and Sheila Sundaram, after their initial studies of these objects; a simsun permutation is a permutation in which, for all \"k\", the subsequence of the smallest \"k\" elements has no three consecutive elements in decreasing order.\n\nSimion also did extensive research on noncrossing partitions, and became \"perhaps the world's leading authority\" on them.\n\nSimion was the main organizer of an exhibit about mathematics, \"Beyond Numbers\", at the Maryland Science Center, based in part on her earlier experience organizing a similar exhibit at George Washington University. She was also a leader in George Washington University's annual Summer Program for Women in Mathematics.\nAs well as being a mathematician, Simion was a poet and painter; her poem \"Immigrant Complex\" was published in a collection of mathematical poetry in 1979.\n\n"}
{"id": "31855224", "url": "https://en.wikipedia.org/wiki?curid=31855224", "title": "Rodion Kuzmin", "text": "Rodion Kuzmin\n\nRodion Osievich Kuzmin (, Nov. 9, 1891, Riabye village in the Haradok district – March 24, 1949, Leningrad) was a Russian mathematician, known for his works in number theory and analysis. His name is sometimes transliterated as Kusmin. He was an Invited Speaker of the ICM in 1928 in Bologna.\n\n\n\n\n"}
{"id": "53404828", "url": "https://en.wikipedia.org/wiki?curid=53404828", "title": "Stechkin's lemma", "text": "Stechkin's lemma\n\nIn mathematics — more specifically, in functional analysis and numerical analysis — Stechkin's lemma is a result about the ℓ norm of the tail of a sequence, when the whole sequence is known to have finite ℓ norm. Here, the term “tail” means those terms in the sequence that are not among the \"N\" largest terms, for an arbitrary natural number \"N\". Stechkin's lemma is often useful when analysing best-\"N\"-term approximations to functions in a given basis of a function space. The result was originally proved by Stechkin in the case formula_1.\n\nLet formula_2 and let formula_3 be a countable index set. Let formula_4 be any sequence indexed by formula_3, and for formula_6 let formula_7 be the indices of the formula_8 largest terms of the sequence formula_4 in absolute value. Then\n\nwhere\n\nThus, Stechkin's lemma controls the ℓ norm of the tail of the sequence formula_4 (and hence the ℓ norm of the difference between the sequence and its approximation using its formula_8 largest terms) in terms of the ℓ norm of the full sequence and an rate of decay.\n\n"}
{"id": "2486019", "url": "https://en.wikipedia.org/wiki?curid=2486019", "title": "Stochastic drift", "text": "Stochastic drift\n\nIn probability theory, stochastic drift is the change of the average value of a stochastic (random) process. A related concept is the drift rate, which is the rate at which the average changes. For example, a process that counts the number of heads in a series of formula_1 fair coin tosses has a drift rate of 1/2 per toss. This is in contrast to the random fluctuations about this average value. The stochastic mean of that coin-toss process is 1/2 and the drift rate of the stochastic mean is 0, assuming 1=heads and 0=tails.\n\nLongitudinal studies of secular events are frequently conceptualized as consisting of a trend component fitted by a polynomial, a cyclical component often fitted by an analysis based on autocorrelations or on a Fourier series, and a random component (stochastic drift) to be removed.\n\nIn the course of the time series analysis, identification of cyclical and stochastic drift components is often attempted by alternating autocorrelation analysis and differencing of the trend. Autocorrelation analysis helps to identify the correct phase of the fitted model while the successive differencing transforms the stochastic drift component into white noise.\n\nStochastic drift can also occur in population genetics where it is known as genetic drift. A \"finite\" population of randomly reproducing organisms would experience changes from generation to generation in the frequencies of the different genotypes. This may lead to the fixation of one of the genotypes, and even the emergence of a new species. In sufficiently small populations, drift can also neutralize the effect of deterministic natural selection on the population.\n\nTime series variables in economics and finance — for example, stock prices, gross domestic product, etc. — generally evolve stochastically and frequently are non-stationary. They are typically modelled as either trend stationary or difference stationary. A trend stationary process {\"y\"} evolves according to\n\nwhere \"t\" is time, \"f\" is a deterministic function, and \"e\" is a zero-long-run-mean stationary random variable. In this case the stochastic term is stationary and hence there is no stochastic drift, though the time series itself may drift with no fixed long-run mean due to the deterministic component \"f\"(\"t\") not having a fixed long-run mean. This non-stochastic drift can be removed from the data by regressing formula_3 on formula_4 using a functional form coinciding with that of \"f\", and retaining the stationary residuals. In contrast, a unit root (difference stationary) process evolves according to \n\nwhere formula_6 is a zero-long-run-mean stationary random variable; here \"c\" is a non-stochastic drift parameter: even in the absence of the random shocks \"u\", the mean of \"y\" would change by \"c\" per period. In this case the non-stationarity can be removed from the data by first differencing, and the differenced variable formula_7 will have a long-run mean of \"c\" and hence no drift. But even in the absence of the parameter \"c\" (that is, even if \"c\"=0), this unit root process exhibits drift, and specifically stochastic drift, due to the presence of the stationary random shocks \"u\": a once-occurring non-zero value of \"u\" is incorporated into the same period's \"y\", which one period later becomes the one-period-lagged value of \"y\" and hence affects the new period's \"y\" value, which itself in the next period becomes the lagged \"y\" and affects the next \"y\" value, and so forth forever. So after the initial shock hits \"y\", its value is incorporated forever into the mean of \"y\", so we have stochastic drift. Again this drift can be removed by first differencing \"y\" to obtain \"z\" which does not drift.\n\nIn the context of monetary policy, one policy question is whether a central bank should attempt to achieve a fixed growth rate of the price level from its current level in each time period, or whether to target a return of the price level to a predetermined growth path. In the latter case no price level drift is allowed away from the predetermined path, while in the former case any stochastic change to the price level permanently affects the expected values of the price level at each time along its future path. In either case the price level has drift in the sense of a rising expected value, but the cases differ according to the type of non-stationarity: difference stationarity in the former case, but trend stationarity in the latter case.\n\n\n"}
{"id": "22826039", "url": "https://en.wikipedia.org/wiki?curid=22826039", "title": "Tibor Szele", "text": "Tibor Szele\n\nTibor Szele (Debrecen, 21 June 1918 – Szeged, 5 April 1955) Hungarian mathematician, working in combinatorics and abstract algebra. After graduating at the Debrecen University, he became a researcher at the Szeged University in 1946, then he went back at the Debrecen University in 1948 where he became full professor in 1952. He worked especially in the theory of Abelian groups and ring theory. He generalized Hajós's theorem. He founded the Hungarian school of algebra. Tibor Szele received the Kossuth Prize in 1952.\n\nA panorama of Hungarian Mathematics in the Twentieth Century, p. 601.\n\n"}
{"id": "29529119", "url": "https://en.wikipedia.org/wiki?curid=29529119", "title": "Time-weighted return", "text": "Time-weighted return\n\nThe time-weighted return (TWR) is a method of calculating investment return. To apply the time-weighted return method, combine the returns over sub-periods, by compounding them together, resulting in the overall period return. The rate of return over each different sub-period is weighted according to the duration of the sub-period.\n\nThe time-weighted method differs from other methods of calculating investment return only in the particular way it compensates for external flows - see below.\n\nThe time-weighted return is a measure of the historical performance of an investment portfolio which compensates for \"external flows\". External flows are net movements of value which result from transfers of cash, securities or other instruments, into or out of the portfolio, with no simultaneous equal and opposite movement of value in the opposite direction, as in the case of a purchase or sale, and which are not income from the investments in the portfolio, such as interest, coupons or dividends.\n\nTo compensate for external flows, the overall time interval under analysis is divided into contiguous sub-periods at each point in time within the overall time period whenever there is an external flow. In general, these sub-periods will be of unequal lengths. The returns over the sub-periods between external flows are linked geometrically (compounded) together, i.e. by multiplying together the growth factors in all the sub-periods. (The growth factor in each sub-period is equal to 1 plus the return over the sub-period.)\n\nTo illustrate the problem of external flows, consider the following example.\n\nSuppose an investor transfers $500 into a portfolio at the beginning of Year 1, and another $1,000 at the beginning of Year 2, and the portfolio has a total value of $1,500 at the end of the Year 2. The net gain over the two-year period is zero, so intuitively, we might expect that the return over the whole 2-year period to be 0%. If the cash flow of $1,000 at the beginning of Year 2 is ignored, then the simple method of calculating the return without compensating for the flow will be 200% ($1,000 divided by $500). Intuitively, 200% is incorrect.\n\nIf we add further information however, a different picture emerges. If the initial investment gained 100% in value over the first year, but the portfolio then declined by 25% during the second year, we would expect the overall return over the two-year period to be the result of compounding a 100% gain ($500) with a 25% loss (also $500). The time-weighted return is found by multiplying together the growth factors for each year, i.e. the growth factors before and after the second transfer into the portfolio, then subtracting one, and expressing the result as a percentage:\n\nWe can see from the time-weighted return that the absence of any net gain over the two-year period was due to bad timing of the cash inflow at the beginning of the second year.\n\nThe time-weighted return appears in this example to overstate the return to the investor, because he sees no net gain. However, by reflecting the performance each year compounded together on an equalized basis, the time-weighted return recognizes the performance of the investment activity independently of the poor timing of the cash flow at the beginning of Year 2. If all the money had been invested at the beginning of Year 1, the return by any measure would most likely have been 50%. $1,500 would have grown by 100% to $3,000 at the end of Year 1, and then declined by 25% to $2,250 at the end of Year 2, resulting in an overall gain of $750, i.e. 50% of $1,500.\n\nMeasuring the performance of a portfolio in the absence of flows is trivial:\n\nwhere formula_3 is the portfolio's final value, formula_4 is the portfolio's initial value, and formula_5 is the portfolio's return over the period.\n\nThe growth factor is:\n\nExternal flows during the period being analyzed complicate the performance calculation. If external flows are not taken into account, the performance measurement is distorted: a flow into the portfolio would cause this method to overstate the true performance, while flows out of the portfolio would cause it to understate the true performance.\n\nTo compensate for an external flow formula_7 into the portfolio at the \"beginning\" of the period, adjust the portfolio's initial value formula_4 by adding formula_7. The return is:\n\nand the corresponding growth factor is:\n\nTo compensate for an external flow formula_12 into the portfolio just before the valuation formula_3 at the \"end\" of the period, adjust the portfolio's final value formula_3 by subtracting formula_12. The return is:\n\nand the corresponding growth factor is:\n\nSuppose that the portfolio is valued immediately after each external flow. The value of the portfolio at the end of each sub-period is adjusted for the external flow which takes place immediately before. External flows into the portfolio are considered positive and flows out of the portfolio are negative.\n\nwhere\n\nand\n\nNote that if there is an external flow occurring at the end of the overall period, then the number of sub-periods formula_27 matches the number of flows. However, if there is no flow at the end of the overall period, then formula_29 is zero, and the number of sub-periods formula_27 is one greater than the number of flows.\n\nNote also that if the portfolio is valued immediately before each flow instead of immediately after, then each flow should be used to adjust the starting value within each sub-period, instead of the ending value, resulting in a different formula:\n\nwhere\n\nand\n\nThe term \"time-weighted\" is best illustrated with \"continuous (logarithmic) rates of return\". The overall rate of return is the time-weighted average of the continuous rate of return in each sub-period.\n\nIn the absence of flows,\n\nwhere formula_5 is the \"continuous rate of return\" and formula_22 is the length of time.\n\nOver a period of a decade, a portfolio grows by a continuous rate of return of 5% p.a. over three of those years, and 10% p.a. over the other seven years.\n\nThe continuous time-weighted rate of return over the ten-year period is the time-weighted average:\n\nConsider another example to calculate the annualized ordinary rate of return over a five-year period of an investment which returns 10% p.a. for two of the five years, and -3% p.a. for the other three. The ordinary time-weighted return over the five-year period is:\n\nand after annualization, the rate of return is:\n\nThe length of time over which the rate of return was 10% was two years, which appears in the power of two on the 1.1 factor:\n\nLikewise, the rate of return was -3% for three years, which appears in the power of three on the 0.97 factor. The result is then annualized over the overall five-year period.\n\nInvestment managers are judged on investment activity which is under their control. If they have no control over the timing of flows, then compensating for the timing of flows, applying the true time-weighted return method to a portfolio, is a superior measure of the performance of the investment manager, at the overall portfolio level.\n\n\"Internal flows\" are transactions such as purchases and sales of holdings within a portfolio, in which the cash used for purchases, and the cash proceeds of sales, is also contained in the same portfolio, so there is no external flow. A cash dividend on a stock in a portfolio, which is retained in the same portfolio as the stock, is a flow from the stock to the cash account within the portfolio. It is internal to the portfolio, but external to both the stock and the cash account when they are considered individually, in isolation from one another.\n\nThe time-weighted method only captures the effect attributable to the size and timing of internal flows in aggregate, i.e. insofar as they result in the overall performance of the portfolio. This is for the same reason, which is the time-weighted method neutralizes the effect of flows. It therefore does not capture the performance of parts of a portfolio, such as the performance due to individual security-level decisions, so effectively as it captures the overall portfolio performance.\n\nThe time-weighted return of a particular security, from initial purchase to eventual final sale, is the same, regardless of the presence or absence of interim purchases and sales, their timing, size and the prevailing market conditions. It always matches the share price performance (including dividends, etc.). Unless this feature of the time-weighted return is the desired objective, it arguably makes the time weighted method less informative than alternative methodologies for investment performance attribution at the level of individual instruments. For performance attribution at individual security level to be meaningful in many cases depends on the return being different from the share price return. If the individual security return matches the share price return, the transaction timing effect is zero.\n\nSee Example 4 below, which illustrates this feature of the time-weighted method.\n\nLet us imagine an investor purchases 10 shares at 10 dollars per share. Then the investor adds another 5 shares in the same company bought at the market price of 12 dollars per share (ignoring transaction costs). The entire holding of 15 shares is then sold at 11 dollars per share.\n\nThe second purchase appears to be badly timed, compared with the first. Is this poor timing apparent, from the time-weighted (holding-period) return of the shares, in isolation from the cash in the portfolio?\n\nTo calculate the time-weighted return of these particular shareholdings, in isolation from the cash used to purchase the shares, treat the purchase of shares as an external inflow. Then the first sub-period growth factor, preceding the second purchase, when there are just the first 10 shares, is:\n\nand growth factor over the second sub-period, following the second purchase, when there are 15 shares altogether, is:\n\nso the overall period growth factor is:\n\nand the time-weighted holding-period return is:\n\nwhich is the same as the simple return calculated using the change in the share price:\n\nThe poor timing of the second purchase has made no difference to the performance of the investment in shares, calculated using the time-weighted method, compared for instance with a pure buy-and-hold strategy (i.e. buying all the shares at the beginning, and holding them until the end of the period).\n\nOther methods exist to compensate for external flows when calculating investment returns. Such methods are known as \"money-weighted\" or \"dollar-weighted\" methods. The time-weighted return is higher than the result of other methods of calculating the investment return when external flows are badly timed - refer to Example 4 above.\n\nOne of these methods is the internal rate of return. Like the true time-weighted return method, the internal rate of return is also based on a compounding principle. It is the discount rate that will set the net present value of all external flows and the terminal value equal to the value of the initial investment. However, solving the equation to find an estimate of the internal rate of return generally requires an iterative numerical method and sometimes returns multiple results.\n\nThe internal rate of return is commonly used for measuring the performance of private equity investments, because the principal partner (the investment manager) has greater control over the timing of cash flows, rather than the limited partner (the end investor).\n\nThe Simple Dietz method applies a simple rate of interest principle, as opposed to the compounding principle underlying the internal rate of return method, and further assumes that flows occur at the midpoint within the time interval (or equivalently that they are distributed evenly throughout the time interval). However, the Simple Dietz method is unsuitable when such assumptions are invalid, and will produce different results to other methods in such a case.\n\nThe simple Dietz returns of two or more different constituent assets in a portfolio over the same period can be combined together to derive the simple Dietz portfolio return, by taking the weighted average. The weights are the start value plus half the net inflow.\n\nApplying the Simple Dietz method to the shares purchased in Example 4 (above):\n\nso\n\nwhich is noticeably lower than the 10% time-weighted return.\n\nThe Modified Dietz method is another method which, like the Simple Dietz method, applies a simple rate of interest principle. Instead of comparing the gain in value (net of flows) with the initial value of the portfolio, it compares the net gain in value with average capital over the time interval. Average capital allows for the timing of each external flow. As the difference between the Modified Dietz method and the internal rate of return method is that the Modified Dietz method is based on a simple rate of interest principle, whereas the internal rate of return method applies a compounding principle, the two methods produce similar results over short time intervals, if the rates of return are low. Over longer time periods, with significant flows relative to the size of the portfolio, and where the returns are not low, then the differences are more significant.\n\nLike the simple Dietz method, the Modified Dietz returns of two or more different constituent assets in a portfolio over the same period can be combined together to derive the Modified Dietz portfolio return, by taking the weighted average. The weight to be applied to the return on each asset in this case is the average capital of the asset.\n\nReferring again to the scenario described in Examples 4 and 5, if the second purchase occurs exactly halfway through the overall period, the Modified Dietz method has the same result as the Simple Dietz method.\n\nIf the second purchase is earlier than halfway through the overall period, the gain, which is 5 dollars, is still the same, but the average capital is greater than the start value plus half the net inflow, making the denominator of the Modified Dietz return greater than that in the Simple Dietz method. In this case, the Modified Dietz return is less than the Simple Dietz return.\n\nIf the second purchase is later than halfway through the overall period, the gain, which is 5 dollars, is still the same, but the average capital is less than the start value plus half the net inflow, making the denominator of the Modified Dietz return less than that in the Simple Dietz method. In this case, the Modified Dietz return is greater than the Simple Dietz return.\n\nNo matter how late during the period the second purchase of shares occurs, the average capital is greater than 100, and so the Modified Dietz return is less than 5 percent. This is still noticeably less than the 10 percent time weighted return.\n\nCalculating the \"true time-weighted return\" depends on the availability of portfolio valuations during the investment period. If valuations are not available when each flow occurs, the time-weighted return can only be estimated by linking returns for contiguous sub-periods together geometrically, using sub-periods at the end of which valuations are available. Such an approximate time-weighted return method is prone to overstate or understate the true time-weighted return.\n\n\"Linked Internal Rate of Return (LIROR)\" is another such method which is sometimes used to approximate the true time-weighted return. It combines the true time-weighted rate of return method with the internal rate of return (IRR) method. The internal rate of return is estimated over regular time intervals, and then the results are linked geometrically. For example, if the internal rate of return over successive years is 4%, 9%, 5% and 11%, then the LIROR equals 1.04 x 1.09 x 1.05 x 1.11 – 1 = 32.12%. If the regular time periods are not years, then either calculate the un-annualized holding period version of the IRR for each time interval, or calculate the IRR for each time interval firstly, and then convert each one to a holding period return over the time interval, then link together these holding period returns to obtain the LIROR.\n\nIf there are no external flows, then all these methods (time-weighted return, internal rate of return, Modified Dietz Method etc.) give identical results - it is only the various ways they handle flows which makes them different from each other.\n\nThe \"continuous\" or \"logarithmic return\" method is not a competing method of compensating for flows. It is simply the natural logarithm formula_71 of the growth factor.\n\nTo measure returns net of fees, allow the value of the portfolio to be reduced by the amount of the fees. To calculate returns gross of fees, compensate for them by treating them as an external flow, and exclude the negative effect of accrued fees from valuations.\n\nAny confusion over the meaning of the term \"return\" or \"rate of return\" should be avoided. The return calculated by these methods is the return per dollar (or per some other unit of currency), not per year (or other unit of time). Annualization, which means conversion to an annual rate of return, is a separate process. Refer to the article rate of return.\n\n\n"}
{"id": "21137319", "url": "https://en.wikipedia.org/wiki?curid=21137319", "title": "Timed event system", "text": "Timed event system\n\nThe General System has been described in [Zeigler76] and [ZPK00] with the stand points to define (1) the time base, (2) the admissible input segments, (3) the system states, (4) the state trajectory with an admissible input segment, (5) the output for an given state.\n\nA Timed Event System defining the state trajectory associated with the current and event segments came from the class of General System to allows non-deterministic behaviors in it [Hwang2012]. Since the behaviors of DEVS can be described by Timed Event System, DEVS and RTDEVS is a sub-class or an equivalent class of Timed Event System.\n\nA timed event system is a structure \nwhere\n\nGiven a timed event system formula_19, \"the set of its behaviors\" is called its \"language\" depending on the\nobservation time length. Let formula_20 be the observation time length.\nIf formula_21, \"formula_20-length observation language of\"\nformula_23 is denoted by formula_24, and defined as\nformula_25 \nWe call an event segment formula_26 a formula_20-length behavior of formula_28, if formula_29.\n\nBy sending the observation time length formula_20 to infinity, we define \"infinite length observation language of\" formula_23\nis denoted by formula_32, and defined as\nformula_33 \nWe call an event segment formula_34 an infinite-length behavior of formula_28, if formula_36.\n\nState Transition System\n\n"}
{"id": "268475", "url": "https://en.wikipedia.org/wiki?curid=268475", "title": "Two New Sciences", "text": "Two New Sciences\n\nThe Discourses and Mathematical Demonstrations Relating to Two New Sciences (, ), published in 1638 was Galileo's final book and a scientific testament covering much of his work in physics over the preceding thirty years.\n\nAfter his \"Dialogue Concerning the Two Chief World Systems\", the Roman Inquisition had banned the publication of any of Galileo's works, including any he might write in the future. After the failure of his initial attempts to publish \"Two New Sciences\" in France, Germany, and Poland, it was published by Lodewijk Elzevir who was working in Leiden, South Holland, where the writ of the Inquisition was of less consequence (see House of Elzevir). Fra Fulgenzio Micanzio, the official theologian of the Republic of Venice, had initially offered to help Galileo publish in Venice the new work, but he pointed out that publishing the \"Two New Sciences\" in Venice might cause Galileo unnecessary trouble; thus, the book was eventually published in Holland. Galileo did not seem to suffer any harm from the Inquisition for publishing this book since in January 1639, the book reached Rome's bookstores, and all available copies (about fifty) were quickly sold.\n\n\"Discourses\" was written in a style similar to \"Dialogues\", in which three men (Simplicio, Sagredo, and Salviati) discuss and debate the various questions Galileo is seeking to answer. There is a notable change in the men, however; Simplicio, in particular, is no longer quite as simple-minded, stubborn and Aristotelian as his name implies. His arguments are representative of Galileo's own early beliefs, as Sagredo represents his middle period, and Salviati proposes Galileo's newest models.\n\nThe book is divided into four days, each addressing different areas of physics. Galileo dedicates \"Two New Sciences\" to Lord Count of Noailles.\n\nIn the First Day, Galileo addressed topics that were discussed in Aristotle's Physics and also the Aristotelian school Mechanics. It also provides an introduction to the discussion of both of the new sciences. The likeness between the topics discussed, specific questions that are hypothesized, and the style and sources throughout give Galileo the backbone to his First Day. The First Day introduces the speakers in the dialogue: Salviati, Sagredo, and Simplicio, the same as in the Dialogue. These three people are all Galileo just at different stages of his life, Simplicio the youngest and Salviati, Galileo's closest counterpart. It also provides an introduction to the discussion of both of the new sciences. The second day addresses the question of the strength of materials.\n\nThe Third and Fourth days address the science of motion. The Third day discusses uniform and naturally accelerated motion, the issue of terminal velocity having been addressed in the First day. The Fourth day discusses projectile motion.\n\nIn \"Two Sciences\" uniform motion is defined as a motion that, over \"any\" equal periods of time, covers equal distance. With the use of the quantifier ″any″, uniformity is introduced and expressed more explicitly than in previous definitions.\n\nPage numbers at the start of each paragraph are from the 1898 standard Italian version and are found in the Crew and Drake translations.\n\n[50] Preliminary discussions. \nSagredo (taken to be the younger Galileo) cannot understand why with machines one cannot argue from the small to the large: \"I do not see that the properties of circles, triangles and...solid figures should change with their size\". Salviati (speaking for Galileo) says the common opinion is wrong. Scale matters: a horse falling from a height of 3 or 4 cubits will break its bones whereas a cat falling from twice the height won't, nor will a grasshopper falling from a tower.\n\n[56] The first example is a hemp rope which is constructed from small fibres which bind together in the same way as a rope round a windlass to produce something much stronger. Then the vacuum that prevents two highly polished plates from separating even though they slide easily gives rise to an experiment to test whether water can be expanded or whether a vacuum is caused. In fact, Sagredo had observed that a suction pump could not lift more than 18 cubits of water and Salviati observes that the weight of this is the amount of resistance to a void. The discussion turns to the strength of a copper wire and whether there are minute void spaces inside the metal or whether there is some other explanation for its strength.\n\n[68] This leads into a discussion of infinites and the continuum and thence to the observation that the number of squares equal the number of roots. He comes eventually to the view that \"if any number can be said to be infinite, it must be unity\" and demonstrates a construction in which an infinite circle is approached and another to divide a line.\n\n[85] The difference between a fine dust and a liquid leads to a discussion of light and how the concentrated power of the sun can melt metals. He deduces that light has motion and describes an (unsuccessful) attempt to measure its speed.\n\n[106] Aristotle believed that bodies fell at a speed proportional to weight but Salviati doubts that Aristotle ever tested this. He also did not believe that motion in a void was possible, but since air is much less dense than water Salviati asserts that in a medium devoid of resistance (a vacuum) all bodies—a lock of wool or a bit of lead—would fall at the same speed. Large and small bodies fall at the same speed through air or water providing they are of the same density. Since ebony weighs a thousand times as much as air (which he had measured), it will fall only a very little more slowly than lead which weighs ten times as much. But shape also matters—even a piece of gold leaf (the heaviest of metals) floats through the air and a bladder filled with air falls much more slowly than lead.\n\n[128] Measuring the speed of a fall is difficult because of the small time intervals involved and his first way round this used pendulums of the same length but with lead or cork weights. The period of oscillation was the same, even when the cork was swung more widely to compensate for the fact that it soon stopped.\n\n[139] This leads to a discussion of the vibration of strings and he suggests that not only the length of the string is important for pitch but also the tension and the weight of the string.\n\n[151] Salviati proves that a balance can not only be used with equal arms but with unequal arms with weights inversely proportional to the distances from the fulcrum. Following this he shows that the moment of a weight suspended by a beam supported at one end is proportional to the square of the length. The resistance to fracture of beams of various sizes and thicknesses is demonstrated, supported at one or both ends.\n\n[169] He shows that animal bones have to be proportionately larger for larger animals and the length of a cylinder that will break under its own weight. He proves that the best place to break a stick placed upon the knee is the middle and shows how far along a beam that a larger weight can be placed without breaking it.\n\n[178] He proves that the optimum shape for a beam supported at one end and bearing a load at the other is parabolic. He also shows that hollow cylinders are stronger than solid ones of the same weight.\n\n[191] He first defines uniform (steady) motion and shows the relationship between speed, time and distance. He then defines uniformly accelerated motion where the speed increases by the same amount in increments of time. Falling bodies start very slowly and he sets out to show that their velocity increases in simple proportionality to time, not to distance which he shows is impossible.\n\n[208] He shows that the distance travelled in naturally accelerated motion is proportional to the square of the time. He describes an experiment in which a steel ball was rolled down a groove in a piece of wooden moulding 12 cubits long (about 5.5m) with one end raised by one or two cubits. This was repeated, measuring times by accurately weighing the amount of water that came out of a thin pipe in a jet from the bottom of a large jug of water. By this means he was able to verify the uniformly accelerated motion. He then shows that whatever the inclination of the plane, the square of the time taken to fall a given vertical height is proportional to the inclined distance.\n\n[221] He next considers descent along the chords of a circle, showing that the time is the same as that falling from the vertex, and various other combinations of planes. He gives an erroneous solution to the brachistochrone problem, claiming to prove that the arc of the circle is the fastest descent. 16 problems with solutions are given.\n\n[268] The motion of projectiles consists of a combination of uniform horizontal motion and a naturally accelerated vertical motion which produces a parabolic curve. Two motions at right angles can be calculated using the sum of the squares. He shows in detail how to construct the parabolas in various situations and gives tables for altitude and range depending on the projected angle.\n\n[274] Air resistance shows itself in two ways: by affecting less dense bodies more and by offering greater resistance to faster bodies. A lead ball will fall slightly faster than an oak ball, but the difference with a stone ball is negligible. However the speed does not go on increasing indefinitely but reaches a maximum. Though at small speeds the effect of air resistance is small, it is greater when considering, say, a ball fired from a cannon.\n\n[292] The effect of a projectile hitting a target is reduced if the target is free to move. The velocity of a moving body can overcome that of a larger body if its speed is proportionately greater than the resistance.\n\n[310] A cord or chain stretched out is never level but also approximates to a parabola. (But see also catenary.)\n\nMany contemporary scientists, such as Gassendi, dispute Galileo’s methodology for conceptualizing his law of falling bodies. Two of the main arguments are that his epistemology followed the example of Platonist thought or hypothetico-deductivist. It has now been considered to be \"ex suppositione\", or knowing the how and why effects from past events in order to determine the requirements for the production of similar effects in the future. Galileo methodology mirrored that of Aristotelian and Archimedean epistemology. Following a letter from Cardinal Bellarmine in 1615 Galileo distinguished his arguments and Copernicus' as natural suppositions as opposed to the “fictive” that are “introduced only for the sake of astronomical computations,” such as Plato's hypothesis on eccentrics and equants.\n\nGalileo’s earlier writing considered Juvenilia, or youthful writings, are considered his first attempts at creating lecture notes for his course “hypothesis of the celestial motions” while teaching in at the University of Padua. These notes mirrored those of his contemporaries at the Collegio as well as contained an “Aristotelian context with decided Thomistic (St. Thomas Aquinas) overtones.” These earlier papers are believed to have encouraged him to apply demonstrative proof in order to give validity to his discoveries on motion.\n\nDiscovery of folio 116v gives evidence of experiments that had previously not been reported and therefore demonstrated Galileo’s actual calculations for the Law of Falling Bodies.\n\nHis methods of experimentation have been proved by the recording and recreation done by scientists such as James MacLachlan, Stillman Drake, R.H. Taylor and others in order to prove he did not merely imagine his ideas as historian Alexandre Koyré argued, but sought to prove them mathematically.\n\nGalileo believed that knowledge could be acquired through reason, and reinforced through observation and experimentation. Thus, it can be argued that Galileo was a rationalist, and also that he was an empiricist.\n\nThe two sciences mentioned in the title are the strength of materials and the motion of objects (the forbearers of modern material engineering and kinematics). In the title of the book \"mechanics\" and \"motion\" are separate, since at Galileo's time \"mechanics\" meant only statics and strength of materials.\n\nGalileo had started an additional section on the force of percussion, but was not able to complete it to his own satisfaction.\n\nThe discussion begins with a demonstration of the reasons that a large structure proportioned in exactly the same way as a smaller one must necessarily be weaker known as the square–cube law. Later in the discussion this principle is applied to the thickness required of the bones of a large animal, possibly the first quantitative result in biology, anticipating J. B. S. Haldane's work \"On Being the Right Size, and other essays,\" edited by John Maynard Smith.\n\nGalileo expresses clearly for the first time the constant acceleration of a falling body which he was able to measure accurately by slowing it down using an inclined plane.\n\nIn \"Two New Sciences\", Galileo (Salviati speaks for him) used a wood molding, \"12 cubits long, half a cubit wide and three finger-breadths thick\" as a ramp with a straight, smooth, polished groove to study rolling balls (\"a hard, smooth and very round bronze ball\"). He lined the groove with \"parchment, also smooth and polished as possible\". He inclined the ramp at various angles, effectively slowing down the acceleration enough so that he could measure the elapsed time. He would let the ball roll a known distance down the ramp, and use a water clock to measure the time taken to move the known distance. This clock was\na large vessel of water placed in an elevated position; to the bottom of this vessel was soldered a pipe of small diameter giving a thin jet of water, which we collected in a small glass during the time of each descent, whether for the whole length of the channel or for a part of its length. The water collected was weighed, and after each descent on a very accurate balance, the differences and ratios of these weights gave him the differences and ratios of the times. This was done with such accuracy that although the operation was repeated many, many times, there was no appreciable discrepancy in the results.\n\nWhile Aristotle had observed that heavier objects fall more quickly than lighter ones, in \"Two New Sciences\" Galileo postulated that this was due \"not\" to inherently stronger forces acting on the heavier objects, but to the countervailing forces of air resistance and friction. To compensate, he conducted experiments using a shallowly inclined ramp, smoothed so as to eliminate as much friction as possible, on which he rolled down balls of different weights. In this manner, he was able to provide empirical evidence that matter accelerates vertically downward at a constant rate, regardless of mass, due to the effects of gravity.\n\nThe unreported experiment found in folio 116V tested the constant rate of acceleration in falling bodies due to gravity. This experiment consisted of dropping a ball from specified heights onto a deflector in order to transfer its motion from vertical to horizontal. The data from the inclined plane experiments were used to calculate the expected horizontal motion. However, discrepancies were found in the results of the experiment: the observed horizontal distances disagreed with the calculated distances expected for a constant rate of acceleration. Galileo attributed the discrepancies to air resistance in the unreported experiment, and friction in the inclined plane experiment. These discrepancies forced Galileo to assert that the postulate held only under \"ideal conditions,\" i.e., in the absence of friction and/or air resistance.\n\nAristotelian physics argued that the Earth must not move as humans are unable to perceive the effects of this motion. A popular justification of this is the experiment of an archer shooting an arrow straight up into the air. If the Earth were moving, Aristotle argued, the arrow should fall in a different location than the launch point. Galileo refuted this argument in \"Two New Sciences\". He provided the example of sailors aboard a boat at sea. The boat is obviously in motion, but the sailors are unable to perceive this motion. If a sailor were to drop a weighted object from the mast, this object would fall at the base of the mast rather than behind it (due to the ship's forward motion). This was the result of simultaneously the horizontal and vertical motion of the ship, sailors, and ball.\n\nOne of Galileo’s experiments regarding falling bodies was that describing the relativity of motions, explaining that, under the right circumstances, “one motion may be superimposed upon another without effect upon either…”. In \"Two New Sciences\", Galileo made his case for this argument and it would become the basis of Newton's first law, the law of inertia.\n\nHe poses the question of what happens to a ball dropped from the mast of a sailing ship or an arrow fired into the air on the deck. According to Aristotle's physics, the ball dropped should land at the stern of the ship as it falls straight down from the point of origin. Likewise the arrow when fired straight up should not land in the same spot if the ship is in motion. Galileo offers that there are two independent motions at play. One is the accelerating vertical motion caused by gravity while the other is the uniform horizontal motion caused by the moving ship which continues to influence the trajectory of the ball through the principle of inertia. The combination of these two motions results in a parabolic curve. The observer cannot identify this parabolic curve because the ball and observer share the horizontal movement imparted to them by the ship, meaning only the perpendicular, vertical motion is perceivable. Surprisingly, nobody had tested this theory with the simple experiments needed to gain a conclusive result until Pierre Gassendi published the results of said experiments in his letters entitled \"De Motu Impresso a Motore Translato\" (1642).\n\nThe book also contains a discussion of infinity. Galileo considers the example of numbers and their squares. He starts by noting that:\nIt cannot be denied that there are as many [squares] as there are numbers because every number is a [square] root of some square:\n\n1 ↔ 1, 2 ↔ 4, 3 ↔ 9, 4 ↔ 16, and so on. \n\n(In modern language, there is a bijection between the elements of the set of positive integers N and the set of squares S, and S is a proper subset of density zero.) But he notes what appears to be a contradiction: \nYet at the outset we said there are many more numbers than squares, since the larger portion of them are not squares. Not only so, but the proportionate number of squares diminishes as we pass to larger numbers.\n\nHe resolves the contradiction by denying the possibility of comparing infinite numbers (and of comparing infinite and finite numbers): \nWe can only infer that the totality of all numbers is infinite, that the number of squares is infinite, and that the number of their roots is infinite; neither is the number of squares less than the totality of all numbers, nor the latter greater than the former; and finally the attributes \"equal,\" greater,\" and \"less,\" are not applicable to infinite, but only to finite, quantities.\nThis conclusion, that ascribing sizes to infinite sets should be ruled impossible, owing to the contradictory results obtained from these two ostensibly natural ways of attempting to do so, is a resolution to the problem that is consistent with, but less powerful than, the methods used in modern mathematics. The resolution to the problem may be generalized by considering Galileo's first definition of what it means for sets to have equal sizes, that is, the ability to put them in one-to-one correspondence. This turns out to yield a way of comparing the sizes of infinite sets that is free from contradictory results.\n\nThese issues of infinity arise from problems of rolling circles. If two concentric circles of different radii roll along lines, then if the larger does not slip it appears clear that the smaller must slip. But in what way? Galileo attempts to clarify the matter by considering hexagons and then extending to rolling 100 000-gons, or n-gons, where he shows that a finite number of finite slips occur on the inner shape. Eventually, he concludes \"the line traversed by the larger circle consists then of an infinite number of points which completely fill it; while that which is traced by the smaller circle consists of an infinite number of points which leave empty spaces and only partly fill the line,\" which would not be considered satisfactory now.\n\nPart of \"Two New Sciences\" was pure mathematics, as has been pointed out by the mathematician Alfréd Rényi, who argued that it was the most significant book on mathematics in over 2000 years: Greek mathematics did not deal with motion, and so they never formulated mathematical laws of motion, even though Archimedes developed differentiation and integration. \"Two New Sciences\" opened the way to treating physics mathematically by treating motion mathematically for the first time. The Greek mathematician Zeno had designed his paradoxes to prove that motion could not be treated mathematically, and that any attempt to do so would lead to paradoxes. (He regarded this as an inevitable limitation of mathematics.) Aristotle reinforced this belief, saying that mathematic could only deal with abstract objects that were immutable. Galileo used the very methods of the Greeks to show that motion could indeed be treated mathematically. His idea was to separate out the paradoxes of the infinite from Zeno's paradoxes. He did this in several steps. First, he showed that the infinite sequence S of the squares 1, 4, 9, 16, ...contained as many elements as the sequence N of all positive integers. See the Infinity section.This is now referred to as Galileo's paradox. Then, using Greek style geometry, he showed a short line interval contained as many points as a longer interval. At some point he formulates the general principle that a smaller infinite set can have just as many points as a larger infinite set containing it. It was then clear that Zeno's paradoxes on motion resulted entirely from this paradoxical behavior of infinite quantities. Having removed this 2000-year-old stumbling block, Galileo went on to introduce his mathematical laws of motion, anticipating Newton.\n\nPierre Gassendi defended Galileo's opinions in his book, \"De Motu Impresso a Motore Translato\". In Howard Jones' article, \"Gassendi's Defence of Galileo: The Politics of Discretion\", Jones says Gassendi displayed an understanding of Galileo's arguments and a clear grasp of their implications for the physical objections to the earth's motion.\n\nThe law of falling bodies was published by Galileo in 1638. But in the 20th century some authorities challenged the reality of Galileo's experiments. In particular, the French historian of science Alexandre Koyré bases his doubt on the fact that the experiments reported in \"Two New Sciences\" to determine the law of acceleration of falling bodies, required accurate measurements of time which appeared to be impossible with the technology of 1600. According to Koyré, the law was created deductively, and the experiments were merely illustrative thought experiments. In fact, Galileo's water clock (described above) provided sufficiently accurate measurements of time to confirm his conjectures.\n\nLater research, however, has validated the experiments. The experiments on falling bodies (actually rolling balls) were replicated using the methods described by Galileo, and the precision of the results was consistent with Galileo's report. Later research into Galileo's unpublished working papers from 1604 clearly showed the reality of the experiments and even indicated the particular results that led to the time-squared law.\n\n\n\n"}
{"id": "45200", "url": "https://en.wikipedia.org/wiki?curid=45200", "title": "Universal algebra", "text": "Universal algebra\n\nUniversal algebra (sometimes called general algebra) is the field of mathematics that studies algebraic structures themselves, not examples (\"models\") of algebraic structures.\nFor instance, rather than take particular groups as the object of study, in universal algebra one takes the class of groups as an object of study.\n\nIn universal algebra, an algebra (or algebraic structure) is a set \"A\" together with a collection of operations on \"A\". An \"n\"-ary operation on \"A\" is a function that takes \"n\" elements of \"A\" and returns a single element of \"A\". Thus, a 0-ary operation (or \"nullary operation\") can be represented simply as an element of \"A\", or a \"constant\", often denoted by a letter like \"a\". A 1-ary operation (or \"unary operation\") is simply a function from \"A\" to \"A\", often denoted by a symbol placed in front of its argument, like ~\"x\". A 2-ary operation (or \"binary operation\") is often denoted by a symbol placed between its arguments, like \"x\" ∗ \"y\". Operations of higher or unspecified \"arity\" are usually denoted by function symbols, with the arguments placed in parentheses and separated by commas, like \"f\"(\"x\",\"y\",\"z\") or \"f\"(\"x\"...,\"x\"). Some researchers allow infinitary operations, such as formula_1 where \"J\" is an infinite index set, thus leading into the algebraic theory of complete lattices. One way of talking about an algebra, then, is by referring to it as an algebra of a certain type formula_2, where formula_2 is an ordered sequence of natural numbers representing the arity of the operations of the algebra.\n\nIt is also possible to define an algebra via the relations in the algebra instead of the operations. Birkhoff's Theorem states that the two definitions are equivalent, i.e., there is a Galois connection between relational and operational structures. This connection can be easily illustrated on the case of lattices, where the algebraic structure can be given by the operations join and meet or by introducing a partial order relation. The relational point of view is useful in computational problems, in particular for the constraint satisfaction problem (CSP).\n\nAfter the operations have been specified, the nature of the algebra is further defined by axioms, which in universal algebra often take the form of identities, or equational laws. An example is the associative axiom for a binary operation, which is given by the equation \"x\" ∗ (\"y\" ∗ \"z\") = (\"x\" ∗ \"y\") ∗ \"z\". The axiom is intended to hold for all elements \"x\", \"y\", and \"z\" of the set \"A\".\n\nA collection of algebraic structures defined by identities is called a variety or equational class. Some authors consider varieties to be the main focus of universal algebra.\n\nRestricting one's study to varieties rules out:\n\n\nThe study of equational classes can be seen as a special branch of model theory, typically dealing with structures having operations only (i.e. the type can have symbols for functions but not for relations other than equality), and in which the language used to talk about these structures uses equations only.\n\nNot all algebraic structures in a wider sense fall into this scope. For example, ordered groups involve an ordering relation, so would not fall within this scope.\n\nThe class of fields is not an equational class because there is no type (or \"signature\") in which all field laws can be written as equations (inverses of elements are defined for all \"non-zero\" elements in a field, so inversion cannot be added to the type).\n\nOne advantage of this restriction is that the structures studied in universal algebra can be defined in any category that has \"finite products\". For example, a topological group is just a group in the category of topological spaces.\n\nMost of the usual algebraic systems of mathematics are examples of varieties, but not always in an obvious way – the usual definitions often involve quantification or inequalities.\n\nTo see how this works, let's consider the definition of a group. Normally a group is defined in terms of a single binary operation ∗, subject to these axioms:\n\n\nThis definition of a group is problematic from the point of view of universal algebra. The reason is that the axioms of the identity element and inversion are not stated purely in terms of equational laws but also have clauses involving the phrase \"there exists ... such that ...\". This is inconvenient; the list of group properties can be simplified to universally quantified equations by adding a nullary operation \"e\" and a unary operation ~ in addition to the binary operation ∗. Then list the axioms for these three operations as follows:\n\n(Of course, we usually write \"\"x\"\" instead of \"~\"x\"\", which shows that the notation for operations of low arity is not \"always\" as given in the second paragraph.)\n\nWhat has changed is that in the usual definition there are:\n\n...while in the universal algebra definition there are\n\nIt is important to check that this really does capture the definition of a group. The reason that it might not is that specifying one of these universal groups might give more information than specifying one of the usual kind of group. After all, nothing in the usual definition said that the identity element \"e\" was \"unique\"; if there is another identity element \"e\"', then it is ambiguous which one should be the value of the nullary operator \"e\". Proving that it is unique is a common beginning exercise in classical group theory textbooks. The same thing is true of inverse elements. So, the universal algebraist's definition of a group is equivalent to the usual definition.\n\nAt first glance this is simply a technical difference, replacing quantified laws with equational laws. However, it has immediate practical consequences – when defining a group object in category theory, where the object in question may not be a set, one must use equational laws (which make sense in general categories), and cannot use quantified laws (which do not make sense, as objects in general categories do not have elements). Further, the perspective of universal algebra insists not only that the inverse and identity exist, but that they be maps in the category. The basic example is a topological group – not only must the inverse exist element-wise, but the inverse map must be continuous (some authors also require the identity map to be a closed inclusion, hence cofibration, again referring to properties of the map).\n\nMost algebraic structures are examples of universal algebras.\n\n\nExamples of relational algebras include semilattices, lattices or Boolean algebras.\n\nWe assume that the type, formula_2, has been fixed. Then there are three basic constructions in universal algebra: homomorphic image, subalgebra, and product.\n\nA homomorphism between two algebras \"A\" and \"B\" is a function \"h\": \"A\" → \"B\" from the set A to the set B such that, for every operation \"f\" of A and corresponding \"f\" of B (of arity, say, \"n\"), \"h\"(\"f\"(\"x\"...,\"x\")) = \"f\"(\"h\"(\"x\")...,\"h\"(\"x\")). (Sometimes the subscripts on \"f\" are taken off when it is clear from context which algebra the function is from.) For example, if \"e\" is a constant (nullary operation), then \"h\"(\"e\") = \"e\". If ~ is a unary operation, then \"h\"(~\"x\") = ~\"h\"(\"x\"). If ∗ is a binary operation, then \"h\"(\"x\" ∗ \"y\") = \"h\"(\"x\") ∗ \"h\"(\"y\"). And so on. A few of the things that can be done with homomorphisms, as well as definitions of certain special kinds of homomorphisms, are listed under the entry Homomorphism. In particular, we can take the homomorphic image of an algebra, \"h\"(\"A\").\n\nA subalgebra of \"A\" is a subset of \"A\" that is closed under all the operations of \"A\". A product of some set of algebraic structures is the cartesian product of the sets with the operations defined coordinatewise.\n\n\nIn addition to its unifying approach, universal algebra also gives deep theorems and important examples and counterexamples. It provides a useful framework for those who intend to start the study of new classes of algebras.\nIt can enable the use of methods invented for some particular classes of algebras to other classes of algebras, by recasting the methods in terms of universal algebra (if possible), and then interpreting these as applied to other classes. It has also provided conceptual clarification; as J.D.H. Smith puts it, \"What looks messy and complicated in a particular framework may turn out to be simple and obvious in the proper general one.\"\n\nIn particular, universal algebra can be applied to the study of monoids, rings, and lattices. Before universal algebra came along, many theorems (most notably the isomorphism theorems) were proved separately in all of these classes, but with universal algebra, they can be proven once and for all for every kind of algebraic system.\n\nThe 1956 paper by Higgins referenced below has been well followed up for its framework for a range of particular algebraic systems, while his 1963 paper is notable for its discussion of algebras with operations which are only partially defined, typical examples for this being categories and groupoids. This leads on to the subject of higher-dimensional algebra which can be defined as the study of algebraic theories with partial operations whose domains are defined under geometric conditions. Notable examples of these are various forms of higher-dimensional categories and groupoids.\n\nUniversal algebra provides a natural language for the constraint satisfaction problem (CSP). CSP refers to an important class of computational problems where, given a relational algebra and an existential sentence formula_7 over this algebra, the question is to find out whether formula_7 can be satisfied in . The algebra is often fixed, so that refers to the problem whose instance is only the existential sentence formula_7.\n\nIt is proved that every computational problem can be formulated as for some algebra .\n\nFor example, the \"n\"-coloring problem can be stated as CSP of the algebra formula_10, i.e. an algebra with formula_11 elements and a single relation, inequality.\n\nThe dichotomy conjecture (proved in April 2017) states that if is a finite algebra, then is either P or NP-complete.\n\nUniversal algebra has also been studied using the techniques of category theory. In this approach, instead of writing a list of operations and equations obeyed by those operations, one can describe an algebraic structure using categories of a special sort, known as Lawvere theories or more generally algebraic theories. Alternatively, one can describe algebraic structures using monads. The two approaches are closely related, with each having their own advantages.\nIn particularly, every Lawvere theory gives a monad on the category of sets, while any 'finitary' monad on the category of sets arises from a Lawvere theory. However, a monad describes algebraic structures within one particular category (for example the category of sets), while algebraic theories describe structure within any of a large class of categories (namely those having finite products).\n\nA more recent development in category theory is operad theory – an operad is a set of operations, similar to a universal algebra, but restricted in that equations are only allowed between expressions with the variables, with no duplication or omission of variables allowed. Thus, rings can be described as the so-called 'algebras' of some operad, but not groups, since the law formula_12 duplicates the variable \"g\" on the left side and omits it on the right side. At first this may seem to be a troublesome restriction, but the payoff is that operads have certain advantages: for example, one can hybridize the concepts of ring and vector space to obtain the concept of associative algebra, but one cannot form a similar hybrid of the concepts of group and vector space.\n\nAnother development is partial algebra where the operators can be partial functions. Certain partial functions can also be handled by a generalization of Lawvere theories known as essentially algebraic theories.\n\nAnother generalization of universal algebra is model theory, which is sometimes described as \"universal algebra + logic\".\n\nIn Alfred North Whitehead's book \"A Treatise on Universal Algebra,\" published in 1898, the term \"universal algebra\" had essentially the same meaning that it has today. Whitehead credits William Rowan Hamilton and Augustus De Morgan as originators of the subject matter, and James Joseph Sylvester with coining the term itself.\n\nAt the time structures such as Lie algebras and hyperbolic quaternions drew attention to the need to expand algebraic structures beyond the associatively multiplicative class. In a review Alexander Macfarlane wrote: \"The main idea of the work is not unification of the several methods, nor generalization of ordinary algebra so as to include them, but rather the comparative study of their several structures.\" At the time George Boole's algebra of logic made a strong counterpoint to ordinary number algebra, so the term \"universal\" served to calm strained sensibilities.\n\nWhitehead's early work sought to unify quaternions (due to Hamilton), Grassmann's Ausdehnungslehre, and Boole's algebra of logic. Whitehead wrote in his book:\n\nWhitehead, however, had no results of a general nature. Work on the subject was minimal until the early 1930s, when Garrett Birkhoff and Øystein Ore began publishing on universal algebras. Developments in metamathematics and category theory in the 1940s and 1950s furthered the field, particularly the work of Abraham Robinson, Alfred Tarski, Andrzej Mostowski, and their students (Brainerd 1967).\n\nIn the period between 1935 and 1950, most papers were written along the lines suggested by Birkhoff's papers, dealing with free algebras, congruence and subalgebra lattices, and homomorphism theorems. Although the development of mathematical logic had made applications to algebra possible, they came about slowly; results published by Anatoly Maltsev in the 1940s went unnoticed because of the war. Tarski's lecture at the 1950 International Congress of Mathematicians in Cambridge ushered in a new period in which model-theoretic aspects were developed, mainly by Tarski himself, as well as C.C. Chang, Leon Henkin, Bjarni Jónsson, Roger Lyndon, and others.\n\nIn the late 1950s, Edward Marczewski emphasized the importance of free algebras, leading to the publication of more than 50 papers on the algebraic theory of free algebras by Marczewski himself, together with Jan Mycielski, Władysław Narkiewicz, Witold Nitka, J. Płonka, S. Świerczkowski, K. Urbanik, and others.\n\nStarting with William Lawvere's thesis in 1963, techniques from category theory have become important in universal algebra.\n\n\n\n"}
