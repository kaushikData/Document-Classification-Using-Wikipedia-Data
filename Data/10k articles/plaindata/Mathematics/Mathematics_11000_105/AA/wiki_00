{"id": "403320", "url": "https://en.wikipedia.org/wiki?curid=403320", "title": "64 (number)", "text": "64 (number)\n\n64 (sixty-four) is the natural number following 63 and preceding 65.\n\nSixty-four is the square of 8, the cube of 4, and the sixth power of 2. It is the smallest number with exactly seven divisors. It is the lowest positive power of two that is adjacent to neither a Mersenne prime nor a Fermat prime. 64 is the sum of Euler's totient function for the first fourteen integers. It is also a dodecagonal number and a centered triangular number. 64 is also the first whole number that is both a perfect square and a perfect cube.\n\nSince it is possible to find sequences of 64 consecutive integers such that each inner member shares a factor with either the first or the last member, 64 is an Erdős–Woods number.\n\nIn base 10, no integer added up to its own digits yields 64, hence it is a self number.\n\n64 is a superperfect number—a number such that σ(σ(\"n\")) = 2\"n\".\n\n64 is the index of Graham's number in the rapidly growing sequence 3↑↑↑↑3, 3 ↑ 3,…\n\n\n\n\nSixty-four is:\n\n\n"}
{"id": "21765779", "url": "https://en.wikipedia.org/wiki?curid=21765779", "title": "András Hajnal", "text": "András Hajnal\n\nAndrás Hajnal (May 13, 1931 – July 30, 2016) was a professor of mathematics at Rutgers University and a member of the Hungarian Academy of Sciences known for his work in set theory and combinatorics.\n\nHajnal was born on 13 May 1931, in Budapest, Hungary.\n\nHe received his university diploma (M.Sc. degree) in 1953 from the Eötvös Loránd University, his Candidate of Mathematical Science degree (roughly equivalent to Ph.D.) in 1957, under the supervision of László Kalmár, and his Doctor of Mathematical Science degree in 1962. From 1956 to 1995 he was a faculty member at the Eötvös Loránd University; in 1994, he moved to Rutgers University to become the director of DIMACS, and he remained there as a professor until his retirement in 2004. He became a member of the Hungarian Academy of Sciences in 1982, and directed its mathematical institute from 1982 to 1992. He was general secretary of the János Bolyai Mathematical Society from 1980 to 1990, and president of the society from 1990 to 1994. Since 1981, he has been an advisory editor of the journal Combinatorica. Hajnal was also one of the Honorary Presidents of the European Set Theory Society.\n\nHajnal was an avid chess player.\n\nHajnal was the father of Peter Hajnal, the co-dean of the European College of Liberal Arts.\n\nHajnal was the author of over 150 publications. Among the many co-authors of Paul Erdős, he had the second largest number of joint papers, 56.\nWith Peter Hamburger, he wrote a textbook, \"Set Theory\" (Cambridge University Press, 1999, ). Some of his more well-cited research papers include\nOther selected results include:\n\nIn 1992, Hajnal was awarded the Officer's Cross of the Order of the Republic of Hungary. In 1999, a conference in honor of his 70th birthday was held at DIMACS, and a second conference honoring the 70th birthdays of both Hajnal and Vera Sós was held in 2001 in Budapest. Hajnal became a fellow of the American Mathematical Society in 2012.\n\n"}
{"id": "54237654", "url": "https://en.wikipedia.org/wiki?curid=54237654", "title": "Australasian Journal of Combinatorics", "text": "Australasian Journal of Combinatorics\n\nThe Australasian Journal of Combinatorics is a triannual peer-reviewed open-access scientific journal covering combinatorics. It was established in 1990 and is published by the Centre for Discrete Mathematics and Computing (University of Queensland) on behalf of the Combinatorial Mathematics Society of Australasia. Originally published biannually, it has been published three times per year since 2005. The editor-in-chief is Elizabeth J. Billington (University of Queensland). Since 2014, the journal has been diamond open access, charging fees neither to readers nor to authors.\n\nThe journal is abstracted and indexed in \"Mathematical Reviews\", the Emerging Sources Citation Index, Scopus, and \"Zentralblatt MATH\".\n"}
{"id": "1510587", "url": "https://en.wikipedia.org/wiki?curid=1510587", "title": "Bayesian search theory", "text": "Bayesian search theory\n\nBayesian search theory is the application of Bayesian statistics to the search for lost objects. It has been used several times to find lost sea vessels, for example the USS \"Scorpion\". It also played a key role in the recovery of the flight recorders in the Air France Flight 447 disaster of 2009. Currently, it is being used to locate the remains of Malaysia Airlines Flight 370.\n\nThe usual procedure is as follows:\n\nIn other words, first search where it most probably will be found, then search where finding it is less probable, then search where the probability is even less (but still possible due to limitations on fuel, range, water currents, etc.), until insufficient hope of locating the object at acceptable cost remains.\n\nThe advantages of the Bayesian method are that all information available is used coherently (i.e., in a \"leak-proof\" manner) and the method automatically produces estimates of the cost for a given success probability. That is, even before the start of searching, one can say, hypothetically, \"there is a 65% chance of finding it in a 5-day search. That probability will rise to 90% after a 10-day search and 97% after 15 days\" or a similar statement. Thus the economic viability of the search can be estimated before committing resources to a search.\n\nApart from the USS \"Scorpion\", other vessels located by Bayesian search theory include the MV \"Derbyshire\", the largest British vessel ever lost at sea, and the SS \"Central America\". It also proved successful in the search for a lost hydrogen bomb following the 1966 Palomares B-52 crash in Spain, and the recovery in the Atlantic Ocean of the crashed Air France Flight 447.\n\nBayesian search theory is incorporated into the CASP (Computer Assisted Search Program) mission planning software used by the United States Coast Guard for search and rescue. This program was later adapted for inland search by adding terrain and ground cover factors for use by the United States Air Force and Civil Air Patrol.\n\nSuppose a grid square has a probability \"p\" of containing the wreck and that the probability of successfully detecting the wreck if it is there is \"q\". If the square is searched and no wreck is found, then, by Bayes' theorem, the revised probability of the wreck being in the square is given by\n\nFor every other grid square, if its prior probability is \"r\", its posterior probability is given by\n\nIn May 1968, the U.S. Navy's nuclear submarine USS \"Scorpion\" (SSN-589) failed to arrive as expected at her home port of Norfolk, Virginia. The command officers of the U.S. Navy were nearly convinced that the vessel had been lost off the Eastern Seaboard, but an extensive search there failed to discover the remains of \"Scorpion\".\n\nThen, a Navy deep-water expert, John P. Craven, suggested that \"Scorpion\" had sunk elsewhere. Craven organised a search southwest of the Azores based on a controversial approximate triangulation by hydrophones. He was allocated only a single ship, \"Mizar\", and he took advice from a firm of consultant mathematicians in order to maximise his resources. A Bayesian search methodology was adopted. Experienced submarine commanders were interviewed to construct hypotheses about what could have caused the loss of \"Scorpion\".\n\nThe sea area was divided up into grid squares and a probability assigned to each square, under each of the hypotheses, to give a number of probability grids, one for each hypothesis. These were then added together to produce an overall probability grid. The probability attached to each square was then the probability that the wreck was in that square. A second grid was constructed with probabilities that represented the probability of successfully finding the wreck if that square were to be searched and the wreck were to be actually there. This was a known function of water depth. The result of combining this grid with the previous grid is a grid which gives the probability of finding the wreck in each grid square of the sea if it were to be searched.\n\nAt the end of October 1968, the Navy's oceanographic research ship, , located sections of the hull of \"Scorpion\" on the seabed, about southwest of the Azores, under more than of water. This was after the Navy had released sound tapes from its underwater \"SOSUS\" listening system, which contained the sounds of the destruction of \"Scorpion\". The court of inquiry was subsequently reconvened and other vessels, including the bathyscaphe \"Trieste II\", were dispatched to the scene, collecting many pictures and other data.\n\nAlthough Craven received much credit for locating the wreckage of \"Scorpion\", Gordon Hamilton, an acoustics expert who pioneered the use of hydroacoustics to pinpoint Polaris missile splashdown locations, was instrumental in defining a compact \"search box\" wherein the wreck was ultimately found. Hamilton had established a listening station in the Canary Islands that obtained a clear signal of what some scientists believe was the noise of the vessel's pressure hull imploding as she passed crush depth. A Naval Research Laboratory scientist named Chester \"Buck\" Buchanan, using a towed camera sled of his own design aboard \"Mizar\", finally located \"Scorpion\". The towed camera sled, which was fabricated by J. L. \"Jac\" Hamm of Naval Research Laboratory's Engineering Services Division, is housed in the National Museum of the United States Navy. Buchanan had located the wrecked hull of \"Thresher\" in 1964 using this technique.\n\nThe classical book on this subject \"The Theory of Optimal Search\" (Operations Research Society of America, 1975) by Lawrence D. Stone won the 1975 Lanchester Prize by the American Operations Research Society.\n\nAssume that a stationary object is hidden in one of n boxes. For each box formula_3 there are three known parameters: the cost of a single\nsearch at location formula_3, formula_5; the probability of finding the object by a single search at location formula_3 if it is at this location, formula_7; and the probability that the object is at location formula_3, formula_9. A searcher looks for the object. They know the a priori probabilities at the beginning and update them by Bayes’ law after each (unsuccessful) attempt.\nThe problem of finding the object in minimal expected cost is a classical problem\nsolved by David Blackwell. Surprisingly, the optimal policy is easy to describe: at each stage look into the location which maximizes\nformula_10. This is actually a special case of Gittins index.\n\n\n"}
{"id": "5871105", "url": "https://en.wikipedia.org/wiki?curid=5871105", "title": "Bedlam cube", "text": "Bedlam cube\n\nThe Bedlam cube is a solid dissection puzzle invented by British puzzle expert Bruce Bedlam.\n\nThe puzzle consists of thirteen polycubic pieces: twelve pentacubes and one tetracube. The objective is to assemble these pieces into a 4 x 4 x 4 cube. There are 19,186 distinct ways of doing so, up to rotations and reflections.\n\nThe Bedlam cube is one unit per side larger than the 3 x 3 x 3 Soma cube, and is much more difficult to solve.\n\nTwo of the BBC's 'Dragons' from \"Dragons' Den\", Rachel Elnaugh and Theo Paphitis, were to invest in the Bedlam cube during the second series. They offered £100,000 for a 15% share of equity in Bedlam Puzzles. Danny Bamping (the entrepreneur behind Bedlam cube) finally chose a bank loan instead of their investment, as seen in the relevant \"Where Are They Now\" episode of \"Dragons' Den\".\n\nAccording to \"Guinness World Records\", the official world record for assembling the Bedlam Cube is 11.03 seconds by Danny Bamping on 9 November 2006. The blindfolded record is 27.21 seconds by Aleksandr Iljasov on 25 February 2008.\n\n\n"}
{"id": "40343255", "url": "https://en.wikipedia.org/wiki?curid=40343255", "title": "Carl B. Allendoerfer Award", "text": "Carl B. Allendoerfer Award\n\nThe Carl B. Allendoerfer Award is presented annually by the Mathematical Association of America (MAA) for \"expository excellence published in \"Mathematics Magazine\".\" it is named after mathematician Carl B. Allendoerfer who was president of the MAA 1959–60.\n\nRecipients of the Carl B. Allendoerfer Award have included:\n"}
{"id": "57557045", "url": "https://en.wikipedia.org/wiki?curid=57557045", "title": "David Gottlieb (mathematician)", "text": "David Gottlieb (mathematician)\n\nDavid Gottlieb (November 14, 1944 – December 6, 2008) was an Israeli mathematician. Gottlieb received his PhD in 1972 from the Department of Mathematics at Tel Aviv University under the guidance of Saul Abarbanel. He was a professor of applied mathematics at Brown from 1985 until his death.\n\nHis research focused on numerical analysis, especially as applied to nonlinear partial differential equations.\n\nHe was a member of the National Academy of Sciences and the American Academy of Arts and Sciences.\n\n"}
{"id": "13353871", "url": "https://en.wikipedia.org/wiki?curid=13353871", "title": "Dawson–Gärtner theorem", "text": "Dawson–Gärtner theorem\n\nIn mathematics, the Dawson–Gärtner theorem is a result in large deviations theory. Heuristically speaking, the Dawson–Gärtner theorem allows one to transport a large deviation principle on a “smaller” topological space to a “larger” one.\n\nLet (\"Y\") be a projective system of Hausdorff topological spaces with maps \"p\" : \"Y\" → \"Y\". Let \"X\" be the projective limit (also known as the inverse limit) of the system (\"Y\", \"p\"), i.e.\n\nLet (\"μ\") be a family of probability measures on \"X\". Assume that, for each \"j\" ∈ \"J\", the push-forward measures (\"p\"\"μ\") on \"Y\" satisfy the large deviation principle with good rate function \"I\" : \"Y\" → R ∪ {+∞}. Then the family (\"μ\") satisfies the large deviation principle on \"X\" with good rate function \"I\" : \"X\" → R ∪ {+∞} given by\n\n"}
{"id": "28417424", "url": "https://en.wikipedia.org/wiki?curid=28417424", "title": "Equilateral dimension", "text": "Equilateral dimension\n\nIn mathematics, the equilateral dimension of a metric space is the maximum number of points that are all at equal distances from each other. Equilateral dimension has also been called \"metric dimension\", but the term \"metric dimension\" also has many other inequivalent usages. The equilateral dimension of a \"d\"-dimensional Euclidean space is , and the equilateral dimension of a \"d\"-dimensional vector space with the Chebyshev distance (L norm) is 2. However, the equilateral dimension of a space with the Manhattan distance (L norm) is not known; Kusner's conjecture, named after Robert B. Kusner, states that it is exactly 2\"d\".\n\nThe equilateral dimension has been particularly studied for Lebesgue spaces, finite-dimensional normed vector spaces with the L norm\n\nThe equilateral dimension of L spaces of dimension \"d\" behaves differently depending on the value of \"p\":\n\nEquilateral dimension has also been considered for normed vector spaces with norms other than the L norms. The problem of determining the equilateral dimension for a given norm is closely related to the kissing number problem: the kissing number in a normed space is the maximum number of disjoint translates of a unit ball that can all touch a single central ball, whereas the equilateral dimension is the maximum number of disjoint translates that can all touch each other.\n\nFor a normed vector space of dimension \"d\", the equilateral dimension is at most 2; that is, the L norm has the highest equilateral dimension among all normed spaces. asked whether every normed vector space of dimension \"d\" has equilateral dimension at least , but this remains unknown. There exist normed spaces in any dimension for which certain sets of four equilateral points cannot be extended to any larger equilateral set but these spaces may have larger equilateral sets that do not include these four points. For norms that are sufficiently close in Banach–Mazur distance to an L norm, Petty's question has a positive answer: the equilateral dimension is at least .\n\nIt is not possible for high-dimensional spaces to have bounded equilateral dimension: for any integer \"k\", all normed vector spaces of sufficiently high dimension have equilateral dimension at least \"k\". more specifically, according to a variation of Dvoretzky's theorem by , every \"d\"-dimensional normed space has a \"k\"-dimensional subspace that is close either to a Euclidean space or to a Chebyshev space, where\nfor some constant \"c\". Because it is close to a Lebesgue space, this subspace and therefore also the whole space contains an equilateral set of at least \"k\" + 1 points. Therefore, the same superlogarithmic dependence on \"d\" holds for the lower bound on the equilateral dimension of \"d\"-dimensional space.\n\nFor any \"d\"-dimensional Riemannian manifold the equilateral dimension is at least . For a \"d\"-dimensional sphere, the equilateral dimension is , the same as for a Euclidean space of one higher dimension into which the sphere can be embedded. At the same time as he posed Kusner's conjecture, Kusner asked whether there exist Riemannian metrics with bounded dimension as a manifold but arbitrarily high equilateral dimension.\n\n"}
{"id": "2235037", "url": "https://en.wikipedia.org/wiki?curid=2235037", "title": "Existential graph", "text": "Existential graph\n\nAn existential graph is a type of diagrammatic or visual notation for logical expressions, proposed by Charles Sanders Peirce, who wrote on graphical logic as early as 1882, and continued to develop the method until his death in 1914.\n\nPeirce proposed three systems of existential graphs:\n\"Alpha\" nests in \"beta\" and \"gamma\". \"Beta\" does not nest in \"gamma\", quantified modal logic being more general than put forth by Peirce.\n\nThe syntax is:\nAny well-formed part of a graph is a subgraph.\n\nThe semantics are:\nHence the \"alpha\" graphs are a minimalist notation for sentential logic, grounded in the expressive adequacy of And and Not. The \"alpha\" graphs constitute a radical simplification of the two-element Boolean algebra and the truth functors.\n\nThe \"depth\" of an object is the number of cuts that enclose it.\n\n\"Rules of inference\":\n\n\"Rules of equivalence\":\n\nA proof manipulates a graph by a series of steps, with each step justified by one of the above rules. If a graph can be reduced by steps to the blank page or an empty cut, it is what is now called a tautology (or the complement thereof). Graphs that cannot be simplified beyond a certain point are analogues of the satisfiable formulas of first-order logic.\n\nPeirce notated predicates using intuitive English phrases; the standard notation of contemporary logic, capital Latin letters, may also be employed. A dot asserts the existence of some individual in the domain of discourse. Multiple instances of the same object are linked by a line, called the \"line of identity\". There are no literal variables or quantifiers in the sense of first-order logic. A line of identity connecting two or more predicates can be read as asserting that the predicates share a common variable. The presence of lines of identity requires modifying the \"alpha\" rules of Equivalence.\n\nThe beta graphs can be read as a system in which all formula are to be taken as closed, because all variables are implicitly quantified. If the \"shallowest\" part of a line of identity has even (odd) depth, the associated variable is tacitly existentially (universally) quantified. \n\nZeman was the first to note that the \"beta\" graphs are isomorphic to first-order logic with equality (also see Zeman 1967). However, the secondary literature, especially Roberts (1973) and Shin (2002), does not agree on just how this is so. Peirce's writings do not address this question, because first-order logic was first clearly articulated only some years after his death, in the 1928 first edition of David Hilbert and Wilhelm Ackermann's \"Principles of Mathematical Logic\".\n\nAdd to the syntax of \"alpha\" a second kind of simple closed curve, written using a dashed rather than a solid line. Peirce proposed rules for this second style of cut, which can be read as the primitive unary operator of modal logic.\n\nZeman (1964) was the first to note that straightforward emendations of the \"gamma\" graph rules yield the well-known modal logics S4 and S5. Hence the \"gamma\" graphs can be read as a peculiar form of normal modal logic. This finding of Zeman's has gone unremarked to this day, but is nonetheless included here as a point of interest.\n\nThe existential graphs are a curious offspring of Peirce the logician/mathematician with Peirce the founder of a major strand of semiotics. Peirce's graphical logic is but one of his many accomplishments in logic and mathematics. In a series of papers beginning in 1867, and culminating with his classic paper in the 1885 \"American Journal of Mathematics\", Peirce developed much of the two-element Boolean algebra, propositional calculus, quantification and the predicate calculus, and some rudimentary set theory. Model theorists consider Peirce the first of their kind. He also extended De Morgan's relation algebra. He stopped short of metalogic (which eluded even \"Principia Mathematica\").\n\nBut Peirce's evolving semiotic theory led him to doubt the value of logic formulated using conventional linear notation, and to prefer that logic and mathematics be notated in two (or even three) dimensions. His work went beyond Euler's diagrams and Venn's 1880 revision thereof. Frege's 1879 \"Begriffsschrift\" also employed a two-dimensional notation for logic, but one very different from Peirce's.\n\nPeirce's first published paper on graphical logic (reprinted in Vol. 3 of his \"Collected Papers\") proposed a system dual (in effect) to the \"alpha\" existential graphs, called the entitative graphs. He very soon abandoned this formalism in favor of the existential graphs. The graphical logic went unremarked during his lifetime, and was invariably denigrated or ignored after his death, until the Ph.D. theses by Roberts (1964) and Zeman (1964).\n\n\n\nCurrently, the chronological critical edition of Peirce's works, the \"Writings\", extends only to 1892. Much of Peirce's work on logical graphs consists of manuscripts written after that date and still unpublished. Hence our understanding of Peirce's graphical logic is likely to change as the remaining 23 volumes of the chronological edition appear.\n\n\n"}
{"id": "5006371", "url": "https://en.wikipedia.org/wiki?curid=5006371", "title": "Gottesman–Knill theorem", "text": "Gottesman–Knill theorem\n\nIn quantum computing, the Gottesman–Knill theorem is a theoretical result by Daniel Gottesman and Emanuel Knill that states that stabilizer circuits, circuits that only consist of gates from the normalizer of the qubit Pauli group, also called Clifford group, can be perfectly simulated in polynomial time on a probabilistic classical computer. The Clifford group can be generated solely by using CNOT, Hadamard, and phase gates \n, therefore stabilizer circuits can be constructed using only these gates. \n\nThe reason for the speed up of quantum computers is not yet fully understood. The theorem proves that, for all quantum algorithms with a speed up that relies on entanglement which can be achieved with a CNOT and a Hadamard gate to produce entangled states, this kind of entanglement alone does not give any computing advantage.\n\nThere exists a more efficient simulation of stabilizer circuits than the construction of the original publication with an implementation<ref name=\"https://www.scottaaronson.com/chp/\"></ref>.\nThe Gottesman–Knill theorem was published in a single author paper by Gottesman in which he credits Knill with the result through private communication.\n\nTheorem: A quantum circuit using only the following elements can be simulated efficiently on a classical computer:\n\nThe Gottesman–Knill theorem shows that even some highly entangled states can be simulated efficiently. Several important types of quantum algorithms use only Clifford gates, most importantly the standard algorithms for entanglement purification and for quantum error correction. From a practical point of view, stabilizer circuits have been simulated in O(\"n\" log \"n\") time using the graph state formalism.\n\n"}
{"id": "3458326", "url": "https://en.wikipedia.org/wiki?curid=3458326", "title": "Gravity model of trade", "text": "Gravity model of trade\n\nThe gravity model of international trade in international economics is a model that, in its traditional form, predicts bilateral trade flows based on the economic sizes (often using GDP measurements) and distance between two units.\n\nThe model was first introduced in economics world by Walter Isard in 1954. The basic model for trade between two countries (\"i\" and \"j\") takes the form of\nIn this formula G is the constant, F stands for trade flow, D stands for the distance and M stands for the economic dimensions of the countries that are being measured. The equation can be changed into a linear form for the purpose of econometric analyses by employing logarithms. The model has been used by economists to analyse the determinants of bilateral trade flows such as common borders, common languages, common legal systems, common currencies, common colonial legacies, and it has been used to test the effectiveness of trade agreements and organizations such as the North American Free Trade Agreement (NAFTA) and the World Trade Organization (WTO) (Head and Mayer 2014). The model has also been used in international relations to evaluate the impact of treaties and alliances on trade (Head and Mayer).\n\nThe model has also been applied to other bilateral flow data (also 'dyadic' data) such as migration, traffic, remittances and foreign direct investment.\n\nThe model has been an empirical success in that it accurately predicts trade flows between countries for many goods and services, but for a long time some scholars believed that there was no theoretical justification for the gravity equation. However, a gravity relationship can arise in almost any trade model that includes trade costs that increase with distance.\n\nThe gravity model estimates the pattern of international trade. While the model’s basic form consists of factors that have more to do with geography and spatiality, the gravity model has been used to test hypotheses rooted in purer economic theories of trade as well. One such theory predicts that trade will be based on relative factor abundances. One of the common relative factor abundance models is the Heckscher–Ohlin model. This theory would predict that trade patterns would be based on relative factor abundance. Those countries with a relative abundance of one factor would be expected to produce goods that require a relatively large amount of that factor in their production. While a generally accepted theory of trade, many economists in the Chicago School believed that the Heckscher–Ohlin model alone was sufficient to describe all trade, while Bertil Ohlin himself argued that in fact the world is more complicated. Investigations into real-world trading patterns have produced a number of results that do not match the expectations of comparative advantage theories. Notably, a study by Wassily Leontief found that the United States, the most capital-endowed country in the world, actually exports more in labor-intensive industries. Comparative advantage in factor endowments would suggest the opposite would occur. Other theories of trade and explanations for this relationship were proposed in order to explain the discrepancy between Leontief’s empirical findings and economic theory. The problem has become known as the Leontief paradox.\n\nAn alternative theory, first proposed by Staffan Linder, predicts that patterns of trade will be determined by the aggregated preferences for goods within countries. Those countries with similar preferences would be expected to develop similar industries. With continued similar demand, these countries would continue to trade back and forth in differentiated but similar goods since both demand and produce similar products. For instance, both Germany and the United States are industrialized countries with a high preference for automobiles. Both countries have automobile industries, and both trade cars. The empirical validity of the Linder hypothesis is somewhat unclear. Several studies have found a significant impact of the Linder effect, but others have had weaker results. Studies that do not support Linder have only counted countries that actually trade; they do not input zero values for the dyads where trade could happen but does not. This has been cited as a possible explanation for their findings. Also, Linder never presented a formal model for his theory, so different studies have tested his hypothesis in different ways.\n\nElhanan Helpman and Paul Krugman asserted that the theory behind comparative advantage does not predict the relationships in the gravity model. Using the gravity model, countries with similar levels of income have been shown to trade more. Helpman and Krugman see this as evidence that these countries are trading in differentiated goods because of their similarities. This casts some doubt about the impact Heckscher–Ohlin has on the real world. Jeffrey Frankel sees the Helpman–Krugman setup here as distinct from Linder’s proposal. However, he does say Helpman–Krugman is different from the usual interpretation of Linder, but, since Linder made no clear model, the association between the two should not be completely discounted. Alan Deardorff adds the possibility, that, while not immediately apparent, the basic gravity model can be derived from Heckscher–Ohlin as well as the Linder and Helpman–Krugman hypotheses. Deardorff concludes that, considering how many models can be tied to the gravity model equation, it is not useful for evaluating the empirical validity of theories.\n\nBridging economic theory with empirical tests, James Anderson and Jeffrey Bergstrand develop econometric models, grounded in the theories of differentiated goods, which measure the gains from trade liberalizations and the magnitude of the border barriers on trade (see Home bias in trade puzzle). A recent synthesis of empirical research using the gravity equations, however, shows that the effect of border barriers on trade is relatively modest.\n\nAdding to the problem of bridging economic theory with empirical results, some economists have pointed to the possibility of intra-industry trade not as the result of differentiated goods, but because of “reciprocal dumping.” In these models, the countries involved are said to have imperfect competition and segmented markets in homogeneous goods, which leads to intra-industry trade as firms in imperfect competition seek to expand their markets to other countries and trade goods that are not differentiated yet for which they do not have a comparative advantage, since there is no specialization. This model of trade is consistent with the gravity model as it would predict that trade depends on country size.\n\nThe reciprocal dumping model has held up to some empirical testing, suggesting that the specialization and differentiated goods models for the gravity equation might not fully explain the gravity equation. Feenstra, Markusen, and Rose (2001) provided evidence for reciprocal dumping by assessing the \"home market effect\" in separate gravity equations for differentiated and homogeneous goods. The home market effect showed a relationship in the gravity estimation for differentiated goods, but showed the inverse relationship for homogeneous goods. The authors show that this result matches the theoretical predictions of reciprocal dumping playing a role in homogeneous markets.\n\nPast research using the gravity model has also sought to evaluate the impact of various variables in addition to the basic gravity equation. Among these, price level and exchange rate variables have been shown to have a relationship in the gravity model that accounts for a significant amount of the variance not explained by the basic gravity equation. According to empirical results on price level, the effect of price level varies according to the relationship being examined. For instance, if exports are being examined, a relatively high price level on the part of the importer would be expected to increase trade with that country. A non-linear system of equations are used by Anderson and van Wincoop (2003) to account for the endogenous change in these price terms from trade liberalization. A more simple method is to use a first order log-linearization of this system of equations (Baier and Bergstrand (2009)), or exporter-country-year and importer-country-year dummy variables. For counterfactual analysis, however, one would still need to account for the change in world prices.\n\nSince the gravity model for trade does not hold exactly, in econometric applications it is customary to specify\n\nwhere formula_3 represents volume of trade from country formula_4 to country formula_5, formula_6 and formula_7 typically represent the GDPs for countries formula_4 and formula_5, formula_10 denotes the distance between the two countries, and formula_11 represents an error term with expectation equal to 1.\n\nThe traditional approach to estimating this equation consists in taking logs of both sides, leading to a log-log model of the form (note: constant G becomes part of formula_12):\n\nHowever, this approach has two major problems. First, it obviously cannot be used when there are observations for which formula_3 is equal to zero. Second, iSantos Silva and Tenreyro (2006) argued that estimating the log-linearized equation by least squares (OLS) can lead to significant biases. As an alternative, these authors have suggested that the model should be estimated in its multiplicative form, i.e.,\n\nusing a Poisson pseudo-maximum likelihood (PPML) estimator usually used for count data. One of the authors' more surprising findings was that, when controlling for sharing a common language, having past colonial ties does not increase trade. This is despite the fact that simpler methods, such as taking simple averages of trade shares of countries with and without former colonial ties suggest that countries with former colonial ties continue to trade more. Santos Silva and Tenreyro (2006) did not explain where their result came from and even failed to realize their results were highly anomalous. Martin and Pham (2008) argued that using PPML on gravity severely biases estimates when zero trade flows are frequent. However, their results were challenged by Santos Silva and Tenreyro (2011), who argued that the simulation results of Martin and Pham (2008) are based on misspecified models and showed that the PPML estimator performs well even when the proportions of zeros is very large.\n\nIn applied work, the model is often extended by including variables to account for language relationships, tariffs, contiguity, access to sea, colonial history, and exchange rate regimes. Yet the estimation of structural gravity, based on Anderson and van Wincoop (2003), requires the inclusion of importer and exporter fixed effects, thus limiting the gravity analysis to bilateral trade costs (Baldwin and Taglioni 2007).\n\n\n\n"}
{"id": "972333", "url": "https://en.wikipedia.org/wiki?curid=972333", "title": "Highly totient number", "text": "Highly totient number\n\nA highly totient number \"k\" is an integer that has more solutions to the equation φ(\"x\") = \"k\", where φ is Euler's totient function, than any integer below it. The first few highly totient numbers are\n\n1, 2, 4, 8, 12, 24, 48, 72, 144, 240, 432, 480, 576, 720, 1152, 1440 , with 1, 3, 4, 5, 6, 10, 11, 17, 21, 31, 34, 37, 38, 49, 54, and 72 totient solutions respectively. The sequence of highly totient numbers is a subset of the sequence of smallest number \"k\" with exactly \"n\" solutions to φ(\"x\") = \"k\".\n\nThe totient of a number \"x\", with prime factorization formula_1, is the product:\nThus, a highly totient number is a number that has more ways of being expressed as a product of this form than does any smaller number.\n\nThe concept is somewhat analogous to that of highly composite numbers, and in the same way that 1 is the only odd highly composite number, it is also the only odd highly totient number (indeed, the only odd number to not be a nontotient). And just as there are infinitely many highly composite numbers, there are also infinitely many highly totient numbers, though the highly totient numbers get tougher to find the higher one goes, since calculating the totient function involves factorization into primes, something that becomes extremely difficult as the numbers get larger.\n\nThere are five numbers (15, 16, 20, 24, and 30) whose totient number is 8. No positive integer smaller than 8 has as many such numbers, so 8 is highly totient.\n\n\n"}
{"id": "50466674", "url": "https://en.wikipedia.org/wiki?curid=50466674", "title": "History of the compass", "text": "History of the compass\n\nThe compass was invented almost 2,000 years ago. The first compasses were made of lodestone, a naturally magnetized ore of iron, in Han dynasty China between 300 and 200 BC. The compass was later used for navigation by the Song Dynasty. Later compasses were made of iron needles, magnetized by striking them with a lodestone. Dry compasses begin appearing around 1300 in Medieval Europe and the Medieval Islamic world. This was replaced in the early 20th century by the liquid-filled magnetic compass.\n\nPrior to the introduction of the compass, geographical position and direction at sea were primarily determined by the sighting of landmarks, supplemented with the observation of the position of celestial bodies. Objects that have been understood as having been used for navigation by measuring the angles between celestial objects, were discovered in the Indus Valley site of Lothal. On cloudy days, the Vikings may have used cordierite or some other birefringent crystal to determine the sun's direction and elevation from the polarization of daylight; their astronomical knowledge was sufficient to let them use this information to determine their proper heading. The invention of the compass made it possible to determine a heading when the sky was overcast or foggy, and when landmarks were not in sight. This enabled mariners to navigate safely far from land, increasing sea trade, and contributing to the Age of Discovery.\n\nThe compass was invented in China during the Han Dynasty between the 2nd century BC and 1st century AD, where it was called the \"south-governor\" (\"sīnán\" ). The magnetic compass was not, at first, used for navigation, but for geomancy and fortune-telling by the Chinese. The earliest Chinese magnetic compasses were possibly used to order and harmonize buildings in accordance with the geomantic principles of \"feng shui\". These early compasses were made with lodestone, a form of the mineral magnetite that is a naturally occurring magnet and aligns itself with the Earth’s magnetic field. People in ancient China discovered that if a lodestone was suspended so it could turn freely, it would always point toward the magnetic poles. Early compasses were used to choose areas suitable for building houses, growing crops, and to search for rare gems. Compasses were later adapted for navigation during the Song Dynasty in the 11th century.\n\nBased on Krotser and Coe's discovery of an Olmec hematite artifact in Mesoamerica, radiocarbon dated to 1400–1000 BC, astronomer John Carlson has hypothesized that the Olmec might have used the geomagnetic lodestone earlier than 1000 BC for geomancy, a method of divination, which if proven true, predates the Chinese use of magnetism for feng shui by a millennium. Carlson speculates that the Olmecs used similar artifacts as a directional device for astronomical or geomantic purposes but does not suggest navigational usage. The artifact is part of a polished hematite bar with a groove at one end, possibly used for sighting. Carlson's claims have been disputed by other scientific researchers, who have suggested that the artifact is actually a constituent piece of a decorative ornament and not a purposely built compass. Several other hematite or magnetite artifacts have been found at pre-Columbian archaeological sites in Mexico and Guatemala.\n\nA number of early cultures used lodestones, suspended so they could turn, as magnetic compasses for navigation. Early mechanical compasses are referenced in written records of the Chinese, who began using it for navigation sometime between the 9th and 11th century, \"some time before 1050, possibly as early as 850.\" At present, according to Kreutz, scholarly consensus is that the Chinese invention used in navigation pre-dates the first European mention of a compass by 150 years. The first recorded appearance of the use of the compass in Europe (1190) is earlier than in the Muslim world (1232), as a description of a magnetized needle and its use among sailors occurs in Alexander Neckam's \"De naturis rerum\" (On the Natures of Things), written in 1190. \n\nHowever, there are questions over diffusion. Some historians suggest that the Arabs introduced the compass from China to Europe. Some suggested the compass was transmitted from China to Europe and the Muslim world via the Indian Ocean, or was brought by the crusaders to Europe from China. However, some scholars proposed an independent European invention of the compass: \n\nThere is disagreement as to exactly when the compass was invented. These are noteworthy Chinese literary references in evidence for its antiquity:\n\nThus, the use of a magnetic compass by the military for land navigation occurred sometime before 1044, but incontestable evidence for the use of the compass as a maritime navigational device did not appear until 1117.\n\nThe typical Chinese navigational compass was in the form of a magnetic needle floating in a bowl of water. According to Needham, the Chinese in the Song Dynasty and continuing Yuan Dynasty did make use of a dry compass, although this type never became as widely used in China as the wet compass. Evidence of this is found in the \"Shilin guangji\" (\"Guide Through the Forest of Affairs\"), published in 1325 by Chen Yuanjing, although its compilation had taken place between 1100 and 1250. The dry compass in China was a dry suspension compass, a wooden frame crafted in the shape of a turtle hung upside down by a board, with the lodestone sealed in by wax, and if rotated, the needle at the tail would always point in the northern cardinal direction. Although the European compass-card in box frame and dry pivot needle was adopted in China after its use was taken by Japanese pirates in the 16th century (who had in turn learned of it from Europeans), the Chinese design of the suspended dry compass persisted in use well into the 18th century. However, according to Kreutz there is only a single Chinese reference to a dry-mounted needle (built into a pivoted wooden tortoise) which is dated to between 1150 and 1250, and claims that there is no clear indication that Chinese mariners ever used anything but the floating needle in a bowl until the 16th century.\n\nThe first recorded use of a 48 position mariner's compass on sea navigation was noted in \"The Customs of Cambodia\" by Yuan Dynasty diplomat Zhou Daguan, he described his 1296 voyage from Wenzhou to Angkor Thom in detail; when his ship set sail from Wenzhou, the mariner took a needle direction of “ding wei” position, which is equivalent to 22.5 degree SW. After they arrived at Baria, the mariner took \"Kun Shen needle\", or 52.5 degree SW. Zheng He's Navigation Map, also known as the \"Mao Kun Map\", contains a large amount of detail \"needle records\" of Zheng He's expeditions.\n\nAlexander Neckam reported the use of a magnetic compass for the region of the English Channel in the texts \"De utensilibus\" and \"De naturis rerum\", written between 1187 and 1202, after he returned to England from France and prior to entering the Augustinian abbey at Cirencester. In his 1863 edition of Neckam's \"De naturis rerum\", Thomas Wright provides a translation of the passage in which Neckam mentions sailors being guided by a compass' needle: \"The sailors, moreover, as they sail over the sea, when in cloudy whether they can no longer profit by the light of the sun, or when the world is wrapped up in the darkness of the shades of night, and they are ignorant to what point of the compass their ship's course is directed, they touch the magnet with a needle, which (the needle) is whirled round in a circle until, when its motion ceases, its point looks direct to the north.\" Neckam's clear understanding of the mariner's compass in the late twelfth-century, and his description of its use in marine navigation, has cast doubt on whether the compass was, as Professor Derk Bodde has argued, one of 'China's gifts to the West'. From Neckam's description, some scholars have suggested it is not unlikely that the compass was also 'invented' in Europe, independently of Eastern technologies. This is further supported by Jacques de Vitry's mention of the compass being used at sea in 1218, which indicates a broader knowledge of the compass and its uses in Medieval Northern Europe: \"An iron needle, after having been in contact with the loadstone, turns itself always toward the northern star, which, like the axis of the firmament, remains immoveable, while the others follow their course, so that it is very necessary to those who navigate the sea.\"Robert Southey suggested that the Siete Partidas contained a reference from the 1250s to the needle being used for navigation.\nIn 1269 Petrus Peregrinus of Maricourt described a floating compass for astronomical purposes as well as a dry compass for seafaring, in his well-known \"Epistola de magnete\".\n\nIn the Mediterranean, the introduction of the compass, at first only known as a magnetized pointer floating in a bowl of water, went hand in hand with improvements in dead reckoning methods, and the development of Portolan charts, leading to more navigation during winter months in the second half of the 13th century. While the practice from ancient times had been to curtail sea travel between October and April, due in part to the lack of dependable clear skies during the Mediterranean winter, the prolongation of the sailing season resulted in a gradual, but sustained increase in shipping movement; by around 1290 the sailing season could start in late January or February, and end in December. The additional few months were of considerable economic importance. For instance, it enabled Venetian convoys to make two round trips a year to the Levant, instead of one.\n\nAt the same time, traffic between the Mediterranean and northern Europe also increased, with first evidence of direct commercial voyages from the Mediterranean into the English Channel coming in the closing decades of the 13th century, and one factor may be that the compass made traversal of the Bay of Biscay safer and easier. However, critics like Kreutz have suggested that it was later in 1410 that anyone really started steering by compass.\n\nThe earliest reference to a compass in the Muslim world occurs in a Persian talebook from 1232, where a compass is used for navigation during a trip in the Red Sea or the Persian Gulf. The fish-shaped iron leaf described indicates that this early Chinese design has spread outside of China. The earliest Arabic reference to a compass, in the form of magnetic needle in a bowl of water, comes from a work by Baylak al-Qibjāqī, written in 1282 while in Cairo. Al-Qibjāqī described a needle-and-bowl compass used for navigation on a voyage he took from Syria to Alexandria in 1242. Since the author describes having witnessed the use of a compass on a ship trip some forty years earlier, some scholars are inclined to antedate its first appearance in the Arab world accordingly. Al-Qibjāqī also reports that sailors in the Indian Ocean used iron fish instead of needles.\n\nLate in the 13th century, the Yemeni Sultan and astronomer al-Malik al-Ashraf described the use of the compass as a \"Qibla indicator\" to find the direction to Mecca. In a treatise about astrolabes and sundials, al-Ashraf includes several paragraphs on the construction of a compass bowl (ṭāsa). He then uses the compass to determine the north point, the meridian (khaṭṭ niṣf al-nahār), and the Qibla. This is the first mention of a compass in a medieval Islamic scientific text and its earliest known use as a Qibla indicator, although al-Ashraf did not claim to be the first to use it for this purpose.\n\nIn 1300, an Arabic treatise written by the Egyptian astronomer and muezzin Ibn Simʿūn describes a dry compass used for determining qibla. Like Peregrinus' compass, however, Ibn Simʿūn's compass did not feature a compass card. In the 14th century, the Syrian astronomer and timekeeper Ibn al-Shatir (1304–1375) invented a timekeeping device incorporating both a universal sundial and a magnetic compass. He invented it for the purpose of finding the times of prayers. Arab navigators also introduced the 32-point compass rose during this time. In 1399, an Egyptian reports two different kinds of magnetic compass. One instrument is a “fish” made of willow wood or pumpkin, into which a magnetic needle is inserted and sealed with tar or wax to prevent the penetration of water. The other instrument is a dry compass.\nIn the 15th century, the description given by Ibn Majid while aligning the compass with the pole star indicates that he was aware of magnetic declination. An explicit value for the declination is given by ʿIzz al-Dīn al-Wafāʾī (fl. 1450s in Cairo).\n\nPre modern Arabic sources refer to the compass using the term \"ṭāsa\" (lit. \"bowl\") for the floating compass, or \"ālat al-qiblah\" (\"qibla instrument\") for a device used for orienting towards Mecca.\n\nFriedrich Hirth suggested that Arab and Persian traders, who learned about the polarity of the magnetic needle from the Chinese, applied the compass for navigation before the Chinese did. However, Needham described this theory as \"erroneous\" and \"it originates because of a mistraslation\" of the term \"chia-ling\" found in Zhu Yu's book \"Pingchow Table Talks\".\n\nThe development of the magnetic compass is highly uncertain. The compass is mentioned in fourth-century AD Tamil nautical books; moreover, its early name of \"macchayantra\" (fish machine) suggest a Chinese origin. In its Indian form, the wet compass often consisted of a fish-shaped magnet, float in a bowl filled with oil. This fish shape is due to its name, which is composed of the words \"maccha\" meaning \"fish\" and \"yantra\" meaning \"device\".\n\nThere is evidence that the distribution of the compass from China likely also reached eastern Africa by way of trade through the end of the Silk Road that ended in East African centre of trade in Somalia and the Swahili city-state kingdoms. There is evidence that Swahili maritime merchants and sailors acquired the compass at some point and used it for navigation.\n\nThe dry mariner's compass consists of three elements: A freely pivoting needle on a pin enclosed in a little box with a glass cover and a wind rose, whereby \"the wind rose or compass card is attached to a magnetized needle in such a manner that when placed on a pivot in a box fastened in line with the keel of the ship the card would turn as the ship changed direction, indicating always what course the ship was on\". Later, compasses were often fitted into a gimbal mounting to reduce grounding of the needle or card when used on the pitching and rolling deck of a ship.\n\nWhile pivoting needles in glass boxes had already been described by the French scholar Peter Peregrinus in 1269, and by the Egyptian scholar Ibn Simʿūn in 1300, traditionally Flavio Gioja (fl. 1302), an Italian pilot from Amalfi, has been credited with perfecting the sailor's compass by suspending its needle over a compass card, thus giving the compass its familiar appearance. Such a compass with the needle attached to a rotating card is also described in a commentary on Dante's \"Divine Comedy\" from 1380, while an earlier source refers to a portable compass in a box (1318), supporting the notion that the dry compass was known in Europe by then.\n\nA \"bearing compass\" is a magnetic compass mounted in such a way that it allows the taking of bearings of objects by aligning them with the lubber line of the bearing compass. A \"surveyor's compass\" is a specialized compass made to accurately measure heading of landmarks and measure horizontal angles to help with map making. These were already in common use by the early 18th century and are described in the 1728 Cyclopaedia. The bearing compass was steadily reduced in size and weight to increase portability, resulting in a model that could be carried and operated in one hand. In 1885, a patent was granted for a hand compass fitted with a viewing prism and lens that enabled the user to accurately sight the heading of geographical landmarks, thus creating the \"prismatic compass\". Another sighting method was by means of a reflective mirror. First patented in 1902, the \"Bézard compass\" consisted of a field compass with a mirror mounted above it. This arrangement enabled the user to align the compass with an objective while simultaneously viewing its bearing in the mirror.\n\nIn 1928, Gunnar Tillander, a Swedish unemployed instrument maker and avid participant in the sport of orienteering, invented a new style of bearing compass. Dissatisfied with existing field compasses, which required a separate protractor in order to take bearings from a map, Tillander decided to incorporate both instruments into a single instrument. It combined a compass with a protractor built into the base. His design featured a metal compass capsule containing a magnetic needle with orienting marks mounted into a transparent protractor baseplate with a lubber line (later called a \"direction of travel indicator\"). By rotating the capsule to align the needle with the orienting marks, the course bearing could be read at the lubber line. Moreover, by aligning the baseplate with a course drawn on a map – ignoring the needle – the compass could also function as a protractor. Tillander took his design to fellow orienteers Björn, Alvid, and Alvar Kjellström, who were selling basic compasses, and the four men modified Tillander's design. In December 1932, the Silva Company was formed with Tillander and the three Kjellström brothers, and the company began manufacturing and selling its Silva orienteering compass to Swedish orienteers, outdoorsmen, and army officers.\n\nThe liquid compass is a design in which the magnetized needle or card is damped by fluid to protect against excessive swing or wobble, improving readability while reducing wear. A rudimentary working model of a liquid compass was introduced by Sir Edmund Halley at a meeting of the Royal Society in 1690. However, as early liquid compasses were fairly cumbersome and heavy, and subject to damage, their main advantage was aboard ship. Protected in a binnacle and normally gimbal-mounted, the liquid inside the compass housing effectively damped shock and vibration, while eliminating excessive swing and grounding of the card caused by the pitch and roll of the vessel. The first liquid mariner's compass believed practicable for limited use was patented by the Englishman Francis Crow in 1813. Liquid-damped marine compasses for ships and small boats were occasionally used by the Royal Navy from the 1830s through 1860, but the standard Admiralty compass remained a dry-mount type. In the latter year, the American physicist and inventor Edward Samuel Ritchie patented a greatly improved liquid marine compass that was adopted in revised form for general use by the United States Navy, and later purchased by the Royal Navy as well.\n\nDespite these advances, the liquid compass was not introduced generally into the Royal Navy until 1908. An early version developed by RN Captain Creak proved to be operational under heavy gunfire and seas, but was felt to lack navigational precision compared with the design by Lord Kelvin. However, with ship and gun sizes continuously increasing, the advantages of the liquid compass over the Kelvin compass became unavoidably apparent to the Admiralty, and after widespread adoption by other navies, the liquid compass was generally adopted by the Royal Navy.\nLiquid compasses were next adapted for aircraft. In 1909, Captain F.O. Creagh-Osborne, Superintendent of Compasses at the Admiralty, introduced his \"Creagh-Osborne\" aircraft compass, which used a mixture of alcohol and distilled water to damp the compass card. After the success of this invention, Capt. Creagh-Osborne adapted his design to a much smaller pocket model for individual use by officers of artillery or infantry, receiving a patent in 1915.\n\nIn December 1932, the newly founded Silva Company of Sweden introduced its first baseplate or bearing compass that used a liquid-filled capsule to damp the swing of the magnetized needle. The liquid-damped Silva took only four seconds for its needle to settle in comparison to thirty seconds for the original version.\n\nIn 1933 Tuomas Vohlonen, a surveyor by profession, applied for a patent for a unique method of filling and sealing a lightweight celluloid compass housing or capsule with a petroleum distillate to dampen the needle and protect it from shock and wear caused by excessive motion. Introduced in a wrist-mount model in 1936 as the Suunto Oy \"Model M-311\", the new capsule design led directly to the lightweight liquid-filled field compasses of today.\n\nThe first gyroscope for scientific use was made by the French physicist Jean Bernard Léon Foucault (1819–1868) in 1852, who also named the device, while researching in the same line that led him to use the eponymous pendulum, for which he was awarded a Copley Medal by the Royal Society. The gyrocompass was patented in 1885 by Marinus Gerardus van den Bos in The Netherlands after continuous spinning was made possible by small electric motors, which were in turn a technological outcome of the discovery of magnetic induction. Yet only in 1906 was the German inventor Hermann Anschütz-Kaempfe (1872–1931) able to build the first practical gyrocompass. It had two major advantages over magnetic compasses: it indicated true north and was unaffected by ferromagnetic materials, such as the steel hull of ships. Thus, it was widely used in the warships of World War I and modern aircraft.\n\nThree compasses meant for establishing the meridian were described by Peter Peregrinus in 1269 (referring to experiments made before 1248) Late in the 13th century, al-Malik al-Ashraf of Yemen wrote a treatise on astrolabes, which included instructions and diagrams on using the compass to determine the meridian (khaṭṭ niṣf al-nahār) and Qibla. In 1300, a treatise written by the Egyptian astronomer and muezzin Ibn Simʿūn describes a dry compass for use as a \"Qibla indicator\" to find the direction to Mecca. Ibn Simʿūn's compass, however, did not feature a compass card nor the familiar glass box. In the 14th century, the Syrian astronomer and timekeeper Ibn al-Shatir (1304–1375) invented a timekeeping device incorporating both a universal sundial and a magnetic compass. He invented it for the purpose of finding the times of salat prayers.\n\nEvidence for the orientation of buildings by the means of a magnetic compass can be found in 12th-century Denmark: one fourth of its 570 Romanesque churches are rotated by 5–15 degrees clockwise from true east-west, thus corresponding to the predominant magnetic declination of the time of their construction. Most of these churches were built in the 12th century, indicating a fairly common usage of magnetic compasses in Europe by then.\n\nThe use of a compass as a direction finder underground was pioneered in the Tuscan mining town Massa where floating magnetic needles were employed for tunnelling, and for defining the claims of the various mining companies, as early as the 13th century. In the second half of the 15th century, the compass became standard equipment for Tyrolian miners. Shortly afterwards the first detailed treatise dealing with the underground use of compasses was published by a German miner Rülein von Calw (1463–1525).\n\nA sun compass uses the position of the Sun in the sky to determine the directions of the cardinal points, making allowance for the local latitude and longitude, time of day, equation of time, and so on. At fairly high latitudes, an analog-display watch can be used as a very approximate sun compass. A simple sundial can be used as a much better one. An automatic sun compass developed by Lt. Col. James Allason, a mechanised cavalry officer, was adopted by the British Army in India in 1938 for use in tanks and other armoured vehicles where the magnetic field was subject to distortion, affecting the standard issue prismatic compass. Cloudy skies prohibited its use in European theatres. A copy of the manual is preserved in the Imperial War Museum in London.\n\n"}
{"id": "1229416", "url": "https://en.wikipedia.org/wiki?curid=1229416", "title": "Hyperbolic motion", "text": "Hyperbolic motion\n\nIn geometry, hyperbolic motions are isometric automorphisms of a hyperbolic space. Under composition of mappings, the hyperbolic motions form a continuous group. This group is said to characterize the hyperbolic space. Such an approach to geometry was cultivated by Felix Klein in his Erlangen program. The idea of reducing geometry to its characteristic group was developed particularly by Mario Pieri in his reduction of the primitive notions of geometry to merely point and \"motion\".\n\nHyperbolic motions are often taken from inversive geometry: these are mappings composed of reflections in a line or a circle (or in a hyperplane or a hypersphere for hyperbolic spaces of more than two dimensions). To distinguish the hyperbolic motions, a particular line or circle is taken as the absolute. The proviso is that the absolute must be an invariant set of all hyperbolic motions. The absolute divides the plane into two connected components, and hyperbolic motions must \"not\" permute these components.\n\nOne of the most prevalent contexts for inversive geometry and hyperbolic motions is in the study of mappings of the complex plane by Möbius transformations. Textbooks on complex functions often mention two common models of hyperbolic geometry: the Poincaré half-plane model where the absolute is the real line on the complex plane, and the Poincaré disk model where the absolute is the unit circle in the complex plane.\nHyperbolic motions can also be described on the hyperboloid model of hyperbolic geometry.\n\nThis article exhibits these examples of the use of hyperbolic motions: the extension of the metric formula_1 to the half-plane, and in the location of a quasi-sphere of a hypercomplex number system.\n\nEvery motion (transformation or isometry) of the hyperbolic plane to itself can be realized as the composition of at most three reflections. In \"n\"-dimensional hyperbolic space, up to \"n\"+1 reflections might be required. (These are also true for Euclidean and spherical geometries, but the classification below is different.)\n\nAll the isometries of the hyperbolic plane can be classified into these classes:\n\nThe points of the Poincaré half-plane model HP are given in Cartesian coordinates as {(\"x\",\"y\"): \"y\" > 0} or in polar coordinates as {(\"r\" cos \"a\", \"r\" sin \"a\"): 0 < \"a\" < π, \"r\" > 0 }.\nThe hyperbolic motions will be taken to be a composition of three fundamental hyperbolic motions.\nLet \"p\" = (\"x,y\") or \"p\" = (\"r\" cos \"a\", \"r\" sin \"a\"), \"p\" ∈ HP. \n\nThe fundamental motions are:\nNote: the shift and dilation are mappings from inversive geometry composed of a pair of reflections in vertical lines or concentric circles respectively.\n\nConsider the triangle {(0,0),(1,0),(1,tan \"a\")}. Since 1 + tan\"a\" = sec\"a\", the length of the triangle hypotenuse is sec \"a\", where sec denotes the secant function. Set \"r\" = sec \"a\" and apply the third fundamental hyperbolic motion to obtain \"q\" = (\"r\" cos \"a\", \"r\" sin \"a\") where \"r\" = sec\"a\" = cos \"a\". Now\n\nso that \"q\" lies on the semicircle \"Z\" of radius ½ and center (½, 0). Thus the tangent ray at (1, 0) gets mapped to \"Z\" by the third fundamental hyperbolic motion. Any semicircle can be re-sized by a dilation to radius ½ and shifted to \"Z\", then the inversion carries it to the tangent ray. So the collection of hyperbolic motions permutes the semicircles with diameters on \"y\" = 0 sometimes with vertical rays, and vice versa. Suppose one agrees to measure length on vertical rays by using logarithmic measure:\n\nThen by means of hyperbolic motions one can measure distances between points on semicircles too: first move the points to \"Z\" with appropriate shift and dilation, then place them by inversion on the tangent ray where the logarithmic distance is known.\n\nFor \"m\" and \"n\" in HP, let \"b\" be the perpendicular bisector of the line segment connecting \"m\" and \"n\". If \"b\" is parallel to the abscissa, then \"m\" and \"n\" are connected by a vertical ray, otherwise \"b\" intersects the abscissa so there is a semicircle centered at this intersection that passes through \"m\" and \"n\". The set HP becomes a metric space when equipped with the distance \"d\"(\"m\",\"n\") for \"m\",\"n\" ∈ HP as found on the vertical ray or semicircle. One calls the vertical rays and semicircles the \"hyperbolic lines\" in HP.\nThe geometry of points and hyperbolic lines in HP is an example of a non-Euclidean geometry; nevertheless, the construction of the line and distance concepts for HP relies heavily on the original geometry of Euclid.\n\nConsider the disk D = {\"z\" ∈ C : \"z z\"* < 1 } in the complex plane C. The geometric plane of Lobachevsky can be displayed in D with circular arcs perpendicular to the boundary of D signifying \"hyperbolic lines\". Using the arithmetic and geometry of complex numbers, and Möbius transformations, there is the Poincaré disc model of the hyperbolic plane:\n\nSuppose \"a\" and \"b\" are complex numbers with \"a a\"* − \"b b\"* = 1. Note that\nso that |\"z\"| < 1 implies |(\"a\"z + \"b\"*)/(\"bz\" + \"a\"*)| < 1 . Hence the disk D is an invariant set of the Möbius transformation\nSince it also permutes the hyperbolic lines, we see that these transformations are motions of the D model of hyperbolic geometry. A complex matrix \nwith \"aa\"* − \"bb\"* = 1, which represents the Möbius transformation from the projective viewpoint, can be considered to be on the unit quasi-sphere in the ring of coquaternions.\n\n"}
{"id": "13547663", "url": "https://en.wikipedia.org/wiki?curid=13547663", "title": "Jacobsthal number", "text": "Jacobsthal number\n\nIn mathematics, the Jacobsthal numbers are an integer sequence named after the German mathematician Ernst Jacobsthal. Like the related Fibonacci numbers, they are a specific type of Lucas sequence formula_1 for which \"P\" = 1, and \"Q\" = −2—and are defined by a similar recurrence relation: in simple terms, the sequence starts with 0 and 1, then each following number is found by adding the number before it to twice the number before that. The first Jacobsthal numbers are:\n\nJacobsthal numbers are defined by the recurrence relation:\n\nThe next Jacobsthal number is also given by the recursion formula:\n\nor by:\n\nThe first recursion formula above is also satisfied by the powers of <nowiki>2</nowiki>.\n\nThe Jacobsthal number at a specific point in the sequence may be calculated directly using the closed-form equation:\n\nThe generating function for the Jacobsthal numbers is\n\nThe sum of the reciprocals of the Jacobsthal numbers is approximately 2.7186, slightly larger than e.\n\nJacobsthal-Lucas numbers represent the complementary Lucas sequence formula_7. They satisfy the same recurrence relation as Jacobsthal numbers but have different initial values: \n\nThe following Jacobsthal-Lucas number also satisfies:\n\nThe Jacobsthal-Lucas number at a specific point in the sequence may be calculated directly using the closed-form equation:\n\nThe first Jacobsthal-Lucas numbers are:\n"}
{"id": "31954655", "url": "https://en.wikipedia.org/wiki?curid=31954655", "title": "Jamshidian's trick", "text": "Jamshidian's trick\n\nJamshidian's trick is a technique for one-factor asset price models, which re-expresses an option on a portfolio of assets as a portfolio of options. It was developed by Farshid Jamshidian in 1989.\n\nThe trick relies on the following simple, but very useful mathematical observation. Consider a sequence of monotone (increasing) functions formula_1 of one real variable (which map onto formula_2), a random variable formula_3, and a constant formula_4. \n\nSince the function formula_5 is also increasing and maps onto formula_2, there is a unique solution formula_7 to the equation formula_8\n\nSince the functions formula_1 are increasing: \nformula_10\n\nIn financial applications, each of the random variables formula_11 represents an asset value, the number formula_12 is the strike of the option on the portfolio of assets. We can therefore express the payoff of an option on a portfolio of assets in terms of a portfolio of options on the individual assets formula_11 with corresponding strikes formula_14.\n\n"}
{"id": "39712387", "url": "https://en.wikipedia.org/wiki?curid=39712387", "title": "Linear function (calculus)", "text": "Linear function (calculus)\n\nIn calculus and related areas of mathematics, a linear function from the real numbers to the real numbers is a function whose graph (in Cartesian coordinates with uniform scales) is a line in the plane. \nThe characteristic property of linear functions is that when the input variable is changed, the change in the output is proportional to the change in the input.\n\nLinear functions are related to linear equations.\n\nA linear function is a polynomial function in which the variable has degree at most one:\nSuch a function is called \"linear\" because its graph, the set of all points formula_2 in the Cartesian plane, is a line. The coefficient \"a\" is called the \"slope\" of the function and of the line (see below).\nIf the slope is formula_3, this is a \"constant function\" formula_4 defining a horizontal line, which some authors exclude from the class of linear functions. With this definition, the degree of a linear polynomial would be exactly one, its graph a diagonal line neither vertical nor horizontal. However, we will not require formula_5 in this article, so constant functions will be considered linear.\n\nThe natural domain of a linear function formula_6, the set of allowed input values for \"x\", is the entire set of real numbers, formula_7. One can also consider such functions with \"x\" in an arbitrary field, taking the coefficients \"a,b\" in that field. \n\nThe graph formula_8 is a non-vertical line having exactly one intersection with the \"y\"-axis, its \"y\"-intercept point formula_9. The \"y\"-intercept value formula_10 is also called the \"initial value\" of formula_6. If formula_5, the graph is a non-horizontal line having exactly one intersection with the \"x\"-axis, the \"x\"-intercept point formula_13. The \"x\"-intercept value formula_14, the solution of the equation formula_15, is also called the \"root\" or \"zero\" of formula_6.\n\nThe slope of a nonvertical line is a number that measures how steeply the line is slanted (rise-over-run). If the line is the graph of the linear function , this slope is given by the constant . \n\nThe slope measures the constant rate of change of formula_6 per unit change in \"x\": whenever the input is increased by one unit, the output changes by units: formula_18, and more generally formula_19 for any number formula_20. If the slope is positive, formula_21, then the function formula_6 is increasing; if formula_23, then formula_6 is decreasing\n\nIn calculus, the derivative of a general function measures its rate of change. A linear function formula_1 has a constant rate of change equal to its slope , so its derivative is the constant function formula_26. \n\nThe fundamental idea of differential calculus is that any smooth function formula_6 (not necessarily linear) can be closely approximated near a given point formula_28 by a unique linear function. The derivative formula_29 is the slope of this linear function, and the approximation is: formula_30 for formula_31. The graph of the linear approximation is the tangent line of the graph formula_32 at the point formula_33. The derivative slope formula_29 generally varies with the point \"c\". Linear functions can be characterized as the only real functions whose derivative is constant: if formula_26 for all \"x\", then formula_1 for formula_37.\n\nA given linear function formula_6 can be written in several standard formulas displaying its various properties. The simplest is the \"slope-intercept form\": \nfrom which one can immediately see the slope \"a\" and the initial value formula_40, which is the \"y\"-intercept of the graph formula_32. \n\nGiven a slope \"a\" and one known value formula_42, we write the \"point-slope form\": \nIn graphical terms, this gives the line formula_32 with slope \"a\" passing through the point formula_45. \n\nThe \"two-point form\" starts with two known values formula_42 and formula_47. One computes the slope formula_48 and inserts this into the point-slope form: \nIts graph formula_32 is the unique line passing through the points formula_51. The equation formula_32 may also be written to emphasize the constant slope:\n\nLinear functions commonly arise from practical problems involving variables formula_54 with a linear relationship, that is, obeying a linear equation formula_55. If formula_56, one can solve this equation for \"y\", obtaining\nwhere we denote formula_58 and formula_59. That is, one may consider \"y\" as a dependent variable (output) obtained from the independent variable (input) \"x\" via a linear function: formula_60. In the \"xy\"-coordinate plane, the possible values of formula_61 form a line, the graph of the function formula_6. If formula_63 in the original equation, the resulting line formula_64 is vertical, and cannot be written as formula_32.\n\nThe features of the graph formula_60 can be interpreted in terms of the variables \"x\" and \"y\". The \"y\"-intercept is the initial value formula_10 at formula_68. The slope \"a\" measures the rate of change of the output \"y\" per unit change in the input \"x\". In the graph, moving one unit to the right (increasing \"x\" by 1) moves the \"y\"-value up by \"a\": that is, formula_69. Negative slope \"a\" indicates a decrease in \"y\" for each increase in \"x\".\n\nFor example, the linear function formula_70 has slope formula_71, \"y\"-intercept point formula_72, and \"x\"-intercept point formula_73.\n\nSuppose salami and sausage cost €6 and €3 per kilogram, and we wish to buy €12 worth. How much of each can we purchase? Letting \"x\" and \"y\" be the weights of salami and sausage, the total cost is: formula_74. Solving for \"y\" gives the point-slope form formula_70, as above. That is, if we first choose the amount of salami \"x\", the amount of sausage can be computed as a function formula_76. Since salami costs twice as much as sausage, adding one kilo of salami decreases the sausage by 2 kilos: formula_77, and the slope is −2. The \"y\"-intercept point formula_78 corresponds to buying only 4kg of sausage; while the \"x\"-intercept point formula_79 corresponds to buying only 2kg of salami. \n\nNote that the graph includes points with negative values of \"x\" or \"y\", which have no meaning in terms of the original variables (unless we imagine selling meat to the butcher). Thus we should restrict our function formula_6 to the domain formula_81. \n\nAlso, we could choose \"y\" as the independent variable, and compute \"x\" by the inverse linear function: formula_82 over the domain formula_83.\n\nIf the coefficient of the variable is not zero (), then a linear function is represented by a degree 1 polynomial (also called a \"linear polynomial\"), otherwise it is a constant function – also a polynomial function, but of zero degree.\n\nA straight line, when drawn in a different kind of coordinate system may represent other functions.\n\nFor example, it may represent an exponential function when its values are expressed in the logarithmic scale. It means that when is a linear function of , the function is exponential. With linear functions, increasing the input by one unit causes the output to increase by a fixed amount, which is the slope of the graph of the function. With exponential functions, increasing the input by one unit causes the output to increase by a fixed multiple, which is known as the base of the exponential function.\n\nIf \"both\" arguments and values of a function are in the logarithmic scale (i.e., when is a linear function of ), then the straight line represents a power law:\nOn the other hand, the graph of a linear function in terms of polar coordinates: \nis an Archimedean spiral if formula_86 and a circle otherwise.\n\n\n\n"}
{"id": "52737461", "url": "https://en.wikipedia.org/wiki?curid=52737461", "title": "List of Protein subcellular localization prediction tools", "text": "List of Protein subcellular localization prediction tools\n\nThis list of protein subcellular localisation prediction tools includes software, databases, and web services that are used for protein subcellular localization prediction.\n\nSome tools are included that are commonly used to infer location through predicted structural properties, such as signal peptide or transmembrane helices, and these tools output predictions of these features rather than specific locations. These software related to protein structure prediction may also appear in lists of protein structure prediction software.\n\n"}
{"id": "5971820", "url": "https://en.wikipedia.org/wiki?curid=5971820", "title": "List of mathematicians (N)", "text": "List of mathematicians (N)\n\n\n\n\n\n\n\n\n"}
{"id": "444250", "url": "https://en.wikipedia.org/wiki?curid=444250", "title": "List of numerical analysis topics", "text": "List of numerical analysis topics\n\nThis is a list of numerical analysis topics.\n\n\nError analysis (mathematics)\n\n\nNumerical linear algebra — study of numerical algorithms for linear algebra problems\n\n\n\nEigenvalue algorithm — a numerical algorithm for locating the eigenvalues of a matrix\n\n\nInterpolation — construct a function going through some given data points\n\nPolynomial interpolation — interpolation by polynomials\n\nSpline interpolation — interpolation by piecewise polynomials\n\nTrigonometric interpolation — interpolation by trigonometric polynomials\n\n\nApproximation theory\n\n\nRoot-finding algorithm — algorithms for solving the equation \"f\"(\"x\") = 0\n\nMathematical optimization — algorithm for finding maxima or minima of a given function\n\n\nLinear programming (also treats \"integer programming\") — objective function and constraints are linear\n\nConvex optimization\n\nNonlinear programming — the most general optimization problem in the usual framework\n\nOptimal control\n\nInfinite-dimensional optimization\n\n\n\n\n\nNumerical integration — the numerical evaluation of an integral\n\nNumerical methods for ordinary differential equations — the numerical solution of ordinary differential equations (ODEs)\n\nNumerical partial differential equations — the numerical solution of partial differential equations (PDEs)\n\nFinite difference method — based on approximating differential operators with difference operators\n\nFinite element method — based on a discretization of the space of solutions\nGradient discretisation method — based on both the discretization of the solution and of its gradient\n\n\n\n\n\n\n\nFor a large list of software, see the list of numerical analysis software.\n"}
{"id": "837875", "url": "https://en.wikipedia.org/wiki?curid=837875", "title": "Malliavin calculus", "text": "Malliavin calculus\n\nIn probability theory and related fields, Malliavin calculus is a set of mathematical techniques and ideas that extend the mathematical field of calculus of variations from deterministic functions to stochastic processes. In particular, it allows the computation of derivatives of random variables. Malliavin calculus is also called the stochastic calculus of variations. \n\nMalliavin calculus is named after Paul Malliavin whose ideas led to a proof that Hörmander's condition implies the existence and smoothness of a density for the solution of a stochastic differential equation; Hörmander's original proof was based on the theory of partial differential equations. The calculus has been applied to stochastic partial differential equations as well.\n\nThe calculus allows integration by parts with random variables; this operation is used in mathematical finance to compute the sensitivities of financial derivatives. The calculus has applications in, for example, stochastic filtering.\n\nMalliavin introduced Malliavin calculus to provide a stochastic proof that Hörmander's condition implies the existence of a density for the solution of a stochastic differential equation; Hörmander's original proof was based on the theory of partial differential equations. His calculus enabled Malliavin to prove regularity bounds for the solution's density. The calculus has been applied to stochastic partial differential equations.\n\nThe usual invariance principle for Lebesgue integration over the whole real line is that, for any real number ε and integrable function \"f\", the\nfollowing holds\n\nThis can be used to derive the integration by parts formula since, setting \"f\" = \"gh\", it implies\n\nA similar idea can be applied in stochastic analysis for the differentiation along a Cameron-Martin-Girsanov direction. Indeed, let formula_4 be a square-integrable predictable process and set\n\nIf formula_6 is a Wiener process, the Girsanov theorem then yields the following analogue of the invariance principle:\n\nDifferentiating with respect to ε on both sides and evaluating at ε=0, one obtains the following integration by parts formula:\n\nHere, the left-hand side is the Malliavin derivative of the random variable formula_9 in the direction formula_10 and the integral appearing on the right hand side should be interpreted as an Itô integral. This expression also remains true (by definition) if formula_11 is not adapted, provided that the right hand side is interpreted as a Skorokhod integral.\n\nOne of the most useful results from Malliavin calculus is the Clark-Ocone theorem, which allows the process in the martingale representation theorem to be identified explicitly. A simplified version of this theorem is as follows:\n\nFor formula_12 satisfying formula_13 which is Lipschitz and such that \"F\" has a strong derivative kernel, in the sense that\nfor formula_10 in \"C\"[0,1]\n\nthen\n\nwhere \"H\" is the previsible projection of \"F\"<nowiki>'</nowiki>(\"x\", (\"t\",1]) which may be viewed as the derivative of the function \"F\" with respect to a suitable parallel shift of the process \"X\" over the portion (\"t\",1] of its domain.\n\nThis may be more concisely expressed by\n\nMuch of the work in the formal development of the Malliavin calculus involves extending this result to the largest possible class of functionals \"F\" by replacing the derivative kernel used above by the \"Malliavin derivative\" denoted formula_18 in the above statement of the result. \n\nThe Skorokhod integral operator which is conventionally denoted δ is defined as the adjoint of the Malliavin derivative thus for u in the domain of the operator which is a subset of formula_19,\nfor F in the domain of the Malliavin derivative, we require\n\nwhere the inner product is that on formula_21 viz\n\nThe existence of this adjoint follows from the Riesz representation theorem for linear operators on Hilbert spaces.\n\nIt can be shown that if \"u\" is adapted then\n\nwhere the integral is to be understood in the Itô sense. Thus this provides a method of extending the Itô integral to non adapted integrands.\n\nThe calculus allows integration by parts with random variables; this operation is used in mathematical finance to compute the sensitivities of financial derivatives. The calculus has applications for example in stochastic filtering.\n\n\n\n"}
{"id": "32244285", "url": "https://en.wikipedia.org/wiki?curid=32244285", "title": "Max–min inequality", "text": "Max–min inequality\n\nIn mathematics, the max–min inequality is as follows: for any function \"f\": \"Z\" × \"W\" → ℝ,\n\nWhen equality holds one says that \"f\", \"W\" and \"Z\" satisfies a strong max–min property (or a saddle-point property). As the function \"f\"(\"z\",\"w\")=sin(\"z\"+\"w\") illustrates, this equality does not always hold. A theorem giving conditions on \"f\", \"W\" and \"Z\" in order to guarantee the saddle point property is called a minimax theorem.\n\nDefine formula_2. \n\nformula_3\n\nformula_4\n\nformula_5\n\nformula_6\n\n"}
{"id": "22411175", "url": "https://en.wikipedia.org/wiki?curid=22411175", "title": "Michael Aizenman", "text": "Michael Aizenman\n\nMichael Aizenman (born 28 August 1945 in Nizhny Tagil, Russia) is a mathematician and a physicist at Princeton University, working in the fields of mathematical physics, statistical mechanics, functional analysis and probability theory.\n\nThe highlights of his work include: the triviality of a class of scalar quantum field theories in more than four dimensions; a description of the phase transition in the Ising model in three and more dimensions; the sharpness of the phase transition in percolation theory; a method for the study of spectral and dynamical localization for random Schrödinger operators; and insights concerning conformal invariance in two-dimensional percolation.\n\nAizenman was an undergraduate at the Hebrew University of Jerusalem. He was awarded his PhD in 1975 at Yeshiva University (Belfer Graduate School of Science), New York City, with advisor Joel Lebowitz. After postdoctoral appointments at the Courant Institute of Mathematical Sciences of New York University (1974–75), and Princeton University (1975–1977), with Elliott H. Lieb, he was appointed Assistant Professor at Princeton. In 1982 he moved to Rutgers University as Associate Professor and then full Professor. In 1987 he moved to the Courant Institute and in 1990 returned to Princeton as Professor of Mathematics and Physics. He was several times a visiting scholar at the Institute for Advanced Study, in 1984-85, 1991–92, and 1997–98, and is a regular Visiting Scholar at the Weizmann Institute of Science.\n\n\nM. Aizenman received honorary degrees (DHC) from Université de Cergy-Pontoise (2009) and Technion (2018), and is a member of \nNational Academy of Sciences (1997), American Academy of Arts and Sciences (2017), and Academia Europea (2016). \n\nDuring 2001-2012 he served as the editor-in-chief of Communications in Mathematical Physics. \n\n\n"}
{"id": "996278", "url": "https://en.wikipedia.org/wiki?curid=996278", "title": "Molecular geometry", "text": "Molecular geometry\n\nMolecular geometry is the three-dimensional arrangement of the atoms that constitute a molecule. It includes the general shape of the molecule as well as bond lengths, bond angles, torsional angles and any other geometrical parameters that determine the position of each atom.\n\nMolecular geometry influences several properties of a substance including its reactivity, polarity, phase of matter, color, magnetism and biological activity. The angles between bonds that an atom forms depend only weakly on the rest of molecule, i.e. they can be understood as approximately local and hence transferable properties.\n\nThe molecular geometry can be determined by various spectroscopic methods and diffraction methods. IR, microwave and Raman spectroscopy can give information about the molecule geometry from the details of the vibrational and rotational absorbance detected by these techniques. X-ray crystallography, neutron diffraction and electron diffraction can give molecular structure for crystalline solids based on the distance between nuclei and concentration of electron density. Gas electron diffraction can be used for small molecules in the gas phase. NMR and FRET methods can be used to determine complementary information including relative distances,\ndihedral angles,\n\nangles, and connectivity. Molecular geometries are best determined at low temperature because at higher temperatures the molecular structure is averaged over more accessible geometries (see next section). Larger molecules often exist in multiple stable geometries (conformational isomerism) that are close in energy on the potential energy surface. Geometries can also be computed by ab initio quantum chemistry methods to high accuracy. The molecular geometry can be different as a solid, in solution, and as a gas.\n\nThe position of each atom is determined by the nature of the chemical bonds by which it is connected to its neighboring atoms. The molecular geometry can be described by the positions of these atoms in space, evoking bond lengths of two joined atoms, bond angles of three connected atoms, and torsion angles (dihedral angles) of three consecutive bonds.\n\nSince the motions of the atoms in a molecule are determined by quantum mechanics, one\nmust define \"motion\" in a quantum mechanical way. The overall (external) quantum mechanical motions translation and rotation hardly change the geometry of the molecule. (To some extent rotation influences\nthe geometry via Coriolis forces and centrifugal distortion, but this is negligible for the present discussion.)\nIn addition to translation and rotation, a third type of motion is molecular vibration, which corresponds to internal motions of the atoms such as bond stretching and bond angle variation. The molecular vibrations are harmonic (at least to good approximation), and the atoms oscillate about their equilibrium positions, even at the absolute zero of temperature. At absolute zero all atoms are in their vibrational ground state and show zero point quantum mechanical motion, so that the wavefunction of a single vibrational mode is not a sharp peak, but an exponential of finite width (the wavefunction for \"n\" = 0 depicted in the article on the quantum harmonic oscillator). At higher temperatures the vibrational modes may be thermally excited (in a classical interpretation one expresses this by stating that \"the molecules will vibrate faster\"), but they oscillate still around the recognizable geometry of the molecule.\n\nTo get a feeling for the probability that the vibration of molecule may be thermally excited,\nwe inspect the Boltzmann factor formula_1,\nwhere formula_2 is the excitation energy of the vibrational mode, formula_3 the Boltzmann constant and formula_4 the absolute temperature. At 298 K (25 °C), typical values for the Boltzmann factor β are:\nβ = 0.089 for ΔE = 500 cm ; β = 0.008 for ΔE = 1000 cm ; β = 7×10 for ΔE = 1500 cm. (The reciprocal centimeter is an energy unit that is commonly used in infrared spectroscopy; 1 cm corresponds to 1.23984×10 eV). When an excitation energy is 500 cm, then about 8.9 percent of the molecules are thermally excited at room temperature. To put this in perspective: the lowest excitation vibrational energy in water is the bending mode (about 1600 cm). Thus, at room temperature less than 0.07 percent of all the molecules of a given amount of water will vibrate faster than at absolute zero.\n\nAs stated above, rotation hardly influences the molecular geometry. But, as a quantum mechanical motion, it is thermally excited at relatively (as compared to vibration) low temperatures. From a classical point of view it can be stated that at higher temperatures more molecules will rotate faster,\nwhich implies that they have higher angular velocity and angular momentum. In quantum mechanical language: more eigenstates of higher angular momentum become thermally populated with rising temperatures. Typical rotational excitation energies are on the order of a few cm. The results of many spectroscopic experiments are broadened because they involve an averaging over rotational states. It is often difficult to extract geometries from spectra at high temperatures, because the number of rotational states probed in the experimental averaging increases with increasing temperature. Thus, many spectroscopic observations can only be expected to yield reliable molecular geometries at temperatures close to absolute zero, because at higher temperatures too many higher rotational states are thermally populated.\n\nMolecules, by definition, are most often held together with covalent bonds involving single, double, and/or triple bonds, where a \"bond\" is a shared pair of electrons (the other method of bonding between atoms is called ionic bonding and involves a positive cation and a negative anion).\n\nMolecular geometries can be specified in terms of bond lengths, bond angles and torsional angles. The bond length is defined to be the average distance between the nuclei of two atoms bonded together in any given molecule. A bond angle is the angle formed between three atoms across at least two bonds. For four atoms bonded together in a chain, the torsional angle is the angle between the plane formed by the first three atoms and the plane formed by the last three atoms.\n\nThere exists a mathematical relationship among the bond angles for one central atom and four peripheral atoms (labeled 1 through 4) expressed by the following determinant. This constraint removes one degree of freedom from the choices of (originally) six free bond angles to leave only five choices of bond angles. (Note that the angles formula_5, formula_6, formula_7, and formula_8 are always zero and that this relationship can be modified for a different number of peripheral atoms by expanding/contracting the square matrix.)\n\nMolecular geometry is determined by the quantum mechanical behavior of the electrons. Using the valence bond approximation this can be understood by the type of bonds between the atoms that make up the molecule. When atoms interact to form a chemical bond, the atomic orbitals of each atom are said to combine in a process called orbital hybridisation. The two most common types of bonds are sigma bonds (usually formed by hybrid orbitals) and pi bonds (formed by unhybridized p orbitals for atoms of main group elements). The geometry can also be understood by molecular orbital theory where the electrons are delocalised.\n\nAn understanding of the wavelike behavior of electrons in atoms and molecules is the subject of quantum chemistry.\n\nIsomers are types of molecules that share a chemical formula but have different geometries, resulting in different properties:\n\nA bond angle is the geometric angle between two adjacent bonds. Some common shapes of simple molecules include:\n\nThe bond angles in the table below are ideal angles from the simple VSEPR theory, followed by the actual angle for the example given in the following column where this differs. For many cases, such as trigonal pyramidal and bent, the actual angle for the example differs from the ideal angle, and examples differ by different amounts. For example, the angle in HS (92°) differs from the tetrahedral angle by much more than the angle for HO (104.48°) does.\n\n\nThe greater the amount of lone pairs contained in a molecule the smaller the angles between the atoms of that molecule. The VSEPR theory predicts that lone pairs repel each other, thus pushing the different atoms away from them.\n\n\n"}
{"id": "56043115", "url": "https://en.wikipedia.org/wiki?curid=56043115", "title": "National Institute of Statistical Sciences", "text": "National Institute of Statistical Sciences\n\nThe National Institute of Statistical Sciences (NISS) is an American institute that researches statistical science and quantitative analysis. \n\nIn 1985 the National Science Foundation funded a proposal by the Institute of Mathematical Statistics (IMS) to assess the status of cross-disciplinary statistical research and to make recommendations for its future. The IMS formed a panel consisted of twelve members from statistics, pure and applied mathematics, chemistry, engineering, computer science, and public affairs, including Ingram Olkin (Co-Chair), Jerome Sacks (Co-Chair), Alfred Blumstein, Amos Eddy, Bill Eddy, Peter C. Jurs, William Kruskal, Thomas Kutz, Gary C. McDonald, Ronald Peierls, Paul Shaman, and William Spurgeon. In 1990, the panel published a report on Cross-Disciplinary Research in the Statistical Sciences that led to the founding of the National Institute of Statistical Sciences.\n\nThe National Institute of Statistical Sciences was established in 1990 and located in Research Triangle Park, North Carolina by the American Statistical Association, the International Biometric Society, the Institute of Mathematical Statistics, Duke University, North Carolina State University, and the University of North Carolina at Chapel Hill and RTI International (formerly Research Triangle Institute). Dan Horvitz of RTI became the interim director. Jerome Sacks became the founding director in 1991. Alan F. Karr joined NISS as the associate director in 1992 and became the director in 2000. Nell Sedransk appointed as the associate director in 2005 and became the director in 2015. James L. Rosenberger became the director of NISS in 2017. \n\nIn 1993, the first NISS postdoctoral fellows joined. There are now nearly 80 of former NISS postdoctoral fellows around the world and in various organizations in each sector: academia, government, and industry. Here is a list of notable alumni:\n\n\nIn 2000, the NISS affiliates program was created to address challenges arising in government and industry. In 2005, the NISS affiliates program was recognized by the American Statistical Association with the Statistical Partnerships among Academia, Industry, and Government (SPAIG) Award.\n\nIn 2002, the Statistical and Applied Mathematical Sciences Institute was funded by the National Science Foundation, and it was partnered with Duke University, North Carolina State University, the University of North Carolina at Chapel Hill, and the National Institute of Statistical Sciences.\n\nFrom 2011 to 2018, the NISS and Duke University collaborated on the Triangle Census Research Network (TCRN), one of eight research nodes that worked on the National Census Research Network (NCRN). In 2017, the NCRN was recognized by the American Statistical Association with the Statistical Partnerships among Academia, Industry, and Government (SPAIG) Award.\n\nThe Jerome Sacks Award for Outstanding Cross-Disciplinary Research was created in 2001 in honor of Jerome Sacks, the founding director of NISS. The following are the winners of the award:\n\n\nThe NISS Distinguished Service Awards were established by the Board of Trustees in 2005 to recognize individuals who have given extraordinary service that significantly advances the mission of NISS. \n\n\nThe Writing Workshop for Junior Researchers in Statistics and Data Science has been organized by the National Institute of Statistical Sciences from 2007 through 2016 and 2018. It has been led by Nell Sedransk and Keith Crank. It is frequently co-sponsored by the American Statistical Association, the Institute of Mathematical Statistics, the Eastern North American Region of the International Biometric Society, the Statistical Society of Canada, the International Chinese Statistical Association, the International Indian Statistical Association, the Korean International Statistics Society, and the National Science Foundation. The writing workshop provides individual hands-on guidance on how to write journal articles and funding proposals for junior researchers in statistics, biostatistics and data science.\n\nThe following are the senior mentors of NISS Writing Workshop. (Numbers indicate that the person has assisted in a previous Writing Workshop and which year(s).) \n\nThe success of NISS writing workshops is partially evident in the success of workshop graduates. Many workshop graduates are serving on the editorial boards of major statistical and biostatistical journals, including Annals of Statistics, Journal of the American Statistical Association, Journal of the Royal Statistical Society, Technometrics, Journal of Computational and Graphical Statistics, Computational Statistics & Data Analysis, Journal of Multivariate Analysis, Bernoulli, Statistica Sinica, Electronic Journal of Statistics, Journal of Statistical Planning and Inference, Statistics and Its Interface, Journal of Statistical Computation and Simulation, Statistics in Medicine and Statistical Communications in Infectious Diseases.\n"}
{"id": "14643313", "url": "https://en.wikipedia.org/wiki?curid=14643313", "title": "National Office of Statistics", "text": "National Office of Statistics\n\nThe National Office of Statistics (NOS, , ONS, ) is the Algerian ministry charged with the collection and publication of statistics related to the economy, population, and society of Algeria at national and local levels. Its head office is in Algiers.\n\n"}
{"id": "17590563", "url": "https://en.wikipedia.org/wiki?curid=17590563", "title": "Network dynamics", "text": "Network dynamics\n\nNetwork dynamics is a research field for the study of networks whose status changes in time. The dynamics may refer to the structure of connections of the units of a network, to the collective internal state of the network, or both. The networked systems could be from the fields of biology, chemistry, physics, sociology, economics, computer science, etc. Networked systems are typically characterized as complex systems consisting of many units coupled by specific, potentially changing, interaction topologies.\n\nFor a dynamical systems' approach to discrete network dynamics, see sequential dynamical system.\n\n"}
{"id": "7857636", "url": "https://en.wikipedia.org/wiki?curid=7857636", "title": "Newick format", "text": "Newick format\n\nIn mathematics, Newick tree format (or Newick notation or New Hampshire tree format) is a way of representing graph-theoretical trees with edge lengths using parentheses and commas. It was adopted by James Archie, William H. E. Day, Joseph Felsenstein, Wayne Maddison, Christopher Meacham, F. James Rohlf, and David Swofford, at two meetings in 1986, the second of which was at Newick's restaurant in Dover, New Hampshire, US. The adopted format is a generalization of the format developed by Meacham in 1984 for the first tree-drawing programs in Felsenstein's PHYLIP package.\n\nThe following tree:\n\ncould be represented in Newick format in several ways \n\nNewick format is typically used for tools like PHYLIP and is a minimal definition for a phylogenetic tree.\n\nWhen an \"unrooted\" tree is represented in Newick notation, an arbitrary node is chosen as its root. Whether rooted or unrooted, typically a tree's representation is rooted on an internal node and it is rare (but legal) to root a tree on a leaf node. \n\nA \"rooted binary tree\" that is rooted on an internal node has exactly two immediate descendant nodes for each internal node.\nAn \"unrooted binary\" tree that is rooted on an arbitrary internal node has exactly three immediate descendant nodes for the root node, and each other internal node has exactly two immediate descendant nodes.\nA \"binary tree rooted from a leaf\" has at most one immediate descendant node for the root node, and each internal node has exactly two immediate descendant nodes.\n\nA grammar for parsing the Newick format:\n\n Tree: The full input Newick Format for a single tree\n\nNote, \"|\" separates alternatives.\n\nWhitespace (spaces, tabs, carriage returns, and linefeeds) within \"number\" is prohibited. Whitespace within \"string\" is often prohibited. Whitespace elsewhere is ignored. Sometimes the Name \"string\" must be of a specified fixed length; otherwise the punctuation characters from the grammar (semicolon, parentheses, comma, and colon) are prohibited. The Tree --> Branch \";\" production makes the entire tree descendant from nowhere, which can be nonsensical, and is sometimes prohibited.\n\nNote that when a tree having more than one leaf is rooted from one of its leaves, a representation that is rarely seen in practice, the root leaf is characterized as an Internal node by the above grammar. Generally, a \"root node\" labeled as Internal should be construed as a leaf if and only if it has exactly one Branch in its BranchSet. One can make a grammar that formalizes this distinction by replacing the above Tree production rule with\n\nThe first RootLeaf production is for a tree with exactly one leaf. The second RootLeaf production is for rooting a tree from one of its two or more leaves.\n\n\n"}
{"id": "13021753", "url": "https://en.wikipedia.org/wiki?curid=13021753", "title": "Ostrowski–Hadamard gap theorem", "text": "Ostrowski–Hadamard gap theorem\n\nIn mathematics, the Ostrowski–Hadamard gap theorem is a result about the analytic continuation of complex power series whose non-zero terms are of orders that have a suitable \"gap\" between them. Such a power series is \"badly behaved\" in the sense that it cannot be extended to be an analytic function anywhere on the boundary of its disc of convergence. The result is named after the mathematicians Alexander Ostrowski and Jacques Hadamard.\n\nLet 0 < \"p\" < \"p\" < ... be a sequence of integers such that, for some \"λ\" > 1 and all \"j\" ∈ N,\n\nLet (\"α\") be a sequence of complex numbers such that the power series\n\nhas radius of convergence 1. Then no point \"z\" with |\"z\"| = 1 is a regular point for \"f\", i.e. \"f\" cannot be analytically extended from the open unit disc \"D\" to any larger open set including even a single point of the boundary of \"D\".\n\n"}
{"id": "667304", "url": "https://en.wikipedia.org/wiki?curid=667304", "title": "Over-the-counter (finance)", "text": "Over-the-counter (finance)\n\nOver-the-counter (OTC) or off-exchange trading is done directly between two parties, without the supervision of an exchange. It is contrasted with exchange trading, which occurs via exchanges. A stock exchange has the benefit of facilitating liquidity, providing transparency, and maintaining the current market price. In an OTC trade, the price is not necessarily published for the public.\n\nOTC trading, as well as exchange trading, occurs with commodities, financial instruments (including stocks), and derivatives of such products. Products traded on the exchange must be well standardized. This means that exchanged deliverables match a narrow range of quantity, quality, and identity which is defined by the exchange and identical to all transactions of that product. This is necessary for there to be transparency in trading. The OTC market does not have this limitation. They may agree on an unusual quantity, for example. In OTC, market contracts are bilateral (i.e. the contract is only between two parties), and each party could have credit risk concerns with respect to the other party. The OTC derivative market is significant in some asset classes: interest rate, foreign exchange, stocks, and commodities.\n\nIn 2008 approximately 16 percent of all U.S. stock trades were \"off-exchange trading\"; by April 2014 that number increased to about 40 percent. Although the notional amount outstanding of OTC derivatives in late 2012 had declined 3.3% over the previous year, the volume of cleared transactions at the end of 2012 totalled US$346.4 trillion. \"The Bank for International Settlements statistics on OTC derivatives markets showed that notional amounts outstanding totalled $693 trillion at the end of June 2013... The gross market value of OTC derivatives – that is, the cost of replacing all outstanding contracts at current market prices – declined between end-2012 and end-June 2013, from $25 trillion to $20 trillion.\"\n\nIn the United States, over-the-counter trading in stock is carried out by market makers using inter-dealer quotation services such as OTC Link (a service offered by OTC Markets Group) and the OTC Bulletin Board (OTCBB, operated by FINRA). The OTCBB licenses the services of OTC Link for their OTCBB securities. Although exchange-listed stocks can be traded OTC on the third market, it is rarely the case. Usually OTC stocks are not listed nor traded on exchanges, and vice versa. Stocks quoted on the OTCBB must comply with certain limited U.S. Securities and Exchange Commission (SEC) reporting requirements. The SEC imposes more stringent financial and reporting requirements on other OTC stocks, specifically the OTCQX stocks (traded through the OTC Market Group Inc). Other OTC stocks have no reporting requirements, for example Pink Sheets securities and \"gray market\" stocks.\n\nSome companies, with Wal-Mart as one of the largest, began trading as OTC stocks and eventually upgraded to a listing on fully regulated market. By 1969 Wal-Mart Stores Inc. was incorporated. In 1972, with stores in five states, including Arkansas, Kansas, Louisiana, Oklahoma and Missouri, Wal-Mart began trading as over-the-counter (OTC) stocks. By 1972 Walmart had earned over US$1 billion in sales — the fastest company to ever accomplish this. In 1972 Wal-Mart was listed on the New York Stock Exchange (NYSE) under the ticker symbol WMT.\n\nAn over-the-counter is a bilateral contract in which two parties (or their brokers or bankers as intermediaries) agree on how a particular trade or agreement is to be settled in the future. It is usually from an investment bank to its clients directly. Forwards and swaps are prime examples of such contracts. It is mostly done online or by telephone. For derivatives, these agreements are usually governed by an International Swaps and Derivatives Association agreement. This segment of the OTC market is occasionally referred to as the \"Fourth Market.\" Critics have labelled the OTC market as the \"dark market\" because prices are often unpublished and unregulated.\n\nOver-the-counter derivatives are especially important for hedging risk in that they can be used to create a \"perfect hedge.\" With exchange traded contracts, standardization does not allow for as much flexibility to hedge risk because the contract is a one-size-fits-all instrument. With OTC derivatives, though, a firm can tailor the contract specifications to best suit its risk exposure.\n\nOTC derivatives can lead to significant risks. Especially counterparty risk has gained particular emphasis due to the credit crisis in 2007. Counterparty risk is the risk that a counterparty in a derivatives transaction will default prior to expiration of the trade and will not make the current and future payments required by the contract. There are many ways to limit counterparty risk. One of them focuses on controlling credit exposure with diversification, netting, collateralisation and hedging. Central counterparty clearing of OTC trades has become more common in recent years, with regulators placing pressure on the OTC markets to clear and display trades openly.\n\nIn their market review published in 2010 the International Swaps and Derivatives Association examined OTC Derivative Bilateral Collateralization Practice as one way of mitigating risk.\n\nOTC derivatives are significant part of the world of global finance. The OTC derivatives markets grew exponentially from 1980 through 2000. This expansion has been driven by interest rate products, foreign exchange instruments and credit default swaps. The notional outstanding of OTC derivatives markets rose throughout the period and totalled approximately US$601 trillion at December 31, 2010.\n\nIn their 2000 paper by Schinasi et al. published by the International Monetary Fund in 2001, the authors observed that the increase in OTC derivatives transactions would have been impossible \"without the dramatic advances in information and computer technologies\" that occurred from 1980 to 2000. During that time, major internationally active financial institutions significantly increased the share of their earnings from derivatives activities. These institutions manage portfolios of derivatives involving tens of thousands of positions and aggregate global turnover over $1 trillion. At that time prior to the financial crisis of 2008, the OTC market was an informal network of bilateral counterparty relationships and dynamic, time-varying credit exposures whose size and distribution tied to important asset markets. International financial institutions increasingly nurtured the ability to profit from OTC derivatives activities and financial markets participants benefitted from them. In 2000 the authors acknowledged that the growth in OTC transactions \"in many ways made possible, the modernization of commercial and investment banking and the globalization of finance.\" However, in September, an IMF team led by Mathieson and Schinasi cautioned that \"episodes of turbulence\" in the late 1990s \"revealed the risks posed to market stability originated in features of OTC derivatives instruments and markets.\n\nThe NYMEX has created a clearing mechanism for a slate of commonly traded OTC energy derivatives which allows counterparties of many bilateral OTC transactions to mutually agree to transfer the trade to ClearPort, the exchange's clearing house, thus eliminating credit and performance risk of the initial OTC transaction counterparts.\n\n\n\n"}
{"id": "13844097", "url": "https://en.wikipedia.org/wiki?curid=13844097", "title": "Packing dimension", "text": "Packing dimension\n\nIn mathematics, the packing dimension is one of a number of concepts that can be used to define the dimension of a subset of a metric space. Packing dimension is in some sense dual to Hausdorff dimension, since packing dimension is constructed by \"packing\" small open balls inside the given subset, whereas Hausdorff dimension is constructed by covering the given subset by such small open balls. The packing dimension was introduced by C. Tricot Jr. in 1982.\n\nLet (\"X\", \"d\") be a metric space with a subset \"S\" ⊆ \"X\" and let \"s\" ≥ 0. The \"s\"-dimensional packing pre-measure of \"S\" is defined to be\n\nUnfortunately, this is just a pre-measure and not a true measure on subsets of \"X\", as can be seen by considering dense, countable subsets. However, the pre-measure leads to a \"bona fide\" measure: the \"s\"-dimensional packing measure of \"S\" is defined to be\n\ni.e., the packing measure of \"S\" is the infimum of the packing pre-measures of countable covers of \"S\".\n\nHaving done this, the packing dimension dim(\"S\") of \"S\" is defined analogously to the Hausdorff dimension:\n\nThe following example is the simplest situation where Hausdorff and packing dimensions may differ.\n\nFix a sequence formula_4 such that formula_5 and formula_6. Define inductively a nested sequence formula_7 of compact subsets of the real line as follows: Let formula_8. For each connected component of formula_9 (which will necessarily be an interval of length formula_10), delete the middle interval of length formula_11, obtaining two intervals of length formula_12, which will be taken as connected components of formula_13. Next, define formula_14. Then formula_15 is topologically a Cantor set (i.e., a compact totally disconnected perfect space). For example, formula_15 will be the usual middle-thirds Cantor set if formula_17.\n\nIt is possible to show that the Hausdorff and the packing dimensions of the set formula_15 are given respectively by:\n\nIt follows easily that given numbers formula_20, one can choose a sequence formula_4 as above such that the associated (topological) Cantor set formula_15 has Hausdorff dimension formula_23 and packing dimension formula_24.\n\nOne can consider dimension functions more general than \"diameter to the \"s\"\": for any function \"h\" : [0, +∞) → [0, +∞], let the packing pre-measure of \"S\" with dimension function \"h\" be given by\n\nand define the packing measure of \"S\" with dimension function \"h\" by\n\nThe function \"h\" is said to be an exact (packing) dimension function for \"S\" if \"P\"(\"S\") is both finite and strictly positive.\n\n\nNote, however, that the packing dimension is \"not\" equal to the box dimension. For example, the set of rationals Q has box dimension one and packing dimension zero.\n\n"}
{"id": "316042", "url": "https://en.wikipedia.org/wiki?curid=316042", "title": "Partisan game", "text": "Partisan game\n\nIn combinatorial game theory, a game is partisan if it is not impartial. That is, some moves are available to one player and not to the other.\n\nMost games are partisan. For example, in chess, only one player can move the white pieces. More strongly, when analyzed using combinatorial game theory, many chess positions have values that cannot be expressed as the value of an impartial game, for instance when one side has a number of extra tempos that can be used to put the other side into zugzwang.\n\nPartisan games are more difficult to analyze than impartial games, as the Sprague–Grundy theorem does not apply. However, the application of combinatorial game theory to partisan games allows the significance of \"numbers as games\" to be seen, in a way that is not possible with impartial games.\n"}
{"id": "10163390", "url": "https://en.wikipedia.org/wiki?curid=10163390", "title": "Picone identity", "text": "Picone identity\n\nIn the field of ordinary differential equations, the Picone identity, named after Mauro Picone, is a classical result about homogeneous linear second order differential equations. Since its inception in 1910 it has been used with tremendous success in association with an almost immediate proof of the Sturm comparison theorem, a theorem whose proof took up many pages in Sturm's original memoir of 1836. It is also useful in studying the oscillation of such equations and has been generalized to other type of differential equations and difference equations.\n\nThe Picone identity is used to prove the Sturm comparison theorem (see also Sturm–Picone comparison theorem).\n\nSuppose that \"u\" and \"v\" are solutions of the two homogeneous linear second order differential equations in self-adjoint form\nand\nThen, for all \"x\" with \"v\"(\"x\") ≠ 0, the following identity holds\n\nformula_4\n\nformula_5\n\n"}
{"id": "33639818", "url": "https://en.wikipedia.org/wiki?curid=33639818", "title": "Project SEED", "text": "Project SEED\n\nProject SEED is a mathematics education program which works in school districts across the United States. Project SEED is a nonprofit organization that works in partnership with school districts, universities, foundations, and corporations to teach advanced math to elementary and middle school students as a supplement to their regular math instruction. Project SEED also provides professional development for classroom teachers. Founded in 1963 by William F. Johntz, its primary goal is to use mathematics to increase the educational options of low-achieving, at-risk students.\n\nProject SEED is primarily a mathematics instruction program delivered to intact classes of elementary and middle school students, many from low-income backgrounds, to better prepare them for high school and college math. SEED Instruction utilizes the Socratic method, in which instructors use a question-and-answer approach to guide students to the discovery of mathematical principles. \n\nThe SEED instructors are math subject specialists, with degrees in mathematics or math-based sciences, who use a variety of techniques including hand and arm signals to encourage high levels of involvement, focus and feedback from students of all achievement levels. The approach is intended to encourage active student learning, develop critical thinking, and strengthen articulation skills. The program also emphasizes assessment of student learning and adaptation of instruction to accommodate different math ability levels.\n\nProject SEED curriculum includes topics from advanced mathematics, such as advanced algebra, pre-calculus, group theory, number theory, calculus, and geometry. SEED instruction is supplemental to the regular math program. While teaching students, Project SEED Mathematics Specialists simultaneously provide professional development training for classroom teachers, through modeling and coaching in its instructional strategies.\n\nFounded by math teacher and psychologist William Johntz in 1963 to improve the educational outcomes of low-income and minority students, Project SEED is currently run by CEO and National Director Hamid Ebrahimi, and continues to provide direct instruction and professional development for students and teachers across the U.S.\n\nProject SEED started as a result of Johntz teaching a remedial math class at Berkeley High School (Berkeley, California) in 1963. Frustrated by the failure of standard remediation to improve the basic math skills of his students, he began teaching them algebra using a Socratic, question-and-answer technique. They responded well to this new material that allowed them to think conceptually about mathematics, but since they were already in high school, there was little time left for them to turn around their academic careers.\n\nJohntz began using his free periods to try the same strategies to teach Algebra in a nearby elementary school. These fifth and sixth graders responded with enthusiasm to succeeding in the study of a high school subject. Also, this exploration of advanced concepts gave Johntz a chance to revisit and reinforce the grade-level curriculum.\n\nGraduate students and faculty from U.C. Berkeley soon joined Johntz in other Berkeley schools. The program spread through presentations for school districts, corporations and conferences, and became a component of the Miller Mathematics Improvement Program, a program funded statewide in California from 1968 to 1970. Many of the early instructors were university faculty, graduate students, and corporate volunteers.\nProject SEED became a non-profit, tax-exempt corporation in Michigan in 1970 where state funding brought the program to ten different cities between 1970 and 1975. When it was founded in 1963, the name Project S.E.E.D. was an acronym for “Special Elementary Education for the Disadvantaged.” The program was reincorporated in California in 1987 as Project SEED, Inc. dropping the acronym. This was done primarily to avoid confusion with “Special Education” which had taken on a specific meaning.\n\nOver the years Project SEED has operated in a number of cities and states with funding from state governments, federal grants, school districts, and foundations and corporations. From 1982 through 2002, a district funded program in Dallas, Texas reached tens of thousands of students and hundreds of teachers in dozens of schools. The series of longitudinal studies done by the district evaluation department during that time constitutes the most thorough examination of the effectiveness of Project SEED. Students in identified schools received a semester of Project SEED instruction for three consecutive years beginning in the fourth grade, a program design that is now regarded as the preferred model. District teachers working in kindergarten through twelfth grade classrooms received workshops, in-class modeling, and coaching from SEED staff as a part of the Urban Systemic Initiative that was implemented in the Dallas & Detroit school districts in the mid 1990s. The current Project SEED professional development program is based on this model. The Dallas & Detroit experiences continue to inform much of what Project SEED is today.\n\nHundreds of articles about Project SEED have appeared in newspapers and magazines as well as a number of academic books about successful intervention programs. Many former SEED instructors have gone on to make further important contributions to the field of mathematics education. Currently, Project SEED operates programs in California, Michigan, Indiana, Maryland, Pennsylvania, New Jersey, North Carolina, and Washington state.\n\nLongitudinal evaluations over a number of years in different locations with different instructors demonstrate that: Project SEED instruction has a positive impact on immediate mathematics achievement scores, Project SEED instruction has a long-term impact on mathematics achievement, and Project SEED students take more high-level mathematics courses in secondary schools.\n\nThe following organizations have recognized Project SEED as an effective mathematics education program:\n\n\n"}
{"id": "9679418", "url": "https://en.wikipedia.org/wiki?curid=9679418", "title": "Pseudomedian", "text": "Pseudomedian\n\nIn statistics, the pseudomedian is a measure of centrality for data-sets and populations defined as the median of all of the midpoints of pairs of observations. It agrees with the median for symmetric data-sets or populations. In mathematical statistics, the pseudomedian is also a location parameter for probability distributions.\n\nIn descriptive statistics, the pseudomedian of a data-set is measure of centrality, similar to a sample median. Other centrality statistics include the sample mean and a mode. \n\nIn inferential statistics, the pseudomedian of a finite populations is the location parameter computed by the Hodges–Lehmann statistic. It coincides with a population median when the population is symmetric. \n\nIn the statistical theory of probability distributions, the pseudomedian is the location parameter that is estimated by Hodges–Lehmann statistic. When the distribution is symmetric about a median, its pseudomedian coincides with that median. For nonsymmetric distributions, the pseudomedian is defined as the median of all of the midpoints of pairs of observations. Like the set of medians, the pseudomedian is well defined for all probability distributions, even for the many distributions that lack modes or means.\n\nIn Signal Processing there is another definition of pseudomedian filter for discrete signal. For a window of width 2N+1 pseudomedian defined as the average of the maximum of the minima and the minimum of the maxima of the N+1 sliding subwindows of length N+1. \n\n\n"}
{"id": "10353119", "url": "https://en.wikipedia.org/wiki?curid=10353119", "title": "Realization (systems)", "text": "Realization (systems)\n\nIn systems theory, a realization of a state space model is an implementation of a given input-output behavior. That is, given an input-output relationship, a realization is a quadruple of (time-varying) matrices formula_1 such that\nwith formula_4 describing the input and output of the system at time formula_5.\n\nFor a linear time-invariant system specified by a transfer matrix, formula_6, a realization is any quadruple of matrices formula_7 such that formula_8.\n\nAny given transfer function which is strictly proper can easily be transferred into state-space by the following approach (this example is for a 4-dimensional, single-input, single-output system)):\n\nGiven a transfer function, expand it to reveal all coefficients in both the numerator and denominator. This should result in the following form:\n\nThe coefficients can now be inserted directly into the state-space model by the following approach:\n\nThis state-space realization is called controllable canonical form (also known as phase variable canonical form) because the resulting model is guaranteed to be controllable (i.e., because the control enters a chain of integrators, it has the ability to move every state).\n\nThe transfer function coefficients can also be used to construct another type of canonical form\n\nThis state-space realization is called observable canonical form because the resulting model is guaranteed to be observable (i.e., because the output exits from a chain of integrators, every state has an effect on the output).\n\nIf we have an input formula_14, an output formula_15, and a weighting pattern formula_16 then a realization is any triple of matrices formula_17 such that formula_18 where formula_19 is the state-transition matrix associated with the realization.\n\nSystem identification techniques take the experimental data from a system and output a realization. Such techniques can utilize both input and output data (e.g. eigensystem realization algorithm) or can only include the output data (e.g. frequency domain decomposition). Typically an input-output technique would be more accurate, but the input data is not always available.\n\n"}
{"id": "14031360", "url": "https://en.wikipedia.org/wiki?curid=14031360", "title": "Reprojection error", "text": "Reprojection error\n\nThe reprojection error is a geometric error corresponding to the image distance between a projected point and a measured one. It is used to quantify how closely an estimate of a 3D point formula_1 recreates the point's true projection formula_2. More precisely, let formula_3 be the projection matrix of a camera and formula_4 be the image projection of formula_1, i.e. formula_6. The reprojection error of formula_1 is given by formula_8, where formula_8 denotes the Euclidean distance between the image points represented by vectors formula_2 and formula_4.\n\nMinimizing the reprojection error can be used for estimating the error from point correspondences between two images. Suppose we are given 2D to 2D point imperfect correspondences formula_12. We wish to find a homography formula_13 and pairs of perfectly matched points formula_14 and formula_15, i.e. points that satisfy formula_16 that minimize the reprojection error function given by\nSo the correspondences can be interpreted as imperfect images of a world point and the reprojection error quantifies their deviation from the true image projections formula_18\n"}
{"id": "8562999", "url": "https://en.wikipedia.org/wiki?curid=8562999", "title": "Rippling", "text": "Rippling\n\nRippling refers to a group of meta-level heuristics, developed primarily in the Mathematical Reasoning Group in the School of Informatics at the University of Edinburgh, and most commonly used to guide inductive proofs in automated theorem proving systems. Rippling may be viewed as a restricted form of rewrite system, where special object level annotations are used to ensure fertilization upon the completion of rewriting, with a measure decreasing requirement ensuring termination for any set of rewrite rules and expression.\n\nRaymond Aubin was the first person to use the term \"rippling out\" whilst working on his 1976 PhD thesis at the University of Edinburgh. He recognised a common pattern of movement during the rewriting stage of inductive proofs. Alan Bundy later turned this concept on its head by defining rippling to be this pattern of movement, rather than a side effect.\n\nSince then, \"rippling sideways\", \"rippling in\" and \"rippling past\" were coined, so the term was generalised to rippling. Rippling continues to be developed at Edinburgh, and elsewhere, to this day.\n\nRippling has been applied to many problems traditionally viewed as being hard in the inductive theorem proving community, including Bledsoe's limit theorems and a proof of the Gordon microprocessor, a miniature computer developed by Mike Gordon and his team at Cambridge.\n\nVery often, when attempting to prove a proposition, we are given a source expression and a target expression, which differ only by the inclusion of a few extra syntactic elements.\n\nThis is especially true in inductive proofs, where the given expression is taken to be the inductive hypothesis, and the target expression the inductive conclusion. Usually, the differences between the hypothesis and conclusion are only minor, perhaps the inclusion of a successor function (e.g., +1) around the induction variable.\n\nAt the start of rippling the differences between the two expressions, known as wave-fronts in rippling parlance, are identified. Typically these differences prevent the completion of the proof and need to be \"moved away\". The target expression is annotated to distinguish the wavefronts (differences) and skeleton (common structure) between the two expressions. Special rules, called wave rules, can then be used in a terminating fashion to manipulate the target expression until the source expression can be used to complete the proof.\n\nWe aim to show that the addition of natural numbers is commutative. This is an elementary property, and the proof is by routine induction. Nevertheless, the search space for finding such a proof may become quite large.\n\nTypically, the base case of any inductive proof is solved by methods other than rippling. For this reason, we will concentrate on the step case.\nOur step case takes the following form, where we have chosen to use x as the induction variable:\n\nWe may also possess several rewrite rules, drawn from lemmas, inductive definitions or elsewhere, that can be used to form wave-rules.\nSuppose we have the following three rewrite rules:\n\nthen these can be annotated, to form:\n\nNote that all these annotated rules preserve the skeleton (x + y = y + x, in the first case and x + y in the second/third). Now, annotating the inductive step case, gives us:\n\nAnd we are all set to perform rippling:\n\nNote, the final rewrite causes all wave-fronts to disappear, and we may now apply fertilization, the application of the inductive hypotheses, to complete the proof.\n"}
{"id": "31308010", "url": "https://en.wikipedia.org/wiki?curid=31308010", "title": "Special case", "text": "Special case\n\nIn logic, especially as applied in mathematics, concept is a special case or specialization of concept precisely if every instance of is also an instance of but not vice versa, or equivalently, if is a generalization of . A limiting case is a type of special case which is arrived at by taking some aspect of the concept to the extreme of what is permitted in the general case. A degenerate case is a special case which is in some way qualitatively different from almost all of the cases allowed.\n\nSpecial case examples include the following:\n\n"}
{"id": "4375347", "url": "https://en.wikipedia.org/wiki?curid=4375347", "title": "Sphere theorem (3-manifolds)", "text": "Sphere theorem (3-manifolds)\n\nIn mathematics, in the topology of 3-manifolds, the sphere theorem of gives conditions for elements of the second homotopy group of a 3-manifold to be represented by embedded spheres.\n\nOne example is the following:\n\nLet formula_1 be an orientable 3-manifold such that formula_2 is not the trivial group. Then there exists a non-zero element of formula_2 having a representative that is an embedding formula_4.\n\nThe proof of this version of the theorem can be based on transversality methods, see .\n\nAnother more general version (also called the projective plane theorem, and due to David B. A. Epstein) is:\n\nLet formula_1 be any 3-manifold and formula_6 a formula_7-invariant subgroup of formula_2. If formula_9 is a general position map such that formula_10 and formula_11 is any neighborhood of the singular set formula_12, then there is a map formula_13 satisfying\n\nquoted in .\n"}
{"id": "27647", "url": "https://en.wikipedia.org/wiki?curid=27647", "title": "Star height problem", "text": "Star height problem\n\nThe star height problem in formal language theory is the question whether all regular languages can be expressed using regular expressions of limited star height, i.e. with a limited nesting depth of Kleene stars. Specifically, is a nesting depth of one always sufficient? If not, is there an algorithm to determine how many are required? The problem was raised by .\n\nThe first question was answered in the negative when in 1963, Eggan gave examples of regular languages of star height \"n\" for every \"n\". Here, the star height \"h\"(\"L\") of a regular language \"L\" is defined as the minimum star height among all regular expressions representing \"L\". The first few languages found by are described in the following, by means of giving a regular expression for each language:\n\nThe construction principle for these expressions is that expression formula_2 is obtained by concatenating two copies of formula_3, appropriately renaming the letters of the second copy using fresh alphabet symbols, concatenating the result with another fresh alphabet symbol, and then by surrounding the resulting expression with a Kleene star. The remaining, more difficult part, is to prove that for formula_3 there is no equivalent regular expression of star height less than \"n\"; a proof is given in .\n\nHowever, Eggan's examples use a large alphabet, of size 2-1 for the language with star height \"n\". He thus asked whether we can also find examples over binary alphabets. This was proved to be true shortly afterwards by . \nTheir examples can be described by an inductively defined family of regular expressions over the binary alphabet formula_5 as follows–cf. :\n\nAgain, a rigorous proof is needed for the fact that formula_3 does not admit an equivalent regular expression of lower star height. Proofs are given by and by .\n\nIn contrast, the second question turned out to be much more difficult, and the question became a famous open problem in formal language theory for over two decades . For years, there was only little progress. The pure-group languages were the first interesting family of regular languages for which the star height problem was proved to be decidable . But the general problem remained open for more than 25 years until it was settled by Hashiguchi, who in 1988 published an algorithm to determine the star height of any regular language. The algorithm wasn't at all practical, being of non-elementary complexity. To illustrate the immense resource consumptions of that algorithm, Lombardy and Sakarovitch (2002) give some actual numbers:\n\nNotice that alone the number formula_8 has 10 billion zeros when written down in decimal notation, and is already \"by far\" larger than the number of atoms in the observable universe.\n\nA much more efficient algorithm than Hashiguchi's procedure was devised by Kirsten in 2005. This algorithm runs, for a given nondeterministic finite automaton as input, within double-exponential space. Yet the resource requirements of this algorithm still greatly exceed the margins of what is considered practically feasible.\n\nThis algorithm has been optimized and generalized to trees by Colcombet and Löding in 2008, as part of the theory of regular cost functions.\nIt has been implemented in 2017 in the tool suite Stamina.\n\n\n"}
{"id": "18813336", "url": "https://en.wikipedia.org/wiki?curid=18813336", "title": "Szpiro's conjecture", "text": "Szpiro's conjecture\n\nIn number theory, Szpiro's conjecture concerns a relationship between the conductor and the discriminant of an elliptic curve. In a general form, it is equivalent to the well-known abc conjecture. It is named for Lucien Szpiro who formulated it in the 1980s.\n\nThe conjecture states that: given ε > 0, there exists a constant \"C\"(ε) such that for any elliptic curve \"E\" defined over Q with minimal discriminant Δ and conductor \"f\", we have\n\nThe modified Szpiro conjecture states that: given ε > 0, there exists a constant \"C\"(ε) such that for any elliptic curve \"E\" defined over Q with invariants \"c\", \"c\" and conductor \"f\" (see Tate's algorithm#Notation), we have\n\n"}
{"id": "22424909", "url": "https://en.wikipedia.org/wiki?curid=22424909", "title": "Turán–Kubilius inequality", "text": "Turán–Kubilius inequality\n\nThe Turán–Kubilius inequality is a mathematical theorem in probabilistic number theory. It is useful for proving results about the normal order of an arithmetic function. The theorem was proved in a special case in 1934 by Pál Turán and generalized in 1956 and 1964 by Jonas Kubilius.\n\nThis formulation is from Tenenbaum. Other formulations are in Narkiewicz\nand in Cojocaru & Murty.\n\nSuppose \"f\" is an additive complex-valued arithmetic function, and write \"p\" for an arbitrary prime and for an arbitrary positive integer. Write\n\nand\n\nThen there is a function ε(\"x\") that goes to zero when \"x\" goes to infinity, and such that for \"x\" ≥ 2 we have\n\nTurán developed the inequality to create a simpler proof of the Hardy–Ramanujan theorem about the normal order of the number ω(\"n\") of distinct prime divisors of an integer \"n\". There is an exposition of Turán's proof in Hardy & Wright, §22.11.\nTenenbaum gives a proof of the Hardy–Ramanujan theorem using the Turán–Kubilius inequality and states without proof several other applications.\n"}
{"id": "6706053", "url": "https://en.wikipedia.org/wiki?curid=6706053", "title": "Varifold", "text": "Varifold\n\nIn mathematics, a varifold is, loosely speaking, a measure-theoretic generalization of the concept of a differentiable manifold, by replacing differentiability requirements with those provided by rectifiable sets, while maintaining the general algebraic structure usually seen in differential geometry. Varifolds generalize the idea of a rectifiable current, and are studied in geometric measure theory.\n\nVarifolds were first introduced by Laurence Chisholm Young in , under the name \"\"generalized surfaces\". Frederick J. Almgren Jr. slightly modified the definition in his mimeographed notes and coined the name \"varifold\": he wanted to emphasize that these objects are substitutes for ordinary manifolds in problems of the calculus of variations. The modern approach to the theory was based on Almgren's notes and laid down by William K. Allard, in the paper .\n\nGiven an open subset formula_1 of Euclidean space formula_2, an \"m\"-dimensional varifold on formula_1 is defined as a Radon measure on the set\n\nwhere formula_5 is the Grassmannian of all \"m\"-dimensional linear subspaces of an \"n\"-dimensional vector space. The Grassmannian is used to allow the construction of analogs to differential forms as duals to vector fields in the approximate tangent space of the set formula_1.\n\nThe particular case of a rectifiable varifold is the data of a \"m\"-rectifiable set \"M\" (which is measurable with respect to the \"m\"-dimensional Hausdorff measure), and a density function defined on \"M\", which is a positive function θ measurable and locally integrable with respect to the \"m\"-dimensional Hausdorff measure. It defines a Radon measure \"V\" on the Grassmannian bundle of ℝ\"\"\n\nwhere\nRectifiable varifolds are weaker objects than locally rectifiable currents: they do not have any orientation. Replacing \"M\" with more regular sets, one easily see that differentiable submanifolds are particular cases of rectifiable manifolds.\n\nDue to the lack of orientation, there is no boundary operator defined on the space of varifolds.\n\n\n"}
{"id": "562782", "url": "https://en.wikipedia.org/wiki?curid=562782", "title": "Vertex cover", "text": "Vertex cover\n\nIn the mathematical discipline of graph theory, a vertex cover (sometimes node cover) of a graph is a set of vertices such that each edge of the graph is incident to at least one vertex of the set.\nThe problem of finding a minimum vertex cover is a classical optimization problem in computer science and is a typical example of an NP-hard optimization problem that has an approximation algorithm. Its decision version, the vertex cover problem, was one of Karp's 21 NP-complete problems and is therefore a classical NP-complete problem in computational complexity theory. Furthermore, the vertex cover problem is fixed-parameter tractable and a central problem in parameterized complexity theory.\n\nThe minimum vertex cover problem can be formulated as a half-integral linear program whose dual linear program is the maximum matching problem.\n\nFormally, a vertex cover formula_1 of an undirected graph formula_2 is a subset of formula_3 such that formula_4, that is to say it is a set of vertices formula_1 where every edge has at least one endpoint in the vertex cover formula_1. Such a set is said to \"cover\" the edges of formula_7. The following figure shows two examples of vertex covers, with some vertex cover formula_1 marked in red.\n\nA \"minimum vertex cover\" is a vertex cover of smallest possible size. The vertex cover number formula_9 is the size of a minimum vertex cover, i.e. formula_10. The following figure shows examples of minimum vertex covers in the previous graphs.\n\n\n\nThe minimum vertex cover problem is the optimization problem of finding a smallest vertex cover in a given graph.\nIf the problem is stated as a decision problem, it is called the vertex cover problem:\n\nThe vertex cover problem is an NP-complete problem: it was one of Karp's 21 NP-complete problems. It is often used in computational complexity theory as a starting point for NP-hardness proofs.\n\nAssume that every vertex has an associated cost of formula_21.\nThe (weighted) minimum vertex cover problem can be formulated as the following integer linear program (ILP).\nThis ILP belongs to the more general class of ILPs for covering problems.\nThe integrality gap of this ILP is formula_22, so its relaxation gives a factor-formula_22 approximation algorithm for the minimum vertex cover problem.\nFurthermore, the linear programming relaxation of that ILP is \"half-integral\", that is, there exists an optimal solution for which each entry formula_24 is either 0, 1/2, or 1.\n\nThe decision variant of the vertex cover problem is NP-complete, which means it is unlikely that there is an efficient algorithm to solve it exactly. NP-completeness can be proven by reduction from 3-satisfiability or, as Karp did, by reduction from the clique problem. Vertex cover remains NP-complete even in cubic graphs and even in planar graphs of degree at most 3.\n\nFor bipartite graphs, the equivalence between vertex cover and maximum matching described by Kőnig's theorem allows the bipartite vertex cover problem to be solved in polynomial time.\n\nFor tree graphs, an algorithm finds a minimal vertex cover in polynomial time by finding the first leaf in the tree and adding its parent to the minimal vertex cover, then deleting the leaf and parent and all associated edges and continuing repeatedly until no nodes remain in the tree.\n\nAn exhaustive search algorithm can solve the problem in time 2\"n\". Vertex cover is therefore fixed-parameter tractable, and if we are only interested in small \"k\", we can solve the problem in polynomial time. One algorithmic technique that works here is called \"bounded search tree algorithm\", and its idea is to repeatedly choose some vertex and recursively branch, with two cases at each step: place either the current vertex or all its neighbours into the vertex cover.\nThe algorithm for solving vertex cover that achieves the best asymptotic dependence on the parameter runs in time formula_25. The klam value of this time bound (an estimate for the largest parameter value that could be solved in a reasonable amount of time) is approximately 190. That is, unless additional algorithmic improvements can be found, this algorithm is suitable only for instances whose vertex cover number is 190 or less. Under reasonable complexity-theoretic assumptions, namely the exponential time hypothesis, the running time cannot be improved to 2, even when formula_26 is formula_27.\n\nHowever, for planar graphs, and more generally, for graphs excluding some fixed graph as a minor, a vertex cover of size \"k\" can be found in time \"formula_28\", i.e., the problem is subexponential fixed-parameter tractable. This algorithm is again optimal, in the sense that, under the exponential time hypothesis, no algorithm can solve vertex cover on planar graphs in time \"formula_29\".\n\nOne can find a factor-2 approximation by repeatedly taking \"both\" endpoints of an edge into the vertex cover, then removing them from the graph. Put otherwise, we find a maximal matching \"M\" with a greedy algorithm and construct a vertex cover \"C\" that consists of all endpoints of the edges in \"M\". In the following figure, a maximal matching \"M\" is marked with red, and the vertex cover \"C\" is marked with blue.\n\nThe set \"C\" constructed this way is a vertex cover: suppose that an edge \"e\" is not covered by \"C\"; then \"M\" ∪ {\"e\"} is a matching and \"e\" ∉ \"M\", which is a contradiction with the assumption that \"M\" is maximal. Furthermore, if \"e\" = {\"u\", \"v\"} ∈ \"M\", then any vertex cover – including an optimal vertex cover – must contain \"u\" or \"v\" (or both); otherwise the edge \"e\" is not covered. That is, an optimal cover contains at least \"one\" endpoint of each edge in \"M\"; in total, the set \"C\" is at most 2 times as large as the optimal vertex cover.\n\nThis simple algorithm was discovered independently by Fanica Gavril and Mihalis Yannakakis.\n\nMore involved techniques show that there are approximation algorithms with a slightly better approximation factor. For example, an approximation algorithm with an approximation factor of formula_30 is known. The problem can be approximated with an approximation factor formula_31 in formula_32 - dense graphs.\n\nNo better constant-factor approximation algorithm than the above one is known.\nThe minimum vertex cover problem is APX-complete, that is, it cannot be approximated arbitrarily well unless P = NP.\nUsing techniques from the PCP theorem, Dinur and Safra proved in 2005 that minimum vertex cover cannot be approximated within a factor of 1.3606 for any sufficiently large vertex degree unless P = NP. Moreover, if the unique games conjecture is true then minimum vertex cover cannot be approximated within any constant factor better than 2.\n\nAlthough finding the minimum-size vertex cover is equivalent to finding the maximum-size independent set, as described above, the two problems are not equivalent in an approximation-preserving way: The Independent Set problem has \"no\" constant-factor approximation unless P = NP.\n\nGiven a collection of sets, a set which intersects all sets in the collection in at least one element is called a hitting set, and a simple NP-hard problem is to find a hitting set of smallest size or minimum hitting set. By mapping the sets in the collection onto hyperedges, this can be understood as a natural generalization of the vertex cover problem to hypergraphs which is often just called vertex cover for hypergraphs and, in a more combinatorial context, transversal.\nThe notions of hitting set and set cover are equivalent.\n\nFormally, let \"H\" = (\"V\", \"E\") be a hypergraph with vertex set \"V\" and hyperedge set \"E\". Then a set \"S\" ⊆ \"V\" is called \"hitting set\" of \"H\" if, for all edges \"e\" ∈ \"E\", it holds \"S\" ∩ \"e\" ≠ ∅.\nThe computational problems \"minimum hitting set\" and \"hitting set\" are defined as in the case of graphs. Note that we get back the case of vertex covers for simple graphs if the maximum size of the hyperedges is 2.\n\nIf that size is restricted to \"d\", the problem of finding a minimum \"d\"-hitting set permits a \"d\"-approximation algorithm. Assuming the unique games conjecture, this is the best constant-factor algorithm that is possible and otherwise there is the possibility of improving the approximation to \"d\" − 1.\n\nFor the hitting set problem, different parametrizations make sense.\nThe hitting set problem is \"W\"[2]-complete for the parameter OPT, that is, it is unlikely that there is an algorithm that runs in time \"f\"(OPT)\"n\" where OPT is the cardinality of the smallest hitting set.\nThe hitting set problem is fixed-parameter tractable for the parameter OPT + \"d\", where \"d\" is the size of the largest edge of the hypergraph. More specifically, there is an algorithm for hitting set that runs in time \"d\"\"n\".\n\nThe hitting set problem is equivalent to the set cover problem:\nAn instance of set cover can be viewed as an arbitrary bipartite graph, with sets represented by vertices on the left, elements of the universe represented by vertices on the right, and edges representing the inclusion of elements in sets. The task is then to find a minimum cardinality subset of left-vertices which covers all of the right-vertices. In the hitting set problem, the objective is to cover the left-vertices using a minimum subset of the right vertices. Converting from one problem to the other is therefore achieved by interchanging the two sets of vertices.\n\nAn example of a practical application involving the hitting set problem arises in efficient dynamic detection of race conditions. In this case, each time global memory is written, the current thread and set of locks held by that thread are stored. Under lockset-based detection, if later another thread writes to that location and there is \"not\" a race, it must be because it holds at least one lock in common with each of the previous writes. Thus the size of the hitting set represents the minimum lock set size to be race-free. This is useful in eliminating redundant write events, since large lock sets are considered unlikely in practice.\n\n\n"}
{"id": "4705059", "url": "https://en.wikipedia.org/wiki?curid=4705059", "title": "Whitehead's lemma", "text": "Whitehead's lemma\n\nWhitehead's lemma is a technical result in abstract algebra used in algebraic K-theory. It states that a matrix of the form \n\nis equivalent to the identity matrix by elementary transformations (that is, transvections):\n\nHere, formula_3 indicates a matrix whose diagonal block is formula_4 and formula_5 entry is formula_6.\n\nThe name \"Whitehead's lemma\" also refers to the closely related result that the derived group of the stable general linear group is the group generated by elementary matrices. In symbols, \n\nThis holds for the stable group (the direct limit of matrices of finite size) over any ring, but not in general for the unstable groups, even over a field. For instance for \n\none has:\nwhere Alt(3) and Sym(3) denote the alternating resp. symmetric group on 3 letters.\n\n"}
{"id": "25046926", "url": "https://en.wikipedia.org/wiki?curid=25046926", "title": "Zermelo's theorem (game theory)", "text": "Zermelo's theorem (game theory)\n\nIn game theory, Zermelo’s theorem, named after Ernst Zermelo, says that in any finite two-person game of perfect information in which the players move alternatingly and in which chance does not affect the decision making process, if the game cannot end in a draw, then one of the two players must have a winning strategy (i.e. force a win). It can alternately be stated as saying that in such a game, either the first-player can force a win, or the second-player can force a win, or both players can force a draw.\n\nZermelo's work shows that in two-person zero-sum games with perfect information, if a player is in a winning position, then he can always force a win no matter what strategy the other player may employ. Furthermore, and as a consequence, if a player is in a winning position, it will never require more moves than there are positions in the game (with a position defined as position of pieces as well as the player next to move).\n\nZermelo's original paper describing the theorem,\n\"Über eine Anwendung der Mengenlehre auf die Theorie des Schachspiels\", was published in German in 1913. Ulrich Schwalbe and Paul Walker translated Zermelo's paper into English in 1997 and published the translation in the appendix to \"Zermelo and the Early History of Game Theory\".\n\nZermelo considers the class of two-person games without chance, where players have strictly opposing interests and where only a finite number of positions are possible. Although in the game only finitely many positions are possible, Zermelo allows infinite sequences of moves since he does not consider stopping rules. Thus, he allows for the possibility of infinite games. Then he addresses two problems:\n\n\nTo answer the first question, Zermelo states that a necessary and sufficient condition is the nonemptyness of a certain set, containing all possible sequences of moves such that a player wins independently of how the other player plays. But should this set be empty, the best a player could achieve would be a draw. So he defines another set containing all possible sequences of moves such that a player can postpone his loss for an infinite number of moves, which implies a draw. This set may also be empty, i. e., the player can avoid his loss for only finitely many moves if his opponent plays correctly. But this is equivalent to the opponent being able to force a win. This is the basis for all modern versions of Zermelo's theorem.\n\nAbout the second question, Zermelo claimed that it will never take more moves than there are positions in the game. His proof is a proof by contradiction: Assume that a player can win in a number of moves larger than the number of positions. Of course, at least one winning position must have appeared twice. So the player could have played at the first occurrence in the same way as he does at the second and thus could have won in fewer moves than there are positions.\n\nWhen applied to chess, Zermelo's Theorem states \"either White can force a win, or Black can force a win, or both sides can force at least a draw\".\n\n"}
