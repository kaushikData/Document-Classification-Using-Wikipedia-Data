{"id": "22418373", "url": "https://en.wikipedia.org/wiki?curid=22418373", "title": "Abstract differential geometry", "text": "Abstract differential geometry\n\nThe adjective \"abstract\" has often been applied to differential geometry before, but the abstract differential geometry (ADG) of this article is a form of differential geometry without the calculus notion of smoothness, developed by Anastasios Mallios and Ioannis Raptis from 1998 onwards.\n\nInstead of calculus, an axiomatic treatment of differential geometry is built via sheaf theory and sheaf cohomology using vector sheaves in place of bundles based on arbitrary topological spaces. Mallios says noncommutative geometry can be considered a special case of ADG, and that ADG is similar to synthetic differential geometry.\n\nMallios and Raptis use ADG to avoid the singularities in general relativity and propose this as a route to quantum gravity.\n\n\n"}
{"id": "54589646", "url": "https://en.wikipedia.org/wiki?curid=54589646", "title": "Aequationes Mathematicae", "text": "Aequationes Mathematicae\n\nAequationes Mathematicae is a mathematical journal. It is primarily devoted to functional equations, but also publishes papers in dynamical systems, combinatorics, and geometry. As well as publishing regular journal submissions on these topics, it also regularly reports on international symposia on functional equations and produces bibliographies on the subject.\n\nJános Aczél founded the journal in 1968 at the University of Waterloo, in part because of the long publication delays of up to four years in other journals at the time of its founding.\nIt is currently published by Springer Science+Business Media, with Zsolt Páles of the University of Debrecen as its editor in chief. János Aczél remains its honorary editor in chief.\n"}
{"id": "47560654", "url": "https://en.wikipedia.org/wiki?curid=47560654", "title": "Albatross (programming language)", "text": "Albatross (programming language)\n\nAlbatross\nis a general purpose programming language which can be verified statically.\n"}
{"id": "6096986", "url": "https://en.wikipedia.org/wiki?curid=6096986", "title": "Alexander Skopin", "text": "Alexander Skopin\n\nAlexander Ivanovich Skopin (Александр Иванович Скопин) (1927–2003) was a Russian mathematician known for his contributions to abstract algebra.\n\nSkopin was born on October 22, 1927, in Leningrad, the son of Ivan Alexandrovich Skopin, who was himself also a number theorist and a student of Ivan Matveyevich Vinogradov, and who died in the Siege of Leningrad. After the war, Alexander Skopin studied at Leningrad University, where he was a student of Dmitry Faddeev; From that point to the end of his life, he worked as a researcher at the Steklov Mathematical Institute (where he was scientific secretary from the mid-1960s to the early 1970s) and taught algebra at the St. Petersburg University. He died on September 15, 2003, in St. Petersburg.\n\nSkopin's student work was in abstract algebra, and concerned upper central series of groups and extensions of fields. In the 1970s, Skopin received a second doctorate concerning the application of computer algebra systems to group theory. From that point onward he used computational methods extensively in his research, which focussed on lower central series of Burnside groups. He related this problem to problems in other areas of mathematics including linear algebra and topological sorting of graphs.\n"}
{"id": "3858892", "url": "https://en.wikipedia.org/wiki?curid=3858892", "title": "Ars Combinatoria (journal)", "text": "Ars Combinatoria (journal)\n\nArs Combinatoria, a Canadian Journal of Combinatorics is an English language research journal in combinatorics, issued in Winnipeg, Manitoba, .\n\nBeginning in June, 1976, two volumes were issued each year, dated June and December. Starting in 1995 three volumes were published each year. Starting in 2000 four volumes are published each year. It is published by The Charles Babbage Research Centre, Winnipeg, Manitoba, Canada.\n\nTables of contents of the issues are available online. The journal is indexed in \"MathSciNet\" and \"Zentralblatt\". As of 2012, its SCImago Journal Rank two-year citation index (computed using the same formula as the impact factor) was 0.5.\n\n"}
{"id": "559868", "url": "https://en.wikipedia.org/wiki?curid=559868", "title": "Biopython", "text": "Biopython\n\nThe Biopython Project is an open-source collection of non-commercial Python tools for computational biology and bioinformatics, created by an international association of developers. It contains classes to represent biological sequences and sequence annotations, and it is able to read and write to a variety of file formats. It also allows for a programmatic means of accessing online databases of biological information, such as those at NCBI. Separate modules extend Biopython's capabilities to sequence alignment, protein structure, population genetics, phylogenetics, sequence motifs, and machine learning. Biopython is one of a number of Bio* projects designed to reduce code duplication in computational biology.\n\nBiopython development began in 1999 and it was first released in July 2000. It was developed during a similar time frame and with analogous goals to other projects that added bioinformatics capabilities to their respective programming languages, including BioPerl, BioRuby and BioJava. Early developers on the project included Jeff Chang, Andrew Dalke and Brad Chapman, though over 100 people have made contributions to date. In 2007, a similar Python project, namely PyCogent, was established.\n\nThe initial scope of Biopython involved accessing, indexing and processing biological sequence files. While this is still a major focus, over the following years added modules have extended its functionality to cover additional areas of biology (see Key features and examples).\n\nAs of version 1.62, Biopython supports running on Python 3 as well as Python 2.\n\nWherever possible, Biopython follows the conventions used by the Python programming language to make it easier for users familiar with Python. For example, codice_1 and codice_2 objects can be manipulated via , in a manner similar to Python’s strings and lists. It is also designed to be functionally similar to other Bio* projects, such as BioPerl.\n\nBiopython is able to read and write most common file formats for each of its functional areas, and its license is permissive and compatible with most other software licenses, which allow Biopython to be used in a variety of software projects.\n\nA core concept in Biopython is the biological sequence, and this is represented by the codice_1 class. A Biopython codice_1 object is similar to a Python string in many respects: it supports the Python slice notation, can be concatenated with other sequences and is immutable. In addition, it includes sequence-specific methods and specifies the particular biological alphabet used.\n»> # This script creates a DNA sequence and performs some typical manipulations\n»> from Bio.Seq import Seq\n»> from Bio.Alphabet import IUPAC\n»> dna_sequence = Seq('AGGCTTCTCGTA', IUPAC.unambiguous_dna)\n»> dna_sequence\nSeq('AGGCTTCTCGTA', IUPACUnambiguousDNA())\n»> dna_sequence[2:7]\nSeq('GCTTC', IUPACUnambiguousDNA())\n»> dna_sequence.reverse_complement()\nSeq('TACGAGAAGCCT', IUPACUnambiguousDNA())\n»> rna_sequence = dna_sequence.transcribe()\n»> rna_sequence\nSeq('AGGCUUCUCGUA', IUPACUnambiguousRNA())\n»> rna_sequence.translate()\nSeq('RLLV', IUPACProtein())\nThe codice_2 class describes sequences, along with information such as name, description and features in the form of codice_6 objects. Each codice_6 object specifies the type of the feature and its location. Feature types can be ‘gene’, ‘CDS’ (coding sequence), ‘repeat_region’, ‘mobile_element’ or others, and the position of features in the sequence can be exact or approximate.\n»> # This script loads an annotated sequence from file and views some of its contents.\n»> from Bio import SeqIO\n»> seq_record = SeqIO.read('pTC2.gb', 'genbank')\n»> seq_record.name\n'NC_019375'\n»> seq_record.description\n'Providencia stuartii plasmid pTC2, complete sequence.'\n»> seq_record.features[14]\nSeqFeature(FeatureLocation(ExactPosition(4516), ExactPosition(5336), strand=1), type='mobile_element')\n»> seq_record.seq\nSeq('GGATTGAATATAACCGACGTGACTGTTACATTTAGGTGGCTAAACCCGTCAAGC...GCC', IUPACAmbiguousDNA())\nBiopython can read and write to a number of common sequence formats, including FASTA, FASTQ, GenBank, Clustal, PHYLIP and NEXUS. When reading files, descriptive information in the file is used to populate the members of Biopython classes, such as codice_2. This allows records of one file format to be converted into others.\n\nVery large sequence files can exceed a computer's memory resources, so Biopython provides various options for accessing records in large files. They can be loaded entirely into memory in Python data structures, such as lists or dictionaries, providing fast access at the cost of memory usage. Alternatively, the files can be read from disk as needed, with slower performance but lower memory requirements.\n»> # This script loads a file containing multiple sequences and saves each one in a different format.\n»> from Bio import SeqIO\n»> genomes = SeqIO.parse('salmonella.gb', 'genbank')\n»> for genome in genomes:\n... SeqIO.write(genome, genome.id + '.fasta', 'fasta')\n\nThrough the Bio.Entrez module, users of Biopython can download biological data from NCBI databases. Each of the functions provided by the Entrez search engine is available through functions in this module, including searching for and downloading records.\n»> # This script downloads genomes from the NCBI Nucleotide database and saves them in a FASTA file.\n»> from Bio import Entrez\n»> from Bio import SeqIO\n»> output_file = open('all_records.fasta', \"w\")\n»> Entrez.email = 'my_email@example.com'\n»> records_to_download = ['FO834906.1', 'FO203501.1']\n»> for record_id in records_to_download:\n... handle = Entrez.efetch(db='nucleotide', id=record_id, rettype='gb')\n... seqRecord = SeqIO.read(handle, format='gb')\n... handle.close()\n... output_file.write(seqRecord.format('fasta'))\n\nThe Bio.Phylo module provides tools for working with and visualising phylogenetic trees. A variety of file formats are supported for reading and writing, including Newick, NEXUS and phyloXML. Common tree manipulations and traversals are supported via the codice_9 and codice_10 objects. Examples include converting and collating tree files, extracting subsets from a tree, changing a tree's root, and analysing branch features such as length or score.\n\nRooted trees can be drawn in ASCII or using matplotlib (see Figure 1), and the Graphviz library can be used to create unrooted layouts (see Figure 2).\n\nThe GenomeDiagram module provides methods of visualising sequences within Biopython. Sequences can be drawn in a linear or circular form (see Figure 3), and many output formats are supported, including PDF and PNG. Diagrams are created by making tracks and then adding sequence features to those tracks. By looping over a sequence's features and using their attributes to decide if and how they are added to the diagram's tracks, one can exercise much control over the appearance of the final diagram. Cross-links can be drawn between different tracks, allowing one to compare multiple sequences in a single diagram.\n\nThe Bio.PDB module can load molecular structures from PDB and mmCIF files, and was added to Biopython in 2003. The codice_11 object is central to this module, and it organises macromolecular structure in a hierarchical fashion: codice_11 objects contain codice_13 objects which contain codice_14 objects which contain codice_15 objects which contain codice_16 objects. Disordered residues and atoms get their own classes, codice_17 and codice_18, that describe their uncertain positions.\n\nUsing Bio.PDB, one can navigate through individual components of a macromolecular structure file, such as examining each atom in a protein. Common analyses can be carried out, such as measuring distances or angles, comparing residues and calculating residue depth.\n\nThe Bio.PopGen module adds support to Biopython for Genepop, a software package for statistical analysis of population genetics. This allows for analyses of Hardy–Weinberg equilibrium, linkage disequilibrium and other features of a population's allele frequencies.\n\nThis module can also carry out population genetic simulations using coalescent theory with the fastsimcoal2 program.\n\nMany of Biopython's modules contain command line wrappers for commonly used tools, allowing these tools to be used from within Biopython. These wrappers include BLAST, Clustal, PhyML, EMBOSS and SAMtools. Users can subclass a generic wrapper class to add support for any other command line tool.\n\n\n"}
{"id": "20020422", "url": "https://en.wikipedia.org/wiki?curid=20020422", "title": "Breeder (cellular automaton)", "text": "Breeder (cellular automaton)\n\nIn cellular automata such as Conway's Game of Life, a breeder is a pattern that exhibits quadratic growth, by generating multiple copies of a secondary pattern, each of which then generates multiple copies of a tertiary pattern.\n\nBreeders can be classed by the relative motion of the patterns. The classes are denoted by three-letter codes, which denote whether the primary, secondary and tertiary elements respectively are moving (M) or stationary (S). The four basic types are:\n\nA spacefiller (which also undergoes quadratic growth) may be thought of as a fifth class of breeder. However it differs from a true breeder in that it expands a single island of cells, rather than creating independent objects.\n"}
{"id": "26236727", "url": "https://en.wikipedia.org/wiki?curid=26236727", "title": "Brooks–Iyengar algorithm", "text": "Brooks–Iyengar algorithm\n\nThe Brooks–Iyengar algorithm or Brooks–Iyengar hybrid algorithm is a distributed algorithm that improves both the precision and accuracy of the interval measurements taken by a distributed sensor network, even in the presence of faulty sensors. The sensor network does this by exchanging the measured value and accuracy value at every node with every other node, and computes the accuracy range and a measured value for the whole network from all of the values collected. Even if some of the data from some of the sensors is faulty, the sensor network will not malfunction. The algorithm is fault-tolerant and distributed. It could also be used as a sensor fusion method. The precision and accuracy bound of this algorithm have been proved in 2016.\n\nThe Brooks–Iyengar hybrid algorithm for distributed control in the presence of noisy data combines Byzantine agreement with sensor fusion. It bridges the gap between sensor fusion and Byzantine fault tolerance. This seminal algorithm unified these disparate fields for the first time. Essentially, it combines Dolev’s algorithm for approximate agreement with Mahaney and Schneider’s fast convergence algorithm (FCA). The algorithm assumes \"N\" processing elements (PEs), \"t\" of which are faulty and can behave maliciously. It takes as input either real values with inherent inaccuracy or noise (which can be unknown), or a real value with apriori defined uncertainty, or an interval. The output of the algorithm is a real value with an explicitly specified accuracy. The algorithm runs in O(\"N\"log\"N\") where \"N\" is the number of PEs: see Big O notation. It is possible to modify this algorithm to correspond to Crusader’s Convergence Algorithm (CCA), however, the bandwidth requirement will also increase. The algorithm has applications in distributed control, software reliability, High-performance computing, etc.\n\nThe Brooks–Iyengar algorithm is executed in every processing element (PE) of a distributed sensor network. Each PE exchanges their measured interval with all other PEs in the network. The \"fused\" measurement is a weighted average of the midpoints of the regions found. The concrete steps of Brooks–Iyengar algorithm are shown in this section. Each PE performs the algorithm separately:\n\nInput:\n\nThe measurement sent by PE \"k\" to PE \"i\" is a closed interval formula_1, formula_2\n\nOutput:\n\nThe output of PE \"i\" includes a point estimate and an interval estimate\nExample:\n\nConsider an example of 5 PEs, in which PE 5 (formula_14) is sending wrong values to other PEs and they all exchange the values.\nThe values received by formula_15 are in the next Table.\nWe draw a Weighted Region Diagram (WRD) of these intervals, then we can determine formula_16 for PE 1 according to the Algorithm:\n\nformula_17\n\nwhich consists of intervals where at least 4(= formula_3 = 5−1) measurements intersect. The output of PE 1 is equal to\n\nformula_19\n\nand the interval estimate is formula_20\n\nSimilar, we could obtain all the inputs and results of the 5 PEs:\n1982 Byzantine Problem: The Byzantine General Problem as an extension of Two Generals' Problem could be viewed as a binary problem.\n\n1983 Approximate Consensus: The method removes some values from the set consists of scalars to tolerant faulty inputs.\n\n1985 In-exact Consensus: The method also uses scalar as the input.\n\n1996 Brooks-Iyengar Algorithm: The method is based on intervals.\n\n2013 Byzantine Vector Consensus: The method uses vectors as the input.\n\n2013 Multidimensional Agreement: The method also use vectors as the input while the measure of distance is different.\n\nWe could use Approximate Consensus (scalar-based), Brooks-Iyengar Algorithm (interval-based) and Byzantine Vector Consensus (vector-based) to deal with interval inputs, and the paper proved that Brooks–Iyengar algorithm is the best here.\n\nBrooks–Iyengar algorithm is a seminal work and a major milestone in distributed sensing, and could be used as a fault tolerant solution for many redundancy scenarios. Also, it is easy to implement and embed in any networking systems.\n\nIn 1996, the algorithm was used in MINIX to provide more accuracy and precision, which leads to the development of the first version of RT-Linux.\n\nIn 2000, the algorithm was also central to the DARPA SensIT program’s distributed tracking program. Acoustic, seismic and motion detection readings from multiple sensors are combined and fed into a distributed tracking system. Besides, it was used to combine heterogeneous sensor feeds in the application fielded by BBN Technologies, BAE systems, Penn State Applied Research Lab(ARL), and USC/ISI.\n\nBesides, the Thales Group, a UK Defense Manufacturer, used this work in its Global Operational Analysis Laboratory. It is applied to Raytheon’s programs where many systems need extract reliable data from unreliable sensor network, this exempts the increasing investment in improving sensor reliability. Also, the research in developing this algorithm results in the tools used by the US Nav in its maritime domain awareness software.\n\nIn education, Brooks–Iyengar algorithm has been widely used in teaching classes such as University of Wisconsin, Purdue, Georgia Tech, Clemson University, University of Maryland, etc.\n\nIn addition to the area of sensor network, other fields such as time-triggered architecture, safety of cyber-physical systems, data fusion, robot convergence, high-performance computing, software/hardware reliability, ensemble learning in artificial intelligence systems could also benefit from Brooks–Iyengar algorithm.\n\n\n"}
{"id": "10572379", "url": "https://en.wikipedia.org/wiki?curid=10572379", "title": "Calendrical calculation", "text": "Calendrical calculation\n\nA calendrical calculation is a calculation concerning calendar dates. Calendrical calculations can be considered an area of applied mathematics. Calendrical calculation is one of the five major Savant syndrome characteristics.\nSome examples of calendrical calculations:\n\n\n"}
{"id": "16696644", "url": "https://en.wikipedia.org/wiki?curid=16696644", "title": "Carathéodory's criterion", "text": "Carathéodory's criterion\n\nCarathéodory's criterion is a result in measure theory that was formulated by Greek mathematician Constantin Carathéodory. Its statement is as follows: Let formula_1 denote the Lebesgue outer measure on formula_2, and let formula_3. Then formula_4 is Lebesgue measurable if and only if formula_5 for every formula_6.\nNotice that formula_7 is not required to be a measurable set. \n\nThe Carathéodory criterion is of considerable importance because, in contrast to Lebesgue's original formulation of measurability, which relies on certain topological properties of formula_8, this criterion readily generalizes to a characterization of measurability in abstract spaces. Indeed, in the generalization to abstract measures, this theorem is sometimes extended to a \"definition\" of measurability. Thus, we have the following definition: Let formula_9 be an outer measure on a set formula_10. Then formula_11 is called formula_9–\"measurable\" if for every formula_13, the equality formula_14 holds.\n\n"}
{"id": "29023273", "url": "https://en.wikipedia.org/wiki?curid=29023273", "title": "Christopher Hacon", "text": "Christopher Hacon\n\nChristopher Derek Hacon (born 14 February 1970) is a mathematician with British, Italian and US nationalities. He is currently distinguished professor of mathematics at the University of Utah where he holds a Presidential Endowed Chair. His research interests include algebraic geometry.\n\nHacon was born in Manchester, but grew up in Italy where he studied at the Scuola Normale Superiore and received a degree in mathematics at the University of Pisa in 1992. He received his doctorate from the University of California, Los Angeles in 1998, under supervision of Robert Lazarsfeld.\n\nIn 2007 he was awarded a Clay Research Award for his work, joint with James McKernan, on \"the birational geometry of algebraic varieties in dimension greater than three, in particular, for [an] inductive proof of the existence of flips.\"\nIn 2009 he was awarded the Cole Prize for outstanding contribution to algebra, along with McKernan.\n\nHe was an invited speaker at the International Congress of Mathematicians 2010 in Hyderabad, on the topic of \"Algebraic Geometry.\"\n\nIn 2011 he was awarded the Antonio Feltrinelli Prize in Mathematics, Mechanics and Applications by Italy's prestigious Accademia Nazionale dei Lincei.\n\nIn 2012 he became a fellow of the American Mathematical Society.\n\nIn 2012 he became a Simons Investigator.\n\nIn 2015 he won the American Mathematical Society Moore Prize.\n\nIn 2017 he was elected to the American Academy of Arts and Sciences.\n\nIn 2017 he won the 2018 Breakthrough Prize in Mathematics (with James McKernan).\n\nIn 2018 he was elected to the National Academy of Sciences.\n\n"}
{"id": "25521220", "url": "https://en.wikipedia.org/wiki?curid=25521220", "title": "Communications in Contemporary Mathematics", "text": "Communications in Contemporary Mathematics\n\n\"Communications in Contemporary Mathematics\" (CCM) is a journal published by World Scientific since 1999. It covers research in the fields such as applied mathematics, dynamical systems, mathematical physics, and topology.\n\nThe journal is indexed in \"Zentralblatt MATH, Mathematical Reviews,\" ISI Alerting Services, CompuMath Citation Index, \"Current Contents/Physical, Chemical and Earth Sciences\", and the \"Science Citation Index\".\n"}
{"id": "24758677", "url": "https://en.wikipedia.org/wiki?curid=24758677", "title": "Continuum (set theory)", "text": "Continuum (set theory)\n\nIn the mathematical field of set theory, the continuum means the real numbers, or the corresponding (infinite) cardinal number, formula_1. Georg Cantor proved that the cardinality formula_1 is larger than the smallest infinity, namely, formula_3. He also proved that formula_1 equals formula_5, the cardinality of the power set of the natural numbers.\n\nThe \"cardinality of the continuum\" is the size of the set of real numbers. The continuum hypothesis is sometimes stated by saying that no cardinality lies between that of the continuum and that of the natural numbers, formula_3.\n\nAccording to Raymond Wilder (1965) there are four axioms that make a set \"C\" and the relation < into a linear continuum:\nThese axioms characterize the order type of the real number line.\n\n"}
{"id": "37847650", "url": "https://en.wikipedia.org/wiki?curid=37847650", "title": "Cypherpunks (book)", "text": "Cypherpunks (book)\n\nCypherpunks: Freedom and the Future of the Internet is a 2012 book by Julian Assange, in discussion with Internet activists and cypherpunks Jacob Appelbaum, Andy Müller-Maguhn and Jérémie Zimmermann. Its primary topic is society's relationship with information security. In the book, the authors warn that the Internet has become a tool of the police state, and that the world is inadvertently heading toward a form of totalitarianism. They promote the use of cryptography to protect against state surveillance.\n\nIn the introduction, Assange says that the book is \"not a manifesto... [but] a warning.\" He told \"Guardian\" journalist Decca Aitkenhead:\n\nAssange later wrote in \"The Guardian\": \"Strong cryptography is a vital tool in fighting state oppression.\" saying that was the message of his book, \"Cypherpunks\".\n\n\"Cypherpunks\" is published by OR Books. Its content derives from discussions in June 2012 with Appelbaum, Müller-Maguhn and Zimmermann on Assange's TV show \"World Tomorrow\".\n\n\n"}
{"id": "42035548", "url": "https://en.wikipedia.org/wiki?curid=42035548", "title": "Decentralized autonomous organization", "text": "Decentralized autonomous organization\n\nA decentralized autonomous organization (DAO), sometimes labeled a decentralized autonomous corporation (DAC), is an organization represented by rules encoded as a computer program that is transparent, controlled by shareholders and not influenced by a central government. A DAO's financial transaction record and program rules are maintained on a blockchain. The precise legal status of this type of business organization is unclear.\n\nA well-known example, intended for venture capital funding, was The DAO, which launched with $150 million in crowdfunding in June 2016, and was immediately hacked and drained of in cryptocurrency. This hack was reversed in the following weeks, and the money restored, via a hard fork of the Ethereum blockchain. This bailout was made possible by the Ethereum miners and clients switching to the new fork.\n\nDecentralized autonomous organizations are typified by the use of blockchain technology to provide a secure digital ledger to track financial interactions across the internet, hardened against forgery by trusted timestamping and dissemination of a distributed database. This approach eliminates the need to involve a mutually acceptable trusted third party in a financial transaction, thus simplifying the transaction. The costs of a blockchain-enabled transaction and of the associated data reporting may be substantially offset by the elimination of both the trusted third party and of the need for repetitive recording of contract exchanges in different records. For example, the blockchain data could, in principle and if regulatory structures permit it, replace public documents such as deeds and titles. In theory, a blockchain approach allows multiple cloud computing users to enter a loosely coupled peer-to-peer smart contract collaboration.\n\nDaniel Larimer first proposed the concept of a \"Decentralized Organized Company\" in an article published on September 7, 2013 and implemented in Bitshares in 2014.\n\nVitalik Buterin proposed that after a DAO was launched, it might be organized to run without human managerial interactivity, provided the smart contracts were supported by a Turing complete platform. Ethereum, built on a blockchain and launched in 2015, has been described as meeting that Turing threshold, thus enabling such DAOs. Decentralized autonomous organizations aim to be open platforms where individuals control their identities and their personal data.\n\nThe first DAO's were Dash and Bitshares. Others are The DAO and Digix.\n\nIt can be difficult for a DAO to retain an attorney, or enter any kind of written contract because nobody in the company is interested in putting their personal name on agreements.\n\nShareholder participation in DAOs can be problematic. For example, \"BitShares\" has seen a lack of voting participation, because it takes time and energy to consider proposals.\n\nThe precise legal status of this type of business organization is unclear; some similar approaches have been regarded by the U.S. Securities and Exchange Commission as illegal offers of unregistered securities. Although unclear, a DAO may functionally be a corporation without legal status as a corporation: a general partnership. This means potentially unlimited legal liability for participants, even if the smart contract code or the DAO's promoters say otherwise. Known participants, or those at the interface between a DAO and regulated financial systems, may be targets for regulatory enforcement or civil actions.\n\nThe code of a given DAO will be difficult to alter once the system is up and running, including bug fixes that would be otherwise trivial in centralised code. Corrections for a DAO would require writing new code and agreement to migrate all the funds. Although the code is visible to all, it is hard to repair, thus leaving known security holes open to exploitation unless a moratorium is called to enable bug fixing.\n\nIn 2016, a specific DAO, \"The DAO\", set a record for the largest crowdfunding campaign to date. Researchers pointed out multiple issues in the code of The DAO. The operational procedure for The DAO allowed investors to withdraw at will any money that had not yet been committed to a project; the funds could thus deplete quickly. Although safeguards aimed to prevent gaming the voting of shareholders to win investments, there were a \"number of security vulnerabilities\". These enabled an attempted large withdrawal of funds from The DAO to be initiated in mid-June 2016. On the 20th of July 2016, the Ethereum community arrived at a consensus decision to hard fork the Ethereum blockchain to bail out the original contract.\n\n\n"}
{"id": "11415890", "url": "https://en.wikipedia.org/wiki?curid=11415890", "title": "Edmonds matrix", "text": "Edmonds matrix\n\nIn graph theory, the Edmonds matrix formula_1 of a balanced bipartite graph formula_2 with sets of vertices formula_3 and formula_4 is defined by \n\nwhere the \"x\" are indeterminates. One application of the Edmonds matrix of a bipartite graph is that the graph admits a perfect matching if and only if the polynomial det(\"A\") in the \"x\" is not identically zero. Furthermore, the number of perfect matchings is equal to the number of monomials in the polynomial det(\"A\"), and is also equal to the permanent of formula_1. In addition, rank of formula_1 is equal to the maximum matching size of formula_8.\n\nThe Edmonds matrix is named after Jack Edmonds. The Tutte matrix is a generalisation to non-bipartite graphs.\n\n"}
{"id": "27005472", "url": "https://en.wikipedia.org/wiki?curid=27005472", "title": "Flat function", "text": "Flat function\n\nIn mathematics, especially real analysis, a flat function is a smooth function ƒ : ℝ → ℝ all of whose derivatives vanish at a given point \"x\" ∈ ℝ. The flat functions are, in some sense, the antitheses of the analytic functions. An analytic function ƒ : ℝ → ℝ is given by a convergent power series close to some point \"x\" ∈ ℝ:\nIn the case of a flat function we see that all derivatives vanish at \"x\" ∈ ℝ, i.e. ƒ(\"x\") = 0 for all \"k\" ∈ ℕ. This means that a meaningful Taylor series expansion in a neighbourhood of \"x\" is impossible. In the language of Taylor's theorem, the non-constant part of the function always lies in the remainder \"R\"(\"x\") for all \"n\" ∈ ℕ.\n\nThe function need not be flat at just one point. Trivially, constant functions on ℝ are flat everywhere. But there are other, less trivial, examples.\n\nThe function defined by\n\nis flat at \"x\" = 0. Thus, this is an example of a non-analytic smooth function.\n"}
{"id": "164040", "url": "https://en.wikipedia.org/wiki?curid=164040", "title": "Formula", "text": "Formula\n\nIn science, a formula is a concise way of expressing information symbolically, as in a mathematical formula or a chemical formula. The informal use of the term \"formula\" in science refers to the general construct of a relationship between given quantities. \n\nThe plural of \"formula\" can be spelled either as \"formulas\" (from the most common English plural noun form) or, under the influence of scientific Latin, \"formulae\" (from the original Latin).\n\nIn mathematics, a formula is an entity constructed using the symbols and formation rules of a given logical language. For example, determining the volume of a sphere requires a significant amount of integral calculus or its geometrical analogue, the method of exhaustion; but, having done this once in terms of some parameter (the radius for example), mathematicians have produced a formula to describe the volume:\n\nHaving obtained this result, the volume of any sphere can be computed as long as its radius is known. Note that the volume \"V\" and the radius \"r\" are expressed as single letters instead of words or phrases. This convention, while less important in a relatively simple formula, means that mathematicians can more quickly manipulate larger and more complex formulas. Mathematical formulas are often algebraic, closed form, and/or analytical.\n\nIn modern chemistry, a chemical formula is a way of expressing information about the proportions of atoms that constitute a particular chemical compound, using a single line of chemical element symbols, numbers, and sometimes other symbols, such as parentheses, brackets, and plus (+) and minus (−) signs. For example, HO is the chemical formula for water, specifying that each molecule consists of two hydrogen (H) atoms and one oxygen (O) atom. Similarly, O denotes an ozone molecule consisting of three oxygen atoms and having a net negative charge.\n\nIn a general context, formulas are applied to provide a mathematical solution for real world problems. Some may be general: for example,\n\nThis is one expression of Newton's second law, is applicable to a wide range of physical situations. Other formulas may be created to solve a particular problem: for example, using the equation of a sine curve to model the movement of the tides in a bay. In all cases, however, formulas form the basis for calculations.\n\nExpressions are distinct from formulas in that they cannot contain an equals sign (=). When comparing formulas to grammatical sentences, expressions are more like phrases.\n\n-\\overset{\\displaystyle H \\atop |}{\\underset{\\underset{\\underset{| \\atop \\displaystyle H}{C}}-H</chem>|caption=The structural formula for butane. There are three common non-pictorial types of chemical formulas for this molecule:\n\nA chemical formula identifies each constituent element by its chemical symbol and indicates the proportionate number of atoms of each element.\n\nIn empirical formulas, these proportions begin with a key element and then assign numbers of atoms of the other elements in the compound, as ratios to the key element. For molecular compounds, these ratio numbers can all be expressed as whole numbers. For example, the empirical formula of ethanol may be written CHO because the molecules of ethanol all contain two carbon atoms, six hydrogen atoms, and one oxygen atom. Some types of ionic compounds, however, cannot be written with entirely whole-number empirical formulas. An example is boron carbide, whose formula of CB is a variable non-whole number ratio with n ranging from over 4 to more than 6.5.\n\nWhen the chemical compound of the formula consists of simple molecules, chemical formulas often employ ways to suggest the structure of the molecule. There are several types of these formulas, including molecular formulas and condensed formulas. A molecular formula enumerates the number of atoms to reflect those in the molecule, so that the molecular formula for glucose is CHO rather than the glucose empirical formula, which is CHO. Except for very simple substances, molecular chemical formulas lack needed structural information, and are ambiguous.\n\nA structural formula is a drawing that shows the location of each atom, and which atoms it binds to.\n\nIn computing, a formula typically describes a calculation, such as addition, to be performed on one or more variables. A formula is often implicitly provided in the form of a computer instruction such as.\n\nIn computer spreadsheet software, a formula indicating how to compute the value of a cell, say \"A3\", would be written as\n\nwhere \"A1\" and \"A2\" refer to other cells (column A, row 1 or 2) within the spreadsheet. This is a shortcut for the \"paper\" form \"A3 = A1+A2\" where \"A3\" is, by convention, omitted because the result is always stored in the cell itself and stating its name would be redundant.\n\nA physical quantity can be expressed as the product of a number and a physical unit. A formula expresses a relationship between physical quantities. A necessary condition for a formula to be valid is that all terms have the same dimension, meaning every term in the formula could be potentially converted to contain the identical unit (or product of identical units).\n\nIn the example of the volume of a sphere () we may wish to compute with \"r\" = 2.0 cm, which yields\n\nThere is vast educational training about retaining units in computations, and converting units to a desirable form, such as in units conversion by factor-label.\n\nThe vast majority of computations with measurements are done in computer programs with no facility for retaining a symbolic computation of the units. Only the numerical quantity is used in the computation. This requires that the universal formula be converted to a formula that is intended to be used only with prescribed units, meaning the numerical quantity is implicitly assumed to be multiplying a particular unit. The requirements about the prescribed units must be given to users of the input and the output of the formula.\n\nFor example, suppose the formula is to require that formula_3, where tbsp is the US tablespoon and VOL is the name for the number used by the computer. Similarly, the formula is to require\nformula_4. The derivation of the formula proceeds as:\n\nGiven that formula_6,\nthe formula with prescribed units is\n\nThe formula is not complete without words such as:\n\"VOL is volume in tbsp and RAD is radius in cm\".\nOther possible words are \"VOL is the ratio of formula_8 to tbsp and RAD is the ratio of formula_9 to cm.\"\n\nThe formula with prescribed units could also appear with simple symbols,\nperhaps even the identical symbols as in the original dimensional formula:\nand the accompanying words could be: \"where V is volume (tbsp) and r is radius (cm)\".\n\nIf the physical formula is not dimensionally homogeneous, and therefore erroneous,\nthe falsehood becomes apparent in the impossibility\nto derive a formula with prescribed units. It would not be possible to\nderive a formula consisting only of numbers and dimensionless ratios.\n\nFormulas used in science almost always require a choice of units. Formulas are used to express relationships between various quantities, such as temperature, mass, or charge in physics; supply, profit, or demand in economics; or a wide range of other quantities in other disciplines.\n\nAn example of a formula used in science is Boltzmann's entropy formula. In statistical thermodynamics, it is a probability equation relating the entropy \"S\" of an ideal gas to the quantity \"W\", which is the number of microstates corresponding to a given macrostate:\n\nwhere \"k\" is Boltzmann's constant equal to 1.38062 x 10 joule/kelvin and \"W\" is the number of microstates consistent with the given macrostate.\n\n\n"}
{"id": "2445607", "url": "https://en.wikipedia.org/wiki?curid=2445607", "title": "Formulario mathematico", "text": "Formulario mathematico\n\nFormulario Mathematico (Latino sine flexione: \"Formulation of mathematics\") is a book by Giuseppe Peano which expresses fundamental theorems of mathematics in a symbolic language developed by Peano. The author was assisted by Giovanni Vailati, Mario Pieri, Alessandro Padoa, Giovanni Vacca, Vincenzo Vivanti, Gino Fano and Cesare Burali-Forti.\n\nThe \"Formulario\" was first published in 1895. The fifth and last edition was published in 1908. \n\nKennedy wrote \"the development and use of mathematical logic is the guiding motif of the project\". He also explains the variety of Peano's publication under the title:\n\nPeano believed that students needed only precise statement of their lessons. He wrote:\nSuch a dismissal of the oral tradition in lectures at universities was the undoing of Peano's own teaching career.\n\n"}
{"id": "27801252", "url": "https://en.wikipedia.org/wiki?curid=27801252", "title": "Frege system", "text": "Frege system\n\nIn proof complexity, a Frege system is a propositional proof system whose proofs are sequences of formulas derived using a finite set of sound and implicationally complete inference rules. Frege systems (more often known as Hilbert systems in general proof theory) are named after Gottlob Frege.\n\nLet \"K\" be a finite functionally complete set of Boolean connectives, and consider propositional formulas built from variables \"p\", \"p\", \"p\", ... using \"K\"-connectives. A Frege rule is an inference rule of the form\nwhere \"B\", ..., \"B\", \"B\" are formulas. If \"R\" is a finite set of Frege rules, then \"F\" = (\"K\",\"R\") defines a derivation system in the following way. If \"X\" is a set of formulas, and \"A\" is a formula, then an \"F\"-derivation of \"A\" from axioms \"X\" is a sequence of formulas \"A\", ..., \"A\" such that \"A\" = \"A\", and every \"A\" is a member of \"X\", or it is derived from some of the formulas \"A\", \"i\" < \"k\", by a substitution instance of a rule from \"R\". An \"F\"-proof of a formula \"A\" is an \"F\"-derivation of \"A\" from the empty set of axioms (X=∅). \"F\" is called a Frege system if\nThe length (number of lines) in a proof \"A\", ..., \"A\" is \"m\". The size of the proof is the total number of symbols.\n\nA derivation system \"F\" as above is refutationally complete, if for every inconsistent set of formulas \"X\", there is an \"F\"-derivation of a fixed contradiction from \"X\".\n\n\n"}
{"id": "2493181", "url": "https://en.wikipedia.org/wiki?curid=2493181", "title": "Full Domain Hash", "text": "Full Domain Hash\n\nIn cryptography, the Full Domain Hash (FDH) is an RSA-based signature scheme that follows the \"hash-and-sign\" paradigm. It is provably secure (i.e., is existentially unforgeable under adaptive chosen-message attacks) in the random oracle model. FDH involves hashing a message using a function whose image size equals the size of the RSA modulus, and then raising the result to the secret RSA exponent.\n\nIn the random oracle model, if RSA is formula_1-secure, then the full domain hash RSA signature scheme is formula_2-secure where,\n\nFor large formula_4 this reduces to formula_5.\n\nThis means that if there exists an algorithm that can forge a new FDH signature that runs in time \"t\", computes at most formula_6 hashes, asks for at most formula_4 signatures and succeeds with probability formula_8, then there must also exist an algorithm that breaks RSA with probability formula_9 in time formula_10.\n\n"}
{"id": "301521", "url": "https://en.wikipedia.org/wiki?curid=301521", "title": "Functional integration", "text": "Functional integration\n\nFunctional integration is a collection of results in mathematics and physics where the domain of an integral is no longer a region of space, but a space of functions. Functional integrals arise in probability, in the study of partial differential equations, and in the path integral approach to the quantum mechanics of particles and fields.\n\nIn an ordinary integral there is a function to be integrated (the integrand) and a region of space over which to integrate the function (the domain of integration). The process of integration consists of adding up the values of the integrand for each point of the domain of integration. Making this procedure rigorous requires a limiting procedure, where the domain of integration is divided into smaller and smaller regions. For each small region, the value of the integrand cannot vary much, so it may be replaced by a single value. In a functional integral the domain of integration is a space of functions. For each function, the integrand returns a value to add up. Making this procedure rigorous poses challenges that continue to be topics of current research.\n\nFunctional integration was developed by Percy John Daniell in an article of 1919 and Norbert Wiener in a series of studies culminating in his articles of 1921 on Brownian motion. They developed a rigorous method (now known as the Wiener measure) for assigning a probability to a particle's random path. Richard Feynman developed another functional integral, the path integral, useful for computing the quantum properties of systems. In Feynman's path integral, the classical notion of a unique trajectory for a particle is replaced by an infinite sum of classical paths, each weighted differently according to its classical properties.\n\nFunctional integration is central to quantization techniques in theoretical physics. The algebraic properties of functional integrals are used to develop series used to calculate properties in quantum electrodynamics and the standard model of particle physics.\n\nWhereas standard Riemann integration sums a function \"f\"(\"x\") over a continuous range of values of \"x\", functional integration sums a functional \"G\"[\"f\"], which can be thought of as a \"function of a function\" over a continuous range (or space) of functions \"f\". Most functional integrals cannot be evaluated exactly but must be evaluated using perturbation methods. The formal definition of a functional integral is\n\nHowever, in most cases the functions \"f\"(\"x\") can be written in terms of an infinite series of orthogonal functions such as formula_2, and then the definition becomes\n\nwhich is slightly more understandable. The integral is shown to be a functional integral with a capital \"D\". Sometimes it is written in square brackets: [\"Df\"] or \"D\"[\"f\"], to indicate that \"f\" is a function.\n\nMost functional integrals are actually infinite, but the quotient of two functional integrals can be finite. The functional integrals that can be solved exactly usually start with the following Gaussian integral:\n\nBy functionally differentiating this with respect to \"J\"(\"x\") and then setting \"J\" to 0 this becomes an exponential multiplied by a polynomial in \"f\". For example, setting formula_5, we find:\n\nwhere \"a\", \"b\" and \"x\" are 4-dimensional vectors. This comes from the formula for the propagation of a photon in quantum electrodynamics. Another useful integral is the functional delta function:\n\nwhich is useful to specify constraints. Functional integrals can also be done over Grassmann-valued functions formula_8, where formula_9, which is useful in quantum electrodynamics for calculations involving fermions.\n\nFunctional integrals where the space of integration consists of paths (\"ν\" = 1) can be defined in many different ways. The definitions fall in two different classes: the constructions derived from Wiener's theory yield an integral based on a measure, whereas the constructions following Feynman's path integral do not. Even within these two broad divisions, the integrals are not identical, that is, they are defined differently for different classes of functions.\n\nIn the Wiener integral, a probability is assigned to a class of Brownian motion paths. The class consists of the paths \"w\" that are known to go through a small region of space at a given time. The passage through different regions of space is assumed independent of each other, and the distance between any two points of the Brownian path is assumed to be Gaussian-distributed with a variance that depends on the time \"t\" and on a diffusion constant \"D\":\n\nThe probability for the class of paths can be found by multiplying the probabilities of starting in one region and then being at the next. The Wiener measure can be developed by considering the limit of many small regions.\n\n\n\n\n\n"}
{"id": "12308", "url": "https://en.wikipedia.org/wiki?curid=12308", "title": "Gregory Chaitin", "text": "Gregory Chaitin\n\nGregory John Chaitin ( ; born 15 November 1947) is an Argentine-American mathematician and computer scientist. Beginning in the late 1960s, Chaitin made contributions to algorithmic information theory and metamathematics, in particular a computer-theoretic result equivalent to Gödel's incompleteness theorem. He is considered to be one of the founders of what is today known as Kolmogorov (or Kolmogorov-Chaitin) complexity together with Andrei Kolmogorov and Ray Solomonoff. Today, algorithmic information theory is a common subject in any computer science curriculum. \n\nHe attended the Bronx High School of Science and City College of New York, where he (still in his teens) developed the theory that led to his independent discovery of Kolmogorov complexity.\n\nChaitin has defined Chaitin's constant Ω, a real number whose digits are equidistributed and which is sometimes informally described as an expression of the probability that a random program will halt. Ω has the mathematical property that it is definable but not computable.\n\nChaitin's early work on algorithmic information theory followed after the work of Solomonoff, Kolmogorov, and Martin-Löf.\n\nChaitin is also the originator of using graph coloring to do register allocation in compiling, a process known as Chaitin's algorithm.\n\nHe was formerly a researcher at IBM's Thomas J. Watson Research Center in New York and remains an emeritus researcher. He has written more than 10 books that have been translated to about 15 languages. He is today interested in questions of metabiology and information-theoretic formalizations of the theory of evolution.\n\nChaitin also writes about philosophy, especially metaphysics and philosophy of mathematics (particularly about epistemological matters in mathematics). In metaphysics, Chaitin claims that algorithmic information theory is the key to solving problems in the field of biology (obtaining a formal definition of 'life', its origin and evolution) and neuroscience (the problem of consciousness and the study of the mind).\n\nIn recent writings, he defends a position known as digital philosophy. In the epistemology of mathematics, he claims that his findings in mathematical logic and algorithmic information theory show there are \"mathematical facts that are true for no reason, they're true by accident. They are random mathematical facts\". Chaitin proposes that mathematicians must abandon any hope of proving those mathematical facts and adopt a quasi-empirical methodology.\n\nIn 1995 he was given the degree of doctor of science \"honoris causa\" by the University of Maine. In 2002 he was given the title of honorary professor by the University of Buenos Aires in Argentina, where his parents were born and where Chaitin spent part of his youth. In 2007 he was given a Leibniz Medal by Wolfram Research. In 2009 he was given the degree of doctor of philosophy \"honoris causa\" by the National University of Córdoba. He was formerly a researcher at IBM's Thomas J. Watson Research Center and is now a professor at the Federal University of Rio de Janeiro.\n\nSome philosophers and logicians disagree with the philosophical conclusions that Chaitin has drawn from his theorems related to what Chaitin thinks is a kind of fundamental arithmetic randomness.\nThe logician Torkel Franzén criticized Chaitin’s interpretation of Gödel's incompleteness theorem and the alleged explanation for it that Chaitin’s work represents.\n\n\n\n"}
{"id": "34383018", "url": "https://en.wikipedia.org/wiki?curid=34383018", "title": "Hugh William Segar", "text": "Hugh William Segar\n\nHugh William Segar (1868–1954) was a notable New Zealand mathematician and university professor. He was born in Liverpool, Lancashire, England in 1868.\n"}
{"id": "19327282", "url": "https://en.wikipedia.org/wiki?curid=19327282", "title": "Hyperinteger", "text": "Hyperinteger\n\nIn non-standard analysis, a hyperinteger \"n\" is a hyperreal number that is equal to its own integer part. A hyperinteger may be either finite or infinite. A finite hyperinteger is an ordinary integer. An example of an infinite hyperinteger is given by the class of the sequence in the ultrapower construction of the hyperreals.\n\nThe standard integer part function: \nis defined for all real \"x\" and equals the greatest integer not exceeding \"x\". By the transfer principle of non-standard analysis, there exists a natural extension: \ndefined for all hyperreal \"x\", and we say that \"x\" is a hyperinteger if: \nThus the hyperintegers are the image of the integer part function on the hyperreals.\n\nThe set formula_4 of all hyperintegers is an internal subset of the hyperreal line formula_5. The set of all finite hyperintegers (i.e. formula_6 itself) is not an internal subset. Elements of the complement\n\nare called, depending on the author, \"non-standard\", \"unlimited\", or \"infinite\" hyperintegers. The reciprocal of an infinite hyperinteger is always an infinitesimal.\n\nNonnegative hyperintegers are sometimes called \"hypernatural\" numbers. Similar remarks apply to the sets formula_8 and formula_9. Note that the latter gives a non-standard model of arithmetic in the sense of Skolem.\n\n"}
{"id": "41553887", "url": "https://en.wikipedia.org/wiki?curid=41553887", "title": "Janet basis", "text": "Janet basis\n\nIn mathematics, a Janet basis is a normal form for systems of linear homogeneous partial differential equations (PDEs) that removes the inherent arbitrariness of any such system. It was introduced in 1920 by Maurice Janet. It was first called the Janet basis by Fritz Schwarz in 1998.\n\nThe left hand sides of such systems of equations may be considered as differential polynomials of a ring, and Janet's normal form as a special basis of the ideal that they generate. By abuse of language, this terminology will be applied both to the original system and the ideal of differential polynomials generated by the left hand sides. A Janet basis is the predecessor of a Gröbner basis introduced by Bruno Buchberger for polynomial ideals. In order to generate a Janet basis for any given system of linear pde's a ranking of its derivatives must be provided; then the corresponding Janet basis is unique. If a system of linear pde's is given in terms of a Janet basis its differential dimension may easily be determined; it is a measure for the degree of indeterminacy of its general solution. In order to generate a Loewy decomposition of a system of linear pde's its Janet basis must be determined first.\n\nAny system of linear homogeneous pde's is highly non-unique, e.g. an arbitrary linear combination of its elements may be added to the system without changing its solution set. A priori it is not known whether it has any nontrivial solutions. More generally, the degree of arbitrariness of its general solution is not known, i.e. how many undetermined constants or functions it may contain. These questions were the starting point of Janet's work; he considered systems of linear pde's in any number of dependent and independent variables and generated a normal form for them. Here mainly linear pde's in the plane with the coordinates formula_1 and formula_2 will be considered; the number of unknown functions is one or two. Most results described here may be generalized in an obvious way to any number of variables or functions.\nIn order to generate a unique representation for a given system of linear pde's, at first a ranking of its derivatives must be defined.\n\nDefinition\nA ranking of derivatives is a total ordering such that for any two derivatives formula_3, formula_4 and\nformula_5, and any derivation operator formula_6 the relations formula_7 and\nformula_8 are valid.\n\nA derivative formula_5 is called \"higher\" than formula_4 if formula_11. The highest\nderivative in an equation is called its \"leading derivative\". For the derivatives up to order two of a single function formula_12 depending on formula_13 and formula_14 with formula_15 two possible order are\n\nHere the usual notation formula_20 is used. If the number of functions is higher than one, these orderings have to be generalized appropriately, e.g. the orderings formula_21 or formula_22 may be applied.\nThe first basic operation to be applied in generating a Janet basis is the \"reduction\" of an equation formula_23 w.r.t. another one formula_24. In colloquial terms this means the following: Whenever a derivative of formula_23 may be\nobtained from the leading derivative of formula_24 by suitable differentiation, this differentiation is performed and the\nresult is subtracted from formula_23. Reduction w.r.t. a system of pde's means reduction w.r.t. all elements of the system. A system of linear pde's is called \"autoreduced\" if all possible reductions have been performed.\n\nThe second basic operation for generating a Janet basis is the inclusion of \"integrability conditions\". They are obtained\nas follows: If two equations formula_23 and formula_24 are such that by suitable differentiations two new equations\nmay be obtained with like leading derivatives, by cross-multiplication with its leading coefficients and subtraction of the resulting equations a new equation is obtained, it is called an integrability condition. If by reduction w.r.t. the remaining equations of the system it does not vanish it is included as a new equation to the system.\n\nIt may be shown that repeating these operations always terminates after a finite number of steps with a unique answer which is called the Janet basis for the input system. Janet has organized them in terms of the following algorithm.\n\nJanet's Algorithm Given a system of linear differential polynomials formula_30, the Janet basis corresponding to formula_31 is returned.\n\nand determine the integrability conditions\n\nHere formula_48 is a subalgorithm that returns its argument with all possible reductions performed, formula_49 adds certain equations to the system in order to facilitate determining the integrability conditions. To this\nend the variables are divides into \"multipliers\" and \"non-multipliers\"; details may be found in the above references. Upon successful termination a Janet basis for the input system will be returned.\n\nExample 1 Let the system formula_50 be given with ordering formula_51 and formula_15. Step S1 returns the autoreduced system\n\nSteps S3 and S4 generate the integrability condition formula_54 and reduces it to formula_55, i.e. the Janet basis for the originally given system is formula_56 with the trivial solution formula_55.\n\nThe next example involves two unknown functions formula_58 and formula_12, both depending on formula_13 and formula_14.\n\nExample 2 Consider the system\n\nin formula_63 ordering. The system is already autoreduced, i.e. step S1 returns it unchanged. Step S3 generates the two integrability conditions\n\nUpon reduction in step S4 they are\n\nIn step S5 they are included into the system and the algorithms starts again with step S1 with the extended system. After a few more iterations finally the Janet basis\n\nis obtained. It yields the general solution formula_67 with two undetermined constants formula_68 and formula_69.\n\nThe most important application of a Janet basis is its use for deciding the degree of indeterminacy of a system of linear homogeneous partial differential equations. The answer in the above Example 1 is that the system under consideration allows only the trivial solution. In the second Example 2 a two-dimensional solution space is obtained. In general, the answer may be more involved, there may be infinitely many free constants in the general solution; they may be obtained from the Loewy decomposition of the respective Janet basis. Furthermore, the Janet basis of a module allows to read off a Janet basis for the syzygy module.\n\nJanet's algorithm has been implemented in Maple. \n\n"}
{"id": "15807", "url": "https://en.wikipedia.org/wiki?curid=15807", "title": "John Horton Conway", "text": "John Horton Conway\n\nJohn Horton Conway FRS (born 26 December 1937) is an English mathematician active in the theory of finite groups, knot theory, number theory, combinatorial game theory and coding theory. He has also contributed to many branches of recreational mathematics, notably the invention of the cellular automaton called the Game of Life. Conway is currently Professor Emeritus of Mathematics at Princeton University in New Jersey.\n\nConway was born in Liverpool, the son of Cyril Horton Conway and Agnes Boyce. He became interested in mathematics at a very early age; his mother has recalled that he could recite the powers of two when he was four years old. By the age of eleven his ambition was to become a mathematician.\n\nAfter leaving sixth form, Conway entered Gonville and Caius College, Cambridge to study mathematics. Conway, who was a \"terribly introverted adolescent\" in school, interpreted his admission to Cambridge as an opportunity to transform himself into a new person: an \"extrovert\".\n\nHe was awarded his Bachelor of Arts degree in 1959 and began to undertake research in number theory supervised by Harold Davenport. Having solved the open problem posed by Davenport on writing numbers as the sums of fifth powers, Conway began to become interested in infinite ordinals. It appears that his interest in games began during his years studying the Cambridge Mathematical Tripos, where he became an avid backgammon player, spending hours playing the game in the common room. He was awarded his doctorate in 1964 and was appointed as College Fellow and Lecturer in Mathematics at the University of Cambridge.\n\nAfter leaving Cambridge in 1986, he took up the appointment to the John von Neumann Chair of Mathematics at Princeton University.\n\nConway is especially known for the invention of the Game of Life, one of the early examples of a cellular automaton. His initial experiments in that field were done with pen and paper, long before personal computers existed.\n\nSince the game was introduced by Martin Gardner in \"Scientific American\" in 1970, it has spawned hundreds of computer programs, web sites, and articles. It is a staple of recreational mathematics. There is an extensive wiki devoted to curating and cataloging the various aspects of the game. From the earliest days it has been a favorite in computer labs, both for its theoretical interest and as a practical exercise in programming and data display. At times Conway has said he hates the Game of Life–largely because it has come to overshadow some of the other deeper and more important things he has done. Nevertheless, the game did help launch a new branch of mathematics, the field of cellular automata.\n\nThe Game of Life is now known to be Turing complete.\n\nConway's career is intertwined with mathematics popularizer and \"Scientific American\" columnist Martin Gardner. When Gardner featured Conway's Game of Life in his Mathematical Games column in October 1970, it became the most widely read of all his columns and made Conway an instant celebrity. Gardner and Conway had first corresponded in the late 1950s, and over the years Gardner had frequently written about recreational aspects of Conway's work. For instance, he discussed Conway's game of Sprouts (Jul 1967), Hackenbush (Jan 1972), and his angel and devil problem (Feb 1974). In the September 1976 column he reviewed Conway's book \"On Numbers and Games\" and introduced the public to Conway's surreal numbers. Conferences called Gathering 4 Gardner are held every two years to celebrate the legacy of Martin Gardner, and Conway himself has often been a featured speaker at these events, discussing various aspects of recreational mathematics.\n\nConway is widely known for his contributions to combinatorial game theory (CGT), a theory of partisan games. This he developed with Elwyn Berlekamp and Richard Guy, and with them also co-authored the book \"Winning Ways for your Mathematical Plays\". He also wrote the book \"On Numbers and Games\" (\"ONAG\") which lays out the mathematical foundations of CGT.\n\nHe is also one of the inventors of sprouts, as well as philosopher's football. He developed detailed analyses of many other games and puzzles, such as the Soma cube, peg solitaire, and Conway's soldiers. He came up with the angel problem, which was solved in 2006.\n\nHe invented a new system of numbers, the surreal numbers, which are closely related to certain games and have been the subject of a mathematical novel by Donald Knuth. He also invented a nomenclature for exceedingly large numbers, the Conway chained arrow notation. Much of this is discussed in the 0th part of \"ONAG\".\n\nIn the mid-1960s with Michael Guy, son of Richard Guy, Conway established that there are sixty-four convex uniform polychora excluding two infinite sets of prismatic forms. They discovered the grand antiprism in the process, the only non-Wythoffian uniform polychoron. Conway has also suggested a system of notation dedicated to describing polyhedra called Conway polyhedron notation.\n\nIn the theory of tessellations, he devised the Conway criterion which describes rules for deciding if a prototile will tile the plane.\n\nHe investigated lattices in higher dimensions, and was the first to determine the symmetry group of the Leech lattice.\n\nIn knot theory, Conway formulated a new variation of the Alexander polynomial and produced a new invariant now called the Conway polynomial. After lying dormant for more than a decade, this concept became central to work in the 1980s on the novel knot polynomials. Conway further developed tangle theory and invented a system of notation for tabulating knots, nowadays known as Conway notation, while correcting a number of errors in the 19th century knot tables and extending them to include all but four of the non-alternating primes with 11 crossings [Topology Proceedings 7 (1982) 118].\n\nHe was the primary author of the \"ATLAS of Finite Groups\" giving properties of many finite simple groups. Working with his colleagues Robert Curtis and Simon P. Norton he constructed the first concrete representations of some of the sporadic groups. More specifically, he discovered three sporadic groups based on the symmetry of the Leech lattice, which have been designated the Conway groups. This work made him a key player in the successful classification of the finite simple groups.\n\nBased on a 1978 observation by mathematician John McKay, Conway and Norton formulated the complex of conjectures known as monstrous moonshine. This subject, named by Conway, relates the monster group with elliptic modular functions, thus bridging two previously distinct areas of mathematics–finite groups and complex function theory. Monstrous moonshine theory has now been revealed to also have deep connections to string theory.\n\nConway introduced the Mathieu groupoid, an extension of the Mathieu group M to 13 points.\n\nAs a graduate student, he proved one case of a conjecture by Edward Waring, that in which every integer could be written as the sum of 37 numbers, each raised to the fifth power, though Chen Jingrun solved the problem independently before the work could be published.\n\nConway has written textbooks and done original work in algebra, focusing particularly on quaternions and octonions. Together with Neil Sloane, he invented the icosians.\n\nHe invented a base 13 function as a counterexample to the converse of the intermediate value theorem: the function takes on every real value in each interval on the real line, so it has a Darboux property but is \"not\" continuous.\n\nFor calculating the day of the week, he invented the Doomsday algorithm. The algorithm is simple enough for anyone with basic arithmetic ability to do the calculations mentally. Conway can usually give the correct answer in under two seconds. To improve his speed, he practices his calendrical calculations on his computer, which is programmed to quiz him with random dates every time he logs on. One of his early books was on finite state machines.\n\nIn 2004, Conway and Simon B. Kochen, another Princeton mathematician, proved the free will theorem, a startling version of the 'no hidden variables' principle of quantum mechanics. It states that given certain conditions, if an experimenter can freely decide what quantities to measure in a particular experiment, then elementary particles must be free to choose their spins to make the measurements consistent with physical law. In Conway's provocative wording: \"if experimenters have free will, then so do elementary particles.\"\n\nConway received the Berwick Prize (1971), was elected a Fellow of the Royal Society (1981), was the first recipient of the Pólya Prize (LMS) (1987), won the Nemmers Prize in Mathematics (1998) and received the Leroy P. Steele Prize for Mathematical Exposition (2000) of the American Mathematical Society.\nHis nomination, in 1981, reads: \n\nIn 2017 Conway was given honorary membership of the (British) Mathematical Association.\n\n\n\n"}
{"id": "161246", "url": "https://en.wikipedia.org/wiki?curid=161246", "title": "Karl Wilhelm Feuerbach", "text": "Karl Wilhelm Feuerbach\n\nKarl Wilhelm von Feuerbach (30 May 1800 – 12 March 1834) was a German geometer and the son of legal scholar Paul Johann Anselm Ritter von Feuerbach, and the brother of philosopher Ludwig Feuerbach. After receiving his doctorate at age 22, he became a professor of mathematics at the Gymnasium at Erlangen. In 1822 he wrote a small book on mathematics noted mainly for a theorem on the nine-point circle, which is now known as Feuerbach's theorem. In 1827 he introduced homogeneous coordinates, independently of Möbius.\n\n\n"}
{"id": "36749660", "url": "https://en.wikipedia.org/wiki?curid=36749660", "title": "Matroid minor", "text": "Matroid minor\n\nIn the mathematical theory of matroids, a minor of a matroid \"M\" is another matroid \"N\" that is obtained from \"M\" by a sequence of restriction and contraction operations. Matroid minors are closely related to graph minors, and the restriction and contraction operations by which they are formed correspond to edge deletion and edge contraction operations in graphs. The theory of matroid minors leads to structural decompositions of matroids, and characterizations of matroid families by forbidden minors, analogous to the corresponding theory in graphs.\n\nIf \"M\" is a matroid on the set \"E\" and \"S\" is a subset of \"E\", then the restriction of \"M\" to \"S\", written \"M\" |\"S\", is the matroid on the set \"S\" whose independent sets are the independent sets of \"M\" that are contained in \"S\". Its circuits are the circuits of \"M\" that are contained in \"S\" and its rank function is that of \"M\" restricted to subsets of \"S\".\n\nIf \"T\" is an independent subset of \"E\", the contraction of \"M\" by \"T\", written \"M\"/\"T\", is the matroid on the underlying set \"E − T\" whose independent sets are the sets whose union with \"T\" is independent in \"M\". This definition may be extended to arbitrary \"T\" by choosing a basis for \"T\" and defining a set to be independent in the contraction if its union with this basis remains independent in \"M\". The rank function of the contraction is formula_1\n\nA matroid \"N\" is a minor of a matroid \"M\" if it can be constructed from \"M\" by restriction and contraction operations.\n\nIn terms of the geometric lattice formed by the flats of a matroid, taking a minor of a matroid corresponds to taking an interval of the lattice, the part of the lattice lying between a given lower bound and upper bound element.\n\nMany important families of matroids are closed under the operation of taking minors: if a matroid \"M\" belongs to the family, then every minor of \"M\" also belongs to the family. In this case, the family may be characterized by its set of \"forbidden matroids\", the minor-minimal matroids that do not belong to the family. A matroid belongs to the family if and only if it does not have a forbidden matroid as a minor. Often, but not always, the set of forbidden matroids is finite, paralleling the Robertson–Seymour theorem which states that the set of forbidden minors of a minor-closed graph family is always finite.\n\nAn example of this phenomenon is given by the regular matroids, matroids that are representable over all fields. Equivalently a matroid is regular if it can be represented by a totally unimodular matrix (a matrix whose square submatrices all have determinants equal to 0, 1, or −1). proved that a matroid is regular if and only if it does not have one of three forbidden minors: the uniform matroid formula_2 (the four-point line), the Fano plane, or the dual matroid of the Fano plane. For this he used his difficult homotopy theorem. Simpler proofs have since been found.\n\nThe graphic matroids, matroids whose independent sets are the forest subgraphs of a graph, have five forbidden minors: the three for the regular matroids, and the two duals of the graphic matroids for the graphs \"K\" and \"K\" that by Wagner's theorem are forbidden minors for the planar graphs.\n\nThe binary matroids, matroids representable over the two-element finite field, include both graphic and regular matroids. Tutte again showed that these matroids have a forbidden minor characterization: they are the matroids that do not have the four-point line as a minor. Rota conjectured that, for any finite field, the matroids representable over that field have finitely many forbidden minors. A full proof of this conjecture has been announced by Geelen, Gerards, and Whittle; it has not appeared. However, the matroids that can be represented over the real numbers have infinitely many forbidden minors.\n\nBranch-decompositions of matroids may be defined analogously to their definition for graphs.\nA branch-decomposition of a matroid is a hierarchical clustering of the matroid elements, represented as an unrooted binary tree with the elements of the matroid at its leaves. Removing any edge of this tree partitions the matroids into two disjoint subsets; such a partition is called an e-separation. If \"r\" denotes the rank function of the matroid, then the width of an e-separation is defined as . The width of a decomposition is the maximum width of any of its e-separations, and the branchwidth of a matroid is the minimum width of any of its branch-decompositions.\n\nThe branchwidth of a graph and the branchwidth of the corresponding graphic matroid may differ: for instance, the three-edge path graph and the three-edge star have different branchwidths, 2 and 1 respectively, but they both induce the same graphic matroid with branchwidth 1. However, for graphs that are not trees, the branchwidth of the graph is equal to the branchwidth of its associated graphic matroid. The branchwidth of a matroid always equals the branchwidth of its dual.\n\nBranchwidth is an important component of attempts to extend the theory of graph minors to matroids: although treewidth can also be generalized to matroids, and plays a bigger role than branchwidth in the theory of graph minors, branchwidth has more convenient properties in the matroid setting.\nIf a minor-closed family of matroids representable over a finite field does not include the graphic matroids of all planar graphs, then there is a constant bound on the branchwidth of the matroids in the family, generalizing similar results for minor-closed graph families.\n\nThe Robertson–Seymour theorem implies that every matroid property of \"graphic\" matroids characterized by a list of forbidden minors can be characterized by a finite list. Another way of saying the same thing is that the partial order on graphic matroids formed by the minor operation is a well-quasi-ordering. However, the example of the real-representable matroids, which have infinitely many forbidden minors, shows that the minor ordering is not a well-quasi-ordering on all matroids.\n\nRobertson and Seymour conjectured that the matroids representable over any particular finite field are well-quasi-ordered. So far this has been proven only for the matroids of bounded branchwidth.\n\nThe graph structure theorem is an important tool in the theory of graph minors, according to which the graphs in any minor-closed family can be built up from simpler graphs by clique-sum operations. Some analogous results are also known in matroid theory. In particular, Seymour's decomposition theorem states that all regular matroids can be built up in a simple way as the clique-sum of graphic matroids, their duals, and one special 10-element matroid. As a consequence, linear programs defined by totally unimodular matrices may be solved combinatorially by combining the solutions to a set of minimum spanning tree problems corresponding to the graphic and co-graphic parts of this decomposition.\n\nOne of the important components of graph minor theory is the existence of an algorithm for testing whether a graph \"H\" is a minor of another graph \"G\", taking an amount of time that is polynomial in \"G\" for any fixed choice of \"H\" (and more strongly fixed-parameter tractable if the size of \"H\" is allowed to vary). By combining this result with the Robertson–Seymour theorem, it is possible to recognize the members of any minor-closed graph family in polynomial time. Correspondingly, in matroid theory, it would be desirable to develop efficient algorithms for recognizing whether a given fixed matroid is a minor of an input matroid. Unfortunately, such a strong result is not possible: in the matroid oracle model, the only minors that can be recognized in polynomial time are the uniform matroids with rank or corank one. However, if the problem is restricted to the matroids that are representable over some fixed finite field (and represented as a matrix over that field) then, as in the graph case, it is conjectured to be possible to recognize the matroids that contain any fixed minor in polynomial time.\n\n"}
{"id": "1773278", "url": "https://en.wikipedia.org/wiki?curid=1773278", "title": "Model of computation", "text": "Model of computation\n\nIn computer science, and more specifically in computability theory and computational complexity theory, a model of computation is a model which describes how a set of outputs are computed given a set of inputs. This model describes how units of computations, memories, and communications are organized. The computational complexity of an algorithm can be measured given a model of computation. Using a model allows studying the performance of algorithms independently of the variations that are specific to particular implementations and specific technology.\n\nModels of computation can be classified in three categories: sequential models, functional models, and concurrent models.\n\nSequential models include:\n\nFunctional models include:\n\nConcurrent models include:\n\nIn the field of runtime analysis of algorithms, it is common to specify a computational model in terms of \"primitive operations\" allowed which have unit cost, or simply unit-cost operations. A commonly used example is the random access machine, which has unit cost for read and write access to all of its memory cells. In this respect, it differs from the above-mentioned Turing machine model.\n\nIn \"model-driven engineering\", the model of computation explains how the behaviour of the whole system is the result of the behaviour of each of its components.\n\nA key point which is often overlooked is that published lower bounds for problems are often given for a model of computation that is more restricted than the set of operations that one could use in practice and therefore there may be algorithms that are faster than what would naïvely be thought possible.\n\nThere are many models of computation, differing in the set of admissible operations and their computations cost. They fall into the following broad categories: abstract machine and models equivalent to it (e.g. lambda calculus is equivalent to the Turing machine), used in proofs of computability and upper bounds on computational complexity of algorithms, and decision tree models, used in proofs of lower bounds on computational complexity of algorithmic problems.\n\n\n"}
{"id": "13496987", "url": "https://en.wikipedia.org/wiki?curid=13496987", "title": "Nachman Aronszajn", "text": "Nachman Aronszajn\n\nNachman Aronszajn (26 July 1907 – 5 February 1980) was a Polish American mathematician. Aronszajn's main field of study was mathematical analysis. He also contributed to mathematical logic.\n\nAn Ashkenazi Jew, Aronszajn received his Ph.D. from the University of Warsaw, in 1930, in Poland. Stefan Mazurkiewicz was his thesis advisor. He also received a Ph.D. from Paris University, in 1935; this time Maurice Fréchet was his thesis advisor. He joined the Oklahoma A&M faculty, but moved to the University of Kansas in 1951 with his colleague Ainsley Diamond after Diamond, a Quaker, was fired for refusing to sign a newly instituted loyalty oath. Aronszajn retired in 1977. He was a Summerfield Distinguished Scholar from 1964 to his death.\n\nHe introduced, together with Prom Panitchpakdi, the injective metric spaces under the name of \"hyperconvex metric spaces\". Together with Kennan T. Smith, Aronszajn offered proof of the Aronszajn–Smith theorem. Also, the existence of Aronszajn trees was proven by Aronszajn; Aronszajn lines, also named after him, are the lexicographic orderings of Aronszajn trees.\n\nHe also made a contribution to the theory of reproducing kernel Hilbert space. The Moore–Aronszajn theorem is named after him.\n\n"}
{"id": "53059902", "url": "https://en.wikipedia.org/wiki?curid=53059902", "title": "Nondeterministic constraint logic", "text": "Nondeterministic constraint logic\n\nIn theoretical computer science, nondeterministic constraint logic is a combinatorial system in which an orientation is given to the edges of a weighted undirected graph, subject to certain constraints. One can change this orientation by steps in which a single edge is reversed, subject to the same constraints. It is PSPACE-complete to determine whether there exists a sequence of moves that reverses a specified edge.\n\nThis is a form of reversible logic in that each sequence of edge orientation changes can be undone. The hardness of this problem has been used to prove that many games and puzzles have high game complexity.\n\nIn the simplest version of nondeterministic constraint logic, each edge of an undirected graph has weight either one or two. (The weights may also be represented graphically by drawing edges of weight one as red and edges of weight two as blue.) The graph is required to be a cubic graph: each vertex is incident to three edges, and additionally each vertex should be incident to an even number of red edges.\n\nThe edges are required to be oriented in such a way that at least two units of weight are oriented towards each vertex: there must be either at least one incoming blue edge, or at least two incoming red edges. An orientation can change by steps in which a single edge is reversed, respecting these constraints.\n\nMore general forms of nondeterministic constraint logic allow a greater variety of edge weights, more edges per vertex, and different thresholds for how much incoming weight each vertex must have.\nA graph with a system of edge weights and vertex thresholds is called a \"constraint graph\". The restricted case where the edge weights are all one or two, the vertices require two units of incoming weight, and the vertices all have three incident edges with an even number of red edges, are called \"and/or constraint graphs\".\n\nThe reason for the name \"and/or constraint graphs\" is that the two possible types of vertex in an and/or constraint graph behave in some ways like an AND gate and OR gate in Boolean logic. A vertex with two red edges and one blue edge behaves like an AND gate in that it requires both red edges to point inwards before the blue edge can be made to point outwards. A vertex with three blue edges behaves like an OR gate, with two of its edges designated as inputs and the third as an output, in that it requires at least one input edge to point inwards before the output edge can be made to point outwards.\n\nThe following problems, on and/or constraint graphs and their orientations, are PSPACE-complete:\nThe proof that these problems are hard involves a reduction from quantified Boolean formulas, based on the logical interpretation of and/or constraint graphs. It requires additional gadgets for simulating quantifiers and for converting signals carried on red edges into signals carried on blue edges (or vice versa), which can all be accomplished by combinations of and-vertices and or-vertices.\n\nThese problems remain PSPACE-complete even for and/or constraint graphs that form planar graphs. The proof of this involves the construction of crossover gadgets that allow two independent signals to cross each other. It is also possible to impose an additional restriction, while preserving the hardness of these problems: each vertex with three blue edges can be required to be part of a triangle with a red edge. Such a vertex is called a \"protected or\", and it has the property that (in any valid orientation of the whole graph) it is not possible for both of the blue edges in the triangle to be directed inwards. This restriction makes it easier to simulate these vertices in hardness reductions for other problems. Additionally, the contraint graphs can be required to have bounded bandwidth, and the problems on them will still remain PSPACE-complete.\n\nThe original applications of nondeterministic constraint logic used it to prove the PSPACE-completeness of sliding block puzzles such as Rush Hour and Sokoban. To do so, one needs only to show how to simulate edges and edge orientations, and vertices, and protected or vertices in these puzzles.\n\nNondeterministic constraint logic has also been used to prove the hardness of reconfiguration versions of classical graph optimization problems including the independent set, vertex cover, and dominating set, on planar graphs of bounded bandwidth. In these problems, one must change one solution to the given problem into another, by moving one vertex at a time into or out of the solution set while maintaining the property that at all times the remaining vertices form a solution.\n"}
{"id": "3298657", "url": "https://en.wikipedia.org/wiki?curid=3298657", "title": "P-compact group", "text": "P-compact group\n\nIn mathematics, in particular algebraic topology, a \"p\"-compact group is (roughly speaking) a space that is a homotopical version of a compact Lie group, but with all the structure concentrated at a single prime \"p\". This concept was introduced by Dwyer and Wilkerson. Subsequently the name homotopy Lie group has also been used.\n\nExamples include the p-completion of a compact and connected Lie group, and the Sullivan spheres, i.e. the \"p\"-completion of a sphere of dimension\n\nif \"n\" divides \"p\" − 1.\n\nThe classification of p-compact groups states that there is a 1-1 correspondence between connected p-compact groups, and root data over the p-adic integers. This is analogous to the classical classification of connected compact Lie groups, with the p-adic integers replacing the rational integers.\n\n"}
{"id": "1308936", "url": "https://en.wikipedia.org/wiki?curid=1308936", "title": "P system", "text": "P system\n\nA P system is a computational model in the field of computer science that performs calculations using a biologically-inspired process. They are based upon the structure of biological cells, abstracting from the way in which chemicals interact and cross cell membranes. The concept was first introduced in a 1998 report by the computer scientist Gheorghe Păun, whose last name is the origin of the letter P in 'P Systems'. Variations on the P system model led to the formation of a branch of research known as 'membrane computing.'\n\nAlthough inspired by biology, the primary research interest in P systems is concerned with their use as a computational model, rather than for biological modeling, although this is also being investigated.\n\nA P system is defined as a series of membranes containing chemicals (in quantities), catalysts and rules which determine possible ways in which chemicals may react with one another to form products. Rules may also cause chemicals to pass through membranes or even cause membranes to dissolve.\n\nJust as in a biological cell, where a chemical reaction may only take place upon the chance event that the required chemical molecules collide and interact (possibly also with a catalyst), the rules in a P system are applied at random. This causes the computation to proceed in a non-deterministic manner, often resulting in multiple solutions being encountered if the computation is repeated.\n\nA P system continues until it reaches a state where no further reactions are possible. At this point the result of the computation is all those chemicals that have been passed outside of the outermost membrane, or otherwise those passed into a designated 'result' membrane.\n\nAlthough many varieties of P system exist, most share the same basic components. Each element has a specific role to play, and each has a founding in the biological cell architecture upon which P systems are based.\n\nThe environment is the surroundings of the P system. In the initial state of a P system it contains only the container-membrane, and while the environment can never hold rules, it may have objects passed into it during the computation. The objects found within the environment at the end of the computation constitute all or part of its “result.”\n\nMembranes are the main “structures” within a P system. A membrane is a discrete unit which can contain a set of objects (symbols/catalysts), a set of rules, and a set of other membranes contained within. The outermost membrane, held within the environment, is often referred to as the 'container membrane' or 'skin membrane'. As implied to by their namesake, membranes are permeable and symbols resulting from a rule may cross them. A membrane (but not the container membrane) may also “dissolve”, in which case its content, except for rules (which are lost), migrate into the membrane in which it was contained.\n\nSome P system variants allow for a membrane to divide, possess a charge or have varying permeability by changing membrane thickness.\n\nSymbols represent chemicals which may react with other chemicals to form some product. In a P system each type of symbol is typically represented by a different \nletter. The symbol content of a membrane is therefore represented by a string of letters. Because the multiplicity of symbols in a region matters, multisets are commonly used to represent the symbol content of a region.\n\nSpecial case symbols exist, for example a lower case delta (δ) is often used to initiate the dissolving of a membrane, and this will only ever be found in the output of a rule: upon being encountered it invokes a reaction, and is used in the process.\n\nCatalysts are similar to their namesakes in chemistry. They are represented and used in the same way as symbols, but are never consumed during a “reaction,” they are simply a requirement for it to occur.\n\nRules represent a possible chemical reaction within a membrane, causing it to evolve to a new state. A rule has a required set of input objects (symbols or catalysts) which must be present in order for it to be applied. If the required objects are present, it consumes them and produces a set of output objects. A rule may also be specified to have a priority over other rules, in which case less dominant rules will only be applied when it is not possible to apply a more dominant rule (i.e. the required inputs are not present).\n\nThere are three (in the basic P system model) distinct ways in a rule may handle its output objects. Usually the output objects are passed into the current membrane (the same membrane in which the rule and the inputs reside), known as a \"here\" rule. However there are two modifiers which can be specified upon output objects when rules are defined, \"in\" and \"out\". The \"in\" modifier causes the object to be passed to one of the current membrane’s children (travelling inwards relative to the structure of the P system), chosen at random during the computation. The \"out\" modifier causes the object to be passed out of the current membrane and into either its parent membrane or to a sibling membrane, specified during specification of the P system.\n\nA computation works from an initial starting state towards an end state through a number of discrete steps. Each step involves iterating through all membranes in the P system and the application of rules, which occurs in both a maximally parallel and non-deterministic manner.\n\nWorking through step-by-step, a computation halts when no further evolution can take place (i.e. when no rules are able to be applied). At this point whatever objects have been passed to the environment, or into a designated 'result' membrane, are counted as the result of the computation.\n\nAt each step of a computation an object may only be used once, as they are consumed by rules when applied. The method of applying a rule within a membrane is as follows: \nOutputs are not passed immediately into membranes because this would contravene the maximally parallel nature of rule application, instead they are distributed after all possible rules have been applied.\n\nThe order of rule application is chosen at random. Rule application order can have a significant effect on which rules may be applied at any given time, and the outcome of a step of execution.\n\nConsider a membrane containing only a single \"a\" symbol, and the two rules a → ab and a → aδ. As both rules rely on an “a” symbol being present, of which there is only one, the first step of computation will allow either the first or second rule to be applied, but not both. The two possible results of this step are very different:\n\nThis is a property of rule application whereby all possible rule assignments must take place during every step of the computation. In essence this means that the rule a → aa has the effect of doubling the number of \"a\" symbols in its containing membrane each step, because the rule is applied to every occurrence of an \"a\" symbol present.\n\nMost P systems variants are computationally universal. This extends even to include variants that do not use rule priorities, usually a fundamental aspect of P systems.\n\nAs a model for computation, P systems offer the attractive possibility of solving NP-complete problems in less-than exponential time. Some P system variants are known to be capable of solving the SAT (boolean satisfiability) problem in linear time and, owing to all NP-complete problems being equivalent, this capability then applies to all such problems. As there is no current method of directly implementing a P system in its own right, their functionality is instead emulated and therefore solving NP-complete problems in linear time remains theoretical. However, it has also been proven that any deterministic P system may be simulated on a Turing Machine in polynomial time.\n\nThe image shown depicts the initial state of a P system with three membranes. Because of their hierarchical nature, P systems are often depicted graphically with drawings that resemble Venn diagrams or David Harel's Higraph (see Statechart).\n\nThe outermost membrane, 1, is the container membrane for this P system and contains a single \"out\" rule. Membrane 2 contains four \"here\" rules, with two in a priority relationship: cc → c will always be applied in preference to c → δ. The delta symbol represents the special “dissolve” symbol. The innermost membrane, 3, contains a set of symbols (“ac”) and three rules, of type \"here\". In this initial state no rules outside of membrane 3 are applicable: there are no \nsymbols outside of that membrane. However, during evolution of the system, as objects are passed between membranes, the rules in other membranes will become active.\n\nBecause of the non-deterministic nature of P systems, there are many different paths of computation a single P system is capable of, leading to different results. The following is one possible path of computation for the P system depicted.\n\nFrom the initial configuration only membrane 3 has any object content: \"ac\"\n\nMembrane 3 now contains: \"abcc\" \nNotice the maximally parallel behaviour of rule application leading to the same rule being applied twice during one step.\n\nNotice also that the application of the second rule (a → bδ) as opposed to the first (a → ab) is non-deterministic and can be presumed random. The system could just as well have continued applying the first rule (and at the same time doubling the c particles) indefinitely.\n\nMembrane 3 now dissolves, as the dissolve symbol (δ) has been encountered and all object content from this membrane passes into membrane 2.\n\nMembrane 2 now contains: \"\" \n\nMembrane 2 now contains: \"\" \n\nMembrane 2 now contains: \"\" \n\nNotice that the priority over c → δ has been lifted now the required inputs for cc→ c no longer exist. Membrane 2 now dissolves, and all object content passes \nto membrane 1.\n\nMembrane 1 now contains: \"\" \n\nMembrane 1 now contains: \"dd\" and, due to the out rule e → e, the environment contains: \"eeee.\" At this point the computation halts as no further \nassignments of objects to rules is possible. The result of the computation is four \"e\" symbols.\n\nThe only non-deterministic choices occurred during steps 1 and 2, when choosing where to assign the solitary \"a\" symbol. Consider the case where \"a\" is assigned to a → bδ during step 1: upon membrane 3 dissolving only a single \"b\" and two \"c\" objects would exist, leading to the creation of only a single \"e\" object to eventually be passed out as the computation’s result.\n\n\n"}
{"id": "2161709", "url": "https://en.wikipedia.org/wiki?curid=2161709", "title": "Padovan sequence", "text": "Padovan sequence\n\nThe Padovan sequence is the sequence of integers \"P\"(\"n\") defined by the initial values\n\nand the recurrence relation\n\nThe first few values of \"P\"(\"n\") are\n\nThe Padovan sequence is named after Richard Padovan who attributed its discovery to Dutch architect Hans van der Laan in his 1994 essay \"Dom. Hans van der Laan : Modern Primitive\". The sequence was described by Ian Stewart in his Scientific American column \"Mathematical Recreations\" in June 1996. He also writes about it in one of his books, \"Math Hysteria: Fun Games With Mathematics\".\n\n\"The above definition is the one given by Ian Stewart and by MathWorld. Other sources may start the sequence at a different place, in which case some of the identities in this article must be adjusted with appropriate offsets.\"\n\nIn the spiral, each triangle shares a side with two others giving a visual proof that \nthe Padovan sequence also satisfies the recurrence relation\n\nStarting from this, the defining recurrence and other recurrences as they are discovered,\none can create an infinite number of further recurrences by repeatedly replacing formula_4 by formula_5\n\nThe Perrin sequence satisfies the same recurrence relations as the Padovan sequence, although it has different initial values. This is a property of recurrence relations.\n\nThe Perrin sequence can be obtained from the Padovan sequence by the \nfollowing formula:\n\nAs with any sequence defined by a recurrence relation, Padovan numbers \"P\"(\"m\") for \"m<0\" can be defined by rewriting the recurrence relation as\n\nStarting with \"m\" = −1 and working backwards, we extend \"P\"(\"m\") to negative indices:\n\nThe sum of the first \"n\" terms in the Padovan sequence is 2 less than \"P\"(\"n\" + 5) i.e.\n\nSums of alternate terms, sums of every third term and sums of every fifth term are also related to other terms in the sequence:\n\nSums involving products of terms in the Padovan sequence satisfy the following identities:\n\nThe Padovan sequence also satisfies the identity\n\nThe Padovan sequence is related to sums of binomial coefficients by the following identity:\n\nFor example, for \"k\" = 12, the values for the pair (\"m\", \"n\") with 2\"m\" + \"n\" = 12 which give non-zero binomial coefficients are (6, 0), (5, 2) and (4, 4), and:\n\nThe Padovan sequence numbers can be written in terms of powers of the roots of the equation\n\nThis equation has 3 roots; one real root \"p\" (known as the plastic number) and two complex conjugate roots \"q\" and \"r\". Given these three roots, the Padovan sequence can be expressed by a formula involving \"p\", \"q\" and \"r\":\n\nwhere \"a\", \"b\" and \"c\" are constants.\n\nSince the magnitudes of the complex roots \"q\" and \"r\" are both less than 1 (and hence \"p\" is a Pisot–Vijayaraghavan number), the powers of these roots approach 0 for large \"n\", and formula_23 tends to zero.\nFor all formula_24, P(n) is the integer closest to formula_25, \nwhere \"s\" = \"p\"/\"a\" = 1.0453567932525329623... is the only real root of \"s\" − 2\"s\" + 23\"s\" − 23 = 0. The ratio of successive terms in the Padovan sequence approaches \"p\", which has a value of approximately 1.324718. This constant bears the same relationship to the Padovan sequence and the Perrin sequence as the golden ratio does to the Fibonacci sequence.\n\n\n\n\n\nThe generating function of the Padovan sequence is\n\nThis can be used to prove identities involving products of the Padovan sequence with geometric terms, such as:\n\nIn a similar way to the Fibonacci numbers that can be generalized to a set of polynomials\ncalled the Fibonacci polynomials, the Padovan sequence numbers can be generalized to\nyield the Padovan polynomials.\n\nIf we define the following simple grammar:\n\nthen this Lindenmayer system or L-system produces the following sequence of strings:\n\nand if we count the length of each string, we obtain the Padovan sequence of numbers:\n\nAlso, if you count the number of \"A\"s, \"B\"s and \"C\"s in each string, then for the \"n\"th\nstring, you have \"P\"(\"n\" − 5) \"A\"s, \"P\"(\"n\" − 3) \"B\"s and \"P\"(\"n\" − 4) \"C\"s. The count of \"BB\" pairs, \"AA\" pairs\nand \"CC\" pairs are also Padovan numbers.\n\nA spiral can be formed based on connecting the corners of a set of 3-dimensional cuboids.\nThis is the Padovan cuboid spiral. Successive sides of this spiral have lengths that are\nthe Padovan sequence numbers multiplied by the square root of 2.\n\nErv Wilson in his paper \"The Scales of Mt. Meru\" discovered the Padovan sequence in Pascal's Triangle by summing the diagonal numbers (see diagram). Correction, Erv Wilson noticed these diagonals and drew them on paper in 1993, yet the Padovan numbers were not discovered until 1994. Paul Barry (2004) actually defined these diagonals as being related to the Padovan sequence. Below is the image.\n\n\n"}
{"id": "30863738", "url": "https://en.wikipedia.org/wiki?curid=30863738", "title": "Perrin number", "text": "Perrin number\n\nIn mathematics, the Perrin numbers are defined by the recurrence relation\n\nwith initial values\n\nThe sequence of Perrin numbers starts with\n\nThe number of different maximal independent sets in an -vertex cycle graph is counted by the th Perrin number for .\n\nThis sequence was mentioned implicitly by Édouard Lucas (1876). In 1899, the same sequence was mentioned explicitly by\nFrançois Olivier Raoul Perrin. The most extensive treatment of this sequence was given by Adams and Shanks (1982).\n\nThe generating function of the Perrin sequence is\n\nThe Perrin sequence numbers can be written in terms of powers of the roots of the equation\n\nThis equation has 3 roots; one real root \"p\" (known as the plastic number) and two complex conjugate roots \"q\" and \"r\". Given these three roots, the Perrin sequence analogue of the Lucas sequence Binet formula is\n\nSince the magnitudes of the complex roots \"q\" and \"r\" are both less than 1, the powers of these roots approach 0 for large \"n\". For large \"n\" the formula reduces to\n\nThis formula can be used to quickly calculate values of the Perrin sequence for large n. The ratio of successive terms in the Perrin sequence approaches \"p\", a.k.a. the plastic number, which has a value of approximately 1.324718. This constant bears the same relationship to the Perrin sequence as the golden ratio does to the Lucas sequence. Similar connections exist also between \"p\" and the Padovan sequence, between the golden ratio and Fibonacci numbers, and between the silver ratio and Pell numbers.\n\nFrom the Binet formula, we can obtain a formula for \"G\"(\"kn\") in terms of \"G\"(\"n\"−1), \"G\"(\"n\") and \"G\"(\"n\"+1); we know\n\nwhich gives us three linear equations with coefficients over the splitting field of formula_7; by inverting a matrix we can solve for formula_8 and then we can raise them to the \"k\"th power and compute the sum.\n\nExample magma code:\n\nwith the result that, if we have formula_9, then\n\nThe number 23 here arises from the discriminant of the defining polynomial of the sequence.\n\nThis allows computation of the nth Perrin number using integer arithmetic in formula_11 multiplies.\n\nIt has been proven that for all primes \"p\", \"p\" divides \"P\"(\"p\"). However, the converse is not true: for some composite numbers \"n\", \"n\" may still divide \"P\"(\"n\"). If \"n\" has this property, it is called a \"Perrin pseudoprime\".\n\nThe first few Perrin pseudoprimes are\n\nThe question of the existence of Perrin pseudoprimes was considered by Perrin himself, but it was not known whether they existed until Adams and Shanks (1982) discovered the smallest one, 271441 = 521; the next-smallest is 904631 = 7 x 13 x 9941. There are seventeen of them less than a billion; Jon Grantham has proved that there are infinitely many Perrin pseudoprimes.\n\nAdams and Shanks (1982) noted that primes also meet the condition that \"P\"(\"-p\") = \"-1\" mod \"p\". Composites where both properties hold are called \"restricted Perrin pseudoprimes\" . Further conditions can be applied using the six element signature of \"n\" which must be one of three forms (e.g. and ).\n\nWhile Perrin pseudoprimes are rare, they have significant overlap with Fermat pseudoprimes. This contrasts with the Lucas pseudoprimes which are anti-correlated. The latter condition is exploited to yield the popular, efficient, and more effective BPSW test which has no known pseudoprimes, and the smallest is known to be greater than 2.\n\nA Perrin prime is a Perrin number that is prime. The first few Perrin primes are:\n\nFor these Perrin primes, the index of is\n\nGenerating P(n) where n is a negative integer yields a similar property regarding primality: if n is a negative, then P(n) is prime when P(n) mod -n = -n - 1. The following sequence represents P(n) for all n that are negative integers:\n\n\n"}
{"id": "1531369", "url": "https://en.wikipedia.org/wiki?curid=1531369", "title": "Phase problem", "text": "Phase problem\n\nIn physics, the phase problem is the problem of loss of information concerning the phase that can occur when making a physical measurement. The name comes from the field of X-ray crystallography, where the phase problem has to be solved for the determination of a structure from diffraction data. The phase problem is also met in the fields of imaging and signal processing. Various approaches have been developed over the years that attempt to solve it.\n\nLight detectors, such as photographic plates or CCDs, measure only the intensity of the light that hits them. This measurement is incomplete (even when neglecting other degrees of freedom such as polarization and angle of incidence) because a light wave has not only an amplitude (related to the intensity), but also a phase, which is systematically lost in a measurement. In diffraction or microscopy experiments, the phase part of the wave often contains valuable information on the studied specimen. The phase problem constitutes a fundamental limitation ultimately related to the nature of measurement in quantum mechanics.\n\nIn X-ray crystallography, the diffraction data when properly assembled gives the amplitude of the 3D Fourier transform of the molecule's electron density in the unit cell. If the phases are known, the electron density can be simply obtained by Fourier synthesis. This Fourier transform relation also holds for two-dimensional far-field diffraction patterns (also called Fraunhofer diffraction) giving rise to a similar type of phase problem.\n\nIn X-ray crystallography, there are several ways to recover the lost phases. A powerful solution is the \"Multi-wavelength Anomalous Diffraction\" (MAD) method. In this technique, atoms' inner electrons absorb X-rays of particular wavelengths, and reemit the X-rays after a delay, inducing a phase shift in all of the reflections, known as the \"anomalous dispersion effect\". Analysis of this phase shift (which may be different for individual reflections) results in a solution for the phases. Since X-ray fluorescence techniques (like this one) require excitation at very specific wavelengths, it is necessary to use synchrotron radiation when using the MAD method.\n\nOther methods of experimental phase determination include \"Multiple Isomorphous Replacement\" (MIR), where heavy atoms are inserted into structure (usually by synthesizing proteins with analogs or by soaking), \"molecular replacement\" (MR) and \"Single-wavelength Anomalous Dispersion\" (SAD).\n\nPhases can also be inferred by using a process called molecular replacement, where a similar molecule's already-known phases are grafted onto the intensities of the molecule at hand, which are observationally determined. These phases can be obtained experimentally from a homologous molecule or if the phases are known for the same molecule but in a different crystal, by simulating the molecule's packing in the crystal and obtaining theoretical phases. Generally, these techniques are less desirable since they can severely bias the solution of the structure. They are useful, however, for ligand binding studies, or between molecules with small differences and relatively rigid structures (for example derivatizing a small molecule).\n\nThere are two major processes for recovering the phases using the data obtained by regular equipment. One is the direct method, which estimates the initial phases and expanding phases using a triple relation. (A trio of reflections in which the intensity and phase of one reflection can be explained by the other two has a triple relation.) A number of initial phases are tested and selected by this method. The other is the Patterson method, which directly determines the positions of heavy atoms. The Patterson function gives a large value in a position which corresponds to interatomic vectors. This method can be applied only when the crystal contains heavy atoms or when a significant fraction of the structure is already known. Because of the development of computers, the direct method is now the most useful technique for solving the phase problem.\n\nFor molecules whose crystals provide reflections in the sub-Ångström range, it is possible to determine phases by brute force methods, testing a series of phase values until spherical structures are observed in the resultant electron density map. This works because atoms have a characteristic structure when viewed in the sub-Ångström range. The technique is limited by processing power and data quality. For practical purposes, it is limited to \"small molecules\" because they consistently provide high-quality diffraction with very few reflections.\n\nIn many cases, an initial set of phases are determined, and the electron density map for the diffraction pattern is calculated. Then the map is used to determine portions of the structure, which portions are used to simulate a new set of phases. This new set of phases is known as a \"refinement\". These phases are reapplied to the original amplitudes, and an improved electron density map is derived, from which the structure is corrected. This process is repeated until an error term (usually Rfree) has stabilized to a satisfactory value. Because of the phenomenon of phase bias, it is possible for an incorrect initial assignment to propagate through successive refinements, so satisfactory conditions for a structure assignment are still a matter of debate. Indeed, some spectacular incorrect assignments have been reported, including a protein where the entire sequence was threaded backwards.\n\n\n"}
{"id": "876732", "url": "https://en.wikipedia.org/wiki?curid=876732", "title": "Poincaré map", "text": "Poincaré map\n\nIn mathematics, particularly in dynamical systems, a first recurrence map or Poincaré map, named after Henri Poincaré, is the intersection of a periodic orbit in the state space of a continuous dynamical system with a certain lower-dimensional subspace, called the Poincaré section, transversal to the flow of the system. More precisely, one considers a periodic orbit with initial conditions within a section of the space, which leaves that section afterwards, and observes the point at which this orbit first returns to the section. One then creates a map to send the first point to the second, hence the name \"first recurrence map\". The transversality of the Poincaré section means that periodic orbits starting on the subspace flow through it and not parallel to it.\n\nA Poincaré map can be interpreted as a discrete dynamical system with a state space that is one dimension smaller than the original continuous dynamical system. Because it preserves many properties of periodic and quasiperiodic orbits of the original system and has a lower-dimensional state space, it is often used for analyzing the original system in a simpler way. In practice this is not always possible as there is no general method to construct a Poincaré map.\n\nA Poincaré map differs from a recurrence plot in that space, not time, determines when to plot a point. For instance, the locus of the Moon when the Earth is at perihelion is a recurrence plot; the locus of the Moon when it passes through the plane perpendicular to the Earth's orbit and passing through the Sun and the Earth at perihelion is a Poincaré map. It was used by Michel Hénon to study the motion of stars in a galaxy, because the path of a star projected onto a plane looks like a tangled mess, while the Poincaré map shows the structure more clearly.\n\nLet (R, \"M\", \"φ\") be a global dynamical system, with R the real numbers, \"M\" the phase space and \"φ\" the evolution function. Let γ be a periodic orbit through a point \"p\" and \"S\" be a local differentiable and transversal section of \"φ\" through \"p\", called a Poincaré section through \"p\".\n\nGiven an open and connected neighborhood formula_1 of \"p\", a function \nis called Poincaré map for the orbit γ on the Poincaré section \"S\" through the point \"p\" if\n\nPoincaré maps can be interpreted as a discrete dynamical system. The stability of a periodic orbit of the original system is closely related to the stability of the fixed point of the corresponding Poincaré map.\n\nLet (R, \"M\", \"φ\") be a differentiable dynamical system with periodic orbit γ through \"p\". Let\nbe the corresponding Poincaré map through \"p\". We define\n\nand \n\nthen (Z, \"U\", \"P\") is a discrete dynamical system with state space \"U\" and evolution function\n\nPer definition this system has a fixed point at \"p\".\n\nThe periodic orbit γ of the continuous dynamical system is stable if and only if the fixed point \"p\" of the discrete dynamical system is stable.\n\nThe periodic orbit γ of the continuous dynamical system is asymptotically stable if and only if the fixed point \"p\" of the discrete dynamical system is asymptotically stable.\n\n\n"}
{"id": "1345771", "url": "https://en.wikipedia.org/wiki?curid=1345771", "title": "Position (vector)", "text": "Position (vector)\n\nIn geometry, a position or position vector, also known as location vector or radius vector, is a Euclidean vector that represents the position of a point \"P\" in space in relation to an arbitrary reference origin \"O\". Usually denoted x, r, or s, it corresponds to the straight-line from \"O\" to \"P\".\nIn other words, it is the displacement or translation that maps the origin to \"P\":\n\nThe term \"position vector\" is used mostly in the fields of differential geometry, mechanics and occasionally vector calculus.\n\nFrequently this is used in two-dimensional or three-dimensional space, but can be easily generalized to Euclidean spaces and affine spaces of any dimension.\n\nIn three dimensions, any set of three-dimensional coordinates and their corresponding basis vectors can be used to define the location of a point in space—whichever is the simplest for the task at hand may be used.\n\nCommonly, one uses the familiar Cartesian coordinate system, or sometimes spherical polar coordinates, or cylindrical coordinates:\n\nwhere \"t\" is a parameter, owing to their rectangular or circular symmetry. These different coordinates and corresponding basis vectors represent the same position vector. More general curvilinear coordinates could be used instead and are in contexts like continuum mechanics and general relativity (in the latter case one needs an additional time coordinate).\n\nLinear algebra allows for the abstraction of an \"n\"-dimensional position vector. A position vector can be expressed as a linear combination of basis vectors:\n\nThe set of all position vectors forms position space (a vector space whose elements are the position vectors), since positions can be added (vector addition) and scaled in length (scalar multiplication) to obtain another position vector in the space. The notion of \"space\" is intuitive, since each \"x\" (\"i\" = 1, 2, …, \"n\") can have any value, the collection of values defines a point in space.\n\nThe \"dimension\" of the position space is \"n\" (also denoted dim(\"R\") = \"n\"). The \"coordinates\" of the vector r with respect to the basis vectors e are \"x\". The vector of coordinates forms the coordinate vector or \"n\"-tuple (\"x\", \"x\", …, \"x\").\n\nEach coordinate \"x\" may be parameterized a number of parameters \"t\". One parameter \"x\"(\"t\") would describe a curved 1D path, two parameters \"x\"(\"t\", \"t\") describes a curved 2D surface, three \"x\"(\"t\", \"t\", \"t\") describes a curved 3D volume of space, and so on.\n\nThe linear span of a basis set \"B\" = {e, e, …, e} equals the position space \"R\", denoted span(\"B\") = \"R\".\n\nPosition vector fields are used to describe continuous and differentiable space curves, in which case the independent parameter needs not be time, but can be (e.g.) arc length of the curve.\n\nIn any equation of motion, the position vector r(\"t\") is usually the most sought-after quantity because this function defines the motion of a particle (i.e. a point mass) – its location relative to a given coordinate system at some time \"t\".\n\nTo define motion in terms of position, each coordinate may be parametrized by time; since each successive value of time corresponds to a sequence of successive spatial locations given by the coordinates, the continuum limit of many successive locations is a path the particle traces.\n\nIn the case of one dimension, the position has only one component, so it effectively degenerates to a scalar coordinate. It could be, say, a vector in the \"x\" direction, or the radial \"r\" direction. Equivalent notations include\n\nFor a position vector r that is a function of time \"t\", the time derivatives can be computed with respect to \"t\". These derivatives have common utility in the study of kinematics, control theory, engineering and other sciences.\n\nwhere dr is an infinitesimally small displacement (vector).\n\n\n\nThese names for the first, second and third derivative of position are commonly used in basic kinematics. By extension, the higher-order derivatives can be computed in a similar fashion. Study of these higher-order derivatives can improve approximations of the original displacement function. Such higher-order terms are required in order to accurately represent the displacement function as a sum of an infinite sequence, enabling several analytical techniques in engineering and physics.\n\n\n"}
{"id": "54337393", "url": "https://en.wikipedia.org/wiki?curid=54337393", "title": "Precoloring extension", "text": "Precoloring extension\n\nIn graph theory, precoloring extension is the problem of extending a graph coloring of a subset of the vertices of a graph, with a given set of colors, to a coloring of the whole graph that does not assign the same color to any two adjacent vertices.\nIt has the usual graph coloring problem as a special case, in which the initially colored subset of vertices is empty; therefore, it is NP-complete.\nHowever, it is also NP-complete for some other classes of graphs on which the usual graph coloring problem is easier, such as the rook's graphs.\n\nPrecoloring extension may be seen as a special case of list coloring, the problem of coloring a graph in which no vertices have been colored, but each vertex has an assigned list of available colors.\nTo transform a precoloring extension problem into a list coloring problem, assign each uncolored vertex in the precoloring extension problem a list of the colors not yet used by its initially-colored neighbors,\nand then remove the colored vertices from the graph.\n\n"}
{"id": "34957160", "url": "https://en.wikipedia.org/wiki?curid=34957160", "title": "Rademacher–Menchov theorem", "text": "Rademacher–Menchov theorem\n\nIn mathematical analysis, the Rademacher–Menchov theorem, introduced by and , gives a sufficient condition for a series of orthogonal functions on an interval to converge almost everywhere.\n\nIf the coefficients \"c\" of a series of bounded orthogonal functions on an interval satisfy\nthen the series converges almost everywhere.\n\n"}
{"id": "292169", "url": "https://en.wikipedia.org/wiki?curid=292169", "title": "Richard Borcherds", "text": "Richard Borcherds\n\nRichard Ewen Borcherds (; born 29 November 1959) is a British-American mathematician currently working in quantum field theory. He is known for his work in lattices, number theory, group theory, and infinite-dimensional algebras, for which he was awarded the Fields Medal in 1998.\n\nBorcherds was born in Cape Town, but the family moved to Birmingham in the United Kingdom when he was six months old. His father is a physicist and he has three brothers, two of whom are mathematics teachers. He was a promising mathematician and chess player as a child, winning several national mathematics championships and \"was in line for becoming a chess master\" before giving up chess after coming to believe that the higher levels of competitive chess are merely about the competition rather than the fun of playing.\n\nHe was educated at King Edward's School, Birmingham and Trinity College, Cambridge, where he studied under John Horton Conway.\n\nAfter receiving his doctorate in 1985 he has held various alternating positions at Cambridge and the University of California, Berkeley, serving as Morrey Assistant Professor of Mathematics at Berkeley from 1987 to 1988. He was a Royal Society University Research Fellow. From 1996 he held a Royal Society Research Professorship at Cambridge before returning to Berkeley in 1999 as Professor of mathematics. At Berkeley, he held a Miller Research Professorship from 2000 to 2001. \n\nBorcherds's early work included pioneering results on classification of unimodular lattices, and the introduction of new algebraic objects, most notably vertex algebras and Borcherds-Kac-Moody algebras. These ideas came together in his vertex-algebraic construction and analysis of the fake monster Lie algebra (called the monster Lie algebra at the time). \n\nBorcherds is best known for his resolution of the Conway-Norton monstrous moonshine conjecture, which describes an intricate relation between the monster group and modular functions on the complex upper half-plane. To prove this conjecture, he drew upon theories that he had previously introduced, namely those of vertex algebras and Borcherds-Kac-Moody algebras, together with techniques of string theory, and applied them to the \"moonshine module\", a vertex operator algebra with monster symmetry constructed by Igor Frenkel, James Lepowsky, and Arne Meurman. Additional work in moonshine concerned mod p variants of this conjecture, and were known as modular moonshine.\n\nLater contributions include the theory of Borcherds products, which are holomorphic automorphic forms on \"O\"(\"n\",2) that have well-behaved infinite product expansions at cusps. Borcherds used this theory to resolve some long-standing conjectures concerning quasi-affineness of certain moduli spaces of algebraic surfaces. More recently, Borcherds has rendered perturbative renormalization, in particular the 't Hooft-Veltman proof of perturbative renormalizability of gauge theory, into rigorous mathematical language.\n\nAn interview with Simon Singh for \"The Guardian\", in which Borcherds suggested he might have some traits associated with Asperger syndrome, subsequently led to a chapter about him in a book on autism by Simon Baron-Cohen. Baron-Cohen concluded that while Borcherds had many autistic traits, he did not merit a formal diagnosis of Asperger syndrome.\n\nIn 1992 he was one of the first recipients of the EMS prizes awarded at the first European Congress of Mathematics in Paris, and in 1994 he was an invited speaker at the International Congress of Mathematicians in Zurich. In 1994, he was elected to the Royal Society of Fellows. In 1998 at the 23rd International Congress of Mathematicians in Berlin, Germany he received the Fields Medal together with Maxim Kontsevich, William Timothy Gowers and Curtis T. McMullen. The award cited him \"for his contributions to algebra, the theory of automorphic forms, and mathematical physics, including the introduction of vertex algebras and Borcherds' Lie algebras, the proof of the Conway-Norton moonshine conjecture and the discovery of a new class of automorphic infinite products.\" In 2012 he became a fellow of the American Mathematical Society, and in 2014 he was elected to the National Academy of Sciences.\n\n"}
{"id": "12498127", "url": "https://en.wikipedia.org/wiki?curid=12498127", "title": "Risk measure", "text": "Risk measure\n\nIn financial mathematics, a risk measure is used to determine the amount of an asset or set of assets (traditionally currency) to be kept in reserve. The purpose of this reserve is to make the risks taken by financial institutions, such as banks and insurance companies, acceptable to the regulator. In recent years attention has turned towards convex and coherent risk measurement.\n\nA risk measure is defined as a mapping from a set of random variables to the real numbers. This set of random variables represents portfolio returns. The common notation for a risk measure associated with a random variable formula_1 is formula_2. A risk measure formula_3 should have certain properties:\n\n\n\n\nIn a situation with formula_7-valued portfolios such that risk can be measured in formula_8 of the assets, then a set of portfolios is the proper way to depict risk. Set-valued risk measures are useful for markets with transaction costs.\n\nA set-valued risk measure is a function formula_9, where formula_10 is a formula_11-dimensional Lp space, formula_12, and formula_13 where formula_14 is a constant solvency cone and formula_15 is the set of portfolios of the formula_16 reference assets. formula_17 must have the following properties:\n\n\n\n\n\nVariance (or standard deviation) is not a risk measure in the above sense. This can be seen since it has neither the translation property nor monotonicity. That is, formula_21 for all formula_22, and a simple counterexample for monotonicity can be found. The standard deviation is a deviation risk measure. To avoid any confusion, note that deviation risk measures, such as variance and standard deviation are sometimes called risk measures in different fields.\n\nThere is a one-to-one correspondence between an acceptance set and a corresponding risk measure. As defined below it can be shown that formula_23 and formula_24.\n\n\n\nThere is a one-to-one relationship between a deviation risk measure \"D\" and an expectation-bounded risk measure formula_25 where for any formula_34\nformula_25 is called expectation bounded if it satisfies formula_38 for any nonconstant \"X\" and formula_39 for any constant \"X\".\n\n\n"}
{"id": "38701873", "url": "https://en.wikipedia.org/wiki?curid=38701873", "title": "Ronitt Rubinfeld", "text": "Ronitt Rubinfeld\n\nRonitt Rubinfeld is a professor of electrical engineering and computer science at MIT and a professor of computer sciences at Tel-Aviv University, Israel.\n\nRubinfeld graduated from the University of Michigan with a BSE in Electrical and Computer Engineering. Following that, she received her PhD from the University of California, Berkeley under the supervision of Manuel Blum.\n\nRubinfeld's research interests include randomized and sublinear time algorithms. In particular, her work focuses on what can be understood about data by looking at only a very small portion of it.\n\nShe gave an invited lecture at the International Congress of Mathematicians in 2006.\nShe became a fellow of the Association for Computing Machinery in 2014 for \"contributions to delegated computation, sublinear time algorithms and property testing\".\n\n"}
{"id": "1373168", "url": "https://en.wikipedia.org/wiki?curid=1373168", "title": "Rooted graph", "text": "Rooted graph\n\nIn mathematics, and, in particular, in graph theory, a rooted graph is a graph in which one vertex has been distinguished as the root. Both directed and undirected versions of rooted graphs have been studied, and there are also variant definitions that allow multiple roots.\nRooted graphs may also be known (depending on their application) as pointed graphs or flow graphs. In some of the applications of these graphs, there is an additional requirement that the whole graph be reachable from the root vertex.\n\nIn topological graph theory, the notion of a rooted graph may be extended to consider multiple vertices or multiple edges as roots. The former are sometimes called vertex-rooted graphs in order to distinguish them from edge-rooted graphs in this context. Graphs with multiple nodes designated as roots are also of some interest in combinatorics, in the area of random graphs. These graphs are also called multiply rooted graphs.\n\nThe terms rooted directed graph or rooted digraph also see variation in definitions. The obvious transplant is to consider a digraph rooted by identifying a particular node as root. However, in computer science, these terms commonly refer to a narrower notion, namely a rooted directed graph is a digraph with a distinguished node \"r\", such that there is a directed path from \"r\" to any node other than \"r\". Authors which give the more general definition, may refer to these as \"connected\" (or \"1-connected\") rooted digraphs.\n\n\"The Art of Computer Programming\" defines rooted digraphs slightly more broadly, namely a directed graph is called rooted if it has \"at least one\" node that can reach all the other nodes; Knuth notes that the notion thus defined is a sort of intermediate between the notions of strongly connected and connected digraph.\n\nIn computer science, rooted graphs in which the root vertex can reach all other vertices are called flow graphs or flowgraphs. Sometimes an additional restriction is added specifying that a flow graph must have a single exit (sink) vertex as well.\n\nFlow graphs may be viewed as abstractions of flow charts, with the non-structural elements (node contents and types) removed. Perhaps the best known sub-class of flow graphs are control flow graphs, used in compilers and program analysis. An arbitrary flow graph may converted to a control flow graph by performing an edge contraction on every edge that is the only outgoing edge from its source and the only incoming edge into its target. Another type of flow graph commonly used is the call graph, in which nodes correspond to entire subroutines.\n\nThe general notion of flow graph has been called program graph, but the same term has also been used to denote only control flow graphs. Flow graphs have also been called unlabeled flowgraphs, and proper flowgraphs. These graphs are sometimes used in software testing.\n\nWhen required to have a single exit, flow graphs have two properties not shared with directed graphs in general. Flow graphs can be nested, which is the equivalent of a subroutine call (although there is no notion of passing parameters), and flow graphs can also be sequenced, which is the equivalent of sequential execution of two pieces of code. \"Prime flow graphs\" are defined as flow graphs that cannot be decomposed via nesting or sequencing using a chosen pattern of subgraphs, for example the primitives of structured programming. Theoretical research has been done on determining, for example, the proportion of prime flow graphs given a chosen set of graphs.\n\nPeter Aczel has used rooted directed graphs in which the whole graph is reachable from the root (which he calls accessible pointed graphs) to formulate Aczel's anti-foundation axiom in non-well-founded set theory. In this context, the vertices of a graph model sets within the set theory, and their adjacency relation models the relation of one set belonging to another. The anti-foundation axiom states that every accessible pointed graph models a family of sets in this way.\n\nThe number of rooted undirected graphs for 1, 2, ... nodes is 1, 2, 6, 20, 90, 544, ... \n\nA special case of interest are rooted trees, the trees with a distinguished root vertex. If the directed paths from the root in the rooted digraph are additionally restricted to be unique, then the notion obtained is that of (rooted) arborescence—the directed-graph equivalent of a rooted tree. A rooted graph contains an arborescence with the same root if and only if the whole graph can be reached from the root, and computer scientists have studied algorithmic problems of finding optimal arborescences.\n\nRooted graphs may be combined using the rooted product of graphs.\n\n\n"}
{"id": "29116756", "url": "https://en.wikipedia.org/wiki?curid=29116756", "title": "Semantic analysis (computational)", "text": "Semantic analysis (computational)\n\nSemantic analysis (computational) is a composite of the \"semantic analysis\" and the \"computational\" components.\n\n\"Semantic analysis\" refers to a formal analysis of meaning, and \"computational\" refer to approaches that in principle support effective implementation.\n\n\n"}
{"id": "3326018", "url": "https://en.wikipedia.org/wiki?curid=3326018", "title": "Teaching dimension", "text": "Teaching dimension\n\nIn computational learning theory, the teaching dimension of a concept class \"C\" is defined to be formula_1, where formula_2 is the minimum size of a witness set for \"c\" in \"C\".\n\nThe teaching dimension of a finite concept class can be used to give a lower and an upper bound on the membership query cost of the concept class.\n\nIn Stasys Jukna's book \"Extremal Combinatorics\", a lower bound is given for the teaching dimension:\n\nLet \"C\" be a concept class over a finite domain \"X\". If the size of \"C\" is greater than \nthen the teaching dimension of \"C\" is greater than \"k\".\n"}
{"id": "711799", "url": "https://en.wikipedia.org/wiki?curid=711799", "title": "Unbounded system", "text": "Unbounded system\n\nIn the theory of dynamical systems, an unbounded system is a system that has no bound; i.e., one that can expand forever with no limit.\n"}
{"id": "39904859", "url": "https://en.wikipedia.org/wiki?curid=39904859", "title": "Yves Gaucher", "text": "Yves Gaucher\n\nYves Gaucher (January 3, 1934 – September 8, 2000) was a Canadian abstract painter and printmaker. He became a member of the Order of Canada in 1981, and is considered the leader amongst Quebec's printmakers in the 1950s and 60s. His work has been featured in multiple important galleries, including the National Gallery of Canada in Ottawa, the Museum of Modern Art in New York City, and the Victoria and Albert Museum in London.\n\nYves Gaucher was born on January 3, 1934 in Montreal to Tancrède Gaucher, a pharmacist and optician, and Laura Élie Gaucher, as the sixth of eight children.\n\nHe attended the Collège Brébeuf in Montreal in 1948, but was expelled for drawing immoral pictures. These pictures were in fact copied from his textbooks on Ancient Greek and Egyptian art. A year after his expulsion he switched to an English-language Protestant school, Sir George Williams College and it was there that he took his first art course.\n\nMusic was very important to Gaucher. Raised in a musical home, where everyone played an instrument, Gaucher took up the trumpet at age twelve. His first full-time job was with the CBC, where he started in the mailroom. His ambition, however, was to become a radio announcer with his own jazz program. In the meantime he played gigs at night, and also organized a few jam sessions in 1955–56 at Galerie L'Actuelle, founded by Guido Molinari.\n\nAfter the CBC he then went on to become an employee of the Canadian Pacific Steamship Line, working in its Montreal and Halifax offices.\n\nAfter meeting Arthur Lismer, a Group of Seven artist, Gaucher decided to study art seriously. He enrolled at the École des Beaux-Arts in Montreal in 1954, but was expelled in 1956 for taking only the courses he was interested in. After his expulsion, he continued to study art on his own, while earning an income through various jobs. Gaucher then returned to the École to study printmaking with Albert Dumouchel, where he created a controversial technique of heavy embossing. When met with criticism, he described his technique as a way of challenging the traditional \"taboos\".\n\nGaucher's art career began when he set up an exhibition at the Galerie d'Échange in Montreal in 1957. He enjoyed success afterwards, and as a result became the founding president of Associations des Peintures-Gravures de Montreal in 1960.\n\nIn 1962, Gaucher travelled to Europe on a grant from the Canada Council. There, in Paris, he encountered the music of Anton Webern, which became a major influence on him. In his artworks, he began to incorporate more irregular geometries as opposed to strictly geometric forms, as well as greater contrasts of colour. This, he felt, would better represent the atonality of Webern's music.\n\nGaucher's prints of the late 1950s and early 1960s were technically innovative and demonstrated extensive experimentation with relief and lamination, and have been described as delighting in \"material physicality.\"\n\nBy 1964, however, Gaucher began to focus on painting instead of printmaking. A major influence in his early paintings was the style of artists such as Barnett Newman and Mark Rothko, part of an art school known as the New York Modernists. This led him to create similar works which emulated the structure of modernist art. Some of the characteristics of the artwork he produced in this period include the use of regular geometric objects and flat panels of colour on unusually large canvases. He also created art using mathematical relationships, including symmetry, patterning, and spatial relationships, which eventually led to monochromatic works.\n\nIn 1966, works by Gaucher along with those of Alex Colville and Sorel Etrog represented Canada at the Venice Biennale.\n\nFrom 1967 to 1969, Gaucher created a series of \"Grey on Grey\" paintings. These works, amongst his most important, were meant to be interpreted in two different ways. As individual paintings, they would be seen based on their linear movement; as a whole, they were an environment, based on colour.\n\nGaucher was crucial in the development of the colour band style of art, which was first created in 1970. This form of painting consists of wide stripes of uniform colours. Gaucher also extended colour band painting to include works of horizontal planes of contrasting colour. Gaucher's interest in mathematical art persisted, as he created works based on chaos theory and the diagonal line.\n\nIn 1980, Gaucher was nominated for the Order of Canada, and was named a member in 1981. In this period he taught at Concordia University in Montreal, where among his pupils was Joan Rankin. However, a shoulder injury and other health problems would force him to paint on smaller surfaces, and he returned to creating collages, one of his earlier practices.\n\nHe was a member of the Royal Canadian Academy of Arts\n\nHe died in Montreal on September 8, 2000.\n\nIn 1964, he married Germaine Chaussé. They had two sons; Benoit Gaucher was born in 1968, and Denis Gaucher in 1970.\n\nAn incomplete list of works by Gaucher:\n\n\n"}
{"id": "4452009", "url": "https://en.wikipedia.org/wiki?curid=4452009", "title": "∂", "text": "∂\n\nThe character ∂ (HTML element: &#8706; or &part;, Unicode: U+2202) or formula_1 is a stylized \"d\" mainly used as a mathematical symbol to denote a partial derivative such as formula_2 (read as \"the partial derivative of \"z\" with respect to \"x\"\").\n\nThe symbol was originally introduced by Adrien-Marie Legendre in 1786, but gained popularity when it was used by Carl Gustav Jacob Jacobi in 1841.\n\n∂ is also used to denote the following:\n\nThe symbol is referred to as \"del\" (not to be confused with ∇, also known as \"del\"), \"dee\", \"partial dee\", \"partial\" (especially in LaTeX), \"round d\", \"curly dee\", \"doh\", \"die\" or \"dabba\".\n\nThe lowercase Cyrillic De looks similar when italicized.\n\n"}
