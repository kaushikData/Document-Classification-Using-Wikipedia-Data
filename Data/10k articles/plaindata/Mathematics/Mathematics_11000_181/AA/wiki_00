{"id": "24497685", "url": "https://en.wikipedia.org/wiki?curid=24497685", "title": "Algebraic theory", "text": "Algebraic theory\n\nInformally in mathematical logic, an algebraic theory is one that uses axioms stated entirely in terms of equations between terms with free variables. Inequalities and quantifiers are specifically disallowed. Sentential logic is the subset of first-order logic involving only algebraic sentences.\n\nThe notion is very close to the notion of algebraic structure, which, arguably, may be just a synonym.\n\nSaying that a theory is algebraic is a stronger condition than saying it is elementary.\n\nAn algebraic theory consists of a collection of \"n\"-ary functional terms with additional rules (axioms).\n\nE.g. a group theory is an algebraic theory because it has three functional terms: a binary operation \"a * b\", a nullary operation \"1\" (neutral element), and a unary operation \"x\" → \"x\" with the rules of associativity, neutrality and inversion respectively.\n\nThis is opposed to geometric theory which involves partial functions (or binary relationships) or existential quantors - see e.g. Euclidean geometry where the existence of points or lines is postulated.\n\nAn Algebraic Theory T is a category whose objects are natural numbers 0, 1, 2..., and which, for each n, has an n-tuple of morphisms:\n\n\"proj\": \"n\" → 1, \"i\" = 1..., \"n\"\n\nThis allows interpreting \"n\" as a cartesian product of \"n\" copies of 1.\n\nExample. Let's define an algebraic theory T taking hom(\"n\", \"m\") to be \"m\"-tuples of polynomials of \"n\" free variables \"X\"..., \"X\" with integer coefficients and with substitution as composition. In this case \"proj\" is the same as \"X\". This theory \"T\" is called the theory of commutative rings.\n\nIn an algebraic theory, any morphism \"n\" → \"m\" can be described as \"m\" morphisms of signature \"n\" → 1. These latter morphisms are called \"n\"-ary \"operations\" of the theory.\n\nIf \"E\" is a category with finite Cartesian products, the full subcategory Alg(T, \"E\") of the category of functors [T, \"E\"] consisting of those functors that preserve finite products is called \"the category of\" T-\"models\" or T-\"algebras\".\n\nNote that for the case of operation 2 → 1, the appropriate algebra \"A\" will define a morphism\n\n\"A\"(2) ≈ \"A\"(1)×\"A\"(1) → \"A\"(1)\n\n\n"}
{"id": "1558470", "url": "https://en.wikipedia.org/wiki?curid=1558470", "title": "Asaṃkhyeya", "text": "Asaṃkhyeya\n\nAn (Sanskrit: असंख्येय) is a Hindu/Buddhist name for the number 10 or alternatively for the number formula_1 as it is listed in the Avatamsaka Sutra. Depending on the translation, the value is different. It is formula_2 in the translation of Buddhabhadra, formula_3 in that of Shikshananda and formula_4 in that of Thomas Cleary, who may have made an error in calculation.\n\nAsamkhyeya is a Sanskrit word that appears often in the Buddhist texts. For example, Shakyamuni Buddha is said to have practiced for four great asamkhyeya kalpas before becoming a Buddha. Asamkhyeya means ‘incalculable’.\n\nThe word \"\" literally means \"innumerable\" in the sense of \"infinite\" in Sanskrit.\nIt is also a title of Vishnu and of Shiva.\nThe word comes up in Vishnu Sahasranama Stanza 27, \"Asankyeyo-aprameyaatmaa:\" one who has innumerable names and forms.\n\n"}
{"id": "13341540", "url": "https://en.wikipedia.org/wiki?curid=13341540", "title": "Asymmetric norm", "text": "Asymmetric norm\n\nIn mathematics, an asymmetric norm on a vector space is a generalization of the concept of a norm.\n\nAn asymmetric norm on a real vector space \"V\" is a function formula_1 that has the following properties:\n\n\nAsymmetric norms differ from norms in that they need not satisfy the equality \"p\"(-\"v\") = \"p\"(\"v\").\n\nIf the condition of positive definiteness is omitted, then \"p\" is an asymmetric seminorm. A weaker condition than positive definiteness is non-degeneracy: that for \"v\" ≠ 0, at least one of the two numbers \"p\"(\"v\") and \"p\"(-\"v\") is not zero.\n\n\n\nIf formula_12 is a convex set that contains the origin, then an asymmetric seminorm formula_13 can be defined on formula_14 by the formula\nFor instance, if formula_16 is the square with vertices formula_17, then formula_13 is the taxicab norm formula_19. Different convex sets yield different seminorms, and every asymmetric seminorm on formula_14 can be obtained from some convex set, called its dual unit ball. Therefore, asymmetric seminorms are in one-to-one correspondence with convex sets that contain the origin. The seminorm formula_13 is \n\nMore generally, if formula_3 is a finite-dimensional real vector space and formula_27 is a compact convex subset of the dual space formula_28 that contains the origin, then formula_29 is an asymmetric seminorm on formula_3.\n\n"}
{"id": "16815031", "url": "https://en.wikipedia.org/wiki?curid=16815031", "title": "Calogero–Degasperis–Fokas equation", "text": "Calogero–Degasperis–Fokas equation\n\nIn mathematics, the Calogero–Degasperis–Fokas equation is the nonlinear partial differential equation\n\nThis equation was named after F. Calogero, A. Degasperis, and A. Fokas.\n\n"}
{"id": "5503176", "url": "https://en.wikipedia.org/wiki?curid=5503176", "title": "Carry flag", "text": "Carry flag\n\nIn computer processors the carry flag (usually indicated as the C flag) is a single bit in a system status (flag) register used to indicate when an arithmetic carry or borrow has been generated out of the most significant ALU bit position. The carry flag enables numbers larger than a single ALU width to be added/subtracted by carrying (adding) a binary digit from a partial addition/subtraction to the least significant bit position of a more significant word. It is also used to extend bit shifts and rotates in a similar manner on many processors (sometimes done via a dedicated X flag). For subtractive operations, two (opposite) conventions are employed as most machines set the carry flag on borrow while some machines (such as the 6502 and the PIC) instead reset the carry flag on borrow (and vice versa).\n\nThe carry flag is affected by the result of most arithmetic (and typically several bit wise) instructions and is also used as an input to many of them. Several of these instructions have two forms which either read or ignore the carry. In assembly languages these instructions are represented by mnemonics such as ADD/SUB, ADC/SBC (ADD/SUB including carry), SHL/SHR (bit shifts), ROL/ROR (bit rotates), RCR/RCL (rotate through carry), and so on. The use of the carry flag in this manner enables multi-word add, subtract, shift, and rotate operations.\n\nAn example is what happens if one were to add 255 and 255 using 8-bit registers. The result should be 510 which is 1_1111_1110 in binary, requiring 9 bits. The 8 least significant bits always stored in the register would be 1111_1110 binary (254 decimal) but since there is carry out of bit 7 (the eight bit), the carry is set, indicating that the result needs 9 bits. The valid 9-bit result is the concatenation of the carry flag with the result.\n\nFor x86 ALU size of 8 bits, an 8-bit two's complement interpretation, the addition operation 1111_1111 + 1111_1111 results in 1_1111_1110, Carry_Flag set, Sign_Flag set, and Overflow_Flag clear.\n\nIf 1111_1111 represents two's complement signed integer -1 (ADD al,-1), then the interpertation of the result is 1111_1110 because Overflow_Flag is clear, and Carry_Flag is ignored. The sign of the result is negative, because Sign_Flag is set. 1111_1110 is the two's complement form of signed integer -2.\n\nIf 1111_1111 represents unsigned integer binary number 255 (ADD al,255), then the interpertation of the result that the Carry_Flag cannot be ignored. The Overflow_Flag and the Sign_Flag are ignored.\n\nAnother example may be an 8-bit register with the bit pattern 0101_0101 and the carry flag set; if we execute a \"rotate left through carry\" instruction, the result would be 1010_1011 with the carry flag cleared because the most significant bit (bit 7) was rotated into the carry while the carry was rotated into the least significant bit (bit 0).\n\nThe early microprocessors Intel 4004 and Intel 8008 had specific instructions to set as well as reset the carry flag explicitly. However, the later Intel 8080 (and Z80) did not include an explicit reset carry opcode as this could be done equally fast via one of the bitwise AND, OR or XOR instructions (which do not use the carry flag).\n\nThe carry flag is also often used following comparison instructions, which are typically implemented by subtractive operations, to allow a decision to be made about which of the two compared values is lower than (or greater or equal to) the other. Branch instructions which examine the carry flag are often represented by mnemonics such as BCC and BCS to branch if the carry is clear, or branch if the carry is set respectively. When used in this way the carry flag provides a mechanism for comparing the values as unsigned integers. This is in contrast to the overflow flag which provides a mechanism for comparing the values as signed integer values.\n\nWhile the carry flag is well-defined for addition, there are two ways in common use to use the carry flag for subtraction operations.\n\nThe first uses the bit as a borrow flag, setting it if \"a\"<\"b\" when computing \"a\"−\"b\", and a borrow must be performed. If \"a\"≥\"b\", the bit is cleared. A subtract with borrow (SBB) instruction will compute \"a\"−\"b\"−\"C\" = \"a\"−(\"b\"+\"C\"), while a subtract without borrow (SUB) acts as if the borrow bit were clear. The 8080, Z80, 8051, x86 and 68k families (among others) use a borrow bit.\n\nThe second takes advantage of the identity that −\"x\" = not(\"x\")+1 and computes \"a\"−\"b\" as \"a\"+not(\"b\")+1. The carry flag is set according to this addition, and subtract with carry computes \"a\"+not(\"b\")+\"C\", while subtract without carry acts as if the carry bit were set. The result is that the carry bit is set if \"a\"≥\"b\", and clear if \"a\"<\"b\". The System/360, 6502, MSP430, ARM and PowerPC processors use this convention. The 6502 is a particularly well-known example because it does not have a subtract \"without\" carry operation, so programmers must ensure that the carry flag is set before every subtract operation where a borrow is not required.\nMost commonly, the first alternative is referred to as a \"subtract with borrow\", while the second is called a \"subtract with carry\". However, there are exceptions in both directions; the VAX, NS320xx, and Atmel AVR architectures use the borrow bit convention, but call their \"a\"−\"b\"−\"C\" operation \"subtract with carry\" (codice_1, codice_2 and codice_3). The PA-RISC and PICmicro architectures use the carry bit convention, but call their \"a\"+not(\"b\")+\"C\" operation \"subtract with borrow\" (codice_4 and codice_5).\nThe ST6/ST7 8-bit microcontrollers are perhaps the most confusing of all. Although they do not have any sort of \"subtract with carry\" instruction, they do have a carry bit which is set by a subtract instruction, and the convention depends on the processor model. The ST60 processor uses the \"carry\" convention, while the ST62 and ST63 processors use the \"borrow\" convention.\n\n\n"}
{"id": "30839740", "url": "https://en.wikipedia.org/wiki?curid=30839740", "title": "Cavalieri's quadrature formula", "text": "Cavalieri's quadrature formula\n\nIn calculus, Cavalieri's quadrature formula, named for 17th-century Italian mathematician Bonaventura Cavalieri, is the integral\n\nand generalizations thereof. This is the definite integral form; the indefinite integral form is:\n\nThere are additional forms, listed below. Together with the linearity of the integral, this formula allows one to compute the integrals of all polynomials.\n\nThe term \"quadrature\" is a traditional term for area; the integral is geometrically interpreted as the area under the curve \"y\" = \"x\". Traditionally important cases are \"y\" = \"x\", the quadrature of the parabola, known in antiquity, and \"y\" = 1/\"x\", the quadrature of the hyperbola, whose value is a logarithm.\n\nFor negative values of \"n\" (negative powers of \"x\"), there is a singularity at \"x\" = 0, and thus the definite integral is based at 1, rather than 0, yielding:\n\nFurther, for negative fractional (non-integer) values of \"n,\" the power \"x\" is not well-defined, hence the indefinite integral is only defined for positive \"x.\" However, for \"n\" a negative integer the power \"x\" is defined for all non-zero \"x,\" and the indefinite integrals and definite integrals are defined, and can be computed via a symmetry argument, replacing \"x\" by −\"x,\" and basing the negative definite integral at −1.\n\nOver the complex numbers the definite integral (for negative values of \"n\" and \"x\") can be defined via contour integration, but then depends on choice of path, specifically winding number – the geometric issue is that the function defines a covering space with a singularity at 0.\n\nThere is also the exceptional case \"n\" = −1, yielding a logarithm instead of a power of \"x:\"\n\n(where \"ln\" means the natural logarithm, i.e. the logarithm to the base \"e\" = 2.71828...).\n\nThe improper integral is often extended to negative values of \"x\" via the conventional choice:\n\nNote the use of the absolute value in the indefinite integral; this is to provide a unified form for the integral, and means that the integral of this odd function is an even function, though the logarithm is only defined for positive inputs, and in fact, different constant values of \"C\" can be chosen on either side of 0, since these do not change the derivative. The more general form is thus:\nOver the complex numbers there is not a global antiderivative for 1/\"x\", due this function defining a non-trivial covering space; this form is special to the real numbers.\n\nNote that the definite integral starting from 1 is not defined for negative values of \"a,\" since it passes through a singularity, though since 1/\"x\" is an odd function, one can base the definite integral for negative powers at −1. If one is willing to use improper integrals and compute the Cauchy principal value, one obtains formula_8 which can also be argued by symmetry (since the logarithm is odd), so formula_9 so it makes no difference if the definite integral is based at 1 or −1. As with the indefinite integral, this is special to the real numbers, and does not extend over the complex numbers.\n\nThe integral can also be written with indexes shifted, which simplify the result and make the relation to \"n\"-dimensional differentiation and the \"n\"-cube clearer:\n\nMore generally, these formulae may be given as:\n\nThe modern proof is to use an anti-derivative: the derivative of \"x\" is shown to be \"nx\" – for non-negative integers. This is shown from the binomial formula and the definition of the derivative – and thus by the fundamental theorem of calculus the antiderivative is the integral. This method fails for formula_15 as the candidate antiderivative is formula_16, which is undefined due to division by zero. The logarithm function, which is the actual antiderivative of 1/\"x\", must be introduced and examined separately.\nFor positive integers, this proof can be geometrized: if one considers the quantity \"x\" as the volume of the \"n\"-cube (the hypercube in \"n\" dimensions), then the derivative is the change in the volume as the side length is changed – this is \"x\", which can be interpreted as the area of \"n\" faces, each of dimension \"n\" − 1 (fixing one vertex at the origin, these are the \"n\" faces not touching the vertex), corresponding to the cube increasing in size by growing in the direction of these faces – in the 3-dimensional case, adding 3 infinitesimally thin squares, one to each of these faces. Conversely, geometrizing the fundamental theorem of calculus, stacking up these infinitesimal (\"n\" − 1) cubes yields a (hyper)-pyramid, and \"n\" of these pyramids form the \"n\"-cube, which yields the formula. Further, there is an \"n\"-fold cyclic symmetry of the \"n\"-cube around the diagonal cycling these pyramids (for which a pyramid is a fundamental domain). In the case of the cube (3-cube), this is how the volume of a pyramid was originally rigorously established: the cube has 3-fold symmetry, with fundamental domain a pyramids, dividing the cube into 3 pyramids, corresponding to the fact that the volume of a pyramid is one third of the base times the height. This illustrates geometrically the equivalence between the quadrature of the parabola and the volume of a pyramid, which were computed classically by different means.\n\nAlternative proofs exist – for example, Fermat computed the area via an algebraic trick of dividing the domain into certain intervals of unequal length; alternatively, one can prove this by recognizing a symmetry of the graph \"y\" = \"x\" under inhomogeneous dilation (by \"d\" in the \"x\" direction and \"d\" in the \"y\" direction, algebraicizing the \"n\" dimensions of the \"y\" direction), or deriving the formula for all integer values by expanding the result for \"n\" = −1 and comparing coefficients.\n\nA detailed discussion of the history, with original sources, is given in ; see also history of calculus and history of integration.\n\nThe case of the parabola was proven in antiquity by the ancient Greek mathematician Archimedes in his \"The Quadrature of the Parabola\" (3rd century BC), via the method of exhaustion. Of note is that Archimedes computed the area \"inside\" a parabola – a so-called \"parabolic segment\" – rather than the area under the graph \"y\" = \"x\", which is instead the perspective of Cartesian geometry. These are equivalent computations, but reflect a difference in perspective. The Ancient Greeks, among others, also computed the volume of a pyramid or cone, which is mathematically equivalent.\n\nIn the 11th century, the Islamic mathematician Ibn al-Haytham (known as \"Alhazen\" in Europe) computed the integrals of cubics and quartics (degree three and four) via mathematical induction, in his \"Book of Optics\".\n\nThe case of higher integers was computed by Cavalieri for \"n\" up to 9, using his method of indivisibles (Cavalieri's principle). He interpreted these as higher integrals as computing higher-dimensional volumes, though only informally, as higher-dimensional objects were as yet unfamiliar. This method of quadrature was then extended by Italian mathematician Evangelista Torricelli to other curves such as the cycloid, then the formula was generalized to fractional and negative powers by English mathematician John Wallis, in his \"Arithmetica Infinitorum\" (1656), which also standardized the notion and notation of rational powers – though Wallis incorrectly interpreted the exceptional case \"n\" = −1 (quadrature of the hyperbola) – before finally being put on rigorous ground with the development of integral calculus.\n\nPrior to Wallis's formalization of fractional and negative powers, which allowed \"explicit\" functions formula_17 these curves were handled \"implicitly,\" via the equations formula_18 and formula_19 (\"p\" and \"q\" always positive integers) and referred to respectively as higher parabolae and higher hyperbolae (or \"higher parabolas\" and \"higher hyperbolas\"). Pierre de Fermat also computed these areas (except for the exceptional case of −1) by an algebraic trick – he computed the quadrature of the higher hyperbolae via dividing the line into equal intervals, and then computed the quadrature of the higher parabolae by using a division into \"unequal\" intervals, presumably by inverting the divisions he used for hyperbolae. However, as in the rest of his work, Fermat's techniques were more ad hoc tricks than systematic treatments, and he is not considered to have played a significant part in the subsequent development of calculus.\n\nOf note is that Cavalieri only compared areas to areas and volumes to volumes – these always having \"dimensions,\" while the notion of considering an area as consisting of \"units\" of area (relative to a standard unit), hence being unitless, appears to have originated with Wallis; Wallis studied fractional and negative powers, and the alternative to treating the computed values as unitless numbers was to interpret fractional and negative dimensions.\n\nThe exceptional case of −1 (the standard hyperbola) was first successfully treated by Grégoire de Saint-Vincent in his \"Opus geometricum quadrature circuli et sectionum coni\" (1647), though a formal treatment had to wait for the development of the natural logarithm, which was accomplished by Nicholas Mercator in his \"Logarithmotechnia\" (1668).\n\n"}
{"id": "40885", "url": "https://en.wikipedia.org/wiki?curid=40885", "title": "Closed-loop transfer function", "text": "Closed-loop transfer function\n\nA closed-loop transfer function in control theory is a mathematical expression (algorithm) describing the net result of the effects of a closed (feedback) loop on the input signal to the circuits enclosed by the loop.\n\nThe closed-loop transfer function is measured at the output. The output signal waveform can be calculated from the closed-loop transfer function and the input signal waveform.\n\nAn example of a closed-loop transfer function is shown below:\n\nThe summing node and the \"G\"(\"s\") and \"H\"(\"s\") blocks can all be combined into one block, which would have the following transfer function:\n\nWe define an intermediate signal Z shown as follows:\n\nUsing this figure we write:\n\n"}
{"id": "18900634", "url": "https://en.wikipedia.org/wiki?curid=18900634", "title": "Codd's theorem", "text": "Codd's theorem\n\nCodd's theorem states that relational algebra and the domain-independent relational calculus queries, two well-known foundational query languages for the relational model, are precisely equivalent in expressive power. That is, a database query can be formulated in one language if and only if it can be expressed in the other.\n\nThe theorem is named after Edgar F. Codd, the father of the relational model for database management.\n\nThe domain independent relational calculus queries are precisely those relational calculus queries that are invariant under choosing domains of values beyond those appearing in the database itself. That is, queries that may return different results for different domains are excluded. An example of such a forbidden query is the query \"select all tuples other than those occurring in relation R\", where R is a relation in the database. Assuming different domains, i.e., sets of atomic data items from which tuples can be constructed, this query returns different results and thus is clearly not domain independent.\n\nCodd's Theorem is notable since it establishes the equivalence of two syntactically quite dissimilar languages: relational algebra is a variable-free language, while relational calculus is a logical language with variables and quantification.\n\nRelational calculus is essentially equivalent to first-order logic, and indeed, Codd's Theorem had been known to logicians since the late 1940s.\n\nQuery languages that are equivalent in expressive power to relational algebra were called relationally complete by Codd. By Codd's Theorem, this includes relational calculus. Relational completeness clearly does not imply that any interesting database query can be expressed in relationally complete languages. Well-known examples of inexpressible queries include simple aggregations (counting tuples, or summing up values occurring in tuples, which are operations expressible in SQL but not in relational algebra) and computing the transitive closure of a graph given by its binary edge relation (see also expressive power). Codd's theorem also doesn't consider SQL nulls and the three-valued logic they entail; the logical treatment of nulls remains mired in controversy. (For recent work extending Codd's theorem in this direction see the 2012 paper of Franconi and Tessaris.) Additionally, SQL allows duplicate rows (has multiset semantics.) Nevertheless, relational completeness constitutes an important yardstick by which the expressive power of query languages can be compared.\n\n\n"}
{"id": "3124950", "url": "https://en.wikipedia.org/wiki?curid=3124950", "title": "Colin de Verdière graph invariant", "text": "Colin de Verdière graph invariant\n\nColin de Verdière's invariant is a graph parameter formula_1 for any graph \"G,\" introduced by Yves Colin de Verdière in 1990. It was motivated by the study of the maximum multiplicity of the second eigenvalue of certain Schrödinger operators.\n\nLet formula_2 be a loopless simple graph. Assume without loss of generality that formula_3. Then formula_1 is the largest corank of any symmetric matrix formula_5 such that:\n\nSeveral well-known families of graphs can be characterized in terms of their Colin de Verdière invariants:\n\nThese same families of graphs also show up in connections between the Colin de Verdière invariant of a graph and the structure of its complement graph:\n\nA minor of a graph is another graph formed from it by contracting edges and by deleting edges and vertices. The Colin de Verdière invariant is minor-monotone, meaning that taking a minor of a graph can only decrease or leave unchanged its invariant:\nBy the Robertson–Seymour theorem, for every \"k\" there exists a finite set \"H\" of graphs such that the graphs with invariant at most \"k\" are the same as the graphs that do not have any member of \"H\" as a minor. lists these sets of forbidden minors for \"k\" ≤ 3; for \"k\" = 4 the set of forbidden minors consists of the seven graphs in the Petersen family, due to the two characterizations of the linklessly embeddable graphs as the graphs with μ ≤ 4 and as the graphs with no Petersen family minor.\n\n conjectured that any graph with Colin de Verdière invariant μ may be colored with at most μ + 1 colors. For instance, the linear forests have invariant 1, and can be 2-colored; the outerplanar graphs have invariant two, and can be 3-colored; the planar graphs have invariant 3, and (by the four color theorem) can be 4-colored.\n\nFor graphs with Colin de Verdière invariant at most four, the conjecture remains true; these are the linklessly embeddable graphs, and the fact that they have chromatic number at most five is a consequence of a proof by of the Hadwiger conjecture for \"K\"-minor-free graphs.\n\nIf a graph has crossing number formula_18, it has Colin de Verdière invariant at most formula_19. For instance, the two Kuratowski graphs formula_20 and formula_21 can both be drawn with a single crossing, and have Colin de Verdière invariant at most four.\n\nLouis Esperet proved the following connection between the Colin de Verdière invariant and boxicity of the same graph:\nand conjectured that the boxicity of \"G\" is at most the Colin de Verdière invariant of \"G\".\n\nColin de Verdière invariant is defined from a special class of matrices corresponding to a graph instead of just a single matrix related to the graph. Along the same lines other graph parameters are defined and studied, such as minimum rank of a graph, minimum semidefinite rank of a graph and minimum skew rank of a graph.\n\n"}
{"id": "1097925", "url": "https://en.wikipedia.org/wiki?curid=1097925", "title": "Conserved current", "text": "Conserved current\n\nIn physics a conserved current is a current, formula_1, that satisfies the continuity equation formula_2. The continuity equation represents a conservation law, hence the name.\n\nIndeed, integrating the continuity equation over a volume formula_3, large enough to have no net currents through its surface, leads to the conservation law\nwhere formula_5 is the conserved quantity.\n\nIn gauge theories the gauge fields couple to conserved currents. For example, the electromagnetic field couples to the conserved electric current.\n\nConserved current is the flow of the canonical conjugate of a quantity possessing a continuous translational symmetry. The continuity equation for the conserved current is a statement of a \"conservation law\".\nExamples of canonical conjugate quantities are:\n\nConserved currents play an extremely important role in theoretical physics, because Noether's theorem connects the existence of a conserved current to the existence of a symmetry of some quantity in the system under study. In practical terms, all conserved currents are the Noether currents, as the existence of a conserved current implies the existence of a symmetry. Conserved currents play an important role in the theory of partial differential equations, as the existence of a conserved current points to the existence of constants of motion, which are required to define a foliation and thus an integrable system. The conservation law is expressed as the vanishing of a 4-divergence, where the Noether charge forms the zeroth component of the 4-current.\n\nThe \"conservation of charge\", for example, in the notation of Maxwell's equations,\n\nwhere:\n\nρ is the \"free\" electric charge density (in units of C/m³)\n\nJ is the current density:\n\nv is the velocity of the charges.\n\nThe equation would apply equally to masses (or other conserved quantities), where the word \"mass\" is substituted for the words \"electric charge\" above.\n\n"}
{"id": "2555833", "url": "https://en.wikipedia.org/wiki?curid=2555833", "title": "Crypto API (Linux)", "text": "Crypto API (Linux)\n\nCrypto API is a cryptography framework in the Linux kernel, for various parts of the kernel that deal with cryptography, such as IPsec and dm-crypt. It was introduced in kernel version 2.5.45 and has since expanded to include essentially all popular block ciphers and hash functions.\n\nMany platforms that provide hardware acceleration of AES encryption expose this to programs through an extension of the instruction set architecture (ISA) of the various chipsets (e.g. AES instruction set for x86). With this sort of implementation any program (kernel-mode or user-space) may utilize these features directly.\n\nSome platforms, such as the ARM Kirkwood SheevaPlug and AMD Geode processors, however, are not implemented as ISA extensions, and are only accessible through kernel-mode drivers. In order for user-mode applications that utilize encryption, such as OpenSSL or GnuTLS, to take advantage of such acceleration, they must interface with the kernel.\n\n\n\n"}
{"id": "4207234", "url": "https://en.wikipedia.org/wiki?curid=4207234", "title": "Dehn plane", "text": "Dehn plane\n\nIn geometry, Dehn introduced two examples of planes, a semi-Euclidean geometry and a non-Legendrian geometry, that have infinitely many lines parallel to a given one that pass through a given point, but where the sum of the angles of a triangle is at least π. A similar phenomenon occurs in hyperbolic geometry, except that the sum of the angles of a triangle is less than π. Dehn's examples use a non-Archimedean field, so that the Archimedean axiom is violated. They were introduced by and discussed by .\n\nTo construct his geometries, Dehn used a non-Archimedean ordered Pythagorean field Ω(\"t\"), a Pythagorean closure of the field of rational functions R(\"t\"), consisting of the smallest field of real-valued functions on the real line containing the real constants, the identity function \"t\" (taking any real number to itself) and closed under the operation ω → . The field Ω(\"t\") is ordered by putting \"x\">\"y\" if the function \"x\" is larger than \"y\" for sufficiently large reals. An element \"x\" of Ω(\"t\") is called finite if \"m\"<\"x\"<\"n\" for some integers \"m\",\"n\", and is called infinite otherwise.\n\nThe set of all pairs (\"x\", \"y\"), where \"x\" and \"y\" are any (possibly infinite) elements of the field Ω(\"t\"), and with the usual metric\n\nwhich takes values in Ω(\"t\"), gives a model of Euclidean geometry. The parallel postulate is true in this model, but if the deviation from the perpendicular is infinitesimal (meaning smaller than any positive rational number), the intersecting lines intersect at a point that is not in the finite part of the plane. Hence, if the model is restricted to the finite part of the plane (points (\"x\",\"y\") with \"x\" and \"y\" finite), a geometry is obtained in which the parallel postulate fails but the sum of the angles of a triangle is π. This is Dehn's semi-Euclidean geometry. It is discussed in .\n\nIn the same paper, Dehn also constructed an example of a non-Legendrian geometry where there are infinitely many lines through a point not meeting another line, but the sum of the angles in a triangle exceeds π. Riemann's elliptic geometry over Ω(\"t\") consists of the projective plane over Ω(\"t\"), which can be identified with the affine plane of points (\"x\":\"y\":1) together with the \"line at infinity\", and has the property that the sum of the angles of any triangle is greater than π The non-Legendrian geometry consists of the points (\"x\":\"y\":1) of this affine subspace such that \"tx\" and \"ty\" are finite (where as above \"t\" is the element of Ω(\"t\") represented by the identity function). Legendre's theorem states that the sum of the angles of a triangle is at most π, but assumes Archimedes's axiom, and Dehn's example shows that Legendre's theorem need not hold if Archimedes' axiom is dropped.\n\n"}
{"id": "358488", "url": "https://en.wikipedia.org/wiki?curid=358488", "title": "Desargues's theorem", "text": "Desargues's theorem\n\nIn projective geometry, Desargues's theorem, named after Girard Desargues, states:\n\nDenote the three vertices of one triangle by and , and those of the other by and . Axial perspectivity means that lines and meet in a point, lines and meet in a second point, and lines and meet in a third point, and that these three points all lie on a common line called the \"axis of perspectivity\". Central perspectivity means that the three lines and are concurrent, at a point called the \"center of perspectivity\".\n\nThis intersection theorem is true in the usual Euclidean plane but special care needs to be taken in exceptional cases, as when a pair of sides are parallel, so that their \"point of intersection\" recedes to infinity. Commonly, to remove these exceptions, mathematicians \"complete\" the Euclidean plane by \"adding\" points at infinity following Jean-Victor Poncelet. This results in a projective plane.\n\nDesargues's theorem is true for the real projective plane, for any projective space defined arithmetically from a field or division ring, for any projective space of dimension unequal to two, and for any projective space in which Pappus's theorem holds. However, there are some non-Desarguesian planes in which Desargues's theorem is false.\n\nDesargues never published this theorem, but it appeared in an appendix entitled \"Universal Method of M. Desargues for Using Perspective (Maniére universelle de M. Desargues pour practiquer la perspective)\" of a practical book on the use of perspective published in 1648 by his friend and pupil Abraham Bosse (1602–1676).\n\nIn an affine space such as the Euclidean plane a similar statement is true, but only if one lists various exceptions involving parallel lines. Desargues's theorem is therefore one of the simplest geometric theorems whose natural home is in projective rather than affine space.\n\nBy definition, two triangles are perspective if and only if they are in perspective centrally (or, equivalently according to this theorem, in perspective axially). Note that perspective triangles need not be similar.\n\nUnder the standard duality of plane projective geometry (where points correspond to lines and collinearity of points corresponds to concurrency of lines), the statement of Desargues's theorem is self-dual: axial perspectivity is translated into central perspectivity and vice versa. The Desargues configuration (below) is a self-dual configuration.\n\nDesargues's theorem holds for projective space of any dimension over any field or division ring, and also holds for abstract projective spaces of dimension at least 3. In dimension 2 the planes for which it holds are called Desarguesian planes and are the same as the planes that can be given coordinates over a division ring. There are also many non-Desarguesian planes where Desargues's theorem does not hold.\n\nDesargues's theorem is true for any projective space of dimension at least 3, and more generally for any projective space that can be embedded in a space of dimension at least 3.\n\nDesargues's theorem can be stated as follows:\n\nThe points and are coplanar (lie in the same plane) because of the assumed concurrency of and . Therefore, the lines and belong to the same plane and must intersect. Further, if the two triangles lie on different planes, then the point belongs to both planes. By a symmetric argument, the points and also exist and belong to the planes of both triangles. Since these two planes intersect in more than one point, their intersection is a line that contains all three points.\n\nThis proves Desargues's theorem if the two triangles are not contained in the same plane. If they are in the same plane, Desargues's theorem can be proved by choosing a point not in the plane, using this to lift the triangles out of the plane so that the argument above works, and then projecting back into the plane. \nThe last step of the proof fails if the projective space has dimension less than 3, as in this case it may not be possible to find a point outside the plane.\n\nMonge's theorem also asserts that three points lie on a line, and has a proof using the same idea of considering it in three rather than two dimensions and writing the line as an intersection of two planes.\n\nAs there are non-Desarguesian projective planes in which Desargues's theorem is not true, some extra conditions need to be met in \norder to prove it. These conditions usually take the form of assuming the existence of sufficiently many collineations of a certain type, which in turn leads to showing that the underlying algebraic coordinate system must be a division ring (skewfield).\n\nPappus's hexagon theorem states that, if a hexagon is drawn in such a way that vertices and lie on a line and vertices and lie on a second line, then each two opposite sides of the hexagon lie on two lines that meet in a point and the three points constructed in this way are collinear. A plane in which Pappus's theorem is universally true is called \"Pappian\".\n\nThe converse of this result is not true, that is, not all Desarguesian planes are Pappian. Satisfying Pappus's theorem universally is equivalent to having the underlying coordinate system be commutative. A plane defined over a non-commutative division ring (a division ring that is not a field) would therefore be Desarguesian but not Pappian. However, due to Wedderburn's little theorem, which states that all \"finite\" division rings are fields, all \"finite\" Desarguesian planes are Pappian. There is no known completely geometric proof of this fact, although give a proof that uses only \"elementary\" algebraic facts (rather than the full strength of Wedderburn's little theorem).\n\nThe ten lines involved in Desargues's theorem (six sides of triangles, the three lines and , and the axis of perspectivity) and the ten points involved (the six vertices, the three points of intersection on the axis of perspectivity, and the center of perspectivity) are so arranged that each of the ten lines passes through three of the ten points, and each of the ten points lies on three of the ten lines. Those ten points and ten lines make up the Desargues configuration, an example of a projective configuration. Although Desargues's theorem chooses different roles for these ten lines and points, the Desargues configuration itself is more symmetric: \"any\" of the ten points may be chosen to be the center of perspectivity, and that choice determines which six points will be the vertices of triangles and which line will be the axis of perspectivity.\n\n\n\n"}
{"id": "1239472", "url": "https://en.wikipedia.org/wiki?curid=1239472", "title": "Eccentricity (mathematics)", "text": "Eccentricity (mathematics)\n\nIn mathematics, the eccentricity, denoted \"e\" or formula_1, is a parameter associated with every conic section. It can be thought of as a measure of how much the conic section deviates from being circular.\n\nIn particular,\n\nFurthermore, two conic sections are similar (identically shaped) if and only if they have the same eccentricity.\n\nAny conic section can be defined as the locus of points whose distances to a point (the focus) and a line (the directrix) are in a constant ratio. That ratio is called the eccentricity, commonly denoted as \"e\".\n\nThe eccentricity can also be defined in terms of the intersection of a plane and a double-napped cone associated with the conic section. If the cone is oriented with its axis vertical, the eccentricity is\n\nwhere β is the angle between the plane and the horizontal and α is the angle between the cone's slant generator and the horizontal. For formula_3 the plane section is a circle, for formula_4 a parabola. (The plane must not meet the vertex of the cone.)\n\nThe linear eccentricity of an ellipse or hyperbola, denoted \"c\" (or sometimes \"f\" or \"e\"), is the distance between its center and either of its two foci. The eccentricity can be defined as the ratio of the linear eccentricity to the semimajor axis \"a\": that is, formula_5. (Lacking a center, the linear eccentricity for parabolas is not defined.)\n\nThe eccentricity is sometimes called the first eccentricity to distinguish it from the second eccentricity and third eccentricity defined for ellipses (see below). The eccentricity is also sometimes called the numerical eccentricity.\n\nIn the case of ellipses and hyperbolas the linear eccentricity is sometimes called the half-focal separation.\n\nThree notational conventions are in common use:\nThis article uses the first notation.\n\nHere, for the ellipse and the hyperbola, \"a\" is the length of the semi-major axis and \"b\" is the length of the semi-minor axis.\n\nWhen the conic section is given in the general quadratic form\n\nthe following formula gives the eccentricity \"e\" if the conic section is not a parabola (which has eccentricity equal to 1), not a degenerate hyperbola or degenerate ellipse, and not an imaginary ellipse:\n\nwhere formula_10 if the determinant of the 3×3 matrix\n\nis negative or formula_12 if that determinant is positive.\n\nThe eccentricity of an ellipse is strictly less than 1. When circles (which have eccentricity 0) are counted as ellipses, the eccentricity of an ellipse is greater than or equal to 0; if circles are given a special category and are excluded from the category of ellipses, then the eccentricity of an ellipse is strictly greater than 0.\n\nFor any ellipse, let \"a\" be the length of its semi-major axis and \"b\" be the length of its semi-minor axis.\n\nWe define a number of related additional concepts (only for ellipses):\n\nThe eccentricity of an ellipse is, most simply, the ratio of the distance \"f\" between the center of the ellipse and each focus to the length of the semimajor axis \"a\".\n\nThe eccentricity is also the ratio of the semimajor axis \"a\" to the distance \"d\" from the center to the directrix:\n\nThe eccentricity can be expressed in terms of the flattening \"g\" (defined as \"g\" = 1 – \"b\"/\"a\" for semimajor axis \"a\" and semiminor axis \"b\"):\n\nDefine the maximum and minimum radii formula_16 and formula_17 as the maximum and minimum distances from either focus to the ellipse (that is, the distances from either focus to the two ends of the major axis). Then with semimajor axis \"a\", the eccentricity is given by\n\nwhich is the distance between the foci divided by the length of the major axis.\n\nThe eccentricity of a hyperbola can be any real number greater than 1, with no upper bound. The eccentricity of a rectangular hyperbola is formula_19.\n\nThe eccentricity of a three-dimensional quadric is the eccentricity of a designated section of it. For example, on a triaxial ellipsoid, the \"meridional eccentricity\" is that of the ellipse formed by a section containing both the longest and the shortest axes (one of which will be the polar axis), and the \"equatorial eccentricity\" is the eccentricity of the ellipse formed by a section through the centre, perpendicular to the polar axis (i.e. in the equatorial plane). But: conic sections may occur on surfaces of higher order, too (see image).\n\nIn celestial mechanics, for bound orbits in a spherical potential, the definition above is informally generalized. When the apocenter distance is close to the pericenter distance, the orbit is said to have low eccentricity; when they are very different, the orbit is said be eccentric or having eccentricity near unity. This definition coincides with the mathematical definition of eccentricity for ellipses, in Keplerian, i.e., formula_20 potentials.\n\nA number of classifications in mathematics use derived terminology from the classification of conic sections by eccentricity:\n\nThe eccentricity is also a concept which is used to characterize statistical distribution of data points around a common axis. For example, the eccentricity can be used to characterize shapes of jets of many particles.\n\nThe definition closely follows the original \"geometrical\" concept, with one important difference – data points can have \"weights\". Such weights can lead to a deviation from the standard geometrical concept that assumes that all data points have the same contributions.\n\n\n"}
{"id": "26374518", "url": "https://en.wikipedia.org/wiki?curid=26374518", "title": "Generic flatness", "text": "Generic flatness\n\nIn algebraic geometry and commutative algebra, the theorems of generic flatness and generic freeness state that under certain hypotheses, a sheaf of modules on a scheme is flat or free. They are due to Alexander Grothendieck.\n\nGeneric flatness states that if \"Y\" is an integral locally noetherian scheme, is a finite type morphism of schemes, and \"F\" is a coherent \"O\"-module, then there is a non-empty open subset \"U\" of \"Y\" such that the restriction of \"F\" to \"u\"(\"U\") is flat over \"U\".\n\nBecause \"Y\" is integral, \"U\" is a dense open subset of \"Y\". This can be applied to deduce a variant of generic flatness which is true when the base is not integral. Suppose that \"S\" is a noetherian scheme, is a finite type morphism, and \"F\" is a coherent \"O\" module. Then there exists a partition of \"S\" into locally closed subsets \"S\", ..., \"S\" with the following property: Give each \"S\" its reduced scheme structure, denote by \"X\" the fiber product , and denote by \"F\" the restriction ; then each \"F\" is flat.\n\nGeneric flatness is a consequence of the generic freeness lemma. Generic freeness states that if \"A\" is a noetherian integral domain, \"B\" is a finite type \"A\"-algebra, and \"M\" is a finite type \"B\"-module, then there exists a non-zero element \"f\" of \"A\" such that \"M\" is a free \"A\"-module. Generic freeness can be extended to the graded situation: If \"B\" is graded by the natural numbers, \"A\" acts in degree zero, and \"M\" is a graded \"B\"-module, then \"f\" may be chosen such that each graded component of \"M\" is free.\n\nGeneric freeness is proved using Grothendieck's technique of dévissage. See for a proof of a version of generic freeness.\n\n"}
{"id": "52405825", "url": "https://en.wikipedia.org/wiki?curid=52405825", "title": "Graded-commutative ring", "text": "Graded-commutative ring\n\nIn algebra, a graded-commutative ring (also called a skew-commutative ring) is a graded ring that is commutative in the graded sense; that is, homogeneous elements \"x\", \"y\" satisfy\nwhere |\"x\"|, |\"y\"| denote the degrees of \"x\", \"y\".\n\nA commutative (non-graded) ring, with trivial grading, is a basic example. An exterior algebra is an example of a graded-commutative ring that is not commutative in the non-graded sense.\n\nA cup product on cohomology satisfies the skew-commutative relation; hence, a cohomology ring is graded-commutative. In fact, many examples of graded-commutative rings come from algebraic topology and homological algebra.\n\n\n"}
{"id": "1186249", "url": "https://en.wikipedia.org/wiki?curid=1186249", "title": "Guard (computer science)", "text": "Guard (computer science)\n\nIn computer programming, a guard is a boolean expression that must evaluate to true if the program execution is to continue in the branch in question.\n\nRegardless of which programming language is used, guard code or a guard clause is a check of integrity preconditions used to avoid errors during execution. A typical example is checking that a reference about to be processed be not null, which avoids null-pointer failures. Other uses include using a boolean field for idempotence (so subsequent calls are nops), as in the dispose pattern. Guard code provides an early exit from a subroutine, and is a commonly used deviation from structured programming, removing one level of nesting and resulting in flatter code: replacing codice_1 with codice_2.\n\nThe term is used with specific meaning in APL, Haskell, Clean, Erlang, occam, Promela, OCaml, Swift and Scala programming languages. In Mathematica, guards are called \"constraints\". Guards are the fundamental concept in Guarded Command Language, a language in formal methods. Guards can be used to augment pattern matching with the possibility to skip a pattern even if the structure matches. Boolean expressions in conditional statements usually also fit this definition of a guard although they are called \"conditions\".\n\nIn the following Haskell example, the guards occur between each pair of \"|\" and \"=\":\n\nThis is similar to the respective mathematical notation:\n\nformula_1\n\nIn this case the guards are in the \"if\" and \"otherwise\" clauses.\n\nIf there are several parallel guards, such as in the example above, they are normally tried in a top-to-bottom order, and the branch of the first to pass is chosen. Guards in a list of cases are typically parallel.\n\nHowever, in Haskell list comprehensions the guards are in series, and if any of them fails, the list element is not produced. This would be the same as combining the separate guards with logical AND, except that there can be other list comprehension clauses among the guards.\n\nA simple conditional expression, already present in CPL in 1963, has a guard on first sub-expression, and another sub-expression to use in case the first one cannot be used. Some common ways to write this:\n\nIf the second sub-expression can be a further simple conditional expression, we can give more alternatives to try before the last \"fall-through\":\n\nIn 1966 ISWIM had a form of conditional expression without an obligatory fall-through case, thus separating guard from the concept of choosing either-or. In the case of ISWIM, if none of the alternatives could be used, the value was to be \"undefined\", which was defined to never compute into a value.\n\nKRC, a \"miniaturized version\" of SASL (1976), was one of the first programming languages to use the term \"guard\". Its function definitions could have several clauses, and the one to apply was chosen based on the guards that followed each clause:\n\nUse of guard clauses, and the term \"guard clause\", dates at least to Smalltalk practice in the 1990s, as codified by Kent Beck.\n\nIn 1996, Dyalog APL adopted an alternative pure functional style in which the guard is the only control structure. This example, in APL, computes the parity of the input number:\nparity←{\n\nIn addition to a guard attached to a pattern, pattern guard can refer to the use of pattern matching in the context of a guard. In effect, a match of the pattern is taken to mean pass. This meaning was introduced in a proposal for Haskell by Simon Peyton Jones titled A new view of guards in April 1997 and was used in the implementation of the proposal. The feature provides the ability to use patterns in the guards of a pattern.\n\nAn example in extended Haskell:\n\nThis would read: \"Clunky for an environment and two variables, \"in case the lookups of the variables from the environment produce values\", is the sum of the values. ...\" As in list comprehensions, the guards are in series, and if any of them fails the branch is not taken.\n\n\n"}
{"id": "51716359", "url": "https://en.wikipedia.org/wiki?curid=51716359", "title": "Haar space", "text": "Haar space\n\nIn approximation theory, a Haar space or Chebyshev space is a finite-dimensional subspace formula_1 of formula_2, where formula_3 is a compact space and formula_4 either the real numbers or the complex numbers, such that for any given formula_5 there is exactly one element of formula_1 that approximates formula_7 \"best\", i.e. with minimum distance to formula_7 in supremum norm.\n"}
{"id": "507553", "url": "https://en.wikipedia.org/wiki?curid=507553", "title": "Hadamard matrix", "text": "Hadamard matrix\n\nIn mathematics, a Hadamard matrix, named after the French mathematician Jacques Hadamard, is a square matrix whose entries are either +1 or −1 and whose rows are mutually orthogonal. In geometric terms, this means that each pair of rows in a Hadamard matrix represents two perpendicular vectors, while in combinatorial terms, it means that each pair of rows has matching entries in exactly half of their columns and mismatched entries in the remaining columns. It is a consequence of this definition that the corresponding properties hold for columns as well as rows. The \"n\"-dimensional parallelotope spanned by the rows of an \"n\"×\"n\" Hadamard matrix has the maximum possible \"n\"-dimensional volume among parallelotopes spanned by vectors whose entries are bounded in absolute value by 1. Equivalently, a Hadamard matrix has maximal determinant among matrices with entries of absolute value less than or equal to 1 and so is an extremal solution of Hadamard's maximal determinant problem.\n\nCertain Hadamard matrices can almost directly be used as an error-correcting code using a Hadamard code (generalized in Reed–Muller codes), and are also used in balanced repeated replication (BRR), used by statisticians to estimate the variance of a parameter estimator.\n\nLet \"H\" be a Hadamard matrix of order \"n\". The transpose of \"H\" is closely related to its inverse. In fact:\n\nwhere \"I\" is the \"n\" × \"n\" identity matrix and \"H\" is the transpose of \"H\". To see that this is true, notice that the rows of \"H\" are all orthogonal vectors over the field of real numbers and each have length formula_2. Dividing \"H\" through by this length gives an orthogonal matrix whose transpose is thus its inverse. Multiplying by the length again gives the equality above. As a result,\n\nwhere det(\"H\") is the determinant of \"H\".\n\nSuppose that \"M\" is a complex matrix of order \"n\", whose entries are bounded by |\"M\"| ≤1, for each \"i\", \"j\" between 1 and \"n\". Then Hadamard's determinant bound states that\n\nEquality in this bound is attained for a real matrix \"M\" if and only if \"M\" is a Hadamard matrix.\n\nThe order of a Hadamard matrix must be 1, 2, or a multiple of 4.\n\nExamples of Hadamard matrices were actually first constructed by James Joseph Sylvester in 1867. Let \"H\" be a Hadamard matrix of order \"n\". Then the partitioned matrix\nis a Hadamard matrix of order 2\"n\". This observation can be applied repeatedly and leads to the following sequence of matrices, also called Walsh matrices.\n\nand\n\nfor formula_10, where formula_11 denotes the Kronecker product.\n\nIn this manner, Sylvester constructed Hadamard matrices of order 2 for every non-negative integer \"k\".\n\nSylvester's matrices have a number of special properties. They are symmetric and, when \"k\" ≥ 1, have trace zero. The elements in the first column and the first row are all positive. The elements in all the other rows and columns are evenly divided between positive and negative. Sylvester matrices are closely connected with Walsh functions.\n\nIf we map the elements of the Hadamard matrix using the group homomorphism formula_12, we can describe an alternative construction of Sylvester's Hadamard matrix. First consider the matrix formula_13, the formula_14 matrix whose columns consist of all \"n\"-bit numbers arranged in ascending counting order. We may define formula_13 recursively by\n\nIt can be shown by induction that the image of the Hadamard matrix under the above homomorphism is given by\n\nThis construction demonstrates that the rows of the Hadamard matrix formula_19 can be viewed as a length formula_20 linear error-correcting code of rank \"n\", and minimum distance formula_21 with generating matrix formula_22\n\nThis code is also referred to as a Walsh code. The Hadamard code, by contrast, is constructed from the Hadamard matrix formula_19 by a slightly different procedure.\n\nThe most important open question in the theory of Hadamard matrices is that of existence. The Hadamard conjecture proposes that a Hadamard matrix of order 4\"k\" exists for every positive integer \"k\". The Hadamard conjecture has also been attributed to Paley, although it was considered implicitly by others prior to Paley's work.\n\nA generalization of Sylvester's construction proves that if formula_24 and formula_25 are Hadamard matrices of orders \"n\" and \"m\" respectively, then formula_26 is a Hadamard matrix of order \"nm\". This result is used to produce Hadamard matrices of higher order once those of smaller orders are known.\n\nSylvester's 1867 construction yields Hadamard matrices of order 1, 2, 4, 8, 16, 32, etc. Hadamard matrices of orders 12 and 20 were subsequently constructed by Hadamard (in 1893). In 1933, Raymond Paley discovered the Paley construction, which produces a Hadamard matrix of order \"q\"+1 when \"q\" is any prime power that is congruent to 3 modulo 4 and that produces a Hadamard matrix of order 2(\"q\"+1) when \"q\" is a prime power that is congruent to 1 modulo 4. His method uses finite fields.\n\nThe smallest order that cannot be constructed by a combination of Sylvester's and Paley's methods is 92. A Hadamard matrix of this order was found using a computer by Baumert, Golomb, and Hall in 1962 at JPL. They used a construction, due to Williamson, that has yielded many additional orders. Many other methods for constructing Hadamard matrices are now known.\n\nIn 2005, Hadi Kharaghani and Behruz Tayfeh-Rezaie published their construction of a Hadamard matrix of order 428. As a result, the smallest order for which no Hadamard matrix is presently known is 668. \n\n, there are 13 multiples of 4 less than or equal to 2000 for which no Hadamard matrix of that order is known. They are:\n668, 716, 892, 1004, 1132, 1244, 1388, 1436, 1676, 1772, 1916, 1948, and 1964.\n\nTwo Hadamard matrices are considered equivalent if one can be obtained from the other by negating rows or columns, or by interchanging rows or columns. Up to equivalence, there is a unique Hadamard matrix of orders 1, 2, 4, 8, and 12. There are 5 inequivalent matrices of order 16, 3 of order 20, 60 of order 24, and 487 of order 28. Millions of inequivalent matrices are known for orders 32, 36, and 40. Using a coarser notion of equivalence that also allows transposition, there are 4 inequivalent matrices of order 16, 3 of order 20, 36 of order 24, and 294 of order 28.\n\nA Hadamard matrix \"H\" is \"skew\" if formula_27\n\nReid and Brown in 1972 showed that there exists a \"doubly regular tournament of order \"n\"\" if and only if there exists a skew Hadamard matrix of order \"n\" + 1.\n\nMany generalizations and special cases of Hadamard matrices have been investigated in the mathematical literature. One basic generalization is the weighing matrix, a square matrix in which entries may also be zero and which satisfies formula_28 for some w, its weight. A weighing matrix with its weight equal to its order is a Hadamard matrix.\n\nAnother generalization defines a complex Hadamard matrix to be a matrix in which the entries are complex numbers of unit modulus and which satisfies \"H H= n I\" where \"H\" is the conjugate transpose of \"H\". Complex Hadamard matrices arise in the study of operator algebras and the theory of quantum computation.\nButson-type Hadamard matrices are complex Hadamard matrices in which the entries are taken to be \"q\" roots of unity. The term \"complex Hadamard matrix\" has been used by some authors to refer specifically to the case \"q\" = 4.\n\nRegular Hadamard matrices are real Hadamard matrices whose row and column sums are all equal. A necessary condition on the existence of a regular \"n\"×\"n\" Hadamard matrix is that \"n\" be a perfect square. A circulant matrix is manifestly regular, and therefore a circulant Hadamard matrix would have to be of perfect square order. Moreover, if an \"n\"×\"n\" circulant Hadamard\nmatrix existed with \"n\" > 1 then \"n\" would necessarily have to be of the form 4\"u\" with \"u\" odd.\n\nThe circulant Hadamard matrix conjecture, however, asserts that, apart from the known 1×1 and 4×4 examples, no such matrices exist. This was verified for all but 26 values of \"u\" less than 10.\n\n\n\n\n"}
{"id": "34712196", "url": "https://en.wikipedia.org/wiki?curid=34712196", "title": "Hattori–Stong theorem", "text": "Hattori–Stong theorem\n\nIn algebraic topology, the Hattori–Stong theorem, proved by and , gives an isomorphism between the stable homotopy of a Thom spectrum and the primitive elements of its K-homology.\n\n"}
{"id": "481119", "url": "https://en.wikipedia.org/wiki?curid=481119", "title": "Homotopy principle", "text": "Homotopy principle\n\nIn mathematics, the homotopy principle (or h-principle) is a very general way to solve partial differential equations (PDEs), and more generally partial differential relations (PDRs). The h-principle is good for underdetermined PDEs or PDRs, such as occur in the immersion problem, isometric immersion problem, fluid dynamics, and other areas.\n\nThe theory was started by Yakov Eliashberg, Mikhail Gromov and Anthony V. Phillips. It was based on earlier results that reduced partial differential relations to homotopy, particularly for immersions. The first evidence of h-principle appeared in the Whitney–Graustein theorem. This was followed by the Nash-Kuiper Isometric formula_1 embedding theorem and the Smale-Hirsch Immersion theorem.\n\nAssume we want to find a function \"ƒ\" on R which satisfies a partial differential equation of degree \"k\", in co-ordinates formula_2. One can rewrite it as\n\nwhere formula_4 stands for all partial derivatives of \"ƒ\" up to order \"k\". Let us exchange every variable in formula_4 for new independent variables formula_6\nThen our original equation can be thought as a system of\n\nand some number of equations of the following type \n\nA solution of\n\nis called a non-holonomic solution, and a solution of the system (which is a solution of our original PDE) is called a holonomic solution.\n\nIn order to check whether a solution exists, first check if there is a non-holonomic solution (usually it is quite easy and if not then our original equation did not have any solutions).\n\nA PDE \"satisfies the h-principle\" if any non-holonomic solution can be deformed into a holonomic one in the class of non-holonomic solutions. Thus in the presence of h-principle, a differential topological problem reduces to an algebraic topological problem. More explicitly this means that apart from the topological obstruction there is no other obstruction to the existence of a holonomic solution. The topological problem of finding a \"non-holonomic solution\" is much easier to handle and can be addressed with the obstruction theory for topological bundles.\n\nMany underdetermined partial differential equations satisfy the h-principle. However, the falsity of an h-principle is also an interesting statement, intuitively this means the objects being studied have non-trivial geometry that cannot be reduced to topology. As an example, embedded Lagrangians in a symplectic manifold do not satisfy an h-principle, to prove this one needs to find invariants coming from pseudo-holomorphic curves.\n\nPerhaps the simplest partial differential relation is for the derivative to not vanish: formula_10 Properly, this is an \"ordinary\" differential relation, as this is a function in one variable. \n\nA holonomic solution to this relation is a function whose derivative is nowhere vanishing. I.e., a strictly monotone differentiable functions, either increasing or decreasing. The space of such functions consists of two disjoint convex sets: the increasing ones and the decreasing ones, and has the homotopy type of two points.\n\nA non-holonomic solution to this relation would consist in the data of two functions, a differentiable function f(x), and a continuous function g(x), with g(x) nowhere vanishing. A holonomic solution gives rise to a non-holonomic solution by taking g(x) = f'(x). The space of non-holonomic solutions again consists of two disjoint convex sets, according as g(x) is positive or negative. \n\nThus the inclusion of holonomic into non-holonomic solutions satisfies the h-principle. \nThis trivial example has nontrivial generalizations:\nextending this to immersions of a circle into itself classifies them by order (or winding number), by lifting the map to the universal covering space and applying the above analysis to the resulting monotone map – the linear map corresponds to multiplying angle: formula_11 (formula_12 in complex numbers). Note that here there are no immersions of order 0, as those would need to turn back on themselves. Extending this to circles immersed in the plane – the immersion condition is precisely the condition that the derivative does not vanish – the Whitney–Graustein theorem classified these by turning number by considering the homotopy class of the Gauss map and showing that this satisfies an h-principle; here again order 0 is more complicated.\n\nSmale's classification of immersions of spheres as the homotopy groups of Stiefel manifolds, and Hirsch's generalization of this to immersions of manifolds being classified as homotopy classes of maps of frame bundles are much further-reaching generalizations, and much more involved, but similar in principle – immersion requires the derivative to have rank \"k,\" which requires the partial derivatives in each direction to not vanish and to be linearly independent, and the resulting analog of the Gauss map is a map to the Stiefel manifold, or more generally between frame bundles.\n\nAs another simple example, consider a car moving in the plane. The position of a car in the plane is determined by three parameters: two coordinates formula_13 and formula_14 for the location (a good choice is the location of the midpoint between the back wheels) and an angle formula_15 which describes the orientation of the car. The motion of the car satisfies the equation\n\nsince a non-skidding car must move in the direction of its wheels. In robotics terms, not all paths in the task space are holonomic.\n\nA non-holonomic solution in this case, roughly speaking, corresponds to a motion of the car by sliding in the plane. In this case the non-holonomic solutions are not only homotopic to holonomic ones but also can be arbitrarily well approximated by the holonomic ones (by going back and forth, like parallel parking in a limited space) – note that this approximates both the position and the angle of the car arbitrarily closely. This implies that, theoretically, it is possible to parallel park in any space longer than the length of your car. It also implies that, in a contact 3 manifold, any curve is formula_17-close to a Legendrian curve.\nThis last property is stronger than the general h-principle; it is called the formula_17-dense h-principle.\n\nWhile this example is simple, compare to the Nash embedding theorem, specifically the Nash–Kuiper theorem, which says that any short smooth (formula_19) embedding or immersion of formula_20 in formula_21 or larger can be arbitrarily well approximated by an isometric formula_1-embedding (respectively, immersion). This is also a dense h-principle, and can be proven by an essentially similar \"wrinkling\" – or rather, circling – technique to the car in the plane, though it is much more involved.\n\n\nHere we list a few counter-intuitive results which can be proved by applying the \nh-principle:\n\n\n"}
{"id": "1227519", "url": "https://en.wikipedia.org/wiki?curid=1227519", "title": "Horn-satisfiability", "text": "Horn-satisfiability\n\nIn formal logic, Horn-satisfiability, or HORNSAT, is the problem of deciding whether a given set of propositional Horn clauses is satisfiable or not. Horn-satisfiability and Horn clauses are named after Alfred Horn.\n\nA Horn clause is a clause with at most one positive literal, called the \"head\" of the clause, and any number of negative literals, forming the \"body\" of the clause. A Horn formula is a propositional formula formed by conjunction of Horn clauses.\n\nThe problem of Horn satisfiability is solvable in linear time. \nThe problem of deciding the truth of quantified Horn formulas can be also solved in polynomial time.\nA polynomial-time algorithm for Horn satisfiability is based on the rule of unit propagation: if the formula contains a clause composed of a single literal formula_1 (a unit clause), then all clauses containing formula_1 (except the unit clause itself) are removed, and all clauses containing formula_3 have this literal removed. The result of the second rule may itself be a unit clause, which is propagated in the same manner. If there are no unit clauses, the formula can be satisfied by simply setting all remaining variables negative. The formula is unsatisfiable if this transformation generates a pair of opposite unit clauses formula_1 and formula_3. Horn satisfiability is actually one of the \"hardest\" or \"most expressive\" problems which is known to be computable in polynomial time, in the sense that it is a P-complete problem.\n\nThis algorithm also allows determining a truth assignment of satisfiable Horn formulae: all variables contained in a unit clause are set to the value satisfying that unit clause; all other literals are set to false. The resulting assignment is the minimal model of the Horn formula, that is, the assignment having a minimal set of variables assigned to true, where comparison is made using set containment.\n\nUsing a linear algorithm for unit propagation, the algorithm is linear in the size of the formula.\n\nA generalization of the class of Horn formulae is that of renamable-Horn formulae, which is the set of formulae that can be placed in Horn form by replacing some variables with their respective negation. Checking the existence of such a replacement can be done in linear time; therefore, the satisfiability of such formulae is in P as it can be solved by first performing this replacement and then checking the satisfiability of the resulting Horn formula. Horn satisfiability and renamable Horn satisfiability provide one of two important subclasses of satisfiability that are solvable in polynomial time; the other such subclass is 2-satisfiability.\n\nThe Horn satisfiability problem can also be asked for propositional many-valued logics. The algorithms are not usually linear, but some are polynomial; see Hähnle (2001 or 2003) for a survey.\n\n"}
{"id": "254777", "url": "https://en.wikipedia.org/wiki?curid=254777", "title": "Isometry", "text": "Isometry\n\nIn mathematics, an isometry (or congruence, or congruent transformation) is a distance-preserving transformation between metric spaces, usually assumed to be bijective. \nGiven a metric space (loosely, a set and a scheme for assigning distances between elements of the set), an isometry is a transformation which maps elements to the same or another metric space such that the distance between the image elements in the new metric space is equal to the distance between the elements in the original metric space. In a two-dimensional or three-dimensional Euclidean space, two geometric figures are congruent if they are related by an isometry; the isometry that relates them is either a rigid motion (translation or rotation), or a composition of a rigid motion and a reflection. \n\nIsometries are often used in constructions where one space is embedded in another space. For instance, the completion of a metric space \"M\" involves an isometry from \"M\" into \"M\"', a quotient set of the space of Cauchy sequences on \"M\". The original space \"M\" is thus isometrically isomorphic to a subspace of a complete metric space, and it is usually identified with this subspace. Other embedding constructions show that every metric space is isometrically isomorphic to a closed subset of some normed vector space and that every complete metric space is isometrically isomorphic to a closed subset of some Banach space.\n\nAn isometric surjective linear operator on a Hilbert space is called a unitary operator.\n\nLet \"X\" and \"Y\" be metric spaces with metrics \"d\" and \"d\". A map \"f\" : \"X\" → \"Y\" is called an isometry or distance preserving if for any \"a\",\"b\" ∈ \"X\" one has\n\nAn isometry is automatically injective; otherwise two distinct points, \"a\" and \"b\", could be mapped to the same point, thereby contradicting the coincidence axiom of the metric \"d\". This proof is similar to the proof that an order embedding between partially ordered sets is injective. Clearly, every isometry between metric spaces is a topological embedding.\n\nA global isometry, isometric isomorphism or congruence mapping is a bijective isometry. Like any other bijection, a global isometry has a function inverse. The inverse of a global isometry is also a global isometry.\n\nTwo metric spaces \"X\" and \"Y\" are called isometric if there is a bijective isometry from \"X\" to \"Y\". The set of bijective isometries from a metric space to itself forms a group with respect to function composition, called the isometry group.\n\nThere is also the weaker notion of \"path isometry\" or \"arcwise isometry\":\n\nA path isometry or arcwise isometry is a map which preserves the lengths of curves; such a map is not necessarily an isometry in the distance preserving sense, and it need not necessarily be bijective, or even injective. This term is often abridged to simply \"isometry\", so one should take care to determine from context which type is intended.\n\n\nGiven two normed vector spaces \"V\" and \"W\", a linear isometry is a linear map \"f\" : \"V\" → \"W\" that preserves the norms:\nfor all \"v\" in \"V\". Linear isometries are distance-preserving maps in the above sense. They are global isometries if and only if they are surjective.\n\nBy the Mazur-Ulam theorem, any isometry of normed vector spaces over R is affine.\n\nIn an inner product space, the fact that any linear isometry is an orthogonal transformation can be shown by using polarization identities to\nprove \"<Ax, Ay> = <x, y>\" and then applying the Riesz representation theorem.\n\n\n\n\n"}
{"id": "48245199", "url": "https://en.wikipedia.org/wiki?curid=48245199", "title": "John Brown Clark", "text": "John Brown Clark\n\nJohn Brown Clark or Clarke CBE LLD FRSE (1861–1947) was a Scottish mathematician. He was headmaster of George Heriot’s School from 1908. He served as Vice President of the Royal Society of Edinburgh 1931–34.\n\nHe was born in West Linton on 30 April 1861 the son of George Clark from Newbigging, South Lanarkshire. He attended West Linton School and then from 1877 the Heriot School at Abbeyhill in Edinburgh. From 1881 he trained at the Established Church Training College in Edinburgh. From 1883 to 1885 he served as an assistant teacher at St Leonards school in Edinburgh, then studied for a degree at Edinburgh University, graduating MA in 1889. He then obtained a job teaching mathematics at George Heriot’s School. In 1908 he succeeded David Fowler Lowe as headmaster and served in that role until 1926. He was succeeded in his role by William Gentle FRSE.\n\nIn 1891 he was elected a Fellow of the Royal Society of Edinburgh, his proposers including Sir John Murray, George Chrystal, Peter Guthrie Tait and David Fowler Lowe. He served there as a Councillor 1928–31 and as their Vice-President from 1931–34. He was awarded a Commander of the Order of the British Empire in 1935.\n\nHe died on 19 July 1947.\n\nHe married Mary Mackay in 1891.\n\nHe joined the Edinburgh Mathematical Society in December 1885. He served as their Secretary 1891–96, Vice-President 1896–97 and President 1897–98.\n"}
{"id": "469391", "url": "https://en.wikipedia.org/wiki?curid=469391", "title": "List of curves topics", "text": "List of curves topics\n\nThis is an alphabetical index of articles related to curves. See also curve, list of curves, and list of differential geometry topics.\n"}
{"id": "49068749", "url": "https://en.wikipedia.org/wiki?curid=49068749", "title": "Margin at risk", "text": "Margin at risk\n\nThe Margin-at-Risk (short: MaR) is a quantity used to manage short-term liquidity risks due to variation of margin requirements, i.e. it is a financial risk occurring when trading commodities. Similar to the Value-at-Risk (VaR), but instead of the EBIT it is a quantile of the (expected) cash flow distribution. \n\nA MaR requires (1) a currency, (2) a confidence level (e.g. 90%) and (3) a holding period (e.g. 3 days).\nThe idea is that a given portfolio loss will be compensated by a margin call by the same amount.\nThe MaR quantifies the \"worst case\" margin-call and is only driven by market prices.\n\n"}
{"id": "244516", "url": "https://en.wikipedia.org/wiki?curid=244516", "title": "Material implication (rule of inference)", "text": "Material implication (rule of inference)\n\nIn propositional logic, material implication is a valid rule of replacement that allows for a conditional statement to be replaced by a disjunction in which the antecedent is negated. The rule states that \"P implies Q\" is logically equivalent to \"not-P or Q\" and that either form can replace the other in logical proofs.\n\nWhere \"formula_2\" is a metalogical symbol representing \"can be replaced in a proof with.\"\n\nThe \"material implication\" rule may be written in sequent notation:\nwhere formula_4 is a metalogical symbol meaning that formula_5 is a syntactic consequence of formula_6 in some logical system;\n\nor in rule form:\nwhere the rule is that wherever an instance of \"formula_8\" appears on a line of a proof, it can be replaced with \"formula_9\";\n\nor as the statement of a truth-functional tautology or theorem of propositional logic:\n\nwhere formula_11 and formula_12 are propositions expressed in some formal system.\n\nAn example is:\n\nwhere formula_11 is the statement \"it is a bear\" and formula_12 is the statement \"it can swim\".\n\nIf it was found that the bear could not swim, written symbolically as formula_15, then both sentences are false but otherwise they are both true.\n"}
{"id": "1372589", "url": "https://en.wikipedia.org/wiki?curid=1372589", "title": "Modular invariance", "text": "Modular invariance\n\nIn theoretical physics, modular invariance is the invariance under the group such as SL(2,Z) of large diffeomorphisms of the torus. The name comes from the classical name modular group of this group, as in modular form theory.\n\nIn string theory, modular invariance is an additional requirement for one-loop diagrams. This helps in getting rid of some global anomalies such as the gravitational anomalies.\n"}
{"id": "21393064", "url": "https://en.wikipedia.org/wiki?curid=21393064", "title": "Neural Networks (journal)", "text": "Neural Networks (journal)\n\nNeural Networks is a monthly peer-reviewed scientific journal and an official journal of the International Neural Network Society, European Neural Network Society, and Japanese Neural Network Society. It was established in 1988 and is published by Elsevier. The journal covers all aspects of research on artificial neural networks. The founding editor-in-chief was Stephen Grossberg (Boston University), the current editors-in-chief are DeLiang Wang (Ohio State University) and Kenji Doya (Okinawa Institute of Science and Technology). The journal is abstracted and indexed in Scopus and the Science Citation Index. According to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 5.287.\n"}
{"id": "20914085", "url": "https://en.wikipedia.org/wiki?curid=20914085", "title": "Non-credible threat", "text": "Non-credible threat\n\nA non-credible threat is a term used in game theory and economics to describe a threat in a sequential game that a \"rational\" player would actually not carry out, because it would not be in his best interest to do so. \n\nFor a simple example, suppose person A walks up, carrying a bomb, to another person B. A tells B he will set off the bomb, killing them both, unless B gives him all his money. If A is rational and non-suicidal he stands nothing to gain from setting off the bomb, so his threat cannot be considered credible. On the other hand, a person in the situation of B might give A his money, fearing that A is not rational, or might even be suicidal.\n\nA non-credible threat is made on the hope that it will be believed, and therefore the threatening undesirable action will not need to be carried out. For a threat to be credible within an equilibrium, whenever a node is reached where a threat should be fulfilled, it will be fulfilled. Those Nash equilibria that rely on non-credible threats can be eliminated through backward induction; the remaining equilibria are called subgame perfect Nash equilibria.\n\n"}
{"id": "30872150", "url": "https://en.wikipedia.org/wiki?curid=30872150", "title": "Object-Z", "text": "Object-Z\n\nObject-Z is an object-oriented extension to the Z notation developed at the University of Queensland, Australia.\n\nObject-Z extends Z by the addition of language constructs resembling the object-oriented paradigm, most notably, classes. Other object-oriented notions such as polymorphism and inheritance are also supported.\n\nWhile not as popular as its base language Z, Object-Z has still received significant attention in the formal methods community, and research on aspects of the language are ongoing, including hybrid languages using Object-Z, tool support (e.g., through the Community Z Tools project) and refinement calculi.\n\n\n"}
{"id": "1003283", "url": "https://en.wikipedia.org/wiki?curid=1003283", "title": "Olinde Rodrigues", "text": "Olinde Rodrigues\n\nBenjamin Olinde Rodrigues (6 October 1795 – 17 December 1851), more commonly known as Olinde Rodrigues, was a French banker, mathematician, and social reformer.\n\nRodrigues was born into a well-to-do Sephardi Jewish family in Bordeaux.\n\nRodrigues was awarded a doctorate in mathematics on 28 June 1815 by the University of Paris. His dissertation contains the result now called Rodrigues' formula.\n\nAfter graduation, Rodrigues became a banker. A close associate of the Comte de Saint-Simon, Rodrigues continued, after Saint-Simon's death in 1825, to champion the older man's socialist ideals, a school of thought that came to be known as Saint-Simonianism. During this period, Rodrigues published writings on politics, social reform, and banking.\n\nIn 1840 he published a result on transformation groups, which applied Leonhard Euler's four squares formula, a precursor to the quaternions of William Rowan Hamilton, to the problem of representing rotations in space. \nIn 1846 Arthur Cayley acknowledged Euler's and Rodrigues' priority describing orthogonal transformations.\n\nIn mathematics Rodrigues is remembered for three results: Rodrigues' rotation formula for vectors, the Rodrigues formula about series of orthogonal polynomials and the Euler–Rodrigues parameters. He is also credited as originating the idea of the artist as an avant-garde. \n\n\n\n\n"}
{"id": "161565", "url": "https://en.wikipedia.org/wiki?curid=161565", "title": "Olry Terquem", "text": "Olry Terquem\n\nOlry Terquem (16 June 1782 – 6 May 1862) was a French mathematician. He is known for his works in geometry and for founding two scientific journals, one of which was the first journal about the history of mathematics. He was also the pseudonymous author (as Tsarphati) of a sequence of letters advocating radical reform in Judaism. He was French Jewish.\n\nTerquem grew up speaking Yiddish, and studying only the Hebrew language and the Talmud. However, after the French revolution his family came into contact with a wider society, and his studies broadened. Despite his poor French he was admitted to study mathematics at the École Polytechnique in Paris, beginning in 1801, as only the second Jew to study there. He became an assistant there in 1803, and earned his doctorate in 1804.\n\nAfter finishing his studies he moved to Mainz (at that time known as Mayence and part of imperial France), where he taught at the Imperial Lycée. In 1811 he moved to the artillery school in the same city, in 1814 he moved again to the artillery school in Grenoble, and in 1815 he became the librarian of the Dépôt Central de l'Artillerie in Paris, where he remained for the rest of his life. He became an officer of the Legion of Honor in 1852. After he died, his funeral was officiated by Lazare Isidor, the Chief Rabbi of Paris and later of France, and attended by over 12 generals headed by Edmond Le Bœuf.\n\nTerquem translated works concerning artillery, was the author of several textbooks, and became an expert on the history of mathematics. Terquem and Camille-Christophe Gerono were the founding editors of the \"Nouvelles Annales de Mathématiques\" in 1842. Terquem also founded another journal in 1855, the \"Bulletin de Bibliographie, d'Histoire et de Biographie de Mathématiques\", which was published as a supplement to the \"Nouvelles Annales\", and he continued editing it until 1861. This was the first journal dedicated to the history of mathematics.\nIn geometry, Terquem is known for naming the nine-point circle and fully proving its properties. This is a circle that passes through nine special points of any given triangle. Karl Wilhelm Feuerbach had previously observed that the three feet of the altitudes of a triangle and the three midpoints of its sides all lie on a single circle, but Terquem was the first to prove that this circle also contains the midpoints of the line segments connecting each vertex to the orthocenter of the triangle. He also gave a new proof of Feuerbach's theorem that the nine-point circle is tangent to the incircle and excircles of a triangle.\n\nTerquem's other contributions to mathematics include naming the pedal curve of another curve, and counting the number of perpendicular lines from a point to an algebraic curve as a function of the degree of the curve. He was also the first to observe that the minimum or maximum value of a symmetric function is often obtained by setting all variables equal to each other.\n\nTerquem has been called the first, most radical, and most outspoken of the major proponents of Jewish reform in France, \"the \"enfant terrible\" of French Judaism\". He published 27 \"letters of an Israelite\" under the name \"Tsarphati\" (a Hebrew word for a Frenchman), pushing for reforms that in his view would better assimilate Jews into modern life and better accommodate working-class Jews. The first nine of these appeared in \"L'Israélite Français\", and the remaining 18 as letters to the editor in \"Courrier de la Moselle\". Terquem rejected the Talmud, proposed to codify intermarriage between Jews and non-Jews, pushed to move the sabbath to Sunday, advocated using other languages than Hebrew for prayers, and fought against circumcision, regressive attitudes towards women, and the Jewish calendar. However, he had little effect on the Jewish practices of the time.\n\nDespite Terquem's calls for reform, and despite having married a Catholic woman and raised his children as Catholic, he requested that his funeral be held with all the proper Jewish rites.\n"}
{"id": "7528959", "url": "https://en.wikipedia.org/wiki?curid=7528959", "title": "Peptide computing", "text": "Peptide computing\n\nPeptide computing is a form of computing which uses peptides and molecular biology, instead of traditional silicon-based computer technologies. The basis of this computational model is the affinity of antibodies towards peptide sequences. Similar to DNA computing, the parallel interactions of peptide sequences and antibodies have been used by this model to solve a few NP-complete problems. Specifically, the hamiltonian path problem (HPP) and some versions of the set cover problem are a few NP-complete problems which have been solved using this computational model so far. This model of computation has also been shown to be computationally universal (or Turing complete).\n\nThis model of computation has some critical advantages over DNA computing. For instance, while DNA is made of four building blocks, peptides are made of twenty building blocks. The peptide-antibody interactions are also more flexible with respect to recognition and affinity than an interaction between a DNA strand and its reverse complement. However, unlike DNA computing, this model is yet to be practically realized. The main limitation is the availability of specific monoclonal antibodies required by the model.\n\n"}
{"id": "13511542", "url": "https://en.wikipedia.org/wiki?curid=13511542", "title": "Pseudoforest", "text": "Pseudoforest\n\nIn graph theory, a pseudoforest is an undirected graph in which every connected component has at most one cycle. That is, it is a system of vertices and edges connecting pairs of vertices, such that no two cycles of consecutive edges share any vertex with each other, nor can any two cycles be connected to each other by a path of consecutive edges. A pseudotree is a connected pseudoforest.\n\nThe names are justified by analogy to the more commonly studied trees and forests. (A tree is a connected graph with no cycles; a forest is a disjoint union of trees.) Gabow and Tarjan attribute the study of pseudoforests to Dantzig's 1963 book on linear programming, in which pseudoforests arise in the solution of certain network flow problems. Pseudoforests also form graph-theoretic models of functions and occur in several algorithmic problems. Pseudoforests are sparse graphs – they have very few edges relative to their number of vertices – and their matroid structure allows several other families of sparse graphs to be decomposed as unions of forests and pseudoforests. The name \"pseudoforest\" comes from .\n\nWe define an undirected graph to be a set of vertices and edges such that each edge has two vertices (which may coincide) as endpoints. That is, we allow multiple edges (edges with the same pair of endpoints) and loops (edges whose two endpoints are the same vertex). A subgraph of a graph is the graph formed by any subsets of its vertices and edges such that each edge in the edge subset has both endpoints in the vertex subset.\nA connected component of an undirected graph is the subgraph consisting of the vertices and edges that can be reached by following edges from a single given starting vertex. A graph is connected if every vertex or edge is reachable from every other vertex or edge. A cycle in an undirected graph is a connected subgraph in which each vertex is incident to exactly two edges, or is a loop.\nA pseudoforest is an undirected graph in which each connected component contains at most one cycle. Equivalently, it is an undirected graph in which each connected component has no more edges than vertices. The components that have no cycles are just trees, while the components that have a single cycle within them are called 1-trees or unicyclic graphs. That is, a 1-tree is a connected graph containing exactly one cycle. A pseudoforest with a single connected component (usually called a pseudotree, although some authors define a pseudotree to be a 1-tree) is either a tree or a 1-tree; in general a pseudoforest may have multiple connected components as long as all of them are trees or 1-trees.\n\nIf one removes from a 1-tree one of the edges in its cycle, the result is a tree. Reversing this process, if one augments a tree by connecting any two of its vertices by a new edge, the result is a 1-tree; the path in the tree connecting the two endpoints of the added edge, together with the added edge itself, form the 1-tree's unique cycle. If one augments a 1-tree by adding an edge that connects one of its vertices to a newly added vertex, the result is again a 1-tree, with one more vertex; an alternative method for constructing 1-trees is to start with a single cycle and then repeat this augmentation operation any number of times. The edges of any 1-tree can be partitioned in a unique way into two subgraphs, one of which is a cycle and the other of which is a forest, such that each tree of the forest contains exactly one vertex of the cycle.\n\nCertain more specific types of pseudoforests have also been studied.\n\nVersions of these definitions are also used for directed graphs. Like an undirected graph, a directed graph consists of vertices and edges, but each edge is directed from one of its endpoints to the other endpoint. A directed pseudoforest is a directed graph in which each vertex has at most one outgoing edge; that is, it has outdegree at most one. A directed 1-forest – most commonly called a functional graph (see below), sometimes maximal directed pseudoforest – is a directed graph in which each vertex has outdegree exactly one. If \"D\" is a directed pseudoforest, the undirected graph formed by removing the direction from each edge of \"D\" is an undirected pseudoforest.\n\nEvery pseudoforest on a set of \"n\" vertices has at most \"n\" edges, and every maximal pseudoforest on a set of \"n\" vertices has exactly \"n\" edges. Conversely, if a graph \"G\" has the property that, for every subset \"S\" of its vertices, the number of edges in the induced subgraph of \"S\" is at most the number of vertices in \"S\", then \"G\" is a pseudoforest. 1-trees can be defined as connected graphs with equally many vertices and edges.\n\nMoving from individual graphs to graph families, if a family of graphs has the property that every subgraph of a graph in the family is also in the family, and every graph in the family has at most as many edges as vertices, then the family contains only pseudoforests. For instance, every subgraph of a thrackle (a graph drawn so that every pair of edges has one point of intersection) is also a thrackle, so Conway's conjecture that every thrackle has at most as many edges as vertices can be restated as saying that every thrackle is a pseudoforest. A more precise characterization is that, if the conjecture is true, then the thrackles are exactly the pseudoforests with no four-vertex cycle and at most one odd cycle.\n\nStreinu and Theran generalize the sparsity conditions defining pseudoforests: they define a graph as being (\"k\",\"l\")-sparse if every nonempty subgraph with \"n\" vertices has at most \"kn\" − \"l\" edges, and (\"k\",\"l\")-tight if it is (\"k\",\"l\")-sparse and has exactly \"kn\" − \"l\" edges. Thus, the pseudoforests are the (1,0)-sparse graphs, and the maximal pseudoforests are the (1,0)-tight graphs. Several other important families of graphs may be defined from other values of \"k\" and \"l\",\nand when \"l\" ≤ \"k\" the (\"k\",\"l\")-sparse graphs may be characterized as the graphs formed as the edge-disjoint union of \"l\" forests and \"k\" − \"l\" pseudoforests.\n\nAlmost every sufficiently sparse random graph is pseudoforest. That is, if \"c\" is a constant with 0 < \"c\" < 1/2, and P(\"n\") is the probability that choosing uniformly at random among the \"n\"-vertex graphs with \"cn\" edges results in a pseudoforest, then P(\"n\") tends to one in the limit for large \"n\". However, for \"c\" > 1/2, almost every random graph with \"cn\" edges has a large component that is not unicyclic.\n\nA graph is \"simple\" if it has no self-loops and no multiple edges with the same endpoints. The number of simple 1-trees with \"n\" labelled vertices is\nThe values for \"n\" up to 300 can be found in sequence of the On-Line Encyclopedia of Integer Sequences.\n\nThe number of maximal directed pseudoforests on \"n\" vertices, allowing self-loops, is \"n\", because for each vertex there are \"n\" possible endpoints for the outgoing edge. André Joyal used this fact to provide a bijective proof of Cayley's formula, that the number of undirected trees on \"n\" nodes is \"n\", by finding a bijection between maximal directed pseudoforests and undirected trees with two distinguished nodes. If self-loops are not allowed, the number of maximal directed pseudoforests is instead (\"n\" − 1).\n\nDirected pseudoforests and endofunctions are in some sense mathematically equivalent. Any function ƒ from a set \"X\" to itself (that is, an endomorphism of \"X\") can be interpreted as defining a directed pseudoforest which has an edge from \"x\" to \"y\" whenever ƒ(\"x\") = \"y\". The resulting directed pseudoforest is maximal, and may include self-loops whenever some value \"x\" has ƒ(\"x\") = \"x\". Alternatively, omitting the self-loops produces a non-maximal pseudoforest. In the other direction, any maximal directed pseudoforest determines a function ƒ such that ƒ(\"x\") is the target of the edge that goes out from \"x\", and any non-maximal directed pseudoforest can be made maximal by adding self-loops and then converted into a function in the same way. For this reason, maximal directed pseudoforests are sometimes called functional graphs. Viewing a function as a functional graph provides a convenient language for describing properties that are not as easily described from the function-theoretic point of view; this technique is especially applicable to problems involving iterated functions, which correspond to paths in functional graphs.\n\nCycle detection, the problem of following a path in a functional graph to find a cycle in it, has applications in cryptography and computational number theory, as part of Pollard's rho algorithm for integer factorization and as a method for finding collisions in cryptographic hash functions. In these applications, ƒ is expected to behave randomly; Flajolet and Odlyzko study the graph-theoretic properties of the functional graphs arising from randomly chosen mappings. In particular, a form of the birthday paradox implies that, in a random functional graph with \"n\" vertices, the path starting from a randomly selected vertex will typically loop back on itself to form a cycle within O() steps. Konyagin et al. have made analytical and computational progress on graph statistics.\n\nMartin, Odlyzko, and Wolfram investigate pseudoforests that model the dynamics of cellular automata. These functional graphs, which they call \"state transition diagrams\", have one vertex for each possible configuration that the ensemble of cells of the automaton can be in, and an edge connecting each configuration to the configuration that follows it according to the automaton's rule. One can infer properties of the automaton from the structure of these diagrams, such as the number of components, length of limiting cycles, depth of the trees connecting non-limiting states to these cycles, or symmetries of the diagram. For instance, any vertex with no incoming edge corresponds to a Garden of Eden pattern and a vertex with a self-loop corresponds to a still life pattern.\n\nAnother early application of functional graphs is in the \"trains\" used to study Steiner triple systems. The train of a triple system is a functional graph having a vertex for each possible triple of symbols; each triple \"pqr\" is mapped by ƒ to \"stu\", where \"pqs\", \"prt\", and \"qru\" are the triples that belong to the triple system and contain the pairs \"pq\", \"pr\", and \"qr\" respectively. Trains have been shown to be a powerful invariant of triple systems although somewhat cumbersome to compute.\n\nA matroid is a mathematical structure in which certain sets of elements are defined to be independent, in such a way that the independent sets satisfy properties modeled after the properties of linear independence in a vector space. One of the standard examples of a matroid is the graphic matroid in which the independent sets are the sets of edges in forests of a graph; the matroid structure of forests is important in algorithms for computing the minimum spanning tree of the graph. Analogously, we may define matroids from pseudoforests.\n\nFor any graph \"G\" = (\"V\",\"E\"), we may define a matroid on the edges of \"G\", in which a set of edges is independent if and only if it forms a pseudoforest; this matroid is known as the bicircular matroid (or bicycle matroid) of \"G\". The smallest dependent sets for this matroid are the minimal connected subgraphs of \"G\" that have more than one cycle, and these subgraphs are sometimes called bicycles. There are three possible types of bicycle: a theta graph has two vertices that are connected by three internally disjoint paths, a figure 8 graph consists of two cycles sharing a single vertex, and a handcuff graph is formed by two disjoint cycles connected by a path.\nA graph is a pseudoforest if and only if it does not contain a bicycle as a subgraph.\n\nForming a minor of a pseudoforest by contracting some of its edges and deleting others produces another pseudoforest. Therefore, the family of pseudoforests is closed under minors, and the Robertson–Seymour theorem implies that pseudoforests can be characterized in terms of a finite set of forbidden minors, analogously to Wagner's theorem characterizing the planar graphs as the graphs having neither the complete graph K nor the complete bipartite graph K as minors.\nAs discussed above, any non-pseudoforest graph contains as a subgraph a handcuff, figure 8, or theta graph; any handcuff or figure 8 graph may be contracted to form a \"butterfly graph\" (five-vertex figure 8), and any theta graph may be contracted to form a \"diamond graph\" (four-vertex theta graph), so any non-pseudoforest contains either a butterfly or a diamond as a minor, and these are the only minor-minimal non-pseudoforest graphs. Thus, a graph is a pseudoforest if and only if it does not have the butterfly or the diamond as a minor. If one forbids only the diamond but not the butterfly, the resulting larger graph family consists of the cactus graphs and disjoint unions of multiple cactus graphs.\n\nMore simply, if multigraphs with self-loops are considered, there is only one forbidden minor, a vertex with two loops.\n\nAn early algorithmic use of pseudoforests involves the \"network simplex\" algorithm and its application to generalized flow problems modeling the conversion between commodities of different types. In these problems, one is given as input a flow network in which the vertices model each commodity and the edges model allowable conversions between one commodity and another. Each edge is marked with a \"capacity\" (how much of a commodity can be converted per unit time), a \"flow multiplier\" (the conversion rate between commodities), and a \"cost\" (how much loss or, if negative, profit is incurred per unit of conversion). The task is to determine how much of each commodity to convert via each edge of the flow network, in order to minimize cost or maximize profit, while obeying the capacity constraints and not allowing commodities of any type to accumulate unused. This type of problem can be formulated as a linear program, and solved using the simplex algorithm. The intermediate solutions arising from this algorithm, as well as the eventual optimal solution, have a special structure: each edge in the input network is either unused or used to its full capacity, except for a subset of the edges, forming a spanning pseudoforest of the input network, for which the flow amounts may lie between zero and the full capacity. In this application, unicyclic graphs are also sometimes called \"augmented trees\" and maximal pseudoforests are also sometimes called \"augmented forests\".\n\nThe \"minimum spanning pseudoforest problem\" involves finding a spanning pseudoforest of minimum weight in a larger edge-weighted graph \"G\".\nDue to the matroid structure of pseudoforests, minimum-weight maximal pseudoforests may be found by greedy algorithms similar to those for the minimum spanning tree problem. However, Gabow and Tarjan found a more efficient linear-time approach in this case.\n\nThe pseudoarboricity of a graph \"G\" is defined by analogy to the arboricity as the minimum number of pseudoforests into which its edges can be partitioned; equivalently, it is the minimum \"k\" such that \"G\" is (\"k\",0)-sparse, or the minimum \"k\" such that the edges of \"G\" can be oriented to form a directed graph with outdegree at most \"k\". Due to the matroid structure of pseudoforests, the pseudoarboricity may be computed in polynomial time.\n\nA random bipartite graph with \"n\" vertices on each side of its bipartition, and with \"cn\" edges chosen independently at random from each of the \"n\" possible pairs of vertices, is a pseudoforest with high probability whenever \"c\" is a constant strictly less than one. This fact plays a key role in the analysis of cuckoo hashing, a data structure for looking up key-value pairs by looking in one of two hash tables at locations determined from the key: one can form a graph, the \"cuckoo graph\", whose vertices correspond to hash table locations and whose edges link the two locations at which one of the keys might be found, and the cuckoo hashing algorithm succeeds in finding locations for all of its keys if and only if the cuckoo graph is a pseudoforest.\n\nPseudoforests also play a key role in parallel algorithms for graph coloring and related problems.\n"}
{"id": "632489", "url": "https://en.wikipedia.org/wiki?curid=632489", "title": "Quantum algorithm", "text": "Quantum algorithm\n\nIn quantum computing, a quantum algorithm is an algorithm which runs on a realistic model of quantum computation, the most commonly used model being the quantum circuit model of computation. A classical (or non-quantum) algorithm is a finite sequence of instructions, or a step-by-step procedure for solving a problem, where each step or instruction can be performed on a classical computer. Similarly, a quantum algorithm is a step-by-step procedure, where each of the steps can be performed on a quantum computer. Although all classical algorithms can also be performed on a quantum computer, the term quantum algorithm is usually used for those algorithms which seem inherently quantum, or use some essential feature of quantum computation such as quantum superposition or quantum entanglement.\n\nProblems which are undecidable using classical computers remain undecidable using quantum computers. What makes quantum algorithms interesting is that they might be able to solve some problems faster than classical algorithms.\n\nThe most well known algorithms are Shor's algorithm for factoring, and Grover's algorithm for searching an unstructured database or an unordered list. Shor's algorithms runs exponentially faster than the best known classical algorithm for factoring, the general number field sieve. Grover's algorithm runs quadratically faster than the best possible classical algorithm for the same task, a linear search.\n\nQuantum algorithms are usually described, in the commonly used circuit model of quantum computation, by a quantum circuit which acts on some input qubits and terminates with a measurement. A quantum circuit consists of simple quantum gates which act on at most a fixed number of qubits, usually two or three. Quantum algorithms may also be stated in other models of quantum computation, such as the Hamiltonian oracle model.\n\nQuantum algorithms can be categorized by the main techniques used by the algorithm. Some commonly used techniques/ideas in quantum algorithms include phase kick-back, phase estimation, the quantum Fourier transform, quantum walks, amplitude amplification and topological quantum field theory. Quantum algorithms may also be grouped by the type of problem solved, for instance see the survey on quantum algorithms for algebraic problems.\n\nThe quantum Fourier transform is the quantum analogue of the discrete Fourier transform, and is used in several quantum algorithms. The Hadamard transform is also an example of a quantum Fourier transform over an n-dimensional vector space over the field F. The quantum Fourier transform can be efficiently implemented on a quantum computer using only a polynomial number of quantum gates.\n\nThe Deutsch–Jozsa algorithm solves a black-box problem which probably requires exponentially many queries to the black box for any deterministic classical computer, but can be done with exactly one query by a quantum computer. If we allow both bounded-error quantum and classical algorithms, then there is no speedup since a classical probabilistic algorithm can solve the problem with a constant number of queries with small probability of error. The algorithm determines whether a function \"f\" is either constant (0 on all inputs or 1 on all inputs) or balanced (returns 1 for half of the input domain and 0 for the other half).\n\nSimon's algorithm solves a black-box problem exponentially faster than any classical algorithm, including bounded-error probabilistic algorithms. This algorithm, which achieves an exponential speedup over all classical algorithms that we consider efficient, was the motivation for Shor's factoring algorithm.\n\nThe quantum phase estimation algorithm is used to determine the eigenphase of an eigenvector of a unitary gate given a quantum state proportional to the eigenvector and access to the gate. The algorithm is frequently used as a subroutine in other algorithms.\n\nShor's algorithm solves the discrete logarithm problem and the integer factorization problem in polynomial time, whereas the best known classical algorithms take super-polynomial time. These problems are not known to be in P or NP-complete. It is also one of the few quantum algorithms that solves a non–black-box problem in polynomial time where the best known classical algorithms run in super-polynomial time.\n\nThe abelian hidden subgroup problem is a generalization of many problems that can be solved by a quantum computer, such as Simon's problem, solving Pell's equation, testing the principal ideal of a ring R and factoring. There are efficient quantum algorithms known for the Abelian hidden subgroup problem. The more general hidden subgroup problem, where the group isn't necessarily abelian, is a generalization of the previously mentioned problems and graph isomorphism and certain lattice problems. Efficient quantum algorithms are known for certain non-abelian groups. However, no efficient algorithms are known for the symmetric group, which would give an efficient algorithm for graph isomorphism and the dihedral group, which would solve certain lattice problems.\n\nThe Boson Sampling Problem in an experimental configuration assumes an input of bosons (ex. photons of light) of moderate number getting randomly scattered into a large number of output modes constrained by a defined unitarity. The problem is then to produce a fair sample of the probability distribution of the output which is dependent on the input arrangement of bosons and the Unitarity. Solving this problem with a classical computer algorithm requires computing the permanent of the unitary transform matrix, which may be either impossible or take a prohibitively long time. In 2014, it was proposed that existing technology and standard probabilistic methods of generating single photon states could be used as input into a suitable quantum computable linear optical network and that sampling of the output probability distribution would be demonstrably superior using quantum algorithms. In 2015, investigation predicted the sampling problem had similar complexity for inputs other than Fock state photons and identified a transition in computational complexity from classically simulatable to just as hard as the Boson Sampling Problem, dependent on the size of coherent amplitude inputs.\n\nA Gauss sum is a type of exponential sum. The best known classical algorithm for estimating these sums takes exponential time. Since the discrete logarithm problem reduces to Gauss sum estimation, an efficient classical algorithm for estimating Gauss sums would imply an efficient classical algorithm for computing discrete logarithms, which is considered unlikely. However, quantum computers can estimate Gauss sums to polynomial precision in polynomial time.\n\nWe have an oracle consisting of n random Boolean functions mapping n-bit strings to a Boolean value. We are required to find n n-bit strings z..., z such that for the Hadamard-Fourier transform, at least 3/4 of the strings satisfy\n\nand at least 1/4 satisfies\n\nThis can be done in BQP.\n\nAmplitude amplification is a technique that allows the amplification of a chosen subspace of a quantum state. Applications of amplitude amplification usually lead to quadratic speedups over the corresponding classical algorithms. It can be considered to be a generalization of Grover's algorithm.\n\nGrover's algorithm searches an unstructured database (or an unordered list) with N entries, for a marked entry, using only formula_3 queries instead of the formula_4 queries required classically. Classically, formula_4 queries are required, even if we allow bounded-error probabilistic algorithms.\n\nBohmian Mechanics is a non-local hidden variable interpretation of quantum mechanics. It has been shown that a non-local hidden variable quantum computer could implement a search of an N-item database at most in formula_6 steps. This is slightly faster than the formula_3 steps taken by Grover's algorithm. Neither search method will allow quantum computers to solve NP-Complete problems in polynomial time.\n\nQuantum counting solves a generalization of the search problem. It solves the problem of counting the number of marked entries in an unordered list, instead of just detecting if one exists. Specifically, it counts the number of marked entries in an formula_8-element list, with error formula_9 making only formula_10 queries, where formula_11 is the number of marked elements in the list. More precisely, the algorithm outputs an estimate formula_12 for formula_11, the number of marked entries, with the following accuracy: formula_14.\n\nA quantum walk is the quantum analogue of a classical random walk, which can be described by a probability distribution over some states. A quantum walk can be described by a quantum superposition over states. Quantum walks are known to give exponential speedups for some black-box problems. They also provide polynomial speedups for many problems. A framework for the creation of quantum walk algorithms exists and is quite a versatile tool.\n\nThe element distinctness problem is the problem of determining whether all the elements of a list are distinct. Classically, Ω(\"N\") queries are required for a list of size \"N\", since this problem is harder than the search problem which requires Ω(\"N\") queries. However, it can be solved in formula_15 queries on a quantum computer. The optimal algorithm is by Andris Ambainis. Yaoyun Shi first proved a tight lower bound when the size of the range is sufficiently large. Ambainis and Kutin independently (and via different proofs) extended his work to obtain the lower bound for all functions.\n\nThe triangle-finding problem is the problem of determining whether a given graph contains a triangle (a clique of size 3). The best-known lower bound for quantum algorithms is Ω(\"N\"), but the best algorithm known requires O(\"N\") queries, an improvement over the previous best O(\"N\") queries.\n\nA formula is a tree with a gate at each internal node and an input bit at each leaf node. The problem is to evaluate the formula, which is the output of the root node, given oracle access to the input.\n\nA well studied formula is the balanced binary tree with only NAND gates. This type of formula requires Θ(\"N\") queries using randomness, where formula_16. With a quantum algorithm however, it can be solved in Θ(\"N\") queries. No better quantum algorithm for this case was known until one was found for the unconventional Hamiltonian oracle model. The same result for the standard setting soon followed.\n\nFast quantum algorithms for more complicated formulas are also known.\n\nThe problem is to determine if a black box group, given by \"k\" generators, is commutative. A black box group is a group with an oracle function, which must be used to perform the group operations (multiplication, inversion, and comparison with identity). We are interested in the query complexity, which is the number of oracle calls needed to solve the problem. The deterministic and randomized query complexities are formula_17 and formula_18 respectively. A quantum algorithm requires formula_19 queries but the best known algorithm uses formula_20 queries.\n\nWitten had shown that the Chern-Simons topological quantum field theory (TQFT) can be solved in terms of Jones polynomials. A quantum computer can simulate a TQFT, and thereby approximate the Jones polynomial, which as far as we know, is hard to compute classically in the worst-case scenario.\n\nThe idea that quantum computers might be more powerful than classical computers originated in Richard Feynman's observation that classical computers seem to require exponential time to simulate many-particle quantum systems. Since then, the idea that quantum computers can simulate quantum physical processes exponentially faster than classical computers has been greatly fleshed out and elaborated. Efficient (that is, polynomial-time) quantum algorithms have been developed for simulating both Bosonic and Fermionic systems and in particular, the simulation of chemical reactions beyond the capabilities of current classical supercomputers requires only a few hundred qubits. Quantum computers can also efficiently simulate topological quantum field theories. In addition to its intrinsic interest, this result has led to efficient quantum algorithms for estimating quantum topological invariants such as Jones and HOMFLY polynomials, and the Turaev-Viro invariant of three-dimensional manifolds.\n\nHybrid Quantum/Classical Algorithms combine quantum state preparation and measurement with classical optimization. These algorithms generally aim to determine the ground state eigenvector and eigenvalue of a Hermitian Operator.\n\nThe quantum approximate optimization algorithm is a toy model of quantum annealing which can be used to solve problems in graph theory. The algorithm makes use of classical optimization of quantum operations to maximize an objective function.\n\nThe VQE algorithm applies classical optimization to minimize the energy expectation of an ansatz state to find the ground state energy of a molecule. This can also be extended to find excited energies of molecules.\n\n\n\n"}
{"id": "45600", "url": "https://en.wikipedia.org/wiki?curid=45600", "title": "Reed–Solomon error correction", "text": "Reed–Solomon error correction\n\nReed–Solomon codes are a group of error-correcting codes that were introduced by Irving S. Reed and Gustave Solomon in 1960.\nThey have many applications, the most prominent of which include consumer technologies such as CDs, DVDs, Blu-ray Discs, QR Codes, data transmission technologies such as DSL and WiMAX, broadcast systems such as satellite communications, DVB and ATSC, and storage systems such as RAID 6.\n\nReed–Solomon codes operate on a block of data treated as a set of finite field elements called symbols. For example, a block of 4096 bytes (32,768 bits) could be treated as a set of 2731 12-bit symbols, where each symbol is a finite field element of GF(2), the last symbol padded with four 0 bits. Reed–Solomon codes are able to detect and correct multiple symbol errors. By adding check symbols to the data, a Reed–Solomon code can detect any combination of up to and including erroneous symbols, or correct up to and including symbols. As an erasure code, it can correct up to and including known erasures, or it can detect and correct combinations of errors and erasures. Reed–Solomon codes are also suitable as multiple-burst bit-error correcting codes, since a sequence of consecutive bit errors can affect at most two symbols of size . The choice of is up to the designer of the code, and may be selected within wide limits.\n\nThere are two basic types of Reed–Solomon codes, and , with BCH view being the most common as BCH view decoders are faster and require less working storage than original view decoders.\n\nReed–Solomon codes were developed in 1960 by Irving S. Reed and Gustave Solomon, who were then staff members of MIT Lincoln Laboratory. Their seminal article was titled \"Polynomial Codes over Certain Finite Fields\". . The original encoding scheme described in the Reed & Solomon article used a variable polynomial based on the message to be encoded where only a fixed set of values (evaluation points) to be encoded are known to encoder and decoder. The original theoretical decoder generated potential polynomials based on subsets of \"k\" (unencoded message length) out of \"n\" (encoded message length) values of a received message, choosing the most popular polynomial as the correct one, which was impractical for all but the simplest of cases. This was initially resolved by changing the original scheme to a BCH code like scheme based on a fixed polynomial known to both encoder and decoder, but later, practical decoders based on the original scheme were developed, although slower than the BCH schemes. The result of this is that there are two main types of Reed Solomon codes, ones that use the original encoding scheme, and ones that use the BCH encoding scheme.\n\nAlso in 1960, a practical fixed polynomial decoder for BCH codes codes developed by Daniel Gorenstein and Neal Zierler was described in an MIT Lincoln Laboratory report by Zierler in January 1960 and later in a paper in June 1961. The Gorenstein-Zierler decoder and the related work on BCH codes are described in a book Error Correcting Codes by W. Wesley Peterson (1961). By 1963 (or possibly earlier), J. J. Stone (and others) recognized that Reed Solomon codes could use the BCH scheme of using a fixed generator polynomial, making such codes a special class of BCH codes, but Reed Solomon codes based on the original encoding scheme, are not a class of BCH codes, and depending on the set of evaluation points, they are not even cyclic codes.\n\nIn 1969, an improved BCH scheme decoder was developed by Elwyn Berlekamp and James Massey, and is since known as the Berlekamp–Massey decoding algorithm.\n\nIn 1975, another improved BCH scheme decoder was developed by Yasuo Sugiyama, based on the extended Euclidean algorithm.\n\nIn 1977, Reed–Solomon codes were implemented in the Voyager program in the form of concatenated error correction codes. The first commercial application in mass-produced consumer products appeared in 1982 with the compact disc, where two interleaved Reed–Solomon codes are used. Today, Reed–Solomon codes are widely implemented in digital storage devices and digital communication standards, though they are being slowly replaced by more modern low-density parity-check (LDPC) codes or turbo codes. For example, Reed–Solomon codes are used in the Digital Video Broadcasting (DVB) standard DVB-S, but LDPC codes are used in its successor, DVB-S2.\n\nIn 1986, an original scheme decoder known as the Berlekamp–Welch algorithm was developed.\n\nIn 1996, variations of original scheme decoders called list decoders or soft decoders were developed by Madhu Sudan and others, and work continues on these type of decoders – see \"Guruswami–Sudan list decoding algorithm\".\n\nIn 2002, another original scheme decoder was developed by Shuhong Gao, based on the Extended Euclidean algorithm Gao_RS.pdf .\n\nReed–Solomon coding is very widely used in mass storage systems to correct\nthe burst errors associated with media defects.\n\nReed–Solomon coding is a key component of the compact disc. It was the first use of strong error correction coding in a mass-produced consumer product, and DAT and DVD use similar schemes. In the CD, two layers of Reed–Solomon coding separated by a 28-way convolutional interleaver yields a scheme called Cross-Interleaved Reed–Solomon Coding (CIRC). The first element of a CIRC decoder is a relatively weak inner (32,28) Reed–Solomon code, shortened from a (255,251) code with 8-bit symbols. This code can correct up to 2 byte errors per 32-byte block. More importantly, it flags as erasures any uncorrectable blocks, i.e., blocks with more than 2 byte errors. The decoded 28-byte blocks, with erasure indications, are then spread by the deinterleaver to different blocks of the (28,24) outer code. Thanks to the deinterleaving, an erased 28-byte block from the inner code becomes a single erased byte in each of 28 outer code blocks. The outer code easily corrects this, since it can handle up to 4 such erasures per block.\n\nThe result is a CIRC that can completely correct error bursts up to 4000 bits, or about 2.5 mm on the disc surface. This code is so strong that most CD playback errors are almost certainly caused by tracking errors that cause the laser to jump track, not by uncorrectable error bursts.\n\nDVDs use a similar scheme, but with much larger blocks, a (208,192) inner code, and a (182,172) outer code.\n\nReed–Solomon error correction is also used in parchive files which are commonly posted accompanying multimedia files on USENET. The Distributed online storage service Wuala (discontinued in 2015) also used to make use of Reed–Solomon when breaking up files.\n\nAlmost all two-dimensional bar codes such as PDF-417, MaxiCode, Datamatrix, QR Code, and Aztec Code use Reed–Solomon error correction to allow correct reading even if a portion of the bar code is damaged. When the bar code scanner cannot recognize a bar code symbol, it will treat it as an erasure.\n\nReed–Solomon coding is less common in one-dimensional bar codes, but is used by the PostBar symbology.\n\nSpecialized forms of Reed–Solomon codes, specifically Cauchy-RS and Vandermonde-RS, can be used to overcome the unreliable nature of data transmission over erasure channels. The encoding process assumes a code of RS(\"N\", \"K\") which results in \"N\" codewords of length \"N\" symbols each storing \"K\" symbols of data, being generated, that are then sent over an erasure channel.\n\nAny combination of \"K\" codewords received at the other end is enough to reconstruct all of the \"N\" codewords. The code rate is generally set to 1/2 unless the channel's erasure likelihood can be adequately modelled and is seen to be less. In conclusion, \"N\" is usually 2\"K\", meaning that at least half of all the codewords sent must be received in order to reconstruct all of the codewords sent.\n\nReed–Solomon codes are also used in xDSL systems and CCSDS's Space Communications Protocol Specifications as a form of forward error correction.\n\nOne significant application of Reed–Solomon coding was to encode the digital pictures sent back by the Voyager space probe.\n\nVoyager introduced Reed–Solomon coding concatenated with convolutional codes, a practice that has since become very widespread in deep space and satellite (e.g., direct digital broadcasting) communications.\nViterbi decoders tend to produce errors in short bursts. Correcting these burst errors is a job best done by short or simplified Reed–Solomon codes.\n\nModern versions of concatenated Reed–Solomon/Viterbi-decoded convolutional coding were and are used on the Mars Pathfinder, Galileo, Mars Exploration Rover and Cassini missions, where they perform within about 1–1.5 dB of the ultimate limit, being the Shannon capacity.\n\nThese concatenated codes are now being replaced by more powerful turbo codes.\n\nThe Reed–Solomon code is actually a family of codes, where every code is characterised by three parameters: an alphabet size \"q\", a block length \"n\", and a message length \"k,\" with \"k < n ≤ q.\" The set of alphabet symbols is interpreted as the finite field of order \"q\", and thus, \"q\" has to be a prime power. In the most useful parameterizations of the Reed–Solomon code, the block length is usually some constant multiple of the message length, that is, the rate is some constant, and furthermore, the block length is equal to or one less than the alphabet size, that is, or .\n\nThere are different encoding procedures for the Reed–Solomon code, and thus, there are different ways to describe the set of all codewords.\nIn the original view of , every codeword of the Reed–Solomon code is a sequence of function values of a polynomial of degree less than \"k\". In order to obtain a codeword of the Reed–Solomon code, the message is interpreted as the description of a polynomial \"p\" of degree less than \"k\" over the finite field \"F\" with \"q\" elements.\nIn turn, the polynomial \"p\" is evaluated at \"n\" ≤ \"q\" distinct points formula_1 of the field \"F\", and the sequence of values is the corresponding codeword. Common choices for a set of evaluation points include {0, 1, 2, ..., \"n\" − 1}, {0, \"α\", \"α\", ..., \"α\", 1}, {1, \"α\", \"α\", ..., \"α\"}, ... , where \"α\" is a primitive element of \"F\". \n\nFormally, the set formula_2 of codewords of the Reed–Solomon code is defined as follows:\nSince any two \"distinct\" polynomials of degree less than formula_4 agree in at most formula_5 points, this means that any two codewords of the Reed–Solomon code disagree in at least formula_6 positions.\nFurthermore, there are two polynomials that do agree in formula_7 points but are not equal, and thus, the distance of the Reed–Solomon code is exactly formula_8.\nThen the relative distance is formula_9, where formula_10 is the rate.\nThis trade-off between the relative distance and the rate is asymptotically optimal since, by the Singleton bound, \"every\" code satisfies formula_11.\nBeing a code that achieves this optimal trade-off, the Reed–Solomon code belongs to the class of maximum distance separable codes.\n\nWhile the number of different polynomials of degree less than \"k\" and the number of different messages are both equal to formula_12, and thus every message can be uniquely mapped to such a polynomial, there are different ways of doing this encoding.\nThe original construction of interprets the message \"x\" as the \"coefficients\" of the polynomial \"p\", whereas subsequent constructions interpret the message as the \"values\" of the polynomial at the first \"k\" points formula_13 and obtain the polynomial \"p\" by interpolating these values with a polynomial of degree less than \"k\".\nThe latter encoding procedure, while being slightly less efficient, has the advantage that it gives rise to a systematic code, that is, the original message is always contained as a subsequence of the codeword.\n\nIn the original construction of , the message formula_14 is mapped to the polynomial formula_15 with\nThe codeword of formula_17 is obtained by evaluating formula_15 at formula_19 different points formula_1 of the field formula_21.\nThus the classical encoding function formula_22 for the Reed–Solomon code is defined as follows: \nThis function formula_24 is a linear mapping, that is, it satisfies formula_25 for the following formula_26-matrix formula_27 with elements from formula_21:\n\nThis matrix is the transpose of a Vandermonde matrix over formula_21.\nIn other words, the Reed–Solomon code is a linear code, and in the classical encoding procedure, its generator matrix is formula_27.\n\nThere is an alternative encoding procedure that also produces the Reed–Solomon code, but that does so in a systematic way. Here, the mapping from the message formula_17 to the polynomial formula_15 works differently: the polynomial formula_15 is now defined as the unique polynomial of degree less than formula_4 such that\nTo compute this polynomial formula_15 from formula_17, one can use Lagrange interpolation.\nOnce it has been found, it is evaluated at the other points formula_40 of the field.\nThe alternative encoding function formula_22 for the Reed–Solomon code is then again just the sequence of values: \nSince the first formula_4 entries of each codeword formula_44 coincide with formula_17, this encoding procedure is indeed systematic.\nSince Lagrange interpolation is a linear transformation, formula_46 is a linear mapping. In fact, we have formula_47, where\n\nA discrete Fourier transform is essentially the same as the encoding procedure; it uses the generator polynomial \"p\"(x) to map a set of evaluation points into the message values as shown above:\n\nThe inverse Fourier transform could be used to convert an error free set of \"n\" < \"q\" message values back into the encoding polynomial of \"k\" coefficients, with the constraint that in order for this to work, the set of evaluation points used to encode the message must be a set of increasing powers of \"α\": \n\nHowever, Lagrange interpolation performs the same conversion without the constraint on the set of evaluation points or the requirement of an error free set of message values and is used for systematic encoding, and in one of the steps of the Gao decoder.\n\nIn this view, the sender again maps the message formula_17 to a polynomial formula_15, and for this, any of the two mappings just described can be used (where the message is either interpreted as the coefficients of formula_15 or as the initial sequence of values of formula_15). Once the sender has constructed the polynomial formula_15 in some way, however, instead of sending the \"values\" of formula_15 at all points, the sender computes some related polynomial formula_58 of degree at most formula_59 for formula_60 and sends the formula_19 \"coefficients\" of that polynomial. The polynomial formula_62 is constructed by multiplying the message polynomial formula_63, which has degree at most formula_5, with a generator polynomial formula_65 of degree formula_66 that is known to both the sender and the receiver. The generator polynomial formula_67 is defined as the polynomial whose roots are exactly formula_68, i.e.,\n\nThe transmitter sends the formula_60 coefficients of formula_71. Thus, in the BCH view of Reed–Solomon codes, the set formula_2 of codewords is defined for formula_60 as follows:\n\nThe encoding procedure for the BCH view of Reed–Solomon codes can be modified to yield a systematic encoding procedure, in which each codeword contains the message as a prefix. Here, instead of sending formula_75, the encoder constructs the transmitted polynomial formula_76 such that the coefficients of the formula_4 largest monomials are equal to the corresponding coefficients of formula_78, and the lower-order coefficients of formula_76 are chosen exactly in such a way that formula_76 becomes divisible by formula_67. Then the coefficients of formula_78 are a subsequence (specifically, a prefix) of the coefficients of formula_76. To get a code that is overall systematic, we construct the message polynomial formula_78 by interpreting the message as the sequence of its coefficients.\n\nFormally, the construction is done by multiplying formula_78 by formula_86 to make room for the formula_87 check symbols, dividing that product by formula_67 to find the remainder, and then compensating for that remainder by subtracting it. The formula_89 check symbols are created by computing the remainder formula_90:\n\nNote that the remainder has degree at most formula_92, whereas the coefficients of formula_93 in the polynomial formula_94 are zero. Therefore, the following definition of the codeword formula_76 has the property that the first formula_4 coefficients are identical to the coefficients of formula_78:\n\nAs a result, the codewords formula_76 are indeed elements of formula_2, that is, they are divisible by the generator polynomial formula_67:\n\nThe Reed–Solomon code is a [\"n\", \"k\", \"n\" − \"k\" + 1] code; in other words, it is a linear block code of length \"n\" (over \"F\") with dimension \"k\" and minimum Hamming distance \"n\" − \"k\" + 1. The Reed–Solomon code is optimal in the sense that the minimum distance has the maximum value possible for a linear code of size (\"n\", \"k\"); this is known as the Singleton bound. Such a code is also called a maximum distance separable (MDS) code.\n\nThe error-correcting ability of a Reed–Solomon code is determined by its minimum distance, or equivalently, by formula_103, the measure of redundancy in the block. If the locations of the error symbols are not known in advance, then a Reed–Solomon code can correct up to formula_104 erroneous symbols, i.e., it can correct half as many errors as there are redundant symbols added to the block. Sometimes error locations are known in advance (e.g., \"side information\" in demodulator signal-to-noise ratios)—these are called erasures. A Reed–Solomon code (like any MDS code) is able to correct twice as many erasures as errors, and any combination of errors and erasures can be corrected as long as the relation 2\"E\" + \"S\" ≤ \"n\" − \"k\" is satisfied, where formula_105 is the number of errors and formula_106 is the number of erasures in the block.\n\nFor practical uses of Reed–Solomon codes, it is common to use a finite field formula_21 with formula_108 elements. In this case, each symbol can be represented as an formula_109-bit value. \nThe sender sends the data points as encoded blocks, and the number of symbols in the encoded block is formula_110. Thus a Reed–Solomon code operating on 8-bit symbols has formula_111 symbols per block. (This is a very popular value because of the prevalence of byte-oriented computer systems.) The number formula_4, with formula_113, of \"data\" symbols in the block is a design parameter. A commonly used code encodes formula_114 eight-bit data symbols plus 32 eight-bit parity symbols in an formula_115-symbol block; this is denoted as a formula_116 code, and is capable of correcting up to 16 symbol errors per block.\n\nThe Reed–Solomon code properties discussed above make them especially well-suited to applications where errors occur in bursts. This is because it does not matter to the code how many bits in a symbol are in error — if multiple bits in a symbol are corrupted it only counts as a single error. Conversely, if a data stream is not characterized by error bursts or drop-outs but by random single bit errors, a Reed–Solomon code is usually a poor choice compared to a binary code.\n\nThe Reed–Solomon code, like the convolutional code, is a transparent code. This means that if the channel symbols have been inverted somewhere along the line, the decoders will still operate. The result will be the inversion of the original data. However, the Reed–Solomon code loses its transparency when the code is shortened. The \"missing\" bits in a shortened code need to be filled by either zeros or ones, depending on whether the data is complemented or not. (To put it another way, if the symbols are inverted, then the zero-fill needs to be inverted to a one-fill.) For this reason it is mandatory that the sense of the data (i.e., true or complemented) be resolved before Reed–Solomon decoding.\n\nWhether the Reed–Solomon code is cyclic or not depends on subtle details of the construction. In the original view of Reed and Solomon, where the codewords are the values of a polynomial, one can choose the sequence of evaluation points in such a way as to make the code cyclic. In particular, if formula_117 is a primitive root of the field formula_21, then by definition all non-zero elements of formula_21 take the form formula_120 for formula_121, where formula_122. Each polynomial formula_123 over formula_21 gives rise to a codeword formula_125. Since the function formula_126 is also a polynomial of the same degree, this function gives rise to a codeword formula_127; since formula_128 holds, this codeword is the cyclic left-shift of the original codeword derived from formula_123. So choosing a sequence of primitive root powers as the evaluation points makes the original view Reed–Solomon code cyclic. Reed–Solomon codes in the BCH view are always cyclic because BCH codes are cyclic.\n\nDesigners are not required to use the \"natural\" sizes of Reed–Solomon code blocks. A technique known as \"shortening\" can produce a smaller code of any desired size from a larger code. For example, the widely used (255,223) code can be converted to a (160,128) code by padding the unused portion of the source block with 95 binary zeroes and not transmitting them. At the decoder, the same portion of the block is loaded locally with binary zeroes. The Delsarte-Goethals-Seidel theorem illustrates an example of an application of shortened Reed–Solomon codes. In parallel to shortening, a technique known as puncturing allows omitting some of the encoded parity symbols.\n\nThe decoders described in this section use the Reed Solomon original view of a codeword as a sequence of polynomial values where the polynomial is based on the message to be encoded. The same set of fixed values are used by the encoder and decoder, and the decoder recovers the encoding polynomial (and optionally an error locating polynomial) from the received message.\n\n described a theoretical decoder that corrected errors by finding the most popular message polynomial. The decoder only knows the set of values formula_130 to formula_131 and which encoding method was used to generate the codeword's sequence of values. The original message, the polynomial, and any errors are unknown. A decoding procedure could use a method like Lagrange interpolation on various subsets of n codeword values taken k at a time to repeatedly produce potential polynomials, until a sufficient number of matching polynomials are produced to reasonably eliminate any errors in the received codeword. Once a polynomial is determined, then any errors in the codeword can be corrected, by recalculating the corresponding codeword values. Unfortunately, in all but the simplest of cases, there are too many subsets, so the algorithm is impractical. The number of subsets is the binomial coefficient, formula_132, and the number of subsets is infeasible for even modest codes. For a formula_133 code that can correct 3 errors, the naive theoretical decoder would examine 359 billion subsets. \n\nIn 1986, a decoder known as the Berlekamp–Welch algorithm was developed as a decoder that is able to recover the original message polynomial as well as an error \"locator\" polynomial that produces zeroes for the input values that correspond to errors, with time complexity O(n^3), where n is the number of values in a message. The recovered polynomial is then used to recover (recalculate as needed) the original message.\n\nUsing RS(7,3), GF(929), and the set of evaluation points \"a\" = i-1\n\nIf the message polynomial is\n\nThe codeword is\n\nErrors in transmission might cause this to be received instead.\n\nThe key equations are:\n\nAssume maximum number of errors: \"e\" = 2. The key equations become:\n\n<br>\n\nUsing Gaussian elimination:\n\nRecalculate where to correct resulting in the corrected codeword:\n\nIn 2002, an improved decoder was developed by Shuhong Gao, based on the extended Euclid algorithm Gao_RS.pdf .\n\nUsing the same data as the Berlekamp Welch example above:\n\ndivide \"Q\"(x) and \"E\"(x) by most significant coeficient of \"E\"(x) = 708. (Optional)\n\nRecalculate where to correct resulting in the corrected codeword:\n\nThe decoders described in this section use the BCH view of a codeword as a sequence of coefficients. They use a fixed generator polynomial known to both encoder and decoder.\n\nDaniel Gorenstein and Neal Zierler developed a decoder that was described in a MIT Lincoln Laboratory report by Zierler in January 1960 and later in a paper in June 1961. The Gorenstein-Zierler decoder and the related work on BCH codes are described in a book Error Correcting Codes by W. Wesley Peterson (1961).\n\nThe transmitted message is viewed as the coefficients of a polynomial \"s\"(\"x\") that is divisible by a generator polynomial \"g\"(\"x\").\n\nwhere \"α\" is a primitive root.\n\nSince \"s\"(\"x\") is divisible by generator \"g\"(\"x\"), it follows that\n\nThe transmitted polynomial is corrupted in transit by an error polynomial \"e\"(\"x\") to produce the received polynomial \"r\"(\"x\").\n\nwhere \"e\" is the coefficient for the \"i\"-th power of \"x\". Coefficient \"e\" will be zero if there is no error at that power of \"x\" and nonzero if there is an error. If there are \"ν\" errors at distinct powers \"i\" of \"x\", then\n\nThe goal of the decoder is to find the number of errors (\"ν\"), the positions of the errors (\"i\"), and the error values at those positions (\"e\"). From those, e(x) can be calculated and subtracted from r(x) to get the originally sent message s(x).\n\nThe decoder starts by evaluating the polynomial as received at certain points. We call the results of that evaluation the \"syndromes\", \"S\". They are defined as:\n\nThe advantage of looking at the syndromes is that the message polynomial drops out. In other words, the syndromes only relate to the error, and are unaffected by the actual contents of the message being transmitted. If the syndromes are all zero, the algorithm stops here and reports that the message was not corrupted in transit.\n\nFor convenience, define the error locators \"X\" and error values \"Y\" as:\n\nThen the syndromes can be written in terms of the error locators and error values as\n\nThis definition of the syndrome values is equivalent to the previous since formula_152.\n\nThe syndromes give a system of \"n\" − \"k\" ≥ 2\"ν\" equations in 2\"ν\" unknowns, but that system of equations is nonlinear in the \"X\" and does not have an obvious solution. However, if the \"X\" were known (see below), then the syndrome equations provide a linear system of equations that can easily be solved for the \"Y\" error values.\n\nConsequently, the problem is finding the \"X\", because then the leftmost matrix would be known, and both sides of the equation could be multiplied by its inverse, yielding Y\"\"\n\nThere is a linear recurrence relation that gives rise to a system of linear equations. Solving those equations identifies those error locations \"X\".\n\nDefine the error locator polynomial Λ(\"x\") as\n\nThe zeros of Λ(\"x\") are the reciprocals formula_155. This follows from the above product notation construction since if formula_156 then one of the multiplied terms will be zero formula_157, making the whole polynomial evaluate to zero. Explicitly:\n\nMultiply both sides by formula_160 and it will still be zero. j is any number such that formula_161.\n\nSum for \"k\" = 1 to \"ν\"\n\nCollect each term into its own sum, and extract the constant values of formula_164 that are unaffected by the summation\n\nNote that these summations are now equivalent to the syndrome values, which we know and can substitute in! This therefore reduces to\n\nSubtracting formula_167 from both sides yields\n\nRecall that j was chosen to be any integer between 1 and v inclusive, and this equivalence is true for any and all such values. Therefore, we have v linear equations, not just one. This system of linear equations can therefore be solved for the coefficients Λ of the error location polynomial:\n\nThe above assumes the decoder knows the number of errors \"ν\", but that number has not been determined yet. The PGZ decoder does not determine \"ν\" directly but rather searches for it by trying successive values. The decoder first assumes the largest value for a trial \"ν\" and sets up the linear system for that value. If the equations can be solved (i.e., the matrix determinant is nonzero), then that trial value is the number of errors. If the linear system cannot be solved, then the trial \"ν\" is reduced by one and the next smaller system is examined. \n\nUse the coefficients Λ found in the last step to build the error location polynomial. The roots of the error location polynomial can be found by exhaustive search. The error locators \"X\" are the reciprocals of those roots. Note that the order of coefficients of the error location polynomial can be reversed, in which case the roots of that reversed polynomial are the error locators formula_170 (not their reciprocals formula_155). Chien search is an efficient implementation of this step.\n\nOnce the error locators \"X\" are known, the error values can be determined. This can be done by direct solution for \"Y\" in the error equations matrix given above, or using the Forney algorithm.\n\nCalculate \"i\" by taking the log base formula_117 of \"X\". This is generally done using a precomputed lookup table.\n\nFinally, e(x) is generated from \"i\" and \"e\" and then is subtracted from r(x) to get the sent message s(x).\n\nConsider the Reed–Solomon code defined in with and (this is used in PDF417 barcodes) for a RS(7,3) code. The generator polynomial is\nIf the message polynomial is , then a systematic codeword is encoded as follows.\nErrors in transmission might cause this to be received instead.\nThe syndromes are calculated by evaluating \"r\" at powers of \"α\".\n\nUsing Gaussian elimination:\n\nThe coefficients can be reversed to produce roots with positive exponents, but typically this isn't used:\nwith the log of the roots corresponding to the error locations (right to left, location 0 is the last term in the codeword).\n\nTo calculate the error values, apply the Forney algorithm.\n\nSubtracting \"e\" \"x\" and \"e\" \"x\" from the received polynomial \"r\" reproduces the original codeword \"s\".\n\nThe Berlekamp–Massey algorithm is an alternate iterative procedure for finding the error locator polynomial. During each iteration, it calculates a discrepancy based on a current instance of Λ(x) with an assumed number of errors \"e\":\n\nand then adjusts Λ(x) and \"e\" so that a recalculated Δ would be zero. The article Berlekamp–Massey algorithm has a detailed description of the procedure. In the following example, C(x) is used to represent Λ(x).\n\nUsing the same data as the Peterson Gorenstein Zierler example above:\n\nThe final value of \"C\" is the error locator polynomial, Λ(\"x\").\n\nAnother iterative method for calculating both the error locator polynomial and the error value polynomial is based on Sugiyama's adaptation of the Extended Euclidean algorithm .\n\nDefine S(x), Λ(x), and Ω(x) for \"t\" syndromes and \"e\" errors:\n\nThe key equation is:\n\nFor \"t\" = 6 and \"e\" = 3:\n\nThe middle terms are zero due to the relationship between Λ and syndromes.\n\nThe extended Euclidean algorithm can find a series of polynomials of the form\n\nA(x) S(x) + B(x) x = R(x)\n\nwhere the degree of R decreases as i increases. Once the degree of R(x) < t/2, then\n\nA(x) = Λ(x)\n\nB(x) = -Q(x)\n\nR(x) = Ω(x).\n\nB(x) and Q(x) don't need to be saved, so the algorithm becomes:\n\nto set low order term of Λ(x) to 1, divide Λ(x) and Ω(x) by A(0):\n\nA(0) is the constant (low order) term of A.\n\nUsing the same data as the Peterson Gorenstein Zierler example above:\n\nA discrete Fourier transform can be used for decoding. To avoid conflict with syndrome names, let c(x) = s(x) the encoded codeword. r(x) and e(x) are the same as above. Define C(x), E(x), and R(x) as the discrete Fourier transforms of c(x), e(x), and r(x). Since r(x) = c(x) + e(x), and since a discrete Fourier transform is a linear operator, R(x) = C(x) + E(x).\n\nTransform r(x) to R(x) using discrete Fourier transform. Since the calculation for a discrete Fourier transform is the same as the calculation for syndromes, t coefficients of R(x) and E(x) are the same as the syndromes:\n\nUse formula_189 through formula_190 as syndromes (they're the same) and generate the error locator polynomial using the methods from any of the above decoders.\n\nLet v = number of errors. Generate E(x) using the known coefficients formula_191 to formula_192, the error locator polynomial, and these formulas\n\nThen calculate C(x) = R(x) - E(x) and take the inverse transform (polynomial interpolation) of C(x) to produce c(x).\n\nThe Singleton bound states that the minimum distance \"d\" of a linear block code of size (\"n\",\"k\") is upper-bounded by \"n\" − \"k\" + 1. The distance \"d\" was usually understood to limit the error-correction capability to ⌊\"d\"/2⌋. The Reed–Solomon code achieves this bound with equality, and can thus correct up to ⌊(\"n\" − \"k\" + 1)/2⌋ errors. However, this error-correction bound is not exact.\n\nIn 1999, Madhu Sudan and Venkatesan Guruswami at MIT published \"Improved Decoding of Reed–Solomon and Algebraic-Geometry Codes\" introducing an algorithm that allowed for the correction of errors beyond half the minimum distance of the code. It applies to Reed–Solomon codes and more generally to algebraic geometric codes. This algorithm produces a list of codewords (it is a list-decoding algorithm) and is based on interpolation and factorization of polynomials over formula_196 and its extensions.\n\nThe algebraic decoding methods described above are hard-decision methods, which means that for every symbol a hard decision is made about its value. For example, a decoder could associate with each symbol an additional value corresponding to the channel demodulator's confidence in the correctness of the symbol. The advent of LDPC and turbo codes, which employ iterated soft-decision belief propagation decoding methods to achieve error-correction performance close to the theoretical limit, has spurred interest in applying soft-decision decoding to conventional algebraic codes. In 2003, Ralf Koetter and Alexander Vardy presented a polynomial-time soft-decision algebraic list-decoding algorithm for Reed–Solomon codes, which was based upon the work by Sudan and Guruswami.\nIn 2016, Steven J. Franke and Joseph H. Taylor published a novel soft-decision decoder.\n\nHere we present a simple Matlab implementation for an encoder.\nNow the decoding part:\n\n\n\n\n"}
{"id": "31375187", "url": "https://en.wikipedia.org/wiki?curid=31375187", "title": "Robert Goldblatt", "text": "Robert Goldblatt\n\nRobert Ian Goldblatt (born 1949) is a mathematical logician who is Emeritus Professor in the School of Mathematics and Statistics at Victoria University, Wellington, New Zealand. His most popular books are \"Logics of Time and Computation\" and \"Topoi: the Categorial Analysis of Logic\". He has also written a graduate level textbook on hyperreal numbers which is an introduction to nonstandard analysis.\n\nHe has been Coordinating Editor of The \"Journal of Symbolic Logic\" and a Managing Editor of \"Studia Logica\".\nHe was elected Fellow and Councillor of the Royal Society of New Zealand, President of the New Zealand Mathematical Society, and represented New Zealand to the International Mathematical Union.\nIn 2012 he was awarded the \nJones Medal for lifetime achievement in mathematics.\n\n\n"}
{"id": "36707156", "url": "https://en.wikipedia.org/wiki?curid=36707156", "title": "Skew partition", "text": "Skew partition\n\nIn graph theory, a skew partition of a graph is a partition of its vertices into two subsets, such that the induced subgraph formed by one of the two subsets is disconnected and the induced subgraph formed by the other subset is the complement of a disconnected graph. Skew partitions play an important role in the theory of perfect graphs.\n\nA skew partition of a graph formula_1 is a partition of its vertices into two subsets formula_2 and formula_3 for which the induced subgraph formula_4 is disconnected and the induced subgraph formula_5 is the complement of a disconnected graph (co-disconnected).\nEquivalently, a skew partition of a graph formula_1 may be described by a partition of the vertices of formula_1 into four subsets formula_8, formula_9, formula_10, and formula_11, such that there are no edges from formula_8 to formula_9 and such that all possible edges from formula_10 to formula_11 exist; for such a partition, the induced subgraphs formula_16 and formula_17 are disconnected and co-disconnected respectively, so we may take formula_18 and formula_19.\n\nEvery path graph with four or more vertices has a skew partition, in which the co-disconnected set formula_3 is one of the interior edges of the path and the disconnected set formula_2 consists of the vertices on either side of this edge. However, it is not possible for a cycle graph of any length to have a skew partition: no matter which subsets of the cycle are chosen as the set formula_2, the complementary set formula_3 will have the same number of connected components, so it is not possible for formula_2 to be disconnected and formula_3 to be co-disconnected.\n\nIf a graph has a skew partition, so does its complement. For instance, the complements of path graphs have skew partitions, and the complements of cycle graphs do not.\n\nIf a graph is itself disconnected, then with only three simple exceptions (an empty graph, a graph with one edge and three vertices, or a four-vertex perfect matching) it has a skew partition, in which the co-disconnected side of the partition consists of the endpoints of a single edge and the disconnected side consists of all other vertices. For the same reason, if the complement of a graph is disconnected, then with a corresponding set of three exceptions it must have a skew partition.\n\nIf a graph has a clique separator (a clique whose removal would disconnect the remaining vertices) with more than one vertex, then the partition into the clique and the remaining vertices forms a skew partition. A clique cutset with one vertex is an articulation point; if such a vertex exists, then with a small number of simple exceptions, there is a skew partition in which the co-disconnected side consists of this vertex and one of its neighbors.\n\nA \"star cutset\" in a graph formula_1 is a vertex separator in which one of the separator vertices is adjacent to all the others. Every clique separator is a star cutset. Necessarily, a graph with a star cutset (with more than one vertex) has a skew partition in which the co-disconnected subgraph consists of the vertices in the star cutset and the disconnected subgraph consists of all the remaining vertices.\n\nA module (or homogeneous set) is a nontrivial subset formula_27 of the vertices of formula_1 such that, for every vertex formula_29 that is not in formula_27, either formula_29 is adjacent to all vertices in formula_27 or to none of them. If a graph formula_1 has a module formula_27 and, outside it, there exist both vertices adjacent to all vertices in formula_27 and other vertices adjacent to none of them, then formula_1 has a star cutset consisting of one vertex in the module together with its neighbors outside the module. On the other hand, if there exists a module in which one of these two subsets is empty, then the graph is disconnected or co-disconnected and again (with the three simple exceptions) it has a skew cutset.\n\nSkew partitions were introduced by , in connection with perfect graphs. Chvátal proved that a minimally imperfect graph could not have a star cutset. Trivially, disconnected graphs cannot be minimally imperfect, and it was also known that graphs with clique separators or modules could not be minimally imperfect. Claude Berge had conjectured in the early 1960s that perfect graphs were the same as the Berge graphs, graphs with no induced odd cycle (of length five or more) or its complement, and (because cycles and their complements do not have skew partitions) no minimal non-Berge graph can have a skew partition. Motivated by these results, Chvátal conjectured that no minimally imperfect graph could have a skew partition. Several authors proved special cases of this conjecture, but it remained unsolved for many years.\n\nSkew partitions gained significance when they were used by to prove the strong perfect graph theorem that the Berge graphs are indeed the same as the perfect graphs. Chudnovsky et al. were unable to prove Chvátal's conjecture directly, but instead proved a weaker result, that a minimal counterexample to the theorem (if it existed) could not have a balanced skew partition, a skew partition in which every induced path with endpoints on one side of the partition and interior vertices on the other side has even length. This result formed a key lemma in their proof, and the full version of Chvátal's lemma follows from their theorem.\n\nSkew partitions form one of the key components of a structural decomposition of perfect graphs used by as part of their proof of the strong perfect graph theorem. Chudnovsky et al. showed that every perfect graph either belongs to one of five basic classes of perfect graphs, or it has one of four types of decomposition into simpler graphs, one of which is a skew partition.\n\nA simpler example of a structural decomposition using skew partitions is given by . He observes that every comparability graph is complete, is bipartite, or has a skew partition. For, if every element of a partially ordered set is either a minimal element or a maximal element, then the corresponding comparability graph is bipartite. If the ordering is a total order, then the corresponding comparability graph is complete. If neither of these two cases arise, but every element that is neither minimal nor maximal is comparable to all other elements, then either the partition into the minimal and non-minimal elements (if there is more than one minimal element) or the partition into the maximal and non-maximal elements (if there is more than one maximal element) forms a star cutset. And in the remaining case, there exists an element formula_37 of the partial order that is not minimal, not maximal, and not comparable with all other elements; in this case, there is a skew partition (the complement of a star cutset) in which the co-disconnected side consists of the elements comparable to formula_37 (not including formula_37 itself) and the disconnected side consists of the remaining elements.\n\nThe chordal graphs have an even simpler decomposition of a similar type: they are either complete or they have a clique separator.\n\nA skew partition of a given graph, if it exists, may be found in polynomial time. This was originally shown by but with an impractically large running time of formula_40, where formula_41 is the number of vertices in the input graph. improved the running time to formula_42; here formula_43 is the number of input edges.\n\nIt is NP-complete to test whether a graph contains a skew partition in which one of the parts of the co-disconnected side is independent.\nTesting whether a given graph contains a balanced skew partition is also NP-complete in arbitrary graphs, but may be solved in polynomial time in perfect graphs.\n\n"}
{"id": "16066580", "url": "https://en.wikipedia.org/wiki?curid=16066580", "title": "Tarski's plank problem", "text": "Tarski's plank problem\n\nIn mathematics, Tarski's plank problem is a question about coverings of convex regions in \"n\"-dimensional Euclidean space by \"planks\": regions between two hyperplanes. Tarski asked if the sum of the widths of the planks must be at least the minimum width of the convex region. The question was answered affirmatively by \n\nGiven a convex body \"C\" in R and a hyperplane \"H\", the width of \"C\" parallel to \"H\", \"w\"(\"C\",\"H\"), is the distance between the two supporting hyperplanes of \"C\" that are parallel to \"H\". The smallest such distance (i.e. the infimum over all possible hyperplanes) is called the minimal width of \"C\", \"w\"(\"C\").\n\nThe (closed) set of points \"P\" between two distinct, parallel hyperplanes in R is called a plank, and the distance between the two hyperplanes is called the width of the plank, \"w\"(\"P\"). Tarski conjectured that if a convex body \"C\" of minimal width \"w\"(\"C\") was covered by a collection of planks, then the sum of the widths of those planks must be at least \"w\"(\"C\"). That is, if \"P\",…,\"P\" are planks such that\nthen\nBang proved this is indeed the case.\n\nThe name of the problem, specifically for the sets of points between parallel hyperplanes, comes from the visualisation of the problem in R. Here, hyperplanes are just straight lines and so planks become the space between two parallel lines. Thus the planks can be thought of as (infinitely long) planks of wood, and the question becomes how many planks does one need to completely cover a convex tabletop of minimal width \"w\"? Bang's theorem shows that, for example, a circular table of diameter \"d\" feet can't be covered by fewer than \"d\" planks of wood of width one foot each.\n\n"}
{"id": "13392068", "url": "https://en.wikipedia.org/wiki?curid=13392068", "title": "Tennenbaum's theorem", "text": "Tennenbaum's theorem\n\nTennenbaum's theorem, named for Stanley Tennenbaum who presented the theorem in 1959, is a result in mathematical logic that states that no countable nonstandard model of first-order Peano arithmetic (PA) can be recursive (Kaye 1991:153ff). \n\nA structure formula_1 in the language of PA is recursive if there are recursive functions + and × from formula_2 to formula_3, a recursive two-place relation < on formula_3, and distinguished constants formula_5 such that\nwhere formula_7 indicates isomorphism and formula_8 is the set of (standard) natural numbers. Because the isomorphism must be a bijection, every recursive model is countable. There are many nonisomorphic countable nonstandard models of PA.\n\nTennenbaum's theorem states that no countable nonstandard model of PA is recursive. Moreover, neither the addition nor the multiplication of such a model can be recursive.\n\nThis sketch follows the argument presented by Kaye (1991). The first step in the proof is to show that, if \"M\" is any countable nonstandard model of PA, then the standard system of M (defined below) contains at least one nonrecursive set \"S\". The second step is to show that, if either the addition or multiplication operation on \"M\" were recursive, then this set \"S\" would be recursive, which is a contradiction. \n\nThrough the methods used to code ordered tuples, each element formula_9 can be viewed as a code for a set formula_10 of elements of \"M\". In particular, if we let formula_11 be the \"i\"th prime in \"M\", then formula_12. Each set formula_10 will be bounded in \"M\", but if \"x\" is nonstandard then the set formula_10 may contain infinitely many standard natural numbers. The standard system of the model is the collection formula_15. It can be shown that the standard system of any nonstandard model of PA contains a nonrecursive set, either by appealing to the incompleteness theorem or by directly considering a pair of recursively inseparable r.e. sets (Kaye 1991:154). These are disjoint r.e. sets formula_16 so that that there is no recursive set formula_17 with formula_18 and formula_19. \n\nFor the latter construction, begin with a pair of recursively inseparable r.e. sets \"A\" and \"B\". For natural number \"x\" there is a \"y\" such that, for all \"i < x\", if formula_20 then formula_21 and if formula_22 then formula_23. By the overspill property, this means that there is some nonstandard \"x\" in \"M\" for which there is a (necessarily nonstandard) \"y\" in \"M\" so that, for every formula_24 with formula_25, we have\nLet formula_27 be the corresponding set in the standard system of \"M\". Because \"A\" and \"B\" are r.e., one can show that formula_28 and formula_29. Hence \"S\" is a separating set for \"A\" and \"B\", and by the choice of \"A\" and \"B\" this means \"S\" is nonrecursive. \n\nNow, to prove Tennenbaum's theorem, begin with a nonstandard countable model \"M\" and an element \"a\" in \"M\" so that formula_30 is nonrecursive. The proof method shows that, because of the way the standard system is defined, it is possible to compute the characteristic function of the set \"S\" using the addition function formula_31 of \"M\" as an oracle. In particular, if formula_32 is the element of \"M\" corresponding to 0, and formula_33 is the element of \"M\" corresponding to 1, then for each formula_34 we can compute formula_35 (\"i\" times). To decide if a number \"n\" is in \"S\", first compute \"p\", the \"n\"th prime in \"N\". Then, search for an element \"y\" of \"M\" so that \nfor some formula_37. This search will halt because the Euclidean algorithm can be applied to any model of PA. Finally, we have formula_38 if and only if the \"i\" found in the search was 0. Because \"S\" is not recursive, this means that the addition operation on \"M\" is nonrecursive. \n\nA similar argument shows that it is possible to compute the characteristic function of \"S\" using the multiplication of \"M\" as an oracle, so the multiplication operation on \"M\" is also nonrecursive (Kaye 1991:154).\n\n"}
{"id": "47231784", "url": "https://en.wikipedia.org/wiki?curid=47231784", "title": "Teofilo F. Gonzalez", "text": "Teofilo F. Gonzalez\n\nTeofilo Francisco Gonzalez Arce (born January 26, 1948 in Monterrey, Mexico) is a Mexican-American computer scientist who is professor emeritus of computer science at the University of California, Santa Barbara. \n\nIn 1972 Gonzalez was one of the first students who earned a bachelor's degree in computer science (Ingeniero en Sistemas Computacionales) in Mexico, at the Monterrey Institute of Technology and Higher Education.\nHe completed his Ph.D. in 1975 from the University of Minnesota under the supervision of Sartaj Sahni. He taught at the University of Oklahoma from 1975 to 1976, at the Pennsylvania State University from 1976 to 1979, at the Monterrey Institute of Technology and Higher Education from 1979 to 1980, and at the University of Texas at Dallas from 1980 to 1984, before joining the UCSB computer science faculty in 1984. He spent Sabbatical Leaves at Utrecht University (1990) in the Netherlands and the Monterrey Institute of Technology and Higher Education. Professor Gonzalez became Fellow of IASTED in 2009.\n\nHe is known for his highly cited pioneering research in the hardness of approximation;\nfor his sub-linear and best possible approximation algorithm (unless P = NP) based on the farthest-first traversal for the metric \"k\"-center problem (k-tMM clustering);\nand for introducing the open-shop scheduling problem as well as algorithms for its solution that have found numerous applications in several research areas as well as for his research on flow shop scheduling, and job shop scheduling algorithms.\nHe is the editor of the \"Handbook on Approximation Algorithms and Metaheuristics\", and he is co-editor of Volume 1 (Computer Science and Software Engineering) of the \"Computing Handbook Set\".\n\n"}
{"id": "11141222", "url": "https://en.wikipedia.org/wiki?curid=11141222", "title": "Tetrad formalism", "text": "Tetrad formalism\n\nThe tetrad formalism is an approach to general relativity that replaces the choice of a coordinate basis by the less restrictive choice of a local basis for the tangent bundle, i.e. a locally defined set of four linearly independent vector fields called a tetrad.\n\nIn the tetrad formalism all tensors are represented in terms of a chosen basis. (When generalised to other than four dimensions this approach is given other names, see Cartan formalism.) As a formalism rather than a theory, it does not make different predictions but does allow the relevant equations to be expressed differently.\n\nThe advantage of the tetrad formalism over the standard coordinate-based approach to general relativity lies in the ability to choose the tetrad basis to reflect important physical aspects of the spacetime. The abstract index notation denotes tensors as if they were represented by their coefficients with respect to a fixed local tetrad. Compared to a completely coordinate free notation, which is often conceptually clearer, it allows an easy and computationally explicit way to denote contractions.\n\nIn the tetrad formalism, a tetrad basis is chosen: a set of four independent vector fields formula_1 that together span the 4D vector tangent space at each point in spacetime. Dually, a tetrad determines (and is determined by) a dual co-tetrad—a set of four independent covectors (1-forms) formula_2 such that \nwhere formula_4 is the Kronecker delta. A tetrad is usually specified by its coefficients formula_5 with respect to a coordinate basis, despite the fact that the choice of a tetrad does not actually require the additional choice of a set of (local) coordinates formula_6.\n\nFrom a mathematical point of view, the four vector fields formula_7 define a section of the \nframe bundle i.e. a parallelization of formula_8 which is equivalent to an isomorphism formula_9. Since not every manifold is parallelizable, a tetrad can generally only be chosen locally.\n\nAll tensors of the theory can be expressed in the vector and covector basis, by expressing them as linear combinations of members of the (co)tetrad. For example, the spacetime metric itself can be transformed from a coordinate basis to the tetrad basis. \n\nPopular tetrad bases include orthonormal tetrads and null tetrads. Null tetrads are composed of four null vectors, so are used frequently in problems dealing with radiation, and are the basis of the Newman–Penrose formalism and the GHP formalism.\n\nThe standard formalism of differential geometry (and general relativity) consists simply of using the coordinate tetrad in the tetrad formalism. The coordinate tetrad is the canonical set of vectors associated with the coordinate chart. The coordinate tetrad is commonly denoted formula_10 whereas the dual cotetrad is denoted formula_11. These tangent vectors are usually defined as directional derivative operators: given a chart formula_12 which maps a subset of the manifold into coordinate space formula_13, and any scalar field formula_14, the coordinate vectors are such that:\nThe definition of the cotetrad uses the usual abuse of notation formula_16 to define covectors (1-forms) on formula_8. The involvement of the coordinate tetrad is not usually made explicit in the standard formalism. In the tetrad formalism, instead of writing tensor equations out fully (including tetrad elements and tensor products formula_18 as above) only \"components\" of the tensors are mentioned. For example, the metric is written as \"formula_19\". When the tetrad is unspecified this becomes a matter of specifying the type of the tensor called abstract index notation. It allows to easily specify contraction between tensors by repeating indices as in the Einstein summation convention.\n\nChanging tetrads is a routine operation in the standard formalism, as it is involved in every coordinate transformation (i.e., changing from one coordinate tetrad basis to another). Switching between multiple coordinate charts is necessary because, except in trivial cases, it is not possible for a single coordinate chart to cover the entire manifold. Changing to and between general tetrads is much similar and equally necessary (except for parallelizable manifolds). Any tensor can locally be written in terms of this coordinate tetrad or a general (co)tetrad.\n\nFor example, the metric tensor formula_20 can be expressed as:\n\n(here we use the Einstein summation convention). Likewise, the metric can be expressed with respect to an arbitrary (co)tetrad as\n\nWe can translate from a general co-tetrad to the coordinate co-tetrad by expanding the covector formula_23. We then get \n\nfrom which it follows that formula_25. Likewise\nexpanding formula_26 with respect to the general tetrad we get\n\nwhich shows that formula_28. For notational simplicity one usually drops the round brackets around the indices, recognizing that they can both label a set of (co)vectors and tensor components with respect to the (co)tetrad defined by these (co)vectors.\n\nThe manipulation with tetrad coefficients shows that abstract index formulas can, in principle, be obtained from tensor formulas with respect to a coordinate tetrad by \"replacing greek by latin indices\". However care must be taken that a coordinate tetrad formula defines a genuine tensor when differentiation is involved. Since the coordinate vectorfields have vanishing Lie bracket (i.e. commute: formula_29), naive substitutions of formulas that correctly compute tensor coefficients with respect to a coordinate tetrad may not correctly define a tensor with respect to a general tetrad because the Lie bracket formula_30. For example, the Riemann curvature tensor is defined for general vectorfields formula_31 by\n\nIn a coordinate tetrad this gives tensor coefficients\n\nThe naive \"Greek to Latin\" substitution of the latter expression\nis incorrect because for fixed \"c\" and \"d\", formula_35 is, in general, a first order differential operator rather than a zeroth order operator which defines a tensor coefficient. Substituting a general tetrad basis in the abstract formula we find the proper definition of the curvature in abstract index notation, however: \n\nwhere formula_37. Note that the expression formula_38 is indeed a zeroth order operator, hence (the (\"c\" \"d\")-component of) a tensor. Since it agrees with the coordinate expression for the curvature when specialised to a coordinate tetrad it is clear, even without using the abstract definition of the curvature, that it defines the same tensor as the coordinate basis expression.\n\n\n\n"}
{"id": "58904582", "url": "https://en.wikipedia.org/wiki?curid=58904582", "title": "Tetradic number", "text": "Tetradic number\n\nA tetradic number, also known as a four-way number, is a number that remains the same when flipped back to front, flipped front to back, mirrored up-down, or flipped up-down. The only numbers that remain the same which turned up-side-down or mirrored are 0, 1, and 8, so a tetradic number is a palindromic number containing only 0, 1, and 8 as digits. The first few tetradic numbers are 1, 8, 11, 88, 101, 111, 181, 808, 818, ... (OEIS A006072).\n\nTetradic numbers are also known as four-way numbers due to the fact that they have four-way symmetry and can flipped back to front, flipped front to back, mirrored up-down, or flipped up-down and always stay the same. The four-way symmetry explains the name, due to tetra- being the Greek prefix for four. Tetradic numbers are both strobogrammatic and palindromic.\n\nIf you have a tetradic number, a larger one can always be generated by adding another tetradic number to each end, retaining the symmetry.\n\nTetradic primes are a specific type of tetradic number defined as tetradic numbers that are also prime numbers. The first few tetradic primes are 11, 101, 181, 18181, 1008001, 1180811, 1880881, 1881881, ... (OEIS A068188).\n\nThe largest known tetradic prime is \n\nwhere formula_2 is a repunit. The prime has 180,055 decimal digits.\n"}
{"id": "33301481", "url": "https://en.wikipedia.org/wiki?curid=33301481", "title": "Transmission curve", "text": "Transmission curve\n\nThe transmission curve or transmission characteristic is the mathematical function or graph that describes the transmission fraction of an optical or electronic filter as a function of frequency or wavelength. It is an instance of a transfer function but, unlike the case of, for example, an amplifier, output never exceeds input (maximum transmission is 100%). The term is often used in commerce, science, and technology to characterise filters.\n\nThe term has also long been used in fields such as geophysics and astronomy to characterise the properties of regions through which radiation passes, such as the ionosphere.\n\n"}
{"id": "32169", "url": "https://en.wikipedia.org/wiki?curid=32169", "title": "Unified Modeling Language", "text": "Unified Modeling Language\n\nThe Unified Modeling Language (UML) is a general-purpose, developmental, modeling language in the field of software engineering, that is intended to provide a standard way to visualize the design of a system.\n\nThe creation of UML was originally motivated by the desire to standardize the disparate notational systems and approaches to software design. It was developed by Grady Booch, Ivar Jacobson and James Rumbaugh at Rational Software in 1994–1995, with further development led by them through 1996.\n\nIn 1997 UML was adopted as a standard by the Object Management Group (OMG), and has been managed by this organization ever since. In 2005 UML was also published by the International Organization for Standardization (ISO) as an approved ISO standard. Since then the standard has been periodically revised to cover the latest revision of UML.\n\nUML has been evolving since the second half of the 1990s and has its roots in the object-oriented programming methods developed in the late 1980s and early 1990s. The timeline (see image) shows the highlights of the history of object-oriented modeling methods and notation.\n\nIt is originally based on the notations of the Booch method, the object-modeling technique (OMT) and object-oriented software engineering (OOSE), which it has integrated into a single language.\n\nRational Software Corporation hired James Rumbaugh from General Electric in 1994 and after that the company became the source for two of the most popular object-oriented modeling approaches of the day: Rumbaugh's object-modeling technique (OMT) and Grady Booch's method. They were soon assisted in their efforts by Ivar Jacobson, the creator of the object-oriented software engineering (OOSE) method, who joined them at Rational in 1995.\n\nUnder the technical leadership of those three (Rumbaugh, Jacobson and Booch), a consortium called the UML Partners was organized in 1996 to complete the \"Unified Modeling Language (UML)\" specification, and propose it to the Object Management Group (OMG) for standardisation. The partnership also contained additional interested parties (for example HP, DEC, IBM and Microsoft). The UML Partners' UML 1.0 draft was proposed to the OMG in January 1997 by the consortium. During the same month the UML Partners formed a group, designed to define the exact meaning of language constructs, chaired by Cris Kobryn and administered by Ed Eykholt, to finalize the specification and integrate it with other standardization efforts. The result of this work, UML 1.1, was submitted to the OMG in August 1997 and adopted by the OMG in November 1997.\n\nAfter the first release a task force was formed to improve the language, which released several minor revisions, 1.3, 1.4, and 1.5.\n\nThe standards it produced (as well as the original standard) have been noted as being ambiguous and inconsistent.\n\nAs with database Chen, Bachman, and ISO ER diagrams, class models are specified to use \"look-across\" cardinalities, even though several authors (Merise, Elmasri & Navathe amongst others) prefer same-side or \"look-here\" for roles and both minimum and maximum cardinalities. Recent researchers (Feinerer, Dullea et al.) have shown that the \"look-across\" technique used by UML and ER diagrams is less effective and less coherent when applied to \"n\"-ary relationships of order strictly greater than 2.\n\nFeinerer says: \"Problems arise if we operate under the look-across semantics as used for UML associations. Hartmann investigates this situation and shows how and why different transformations fail.\", and: \"As we will see on the next few pages, the look-across interpretation introduces several difficulties which prevent the extension of simple mechanisms from binary to \"n\"-ary associations.\"\n\nUML 2.0 major revision replaced version 1.5 in 2005, which was developed with an enlarged consortium to improve the language further to reflect new experience on usage of its features.\n\nAlthough UML 2.1 was never released as a formal specification, versions 2.1.1 and 2.1.2 appeared in 2007, followed by UML 2.2 in February 2009. UML 2.3 was formally released in May 2010. UML 2.4.1 was formally released in August 2011. UML 2.5 was released in October 2012 as an \"In progress\" version and was officially released in June 2015. Formal version 2.5.1 was adopted in December 2017.\n\nThere are four parts to the UML 2.x specification:\n\n\nThe current versions of these standards are:\n\nIt continues to be updated and improved by the revision task force, who resolve any issues with the language.\n\nUML offers a way to visualize a system's architectural blueprints in a diagram, including elements such as:\n\n\nAlthough originally intended for object-oriented design documentation, UML has been extended to a larger set of design documentation (as listed above), and been found useful in many contexts.\n\nUML is not a development method by itself; however, it was designed to be compatible with the leading object-oriented software development methods of its time, for example OMT, Booch method, Objectory and especially RUP that it was originally intended to be used with when work began at Rational Software.\n\nIt is important to distinguish between the UML model and the set of diagrams of a system. A diagram is a partial graphic representation of a system's model. The set of diagrams need not completely cover the model and deleting a diagram does not change the model. The model may also contain documentation that drives the model elements and diagrams (such as written use cases).\n\nUML diagrams represent two different views of a system model:\n\n\nUML models can be exchanged among UML tools by using the XML Metadata Interchange (XMI) format.\n\nIn UML, one of the key tools for behavior modelling is the use-case model, caused by OOSE. Use cases are a way of specifying required usages of a system. Typically, they are used to capture the requirements of a system, that is, what a system is supposed to do.\n\nUML 2 has many types of diagrams, which are divided into two categories. Some types represent \"structural\" information, and the rest represent general types of \"behavior\", including a few that represent different aspects of \"interactions\". These diagrams can be categorized hierarchically as shown in the following class diagram:\n\nThese diagrams may all contain comments or notes explaining usage, constraint, or intent.\n\nStructure diagrams emphasize the things that must be present in the system being modeled. Since structure diagrams represent the structure, they are used extensively in documenting the software architecture of software systems. For example, the component diagram describes how a software system is split up into components and shows the dependencies among these components.\n\nBehavior diagrams emphasize what must happen in the system being modeled. Since behavior diagrams illustrate the behavior of a system, they are used extensively to describe the functionality of software systems. As an example, the activity diagram describes the business and operational step-by-step activities of the components in a system.\n\nInteraction diagrams, a subset of behavior diagrams, emphasize the flow of control and data among the things in the system being modeled. For example, the sequence diagram shows how objects communicate with each other regarding a sequence of messages.\n\nThe Object Management Group (OMG) has developed a metamodeling architecture to define the UML, called the Meta-Object Facility. MOF is designed as a four-layered architecture, as shown in the image at right. It provides a meta-meta model at the top, called the M3 layer. This M3-model is the language used by Meta-Object Facility to build metamodels, called M2-models.\n\nThe most prominent example of a Layer 2 Meta-Object Facility model is the UML metamodel, which describes the UML itself. These M2-models describe elements of the M1-layer, and thus M1-models. These would be, for example, models written in UML. The last layer is the M0-layer or data layer. It is used to describe runtime instances of the system.\n\nThe meta-model can be extended using a mechanism called stereotyping. This has been criticised as being insufficient/untenable by Brian Henderson-Sellers and Cesar Gonzalez-Perez in \"Uses and Abuses of the Stereotype Mechanism in UML 1.x and 2.0\".\n\nUML has been marketed for many contexts.\n\nIt has been treated, at times, as a design silver bullet, which leads to problems. UML misuse includes overuse (designing every part of the system with it, which is unnecessary) and assuming that novices can design with it.\n\nIt is considered a large language, with many constructs. Some people (including Jacobson) feel that UML's size hinders learning (and therefore, using) it.\n\n\n"}
{"id": "7349264", "url": "https://en.wikipedia.org/wiki?curid=7349264", "title": "Weak n-category", "text": "Weak n-category\n\nIn category theory, a weak \"n\"-category is a generalization of the notion of strict \"n\"-category where composition and identities are not strictly associative and unital, but only associative and unital up to coherent equivalence. This generalisation only becomes noticeable at dimensions two and above where weak 2-, 3- and 4-categories are typically referred to as bicategories, tricategories, and tetracategories. The subject of weak n-categories is an area of ongoing research.\n\nThere is currently much work to determine what the coherence laws for weak n-categories should be. Weak \"n\"-categories have become the main object of study in higher category theory. There are basically two classes of theories: those in which the higher cells and higher compositions are realized algebraically (most remarkably the Michael Batanin's theory of weak higher categories) and those in which more topological models are used (e.g. a higher category as a simplicial set satisfying some universality properties). \n\nIn a terminology due to John Baez and James Dolan, a (\"n\",\"k\")-category is a weak \"n\"-category, such that all \"h\"-cells for \"h\">\"k\" are invertible. Some of the formalism for (\"n\",\"k\")-categories are much simpler than those for general \"n\"-categories. In particular, several technically accessible formalisms of (infinity,1)-categories are now known. Now the most popular such a formalism centers on a notion of quasi-category, other approaches include a properly understood theory of simplicially enriched categories and the approach via Segal categories; a class of examples of \"stable\" (infinity,1)-categories can be modeled (in the case of characteristics zero) also via pretriangulated A-infinity categories of Maxim Kontsevich. Quillen model categories are viewed as a presentation of an (infinity,1)-category; however not all (infinity,1)-categories can be presented via model categories.\n\n\n"}
{"id": "5864214", "url": "https://en.wikipedia.org/wiki?curid=5864214", "title": "Yuktibhāṣā", "text": "Yuktibhāṣā\n\nYuktibhāṣā (; \"Rationale in the Malayalam/Sanskrit language\") also known as Gaṇitanyāyasaṅgraha (\"Compendium of astronomical rationale\"), is a major treatise on mathematics and astronomy, written by Indian astronomer Jyesthadeva of the Kerala school of mathematics in about AD 1530. The treatise, written in Malayalam, is a consolidation of the discoveries by Madhava of Sangamagrama, Nilakantha Somayaji, Parameshvara, Jyeshtadeva, Achyuta Pisharati and other astronomer-mathematicians of the Kerala school. \"Yuktibhasa\" is mainly based on Nilakantha's \"Tantra Samgraha\". It is considered, possibly the first text, on the foundations of calculus and predates those of European mathematicians such as James Gregory and Newton by many centuries. The treatise was largely unnoticed as the book was written in the local language of Malayalam and it was thought that many Indian ideas in astronomy and computation lacked proofs or foundations. Yuktibhasa however demonstrates founding principles and the development and proofs of theorems. However, both Oxford University and Royal Society of Great Britain have accepted that Calculus and many such pioneering mathematical theorems originated in India.\n\nThe work was unique for its time, since it contained proofs and derivations of the theorems that it presented; something that was not usually done by any Indian mathematicians of that era. Some of its important developments in analysis include: the infinite series expansion of a function, the power series, the Taylor series, the trigonometric series for sine, cosine, tangent and arctangent, the second and third order Taylor series approximations of sine and cosine, the power series of π, π/4, θ, the radius, diameter and circumference, and tests of convergence.\n\n\"Yuktibhasa\" contains most of the developments of earlier Kerala School mathematicians, particularly Madhava and Nilakantha. The text is divided into two parts — the former deals with mathematical analysis of arithmetic, algebra, trigonometry and geometry, logistics, algebraic problems, fractions, Rule of three, \"Kuttakaram\", circle and disquisition on R-Sine; and the latter about astronomy.\n\nAs per the old Indian tradition of starting off new chapters with elementary content, the first four chapters of the \"Yuktibhasa\" contain elementary mathematics, such as division, proof of Pythagorean theorem, square root determination, etc. The radical ideas are not discussed until the sixth chapter on circumference of a circle.\nYuktibhasa contains the derivation and proof of the power series for inverse tangent, discovered by Madhava. In the text, Jyesthadeva describes Madhava's series in the following manner:\n\nThis yields\n\nwhich further yields the theorem\n\nsometimes mistakenly attributed to James Gregory, who published it in 1667.\n\nThe text also elucidates Madhava's infinite series expansion of π:\n\nwhich he obtained from the power series expansion of the arc-tangent function.\n\nUsing a rational approximation of this series, he gave values of the number π as 3.14159265359 - correct to 11 decimals; and as 3.1415926535898 - correct to 13 decimals. These were the most accurate approximations of π after almost a thousand years.\n\nThe text describes that he gave two methods for computing the value of π.\n\n\nand used the first 21 terms to compute an approximation of π correct to 11 decimal places as 3.14159265359.\n\n\nin the infinite series expansion of formula_6 to improve the approximation of π to 13 decimal places of accuracy when n = 76.\n\nApart from these, the \"Yukthibhasa\" contains many elementary, and complex mathematics, including,\n\nMost of these results were long before their European counterparts, to whom credit was traditionally attributed.\n\nChapters seven to seventeen of the text deals essentially with subjects of astronomy, viz. Planetary orbit, Celestial sphere, ascension, declination, directions and shadows, spherical triangles, ellipses and parallax correction. The planetary theory described in the book is similar to that later adopted by Danish astronomer Tycho Brahe.\n\nThe importance of \"Yuktibhasa\" was brought to the attention of modern scholarship by C.M. Whish in 1834 through a paper published in the \"Transactions of the Royal Asiatic Society of Great Britain and Ireland\". However, an edition of the mathematics part of the text (along with notes in Malayalam) was published only in 1948 by Rama Varma Maru Thampuran and Akhileswara Aiyar. For the first time, a critical edition of the entire Malayalam text, along with English translation and detailed explanatory notes, has been brought out in two volumes by Springer\nin 2008.\nA third volume presenting a critical edition of the Sanskrit Ganitayuktibhasa has been brought out by the Indian Institute of Advanced Study, Shimla in 2009.\n\n\n"}
{"id": "15389789", "url": "https://en.wikipedia.org/wiki?curid=15389789", "title": "Zigzag code", "text": "Zigzag code\n\nIn coding theory, a zigzag code is a type of linear error-correcting code introduced by . They are defined by partitioning the input data into segments of fixed size, and adding sequence of check bits to the data, where each check bit is the exclusive or of the bits in a single segment and of the previous check bit in the sequence.\nThe code rate is high: where is the number of bits per segment. Its worst-case ability to correct transmission errors is very limited: in the worst case it can only detect a single bit error and cannot correct any errors. However, it works better in the soft-decision model of decoding: its regular structure allows the task of finding a maximum-likelihood decoding or a posteriori probability decoding to be performed in constant time per input bit.\n"}
