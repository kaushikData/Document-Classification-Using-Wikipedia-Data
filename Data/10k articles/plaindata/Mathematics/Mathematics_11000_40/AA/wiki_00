{"id": "856626", "url": "https://en.wikipedia.org/wiki?curid=856626", "title": "155 (number)", "text": "155 (number)\n\n155 (one hundred [and] fifty-five) is the natural number following 154 and preceding 156.\n\n155 is:\n\nThere are 155 primitive permutation groups of degree 81. \n\nIf one adds up all the primes from the least through the greatest prime factors of 155, that is, 5 and 31, the result is 155. Only three other \"small\" semiprimes (10, 39, and 371) share this attribute.\n\n\n\n\n155 is also:\n\n"}
{"id": "39448532", "url": "https://en.wikipedia.org/wiki?curid=39448532", "title": "Alan Baker (philosopher)", "text": "Alan Baker (philosopher)\n\nAlan R. Baker is a professor of Philosophy in Swarthmore College (Pennsylvania, United States), specializing in the philosophy of mathematics and the philosophy of science. He is also a former U.S. shogi champion and created the only active shogi club at an American university.\n\nBaker did his undergraduate studies at the University of Cambridge, earning a bachelor's degree in philosophy with first class honours in 1991. He then moved to the U.S. for graduate school, earning a master's degree in 1995 and a Ph.D. in 1999, both in philosophy from Princeton University. His doctoral supervisors were Paul Benacerraf and Gideon Rosen. After working as an assistant professor at Xavier University, he moved to Swarthmore in 2003.\n\nPhilosophically, Baker is a mathematical realist who has used examples from evolutionary biology to show the necessity of mathematics in scientific reasoning.\n\nIn 2005 the \"New York Times\" published an excerpt from the exam from his \"“Introduction to Metaphysics and Epistemology”\" course in its \"“pop quiz”\" column.\n\nAlan Baker has played shogi since 1996 and holds 29th place on the FESA list as of June 1, 2014. His highest place on the FESA list was 19th, on January 1, 2009.\n\nIn 2005 he founded a shogi club at Swarthmore College, outside Philadelphia, which is the only active college-based shogi club in the U.S.\n\nTournament results:\n\n"}
{"id": "32376723", "url": "https://en.wikipedia.org/wiki?curid=32376723", "title": "Alvis–Curtis duality", "text": "Alvis–Curtis duality\n\nIn mathematics, Alvis–Curtis duality is a duality operation on the characters of a reductive group over a finite field, introduced by and studied by his student . introduced a similar duality operation for Lie algebras.\n\nAlvis–Curtis duality has order 2 and is an isometry on generalized characters.\n\nThe dual ζ* of a character ζ of a finite group \"G\" with a split BN-pair is defined to be\nHere the sum is over all subsets \"J\" of the set \"R\" of simple roots of the Coxeter system of \"G\". The character ζ is the truncation of ζ to the parabolic subgroup \"P\" of the subset \"J\", given by restricting ζ to \"P\" and then taking the space of invariants of the unipotent radical of \"P\", and ζ is the induced representation of \"G\". (The operation of truncation is the adjoint functor of parabolic induction.)\n\n\n"}
{"id": "5263012", "url": "https://en.wikipedia.org/wiki?curid=5263012", "title": "Calabi conjecture", "text": "Calabi conjecture\n\nIn mathematics, the Calabi conjecture was a conjecture about the existence of certain \"nice\" Riemannian metrics on certain complex manifolds, made by and proved by . Yau received the Fields Medal in 1982 in part for this proof.\n\nThe Calabi conjecture states that a compact Kähler manifold has a unique Kähler metric in the same class whose Ricci form is any given 2-form representing the first Chern class. In particular if the first Chern class vanishes there is a unique Kähler metric in the same class with vanishing Ricci curvature; these are called Calabi–Yau manifolds.\n\nMore formally, the Calabi conjecture states:\n\nThe Calabi conjecture is closely related to the question of which Kähler manifolds have Kähler–Einstein metrics.\n\nA conjecture closely related to the Calabi conjecture states that if a compact Kähler variety has a negative, zero, or positive first Chern class then it has a Kähler–Einstein metric in the same class as its Kähler metric, unique up to rescaling. \nThis was proved for negative first Chern classes independently by Thierry Aubin and Shing-Tung Yau in 1976. When the Chern class is zero it was proved by Yau as an easy consequence of the Calabi conjecture.\n\nIt was disproved for positive first Chern classes by Yau, who observed that the complex projective plane blown up at 2 points has no Kähler–Einstein metric and so is a counterexample. Also even when Kähler–Einstein metric exists it need not be unique. There has been a lot of further work on the positive first Chern class case. A necessary condition for the existence of a Kähler–Einstein metric is that the Lie algebra of holomorphic vector fields is reductive. Yau conjectured that when the first Chern class is positive, a Kähler variety has a Kähler–Einstein metric if and only if it is stable in the sense of geometric invariant theory.\n\nThe case of complex surfaces has been settled by Gang Tian. The complex surfaces with positive Chern class are either a product of two copies of a projective line (which obviously has a Kähler–Einstein metric) or a blowup of the projective plane in at most 8 points in \"general position\", in the sense that no 3 lie on a line and no 6 lie on a quadric. The projective plane has a Kähler–Einstein metric, and the projective plane blown up in 1 or 2 points does not, as the Lie algebra of holomorphic vector fields is not reductive.\nTian showed that the projective plane blown up in 3, 4, 5, 6, 7, or 8 points in general position has a Kähler–Einstein metric.\n\nCalabi transformed the Calabi conjecture into a non–linear partial differential equation of complex Monge–Ampere type, and showed that this equation has at most one solution, thus establishing the uniqueness of the required Kähler metric.\n\nYau proved the Calabi conjecture by constructing a solution of this equation using the continuity method. This involves first solving an easier equation, and then showing that a solution to the easy equation can be continuously deformed to a solution of the hard equation. The hardest part of Yau's solution is proving certain a priori estimates for the derivatives of solutions.\n\nSuppose that \"M\" is a complex compact manifold with a Kahler form ω.\nAny other Kahler form in the same class is of the form\nfor some smooth function φ on \"M\", unique up to addition of a constant. The Calabi conjecture is therefore equivalent to the following problem:\n\nThis is an equation of complex Monge–Ampere type for a single function φ.\nIt is a particularly hard partial differential equation to solve, as it is non-linear in the terms of highest order. \nIt is trivial to solve it when \"f\"=0, as φ=0 is a solution. The idea of the continuity method is to show that it can be solved for all \"f\" by showing that the set of \"f\" for which it can be solved is both open and closed. Since the set of \"f\" for which it can be solved is non-empty, and the set of all \"f\" is connected, this shows that it can be solved for all \"f\".\n\nThe map from smooth functions to smooth functions taking φ to \"F\" defined by \nis neither injective nor surjective. It is not injective because adding a constant to φ does not change \"F\", and it is not surjective \nbecause \"F\" must be positive and have average value 1. So we consider the map restricted to functions φ that are normalized to have average value 0, and ask if this map is an isomorphism onto the set of positive \"F\"=\"e\" with average value 1. Calabi and Yau proved that it is indeed an isomorphism. This is done in several steps, described below.\n\nProving that the solution is unique involves showing that if \nthen φ and φ differ by a constant\n(so must be the same if they are both normalized to have average value 0). \nCalabi proved this by showing that the average value of\nis given by an expression that is at most 0. As it is obviously at least 0, it must be 0, so \nwhich in turn forces φ and φ to differ by a constant.\n\nProving that the set of possible \"F\" is open (in the set of smooth functions with average value 1) involves showing that if it is possible to solve the equation for some \"F\", then it is possible to solve it for all sufficiently close \"F\". Calabi proved this by using the implicit function theorem for Banach spaces: in order to apply this, the main step is to show that the \"linearization\" of the differential operator above is invertible.\n\nThis is the hardest part of the proof, and was the part done by Yau.\nSuppose that \"F\" is in the closure of the image of possible\nfunctions φ. This means that there is a sequence of \nfunctions φ, φ, ...\nsuch that the corresponding functions \"F\", \"F\"...\nconverge to \"F\", and the problem is to show that some subsequence of the φs converges to a solution φ. In order to do this, Yau finds some a priori bounds for the functions φ and their higher derivatives\nin terms of the higher derivatives of log(\"f\"). Finding these bounds requires a long sequence of hard estimates, each improving slightly on the previous estimate. The bounds Yau gets are enough to show that the functions φ all lie in a compact subset of a suitable Banach space of functions, so it is possible to find a convergent subsequence.\nThis subsequence converges to a function φ with image \"F\", which \nshows that the set of possible images \"F\" is closed.\n\n"}
{"id": "39406", "url": "https://en.wikipedia.org/wiki?curid=39406", "title": "Central limit theorem", "text": "Central limit theorem\n\nIn probability theory, the central limit theorem (CLT) establishes that, in some situations, when independent random variables are added, their properly normalized sum tends toward a normal distribution (informally a \"bell curve\") even if the original variables themselves are not normally distributed. The theorem is a key concept in probability theory because it implies that probabilistic and statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions.\n\nFor example, suppose that a sample is obtained containing a large number of observations, each observation being randomly generated in a way that does not depend on the values of the other observations, and that the arithmetic mean of the observed values is computed. If this procedure is performed many times, the central limit theorem says that the distribution of the average will be closely approximated by a normal distribution. A simple example of this is that if one flips a coin many times the probability of getting a given number of heads in a series of flips will approach a normal curve, with mean equal to half the total number of flips in each series. (In the limit of an infinite number of flips, it will equal a normal curve.)\n\nThe central limit theorem has a number of variants. In its common form, the random variables must be identically distributed. In variants, convergence of the mean to the normal distribution also occurs for non-identical distributions or for non-independent observations, given that they comply with certain conditions.\n\nThe earliest version of this theorem, that the normal distribution may be used as an approximation to the binomial distribution, is now known as the de Moivre–Laplace theorem.\n\nIn more general usage, a central limit theorem is any of a set of weak-convergence theorems in probability theory. They all express the fact that a sum of many independent and identically distributed (i.i.d.) random variables, or alternatively, random variables with specific types of dependence, will tend to be distributed according to one of a small set of \"attractor distributions\". When the variance of the i.i.d. variables is finite, the attractor distribution is the normal distribution. In contrast, the sum of a number of i.i.d. random variables with power law tail distributions decreasing as where (and therefore having infinite variance) will tend to an alpha-stable distribution with stability parameter (or index of stability) of as the number of variables grows.\n\nLet } be a random sample of size — that is, a sequence of independent and identically distributed (i.i.d.) random variables drawn from a distribution of expected value given by and finite variance given by . Suppose we are interested in the sample average\n\nof these random variables. By the law of large numbers, the sample averages converge in probability and almost surely to the expected value as . The classical central limit theorem describes the size and the distributional form of the stochastic fluctuations around the deterministic number during this convergence. More precisely, it states that as gets larger, the distribution of the difference between the sample average and its limit , when multiplied by the factor (that is ), approximates the normal distribution with mean 0 and variance . For large enough , the distribution of is close to the normal distribution with mean and variance /. The usefulness of the theorem is that the distribution of approaches normality regardless of the shape of the distribution of the individual . Formally, the theorem can be stated as follows:\n\nLindeberg–Lévy CLT. Suppose } is a sequence of i.i.d. random variables with and . Then as approaches infinity, the random variables converge in distribution to a normal :\n\nIn the case , convergence in distribution means that the cumulative distribution functions of converge pointwise to the cdf of the distribution: for every real number ,\n\nwhere is the standard normal cdf evaluated at . Note that the convergence is uniform in in the sense that\n\nwhere sup denotes the least upper bound (or supremum) of the set.\n\nThe theorem is named after Russian mathematician Aleksandr Lyapunov. In this variant of the central limit theorem the random variables have to be independent, but not necessarily identically distributed. The theorem also requires that random variables have moments of some order , and that the rate of growth of these moments is limited by the Lyapunov condition given below.\n\nLyapunov CLT. Suppose } is a sequence of independent random variables, each with finite expected value and variance . Define\n\nIf for some , \"Lyapunov’s condition\"\n\nis satisfied, then a sum of converges in distribution to a standard normal random variable, as goes to infinity:\n\nIn practice it is usually easiest to check Lyapunov's condition for .\n\nIf a sequence of random variables satisfies Lyapunov's condition, then it also satisfies Lindeberg's condition. The converse implication, however, does not hold.\n\nIn the same setting and with the same notation as above, the Lyapunov condition can be replaced with the following weaker one (from Lindeberg in 1920).\n\nSuppose that for every \n\nwhere is the indicator function. Then the distribution of the standardized sums\n\nconverges towards the standard normal distribution .\n\nProofs that use characteristic functions can be extended to cases where each individual is a random vector in , with mean vector and covariance matrix (among the components of the vector), and these random vectors are independent and identically distributed. Summation of these vectors is being done componentwise. The multidimensional central limit theorem states that when scaled, sums converge to a multivariate normal distribution.\n\nLet\n\nbe the -vector. The bold in means that it is a random vector, not a random (univariate) variable. Then the sum of the random vectors will be\n\nand the average is\n\nand therefore\n\nThe multivariate central limit theorem states that\n\nwhere the covariance matrix is equal to\n\nThe rate of convergence is given by the following Berry–Esseen type result:\n\nTheorem. Let formula_16 be independent formula_17-valued random vectors, each having mean zero. Write formula_18 and assume formula_19 is invertible. Let formula_20 be a formula_21-dimensional Gaussian with the same mean and covariance matrix as formula_22. Then for all convex sets formula_23,\n\nwhere formula_25 is a universal constant, formula_26, and formula_27 denotes the Euclidean norm on formula_17.\nIt is unknown whether the factor formula_29 is necessary.\n\nThe central limit theorem states that the sum of a number of independent and identically distributed random variables with finite variances will tend to a normal distribution as the number of variables grows. A generalization due to Gnedenko and Kolmogorov states that the sum of a number of random variables with a power-law tail (Paretian tail) distributions decreasing as where (and therefore having infinite variance) will tend to a stable distribution as the number of summands grows. If then the sum converges to a stable distribution with stability parameter equal to 2, i.e. a Gaussian distribution.\n\nA useful generalization of a sequence of independent, identically distributed random variables is a mixing random process in discrete time; \"mixing\" means, roughly, that random variables temporally far apart from one another are nearly independent. Several kinds of mixing are used in ergodic theory and probability theory. See especially strong mixing (also called α-mixing) defined by where is so-called strong mixing coefficient.\n\nA simplified formulation of the central limit theorem under strong mixing is:\n\nTheorem. Suppose that is stationary and -mixing with and that and . Denote , then the limit\n\nexists, and if then converges in distribution to .\n\nIn fact,\n\nwhere the series converges absolutely.\n\nThe assumption cannot be omitted, since the asymptotic normality fails for where are another stationary sequence.\n\nThere is a stronger version of the theorem: the assumption is replaced with , and the assumption is replaced with\n\nExistence of such ensures the conclusion. For encyclopedic treatment of limit theorems under mixing conditions see .\n\nTheorem. Let a martingale satisfy\nthen converges in distribution to as .\n\n\"Caution:\" The restricted expectation should not be confused with the conditional expectation .\n\nFor a theorem of such fundamental importance to statistics and applied probability, the central limit theorem has a remarkably simple proof using characteristic functions. It is similar to the proof of the (weak) law of large numbers.\n\nAs stated above, suppose } are independent and identically distributed random variables, each with mean and finite variance . The sum has mean and variance . Consider the random variable\nwhere in the last step we defined the new random variables , each with zero mean and unit variance (). The characteristic function of is given by\n\nwhere in the last step we used the fact that all of the are identically distributed. The characteristic function of is, by Taylor's theorem,\n\nwhere is \"little notation\" for some function of that goes to zero more rapidly than . By the limit of the exponential function (), the characteristic function of equals\n\nNote that all of the higher order terms vanish in the limit . The right hand side equals the characteristic function of a standard normal distribution , which implies through Lévy's continuity theorem that the distribution of will approach as . Therefore, the sum will approach that of the normal distribution , and the sample average\n\nconverges to the normal distribution , from which the central limit theorem follows.\n\nThe central limit theorem gives only an asymptotic distribution. As an approximation for a finite number of observations, it provides a reasonable approximation only when close to the peak of the normal distribution; it requires a very large number of observations to stretch into the tails.\n\nThe convergence in the central limit theorem is uniform because the limiting cumulative distribution function is continuous. If the third central moment exists and is finite, then the speed of convergence is at least on the order of (see Berry–Esseen theorem). Stein's method can be used not only to prove the central limit theorem, but also to provide bounds on the rates of convergence for selected metrics.\n\nThe convergence to the normal distribution is monotonic, in the sense that the entropy of increases monotonically to that of the normal distribution.\n\nThe central limit theorem applies in particular to sums of independent and identically distributed discrete random variables. A sum of discrete random variables is still a discrete random variable, so that we are confronted with a sequence of discrete random variables whose cumulative probability distribution function converges towards a cumulative probability distribution function corresponding to a continuous variable (namely that of the normal distribution). This means that if we build a histogram of the realisations of the sum of independent identical discrete variables, the curve that joins the centers of the upper faces of the rectangles forming the histogram converges toward a Gaussian curve as approaches infinity, this relation is known as de Moivre–Laplace theorem. The binomial distribution article details such an application of the central limit theorem in the simple case of a discrete variable taking only two possible values.\n\nThe law of large numbers as well as the central limit theorem are partial solutions to a general problem: \"What is the limiting behaviour of as approaches infinity?\" In mathematical analysis, asymptotic series are one of the most popular tools employed to approach such questions.\n\nSuppose we have an asymptotic expansion of :\n\nDividing both parts by and taking the limit will produce , the coefficient of the highest-order term in the expansion, which represents the rate at which changes in its leading term.\n\nInformally, one can say: \" grows approximately as \". Taking the difference between and its approximation and then dividing by the next term in the expansion, we arrive at a more refined statement about :\n\nHere one can say that the difference between the function and its approximation grows approximately as . The idea is that dividing the function by appropriate normalizing functions, and looking at the limiting behavior of the result, can tell us much about the limiting behavior of the original function itself.\n\nInformally, something along these lines happens when the sum, , of independent identically distributed random variables, , is studied in classical probability theory. If each has finite mean , then by the law of large numbers, . If in addition each has finite variance , then by the central limit theorem,\n\nwhere is distributed as . This provides values of the first two constants in the informal expansion\n\nIn the case where the do not have finite mean or variance, convergence of the shifted and rescaled sum can also occur with different centering and scaling factors:\n\nor informally\n\nDistributions which can arise in this way are called \"stable\". Clearly, the normal distribution is stable, but there are also other stable distributions, such as the Cauchy distribution, for which the mean or variance are not defined. The scaling factor may be proportional to , for any ; it may also be multiplied by a slowly varying function of .\n\nThe law of the iterated logarithm specifies what is happening \"in between\" the law of large numbers and the central limit theorem. Specifically it says that the normalizing function , intermediate in size between of the law of large numbers and of the central limit theorem, provides a non-trivial limiting behavior.\n\nThe density of the sum of two or more independent variables is the convolution of their densities (if these densities exist). Thus the central limit theorem can be interpreted as a statement about the properties of density functions under convolution: the convolution of a number of density functions tends to the normal density as the number of density functions increases without bound. These theorems require stronger hypotheses than the forms of the central limit theorem given above. Theorems of this type are often called local limit theorems. See Petrov for a particular local limit theorem for sums of independent and identically distributed random variables.\n\nSince the characteristic function of a convolution is the product of the characteristic functions of the densities involved, the central limit theorem has yet another restatement: the product of the characteristic functions of a number of density functions becomes close to the characteristic function of the normal density as the number of density functions increases without bound, under the conditions stated above. Specifically, an appropriate scaling factor needs to be applied to the argument of the characteristic function.\n\nAn equivalent statement can be made about Fourier transforms, since the characteristic function is essentially a Fourier transform.\n\nLet be the sum of random variables. Many central limit theorems provide conditions such that converges in distribution to (the normal distribution with mean 0, variance 1) as . In some cases, it is possible to find a constant and function such that converges in distribution to as .\n\nLemma. Suppose formula_47 is a sequence of real-valued and strictly stationary random variables with formula_48 for all formula_49, formula_50, and formula_51. Construct\n\nThe logarithm of a product is simply the sum of the logarithms of the factors. Therefore, when the logarithm of a product of random variables that take only positive values approaches a normal distribution, the product itself approaches a log-normal distribution. Many physical quantities (especially mass or length, which are a matter of scale and cannot be negative) are the products of different random factors, so they follow a log-normal distribution. This multiplicative version of the central limit theorem is sometimes called Gibrat's law.\n\nWhereas the central limit theorem for sums of random variables requires the condition of finite variance, the corresponding theorem for products requires the corresponding condition that the density function be square-integrable.\n\nAsymptotic normality, that is, convergence to the normal distribution after appropriate shift and rescaling, is a phenomenon much more general than the classical framework treated above, namely, sums of independent random variables (or vectors). New frameworks are revealed from time to time; no single unifying framework is available for now.\n\nTheorem. There exists a sequence for which the following holds. Let , and let random variables have a log-concave joint density such that for all , and for all . Then the distribution of\n\nis -close to in the total variation distance.\n\nThese two -close distributions have densities (in fact, log-concave densities), thus, the total variance distance between them is the integral of the absolute value of the difference between the densities. Convergence in total variation is stronger than weak convergence.\n\nAn important example of a log-concave density is a function constant inside a given convex body and vanishing outside; it corresponds to the uniform distribution on the convex body, which explains the term \"central limit theorem for convex bodies\".\n\nAnother example: where and . If then factorizes into which means are independent. In general, however, they are dependent.\n\nThe condition ensures that are of zero mean and uncorrelated; still, they need not be independent, nor even pairwise independent. By the way, pairwise independence cannot replace independence in the classical central limit theorem.\n\nHere is a Berry–Esseen type result.\n\nTheorem. Let satisfy the assumptions of the previous theorem, then \n\nfor all ; here is a universal (absolute) constant. Moreover, for every such that ,\n\nThe distribution of need not be approximately normal (in fact, it can be uniform). However, the distribution of is close to (in the total variation distance) for most vectors according to the uniform distribution on the sphere .\n\nTheorem (Salem–Zygmund): Let be a random variable distributed uniformly on , and , where\nThen\n\nconverges in distribution to .\n\nTheorem: Let be independent random points on the plane each having the two-dimensional standard normal distribution. Let be the convex hull of these points, and the area of Then\n\nconverges in distribution to as tends to infinity.\n\nThe same also holds in all dimensions greater than 2.\n\nThe polytope is called a Gaussian random polytope.\n\nA similar result holds for the number of vertices (of the Gaussian polytope), the number of edges, and in fact, faces of all dimensions.\n\nA linear function of a matrix is a linear combination of its elements (with given coefficients), where is the matrix of the coefficients; see Trace (linear algebra)#Inner product.\n\nA random orthogonal matrix is said to be distributed uniformly, if its distribution is the normalized Haar measure on the orthogonal group ; see Rotation matrix#Uniform random rotation matrices.\n\nTheorem. Let be a random orthogonal matrix distributed uniformly, and a fixed matrix such that , and let . Then the distribution of is close to in the total variation metric up to .\n\nTheorem. Let random variables be such that weakly in and weakly in . Then there exist integers such that\nconverges in distribution to as tends to infinity.\n\nThe central limit theorem may be established for the simple random walk on a crystal lattice (an infinite-fold abelian covering graph over a finite graph), and is used for design of crystal structures.\n\nA simple example of the central limit theorem is rolling a large number of identical, unbiased dice. The distribution of the sum (or average) of the rolled numbers will be well approximated by a normal distribution. Since real-world quantities are often the balanced sum of many unobserved random events, the central limit theorem also provides a partial explanation for the prevalence of the normal probability distribution. It also justifies the approximation of large-sample statistics to the normal distribution in controlled experiments.\n\nPublished literature contains a number of useful and interesting examples and applications relating to the central limit theorem. One source states the following examples:\n\nFrom another viewpoint, the central limit theorem explains the common appearance of the \"bell curve\" in density estimates applied to real world data. In cases like electronic noise, examination grades, and so on, we can often regard a single measured value as the weighted average of a large number of small effects. Using generalisations of the central limit theorem, we can then see that this would often (though not always) produce a final distribution that is approximately normal.\n\nIn general, the more a measurement is like the sum of independent variables with equal influence on the result, the more normality it exhibits. This justifies the common use of this distribution to stand in for the effects of unobserved variables in models like the linear model.\n\nRegression analysis and in particular ordinary least squares specifies that a dependent variable depends according to some function upon one or more independent variables, with an additive error term. Various types of statistical inference on the regression assume that the error term is normally distributed. This assumption can be justified by assuming that the error term is actually the sum of a large number of independent error terms; even if the individual error terms are not normally distributed, by the central limit theorem their sum can be well approximated by a normal distribution.\n\nGiven its importance to statistics, a number of papers and computer packages are available that demonstrate the convergence involved in the central limit theorem.\n\nDutch mathematician Henk Tijms writes:\n\nSir Francis Galton described the Central Limit Theorem in this way:\n\nThe actual term \"central limit theorem\" (in German: \"zentraler Grenzwertsatz\") was first used by George Pólya in 1920 in the title of a paper. Pólya referred to the theorem as \"central\" due to its importance in probability theory. According to Le Cam, the French school of probability interprets the word \"central\" in the sense that \"it describes the behaviour of the centre of the distribution as opposed to its tails\". The abstract of the paper \"On the central limit theorem of calculus of probability and the problem of moments\" by Pólya in 1920 translates as follows.\n\nA thorough account of the theorem's history, detailing Laplace's foundational work, as well as Cauchy's, Bessel's and Poisson's contributions, is provided by Hald. Two historical accounts, one covering the development from Laplace to Cauchy, the second the contributions by von Mises, Pólya, Lindeberg, Lévy, and Cramér during the 1920s, are given by Hans Fischer. Le Cam describes a period around 1935. Bernstein presents a historical discussion focusing on the work of Pafnuty Chebyshev and his students Andrey Markov and Aleksandr Lyapunov that led to the first proofs of the CLT in a general setting.\n\nThrough the 1930s, progressively more general proofs of the Central Limit Theorem were presented. Many natural systems were found to exhibit Gaussian distributions — a typical example being height distributions for humans. When statistical methods such as analysis of variance became established in the early 1900s, it became increasingly common to assume underlying Gaussian distributions.\n\nA curious footnote to the history of the Central Limit Theorem is that a proof of a result similar to the 1922 Lindeberg CLT was the subject of Alan Turing's 1934 Fellowship Dissertation for King's College at the University of Cambridge. Only after submitting the work did Turing learn it had already been proved. Consequently, Turing's dissertation was not published.\n\n\n\n"}
{"id": "53165511", "url": "https://en.wikipedia.org/wiki?curid=53165511", "title": "Characterization of probability distributions", "text": "Characterization of probability distributions\n\nIn mathematics in general, a characterization theorem says that a particular object – a function, a space, etc. – is the only one that possesses properties specified in the theorem. A characterization of a probability distribution accordingly states that it is the only probability distribution that satisfies specified conditions. More precisely, the model of characterization of\nprobability distribution was described by V.M. Zolotarev in such manner. On the probability space we define the space formula_1 of random variables with values in measurable metric space formula_2 and the space formula_3 of random variables with values in measurable metric space formula_4. By characterizations of probability distributions we understand general problems of description of some set formula_5 in the space formula_6 by extracting the sets formula_7 and formula_8 which describe the properties of random variables formula_9 and their images formula_10, obtained by means of a specially chosen mapping formula_11.\n<br>The description of the properties of the random variables formula_12 and of their images formula_13 is equivalent to the indication of the set formula_7 from which formula_12 must be taken and of the set formula_8 into which its image must fall. So, the set which interests us appears therefore in the following form:\n\nwhere formula_18 denotes the complete inverse image of formula_19 in formula_20. This is the general model of characterization of probability distribution. Some examples of characterization theorems:\n\n\n<br>\nVerification of conditions of characterization theorems in practice is possible only with some error formula_45, i.e., only to a certain degree of accuracy. Such a situation is observed, for instance, in the cases where a sample of finite size is considered. That is why there arises the following natural question. Suppose that the conditions of the characterization theorem are fulfilled not exactly but only approximately. May we assert that the conclusion of the theorem is also fulfilled approximately? The theorems in which the problems of this kind are considered are called stability characterizations of probability distributions.\n\n"}
{"id": "682693", "url": "https://en.wikipedia.org/wiki?curid=682693", "title": "Cohomotopy group", "text": "Cohomotopy group\n\nIn mathematics, particularly algebraic topology, cohomotopy sets are particular contravariant functors from the category of pointed topological spaces and point-preserving continuous maps to the category of sets and functions. They are dual to the homotopy groups, but less studied.\n\nThe \"p\"-th cohomotopy set of a pointed topological space \"X\" is defined by\n\nthe set of pointed homotopy classes of continuous mappings from formula_2 to the \"p\"-sphere formula_3. For \"p=1\" this set has an abelian group structure, and, provided formula_2 is a CW-complex, is isomorphic to the first cohomology group formula_5, since the circle formula_6 is an Eilenberg–MacLane space of type formula_7. In fact, it is a theorem of Heinz Hopf that if formula_2 is a CW-complex of dimension at most \"p\", then formula_9 is in bijection with the \"p\"-th cohomology group formula_10.\n\nThe set formula_9 also has a natural group structure if formula_2 is a suspension formula_13, such as a sphere formula_14 for formula_15.\n\nIf \"X\" is not homotopy equivalent to a CW-complex, then formula_5 might not be isomorphic to formula_17. A counterexample is given by the Warsaw circle, whose first cohomology group vanishes, but admits a map to formula_6 which is not homotopic to a constant map \n\nSome basic facts about cohomotopy sets, some more obvious than others:\n\n"}
{"id": "4415007", "url": "https://en.wikipedia.org/wiki?curid=4415007", "title": "Complex-valued function", "text": "Complex-valued function\n\nIn mathematics, a complex-valued function (not to be confused with complex variable function) is a function whose values are complex numbers. Its domain does not necessarily have any structure related to complex numbers. Most important uses of such functions in complex analysis and in functional analysis are explained below.\n\nA vector space and a commutative algebra of functions over complex numbers can be defined in the same way as for real-valued functions. Also, any complex-valued function on an arbitrary set can be considered as an ordered pair of two real-valued functions: or, alternatively, as a real-valued function on (the disjoint union of two copies of ) such that for any :\nSome properties of complex-valued functions (such as measurability and continuity) are nothing more than corresponding properties of real-valued functions.\n\nComplex analysis considers holomorphic functions on complex manifolds, such as Riemann surfaces. The property of analytic continuation makes them very dissimilar from smooth functions, for example. Namely, if a function defined in a neighborhood can be continued to a wider domain, then this continuation is unique.\n\nAs real functions, any holomorphic function is infinitely smooth and analytic. But there is much less freedom in construction of a holomorphic function than in one of a smooth function.\nComplex-valued L spaces on sets with a measure have a particular importance because they are Hilbert spaces. They often appear in functional analysis (for example, in relation with Fourier transform) and operator theory. A major user of such spaces is quantum mechanics, as wave functions.\n\nThe sets on which the complex-valued L is constructed have the potential to be more exotic than their real-valued analog. For example, complex-valued function spaces are used in some branches of -adic analysis for algebraic reasons: complex numbers form an algebraically closed field (which facilitates operator theory), whereas neither real numbers nor -adic numbers are not.\n\nAlso, complex-valued continuous functions are an important example in the theory of C*-algebras: see Gelfand representation.\n"}
{"id": "6206", "url": "https://en.wikipedia.org/wiki?curid=6206", "title": "Computable number", "text": "Computable number\n\nIn mathematics, computable numbers are the real numbers that can be computed to within any desired precision by a finite, terminating algorithm. They are also known as the recursive numbers or the computable reals or recursive reals.\n\nEquivalent definitions can be given using μ-recursive functions, Turing machines, or λ-calculus as the formal representation of algorithms. The computable numbers form a real closed field and can be used in the place of real numbers for many, but not all, mathematical purposes.\n\nIn the following, Marvin Minsky defines the numbers to be computed in a manner similar to those defined by Alan Turing in 1936; i.e., as \"sequences of digits interpreted as decimal fractions\" between 0 and 1:\n\nThe key notions in the definition are (1) that some \"n\" is specified at the start, (2) for any \"n\" the computation only takes a finite number of steps, after which the machine produces the desired output and terminates.\n\nAn alternate form of (2) – the machine successively prints all n of the digits on its tape, halting after printing the n – emphasizes Minsky's observation: (3) That by use of a Turing machine, a \"finite\" definition – in the form of the machine's table – is being used to define what is a potentially-\"infinite\" string of decimal digits.\n\nThis is however not the modern definition which only requires the result be accurate to within any given accuracy. The informal definition above is subject to a rounding problem called the table-maker's dilemma whereas the modern definition is not.\n\nA real number \"a\" is computable if it can be approximated by some computable function formula_1 in the following manner: given any positive integer \"n\", the function produces an integer \"f\"(\"n\") such that:\n\nThere are two similar definitions that are equivalent:\n\nThere is another equivalent definition of computable numbers via computable Dedekind cuts. A computable Dedekind cut is a computable function formula_8 which when provided with a rational number formula_9 as input returns formula_10 or formula_11, satisfying the following conditions:\nAn example is given by a program \"D\" that defines the cube root of 3. Assuming formula_15 this is defined by:\n\nA real number is computable if and only if there is a computable Dedekind cut \"D\" corresponding to it. The function \"D\" is unique for each computable number (although of course two different programs may provide the same function).\n\nA complex number is called computable if its real and imaginary parts are computable.\n\nWhile the set of real numbers is uncountable, the set of computable numbers is only countable and thus almost all real numbers are not computable. That the computable numbers are at most countable intuitively comes from the fact that they are produced by Turing machines, of which there are only countably many. More precisely, assigning a Gödel number to each Turing machine definition produces a subset formula_18 of the natural numbers corresponding to the computable numbers and identifies a surjection from formula_18 to the computable numbers, which shows that the computable numbers are subcountable. Moreover, for any computable number formula_20 the well ordering principle provides that there is a minimal element in formula_18 which corresponds to formula_22, and therefore there exists a subset formula_23 consisting of the minimal elements, on which the map is a bijection. The inverse of this bijection is an injection into the natural numbers of the computable numbers, proving that they are countable.\n\nThe set formula_18 of these Gödel numbers, however, is not computably enumerable (nor consequently is formula_25), even though the computable reals are themselves ordered. This is because there is no algorithm to determine which Gödel numbers correspond to Turing machines that produce computable reals. In order to produce a computable real, a Turing machine must compute a total function, but the corresponding decision problem is in Turing degree 0′′. Consequently, there is no surjective computable function from the natural numbers to the computable reals, and Cantor's diagonal argument cannot be used constructively to demonstrate uncountably many of them.\n\nThe arithmetical operations on computable numbers are themselves computable in the sense that whenever real numbers \"a\" and \"b\" are computable then the following real numbers are also computable: \"a + b\", \"a - b\", \"ab\", and \"a/b\" if \"b\" is nonzero.\nThese operations are actually \"uniformly computable\"; for example, there is a Turing machine which on input (\"A\",\"B\",formula_26) produces output \"r\", where \"A\" is the description of a Turing machine approximating \"a\", \"B\" is the description of a Turing machine approximating \"b\", and \"r\" is an formula_26 approximation of \"a\"+\"b\".\n\nThe fact that computable real numbers form a field was first proved by Henry Gordon Rice (1954).\n\nComputable reals do not form however a computable field, because the definition of the latter notion requires effective equality.\n\nThe order relation on the computable numbers is not computable. Let \"A\" be the description of a Turing machine approximating the number formula_6. Then there is no Turing machine which on input \"A\" outputs \"YES\" if formula_29 and \"NO\" if formula_30 To see why, suppose the machine described by \"A\" keeps outputting 0 as formula_26 approximations. It is not clear how long to wait before deciding that the machine will \"never\" output an approximation which forces \"a\" to be positive. Thus the machine will eventually have to guess that the number will equal 0, in order to produce an output; the sequence may later become different from 0. This idea can be used to show that the machine is incorrect on some sequences if it computes a total function. A similar problem occurs when the computable reals are represented as Dedekind cuts. The same holds for the equality relation : the equality test is not computable.\n\nWhile the full order relation is not computable, the restriction of it to pairs of unequal numbers is computable. That is, there is a program that takes as input two Turing machines \"A\" and \"B\" approximating numbers \"a\" and \"b\", where \"a\" ≠ \"b\", and outputs whether formula_32 or formula_33 It is sufficient to use \"ε\"-approximations where formula_34 so by taking increasingly small ε (approaching 0), one eventually can decide whether formula_32 or formula_33\n\nThe computable real numbers do not share all the properties of the real numbers used in analysis. For example, the least upper bound of a bounded increasing computable sequence of computable real numbers need not be a computable real number (Bridges and Richman, 1987:58). A sequence with this property is known as a Specker sequence, as the first construction is due to E. Specker (1949). Despite the existence of counterexamples such as these, parts of calculus and real analysis can be developed in the field of computable numbers, leading to the study of computable analysis.\n\nEvery computable number is definable, but not vice versa. There are many definable, noncomputable real numbers, including:\nBoth of these examples in fact define an infinite set of definable, uncomputable numbers, one for each Universal Turing machine.\nA real number is computable if and only if the set of natural numbers it represents (when written in binary and viewed as a characteristic function) is computable.\n\nEvery computable number is arithmetical.\n\nThe set of computable real numbers (as well as every countable, densely ordered subset of computable reals without ends) is order-isomorphic to the set of rational numbers.\n\nTuring's original paper defined computable numbers as follows:\n\nTuring was aware that this definition is equivalent to the formula_26-approximation definition given above. The argument proceeds as follows: if a number is computable in the Turing sense, then it is also computable in the formula_26 sense: if formula_42, then the first \"n\" digits of the decimal expansion for \"a\" provide an formula_26 approximation of \"a\". For the converse, we pick an formula_26 computable real number \"a\" and generate increasingly precise approximations until the \"n\"th digit after the decimal point is certain. This always generates a decimal expansion equal to \"a\" but it may improperly end in an infinite sequence of 9's in which case it must have a finite (and thus computable) proper decimal expansion.\n\nUnless certain topological properties of the real numbers are relevant it is often more convenient to deal with elements of formula_45 (total 0,1 valued functions) instead of reals numbers in formula_46. The members of formula_45 can be identified with binary decimal expansions but since the decimal expansions formula_48 and formula_49 denote the same real number the interval formula_46 can only be bijectively (and homeomorphically under the subset topology) identified with the subset of formula_45 not ending in all 1's.\n\nNote that this property of decimal expansions means it's impossible to effectively identify computable real numbers defined in terms of a decimal expansion and those defined in the formula_26 approximation sense. Hirst has shown there is no algorithm which takes as input the description of a Turing machine which produces formula_26 approximations for the computable number \"a\", and produces as output a Turing machine which enumerates the digits of \"a\" in the sense of Turing's definition (see Hirst 2007). Similarly it means that the arithmetic operations on the computable reals are not effective on their decimal representations as when adding decimal numbers, in order to produce one digit it may be necessary to look arbitrarily far to the right to determine if there is a carry to the current location. This lack of uniformity is one reason that the contemporary definition of computable numbers uses formula_26 approximations rather than decimal expansions.\n\nHowever, from a computational or measure theoretic perspective the two structures formula_45 and formula_46 are essentially identical, and computability theorists often refer to members of formula_45 as reals. While formula_46 formula_45 is totally disconnected for questions about formula_60 classes or randomness it's much less messy to work in formula_45.\n\nElements of formula_62 are sometimes called reals as well and though containing a homeomorphic image of formula_63 formula_62 in addition to being totally disconnected isn't even locally compact. This leads to genuine differences in the computational properties. For instance the formula_65 satisfying formula_66 with formula_67 quantifier free must be computable while the unique formula_68 satisfying a universal formula can be arbitrarily high in the hyperarithmetic hierarchy.\n\nThe computable numbers include many of the specific real numbers which appear in practice, including all real algebraic numbers, as well as \"e\", formula_69, and many other transcendental numbers. Though the computable reals exhaust those reals we can calculate or approximate, the assumption that all reals are computable leads to substantially different conclusions about the real numbers. The question naturally arises of whether it is possible to dispose of the full set of reals and use computable numbers for all of mathematics. This idea is appealing from a constructivist point of view, and has been pursued by what Bishop and Richman call the \"Russian school\" of constructive mathematics.\n\nTo actually develop analysis over computable numbers, some care must be taken. For example, if one uses the classical definition of a sequence, the set of computable numbers is not closed under the basic operation of taking the supremum of a bounded sequence (for example, consider a Specker sequence). This difficulty is addressed by considering only sequences which have a computable modulus of convergence. The resulting mathematical theory is called computable analysis.\n\nThere are some computer packages that work with computable real numbers, representing the real numbers as programs computing approximations. One example is the RealLib package.\n\n\n\nComputable numbers were defined independently by Turing, Post and Church. See \"The Undecidable\", ed. Martin Davis, for further original papers.\n"}
{"id": "23170285", "url": "https://en.wikipedia.org/wiki?curid=23170285", "title": "Conrad Wolfram", "text": "Conrad Wolfram\n\nConrad Wolfram (born 10 June 1970) is a British technologist and businessman known for his work in information technology and its application. In 2012, The Observer placed him at number 11 in its list of Britain's 50 New Radicals.\n\nWolfram's father Hugo Wolfram was a textile manufacturer and novelist (\"Into a Neutral Country\") and his mother Sybil Wolfram was a professor of philosophy at the University of Oxford. He is the younger brother of Stephen Wolfram.\n\nBorn in Oxford, England, in 1970, Wolfram was educated at Dragon School, Eton College and Pembroke College, Cambridge, from which he holds an MA degree in Natural Sciences and Mathematics. He learned to program on a BBC Micro. He is married to primary care ophthalmology consultant Stella Hornby and has a daughter Sophia Wolfram.\n\nWolfram has been a proponent of 'Computer-Based Math'—a reform of mathematics education to \"rebuild the curriculum assuming computers exist.\"\n\nHe argues, \"There are a few cases where it is important to do calculations by hand, but these are small fractions of cases. The rest of the time you should assume that students should use a computer just like everyone does in the real world.\". And that \"School mathematics is very disconnected from mathematics used to solve problems in the real world\". In an interview with the Guardian he described the replacement of hand calculation by computer use as \"democratising expertise\". He argues that \"A good guide to how and what you should do with a computer in the classroom is what you'd do with it outside. As much as possible, use real-world tools in the classroom in an open-ended way not special education-only closed-ended approaches.\" \n\nIn 2009, he spoke about education reform at the TEDx Conference at the EU Parliament. and again at TED Global 2010 where he argued that \"Maths should be more practical and more conceptual, but less mechanical,\" and that \"Calculating is the machinery of math - a means to an end.\"\n\nIn August 2012, he was a member of the judging panel at the Festival of Code, the culmination of Young Rewired State 2012. Wolfram is also part of Flooved advisory board.\n\nConrad Wolfram founded Wolfram Research Europe Ltd. in 1991 and remains its CEO. In 1996, he additionally became Strategic and International Director of Wolfram Research, Inc., making him also responsible for Wolfram Research Asia Ltd, and communications such as the wolfram.com website.\n\nWolfram Research was founded by his brother Stephen Wolfram, the maker of Mathematica software and the Wolfram Alpha knowledge engine.\n\nConrad Wolfram has led the effort to move the use of Mathematica from pure computation system to development and deployment engine, instigating technology such as the Mathematica Player family and web Mathematica and by pushing greater automation within the system.\n\nHe has also led the focus on interactive publishing technology with the stated aim of \"making new applications as everyday as new documents\" claiming that \"If a picture is worth a thousand words, an interactive document is worth a thousand pictures.\" These technologies converged to form the Computable Document Format which Wolfram says can \"transfer knowledge in a much higher-bandwidth way\".\n\n"}
{"id": "29462016", "url": "https://en.wikipedia.org/wiki?curid=29462016", "title": "Daniel Biss", "text": "Daniel Biss\n\nDaniel Kálmán Biss (born August 27, 1977) is an American politician serving as the member of the Illinois Senate for the 9th district since January 2013. The district includes Chicago's northern suburbs, including Evanston, Glencoe, Glenview, Morton Grove, Northbrook, Northfield, Skokie, Wilmette, and Winnetka. Biss first ran for office in 2008 and was a member of the Illinois House of Representatives from 2011 to 2013. He was also a candidate in the Democratic Party primary for Governor of Illinois in the 2018 election.\n\nPrior to pursuing a political career, Biss was an Assistant Professor of Mathematics at the University of Chicago from 2002 to 2008.\n\nBiss was born into a Jewish Israeli family of musicians: his brother is the noted pianist Jonathan Biss, his parents are the violinists Paul Biss and Miriam Fried, and his grandmother was the Russian-born cellist Raya Garbousova.\n\nBiss received an undergraduate degree from Harvard University, graduating \"summa cum laude\" in 1998, and a Ph.D. at MIT in 2002, both in mathematics. He won the 1999 Morgan Prize for outstanding research as an undergraduate, and was a Clay Research Fellow from 2002 to 2007. His doctoral advisor was Michael J. Hopkins. He was a visiting scholar at the Institute for Advanced Study in the fall of 2003.\n\nPrior to full-time pursuit of a political career, Biss was an Assistant Professor of Mathematics at the University of Chicago from 2002 to 2008.\n\nBiss wrote the key mathematical formula and an explanatory appendix for the popular 2006 young adult novel \"An Abundance of Katherines,\" by Vlogbrothers co-creator and author John Green, and is a close friend of the author. The appendix explains the mathematics concepts used in the plot, and a book reviewer for the American Mathematical Society called it well written, saying \"Biss does a stand-up job of communicating the basic concepts underlying the math woven throughout the novel. It is natural for the reader to wonder to what extent Biss himself associates with Colin [the book's main character] and to what extent the relationship between Colin and Hassan approximate that of Green and Biss.\"\n\nAt least four of the mathematics papers that Biss published in academic journals were later discovered to contain major errors. Mathematician Nikolai Mnëv published a report in 2007 that there was a \"serious flaw\" in two of Biss' works published in \"Annals of Mathematics\" and \"Advances in Mathematics\" in 2003, saying \"unfortunately this simple mistake destroys the main theorems of both papers\". In 2008 and 2009, Biss acknowledged the flaw and published erratum reports for the two papers, thanking Mnëv for drawing his attention to the error. He and a co-author also acknowledged in 2009 that there was a \"fatal error\" in a paper they had published in \"Inventiones Mathematicae\" in 2006, thanking mathematicians Masatoshi Sato and Tom Church for helping to explain the problem. Another of his papers published in \"Topology and its Applications\" was formally retracted by the publisher in 2017, fifteen years after its 2002 publication, with the journal saying \"This article has been retracted at the request of the Editors-in-Chief after receiving a complaint about anomalies in this paper. The editors solicited further independent reviews which indicated that the definitions in the paper are ambiguous and most results are false. The author was contacted and does not dispute these findings.\" The journal said they had identified twelve specific errors in the paper, but clarified that they had concluded that the paper's findings were merely inaccurate, not fraudulent. When contacted by the journal, Biss had responded saying \"Thank you for writing. I am no longer in mathematics and so don't feel equipped to fully evaluate these claims. I certainly do not dispute them. If you would like to publish a retraction to that effect, that would seem to me to be an appropriate approach.\"\n\nWhen the 2017 retraction and the previously identified errors were reported by the \"Chicago Sun-Times\" in September 2017, his campaign blamed operatives for the perceived front-runner for the Democratic Party candidate for governor of Illinois, J. B. Pritzker, for raising it as a political issue. They said \"Whether it was training at MIT or the University of Chicago, Daniel has had dozens of academic papers reviewed by his peers and published. In a few cases, further research has found that the case posited in the original article didn't stand up, and he revised his findings.\" They referred to the raising of the issue as \"silly opposition research\".\n\n\nAccording to a 2008 Political Courage Test, Daniel Biss supports carbon emissions limits. Biss is pro-choice, supporting legal abortion. He supports allowing high school graduates to pay in-state tuition at public universities regardless of immigration status, as well as state funding to raise the salaries of teachers. He received a 7% rating by the NRA in 2010. Biss has expressed support of labor unions and has received $20,000 from AFSCME, the second largest donation to a state legislator. Biss also supports legalizing marijuana in Illinois.\n\nIn 2013, Biss cosponsored SB 1, a bill that aimed to limit the annual growth of retirement annuities within state employee's pension plans in an attempt to reduce debts in the state retirement system. In May 2015, the Illinois Supreme Court found the law unconstitutional. In rejecting the constitutionality of SB-1, the Illinois Supreme Court stated: \"These modifications to pension benefits unquestionably diminish the value of the retirement annuities the members…were promised when they joined the pension system. Accordingly, based on the plain language of the Act, these annuity-reducing provisions contravene the pension protection clause’s absolute prohibition against diminishment of pension benefits and exceed the General Assembly’s authority,\" the ruling states. Later, Biss acknowledged that his work on SB1 was an error saying, \"I decided this was the least bad of the bad options. I allowed myself to think we couldn't do better.\" Biss is now in support of fully honoring the pension payments by instituting progressive tax reforms to fully fund them.\n\nIn March 2017, Biss sponsored SB1424, a bill proposing a system of matching state funds for small-donor political contributions and SB 780, a bill proposing to elect a number of statewide offices by ranked-choice ballot. He also co-sponsored SB 1933, a bill by State Sen. Andy Manar to allow for automatic voter registration when applying for an Illinois drivers’ license.\n\nBiss supports universal health care and advocates specifically for a state-level single-payer healthcare system. In June 2017, Biss voted to reinforce the Affordable Care Act in Illinois by preventing insurance companies from discriminating against customers with pre-existing conditions.\n\nBiss ran for a seat in the Illinois State House of Representatives in 2008, losing to Republican Elizabeth Coulson. Starting in 2009, he then worked as a policy adviser to Rod Blagojevich, the Democratic governor of Illinois.\nOn November 10, 2011, Biss announced his intent to run for the Illinois Senate seat held by retiring Senator Jeffrey Schoenberg. He won the election on November 6, 2012, receiving over 66% of the vote.\n\nBiss announced a run for Illinois Comptroller in the 2016 special election but dropped out and endorsed opponent Susana Mendoza.\n\nOn March 20, 2017, Biss announced his candidacy for the Democratic nomination for Governor of Illinois for the 2018 election on a Facebook Live video, attacking incumbent governor Bruce Rauner and Illinois House Speaker Mike Madigan. Biss joined a growing field of Democratic contenders, including businessman Chris Kennedy and Chicago alderman Ameya Pawar.\n\nBiss briefly named Chicago alderman and Democratic Socialists of America member Carlos Ramirez-Rosa as his gubernatorial running mate, but dropped him from the ticket after just six days after Ramirez-Rosa expressed some support for the Boycott, Divestment and Sanctions boycott movement against Israel. Biss later announced his selection of Rockford-based state representative Litesa Wallace, a single mother and former social worker.\n\nBiss had been endorsed by many of his colleagues in the Illinois General Assembly, high profile academics and activists including Nobel laureate Richard Thaler and presidential candidate Lawrence Lessig, National Nurses United, the largest organization of registered nurses in the United States, and Our Revolution, the successor organization to Bernie Sanders' 2016 presidential campaign. Biss received two-thirds of preferential votes from Illinois members of the progressive advocacy group MoveOn.org. \n\nOn March 20, 2018, Biss lost the Democratic primary to J.B. Pritzker. He earned 26.70% of the total vote, behind Prizker with 45.13% and ahead of C.G. Kennedy with 24.37%. \nBiss carried two counties, McLean and Champaign.\n\nOn September 18, 2018, Biss announced in an email to supporters that he has accepted the position of executive director of the nonprofit Rust Belt Rising, which aims to train and support Democratic candidates in the Great Lakes states. Biss has stated he intends to fully serve out his state senate term, which expires in January 2019.\n\n\n"}
{"id": "320283", "url": "https://en.wikipedia.org/wiki?curid=320283", "title": "Delta encoding", "text": "Delta encoding\n\nDelta encoding is a way of storing or transmitting data in the form of \"differences\" (deltas) between sequential data rather than complete files; more generally this is known as data differencing. Delta encoding is sometimes called delta compression, particularly where archival histories of changes are required (e.g., in revision control software).\n\nThe differences are recorded in discrete files called \"deltas\" or \"diffs\". In situations where differences are small – for example, the change of a few words in a large document or the change of a few records in a large table – delta encoding greatly reduces data redundancy. Collections of unique deltas are substantially more space-efficient than their non-encoded equivalents.\n\nFrom a logical point of view the difference between two data values is the information required to obtain one value from the other – see relative entropy. The difference between identical values (under some equivalence) is often called \"0\" or the neutral element.\n\nPerhaps the simplest example is storing values of bytes as differences (deltas) between sequential values, rather than the values themselves. So, instead of 2, 4, 6, 9, 7, we would store 2, 2, 2, 3, −2. This reduces the variance (range) of the values when neighbor samples are correlated, enabling a lower bit usage for the same data. IFF 8SVX sound format applies this encoding to raw sound data before applying compression to it. Unfortunately, not even all 8-bit sound samples compress better when delta encoded, and the usability of delta encoding is even smaller for 16-bit and better samples. Therefore, compression algorithms often choose to delta encode only when the compression is better than without. However, in video compression, delta frames can considerably reduce frame size and are used in virtually every video compression codec.\n\nA delta can be defined in 2 ways, \"symmetric delta\" and \"directed delta\". A \"symmetric delta\" can be expressed as:formula_1where formula_2 and formula_3 represent two versions.\n\nA \"directed delta\", also called a change, is a sequence of (elementary) change operations which, when applied to one version formula_2, yields another version formula_3 (note the correspondence to transaction logs in databases).\n\nA variation of delta encoding which encodes differences between the prefixes or suffixes of strings is called incremental encoding. It is particularly effective for sorted lists with small differences between strings, such as a list of words from a dictionary.\n\nThe nature of the data to be encoded influences the effectiveness of a particular compression algorithm.\n\nDelta encoding performs best when data has small or constant variation; for an unsorted data set, there may be little to no compression possible with this method.\n\nIn delta encoded transmission over a network where only a single copy of the file is available at each end of the communication channel, special error control codes are used to detect which parts of the file have changed since its previous version.\nFor example, rsync uses a rolling checksum algorithm based on Mark Adler's adler-32 checksum.\n\nThe following C code performs a simple form of delta encoding and decoding: \n\nAnother instance of use of delta encoding is RFC 3229, \"Delta encoding in HTTP\", which proposes that HTTP servers should be able to send updated Web pages in the form of differences between versions (deltas), which should decrease Internet traffic, as most pages change slowly over time, rather than being completely rewritten repeatedly:\n\n\"Delta copying\" is a fast way of copying a file that is partially changed, when a previous version is present on the destination location. With delta copying, only the changed part of a file is copied. It is usually used in backup or file copying software, often to save bandwidth when copying between computers over a private network or the internet.\n\nMany of the online backup services adopt this methodology, often known simply as \"deltas\", in order to give their users previous versions of the same file from previous backups. This reduces associated costs, not only in the amount of data that has to be stored as differing versions (as the whole of each changed version of a file has to be offered for users to access), but also those costs in the uploading (and sometimes the downloading) of each file that has been updated (by just the smaller delta having to be used, rather than the whole file).\n\nThe Git source code control system employs delta compression in an auxiliary \"git repack\" operation. Objects in the repository that have not yet been delta-compressed (\"loose objects\") are compared against a heuristically chosen subset of all other objects, and the common data and differences are concatenated into a \"pack file\" which is then compressed using conventional methods. In common use cases, where source or data files are changed incrementally between commits, this can result in significant space savings. The repack operation is typically performed as part of the \"git gc\" process, which is triggered automatically when the numbers of loose objects or pack files exceed configured thresholds.\n\nOne general format for delta encoding is VCDIFF, described in RFC 3284. Free software implementations include Xdelta and open-vcdiff.\n\nGeneric Diff Format (GDIFF) is another delta encoding format. It was submitted to W3C in 1997. In many cases, VCDIFF has better compression rate than GDIFF.\n\nDiff is a file comparison program, which is mainly used for text files.\n\nBsdiff is a binary diff program using suffix sorting.\n\n\n"}
{"id": "1758864", "url": "https://en.wikipedia.org/wiki?curid=1758864", "title": "Derived type", "text": "Derived type\n\nIn computer science, derived type can mean:\n\n"}
{"id": "12181958", "url": "https://en.wikipedia.org/wiki?curid=12181958", "title": "Eikonal approximation", "text": "Eikonal approximation\n\nIn theoretical physics, the eikonal approximation (Greek εἰκών for likeness, icon or image) is an approximative method useful in wave scattering equations which occur in optics, seismology, quantum mechanics, quantum electrodynamics, and partial wave expansion.\n\nThe main advantage that the eikonal approximation offers is that the equations reduce to a differential equation in a single variable. This reduction into a single variable is the result of the straight line approximation or the eikonal approximation which allows us to choose the straight line as a special direction.\n\nThe early steps involved in the eikonal approximation in quantum mechanics are very closely related to the WKB approximation for one-dimensional waves. The WKB method, like the eikonal approximation, reduces the equations into a differential equation in a single variable. But the difficulty with the WKB approximation is that this variable is described by the trajectory of the particle which, in general, is complicated.\n\nMaking use of WKB approximation we can write the wave function of the scattered system in terms of action \"S\":\n\nInserting the wavefunction Ψ in the Schrödinger equation without the presence of a magnetic field we obtain\n\nWe write \"S\" as a power series in \"ħ\"\n\nFor the zero-th order:\n\nIf we consider the one-dimensional case then formula_6.\n\nWe obtain a differential equation with the boundary condition:\n\nfor formula_8, formula_9.\n"}
{"id": "1284226", "url": "https://en.wikipedia.org/wiki?curid=1284226", "title": "Elementary class", "text": "Elementary class\n\nIn model theory, a branch of mathematical logic, an elementary class (or axiomatizable class) is a class consisting of all structures satisfying a fixed first-order theory.\n\nA class \"K\" of structures of a signature σ is called an elementary class if there is a first-order theory \"T\" of signature σ, such that \"K\" consists of all models of \"T\", i.e., of all σ-structures that satisfy \"T\". If \"T\" can be chosen as a theory consisting of a single first-order sentence, then \"K\" is called a basic elementary class.\n\nMore generally, \"K\" is a pseudo-elementary class if there is a first-order theory \"T\" of a signature that extends σ, such that \"K\" consists of all σ-structures that are reducts to σ of models of \"T\". In other words, a class \"K\" of σ-structures is pseudo-elementary iff there is an elementary class \"K<nowiki>'</nowiki>\" such that \"K\" consists of precisely the reducts to σ of the structures in \"K<nowiki>'</nowiki>\".\n\nFor obvious reasons, elementary classes are also called axiomatizable in first-order logic, and basic elementary classes are called finitely axiomatizable in first-order logic. These definitions extend to other logics in the obvious way, but since the first-order case is by far the most important, axiomatizable implicitly refers to this case when no other logic is specified.\n\nWhile the above is nowadays standard terminology in \"infinite\" model theory, the slightly different earlier definitions are still in use in finite model theory, where an elementary class may be called a Δ-elementary class, and the terms elementary class and first-order axiomatizable class are reserved for basic elementary classes (Ebbinghaus et al. 1994, Ebbinghaus and Flum 2005). Hodges calls elementary classes axiomatizable classes, and he refers to basic elementary classes as definable classes. He also uses the respective synonyms EC class and ECformula_1 class (Hodges, 1993).\n\nThere are good reasons for this diverging terminology. The signatures that are considered in general model theory are often infinite, while a single first-order sentence contains only finitely many symbols. Therefore, basic elementary classes are atypical in infinite model theory. Finite model theory, on the other hand, deals almost exclusively with finite signatures. It is easy to see that for every finite signature σ and for every class \"K\" of σ-structures closed under isomorphism there is an elementary class formula_2 of σ-structures such that \"K\" and formula_2 contain precisely the same finite structures. Hence elementary classes are not very interesting for finite model theorists.\n\nClearly every basic elementary class is an elementary class, and every elementary class is a pseudo-elementary class. Moreover, as an easy consequence of the compactness theorem, a class of σ-structures is basic elementary if and only if it is elementary and its complement is also elementary.\n\nLet σ be a signature consisting only of a unary function symbol \"f\". The class \"K\" of σ-structures in which \"f\" is one-to-one is a basic elementary class. This is witnessed by the theory \"T\", which consists only of the single sentence \n\nLet σ be an arbitrary signature. The class \"K\" of all infinite σ-structures is elementary. To see this, consider the sentences\n\nand so on. (So the sentence formula_9 says that there are at least \"n\" elements.) The infinite σ-structures are precisely the models of the theory\n\nBut \"K\" is not a basic elementary class. Otherwise the infinite σ-structures would be precisely those that satisfy a certain first-order sentence τ. But then the set\nformula_11 would be inconsistent. By the compactness theorem, for some natural number \"n\" the set formula_12 would be inconsistent. But this is absurd, because this theory is satisfied by any σ-structure with formula_13 or more elements.\n\nHowever, there is a basic elementary class \"K<nowiki>'</nowiki>\" in the signature σ' = σ formula_14 {\"f\"}, where \"f\" is a unary function symbol, such that \"K\" consists exactly of the reducts to σ of σ'-structures in \"K<nowiki>'</nowiki>\". \"K<nowiki>'</nowiki>\" is axiomatised by the single sentence formula_15, which expresses that \"f\" is injective but not surjective. Therefore, \"K\" is elementary and what could be called basic pseudo-elementary, but not basic elementary.\n\nFinally, consider the signature σ consisting of a single unary relation symbol \"P\". Every σ-structure is partitioned into two subsets: Those elements for which \"P\" holds, and the rest. Let \"K\" be the class of all σ-structures for which these two subsets have the same cardinality, i.e., there is a bijection between them. This class is not elementary, because a σ-structure in which both the set of realisations of \"P\" and its complement are countably infinite satisfies precisely the same first-order sentences as a σ-structure in which one of the sets is countably infinite and the other is uncountable.\n\nNow consider the signature formula_16, which consists of \"P\" along with a unary function symbol \"f\". Let formula_2 be the class of all formula_16-structures such that \"f\" is a bijection and \"P\" holds for \"x\" iff \"P\" does not hold for \"f(x)\". formula_2 is clearly an elementary class, and therefore \"K\" is an example of a pseudo-elementary class that is not elementary.\n\nLet σ be an arbitrary signature. The class \"K\" of all finite σ-structures is not elementary, because (as shown above) its complement is elementary but not basic elementary. Since this is also true for every signature extending σ, \"K\" is not even a pseudo-elementary class.\n\nThis example demonstrates the limits of expressive power inherent in first-order logic as opposed to the far more expressive second-order logic. Second-order logic, however, fails to retain many desirable properties of first-order logic, such as the completeness and compactness theorems.\n\n"}
{"id": "14345961", "url": "https://en.wikipedia.org/wiki?curid=14345961", "title": "Embedded pushdown automaton", "text": "Embedded pushdown automaton\n\nAn embedded pushdown automaton or EPDA is a computational model for parsing languages generated by tree-adjoining grammars (TAGs). It is similar to the context-free grammar-parsing pushdown automaton, except that instead of using a plain stack to store symbols, it has a stack of iterated stacks that store symbols, giving TAGs a generative capacity between context-free grammars and context-sensitive grammars, or a subset of the mildly context-sensitive grammars.\nEmbedded pushdown automata should not be confused with nested stack automata which have more computational power.\n\nEPDAs were first described by K. Vijay-Shanker in his 1988 doctoral thesis. They have since been applied to more complete descriptions of classes of mildly context-sensitive grammars and have had important roles in refining the Chomsky hierarchy. Various subgrammars, such as the linear indexed grammar, can thus be defined. EPDAs are also beginning to play an important role in natural language processing.\n\nWhile natural languages have traditionally been analyzed using context-free grammars (see transformational-generative grammar and computational linguistics), this model does not work well for languages with crossed dependencies, such as Dutch, situations for which an EPDA is well suited. A detailed linguistic analysis is available in Joshi, Schabes (1997).\n\nAn EPDA is a finite state machine with a set of stacks that can be themselves accessed through the \"embedded stack\". Each stack contains elements of the \"stack alphabet\" formula_1, and so we define an element of a stack by formula_2, where the star is the Kleene closure of the alphabet.\n\nEach stack can then be defined in terms of its elements, so we denote the formula_3th stack in the automaton using a double-dagger symbol: formula_4, where formula_5 would be the next accessible symbol in the stack. The \"embedded stack\" of formula_6 stacks can thus be denoted by formula_7.\n\nWe define an EPDA by the septuple (7-tuple)\n\n\nThus the transition function takes a state, the next symbol of the input string, and the top symbol of the current stack and generates the next state, the stacks to be pushed and popped onto the \"embedded stack\", the pushing and popping of the current stack, and the stacks to be considered the current stacks in the next transition. More conceptually, the \"embedded stack\" is pushed and popped, the current stack is optionally pushed back onto the \"embedded stack\", and any other stacks one would like are pushed on top of that, with the last stack being the one read from in the next iteration. Therefore, stacks can be pushed both above and below the current stack.\n\nA given configuration is defined by\n\nwhere formula_19 is the current state, the formula_20s are the stacks in the \"embedded stack\", with formula_21 the current stack, and for an input string formula_22, formula_23 is the portion of the string already processed by the machine and formula_24 is the portion to be processed, with its head being the current symbol read. Note that the empty string formula_25 is implicitly defined as a terminating symbol, where if the machine is at a final state when the empty string is read, the entire input string is \"accepted\", and if not it is \"rejected\". Such \"accepted\" strings are elements of the language\n\nwhere formula_27 and formula_28 defines the transition function applied over as many times as necessary to parse the string.\n\nAn informal description of EPDA can also be found in Joshi, Schabes (1997), Sect.7, p. 23-25.\n\nA more precisely defined hierarchy of languages that correspond to the mildly context-sensitive class was defined by David J. Weir.\nBased on the work of Nabil A. Khabbaz,\nWeir's Control Language Hierarchy is a containment where the \"Level-1\" is defined as context-free, and \"Level-2\" is the class of tree-adjoining and the other three grammars.\n\nFollowing are some of the properties of Level-\"k\" languages in the hierarchy:\nThose properties correspond well (at least for small \"k\" > 1) to the conditions of mildly context-sensitive languages imposed by Joshi, and as \"k\" gets bigger, the language class becomes, in a sense, less mildly context-sensitive.\n\n"}
{"id": "15445", "url": "https://en.wikipedia.org/wiki?curid=15445", "title": "Entropy (information theory)", "text": "Entropy (information theory)\n\nInformation entropy is the average rate at which information is produced by a stochastic source of data.\n\nThe measure of information entropy associated with each possible data value is the negative logarithm of the probability mass function for the value: formula_1 . Thus, when the data source has a lower-probability value (i.e., when a low-probability event occurs), the event carries more \"information\" (\"surprisal\") than when the source data has a higher-probability value. The amount of information conveyed by each event defined in this way becomes a random variable whose expected value is the information entropy. Generally, \"entropy\" refers to disorder or uncertainty, and the definition of entropy used in information theory is directly analogous to the definition used in statistical thermodynamics. The concept of information entropy was introduced by Claude Shannon in his 1948 paper \"A Mathematical Theory of Communication\".\n\nThe basic model of a data communication system is composed of three elements, a source of data, a communication channel, and a receiver, and – as expressed by Shannon – the \"fundamental problem of communication\" is for the receiver to be able to identify what data was generated by the source, based on the signal it receives through the channel. The entropy provides an absolute limit on the shortest possible average length of a lossless compression encoding of the data produced by a source, and if the entropy of the source is less than the channel capacity of the communication channel, the data generated by the source can be reliably communicated to the receiver (at least in theory, possibly neglecting some practical considerations such as the complexity of the system needed to convey the data and the amount of time it may take for the data to be conveyed).\n\nInformation entropy is typically measured in bits (alternatively called \"shannons\") or sometimes in \"natural units\" (nats) or decimal digits (called \"dits\", \"bans\", or \"hartleys\"). The unit of the measurement depends on the base of the logarithm that is used to define the entropy.\n\nThe logarithm of the probability distribution is useful as a measure of entropy because it is additive for independent sources. For instance, the entropy of a fair coin toss is 1 bit, and the entropy of tosses is bits. In a straightforward representation, bits are needed to represent a variable that can take one of values if is a power of 2. If these values are equally probable, the entropy (in bits) is equal to this number. If one of the values is more probable to occur than the others, an observation that this value occurs is less informative than if some less common outcome had occurred. Conversely, rarer events provide more information when observed. Since observation of less probable events occurs more rarely, the net effect is that the entropy (thought of as average information) received from non-uniformly distributed data is always less than or equal to . Entropy is zero when one outcome is certain to occur. The entropy quantifies these considerations when a probability distribution of the source data is known. The \"meaning\" of the events observed (the meaning of \"messages\") does not matter in the definition of entropy. Entropy only takes into account the probability of observing a specific event, so the information it encapsulates is information about the underlying probability distribution, not the meaning of the events themselves.\n\nThe basic idea of information theory is the more one knows about a topic, the less new information one is apt to get about it. If an event is very probable, it is no surprise when it happens and thus provides little new information. Inversely, if the event was improbable, it is much more informative that the event happened. Therefore, the information content is an increasing function of the inverse of the probability of the event (1/p). Now, if more events may happen, entropy measures the average information content you can expect to get if one of the events actually happens. This implies that casting a die has more entropy than tossing a coin because each outcome of the die has smaller probability than each outcome of the coin.\n\nThus, entropy is a measure of \"unpredictability\" of the state, or equivalently, of its \"average information content\". To get an intuitive understanding of these terms, consider the example of a political poll. Usually, such polls happen because the outcome of the poll is not already known. In other words, the outcome of the poll is relatively \"unpredictable\", and actually performing the poll and learning the results gives some new \"information\"; these are just different ways of saying that the \"a priori\" entropy of the poll results is large. Now, consider the case that the same poll is performed a second time shortly after the first poll. Since the result of the first poll is already known, the outcome of the second poll can be predicted well and the results should not contain much new information; in this case the \"a priori\" entropy of the second poll result is small relative to that of the first.\n\nNow consider the example of a coin toss. Assuming the probability of heads is the same as the probability of tails, then the entropy of the coin toss is as high as it could be. This is because there is no way to predict the outcome of the coin toss ahead of time: if we have to choose, the best we can do is predict that the coin will come up heads, and this prediction will be correct with probability 1/2. Such a coin toss has one bit of entropy since there are two possible outcomes that occur with equal probability, and learning the actual outcome contains one bit of information. In contrast, a coin toss using a coin that has two heads and no tails has zero entropy since the coin will always come up heads, and the outcome can be predicted perfectly. Analogously, a binary event with equiprobable outcomes has a Shannon entropy of formula_2 bit. Similarly, one trit with equiprobable values contains formula_3 (about 1.58496) bits of information because it can have one of three values.\n\nEnglish text, treated as a string of characters, has fairly low entropy, i.e., is fairly predictable. Even if we do not know exactly what is going to come next, we can be fairly certain that, for example, 'e' will be far more common than 'z', that the combination 'qu' will be much more common than any other combination with a 'q' in it, and that the combination 'th' will be more common than 'z', 'q', or 'qu'. After the first few letters one can often guess the rest of the word. English text has between 0.6 and 1.3 bits of entropy per character of the message.\n\nIf a compression scheme is lossless—that is, you can always recover the entire original message by decompressing—then a compressed message has the same quantity of information as the original, but communicated in fewer characters. That is, it has more information, or a higher entropy, per character. This means a compressed message has less redundancy. Roughly speaking, Shannon's source coding theorem says that a lossless compression scheme cannot compress messages, on average, to have \"more\" than one bit of information per bit of message, but that any value \"less\" than one bit of information per bit of message can be attained by employing a suitable coding scheme. The entropy of a message per bit multiplied by the length of that message is a measure of how much total information the message contains.\n\nIntuitively, imagine that we wish to transmit sequences comprising the 4 characters 'A', 'B', 'C', and 'D'. Thus, a message to be transmitted might be 'ABADDCAB'. Information theory gives a way of calculating the smallest possible amount of information that will convey this. If all 4 letters are equally likely (25%), we can do no better (over a binary channel) than to have 2 bits encode (in binary) each letter: 'A' might code as '00', 'B' as '01', 'C' as '10', and 'D' as '11'. Now suppose 'A' occurs with 70% probability, 'B' with 26%, and 'C' and 'D' with 2% each. We could assign variable length codes, so that receiving a '1' tells us to look at another bit unless we have already received 2 bits of sequential 1s. In this case, 'A' would be coded as '0' (one bit), 'B' as '10', and 'C' and 'D' as '110' and '111'. It is easy to see that 70% of the time only one bit needs to be sent, 26% of the time two bits, and only 4% of the time 3 bits. On average, then, fewer than 2 bits are required since the entropy is lower (owing to the high prevalence of 'A' followed by 'B' – together 96% of characters). The calculation of the sum of probability-weighted log probabilities measures and captures this effect.\n\nShannon's theorem also implies that no lossless compression scheme can shorten \"all\" messages. If some messages come out shorter, at least one must come out longer due to the pigeonhole principle. In practical use, this is generally not a problem, because we are usually only interested in compressing certain types of messages, for example English documents as opposed to gibberish text, or digital photographs rather than noise, and it is unimportant if a compression algorithm makes some unlikely or uninteresting sequences larger.\n\nNamed after Boltzmann's Η-theorem, Shannon defined the entropy (Greek capital letter eta) of a discrete random variable formula_4 with possible values formula_5 and probability mass function formula_6 as:\n\nHere formula_8 is the expected value operator, and is the information content of .\n\nThe entropy can explicitly be written as\n\nwhere is the base of the logarithm used. Common values of are 2, Euler's number, and 10, and the corresponding units of entropy are the bits for , nats for , and bans for .\n\nIn the case of for some , the value of the corresponding summand is taken to be , which is consistent with the limit:\n\nOne may also define the conditional entropy of two events and taking values and respectively, as\n\nwhere is the probability that and . This quantity should be understood as the amount of randomness in the random variable given the event .\n\nConsider tossing a coin with known, not necessarily fair, probabilities of coming up heads or tails; this can be modelled as a Bernoulli process.\n\nThe entropy of the unknown result of the next toss of the coin is maximized if the coin is fair (that is, if heads and tails both have equal probability 1/2). This is the situation of maximum uncertainty as it is most difficult to predict the outcome of the next toss; the result of each toss of the coin delivers one full bit of information. This is because\n\nHowever, if we know the coin is not fair, but comes up heads or tails with probabilities and , where , then there is less uncertainty. Every time it is tossed, one side is more likely to come up than the other. The reduced uncertainty is quantified in a lower entropy: on average each toss of the coin delivers less than one full bit of information. For example, if =0.7, then\n\nThe extreme case is that of a double-headed coin that never comes up tails, or a double-tailed coin that never results in a head. Then there is no uncertainty. The entropy is zero: each toss of the coin delivers no new information as the outcome of each coin toss is always certain.\n\nEntropy can be normalized by dividing it by information length. This ratio is called metric entropy and is a measure of the randomness of the information.\n\nTo understand the meaning of , first define an information function in terms of an event with probability . The amount of information acquired due to the observation of event follows from Shannon's solution of the fundamental properties of information:\n\nThe last is a crucial property. It states that joint probability of independent sources of information communicates as much information as the two individual events separately. Particularly, if the first event can yield one of equiprobable outcomes and another has one of equiprobable outcomes then there are possible outcomes of the joint event. This means that if bits are needed to encode the first value and to encode the second, one needs to encode both. Shannon discovered that the proper choice of function to quantify information, preserving this additivity, is logarithmic, i.e.,\n\nlet formula_15 be the information function which one assumes to be twice continuously differentiable, one has:\n\nformula_16\n\nThis differential equation leads to the solution formula_17 for any formula_18. Condition 2. leads to formula_19 and especially, formula_20 can be chosen on the form formula_21 with formula_22, which is equivalent to choosing a specific base for the logarithm. The different units of information (bits for the binary logarithm , nats for the natural logarithm , bans for the decimal logarithm and so on) are constant multiples of each other. For instance, in case of a fair coin toss, heads provides bit of information, which is approximately 0.693 nats or 0.301 decimal digits. Because of additivity, tosses provide bits of information, which is approximately nats or decimal digits.\n\nIf there is a distribution where event can happen with probability , and it is sampled times with an outcome occurring times, the total amount of information we have received is \nThe \"average\" amount of information that we receive per event is therefore\n\nThe inspiration for adopting the word \"entropy\" in information theory came from the close resemblance between Shannon's formula and very similar known formulae from statistical mechanics.\n\nIn statistical thermodynamics the most general formula for the thermodynamic entropy of a thermodynamic system is the Gibbs entropy,\nwhere is the Boltzmann constant, and is the probability of a microstate. The Gibbs entropy was defined by J. Willard Gibbs in 1878 after earlier work by Boltzmann (1872).\n\nThe Gibbs entropy translates over almost unchanged into the world of quantum physics to give the von Neumann entropy, introduced by John von Neumann in 1927,\nwhere ρ is the density matrix of the quantum mechanical system and Tr is the trace.\n\nAt an everyday practical level the links between information entropy and thermodynamic entropy are not evident. Physicists and chemists are apt to be more interested in \"changes\" in entropy as a system spontaneously evolves away from its initial conditions, in accordance with the second law of thermodynamics, rather than an unchanging probability distribution. And, as the minuteness of Boltzmann's constant indicates, the changes in for even tiny amounts of substances in chemical and physical processes represent amounts of entropy that are extremely large compared to anything in data compression or signal processing. Furthermore, in classical thermodynamics the entropy is defined in terms of macroscopic measurements and makes no reference to any probability distribution, which is central to the definition of information entropy.\n\nThe connection between thermodynamics and what is now known as information theory was first made by Ludwig Boltzmann and expressed by his famous equation:\n\nwhere \"S\" is the thermodynamic entropy of a particular macrostate (defined by thermodynamic parameters such as temperature, volume, energy, etc.), \"W\" is the number of microstates (various combinations of particles in various energy states) that can yield the given macrostate, and \"k\" is Boltzmann's constant. It is assumed that each microstate is equally likely, so that the probability of a given microstate is \"p = 1/W\". When these probabilities are substituted into the above expression for the Gibbs entropy (or equivalently \"k\" times the Shannon entropy), Boltzmann's equation results. In information theoretic terms, the information entropy of a system is the amount of \"missing\" information needed to determine a microstate, given the macrostate.\n\nIn the view of Jaynes (1957), thermodynamic entropy, as explained by statistical mechanics, should be seen as an \"application\" of Shannon's information theory: the thermodynamic entropy is interpreted as being proportional to the amount of further Shannon information needed to define the detailed microscopic state of the system, that remains uncommunicated by a description solely in terms of the macroscopic variables of classical thermodynamics, with the constant of proportionality being just the Boltzmann constant. For example, adding heat to a system increases its thermodynamic entropy because it increases the number of possible microscopic states of the system that are consistent with the measurable values of its macroscopic variables, thus making any complete state description longer. (See article: \"maximum entropy thermodynamics\"). Maxwell's demon can (hypothetically) reduce the thermodynamic entropy of a system by using information about the states of individual molecules; but, as Landauer (from 1961) and co-workers have shown, to function the demon himself must increase thermodynamic entropy in the process, by at least the amount of Shannon information he proposes to first acquire and store; and so the total thermodynamic entropy does not decrease (which resolves the paradox). Landauer's principle imposes a lower bound on the amount of heat a computer must generate to process a given amount of information, though modern computers are far less efficient.\n\nEntropy is defined in the context of a probabilistic model. Independent fair coin flips have an entropy of 1 bit per flip. A source that always generates a long string of B's has an entropy of 0, since the next character will always be a 'B'.\n\nThe entropy rate of a data source means the average number of bits per symbol needed to encode it. Shannon's experiments with human predictors show an information rate between 0.6 and 1.3 bits per character in English; the PPM compression algorithm can achieve a compression ratio of 1.5 bits per character in English text.\n\nFrom the preceding example, note the following points:\n\n\nShannon's definition of entropy, when applied to an information source, can determine the minimum channel capacity required to reliably transmit the source as encoded binary digits (see caveat below in italics). The formula can be derived by calculating the mathematical expectation of the \"amount of information\" contained in a digit from the information source. \"See also\" Shannon–Hartley theorem.\n\nShannon's entropy measures the information contained in a message as opposed to the portion of the message that is determined (or predictable). \"Examples of the latter include redundancy in language structure or statistical properties relating to the occurrence frequencies of letter or word pairs, triplets etc.\" See Markov chain.\n\nEntropy is one of several ways to measure diversity. Specifically, Shannon entropy is the logarithm of , the true diversity index with parameter equal to 1.\n\nEntropy effectively bounds the performance of the strongest lossless compression possible, which can be realized in theory by using the typical set or in practice using Huffman, Lempel–Ziv or arithmetic coding. See also Kolmogorov complexity. In practice, compression algorithms deliberately include some judicious redundancy in the form of checksums to protect against errors.\n\nA 2011 study in \"Science\" estimates the world's technological capacity to store and communicate optimally compressed information normalized on the most effective compression algorithms available in the year 2007, therefore estimating the entropy of the technologically available sources. \n\nThe authors estimate humankind technological capacity to store information (fully entropically compressed) in 1986 and again in 2007. They break the information into three categories—to store information on a medium, to receive information through a one-way broadcast networks, or to exchange information through two-way telecommunication networks.\n\nThere are a number of entropy-related concepts that mathematically quantify information content in some way:\n(The \"rate of self-information\" can also be defined for a particular sequence of messages or symbols generated by a given stochastic process: this will always be equal to the entropy rate in the case of a stationary process.) Other quantities of information are also used to compare or relate different sources of information.\n\nIt is important not to confuse the above concepts. Often it is only clear from context which one is meant. For example, when someone says that the \"entropy\" of the English language is about 1 bit per character, they are actually modeling the English language as a stochastic process and talking about its entropy \"rate\". Shannon himself used the term in this way.\n\nHowever, if we use very large blocks, then the estimate of per-character entropy rate may become artificially low. This is because in reality, the probability distribution of the sequence is not knowable exactly; it is only an estimate. For example, suppose one considers the text of every book ever published as a sequence, with each symbol being the text of a complete book. If there are published books, and each book is only published once, the estimate of the probability of each book is , and the entropy (in bits) is . As a practical code, this corresponds to assigning each book a unique identifier and using it in place of the text of the book whenever one wants to refer to the book. This is enormously useful for talking about books, but it is not so useful for characterizing the information content of an individual book, or of language in general: it is not possible to reconstruct the book from its identifier without knowing the probability distribution, that is, the complete text of all the books. The key idea is that the complexity of the probabilistic model must be considered. Kolmogorov complexity is a theoretical generalization of this idea that allows the consideration of the information content of a sequence independent of any particular probability model; it considers the shortest program for a universal computer that outputs the sequence. A code that achieves the entropy rate of a sequence for a given model, plus the codebook (i.e. the probabilistic model), is one such program, but it may not be the shortest.\n\nFor example, the Fibonacci sequence is 1, 1, 2, 3, 5, 8, 13, …. Treating the sequence as a message and each number as a symbol, there are almost as many symbols as there are characters in the message, giving an entropy of approximately . So the first 128 symbols of the Fibonacci sequence has an entropy of approximately 7 bits/symbol. However, the sequence can be expressed using a formula [ for , , ] and this formula has a much lower entropy and applies to any length of the Fibonacci sequence.\n\nIn cryptanalysis, entropy is often roughly used as a measure of the unpredictability of a cryptographic key, though its real uncertainty is unmeasurable. For example, a 128-bit key that is uniformly randomly generated has 128 bits of entropy. It also takes (on average) formula_28 guesses to break by brute force. However, entropy fails to capture the number of guesses required if the possible keys are not chosen uniformly. Instead, a measure called \"guesswork\" can be used to measure the effort required for a brute force attack.\n\nOther problems may arise from non-uniform distributions used in cryptography. For example, consider a 1,000,000-digit binary one-time pad using exclusive or. If the pad has 1,000,000 bits of entropy, it is perfect. If the pad has 999,999 bits of entropy, evenly distributed (each individual bit of the pad having 0.999999 bits of entropy) it may provide good security. But if the pad has 999,999 bits of entropy, where the first bit is fixed and the remaining 999,999 bits are perfectly random, then the first bit of the ciphertext will not be encrypted at all.\n\nA common way to define entropy for text is based on the Markov model of text. For an order-0 source (each character is selected independent of the last characters), the binary entropy is:\n\nwhere is the probability of . For a first-order Markov source (one in which the probability of selecting a character is dependent only on the immediately preceding character), the entropy rate is:\n\nwhere is a state (certain preceding characters) and formula_31 is the probability of given as the previous character.\n\nFor a second order Markov source, the entropy rate is\n\nIn general the -ary entropy of a source formula_33 with source alphabet } and discrete probability distribution } where is the probability of (say is defined by:\n\nNote: the in \"-ary entropy\" is the number of different symbols of the \"ideal alphabet\" used as a standard yardstick to measure source alphabets. In information theory, two symbols are necessary and sufficient for an alphabet to encode information. Therefore, the default is to let (\"binary entropy\"). Thus, the entropy of the source alphabet, with its given empiric probability distribution, is a number equal to the number (possibly fractional) of symbols of the \"ideal alphabet\", with an optimal probability distribution, necessary to encode for each symbol of the source alphabet. Also note that \"optimal probability distribution\" here means a uniform distribution: a source alphabet with symbols has the highest possible entropy (for an alphabet with symbols) when the probability distribution of the alphabet is uniform. This optimal entropy turns out to be .\n\nA source alphabet with non-uniform distribution will have less entropy than if those symbols had uniform distribution (i.e. the \"optimized alphabet\"). This deficiency in entropy can be expressed as a ratio called efficiency:\n\nEfficiency has utility in quantifying the effective use of a communication channel. This formulation is also referred to as the normalized entropy, as the entropy is divided by the maximum entropy formula_36. Furthermore, the efficiency is indifferent to choice of (positive) base , as indicated by the insensitivity within the final logarithm above thereto.\n\nShannon entropy is characterized by a small number of criteria, listed below. Any definition of entropy satisfying these assumptions has the form\nwhere is a constant corresponding to a choice of measurement units.\n\nIn the following, and .\n\nThe measure should be continuous, so that changing the values of the probabilities by a very small amount should only change the entropy by a small amount.\n\nThe measure should be unchanged if the outcomes are re-ordered.\n\nThe measure should be maximal if all the outcomes are equally likely (uncertainty is highest when all possible events are equiprobable).\n\nFor equiprobable events the entropy should increase with the number of outcomes.\n\nFor continuous random variables, the multivariate Gaussian is the distribution with maximum differential entropy.\n\nThe amount of entropy should be independent of how the process is regarded as being divided into parts.\n\nThis last functional relationship characterizes the entropy of a system with sub-systems. It demands that the entropy of a system can be calculated from the entropies of its sub-systems if the interactions between the sub-systems are known.\n\nGiven an ensemble of uniformly distributed elements that are divided into boxes (sub-systems) with elements each, the entropy of the whole ensemble should be equal to the sum of the entropy of the system of boxes and the individual entropies of the boxes, each weighted with the probability of being in that particular box.\n\nFor positive integers where ,\n\nChoosing , this implies that the entropy of a certain outcome is zero: . This implies that the efficiency of a source alphabet with symbols can be defined simply as being equal to its -ary entropy. See also Redundancy (information theory).\n\nThe Shannon entropy satisfies the following properties, for some of which it is useful to interpret entropy as the amount of information learned (or uncertainty eliminated) by revealing the value of a random variable :\n\nfor all probability mass functions formula_57 and formula_58.\n\nThe Shannon entropy is restricted to random variables taking discrete values. The corresponding formula for a continuous random variable with probability density function with finite or infinite support formula_59 on the real line is defined by analogy, using the above form of the entropy as an expectation:\n\nThis formula is usually referred to as the continuous entropy, or differential entropy. A precursor of the continuous entropy is the expression for the functional in the H-theorem of Boltzmann.\n\nAlthough the analogy between both functions is suggestive, the following question must be set: is the differential entropy a valid extension of the Shannon discrete entropy? Differential entropy lacks a number of properties that the Shannon discrete entropy has – it can even be negative – and thus corrections have been suggested, notably limiting density of discrete points.\n\nTo answer this question, we must establish a connection between the two functions:\n\nWe wish to obtain a generally finite measure as the bin size goes to zero. In the discrete case, the bin size is the (implicit) width of each of the (finite or infinite) bins whose probabilities are denoted by . As we generalize to the continuous domain, we must make this width explicit.\n\nTo do this, start with a continuous function discretized into bins of size formula_61.\n\nBy the mean-value theorem there exists a value in each bin such that\n\nand thus the integral of the function can be approximated (in the Riemannian sense) by\n\nwhere this limit and \"bin size goes to zero\" are equivalent.\n\nWe will denote\n\nand expanding the logarithm, we have\n\nAs Δ → 0, we have\n\nBut note that as , therefore we need a special definition of the differential or continuous entropy:\n\nwhich is, as said before, referred to as the differential entropy. This means that the differential entropy \"is not\" a limit of the Shannon entropy for . Rather, it differs from the limit of the Shannon entropy by an infinite offset (see also the article on information dimension)\n\nIt turns out as a result that, unlike the Shannon entropy, the differential entropy is \"not\" in general a good measure of uncertainty or information. For example, the differential entropy can be negative; also it is not invariant under continuous co-ordinate transformations. This problem may be illustrated by a change of units when \"x\" is a dimensioned variable. \"f(x)\" will then have the units of \"1/x\". The argument of the logarithm must be dimensionless, otherwise it is improper, so that the differential entropy as given above will be improper. If \"Δ\" is some \"standard\" value of \"x\" (i.e. \"bin size\") and therefore has the same units, then a modified differential entropy may be written in proper form as:\n\nand the result will be the same for any choice of units for \"x\". In fact, the limit of discrete entropy as formula_69 would also include a term of formula_70, which would in general be infinite. This is expected, continuous variables would typically have infinite entropy when discretized. The limiting density of discrete points is really a measure of how much easier a distribution is to describe than a distribution that is uniform over its quantization scheme.\n\nAnother useful measure of entropy that works equally well in the discrete and the continuous case is the relative entropy of a distribution. It is defined as the Kullback–Leibler divergence from the distribution to a reference measure as follows. Assume that a probability distribution is absolutely continuous with respect to a measure , i.e. is of the form for some non-negative -integrable function with -integral 1, then the relative entropy can be defined as\n\nIn this form the relative entropy generalises (up to change in sign) both the discrete entropy, where the measure is the counting measure, and the differential entropy, where the measure is the Lebesgue measure. If the measure is itself a probability distribution, the relative entropy is non-negative, and zero if as measures. It is defined for any measure space, hence coordinate independent and invariant under co-ordinate reparameterizations if one properly takes into account the transformation of the measure . The relative entropy, and implicitly entropy and differential entropy, do depend on the \"reference\" measure .\n\nEntropy has become a useful quantity in combinatorics.\n\nA simple example of this is an alternate proof of the Loomis–Whitney inequality: for every subset , we have\nwhere is the orthogonal projection in the th coordinate:\n\nThe proof follows as a simple corollary of Shearer's inequality: if are random variables and are subsets of } such that every integer between 1 and lies in exactly of these subsets, then\nwhere formula_75 is the Cartesian product of random variables with indexes in (so the dimension of this vector is equal to the size of ).\n\nWe sketch how Loomis–Whitney follows from this: Indeed, let be a uniformly distributed random variable with values in and so that each point in occurs with equal probability. Then (by the further properties of entropy mentioned above) , where denotes the cardinality of . Let }. The range of formula_76 is contained in and hence formula_77. Now use this to bound the right side of Shearer's inequality and exponentiate the opposite sides of the resulting inequality you obtain.\n\nFor integers let . Then\nwhere \n\nHere is a sketch proof. Note that formula_80 is one term of the expression\nRearranging gives the upper bound. For the lower bound one first shows, using some algebra, that it is the largest term in the summation. But then,\nsince there are terms in the summation. Rearranging gives the lower bound.\n\nA nice interpretation of this is that the number of binary strings of length with exactly many 1's is approximately formula_83.\n\n\n"}
{"id": "3925533", "url": "https://en.wikipedia.org/wiki?curid=3925533", "title": "Enumerative combinatorics", "text": "Enumerative combinatorics\n\nEnumerative combinatorics is an area of combinatorics that deals with the number of ways that certain patterns can be formed. Two examples of this type of problem are counting combinations and counting permutations. More generally, given an infinite collection of finite sets \"S\" indexed by the natural numbers, enumerative combinatorics seeks to describe a \"counting function\" which counts the number of objects in \"S\" for each \"n\". Although counting the number of elements in a set is a rather broad mathematical problem, many of the problems that arise in applications have a relatively simple combinatorial description. The twelvefold way provides a unified framework for counting permutations, combinations and partitions.\n\nThe simplest such functions are \"closed formulas\", which can be expressed as a composition of elementary functions such as factorials, powers, and so on. For instance, as shown below, the number of different possible orderings of a deck of \"n\" cards is \"f\"(\"n\") = \"n\"!. The problem of finding a closed formula is known as algebraic enumeration, and frequently involves deriving a recurrence relation or generating function and using this to arrive at the desired closed form.\n\nOften, a complicated closed formula yields little insight into the behavior of the counting function as the number of counted objects grows. \nIn these cases, a simple asymptotic approximation may be preferable. A function formula_1 is an asymptotic approximation to formula_2 if formula_3 as formula_4. In this case, we write formula_5\n\nGenerating functions are used to describe families of combinatorial objects. Let formula_6 denote the family of objects and let \"F\"(\"x\") be its generating function. Then\nwhere formula_8 denotes the number of combinatorial objects of size \"n\". The number of combinatorial objects of size \"n\" is therefore given by the coefficient of formula_9. Some common operation on families of combinatorial objects and its effect on the generating function will now be developed.\nThe exponential generating function is also sometimes used. In this case it would have the form\n\nOnce determined, the generating function yields the information given by the previous approaches. In addition, the various natural operations on generating functions such as addition, multiplication, differentiation, etc., have a combinatorial significance; this allows one to extend results from one combinatorial problem in order to solve others.\n\nGiven two combinatorial families, formula_6 and formula_12 with generating functions \"F\"(\"x\") and \"G\"(\"x\") respectively, the disjoint union of the two families (formula_13) has generating function \"F\"(\"x\") + \"G\"(\"x\").\n\nFor two combinatorial families as above the Cartesian product (pair) of the two families (formula_14) has generating function \"F\"(\"x\")\"G\"(\"x\").\n\nA sequence generalizes the idea of the pair as defined above. Sequences are arbitrary Cartesian products of a combinatorial object with itself. Formally:\n\nTo put the above in words: An empty sequence or a sequence of one element or a sequence of two elements or a sequence of three elements, etc.\nThe generating function would be:\n\nThe above operations can now be used to enumerate common combinatorial objects including trees (binary and plane), Dyck paths and cycles. A combinatorial structure is composed of atoms. For example, with trees the atoms would be the nodes. The atoms which compose the object can either be labeled or unlabeled. Unlabeled atoms are indistinguishable from each other, while labelled atoms are distinct. Therefore, for a combinatorial object consisting of labeled atoms a new object can be formed by simply swapping two or more atoms.\n\nBinary and plane trees are examples of an unlabeled combinatorial structure. Trees consist of nodes linked by edges in such a way that there are no cycles. There is generally a node called the root, which has no parent node. In Plane trees each node can have an arbitrary number of children. In binary trees, a special case of plane trees, each node can have either two or no children. Let formula_17 denote the family of all plane trees. Then this family can be recursively defined as follows:\nIn this case formula_19 represents the family of objects consisting of one node. This has generating function \"x\". Let \"P\"(\"x\") denote the generating function formula_17.\nPutting the above description in words: A plane tree consists of a node to which is attached an arbitrary number of subtrees, each of which is also a plane tree. Using the operation on families of combinatorial structures developed earlier this translates to a recursive generating function:\nAfter solving for \"P\"(\"x\"):\n\nAn explicit formula for the number of plane trees of size \"n\" can now be determined by extracting the coefficient of \"x\".\n\nNote: The notation [\"x\"] \"f\"(\"x\") refers to the coefficient of \"x\" in \"f\"(\"x\").\nThe series expansion of the square root is based on Newton's generalization of the binomial theorem. To get from the fourth to fifth line manipulations using the generalized binomial coefficient is needed.\n\nThe expression on the last line is equal to the (\"n\" − 1) Catalan number. Therefore \"p\" = \"c\".\n\n"}
{"id": "245658", "url": "https://en.wikipedia.org/wiki?curid=245658", "title": "Euler integral", "text": "Euler integral\n\nIn mathematics, there are two types of Euler integral:\n\nFor positive integers \"m\" and \"n\"\n\n"}
{"id": "11168195", "url": "https://en.wikipedia.org/wiki?curid=11168195", "title": "Existentially closed model", "text": "Existentially closed model\n\nIn model theory, a branch of mathematical logic, the notion of an existentially closed model (or existentially complete model) of a theory generalizes the notions of algebraically closed fields (for the theory of fields), real closed fields (for the theory of ordered fields), existentially closed groups (for the class of groups), and dense linear orders without endpoints (for the class of linear orders).\n\nA substructure \"M\" of a structure \"N\" is said to be existentially closed in (or existentially complete in) formula_1 if for every quantifier-free formula φ(\"x\",…,\"x\",\"y\",…,\"y\") and all elements \"b\",…,\"b\" of \"M\" such that φ(\"x\",…,\"x\",\"b\",…,\"b\") is realized in \"N\", then φ(\"x\",…,\"x\",\"b\",…,\"b\") is also realized in \"M\". In other words: If there is a tuple \"a\",…,\"a\" in \"N\" such that φ(\"a\",…,\"a\",\"b\",…,\"b\") holds in \"N\", then such a tuple also exists in \"M\". This notion is often denoted formula_2.\n\nA model \"M\" of a theory \"T\" is called existentially closed in \"T\" if it is existentially closed in every superstructure \"N\" that is itself a model of \"T\". More generally, a structure \"M\" is called existentially closed in a class \"K\" of structures (in which it is contained as a member) if \"M\" is existentially closed in every superstructure \"N\" that is itself a member of \"K\".\n\nThe existential closure in \"K\" of a member \"M\" of \"K\", when it exists, is, up to isomorphism, the least existentially closed superstructure of \"M\". More precisely, it is any extensionally closed superstructure \"M\" of \"M\" such that for every existentially closed superstructure \"N\" of \"M\", \"M\" is isomorphic to a substructure of \"N\" via an isomorphism that is the identity on \"M\".\n\nLet σ = (+,×,0,1) be the signature of fields, i.e. +,× are binary relation symbols and 0,1 are constant symbols. Let \"K\" be the class of structures of signature σ which are fields.\nIf \"A\" is a subfield of \"B\", then \"A\" is existentially closed in \"B\" if and only if every system of polynomials over \"A\" which has a solution in \"B\" also has a solution in \"A\". It follows that the existentially closed members of \"K\" are exactly the algebraically closed fields.\n\nSimilarly in the class of ordered fields, the existentially closed structures are the real closed fields. In the class of totally ordered structures, the existentially closed structures are those that are dense without endpoints, while the existential closure of any countable (including empty) total order is, up to isomorphism, the countable dense total order without endpoints, namely the order type of the rationals.\n\n\n"}
{"id": "3962591", "url": "https://en.wikipedia.org/wiki?curid=3962591", "title": "Experience modifier", "text": "Experience modifier\n\nIn the insurance industry in the United States, an experience modifier or experience modification is an adjustment of an employer's premium for worker's compensation coverage based on the losses the insurer has experienced from that employer. An experience modifier of 1 would be applied for an employer that had demonstrated the actuarially expected performance. Poorer loss experience leads to a modifier greater than 1, and better experience to a modifier less than 1. The loss experience used in determining the modifier typically comprises three years but excluding the immediate past year. For instance, if a policy expired on January 1, 2018, the period reflected by the experience modifier would run from January 1, 2014 to January 1, 2017.\n\nExperience modifiers are normally recalculated for an employer annually by using experience ratings. The rating is a method used by insurers to determine pricing of premiums for different groups or individuals based on the group or individual's history of claims. The experience rating approach uses an individual's or group’s historic data as a proxy for future risk, and insurers adjust and set insurance premiums and plans accordingly. Each year, a newer year's data is added to the three year window of experience used in the calculation, and the oldest year from the prior calculation is dropped off. The other two years worth of data in the rating window are also updated on an annual basis.\n\nExperience modifiers are calculated by organizations known as \"rating bureaus\" and rely on information reported by insurance companies. The rating bureau used by most states is the NCCI, the National Council on Compensation Insurance. But a number of states have independent rating bureaus: California, Michigan, Delaware, and Pennsylvania have stand-alone rating bureaus that do not integrate data with NCCI. Other states such as Wisconsin, Texas, New York, New Jersey, Indiana, and North Carolina, maintain their own rating bureaus but integrate with NCCI for multi-state employers.\n\nThe experience modifier adjusts workers compensation insurance premiums for a particular employer based on a comparison of past losses of that employer to what is calculated to be \"average\" losses of other employers in that state in the same business, adjusted for size. To do this, experience modifier calculations use loss information reported in by an employer's past insurers. This is compared to a calculation of expected losses for a company in that line of work, in that particular state, and adjusted for the size of the employer. The calculation of expected losses utilizes past audited payroll information for a particular employer, by classification code and state. These payrolls are multiplied by Expected Loss Rates, which are calculated by rating bureaus based on past reported claims costs per classification.\n\nErrors in experience modifiers can occur if inaccurate information is reported to a rating bureau by a past insurer of an employer. Some states (Illinois and Tennessee) prohibit increases in experience modifiers once a workers compensation policy begins, even if the higher modifier has been correctly calculated under the rules. Most states allow increases in experience modifiers if done relatively early in the term of the workers compensation insurance policy, and most states prohibit increases in experience modifier late in the term of the policy.\n\nThe detailed rules governing calculation of experience modifiers are developed by the various rating bureaus. Although all states use similar methodology, there can be differences in details in the formulas used by independent rating bureaus and the NCCI.\n\nIn many NCCI states, the Experience Rating Adjustment plan is in place, allowing for the 70% reduction in the reportable amount of medical-only claims. That is, for claims where there has been no payment to the worker for lost time, but only for medical expenses. This gives employers an incentive to report all claims to their insurers, rather than trying to pay for medical-only claims out of pocket. Discounting medical-only claims in the experience modifier calculation greatly reduces the impact of medical-only claims on the modifier.\n\nThe formula primarily used by the NCCI is the following.\n\nformula_1\nThe formula is broken down into 3 main categories or subsections for understanding.\n\nThese 3 categories are summed up, with Actual numbers divided by Expected numbers, notice that the Stabilizing value does not change between the numerator and denominator.\n\nformula_7\n\nIn the EMR calculation there are 4 fundamental losses that are necessary for the calculation, they are:\n\nThe losses that are not part of this fundamental 4 are,\n\nUnemployment insurance is experience rated in the United States; companies that have more claims resulting from past workers face higher unemployment insurance rates. The logic of this approach is that these are the companies that are more likely to cause someone to be unemployed, so they should pay more into the pool from which unemployment compensation is paid. Unemployment insurance is financed by a payroll tax paid by employers. Experience rating in unemployment insurance is described as imperfect, due in large part to the fact that there are statutory maximum and minimum rates that an employer can receive without regard to its history of lay-off. If a worker is laid off, generally the increased costs to the employer due to the higher value of unemployment insurance tax rates are less than the UI benefits received by the worker.\n"}
{"id": "17644390", "url": "https://en.wikipedia.org/wiki?curid=17644390", "title": "Factory Physics", "text": "Factory Physics\n\nFactory Physics is a book written by Wallace Hopp and Mark Spearman, which introduces a science of operations for manufacturing management. According to the book's preface, Factory Physics is \"a systematic description of the underlying behavior of manufacturing systems. Understanding it enables managers and engineers to work with the natural tendencies of manufacturing systems to:\n\n\nThe book is used both in industry and in academia for reference and teaching on operations management. It describes a new approach to manufacturing management based on the laws of Factory Physics science. The fundamental Factory Physics framework states that the essential components of all value streams or production processes or service processes are demand and transformation which are described by structural elements of flows and stocks. There are very specific practical, mathematical relationships that enable one to describe and control the performance of flows and stocks. The book states that, in the presence of variability, there are only three buffers available to synchronize demand and transformation with lowest cost and highest service level:\n\n\nThe book states that its approach enables practical, predictive understanding of flows and stocks and how to best use the three levers to optimally synchronize demand and transformation.\n\nThis work won the 1996 Institute of Industrial Engineers IIE/Joint Publishers Book of the Year Award.\n\n\n"}
{"id": "36617753", "url": "https://en.wikipedia.org/wiki?curid=36617753", "title": "Frederik Schuh", "text": "Frederik Schuh\n\nFrederik Schuh (7 February 1875, Amsterdam – 6 January 1966, The Hague) was a Dutch mathematician.\n\nHe completed his PhD in algebraic geometry from Amsterdam University in 1905, where his advisor was Diederik Johannes Korteweg. He taught at the Technische Hoogeschool at Delft (1907–1909 and 1916–1945) and at Groningen (1909–1916).\n\nHe was the inventor of the Chomp game and wrote \"The Master Book of Mathematical Recreations\" (1943).\n\n"}
{"id": "746550", "url": "https://en.wikipedia.org/wiki?curid=746550", "title": "Generalized game", "text": "Generalized game\n\nIn computational complexity theory, a generalized game is a game or puzzle that has been generalized so that it can be played on a board or grid of any size. For example, generalized chess is the game of chess played on an \"n×n\" board, with 2\"n\" pieces on each side. Generalized Sudoku includes Sudokus constructed on an \"n×n\" grid.\n\nComplexity theory studies the asymptotic difficulty of problems, so generalizations of games are needed, as games on a fixed size of board are finite problems.\n\nFor many generalized games which last for a number of moves polynomial in the size of the board, the problem of determining if there is a win for the first player in a given position is PSPACE-complete. Generalized hex and reversi are PSPACE-complete.\n\nFor many generalized games which may last for a number of moves exponential in the size of the board, the problem of determining if there is a win for the first player in a given position is EXPTIME-complete. Generalized chess, go and checkers are EXPTIME-complete.\n\n"}
{"id": "37595113", "url": "https://en.wikipedia.org/wiki?curid=37595113", "title": "Glaeser's composition theorem", "text": "Glaeser's composition theorem\n\nIn mathematics, Glaeser's theorem, introduced by , is a theorem giving conditions for a smooth function to be a composition of \"F\" and θ for some given smooth function θ. One consequence is a generalization of Newton's theorem that every symmetric polynomial is a polynomial in the elementary symmetric polynomials, from polynomials to smooth functions.\n"}
{"id": "7423424", "url": "https://en.wikipedia.org/wiki?curid=7423424", "title": "Identric mean", "text": "Identric mean\n\nThe identric mean of two positive real numbers \"x\", \"y\" is defined as: \n\nIt can be derived from the mean value theorem by considering the secant of the graph of the function formula_2. It can be generalized to more variables according by the mean value theorem for divided differences. The identric mean is a special case of the Stolarsky mean.\n\n"}
{"id": "17920985", "url": "https://en.wikipedia.org/wiki?curid=17920985", "title": "John Hudson (mathematician)", "text": "John Hudson (mathematician)\n\nJohn Hudson (1773 – 31 October 1843) was an English mathematician and clergyman. He was notable for being a senior wrangler as well as the tutor of George Peacock.\n\nJohn Hudson was the son of John Hudson, a farmer at Haverbrack in the parish of Beetham. He attended Heversham School and entered Trinity College, Cambridge in 1793. He became senior wrangler in 1797, also winning the Smith's prize in that year, and obtained his MA in 1800.\n\nHe became a Fellow, in 1798, and tutor, in 1807, of Trinity College, Cambridge, where he notably tutored George Peacock: he also tutored John Martin Frederick Wright. In 1815, he became the vicar of Kendal, Westmoreland. In 1815, he married the daughter of an army officer by the name of Culliford.\n\nAt Cambridge, Hudson also tutored Charles James Blomfield who became a prominent bishop. As a bishop, Blomfield visited Hudson's parish and at a dinner party declared \"I remember well, Mr. Hudson, how much I stood in awe of you at College.\" To which Hudson retorted, \"Perhaps so, but your Lordship has turned the tables on me now.\"\n\nHudson died at Haverbrack, Tuesday, October 31, 1843 at the age of 71 and was buried in the interior of the parish church at Kendal.\n\n"}
{"id": "45391945", "url": "https://en.wikipedia.org/wiki?curid=45391945", "title": "Kneser–Ney smoothing", "text": "Kneser–Ney smoothing\n\nKneser–Ney smoothing is a method primarily used to calculate the probability distribution of \"n\"-grams in a document based on their histories. It is widely considered the most effective method of smoothing due to its use of absolute discounting by subtracting a fixed value from the probability's lower order terms to omit \"n\"-grams with lower frequencies. This approach has been considered equally effective for both higher and lower order \"n\"-grams. The method is due to Reinhard Kneser and Hermann Ney. \n\nA common example that illustrates the concept behind this method is the frequency of the bigram \"San Francisco\". If it appears several times in a training corpus, the frequency of the unigram \"Francisco\" will also be high. Relying on only the unigram frequency to predict the frequencies of \"n\"-grams leads to skewed results; however, Kneser–Ney smoothing corrects this by considering the frequency of the unigram in relation to possible words preceding it.\n\nLet formula_1 be the number of occurrences of the word formula_2 followed by the word formula_3 in the corpus.\n\nThe equation for bigram probabilities is as follows:\n\nformula_4\n\nWhere the unigram probability formula_5 depends on how likely it is to see the word formula_6 in an unfamiliar context, which is estimated as the number of times it appears after any other word divided by the number of distinct pairs of consecutive words in the corpus:\n\nformula_7\n\nPlease note that formula_8 is a proper distribution, as the values defined in above way are non-negative and sum to one.\n\nThe parameter formula_9 is a constant which denotes the discount value subtracted from the count of each n-gram, usually between 0 and 1.\n\nThe value of the normalizing constant formula_10 is calculated to make the sum of conditional probabilities formula_11 over all formula_6 equal to one. \nObserve that (provided formula_13) for each formula_6 which occurs at least once in the context of formula_15 in the corpus we discount the probability by exactly the same constant amount formula_16,\nso the total discount depends linearly on the number of unique words formula_6 that can occur after formula_15.\nThis total discount is a budget we can spread over all formula_11 proportionally to formula_5.\nAs the values of formula_5 sum to one, we can simply define formula_10 to be equal to this total discount:\n\nformula_23\n\nThis equation can be extended to n-grams. Let formula_24 be the formula_25 words before formula_6:\n\nformula_27\nThis model uses the concept of absolute-discounting interpolation which incorporates information from higher and lower order language models. The addition of the term for lower order n-grams adds more weight to the overall probability when the count for the higher order n-grams is zero. Similarly, the weight of the lower order model decreases when the count of the n-gram is non zero.\n"}
{"id": "2058776", "url": "https://en.wikipedia.org/wiki?curid=2058776", "title": "Le Chiffre", "text": "Le Chiffre\n\nLe Chiffre (, \"The Cypher\" or \"The Number\") is a fictional character appearing in Ian Fleming's 1953 first James Bond novel, \"Casino Royale\". On screen Le Chiffre has been portrayed by Peter Lorre in the 1954 television adaptation of the novel for CBS's \"Climax!\" television series, by Orson Welles in the 1967 spoof of the novel and Bond film series, and by Mads Mikkelsen in the 2006 film version of Fleming's novel.\n\nFleming based the character on occultist Aleister Crowley.\n\nLe Chiffre, alias \"Die Nummer\", \"Mr. Number\", \"Herr Ziffer\" and other translations of \"The Number\" or \"The Cipher\" in various languages, is the paymaster of the \"Syndicat des Ouvriers d'Alsace\" (French for \"Alsatian Workmen's Union\"), a SMERSH-controlled trade union.\n\nHe is first encountered as an inmate of the Dachau displaced persons camp in the US zone of Germany in June 1945 and transferred to Alsace-Lorraine and Strasbourg three months later on a stateless passport. There he adopts the name Le Chiffre because as he claims, he is \"only a number on a passport\". Not much else is really known about Le Chiffre's background or where he comes from, except for educated guesses based on his description:\n\nIn the novel, he makes a major investment in a string of brothels with money belonging to SMERSH. The investment fails after a bill is signed into law banning prostitution. Le Chiffre then goes to the casino Royale-les-Eaux in an attempt to replace his lost funds. MI6 sends Bond, an expert baccarat player, to the casino to bankrupt Le Chiffre and force him to take refuge with the British government and inform on SMERSH. Bond bests Le Chiffre in a game of Chemin de Fer, taking all of his money. Le Chiffre kidnaps Bond's love interest, Vesper Lynd, to lure Bond into a trap and get back his money. The trap works, and Le Chiffre tortures Bond to get him to give up the money. He is interrupted by a SMERSH agent, however, who shoots him between the eyes with a silenced TT pistol as punishment for losing the money.\n\nLe Chiffre's death is seen by the Soviet government as an embarrassment, which in addition to the death and defeat of Mr. Big in \"Live and Let Die\", leads to the events of \"From Russia, with Love\".\n\n\nLe Chiffre is a secondary villain in the 1967 satire and appears in one of the few segments of the film actually adapted from Fleming's book. As in the novel, Le Chiffre is charged with recovering a large sum of money for SMERSH after he loses it at the baccarat table. He first attempts to raise the funds by holding an auction of embarrassing photographs of military and political leaders from China, the US and the USSR, but this is foiled by Sir James Bond's daughter, Mata Bond. With no other option, he returns to the baccarat table to try to win back the money. Later, he encounters baccarat Master Evelyn Tremble, who has been recruited by Bond to stop Le Chiffre from raising the money. Le Chiffre attempts to distract Tremble by performing elaborate magic tricks, but fails to prevent Tremble from winning. Afterwards, he arranges for Tremble to be kidnapped and subjects the agent to psychedelic torture in order to get back the money. The torture session is interrupted when his SMERSH masters, led by the film's main villain, Dr. Noah, shoot him dead.\n\nLe Chiffre is the main villain of the official 2006 James Bond film, \"Casino Royale\", portrayed by Danish actor Mads Mikkelsen. Believed by MI6 to be Albanian and officially stateless, Le Chiffre is a financier of international terrorism. M implies that Le Chiffre conspired with al-Qaeda in orchestrating 9/11, or at least deliberately profiteering from the attacks by short selling large quantities of airline stocks beforehand. In the video game version of \"Quantum of Solace\", it is said that his birth name is \"Jean Duran\", in the MI6 mission briefings. A mathematical genius and a chess prodigy, his abilities enable him to earn large sums of money on games of chance and probabilities, and he likes to show off by playing poker. He suffers from haemolacria, which causes him to weep blood out of a damaged vessel in his left eye. As in Fleming's novel, he dresses in immaculate black suits and uses a Salbutamol inhaler, here plated with platinum.\n\nLe Chiffre is contacted by Mr. White, a representative of an elite criminal organisation known as SPECTRE. White introduces Steven Obanno, a leader of the Lord's Resistance Army in Uganda, to Le Chiffre, and arranges to bank several briefcases full of cash for Obanno. Le Chiffre invests the money along with his other creditors' money in the aircraft manufacturer SkyFleet. Though SkyFleet's shares have been skyrocketing, he plans to short the company by purchasing put options, and ordering the destruction of the company's new prototype airliner, set to make its first flight out of Miami International Airport. Bond intervenes and foils the plan by killing the person Le Chiffre hired to destroy the plane. Le Chiffre finds out he lost millions and realises leaked information about his plans.\n\nIn order to win the money back, Le Chiffre sets up and enters a high-stakes Texas hold 'em tournament at Casino Royale in Montenegro in an attempt to recoup his losses before his clients find out that their money has been misappropriated. Bond is sent to make sure that Le Chiffre does not win back the money; if Le Chiffre is bankrupt, he will be forced to turn to MI6 for asylum, in exchange for information on his creditors and employers.\n\nDuring the tournament, an angry Obanno and his lieutenant break into Le Chiffre's hotel room and threaten him and his girlfriend, Valenka. Le Chiffre, who does not object to the threatened amputation of Valenka's arm, is granted one last chance to win their money back. As Obanno leaves the room, his bodyguard spots Bond and hears Valenka's cries coming from Bond's earpiece. Bond kills the bodyguard by throwing him over a railing, then chokes Obanno to death after relieving Obanno of his machete. Rene Mathis arranges the blame to be placed on Le Chiffre's bodyguard Leo.\n\nDuring the tournament, Le Chiffre initially outwits and bankrupts Bond, who cannot get additional funding from HM Treasury accountant Vesper Lynd, who has accompanied Bond to make sure the money is used properly. However, Felix Leiter, a CIA agent sent to participate in the game, also in hopes of bankrupting Le Chiffre, agrees to bankroll Bond, on the condition that CIA is allowed to take Le Chiffre in afterwards. Desperate, Le Chiffre has Valenka spike Bond's drink. Bond almost dies, but, thanks to an antitoxin kit in his car, a defibrillator, and Vesper's timely interference, he is revived at the last moment and returns to the game. During the final round, Le Chiffre's full house bests the hands of the two players preceding him, but loses to Bond's straight flush.\n\nLe Chiffre kidnaps Vesper, forcing Bond to give chase, and leads him straight into a trap. Le Chiffre leaves Vesper, bound at the feet and hands, in the middle of the road, and Bond is forced to swerve to avoid hitting her and crashes his car.\n\nSemiconscious, Bond is stripped naked and bound to a chair with the seat removed. Le Chiffre proceeds to bludgeon Bond in the testicles repeatedly with the knotted end of a ship's lanyard, each time demanding the password for the account into which the tournament winnings will be transferred. Bond refuses to give in, taunting him with the knowledge that he knows Le Chiffre's clients will track and kill him. Le Chiffre gloats that, even after he kills Bond and Vesper, MI6 will still give him sanctuary in return for information. When Bond refuses to give in, Le Chiffre brandishes a knife and is about to castrate him when he hears gunshots from outside. Seconds later, Mr. White bursts into the room brandishing a handgun. Le Chiffre pleads for his life, but Mr. White nevertheless shoots him above the left eye. To date, he is the only main Bond villain to die before the film's final act.\n\nLe Chiffre is mentioned in the Quantum of Solace and is also seen in a background image inside MI6. \nLe Chiffre appears in several images in \"Spectre\" as it is revealed that he, more or less conscious was an associate of Spectre and Ernst Stravo Blofeld. Blofeld states to Bond Le Chiffre's affiliation with Spectre and how Bond's interference in Blofeld's world caused him to destroy Bond's, implying Bond's foiling of Le Chiffre's and Quantum's scheme led him to enlist Raoul Silva to destroy MI6 and also kill M.\n\n\n\n\n"}
{"id": "12313191", "url": "https://en.wikipedia.org/wiki?curid=12313191", "title": "Limits of integration", "text": "Limits of integration\n\nIn calculus and mathematical analysis the limits of integration of the integral\nof a Riemann integrable function \"f\" defined on a closed and bounded [interval] are the real numbers \"a\" and \"b\".\n\nLimits of integration can also be defined for improper integrals, with the limits of integration of both\nand\nagain being \"a\" and \"b\". For an improper integral\nor\nthe limits of integration are \"a\" and ∞, or −∞ and \"b\", respectively.\n\n"}
{"id": "101863", "url": "https://en.wikipedia.org/wiki?curid=101863", "title": "Linear independence", "text": "Linear independence\n\nIn the theory of vector spaces, a set of vectors is said to be ' if one of the vectors in the set can be defined as a linear combination of the others; if no vector in the set can be written in this way, then the vectors are said to be '. These concepts are central to the definition of dimension.\n\nA vector space can be of finite-dimension or infinite-dimension depending on the number of linearly independent basis vectors. The definition of linear dependence and the ability to determine whether a subset of vectors in a vector space is linearly dependent are central to determining a basis for a vector space.\n\nThe vectors in a subset formula_1 of a vector space \"V\" are said to be \"linearly dependent\", if there exist scalars formula_2 , not all zero, such that\nwhere formula_4 denotes the zero vector.\n\nNotice that if not all of the scalars are zero, then at least one is non-zero, say formula_5, in which case this equation can be written in the form\nThus, formula_7 is shown to be a linear combination of the remaining vectors.\nThe vectors in a set formula_8 are said to be \"linearly independent\" if the equation\ncan only be satisfied by formula_10 for formula_11. This implies that no vector in the set can be represented as a linear combination of the remaining vectors in the set. In other words, a set of vectors is linearly independent if the only representations of formula_4 as a linear combination of its vectors is the trivial representation in which all the scalars formula_13 are zero.\n\nThe alternate definition, that a set of vectors is linearly dependent if and only if some vector in that set can be written as a linear combination of the other vectors, is only useful when the set contains two or more vectors. When the set contains zero or one vector, the original definition is used.\n\nIn order to allow the number of linearly independent vectors in a vector space to be countably infinite, it is useful to define linear dependence as follows. More generally, let \"V\" be a vector space over a field \"K\", and let {\"v\" | \"i\"∈\"I\"} be a family of elements of \"V\". The family is \"linearly dependent\" over \"K\" if there exists a family {\"a\" | \"j\"∈\"J\"} of elements of \"K\", not all zero, such that\nwhere the index set \"J\" is a nonempty, finite subset of \"I\".\n\nA set \"X\" of elements of \"V\" is \"linearly independent\" if the corresponding family {\"x\"} is linearly independent. Equivalently, a family is dependent if a member is in the closure of the linear span of the rest of the family, i.e., a member is a linear combination of the rest of the family. The trivial case of the empty family must be regarded as linearly independent for theorems to apply.\n\nA set of vectors which is linearly independent and spans some vector space, forms a basis for that vector space. For example, the vector space of all polynomials in \"x\" over the reals has the (infinite) subset {1, \"x\", \"x\", ...} as a basis.\n\nA geographic example may help to clarify the concept of linear independence. A person describing the location of a certain place might say, \"It is 3 miles north and 4 miles east of here.\" This is sufficient information to describe the location, because the geographic coordinate system may be considered as a 2-dimensional vector space (ignoring altitude and the curvature of the Earth's surface). The person might add, \"The place is 5 miles northeast of here.\" Although this last statement is \"true\", it is not necessary.\n\nIn this example the \"3 miles north\" vector and the \"4 miles east\" vector are linearly independent. That is to say, the north vector cannot be described in terms of the east vector, and vice versa. The third \"5 miles northeast\" vector is a linear combination of the other two vectors, and it makes the set of vectors \"linearly dependent\", that is, one of the three vectors is unnecessary.\n\nAlso note that if altitude is not ignored, it becomes necessary to add a third vector to the linearly independent set. In general, \"n\" linearly independent vectors are required to describe all locations in \"n\"-dimensional space.\n\nThree vectors: Consider the set of vectors \"v\" = (1, 1), \"v\" = (−3, 2) and \"v\" = (2, 4), then the condition for linear dependence seeks a set of non-zero scalars, such that\nor\n\nRow reduce this matrix equation by subtracting the first row from the second to obtain,\nContinue the row reduction by (i) dividing the second row by 5, and then (ii) multiplying by 3 and adding to the first row, that is\n\nWe can now rearrange this equation to obtain \nwhich shows that non-zero \"a\" exist such that \"v\" = (2, 4) can be defined in terms of \"v\" = (1, 1), \"v\" = (−3, 2). Thus, the three vectors are linearly dependent.\n\nTwo vectors: Now consider the linear dependence of the two vectors \"v\" = (1, 1), \"v\" = (−3, 2), and check, \nor\n\nThe same row reduction presented above yields,\nThis shows that \"a\" = 0, which means that the vectors \"v\" = (1, 1) and \"v\" = (−3, 2) are linearly independent.\n\nIn order to determine if the three vectors in R,\nare linearly dependent, form the matrix equation,\n\nRow reduce this equation to obtain,\nRearrange to solve for v and obtain,\nThis equation is easily solved to define non-zero \"a\",\nwhere \"a\" can be chosen arbitrarily. Thus, the vectors \"v\", \"v\" and \"v\" are linearly dependent.\n\nAn alternative method relies on the fact that \"n\" vectors in formula_28 are linearly independent if and only if the determinant of the matrix formed by taking the vectors as its columns is non-zero.\n\nIn this case, the matrix formed by the vectors is\nWe may write a linear combination of the columns as\nWe are interested in whether \"A\"Λ = 0 for some nonzero vector Λ. This depends on the determinant of \"A\", which is\nSince the determinant is non-zero, the vectors (1, 1) and (−3, 2) are linearly independent.\n\nOtherwise, suppose we have \"m\" vectors of \"n\" coordinates, with \"m\" < \"n\". Then \"A\" is an \"n\"×\"m\" matrix and Λ is a column vector with \"m\" entries, and we are again interested in \"A\"Λ = 0. As we saw previously, this is equivalent to a list of \"n\" equations. Consider the first \"m\" rows of \"A\", the first \"m\" equations; any solution of the full list of equations must also be true of the reduced list. In fact, if 〈\"i\"...,\"i\"〉 is any list of \"m\" rows, then the equation must be true for those rows.\n"}
{"id": "1436104", "url": "https://en.wikipedia.org/wiki?curid=1436104", "title": "Maximum principle", "text": "Maximum principle\n\nIn mathematics, the maximum principle is a property of solutions to certain partial differential equations, of the elliptic and parabolic types. Roughly speaking, it says that the maximum of a function in a domain is to be found on the boundary of that domain. Specifically, the \"strong\" maximum principle says that if a function achieves its maximum in the interior of the domain, the function is uniformly a constant. The \"weak\" maximum principle says that the maximum of the function is to be found on the boundary, but may re-occur in the interior as well. Other, even weaker maximum principles exist which merely bound a function in terms of its maximum on the boundary. \n\nIn convex optimization, the maximum principle states that the maximum of a convex function on a compact convex set is attained on the boundary.\n\nHarmonic functions are the classical example to which the strong maximum principle applies. Formally, if \"f\" is a harmonic function, then \"f\" cannot exhibit a true local maximum within the domain of definition of \"f\". In other words, either \"f\" is a constant function, or, for any point formula_1 inside the domain of \"f\", there exist other points arbitrarily close to formula_1 at which \"f\" takes larger values.\n\nLet \"f\" be a harmonic function defined on some connected open subset \"D\" of the Euclidean space R. If formula_1 is a point in \"D\" such that \n\nfor all \"x\" in a neighborhood of formula_1, then the function \"f\" is constant on \"D\".\n\nBy replacing \"maximum\" with \"minimum\" and \"larger\" with \"smaller\", one obtains the minimum principle for harmonic functions.\n\nThe maximum principle also holds for the more general subharmonic functions, while superharmonic functions satisfy the minimum principle.\n\nThe \"weak maximum principle\" for harmonic functions is a simple consequence of facts from calculus. The key ingredient for the proof is the fact that, by the definition of a harmonic function, the Laplacian of \"f\" is zero. Then, if formula_1 is a non-degenerate critical point of \"f\"(\"x\"), we must be seeing a saddle point, since otherwise there is no chance that the sum of the second derivatives of \"f\" is zero. This of course is not a complete proof, and we left out the case of formula_1 being a degenerate point, but this is the essential idea. \n\nThe \"strong maximum principle\" relies on the Hopf lemma, and this is more complicated.\n\n\n"}
{"id": "3980101", "url": "https://en.wikipedia.org/wiki?curid=3980101", "title": "N-monoid", "text": "N-monoid\n\nIn category theory, a (strict) \"n\"-monoid is an \"n\"-category with only one 0-cell. In particular, a 1-monoid is a monoid and a 2-monoid is a strict monoidal category.\n"}
{"id": "35783995", "url": "https://en.wikipedia.org/wiki?curid=35783995", "title": "National Museum of Mathematics", "text": "National Museum of Mathematics\n\nThe National Museum of Mathematics or MoMath is a museum dedicated to mathematics in Manhattan, New York City.\nIt opened on December 15, 2012. It is located at 11 East 26th Street between Fifth and Madison Avenues, across from Madison Square Park in the NoMad neighborhood. It is the only museum dedicated to mathematics in North America, and features over thirty interactive exhibits. The mission of the museum is to \"enhance public understanding and perception of mathematics\". The museum is known for a special tricycle with square wheels, which operates smoothly on a catenary surface.\n\nIn 2006 the Goudreau Museum on Long Island, at the time the only museum in the United States dedicated to mathematics, closed its doors. In response, a group led by founder Glen Whitney met to explore the opening of a new museum. They received a charter from the New York State Department of Education in 2009, and raised over 22 million dollars in under four years. \n\nWith this funding, a space was leased in the Goddard Building at 11-13 East 26th Street, located in the Madison Square North Historic District. Despite some opposition to the architectural plans within the local community, permission for construction was granted by the New York City Landmarks Preservation Commission and the Department of Buildings.\n\n\nIn October 2016, the exhibit \"The Insides of Things: The Art of Miguel Berrocal\" was opened, displaying a collection of puzzle sculptures by Spanish artist Miguel Ortiz Berrocal (1933-2006), donated by the late Samuel Sensiper. Each sculpture can be disassembled into small interlocking pieces, eventually revealing a small piece of jewelry or other surprise.\n\nOn August 2, 2018 MoMath announced the creation of a Distinguished Chair for the Public Dissemination of Mathematics. Princeton professor and Fields Medal winner Manjul Bhargava was named as the first recipient of this position.\n\n\n"}
{"id": "6084375", "url": "https://en.wikipedia.org/wiki?curid=6084375", "title": "Nullable type", "text": "Nullable type\n\nIn programming, nullable types are a feature of the type system of some programming languages which allow the value to be set to the special value NULL instead of the usual possible values of the data type. In statically-typed languages, a nullable type is an option type (in functional programming terms), while in dynamically-typed languages (where values have types, but variables do not), equivalent behavior is provided by having a single null value.\n\nPrimitive types such as integers and booleans cannot generally be null, but the corresponding nullable types (nullable integer and nullable boolean, respectively) can also assume the NULL value. NULL is frequently used to represent a missing value or invalid value, such as from a function that failed to return or a missing field in a database, as in NULL in SQL.\n\nAn integer variable may represent integers, but 0 (zero) is a special case because 0 in many programming languages can mean \"false\". Also this doesn't give us any notion of saying that the variable is empty, a need for which occurs in many circumstances. This need can be achieved with a nullable type. In programming languages like C# 2.0, a nullable integer, for example, can be declared by a question mark (int? x). In programming languages like C# 1.0, nullable types can be defined by an external library as new types (e.g. NullableInteger, NullableBoolean).\n\nA boolean variable makes the effect more clear. Its values can be either \"true\" or \"false\", while a nullable boolean may also contain a representation for \"undecided\". However, the interpretation or treatment of a logical operation involving such a variable depends on the language.\n\nIn contrast, object pointers can be set to NULL by default in most common languages, meaning that the pointer or reference points to nowhere, that no object is assigned (the variable does not point to any object).\nNullable references were invented by C. A. R. Hoare in 1965 as part of the Algol W language. Hoare later described their invention as a \"billion-dollar mistake\". This is because object pointers that can be NULL require the user to check the pointer before using it and require specific code to handle the case when the object pointer is NULL. \n\nJava has classes that correspond to scalar values, such as Integer, Boolean and Float. Combined with autoboxing (automatic usage-driven conversion between object and value), this effectively allows nullable variables for scalar values.\n\nNullable type implementations usually adhere to the null object pattern.\n\nThere is a more general and formal concept that extend the nullable type concept, it comes from option types, which enforce explicit handling of the exceptional case.\nOption type implementations usually adhere to the Special Case pattern.\n\nThe following programming languages support nullable types.\n\nStatically typed languages with native null support include:\n\nStatically typed languages with library null support include:\nDynamically-typed languages with null include:\n\n"}
{"id": "3159411", "url": "https://en.wikipedia.org/wiki?curid=3159411", "title": "Ostrowski Prize", "text": "Ostrowski Prize\n\nThe Ostrowski Prize is a mathematics award given every odd year for outstanding mathematical achievement judged by an international jury from the universities of Basel, Jerusalem, Waterloo and the academies of Denmark and the Netherlands. Alexander Ostrowski, a longtime professor at the University of Basel, left his estate to the foundation in order to establish a prize for outstanding achievements in pure mathematics and the foundations of numerical mathematics. It currently carries a monetary award of 100,000 Swiss francs.\n\nIts recipients are: \n"}
{"id": "6306271", "url": "https://en.wikipedia.org/wiki?curid=6306271", "title": "Outline of logic", "text": "Outline of logic\n\nLogic is the formal science of using reason and is considered a branch of both philosophy and mathematics. Logic investigates and classifies the structure of statements and arguments, both through the study of formal systems of inference and the study of arguments in natural language. The scope of logic can therefore be very large, ranging from core topics such as the study of fallacies and paradoxes, to specialized analyses of reasoning such as probability, correct reasoning, and arguments involving causality. One of the aims of logic is to identify the correct (or valid) and incorrect (or fallacious) inferences. Logicians study the criteria for the evaluation of arguments.\n\nPhilosophy of logic\n\nPhilosophical logic\n\nInformal logic\nCritical thinking\nArgumentation theory\n\n\n\n\n\nLogical connective\n\n\nProposition\n\nRule of inference  (list)\n\n\nObject language\n\nMetalanguage\n\nPropositional logic\n\n\nPredicate logic\n\n\nMathematical relation\n\nMathematical logic\n\nSet theory  (list)\n\nMetalogic – The study of the metatheory of logic.\n\nProof theory – The study of deductive apparatus.\n\nModel theory – The study of interpretation of formal systems.\n\nComputability theory – branch of mathematical logic that originated in the 1930s with the study of computable functions and Turing degrees. The field has grown to include the study of generalized computability and definability. The basic questions addressed by recursion theory are \"What does it mean for a function from the natural numbers to themselves to be computable?\" and \"How can noncomputable functions be classified into a hierarchy based on their level of noncomputability?\". The answers to these questions have led to a rich theory that is still being actively researched.\n\nClassical logic\n\nNon-classical logic\n\n\nModal logic\n\nMathematical logic –\n\nHistory of logic\n\n\n\n\n\n\n"}
{"id": "3688147", "url": "https://en.wikipedia.org/wiki?curid=3688147", "title": "Post–Turing machine", "text": "Post–Turing machine\n\nA Post–Turing machine is a \"program formulation\" of an especially simple type of Turing machine, comprising a variant of Emil Post's Turing-equivalent model of computation described below. (Post's model and Turing's model, though very similar to one another, were developed independently. Turing's paper was received for publication in May 1936, followed by Post's in October.) A Post–Turing machine uses a binary alphabet, an infinite sequence of binary storage locations, and a primitive programming language with instructions for bi-directional movement among the storage locations and alteration of their contents one at a time. The names \"Post–Turing program\" and \"Post–Turing machine\" were used by Martin Davis in 1973–1974 (Davis 1973, p. 69ff). Later in 1980, Davis used the name \"Turing–Post program\" (Davis, in Steen p. 241).\n\nIn his 1936 paper \"Finite Combinatory Processes—Formulation 1\" (which can be found on page 289 of \"Undecidable\"), Emil Post described a model of extreme simplicity which he conjectured is \"logically equivalent to recursiveness\", and which was later proved to be so. The quotes in the following are from this paper.\n\nPost's model of a computation differs from the Turing-machine model in a further \"atomization\" of the acts a human \"computer\" would perform during a computation.\n"}
{"id": "48897477", "url": "https://en.wikipedia.org/wiki?curid=48897477", "title": "Quotient of an abelian category", "text": "Quotient of an abelian category\n\nIn mathematics, the quotient (also called Serre quotient or Gabriel quotient) of an abelian category formula_1 by a Serre subcategory \"formula_2\" is the abelian category \"formula_3\" which, intuitively, is obtained from \"formula_1\" by ignoring (i.e. treating as zero) all objects from \"formula_2.\" There is a canonical exact functor formula_6 whose kernel is \"formula_2\".\n\nFormally, \"formula_3\" is the category whose objects are those of \"formula_1\" and whose morphisms from \"X\" to \"Y\" are given by the direct limit (of abelian groups) formula_10 over subobjects formula_11 and formula_12 such that formula_13 and formula_14. (Here, formula_15 and formula_16 denote quotient objects computed in \"formula_1\".) Composition of morphisms in \"formula_3\" is induced by the universal property of the direct limit. \n\nThe canonical functor formula_6 sends an object \"X\" to itself and a morphism formula_20 to the corresponding element of the direct limit with X'=X and Y'=0.\n\nLet formula_21 be a field and consider the abelian category formula_22 of all vector spaces over formula_21. Then the full subcategory formula_24of finite-dimensional vector spaces is a Serre-subcategory of formula_22. The quotient formula_26 has as objects the formula_21-vector spaces, and the set of morphisms from formula_28 to formula_29 in formula_30 is formula_31(which is a quotient of vector spaces). This has the effect of identifying all finite-dimensional vector spaces with 0, and of identifying two linear maps whenever their difference has finite-dimensional image.\n\nThe quotient \"formula_3\" is an abelian category, and the canonical functor formula_6 is exact. The kernel of formula_34 is \"formula_2\", i.e., formula_36is a zero object of \"formula_3\" if and only if formula_28 belongs to \"formula_2\". \n\nThe quotient and canonical functor are characterized by the following universal property: if \"formula_40\" is any abelian category and formula_41 is an exact functor such that formula_42 is a zero object of \"formula_40\" for each object formula_44, then there is a unique exact functor formula_45 such that formula_46.\n\nThe Gabriel–Popescu theorem states that any Grothendieck category formula_47 is equivalent to a quotient category formula_48, where formula_49denotes the abelian category of right modules over some unital ring formula_50, and formula_51 is some localizing subcategory of formula_49.\n"}
{"id": "1539548", "url": "https://en.wikipedia.org/wiki?curid=1539548", "title": "Reversible computing", "text": "Reversible computing\n\nReversible computing is a model of computing where the computational process to some extent is reversible, i.e., time-invertible. In a model of computation that uses deterministic transitions from one state of the abstract machine to another, a necessary condition for reversibility is that the relation of the mapping from (nonzero-probability) states to their successors must be one-to-one. Reversible computing is a form of unconventional computing.\n\nThere are two major, closely related types of reversibility that are of particular interest for this purpose: \"physical reversibility\" and \"logical reversibility\".\n\nA process is said to be \"physically reversible\" if it results in no increase in physical entropy; it is \"isentropic\". There is a style of circuit design ideally exhibiting this property that is referred to as charge recovery logic, adiabatic circuits, or adiabatic computing. Although \"in practice\" no nonstationary physical process can be \"exactly\" physically reversible or isentropic, there is no known limit to the closeness with which we can approach perfect reversibility, in systems that are sufficiently well isolated from interactions with unknown external environments, when the laws of physics describing the system's evolution are precisely known.\n\nProbably the largest motivation for the study of technologies aimed at actually implementing reversible computing is that they offer what is predicted to be the only potential way to improve the computational energy efficiency of computers beyond the fundamental von Neumann-Landauer limit of \"kT\" ln(2) energy dissipated per irreversible bit operation. Although the Landauer limit was millions of times below the energy consumption of computers in the 2000s and thousands of times less in the 2010s, proponents of reversible computing argue that this can be attributed largely to architectural overheads which effectively magnify the impact of Landauer's limit in practical circuit designs, so that it may prove difficult for practical technology to progress very far beyond current levels of energy efficiency if reversible computing principles are not used.\n\nAs was first argued by Rolf Landauer of IBM, in order for a computational process to be physically reversible, it must also be \"logically reversible\". Landauer's principle is the rigorously valid observation that the oblivious erasure of \"n\" bits of known information must always incur a cost of \"nkT\" ln(2) in thermodynamic entropy. A discrete, deterministic computational process is said to be logically reversible if the transition function that maps old computational states to new ones is a one-to-one function; i.e. the output logical states uniquely determine the input logical states of the computational operation.\n\nFor computational processes that are nondeterministic (in the sense of being probabilistic or random), the relation between old and new states is not a single-valued function, and the requirement needed to obtain physical reversibility becomes a slightly weaker condition, namely that the size of a given ensemble of possible initial computational states does not decrease, on average, as the computation proceeds forwards.\n\nLandauer's principle (and indeed, the second law of thermodynamics itself) can also be understood to be a direct logical consequence of the underlying reversibility of physics, as is reflected in the general Hamiltonian formulation of mechanics, and in the unitary time-evolution operator of quantum mechanics more specifically.\n\nThe implementation of reversible computing thus amounts to learning how to characterize and control the physical dynamics of mechanisms to carry out desired computational operations so precisely that we can accumulate a negligible total amount of uncertainty regarding the complete physical state of the mechanism, per each logic operation that is performed. In other words, we would need to precisely track the state of the active energy that is involved in carrying out computational operations within the machine, and design the machine in such a way that the majority of this energy is recovered in an organized form that can be reused for subsequent operations, rather than being permitted to dissipate into the form of heat.\n\nAlthough achieving this goal presents a significant challenge for the design, manufacturing, and characterization of ultra-precise new physical mechanisms for computing, there is at present no fundamental reason to think that this goal cannot eventually be accomplished, allowing us to someday build computers that generate much less than 1 bit's worth of physical entropy (and dissipate much less than \"kT\" ln 2 energy to heat) for each useful logical operation that they carry out internally.\n\nToday, the field has a substantial body of academic literature behind it. A wide variety of reversible device concepts, logic gates, electronic circuits, processor architectures, programming languages, and application algorithms have been designed and analyzed by physicists, electrical engineers, and computer scientists.\n\nThis field of research awaits the detailed development of a high-quality, cost-effective, nearly reversible logic device technology, one that includes highly energy-efficient clocking and synchronization mechanisms, or avoids the need for these through asynchronous design. This sort of solid engineering progress will be needed before the large body of theoretical research on reversible computing can find practical application in enabling real computer technology to circumvent the various near-term barriers to its energy efficiency, including the von Neumann-Landauer bound. This may only be circumvented by the use of logically reversible computing, due to the Second Law of Thermodynamics.\n\nTo implement reversible computation, estimate its cost, and to judge its limits, it can be formalized in terms of gate-level circuits. A simplified model of such circuits is one in which inputs are consumed (however, note that real logic gates as implemented e.g. in CMOS do not do this). In this modeling framework, an inverter (logic gate) (NOT) gate is reversible because it can be \"undone\". The exclusive or (XOR) gate is irreversible because its two inputs cannot be unambiguously reconstructed from its single output. However, a reversible version of the XOR gate—the controlled NOT gate (CNOT)—can be defined by preserving one of the inputs. The three-input variant of the CNOT gate is called the Toffoli gate. It preserves two of its inputs \"a,b\" and replaces the third \"c\" by formula_1. With formula_2, this gives the AND function, and with formula_3 this gives the NOT function. Thus, the Toffoli gate is universal and can implement any reversible Boolean function (given enough zero-initialized ancillary bits). More generally, reversible gates that consume their input have no more inputs than outputs. A reversible circuit connects reversible gates without fanouts and loops. Therefore, such circuits contain equal numbers of input and output wires, each going through an entire circuit. Similarly, in the Turing machine model of computation, a reversible Turing machine is one whose transition function is invertible, so that each machine state has at most one predecessor.\n\nSurveys of reversible circuits, their construction and optimization, as well as recent research challenges, are available.\n\n\n\n\n "}
{"id": "1970862", "url": "https://en.wikipedia.org/wiki?curid=1970862", "title": "Selmer group", "text": "Selmer group\n\nIn arithmetic geometry, the Selmer group, named in honor of the work of by , is a group constructed from an isogeny of abelian varieties.\n\nThe Selmer group of an abelian variety \"A\" with respect to an isogeny \"f\" : \"A\" → \"B\" of abelian varieties can be defined in terms of Galois cohomology as\n\nwhere \"A\"[\"f\"] denotes the \"f\"-torsion of \"A\" and formula_2 is the local Kummer map formula_3. Note that formula_4 is isomorphic to formula_5. Geometrically, the principal homogeneous spaces coming from elements of the Selmer group have \"K\"-rational points for all places \"v\" of \"K\". The Selmer group is finite. This implies that the part of the Tate–Shafarevich group killed by \"f\" is finite due to the following exact sequence\n\nThe Selmer group in the middle of this exact sequence is finite and effectively computable. This implies the weak Mordell–Weil theorem that its subgroup \"B\"(\"K\")/\"f\"(\"A\"(\"K\")) is finite. There is a notorious problem about whether this subgroup can be effectively computed: there is a procedure for computing it that will terminate with the correct answer if there is some prime \"p\" such that the \"p\"-component of the Tate–Shafarevich group is finite. It is conjectured that the Tate–Shafarevich group is in fact finite, in which case any prime \"p\" would work. However, if (as seems unlikely) the Tate–Shafarevich group has an infinite \"p\"-component for every prime \"p\", then the procedure may never terminate.\n\nMore generally one can define the Selmer group of a finite Galois module \"M\" (such as the kernel of an isogeny) as the elements of \"H\"(\"G\",\"M\") that have images inside certain given subgroups of \"H\"(\"G\",\"M\").\n\n"}
{"id": "95465", "url": "https://en.wikipedia.org/wiki?curid=95465", "title": "Stirling number", "text": "Stirling number\n\nIn mathematics, Stirling numbers arise in a variety of analytic and combinatorial problems. They are named after James Stirling, who introduced them in the 18th century. Two different sets of numbers bear this name: the Stirling numbers of the first kind and the Stirling numbers of the second kind. Additionally, Lah numbers are sometimes referred to as Stirling numbers of the third kind.\nEach kind is detailed in its respective article, this one serving as a description of relations between them.\n\nA common property of all three kinds is that they describe coefficients relating three different sequences of polynomials that frequently arise in combinatorics.\nMoreover, all three can be defined as the number of partitions of \"n\" elements into \"k\" non-empty subsets, with different ways of counting orderings within each subset.\n\nSeveral different notations for Stirling numbers are in use. Common notations are:\n\nfor unsigned Stirling numbers of the first kind,\nwhich count the number of permutations of \"n\" elements with \"k\" disjoint cycles,\n\nfor ordinary (signed) Stirling numbers of the first kind, and\n\nfor Stirling numbers of the second kind, which count the number of ways to partition a set of \"n\" elements into \"k\" nonempty subsets.\n\nFor example, the sum formula_4 is the number of all permutations,\nwhile the sum formula_5 is the \"n\"th Bell number.\n\nAbramowitz and Stegun use an uppercase S and a blackletter S, respectively, for the first and second kinds of Stirling number. The notation of brackets and braces, in analogy to binomial coefficients, was introduced in 1935 by Jovan Karamata and promoted later by Donald Knuth. (The bracket notation conflicts with a common notation for Gaussian coefficients.) The mathematical motivation for this type of notation, as well as additional Stirling number formulae, may be found on the page for Stirling numbers and exponential generating functions.\n\nStirling numbers express coefficients in expansions of falling and rising factorials (also known as the Pochhammer symbol) as polynomials.\n\nThat is, the falling factorial, defined as formula_6, is a polynomial in \"x\" of degree \"n\" whose expansion is\n\nwith (signed) Stirling numbers of the first kind as coefficients.\n\nNote that (\"x\") = 1 because it is an empty product. Combinatorialists also sometimes use the notation formula_8 for the falling factorial, and formula_9 for the rising factorial. (Confusingly, the Pochhammer symbol that many use for \"falling\" factorials is used in special functions for \"rising\" factorials.)\n\nSimilarly, the rising factorial, defined as formula_10, is a polynomial in \"x\" of degree \"n\" whose expansion is\n\nwith unsigned Stirling numbers of the first kind as coefficients.\nOne expansion can be derived from the other by observing that formula_12.\n\nStirling numbers of the second kind express reverse relations:\n\nand\n\nConsidering the set of polynomials in the (indeterminate) variable \"x\" as a vector space,\neach of the three sequences\nis a basis.\nThat is, every polynomial in \"x\" can be written as a sum formula_16 for some unique coefficients formula_17 (similarly for the other two bases).\nThe above relations then express the change of basis between them, as summarized in the following commutative diagram:\nThe coefficients for the two bottom changes are described by the Lah numbers below.\nSince coefficients in any basis are unique, one can define Stirling numbers this way, as the coefficients expressing polynomials of one basis in terms of another, that is, the unique numbers relating formula_18 with falling and rising factorials as above.\n\nFalling factorials define, up to scaling, the same polynomials as binomial coefficients: formula_19. The changes between the standard basis formula_20 and the basis formula_21 are thus described by similar formulas:\n\nThe Stirling numbers of the first and second kinds can be considered inverses of one another:\n\nand\n\nwhere formula_26 is the Kronecker delta. These two relationships may be understood to be matrix inverse relationships. That is, let \"s\" be the lower triangular matrix of Stirling numbers of the first kind, whose matrix elements\nformula_27\nThe inverse of this matrix is \"S\", the lower triangular matrix of Stirling numbers of the second kind, whose entries are formula_28 Symbolically, this is written\n\nAlthough \"s\" and \"S\" are infinite, so calculating a product entry involves an infinite sum, the matrix multiplications work because these matrices are lower triangular, so only a finite number of terms in the sum are nonzero.\n\nExpressing a polynomial in the basis of falling factorials is useful for calculating sums of the polynomial evaluated at consecutive integers.\nIndeed, the sum of a falling factorial is simply expressed as another falling factorial (for \"k≠-1\")\n\nThis is analogous to the integral formula_31, though the sum should be over integers \"i\" strictly less than \"n\".\n\nFor example, the sum of fourth powers of integers up to \"n\" (this time with \"n\" included), is:\n\nHere the Stirling numbers can be computed from their definition as the number of partitions of 4 elements into \"k\" non-empty unlabeled subsets.\n\nIn contrast, the sum formula_33 in the standard basis is given by Faulhaber's formula, which in general is more complex.\n\nThe Lah numbers formula_34 are sometimes called Stirling numbers of the third kind.\nBy convention, formula_35 and formula_36 if formula_37 or formula_38.\n\nThese numbers are coefficients expressing falling factorials in terms of rising factorials and vice versa:\n\nAs above, this means they express the change of basis between the bases formula_41 and formula_42, completing the diagram.\nIn particular, one formula is the inverse of the other, thus:\n\nSimilarly, composing for example the change of basis from formula_44 to formula_18 with the change of basis from formula_18 to formula_47 gives the change of basis directly from formula_44 to formula_47:\n\nIn terms of matrices, if formula_51 denotes the matrix with entries formula_52 and formula_53 denotes the matrix with entries formula_54, then one is the inverse of the other: formula_55.\nSimilarly, composing the matrix of unsigned Stirling numbers of the first kind with the matrix of Stirling numbers of the second kind gives the Lah numbers: formula_56.\n\nThe numbers formula_57 can be defined as the number of partitions of \"n\" elements into \"k\" non-empty unlabeled subsets, each of which is unordered, cyclically ordered, or linearly ordered, respectively. In particular, this implies the following inequalities:\n\nAbramowitz and Stegun give the following symmetric formulae that relate the Stirling numbers of the first and second kind.\n\nand\n\nThe Stirling numbers can be extended to negative integral values, but not all authors do so in the same way. Regardless of the approach taken, it is worth noting that Stirling numbers of first and second kind are connected by the relations:\n\nwhen \"n\" and \"k\" are nonnegative integers. So we have following table for formula_62:\nDonald Knuth defined the more general Stirling numbers by extending a recurrence relation to all integers. In this approach, formula_63 and formula_64 are zero if \"n\" is negative and \"k\" is nonnegative, or if \"n\" is nonnegative and \"k\" is negative, and so we have, for \"any\" integers \"n\" and \"k\",\n\nOn the other hand, for positive integers \"n\" and \"k\", David Branson defined formula_66, formula_67, \nformula_68,\n\nFor example, formula_69. This leads to the following table of values of formula_70.\nIn this case formula_71 where formula_72 is a Bell number, and so one may define the negative Bell numbers by formula_73. For example, this produces formula_74.\n\n\n"}
{"id": "3070794", "url": "https://en.wikipedia.org/wiki?curid=3070794", "title": "Ternary Golay code", "text": "Ternary Golay code\n\nIn coding theory, the ternary Golay codes are two closely related error-correcting codes.\nThe code generally known simply as the ternary Golay code is an formula_1-code, that is, it is a linear code over a ternary alphabet; the relative distance of the code is as large as it possibly can be for a ternary code, and hence, the ternary Golay code is a perfect code.\nThe extended ternary Golay code is a [12, 6, 6] linear code obtained by adding a zero-sum check digit to the [11, 6, 5] code.\nIn finite group theory, the extended ternary Golay code is sometimes referred to as the ternary Golay code.\n\nThe ternary Golay code consists of 3 = 729 codewords. \nIts parity check matrix is\nAny two different codewords differ in at least 5 positions.\nEvery ternary word of length 11 has a Hamming distance of at most 2 from exactly one codeword.\nThe code can also be constructed as the quadratic residue code of length 11 over the finite field F. \n\nUsed in a football pool with 11 games, the ternary Golay code corresponds to 729 bets and guarantees exactly one bet with at most 2 wrong outcomes.\n\nThe set of codewords with Hamming weight 5 is a 3-(11,5,4) design.\n\nThe complete weight enumerator of the extended ternary Golay code is\n\nThe automorphism group of the extended ternary Golay code is 2.\"M\", where \"M\" is the Mathieu group M12. \n\nThe extended ternary Golay code can be constructed as the span of the rows of a Hadamard matrix of order 12 over the field F. \n\nConsider all codewords of the extended code which have just six nonzero digits. The sets of positions at which these nonzero digits occur form the Steiner system S(5, 6, 12).\n\nThe ternary Golay code was discovered by . It was independently discovered two years earlier by the Finnish football pool enthusiast Juhani Virtakallio, who published it in 1947 in issues 27, 28 and 33 of the football magazine \"Veikkaaja\". \n\n\n"}
{"id": "8711785", "url": "https://en.wikipedia.org/wiki?curid=8711785", "title": "Ulam number", "text": "Ulam number\n\nAn Ulam number is a member of an integer sequence devised by and named after Stanislaw Ulam, who introduced it in 1964. The standard Ulam sequence (the (1, 2)-Ulam sequence) starts with \"U\" = 1 and \"U\" = 2. Then for \"n\" > 2, \"U\" is defined to be the smallest integer that is the sum of two distinct earlier terms in exactly one way and larger than all earlier terms.\n\nAs a consequence of the definition, 3 is an Ulam number (1+2); and 4 is an Ulam number (1+3). (Here 2+2 is not a second representation of 4, because the previous terms must be distinct.) The integer 5 is not an Ulam number, because 5 = 1 + 4 = 2 + 3. The first few terms are\n\nThere are infinitely many Ulam numbers. For, after the first \"n\" numbers in the sequence have already been determined, it is always possible to extend the sequence by one more element: is uniquely represented as a sum of two of the first \"n\" numbers, and there may be other smaller numbers that are also uniquely represented in this way, so the next element can be chosen as the smallest of these uniquely representable numbers.\n\nUlam is said to have conjectured that the numbers have zero density, but they seem to have a density of approximately 0.07398.\n\nIt has been observed that the first 10 million Ulam numbers satisfy formula_1 except for the four elements formula_2 (this has now been verified up to formula_3). Inequalities of this type are usually true for sequences exhibiting some form of periodicity but the Ulam sequence does not seem to be periodic and the phenomenon is not understood. It can be exploited to do a fast computation of the Ulam sequence (see external links).\n\nThe idea can be generalized as (\"u\", \"v\")-Ulam numbers by selecting different starting values (\"u\", \"v\"). A sequence of (\"u\", \"v\")-Ulam numbers is \"regular\" if the sequence of differences between consecutive numbers in the sequence is eventually periodic. When \"v\" is an odd number greater than three, the (2, \"v\")-Ulam numbers are regular. When \"v\" is congruent to 1 (mod 4) and at least five, the (4, \"v\")-Ulam numbers are again regular. However, the Ulam numbers themselves do not appear to be regular.\n\nA sequence of numbers is said to be \"s\"-additive if each number in the sequence, after the initial 2\"s\" terms of the sequence, has exactly \"s\" representations as a sum of two previous numbers. Thus, the Ulam numbers and the (\"u\", \"v\")-Ulam numbers are 1-additive sequences.\n\nIf a sequence is formed by appending the largest number with a unique representation as a sum of two earlier numbers, instead of appending the smallest uniquely representable number, then the resulting sequence is the sequence of Fibonacci numbers.\n\n\n"}
{"id": "18476346", "url": "https://en.wikipedia.org/wiki?curid=18476346", "title": "Unique sink orientation", "text": "Unique sink orientation\n\nIn mathematics, a unique sink orientation is an orientation of the edges of a polytope such that, in every face of the polytope (including the whole polytope as one of the faces), there is exactly one vertex for which all adjoining edges are oriented inward (i.e. towards that vertex). If a polytope is given together with a linear objective function, and edges are oriented from vertices with smaller objective function values to vertices with larger objective values, the result is a unique sink orientation. Thus, unique sink orientations can be used to model linear programs as well as certain nonlinear programs such as the smallest circle problem.\n\nThe problem of finding the sink in a unique sink orientation of a hypercube was formulated as an abstraction of linear complementarity problems by .\nIt is possible for an algorithm to determine the unique sink of a -dimensional hypercube in time for , substantially smaller than the time required to examine all vertices. When the orientation has the additional property that the orientation forms a directed acyclic graph, which happens when unique sink orientations are used to model LP-type problems, it is possible to find the sink using a randomized algorithm in expected time exponential in the square root of \"d\" .\n\nA simple \"d\"-dimensional polytope is a polytope in which every vertex has exactly \"d\" incident edges. In a unique-sink orientation of a simple polytope, every subset of \"k\" incoming edges at a vertex \"v\" determines a \"k\"-dimensional face for which \"v\" is the unique sink. Therefore, the number of faces of all dimensions of the polytope (including the polytope itself, but not the empty set) can be computed by the sum of the number of subsets of incoming edges,\nwhere \"G\"(\"P\") is the graph of the polytope, and \"d\"(\"v\") is the in-degree (number of incoming edges) of a vertex \"v\" in the given orientation .\n\nMore generally, for any orientation of a simple polytope, the same sum counts the number of incident pairs of a face of the polytope and a sink of the face. And in an acyclic orientation, every face must have at least one sink. Therefore, an acyclic orientation is a unique sink orientation if and only if there is no other acyclic orientation with a smaller sum. Additionally, a \"k\"-regular subgraph of the given graph forms a face of the polytope if and only if its vertices form a lower set for at least one acyclic unique sink orientation. In this way, the face lattice of the polytope is uniquely determined from the graph . Based on this structure, the face lattices of simple polytopes can be reconstructed from their graphs in polynomial time using linear programming .\n\n"}
{"id": "32394679", "url": "https://en.wikipedia.org/wiki?curid=32394679", "title": "Voorhoeve index", "text": "Voorhoeve index\n\nIn mathematics, the Voorhoeve index is a non-negative real number associated with certain functions on the complex numbers, named after Marc Voorhoeve. It may be used to extend Rolle's theorem from real functions to complex functions, taking the role that for real functions is played by the number of zeros of the function in an interval.\n\nThe Voorhoeve index formula_1 of a complex-valued function \"f\" that is analytic in a complex neighbourhood of the real interval formula_2 = [\"a\", \"b\"] is given by\n\nRolle's theorem states that if \"f\" is a continuously differentiable real-valued function on the real line, and \"f\"(\"a\") = \"f\"(\"b\") = 0, where \"a\" < \"b\", then its derivative \"f\" ' must have a zero strictly between \"a\" and \"b\". Or, more generally, if formula_4 denotes the number of zeros of the continuously differentiable function \"f\" on the interval formula_2, then formula_4 ≤ formula_7(\"f\" ') + 1.\n\nNow one has the analogue of Rolle's theorem:\n\nThis leads to bounds on the number of zeros of an analytic function in a complex region.\n\n"}
{"id": "3301463", "url": "https://en.wikipedia.org/wiki?curid=3301463", "title": "Weighing matrix", "text": "Weighing matrix\n\nIn mathematics, a weighing matrix \"W\" of order \"n\" and weight \"w\" is an \"n\" × \"n\" (0,1,-1)-matrix such that formula_1, where formula_2 is the transpose of formula_3 and formula_4 is the identity matrix of order formula_5.\n\nFor convenience, a weighing matrix of order \"n\" and weight \"w\" is often denoted by \"W\"(\"n\",\"w\"). A \"W\"(\"n\",\"n\") is a Hadamard matrix and a \"W(n,n-1)\" is equivalent to a conference matrix.\n\nSome properties are immediate from the definition. If \"W\" is a \"W\"(\"n\",\"w\"), then:\n\nNote that when weighing matrices are displayed, the symbol formula_13 is used to represent -1. Here are two examples:\n\nThis is a \"W\"(\"2\",\"2\"):\n\nThis is a \"W\"(\"7\",\"4\"):\n\nTwo weighing matrices are considered to be equivalent if one can be obtained from the other by a series of permutations and negations of the rows and columns of the matrix. The classification of weighing matrices is complete for cases where \"w\" ≤ 5 as well as all cases where \"n\" ≤ 15 are also completed. However, very little has been done beyond this with exception to classifying circulant weighing matrices.\n\nThere are many open questions about weighing matrices. The main question about weighing matrices is their existence: for which values of \"n\" and \"w\" does there exist a \"W\"(\"n\",\"w\")? A great deal about this is unknown. An equally important but often overlooked question about weighing matrices is their enumeration: for a given \"n\" and \"w\", how many \"W\"(\"n\",\"w\")'s are there?\n\nThis question has two different meanings. Enumerating up to equivalence and enumerating different matrices with same n,k parametets. Some papers were published on the first question but none were published on the second important question.\n"}
{"id": "8810330", "url": "https://en.wikipedia.org/wiki?curid=8810330", "title": "Zonal and meridional", "text": "Zonal and meridional\n\nThe terms zonal and meridional are used to describe directions on a globe.\n\nZonal means \"along a latitudinal circle\" or \"in the west–east direction.\" Zonal flow is a meteorological term regarding atmospheric circulation following a general flow pattern along latitudinal lines, as opposed to meridional flow along longitudinal lines. Zonal, in the context of physics, connotes a tendency of flux to conform to a pattern parallel to the equator of a sphere. Generally, zonal flow of the atmosphere brings a temperature contrast along the Earth's longitude. Extratropical cyclones in this environment tend to be weaker, moving faster and producing relatively little impact on local weather.\n\nMeridional means \"along a longitudinal circle\" (a.k.a. \"meridian\") or \"in the north–south direction\" . Meridional flow is a general air flow pattern from north to south, or from south to north, along the Earth's longitude lines (perpendicular to a zonal flow). Extratropical cyclones in this environment tend to be stronger and move slower. This pattern is responsible for most instances of extreme weather, as not only are storms stronger in this type of flow regime, but temperatures can reach extremes as well, producing heat waves and cold waves depending on the equator-ward or poleward direction of the flow.\n\nThese terms are often used in the atmospheric and earth sciences to describe global phenomena, such as \"meridional wind\", or \"zonal average temperature\". (Strictly speaking, zonal means more than simply a direction as it also implies a degree of localization in the meridional direction, so that the phenomenon in question is localized to a zone of the planet.)\n\n\"Meridional\" is also used to describe the axis close to the chain orientation in a polymer fiber, while the term \"equatorial\" is used to describe the direction normal to the fiber axis.\n\nFor vector fields (such as wind velocity), the zonal component (or \"x\"-coordinate) is denoted as \"u\", while the meridional component (or \"y\"-coordinate) is denoted as \"v\".\n\nThe word comes from Latin \"meri dies\" (\"midday\"), meaning the position of the Sun at that time. As the original Latin territory was in the Northern Hemisphere, this is still used with that sense in some Romance languages such as Portuguese (Banco Meridional, in Brazil), Spanish, French, Italian (as in Meridione) or even in English (as in the Norma Jean album \"Meridional\").\n\nThe term meridional, sometimes abbreviated to \"Mer.\", was used in historical astronomy to indicate the southern direction on the celestial globe, together with septentrional (\"Sep.\") for northern, oriental (\"Ori.\") for eastern and occidental (\"Occ.\") for western.\n\n"}
{"id": "1674255", "url": "https://en.wikipedia.org/wiki?curid=1674255", "title": "Édouard Goursat", "text": "Édouard Goursat\n\nÉdouard Jean-Baptiste Goursat (21 May 1858 – 25 November 1936) was a French mathematician, now remembered principally as an expositor for his \"Cours d'analyse mathématique\", which appeared in the first decade of the twentieth century. It set a standard for the high-level teaching of mathematical analysis, especially complex analysis. This text was reviewed by William Fogg Osgood for the Bulletin of the American Mathematical Society. This led to its translation in English by Earle Raymond Hedrick published by Ginn and Company. Goursat also published texts on partial differential equations and hypergeometric series.\n\nEdouard Goursat was born in Lanzac, Lot. He was a graduate of the École Normale Supérieure, where he later taught and developed his \"Cours\". At that time the topological foundations of complex analysis were still not clarified, with the Jordan curve theorem considered a challenge to mathematical rigour (as it would remain until L. E. J. Brouwer took in hand the approach from combinatorial topology). Goursat’s work was considered by his contemporaries, including G. H. Hardy, to be exemplary in facing up to the difficulties inherent in stating the fundamental Cauchy integral theorem properly. For that reason it is sometimes called the Cauchy–Goursat theorem.\n\nGoursat was the first to note that the generalized Stokes theorem can be written in the simple form\n\nwhere formula_2 is a \"p\"-form in \"n\"-space and \"S\" is the \"p\"-dimensional boundary of the (\"p\" + 1)-dimensional region \"T\". Goursat also used differential forms to state the Poincaré lemma and its converse, namely, that if formula_2 is a \"p\"-form, then formula_4 if and only if there is a (\"p\" − 1)-form formula_5 with\nformula_6. However Goursat did not notice that the \"only if\" part of the result depends on the domain of formula_2 and is not true in general. E. Cartan himself in 1922 gave a counterexample, which provided one of the impulses in the next decade for the development of the De Rham cohomology of a differential manifold.\n\n\n\n"}
