{"id": "6623196", "url": "https://en.wikipedia.org/wiki?curid=6623196", "title": "9999 (number)", "text": "9999 (number)\n\nNine thousand nine-hundred ninety-nine (9999) is the natural number following 9998 and preceding 10000.\n\n9999 is an auspicious number in Chinese folklore. Many estimations of the rooms contained the Forbidden City point to 9999. Chinese tomb contracts often involved being buried with 9999 coins, relational notion to Joss paper, as it was believed the dead would need that amount to buy the burial plot from the Earth goddess.\n\n9999 is also the emergency telephone number in Oman.\n\n9999 can be used as a divisor to generate 4-digit decimal recurrences. For example, 1234 / 9999 = 0.123412341234... .\n\n9999 is a Kaprekar number.\n\n9999 was the last possible line number in some older programming languages such as BASIC. Often the line \"9999 END\" was the first line written for a new program.\n\nSome very old software used \"9999\" as end of file, however no problems occurred September 9, 1999\n\nThe King of Fighters character K9999 has the number on his name, although it is read as \"kay-four-nine\"\n"}
{"id": "35475502", "url": "https://en.wikipedia.org/wiki?curid=35475502", "title": "Acme Commodity and Phrase Code", "text": "Acme Commodity and Phrase Code\n\nAcme Commodity and Phrase Code is a codebook providing the general-purpose commercial telegraph code known as the \"Acme Code\". It was published in 1923 by the Acme Code Company. The book provides a listing of condensed terms and codes used to shorten telegrams and save money. The book was extremely popular amongst businesses in the 1930s. This code was one of the few telegram codes permitted by the Allied powers during the Second World War.\n\nThe \"Acme\" code consists of one hundred thousand five letter codes each intended to stand in for a phrase.\nIt was designed to be tolerant of transposition errors; the author claims that \"no transposition of any two adjoining letters will\nmake another word in the book\". However, as later discovered by J. Reeds, the code did not provide this level of error correction, containing at least eleven pairs of words differing only by the transposition of two letters.\nDespite these errors, this code is a precursor to more modern error correction codes.\n"}
{"id": "3348350", "url": "https://en.wikipedia.org/wiki?curid=3348350", "title": "Agent Communications Language", "text": "Agent Communications Language\n\nAgent Communication Language (ACL), proposed by the Foundation for Intelligent Physical Agents (FIPA), is a proposed standard language for agent communications. Knowledge Query and Manipulation Language (KQML) is another proposed standard.\n\nThe most popular ACLs are:\n\n\nBoth rely on speech act theory developed by Searle in the 1960s and enhanced by Winograd and Flores in the 1970s. They define a set of performatives, also called Communicative Acts, and their meaning (e.g. ask-one). The content of the performative is not standardized, but varies from system to system.\n\nTo make agents understand each other they have to not only speak the same language, but also have a common ontology. An ontology is a part of the agent's knowledge base that describes what kind of things an agent can deal with and how they are related to each other.\n\nExamples of frameworks that implement a standard agent communication language (FIPA-ACL) include FIPA-OS\nand Jade.\n"}
{"id": "45601812", "url": "https://en.wikipedia.org/wiki?curid=45601812", "title": "Antoni Malet", "text": "Antoni Malet\n\nAntoni Malet (born 23 February 1950) is a Catalan historian of mathematics. He is a professor of history of science at Pompeu Fabra University, Barcelona. His research interests are mostly in the history of mathematics and optics in the sixteenth and seventeenth centuries.\n\nMalet earned his Ph.D. in 1989 from Princeton University as a student of Charles Gillispie, with the thesis \"Studies on James Gregorie (1638–1675)\".\n\n\n"}
{"id": "764613", "url": "https://en.wikipedia.org/wiki?curid=764613", "title": "Axiom schema", "text": "Axiom schema\n\nIn mathematical logic, an axiom schema (plural: axiom schemata or axiom schemas) generalizes the notion of axiom.\n\nAn axiom schema is a formula in the metalanguage of an axiomatic system, in which one or more schematic variables appear. These variables, which are metalinguistic constructs, stand for any term or subformula of the system, which may or may not be required to satisfy certain conditions. Often, such conditions require that certain variables be free, or that certain variables not appear in the subformula or term.\n\nGiven that the number of possible subformulas or terms that can be inserted in place of a schematic variable is countably infinite, an axiom schema stands for a countably infinite set of axioms. This set can usually be defined recursively. A theory that can be axiomatized without schemata is said to be \"finitely axiomatized\". Theories that can be finitely axiomatized are seen as a bit more metamathematically elegant, even if they are less practical for deductive work.\n\nTwo very well known instances of axiom schemata are the:\nCzesław Ryll-Nardzewski proved that Peano arithmetic cannot be finitely axiomatized and Richard Montague proved that ZFC cannot be finitely axiomatized. Hence, the axiom schemata cannot be eliminated from these theories. This is also the case for quite a few other axiomatic theories in mathematics, philosophy, linguistics, etc.\n\nAll theorems of ZFC are also theorems of von Neumann–Bernays–Gödel set theory, but the latter can be finitely axiomatized. The set theory New Foundations can be finitely axiomatized, but only with some loss of elegance.\n\nSchematic variables in first-order logic are usually trivially eliminable in second-order logic, because a schematic variable is often a placeholder for any property or relation over the individuals of the theory. This is the case with the schemata of \"Induction\" and \"Replacement\" mentioned above. Higher-order logic allows quantified variables to range over all possible properties or relations.\n\n\n"}
{"id": "6751583", "url": "https://en.wikipedia.org/wiki?curid=6751583", "title": "Bochner space", "text": "Bochner space\n\nIn mathematics, Bochner spaces are a generalization of the concept of \"L\" spaces to functions whose values lie in a Banach space which is not necessarily the space R or C of real or complex numbers.\n\nThe space \"L(X)\" consists of (equivalence classes of) all Bochner measurable functions \"f\" with values in the Banach space \"X\" whose norm \"||f||\" lies in the standard \"L\" space. Thus, if \"X\" is the set of complex numbers, it is the standard Lebesgue \"L\" space.\n\nAlmost all standard results on \"L\" spaces do hold on Bochner spaces too; in particular, the Bochner spaces \"L(X)\" are Banach spaces for formula_1.\n\nBochner spaces are named for the Polish-American mathematician Salomon Bochner.\n\nBochner spaces are often used in the functional analysis approach to the study of partial differential equations that depend on time, e.g. the heat equation: if the temperature formula_2 is a scalar function of time and space, one can write formula_3 to make \"f\" a family \"f(t)\" (parametrized by time) of functions of space, possibly in some Bochner space.\n\nGiven a measure space (\"T\", Σ, \"μ\"), a Banach space (\"X\", || · ||) and 1 ≤ \"p\" ≤ +∞, the Bochner space \"L\"(\"T\"; \"X\") is defined to be the Kolmogorov quotient (by equality almost everywhere) of the space of all Bochner measurable functions \"u\" : \"T\" → \"X\" such that the corresponding norm is finite:\n\nIn other words, as is usual in the study of \"L\" spaces, \"L\"(\"T\"; \"X\") is a space of equivalence classes of functions, where two functions are defined to be equivalent if they are equal everywhere except upon a \"μ\"-measure zero subset of \"T\". As is also usual in the study of such spaces, it is usual to abuse notation and speak of a \"function\" in \"L\"(\"T\"; \"X\") rather than an equivalence class (which would be more technically correct).\n\nVery often, the space \"T\" is an interval of time over which we wish to solve some partial differential equation, and \"μ\" will be one-dimensional Lebesgue measure. The idea is to regard a function of time and space as a collection of functions of space, this collection being parametrized by time. For example, in the solution of the heat equation on a region Ω in R and an interval of time [0, \"T\"], one seeks solutions\nwith time derivative\nHere formula_8 denotes the Sobolev Hilbert space of once-weakly differentiable functions with first weak derivative in \"L\"²(Ω) that vanish at the boundary of Ω (in the sense of trace, or, equivalently, are limits of smooth functions with compact support in Ω); formula_9 denotes the dual space of formula_8.\n\n"}
{"id": "744589", "url": "https://en.wikipedia.org/wiki?curid=744589", "title": "Column generation", "text": "Column generation\n\nColumn generation or delayed column generation is an efficient algorithm for solving larger linear programs.\n\nThe overarching idea is that many linear programs are too large to consider all the variables explicitly. Since most of the variables will be non-basic and assume a value of zero in the optimal solution, only a subset of variables need to be considered in theory when solving the problem. Column generation leverages this idea to generate only the variables which have the potential to improve the objective function—that is, to find variables with negative reduced cost (assuming without loss of generality that the problem is a minimization problem).\n\nThe problem being solved is split into two problems: the master problem and the subproblem. The master problem is the original problem with only a subset of variables being considered. The subproblem is a new problem created to identify a new variable. The objective function of the subproblem is the reduced cost of the new variable with respect to the current dual variables, and the constraints require that the variable obeys the naturally occurring constraints.\n\nThe process works as follows. The master problem is solved—from this solution, we are able to obtain dual prices for each of the constraints in the master problem. This information is then utilized in the objective function of the subproblem. The subproblem is solved. If the objective value of the subproblem is negative, a variable with negative reduced cost has been identified. This variable is then added to the master problem, and the master problem is re-solved. Re-solving the master problem will generate a new set of dual values, and the process is repeated until no negative reduced cost variables are identified. The subproblem returns a solution with non-negative reduced cost, we can conclude that the solution to the master problem is optimal. \n\nIn many cases, this allows large linear programs that had been previously considered intractable to be solved. The classical example of a problem where this is successfully used is the cutting stock problem. One particular technique in linear programming which uses this kind of approach is the Dantzig–Wolfe decomposition algorithm. Additionally, column generation has been applied to many problems such as crew scheduling, vehicle routing, and the capacitated p-median problem.\n"}
{"id": "1951424", "url": "https://en.wikipedia.org/wiki?curid=1951424", "title": "Critical point (mathematics)", "text": "Critical point (mathematics)\n\nIn mathematics, a critical point or stationary point of a differentiable function of a real or complex variable is any value in its domain where its derivative is 0. Some authors also classify as critical points any limit points where the function may be or where the derivative is not defined. For a differentiable function of several real variables, a critical point is a value in its domain where all partial derivatives are zero. The value of the function at a critical point is a critical value.\n\nThe interest of this notion lies in the fact that the points where the function has local extrema are critical points.\n\nThis definition extends to differentiable maps between R and R, a critical point being, in this case, a point where the rank of the Jacobian matrix is not maximal. It extends further to differentiable maps between differentiable manifolds, as the points where the rank of the Jacobian matrix decreases. In this case, critical points are also called \"bifurcation points\".\n\nIn particular, if \"C\" is a plane curve, defined by an implicit equation \"f\"(\"x\",\"y\") = 0, the critical points of the projection onto the \"x\"-axis, parallel to the \"y\"-axis are the points where the tangent to \"C\" are parallel to the \"y\"-axis, that is the points where formula_1 In other words, the critical points are those where the implicit function theorem does not apply.\n\nThe notion of a \"critical point\" allows the mathematical description of an astronomical phenomenon that was unexplained before the time of Copernicus. A stationary point in the orbit of a planet is a point of the trajectory of the planet on the celestial sphere, where the motion of the planet seems to stop before restarting in the other direction. This occurs because of a critical point of the projection of the orbit into the ecliptic circle.\n\nA critical point or stationary point of a differentiable function of a single real variable, \"f\"(\"x\"), is a value \"x\" in the domain of \"f\" where its derivative is 0: \"f\" ′(\"x\") = 0. A critical value is the image under \"f\" of a critical point. These concepts may be visualized through the graph of \"f\": at a critical point, the graph has a horizontal tangent and the derivative of the function is zero.\n\nAlthough it is easily visualized on the graph (which is a curve), the notion of critical point of a function must not be confused with the notion of critical point, in some direction, of a curve (see below for a detailed definition). If \"g\"(\"x\",\"y\") is a differentiable function of two variables, then \"g\"(\"x\",\"y\") = 0 is the implicit equation of a curve. A critical point of such a curve, for the projection parallel to the \"y\"-axis (the map (\"x\", \"y\") → \"x\"), is a point of the curve where formula_2 This means that the tangent of the curve is parallel to the \"y\"-axis, and that, at this point, \"g\" does not define an implicit function from \"x\" to \"y\" (see implicit function theorem). If (\"x\", \"y\") is such a critical point, then \"x\" is the corresponding critical value. Such a critical point is also called a bifurcation point, as, generally, when \"x\" varies, there are two branches of the curve on a side of \"x\" and zero on the other side.\n\nIt follows from these definitions that the function \"f\"(\"x\") has a critical point \"x\" with critical value \"y\", if and only if (\"x\", \"y\") is a critical point of its graph for the projection parallel to the \"x\"-axis, with the same critical value \"y\".\n\nFor example, the critical points of the unit circle of equation \"x\" + \"y\" - 1 = 0 are (0, 1) and (0, -1) for the projection parallel to the \"y\"-axis, and (1, 0) and (-1, 0) for the direction parallel to the \"x\"-axis. If one considers the upper half circle as the graph of the function formula_3 then \"x\" = 0 is the unique critical point, with critical value 1. The critical points of the circle for the projection parallel to the \"y\"-axis correspond exactly to the points where the derivative of \"f\" is not defined.\n\nSome authors define the critical points of a function \"f\" as the \"x\"-values for which the graph has a critical point for the projection parallel to either axis. In the above example of the upper half circle, the critical points for this enlarged definition are -1, 0 and -1. Such a definition appears, usually, only in elementary textbooks, when the critical points are defined before any definition of other curves than graphs of functions, and when functions of several variables are not considered (the enlarged definition does not extend to this case).\n\n\nBy the Gauss-Lucas theorem, all of a polynomial function's critical points in the complex plane are within the convex hull of the roots of the function. Thus for a polynomial function with only real roots, all critical points are real and are between the greatest and smallest roots.\n\nSendov's conjecture asserts that, if all of a function's roots lie in the unit disk in the complex plane, then there is at least one critical point within unit distance of any given root.\n\nCritical points play an important role in the study of plane curves defined by implicit equations, in particular for sketching them and determining their topology. The notion of critical point that is used in this section, may seem different from that of previous section. In fact it is the specialization to a simple case of the general notion of critical point given below.\n\nThus, we consider a curve defined by an implicit equation formula_4 where is a differentiable function of two variables, commonly a bivariate polynomial. The points of the curve are the points of the Euclidean plane whose Cartesian coordinates satisfy the equation. There are two standard projections formula_5 and formula_6, defined by formula_7 and formula_8 that map the curve onto the coordinate axes. They are called the \"projection parallel to the y-axis\" and the \"projection parallel to the x-axis\", respectively.\n\nA point of is critical for formula_5, if the tangent to exists and is parallel to the \"y\"-axis. In that case, the images by formula_5 of the critical point and of the tangent are the same point of the \"x\"-axis, called the critical value. Thus a point is critical for formula_5 if its coordinates are solution of the system of equations\n\nThis implies that this definition is a special case of the general definition of a critical point, which is given below.\n\nThe definition of a critical point for formula_6 is similar. If is the graph of a function formula_14, then is critical for formula_6 if and only if is a critical point of , and that the critical values are the same.\n\nSome authors define the critical points of as the points that are critical for either formula_6 or formula_5, although they depend not only on , but also on the choice of the coordinate axes. It depends also on the authors if the singular points are considered as critical points. In fact the singular points are the points that satisfy\n\nand are thus solutions of either system of equations characterizing the critical points. With this more general definition, the critical points for formula_5 are exactly the points where the implicit function theorem does not apply.\n\nWhen the curve is algebraic, that is when it is defined by a bivariate polynomial , then the discriminant is a useful tool to compute the critical points.\n\nHere we consider only the projection formula_20 Similar results apply to formula_6 by exchanging and .\n\nLet\nformula_22\nbe the discriminant of viewed as a polynomial in with coefficients that are polynomials in . This discriminant is thus a polynomial in which has the critical values of formula_5 among its roots.\n\nMore precisely, a simple root of formula_22 is either a critical value of formula_5 such the corresponding critical point is a point which is not singular nor an inflection point, or the -coordinate of an asymptote which is parallel to the -axis and is tangent \"at infinity\" to an inflection point (inflexion asymptote).\n\nA multiple root of the discriminant correspond either to several critical points or inflection asymptotes sharing the same critical value, or to a critical point which is also an inflection point, or to a singular point.\n\nFor a continuously differentiable function of several real variables, a point \"P\" (that is a set of values for the input variables, which is viewed as a point in R) is critical if all of the partial derivatives of the function are zero at \"P\", or, equivalently, if its gradient is zero. The critical values are the values of the function at the critical points.\n\nIf the function is smooth, or, at least twice continuously differentiable, a critical point may be either a local maximum, a local minimum or a saddle point. The different cases may be distinguished by considering the eigenvalues of the Hessian matrix of second derivatives.\n\nA critical point at which the Hessian matrix is nonsingular is said to be \"nondegenerate\", and the signs of the eigenvalues of the Hessian determine the local behavior of the function. In the case of a function of a single variable, the Hessian is simply the second derivative, viewed as a 1×1-matrix, which is nonsingular if and only if it is not zero. In this case, a non-degenerate critical point is a local maximum or a local minimum, depending on the sign of the second derivative, which is positive for a local minimum and negative for a local maximum. If the second derivative is null, the critical point is generally an inflection point, but may also be an undulation point, which may be a local minimum or a local maximum.\n\nFor a function of \"n\" variables, the number of negative eigenvalues of the Hessian matrix at a critical point is called the \"index\" of the critical point. A non-degenerate critical point is a local maximum if and only if the index is \"n\", or, equivalently, if the Hessian matrix is negative definite; it is a local minimum if the index is zero, or, equivalently, if the Hessian matrix is positive definite. For the other values of the index, a non-degenerate critical point is a saddle point, that is a point which is a maximum in some directions and a minimum in others.\n\nBy Fermat's theorem, all local maxima and minima of a differentiable function occur at critical points. Therefore, to find the local maxima and minima, it suffices, theoretically, to compute the zeros of the gradient and the eigenvalues of the Hessian matrix at these zeros. This does not work well in practice because it requires the solution of a nonlinear system of simultaneous equations, which is a difficult task.\nThe usual numerical algorithms are much more efficient for finding local extrema, but cannot certify that all extrema have been found.\nIn particular, in global optimization, these methods cannot certify that the output is really the global optimum.\n\nWhen the function to minimize is a multivariate polynomial, the critical points and the critical values are solutions of a system of polynomial equations, and modern algorithms for solving such systems provide competitive certified methods for finding the global minimum.\n\nGiven a differentiable map \"f\" from R into R, the critical points of \"f\" are the points of R, where the rank of the Jacobian matrix of \"f\" is not maximal. The image of a critical point under \"f\" is a called a critical value. A point in the complement of the set of critical values is called a regular value. Sard's theorem states that the set of critical values of a smooth map has measure zero. In particular, if \"n\" = 1, there is a finite number of critical values in each bounded interval.\n\nSome authors give a slightly different definition: a critical point of \"f\" is a point of R where the rank of the Jacobian matrix of \"f\" is less than \"n\". With this convention, all points are critical when \"m\" < \"n\".\n\nThese definitions extend to differential maps between differentiable manifolds in the following way. Let formula_26 be a differential map between two manifolds and of respective dimensions \"m\" and \"n\". In the neighborhood of a point of and of , charts are diffeomorphisms formula_27 and formula_28 The point is critical for if formula_29 is critical for formula_30 This definition does not depend on the choice of the charts because the transitions maps being diffeomorphisms, their Jacobian matrices are invertible and multiplying by them does not modify the rank of the Jacobian matrix of formula_30 If \"M\" is a Hilbert manifold (not necessarily finite dimensional) and \"f\" is a real-valued function then we say that \"p\" is a critical point of \"f\" if \"f\" is \"not\" a submersion at \"p\".\n\nCritical points are fundamental for studying the topology of manifolds and real algebraic varieties. In particular, they are the basic tool for Morse theory and catastrophe theory.\n\nThe link between critical points and topology already appears at a lower level of abstraction. For example, let formula_32 be a sub-manifold of formula_33 and be a point outside formula_34 The square of the distance to of a point of formula_32 is a differential map such that each connected component of formula_32 contains at least a critical point, where the distance is minimal. It follows that the number of connected components of formula_32 is bounded above by the number of critical points.\n\nIn the case of real algebraic varieties, this observation associated with Bézout's theorem allows us to bound the number of connected components by a function of the degrees of the polynomials that define the variety.\n\n"}
{"id": "7352745", "url": "https://en.wikipedia.org/wiki?curid=7352745", "title": "Cybertext", "text": "Cybertext\n\nCybertext is the organization of text in order to analyze the influence of the medium as an integral part of the literary dynamic, as defined by Espen Aarseth in 1997. Aarseth defined it as a type of ergodic literature.\n\nThe term cybertext was coined by speculative fiction poetry author Bruce Boston. It is derived from the word cybernetics, which was coined by Norbert Wiener in his book \"Cybernetics, or Control and Communication in the Animal and the Machine\" (1948), which in turn comes from the Greek word \"kybernetes\" – helmsman. Cybertexts are pieces of literature where the medium matters. Each user obtains a different outcome based on the choices they make. Cybertexts may be equated to the transition between a linear piece of literature, such as a novel, and a game. In a novel, the reader has no choice, the plot and the characters are all chosen by the author, there is no 'user', just a 'reader', this is important because it entails that the person working their way through the novel is not an active participant. In a game, the person makes decisions and decides what to do, what punches to punch, or when to jump. The difference between a game and a cybertext is that cybertexts usually have more depth, there is a method to the madness, the piece usually has a point, or message that is translated to the reader as they work their way through the piece.\n\nCybertext is based on the idea that getting to the message is just as important as the message itself. In order to obtain the message work on the part of the user is required. This may also be referred to as nontrivial work on the part of the user.\n\nThe fundamental idea in the development of the theory of cybernetics is the concept of feedback: a portion of information produced by the system that is taken, total or partially, as input. Cybernetics is the science that studies control and regulation in systems in which there exists flow and feedback of information. Though first used by science fiction poet Bruce Boston, the term cybertext was brought to the literary world's attention by Espen Aarseth in 1997.\n\nAarseth's concept of cybertext focuses on the organization of the text in order to analyze the influence of the medium as an integral part of the literary dynamic. According to Aarseth, cybertext is not a genre in itself; in order to classify traditions, literary genres and aesthetic value, we should inspect texts at a much more local level.\n\nThe concept of cybertext offers a way to expand the reach of literary studies to include phenomena that are perceived today as foreign or marginal. In Aarseth's work, cybertext denotes the general set of text machines which, operated by readers, yield different texts for reading.\n\nFor example, with a book like Raymond Queneau's \"Hundred Thousand Billion Poems\", each reader will encounter not just poems arranged in a different order, but \"different poems\" depending on the precise way in which they turn the sections of page.\n\nAn example of a cybertext is \"12 Blue\" by Michael Joyce. Depending on what link you choose or what portion of the diagram on the side you pick you will be transferred to a different portion of the text. So in the end, you do not really finish reading the entire story or 'novel' you go through random pages and try piecing the story together yourself. You may never really 'finish' the story. But, because it is a cybertext the 'finishing' of the story is not as important as its impact on the reader, or on the conveyance. \"Stir Fry Texts\", by Jim Andrews, is a cybertext where there are many layers of text, and as you move your mouse over the words, the layers beneath them are 'dug' through.\n\"The House\" is another example of a cybertext where one might assume a description of the piece as follows:\nIt is an unruly text, the words don't listen, you are not supreme. You are guided through the piece. This is a cybertext with minimal control. You watch as something unfolds before you, \"a crumbling mania\", you must be able to go with the flow, to read texts upside down, to piece together a reflection of words, to be okay with texts half read disappearing or moving so far away so continuously that you can not make out those very important words.\n\n\n"}
{"id": "237864", "url": "https://en.wikipedia.org/wiki?curid=237864", "title": "Differintegral", "text": "Differintegral\n\nIn fractional calculus, an area of applied mathematics, the differintegral is a combined differentiation/integration operator. Applied to a function ƒ, the \"q\"-differintegral of \"f\", here denoted by\nis the fractional derivative (if \"q\" > 0) or fractional integral (if \"q\" < 0). If \"q\" = 0, then the \"q\"-th differintegral of a function is the function itself. In the context of fractional integration and differentiation, there are several legitimate definitions of the differintegral.\n\nThe three most common forms are:\n\n\n\n\nRecall the continuous Fourier transform, here denoted formula_5 :\n\nUsing the continuous Fourier transform, in Fourier space, differentiation transforms into a multiplication:\n\nSo,\n\nwhich generalizes to\n\nUnder the Laplace transform, here denoted by formula_10, differentiation transforms into a multiplication\n\nGeneralizing to arbitrary order and solving for \"D\"\"f\"(\"t\"), one obtains\n\n\"Linearity rules\"\n\n\"Zero rule\"\n\n\"Product rule\"\n\nIn general, \"composition (or semigroup) rule\" is not satisfied:\n\n\n\n"}
{"id": "19864630", "url": "https://en.wikipedia.org/wiki?curid=19864630", "title": "Dixon's identity", "text": "Dixon's identity\n\nIn mathematics, Dixon's identity (or Dixon's theorem or Dixon's formula) is any of several different but closely related identities proved by A. C. Dixon, some involving finite sums of products of three binomial coefficients, and some evaluating a hypergeometric sum. These identities famously follow from the MacMahon Master theorem, and can now be routinely proved by computer algorithms .\n\nThe original identity, from , is\n\nA generalization, also sometimes called Dixon's identity, is\n\nwhere \"a\", \"b\", and \"c\" are non-negative integers . \nThe sum on the left can be written as the terminating well-poised hypergeometric series\nand the identity follows as a limiting case (as \"a\" tends to an integer) of \nDixon's theorem evaluating a well-poised \"F\" generalized hypergeometric series at 1, from :\n\nThis holds for Re(1 + \"a\" − \"b\" − \"c\") > 0. As \"c\" tends to −∞ it reduces to Kummer's formula for the hypergeometric function F at −1. Dixon's theorem can be deduced from the evaluation of the Selberg integral.\n\nA \"q\"-analogue of Dixon's formula for the basic hypergeometric series in terms of the q-Pochhammer symbol is given by\nwhere |\"qa\"/\"bc\"| < 1.\n\n"}
{"id": "47047358", "url": "https://en.wikipedia.org/wiki?curid=47047358", "title": "Eigenoperator", "text": "Eigenoperator\n\nIn mathematics, an eigenoperator, \"A\", of a matrix \"H\" is a linear operator such that\n\nwhere formula_2 is a corresponding scalar called an eigenvalue.\n"}
{"id": "9672", "url": "https://en.wikipedia.org/wiki?curid=9672", "title": "Entscheidungsproblem", "text": "Entscheidungsproblem\n\nIn mathematics and computer science, the (, German for \"decision problem\") is a challenge posed by David Hilbert in 1928. The problem asks for an algorithm that takes as input a statement of a first-order logic (possibly with a finite number of axioms beyond the usual axioms of first-order logic) and answers \"Yes\" or \"No\" according to whether the statement is \"universally valid\", i.e., valid in every structure satisfying the axioms. By the completeness theorem of first-order logic, a statement is universally valid if and only if it can be deduced from the axioms, so the \"\" can also be viewed as asking for an algorithm to decide whether a given statement is provable from the axioms using the rules of logic.\n\nIn 1936, Alonzo Church and Alan Turing published independent papers showing that a general solution to the \"\" is impossible, assuming that the intuitive notion of \"effectively calculable\" is captured by the functions computable by a Turing machine (or equivalently, by those expressible in the lambda calculus). This assumption is now known as the Church–Turing thesis.\n\nThe origin of the \"\" goes back to Gottfried Leibniz, who in the seventeenth century, after having constructed a successful mechanical calculating machine, dreamt of building a machine that could manipulate symbols in order to determine the truth values of mathematical statements. He realized that the first step would have to be a clean formal language, and much of his subsequent work was directed towards that goal. In 1928, David Hilbert and Wilhelm Ackermann posed the question in the form outlined above.\n\nIn continuation of his \"program\", Hilbert posed three questions at an international conference in 1928, the third of which became known as \"Hilbert's \"\".\" In 1929, Moses Schönfinkel published one paper on special cases of the decision problem that was prepared by Paul Bernays.\n\nAs late as 1930, Hilbert believed that there would be no such thing as an unsolvable problem.\n\nBefore the question could be answered, the notion of \"algorithm\" had to be formally defined. This was done by Alonzo Church in 1936 with the concept of \"effective calculability\" based on his λ calculus and by Alan Turing in the same year with his concept of Turing machines. Turing immediately recognized that these are equivalent models of computation.\n\nThe negative answer to the ' was then given by Alonzo Church in 1935–36 and independently shortly thereafter by Alan Turing in 1936. Church proved that there is no computable function which decides for two given λ-calculus expressions whether they are equivalent or not. He relied heavily on earlier work by Stephen Kleene. Turing reduced the question of the existence of a 'general method' which decides whether any given Turing Machine halts or not (the halting problem) to the question of the existence of an 'algorithm' or 'general method' able to solve the '. If 'Algorithm' is understood as being equivalent to a Turing Machine, and with the answer to the latter question negative (in general), the question about the existence of an Algorithm for the \"\" also must be negative (in general). In his 1936 paper, Turing says: \"Corresponding to each computing machine 'it' we construct a formula 'Un(it)' and we show that, if there is a general method for determining whether 'Un(it)' is provable, then there is a general method for determining whether 'it' ever prints 0\".\n\nThe work of both Church and Turing was heavily influenced by Kurt Gödel's earlier work on his incompleteness theorem, especially by the method of assigning numbers (a Gödel numbering) to logical formulas in order to reduce logic to arithmetic.\n\nThe \"\" is related to Hilbert's tenth problem, which asks for an algorithm to decide whether Diophantine equations have a solution. The non-existence of such an algorithm, established by Yuri Matiyasevich in 1970, also implies a negative answer to the Entscheidungsproblem.\n\nSome first-order theories are algorithmically decidable; examples of this include Presburger arithmetic, real closed fields and static type systems of many programming languages. The general first-order theory of the natural numbers expressed in Peano's axioms cannot be decided with an algorithm, however.\n\nHaving practical decision procedures for classes of logical formulas is of considerable interest for program verification and circuit verification. Pure Boolean logical formulas are usually decided using SAT-solving techniques based on the DPLL algorithm. Conjunctive formulas over linear real or rational arithmetic can be decided using the simplex algorithm, formulas in linear integer arithmetic (Presburger arithmetic) can be decided using Cooper's algorithm or William Pugh's Omega test. Formulas with negations, conjunctions and disjunctions combine the difficulties of satisfiability testing with that of decision of conjunctions; they are generally decided nowadays using SMT-solving techniques, which combine SAT-solving with decision procedures for conjunctions and propagation techniques. Real polynomial arithmetic, also known as the theory of real closed fields, is decidable; this is the Tarski–Seidenberg theorem, which has been implemented in computers by using the cylindrical algebraic decomposition.\n\n\n"}
{"id": "18543655", "url": "https://en.wikipedia.org/wiki?curid=18543655", "title": "Erdős–Fuchs theorem", "text": "Erdős–Fuchs theorem\n\nIn mathematics, in the area of additive number theory, the Erdős–Fuchs theorem is a statement about the number of ways that numbers can be represented as a sum of two elements of a given set, stating that the average order of this number cannot be too close to being a linear function.\n\nThe theorem is named after Paul Erdős and Wolfgang Heinrich Johannes Fuchs, who published it in 1956.\n\nLet formula_1 be an infinite subset of the natural numbers and formula_2 its \"representation function\", which denotes the number of ways that a natural number formula_3 can be expressed as the sum of formula_4 elements of formula_5 (taking order into account). We then consider the \"accumulated representation function\": formula_6which counts (also taking order into account) the number of solutions to formula_7, where formula_8. The theorem then states that, for any given formula_9, the relation formula_10cannot be satisfied; that is, there is no formula_1 satisfying the above estimate.\n\nThe Erdős–Fuchs theorem has an interesting history of precedents and generalizations. In 1915, it was already known by G. H. Hardy that in the case of the sequence formula_12 of perfect squares one haveformula_13This estimate is a little better than that described by Erdős-Fuchs, but at the cost of a slight loss of precision, P. Erdős and W. H. J. Fuchs achieved complete generality in their result (at least for the case formula_14). Another reason this result is so celebrated may be due to the fact that, in 1941, P. Erdős and P. Turán conjectured that, subject to the same hypotheses as in the theorem stated, the relationformula_15could not hold. This fact remained unproved until 1956 when Erdős and Fuchs obtained their theorem, which is much stronger than the previously conjectured estimate.\n\nThis theorem has been extended in a number of different directions. In 1980, A. Sárközy considered two sequences which are \"near\" in some sense. He proved the following:Theorem (Sárközy, 1980). \"If formula_16 and formula_17 are two infinite subsets of natural numbers with formula_18, then formula_19 cannot hold for any constant\" formula_20.In 1990, H. L. Montgomery and R. C. Vaughan where able to remove the log from the right-hand side of Erdős-Fuchs original statement, showing thatformula_21cannot hold. In 2004, G. Horváth extended both these results, proving the following:Theorem (Horváth, 2004). \"If formula_22 and formula_23 are infinite subsets of natural numbers with formula_24 and formula_25, then formula_26 cannot hold for any constant formula_27.\"\n\nThe natural generalization to Erdős–Fuchs theorem, namely for formula_28, is known to hold with same strength as the Montgomery-Vaughan's version. In fact, M. Tang showed in 2009 that, in the same conditions as in the original statement of Erdős-Fuchs, for every formula_29 the relationformula_30cannot hold. In another direction, in 2002, G. Horváth gave a precise generalization of Sárközy's 1980 result, showing thatTheorem (Horváth, 2002) \"If formula_31) are\" formula_32 \"(at least two) infinite subsets of natural numbers and the following estimates are valid:\"\n\"then the relation:\"\"formula_36\"\"cannot hold for any constant formula_37.\"\n\nYet another direction in which the Erdős–Fuchs theorem can be improved is by considering approximations to formula_38 other than formula_39 for some formula_27. In 1963, P. T. Bateman, E. E. Kohlbecker and J. P. Tull proved a slightly stronger version of the following:Theorem (Bateman-Kohlbecker-Tull, 1963). \"Let formula_41 be a slowly varying function which is either convex or concave from some point onward. Then, on the same conditions as in the original Erdős-Fuchs theorem, we cannot have\"\"formula_42\"\"where formula_43 if\" formula_41 \"is bounded and formula_45 otherwise.\"At the end of their paper, it is also remarked that it is possible to extend their method to obtain results considering formula_46 with formula_47, but such results are deemed as not sufficiently definitive.\n\n"}
{"id": "35890997", "url": "https://en.wikipedia.org/wiki?curid=35890997", "title": "Ettore Bortolotti", "text": "Ettore Bortolotti\n\nEttore Bortolotti (6 March 1866 – 17 February 1947) was an Italian mathematician.\n\nBortolotti was born in Bologna. He studied mathematics under Salvatore Pincherle and Cesare Arzelà in Bologna. He graduated in mathematics in 1889 at the University of Bologna, under Pincherle. He was appointed as lecturer to the Lyceum of Modica in Sicily in 1891, then studied one year in Paris as a post-graduate, before lecturing at the University of Rome in 1893.\n\nIn 1900, he became professor for infinitesimal calculus at Modena. There, he became dean from 1913 to 1919, then moved back to the University of Bologna, where he retired in 1936.\n\nHe was an Invited Speaker of the ICM in 1924 in Toronto and in 1928 in Bologna.\n\nBortolotti must also be considered a differential geometer and a relativist too. In fact, in the year 1929, he commented on the geometric basis for Einstein’s absolute parallelism theory in a paper entitled \"Stars of congruences and absolute parallelism: Geometric basis for a recent theory of Einstein\".\n\nBortolotti died in Bologna.\n\n\n"}
{"id": "9795255", "url": "https://en.wikipedia.org/wiki?curid=9795255", "title": "Forward measure", "text": "Forward measure\n\nIn finance, a \"T\"-forward measure is a pricing measure absolutely continuous with respect to a risk-neutral measure but rather than using the money market as numeraire, it uses a bond with maturity \"T\". The use of the forward measure was pioneered by Farshid Jamshidian (1987), and later used as a means of calculating the price of options on bonds.\n\nLet\n\nbe the bank account or money market account numeraire and\n\nbe the discount factor in the market at time 0 for maturity \"T\". If formula_3 is the risk neutral measure, then the forward measure formula_4 is defined via the Radon–Nikodym derivative given by\n\nNote that this implies that the forward measure and the risk neutral measure coincide when interest rates are deterministic. Also, this is a particular form of the change of numeraire formula by changing the numeraire from the money market or bank account \"B\"(\"t\") to a \"T\"-maturity bond \"P\"(\"t\",\"T\"). Indeed, if in general\n\nis the price of a zero coupon bond at time \"t\" for maturity \"T\", where formula_7 is the filtration denoting market information at time \"t\", then we can write\n\nfrom which it is indeed clear that the forward \"T\" measure is associated to the \"T\"-maturity zero coupon bond as numeraire. For a more detailed discussion see Brigo and Mercurio (2001).\n\nThe name \"forward measure\" comes from the fact that under the forward measure, forward prices are martingales, a fact first observed by Geman (1989) (who is responsible for formally defining the measure). Compare with futures prices, which are martingales under the risk neutral measure. Note that when interest rates are deterministic, this implies that forward prices and futures prices are the same.\n\nFor example, the discounted stock price is a martingale under the risk-neutral measure:\n\nThe forward price is given by formula_10. Thus, we have formula_11\n\nby using the Radon-Nikodym derivative formula_13 and the equality formula_11. The last term is equal to unity by definition of the bond price so that we get\n\n"}
{"id": "239997", "url": "https://en.wikipedia.org/wiki?curid=239997", "title": "Grace Chisholm Young", "text": "Grace Chisholm Young\n\nGrace Chisholm Young (née Chisholm) was an English mathematician. She was educated at Girton College, Cambridge, England and continued her studies at Göttingen University in Germany, where in 1895 she became the first woman to receive a doctorate in any field in that country. Her early writings were published under the name of her husband, William Henry Young, and they collaborated on mathematical work throughout their lives. For her work on calculus (1914–16), she was awarded the Gamble Prize for Mathematics by Girton College, University of Cambridge.\n\nShe was the youngest of three surviving children. Her father was a senior civil servant, with the title Warden of the Standards in charge of the Weights and Measures Department. The two girls were taught at home by their mother, father and a governess which was the custom during that time. Her family encouraged her to become involved in social work, helping the poor in London. She had aspirations of studying medicine, but her family would not allow this. However, Chisholm wanted to continue her studies. She passed the senior examination for entrance into Cambridge University at the age of 17.\n\nChisholm entered Girton College in 1889 aged 22, four years after she passed the senior entrance examination having been awarded the Sir Francis Goldsmid Scholarship by the college. At this time the college was only associated with the University of Cambridge with men and women graded on separate but related lists. Although she wanted to study medicine, her mother would not permit this, so, supported by her father, she decided to study mathematics. At the end of her first year, when the Mays list came out, top of the Second class immediately below Isabel Maddison. In 1893, Grace passed her final examinations with the equivalent of a first-class degree, ranked between 23 and 24 relative to 112 men.\n\nShe also took (unofficially, on a challenge, with Isabel Maddison) the exam for the Final Honours School in mathematics at the University of Oxford in 1892 in which she out-performed all the Oxford students. As a result, she became the first person to obtain a First class degree at both Oxford and Cambridge Universities in any subject.\n\nChisholm remained at Cambridge for an additional year to complete Part II of the Mathematical Tripos, which was unusual for women.\n\nShe wanted to continue her studies and since women were not yet admitted to graduate schools in England she went to the University of Göttingen in Germany to study with Felix Klein. This was one of the major mathematical centres in the world. The decision to admit her had to be approved by the Berlin Ministry of Culture and was part of an experiment in admitting women to university studies. In 1895, at the age of 27, Chisholm became the first woman to be awarded a doctorate in any field in Germany. Again government approval had to be obtained to allow her to take the examination, which consisted of probing questions by several professors on sections such as geometry, differential equations, physics, astronomy, and the area of her dissertation, all in German. Along with her test she was required to take courses showing broader knowledge as well as prepare a thesis which was entitled \"Algebraisch-gruppentheoretische Untersuchungen zur sphärischen Trigonometrie\" (\"Algebraic Groups of Spherical Trigonometry\").\n\nAfter returning to England in 1896 to marry, she resumed research she had initiated at Gӧttingen into an equation to determine the orbit of a comet. Her husband continued his work coaching in mathematics. However, in 1897 they both returned to Gӧttingen, encouraged by Felix Klein. Both attend advanced lectures and while she continued her mathematical research her husband started to work creatively for the first time. They visited Turin in Italy to study modern geometry and under Klein's guidance they becan to work in the new area of set theory. From about 1901, the Youngs began to publish papers together. These concerned the theory of functions of a real variable and were heavily influenced by new ideas with which she had come into contact with in Gӧttingen. In 1908 they moved to Geneva in Switzerland where she continued to be based while her husband held a series of academic posts in India and the UK. \nAlthough most of their work was published jointly it is believed that Grace did a large amount of the actual writing, and she also produced some independent work which, according to expert opinion, was deeper and more important than her husband's. In total, they published about 214 papers together. and four books. She began to publish in her own name in 1914, and was awarded the Gamble Prize for Mathematics by Girton College for an essay \"On infinite derivates\" in 1915. This work was stimulated by developments in microscopy that allowed real molecular motion to be viewed. Her work between 1914-16 on relationships between derivatives of an arbitrary function contributed to the Denjoy-Young-Saks theorem.\n\nThey also wrote an elementary geometry book (\"The First Book of Geometry\", 1905) which was translated into 4 languages. In 1906 the Youngs published \"The Theory of Sets of Points\", the first textbook on set theory.\n\nChisholm married William Henry Young in 1896, the year after she received her Ph.D. from Göttingen. He had been her tutor for one term at Cambridge and they had become friends after he was one of the people that she sent a copy of her doctoral thesis. He suggested collaboration in a publication about astronomy but they did not pursue this. They had six children within nine years.\n\nIn addition to her career as a pioneering woman in what was then a discipline with significant barriers to entry, she completed all the requirements for a medical degree except the internship. She also learned six languages and taught each of her children a musical instrument. In addition, she published two books for children (\"Bimbo:A Little Real Story for Jill and Molly\" (1905) and \"Bimbo and the Frogs: Another Real Story\" (1907)). The former was aimed to explain where babies came from to children while the latter was about cells. In 1929 she started a historical novel \"The Crown of England\" set in the sixteenth century. She worked on this for five years but it was never published.\n\nWith the approach of World War II, she left Switzerland in 1940 to take two of her grandchildren to England. She planned to return immediately, but because of the fall of France, she could not. This left William alone, and he died two years later in 1942. Two years after that, Grace Chisholm Young died of a heart attack.\n\nOf their six children, three continued on to study mathematics (including Laurence Chisholm Young and Cecilia Rosalind Tanner), one daughter (Janet) became a physician, and one son (Patrick) became a chemist and pursued a career in finance and business. Their eldest son (Frank) was killed in World War I and his death had a profound effect on his parents, reducing their mathematical creativity. One of Grace's fourteen grandchildren, Sylvia Wiegand (daughter of Laurence), is a mathematician at the University of Nebraska and is a past president of the Association for Women in Mathematics.\n\nIn 1996 Sylvia Wiegand and her husband Roger established a fellowship for graduate student research at the University of Nebraska in honor of Grace Chisholm Young and William Henry Young, called the Grace Chisholm Young and William Henry Young Award. Sylvia is one of Grace's fourteen grandchildren.\n\n\n"}
{"id": "15094186", "url": "https://en.wikipedia.org/wiki?curid=15094186", "title": "Graph automorphism", "text": "Graph automorphism\n\nIn the mathematical field of graph theory, an automorphism of a graph is a form of symmetry in which the graph is mapped onto itself while preserving the edge–vertex connectivity.\n\nFormally, an automorphism of a graph \"G\" = (\"V\",\"E\") is a permutation σ of the vertex set \"V\", such that the pair of vertices (\"u\",\"v\") form an edge if and only if the pair (σ(\"u\"),σ(\"v\")) also form an edge. That is, it is a graph isomorphism from \"G\" to itself. Automorphisms may be defined in this way both for directed graphs and for undirected graphs. \nThe composition of two automorphisms is another automorphism, and the set of automorphisms of a given graph, under the composition operation, forms a group, the automorphism group of the graph. In the opposite direction, by Frucht's theorem, all groups can be represented as the automorphism group of a connected graph – indeed, of a cubic graph.\n\nConstructing the automorphism group is at least as difficult (in terms of its computational complexity) as solving the graph isomorphism problem, determining whether two given graphs correspond vertex-for-vertex and edge-for-edge. For, \"G\" and \"H\" are isomorphic if and only if the disconnected graph formed by the disjoint union of graphs \"G\" and \"H\" has an automorphism that swaps the two components. In fact, just counting the automorphisms is polynomial-time equivalent to graph isomorphism\nThe graph automorphism problem is the problem of testing whether a graph has a nontrivial automorphism. It belongs to the class NP of computational complexity. Similar to the graph isomorphism problem, it is unknown whether it has a polynomial time algorithm or it is NP-complete. \nThere is a polynomial time algorithm for solving the graph automorphism problem for graphs where vertex degrees are bounded by a constant.\nThe graph automorphism problem is polynomial-time many-one reducible to the graph isomorphism problem, but the converse reduction is unknown. By contrast, hardness is known when the automorphisms are constrained in a certain fashion; for instance, determining the existence of a fixed-point-free automorphism (an automorphism that fixes no vertex) is NP-complete, and the problem of counting such automorphisms is #P-complete.\n\nWhile no \"worst-case\" polynomial-time algorithms are known for the general Graph Automorphism problem, finding the automorphism group (and printing out an irredundant set of generators) for many large graphs arising in applications is rather easy. Several open-source software tools are available for this task, including NAUTY, BLISS and SAUCY. SAUCY and BLISS are particularly efficient for sparse graphs, e.g., SAUCY processes some graphs with millions of vertices in mere seconds. However, BLISS and NAUTY can also produce Canonical Labeling, whereas SAUCY is currently optimized for solving Graph Automorphism. An important observation is that for a graph on \"n\" vertices, the automorphism group can be specified by no more than \"n-1\" generators, and the above software packages are guaranteed to satisfy this bound as a side-effect of their algorithms (minimal sets of generators are harder to find and are not particularly useful in practice). It also appears that the total support (i.e., the number of vertices moved) of all generators is limited by a linear function of \"n\", which is important in runtime analysis of these algorithms. However, this has not been established for a fact, as of March 2012.\n\nPractical applications of Graph Automorphism include graph drawing and other visualization tasks, solving structured instances of Boolean Satisfiability arising in the context of Formal verification and Logistics. Molecular symmetry can predict or explain chemical properties.\n\nSeveral graph drawing researchers have investigated algorithms for drawing graphs in such a way that the automorphisms of the graph become visible as symmetries of the drawing. This may be done either by using a method that is not designed around symmetries, but that automatically generates symmetric drawings when possible, or by explicitly identifying symmetries and using them to guide vertex placement in the drawing. It is not always possible to display all symmetries of the graph simultaneously, so it may be necessary to choose which symmetries to display and which to leave unvisualized.\n\nSeveral families of graphs are defined by having certain types of automorphisms:\n\nInclusion relationships between these families are indicated by the following table:\n\n"}
{"id": "1879428", "url": "https://en.wikipedia.org/wiki?curid=1879428", "title": "Hamming bound", "text": "Hamming bound\n\nIn mathematics and computer science, in the field of coding theory, the Hamming bound is a limit on the parameters of an arbitrary block code: it is also known as the sphere-packing bound or the volume bound from an interpretation in terms of packing balls in the Hamming metric into the space of all possible words. It gives an important limitation on the efficiency with which any error-correcting code can utilize the space in which its code words are embedded. A code which attains the Hamming bound is said to be a perfect code.\n\nAn original message and an encoded version are both composed in an alphabet of \"q\" letters. Each code word contains \"n\" letters. The original message (of length \"m\") is shorter than \"n\" letters. The message is converted into an \"n\"-letter codeword by an encoding algorithm, transmitted over a noisy channel, and finally decoded by the receiver. The decoding process interprets a garbled codeword, referred to as simply a \"word\", as the valid codeword \"nearest\" the \"n\"-letter received string.\n\nMathematically, there are exactly \"q\" possible messages of length \"m\", and each message can be regarded as a vector of length \"m\". The encoding scheme converts an \"m\"-dimensional vector into an \"n\"-dimensional vector. Exactly \"q\" valid codewords are possible, but any one of \"q\" garbled codewords (words) can be received, because the noisy channel might distort one or more of the \"n\" letters while the codeword is being transmitted.\n\nLet formula_1 denote the maximum possible size of a formula_2-ary block code formula_3 of length formula_4 and minimum Hamming distance formula_5 (a formula_2-ary block code of length formula_4 is a subset of the strings of formula_8 where the alphabet set formula_9 has formula_2 elements).\n\nThen, the Hamming bound is:\n\nwhere\n\nIt follows from the definition of formula_5 that if at most \nerrors are made during transmission of a codeword then minimum distance decoding will decode it correctly (i.e., it decodes the received word as the codeword that was sent). Thus the code is said to be capable of correcting formula_15 errors.\n\nFor each codeword formula_16, consider a ball of fixed radius formula_15 around formula_18. Every pair of these balls (Hamming spheres) are non-intersecting by the formula_15-error-correcting property. Let formula_20 be the number of words in each ball (in other words, the volume of the ball). A word that is in such a ball can deviate in at most formula_15 components from those of the ball's centre, which is a codeword. The number of such words is then obtained by choosing up to formula_15 of the formula_4 components of a codeword to deviate to one of formula_24 possible other values (recall, the code is formula_2-ary: it takes values in formula_26). Thus,\n\nformula_28 is the (maximum) total number of codewords in formula_29, and so, by the definition of formula_15, the greatest number of balls with no two balls having a word in common. Taking the union of the words in these balls centered at codewords, results in a set of words, each counted precisely once, that is a subset of formula_26 (where formula_32 words) and so:\n\nWhence:\n\nFor an formula_35 code \"C\" (a subset of formula_26), the \"covering radius\" of \"C\" is the smallest value of \"r\" such that every element of formula_26 is contained in at least one ball of radius \"r\" centered at each codeword of \"C\". The \"packing radius\" of \"C\" is the largest value of \"s\" such that the set of balls of radius \"s\" centered at each codeword of \"C\" are mutually disjoint.\n\nFrom the proof of the Hamming bound, it can be seen that for formula_38, we have:\nTherefore, \"s\" ≤ \"r\" and if equality holds then \"s\" = \"r\" = \"t\". The case of equality means that the Hamming bound is attained.\n\nCodes that attain the Hamming bound are called perfect codes. Examples include codes that have only one codeword, and codes that are the whole of formula_39. Another example is given by the \"repeat codes\", where each symbol of the message is repeated an odd fixed number of times to obtain a codeword where \"q\" = 2. All of these examples are often called the \"trivial\" perfect codes.\nIn 1973, it was proved that any non-trivial perfect code over a prime-power alphabet has the parameters of a Hamming code or a Golay code.\n\nA perfect code may be interpreted as one in which the balls of Hamming radius \"t\" centered on codewords exactly fill out the space (\"t\" is the covering radius = packing radius). A quasi-perfect code is one in which the balls of Hamming radius \"t\" centered on codewords are disjoint and the balls of radius \"t\"+1 cover the space, possibly with some overlaps. Another way to say this is that a code is \"quasi-perfect\" if its covering radius is one greater than its packing radius.\n\n\n"}
{"id": "24315796", "url": "https://en.wikipedia.org/wiki?curid=24315796", "title": "Human dynamics", "text": "Human dynamics\n\nHuman dynamics refer to a branch of complex systems research in statistical physics such as the movement of crowds and queues and other systems of complex human interactions including statistical modelling of human networks, including interactions over communications networks.\n\nHuman Dynamics as a branch of statistical physics: Its main goal is to understand human behavior using methods originally developed in statistical physics. Research in this area started to gain momentum in 2005 after the publication of A.-L. Barabási's seminal paper \"The origin of bursts and heavy tails in human dynamics.\" that introduced a queuing model that was alleged to be capable of explaining the long tailed distribution of inter event times that naturally occur in human activity.\n\nThis paper spurred a burst of activity in this new area leading to not only further theoretical development of the Barabasi model, its experimental verification in several different activities and the beginning of interest in using proxy tools, such as web server logs., cell phone records and even the rate at which registration to a major international conference occurs and the distance and rate people around the globe commute from home to work.\n\nIn recent years there has been a growing appetite for access to new data sources that might prove useful in quantifying and understanding human behavior both at the individual and collective scales.\n\nThe term \"Human Dynamics\" or \"Human Dynamics as Personality Dynamics\" has also been used to describe a technique aimed at education and team building which has been subject to some skepticism, having been described in a Dutch newspaper as a personality course with esoteric (occult) roots.\n\nSense Networks\n\n"}
{"id": "21393598", "url": "https://en.wikipedia.org/wiki?curid=21393598", "title": "Initial value theorem", "text": "Initial value theorem\n\nIn mathematical analysis, the initial value theorem is a theorem used to relate frequency domain expressions to the time domain behavior as time approaches zero.\n\nIt is also known under the abbreviation IVT.\n\nLet\n\nbe the (one-sided) Laplace transform of \"ƒ\"(\"t\"). If formula_2 is bounded on formula_3 (or if just formula_4) and formula_5 exists then the initial value theorem says\n\nSuppose first that formula_7 is bounded. Say formula_8. A change of variable in the integral\nformula_9 shows that \nSince formula_2 is bounded the Dominated Convergence Theorem shows that\n\n(Of course we don't really need DCT here, one can give a very simple proof using only elementary calculus:\n\nStart by choosing formula_13 so that formula_14, and then\nnote that formula_15 \"uniformly\" for formula_16.)\n\nThe theorem assuming just that formula_4 follows from the theorem for bounded formula_2:\nDefine formula_19. Then formula_20 is bounded, so we've shown that formula_21.\nBut formula_22 and formula_23, so\nsince formula_25\n\n"}
{"id": "53049031", "url": "https://en.wikipedia.org/wiki?curid=53049031", "title": "Jin Akiyama", "text": "Jin Akiyama\n\nJin Akiyama (, born 1946) is a Japanese mathematician, known for his appearances on Japanese prime-time television (NHK) presenting magic tricks with mathematical explanations. He is director of the Mathematical Education Research Center at the Tokyo University of Science, and professor emeritus at Tokai University.\n\nAkiyama studied mathematics at the Tokyo University of Science, where one of his mentors was Takashi Hamada.\nHe completed a graduate degree at Sophia University under the supervision of Mitio Nagumo, in differential equations, but soon shifted his interests to graph theory. He planned to take a position in Ghana, but\nafter conflict there caused it to be cancelled he joined the faculty at Nippon Ika University, and then moved to the U.S. for 1978 and 1979 to work with Frank Harary at the University of Michigan. In the 1990s, his interests shifted again, from graph theory to discrete geometry.\n\nAkiyama is a founder of the Japan Conference on Discrete and Computational Geometry, Graphs, and Games (JCDCG), the founding managing editor of \"Graphs and Combinatorics\",\nand the author of the books \"A Day's Adventure in Math Wonderland\" (with Mari-Jo Ruiz, World Scientific, 2008), \"Factors and Factorizations of Graphs\" (with Mikio Kano, Lecture Notes in Mathematics 2031, Springer, 2011), and \"Treks Into Intuitive Geometry: The World of Polygons and Polyhedra\" (with Kiyoko Matsunaga, Springer, 2015). He is also the namesake of a Nintendo DS game, \"Master Jin Jin's IQ Challenge\".\n\nAkiyama's lectures sometimes also include musical performances by him, on accordion or xylophone.\n\n"}
{"id": "56137220", "url": "https://en.wikipedia.org/wiki?curid=56137220", "title": "José Babini", "text": "José Babini\n\nJosé Babini (10 May 1897, Buenos Aires – 18 May 1984, Buenos Aires) was a mathematician, engineer, and historian of mathematics and mathematical sciences.\n\nBabini worked for a construction company, where the owners recognized his mathematical talent and made it possible for him to pursue academic study. From 1918 he studied in Buenos Aires. In 1921 he graduated with a qualification to teach natural science and mathematics. In 1922 he received his degree as a civil engineer. Already in 1917 he contacted the well-known Spanish mathematician Julio Rey Pastor. Instead of working as a civil engineer, Babini taught mathematics at the Faculty for Industrial Chemistry of the \"Universidad Nacional del Litoral\" in Rosario. There he introduced new methods of numerical analysis and was considered a leading Argentine expert in this field. He then taught at the Faculty of Sciences of Education (\"Facultad de Ciencias de la Educación\") in Paraná, Entre Ríos and at the \"Colegio Nacional y la Escuela Industrial\". When Aldo Mieli in 1938 came from Paris to Argentina, he and Babini founded at the \"Universidad Nacional del Litoral\" in Rosario the \"Instituto de Historia y Filosofía de la Ciencia\" (with the support of Rey Pastor). The \"Instituto\" existed until 1943. Babini was an editor for the journal \"Archeion\" (founded by Mieli in 1919 with the name \"Archivio di Storia della Scienza\") and with Mieli edited the series \"Panorama general de historia del ciencia\" in 12 volumes. In this series, Babini wrote, with Desiderio Papp, \"El siglo de iluminismo\" on the exact sciences in the 19th century (volume number 8 of the series).\n\nHe was a major organizer of science in Argentina and a member of the national research council CONICET. From 1955 to 1966 he was the dean of the \"Facultad de Ciencias Exactas y Naturales\" of the National University in Buenos Aires. In 1957 he was the rector and interim director of the newly founded \"Universidad Nacional del Nordeste\". He also presided over Argentina's newly founded university publishing house EUDEBA (\"Editorial Universitaria de Buenos Aires\").\n\nBabini was a member of the editorial board of \"Historical Studies in the Physical Sciences\" and was a co-founder of the journal \"Quipu: Revista Latinoamericana de Historia de las Ciencias y la Tecnología\" (based in Mexico City).\n\nHe was Invited Speaker of the ICM in 1928 in Bologna. He was the author, co-author, or editor of numerous books (at least 70) and essays as well as translations. In particular, he also published the first books on the history of science as it developed in Argentina.\n\n"}
{"id": "18869317", "url": "https://en.wikipedia.org/wiki?curid=18869317", "title": "Kayles", "text": "Kayles\n\nIn combinatorial game theory, Kayles is a simple impartial game. In the notation of octal games, Kayles is denoted 0.77.\n\nKayles is played with a row of tokens, which represent bowling pins. The row may be of any length. The two players alternate; each player, on his or her turn, may remove either any one pin (a ball bowled directly at that pin), or two adjacent pins (a ball bowled to strike both). Under the normal play convention, a player loses when he or she has no legal move (that is, when all the pins are gone). The game can also be played using misère rules; in this case, the player who cannot move \"wins\".\n\nKayles was invented by Henry Dudeney. Richard Guy and Cedric Smith were first to completely analyze the normal-play version, using Sprague-Grundy theory. The misère version was analyzed by William Sibert in 1973, but he did not publish his work until 1989.\n\nThe name \"Kayles\" is an Anglicization of the French \"quilles\", meaning \"bowling\".\n\nMost players quickly discover that the first player has a guaranteed win in normal Kayles whenever the row length is greater than zero. This win can be achieved using a symmetry strategy. On his or her first move, the first player should move so that the row is broken into two sections of equal length. This restricts all future moves to one section or the other. Now, the first player merely imitates the second player's moves in the opposite row.\n\nIt is more interesting to ask what the nim-value is of a row of length formula_1. This is often denoted formula_2; it is a nimber, not a number. By the Sprague–Grundy theorem, formula_2 is the mex over all possible moves of the nim-sum of the nim-values of the two resulting sections. For example,\n\nbecause from a row of length 5, one can move to the positions\n\nRecursive calculation of values (starting with formula_6) gives the results summarized in the following table. To find the value of formula_2 on the table, write formula_1 as formula_9, and look at row a, column b:\nAt this point, the nim-value sequence becomes periodic with period 12, so all further rows of the table are identical to the last row.\n\nBecause certain positions in Dots and Boxes reduce to Kayles positions, it is helpful to understand Kayles in order to analyze a generic Dots and Boxes position.\n\nUnder normal play, Kayles can be solved in polynomial time using the Sprague-Grundy theory.\n\n\"Node Kayles\" is a generalization of Kayles to graphs in which each bowl “knocks down” (removes) a desired vertex and all its neighboring vertices. (Alternatively, this game can be viewed as two players finding an independent set together.) Schaefer (1978) proved that deciding the outcome of this game is PSPACE-complete. The same result holds for a partisan version of node Kayles, in which, for every node, only one of the players is allowed to choose that particular node as the knock down target.\n\n"}
{"id": "1631654", "url": "https://en.wikipedia.org/wiki?curid=1631654", "title": "List of mathematical identities", "text": "List of mathematical identities\n\nThis page lists mathematical identities, that is, \"identically true relations\" holding in mathematics.\n\n\n\n"}
{"id": "31178109", "url": "https://en.wikipedia.org/wiki?curid=31178109", "title": "Logarithmically concave sequence", "text": "Logarithmically concave sequence\n\nIn mathematics, a sequence = of nonnegative real numbers is called a logarithmically concave sequence, or a log-concave sequence for short, if holds for .\n\nRemark: some authors (explicitly or not) add two further hypotheses in the definition of log-concave sequences:\nThese hypotheses mirror the ones required for log-concave functions.\n\nSequences that fulfill the three conditions are also called Pòlya Frequency sequences of order 2 (PF sequences). Refer to chapter 2 of for a discussion on the two notions. \nFor instance, the sequence checks the concavity inequalities but not the internal zeros condition.\n\nExamples of log-concave sequences are given by the binomial coefficients along any row of Pascal's triangle and the elementary symmetric means of a finite sequence of real numbers.\n\n\n"}
{"id": "23124209", "url": "https://en.wikipedia.org/wiki?curid=23124209", "title": "M-spline", "text": "M-spline\n\nIn the mathematical subfield of numerical analysis, an M-spline is a non-negative spline function.\n\nA family of \"M-spline\" functions of order \"k\" with \"n\" free parameters is defined by a set of knots \"t\"  ≤ \"t\"  ≤  ...  ≤  \"t\" such that\n\n\nThe family includes \"n\" members indexed by \"i\" = 1...,\"n\".\n\nAn \"M-spline\" \"M\"(\"x\"|\"k\", \"t\") has the following mathematical properties\n\n\n\"M-splines\" can be efficiently and stably computed using the following recursions:\n\nFor \"k\" = 1,\n\nif \"t\" ≤ \"x\" < \"t\", and \"M\"(\"x\"|1,\"t\") = 0 otherwise.\n\nFor \"k\" > 1,\n\n\"M-splines\" can be integrated to produce a family of monotone splines called I-splines. \"M-splines\" can also be used directly as basis splines for regression analysis involving positive response data (constraining the regression coefficients to be non-negative).\n"}
{"id": "20698", "url": "https://en.wikipedia.org/wiki?curid=20698", "title": "Michael Atiyah", "text": "Michael Atiyah\n\nSir Michael Francis Atiyah (; born 22 April 1929) is a British-Lebanese mathematician specialising in geometry.\n\nAtiyah grew up in Sudan and Egypt but spent most of his academic life in the United Kingdom at University of Oxford and University of Cambridge, and in the United States at the Institute for Advanced Study. He has been president of the Royal Society (1990–1995), master of Trinity College, Cambridge (1990–1997), chancellor of the University of Leicester (1995–2005), and president of the Royal Society of Edinburgh (2005–2008). Since 1997, he has been an honorary professor at the University of Edinburgh.\n\nAtiyah's mathematical collaborators include Raoul Bott, Friedrich Hirzebruch and Isadore Singer, and his students include Graeme Segal, Nigel Hitchin and Simon Donaldson. Together with Hirzebruch, he laid the foundations for topological K-theory, an important tool in algebraic topology, which, informally speaking, describes ways in which spaces can be twisted. His best known result, the Atiyah–Singer index theorem, was proved with Singer in 1963 and is used in counting the number of independent solutions to differential equations. Some of his more recent work was inspired by theoretical physics, in particular instantons and monopoles, which are responsible for some subtle corrections in quantum field theory. He was awarded the Fields Medal in 1966 and the Abel Prize in 2004.\n\nAtiyah was born in Hampstead, London, England, the son of Jean (née Levens) and Edward Atiyah. His mother was Scottish and his father was a Lebanese Orthodox Christian. He has two brothers, Patrick and Joe, and a sister, Selma (deceased). Atiyah went to primary school at the Diocesan school in Khartoum, Sudan (1934–1941) and to secondary school at Victoria College in Cairo and Alexandria (1941–1945); the school was also attended by European nobility displaced by the Second World War and some future leaders of Arab nations. He returned to England and Manchester Grammar School for his HSC studies (1945–1947) and did his national service with the Royal Electrical and Mechanical Engineers (1947–1949). His undergraduate and postgraduate studies took place at Trinity College, Cambridge (1949–1955). He was a doctoral student of William V. D. Hodge and was awarded a doctorate in 1955 for a thesis entitled \"Some Applications of Topological Methods in Algebraic Geometry\".\n\nAtiyah spent the academic year 1955–1956 at the Institute for Advanced Study, Princeton, then returned to Cambridge University, where he was a research fellow and assistant lecturer (1957–1958), then a university lecturer and tutorial fellow at Pembroke College, Cambridge (1958–1961). In 1961, he moved to the University of Oxford, where he was a reader and professorial fellow at St Catherine's College (1961–1963). He became Savilian Professor of Geometry and a professorial fellow of New College, Oxford, from 1963 to 1969. He then took up a three-year professorship at the Institute for Advanced Study in Princeton after which he returned to Oxford as a Royal Society Research Professor and professorial fellow of St Catherine's College. He was president of the London Mathematical Society from 1974 to 1976.\nAtiyah was president of the Pugwash Conferences on Science and World Affairs from 1997 to 2002. He also contributed to the foundation of the InterAcademy Panel on International Issues, the Association of European Academies (ALLEA), and the European Mathematical Society (EMS).\n\nWithin the United Kingdom, he was involved in the creation of the Isaac Newton Institute for Mathematical Sciences in Cambridge and was its first director (1990–1996). He was President of the Royal Society (1990–1995), Master of Trinity College, Cambridge (1990–1997), Chancellor of the University of Leicester (1995–2005), and president of the Royal Society of Edinburgh (2005–2008). Since 1997, he has been an honorary professor in the University of Edinburgh. He is a Trustee of the James Clerk Maxwell Foundation.\n\nAtiyah has collaborated with many mathematicians. His three main collaborations were with Raoul Bott on the Atiyah–Bott fixed-point theorem and many other topics, with Isadore M. Singer on the Atiyah–Singer index theorem, and with Friedrich Hirzebruch on topological K-theory, all of whom he met at the Institute for Advanced Study in Princeton in 1955. His other collaborators include J. Frank Adams (Hopf invariant problem), Jürgen Berndt (projective planes), Roger Bielawski (Berry–Robbins problem), Howard Donnelly (L-functions), Vladimir G. Drinfeld (instantons), Johan L. Dupont (singularities of vector fields), Lars Gårding (hyperbolic differential equations), Nigel J. Hitchin (monopoles), William V. D. Hodge (Integrals of the second kind), Michael Hopkins (K-theory), Lisa Jeffrey (topological Lagrangians), John D. S. Jones (Yang–Mills theory), Juan Maldacena (M-theory), Yuri I. Manin (instantons), Nick S. Manton (Skyrmions), Vijay K. Patodi (Spectral asymmetry), A. N. Pressley (convexity), Elmer Rees (vector bundles), Wilfried Schmid (discrete series representations), Graeme Segal (equivariant K-theory), Alexander Shapiro (Clifford algebras), L. Smith (homotopy groups of spheres), Paul Sutcliffe (polyhedra), David O. Tall (lambda rings), John A. Todd (Stiefel manifolds), Cumrun Vafa (M-theory), Richard S. Ward (instantons) and Edward Witten (M-theory, topological quantum field theories).\n\nHis later research on gauge field theories, particularly Yang–Mills theory, stimulated important interactions between geometry and physics, most notably in the work of Edward Witten.\nAtiyah's students include\nPeter Braam 1987,\nSimon Donaldson 1983,\nK. David Elworthy 1967,\nHoward Fegan 1977,\nEric Grunwald 1977,\nNigel Hitchin 1972,\nLisa Jeffrey 1991,\nFrances Kirwan 1984,\nPeter Kronheimer 1986,\nRuth Lawrence 1989,\nGeorge Lusztig 1971,\nJack Morava 1968,\nMichael Murray 1983,\nPeter Newstead 1966,\nIan R. Porteous 1961,\nJohn Roe 1985,\nBrian Sanderson 1963,\nRolph Schwarzenberger 1960,\nGraeme Segal 1967,\nDavid Tall 1966,\nand Graham White 1982.\n\nOther contemporary mathematicians who influenced Atiyah include Roger Penrose, Lars Hörmander, Alain Connes and Jean-Michel Bismut. Atiyah said that the mathematician he most admired was Hermann Weyl, and that his favorite mathematicians from before the 20th century were Bernhard Riemann and William Rowan Hamilton.\n\nThe seven volumes of Atiyah's collected papers include most of his work, except for his commutative algebra textbook; the first five volumes are divided thematically and the sixth and seventh arranged by date.\n\nAtiyah's early papers on algebraic geometry (and some general papers) are reprinted in the first volume of his collected works.\n\nAs an undergraduate Atiyah was interested in classical projective geometry, and wrote his first paper: a short note on twisted cubics. He started research under W. V. D. Hodge and won the Smith's prize for 1954 for a sheaf-theoretic approach to ruled surfaces, which encouraged Atiyah to continue in mathematics, rather than switch to his other interests—architecture and archaeology.\nHis PhD thesis with Hodge was on a sheaf-theoretic approach to Solomon Lefschetz's theory of integrals of the second kind on algebraic varieties, and resulted in an invitation to visit the Institute for Advanced Study in Princeton for a year. While in Princeton he classified vector bundles on an elliptic curve (extending Alexander Grothendieck's classification of vector bundles on a genus 0 curve), by showing that any vector bundle is a sum of (essentially unique) indecomposable vector bundles, and then showing that the space of indecomposable vector bundles of given degree and positive dimension can be identified with the elliptic curve. He also studied double points on surfaces, giving the first example of a flop, a special birational transformation of 3-folds that was later heavily used in Shigefumi Mori's work on minimal models for 3-folds. Atiyah's flop can also be used to show that the universal marked family of K3 surfaces is non-Hausdorff.\n\nAtiyah's works on K-theory, including his book on K-theory are reprinted in volume 2 of his collected works.\n\nThe simplest nontrivial example of a vector bundle is the Möbius band (pictured on the right): a strip of paper with a twist in it, which represents a rank 1 vector bundle over a circle (the circle in question being the centerline of the Möbius band). K-theory is a tool for working with higher-dimensional analogues of this example, or in other words for describing higher-dimensional twistings: elements of the K-group of a space are represented by vector bundles over it, so the Möbius band represents an element of the K-group of a circle.\n\nTopological K-theory was discovered by Atiyah and Friedrich Hirzebruch who were inspired by Grothendieck's proof of the Grothendieck–Riemann–Roch theorem and Bott's work on the periodicity theorem. This paper only discussed the zeroth K-group; they shortly after extended it to K-groups of all degrees, giving the first (nontrivial) example of a generalized cohomology theory.\n\nSeveral results showed that the newly introduced K-theory was in some ways more powerful than ordinary cohomology theory. Atiyah and Todd used K-theory to improve the lower bounds found using ordinary cohomology by Borel and Serre for the James number, describing when a map from a complex Stiefel manifold to a sphere has a cross section. (Adams and Grant-Walker later showed that the bound found by Atiyah and Todd was best possible.) Atiyah and Hirzebruch used K-theory to explain some relations between Steenrod operations and Todd classes that Hirzebruch had noticed a few years before. The original solution of the Hopf invariant one problem operations by J. F. Adams was very long and complicated, using secondary cohomology operations. Atiyah showed how primary operations in K-theory could be used to give a short solution taking only a few lines, and in joint work with Adams also proved analogues of the result at odd primes.\nThe Atiyah–Hirzebruch spectral sequence relates the ordinary cohomology of a space to its generalized cohomology theory. (Atiyah and Hirzebruch used the case of K-theory, but their method works for all cohomology theories).\n\nAtiyah showed that for a finite group \"G\", the K-theory of its classifying space, \"BG\", is isomorphic to the completion of its character ring:\nThe same year they proved the result for \"G\" any compact connected Lie group. Although soon the result could be extended to \"all\" compact Lie groups by incorporating results from Graeme Segal's thesis, that extension was complicated. However a simpler and more general proof was produced by introducing equivariant K-theory, \"i.e.\" equivalence classes of \"G\"-vector bundles over a compact \"G\"-space \"X\". It was shown that under suitable conditions the completion of the equivariant K-theory of \"X\" is isomorphic to the ordinary K-theory of a space, formula_2, which fibred over \"BG\" with fibre \"X\":\n\nThe original result then followed as a corollary by taking \"X\" to be a point: the left hand side reduced to the completion of \"R(G)\" and the right to \"K(BG)\". See Atiyah–Segal completion theorem for more details.\n\nHe defined new generalized homology and cohomology theories called bordism and cobordism, and pointed out that many of the deep results on cobordism of manifolds found by René Thom, C. T. C. Wall, and others could be naturally reinterpreted as statements about these cohomology theories. Some of these cohomology theories, in particular complex cobordism, turned out to be some of the most powerful cohomology theories known.\nHe introduced the J-group \"J\"(\"X\") of a finite complex \"X\", defined as the group of stable fiber homotopy equivalence classes of sphere bundles; this was later studied in detail by J. F. Adams in a series of papers, leading to the Adams conjecture.\n\nWith Hirzebruch he extended the Grothendieck–Riemann–Roch theorem to complex analytic embeddings, and in a related paper they showed that the Hodge conjecture for integral cohomology is false. The Hodge conjecture for rational cohomology is, as of 2008, a major unsolved problem.\n\nThe Bott periodicity theorem was a central theme in Atiyah's work on K-theory, and he repeatedly returned to it, reworking the proof several times to understand it better. With Bott he worked out an elementary proof, and gave another version of it in his book. With Bott and Shapiro he analysed the relation of Bott periodicity to the periodicity of Clifford algebras; although this paper did not have a proof of the periodicity theorem, a proof along similar lines was shortly afterwards found by R. Wood. He found a proof of several generalizations using elliptic operators; this new proof used an idea that he used to give a particularly short and easy proof of Bott's original periodicity theorem.\n\nAtiyah's work on index theory is reprinted in volumes 3 and 4 of his collected works.\n\nThe index of a differential operator is closely related to the number of independent solutions (more precisely, it is the differences of the numbers of independent solutions of the differential operator and its adjoint). There are many hard and fundamental problems in mathematics that can easily be reduced to the problem of finding the number of independent solutions of some differential operator, so if one has some means of finding the index of a differential operator these problems can often be solved. This is what the Atiyah–Singer index theorem does: it gives a formula for the index of certain differential operators, in terms of topological invariants that look quite complicated but are in practice usually straightforward to calculate.\n\nSeveral deep theorems, such as the Hirzebruch–Riemann–Roch theorem, are special cases of the Atiyah–Singer index theorem. In fact the index theorem gave a more powerful result, because its proof applied to all compact complex manifolds, while Hirzebruch's proof only worked for projective manifolds. There were also many new applications: a typical one is calculating the dimensions of the moduli spaces of instantons. The index theorem can also be run \"in reverse\": the index is obviously an integer, so the formula for it must also give an integer, which sometimes gives subtle integrality conditions on invariants of manifolds. A typical example of this is Rochlin's theorem, which follows from the index theorem.\nThe index problem for elliptic differential operators was posed in 1959 by Gel'fand. He noticed the homotopy invariance of the index, and asked for a formula for it by means of topological invariants. Some of the motivating examples included the Riemann–Roch theorem and its generalization the Hirzebruch–Riemann–Roch theorem, and the Hirzebruch signature theorem. Hirzebruch and Borel had proved the integrality of the Â genus of a spin manifold, and Atiyah suggested that this integrality could be explained if it were the index of the Dirac operator (which was rediscovered by Atiyah and Singer in 1961).\n\nThe first announcement of the Atiyah–Singer theorem was their 1963 paper. The proof sketched in this announcement was inspired by Hirzebruch's proof of the Hirzebruch–Riemann–Roch theorem and was never published by them, though it is described in the book by Palais. Their first published proof was more similar to Grothendieck's proof of the Grothendieck–Riemann–Roch theorem, replacing the cobordism theory of the first proof with K-theory, and they used this approach to give proofs of various generalizations in a sequence of papers from 1968 to 1971.\n\nInstead of just one elliptic operator, one can consider a family of elliptic operators parameterized by some space \"Y\". In this case the index is an element of the K-theory of \"Y\", rather than an integer. If the operators in the family are real, then the index lies in the real K-theory of \"Y\". This gives a little extra information, as the map from the real K theory of \"Y\" to the complex K theory is not always injective.\nWith Bott, Atiyah found an analogue of the Lefschetz fixed-point formula for elliptic operators, giving the Lefschetz number of an endomorphism of an elliptic complex in terms of a sum over the fixed points of the endomorphism. As special cases their formula included the Weyl character formula, and several new results about elliptic curves with complex multiplication, some of which were initially disbelieved by experts.\nAtiyah and Segal combined this fixed point theorem with the index theorem as follows.\nIf there is a compact group action of a group \"G\" on the compact manifold \"X\", commuting with the elliptic operator, then one can replace ordinary K theory in the index theorem with equivariant K-theory.\nFor trivial groups \"G\" this gives the index theorem, and for a finite group \"G\" acting with isolated fixed points it gives the Atiyah–Bott fixed point theorem. In general it gives the index as a sum over fixed point submanifolds of the group \"G\".\n\nAtiyah solved a problem asked independently by Hörmander and Gel'fand, about whether complex powers of analytic functions define distributions. Atiyah used Hironaka's resolution of singularities to answer this affirmatively. An ingenious and elementary solution was found at about the same time by J. Bernstein, and discussed by Atiyah.\n\nAs an application of the equivariant index theorem, Atiyah and Hirzebruch showed that manifolds with effective circle actions have vanishing Â-genus. (Lichnerowicz showed that if a manifold has a metric of positive scalar curvature then the Â-genus vanishes.)\n\nWith Elmer Rees, Atiyah studied the problem of the relation between topological and holomorphic vector bundles on projective space. They solved the simplest unknown case, by showing that all rank 2 vector bundles over projective 3-space have a holomorphic structure. Horrocks had previously found some non-trivial examples of such vector bundles, which were later used by Atiyah in his study of instantons on the 4-sphere.\nAtiyah, Bott and Vijay K. Patodi gave a new proof of the index theorem using the heat equation.\n\nIf the manifold is allowed to have boundary, then some restrictions must be put on the domain of the elliptic operator in order to ensure a finite index. These conditions can be local (like demanding that the sections in the domain vanish at the boundary) or more complicated global conditions (like requiring that the sections in the domain solve some differential equation). The local case was worked out by Atiyah and Bott, but they showed that many interesting operators (e.g., the signature operator) do not admit local boundary conditions. To handle these operators, Atiyah, Patodi and Singer introduced global boundary conditions equivalent to attaching a cylinder to the manifold along the boundary and then restricting the domain to those sections that are square integrable along the cylinder, and also introduced the Atiyah–Patodi–Singer eta invariant. This resulted in a series of papers on spectral asymmetry, which were later unexpectedly used in theoretical physics, in particular in Witten's work on anomalies.\nThe fundamental solutions of linear hyperbolic partial differential equations often have Petrovsky lacunas: regions where they vanish identically. These were studied in 1945 by I. G. Petrovsky, who found topological conditions describing which regions were lacunas.\nIn collaboration with Bott and Lars Gårding, Atiyah wrote three papers updating and generalizing Petrovsky's work.\n\nAtiyah showed how to extend the index theorem to some non-compact manifolds, acted on by a discrete group with compact quotient. The kernel of the elliptic operator is in general infinite-dimensional in this case, but it is possible to get a finite index using the dimension of a module over a von Neumann algebra; this index is in general real rather than integer valued. This version is called the \"L index theorem,\" and was used by Atiyah and Schmid to give a geometric construction, using square integrable harmonic spinors, of Harish-Chandra's discrete series representations of semisimple Lie groups. In the course of this work they found a more elementary proof of Harish-Chandra's fundamental theorem on the local integrability of characters of Lie groups.\n\nWith H. Donnelly and I. Singer, he extended Hirzebruch's formula (relating the signature defect at cusps of Hilbert modular surfaces to values of L-functions) from real quadratic fields to all totally real fields.\n\nMany of his papers on gauge theory and related topics are reprinted in volume 5 of his collected works. A common theme of these papers is the study of moduli spaces of solutions to certain non-linear partial differential equations, in particular the equations for instantons and monopoles. This often involves finding a subtle correspondence between solutions of two seemingly quite different equations. An early example of this which Atiyah used repeatedly is the Penrose transform, which can sometimes convert solutions of a non-linear equation over some real manifold into solutions of some linear holomorphic equations over a different complex manifold.\n\nIn a series of papers with several authors, Atiyah classified all instantons on 4-dimensional Euclidean space. It is more convenient to classify instantons on a sphere as this is compact, and this is essentially equivalent to classifying instantons on Euclidean space as this is conformally equivalent to a sphere and the equations for instantons are conformally invariant. With Hitchin and Singer he calculated the dimension of the moduli space of irreducible self-dual connections (instantons) for any principal bundle over a compact 4-dimensional Riemannian manifold (the Atiyah–Hitchin–Singer theorem). For example, the dimension of the space of SU instantons of rank \"k\">0 is 8\"k\"−3. To do this they used the Atiyah–Singer index theorem to calculate the dimension of the tangent space of the moduli space at a point; the tangent space is essentially the space of solutions of an elliptic differential operator, given by the linearization of the non-linear Yang–Mills equations. These moduli spaces were later used by Donaldson to construct his invariants of 4-manifolds.\nAtiyah and Ward used the Penrose correspondence to reduce the classification of all instantons on the 4-sphere to a problem in algebraic geometry. With Hitchin he used ideas of Horrocks to solve this problem, giving the ADHM construction of all instantons on a sphere; Manin and Drinfeld found the same construction at the same time, leading to a joint paper by all four authors. Atiyah reformulated this construction using quaternions and wrote up a leisurely account of this classification of instantons on Euclidean space as a book.\nAtiyah's work on instanton moduli spaces was used in Donaldson's work on Donaldson theory. Donaldson showed that the moduli space of (degree 1) instantons over a compact simply connected 4-manifold with positive definite intersection form can be compactified to give a cobordism between the manifold and a sum of copies of complex projective space. He deduced from this that the intersection form must be a sum of one-dimensional ones, which led to several spectacular applications to smooth 4-manifolds, such as the existence of non-equivalent smooth structures on 4-dimensional Euclidean space. Donaldson went on to use the other moduli spaces studied by Atiyah to define Donaldson invariants, which revolutionized the study of smooth 4-manifolds, and showed that they were more subtle than smooth manifolds in any other dimension, and also quite different from topological 4-manifolds. Atiyah described some of these results in a survey talk.\n\nGreen's functions for linear partial differential equations can often be found by using the Fourier transform to convert this into an algebraic problem. Atiyah used a non-linear version of this idea. He used the Penrose transform to convert the Green's function for the conformally invariant Laplacian into a complex analytic object, which turned out to be essentially the diagonal embedding of the Penrose twistor space into its square. This allowed him to find an explicit formula for the conformally invariant Green's function on a 4-manifold.\n\nIn his paper with Jones, he studied the topology of the moduli space of SU(2) instantons over a 4-sphere. They showed that the natural map from this moduli space to the space of all connections induces epimorphisms of homology groups in a certain range of dimensions, and suggested that it might induce isomorphisms of homology groups in the same range of dimensions. This became known as the Atiyah–Jones conjecture, and was later proved by several mathematicians.\n\nHarder and M. S. Narasimhan described the cohomology of the moduli spaces of stable vector bundles over Riemann surfaces by counting the number of points of the moduli spaces over finite fields, and then using the Weil conjectures to recover the cohomology over the complex numbers.\nAtiyah and R. Bott used Morse theory and the Yang–Mills equations over a Riemann surface to reproduce and extending the results of Harder and Narasimhan.\n\nAn old result due to Schur and Horn states that the set of possible diagonal vectors of an Hermitian matrix with given eigenvalues is the convex hull of all the permutations of the eigenvalues. Atiyah proved a generalization of this that applies to all compact symplectic manifolds acted on by a torus, showing that the image of the manifold under the moment map is a convex polyhedron, and with Pressley gave a related generalization to infinite-dimensional loop groups.\n\nDuistermaat and Heckman found a striking formula, saying that the push-forward of the Liouville measure of a moment map for a torus action is given exactly by the stationary phase approximation (which is in general just an asymptotic expansion rather than exact). Atiyah and Bott showed that this could be deduced from a more general formula in equivariant cohomology, which was a consequence of well-known localization theorems. Atiyah showed that the moment map was closely related to geometric invariant theory, and this idea was later developed much further by his student F. Kirwan. Witten shortly after applied the Duistermaat–Heckman formula to loop spaces and showed that this formally gave the Atiyah–Singer index theorem for the Dirac operator; this idea was lectured on by Atiyah.\n\nWith Hitchin he worked on magnetic monopoles, and studied their scattering using an idea of Nick Manton. His book with Hitchin gives a detailed description of their work on magnetic monopoles. The main theme of the book is a study of a moduli space of magnetic monopoles; this has a natural Riemannian metric, and a key point is that this metric is complete and hyperkähler. The metric is then used to study the scattering of two monopoles, using a suggestion of N. Manton that the geodesic flow on the moduli space is the low energy approximation to the scattering. For example, they show that a head-on collision between two monopoles results in 90-degree scattering, with the direction of scattering depending on the relative phases of the two monopoles. He also studied monopoles on hyperbolic space.\n\nAtiyah showed that instantons in 4 dimensions can be identified with instantons in 2 dimensions, which are much easier to handle. There is of course a catch: in going from 4 to 2 dimensions the structure group of the gauge theory changes from a finite-dimensional group to an infinite-dimensional loop group. This gives another example where the moduli spaces of solutions of two apparently unrelated nonlinear partial differential equations turn out to be essentially the same.\n\nAtiyah and Singer found that anomalies in quantum field theory could be interpreted in terms of index theory of the Dirac operator; this idea later became widely used by physicists.\n\nMany of the papers in the 6th volume of his collected works are surveys, obituaries, and general talks. Since its publication, Atiyah has continued to publish, including several surveys, a popular book, and another paper with Segal on twisted K-theory.\n\nOne paper is a detailed study of the Dedekind eta function from the point of view of topology and the index theorem.\n\nSeveral of his papers from around this time study the connections between quantum field theory, knots, and Donaldson theory. He introduced the concept of a topological quantum field theory, inspired by Witten's work and Segal's definition of a conformal field theory. His book describes the new knot invariants found by Vaughan Jones and Edward Witten in terms of topological quantum field theories, and his paper with L. Jeffrey explains Witten's Lagrangian giving the Donaldson invariants.\n\nHe studied skyrmions with Nick Manton, finding a relation with magnetic monopoles and instantons, and giving a conjecture for the structure of the moduli space of two skyrmions as a certain subquotient of complex projective 3-space.\n\nSeveral papers were inspired by a question of Jonathan Robbins (called the Berry–Robbins problem), who asked if there is a map from the configuration space of \"n\" points in 3-space to the flag manifold of the unitary group. Atiyah gave an affirmative answer to this question, but felt his solution was too computational and studied a conjecture that would give a more natural solution. He also related the question to Nahm's equation, and introduced the Atiyah conjecture on configurations.\nWith Juan Maldacena and Cumrun Vafa, and E. Witten he described the dynamics of M-theory on manifolds with G holonomy. These papers seem to be the first time that Atiyah has worked on exceptional Lie groups.\n\nIn his papers with M. Hopkins and G. Segal he returned to his earlier interest of K-theory, describing some twisted forms of K-theory with applications in theoretical physics.\n\nIn October 2016, he claimed a short proof of the non-existence of complex structures on the 6-sphere. His proof, like many predecessors, is considered flawed by the mathematical community, even after the proof was rewritten in a revised form.\n\nIn September 2018, at the 2018 Heidelberg Laureate Forum, he claimed a simple proof of the Riemann hypothesis, one of the most important and challenging problems in mathematics. His claim was met with skepticism from the mathematical community.\n\nThis subsection lists all books written by Atiyah; it omits a few books that he edited.\n\n\n\nIn 1966, when he was thirty-seven years old, he was awarded the Fields Medal, for his work in developing K-theory, a generalized Lefschetz fixed-point theorem and the Atiyah–Singer theorem, for which he also won the Abel Prize jointly with Isadore Singer in 2004.\nAmong other prizes he has received are the Royal Medal of the Royal Society in 1968, the De Morgan Medal of the London Mathematical Society in 1980, the Antonio Feltrinelli Prize from the Accademia Nazionale dei Lincei in 1981, the King Faisal International Prize for Science in 1987, the Copley Medal of the Royal Society in 1988, the Benjamin Franklin Medal for Distinguished Achievement in the Sciences of the American Philosophical Society in 1993, the Jawaharlal Nehru Birth Centenary Medal\nof the Indian National Science Academy in 1993, the President's Medal from the Institute of Physics in 2008, the Grande Médaille of the French Academy of Sciences in 2010 and the Grand Officier of the French Légion d'honneur in 2011.\nHe was elected a foreign member of the National Academy of Sciences, the American Academy of Arts and Sciences (1969), the Académie des Sciences, the Akademie Leopoldina, the Royal Swedish Academy, the Royal Irish Academy, the Royal Society of Edinburgh, the American Philosophical Society, the Indian National Science Academy, the Chinese Academy of Science, the Australian Academy of Science, the Russian Academy of Science, the Ukrainian Academy of Science, the Georgian Academy of Science, the Venezuela Academy of Science, the Norwegian Academy of Science and Letters, the Royal Spanish Academy of Science, the Accademia dei Lincei and the Moscow Mathematical Society. In 2012, he became a fellow of the American Mathematical Society. He was also appointed as a Honorary Fellow of the Royal Academy of Engineering in 1993.\n\nAtiyah has been awarded honorary degrees by the universities of Birmingham, Bonn, Chicago, Cambridge, Dublin, Durham, Edinburgh, Essex, Ghent, Helsinki, Lebanon, Leicester, London, Mexico, Montreal, Oxford, Reading, Salamanca, St. Andrews, Sussex, Wales, Warwick, the American University of Beirut, Brown University, Charles University in Prague, Harvard University, Heriot–Watt University, Hong Kong (Chinese University), Keele University, Queen's University (Canada), The Open University, Technical University of Catalonia, and UMIST.\nAtiyah was made a Knight Bachelor in 1983 and made a member of the Order of Merit in 1992.\n\nThe Michael Atiyah building at the University of Leicester\nand the Michael Atiyah Chair in Mathematical Sciences at the American University of Beirut were named after him.\n\nAtiyah married Lily Brown on 30 July 1955, with whom he has three sons, John, David and Robin. Atiyah's eldest son John died on 24 June 2002 while on a walking holiday in the Pyrenees with his wife Maj-Lis. Lily Atiyah died on 13 March 2018 at the age of 90.\n\n\n"}
{"id": "30272017", "url": "https://en.wikipedia.org/wiki?curid=30272017", "title": "Nearly Kähler manifold", "text": "Nearly Kähler manifold\n\nIn mathematics, a nearly Kähler manifold is an almost Hermitian manifold formula_1, with almost complex structure formula_2,\nsuch that the (2,1)-tensor formula_3 is skew-symmetric. So,\n\nfor every vector field formula_5 on formula_1.\n\nIn particular, a Kähler manifold is nearly Kähler. The converse is not true. \nFor example, the nearly Kähler six-sphere formula_7 is an example of a nearly Kähler manifold that is not Kähler. The familiar almost complex structure on the six-sphere is not induced by a complex atlas on formula_7.\nUsually, non Kählerian nearly Kähler manifolds are called \"strict nearly Kähler manifolds\".\n\nNearly Kähler manifolds, also known as almost Tachibana manifolds, were studied by Shun-ichi Tachibana in 1959 and then by Alfred Gray from 1970 on.\nFor example, it was proved that any 6-dimensional strict nearly Kähler manifold is an Einstein manifold and has vanishing first Chern class\n(in particular, this implies spin). \nIn the 1980s, strict nearly Kähler manifolds obtained a lot of consideration because of their relation to Killing\nspinors: Thomas Friedrich and Ralf Grunewald showed that a 6-dimensional Riemannian manifold admits\na Riemannian Killing spinor if and only if it is nearly Kähler. This was later given a more fundamental explanation by Christian Bär, who pointed out that\nthese are exactly the 6-manifolds for which the corresponding 7-dimensional Riemannian cone has holonomy G. \n\nThe only compact 6-manifolds known to admit strict nearly Kähler metrics are formula_9, and formula_10. Each of these admits such a unique nearly Kähler metric that is also homogeneous, and these examples are in fact the only compact homogeneous strictly nearly Kähler 6-manifolds.\nHowever, Foscolo and Haskins recently showed that formula_7 and formula_10 also admit strict nearly Kähler metrics that are not homogeneous. \n\nBär's observation about the holonomy of Riemannian cones might seem to indicate that the nearly-Kähler condition is \nmost natural and interesting in dimension 6. This actually borne out by a theorem of Nagy, who proved that any strict, complete nearly Kähler manifold is locally a Riemannian product of homogeneous nearly Kähler spaces, twistor spaces over quaternion-Kähler manifolds, and 6-dimensional nearly Kähler manifolds.\n\nNearly Kähler manifolds are also an interesting class of manifolds admitting a metric connection with\nparallel totally antisymmetric torsion.\n\nNearly Kähler manifolds should not be confused with almost Kähler manifolds.\nAn almost Kähler manifold formula_1 is an almost Hermitian manifold with a closed Kähler form:\nformula_14. The Kähler form or fundamental 2-form formula_15 is defined by\n\nwhere formula_17 is the metric on formula_1. The nearly Kähler condition and the almost Kähler condition are essentially exclusive: an almost Hermitian manifold is both nearly Kähler and almost Kahler if and only if it is Kähler.\n"}
{"id": "16130126", "url": "https://en.wikipedia.org/wiki?curid=16130126", "title": "New digraph reconstruction conjecture", "text": "New digraph reconstruction conjecture\n\nThe reconstruction conjecture of Stanislaw Ulam is one of the best-known open problems in graph theory. Using the terminology of Frank Harary it can be stated as follows: If \"G\" and \"H\" are two graphs on at least three vertices and ƒ is a bijection from \"V\"(\"G\") to \"V\"(\"H\") such that \"G\"\\{\"v\"} and \"H\"\\{ƒ(\"v\")} are isomorphic for all vertices \"v\" in \"V\"(\"G\"), then \"G\" and \"H\" are isomorphic.\n\nIn 1964 Harary extended the reconstruction conjecture to directed graphs on at least five vertices as the so-called digraph reconstruction conjecture. Many results supporting the digraph reconstruction conjecture appeared between 1964 and 1976. However, this conjecture was proved to be false when P. K. Stockmeyer discovered several infinite families of counterexample pairs of digraphs (including tournaments) of arbitrarily large order. The falsity of the digraph reconstruction conjecture caused doubt about the reconstruction conjecture itself. Stockmeyer even observed that “perhaps the considerable effort being spent in attempts to prove the (reconstruction) conjecture should be balanced by more serious attempts to construct counterexamples.”\n\nIn 1979, Ramachandran revived the digraph reconstruction conjecture in a slightly weaker form called the new digraph reconstruction conjecture. In a digraph, the number of arcs incident from (respectively, to) a vertex \"v\" is called the outdegree (indegree) of \"v\" and is denoted by \"od\"(\"v\") (respectively, \"id\"(\"v\")). The new digraph conjecture may be stated as follows:\n\nThe new digraph reconstruction conjecture reduces to the reconstruction conjecture in the undirected case, because if all the vertex-deleted subgraphs of two graphs are isomorphic, then the corresponding vertices must have the same degree. Thus, the new digraph reconstruction conjecture is stronger than the reconstruction conjecture, but weaker than the disproved digraph reconstruction conjecture. Several families of digraphs have been shown to satisfy the new digraph reconstruction conjecture and these include all the digraphs in the known counterexample pairs to the digraph reconstruction conjecture.\n\n\nAs of 2018, no counterexample to the new digraph reconstruction conjecture is known.\n"}
{"id": "35574895", "url": "https://en.wikipedia.org/wiki?curid=35574895", "title": "Oriented Point Relation Algebra", "text": "Oriented Point Relation Algebra\n\nThe Oriented Point Relation Algebra (OPRA) serves for qualitative spatial representation and reasoning. OPRA is an orientation calculus with adjustable granularity. OPRA is based on objects which are represented as oriented points. Oriented points are specified as pair of a point and a direction on the 2D-plane.\n\n"}
{"id": "195063", "url": "https://en.wikipedia.org/wiki?curid=195063", "title": "Orthonormal basis", "text": "Orthonormal basis\n\nIn mathematics, particularly linear algebra, an orthonormal basis for an inner product space \"V\" with finite dimension is a basis for \"V\" whose vectors are orthonormal, that is, they are all unit vectors and orthogonal to each other. For example, the standard basis for a Euclidean space R is an orthonormal basis, where the relevant inner product is the dot product of vectors. The image of the standard basis under a rotation or reflection (or any orthogonal transformation) is also orthonormal, and every orthonormal basis for R arises in this fashion.\n\nFor a general inner product space \"V\", an orthonormal basis can be used to define normalized orthogonal coordinates on \"V\". Under these coordinates, the inner product becomes a dot product of vectors. Thus the presence of an orthonormal basis reduces the study of a finite-dimensional inner product space to the study of R under dot product. Every finite-dimensional inner product space has an orthonormal basis, which may be obtained from an arbitrary basis using the Gram–Schmidt process.\n\nIn functional analysis, the concept of an orthonormal basis can be generalized to arbitrary (infinite-dimensional) inner product spaces. Given a pre-Hilbert space \"H\", an orthonormal basis for \"H\" is an orthonormal set of vectors with the property that every vector in \"H\" can be written as an infinite linear combination of the vectors in the basis. In this case, the orthonormal basis is sometimes called a Hilbert basis for \"H\". Note that an orthonormal basis in this sense is not generally a Hamel basis, since infinite linear combinations are required. Specifically, the linear span of the basis must be dense in \"H\", but it may not be the entire space.\n\nIf we go on to Hilbert spaces, a non-orthonormal set of vectors having the same linear span as an orthonormal basis may not be a basis at all. For instance, any square-integrable function on the interval [−1, 1] can be expressed (almost everywhere) as an infinite sum of Legendre polynomials (an orthonomal basis), but not necessarily as an infinite sum of the monomials \"x\".\n\n\nIf \"B\" is an orthogonal basis of \"H\", then every element \"x\" of \"H\" may be written as\n\nWhen \"B\" is orthonormal, this simplifies to\n\nand the square of the norm of \"x\" can be given by\n\nEven if \"B\" is uncountable, only countably many terms in this sum will be non-zero, and the expression is therefore well-defined. This sum is also called the \"Fourier expansion\" of \"x\", and the formula is usually known as Parseval's identity.\n\nIf \"B\" is an orthonormal basis of \"H\", then \"H\" is \"isomorphic\" to \"ℓ\"(\"B\") in the following sense: there exists a bijective linear map such that\n\nfor all \"x\" and \"y\" in \"H\".\n\nGiven a Hilbert space \"H\" and a set \"S\" of mutually orthogonal vectors in \"H\", we can take the smallest closed linear subspace \"V\" of \"H\" containing \"S\". Then \"S\" will be an orthogonal basis of \"V\"; which may of course be smaller than \"H\" itself, being an \"incomplete\" orthogonal set, or be \"H\", when it is a \"complete\" orthogonal set.\n\nUsing Zorn's lemma and the Gram–Schmidt process (or more simply well-ordering and transfinite recursion), one can show that \"every\" Hilbert space admits a basis and thus an orthonormal basis; furthermore, any two orthonormal bases of the same space have the same cardinality (this can be proven in a manner akin to that of the proof of the usual dimension theorem for vector spaces, with separate cases depending on whether the larger basis candidate is countable or not). A Hilbert space is separable if and only if it admits a countable orthonormal basis. (One can prove this last statement without using the axiom of choice).\n\nThe set of orthonormal bases for a space is a principal homogeneous space for the orthogonal group O(\"n\"), and is called the Stiefel manifold formula_8 of orthonormal \"n\"-frames.\n\nIn other words, the space of orthonormal bases is like the orthogonal group, but without a choice of base point: given an orthogonal space, there is no natural choice of orthonormal basis, but once one is given one, there is a one-to-one correspondence between bases and the orthogonal group.\nConcretely, a linear map is determined by where it sends a given basis: just as an invertible map can take any basis to any other basis, an orthogonal map can take any \"orthogonal\" basis to any other \"orthogonal\" basis.\n\nThe other Stiefel manifolds formula_9 for formula_10 of \"incomplete\" orthonormal bases (orthonormal \"k\"-frames) are still homogeneous spaces for the orthogonal group, but not \"principal\" homogeneous spaces: any \"k\"-frame can be taken to any other \"k\"-frame by an orthogonal map, but this map is not uniquely determined.\n\n"}
{"id": "40565028", "url": "https://en.wikipedia.org/wiki?curid=40565028", "title": "PathVisio", "text": "PathVisio\n\nPathVisio is a free open-source pathway analysis and drawing software. It allows drawing, editing, and analyzing biological pathways.\nVisualization of ones experimental data on the pathways for finding relevant pathways that are over-represented in your data set is possible.\n\nPathVisio provides a basic set of features for pathway drawing, analysis and visualization. Additional features are available as plugins.\n\nPathVisio was created primarily at Maastricht University and Gladstone Institutes. The software is developed in Java and it's also used as part of the WikiPathways framework as an applet.\nStarting from version 3.0 (released in 2012) plugins are OSGi compliant and a plugin directory, describing them, was developed.\nIn 2015 version 3.2 was released. This was the first signed version with a certificate issued by a certification authority. Many of the running issues introduced by java 1.7 and 1.8 with the new security rules were solved.\nSince 2013 a javascript version (PVJS) is being developed to replace the applet. From 2015 it also allows small edits and in the future it will be a full editor.\n\n\n"}
{"id": "35033472", "url": "https://en.wikipedia.org/wiki?curid=35033472", "title": "Peter Bouwknegt", "text": "Peter Bouwknegt\n\nPier Gerard \"Peter\" Bouwknegt (born 20 April 1961, Geldrop) is Professor of Theoretical Physics and Mathematics at the Australian National University (ANU), and Deputy Director of their Mathematical Sciences Institute. He is an adjunct professor at University of Adelaide.\n\nHe studied Theoretical Physics and Mathematics at the University of Utrecht, Netherlands, and at the University of Amsterdam under the direction of Prof F.A. Bais, obtaining his PhD in 1988. After that, he became a postdoctoral fellow at MIT, CERN, and the University of Southern California. He moved to Australia in 1995 and worked at the University of Adelaide as an ARC QEII Fellow and subsequently as an ARC Senior Research Fellow. In 2005, he was appointed Professor of Theoretical Physics and Mathematics at the Australian National University.\n\nIn 2001, he received the 2001 Australian Mathematical Society Medal, and from 2009–2011, he served on the Australian Research Council's College of Experts He was formerly director of the Mathematical Sciences Institute at ANU where is now Deputy Director.\n\nBowknegt specializes in the mathematical foundations of String Theory and Conformal Field Theory. According to his web site at ANU, his specific interests are \"the investigation of mathematical aspects of physical theories, in particular quantum field theories. Main expertise is the structure of two-dimensional conformal field theory and their applications in diverse areas such as condensed matter physics, integrable models of statistical mechanics and string theory, as well as the mathematical structures underlying string theory and D-branes, using mathematical techniques such as K-theory and gerbes.\"\n\n\n"}
{"id": "33007978", "url": "https://en.wikipedia.org/wiki?curid=33007978", "title": "Prabodh Chandra Sengupta", "text": "Prabodh Chandra Sengupta\n\nPrabodh Chandra Sengupta (1876–1962) was a historian of astronomy in ancient India. He was a Professor of Mathematics in Bethune College Calcutta and a lecturer in Indian Astronomy and Mathematics at the University of Calcutta.\n\n\n"}
{"id": "30864999", "url": "https://en.wikipedia.org/wiki?curid=30864999", "title": "Rasiowa–Sikorski lemma", "text": "Rasiowa–Sikorski lemma\n\nIn axiomatic set theory, the Rasiowa–Sikorski lemma (named after Helena Rasiowa and Roman Sikorski) is one of the most fundamental facts used in the technique of forcing. In the area of forcing, a subset \"E\" of a forcing notion (\"P\", ≤) is called dense in \"P\" if for any \"p\" ∈ \"P\" there is \"e\" ∈ \"E\" with \"e\" ≤ \"p\". If \"D\" is a family of dense subsets of \"P\", a filter \"F\" in \"P\" is called \"D\"-generic if\n\nNow we can state the Rasiowa–Sikorski lemma:\n\nThe proof runs as follows: since \"D\" is countable, one can enumerate the dense subsets of \"P\" as \"D\", \"D\", …. By assumption, there exists \"p\" ∈ \"P\". Then by density, there exists \"p\" ≤ \"p\" with \"p\" ∈ \"D\". Repeating, one gets … ≤ \"p\" ≤ \"p\" ≤ \"p\" with \"p\" ∈ \"D\". Then \"G\" = { \"q\" ∈ \"P\": ∃ \"i\", \"q\" ≥ \"p\"} is a \"D\"-generic filter.\n\nThe Rasiowa–Sikorski lemma can be viewed as a weaker form of an equivalent to Martin's axiom. More specifically, it is equivalent to MA(formula_1).\n\n\n\n\n"}
{"id": "61633", "url": "https://en.wikipedia.org/wiki?curid=61633", "title": "René Thom", "text": "René Thom\n\nRené Frédéric Thom (; 2 September 1923 – 25 October 2002) was a French mathematician. He made his reputation as a topologist, moving on to aspects of what would be called singularity theory; he became world-famous among the wider academic community and the educated general public for one aspect of this latter interest, his work as founder of catastrophe theory (later developed by Erik Christopher Zeeman). He received the Fields Medal in 1958.\n\nRené Thom was born in Montbéliard, Doubs. He was educated at the Lycée Saint-Louis and the École Normale Supérieure, both in Paris. He received his PhD in 1951 from the University of Paris. His thesis, titled \"Espaces fibrés en sphères et carrés de Steenrod\" (\"Sphere bundles and Steenrod squares\"), was written under the direction of Henri Cartan. The foundations of cobordism theory, for which he received the Fields Medal at the International Congress of Mathematicians in Edinburgh in 1958, were already present in his thesis.\n\nAfter a fellowship in the United States, he went on to teach at the Universities of Grenoble (1953–1954) and Strasbourg (1954–1963), where he was appointed Professor in 1957. In 1964, he moved to the Institut des Hautes Études Scientifiques, in Bures-sur-Yvette. He was awarded the Brouwer Medal in 1970, the Grand Prix Scientifique de la Ville de Paris in 1974, and became a Member of the Académie des Sciences of Paris in 1976.\n\nWhile René Thom is most known to the public for his development of catastrophe theory between 1968 and 1972, his earlier work was on differential topology. In the early 1950s it concerned what are now called Thom spaces, characteristic classes, cobordism theory, and the Thom transversality theorem. Another example of this line of work is the Thom conjecture, versions of which have been investigated using gauge theory. From the mid 1950s he moved into singularity theory, of which catastrophe theory is just one aspect, and in a series of deep (and at the time obscure) papers between 1960 and 1969 developed the theory of stratified sets and stratified maps, proving a basic stratified isotopy theorem describing the local conical structure of Whitney stratified sets, now known as the Thom–Mather isotopy theorem. Much of his work on stratified sets was developed so as to understand the notion of topologically stable maps, and to eventually prove the result that the set of topologically stable mappings between two smooth manifolds is a dense set. Thom's lectures on the stability of differentiable mappings, given at the University of Bonn in 1960, were written up by Harold Levine and published in the proceedings of a year long symposium on singularities at Liverpool University during 1969-70, edited by C. T. C. Wall. The proof of the density of topologically stable mappings was completed by John Mather in 1970, based on the ideas developed by Thom in the previous ten years. A coherent detailed account was published in 1976 by Cristopher Gibson, Klaus Wirthmüller, Andrew du Plessis, and Eduard Looijenga.\n\nDuring the last twenty years of his life Thom's published work was mainly in philosophy and epistemology, and he undertook a reevaluation of Aristotle's writings on science. In 1992, he was one of eighteen academics who sent a letter to Cambridge University protesting against plans to award Jacques Derrida an honorary doctorate.\n\nBeyond Thom's contributions to algebraic topology, he studied differentiable mappings, through the study of generic properties.\n\n\n\n\n"}
{"id": "16951539", "url": "https://en.wikipedia.org/wiki?curid=16951539", "title": "Rotating calipers", "text": "Rotating calipers\n\nIn computational geometry, the method of rotating calipers is an algorithm design technique that can be used to solve optimization problems including finding the width or diameter of a set of points.\n\nThe method is so named because the idea is analogous to rotating a spring-loaded vernier caliper around the outside of a convex polygon. Every time one blade of the caliper lies flat against an edge of the polygon, it forms an antipodal pair with the point or edge touching the opposite blade. The complete \"rotation\" of the caliper around the polygon detects all antipodal pairs; the set of all pairs, viewed as a graph, forms a thrackle. The method of rotating calipers can be interpreted as the projective dual of a sweep line algorithm in which the sweep is across slopes of lines rather than across - or -coordinates of points.\n\nThe rotating calipers method was first used in the dissertation of Michael Shamos in 1978. Shamos uses this method to generate all antipodal pairs of points on a convex polygon and to compute the diameter of a convex polygon in formula_1 time. Godfried Toussaint coined the phrase \"rotating calipers\" and also demonstrated that the method was applicable in solving many other computational geometry problems.\n\nShamos gave following algorithm in his dissertation (pp 77–82) for the rotating calipers method that generated all antipodal pairs of vertices on convex polygon:\n\nAnother version of this algorithm appeared in the text by Preparata and Shamos in 1985 that avoided calculation of angles:\n\nThis method has several advantages including that it avoids calculation of area or angles as well as sorting by polar angles. The method is based on finding convex hull using Monotone chain method devised by A.M. Andrew which returns upper and lower portions of hull separately that then can be used naturally for rotating calipers analogy.\n\nToussaint and Pirzadeh describes various applications of rotating calipers method.\n\n\n\n\n\n\n\n ARRAY points := {P1, P2, ..., PN};\n\n"}
{"id": "13345571", "url": "https://en.wikipedia.org/wiki?curid=13345571", "title": "Sammon mapping", "text": "Sammon mapping\n\nSammon mapping or Sammon projection is an algorithm that maps a high-dimensional space to a space of lower dimensionality (see multidimensional scaling) by trying to preserve the structure of inter-point distances in high-dimensional space in the lower-dimension projection. It is particularly suited for use in exploratory data analysis. The method was proposed by John W. Sammon in 1969. It is considered a non-linear approach as the mapping cannot be represented as a linear combination of the original variables as possible in techniques such as principal component analysis, which also makes it more difficult to use for classification applications.\n\nDenote the distance between ith and jth objects in the original space by formula_1, and the distance between their projections by formula_2. Sammon's mapping aims to minimize the following error function, which is often referred to as Sammon's stress or Sammon's error:\n\nThe minimization can be performed either by gradient descent, as proposed initially, or by other means, usually involving iterative methods. The number of iterations need to be experimentally determined and convergent solutions are not always guaranteed. Many implementations prefer to use the first Principal Components as a starting configuration.\n\nThe Sammon mapping has been one of the most successful nonlinear metric multidimensional scaling methods since its advent in 1969, but effort has been focused on algorithm improvement rather than on the form of the stress function. The performance of the Sammon mapping has been improved by extending its stress function using left Bregman divergence \n\n"}
{"id": "1730659", "url": "https://en.wikipedia.org/wiki?curid=1730659", "title": "Square–cube law", "text": "Square–cube law\n\nThe square–cube law (or cube–square law) is a mathematical principle, applied in a variety of scientific fields, which describes the relationship between the volume and the surface area as a shape's size increases or decreases. It was first described in 1638 by Galileo Galilei in his \"Two New Sciences\" as the \"...ratio of two volumes is greater than the ratio of their surfaces\".\n\nThis principle states that, as a shape grows in size, its volume grows faster than its surface area. When applied to the real world this principle has many implications which are important in fields ranging from mechanical engineering to biomechanics. It helps explain phenomena including why large mammals like elephants have a harder time cooling themselves than small ones like mice, and why building taller and taller skyscrapers is increasingly difficult.\n\nThe square–cube law can be stated as follows:\n\nRepresented mathematically:\n\nwhere formula_2 is the original surface area and formula_3 is the new surface area.\n\nwhere formula_5 is the original volume, formula_6 is the new volume, formula_7 is the original length and formula_8 is the new length.\n\nFor example, a cube with a side length of 1 meter has a surface area of 6 m and a volume of 1 m. If the dimensions of the cube were multiplied by 2, its surface area would be multiplied by the \"square\" of 2 and become 24 m. Its volume would be multiplied by the \"cube\" of 2 and become 8 m. \n\nThe original cube (1m sides) has a surface area to volume ratio of 6:1. The larger (2m sides) cube has a surface area to volume ratio of (24/8) 3:1. As the dimensions increase, the volume will continue to grow faster than the surface area. Thus the square–cube law. This principle applies to all solids.\n\nWhen a physical object maintains the same density and is scaled up, its volume and mass are increased by the cube of the multiplier while its surface area increases only by the square of said multiplier. This would mean that when the larger version of the object is accelerated at the same rate as the original, more pressure would be exerted on the surface of the larger object.\n\nConsider a simple example of a body of mass, M, having an acceleration, a, and surface area, A, of the surface upon which the accelerating force is acting. The force due to acceleration, formula_9 and the thrust pressure, formula_10.\n\nNow, consider the object be exaggerated by a multiplier factor = x so that it has a new mass, formula_11, and the surface upon which the force is acting has a new surface area, formula_12.\n\nThe new force due to acceleration formula_13 and the resulting thrust pressure,\n\nThus, just scaling up the size of an object, keeping the same material of construction (density), and same acceleration, would increase the thrust by the same scaling factor. This would indicate that the object would have less ability to resist stress and would be more prone to collapse while accelerating.\n\nThis is why large vehicles perform poorly in crash tests and why there are limits to how high buildings can be built. Similarly, the larger an object is, the less other objects would resist its motion, causing its deceleration.\n\n\n\nIf an animal were isometrically scaled up by a considerable amount, its relative muscular strength would be severely reduced, since the cross section of its muscles would increase by the \"square\" of the scaling factor while its mass would increase by the \"cube\" of the scaling factor. As a result of this, cardiovascular and respiratory functions would be severely burdened.\n\nIn the case of flying animals, the wing loading would be increased if they were isometrically scaled up, and they would therefore have to fly faster to gain the same amount of lift. Air resistance per unit mass is also higher for smaller animals, which is why a small animal like an ant cannot be seriously injured from impact with the ground after being dropped from any height.\n\nAs stated by J. B. S. Haldane, large animals do not look like small animals: an elephant cannot be mistaken for a mouse scaled up in size. This is due to allometric scaling: the bones of an elephant are necessarily proportionately much larger than the bones of a mouse, because they must carry proportionately higher weight. Haldane illustrates this in his seminal 1928 essay \"On Being the Right Size\" in referring to allegorical giants: \"...consider a man 60 feet high...Giant Pope and Giant Pagan in the illustrated \"Pilgrim's Progress:\" ...These monsters...weighed 1000 times as much as Christian. Every square inch of a giant bone had to support 10 times the weight borne by a square inch of human bone. As the average human thigh-bone breaks under about 10 times the human weight, Pope and Pagan would have broken their thighs every time they took a step.\" Consequently, most animals show allometric scaling with increased size, both among species and within a species. The giant creatures seen in monster movies (e.g., Godzilla or King Kong) are also unrealistic, as their sheer size would force them to collapse.\n\nHowever, the buoyancy of water negates to some extent the effects of gravity. Therefore, sea creatures can grow to very large sizes without the same musculoskeletal structures that would be required of similarly sized land creatures, and it is no coincidence that the largest animals to ever exist on earth are aquatic animals.\n\nThe metabolic rate of animals scales with a mathematical principle named quarter-power scaling according to the metabolic theory of ecology.\n\nMass transfer such as diffusion to smaller objects such as living cells is faster than diffusion to larger objects such as entire animals. Thus, in chemical processes that take place on a surface - rather than in the bulk - finer-divided material is more active. For example, the activity of a heterogeneous catalyst is higher when it is divided into finer particles.\n\nHeat production from a chemical process scales with the cube of the linear dimension (height, width) of the vessel, but the vessel surface area scales with only the square of the linear dimension. Consequently, larger vessels are much more difficult to cool. Also, large-scale piping for transferring hot fluids is difficult to simulate in small scale, because heat is transferred faster out from smaller pipes. Failure to take this into account in process design may lead to catastrophic thermal runaway.\n\n"}
{"id": "15698614", "url": "https://en.wikipedia.org/wiki?curid=15698614", "title": "Stable roommates problem", "text": "Stable roommates problem\n\nIn mathematics, economics and computer science, particularly in the fields of combinatorics, game theory and algorithms, the stable-roommate problem (SRP) is the problem of finding a stable matching for an even-sized set. A matching is a separation of the set into disjoint pairs (\"roommates\"). The matching is \"stable\" if there are no two elements which are not roommates and which both prefer each other to their roommate under the matching. This is distinct from the stable-marriage problem in that the stable-roommates problem allows matches between any two elements, not just between classes of \"men\" and \"women\".\n\nIt is commonly stated as:\n\nUnlike the stable marriage problem, a stable matching may fail to exist for certain sets of participants and their preferences. For a minimal example of a stable pairing not existing, consider 4 people A, B, C, and D, whose rankings are:\n\nIn this ranking, each of A, B, and C is the most preferable person for someone. In any solution, one of A, B, or C \"must\" be paired with D and the other two with each other (for example AD and BC), yet for anyone who is partnered with D, another member will have rated them highest, and D’s partner will in turn prefer this other member over D. In this example, AC is a more favorable pairing than AD, but the necessary remaining pairing of BD then raises the same issue, illustrating the absence of a stable matching for these participants and their preferences.\n\nAn efficient algorithm was given in . The algorithm will determine, for any instance of the problem, whether a stable matching exists, and if so, will find such a matching. Irving’s algorithm has O(\"n\") complexity, provided suitable data structures are used to implement the necessary manipulation of the preference lists and identification of rotations.\n\nThe algorithm consists of two phases. In Phase 1, participants \"propose\" to each other, in a manner similar to that of the Gale-Shapley algorithm for the stable marriage problem. Each participant orders the other members by preference, resulting in a preference list—an ordered set of the other participants. Participants then propose to each person on their list, in order, continuing to the next person if and when their current proposal is rejected. A participant will reject a proposal if they already hold a proposal from someone they prefer. A participant will also reject a previously-accepted proposal if they later receive a proposal that they prefer. In this case, the rejected participant will then propose to the next person on their list, continuing until a proposal is again accepted. If any participant is eventually rejected by all other participants, this indicates that no stable matching is possible. Otherwise, Phase 1 will end with each person holding a proposal from one of the others.\n\nConsider two participants, \"q\" and \"p\". If \"q\" holds a proposal from \"p\", then we remove from \"q\"s list all participants \"x\" after \"p\", and symmetrically, for each removed participant \"x\", we remove \"q\" from \"x\"s list, so that \"q\" is first in \"p\"s list; and \"p\", last in \"q\"s, since \"q\" and any \"x\" cannot be partners in any stable matching. The resulting reduced set of preference lists together is called the Phase 1 table. In this table, if any reduced list is empty, then there is no stable matching. Otherwise, the Phase 1 table is a \"stable table\". A stable table, by definition, is the set of preference lists from the original table after members have been removed from one or more of the lists, and the following three conditions are satisfied (where reduced list means a list in the stable table):\n\n(i) \"p\" is first on \"q\"s reduced list if and only if \"q\" is last on \"p\"s \n(ii) \"p\" is not on \"q\"s reduced list if and only if \"q\" is not on \"p\"s if and only if \"q\" prefers the last person on their list to \"p\"; or \"p\", the last person on their list to \"q\" \n(iii) no reduced list is empty\n\nStable tables have several important properties, which are used to justify the remainder of the procedure:\n\n1. Any stable table must be a subtable of the Phase 1 table, where subtable is a table where the preference lists of the subtable are those of the supertable with some individuals removed from each other's lists.\n\n2. In any stable table, if every reduced list contains \"exactly\" one individual, then pairing each individual with the single person on their list gives a stable matching.\n\n3. If the stable roommates problem instance has a stable matching, then there is a stable matching contained in any one of the stable tables.\n\n4. Any stable subtable of a stable table, and in particular any stable subtable that specifies a stable matching as in 2, can be obtained by a sequence of \"rotation eliminations\" on the stable table.\n\nThese rotation eliminations comprise Phase 2 of Irving’s algorithm.\n\nBy 2, if each reduced list of the Phase 1 table contains exactly one individual, then this gives a matching.\n\nOtherwise, the algorithm enters Phase 2. A \"rotation\" in a stable table \"T\" is defined as a sequence (\"x\", \"y\"), (\"x\", \"y\"), ..., (\"x\", \"y\") such that the \"x\" are distinct, \"y\" is first on \"x\"'s reduced list (or \"x\" is last on \"y\"'s reduced list) and \"y\" is second on \"x\"'s reduced list, for i = 0, ..., k-1 where the indices are taken modulo k. It follows that in any stable table with a reduced list containing at least two individuals, such a rotation always exists. To find it, start at such a \"p\" containing at least two individuals in their reduced list, and define recursively \"q\" to be the second on \"p\"'s list and \"p\" to be the last on \"q\"'s list, until this sequence repeats some \"p\", at which point a rotation is found: it is the sequence of pairs starting at the first occurrence of (\"p\", \"q\") and ending at the pair before the last occurrence. The sequence of \"p\" up until the \"p\" is called the \"tail\" of the rotation. The fact that it's a stable table in which this search occurs guarantees that each \"p\" has at least two individuals on their list.\n\nTo eliminate the rotation, \"y\" rejects \"x\" so that \"x\" proposes to \"y\", for each \"i\". To restore the stable table properties (i) and (ii), for each \"i\", all successors of \"x\" are removed from \"y\"'s list, and \"y\" is removed from their lists. If a reduced list becomes empty during these removals, then there is no stable matching. Otherwise, the new table is again a stable table, and either already specifies a matching since each list contains exactly one individual or there remains another rotation to find and eliminate, so the step is repeated.\n\nPhase 2 of the algorithm can now be summarized as follows:\n\n<source lang=\"java\">\nT = Phase 1 table;\nwhile (true) {\n"}
{"id": "52301386", "url": "https://en.wikipedia.org/wiki?curid=52301386", "title": "Stefan Bergman Prize", "text": "Stefan Bergman Prize\n\nThe Stefan Bergman Prize is a mathematics award, funded by the estate of the widow of mathematician Stefan Bergman and supported by the American Mathematical Society. The award is granted for mathematical research in: \"1) the theory of the kernel function and its applications in real and complex analysis; or 2) function-theoretic methods in the theory of partial differential equations of elliptic type with attention to Bergman's operator method.\"\n\nThe award is given in honor of Stefan Bergman, a mathematician known for his work on complex analysis. Recipients of the prize are selected by a committee of judges appointed by the American Mathematical Society. The monetary value of the prize is variable and based on the income from the prize fund; in 2005 the award was valued at approximately $17,000. \n\n\n"}
{"id": "23093011", "url": "https://en.wikipedia.org/wiki?curid=23093011", "title": "Strength of a graph", "text": "Strength of a graph\n\nIn the branch of mathematics called graph theory, the strength of an undirected graph corresponds to the minimum ratio \"edges removed\"/\"components created\" in a decomposition of the graph in question. It is a method to compute partitions of the set of vertices and detect zones of high concentration of edges, and is analogous to graph toughness which is defined similarly for vertex removal.\n\nThe strength formula_1 of an undirected simple graph \"G\" = (\"V\", \"E\") admits the three following definitions:\n\n\nComputing the strength of a graph can be done in polynomial time, and the first such algorithm\nwas discovered by Cunningham (1985). The algorithm with best complexity for computing exactly the strength is due to Trubin (1993), uses the flow decomposition of Goldberg and Rao (1998), in time formula_10.\n\n\n"}
{"id": "15342831", "url": "https://en.wikipedia.org/wiki?curid=15342831", "title": "Summa (mathematics)", "text": "Summa (mathematics)\n\nThe term summa is the Latin word for \"sum\". It is also used as the name for the ∫ symbol used to represent the integral in calculus. It was first used to represent the limiting sum (Riemann sum) that inspired the integral by Leibniz on October 29, 1675.\n\n"}
{"id": "40328103", "url": "https://en.wikipedia.org/wiki?curid=40328103", "title": "Trinomial triangle", "text": "Trinomial triangle\n\nThe trinomial triangle is a variation of Pascal's triangle. The difference between the two is that an entry in the trinomial triangle is the sum of the \"three\" (rather than the \"two\" in Pascal's triangle) entries above it:\nformula_1\nThe formula_2-th entry of the formula_3-th row is denoted by\n\nRows are counted starting from 0. The entries of the formula_3-th row are indexed starting with formula_6 from the left, and the middle entry has index 0. The symmetry of the entries of a row about the middle entry is expressed by the relationship\n\nThe formula_3-th row corresponds to the coefficients in the polynomial expansion of the expansion of the trinomial formula_9 raised to the formula_3-th power:\n\nor, symmetrically,\nhence the alternative name trinomial coefficients because of their relationship to the multinomial coefficients:\n\nFurthermore, the diagonals have interesting properties, such as their relationship to the triangular numbers.\n\nThe sum of the elements of formula_3-th row is formula_15.\n\nThe trinomial coefficients can be generated using the following recursion formula:\n\nwhere formula_19 for formula_20 and formula_21.\n\nThe middle entries of the trinomial triangle \n\nwere studied by Euler. The middle entry for the formula_3-th row is given by\n\nThe corresponding generating function is\n\nEuler also noted the following \"exemplum memorabile inductionis fallacis\" (\"notable example of fallacious induction\"):\n\nwhere formula_27 stands for the Fibonacci sequence. For larger formula_3, however, this relationship is incorrect. George Andrews explained this fallacy using the general identity\n\nThe triangle corresponds to the number of possible paths that can be taken by the king in a game of chess. The entry in a cell represents the number of different paths (using a minimum number of moves) the king can take to reach the cell.\n\nThe coefficient of formula_30 in the polynomial expansion of formula_31 specifies the number of different ways of randomly drawing formula_2 cards from two sets of formula_3 identical playing cards. For example, in such a card game with two sets of the three cards A, B, C, the choices look like this:\n\nIn particular, this results in formula_34 as the number of different hands in a game of \"Doppelkopf\".\n\nAlternatively, it is also possible to arrive at this number by considering the number of ways of choosing formula_35 pairs of identical cards from the two sets, which is formula_36. The remaining formula_37 cards can then be chosen in formula_38 ways, which can be written in terms of the binomial coefficients as\n\nFor example,\n\nThe example above corresponds to the three ways of selecting two cards without pairs of identical cards (AB, AC, BC) and the three ways of selecting a pair of identical cards (AA, BB, CC).\n\n"}
{"id": "15393951", "url": "https://en.wikipedia.org/wiki?curid=15393951", "title": "Vague topology", "text": "Vague topology\n\nIn mathematics, particularly in the area of functional analysis and topological vector spaces, the vague topology is an example of the weak-* topology which arises in the study of measures on locally compact Hausdorff spaces.\n\nLet \"X\" be a locally compact Hausdorff space. Let \"M\"(\"X\") be the space of complex Radon measures on \"X\", and \"C\"(\"X\") denote the dual of \"C\"(\"X\"), the Banach space of complex continuous functions on \"X\" vanishing at infinity equipped with the uniform norm. By the Riesz representation theorem \"M\"(\"X\") is isometric to \"C\"(\"X\"). The isometry maps a measure \"μ\" to a linear functional formula_1\n\nThe vague topology is the weak-* topology on \"C\"(\"X\"). The corresponding topology on \"M\"(\"X\") induced by the isometry from \"C\"(\"X\") is also called the vague topology on \"M\"(\"X\"). Thus in particular, a sequence of measures \"(μ)\" converges vaguely to a measure \"μ\" whenever for all test functions \"f ∈ C(X)\",\n\nformula_2\n\nIt is also not uncommon to define the vague topology by duality with continuous functions having compact support \"C(X)\", i.e. a sequence of measures \"(μ)\" converges vaguely to a measure \"μ\" whenever the above convergence holds for all test functions \"f ∈ C(X)\". This construction gives rise to a different topology. In particular, the topology defined by duality with \"C(X)\" can be metrizable whereas the topology defined by duality with \"C(X)\" is not.\n\nOne application of this is to probability theory: for example, the central limit theorem is essentially a statement that if \"μ\" are the probability measures for certain sums of independent random variables, then \"μ\" converge weakly (and then vaguely) to a normal distribution, i.e. the measure \"μ\" is \"approximately normal\" for large \"n\".\n\n"}
{"id": "12125059", "url": "https://en.wikipedia.org/wiki?curid=12125059", "title": "Vector Analysis", "text": "Vector Analysis\n\nVector Analysis is a textbook by Edwin Bidwell Wilson, first published in 1901 and based on the lectures that Josiah Willard Gibbs had delivered on the subject at Yale University. The book did much to standardize the notation and vocabulary of three-dimensional linear algebra and vector calculus, as used by physicists and mathematicians. It went through seven editions (1913, 1916, 1922, 1925, 1929, 1931, and 1943). The work is now in the public domain. It was reprinted by Dover Publications in 1960.\n\nThe book carries the subtitle \"A text-book for the use of students of mathematics and physics. Founded upon the lectures of J. Willard Gibbs, Ph.D., LL.D.\" The first chapter covers vectors in three spatial dimensions, the concept of a (real) scalar, and the product of a scalar with a vector. The second chapter introduces the dot and cross products for pairs of vectors. These are extended to a scalar triple product and a quadruple product. Pages 77–81 cover the essentials of spherical trigonometry, a topic of considerable interest at the time because of its use in celestial navigation. The third chapter introduces the vector calculus notation based on the del operator. The Helmholtz decomposition of a vector field is given on page 237.\n\nThe final eight pages develop bivectors as these were integral to the course on the electromagnetic theory of light that Professor Gibbs taught at Yale. First Wilson associates a bivector with an ellipse. The product of the bivector with a complex number on the unit circle is then called an \"elliptical rotation\". Wilson continues with a description of \"elliptic harmonic motion\" and the case of stationary waves.\n\nProfessor Gibbs produced an 85-page outline of his treatment of vectors for use by his students and had sent a copy to Oliver Heaviside in 1888. In 1892 Heaviside, who was formulating his own vectorial system in the \"Transactions of the Royal Society\", praised Gibbs' \"little book\", saying it \"deserves to be well known\". However, he also noted that it was \"much too condensed for a first introduction to the subject\".\n\nOn the occasion of the bicentennial of Yale University, a series of publications were to be issued to showcase Yale's role in the advancement of knowledge. Gibbs was authoring \"Elementary Principles in Statistical Mechanics\" for that series. Mindful of the demand for innovative university textbooks, the editor of the series, Professor Morris, wished to include also a volume dedicated to Gibbs's lectures on vectors, but Gibbs's time and attention were entirely absorbed by the \"Statistical Mechanics\".\n\nE. B. Wilson was then a new graduate student in mathematics. He had learned about quaternions from James Mills Peirce at Harvard, but Dean A. W. Phillips persuaded him to take Gibbs's course on vectors, which treated similar problems from a rather different perspective. After Wilson had completed the course, Morris approached him about the project of producing a textbook. Wilson wrote the book by expanding his own class notes, providing exercises, and consulting with others (including his father).\n\n\n"}
{"id": "32130416", "url": "https://en.wikipedia.org/wiki?curid=32130416", "title": "Zero to the power of zero", "text": "Zero to the power of zero\n\nZero to the power of zero, denoted by 0, is a mathematical expression with no agreed-upon value. The most common possibilities are 1 or leaving the expression undefined, with justifications existing for each, depending on context.\nIn algebra, combinatorics, or set theory, the generally agreed upon value is 0 = 1, whereas in mathematical analysis, the expression is generally left undefined. Computer programs also have differing ways of handling this expression.\n\nThere are many widely used formulas having terms involving natural-number exponents that require 0 to be evaluated to 1. For example, regarding \"b\" as an empty product assigns it the value 1, even when . Alternatively, the combinatorial interpretation of \"b\" is the number of empty tuples of elements from a set with \"b\" elements; there is exactly one empty tuple, even if . Equivalently, the set-theoretic interpretation of 0 is the number of functions from the empty set to the empty set; there is exactly one such function, the empty function.\n\nLikewise, when working with polynomials, it is often necessary to assign formula_1 the value 1. A polynomial is an expression of the form formula_2 where \"x\" is an indeterminate, and the coefficients formula_3 are real numbers (or, more generally, elements of some ring). The set of all real polynomials in \"x\" is denoted by formula_4. Polynomials are added termwise, and multiplied by the applying the usual rules for exponents in the indeterminate \"x\" (see Cauchy product). With these algebraic rules for manipulation, polynomials form a polynomial ring. The polynomial formula_5 is the identity element of the polynomial ring, meaning that it is the (unique) element such that the product of formula_5 with any polynomial formula_7 is just formula_7. Polynomials can be evaluated by specializing the indeterminate \"x\" to be a real number. More precisely, for any given real number formula_9 there is a unique unital ring homomorphism formula_10 such that formula_11. This is called the \"evaluation homomorphism\". Because it is a unital homomorphism, we have formula_12 That is, formula_13 for all specializations of \"x\" to a real number (including zero).\n\nThis perspective is significant for many polynomial identities appearing in combinatorics. For example, the binomial theorem formula_14 is not valid for unless . Similarly, rings of power series require formula_13 to be true for all specializations of \"x\". Thus identities like formula_16 and formula_17 are only true as functional identities (including at ) if .\n\nIn differential calculus, the power rule formula_18 is not valid for at unless .\n\nLimits involving algebraic operations can often be evaluated by replacing subexpressions by their limits; if the resulting expression does not determine the original limit, the expression is known as an indeterminate form. In fact, when and are real-valued functions both approaching (as approaches a real number or ), with , the function need not approach ; depending on and , the limit of can be any nonnegative real number or , or it can diverge. For example, the functions below are of the form with as (a one-sided limit), but the limits are different:\n\nThus, the two-variable function , though continuous on the set cannot be extended to a continuous function on any set containing , no matter how one chooses to define . However, under certain conditions, such as when and are both analytic functions and is positive on the open interval for some positive , the limit approaching from the right is always .\n\nIn the complex domain, the function may be defined for nonzero by choosing a branch of and defining as . This does not define since there is no branch of defined at , let alone in a neighborhood of .\n\nThe debate over the definition of formula_1 has been going on at least since the early 19th century. At that time, most mathematicians agreed that formula_21, until in 1821 Cauchy listed formula_1 along with expressions like formula_23 in a table of indeterminate forms. In the 1830s Libri published an unconvincing argument for formula_21, and Möbius sided with him, erroneously claiming that formula_25 whenever formula_26. A commentator who signed his name simply as \"S\" provided the counterexample of formula_27, and this quieted the debate for some time. More historical details can be found in Knuth (1992).\n\nMore recent authors interpret the situation above in different ways:\n\nThe IEEE 754-2008 floating-point standard is used in the design of most floating-point libraries. It recommends a number of operations for computing a power: \n\nThe pow variant is inspired by the pow function from C99, mainly for compatibility. It is useful mostly for languages with a single power function. The pown and powr variants have been introduced due to conflicting usage of the power functions and the different points of view (as stated above).\n\nThe C and C++ standards do not specify the result of 0 (a domain error may occur), but as of C99, if the normative annex F is supported, the result is required to be 1 because this value is more useful than NaN for significant applications (for instance, with discrete exponents). The Java standard and the .NET Framework method codice_1 also treat 0 as 1.\n\n\n"}
