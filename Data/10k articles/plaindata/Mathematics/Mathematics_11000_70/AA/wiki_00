{"id": "27751051", "url": "https://en.wikipedia.org/wiki?curid=27751051", "title": "Abdus Salam Award", "text": "Abdus Salam Award\n\nThe Abdus Salam Award (sometimes called the Salam Prize), is a most prestigious award that is awarded annually to Pakistani nationals to the field of chemistry, mathematics, physics, biology. The award is awarded to the scientists who are resident in Pakistan, below 35 years of age on 31 December of the year for which the Prize was to be awarded. It is to consist of a certificate giving a citation and a cash award of US$1,000. It is to be awarded on the basis of the collected research and/or a technical essay written specially for the Prize\n\nThe Award is a brainchild of Professor Abdus Salam's students Dr. Riazuddin, Dr. Fayyazuddin and Dr. Asghar Qadir who first presented the idea to Abdus Salam in 1979. Abdus Salam, who felt that he had no right to use the Prize money for personal purposes but that it must be used to further his mission of development of Science in the Third World. Abdus Salam specially put aside money to help Pakistan and Pakistani students. In 1980, Prof. Salam asked Prof. Fayyazuddin and Dr. Asghar Qadir to formulate the rules and procedures for a Prize to be awarded to young Pakistani scientists for their research in the basic sciences. Professor Asghar Qadir is currently the Secretary of Salam Prize Committee at School of Natural Sciences (SNS) in National University of Sciences and Technology (NUST). \n"}
{"id": "99861", "url": "https://en.wikipedia.org/wiki?curid=99861", "title": "Algorithmics", "text": "Algorithmics\n\nAlgorithmics is the science of algorithms. It includes algorithm design, the art of building a procedure which can solve efficiently a specific problem or a class of problem, algorithmic complexity theory, the study of estimating the hardness of problems by studying the properties of algorithm that solves them, or algorithm analysis, the science of studying the properties of a problem, such as quantifying resources in time and memory space needed by this algorithm to solve this problem.\n"}
{"id": "27119265", "url": "https://en.wikipedia.org/wiki?curid=27119265", "title": "Anabelian geometry", "text": "Anabelian geometry\n\nAnabelian geometry is a theory in number theory, which describes the way to which algebraic fundamental group \"G\" of a certain arithmetic variety \"V\", or some related geometric object, can help to restore \"V\". The first traditional conjectures, originating from Alexander Grothendieck and introduced in \"Esquisse d'un Programme\" were about how topological homomorphisms between two groups of two hyperbolic curves over number fields correspond to maps between the curves. These Grothendieck conjectures were partially solved by H. Nakamura, A. Tamagawa, and complete proofs were given by Shinichi Mochizuki. Before anabelian geometry proper began with the famous letter to Faltings and \"Esquisse d'un Programme\", the Neukirch–Uchida theorem hinted at the program from the perspective of Galois groups, which themselves can be shown to be étale fundamental groups.\n\nMore recently, Shinichi Mochizuki introduced and developed a so called mono-anabelian geometry which restores, for a certain class of hyperbolic curves over number fields, the curve from its algebraic fundamental group. Key results of mono-anabelian geometry were published in Mochizuki's \"Topics in Absolute Anabelian Geometry.\"\nThe \"anabelian question\" has been formulated as\n\nA concrete example is the case of curves, which may be affine as well as projective. Suppose given a hyperbolic curve \"C\", i.e. the complement of \"n\" points in a projective algebraic curve of genus \"g\", taken to be smooth and irreducible, defined over a field \"K\" that is finitely generated (over its prime field), such that\n\nGrothendieck conjectured that the algebraic fundamental group \"G\" of \"C\", a profinite group, determines \"C\" itself (i.e. the isomorphism class of \"G\" determines that of \"C\"). This was proved by Mochizuki. An example is for the case of \"g\" = 0 (the projective line) and \"n\" = 4, when the isomorphism class of \"C\" is determined by the cross-ratio in \"K\" of the four points removed (almost, there being an order to the four points in a cross-ratio, but not in the points removed). There are also results for the case of \"K\" a local field.\n\n"}
{"id": "53960754", "url": "https://en.wikipedia.org/wiki?curid=53960754", "title": "Astrid an Huef", "text": "Astrid an Huef\n\nAstrid an Huef is a German-born New Zealand mathematician who holds a Professorship at Victoria University of Wellington. Until 2017 she held the Chair of Pure Mathematics at the University of Otago. Her research interests include functional analysis, operator algebras, and dynamical systems. She is the president of the New Zealand Mathematical Society for the 2016–2017 term. \n\nAn Huef was born in Karlsruhe and lived in New Zealand for two years as a teenager before moving to Australia in 1985. Because of the disruption to her education caused by these international moves, she was advised not to take higher mathematics in high school, but did so anyway. She began her undergraduate studies in computer science at the University of Newcastle, but ended up doing a double degree, with honours in mathematics. While there, she met Dartmouth College professor Dana Williams, who became her doctoral advisor at Dartmouth beginning in 1994. She completed her doctorate in 1999.\n\nShe took a tenure track position at the University of Denver, and then worked at the University of New South Wales for eight years, before being given the chair at Otago in 2008. She currently coordinates the Women in Mathematics community of the New Zealand Mathematical Society.\n"}
{"id": "2472880", "url": "https://en.wikipedia.org/wiki?curid=2472880", "title": "Biased graph", "text": "Biased graph\n\nIn mathematics, a biased graph is a graph with a list of distinguished circles (edge sets of simple cycles), such that if two circles in the list are contained in a theta graph, then the third circle of the theta graph is also in the list. A biased graph is a generalization of the combinatorial essentials of a gain graph and in particular of a signed graph.\n\nFormally, a biased graph Ω is a pair (\"G\", B) where B is a linear class of circles; this by definition is a class of circles that satisfies the theta-graph property mentioned above.\n\nA subgraph or edge set whose circles are all in B (and which contains no half-edges) is called balanced. For instance, a circle belonging to B is \"balanced\" and one that does not belong to B is \"unbalanced\".\n\nBiased graphs are interesting mostly because of their matroids, but also because of their connection with multiary quasigroups. See below.\n\nA biased graph may have half-edges (one endpoint) and loose edges (no endpoints). The edges with two endpoints are of two kinds: a link has two distinct endpoints, while a loop has two coinciding endpoints.\n\nLinear classes of circles are a special case of linear subclasses of circuits in a matroid.\n\n\nA minor of a biased graph Ω = (\"G\", B) is the result of any sequence of taking subgraphs and contracting edge sets. For biased graphs, as for graphs, it suffices to take a subgraph (which may be the whole graph) and then contract an edge set (which may be the empty set).\n\nA subgraph of Ω consists of a subgraph \"H\" of the underlying graph \"G\", with balanced circle class consisting of those balanced circles that are in \"H\". The deletion of an edge set \"S\", written Ω − \"S\", is the subgraph with all vertices and all edges except those of \"S\".\n\nContraction of Ω is relatively complicated. To contract one edge \"e\", the procedure depends on the kind of edge \"e\" is. If \"e\" is a link, contract it in \"G\". A circle \"C\" in the contraction \"G\"/\"e\" is balanced if either \"C\" or formula_1 is a balanced circle of \"G\". If \"e\" is a balanced loop or a loose edge, it is simply deleted. If it is an unbalanced loop or a half-edge, it and its vertex \"v\" are deleted; each other edge with \"v\" as an endpoint loses that endpoint, so a link with \"v\" as one endpoint becomes a half-edge at its other endpoint, while a loop or half-edge at \"v\" becomes a loose edge.\n\nIn the contraction Ω/\"S\" by an arbitrary edge set \"S\", the edge set is \"E\" − \"S\". (We let \"G\" = (\"V\", \"E\").) The vertex set is the class of vertex sets of balanced components of the subgraph (\"V\", \"S\") of Ω. That is, if (\"V\", \"S\") has balanced components with vertex sets \"V\", ..., \"V\", then Ω/\"S\" has \"k\" vertices \"V\", ..., \"V\" . An edge \"e\" of Ω, not in \"S\", becomes an edge of Ω/\"S\" and each endpoint \"v\" of \"e\" in Ω that belongs to some \"V\" becomes the endpoint \"V\" of \"e\" in Ω/\"S\" ; thus, an endpoint of \"e\" that is not in a balanced component of (\"V\", \"S\") disappears. An edge with all endpoints in unbalanced components of (\"V\", \"S\") becomes a loose edge in the contraction. An edge with only one endpoint in a balanced component of (\"V\", \"S\") becomes a half-edge. An edge with two endpoints that belong to different balanced components becomes a link, and an edge with two endpoints that belong to the same balanced component becomes a loop.\n\nThere are two kinds of matroid associated with a biased graph, both of which generalize the cycle matroid of a graph (Zaslavsky, 1991).\n\nThe frame matroid (sometimes called bias matroid) of a biased graph, \"M\"(Ω), (Zaslavsky, 1989) has for its ground set the edge set \"E\". An edge set is independent if each component contains either no circles or just one circle, which is unbalanced. (In matroid theory a half-edge acts like an unbalanced loop and a loose edge acts like a balanced loop.) \"M\"(Ω) is a frame matroid in the abstract sense, meaning that it is a submatroid of a matroid in which, for at least one basis, the set of lines generated by pairs of basis elements covers the whole matroid. Conversely, every abstract frame matroid is the frame matroid of some biased graph.\n\nThe circuits of the matroid are called frame circuits or bias circuits. There are four kinds. One is a balanced circle. Two other kinds are a pair of unbalanced circles together with a connecting simple path, such that the two circles are either disjoint (then the connecting path has one end in common with each circle and is otherwise disjoint from both) or share just a single common vertex (in this case the connecting path is that single vertex). The fourth kind of circuit is a theta graph in which every circle is unbalanced.\n\nThe rank of an edge set \"S\" is \"n\" − \"b\", where \"n\" is the number of vertices of \"G\" and \"b\" is the number of balanced components of \"S\", counting isolated vertices as balanced components.\n\nMinors of the frame matroid agree with minors of the biased graph; that is, \"M\"(Ω−\"S\") = \"M\"(Ω)−\"S\" and \"M\"(Ω/\"S\") = \"M\"(Ω)/\"S\".\n\nFrame matroids generalize the Dowling geometries associated with a group (Dowling, 1973). The frame matroid of a biased 2\"C\" (see Examples, above) which has no balanced digons is called a swirl. It is important in matroid structure theory.\n\nThe extended lift matroid \"L\"(Ω) has for its ground set the set \"E\", which is the union of \"E\" with an extra point \"e\". The lift matroid \"L\"(Ω) is the extended lift matroid restricted to \"E\". The extra point acts exactly like an unbalanced loop or a half-edge, so we describe only the lift matroid.\n\nAn edge set is independent if it contains either no circles or just one circle, which is unbalanced.\n\nA circuit is a balanced circle, a pair of unbalanced circles that are either disjoint or have just a common vertex, or a theta graph whose circles are all unbalanced.\n\nThe rank of an edge set \"S\" is \"n\" − \"c\" + ε, where \"c\" is the number of components of \"S\", counting isolated vertices, and ε is 0 if \"S\" is balanced and 1 if it is not.\n\nMinors of the lift and extended lift matroids agree in part with minors of the biased graph. Deletions agree: \"L\"(Ω−\"S\") = \"L\"(Ω)−\"S\". Contractions agree only for balanced edge sets: \"M\"(Ω/\"S\") = \"M\"(Ω)/\"S\" if \"S\" is balanced, but not if it is unbalanced. If \"S\" is unbalanced, \"M\"(Ω/\"S\") = \"M\"(\"G\")/\"S\" = \"M\"(\"G\"/\"S\") where \"M\" of a graph denotes the ordinary graphic matroid.\n\nThe lift matroid of a 2\"C\" (see Examples, above) which has no balanced digons is called a spike. Spikes are quite important in matroid structure theory.\n\nJust as a group expansion of a complete graph \"K\" encodes the group (see Dowling geometry), its combinatorial analog expanding a simple cycle of length \"n\" + 1 encodes an \"n\"-ary (multiary) quasigroup. It is possible to prove theorems about multiary quasigroups by means of biased graphs (Zaslavsky, t.a.)\n\n"}
{"id": "32518704", "url": "https://en.wikipedia.org/wiki?curid=32518704", "title": "Braided vector space", "text": "Braided vector space\n\nIn mathematics, a braided vectorspace formula_1 is a vector space together with an additional structure map formula_2 symbolizing interchanging of two vector tensor copies:\n\nsuch that the Yang–Baxter equation is fulfilled. Hence drawing tensor diagrams with formula_2 an overcrossing the corresponding composed morphism is unchanged when a Reidemeister move is applied to the tensor diagram and thus they present a representation of the braid group.\nAs first example, every vector space is braided via the trivial braiding (simply flipping). A superspace has a braiding with negative sign in braiding two odd vectors. More generally, a diagonal braiding means that for a formula_1-base formula_6 we have\n\nA good source for braided vector spaces entire braided monoidal categories with braidings between any objects formula_8, most importantly the modules over quasitriangular Hopf algebras and Yetter–Drinfeld modules over finite groups (such as formula_9 above)\n\nIf formula_10 additionally possesses an algebra structure inside the braided category (\"braided algebra\") one has a braided commutator (e.g. for a superspace the anticommutator): \n\nExamples of such braided algebras (and even Hopf algebras) are the Nichols algebras, that are by definition generated by a given braided vectorspace. They appear as quantum Borel part of quantum groups and often (e.g. when finite or over an abelian group) possess an arithmetic root system, multiple Dynkin diagrams and a PBW-basis made up of braided commutators just like the ones in semisimple Lie algebras.\n"}
{"id": "2620525", "url": "https://en.wikipedia.org/wiki?curid=2620525", "title": "Brocard's conjecture", "text": "Brocard's conjecture\n\nIn number theory, Brocard's conjecture is a conjecture that there are at least four prime numbers between (\"p\") and (\"p\"), for \"n\" formula_1 2, where \"p\" is the \"n\" prime number. It is widely believed that this conjecture is true. However, it remains unproven as of 2017.\n\nThe number of primes between prime squares is 2, 5, 6, 15, 9, 22, 11, 27, ... .\n\nLegendre's conjecture that there is a prime between consecutive integer squares directly implies that there are at least two primes between prime squares for \"p\" ≥ 3 since \"p\" - \"p\" ≥ 2.\n\n"}
{"id": "5176", "url": "https://en.wikipedia.org/wiki?curid=5176", "title": "Calculus", "text": "Calculus\n\nCalculus (from Latin \"calculus\", literally 'small pebble', used for counting and calculations, as on an abacus) is the mathematical study of continuous change, in the same way that geometry is the study of shape and algebra is the study of generalizations of arithmetic operations.\n\nIt has two major branches, differential calculus (concerning instantaneous rates of change and slopes of curves), and integral calculus (concerning accumulation of quantities and the areas under and between curves). These two branches are related to each other by the fundamental theorem of calculus. Both branches make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit.\n\nGenerally, modern calculus is considered to have been developed in the 17th century by Isaac Newton and Gottfried Wilhelm Leibniz. Today, calculus has widespread uses in science, engineering, and economics.\n\nCalculus is a part of modern mathematics education. A course in calculus is a gateway to other, more advanced courses in mathematics devoted to the study of functions and limits, broadly called mathematical analysis. Calculus has historically been called \"the calculus of infinitesimals\", or \"infinitesimal calculus\". The term \"calculus\" (plural \"calculi\") is also used for naming specific methods of calculation or notation as well as some theories, such as propositional calculus, Ricci calculus, calculus of variations, lambda calculus, and process calculus.\n\nModern calculus was developed in 17th-century Europe by Isaac Newton and Gottfried Wilhelm Leibniz (independently of each other, first publishing around the same time) but elements of it appeared in ancient Greece, then in China and the Middle East, and still later again in medieval Europe and in India.\n\nThe ancient period introduced some of the ideas that led to integral calculus, but does not seem to have developed these ideas in a rigorous and systematic way. Calculations of volume and area, one goal of integral calculus, can be found in the Egyptian Moscow papyrus (13th dynasty,  BC), but the formulas are simple instructions, with no indication as to method, and some of them lack major components.\n\nFrom the age of Greek mathematics, Eudoxus ( BC) used the method of exhaustion, which foreshadows the concept of the limit, to calculate areas and volumes, while Archimedes ( BC) developed this idea further, inventing heuristics which resemble the methods of integral calculus.\n\nThe method of exhaustion was later discovered independently in China by Liu Hui in the 3rd century AD in order to find the area of a circle. In the 5th century AD, Zu Gengzhi, son of Zu Chongzhi, established a method that would later be called Cavalieri's principle to find the volume of a sphere.\n\nIn the Middle East, Hasan Ibn al-Haytham, Latinized as Alhazen ( ) derived a formula for the sum of fourth powers. He used the results to carry out what would now be called an integration of this function, where the formulae for the sums of integral squares and fourth powers allowed him to calculate the volume of a paraboloid.\n\nIn the 14th century, Indian mathematicians gave a non-rigorous method, resembling differentiation, applicable to some trigonometric functions. Madhava of Sangamagrama and the Kerala School of Astronomy and Mathematics thereby stated components of calculus. A complete theory encompassing these components is now well known in the Western world as the \"Taylor series\" or \"infinite series approximations\". However, they were not able to \"combine many differing ideas under the two unifying themes of the derivative and the integral, show the connection between the two, and turn calculus into the great problem-solving tool we have today\".\n\nIn Europe, the foundational work was a treatise written by Bonaventura Cavalieri, who argued that volumes and areas should be computed as the sums of the volumes and areas of infinitesimally thin cross-sections. The ideas were similar to Archimedes' in \"The Method\", but this treatise is believed to have been lost in the 13th century, and was only rediscovered in the early 20th century, and so would have been unknown to Cavalieri. Cavalieri's work was not well respected since his methods could lead to erroneous results, and the infinitesimal quantities he introduced were disreputable at first.\n\nThe formal study of calculus brought together Cavalieri's infinitesimals with the calculus of finite differences developed in Europe at around the same time. Pierre de Fermat, claiming that he borrowed from Diophantus, introduced the concept of adequality, which represented equality up to an infinitesimal error term. The combination was achieved by John Wallis, Isaac Barrow, and James Gregory, the latter two proving the second fundamental theorem of calculus around 1670.\nThe product rule and chain rule, the notions of higher derivatives and Taylor series, and of analytic functions were introduced by Isaac Newton in an idiosyncratic notation which he used to solve problems of mathematical physics. In his works, Newton rephrased his ideas to suit the mathematical idiom of the time, replacing calculations with infinitesimals by equivalent geometrical arguments which were considered beyond reproach. He used the methods of calculus to solve the problem of planetary motion, the shape of the surface of a rotating fluid, the oblateness of the earth, the motion of a weight sliding on a cycloid, and many other problems discussed in his \"Principia Mathematica\" (1687). In other work, he developed series expansions for functions, including fractional and irrational powers, and it was clear that he understood the principles of the Taylor series. He did not publish all these discoveries, and at this time infinitesimal methods were still considered disreputable.\n\nThese ideas were arranged into a true calculus of infinitesimals by Gottfried Wilhelm Leibniz, who was originally accused of plagiarism by Newton. He is now regarded as an independent inventor of and contributor to calculus. His contribution was to provide a clear set of rules for working with infinitesimal quantities, allowing the computation of second and higher derivatives, and providing the product rule and chain rule, in their differential and integral forms. Unlike Newton, Leibniz paid a lot of attention to the formalism, often spending days determining appropriate symbols for concepts.\n\nToday, Leibniz and Newton are usually both given credit for independently inventing and developing calculus. Newton was the first to apply calculus to general physics and Leibniz developed much of the notation used in calculus today. The basic insights that both Newton and Leibniz provided were the laws of differentiation and integration, second and higher derivatives, and the notion of an approximating polynomial series. By Newton's time, the fundamental theorem of calculus was known.\n\nWhen Newton and Leibniz first published their results, there was great controversy over which mathematician (and therefore which country) deserved credit. Newton derived his results first (later to be published in his \"Method of Fluxions\"), but Leibniz published his \"Nova Methodus pro Maximis et Minimis\" first. Newton claimed Leibniz stole ideas from his unpublished notes, which Newton had shared with a few members of the Royal Society. This controversy divided English-speaking mathematicians from continental European mathematicians for many years, to the detriment of English mathematics. A careful examination of the papers of Leibniz and Newton shows that they arrived at their results independently, with Leibniz starting first with integration and Newton with differentiation. It is Leibniz, however, who gave the new discipline its name. Newton called his calculus \"the science of fluxions\".\n\nSince the time of Leibniz and Newton, many mathematicians have contributed to the continuing development of calculus. One of the first and most complete works on both infinitesimal and integral calculus was written in 1748 by Maria Gaetana Agnesi.\nIn calculus, \"foundations\" refers to the rigorous development of the subject from axioms and definitions. In early calculus the use of infinitesimal quantities was thought unrigorous, and was fiercely criticized by a number of authors, most notably Michel Rolle and Bishop Berkeley. Berkeley famously described infinitesimals as the ghosts of departed quantities in his book \"The Analyst\" in 1734. Working out a rigorous foundation for calculus occupied mathematicians for much of the century following Newton and Leibniz, and is still to some extent an active area of research today.\n\nSeveral mathematicians, including Maclaurin, tried to prove the soundness of using infinitesimals, but it would not be until 150 years later when, due to the work of Cauchy and Weierstrass, a way was finally found to avoid mere \"notions\" of infinitely small quantities. The foundations of differential and integral calculus had been laid. In Cauchy's \"Cours d'Analyse\", we find a broad range of foundational approaches, including a definition of continuity in terms of infinitesimals, and a (somewhat imprecise) prototype of an (ε, δ)-definition of limit in the definition of differentiation. In his work Weierstrass formalized the concept of limit and eliminated infinitesimals (although his definition can actually validate nilsquare infinitesimals). Following the work of Weierstrass, it eventually became common to base calculus on limits instead of infinitesimal quantities, though the subject is still occasionally called \"infinitesimal calculus\". Bernhard Riemann used these ideas to give a precise definition of the integral. It was also during this period that the ideas of calculus were generalized to Euclidean space and the complex plane.\n\nIn modern mathematics, the foundations of calculus are included in the field of real analysis, which contains full definitions and proofs of the theorems of calculus. The reach of calculus has also been greatly extended. Henri Lebesgue invented measure theory and used it to define integrals of all but the most pathological functions. Laurent Schwartz introduced distributions, which can be used to take the derivative of any function whatsoever.\n\nLimits are not the only rigorous approach to the foundation of calculus. Another way is to use Abraham Robinson's non-standard analysis. Robinson's approach, developed in the 1960s, uses technical machinery from mathematical logic to augment the real number system with infinitesimal and infinite numbers, as in the original Newton-Leibniz conception. The resulting numbers are called hyperreal numbers, and they can be used to give a Leibniz-like development of the usual rules of calculus. There is also smooth infinitesimal analysis, which differs from non-standard analysis in that it mandates neglecting higher power infinitesimals during derivations.\n\nWhile many of the ideas of calculus had been developed earlier in Greece, China, India, Iraq, Persia, and Japan, the use of calculus began in Europe, during the 17th century, when Isaac Newton and Gottfried Wilhelm Leibniz built on the work of earlier mathematicians to introduce its basic principles. The development of calculus was built on earlier concepts of instantaneous motion and area underneath curves.\n\nApplications of differential calculus include computations involving velocity and acceleration, the slope of a curve, and optimization. Applications of integral calculus include computations involving area, volume, arc length, center of mass, work, and pressure. More advanced applications include power series and Fourier series.\n\nCalculus is also used to gain a more precise understanding of the nature of space, time, and motion. For centuries, mathematicians and philosophers wrestled with paradoxes involving division by zero or sums of infinitely many numbers. These questions arise in the study of motion and area. The ancient Greek philosopher Zeno of Elea gave several famous examples of such paradoxes. Calculus provides tools, especially the limit and the infinite series, that resolve the paradoxes.\n\nCalculus is usually developed by working with very small quantities. Historically, the first method of doing so was by infinitesimals. These are objects which can be treated like real numbers but which are, in some sense, \"infinitely small\". For example, an infinitesimal number could be greater than 0, but less than any number in the sequence 1, 1/2, 1/3, ... and thus less than any positive real number. From this point of view, calculus is a collection of techniques for manipulating infinitesimals. The symbols formula_1 and formula_2 were taken to be infinitesimal, and the derivative formula_3 was simply their ratio.\n\nThe infinitesimal approach fell out of favor in the 19th century because it was difficult to make the notion of an infinitesimal precise. However, the concept was revived in the 20th century with the introduction of non-standard analysis and smooth infinitesimal analysis, which provided solid foundations for the manipulation of infinitesimals.\n\nIn the late 19th century, infinitesimals were replaced within academia by the epsilon, delta approach to limits. Limits describe the value of a function at a certain input in terms of its values at nearby inputs. They capture small-scale behavior in the context of the real number system. In this treatment, calculus is a collection of techniques for manipulating certain limits. Infinitesimals get replaced by very small numbers, and the infinitely small behavior of the function is found by taking the limiting behavior for smaller and smaller numbers. Limits were thought to provide a more rigorous foundation for calculus, and for this reason they became the standard approach during the twentieth century.\n\nDifferential calculus is the study of the definition, properties, and applications of the derivative of a function. The process of finding the derivative is called \"differentiation\". Given a function and a point in the domain, the derivative at that point is a way of encoding the small-scale behavior of the function near that point. By finding the derivative of a function at every point in its domain, it is possible to produce a new function, called the \"derivative function\" or just the \"derivative\" of the original function. In formal terms, the derivative is a linear operator which takes a function as its input and produces a second function as its output. This is more abstract than many of the processes studied in elementary algebra, where functions usually input a number and output another number. For example, if the doubling function is given the input three, then it outputs six, and if the squaring function is given the input three, then it outputs nine. The derivative, however, can take the squaring function as an input. This means that the derivative takes all the information of the squaring function—such as that two is sent to four, three is sent to nine, four is sent to sixteen, and so on—and uses this information to produce another function. The function produced by deriving the squaring function turns out to be the doubling function.\n\nIn more explicit terms the \"doubling function\" may be denoted by and the \"squaring function\" by . The \"derivative\" now takes the function , defined by the expression \"\", as an input, that is all the information—such as that two is sent to four, three is sent to nine, four is sent to sixteen, and so on—and uses this information to output another function, the function , as will turn out.\n\nThe most common symbol for a derivative is an apostrophe-like mark called prime. Thus, the derivative of a function called is denoted by , pronounced \"f prime\". For instance, if is the squaring function, then is its derivative (the doubling function from above). This notation is known as Lagrange's notation.\n\nIf the input of the function represents time, then the derivative represents change with respect to time. For example, if is a function that takes a time as input and gives the position of a ball at that time as output, then the derivative of is how the position is changing in time, that is, it is the velocity of the ball.\n\nIf a function is linear (that is, if the graph of the function is a straight line), then the function can be written as , where is the independent variable, is the dependent variable, is the \"y\"-intercept, and:\n\nThis gives an exact value for the slope of a straight line. If the graph of the function is not a straight line, however, then the change in divided by the change in varies. Derivatives give an exact meaning to the notion of change in output with respect to change in input. To be concrete, let be a function, and fix a point in the domain of . is a point on the graph of the function. If is a number close to zero, then is a number close to . Therefore, is close to . The slope between these two points is\n\nThis expression is called a \"difference quotient\". A line through two points on a curve is called a \"secant line\", so is the slope of the secant line between and . The secant line is only an approximation to the behavior of the function at the point because it does not account for what happens between and . It is not possible to discover the behavior at by setting to zero because this would require dividing by zero, which is undefined. The derivative is defined by taking the limit as tends to zero, meaning that it considers the behavior of for all small values of and extracts a consistent value for the case when equals zero:\n\nGeometrically, the derivative is the slope of the tangent line to the graph of at . The tangent line is a limit of secant lines just as the derivative is a limit of difference quotients. For this reason, the derivative is sometimes called the slope of the function .\n\nHere is a particular example, the derivative of the squaring function at the input 3. Let be the squaring function.\n\nThe slope of the tangent line to the squaring function at the point (3, 9) is 6, that is to say, it is going up six times as fast as it is going to the right. The limit process just described can be performed for any point in the domain of the squaring function. This defines the \"derivative function\" of the squaring function, or just the \"derivative\" of the squaring function for short. A computation similar to the one above shows that the derivative of the squaring function is the doubling function.\n\nA common notation, introduced by Leibniz, for the derivative in the example above is\nIn an approach based on limits, the symbol is to be interpreted not as the quotient of two numbers but as a shorthand for the limit computed above. Leibniz, however, did intend it to represent the quotient of two infinitesimally small numbers, being the infinitesimally small change in caused by an infinitesimally small change applied to . We can also think of as a differentiation operator, which takes a function as an input and gives another function, the derivative, as the output. For example:\n\nIn this usage, the in the denominator is read as \"with respect to \". Another example of correct notation could be:\n\nformula_10\n\nEven when calculus is developed using limits rather than infinitesimals, it is common to manipulate symbols like and as if they were real numbers; although it is possible to avoid such manipulations, they are sometimes notationally convenient in expressing operations such as the total derivative.\n\n\"Integral calculus\" is the study of the definitions, properties, and applications of two related concepts, the \"indefinite integral\" and the \"definite integral\". The process of finding the value of an integral is called \"integration\". In technical language, integral calculus studies two related linear operators.\n\nThe \"indefinite integral\", also known as the \"antiderivative\", is the inverse operation to the derivative. is an indefinite integral of when is a derivative of . (This use of lower- and upper-case letters for a function and its indefinite integral is common in calculus.)\n\nThe \"definite integral\" inputs a function and outputs a number, which gives the algebraic sum of areas between the graph of the input and the x-axis. The technical definition of the definite integral involves the limit of a sum of areas of rectangles, called a Riemann sum.\n\nA motivating example is the distances traveled in a given time.\n\nIf the speed is constant, only multiplication is needed, but if the speed changes, a more powerful method of finding the distance is necessary. One such method is to approximate the distance traveled by breaking up the time into many short intervals of time, then multiplying the time elapsed in each interval by one of the speeds in that interval, and then taking the sum (a Riemann sum) of the approximate distance traveled in each interval. The basic idea is that if only a short time elapses, then the speed will stay more or less the same. However, a Riemann sum only gives an approximation of the distance traveled. We must take the limit of all such Riemann sums to find the exact distance traveled.\n\nWhen velocity is constant, the total distance traveled over the given time interval can be computed by multiplying velocity and time. For example, travelling a steady 50 mph for 3 hours results in a total distance of 150 miles. In the diagram on the left, when constant velocity and time are graphed, these two values form a rectangle with height equal to the velocity and width equal to the time elapsed. Therefore, the product of velocity and time also calculates the rectangular area under the (constant) velocity curve. This connection between the area under a curve and distance traveled can be extended to \"any\" irregularly shaped region exhibiting a fluctuating velocity over a given time period. If in the diagram on the right represents speed as it varies over time, the distance traveled (between the times represented by and ) is the area of the shaded region .\n\nTo approximate that area, an intuitive method would be to divide up the distance between and into a number of equal segments, the length of each segment represented by the symbol . For each small segment, we can choose one value of the function . Call that value . Then the area of the rectangle with base and height gives the distance (time multiplied by speed ) traveled in that segment. Associated with each segment is the average value of the function above it, . The sum of all such rectangles gives an approximation of the area between the axis and the curve, which is an approximation of the total distance traveled. A smaller value for will give more rectangles and in most cases a better approximation, but for an exact answer we need to take a limit as approaches zero.\n\nThe symbol of integration is formula_12, an elongated \"S\" (the \"S\" stands for \"sum\"). The definite integral is written as:\n\nand is read \"the integral from \"a\" to \"b\" of \"f\"-of-\"x\" with respect to \"x\".\" The Leibniz notation is intended to suggest dividing the area under the curve into an infinite number of rectangles, so that their width becomes the infinitesimally small . In a formulation of the calculus based on limits, the notation\n\nis to be understood as an operator that takes a function as an input and gives a number, the area, as an output. The terminating differential, , is not a number, and is not being multiplied by , although, serving as a reminder of the limit definition, it can be treated as such in symbolic manipulations of the integral. Formally, the differential indicates the variable over which the function is integrated and serves as a closing bracket for the integration operator.\n\nThe indefinite integral, or antiderivative, is written:\n\nFunctions differing by only a constant have the same derivative, and it can be shown that the antiderivative of a given function is actually a family of functions differing only by a constant. Since the derivative of the function , where is any constant, is , the antiderivative of the latter given by:\nThe unspecified constant present in the indefinite integral or antiderivative is known as the constant of integration.\n\nThe fundamental theorem of calculus states that differentiation and integration are inverse operations. More precisely, it relates the values of antiderivatives to definite integrals. Because it is usually easier to compute an antiderivative than to apply the definition of a definite integral, the fundamental theorem of calculus provides a practical way of computing definite integrals. It can also be interpreted as a precise statement of the fact that differentiation is the inverse of integration.\n\nThe fundamental theorem of calculus states: If a function is continuous on the interval and if is a function whose derivative is on the interval , then\n\nFurthermore, for every in the interval ,\n\nThis realization, made by both Newton and Leibniz, who based their results on earlier work by Isaac Barrow, was key to the proliferation of analytic results after their work became known. The fundamental theorem provides an algebraic method of computing many definite integrals—without performing limit processes—by finding formulas for antiderivatives. It is also a prototype solution of a differential equation. Differential equations relate an unknown function to its derivatives, and are ubiquitous in the sciences.\n\nCalculus is used in every branch of the physical sciences, actuarial science, computer science, statistics, engineering, economics, business, medicine, demography, and in other fields wherever a problem can be mathematically modeled and an optimal solution is desired. It allows one to go from (non-constant) rates of change to the total change or vice versa, and many times in studying a problem we know one and are trying to find the other.\n\nPhysics makes particular use of calculus; all concepts in classical mechanics and electromagnetism are related through calculus. The mass of an object of known density, the moment of inertia of objects, as well as the total energy of an object within a conservative field can be found by the use of calculus. An example of the use of calculus in mechanics is Newton's second law of motion: historically stated it expressly uses the term \"change of motion\" which implies the derivative saying \"The\" change \"of momentum of a body is equal to the resultant force acting on the body and is in the same direction.\" Commonly expressed today as Force = Mass × acceleration, it implies differential calculus because acceleration is the time derivative of velocity or second time derivative of trajectory or spatial position. Starting from knowing how an object is accelerating, we use calculus to derive its path.\n\nMaxwell's theory of electromagnetism and Einstein's theory of general relativity are also expressed in the language of differential calculus. Chemistry also uses calculus in determining reaction rates and radioactive decay. In biology, population dynamics starts with reproduction and death rates to model population changes.\n\nCalculus can be used in conjunction with other mathematical disciplines. For example, it can be used with linear algebra to find the \"best fit\" linear approximation for a set of points in a domain. Or it can be used in probability theory to determine the probability of a continuous random variable from an assumed density function. In analytic geometry, the study of graphs of functions, calculus is used to find high points and low points (maxima and minima), slope, concavity and inflection points.\n\nGreen's Theorem, which gives the relationship between a line integral around a simple closed curve C and a double integral over the plane region D bounded by C, is applied in an instrument known as a planimeter, which is used to calculate the area of a flat surface on a drawing. For example, it can be used to calculate the amount of area taken up by an irregularly shaped flower bed or swimming pool when designing the layout of a piece of property.\n\nDiscrete Green's Theorem, which gives the relationship between a double integral of a function around a simple closed rectangular curve \"C\" and a linear combination of the antiderivative's values at corner points along the edge of the curve, allows fast calculation of sums of values in rectangular domains. For example, it can be used to efficiently calculate sums of rectangular domains in images, in order to rapidly extract features and detect object; another algorithm that could be used is the summed area table.\n\nIn the realm of medicine, calculus can be used to find the optimal branching angle of a blood vessel so as to maximize flow. From the decay laws for a particular drug's elimination from the body, it is used to derive dosing laws. In nuclear medicine, it is used to build models of radiation transport in targeted tumor therapies.\n\nIn economics, calculus allows for the determination of maximal profit by providing a way to easily calculate both marginal cost and marginal revenue.\n\nCalculus is also used to find approximate solutions to equations; in practice it is the standard way to solve differential equations and do root finding in most applications. Examples are methods such as Newton's method, fixed point iteration, and linear approximation. For instance, spacecraft use a variation of the Euler method to approximate curved courses within zero gravity environments.\n\nOver the years, many reformulations of calculus have been investigated for different purposes.\n\nImprecise calculations with infinitesimals were widely replaced with the rigorous (ε, δ)-definition of limit starting in the 1870s. Meanwhile, calculations with infinitesimals persisted and often led to correct results. This led Abraham Robinson to investigate if it were possible to develop a number system with infinitesimal quantities over which the theorems of calculus were still valid. In 1960, building upon the work of Edwin Hewitt and Jerzy Łoś, he succeeded in developing non-standard analysis. The theory of non-standard analysis is rich enough to be applied in many branches of mathematics. As such, books and articles dedicated solely to the traditional theorems of calculus often go by the title non-standard calculus.\n\nThis is another reformulation of the calculus in terms of infinitesimals. Based on the ideas of F. W. Lawvere and employing the methods of category theory, it views all functions as being continuous and incapable of being expressed in terms of discrete entities. One aspect of this formulation is that the law of excluded middle does not hold in this formulation.\n\nConstructive mathematics is a branch of mathematics that insists that proofs of the existence of a number, function, or other mathematical object should give a construction of the object. As such constructive mathematics also rejects the law of excluded middle. Reformulations of calculus in a constructive framework are generally part of the subject of constructive analysis.\n\n\n\n"}
{"id": "2079538", "url": "https://en.wikipedia.org/wiki?curid=2079538", "title": "Cauchy index", "text": "Cauchy index\n\nIn mathematical analysis, the Cauchy index is an integer associated to a real rational function over an interval. By the Routh–Hurwitz theorem, we have the following interpretation: the Cauchy index of\n\nover the real line is the difference between the number of roots of \"f\"(\"z\") located in the right half-plane and those located in the left half-plane. The complex polynomial \"f\"(\"z\") is such that\n\nWe must also assume that \"p\" has degree less than the degree of \"q\".\n\n\n\nWe recognize in \"p\"(\"x\") and \"q\"(\"x\") respectively the Chebyshev polynomials of degree 3 and 5. Therefore, \"r\"(\"x\") has poles formula_6, formula_7, formula_8, formula_9 and formula_10, i.e. formula_11 for formula_12. We can see on the picture that formula_13 and formula_14. For the pole in zero, we have formula_15 since the left and right limits are equal (which is because \"p\"(\"x\") also has a root in zero). \nWe conclude that formula_16 since \"q\"(\"x\") has only five roots, all in [−1,1]. We cannot use here the Routh–Hurwitz theorem as each complex polynomial with \"f\"(\"iy\") = \"q\"(\"y\") + \"ip\"(\"y\") has a zero on the imaginary line (namely at the origin).\n\n"}
{"id": "37103476", "url": "https://en.wikipedia.org/wiki?curid=37103476", "title": "Causal inference", "text": "Causal inference\n\nCausal inference is the process of drawing a conclusion about a causal connection based on the conditions of the occurrence of an effect. The main difference between causal inference and inference of association is that the former analyzes the response of the effect variable when the cause is changed. The science of why things occur is called etiology. Causal inference is an example of causal reasoning.\n\nInferring the cause of something has been described as:\n\nEpidemiological studies employ different epidemiological methods of collecting and measuring evidence of risk factors and effect and different ways of measuring association between the two. A hypothesis is formulated, and then tested with statistical methods (see Statistical hypothesis testing). It is statistical inference that helps decide if data are due to chance, also called random variation, or indeed correlated and if so how strongly. However, correlation does not imply causation, so further methods must be used to infer causation.\n\nCommon frameworks for causal inference are structural equation modeling and the Rubin causal model.\n\nEpidemiology studies patterns of health and disease in defined populations of living beings in order to infer causes and effects. An association between an exposure to a putative risk factor and a disease may be suggestive of, but is not equivalent to causality because correlation does not imply causation. Historically, Koch's postulates have been used since the 19th century to decide if a microorganism was the cause of a disease. In the 20th century the Bradford Hill criteria, described in 1965 have been used to assess causality of variables outside microbiology, although even these criteria are not exclusive ways to determine causality.\n\nIn molecular epidemiology the phenomena studied are on a molecular biology level, including genetics, where biomarkers are evidence of cause or effects.\n\nA recent trend is to identify evidence for influence of the exposure on molecular pathology within diseased tissue or cells, in the emerging interdisciplinary field of molecular pathological epidemiology (MPE). Linking the exposure to molecular pathologic signatures of the disease can help to assess causality. Considering the inherent nature of heterogeneity of a given disease, the unique disease principle, disease phenotyping and subtyping are trends in biomedical and public health sciences, exemplified as personalized medicine and precision medicine.\n\nDetermination of cause and effect from joint observational data for two time-independent variables, say X and Y, has been tackled using asymmetry between evidence for some model in the directions, X → Y and Y → X. One idea is to incorporate an independent noise term in the model to compare the evidences of the two directions.\n\nHere are some of the noise models for the hypothesis Y → X with the noise E:\n\nThe common assumption in these models are:\n\nOn an intuitive level, the idea is that the factorization of the joint distribution P(Cause,Effect) into P(Cause)*P(Effect | Cause) typically yields models of lower total complexity than the factorization into P(Effect)*P(Cause | Effect). Although the notion of “complexity” is intuitively appealing, it is not obvious how it should be precisely defined. A different family of methods attempt to discover causal \"footprints\" from large amounts of labeled data, and allow the prediction of more flexible causal relations.\n\nIn statistics and economics, causality is often tested for using regression. Several methods can be used to distinguish actual causality from spurious indications of causality. First, the explanatory variable could be one that conceptually could not be caused by the dependent variable, thereby avoiding the possibility of being misled by reverse causation: for example, if the independent variable is rainfall and the dependent variable is the futures price of some agricultural commodity. Second, the instrumental variables technique may be employed to remove any reverse causation by introducing a role for other variables (instruments) that are known to be unaffected by the dependent variable. Third, the principle that effects cannot precede causes can be invoked, by including on the right side of the regression only variables that precede in time the dependent variable. Fourth, other regressors are included to ensure that confounding variables are not causing a regressor to spuriously appear to be significant. Correlation by coincidence, as opposed to correlation reflecting actual causation, can be ruled out by using large samples and by performing cross validation to check that correlations are maintained on data that were not used in the regression.\n\nGraduate courses on causal inference have been introduced to the curriculum of many schools.\n\n\n\n"}
{"id": "2668560", "url": "https://en.wikipedia.org/wiki?curid=2668560", "title": "Community Z Tools", "text": "Community Z Tools\n\nThe Community Z Tools (CZT) initiative is based around a SourceForge project to build a set of tools for the Z notation, a formal method useful in software engineering. Tools include support for editing, typechecking and animating Z specifications. There is some support for extensions such as Object-Z and TCOZ. The tools are built using the Java programming language.\n\nCZT was proposed by Andrew Martin of Oxford University in 2001.\n\n"}
{"id": "50834770", "url": "https://en.wikipedia.org/wiki?curid=50834770", "title": "Crossing number inequality", "text": "Crossing number inequality\n\nIn the mathematics of graph drawing, the crossing number inequality or crossing lemma gives a lower bound on the minimum number of crossings of a given graph, as a function of the number of edges and vertices of the graph. It states that, for graphs where the number of edges is sufficiently larger than the number of vertices, the crossing number is at least proportional to .\n\nIt has applications in VLSI design and combinatorial geometry,\nand was discovered independently by Ajtai, Chvátal, Newborn, and Szemerédi\nand by Leighton.\n\nThe crossing number inequality states that, for an undirected simple graph with vertices and edges such that , the crossing number obeys the inequality \n\nThe constant is the best known to date, and is due to Ackerman.\nFor earlier results with weaker constants see and .\nThe constant can be lowered to , but at the expense of replacing with the worse constant of .\n\nThe motivation of Leighton in studying crossing numbers was for applications to VLSI design in theoretical computer science.\n\nLater, realized that this inequality yielded very simple proofs of some important theorems in incidence geometry. For instance, the Szemerédi–Trotter theorem, an upper bound on the number of incidences that are possible between given numbers of points and lines in the plane,\nfollows by constructing a graph whose vertices are the points and whose edges are the segments of lines between incident points. If there were more incidences than the Szemerédi–Trotter bound, this graph would necessarily have more crossings than the total number of pairs of lines, an impossibility.\nThe inequality can also be used to prove Beck's theorem, that if a finite point set does not have a linear number of collinear points, then it determines a quadratic number of distinct lines.\nSimilarly, Tamal Dey used it to prove upper bounds on geometric \"k\"-sets.\n\nWe first give a preliminary estimate: for any graph with vertices and edges, we have\n\nTo prove this, consider a diagram of which has exactly crossings. Each of these crossings can be removed by removing an edge from . Thus we can find a graph with at least edges and vertices with no crossings, and is thus a planar graph. But from Euler's formula we must then have , and the claim follows. (In fact we have for ).\n\nTo obtain the actual crossing number inequality, we now use a probabilistic argument. We let be a probability parameter to be chosen later, and construct a random subgraph of by allowing each vertex of to lie in independently with probability , and allowing an edge of to lie in if and only if its two vertices were chosen to lie in . Let and denote the number of edges, vertices and crossings of , respectively. Since is a subgraph of , this diagram contains a diagram of . By the preliminary crossing number inequality, we have\n\nTaking expectations we obtain\n\nSince each of the vertices in had a probability of being in , we have . Similarly, each of the edges in has a probability of remaining in since both endpoints need to stay in , therefore . Finally, every crossing in the diagram of has a probability of remaining in , since every crossing involves four vertices. To see this consider a diagram of with crossings. We may assume that any two edges in this diagram with a common vertex are disjoint, otherwise we could interchange the intersecting parts of the two edges and reduce the crossing number by one. Thus every crossing in this diagram involves four distinct vertices of . Therefore, and we have\n\nNow if we set (since we assumed that ), we obtain after some algebra\n\nA slight refinement of this argument allows one to replace by for .\n\nFor graphs with girth larger than and , demonstrated an improvement of this inequality to\n"}
{"id": "34818925", "url": "https://en.wikipedia.org/wiki?curid=34818925", "title": "Curtis Greene", "text": "Curtis Greene\n\nCurtis Greene is an American mathematician, specializing in algebraic combinatorics. He is the J. McLain King Professor of Mathematics at Haverford College in Pennsylvania.\n\nGreene did his undergraduate studies at Harvard University, and earned his Ph.D. in 1969 from the California Institute of Technology under the supervision of Robert P. Dilworth. He held positions at the Massachusetts Institute of Technology and the University of Pennsylvania before moving to Haverford.\n\nGreene has written highly cited research papers on Sperner families, Young tableaux, and combinatorial equivalences between hyperplane arrangements, zonotopes, and graph orientations. With Daniel Kleitman, he has also written a highly cited survey paper on combinatorial proof techniques.\n\nIn 2012 he became a fellow of the American Mathematical Society.\n"}
{"id": "35749872", "url": "https://en.wikipedia.org/wiki?curid=35749872", "title": "Cyborg Foundation", "text": "Cyborg Foundation\n\nThe Cyborg Foundation is a nonprofit organization created in 2010 by cyborg activists and artists Moon Ribas and Neil Harbisson. The foundation is a platform for the research, creation and promotion of projects related to extending and creating new senses and perceptions by applying technology to the human body. The Cyborg Foundation was first housed in Tecnocampus Scientific Park (Barcelona) and is currently based in New York City. It collaborates with several institutions, universities and research centers around the world.\n\nTheir mission is to assist humans in becoming cyborgs, promote the use of cybernetics as part of the human body and defend cyborg rights. \nThey have donated cyborg antennas to blind communities and has taught colour to blind children to help them develop the sense of colour.The foundation believes that some cybernetic extensions should be treated as body parts, not as devices.\n\nThe foundation was created as a response to the growing number of letters and emails that Neil Harbisson received from people around the world interested in becoming a cyborg. Since its creation the foundation has kick-started several new-sense development projects and has donated cyborg antennas to blind communities in Europe, Asia and America. The first blind person to try out an eyeborg was Sabriye Tenberken followed by blind students from Braille Without Borders in Tibet and members of the Sociedad de Ciegos de Pichincha in Ecuador. \nIn 2010, the foundation was the overall winner of the Cre@tic Awards, organized by Tecnocampus Mataró.\nIn 2012, Spanish film director Rafel Duran Torrent, created a short film about the Cyborg Foundation. In 2013, the film won the Grand Jury Prize at the Sundance Film Festival's Focus Forward Filmmakers Competition.\n\n\n\n\nIn 2014, the Cyborg Foundation participated in the European Union commission for Robotic Laws.\n\nIn 2016 together with electronic civil rights and civil liberties researcher and activist Rich MacKinnon, a list of Cyborg Civil Rights where exposed and proposed at South by Southwest. The rights exposed the redefinition and defence of cyborg civil liberties and the sanctity of cyborg bodies. And foresaw a battle for the ownership, licensing, and control of augmented, alternative, and synthetic anatomies; the communication, data and telemetry produced by them; and the very definition of what it means to be human.\n\n"}
{"id": "24499402", "url": "https://en.wikipedia.org/wiki?curid=24499402", "title": "Direct method in the calculus of variations", "text": "Direct method in the calculus of variations\n\nIn the calculus of variations, a topic in mathematics, the direct method is a general method for constructing a proof of the existence of a minimizer for a given functional, introduced by Zaremba and David Hilbert around 1900. The method relies on methods of functional analysis and topology. As well as being used to prove the existence of a solution, direct methods may be used to compute the solution to desired accuracy.\n\nThe calculus of variations deals with functionals formula_1, where formula_2 is some function space and formula_3. The main interest of the subject is to find \"minimizers\" for such functionals, that is, functions formula_4 such that:formula_5\n\nThe standard tool for obtaining necessary conditions for a function to be a minimizer is the Euler–Lagrange equation. But seeking a minimizer amongst functions satisfying these may lead to false conclusions if the existence of a minimizer is not established beforehand.\n\nThe functional formula_6 must be bounded from below to have a minimizer. This means\n\nThis condition is not enough to know that a minimizer exists, but it shows the existence of a \"minimizing sequence\", that is, a sequence formula_8 in formula_2 such that formula_10\n\nThe direct method may broken into the following steps\n\nTo see that this shows the existence of a minimizer, consider the following characterization of sequentially lower-semicontinuous functions.\n\nThe conclusions follows from\nin other words\n\nThe direct method may often be applied with success when the space formula_2 is a subset of a separable reflexive Banach space formula_27. In this case the sequential Banach–Alaoglu theorem implies that any bounded sequence formula_8 in formula_2 has a subsequence that converges to some formula_30 in formula_27 with respect to the weak topology. If formula_2 is sequentially closed in formula_27, so that formula_30 is in formula_2, the direct method may be applied to a functional formula_36 by showing\nThe second part is usually accomplished by showing that formula_6 admits some growth condition. An example is\nA functional with this property is sometimes called coercive. Showing sequential lower semi-continuity is usually the most difficult part when applying the direct method. See below for some theorems for a general class of functionals.\n\nThe typical functional in the calculus of variations is an integral of the form\nwhere formula_48 is a subset of formula_49 and formula_50 is a real-valued function on formula_51. The argument of formula_6 is a differentiable function formula_53, and its Jacobian formula_54 is identified with a formula_55-vector.\n\nWhen deriving the Euler–Lagrange equation, the common approach is to assume formula_48 has a formula_57 boundary and let the domain of definition for formula_6 be formula_59. This space is a Banach space when endowed with the supremum norm, but it is not reflexive. When applying the direct method, the functional is usually defined on a Sobolev space formula_60 with formula_61, which is a reflexive Banach space. The derivatives of formula_62 in the formula for formula_6 must then be taken as weak derivatives. The next section presents two theorems regarding weak sequential lower semi-continuity of functionals of the above type.\n\nAs many functionals in the calculus of variations are of the form\nwhere formula_65 is open, theorems characterizing functions formula_50 for which formula_6 is weakly sequentially lower-semicontinuous in formula_60 is of great importance.\n\nIn general we have the following\nWhen formula_88 or formula_89 the following converse-like theorem holds\n\nIn conclusion, when formula_89 or formula_88, the functional formula_6, assuming reasonable growth and boundedness on formula_50, is weakly sequentially lower semi-continuous if, and only if, the function formula_84 is convex. If both formula_105 and formula_106 are greater than 1, it is possible to weaken the necessity of convexity to generalizations of convexity, namely polyconvexity and quasiconvexity.\n\n"}
{"id": "3828419", "url": "https://en.wikipedia.org/wiki?curid=3828419", "title": "Donsker's theorem", "text": "Donsker's theorem\n\nIn probability theory, Donsker's theorem (also known as Donsker's invariance principle, or the functional central limit theorem), named after Monroe D. Donsker, is a functional extension of the central limit theorem. \n\nLet formula_1 be a sequence of independent and identically distributed (i.i.d.) random variables with mean 0 and variance 1. Let formula_2. The stochastic process formula_3 is known as a random walk. Define the diffusively rescaled random walk (partial-sum process) by\n\nThe central limit theorem asserts that formula_5 converges in distribution to a standard Gaussian random variable formula_6 as formula_7. Donsker's invariance principle extends this convergence to the whole function formula_8. More precisely, in its modern form, Donsker's invariance principle states that: As random variables taking values in the Skorokhod space formula_9, the random function formula_10 converges in distribution to a standard Brownian motion formula_11 as formula_12\n\nLet \"F\" be the empirical distribution function of the sequence of i.i.d. random variables formula_1 with distribution function \"F.\" Define the centered and scaled version of \"F\" by\n\nindexed by \"x\" ∈ R. By the classical central limit theorem, for fixed \"x\", the random variable \"G\"(\"x\") converges in distribution to a Gaussian (normal) random variable \"G\"(\"x\") with zero mean and variance \"F\"(\"x\")(1 − \"F\"(\"x\")) as the sample size \"n\" grows.\n\nTheorem (Donsker, Skorokhod, Kolmogorov) The sequence of \"G\"(\"x\"), as random elements of the Skorokhod space formula_15, converges in distribution to a Gaussian process \"G\" with zero mean and covariance given by\n\nThe process \"G\"(\"x\") can be written as \"B\"(\"F\"(\"x\")) where \"B\" is a standard Brownian bridge on the unit interval.\n\nKolmogorov (1933) showed that when \"F\" is continuous, the supremum formula_17 and supremum of absolute value, formula_18 converges in distribution to the laws of the same functionals of the Brownian bridge \"B\"(\"t\"), see the Kolmogorov–Smirnov test. In 1949 Doob asked whether the convergence in distribution held for more general functionals, thus formulating a problem of weak convergence of random functions in a suitable function space.\n\nIn 1952 Donsker stated and proved (not quite correctly) a general extension for the Doob-Kolmogorov heuristic approach. In the original paper, Donsker proved that the convergence in law of \"G\" to the Brownian bridge holds for Uniform[0,1] distributions with respect to uniform convergence in \"t\" over the interval [0,1].\n\nHowever Donsker's formulation was not quite correct because of the problem of measurability of the functionals of discontinuous processes. In 1956 Skorokhod and Kolmogorov defined a separable metric \"d\", called the \"Skorokhod metric\", on the space of cadlag functions on [0,1], such that convergence for \"d\" to a continuous function is equivalent to convergence for the sup norm, and showed that \"G\" converges in law in formula_9 to the Brownian bridge.\n\nLater Dudley reformulated Donsker's result to avoid the problem of measurability and the need of the Skorokhod metric. One can prove that there exist \"X\", iid uniform in [0,1] and a sequence of sample-continuous Brownian bridges \"B\", such that\nis measurable and converges in probability to 0. An improved version of this result, providing more detail on the rate of convergence, is the Komlós–Major–Tusnády approximation.\n\n"}
{"id": "9123977", "url": "https://en.wikipedia.org/wiki?curid=9123977", "title": "ESAIM: Control, Optimisation and Calculus of Variations", "text": "ESAIM: Control, Optimisation and Calculus of Variations\n\nESAIM: Control, Optimisation and Calculus of Variations is a scientific journal in the field of applied mathematics.\n"}
{"id": "30939514", "url": "https://en.wikipedia.org/wiki?curid=30939514", "title": "Effective topos", "text": "Effective topos\n\nIn mathematics, the effective topos is a topos introduced by , based on Kleene's notion of recursive realizability, that captures the idea of effectivity in mathematics.\n\n"}
{"id": "23039174", "url": "https://en.wikipedia.org/wiki?curid=23039174", "title": "Eigengap", "text": "Eigengap\n\nIn linear algebra, the eigengap of a linear operator is the difference between two successive eigenvalues, where eigenvalues are sorted in ascending order.\n\nThe Davis–Kahan theorem, named after Chandler Davis and William Kahan, uses the eigengap to show how eigenspaces of an operator change under perturbation. In spectral clustering, the eigengap is often referred to as the \"spectral gap\"; although the spectral gap may often be defined in a broader sense than that of the eigengap.\n\n"}
{"id": "46508864", "url": "https://en.wikipedia.org/wiki?curid=46508864", "title": "Emma Castelnuovo", "text": "Emma Castelnuovo\n\nEmma Castelnuovo (12 December 1913 – 13 April 2014) was an Italian secondary school teacher, daughter of the mathematician Guido Castelnuovo whose life was intermingled with events that changed the approach to the problems of mathematics in the second half of the twentieth century.\nInternational Commission on Mathematical Instruction (ICMI) in the past created two awards, the Felix Klein Award- honouring a lifetime achievement, and the Hans Freudenthal Award- recognizing a major cumulative program of research. In 2013, to celebrate her 100th birthday ICMI decided to add a third award named after Emma Castelnuovo- to recognize outstanding achievements made in the practice of mathematical education.\n\nEmma Castelnuovo was born in Rome on 12 December 1913 to Elbina and Guido Castelnuovo; her father and her uncle Federigo Enriques were both professors of mathematics. In 1908 Guido was the chairman of the fourth International Congress of Mathematicians in Rome.\n\nCastelnuovo graduated from the University of Rome in 1936 with a thesis on algebraic geometry. After this she worked as a librarian at the same university. She won a permanent position in 1938, but on account of the racial laws against Jews, she was suspended from work. At the beginning of her career, Castelnuovo was looking into the possibility of teaching which would include involving her students.\n\nThe textbook 'Geometria intuitiva, per le scuole medie inferiori' (Intuitive geometry for lower secondary schools), first published in 1948, had various editions till 1964 and was translated into Spanish and English. It launched Castelnuovo to international level so that she was invited to join working groups and meetings.\n\nCastelnuovo was one of the two representatives from Italy taking an active part in discussing Modern Mathematics. Castelnuovo was recognized as an international member when she was appointed as member of the ICMI during the period 1975-1978.\n\nCastelnuovo was a well-respected and a brilliant mathematician. In her many years of teaching, she not only influenced her students but also the young colleagues present at her university. She was emotionally involved in her lectures and transmitted her enthusiasm and motivation to her peers.\n"}
{"id": "1555443", "url": "https://en.wikipedia.org/wiki?curid=1555443", "title": "General insurance", "text": "General insurance\n\nGeneral insurance or non-life insurance policies, including automobile and homeowners policies, provide payments depending on the loss from a particular financial event. General insurance is typically defined as any insurance that is not determined to be life insurance. It is called property and casualty insurance in the United States and Canada and non-life insurance in Continental Europe.\n\nIn the United Kingdom, insurance is broadly divided into three areas: personal lines, commercial lines and London market.\n\nThe London market insures large commercial risks such as supermarkets, football players and other very specific risks. It consists of a number of insurers, reinsurers, P&I Clubs, brokers and other companies that are typically physically located in the City of London. Lloyd's of London is a big participant in this market. The London market also participates in personal lines and commercial lines, domestic and foreign, through reinsurance.\n\nCommercial lines products are usually designed for relatively small legal entities. These would include workers' compensation (employers liability), public liability, product liability, commercial fleet and other general insurance products sold in a relatively standard fashion to many organisations. There are many companies that supply comprehensive commercial insurance packages for a wide range of different industries, including shops, restaurants and hotels.\n\nPersonal lines products are designed to be sold in large quantities. This would include autos (private car), homeowners (household), pet insurance, creditor insurance and others.\n\nACORD, which is the insurance industry global standards organization, has standards for personal and commercial lines and has been working with the Australian General Insurers to develop those XML standards, standard applications for insurance, and certificates of currency.\n\nGeneral insurance can be categorised in to following:\n\nThe United States was the largest market for non-life insurance premiums written in 2005 followed by the European Union and Japan.\n\n"}
{"id": "3356273", "url": "https://en.wikipedia.org/wiki?curid=3356273", "title": "George Andrews (mathematician)", "text": "George Andrews (mathematician)\n\nGeorge Eyre Andrews (born December 4, 1938 in Salem, Oregon) is an American mathematician working in analysis and combinatorics.\n\nHe is currently an Evan Pugh Professor of Mathematics at Pennsylvania State University. He did his undergraduate studies at Oregon State University and received his PhD in 1964 at the University of Pennsylvania where his advisor was Hans Rademacher.\n\nDuring 2008-2009 he was president of the American Mathematical Society.\n\nAndrews's contributions include several monographs and over 250 research and popular articles on q-series, special functions, combinatorics and applications. He is considered to be the world's leading expert in the theory of integer partitions. In 1976 he discovered Ramanujan's Lost Notebook. He is highly interested in mathematical pedagogy.\n\nHis book \"The Theory of Partitions\" is the standard reference on the subject of integer partitions.\n\nAndrews is a member of the National Academy of Sciences. He was elected a Fellow of the American Academy of Arts and Sciences in 1997. In 2012 he became a fellow of the American Mathematical Society.\n\nHe was given honorary doctorates from the University of Parma in 1998, the University of Florida in 2002, the University of Waterloo in 2004, SASTRA University in Kumbakonam, India in 2012, and University of Illinois at Urbana–Champaign in 2014\n\n\n"}
{"id": "5732549", "url": "https://en.wikipedia.org/wiki?curid=5732549", "title": "Gingerbreadman map", "text": "Gingerbreadman map\n\nIn dynamical systems theory, the Gingerbreadman map is a chaotic two-dimensional map. It is given by the piecewise linear transformation:\n\n"}
{"id": "38470542", "url": "https://en.wikipedia.org/wiki?curid=38470542", "title": "Grothendieck local duality", "text": "Grothendieck local duality\n\nIn commutative algebra, Grothendieck local duality is a duality theorem for cohomology of modules over local rings, analogous to Serre duality of coherent sheaves.\n\nSuppose that \"R\" is a Cohen–Macaulay local ring of dimension \"d\" with maximal ideal \"m\" and residue field \"k\" = \"R\"/\"m\". Let \"E\"(\"k\") be a Matlis module, an injective hull of \"k\", and let be the completion of its dualizing module. Then for any \"R\"-module \"M\" there is an isomorphism of modules over the completion of \"R\":\n\nwhere \"H\" is a local cohomology group.\n\nThere is a generalization to Noetherian local rings that are not Cohen–Macaulay, that replaces the dualizing module with a dualizing complex.\n\n"}
{"id": "5259526", "url": "https://en.wikipedia.org/wiki?curid=5259526", "title": "Information algebra", "text": "Information algebra\n\nThe term \"information algebra\" refers to mathematical techniques of information processing. Classical information theory goes back to Claude Shannon. It is a theory of information transmission, looking at communication and storage. However, it has not been considered so far that information comes from different sources and that it is therefore usually combined. It has furthermore been neglected in classical information theory that one wants to extract those parts out of a piece of information that are relevant to specific questions.\n\nA mathematical phrasing of these operations leads to an algebra of information, describing basic modes of information processing. Such an algebra involves several formalisms of computer science, which seem to be different on the surface: relational databases, multiple systems of formal logic or numerical problems of linear algebra. It allows the development of generic procedures of information processing and thus a unification of basic methods of computer science, in particular of distributed information processing.\n\nInformation relates to precise questions, comes from different sources, must be aggregated, and can be focused on questions of interest. Starting from these considerations, information algebras are two-sorted algebras formula_1, where formula_2 is a semigroup, representing combination or aggregation of information, formula_3 is a lattice of domains (related to questions) whose partial order reflects the granularity of the domain or the question, and a mixed operation representing focusing or extraction of information.\n\nMore precisely, in the two-sorted algebra formula_1, the following operations are defined\n\nAdditionally, in formula_3 the usual lattice operations (meet and join) are defined.\n\nThe axioms of the two-sorted algebra formula_1, in addition to the axioms of the lattice formula_3:\n\nA two-sorted algebra formula_1 satisfying these axioms is called an Information Algebra.\n\nA partial order of information can be introduced by defining formula_9 if formula_10. This means that formula_11 is less informative than formula_12 if it adds no new information to formula_12. The semigroup formula_2 is a semilattice relative to this order, i.e. formula_15. Relative to any domain (question) formula_16 a partial order can be introduced by defining formula_17 if formula_18. It represents the order of information content of formula_11 and formula_12 relative to the domain (question) formula_21.\n\nThe pairs formula_22, where formula_23 and formula_16 such that formula_25 form a labeled Information Algebra. More precisely, in the two-sorted algebra formula_26, the following operations are defined\nHere follows an incomplete list of instances of information algebras:\n\nLet formula_27 be a set of symbols, called \"attributes\" (or \"column\nnames\"). For each formula_28 let formula_29 be a non-empty set, the\nset of all possible values of the attribute formula_30. For example, if \nformula_31, then formula_32 could\nbe the set of strings, whereas formula_33 and formula_34 are both\nthe set of non-negative integers.\n\nLet formula_35. An \"formula_21-tuple\" is a function formula_37 so that\nformula_38 and formula_39 for each formula_40 The set\nof all formula_21-tuples is denoted by formula_42. For an formula_21-tuple formula_37 and a subset\nformula_45 the restriction formula_46 is defined to be the\nformula_47-tuple formula_48 so that formula_49 for all formula_50.\n\nA \"relation formula_51 over formula_21\" is a set of formula_21-tuples, i.e. a subset of formula_42.\nThe set of attributes formula_21 is called the \"domain\" of formula_51 and denoted by\nformula_57. For formula_58 the \"projection\" of formula_51 onto formula_47 is defined\nas follows:\nThe \"join\" of a relation formula_51 over formula_21 and a relation formula_64 over formula_47 is\ndefined as follows:\nAs an example, let formula_51 and formula_64 be the following relations:\nThen the join of formula_51 and formula_64 is:\nA relational database with natural join formula_73 as combination and the usual projection formula_74 is an information algebra.\nThe operations are well defined since\nIt is easy to see that relational databases satisfy the axioms of a labeled\ninformation algebra:\n\n\nThe axioms for information algebras are derived from \nthe axiom system proposed in (Shenoy and Shafer, 1990), see also (Shafer, 1991).\n\n"}
{"id": "39584195", "url": "https://en.wikipedia.org/wiki?curid=39584195", "title": "International Symposium on Fundamentals of Computation Theory", "text": "International Symposium on Fundamentals of Computation Theory\n\nFCT, the International Symposia on Fundamentals of Computation Theory is a biennial series of conferences in the field of theoretical computer science. It was established in 1977 for researchers interested in all aspects of theoretical computer science, and in particular algorithms, computational complexity, formal and logical methods. FCT was previously held at the following institutions.\n\n\n \n"}
{"id": "5178038", "url": "https://en.wikipedia.org/wiki?curid=5178038", "title": "Jean Bartik", "text": "Jean Bartik\n\nJean Jennings Bartik (December 27, 1924 – March 23, 2011) was one of the original programmers for the ENIAC computer. She studied mathematics in school then began work at the University of Pennsylvania, first manually calculating ballistics trajectories, then using ENIAC to do so. She and her colleagues developed and codified many of the fundamentals of programming while working on the ENIAC, since it was the first computer of its kind. After her work on ENIAC, Bartik went on to work on BINAC and UNIVAC, and spent time at a variety of technical companies as a writer, manager, engineer and programmer. She spent her later years as a real estate agent and died in 2011 from congestive heart failure complications.\n\nBorn Betty Jean Jennings in Gentry County, Missouri, in 1924, she was the sixth of seven children. Her father, William Smith Jennings (1893-1971) was from Alanthus Grove, where he was a schoolteacher as well as a farmer. Her mother, Lula May Spainhower (1887-1988) was from Alanthus. Jennings had three older brothers, William (January 10, 1915) Robert (March 15, 1918); and Raymond (January 23, 1922); two older sisters, Emma (August 11, 1916) and Lulu (August 22, 1919), and one younger sister, Mable (December 15, 1928).\n\nIn her childhood, she would ride on horseback to visit her grandmother, who bought the young girl a newspaper to read every day and became a role model for the rest of her life. She began her education at a local one-room school, and gained local attention for her softball skill. In order to attend high school, she lived with her older sister in the neighboring town, where the school was located, and then began to drive every day despite being only 14. She graduated from Stanberry High School in 1941, aged 16.\n\nShe attended Northwest Missouri State Teachers College now known Northwest Missouri State University, majoring in mathematics with a minor in English and graduating in 1945. Jennings was awarded the only mathematics degree in her class. Although she had originally intended to study journalism, she decided to change to mathematics because she had a bad relationship with her adviser. Later in her life, she earned a master's degree in English at the University of Pennsylvania in 1967 and was awarded an honorary doctorate degree from Northwest Missouri State University in 2002.\n\nIn 1945, the Army was recruiting mathematicians from universities to aid in the war effort; despite a warning by her adviser that she would be just \"a cog in a wheel\" with the Army, and despite encouragement to become a mathematics teacher instead, Bartik known as Betty Jennings at the time decided to become a human computer. Her calculus professor, encouraged Bartik to take the job at University of Pennsylvania because they had a differential analyzer.\n\nShe applied to both IBM and the University of Pennsylvania at the age of 20. Although rejected by IBM, Jennings was hired by the University of Pennsylvania to work for Army Ordnance at Aberdeen Proving Ground, calculating ballistics trajectories by hand. While working there, Bartik met her husband, William Bartik, who was an engineer working on a Pentagon project at the University of Pennsylvania. They married in December 1946.\n\nWhen the Electronic Numeric Integrator and Computer (ENIAC) was developed for the purpose of calculating the ballistic trajectories human computers like Bartik had been doing by hand, she applied to become a part of the project and was eventually selected to be one of its first programmers. Bartik was asked to set up problems for the ENIAC without being taught any techniques. Six women, including Jean Jennings Bartik, Betty Holberton, Marlyn Wescoff, Kathleen McNulty, Ruth Teitelbaum, and Frances Spence, were chosen to be the main programmers for the ENIAC. Many other women who are often unrecognized contributed to the ENIAC during a period of wartime male labor shortage. Bartik, who became the co-lead programmer (with Betty Holberton), and the other four original programmers became extremely adept at running the ENIAC; with no manual to rely on, the group reviewed diagrams of the device, interviewed the engineers who had built it, and used this information to teach themselves the skills they needed. Initially, they were not allowed to see the ENIAC's hardware at all since it was still classified and they had not received security clearance; they had to learn how to program the machine solely through studying schematic diagrams. The six-woman team was also not initially given space to work together, so they found places to work where they could, in abandoned classrooms and fraternity houses.\n\nWhile the six women worked on ENIAC, they developed subroutines, nesting, and other fundamental programming techniques, and arguably invented the discipline of programming digital computers. Bartik and the other ENIAC female programmers learned to physically modify the machine, moving switches and rerouting cables, in order to program it. In addition to performing the original ballistic trajectories they were hired to compute, the six female programmers soon became operators on the Los Alamos nuclear calculations, and generally expanded the programming repertoire of the machine. Bartik's programming partner on the important trajectory program for the military that would prove that the ENIAC worked to specification was Betty Holberton known at the time as Betty Snyder. Bartik and Holberton's program was chosen to introduce the ENIAC to the public and larger scientific community. That demonstration occurred on February 15, 1946 and was a tremendous success. The ENIAC proved that it operated faster than the Mark I, a well known electromechanical machine at Harvard, and also showed that the work that would take a \"human computer\" 40 hours to complete could be done in 20 seconds.\n\nBartik described the first public demonstration of the ENIAC in 1946:\n\nThe public demonstration was a success, but most of the congratulations on its turnout were given to its engineers, Mauchly and Eckert.\n\nBartik was later asked to form and lead a group of programmers to convert the ENIAC into a stored program computer, working closely with John Von Neumann, Dick Clippinger and Adele Goldstine.\n\nBartik converted the ENIAC into a stored program computer by March 1948. As head of this process, Bartik was charged with the conversion that allowed the ENIAC to be turned into rudimentary stored program computer to assist with Clippinger's wind tunnel programs, which allowed the ENIAC to operate more quickly, efficiently, and accurately. Letters between Bartik and Adele Goldstine were discovered by authors Thomas Haigh and Mark Priestley during the time of the project, as well as, the fact that much of the 60-order Code was in Bartik's handwriting.\n\nAfter the end of the war, Bartik went on to work with the ENIAC designers John Eckert and John Mauchly, and helped them develop the BINAC and UNIVAC I computers. BINAC was the first computer to use magnetic tape instead of punch cards to store data and the first computer to utilize the twin unit concept. The BINAC, which was the first stored program computer hardware wise, worked successfully. BINAC was purchased by Northrop Aircraft to guide the Snark missile, but the BINAC proved to be too large for their purposes. However, according to a Northrop Aircraft programmer, despite claims that the BINAC didn't work once it was moved to Northrop Aircraft were erroneous and the BINAC was working well into the mid-1950s. Besides, BINAC, Jean's more important work involved her responsibilities in designing the UNIVAC's logic circuits among other UNIVAC programming and design tasks. Bartik also co-programmed with her life-long friend Betty Holberton the first generative programming system (SORT/MERGE) for a computer. Recalling her time working with Eckert and Mauchly on these projects, she described their close group of computer engineers as a \"technical Camelot.\"\n\nIn the early 1950s, once the Eckert-Mauchly Corporation was sold to Remington Rand Bartik went on to help train on how to program and use the UNIVAC for the first six UNIVACs sold, including the programmers at the United States Census Bureau (first UNIVAC sold) and Atomic Energy Commission. Later, Bartik moved to Philadelphia when her husband, William \"Bill\" Bartik,took a job with Remington Rand. Unfortunately, due to a company policy at the time about husbands and wives working together, Jean Bartik, was asked to resign from the company. Between 1951 and 1954, prior to her first child's birth, Jean did mostly free-lance programming assignments for John Mauchly and was a helpmate to her husband. Once her son was born, Jean walked away from her career in computing to concentrate on raising a family, during which time she had two other children with her husband.\n\nEven though Bartik played an integral part in developing ENIAC, her work at University of Pennsylvania and on the ENIAC was completely hidden until her pioneering work was documented by columnist Tom Petzinger in several articles for the Wall Street Journal on Bartik and Holberton. Later, a film called helped shed even more light on Bartik's pioneering work in computing along with Kathy Kleiman's ENIAC Programmers Project.\n\nAfter getting her Masters degree from the University of Pennsylvania in 1967 and making the decision to divorce her husband, Bartik joined the Auerbach Corporation writing and editing technical reports on minicomputers. It was at this time that Jean began going by the name \"Jean\" rather than \"Betty\" which is what she had been known as during her ENIAC, UNIVAC and Remington-Rand years. The name change was the result of wanting to be taken more seriously by her male co-workers. Bartik remained with Auerbach for eight years, then moved among positions with a variety of other companies for the rest of her career as a manager, writer, and engineer. Jean Bartik and William Bartik divorced by 1968. Bartik ultimately retired from the computing industry in 1986 when her final employer, Data Decisions (a publication of Ziff-Davis), was sold; Bartik spent the following 25 years as a real estate agent.\n\nStarting in 1996, once the importance of their role in the development of computing was re-discovered, Bartik along with Betty Holberton and Bartik's other friend of over 60 years Kay Mauchly (ENIAC programmer and wife of ENIAC co-inventor John Mauchly) began to finally receive the acknowledgement and honors for their pioneering work in the early field of computing. Bartik and Kay Mauchly became invited speakers both at home and abroad to share their experiences working with the ENIAC,BINAC and UNIVAC. Bartik especially went on to receive many honors and awards for her pioneering role programming the ENIAC, BINAC and UNIVAC, the latter of which helped to launch the commercial computer industry, and for turning the ENIAC into the world's first stored program computer. The Jean Jennings Bartik Computing Museum was opened in 2002 on the campus of Northwest Missouri State University, Bartik's alma mater, in her honoror. Bartik was inducted into the Computer History Museum's distinguished Hall of Fellows along with Linus Torvalds, creator and principal developer of the Linux kernel and Robert Metcalf, father of Ethernet, in 2008. Bartik wrote her autobiography \"Pioneer Programmer: Jean Jennings Bartik and the Computer that Changed the World\" prior to her death in 2011 with the help of long-time colleagues, Dr. Jon T. Rickman and Kim D. Todd. The autobiography was published in 2013 by Truman State Press to positive reviews. Bartik died from congestive heart failure in a Poughkeepsie, New York nursing home on March 23, 2011. She was 86.\n\nJean wrote that one of the best pieces of advice she ever received was: \"Don't ever let anyone tell you that you can't do something because they think you can't. You can do anything, achieve anything, if you think you can and you educate yourself to succeed.\" Encouraging girls and women to follow their dreams, she said, \"If my life has proved anything, it is that women (and girls) should never be afraid to take risks and try new things.\"\n\nThe Jean Jennings Bartik Computing Museum at Northwest Missouri State University in Maryville, Missouri is dedicated to the history of computing and Bartik's career.\n\nContent-management framework Drupal's default theme, \"Bartik,\" is named in honor of her.\n\n\n\n"}
{"id": "373216", "url": "https://en.wikipedia.org/wiki?curid=373216", "title": "Kahan summation algorithm", "text": "Kahan summation algorithm\n\nIn numerical analysis, the Kahan summation algorithm (also known as compensated summation) significantly reduces the numerical error in the total obtained by adding a sequence of finite precision floating point numbers, compared to the obvious approach. This is done by keeping a separate \"running compensation\" (a variable to accumulate small errors).\n\nIn particular, simply summing \"n\" numbers in sequence has a worst-case error that grows proportional to \"n\", and a root mean square error that grows as formula_1 for random inputs (the roundoff errors form a random walk). With compensated summation, the worst-case error bound is independent of \"n\", so a large number of values can be summed with an error that only depends on the floating-point precision.\n\nThe algorithm is attributed to William Kahan. Similar, earlier techniques are, for example, Bresenham's line algorithm, keeping track of the accumulated error in integer operations (although first documented around the same time) and the delta-sigma modulation (integrating, not just summing the error).\n\nIn pseudocode, the algorithm is:\n\nThis example will be given in decimal. Computers typically use binary arithmetic, but the principle being illustrated is the same. Suppose we are using six-digit decimal floating point arithmetic, \"sum\" has attained the value 10000.0, and the next two values of \"input(i)\" are 3.14159 and 2.71828. The exact result is 10005.85987, which rounds to 10005.9. With a plain summation, each incoming value would be aligned with \"sum\" and many low order digits lost (by truncation or rounding). The first result, after rounding, would be 10003.1. The second result would be 10005.81828 before rounding, and 10005.8 after rounding. This is not correct.\n\nHowever, with compensated summation, we get the correct rounded result of 10005.9.\n\nAssume that \"c\" has the initial value zero.\n\nThe sum is so large that only the high-order digits of the input numbers are being accumulated. But on the next step, \"c\" gives the error.\n\nSo the summation is performed with two accumulators: \"sum\" holds the sum, and \"c\" accumulates the parts not assimilated into \"sum\", to nudge the low-order part of \"sum\" the next time around. Thus the summation proceeds with \"guard digits\" in \"c\" which is better than not having any but is not as good as performing the calculations with double the precision of the input. However, simply increasing the precision of the calculations is not practical in general; if \"input\" is already double precision, few systems supply quadruple precision and if they did, \"input\" could then be quadruple precision.\n\nA careful analysis of the errors in compensated summation is needed to appreciate its accuracy characteristics. While it is more accurate than naive summation, it can still give large relative errors for ill-conditioned sums.\n\nSuppose that one is summing \"n\" values \"x\", for \"i\"=1...,\"n\". The exact sum is:\nWith compensated summation, one instead obtains formula_3, where the error formula_4 is bounded by:\nwhere ε is the machine precision of the arithmetic being employed (e.g. ε≈10 for IEEE standard double precision floating point). Usually, the quantity of interest is the relative error formula_6, which is therefore bounded above by:\n\nIn the expression for the relative error bound, the fraction Σ|\"x\"|/|Σ\"x\"| is the condition number of the summation problem. Essentially, the condition number represents the \"intrinsic\" sensitivity of the summation problem to errors, regardless of how it is computed. The relative error bound of \"every\" (backwards stable) summation method by a fixed algorithm in fixed precision (i.e. not those that use arbitrary precision arithmetic, nor algorithms whose memory and time requirements change based on the data), is proportional to this condition number. An \"ill-conditioned\" summation problem is one in which this ratio is large, and in this case even compensated summation can have a large relative error. For example, if the summands \"x\" are uncorrelated random numbers with zero mean, the sum is a random walk and the condition number will grow proportional to formula_1. On the other hand, for random inputs with nonzero mean the condition number asymptotes to a finite constant as formula_9. If the inputs are all non-negative, then the condition number is 1.\n\nGiven a condition number, the relative error of compensated summation is effectively independent of \"n\". In principle, there is the O(\"n\"ε) that grows linearly with \"n\", but in practice this term is effectively zero: since the final result is rounded to a precision ε, the \"n\"ε term rounds to zero unless \"n\" is roughly 1/ε or larger. In double precision, this corresponds to an \"n\" of roughly 10, much larger than most sums. So, for a fixed condition number, the errors of compensated summation are effectively \"O\"(ε), independent of \"n\".\n\nIn comparison, the relative error bound for naive summation (simply adding the numbers in sequence, rounding at each step) grows as formula_10 multiplied by the condition number. This worst-case error is rarely observed in practice, however, because it only occurs if the rounding errors are all in the same direction. In practice, it is much more likely that the rounding errors have a random sign, with zero mean, so that they form a random walk; in this case, naive summation has a root mean square relative error that grows as formula_11 multiplied by the condition number. This is still much worse than compensated summation, however. Note, however, that if the sum can be performed in twice the precision, then ε is replaced by ε and naive summation has a worst-case error comparable to the O(\"n\"ε) term in compensated summation at the original precision.\n\nBy the same token, the Σ|\"x\"| that appears in formula_4 above is a worst-case bound that occurs only if all the rounding errors have the same sign (and are of maximum possible magnitude). In practice, it is more likely that the errors have random sign, in which case terms in Σ|\"x\"| are replaced by a random walk—in this case, even for random inputs with zero mean, the error formula_4 grows only as formula_11 (ignoring the \"n\"ε term), the same rate the sum formula_15 grows, canceling the formula_1 factors when the relative error is computed. So, even for asymptotically ill-conditioned sums, the relative error for compensated summation can often be much smaller than a worst-case analysis might suggest.\n\nNeumaier introduced a slight modification of Kahan's algorithm that also covers the case when the next term to be added is larger in absolute value than the running sum, effectively swapping the role of what is large and what is small. In pseudocode, the algorithm is:\n\nFor many sequences of numbers, both algorithms agree but a simple example due to Peters shows how they can differ. For summing formula_17 in double precision, Kahan's algorithm yields 0.0 whereas Neumaier's algorithm yields the correct value 2.0.\n\nAlthough Kahan's algorithm achieves formula_18 error growth for summing \"n\" numbers, only slightly worse formula_19 growth can be achieved by pairwise summation: one recursively divides the set of numbers into two halves, sums each half, and then adds the two sums. This has the advantage of requiring the same number of arithmetic operations as the naive summation (unlike Kahan's algorithm, which requires four times the arithmetic and has a latency of four times a simple summation) and can be calculated in parallel. The base case of the recursion could in principle be the sum of only one (or zero) numbers, but to amortize the overhead of recursion one would normally use a larger base case. The equivalent of pairwise summation is used in many fast Fourier transform (FFT) algorithms, and is responsible for the logarithmic growth of roundoff errors in those FFTs. In practice, with roundoff errors of random signs, the root mean square errors of pairwise summation actually grow as formula_20.\n\nAnother alternative is to use arbitrary precision arithmetic, which in principle need no rounding at all with a cost of much greater computational effort. A way of performing exactly rounded sums using arbitrary precision is to extend adaptively using multiple floating-point components. This will minimize computational cost in common cases where high precision is not needed. Another method that uses only integer arithmetic, but a large accumulator was described by Kirchner and Kulisch; a hardware implementation was described by Müller, Rüb and Rülling.\n\nIn principle, a sufficiently aggressive optimizing compiler could destroy the effectiveness of Kahan summation: for example, if the compiler simplified expressions according to the associativity rules of real arithmetic, it might \"simplify\" the second step in the sequence codice_1 to codice_2 then to codice_3, eliminating the error compensation. In practice, many compilers do not use associativity rules (which are only approximate in floating-point arithmetic) in simplifications unless explicitly directed to do so by compiler options enabling \"unsafe\" optimizations, although the Intel C++ Compiler is one example that allows associativity-based transformations by default. The original K&R C version of the C programming language allowed the compiler to re-order floating-point expressions according to real-arithmetic associativity rules, but the subsequent ANSI C standard prohibited re-ordering in order to make C better suited for numerical applications (and more similar to Fortran, which also prohibits re-ordering), although in practice compiler options can re-enable re-ordering as mentioned above.\n\nIn general, built-in \"sum\" functions in computer languages typically provide no guarantees that a particular summation algorithm will be employed, much less Kahan summation. The BLAS standard for linear algebra subroutines explicitly avoids mandating any particular computational order of operations for performance reasons, and BLAS implementations typically do not use Kahan summation.\nThe standard library of the Python computer language specifies an fsum function for exactly rounded summation, using the Shewchuk algorithm to track multiple partial sums.\n\nIn the Julia language, the default implementation of the codice_4 function does pairwise summation for high accuracy with good performance, but the standard library also has an implementation of Neumaier's variant named codice_5 for the cases when the highest accuracy is needed.\n\n\n"}
{"id": "34625854", "url": "https://en.wikipedia.org/wiki?curid=34625854", "title": "List of animals featuring external asymmetry", "text": "List of animals featuring external asymmetry\n\nThis is a list of animals that markedly feature external asymmetry in some form. They are exceptions to the general pattern of symmetry in biology. In particular, these animals do not exhibit bilateral symmetry which permits streamlining and is common in animals.\nThe crossbill has an unusual beak in which the upper and lower tips cross each other.\n\nThe wrybill is the only species of bird in the world with a beak that is bent sideways (always to the right).\n\nMany owl species, such as the barn owl, have asymmetrically positioned ears that enhance sound positioning.\n\nMany flatfish, such as flounders, have eyes placed asymmetrically in the adult fish. The fish has the usual symmetrical body structure when it is young, but as it matures and moves to living close to the sea bed, the fish lies on its side, and the head twists so that both eyes are on the top.\n\nThe jaws of the scale-eating cichlid \"Perissodus microlepis\" occur in two distinct morphological forms. One morph has its jaw twisted to the left, allowing it to eat scales more readily on its victim’s right flank. The other morph has its jaw twisted to the right, which makes it easier to eat scales on its victim’s left flank. The relative abundance of the two morphs in populations is regulated by frequency-dependent selection.\n\nThe narwhal has a helical tusk on its upper left jaw. \"Odobenocetops\", an extinct toothed whale, may have possessed similar asymmetrical dentition, though it differed from the narwhal in possessing two erupted, rear-facing tusks with the right significantly longer than the left.\n\nThe sperm whale(\"Physeter macrocephalus\") has a single nostril on its upper left head. The right nostril forms a phonic lip. The source of the air forced through the phonic lips is the right nasal passage. While the left nasal passage opens to the blow hole, the right nasal passage has evolved to supply air to the phonic lips. It is thought that the nostrils of the land-based ancestor of the sperm whale migrated through evolution to their current functions, the left nostril becoming the blowhole and the right nostril becoming the phonic lips.\n\nThe fin whale (\"Balaenoptera physalus\") has complex and asymmetrical coloration on its head. \n\nHoney badgers of the subspecies \"signata\" have a second lower molar on the left side of their jaws, but not the right.\n\nIwasaki's snail-eater snake (\"Pareas iwasakii\") is a snail-eating specialist; even newly hatched individuals feed on snails. It has asymmetric jaws, which facilitates feeding on snails with dextral (clockwise coiled) shells. A consequence of this asymmetry is that this snake is much less adept at preying on sinistral (counterclockwise coiled) snails.\n\n\"Stegosaurus stenops\" had two \"staggered\" rows of plates on its back, thus defying mirror symmetry.\n\nFiddler crabs and hermit crabs have one claw much larger than the other. If a male fiddler loses its large claw, it will grow another on the opposite side after moulting. A soft abdomen is also present in a hermit crab as an asymmetrical modification.\n\nAll gastropods are asymmetrical. This is easily seen in snails and sea snails, which have helical shells. At first glance slugs appear externally symmetrical, but their pneumostome (breathing hole) is always on the right side. The origin of asymmetry in gastropods is a subject of scientific debate. Other gastropods develop external asymmetry, such as \"Glaucus atlanticus\" that develops asymmetrical cerata as they mature.\n\n\"Histioteuthis\" is a genus of squid, commonly known as the cock-eyed squid, because in all species the right eye is normal-sized, round, blue and sunken; whereas the left eye is at least twice the diameter of the right eye, tubular, yellow-green, faces upward, and bulges out of the head.\n\nSessile animals such as sponges are asymmetrical.\n\nCorals build colonies that are not symmetrical, but the individual polyps exhibit radial symmetry.\n\nAlpheidae feature asymmetrical claws that lack pincers, the larger of which can grow on either side of the body, and if lost can develop on the opposite arm instead.\n\nCertain polyopisthocotylean monogeneans are asymmetrical, as an adaptation to their attachment to the gill of their fish hosts. \n\nCertain parasitic copepods which live inside the gill chamber of their fish hosts are asymmetrical.\n\n"}
{"id": "169589", "url": "https://en.wikipedia.org/wiki?curid=169589", "title": "List of continuity-related mathematical topics", "text": "List of continuity-related mathematical topics\n\nIn mathematics, the terms continuity, continuous, and continuum are used in a variety of related ways.\n\n\n"}
{"id": "6789891", "url": "https://en.wikipedia.org/wiki?curid=6789891", "title": "Lévy–Prokhorov metric", "text": "Lévy–Prokhorov metric\n\nIn mathematics, the Lévy–Prokhorov metric (sometimes known just as the Prokhorov metric) is a metric (i.e., a definition of distance) on the collection of probability measures on a given metric space. It is named after the French mathematician Paul Lévy and the Soviet mathematician Yuri Vasilyevich Prokhorov; Prokhorov introduced it in 1956 as a generalization of the earlier Lévy metric.\n\nLet formula_1 be a metric space with its Borel sigma algebra formula_2. Let formula_3 denote the collection of all probability measures on the measurable space formula_4.\n\nFor a subset formula_5, define the ε-neighborhood of formula_6 by\n\nwhere formula_8 is the open ball of radius formula_9 centered at formula_10.\n\nThe Lévy–Prokhorov metric formula_11 is defined by setting the distance between two probability measures formula_12 and formula_13 to be\n\nFor probability measures clearly formula_15.\n\nSome authors omit one of the two inequalities or choose only open or closed formula_6; either inequality implies the other, and formula_17, but restricting to open sets may change the metric so defined (if formula_18 is not Polish).\n\n\n\n"}
{"id": "6691129", "url": "https://en.wikipedia.org/wiki?curid=6691129", "title": "Medical underwriting", "text": "Medical underwriting\n\nMedical underwriting is a health insurance term referring to the use of medical or health information in the evaluation of an applicant for coverage, typically for life or health insurance. As part of the underwriting process, an individual's health information may be used in making two decisions: whether to offer or deny coverage and what premium rate to set for the policy. The two most common methods of medical underwriting are known as moratorium underwriting, a relatively simple process, and full medical underwriting, a more indepth analysis of a client's health information. The use of medical underwriting may be restricted by law in certain insurance markets. If allowed, the criteria used should be objective, clearly related to the likely cost of providing coverage, practical to administer, consistent with applicable law, and designed to protect the long-term viability of the insurance system.\n\nIt is the process in which underwriter takes the notice of the health conditions of the person who is applying for the insurance, keeping in mind certain factors like health condition, age, nature of work, and geographical zone. After looking at all the factors, underwriter suggest whether policy should be given to the person and, if so, what will be the premium.\n\nUnderwriting is the process that a health insurer uses to weigh potential health risks in its pool of insured people against potential costs of providing coverage.\n\nTo search the medical underwriting, an insurer asks people who apply for coverage (typically people applying for individual or family coverage) about pre-existing medical conditions. In most US states, insurance companies are allowed to ask questions about a person's medical history to decide whom to offer coverage, whom to deny and if additional charges should apply to individually-purchased coverage.\n\nWhile most discussions of medical underwriting in health insurance are about medical expense insurance, similar considerations apply for other forms of individually-purchased health insurance, such as disability income and longterm care insurance.\n\nFrom the insurers' point of view, medical underwriting is necessary to prevent people from purchasing health insurance coverage only when they are sick, pregnant or need medical care. Adverse selection is a system that attracts high-users and discourages low-users from participating. Proponents of underwriting believe that if given the ability to purchase coverage without regard for pre-existing medical conditions (no underwriting), people would wait to purchase health insurance until they got sick or needed medical care. Waiting to obtain health insurance coverage until one needs coverage then creates a pool of insureds with \"high use,\" which then increases the premiums that insurance companies must charge to pay for the claims incurred. In turn, high premiums further discourage healthy people from obtaining coverage, particularly when they realize that they will be able to obtain coverage when they need medical care.\n\nProponents of medical underwriting thus argue that it ensures that individual health insurance premiums are kept as low as possible. Critics of medical underwriting believe that it unfairly prevents people with relatively minor and treatable pre-existing conditions from obtaining health insurance. Diseases that can make an individual uninsurable include serious conditions, such as arthritis, cancer, and heart disease but also such common ailments as acne, being 20 lb. over or under the ideal weight, and old sports injuries. An estimated 5 million of those without health insurance are considered \"uninsurable\" because of pre-existing conditions.\n\nOne large industry survey, from 2004, found that roughly 13% of those who applied for individual health insurance were denied coverage after undergoing medical underwriting. Declination rates increased significantly with age, rising from 5% for individuals 18 and under to just under a third for individuals to 64. The same study found that among those who received offers for coverage, 76% received offers at standard rates 22% were quoted higher rates. The frequency of increased premiums also increased with age so for applicants over 40, roughly half were affected by medical underwriting, either in the form of denial or increased premiums. The study did not address how many applicants offered coverage at higher premiums decided to decline the policy. A study conducted by the Commonwealth Fund in 2001 found that, among those 19 to 64 who sought individual health insurance during the previous three years, the majority found it expensive, and less than a third ended up purchasing insurance. However, the study did not distinguish between consumers who were quoted increased rates by medical underwriting and those who qualified for standard or preferred premiums.\n\nMeasuring the percentage of applicants who were denied coverage does not capture any effect that occurs before an application is submitted. If individuals with serious health conditions never apply because they expect that they will be denied coverage, they will not show up in the declination rate. Conversely, if they apply with multiple insurers in hopes of finding one that will issue them a policy, they will be overrepresented in the declination rate. The 2001 Commonwealth Fund study found that a majority of adults reported that it was at least somewhat difficult to find an affordable health insurance policy. Among adults over 30, the percentage reporting difficulty did not vary significantly by age. Those with health problems were somewhat more likely to report having difficulty obtaining affordable health insurance (77% versus 64% of those in good health).\n\nSome American states have made medical underwriting illegal as a prerequisite for health coverage, which means anyone who asks for health insurance and pays for it will get it. States that have outlawed medical underwriting include New York, New Jersey, Maine, Massachusetts, and Vermont, which also have the highest premiums for individual health insurance.\n\nPrior to the passage of the Affordable Care Act in 2010, health insurance was primarily regulated by the states. Some states mandated individual health insurance policies as \"guaranteed renewable:\" once a policy had been issued, the policyholder could keep it forever regardless of medical conditions as long as the required premiums were paid. There had been instances in which insurers increased premiums at annual renewals based on an individual's claim history or changes in their health status. That was possible when coverage was marketed to individuals by discretionary group trusts, escaping some states' rules governing the individual health insurance market. The insurer that was first identified by \"The Wall Street Journal\" as reunderwriting policyholders has since publicly stated it will discontinue the practice.\n\nHowever, in most cases, an insurer's ability to \"re-underwrite\" an existing guaranteed renewable policy is limited by contract provisions and the Affordable Care Act (previously by state law). Even so, premiums fluctuated significantly for existing policies if the average health of the policyholders with a particular product deteriorated, as often happened when rising premiums drove healthier individuals (who were able to buy other policies on more favorable terms) out of the product, leaving those who were relatively less healthy. One factor that drove that is the increase in costs, as individuals who initially pass underwriting develop health problems. In general, claim costs rose significantly over the first five years that an individual health insurance policy is in force.\n\nSeveral solutions were proposed for the \"closed block\" problem, including requiring insurers to \"pre-fund\" for cost increases over the lifetime of a product, providing cross-subsidies between blocks of products by pooling products across durations, providing cross-subsidies by placing limits on the allowed variation in premiums between products, or creating state-sponsored risk pools for individuals trapped in a closed block. The American Academy of Actuaries performed a study of the proposed solutions for the National Association of Insurance Commissioners and modeled the likely impact of each. All of the solutions would increase the initial cost of a new policy and reduce cost increases over time.\n\nInsurers have the right to cancel individually purchased insurance if the insurer finds that the applicant provided incomplete or inaccurate information on the application, thereby affecting the medical underwriting process. The practice, called rescission, protects insurers from intentional fraud and affects only about 1% of individual policyholders but appears to be on the increase. Rescission practices by several large insurers have attracted media attention, class-action lawsuits, and regulatory attention in several states. In 2007, California passed legislation to tighten the rules governing rescissions. In December 2007, a California appeals court ruled that a health insurer could not rescind coverage without showing that either the policyholder willfully misrepresented health or that the insurer had investigated the application before issuing coverage.\n\nA distinction between underwriting of individually purchased life insurance and the underwriting of health insurance is generally recognized in US state-specific regulation of insurance. The general legal posture is for states to view life insurance as less of a necessity than health coverage.\n\nMoratorium underwriting is an alternative method of health insurance which primarily allows for applicants to receive cover without disclosing their entire medical history. Instead, individuals will typically have any pre-existing medical conditions excluded if those have developed within the past five years. If related symptoms occur within a set period of time, then this will affect the final policy.\n\nMoratorium underwriting is, therefore, best suited for healthy individuals who don’t foresee any medical difficulties developing.\n\n"}
{"id": "1652651", "url": "https://en.wikipedia.org/wiki?curid=1652651", "title": "Michael Aschbacher", "text": "Michael Aschbacher\n\nMichael George Aschbacher (born April 8, 1944) is an American mathematician best known for his work on finite groups. He was a leading figure in the completion of the classification of finite simple groups in the 1970s and 1980s. It later turned out that the classification was incomplete, because the case of quasithin groups had not been finished. This gap was fixed by Aschbacher and Stephen D. Smith in 2004, in a pair of books comprising about 1300 pages. Aschbacher is currently the Shaler Arthur Hanisch Professor of Mathematics at the California Institute of Technology.\n\nAschbacher received his B.S. at the California Institute of Technology in 1966 and his Ph.D. at the University of Wisconsin–Madison in 1969. He joined the faculty of the California Institute of Technology in 1970 and became a full professor in 1976. He was a visiting scholar at the Institute for Advanced Study in 1978-79. He was awarded the Cole Prize in 1980, and was elected to the National Academy of Sciences in 1990. In 1992, Aschbacher was elected a Fellow of the American Academy of Arts and Sciences. He was awarded the Rolf Schock Prize for Mathematics by the Royal Swedish Academy of Sciences in 2011. In 2012 he received the Leroy P. Steele Prize for Mathematical Exposition and the Wolf Prize in Mathematics, and became a fellow of the American Mathematical Society.\n\nIn 1973, Aschbacher became a leading figure in the classification of finite simple groups. Aschbacher considered himself somewhat of an outsider in the world of conventional group theory, claiming that he was not \"plugged into the system at that point in time.\" \nAlthough he had access to several preprints that were shared among the practitioners of the field, he reproduced many proofs that had already been discovered by other researchers and published them in his early papers. Aschbacher only became interested in finite simple groups as a postdoctorate. He wrote his dissertation in combinatorics and was able to utilize many techniques developed in this area to make early contributions to the study of finite simple groups which surprised the community of researchers. In particular, Daniel Gorenstein, another leader of the classification of finite simple groups, said that Aschbacher's entrance was \"dramatic.\" \n\nIn fact, the rate of Aschbacher's results proved so astounding that many other mathematicians decided to leave the field to pursue other problems. Aschbacher was proving one major result after another and when he announced his progress at the Duluth conference, mathematicians were convinced that the problem was almost solved. This conference represented a turning point for the problem as many mathematicians (in particular those relatively new to the field) decided to leave the field to pursue other problems.\n\nHowever, Aschbacher's entrance into the field did not come without difficulties. Aschbacher's papers, beginning with the first he wrote in the field for publication, were very difficult to read. Some commented that his proofs lacked explanations of very sophisticated counting arguments. As Aschbacher's proofs became longer, it became even more difficult for others to understand his proofs. Even some of his own coauthors had trouble reading their own papers. From that point on, researchers no longer read papers as independent documents, but rather ones that required the context of its author. As a result, responsibility of finding errors in the classification problem was up to the entire community of researchers rather than just peer-reviewers alone. That Aschbacher's proofs were hard to read was not due to a lack of ability, but rather to the astounding complexity of the ideas he was able to produce.\n\n\n"}
{"id": "42992981", "url": "https://en.wikipedia.org/wiki?curid=42992981", "title": "Module of covariants", "text": "Module of covariants\n\nIn algebra, given an algebraic group \"G\", a \"G\"-module \"M\" and a \"G\"-algebra \"A\", all over a field \"k\", the module of covariants of type \"M\" is the formula_1-module\n\n\n"}
{"id": "52299655", "url": "https://en.wikipedia.org/wiki?curid=52299655", "title": "Moser–de Bruijn sequence", "text": "Moser–de Bruijn sequence\n\nIn number theory, the Moser–de Bruijn sequence is an integer sequence named after Leo Moser and Nicolaas Govert de Bruijn, consisting of the sums of distinct powers of 4. It begins\nFor instance, 69 belongs to this sequence because it equals 64 + 4 + 1, a sum of three distinct powers of 4.\n\nAnother definition of the Moser–de Bruijn sequence is that it is the ordered sequence of numbers whose binary representation has nonzero digits only in the even positions. For instance, 69 belongs to the sequence, because its binary representation 1000101 has nonzero digits in the positions for 2, 2, and 2, all of which have even exponents. The numbers in the sequence can also be described as the numbers whose base-4 representation uses only the digits 0 or 1. For a number in this sequence, the base-4 representation can be found from the binary representation by skipping the binary digits in odd positions, which should all be zero. Another way of looking at it is that these are numbers whose hexadecimal representation contains only the digits 0, 1, 4, 5. For instance, 69 = 1011 = 45.\n\nEquivalently, they are the numbers whose binary and negabinary representations are equal.\nIt follows from either the binary or base-4 definitions of these numbers that they grow roughly in proportion to the square numbers. The number of elements in the Moser–de Bruijn sequence that are below any given threshold formula_1 is proportional to formula_2,\na fact which is also true of the square numbers. In fact the numbers in the Moser–de Bruijn sequence are the squares for a version of arithmetic without carrying on binary numbers, in which the addition and multiplication of single bits are respectively the exclusive or and logical conjunction operations.\n\nIn connection with the Furstenberg–Sárközy theorem on sequences of numbers with no square difference, Imre Z. Ruzsa found a construction for large square-difference-free sets that, like the binary definition of the Moser–de Bruijn sequence, restricts the digits in alternating positions in the base-formula_3 numbers. When applied to the base formula_4, Ruzsa's construction generates the Moser–de Bruijn sequence multiplied by two, a set that is again square-difference-free. However,\nthis set is too sparse to provide nontrivial lower bounds for the Furstenberg–Sárközy theorem.\n\nThe Moser–de Bruijn sequence obeys a property similar to that of a Sidon sequence: the sums formula_5, where formula_6 and formula_7 both belong to the Moser–de Bruijn sequence, are all unique. No two of these sums have the same value. Moreover, every integer formula_1 can be represented as a sum formula_5, where formula_6 and formula_7 both belong to the Moser–de Bruijn sequence. To find the sum that represents formula_1, compute formula_13, the bitwise Boolean and of formula_1 with a binary value (expressed here in hexadecimal) that has ones in all of its even positions, and set formula_15.\n\nThe Moser–de Bruijn sequence is the only sequence with this property, that all integers have a unique expression as formula_5. It is for this reason that the sequence was originally studied by . Extending the property, found infinitely many other linear expressions like formula_5 that, when formula_6 and formula_7 both belong to the Moser–de Bruijn sequence, uniquely represent all integers.\n\nDecomposing a number formula_1 into formula_21, and then applying to formula_6 and formula_7 an order-preserving map from the Moser–de Bruijn sequence to the integers (by replacing the powers of four in each number by the corresponding powers of two) gives a bijection from non-negative integers to ordered pairs of non-negative integers. The inverse of this bijection gives a linear ordering on the points in the plane with non-negative integer coordinates, which may be used to define the Z-order curve.\n\nIn connection with this application, it is convenient to have a formula to generate each successive element of the Moser–de Bruijn sequence from its predecessor.\nThis can be done as follows. If formula_6 is an element of the sequence, then the next member after formula_6 can be obtained by filling in the bits in odd positions of the binary representation of formula_6 by ones, adding one to the result,\nand then masking off the filled-in bits. Filling the bits and adding one can be combined into a single addition operation. That is, the next member is the number given by the formula\nThe two hexadecimal constants appearing in this formula can be interpreted as the 2-adic numbers formula_28 and formula_29, respectively.\n\n investigated a game, analogous to subtract a square, based on this sequence. In Golomb's game, two players take turns removing coins from a pile of formula_1 coins. In each move, a player may remove any number of coins that belongs to the Moser–de Bruijn sequence. Removing any other number of coins is not allowed. The winner is the player who removes the last coin.\nAs Golomb observes, the \"cold\" positions of this game (the ones in which the player who is about to move is losing) are exactly the positions of the form formula_31 where formula_7 belongs to the Moser–de Bruijn sequence. A winning strategy for playing this game is to decompose the current number of coins, formula_1, into formula_5 where formula_6 and formula_7 both belong to the Moser–de Bruijn sequence, and then (if formula_6 is nonzero) to remove formula_6 coins, leaving a cold position to the other player. If formula_6 is zero, this strategy is not possible, and there is no winning move.\n\nThe Moser–de Bruijn sequence forms the basis of an example of an irrational number formula_6 with the unusual property that the decimal representations of formula_6 and formula_42 can both be written simply and explicitly. Let formula_43 denote the Moser–de Bruijn sequence itself, and formula_44 denote the numbers in this sequence multiplied by two: formula_45. Then for\na decimal number whose nonzero digits are in the positions given by the Moser–de Bruijn sequence, it follows that the nonzero digits of its reciprocal are in the positions given by formula_44:\n\nSimilar examples also work in other bases. For instance, the two binary numbers whose nonzero bits are in the same positions as the nonzero digits of the two decimal numbers above are also irrational reciprocals. These binary and decimal numbers, and the numbers defined in the same way for any other base by repeating a single nonzero digit in the positions given by the Moser–de Bruijn sequence, are transcendental numbers. Their transcendence can be proven from the fact that the long strings of zeros in their digits allow them to be approximated more accurately by rational numbers than would be allowed by Roth's theorem if they were algebraic numbers.\n\nThe generating function\nwhose exponents in the expanded form are given by the Moser–de Bruijn sequence,\nobeys the functional equations\nand\nFor example, this function can be used to describe the two decimal reciprocals given above: one is formula_52 and the other is formula_53. The fact that they are reciprocal is an instance of the first of the two functional equations.\nThe partial products of the product form of the generating function can be used to generate the convergents of the continued fraction expansion of these numbers.\n\nThe Moser–de Bruijn sequence obeys a recurrence relation that allows the th value of the sequence, formula_54 (starting at formula_55) to be determined from the value at position formula_56:\nIterating this recurrence allows any subsequence of the form formula_59 to be expressed as a linear function of the original sequence, meaning that the Moser–de Bruijn sequence is a 2-regular sequence.\n\n"}
{"id": "7201629", "url": "https://en.wikipedia.org/wiki?curid=7201629", "title": "Noneism", "text": "Noneism\n\nNoneism, also known as modal Meinongianism, is a theory in logic and metaphysics first coined by Richard Routley and appropriated again in 2005 by Graham Priest.\n\nNoneism holds that some things do not exist. That is, we can quantify over non-existent things using the so-called particular quantifier (also known—misleadingly in the view of noneists—as the existential quantifier). They also hold that \"there is\" is like \"exist\", rather than like the particular quantifier. Thus, they deny that \"there are\" things that do not exist. On this theory, there are no empty names, wherefore the \"problem of empty names\" that afflicts many theories about names (in particular, Millianism), is not a problem at all.\n\nWhile Priest also espouses dialetheism, he maintains that his dialetheism is mostly capable of being separated out from his noneism. The connection is that impossible objects may exist in impossible worlds, much as nonexistent objects may exist in possible (but not actual) worlds.\n\nRoutley's book, \"Exploring Meinong's Jungle and Beyond: An Investigation of Noneism and the Theory of Items\", was published in 1980, while Priest's 2005 book is entitled \"Towards Non-Being: The Logic and Metaphysics of Intentionality\".\n\n"}
{"id": "6172530", "url": "https://en.wikipedia.org/wiki?curid=6172530", "title": "Prefuse", "text": "Prefuse\n\nPrefuse is a Java-based toolkit for building interactive information visualization applications. It supports a rich set of features for data modeling, visualization and interaction. It provides optimized data structures for tables, graphs, and trees, a host of layout and visual encoding techniques, and support for animation, dynamic queries, integrated search, and database connectivity. \n\nPrefuse uses the Java 2D graphics library, and is easily integrated into Swing applications or Java applets. Prefuse is licensed under the terms of a BSD license, and can be used freely for commercial and non-commercial purposes.\n\nPrefuse is a Java-based extensible software framework for creating interactive information visualization applications. It can be used to build standalone applications, visual components and Java applets. Prefuse intends to simplify the processes of visualizing, handling and mapping of data, as well as user interaction.\n\nSome of Prefuse's features include:\n\n(and perhaps most importantly)\n\nPrefuse has been used in school course projects, academic and industrial research, and commercial software development.\n\nThe design of the prefuse toolkit is based upon the information visualization reference model, a software architecture pattern that breaks up the visualization process into a series of discrete steps. \"Prefuse: a toolkit for interactive information visualization\" provides more details on implementation and evaluation.\n\nThe information visualization reference model was developed in the Ph.D. thesis work of Ed Chi, under the name of the data state model. Chi showed that the framework successfully modeled a wide array of visualization applications. Later, Chi's work showed that the model was functionally equivalent to the data flow model used in existing graphics toolkits such as VTK. In their work, \"Readings in Information Visualization: Using Vision to Think\", Stuart K. Card, Jock D. Mackinlay, and Ben Shneiderman present their own interpretation of this pattern, dubbing it the \"information visualization reference model\".\n\n"}
{"id": "972441", "url": "https://en.wikipedia.org/wiki?curid=972441", "title": "Principal branch", "text": "Principal branch\n\nIn mathematics, a principal branch is a function which selects one branch (\"slice\") of a multi-valued function. Most often, this applies to functions defined on the complex plane.\n\nPrincipal branches are used in the definition of many inverse trigonometric functions, such as the selection either to define that\nor that\n\nA more familiar principal branch function, limited to real numbers, is that of a positive real number raised to the power of .\n\nFor example, take the relation , where is any positive real number.\n\nThis relation can be satisfied by any value of equal to a square root of (either positive or negative). By convention, is used to denote the positive square root of .\n\nIn this instance, the positive square root function is taken as the principal branch of the multi-valued relation .\n\nOne way to view a principal branch is to look specifically at the exponential function, and the logarithm, as it is defined in complex analysis.\n\nThe exponential function is single-valued, where is defined as:\n\nwhere formula_4.\n\nHowever, the periodic nature of the trigonometric functions involved makes it clear that the logarithm is not so uniquely determined. One way to see this is to look at the following:\n\nand\n\nwhere is any integer and continues the values of the -function from their principal value range formula_7, corresponding to formula_8 into the principal value range of the -function formula_9, covering all four quadrants in the complex plane.\n\nAny number defined by such criteria has the property that .\n\nIn this manner log function is a multi-valued function (often referred to as a \"multifunction\" in the context of complex analysis). A branch cut, usually along the negative real axis, can limit the imaginary part so it lies between and . These are the chosen principal values.\n\nThis is the principal branch of the log function. Often it is defined using a capital letter, .\n\n\n"}
{"id": "35066763", "url": "https://en.wikipedia.org/wiki?curid=35066763", "title": "Proof compression", "text": "Proof compression\n\nIn proof theory, an area of mathematical logic, proof compression is the problem of algorithmically compressing formal proofs. The developed algorithms can be used to improve the proofs generated by automated theorem proving tools such as sat-solvers, SMT-solvers, first-order theorem provers and proof assistants.\nIn propositional logic a resolution proof of a clause formula_1 from a set of clauses C is a directed acyclic graph (DAG): the input nodes are axiom inferences (without premises) whose conclusions are elements of C, the resolvent nodes are resolution inferences, and the proof has a node with conclusion formula_1.\n\nThe DAG contains an edge from a node formula_3 to a node formula_4 if and only if a premise of formula_3 is the conclusion of formula_4. In this case, formula_3 is a child of formula_4, and formula_4 is a parent of formula_3. A node with no children is a root.\n\nA proof compression algorithm will try to create a new DAG with fewer nodes that represents a valid proof of formula_1 or, in some cases, a valid proof of a subset of formula_1.\n\nLet's take a resolution proof for the clause formula_13 from the set of clauses\n\nHere we can see:\n\nA (resolution) refutation of C is a resolution proof of formula_34 from C. It is a common that given a node formula_35, to refer to the clause formula_35 or formula_35’s clause meaning the conclusion clause of formula_35, and (sub)proof formula_35 meaning the (sub)proof having formula_35 as its only root.\n\nIn some works it can be found an algebraic representation of a resolution inference. The resolvent of formula_41 and formula_42 with pivot formula_18 can be denoted as formula_44. When the pivot is uniquely defined or irrelevant, we omit it and write simply formula_45. In this way, the set of clauses can be seen as an algebra with a commutative operator; and terms in the corresponding term algebra denote resolution proofs in a notation style that is more compact and more convenient for describing resolution proofs than the usual graph notation.\n\nIn our last example the notation of the DAG would be formula_46 or simply formula_47\n\nWe can identify formula_48\n\nAlgorithms for compression of sequent calculus proofs include Cut-introduction and Cut-elimination.\n\nAlgorithms for compression of propositional resolution proofs include \nRecycleUnits,\nRecyclePivots,\nRecyclePivotsWithIntersection,\nLowerUnits,\nLowerUnivalents,\nSplit,\nReduce&Reconstruct, and Subsumption.\n"}
{"id": "6250799", "url": "https://en.wikipedia.org/wiki?curid=6250799", "title": "Proper forcing axiom", "text": "Proper forcing axiom\n\nIn the mathematical field of set theory, the proper forcing axiom (\"PFA\") is a significant strengthening of Martin's axiom, where forcings with the countable chain condition (ccc) are replaced by proper forcings.\n\nA forcing or partially ordered set P is proper if for all regular uncountable cardinals formula_1, forcing with P preserves stationary subsets of formula_2.\n\nThe proper forcing axiom asserts that if P is proper and D is a dense subset of P for each α<ω, then there is a filter G formula_3 P such that D ∩ G is nonempty for all α<ω.\n\nThe class of proper forcings, to which PFA can be applied, is rather large. For example, standard arguments show that if P is ccc or ω-closed, then P is proper. If P is a countable support iteration of proper forcings, then P is proper. Crucially, all proper forcings preserve formula_4.\n\nPFA directly implies its version for ccc forcings, Martin's axiom. In cardinal arithmetic, PFA implies formula_5. PFA implies any two formula_6-dense subsets of R are isomorphic, any two Aronszajn trees are club-isomorphic, and every automorphism of the Boolean algebra formula_7/fin is trivial. PFA implies that the Singular Cardinals Hypothesis holds. An especially notable consequence proved by John R. Steel is that the axiom of determinacy holds in L(R), the smallest inner model containing the real numbers. Another consequence is the failure of square principles and hence existence of inner models with many Woodin cardinals.\n\nIf there is a supercompact cardinal, then there is a model of set theory in which PFA holds. The proof uses the fact that proper forcings are preserved under countable support iteration, and the fact that if formula_8 is supercompact, then there exists a Laver function for formula_8.\n\nIt is not yet known how much large cardinal strength comes from PFA.\n\nThe bounded proper forcing axiom (BPFA) is a weaker variant of PFA which instead of arbitrary dense subsets applies only to maximal antichains of size ω. Martin's maximum is the strongest possible version of a forcing axiom.\n\nForcing axioms are viable candidates for extending the axioms of set theory as an alternative to large cardinal axioms.\n\nThe Fundamental Theorem of Proper Forcing, due to Shelah, states that any countable support iteration of proper forcings is itself proper. This follows from the Proper Iteration Lemma, which states that whenever formula_10 is a countable support forcing iteration based on formula_11 and formula_12 is a countable elementary substructure of formula_13 for a sufficiently large regular cardinal formula_14, and formula_15 and formula_16 and formula_17 is formula_18-generic and formula_17 forces \"formula_20,\" then there exists formula_21 such that formula_22 is formula_12-generic and the restriction of formula_22 to formula_25 equals formula_17 and formula_17 forces the restriction of formula_22 to formula_29 to be stronger or equal to formula_30.\n\nThis version of the Proper Iteration Lemma, in which the name formula_30 is not assumed to be in formula_32, is due to Schlindwein.\n\nThe Proper Iteration Lemma is proved by a fairly straightforward induction on formula_33, and the Fundamental Theorem of Proper Forcing follows by taking formula_34.\n\n\n"}
{"id": "3578575", "url": "https://en.wikipedia.org/wiki?curid=3578575", "title": "Randomization function", "text": "Randomization function\n\nIn computer science, a randomization function or randomizing function is an algorithm or procedure that implements a randomly chosen function between two specific sets, suitable for use in a randomized algorithm.\n\nRandomizing functions are related to random number generators and hash functions, but have somewhat different requirements and uses, and often need specific algorithms.\n\nRandomizing functions are used to turn algorithms that have good expected performance for \"random\" inputs, into algorithms that have the same performance for \"any\" input.\n\nFor example, consider a sorting algorithm like quicksort, which has small expected running time when the input items are presented in random order, but is very slow when they are presented in certain unfavorable orders. A randomizing function from the integers 1 to \"n\" to the integers 1 to \"n\" can be used to rerrange the \"n\" input items in \"random\" order, before calling that algorithm. This modified (randomized) algorithm will have small expected running time, whatever the input order.\n\nIn theory, randomization functions are assumed to be truly random, and yield an unpredictably different function every time the algorithm is executed. The randomization technique would not work if, at every execution of the algorithm, the randomization function always performed the same mapping, or a mapping entirely determined by some externally observable parameter (such as the program's startup time). With such a \"pseudo-randomization\" function, one could in principle construct a sequence of calls such that the function would always yield a \"bad\" case for the underlying deterministic algorithm. For that sequence of calls, the average cost would be closer to the worst-case cost, rather than the average cost for random inputs.\n\nIn practice, however, the main concern is that some \"bad\" cases for the deterministic algorithm may occur in practice much more often than it would be predicted by chance. For example, in a naive variant of quicksort, the worst case is when the input items are already sorted — which is a very common occurrence in many applications. For such algorithms, even a fixed pseudo-random permutation may be good enough. Even though the resulting \"pseudo-randomized\" algorithm would still have as many \"bad\" cases as the original, they will be certain peculiar orders that would be quite unlikely to arise in real applications. So, in practice one often uses randomization functions that are derived from pseudo-random number generators, preferably seeded with external \"random\" data such as the program's startup time.\n\nThe uniformity requirements for a randomizing function are usually much weaker than those of hash functions and pseudo-random generators. The minimum requirement is that it maps any input of the deterministic algorithm into a \"good\" input with a sufficiently high probability. (However, analysis is usually simpler if the randomizing function implements each possible mapping with uniform probability.)\n"}
{"id": "28345620", "url": "https://en.wikipedia.org/wiki?curid=28345620", "title": "Realized kernel", "text": "Realized kernel\n\nThe realized kernel (RK) is an estimator of volatility. The estimator is typically computed with high frequency return data, such as second-by-second returns. Unlike the realized variance, the realized kernel is a robust estimator of volatility, in the sense that the realized kernel estimates the appropriate volatility quantity, even when the returns are contaminated with noise.\n"}
{"id": "55831372", "url": "https://en.wikipedia.org/wiki?curid=55831372", "title": "Sally Elizabeth Carlson", "text": "Sally Elizabeth Carlson\n\nSally Elizabeth Carlson (October 2, 1896 – November 1, 2000) was an American mathematician, the first woman and one of the first two people to obtain a doctorate in mathematics from the University of Minnesota.\n\nCarlson was born in Minneapolis to a large working-class family of Swedish immigrants. She became her high school valedictorian in 1913, graduated from the University of Minnesota in 1917, and earned a master's degree there in 1918. After teaching mathematics for two years, she returned to graduate study in 1920, and completed her Ph.D. at Minnesota in 1924. Both students were supervised by Dunham Jackson; Carlson's dissertation, in functional analysis, was \"On The Convergence of Certain Methods of Closest Approximation\".\n\nShe joined the Minnesota faculty, and remained there until her retirement in 1965 as a full professor.\nShe has no record of supervising doctoral dissertations,\nand published little research after the work of her own dissertation.\nHowever, she supervised several master's students, won a Distinguished Teacher Award,\nand was described as a mentor by Margaret P. Martin, who completed her Ph.D. at Minnesota in 1944.\n\nAfter her 2000 death, the library of the University of Minnesota memorialized her in an exhibit, titled \"Elizabeth Carlson, notable alumna\".\n"}
{"id": "44218028", "url": "https://en.wikipedia.org/wiki?curid=44218028", "title": "Schröder–Bernstein theorem", "text": "Schröder–Bernstein theorem\n\nIn set theory, the Schröder–Bernstein theorem states that, if there exist injective functions and between the sets and , then there exists a bijective function . In terms of the cardinality of the two sets, this means that if and , then ; that is, and are equipotent. This is a useful feature in the ordering of cardinal numbers.\n\nThis theorem does not rely on the axiom of choice. However, its various proofs are non-constructive, as they depend on the law of excluded middle, and are therefore rejected by intuitionists.\n\nThe theorem is named after Felix Bernstein and Ernst Schröder. It is also known as Cantor–Bernstein theorem, or Cantor–Schröder–Bernstein, after Georg Cantor who first published it without proof.\n\nThe following proof is attributed to Julius König.\n\nAssume without loss of generality that \"A\" and \"B\" are disjoint. For any \"a\" in \"A\" or \"b\" in \"B\" we can form a unique two-sided sequence of elements that are alternately in \"A\" and \"B\", by repeatedly applying formula_1 and formula_2 to go from \"A\" to \"B\" and formula_3 and formula_4 to go from \"B\" to \"A\" (where defined).\n\nFor any particular \"a\", this sequence may terminate to the left or not, at a point where formula_4 or formula_2 is not defined.\n\nBy the fact that formula_1 and formula_3 are injective functions, each \"a\" in \"A\" and \"b\" in \"B\" is in exactly one such sequence to within identity: if an element occurs in two sequences, all elements to the left and to the right must be the same in both, by the definition of the sequences. Therefore, the sequences form a partition of the (disjoint) union of \"A\" and \"B\". Hence it suffices to produce a bijection between the elements of \"A\" and \"B\" in each of the sequences separately, as follows:\n\nCall a sequence an \"A-stopper\" if it stops at an element of \"A\", or a \"B-stopper\" if it stops at an element of \"B\". Otherwise, call it \"doubly infinite\" if all the elements are distinct or \"cyclic\" if it repeats. See the picture for examples.\n\nAn earlier proof by Cantor relied, in effect, on the axiom of choice by inferring the result as a corollary of the well-ordering theorem. The argument given above shows that the result can be proved without using the axiom of choice. Note however that the principle of excluded middle is used to do the analysis into cases, so this proof does not work in non-classical logic. \n\nThere is also a proof which uses Tarski's fixed point theorem.\n\nThe traditional name \"Schröder–Bernstein\" is based on two proofs published independently in 1898.\nCantor is often added because he first stated the theorem in 1887,\nwhile Schröder's name is often omitted because his proof turned out to be flawed\nwhile the name of Richard Dedekind, who first proved it, is not connected with the theorem.\nAccording to Bernstein, Cantor had suggested the name \"equivalence theorem\" (Äquivalenzsatz).\n\n\nBoth proofs of Dedekind are based on his famous memoir \"Was sind und was sollen die Zahlen?\" and derive it as a corollary of a proposition equivalent to statement C in Cantor's paper, which reads \"A\" ⊆ \"B\" ⊆ \"C\" and |\"A\"| = |\"C\"| implies |\"A\"| = |\"B\"| = |\"C\"|. Cantor observed this property as early as 1882/83 during his studies in set theory and transfinite numbers and was therefore (implicitly) relying on the Axiom of Choice.\n\nStatement C is the special case of the Schröder–Bernstein theorem where the second function \"g\" is the identity (and, hence, the second set \"B\" is a subset of \"A\"). \n\nIt is easy to see that statement C implies the general form of the theorem (with \"g\" and \"B\" arbitrary):\n\nAssume that \"ƒ\" injects \"A\" into \"B\" and \"g\" injects \"B\" into \"A\". Then their composition \"gf\" injects \"A\" into \"g\"[\"B\"]. However, \"g\"[\"B\"] is a subset of \"A\". Thus, from statement \"C\", we obtain that \"A\" and \"g\"[\"B\"] are equipotent. Obviously, \"B\" and \"g\"[\"B\"] are equipotent as well. It follows, that \"A\" and \"B\" are equipotent.\n\nA proof of statement \"C\" can be obtained by translating the König proof for the general case above to the present situation. The result becomes much more transparent than the original, and looks as follows.\n\nAssume that \"ƒ\" injects \"A\" into its subset \"C\". Consider the subset \"D\" of \"A\" (the set of \"A\"-stoppers, in the König terminology above) which is the union of the infinitely many sets \"A\" − \"C\", \"ƒ\"[\"A\" − \"C\"], \"ƒƒ\"[\"A\" − \"C\"]], ...\n\nConsider the function from \"A\" to \"C\" that (i) maps elements \"a\" of \"D\" to their \"ƒ\"-image \"ƒ\"(\"a\"), whereas (ii) on \"A\" − \"D\" it acts as the identity, mapping elements to themselves.\n\nWe claim that this function is a bijection from \"A\" onto \"C\". For this, it should be verified that every element of \"C\" has exactly one original. However, if it belongs to \"D\", it has one original which also is an \"f\"-original, and if it doesn't belong to \"D\" the only original is the element itself.\n\n\n\n"}
{"id": "218596", "url": "https://en.wikipedia.org/wiki?curid=218596", "title": "Sheffer sequence", "text": "Sheffer sequence\n\nIn mathematics, a Sheffer sequence or poweroid is a polynomial sequence, i.e., a sequence { \"p\"(\"x\") : \"n\" = 0, 1, 2, 3, . . . } of polynomials in which the index of each polynomial equals its degree, satisfying conditions related to the umbral calculus in combinatorics. They are named for Isador M. Sheffer.\n\nFix a polynomial sequence \"p\". Define a linear operator \"Q\" on polynomials in \"x\" by\n\nThis determines \"Q\" on all polynomials. The polynomial sequence \"p\" is a \"Sheffer sequence\" if the linear operator \"Q\" just defined is \"shift-equivariant\". Here, we define a linear operator \"Q\" on polynomials to be \"shift-equivariant\" if, whenever \"f\"(\"x\") = \"g\"(\"x\" + \"a\") = \"T\" \"g\"(\"x\") is a \"shift\" of \"g\"(\"x\"), then (\"Qf\")(\"x\") = (\"Qg\")(\"x\" + \"a\"); i.e., \"Q\" commutes with every shift operator: \"T\"\"Q\" =\"QT\". Such a \"Q\" is a delta operator.\n\nThe set of all Sheffer sequences is a group under the operation of umbral composition of polynomial sequences, defined as follows. Suppose { \"p\"(x) : \"n\" = 0, 1, 2, 3, ... } and { \"q\"(x) : \"n\" = 0, 1, 2, 3, ... } are polynomial sequences, given by\n\nThen the umbral composition formula_3 is the polynomial sequence whose \"n\"th term is\n\n(the subscript \"n\" appears in \"p\", since this is the \"n\" term of that sequence, but not in \"q\", since this refers to the sequence as a whole rather than one of its terms).\n\nThe neutral element of this group is the standard monomial basis\n\nTwo important subgroups are the group of Appell sequences, which are those sequences for which the operator \"Q\" is mere differentiation, and the group of sequences of binomial type, which are those that satisfy the identity\nA Sheffer sequence { \"p\"(\"x\") : \"n\" = 0, 1, 2, . . . } is of binomial type if and only if both\n\nand\n\nThe group of Appell sequences is abelian; the group of sequences of binomial type is not. The group of Appell sequences is a normal subgroup; the group of sequences of binomial type is not. The group of Sheffer sequences is a semidirect product of the group of Appell sequences and the group of sequences of binomial type. It follows that each coset of the group of Appell sequences contains exactly one sequence of binomial type. Two Sheffer sequences are in the same such coset if and only if the operator \"Q\" described above – called the \"delta operator\" of that sequence – is the same linear operator in both cases. (Generally, a \"delta operator\" is a shift-equivariant linear operator on polynomials that reduces degree by one. The term is due to F. Hildebrandt.)\n\nIf \"s\"(\"x\") is a Sheffer sequence and \"p\"(\"x\") is the one sequence of binomial type that shares the same delta operator, then\n\nSometimes the term \"Sheffer sequence\" is \"defined\" to mean a sequence that bears this relation to some sequence of binomial type. In particular, if { \"s\"(\"x\") } is an Appell sequence, then\n\nThe sequence of Hermite polynomials, the sequence of Bernoulli polynomials, and the monomials { \"x\" : \"n\" = 0, 1, 2, ... } are examples of Appell sequences.\n\nA Sheffer sequence \"p\" is characterised by its exponential generating function\n\nwhere \"A\" and \"B\" are (formal) power series in \"t\". Sheffer sequences are thus examples of generalized Appell polynomials and hence have an associated recurrence relation.\n\nExamples of polynomial sequences which are Sheffer sequences include:\n\n"}
{"id": "17533891", "url": "https://en.wikipedia.org/wiki?curid=17533891", "title": "Virtual manipulatives for mathematics", "text": "Virtual manipulatives for mathematics\n\nIn mathematics education, virtual manipulatives are a relatively new technology modeled after existing manipulatives such as base ten blocks, coins, blocks, tangrams, rulers, fraction bars, algebra tiles, geoboards, geometric plane, and solids figures. They are usually in the form of Java or Flash applets. Virtual manipulatives allow teachers to allow for efficient use of multiple representations and to provide concrete models of abstract mathematical concepts for learners of mathematics. Research suggests that students may also develop more connected understandings of mathematical concepts when they use virtual manipulatives (Moyer, Niezgoda, & Stanley, 2005).\n\nMany believe that virtual manipulatives can be particularly helpful to students with language difficulties, including English Language Learners (ELL). ELL students usually have trouble explaining what they are learning in mathematics classes. With virtual manipulatives, such students may be able to clarify their thoughts and demonstrate it to others in a much more effective way. For example, with base ten blocks, students may use the place-value layout to show their understanding.\n\nManipulatives by themselves have little meaning. It is important for teachers to make the mathematical meaning of manipulatives clear and help the students to build connections between the concrete materials and abstract symbols. Virtual manipulatives usually have this built-in structure. Many virtual manipulative activities give students hints and feedback with pop-ups and help features. More traditional concrete manipulatives are not conducive to comprehension without direct instructor assistance. For example, in using tangrams, students can practically copy a design made from pattern blocks. When a block is near a correct location, it will snap into place. This virtual manipulative includes a hint function that will show the correct location of all the blocks.\n\nAlthough relatively new, virtual manipulatives can support learning mathematics for all students which includes those with learning disabilities and ELL learners. Virtual manipulatives can be included into the general academic curriculum and not just used as an extra student activity. If they are used wisely, virtual manipulatives can provide students with opportunities for guided discovery which can help them to build a better understanding of mathematical concepts and ultimately exhibit measurable learning skills.\n\nWolfram Demonstrations Project\n\nhttp://demonstrations.wolfram.com/\n\nWolfram Demonstrations Project contains around 11,000 Virtual manipulatives for math, science and engineering. They are provided in CDF format together with source code.\n\nDidax Free Manipulatives Library\n\nhttp://www.didax.com/virtual-manipulatives-for-math\n\nDidax is the U.S. branch of Philip & Tacey, Ltd of Hampshire, UK, who developed Unifix₢ Cubes in 1960, a popular math manipulative used throughout the world to teach counting and operations. Unifix cubes were created as a replacement for poppet beads, which rolled off student desks and were expensive to manufacture. The virtual manipulatives in this library are designed to be faithful to their physical counterparts and include minimal navigation or symbolic content.\n\nShodor Interactivate Activities\n\nhttp://www.shodor.org/interactivate/activities/\n\nShodor is a national resource for computational science education. They have offered online education tools such as Interactivate and the Computational Science Education Reference Desk (CSERD) since 1994. The activities are sorted from Grade 3 through Undergraduate.\n\nNational Library of Virtual Manipulatives\n\nhttp://nlvm.usu.edu/\n\nUtah State University has offered this collection of internet-based manipulatives since 1999. The activities are sorted from Pre-Kindergarten through High School.\n\nIlluminations: Activities\n\nhttp://illuminations.nctm.org/Default.aspx\n\nIlluminations has been found on a section of the website for the National Council of Teachers of Mathematics since 2000. Students and teachers from Pre-Kindergarten through High School can use these interactivities.\n\nMSTE at the University of Illinois\n\nAccording to their website, \"Mathematics Materials for Tomorrow's Teachers (M2T2) are a set of mathematics modules created in the spring of 2000 by a team consisting of teachers, administrators, university researchers, mathematicians, graduate students, and members of the Illinois State Board of Education.\" They are five modules. Each module is connected to one of the goals for mathematics in the Illinois Learning Standards. The content is at a middle school level.\n\n\n"}
{"id": "56553748", "url": "https://en.wikipedia.org/wiki?curid=56553748", "title": "Volatility tax", "text": "Volatility tax\n\nThe volatility tax is a mathematical finance term, formalized by hedge fund manager Mark Spitznagel, describing the effect of large investment losses (or volatility) on compound returns. It has also been called “volatility drag”.\n\nAs Spitznagel wrote:\nQuantitatively, the volatility tax is the difference between the arithmetic and geometric average (or “ensemble average” and “time average”) returns of an asset or portfolio. It thus represents the degree of “non-ergodicity” of the geometric average.\n\nStandard quantitative finance assumes that a portfolio’s net asset value changes follow a geometric Brownian motion (and thus are log-normally distributed) with arithmetic average return (or “drift”) formula_1, standard deviation (or “volatility”) formula_2, and geometric average return\n\nSo the geometric average return is the difference between the arithmetic average return and a function of volatility. This function of volatility\n\nrepresents the volatility tax. (Though this is under the assumption of log-normality, the volatility tax is in fact independent of the assumed or actual return distribution.)\n\nThe mathematics behind the volatility tax is such that a very large portfolio loss has a disproportionate impact on the volatility tax that it pays and, as Spitznagel wrote, this is why the most effective risk mitigation focuses on large losses:\nAccording to Spitznagel, the goal of risk mitigation strategies is to solve this “vexing non-ergodicity, volatility tax problem” and thus raise a portfolio’s geometric average return, or CAGR, by lowering its volatility tax (and “narrow the gap between our ensemble and time averages”). This is “the very name of the game in successful investing. It is the key to the kingdom, and explains in a nutshell Warren Buffett’s cardinal rule, ‘Don’t lose money.’” Moreover, “the good news is the entire hedge fund industry basically exists to help with this—to help save on volatility taxes paid by portfolios. The bad news is they haven't done that, not at all.”\n\nAs Nassim Nicholas Taleb wrote in his 2018 book \"Skin in the Game\", “more than two decades ago, practitioners such as Mark Spitznagel and myself built our entire business careers around the effect of the difference between ensemble and time.”\n\n"}
