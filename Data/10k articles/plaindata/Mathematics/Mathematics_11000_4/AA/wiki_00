{"id": "9446798", "url": "https://en.wikipedia.org/wiki?curid=9446798", "title": "Abhyankar's lemma", "text": "Abhyankar's lemma\n\nIn mathematics, Abhyankar's lemma (named after Shreeram Shankar Abhyankar) allows one to kill tame ramification by taking an extension of a base field. \n\nMore precisely, Abhyankar's lemma states that if \"A\", \"B\", \"C\" are local fields such that \"A\" and \"B\" are finite extensions of \"C\", with ramification indices \"a\" and \"b\", and \"B\" is tamely ramified over \"C\" and \"b\" divides \"a\", then the compositum\n\"AB\" is an unramified extension of \"A\".\n\n"}
{"id": "3623060", "url": "https://en.wikipedia.org/wiki?curid=3623060", "title": "Actuarial present value", "text": "Actuarial present value\n\nThe actuarial present value (APV) is the expected value of the present value of a contingent cash flow stream (i.e. a series of payments which may or may not be made). Actuarial present values are typically calculated for the benefit-payment or series of payments associated with life insurance and life annuities. The probability of a future payment is based on assumptions about the person's future mortality which is typically estimated using a life table.\n\nWhole life insurance pays a pre-determined benefit either at or soon after the insured's death. The symbol \"(x)\" is used to denote \"a life aged \"x\"\" where \"x\" is a non-random parameter that is assumed to be greater than zero. The actuarial present value of one unit of whole life insurance issued to \"(x)\" is denoted by the symbol formula_1 or formula_2 in actuarial notation. Let \"G>0\" (the \"age at death\") be the random variable that models the age at which an individual, such as \"(x)\", will die. And let \"T\" (the future lifetime random variable) be the time elapsed between age-\"x\" and whatever age \"(x)\" is at the time the benefit is paid (even though \"(x)\" is most likely dead at that time). Since \"T\" is a function of G and x we will write \"T=T(G,x)\". Finally, let \"Z\" be the present value random variable of a whole life insurance benefit of 1 payable at time \"T\". Then:\n\nwhere \"i\" is the effective annual interest rate and δ is the equivalent force of interest.\n\nTo determine the actuarial present value of the benefit we need to calculate the expected value formula_4 of this random variable \"Z\". Suppose the death benefit is payable at the end of year of death. Then \"T(G, x) := ceiling(G - x)\" is the number of \"whole years\" (rounded upwards) lived by \"(x)\" beyond age \"x\", so that the actuarial present value of one unit of insurance is given by:\n\nwhere formula_6 is the probability that \"(x)\" survives to age \"x+t\", and formula_7 is the probability that \"(x+t)\" dies within one year.\n\nIf the benefit is payable at the moment of death, then \"T(G,x): = G - x\" and the actuarial present value of one unit of whole life insurance is calculated as\n\nwhere formula_9 is the probability density function of \"T\", formula_10 is the probability of a life age formula_11 surviving to age formula_12 and formula_13 denotes force of mortality at time formula_14 for a life aged formula_11.\n\nThe actuarial present value of one unit of an \"n\"-year term insurance policy payable at the moment of death can be found similarly by integrating from 0 to \"n\".\n\nThe actuarial present value of an n year pure endowment insurance benefit of 1 payable after n years if alive, can be found as\n\nIn practice the information available about the random variable \"G\" (and in turn \"T\") may be drawn from life tables, which give figures by year. For example, a three year term life insurance of $100,000 payable at the end of year of death has actuarial present value\n\nFor example, suppose that there is a 90% chance of an individual surviving any given year (i.e. \"T\" has a geometric distribution with parameter \"p = 0.9\" and the set \"{1, 2, 3, ...}\" for its support). Then\n\nand at interest rate 6% the actuarial present value of one unit of the three year term insurance is\n\nso the actuarial present value of the $100,000 insurance is $24,244.85.\n\nIn practice the benefit may be payable at the end of a shorter period than a year, which requires an adjustment of the formula.\n\nThe actuarial present value of a life annuity of 1 per year paid continuously can be found in two ways:\n\nAggregate payment technique (taking the expected value of the total present value):\n\nThis is similar to the method for a life insurance policy. This time the random variable \"Y\" is the total present value random variable of an annuity of 1 per year, issued to a life aged \"x\", paid continuously as long as the person is alive, and is given by:\n\nwhere \"T=T(x)\" is the future lifetime random variable for a person age \"x\". The expected value of \"Y\" is:\n\nCurrent payment technique (taking the total present value of the function of time representing the expected values of payments):\n\nwhere \"F\"(\"t\") is the cumulative distribution function of the random variable \"T\".\n\nThe equivalence follows also from integration by parts.\n\nIn practice life annuities are not paid continuously. If the payments are made at the end of each period the actuarial present value is given by\n\nKeeping the total payment per year equal to 1, the longer the period, the smaller the present value is due to two effects:\n\n\nConversely, for contracts costing an equal lumpsum and having the same internal rate of return, the longer the period between payments, the larger the total payment per year.\n\nThe APV of whole-life insurance can be derived from the APV of a whole-life annuity-due this way:\n\nThis is also commonly written as:\n\nIn the continuous case,\n\nIn the case where the annuity and life insurance are not whole life, one should replace the insurance with an n-year endowment insurance (which can be expressed as the sum of an n-year term insurance and an n-year pure endowment), and the annuity with an n-year annuity due.\n\n\n"}
{"id": "24910076", "url": "https://en.wikipedia.org/wiki?curid=24910076", "title": "Bernstein set", "text": "Bernstein set\n\nIn mathematics, a Bernstein set is a subset of the real line that meets every uncountable closed subset of the real line but that contains none of them.\n\nA Bernstein set partitions the real line into two pieces in a peculiar way: every measurable set of positive measure meets both the Bernstein set and its complement, as does every set with the property of Baire that is not a meagre set.\n"}
{"id": "40012017", "url": "https://en.wikipedia.org/wiki?curid=40012017", "title": "Binade", "text": "Binade\n\nIn software engineering a binade is a term used to describe the set of numbers in a binary IEEE 754 floating-point format that all have the same exponent, i.e., a binade is the interval [2, 2) for some value of n.\n"}
{"id": "5007389", "url": "https://en.wikipedia.org/wiki?curid=5007389", "title": "Bispherical coordinates", "text": "Bispherical coordinates\n\nBispherical coordinates are a three-dimensional orthogonal coordinate system that results from rotating the two-dimensional bipolar coordinate system about the axis that connects the two foci. Thus, the two foci formula_1 and formula_2 in bipolar coordinates remain points (on the formula_3-axis, the axis of rotation) in the bispherical coordinate system.\n\nThe most common definition of bispherical coordinates formula_4 is\n\nwhere the formula_8 coordinate of a point formula_9 equals the angle formula_10 and the formula_11 coordinate equals the natural logarithm of the ratio of the distances formula_12 and formula_13 to the foci\n\nSurfaces of constant formula_8 correspond to intersecting tori of different radii\n\nthat all pass through the foci but are not concentric. The surfaces of constant formula_11 are non-intersecting spheres of different radii\n\nthat surround the foci. The centers of the constant-formula_11 spheres lie along the formula_3-axis, whereas the constant-formula_8 tori are centered in the formula_22 plane.\n\nThe formulae for the inverse transformation are:\n\nwhere formula_26 and formula_27\n\nThe scale factors for the bispherical coordinates formula_8 and formula_11 are equal\n\nwhereas the azimuthal scale factor equals\n\nThus, the infinitesimal volume element equals\n\nand the Laplacian is given by\n\nOther differential operators such as formula_34 and formula_35 can be expressed in the coordinates formula_36 by substituting the scale factors into the general formulae found in orthogonal coordinates.\n\nThe classic applications of bispherical coordinates are in solving partial differential equations, \ne.g., Laplace's equation, for which bispherical coordinates allow a \nseparation of variables. However, the Helmholtz equation is not separable in bispherical coordinates. A typical example would be the electric field surrounding two conducting spheres of different radii.\n\n\n"}
{"id": "33776030", "url": "https://en.wikipedia.org/wiki?curid=33776030", "title": "Carlos Biggeri", "text": "Carlos Biggeri\n\nCarlos Biggeri (1908–1965) was an Argentine mathematician who won several prestigious awards including the Association of Culture of Argentina, the Faculty of Science of Madrid (Spain) and the Institution Mitre\n"}
{"id": "44217963", "url": "https://en.wikipedia.org/wiki?curid=44217963", "title": "Clebsch–Gordan coefficients for SU(3)", "text": "Clebsch–Gordan coefficients for SU(3)\n\nIn mathematical physics, Clebsch–Gordan coefficients are the expansion coefficients of total angular momentum eigenstates in an uncoupled tensor product basis. Mathematically, they specify the decomposition of the tensor product of two irreducible representations into a direct sum of irreducible representations, where the type and the multiplicities of these irreducible representations are known abstractly. The name derives from the German mathematicians Alfred Clebsch (1833–1872) and Paul Gordan (1837–1912), who encountered an equivalent problem in invariant theory.\n\nGeneralization to SU(3) of Clebsch–Gordan coefficients is useful because of their utility in characterizing hadronic decays, where a flavor-SU(3) symmetry exists (the Eightfold Way (physics)) that connects the three light quarks: up, down, and strange.\n\nA group is a mathematical structure (usually denoted in the form formula_1 consisting of a set and a binary operation (*) (often called a 'multiplication'), satisfying the following properties:\n\nIn abstract algebra, the symmetry group of an object (image, signal, etc.) is the group of all isometries under which the object is invariant with composition as the operation. It is a subgroup of the isometry group of the space concerned. In quantum mechanics, all transformations of a system that leave the Hamiltonian unchanged comprise the \"symmetry group\" of the Hamiltonian. The group operation is a binary multiplication operator.\n\nThe symmetry operator commutes with the Hamiltonian, that is,\n\nThe set of all formula_21 comprises a group, with the identity element being formula_22 — which corresponds to no transformation on the Hamiltonian. All transformations have an inverse. Thus, these form a group \n\nThe special unitary group SU is the group of unitary matrices whose determinant is equal to 1. This set is closed under matrix multiplication. All transformations characterized by the special unitary group leave norms unchanged. The symmetry appears in quantum chromodynamics, and, as already indicated in the light quark flavour symmetry dubbed the Eightfold Way (physics). The quarks possess colour quantum numbers and form the fundamental (triplet) representation of an group.\n\nThe group is a subgroup of group , the group of all 3×3 unitary matrices. The unitarity condition imposes nine constraint relations on the total 18 degrees of freedom of a 3×3 complex matrix. Thus, the dimension of the group is 9. Furthermore, multiplying a \"U\" by a phase, leaves the norm invariant. Thus can be decomposed into a direct product of . Because of this additional constraint, has dimension 8.\n\nEvery unitary matrix can be written in the form\nwhere \"H\" is hermitian. The elements of can be expressed as\nwhere formula_25 are the 8 linearly independent matrices forming the basis of the Lie algebra of , in the tripet representation. The unit determinant condition requires the formula_25 matrices to be traceless, since\n\nAn explicit basis in the fundamental, 3, representation can be constructed in analogy to the Pauli matrix algebra of the spin operators. It consists of the Gell-Mann matrices,\n\nThese are the generators of the group in the triplet representation, and they are normalized as\n\nThe Lie algebra structure constants of the group are given by the commutators of formula_25\nwhere formula_32 are the structure constants completely antisymmetric and are analogous to the Levi-Civita symbol formula_33 of . \n\nIn general, they vanish, unless they contain an odd number of indices from the set {2,5,7}, corresponding to the antisymmetric s. Note formula_34. \n\nMoreover,\nwhere formula_36 are the completely symmetric coefficient constants. They vanish if the number of indices from the set {2,5,7} is odd.\n\nA slightly differently normalized standard basis consists of the \"F-spin\" operators, which are defined as formula_37 for the 3, and are utilized to apply to \"any representation of this algebra\".\n\nThe Cartan–Weyl basis of the Lie algebra of is obtained by another change of basis, where one defines,\nBecause of the factors of \"i\" in these formulas, this is technically a basis for the complexification of the su(3) Lie algebra, namely sl(3,C). The preceding basis is then essentially the same one used in Hall's book.\n\nThe standard form of generators of the group satisfies the commutation relations given below,\n\nAll other commutation relations follow from hermitian conjugation of these operators.\n\nThese commutation relations can be used to construct the irreducible representations of the group.\n\nThe representations of the group lie in the 2-dimensional plane. Here, formula_59 stands for the z-component of Isospin and formula_60 is the Hypercharge, and they comprise the (abelian) Cartan subalgebra of the full Lie algebra. The maximum number of mutually commuting generators of a Lie algebra is called its \"rank\": has rank 2. The remaining 6 generators, the ± ladder operators, correspond to the 6 roots arranged on the 2-dimensional hexagonal lattice of the figure.\n\nThe Casimir operator is an operator that commutes with all the generators of the Lie group. In the case of , the quadratic operator is the only independent such operator.\n\nIn the case of group, by contrast, two independent Casimir operators can be constructed, a quadratic and a cubic: they are,\n\nThese Casimir operators serve to label the irreducible representations of the Lie group algebra , because all states in a given representation assume the same value for each Casimir operator, which serves as the identity in a space with the dimension of that representation. This is because states in a given representation are connected by the action of the generators of the Lie algebra, and all generators commute with the Casimir operators.\n\nFor example, for the triplet representation, , the eigenvalue of is 4/3, and of , 10/9.\n\nMore generally, from Freudenthal's formula, for generic , the eigenvalue of is formula_63.\n\nThe eigenvalue (\"anomaly coefficient\") of is\nformula_64\nIt is an \"odd function\" under the interchange . Consequently, it vanishes for real representations , such as the adjoint, , i.e. both and anomalies vanish for it.\n\nThe irreducible representations of SU(3) are analyzed in various places, including Hall's book. Since the SU(3) group is simply connected, the representations are in one-to-one correspondence with the representations of its Lie algebra su(3), or the complexification of its Lie algebra, sl(3,C). \n\nThe representations are labeled as \"D\"(p,q), with \"p\" and \"q\" being non-negative integers, where in physical terms, \"p\" is the number of quarks and \"q\" is the number of antiquarks. Mathematically, the representation \"D\"(p,q) may be constructed by tensoring together \"p\" copies of the standard 3-dimensional representation and \"q\" copies of the dual of the standard representation, and then extracting an irreducible invariant subspace. (See also the section of Young tableaux below: is the number of single-box columns and the number of double-box columns). Still another way to think about the parameters \"p\" and \"q\" is as the maximum eigenvalues of the diagonal matrices\nThis is to be compared to the representation theory of SU(2), where the irreducible representations are labeled by the maximum eigenvalue of a single element, \"h\".\n\nThe representations have dimension \n\nAn multiplet may be completely specified by five labels, two of which, the eigenvalues of the two Casimirs, are common to all members of the multiplet. This generalizes the mere two labels for multiplets, namely the eigenvalues of its quadratic Casimir and of .\n\nSince formula_73, we can\nlabel different states by the eigenvalues of formula_59 and formula_60 operators, formula_76, for a given eigenvalue of the isospin Casimir. The action of operators on this states are,\nHere,\nand\n\nAll the other states of the representation can be constructed by the successive application of the ladder operators formula_86 and formula_87 and by identifying the base states which are annihilated by the action of the lowering operators. These operators lie on the vertices and the center of a hexagon.\n\nThe product representation of two irreducible representations formula_88 and formula_89 is generally reducible. Symbolically,\nwhere is an integer.\n\nFor example, two octets (adjoints) compose to\nthat is, their product reduces to an icosaseptet (27), decuplet, two octets, an antidecuplet, and a singlet, 64 states in all.\n\nThe right-hand series is called the Clebsch–Gordan series. It implies that the representation appears times in the reduction of this direct product of formula_88 with formula_89.\n\nNow a complete set of operators is needed to specify uniquely the states of each irreducible representation inside the one just reduced.\nThe complete set of commuting operators (CSCO) in the case of the irreducible representation is\nwhere\n\nThe states of the above direct product representation are thus completely represented by the set of operators\nwhere the number in the parentheses designates the representation on which the operator acts.\n\nAn alternate set of commuting operators can be found for the direct product representation, if one considers the following set of operators,\n\nThus, the set of commuting operators includes\nThis is a set of nine operators only. But the set must contain ten operators to define all the states of the direct product representation uniquely. To find the last operator , one must look outside the group. It is necessary to distinguish different for similar values of and .\n\nThus, any state in the direct product representation can be represented by the ket,\nalso using the second complete set of commuting operator, we can define the states in the direct product representation as\n\nWe can drop the formula_102 from the state and label the states as\nusing the operators from the first set, and,\nusing the operators from the second set.\n\nBoth these states span the direct product representation and any states in the representation can be labeled by suitable choice of the eigenvalues.\n\nUsing the completeness relation,\n_{\\nu_2}</math> are the basis states of formula_89. Also formula_106 are the basis states of the product representation. Here formula_107 represents the combined eigenvalues formula_108 and formula_109 respectively.\n\nThus the unitary transformations that connects the two bases are\nThis is a comparatively compact notation. Here,\nare the Clebsch–Gordan coefficients.\n\nThe Clebsch–Gordan coefficients form a real orthogonal matrix. Therefore,\nAlso, they follow the following orthogonality relations,\n\nIf an irreducible representation formula_115 apperars in the Clebsch–Gordan series of formula_116, then it must appear in the Clebsch–Gordan series of formula_117. Which implies,\nWhere formula_119\nSince the Clebsch–Gordan coefficients are all real, the following symmetry property can be deduced,\nWhere formula_121.\n\nA three-dimensional harmonic oscillator is described by the Hamiltonian\nwhere the spring constant, the mass and Planck's constant have been absorbed into the definition of the variables, ==1.\n\nIt is seen that this Hamiltonian is symmetric under coordinate transformations that preserve the value of formula_123. Thus, any operators in the group formula_124 keep this Hamiltonian invariant.\n\nMore significantly, since the Hamiltonian is Hermitian, it further remains invariant under operation by elements of the much larger group.\n\nMore systematically, operators such as the Ladder operators\ncan be constructed which raise and lower the eigenvalue of the Hamiltonian operator by 1.\n\nThe operators and are not hermitian; but hermitian operators can be constructed from different combinations of them, \nThere are nine such operators for \"i,j\"=1,2,3.\n\nThe nine hermitian operators formed by the bilinear forms are controlled by the fundamental commutators\nand seen to \"not\" commute among themselves. As a result, this complete set of operators don't share their eigenvectors in common, and they cannot be diagonalized simultaneously. The group is thus non-Abelian and degeneracies may be present in the Hamiltonian, as indicated.\n\nThe Hamiltonian of the 3D isotropic harmonic oscillator, when written in terms of the operator formula_130 amounts to\nThe Hamiltonian has 8-fold degeneracy. A successive application of and on the left preserves the Hamiltonian invariant, since it increases by 1 and decrease by 1, thereby keeping the total\nSince the operators belonging to the symmetry group of Hamiltonian do not always form an Abelian group, a common eigenbasis cannot be found that diagonalizes all of them simultaneously. Instead, we take the maximally commuting set of operators from the symmetry group of the Hamiltonian, and try to reduce the matrix representations of the group into irreducible representations.\nThe Hilbert space of two particles is the tensor product of the two Hilbert spaces of the two individual particles,\nwhere formula_134 and formula_135 are the Hilbert space of the first and second particles, respectively.\n\nThe operators in each of the Hilbert spaces have their own commutation relations, and an operator of one Hilbert space commutes with an operator from the other Hilbert space. Thus the symmetry group of the two particle Hamiltonian operator is the superset of the symmetry groups of the Hamiltonian operators of individual particles. If the individual Hilbert spaces are dimensional, the combined Hilbert space is dimensional.\n\nThe symmetry group of the Hamiltonian is . As a result, the Clebsch–Gordan coefficients can be found by expanding the uncoupled basis vectors of the symmetry group of the Hamiltonian into its coupled basis. The Clebsch–Gordan series is obtained by block-diagonalizing the Hamiltonian through the unitary transformation constructed from the eigenstates which diagonalizes the maximal set of commuting operators.\n\nA Young tableau (plural \"tableaux\") is a method for decomposing products of an SU(\"N\") group representation into a sum of irreducible representations. It provides the dimension and symmetry types of the irreducible representations, which is known as the Clebsch–Gordan series. Each irreducible representation corresponds to a single-particle state and a product of more than one irreducible representation indicates a multiparticle state.\n\nSince the particles are mostly indistinguishable in quantum mechanics, this approximately relates to several permutable particles. The permutations of identical particles constitute the symmetric group S. Every -particle state of S that is made up of single-particle states of the fundamental -dimensional SU(N) multiplet belongs to an irreducible SU(N) representation. Thus, it can be used to determine the Clebsch–Gordan series for any unitary group.\n\nAny two particle wavefunctionformula_136, where the indices 1,2 represents the state of particle 1 and 2, can be used to generate states of explicit symmetry using the symmetrizing and the anti-symmetrizing operators.\n\nwhere the formula_139 are the operator that interchanges the particles (Exchange operator).\n\nThe following relation follows:-\n\nthus,\n\nStarting from a multipartice state, we can apply formula_145 and formula_146 repeatedly to construct states that are:-\n\nInstead of using \"ψ\", in Young tableaux, we use square boxes (□) to denote particles and \"i\" to denote the state of the particles. \n\nThe complete set of formula_147 particles are denoted by arrangements of formula_147 □s, each with its own quantum number label (\"i\").\n\nThe tableaux is formed by stacking boxes side by side by side and up-down such that the states symmetrised with respect to all particles are given ia a row and the states anti-symmetrised with respect to all particles lies in a single column. Following rules are followed while constructing the tableaux:\n\nFor \"N\"=3 that is in the case of SU(3), the following situation arises. In SU(3) there are three labels, they are generally designated by (u,d,s) corresponding to up, down and strange quarks which follows the SU(3) algebra. They can also be designated generically as (1,2,3). For two particle system, we have the following six symmetry states:\nand the following three antisymmetric states:\n\nThe 1-column, 3-row tableau is the singlet, and so all tableaux of nontrivial irreps of SU(3) cannot have more than two rows. The representation has\n\nClebsch–Gordan series is the expansion of the direct product of two irreducible representation into direct sum of irreducible representations.\nformula_150. This can be easily found out from the Young tableaux.\n\nThe tensor product of a triplet with an octet reducing to a deciquintuplet (15), an anti-sextet, and a triplet\nappears diagrammatically as-\na total of 24 states.\nUsing the same procedure, any direct product representation is easily reduced.\n\n\n\n"}
{"id": "51208917", "url": "https://en.wikipedia.org/wiki?curid=51208917", "title": "Community search", "text": "Community search\n\nDiscovering communities in a network, known as community detection/discovery, is a fundamental problem in network science, which attracted much attention in the past several decades. In recent years, with the tremendous studies on big data, there is another related but different problem, called community search, which aims to find the most likely community that contains the query node, has attracted great attention from both academic and industry areas. It is a query-dependent variant of the community detection problem.\nIn recent years, there has been many interestings studies focusing on this novel research problem.\n\nAs pointed by the first work on community search published in SIGKDD'2010, many existing community detection/discovery methods consider the \"static\" community detection problem, where the graph needs to be partitioned a-priori with no reference to query nodes. While community search often focuses the most-likely communitie containing the query vertex. The main advantages of community search over community detection/discovery are listed as below:\n\nFor example, a recent work, which focuses on attributed graphs, where nodes are often associated with some attributes like keyword, and tries to find the communities, called attributed communities, which exhibit both strong structure and keyword cohesiveness. The query users are allowed to specify a query node and some other query conditions: (1) a value, k, the minimum degree for the expected communities; and (2) a set of keywords, which control the semantic of the expected communities. The communities returned can be easily interpreted by the keywords shared by all the community members. More details can be fround from.\n\nSome recent studies have shown that, for million-scale graphs, community search often takes less than 1 second to find a well-defined community, which is generally much faster than many existing community detection/discovery methods. This also implies that, community search is more suitable for finding communities from big graphs.\n\nCommunity search often uses some well-defined, fundamental measures of graphs. The commonly used measures are\nminimum degree, k-truss, k-edge-connected \n\n, etc.\nNote that, the minimum degree measure is also used for defining the k-core of a graph.\nAmong these measures, the minimum degree measure is the most popular one, and has been used in many recent studies.\n"}
{"id": "8564", "url": "https://en.wikipedia.org/wiki?curid=8564", "title": "Diffeomorphism", "text": "Diffeomorphism\n\nIn mathematics, a diffeomorphism is an isomorphism of smooth manifolds. It is an invertible function that maps one differentiable manifold to another such that both the function and its inverse are smooth.\n\nGiven two manifolds \"M\" and \"N\", a differentiable map \"f\" : \"M\" → \"N\" is called a diffeomorphism if it is a bijection and its inverse \"f\" : \"N\" → \"M\" is differentiable as well. If these functions are \"r\" times continuously differentiable, \"f\" is called a \"C\"-diffeomorphism.\n\nTwo manifolds \"M\" and \"N\" are diffeomorphic (symbol usually being ≃) if there is a diffeomorphism \"f\" from \"M\" to \"N\". They are \"C\" diffeomorphic if there is an \"r\" times continuously differentiable bijective map between them whose inverse is also \"r\" times continuously differentiable.\n\nGiven a subset \"X\" of a manifold \"M\" and a subset \"Y\" of a manifold \"N\", a function \"f\" : \"X\" → \"Y\" is said to be smooth if for all \"p\" in \"X\" there is a neighborhood \"U\" ⊂ \"M\" of \"p\" and a smooth function \"g\" : \"U\" → \"N\" such that the restrictions agree formula_1 (note that \"g\" is an extension of \"f\"). \"f\" is said to be a diffeomorphism if it is bijective, smooth and its inverse is smooth.\n\nIf \"U\", \"V\" are connected open subsets of R such that \"V\" is simply connected, a differentiable map \"f\" : \"U\" → \"V\" is a diffeomorphism if it is proper and if the differential \"Df\" : R → R is bijective at each point \"x\" in \"U\".\nIt is essential for \"V\" to be simply connected for the function \"f\" to be globally invertible (under the sole condition that its derivative be a bijective map at each point). For example, consider the \"realification\" of the complex square function \nThen \"f\" is surjective and it satisfies \nThus, though \"Df\" is bijective at each point, \"f\" is not invertible because it fails to be injective (e.g. \"f\"(1,0) = (1,0) = \"f\"(−1,0).\n\nSince the differential at a point (for a differentiable function) \nis a linear map, it has a well-defined inverse if and only if \"Df\" is a bijection. The matrix representation of \"Df\" is the \"n\" × \"n\" matrix of first-order partial derivatives whose entry in the \"i\"-th row and \"j\"-th column is formula_5. This so-called Jacobian matrix is often used for explicit computations.\n\nDiffeomorphisms are necessarily between manifolds of the same dimension. Imagine \"f\" going from dimension \"n\" to dimension \"k\". If \"n\" < \"k\" then \"Df\" could never be surjective; and if \"n\" > \"k\" then \"Df\" could never be injective. In both cases, therefore, \"Df\" fails to be a bijection.\n\nIf \"Df\" is a bijection at \"x\" then \"f\" is said to be a local diffeomorphism (since, by continuity, \"Df\" will also be bijective for all \"y\" sufficiently close to \"x\").\n\nGiven a smooth map from dimension \"n\" to dimension \"k\", if \"Df\" (or, locally, \"Df\") is surjective, \"f\" is said to be a submersion (or, locally, a \"local submersion\"); and if \"Df\" (or, locally, \"Df\") is injective, \"f\" is said to be an immersion (or, locally, a \"local immersion\").\n\nA differentiable bijection is \"not\" necessarily a diffeomorphism. \"f\"(\"x\") = \"x\", for example, is not a diffeomorphism from R to itself because its derivative vanishes at 0 (and hence its inverse is not differentiable at 0). This is an example of a homeomorphism that is not a diffeomorphism.\n\nWhen \"f\" is a map between \"differentiable\" manifolds, a diffeomorphic \"f\" is a stronger condition than a homeomorphic \"f\". For a diffeomorphism, \"f\" and its inverse need to be differentiable; for a homeomorphism, \"f\" and its inverse need only be continuous. Every diffeomorphism is a homeomorphism, but not every homeomorphism is a diffeomorphism.\n\"f\" : \"M\" → \"N\" is called a diffeomorphism if, in coordinate charts, it satisfies the definition above. More precisely: Pick any cover of \"M\" by compatible coordinate charts and do the same for \"N\". Let φ and ψ be charts on, respectively, \"M\" and \"N\", with \"U\" and \"V\" as, respectively, the images of φ and ψ. The map ψ\"f\"φ : \"U\" → \"V\" is then a diffeomorphism as in the definition above, whenever \"f\"(φ(U)) ⊂ ψ(V).\n\nSince any manifold can be locally parametrised, we can consider some explicit maps from R into R.\n\n\n\n\nIn mechanics, a stress-induced transformation is called a deformation and may be described by a diffeomorphism.\nA diffeomorphism \"f\" : \"U\" → \"V\" between two surfaces \"U\" and \"V\" has a Jacobian matrix \"Df\" that is an invertible matrix. In fact, it is required that for \"p\" in \"U\", there is a neighborhood of \"p\" in which the Jacobian \"Df\" stays non-singular. Since the Jacobian is a 2 × 2 real matrix, Df can be read as one of three types of complex number: ordinary complex, split complex number, or dual number. Suppose that in a chart of the surface, formula_15\n\nThe total differential of \"u\" is\nThen the image formula_17 is a linear transformation, fixing the origin, and expressible as the action of a complex number of a particular type. When (\"dx, dy\" ) is also interpreted as that type of complex number, the action is of complex multiplication in the appropriate complex number plane. As such, there is a type of angle (Euclidean, hyperbolic, or slope) that is preserved in such a multiplication. Due to \"Df\" being invertible, the type of complex number is uniform over the surface. Consequently, a surface deformation or diffeomorphism of surfaces has the conformal property of preserving (the appropriate type of) angles.\n\nLet \"M\" be a differentiable manifold that is second-countable and Hausdorff. The diffeomorphism group of \"M\" is the group of all \"C\" diffeomorphisms of \"M\" to itself, denoted by Diff(\"M\") or, when \"r\" is understood, Diff(\"M\"). This is a \"large\" group, in the sense that – provided \"M\" is not zero-dimensional – it is not locally compact.\n\nThe diffeomorphism group has two natural topologies: \"weak\" and \"strong\" . When the manifold is compact, these two topologies agree. The weak topology is always metrizable. When the manifold is not compact, the strong topology captures the behavior of functions \"at infinity\" and is not metrizable. It is, however, still Baire.\n\nFixing a Riemannian metric on \"M\", the weak topology is the topology induced by the family of metrics\nas \"K\" varies over compact subsets of \"M\". Indeed, since \"M\" is σ-compact, there is a sequence of compact subsets \"K\" whose union is \"M\". Then:\n\nThe diffeomorphism group equipped with its weak topology is locally homeomorphic to the space of \"C\" vector fields . Over a compact subset of \"M\", this follows by fixing a Riemannian metric on \"M\" and using the exponential map for that metric. If \"r\" is finite and the manifold is compact, the space of vector fields is a Banach space. Moreover, the transition maps from one chart of this atlas to another are smooth, making the diffeomorphism group into a Banach manifold with smooth right translations; left translations and inversion are only continuous. If \"r\" = ∞, the space of vector fields is a Fréchet space. Moreover, the transition maps are smooth, making the diffeomorphism group into a Fréchet manifold and even into a regular Fréchet Lie group. If the manifold is σ-compact and not compact the full diffeomorphism group is not locally contractible for any of the two topologies. One has to restrict the group by controlling the deviation from the identity near infinity to obtain a diffeomorphism group which is a manifold; see .\n\nThe Lie algebra of the diffeomorphism group of \"M\" consists of all vector fields on \"M\" equipped with the Lie bracket of vector fields. Somewhat formally, this is seen by making a small change to the coordinate x at each point in space:\nso the infinitesimal generators are the vector fields\n\n\nFor a connected manifold \"M\", the diffeomorphism group acts transitively on \"M\". More generally, the diffeomorphism group acts transitively on the configuration space \"CM\". If \"M\" is at least two-dimensional, the diffeomorphism group acts transitively on the configuration space \"FM\" and the action on \"M\" is multiply transitive .\n\nIn 1926, Tibor Radó asked whether the harmonic extension of any homeomorphism or diffeomorphism of the unit circle to the unit disc yields a diffeomorphism on the open disc. An elegant proof was provided shortly afterwards by Hellmuth Kneser. In 1945, Gustave Choquet, apparently unaware of this result, produced a completely different proof.\n\nThe (orientation-preserving) diffeomorphism group of the circle is pathwise connected. This can be seen by noting that any such diffeomorphism can be lifted to a diffeomorphism \"f\" of the reals satisfying [\"f\"(\"x\"+1) = \"f\"(\"x\") + 1]; this space is convex and hence path-connected. A smooth, eventually constant path to the identity gives a second more elementary way of extending a diffeomorphism from the circle to the open unit disc (a special case of the Alexander trick). Moreover, the diffeomorphism group of the circle has the homotopy-type of the orthogonal group O(2).\n\nThe corresponding extension problem for diffeomorphisms of higher-dimensional spheres S was much studied in the 1950s and 1960s, with notable contributions from René Thom, John Milnor and Stephen Smale. An obstruction to such extensions is given by the finite abelian group Γ, the \"group of twisted spheres\", defined as the quotient of the abelian component group of the diffeomorphism group by the subgroup of classes extending to diffeomorphisms of the ball \"B\".\n\nFor manifolds, the diffeomorphism group is usually not connected. Its component group is called the mapping class group. In dimension 2 (i.e. surfaces), the mapping class group is a finitely presented group generated by Dehn twists (Dehn, Lickorish, Hatcher). Max Dehn and Jakob Nielsen showed that it can be identified with the outer automorphism group of the fundamental group of the surface.\n\nWilliam Thurston refined this analysis by classifying elements of the mapping class group into three types: those equivalent to a periodic diffeomorphism; those equivalent to a diffeomorphism leaving a simple closed curve invariant; and those equivalent to pseudo-Anosov diffeomorphisms. In the case of the torus S × S = R/Z, the mapping class group is simply the modular group SL(2, Z) and the classification becomes classical in terms of elliptic, parabolic and hyperbolic matrices. Thurston accomplished his classification by observing that the mapping class group acted naturally on a compactification of Teichmüller space; as this enlarged space was homeomorphic to a closed ball, the Brouwer fixed-point theorem became applicable. Smale conjectured that if \"M\" is an oriented smooth closed manifold, the identity component of the group of orientation-preserving diffeomorphisms is simple. This had first been proved for a product of circles by Michel Herman; it was proved in full generality by Thurston.\n\n\nUnlike non-diffeomorphic homeomorphisms, it is relatively difficult to find a pair of homeomorphic manifolds that are not diffeomorphic. In dimensions 1, 2, 3, any pair of homeomorphic smooth manifolds are diffeomorphic. In dimension 4 or greater, examples of homeomorphic but not diffeomorphic pairs have been found. The first such example was constructed by John Milnor in dimension 7. He constructed a smooth 7-dimensional manifold (called now Milnor's sphere) that is homeomorphic to the standard 7-sphere but not diffeomorphic to it. There are, in fact, 28 oriented diffeomorphism classes of manifolds homeomorphic to the 7-sphere (each of them is the total space of a fiber bundle over the 4-sphere with the 3-sphere as the fiber).\n\nMore unusual phenomena occur for 4-manifolds. In the early 1980s, a combination of results due to Simon Donaldson and Michael Freedman led to the discovery of exotic Rs: there are uncountably many pairwise non-diffeomorphic open subsets of R each of which is homeomorphic to R, and also there are uncountably many pairwise non-diffeomorphic differentiable manifolds homeomorphic to R that do not embed smoothly in R.\n\n\n"}
{"id": "4069270", "url": "https://en.wikipedia.org/wiki?curid=4069270", "title": "Difference polynomials", "text": "Difference polynomials\n\nIn mathematics, in the area of complex analysis, the general difference polynomials are a polynomial sequence, a certain subclass of the Sheffer polynomials, which include the Newton polynomials, Selberg's polynomials, and the Stirling interpolation polynomials as special cases. \n\nThe general difference polynomial sequence is given by\n\nwhere formula_2 is the binomial coefficient. For formula_3, the generated polynomials formula_4 are the Newton polynomials\n\nThe case of formula_6 generates Selberg's polynomials, and the case of formula_7 generates Stirling's interpolation polynomials.\n\nGiven an analytic function formula_8, define the moving difference of \"f\" as\n\nwhere formula_10 is the forward difference operator. Then, provided that \"f\" obeys certain summability conditions, then it may be represented in terms of these polynomials as\n\nThe conditions for summability (that is, convergence) for this sequence is a fairly complex topic; in general, one may say that a necessary condition is that the analytic function be of less than exponential type. Summability conditions are discussed in detail in Boas & Buck.\n\nThe generating function for the general difference polynomials is given by\n\nThis generating function can be brought into the form of the generalized Appell representation \n\nby setting formula_14, formula_15, formula_16 and formula_17.\n\n\n"}
{"id": "14834796", "url": "https://en.wikipedia.org/wiki?curid=14834796", "title": "Dominance order", "text": "Dominance order\n\nIn discrete mathematics, dominance order (synonyms: dominance ordering, majorization order, natural ordering) is a partial order on the set of partitions of a positive integer \"n\" that plays an important role in algebraic combinatorics and representation theory, especially in the context of symmetric functions and representation theory of the symmetric group.\n\nIf \"p\" = (\"p\",\"p\",…) and \"q\" = (\"q\",\"q\",…) are partitions of \"n\", with the parts arranged in the weakly decreasing order, then \"p\" precedes \"q\" in the dominance order if for any \"k\" ≥ 1, the sum of the \"k\" largest parts of \"p\" is less than or equal to the sum of the \"k\" largest parts of \"q\":\n\nIn this definition, partitions are extended by appending zero parts at the end as necessary.\n\n\nPartitions of \"n\" form a lattice under the dominance ordering, denoted \"L\", and the operation of conjugation is an antiautomorphism of this lattice. To explicitly describe the lattice operations, for each partition \"p\" consider the associated (\"n\" + 1)-tuple:\n\nThe partition \"p\" can be recovered from its associated (\"n\"+1)-tuple by applying the step 1 difference, formula_5 Moreover, the (\"n\"+1)-tuples associated to partitions of \"n\" are characterized among all integer sequences of length \"n\" + 1 by the following three properties:\n\n\nBy the definition of the dominance ordering, partition \"p\" precedes partition \"q\" if and only if the associated (\"n\" + 1)-tuple of \"p\" is term-by-term less than or equal to the associated (\"n\" + 1)-tuple of \"q\". If \"p\", \"q\", \"r\" are partitions then formula_9 if and only if formula_10 The componentwise minimum of two nondecreasing concave integer sequences is also nondecreasing and concave. Therefore, for any two partitions of \"n\", \"p\" and \"q\", their meet is the partition of \"n\" whose associated (\"n\" + 1)-tuple has components formula_11 The natural idea to use a similar formula for the join \"fails\", because the componentwise maximum of two concave sequences need not be concave. For example, for \"n\" = 6, the partitions [3,1,1,1] and [2,2,2] have associated sequences (0,3,4,5,6,6,6) and (0,2,4,6,6,6,6), whose componentwise maximum (0,3,4,6,6,6,6) does not correspond to any partition. To show that any two partitions of \"n\" have a join, one uses the conjugation antiautomorphism: the join of \"p\" and \"q\" is the conjugate partition of the meet of \"p\"′ and \"q\"′:\n\nFor the two partitions \"p\" and \"q\" in the preceding example, their conjugate partitions are [4,1,1] and [3,3] with meet [3,2,1], which is self-conjugate; therefore, the join of \"p\" and \"q\" is [3,2,1].\n\nThomas Brylawski has determined many invariants of the lattice \"L\", such as the minimal height and the maximal covering number, and classified the intervals of small length. While \"L\" is not distributive for \"n\" ≥ 7, it shares some properties with distributive lattices: for example, its Möbius function takes on only values 0, 1, −1.\n\nPartitions of \"n\" can be graphically represented by Young diagrams on \"n\" boxes.\nStandard Young tableaux are certain ways to fill Young diagrams with numbers, and a partial order on them (sometimes called the \"dominance order on Young tableaux\") can be defined in terms of the dominance order on the Young diagrams. For a Young tableau \"T\" to dominate another Young tableau \"S\", the shape of \"T\" must dominate that of \"S\" as a partition, and moreover the same must hold whenever \"T\" and \"S\" are first truncated to their sub-tableaux containing entries up to a given value \"k\", for each choice of \"k\".\n\nSimilarly, there is a dominance order on the set of standard Young bitableaux, which plays a role in the theory of \"standard monomials\".\n\n\n"}
{"id": "28255586", "url": "https://en.wikipedia.org/wiki?curid=28255586", "title": "Eli Maor", "text": "Eli Maor\n\nEli Maor, an Israel-born historian of mathematics, is the author of several books about the history of mathematics. Eli Maor received his PhD at the Technion – Israel Institute of Technology. He teaches the history of mathematics at Loyola University Chicago. Maor was the editor of the article on trigonometry for the \"Encyclopædia Britannica\".\n\n"}
{"id": "10016360", "url": "https://en.wikipedia.org/wiki?curid=10016360", "title": "Excellent ring", "text": "Excellent ring\n\nIn commutative algebra, a quasi-excellent ring is a Noetherian commutative ring that behaves well with respect to the operation of completion, and is called an excellent ring if it is also universally catenary. Excellent rings are one answer to the problem of finding a natural class of \"well-behaved\" rings containing most of the rings that occur in number theory and algebraic geometry. At one time it seemed that the class of Noetherian rings might be an answer to this problem, but Nagata and others found several strange counterexamples showing that in general Noetherian rings need not be well behaved: for example, a normal Noetherian local ring need not be analytically normal. The class of excellent rings was defined by Alexander Grothendieck (1965) as a candidate for such a class of well-behaved rings. Quasi-excellent rings are conjectured to be the base rings for which the problem of resolution of singularities can be solved; showed this in characteristic 0, but the positive characteristic case is (as of 2016) still a major open problem. Essentially all Noetherian rings that occur naturally in algebraic geometry or number theory are excellent; in fact it is quite hard to construct examples of Noetherian rings that are not excellent.\n\n\nMost naturally occurring commutative rings in number theory or algebraic geometry are excellent. In particular:\n\nHere is an example of a discrete valuation ring \"A\" of dimension 1 and characteristic \"p\">0 which is J-2 but not a G-ring and so is not quasi-excellent. If \"k\" is any field of characteristic \"p\" with [\"k\":\"k\"] = ∞ and \"A\" is the ring of power series Σ\"a\"\"x\" such that [\"k\"(\"a\",\"a\"...):\"k\"] is finite then the formal fibers of \"A\" are not all geometrically regular so \"A\" is not a G-ring. It is a J-2 ring as all Noetherian local rings of dimension at most 1 are J-2 rings. It is also universally catenary as it is a Dedekind domain. Here \"k\" denotes the image of \"k\" under the Frobenius morphism \"a\"→\"a\".\n\nHere is an example of a ring that is a G-ring but not a J-2 ring and so not quasi-excellent. If \"R\" is the subring of the polynomial ring \"k\"[\"x\",\"x\"...] in infinitely many generators generated by the squares and cubes of all generators, and \"S\" is obtained from \"R\" by adjoining inverses to all elements not in any of the ideals generated by some \"x\", then \"S\" is a 1-dimensional Noetherian domain that is not a J-1 ring as \"S\" has a cusp singularity at every closed point, so the set of singular points is not closed, though it is a G-ring.\nThis ring is also universally catenary, as its localization at every prime ideal is a quotient of a regular ring.\n\nNagata's example of a 2-dimensional Noetherian local ring that is catenary but not universally catenary is a G-ring, and is also a J-2 ring as any local G-ring is a J-2 ring . So it is a quasi-excellent catenary local ring that is not excellent.\n\nAny quasi-excellent ring is a Nagata ring.\n\nAny quasi-excellent reduced local ring is analytically reduced.\n\nAny quasi-excellent normal local ring is analytically normal.\n\nQuasi-excellent rings are closely related to the problem of resolution of singularities, and this seems to have been Grothendieck's motivation for defining them. Grothendieck (1965) observed that if it is possible to resolve singularities of all complete integral local Noetherian rings, then it is possible to resolve the singularities of all reduced quasi-excellent rings. Hironaka (1964) proved this for all complete integral Noetherian local rings over a field of characteristic 0, which implies his theorem that all singularities of excellent schemes over a field of characteristic 0 can be resolved. Conversely if it is possible to resolve all singularities of the spectra of all integral finite algebras over a Noetherian ring \"R\" then the ring \"R\" is quasi-excellent.\n\n"}
{"id": "10783043", "url": "https://en.wikipedia.org/wiki?curid=10783043", "title": "Fagnano's problem", "text": "Fagnano's problem\n\nIn geometry, Fagnano's problem is an optimization problem that was first stated by Giovanni Fagnano in 1775:\n\nThe orthic triangle, with vertices at the base points of the altitudes of the given triangle, has the smallest perimeter of all triangles inscribed into an acute triangle, hence it is the solution of Fagnano's problem. Fagnano's original proof used calculus methods and an intermediate result given by his father Giulio Carlo de' Toschi di Fagnano. Later however several geometric proofs were discovered as well, amongst others by Hermann Schwarz and Lipót Fejér. These proofs use the geometrical properties of reflections to determine some minimal path representing the perimeter.\n\nA solution from physics is found by imagining putting a rubber band that follows Hooke's Law around the three sides of a triangular frame formula_1, such that it could slide around smoothly. Then the rubber band would end up in a position that minimizes its elastic energy, and therefore minimize its total length. This position gives the minimal perimeter triangle.\nThe tension inside the rubber band is the same everywhere in the rubber band, so in its resting position, we have, by Lami's theorem, formula_2\n\nTherefore, this minimal triangle is the orthic triangle.\n\n\n"}
{"id": "18742821", "url": "https://en.wikipedia.org/wiki?curid=18742821", "title": "Free loop", "text": "Free loop\n\nIn the mathematical field of topology, a free loop is a variant of the mathematical notion of a loop. Whereas a loop has a distinguished point on it, called a basepoint, a free loop lacks such a distinguished point. Formally, let formula_1 be a topological space. Then a free loop in formula_1 is an equivalence class of continuous functions from the circle formula_3 to formula_1. Two loops are equivalent if they differ by a reparameterization of the circle. That is, formula_5 if there exists a homeomorphism formula_6 such that formula_7.\n\nThus, a free loop, as opposed to a based loop used in the definition of the fundamental group, is a map from the circle to the space without the basepoint-preserving restriction. Free homotopy classes of free loops correspond to conjugacy classes in the fundamental group.\n\nRecently, interest in the space of all free loops formula_8 has grown with the advent of string topology, i.e. the study of new algebraic structures on the homology of the free loop space.\n\n\n"}
{"id": "43118633", "url": "https://en.wikipedia.org/wiki?curid=43118633", "title": "Geometric cryptography", "text": "Geometric cryptography\n\nGeometric cryptography is an area of cryptology where messages and ciphertexts are represented by geometric quantities such as angles or intervals and where computations are performed by ruler and compass constructions. The difficulty or impossibility of solving certain geometric problems like trisection of an angle using ruler and compass only is the basis for the various protocols in geometric cryptography. This field of study was suggested by Mike Burmester, Ronald L. Rivest and Adi Shamir in 1996. Though the cryptographic methods based on geometry have practically no real life applications, they are of use as pedagogic tools for the elucidation of other more complex cryptographic protocols. \nSome of the geometric cryptographic methods are based on the impossibility of trisecting an angle using ruler and compass. Given an arbitrary angle, there is a straightforward ruler and compass construction for finding the triple of the given angle. But there is no ruler and compass construction for finding the angle which is exact one-third of a given angle. Hence the function which assigns the triple of an angle to a given angle can be thought of as a one-way function, the only constructions allowed being ruler and compass constructions.\n\nA geometric identification protocol has been suggested based on the one-way function indicated above. \n\nAssume that Alice wishes to establish a means of proving her identity later to Bob.\n\nInitialization: Alice publishes a copy of an angle Y which is constructed by Alice as the triple of an angle X she has constructed at random. Because trisecting an angle is impossible Alice is confident that she is the only one who knows X.\n\nIdentification Protocol: Alice gives Bob a copy of an angle R which she has constructed as the triple of an angle K that she has selected at random.\n\n\nThe three steps are repeated \"t\" times independently. Bob accepts Alice's proof of identity only if all \"t\" checks are successful.\n\nThis protocol is an interactive proof of knowledge of the angle X (the identity of Alice) with\nerror 2. The protocol is also zero-knowledge.\n"}
{"id": "7999138", "url": "https://en.wikipedia.org/wiki?curid=7999138", "title": "Golden–Thompson inequality", "text": "Golden–Thompson inequality\n\nIn physics and mathematics, the Golden–Thompson inequality is a trace inequality between exponentials of matrices proved independently by and . It has been developed in the context of statistical mechanics, where it has come to have a particular significance.\n\nIf \"a\" and \"b\" are two real numbers, then the exponential of \"a+b\" is the product of the exponential of \"a\" with the exponential of \"b\":\nThis relationship is not true if we replace \"a\" and \"b\" with square matrices \"A\" and \"B\". Golden and Thompson proved that, while the matrix given by formula_2 is not always equal to the matrix given by formula_3, their traces are related by the following inequality:\n\nThe inequality is well defined as the expression on right hand side of the inequality is a positive real number, as can be seen by rewriting it as formula_5 (using the cyclic property of the trace).\n\nIf \"A\" and \"B\" commute, then the equality formula_6 holds, just like in the case of real number. In this situation the Golden-Thompson inequality is actually an equality. proved that this is the only situation in which this happens: if \"A\" and \"B\" are two Hermitian matrices for which the Golden-Thomposon inequality is verified as an equality, then the two matrices commute.\n\nThe inequality has been generalized to three matrices by and furthermore to any arbitrary number of matrices by . For three matrices, it takes the following formulation:\n\nwhere the operator formula_8 is the derivative of the matrix logarithm given by formula_9. \nNote that, if formula_10 and formula_11 commute, then formula_12, and the inequality for three matrices reduces to the original from Golden and Thompson.\n\n"}
{"id": "41799059", "url": "https://en.wikipedia.org/wiki?curid=41799059", "title": "Harder–Narasimhan stratification", "text": "Harder–Narasimhan stratification\n\nIn algebraic geometry and complex geometry, the Harder–Narasimhan stratification is any of a stratification of the moduli stack of principal \"G\"-bundles by locally closed substacks in terms of \"loci of instabilities\". In the original form due to Harder and Narasimhan, \"G\" was the general linear group; i.e., the moduli stack was the moduli stack of vector bundles, but, today, the term refers to any of generalizations. The scheme-theoretic version is due to Shatz and so the term \"Shatz stratification\" is also used synonymously. The general case is due to Behrend.\n\n\n"}
{"id": "55959454", "url": "https://en.wikipedia.org/wiki?curid=55959454", "title": "IDF curve", "text": "IDF curve\n\nAn intensity-duration-frequency (IDF) curve is a mathematical function that relates the rainfall intensity with its duration and frequency of occurrence. These curves are commonly used in hydrology for flood forecasting and civil engineering for urban drainage design. However, the \"IDF curves\" are also analysed in hydrometeorology because of the interest in the \"time concentration\" or \"time-structure\" of the rainfall.\n\nThe IDF curves can take different mathematical expressions, theoretical or empirically fitted to observed rainfall data. For each duration (e.g. 5, 10, 60, 120, 180 ... minutes), the ECDF or \"empirical distribution function\", and a determined frequency or return period is set. Therefore, the empirical IDF curve is given by the union of the points of equal frequency of occurrence and different duration and intensity Likewise, a theoretical or semi-empirical IDF curve is one whose mathematical expression is physically justified, but presents parameters that must be estimated by empirical fits.\n\nThere is a large number of empirical approaches that relate the intensity (\"I\"), the duration (\"t\") and the return period (\"p\"), from fits to power laws such as:\n\n\nIn hydrometeorology, the simple power law (taking formula_4) is used according to Monjo (2016) as a measure of the time-structure of the rainfall:\nwhere formula_6 is defined as a intensity of reference for a fixed time formula_7, i.e. formula_8, and formula_9 is a non-dimensional parameter known as \"n\"-index. In a rainfall event, the equivalent to the IDF curve is called \"Maximum Averaged Intensity\" (MAI) curve.\n\nTo get an IDF curves from a probability distribution, formula_10 it is necessary to mathematically isolate the precipitationformula_11, which is directly related to the average intensity formula_12 and the duration formula_13, by the equation formula_14, and since the return period formula_15 is defined as the inverse of formula_16, the function formula_17 is found as the inverse of formula_10, according to:\n\n\n\n"}
{"id": "33762888", "url": "https://en.wikipedia.org/wiki?curid=33762888", "title": "Inferential theory of learning", "text": "Inferential theory of learning\n\nInferential theory of learning (ITL) is an area of machine learning which describes inferential processes performed by learning agents. ITL has been developed by Ryszard S. Michalski in 1980s. In ITL learning process is viewed as a search (inference) through hypotheses space guided by a specific goal. Results of learning need to be stored, in order to be used in the future.\n\n"}
{"id": "15718008", "url": "https://en.wikipedia.org/wiki?curid=15718008", "title": "International Conference on Developments in Language Theory", "text": "International Conference on Developments in Language Theory\n\nDLT, the International Conference on Developments in Language Theory is an academic conference in the field \nof computer science\nheld annually under the auspices of the European Association for Theoretical Computer Science. Like most theoretical computer science conferences its contributions are strongly peer-reviewed; the articles appear in proceedings published in Springer Lecture Notes in Computer Science. Extended versions of selected papers of each year's conference appear in international journals, such as Theoretical Computer Science and International Journal of Foundations of Computer Science.\n\nTypical topics include:\n\n\nThe DLT conference series was established by Grzegorz Rozenberg and Arto Salomaa in 1993. Since 2010, the Steering Committee chairman is Juhani Karhumäki.\n\n\n\n\n"}
{"id": "9087019", "url": "https://en.wikipedia.org/wiki?curid=9087019", "title": "Inverse Symbolic Calculator", "text": "Inverse Symbolic Calculator\n\nThe Inverse Symbolic Calculator is an online number checker established July 18, 1995 by Peter Benjamin Borwein, Jonathan Michael Borwein and Simon Plouffe of the Canadian Centre for Experimental and Constructive Mathematics (Burnaby, Canada). A user will input a number and the Calculator will use an algorithm to search for and calculate closed-form expressions or suitable functions that have roots near this number. Hence, the calculator is of great importance for those working in numerical areas of experimental mathematics.\n\nThe ISC contains 54 million mathematical constants. Plouffe's Inverter (opened in 1998) contains 214 million. A newer version of the tables with 3.702 billion entries (as of June 19, 2010) exists.\n\nIn 2016, Plouffe released a portable version of Plouffe's Inverter containing 3 billion entries.\n\n\n\n"}
{"id": "681640", "url": "https://en.wikipedia.org/wiki?curid=681640", "title": "Jacques Pelletier du Mans", "text": "Jacques Pelletier du Mans\n\nJacques Pelletier du Mans, also spelled Peletier (, 1517–1582) was a humanist, poet and mathematician of the French Renaissance.\n\nBorn in Le Mans into a bourgeois family, he studied at the Collège de Navarre in Paris, where his brother Jean was a professor of mathematics and philosophy. He subsequently studied law and medicine, frequented the literary circle around Marguerite de Navarre and from 1541 to 1543 he was secretary to René du Bellay. In 1541 he published the first French translation of Horace's \"Ars poetica\" and during this period he also published numerous scientific and mathematical treatises.\n\nIn 1547 he produced a funeral oration for Henry VIII of England and published his first poems (\"Œuvres poétiques\"), which included translations from the first two cantos of Homer's \"Odyssey\" and the first book of Virgil's \"Georgics\", twelve Petrarchian sonnets, three Horacian odes and a Martial-like epigram; this poetry collection also included the first published poems of Joachim Du Bellay and Pierre de Ronsard (Ronsard would include Jacques Pelletier into his list of revolutionary contemporary poets \"La Pléiade\"). He then began to frequent a humanist circle around Théodore de Bèze, Jean Martin, Denis Sauvage.\n\nIn the Renaissance, the French language had acquired many inconsistencies in spelling through a misguided attempt to model French words on their Latin roots (see Middle French). Jacques Pelletier tried to reform French spelling in a 1550 treatise advocating a phonetic-based spelling using new typographic signs which he would continue to use in all his published works. In this system he consistently spells his name with one \"l\": \"Peletier\".\n\nPelletier spent many years in Bordeaux, Poitiers, Piedmont (where he may have been the tutor of the son of Maréchal de Brissac), and Lyon (where he frequented the poets and humanists Maurice Scève, Louise Labé, Olivier de Magny and Pontus de Tyard). In 1555 he published a manual of poetic composition, \"Art poétique français\", a Latin oration calling for peace from Henry II of France and emperor Charles V and a new collection of poetry, \"L'Amour des amours\" (consisting of a sonnet cycle and a series of encyclopedic poems describing meteors, planets and the heavens) which would influence poets Guillaume du Bartas and Jean-Antoine de Baïf.\n\nHis last years were spent in travels to the Savoy, Germany, Switzerland, possibly Italy, and various regions in France, and in publishing numerous works in Latin on algebra, geometry and mathematics, and medicine (including a refutation of Galen, and a work on the Plague). In 1572 he was briefly director of the College of Aquitaine in Bordeaux, but, bored by the position, he resigned. During this period he was friends with Michel de Montaigne and Pierre de Brach. In 1579 he returned to Paris and was named director of the College of Le Mans. A final collection of poetry \"Louanges\" was published in 1581.\n\nPelletier died in Paris in July or August 1582.\n\nWhile maintaining the original system of the French mathematician Nicolas Chuquet (1484) for the names of large numbers, Jacques Pelletier promoted \"milliard\" for 10 which had been used earlier by Budaeus. In the late 17th century, \"milliard\" was subsequently reduced to 10. This convention is used widely in long scale countries.\n\n"}
{"id": "311544", "url": "https://en.wikipedia.org/wiki?curid=311544", "title": "Kenneth Appel", "text": "Kenneth Appel\n\nKenneth Ira Appel (October 8, 1932 – April 19, 2013) was an American mathematician who in 1976, with colleague Wolfgang Haken at the University of Illinois at Urbana-Champaign, solved one of the most famous problems in mathematics, the four-color theorem. They proved that any two-dimensional map, with certain limitations, can be filled in with four colors without any adjacent \"countries\" sharing the same color.\n\nAppel was born in Brooklyn, New York, on October 8, 1932, and he died in Dover, New Hampshire, on April 19, 2013, after being diagnosed with esophageal cancer in October 2012. He grew up in Queens, New York, and was the son of Irwin Appel and Lillian Sender Appel. He worked as an actuary for a brief time and then served in the U.S. Army for two years at Fort Benning, Georgia, and in Baumholder, Germany. In 1959, he finished his doctoral program at the University of Michigan, and he also married Carole S. Stein in Philadelphia. The couple moved to Princeton, New Jersey, where Kenneth worked for the Institute for Defense Analyses from 1959 to 1961. His main work at the Institute for Defense Analyses was doing research in cryptography. Toward the end of his life, in 2012, he was elected a Fellow of the American Mathematical Society.\n\nKenneth Appel was also the treasurer of the Strafford County Democratic Committee. He played tennis through his early 50s. He was a lifelong stamp collector, a player of the game of Go and a baker of bread. He and Carole had two sons, Andrew W. Appel, a noted computer scientist, and Peter H. Appel, and a daughter, Laurel F. Appel, who died on March 4, 2013. He was also a member of the Dover school board from 2010 until his death.\n\nKenneth Appel received his bachelor's degree from Queens College in 1953. After serving the army he attended the University of Michigan where he earned his M.A. in 1956, and then later his Ph.D. in 1959. Roger Lyndon, his doctoral advisor, was a mathematician whose main mathematical focus was in group theory.\n\nAfter working for the Institute for Defense Analyses, in 1961 Appel joined the Mathematics Department faculty at the University of Illinois as an Assistant Professor. While there Kenneth researched in group theory and computability theory. In 1967 he became an Associate Professor and in 1977 was promoted to Professor. It was while he was at this university that he and Wolfgang Haken proved the four color theorem. From their work and proof of this theorem they were later awarded the Delbert Ray Fulkerson prize, in 1979, by the American Mathematical Society and the Mathematical Programming Society.\n\nWhile at the University of Illinois Kenneth took on five students during their doctoral program. Each student helped contribute to the work cited on the Mathematics Genealogy Project.\n\nIn 1993 Appel moved to New Hampshire as Chairman of the Mathematics Department at the University of New Hampshire. In 2003 he retired as professor emeritus. During his retirement he volunteered in mathematics enrichment programs in Dover and in southern Maine public schools. He believed \"that students should be afforded the opportunity to study mathematics at the level of their ability, even if it is well above their grade level.\"\n\nKenneth Appel is known for his work in topology, the branch of mathematics that explores certain properties of geometric figures. His biggest accomplishment was proving the four color theorem in 1976 with Wolfgang Haken. The New York Times wrote in 1976: \n\nNow the four-color conjecture has been proved by two University of Illinois mathematicians, Kenneth Appel and Wolfgang Haken. They had an invaluable tool that earlier mathematicians lacked—modern computers. Their present proof rests in part on 1,200 hours of computer calculation during which about ten billion logical decisions had to be made. The proof of the four-color conjecture is unlikely to be of applied significance. Nevertheless, what has been accomplished is a major intellectual feat. It gives us an important new insight into the nature of two-dimensional space and of the ways in which such space can be broken into discrete portions.\n\nAt first, many mathematicians were unhappy with the fact that Appel and Haken were using computers, since this was new at the time, and even Appel said, \"Most mathematicians, even as late as the 1970s, had no real interest in learning about computers. It was almost as if those of us who enjoyed playing with computers were doing something non-mathematical or suspect.\" The actual proof was described in an article as long as a typical book titled \"Every Planar Map is Four Colorable\", Contemporary Mathematics, vol. 98, American Mathematical Society, 1989.\n\nThe proof has been one of the most controversial of modern mathematics because of its heavy dependence on computer number-crunching to sort through possibilities, which drew criticism from many in the mathematical community for its inelegance: \"a good mathematical proof is like a poem—this is a telephone directory!\" Appel and Haken agreed in a 1977 interview that it was not \"elegant, concise, and completely comprehensible by a human mathematical mind\".\n\nNevertheless, the proof was the start of a change in mathematicians' attitudes toward computers—which they had largely disdained as a tool for engineers rather than for theoreticians—leading to the creation of what is sometimes called experimental mathematics.\n\nKenneth Appel's other publications include an article with P.E. Schupp titled \"Artin Groups and Infinite Coxeter Groups\". In this article Appel and Schupp introduced four theorems that are true about Coxeter groups and then proved them to be true for Artin groups. The proofs of these four theorems used the \"results and methods of small cancellation theory.\"\n\n"}
{"id": "911960", "url": "https://en.wikipedia.org/wiki?curid=911960", "title": "Kripke semantics", "text": "Kripke semantics\n\nKripke semantics (also known as relational semantics or frame semantics, and often confused with possible world semantics) is a formal semantics for non-classical logic systems created in the late 1950s and early 1960s by Saul Kripke and André Joyal. It was first conceived for modal logics, and later adapted to intuitionistic logic and other non-classical systems. The development of Kripke semantics was a breakthrough in the theory of non-classical logics, because the model theory of such logics was almost non-existent before Kripke (algebraic semantics existed, but were considered 'syntax in disguise').\n\nThe language of propositional modal logic consists of a countably infinite set of propositional variables, a set of truth-functional connectives (in this article formula_1 and formula_2), and the modal operator formula_3 (\"necessarily\"). The modal operator formula_4 (\"possibly\") is (classically) the dual of formula_3 and may be defined in terms of necessity like so: formula_6 (\"possibly A\" is defined as equivalent to \"not necessarily not A\").\n\nA Kripke frame or modal frame is a pair formula_7, where \"W\" is a (possibly empty) set, and \"R\" is a binary relation on \"W\". Elements\nof \"W\" are called \"nodes\" or \"worlds\", and \"R\" is known as the accessibility relation.\n\nA Kripke model is a triple formula_8, where\nformula_7 is a Kripke frame, and formula_10 is a relation between\nnodes of \"W\" and modal formulas, such that:\nWe read formula_20 as “\"w\" satisfies\n\"A\"”, “\"A\" is satisfied in \"w\"”, or\n“\"w\" forces \"A\"”. The relation formula_10 is called the\n\"satisfaction relation\", \"evaluation\", or \"forcing relation\".\nThe satisfaction relation is uniquely determined by its\nvalue on propositional variables.\n\nA formula \"A\" is valid in:\nWe define Thm(\"C\") to be the set of all formulas that are valid in\n\"C\". Conversely, if \"X\" is a set of formulas, let Mod(\"X\") be the\nclass of all frames which validate every formula from \"X\".\n\nA modal logic (i.e., a set of formulas) \"L\" is sound with\nrespect to a class of frames \"C\", if \"L\" ⊆ Thm(\"C\"). \"L\" is\ncomplete wrt \"C\" if \"L\" ⊇ Thm(\"C\").\n\nSemantics is useful for investigating a logic (i.e. a derivation system) only if the semantic consequence relation reflects its syntactical counterpart, the \"syntactic consequence\" relation (\"derivability\"). It is vital to know which modal logics are sound and complete with respect to a class of Kripke frames, and to determine also which class that is.\n\nFor any class \"C\" of Kripke frames, Thm(\"C\") is a normal modal logic (in particular, theorems of the minimal normal modal logic, \"K\", are valid in every Kripke model). However, the converse does not hold in general: while most of the modal systems studied are complete of classes of frames described by simple conditions, \nKripke incomplete normal modal logics do exist. A natural example of such a system is Japaridze's Polymodal Logic.\n\nA normal modal logic \"L\" corresponds to a class of frames \"C\", if \"C\" = Mod(\"L\"). In other words, \"C\" is the largest class of frames such that \"L\" is sound wrt \"C\". It follows that \"L\" is Kripke complete if and only if it is complete of its corresponding class.\n\nConsider the schema T : formula_27.\nT is valid in any reflexive frame formula_7: if\nformula_29, then formula_20\nsince \"w\" \"R\" \"w\". On the other hand, a frame which\nvalidates T has to be reflexive: fix \"w\" ∈ \"W\", and\ndefine satisfaction of a propositional variable \"p\" as follows:\nformula_31 if and only if \"w\" \"R\" \"u\". Then\nformula_32, thus formula_33\nby T, which means \"w\" \"R\" \"w\" using the definition of\nformula_10. T corresponds to the class of reflexive\nKripke frames.\n\nIt is often much easier to characterize the corresponding class of\n\"L\" than to prove its completeness, thus correspondence serves as a\nguide to completeness proofs. Correspondence is also used to show\n\"incompleteness\" of modal logics: suppose \n\"L\" ⊆ \"L\" are normal modal logics that\ncorrespond to the same class of frames, but \"L\" does not\nprove all theorems of \"L\". Then \"L\" is\nKripke incomplete. For example, the schema formula_35 generates an incomplete logic, as it\ncorresponds to the same class of frames as GL (viz. transitive and\nconverse well-founded frames), but does not prove the GL-tautology formula_36.\n\nThe following table lists common modal axioms together with their corresponding classes. The naming of the axioms often varies.\n\nFor any normal modal logic, \"L\", a Kripke model (called the canonical model) can be constructed that refutes precisely the non-theorems of\n\"L\", by an adaptation of the standard technique of using maximal consistent sets as models. Canonical Kripke models play a \nrole similar to the Lindenbaum–Tarski algebra construction in algebraic\nsemantics.\n\nA set of formulas is \"L\"-\"consistent\" if no contradiction can be derived from it using the theorems of \"L\", and Modus Ponens. A \"maximal L-consistent set\" (an \"L\"-\"MCS\"\nfor short) is an \"L\"-consistent set that has no proper \"L\"-consistent superset.\n\nThe canonical model of \"L\" is a Kripke model\nformula_8, where \"W\" is the set of all \"L\"-\"MCS\",\nand the relations \"R\" and formula_10 are as follows:\nThe canonical model is a model of \"L\", as every \"L\"-\"MCS\" contains\nall theorems of \"L\". By Zorn's lemma, each \"L\"-consistent set\nis contained in an \"L\"-\"MCS\", in particular every formula\nunprovable in \"L\" has a counterexample in the canonical model.\n\nThe main application of canonical models are completeness proofs. Properties of the canonical model of K immediately imply completeness of K with respect to the class of all Kripke frames.\nThis argument does \"not\" work for arbitrary \"L\", because there is no guarantee that the underlying \"frame\" of the canonical model satisfies the frame conditions of \"L\".\n\nWe say that a formula or a set \"X\" of formulas is canonical\nwith respect to a property \"P\" of Kripke frames, if\nA union of canonical sets of formulas is itself canonical.\nIt follows from the preceding discussion that any logic axiomatized by\na canonical set of formulas is Kripke complete, and\ncompact.\n\nThe axioms T, 4, D, B, 5, H, G (and thus\nany combination of them) are canonical. GL and Grz are not\ncanonical, because they are not compact. The axiom M by itself is\nnot canonical (Goldblatt, 1991), but the combined logic S4.1 (in\nfact, even K4.1) is canonical.\n\nIn general, it is undecidable whether a given axiom is\ncanonical. We know a nice sufficient condition: Henrik Sahlqvist identified a broad class of formulas (now called\nSahlqvist formulas) such that\nThis is a powerful criterion: for example, all axioms\nlisted above as canonical are (equivalent to) Sahlqvist formulas.\n\nA logic has the finite model property (FMP) if it is complete\nwith respect to a class of finite frames. An application of this\nnotion is the decidability question: it\nfollows from\nPost's theorem that a recursively axiomatized modal logic \"L\"\nwhich has FMP is decidable, provided it is decidable whether a given\nfinite frame is a model of \"L\". In particular, every finitely\naxiomatizable logic with FMP is decidable.\n\nThere are various methods for establishing FMP for a given logic.\nRefinements and extensions of the canonical model construction often\nwork, using tools such as filtration or\nunravelling. As another possibility,\ncompleteness proofs based on cut-free\nsequent calculi usually produce finite models\ndirectly.\n\nMost of the modal systems used in practice (including all listed\nabove) have FMP.\n\nIn some cases, we can use FMP to prove Kripke completeness of a logic:\nevery normal modal logic is complete with respect to a class of\nmodal algebras, and a \"finite\" modal algebra can be transformed\ninto a Kripke frame. As an example, Robert Bull proved using this method\nthat every normal extension of S4.3 has FMP, and is Kripke\ncomplete.\n\nKripke semantics has a straightforward generalization to logics with\nmore than one modality. A Kripke frame for a language with\nformula_45 as the set of its necessity operators\nconsists of a non-empty set \"W\" equipped with binary relations\n\"R\" for each \"i\" ∈ \"I\". The definition of a\nsatisfaction relation is modified as follows:\n\nA simplified semantics, discovered by Tim Carlson, is often used for\npolymodal provability logics. A Carlson model is a structure\nformula_48\nwith a single accessibility relation \"R\", and subsets\n\"D\" ⊆ \"W\" for each modality. Satisfaction is\ndefined as\n\nCarlson models are easier to visualize and to work with than usual\npolymodal Kripke models; there are, however, Kripke complete polymodal\nlogics which are Carlson incomplete.\n\nKripke semantics for the intuitionistic logic follows the same\nprinciples as the semantics of modal logic, but it uses a different\ndefinition of satisfaction.\n\nAn intuitionistic Kripke model is a triple\nformula_51, where formula_52 is a preordered Kripke frame, and formula_10 satisfies the following conditions:\n\nThe negation of \"A\", ¬\"A\", could be defined as an abbreviation for \"A\" → ⊥. If for all \"u\" such that \"w\" ≤ \"u\", not \"u\" ⊩ \"A\", then \"w\" ⊩ \"A\" → ⊥ is vacuously true, so \"w\" ⊩ ¬\"A\".\n\nIntuitionistic logic is sound and complete with respect to its Kripke\nsemantics, and it has the finite model property.\n\nLet \"L\" be a first-order language. A Kripke\nmodel of \"L\" is a triple\nformula_68, where\nformula_52 is an intuitionistic Kripke frame, \"M\" is a\n(classical) \"L\"-structure for each node \"w\" ∈ \"W\", and\nthe following compatibility conditions hold whenever \"u\" ≤ \"v\":\nGiven an evaluation \"e\" of variables by elements of \"M\", we\ndefine the satisfaction relation formula_70:\nHere \"e\"(\"x\"→\"a\") is the evaluation which gives \"x\" the\nvalue \"a\", and otherwise agrees with \"e\".\n\nSee a slightly different formalization in.\n\nAs part of the independent development of sheaf theory, it was realised around 1965 that Kripke semantics was intimately related to the treatment of existential quantification in topos theory. That is, the 'local' aspect of existence for sections of a sheaf was a kind of logic of the 'possible'. Though this development was the work of a number of people, the name Kripke–Joyal semantics is often used in this connection.\n\nAs in classical model theory, there are methods for\nconstructing a new Kripke model from other models.\n\nThe natural homomorphisms in Kripke semantics are called\np-morphisms (which is short for \"pseudo-epimorphism\", but the\nlatter term is rarely used). A p-morphism of Kripke frames\nformula_7 and formula_92 is a mapping\nformula_93 such that\nA p-morphism of Kripke models formula_8 and\nformula_95 is a p-morphism of their\nunderlying frames formula_93, which\nsatisfies\n\nP-morphisms are a special kind of bisimulations. In general, a\nbisimulation between frames formula_7 and\nformula_92 is a relation\n\"B ⊆ W × W’\", which satisfies\nthe following “zig-zag” property:\nA bisimulation of models is additionally required to preserve forcing\nof atomic formulas:\nThe key property which follows from this definition is that\nbisimulations (hence also p-morphisms) of models preserve the\nsatisfaction of \"all\" formulas, not only propositional variables.\n\nWe can transform a Kripke model into a tree using\nunravelling. Given a model formula_8 and a fixed\nnode \"w\" ∈ \"W\", we define a model\nformula_95, where \"W’\" is the\nset of all finite sequences\nformula_105 such\nthat \"w R w\" for all\n\"i\" < \"n\", and formula_106 if and only if\nformula_107 for a propositional variable\n\"p\". The definition of the accessibility relation \"R’\"\nvaries; in the simplest case we put\nbut many applications need the reflexive and/or transitive closure of\nthis relation, or similar modifications.\n\nFiltration is a useful construction which uses to prove FMP for many logics. Let \"X\" be a set of\nformulas closed under taking subformulas. An \"X\"-filtration of a\nmodel formula_8 is a mapping \"f\" from \"W\" to a model\nformula_95 such that\nIt follows that \"f\" preserves satisfaction of all formulas from\n\"X\". In typical applications, we take \"f\" as the projection\nonto the quotient of \"W\" over the relation\nAs in the case of unravelling, the definition of the accessibility\nrelation on the quotient varies.\n\nThe main defect of Kripke semantics is the existence of Kripke incomplete logics, and logics which are complete but not compact. It can be remedied by equipping Kripke frames with extra structure which restricts the set of possible valuations, using ideas from algebraic semantics. This gives rise to the general frame semantics.\n\nBlackburn et al. (2001) point out that because a relational structure is simply a set together with a collection of relations on that set, it is unsurprising that relational structures are to be found just about everywhere. As an example from theoretical computer science, they give labeled transition systems, which model program execution. Blackburn et al. thus claim because of this connection that modal languages are ideally suited in providing \"internal, local perspective on relational structures.\" (p. xii)\n\nSimilar work that predated Kripke's revolutionary semantic breakthroughs:\n\n\n\n"}
{"id": "174706", "url": "https://en.wikipedia.org/wiki?curid=174706", "title": "Laplace operator", "text": "Laplace operator\n\nIn mathematics, the Laplace operator or Laplacian is a differential operator given by the divergence of the gradient of a function on Euclidean space. It is usually denoted by the symbols , , or . The Laplacian of a function at a point , is (up to a factor) the rate at which the average value of over spheres centered at deviates from as the radius of the sphere grows. In a Cartesian coordinate system, the Laplacian is given by the sum of second partial derivatives of the function with respect to each independent variable. In other coordinate systems such as cylindrical and spherical coordinates, the Laplacian also has a useful form.\n\nThe Laplace operator is named after the French mathematician Pierre-Simon de Laplace (1749–1827), who first applied the operator to the study of celestial mechanics, where the operator gives a constant multiple of the mass density when it is applied to a given gravitational potential. Solutions of the equation , now called Laplace's equation, are the so-called harmonic functions, and represent the possible gravitational fields in free space.\n\nThe Laplacian occurs in differential equations that describe many physical phenomena, such as electric and gravitational potentials, the diffusion equation for heat and fluid flow, wave propagation, and quantum mechanics. The Laplacian represents the flux density of the gradient flow of a function. For instance, the net rate at which a chemical dissolved in a fluid moves toward or away from some point is proportional to the Laplacian of the chemical concentration at that point; expressed symbolically, the resulting equation is the diffusion equation. For these reasons, it is extensively used in the sciences for modelling all kinds of physical phenomena. The Laplacian is the simplest elliptic operator, and is at the core of Hodge theory as well as the results of de Rham cohomology. In image processing and computer vision, the Laplacian operator has been used for various tasks such as blob and edge detection.\n\nThe Laplace operator is a second order differential operator in the \"n\"-dimensional Euclidean space, defined as the divergence () of the gradient (). Thus if is a twice-differentiable real-valued function, then the Laplacian of is defined by\n\nwhere the latter notations derive from formally writing\nEquivalently, the Laplacian of is the sum of all the \"unmixed\" second partial derivatives in the Cartesian coordinates :\n\nAs a second-order differential operator, the Laplace operator maps functions to functions for . The expression (or equivalently ) defines an operator , or more generally an operator for any open set .\n\nIn the physical theory of diffusion, the Laplace operator (via Laplace's equation) arises naturally in the mathematical description of equilibrium. Specifically, if is the density at equilibrium of some quantity such as a chemical concentration, then the net flux of through the boundary of any smooth region is zero, provided there is no source or sink within :\n\nwhere is the outward unit normal to the boundary of . By the divergence theorem,\n\nSince this holds for all smooth regions , it can be shown that this implies\nThe left-hand side of this equation is the Laplace operator. The Laplace operator itself has a physical interpretation for non-equilibrium diffusion as the extent to which a point represents a source or sink of chemical concentration, in a sense made precise by the diffusion equation.\n\nIf denotes the electrostatic potential associated to a charge distribution , then the charge distribution itself is given by the negative of the Laplacian of :\n\nwhere is the electric constant.\n\nThis is a consequence of Gauss's law. Indeed, if is any smooth region, then by Gauss's law the flux of the electrostatic field is proportional to the charge enclosed:\n\nwhere the first equality is due to the divergence theorem. Since the electrostatic field is the (negative) gradient of the potential, this now gives\n\nSo, since this holds for all regions , we must have\n\nThe same approach implies that the negative of the Laplacian of the gravitational potential is the mass distribution. Often the charge (or mass) distribution are given, and the associated potential is unknown. Finding the potential function subject to suitable boundary conditions is equivalent to solving Poisson's equation.\n\nAnother motivation for the Laplacian appearing in physics is that solutions to in a region are functions that make the Dirichlet energy functional stationary:\n\nTo see this, suppose is a function, and is a function that vanishes on the \nboundary of . Then\n\nwhere the last equality follows using Green's first identity. This calculation shows that if , then is stationary around . Conversely, if is stationary around , then by the fundamental lemma of calculus of variations.\n\nThe Laplace operator in two dimensions is given by:\n\nIn Cartesian coordinates,\n\nwhere and are the standard Cartesian coordinates of the -plane.\n\nIn polar coordinates,\n\nwhere represents the radial distance and the angle.\n\nIn three dimensions, it is common to work with the Laplacian in a variety of different coordinate systems.\n\nIn Cartesian coordinates,\n\nIn cylindrical coordinates,\n\nwhere represents the radial distance, the azimuth angle and the height.\n\nIn spherical coordinates:\n\nwhere represents the azimuthal angle and the zenith angle or co-latitude.\nIn general curvilinear coordinates ():\n\nwhere summation over the repeated indices is implied,\n\nIn arbitrary curvilinear coordinates in dimensions (), we can write the Laplacian in terms of the inverse metric tensor, formula_17:\n\nfrom the Voss-Weyl formula for the divergence.\n\nIn spherical coordinates in dimensions, with the parametrization with representing a positive real radius and an element of the unit sphere ,\n\nwhere is the Laplace–Beltrami operator on the -sphere, known as the spherical Laplacian. The two radial derivative terms can be equivalently rewritten as\n\nAs a consequence, the spherical Laplacian of a function defined on can be computed as the ordinary Laplacian of the function extended to so that it is constant along rays, i.e., homogeneous of degree zero.\n\nThe Laplacian is invariant under all Euclidean transformations: rotations and translations. In two dimensions, for example, this means that\n\nfor all θ, \"a\", and \"b\". In arbitrary dimensions,\nwhenever ρ is a rotation, and likewise\nwhenever τ is a translation. (More generally, this remains true when ρ is an orthogonal transformation such as a reflection.)\n\nIn fact, the algebra of all scalar linear differential operators, with constant coefficients, that commute with all Euclidean transformations, is the polynomial algebra generated by the Laplace operator.\n\nThe spectrum of the Laplace operator consists of all eigenvalues for which there is a corresponding eigenfunction with\n\nThis is known as the Helmholtz equation.\n\nIf is a bounded domain in then the eigenfunctions of the Laplacian are an orthonormal basis for the Hilbert space . This result essentially follows from the spectral theorem on compact self-adjoint operators, applied to the inverse of the Laplacian (which is compact, by the Poincaré inequality and the Rellich–Kondrachov theorem). It can also be shown that the eigenfunctions are infinitely differentiable functions. More generally, these results hold for the Laplace–Beltrami operator on any compact Riemannian manifold with boundary, or indeed for the Dirichlet eigenvalue problem of any elliptic operator with smooth coefficients on a bounded domain. When is the -sphere, the eigenfunctions of the Laplacian are the well-known spherical harmonics.\n\nThe Laplacian also can be generalized to an elliptic operator called the Laplace–Beltrami operator defined on a Riemannian manifold. The d'Alembert operator generalizes to a hyperbolic operator on pseudo-Riemannian manifolds. The Laplace–Beltrami operator, when applied to a function, is the trace () of the function's Hessian:\n\nwhere the trace is taken with respect to the inverse of the metric tensor. The Laplace–Beltrami operator also can be generalized to an operator (also called the Laplace–Beltrami operator) which operates on tensor fields, by a similar formula.\n\nAnother generalization of the Laplace operator that is available on pseudo-Riemannian manifolds uses the exterior derivative, in terms of which the “geometer's Laplacian\" is expressed as\n\nHere is the codifferential, which can also be expressed using the Hodge dual. Note that this operator differs in sign from the \"analyst's Laplacian\" defined above, a point which must always be kept in mind when reading papers in global analysis. More generally, the \"Hodge\" Laplacian is defined on differential forms by\n\nThis is known as the Laplace–de Rham operator, which is related to the Laplace–Beltrami operator by the Weitzenböck identity.\n\nThe Laplacian can be generalized in certain ways to non-Euclidean spaces, where it may be elliptic, hyperbolic, or ultrahyperbolic.\n\nIn the Minkowski space the Laplace–Beltrami operator becomes the D'Alembert operator or D'Alembertian:\n\nIt is the generalisation of the Laplace operator in the sense that it is the differential operator which is invariant under the isometry group of the underlying space and it reduces to the Laplace operator if restricted to time-independent functions. Note that the overall sign of the metric here is chosen such that the spatial parts of the operator admit a negative sign, which is the usual convention in high energy particle physics. The D'Alembert operator is also known as the wave operator, because it is the differential operator appearing in the wave equations and it is also part of the Klein–Gordon equation, which reduces to the wave equation in the massless case.\n\nThe additional factor of in the metric is needed in physics if space and time are measured in different units; a similar factor would be required if, for example, the direction were measured in meters while the direction were measured in centimeters. Indeed, theoretical physicists usually work in units such that in order to simplify the equation.\n\n\n\n"}
{"id": "500958", "url": "https://en.wikipedia.org/wiki?curid=500958", "title": "Lars Hörmander", "text": "Lars Hörmander\n\nLars Valter Hörmander (24 January 1931 – 25 November 2012) was a Swedish mathematician who has been called \"the foremost contributor to the modern theory of linear partial differential equations\". He was awarded the Fields Medal in 1962, the Wolf Prize in 1988, and the Leroy P. Steele Prize in 2006. His \"Analysis of Linear Partial Differential Operators I–IV\" is considered a standard work on the subject of linear partial differential operators.\n\nHörmander completed his Ph.D. in 1955 at Lund University. Hörmander then worked at Stockholm University, at Stanford University, and at the Institute for Advanced Study in Princeton, New Jersey. He returned to Lund University as professor from 1968 until 1996, when he retired with the title of professor emeritus.\n\nHörmander was born in Mjällby, a village in Blekinge in southern Sweden where his father was a teacher. Like his older brothers and sisters before him, he attended the realskola (secondary school), in a nearby town to which he commuted by train, and the gymnasium (high school) in Lund from which he graduated in 1948.\n\nAt the time when he entered the gymnasium, the principal had instituted an experiment of reducing the period of the education from three to two years, and the daily activities to three hours. This freedom to work on his own, \"[greater] than the universities offer in Sweden today\", suited Hörmander \"very well\". He was also positively influenced by his enthusiastic mathematics teacher, a docent at Lund University who encouraged him to study university level mathematics.\n\nAfter proceeding to receive a Master's degree from Lund University in 1950, Hörmander began his graduate studies under Marcel Riesz (who had also been the advisor for Hörmander's gymnasium teacher). He made his first research attempts in classical function theory and harmonic analysis, which \"did not amount to much\" but were \"an excellent preparation for working in the theory of partial differential equations.\" He turned to partial differential equations when Riesz retired and Lars Gårding who worked actively in that area was appointed professor.\n\nHörmander took a one-year break for military service from 1953 to 1954, but due to his position in defense research was able to proceed with his studies even during that time. His Ph.D. thesis \"On the theory of general partial differential operators\" was finished in 1955, inspired by the nearly concurrent Ph.D. work of Bernard Malgrange and techniques for hyperbolic differential operators developed by Lars Gårding and Jean Leray.\n\nHörmander applied for professorship at Stockholm University, but temporarily left for the United States while the request was examined. He spent quarters from winter to fall in respective order at the University of Chicago, the University of Kansas, the University of Minnesota, and finally at the Courant Institute of Mathematical Sciences in New York City. These locations offered \"much to learn\" in partial differential equations, with the exception of Chicago of which he however notes the Antoni Zygmund seminar held by Elias Stein and Guido Weiss to have strengthened his familiarity with harmonic analysis.\n\nIn the theory of linear differential operators, \"many people have contributed but the deepest and most significant results are due to Hörmander\", according to Hörmander's doctoral advisor, Lars Gårding.\n\nHörmander was given a position as part-time professor at Stanford in 1963, but was soon thereafter offered a professorship at the Institute for Advanced Study in Princeton, New Jersey. He first wished not to leave Sweden, but attempts to find a research professorship in Sweden failed and \"the opportunity to do research full time in a mathematically very active environment was hard to resist\", so he accepted the offer and resigned from both Stanford and Stockholm and began at the Institute in the fall of 1964. Within two years of \"hard work\", he felt that the environment at the Institute was too demanding, and in 1967 decided to return to Lund after one year. He later noted that his best work at the Institute was done during the remaining year.\n\nHörmander mostly remained at Lund University as professor after 1968, but made several visits to the United States during the two next decades. He visited the Courant Institute in 1970, and also the Institute for Advanced Study in 1971 and during the academic year 1977–1978 when a special year in microlocal analysis was held. He also visited Stanford in 1971, 1977 and 1982, and the University of California, San Diego in the winter 1990. He was briefly director of the Mittag-Leffler Institute in Stockholm between 1984 and 1986, but only accepted a two-year appointment as he \"suspected that the administrative duties would not agree well\" with him, and found that \"the hunch was right\". He also served as vice president of the International Mathematical Union between 1987 and 1990. Hörmander retired emeritus in Lund in January 1996. In 2006 he was honored with the Leroy P. Steele Prize from the American Mathematical Society.\n\nHe was made a member of the Royal Swedish Academy of Sciences in 1968. In 1970 he gave a plenary address (Linear Differential Operators) at the ICM in Nice.\n\nHe received the 1988 Wolf Prize \"for fundamental work in modern analysis, in particular, the application of pseudo differential and Fourier integral operators to linear partial differential equations\".\n\nIn 2012 he was selected as a fellow of the American Mathematical Society, but died on 25 November 2012, before the list of fellows was released.\n\nHis book \"Linear Partial Differential Operators\", which largely was the cause for his Fields Medal, has been described as \"the first major account of this theory\". It was published by Springer-Verlag in 1963 as part of the \"Grundlehren\" series.\n\nHörmander devoted five years to compiling the four-volume monograph, \"The Analysis of Linear Partial Differential Operators\", first published between 1983 and 1985. A follow-up of his \"Linear Partial Differential Operators\", it \"illustrate[d] the vast expansion of the subject\" over the past 20 years, and is considered the \"standard of the field\". In addition to these works, he has written a recognised introduction to \nseveral complex variables based on his 1964 Stanford lectures, and wrote the entries on differential equations in Nationalencyklopedin.\n\n\n\n"}
{"id": "22105661", "url": "https://en.wikipedia.org/wiki?curid=22105661", "title": "Levi-Civita field", "text": "Levi-Civita field\n\nIn mathematics, the Levi-Civita field, named after Tullio Levi-Civita, is a non-Archimedean ordered field; i.e., a system of numbers containing infinite and infinitesimal quantities. Each member formula_1 can be constructed as a formal series of the form\n\nwhere formula_3 are real numbers, formula_4 is the set of rational numbers, and formula_5 is to be interpreted as a positive infinitesimal. The support of formula_1, i.e., the set of indices of the nonvanishing coefficients formula_7 must be a left-finite set: for any member of formula_4, there are only finitely many members of the set less than it; this restriction is necessary in order to make multiplication and division well defined and unique. The ordering is defined according to the dictionary ordering of the list of coefficients, which is equivalent to the assumption that formula_5 is an infinitesimal.\n\nThe real numbers are embedded in this field as series in which all of the coefficients vanish except formula_10.\n\n\nIf formula_24 and formula_25 are two Levi-Civita series, then \n(One can check that the support of this series is left-finite and that for each of its elements formula_30, the set formula_31 is finite, so the product is well defined.)\n\nEquipped with those operations and order, the Levi-Civita field is indeed an ordered field extension of formula_36 where the series formula_5 is a positive infinitesimal.\n\nThe Levi-Civita field is real-closed, meaning that it can be algebraically closed by adjoining an imaginary unit (\"i\"), or by letting the coefficients be complex. It is rich enough to allow a significant amount of analysis to be done, but its elements can still be represented on a computer in the same sense that real numbers can be represented using floating point. \nIt is the basis of automatic differentiation, a way to perform differentiation in cases that are intractable by symbolic differentiation or finite-difference methods.\n\nThe Levi-Civita field is also Cauchy complete, meaning that relativizing the formula_38 definitions of Cauchy sequence and convergent sequence to sequences of Levi-Civita series, each Cauchy sequence in the field converges. Equivalently, it has no proper dense ordered field extension.\n\nAs an ordered field, it has a natural valuation given by the rational exponent corresponding to the first non zero coefficient of a Levi-Civita series. The valuation ring is that of series bounded by real numbers, the residue field is formula_36, and the value group is formula_40. The resulting valued field is Henselian (being real closed with a convex valuation ring) but not spherically complete. Indeed, the field of Hahn series with real coefficients and value group formula_40 is a proper immediate extension, containing series such as formula_42 which are not in the Levi-Civita field.\n\nThe Levi-Civita field is the Cauchy-completion of the field formula_43 of Puiseux series over the field of real numbers, that is, it is a dense extension of formula_43 without proper dense extension. Here is a list of some of its notable proper subfields and its proper ordered field extensions:\n\n\n\n"}
{"id": "1174850", "url": "https://en.wikipedia.org/wiki?curid=1174850", "title": "Log-space reduction", "text": "Log-space reduction\n\nIn computational complexity theory, a log-space reduction is a reduction computable by a deterministic Turing machine using logarithmic space. Conceptually, this means it can keep a constant number of pointers into the input, along with a logarithmic number of fixed-size integers. It is possible that such a machine may not have space to write down its own output, so the only requirement is that any given bit of the output be computable in log-space. Formally, this reduction is executed via a log-space transducer.\n\nSuch a machine has polynomially-many configurations, so log-space reductions are also polynomial-time reductions. However, log-space reductions are probably weaker than polynomial-time reductions; while any non-empty, non-full language in P is polynomial-time reducible to any other non-empty, non-full language in P, a log-space reduction between a language in NL and a language in L, both subsets of P, would imply the unlikely L = NL. It is an open question if the NP-complete problems are different with respect to log-space and polynomial-time reductions.\n\nLog-space reductions are normally used on languages in P, in which case it usually does not matter whether many-one reductions or Turing reductions are used, since it has been verified that L, SL, NL, and P are all closed under Turing reductions, meaning that Turing reductions can be used to show a problem is in any of these classes. However, other subclasses of P such as NC may not be closed under Turing reductions, and so many-one reductions must be used.\n\nJust as polynomial-time reductions are useless within P and its subclasses, log-space reductions are useless to distinguish problems in L and its subclasses; in particular, every non-empty, non-full problem in L is trivially L-complete under log-space reductions. While even weaker reductions exist, they are not often used in practice, because complexity classes smaller than L (that is, strictly contained or thought to be strictly contained in L) receive relatively little attention.\n\nThe tools available to designers of log-space reductions have been greatly expanded by the result that L = SL; see SL for a list of some SL-complete problems that can now be used as subroutines in log-space reductions.\n"}
{"id": "35761468", "url": "https://en.wikipedia.org/wiki?curid=35761468", "title": "Low-volatility anomaly", "text": "Low-volatility anomaly\n\nThe low-volatility anomaly is the observation that portfolios of low-volatility stocks have higher risk-adjusted returns than portfolios with high-volatility stocks in most markets studied. The capital asset pricing model made some predictions of return versus beta. First, return should be a linear function of beta, and nothing else. Also, the return of a stock with average beta should be the average return of stocks (this is easy to show given the first assumption). Second, the intercept should be equal to the risk-free rate. Then the slope can be computed from these two points. Almost immediately these predictions were challenged on the grounds that they are empirically not true. Studies find that the correct slope is either less than predicted, not significantly different from zero, or even negative. Also, additional factors are predictive of return independent of beta.\n\nBlack proposed a theory where there is a zero-beta return which is different from the risk-free return. This fits the data better since the zero-beta return is different from the risk-free return. It still presumes, on principle, that there is higher return for higher beta.\n\nThe low-volatility anomaly has now been found in the United States over an 85-year period and in global markets for at least the past 20 years.\n\nResearch challenging CAPM's underlying assumptions about risk has been mounting for decades. One challenge was in 1972, when Jensen, Black and Scholes published a study showing what CAPM would look like if one could not borrow at a risk-free rate. Their results indicated that the relationship between beta and realized return was flatter than predicted by CAPM.\n\nShortly after, Robert Haugen and A. James Heins produced a working paper titled “On the Evidence Supporting the Existence of Risk Premiums in the Capital Market”. Studying the period from 1926 to 1971, they concluded that \"over the long run stock portfolios with lesser variance in monthly returns have experienced greater average returns than their ‘riskier’ counterparts\".\n\nThe evidence of the anomaly has been mounting due to numerous studies by both academics and practitioners which confirm the presence of the anomaly throughout the forty years since its initial discovery. Examples include Baker and Haugen (1991), Chan, Karceski and Lakonishok (1999), Jangannathan and Ma (2003), Clarke De Silva and Thorley, (2006) and Baker, Bradley and Wurgler (2011).\n\nFor global equity markets, Blitz and van Vliet (2007),Nielsen and Aylursubramanian (2008), Carvalho, Xiao, Moulin (2011), Blitz, Pang, van Vliet (2012), Baker and Haugen (2012), all find similar results.\n"}
{"id": "19636775", "url": "https://en.wikipedia.org/wiki?curid=19636775", "title": "Maximum cut", "text": "Maximum cut\n\nFor a graph, a maximum cut is a cut whose size is at least the size of any other cut. The problem of finding a maximum cut in a graph is known as the Max-Cut Problem.\n\nThe problem can be stated simply as follows. One wants a subset \"S\" of the vertex set such that the number of edges between \"S\" and the complementary subset is as large as possible.\n\nThere is a more general version of the problem called weighted Max-Cut. In this version each edge has a real number, its weight, and the objective is to maximize not the number of edges but the total weight of the edges between \"S\" and its complement. The weighted Max-Cut problem is often, but not always, restricted to non-negative weights, because negative weights can change the nature of the problem.\n\nThe following decision problem related to maximum cuts has been studied widely in theoretical computer science:\n\nThis problem is known to be NP-complete. It is easy to see that the problem is in NP: a \"yes\" answer is easy to prove by presenting a large enough cut. The NP-completeness of the problem can be shown, for example, by a reduction from maximum 2-satisfiability (a restriction of the maximum satisfiability problem). The weighted version of the decision problem was one of Karp's 21 NP-complete problems; Karp showed the NP-completeness by a reduction from the partition problem.\n\nThe canonical optimization variant of the above decision problem is usually known as the \"Maximum-Cut Problem\" or \"Max-Cut\" and is defined as:\n\nAs the Max-Cut Problem is NP-hard, no polynomial-time algorithms for Max-Cut in general graphs are known.\n\nHowever, in planar graphs, the Maximum-Cut Problem is dual to the route inspection problem (the problem of finding a shortest tour that visits each edge of a graph at least once), in the sense that the edges that do not belong to a maximum cut-set of a graph \"G\" are the duals of the edges that are doubled in an optimal inspection tour of the dual graph of \"G\". The optimal inspection tour forms a self-intersecting curve that separates the plane into two subsets, the subset of points for which the winding number of the curve is even and the subset for which the winding number is odd; these two subsets form a cut that includes all of the edges whose duals appear an odd number of times in the tour. The route inspection problem may be solved in polynomial time, and this duality allows the maximum cut problem to also be solved in polynomial time for planar graphs. The Maximum-Bisection problem is known however to be NP-hard.\n\nThe Max-Cut Problem is APX-hard, meaning that there is no polynomial-time approximation scheme (PTAS), arbitrarily close to the optimal solution, for it, unless P = NP. Thus, every polynomial-time approximation algorithm achieves an approximation ratio strictly less than one.\n\nThere is a simple randomized 0.5-approximation algorithm: for each vertex flip a coin to decide to which half of the partition to assign it. In expectation, half of the edges are cut edges. This algorithm can be derandomized with the method of conditional probabilities; therefore there is a simple deterministic polynomial-time 0.5-approximation algorithm as well. One such algorithm starts with an arbitrary partition of the vertices of the given graph formula_1 and repeatedly moves one vertex at a time from one side of the partition to the other, improving the solution at each step, until no more improvements of this type can be made. The number of iterations is at most formula_2 because the algorithm improves the cut by at least one edge at each step. When the algorithm terminates, at least half of the edges incident to every vertex belong to the cut, for otherwise moving the vertex would improve the cut. Therefore, the cut includes at least formula_3 edges.\n\nThe polynomial-time approximation algorithm for Max-Cut with the best known approximation ratio is a method by Goemans and Williamson using semidefinite programming and randomized rounding that achieves an approximation ratio formula_4, where formula_5. If the unique games conjecture is true, this is the best possible approximation ratio for maximum cut.\nWithout such unproven assumptions, it has been proven to be NP-hard to approximate the max-cut value with an approximation ratio better than formula_6.\n\n\n\n"}
{"id": "1551981", "url": "https://en.wikipedia.org/wiki?curid=1551981", "title": "Medical algorithm", "text": "Medical algorithm\n\nA medical algorithm is any computation, formula, statistical survey, nomogram, or look-up table, useful in healthcare. Medical algorithms include decision tree approaches to healthcare treatment (e.g., if symptoms A, B, and C are evident, then use treatment X) and also less clear-cut tools aimed at reducing or defining uncertainty.\n\nMedical algorithms are part of a broader field which is usually fit under the aims of medical informatics and medical decision-making. Medical decisions occur in several areas of medical activity including medical test selection, diagnosis, therapy and prognosis, and automatic control of medical equipment.\n\nIn relation to logic-based and artificial neural network-based clinical decision support systems, which are also computer applications used in the medical decision-making field, algorithms are less complex in architecture, data structure and user interface. Medical algorithms are not necessarily implemented using digital computers. In fact, many of them can be represented on paper, in the form of diagrams, nomographs, etc.\n\nA wealth of medical information exists in the form of published medical algorithms. These algorithms range from simple calculations to complex outcome predictions. Most clinicians use only a small subset routinely.\n\nExamples of medical algorithms are:\n\nA common class of algorithms are embedded in guidelines on the choice of treatments produced by many national, state, financial and local healthcare organisations and provided as knowledge resources for day to day use and for induction of new physicians. A field which has gained particular attention is the choice of medications for psychiatric conditions. In the United Kingdom, guidelines or algorithms for this have been produced by most of the circa 500 primary care trusts, substantially all of the circa 100 secondary care psychiatric units and many of the circa 10 000 general practices. In the US, there is a national (federal) initiative to provide them for all states, and by 2005 six states were adapting the approach of the Texas Medication Algorithm Project or otherwise working on their production.\n\nA grammar—the Arden syntax—exists for describing algorithms in terms of medical logic modules. An approach such as this should allow exchange of MLMs between doctors and establishments, and enrichment of the common stock of tools.\n\nThe intended purpose of medical algorithms is to improve and standardize decisions made in the delivery of medical care. Medical algorithms assist in standardizing selection and application of treatment regimens, with algorithm automation intended to reduce potential introduction of errors. Some attempt to predict the outcome, for example critical care scoring systems.\n\nComputerized health diagnostics algorithms can provide timely clinical decision support, improve adherence to evidence-based guidelines, and be a resource for education and research. \n\nMedical algorithms based on best practice can assist everyone involved in delivery of standardized treatment via a wide range of clinical care providers. Many are presented as protocols and it is a key task in training to ensure people step outside the protocol when necessary. In our present state of knowledge, generating hints and producing guidelines may be less satisfying to the authors, but more appropriate.\n\nIn common with most science and medicine, algorithms whose contents are not wholly available for scrutiny and open to improvement should be regarded with suspicion. \n\nComputations obtained from medical algorithms should be compared with, and tempered by, clinical knowledge and physician judgment.\n\n"}
{"id": "615222", "url": "https://en.wikipedia.org/wiki?curid=615222", "title": "Multivariable calculus", "text": "Multivariable calculus\n\nMultivariable calculus (also known as multivariate calculus) is the extension of calculus in one variable to calculus with functions of several variables: the differentiation and integration of functions involving multiple variables, rather than just one.\n\nA study of limits and continuity in multivariable calculus yields many counter-intuitive results not demonstrated by single-variable functions. For example, there are scalar functions of two variables with points in their domain which give different limits when approached along different paths. E.g., the function\napproaches zero, whenever the point is approached along lines through the origin However, when the origin is approached along a parabola formula_2, the function value has a limit of formula_3. Since taking different paths toward the same point yields different limit values, a general limit does not exist there.\n\nContinuity in each argument not being sufficient for multivariate continuity can also be seen from the following example. In particular, for a real-valued function with two real-valued parameters, formula_4, continuity of formula_5 in formula_6 for fixed formula_7 and continuity of formula_5 in formula_7 for fixed formula_6 does not imply continuity of formula_5.\n\nConsider\n\nIt is easy to verify that this function is zero by definition on the boundary and outside of the quadrangle formula_13. Furthermore, the functions defined for constant and and formula_14 by\nare continuous. Specifically,\n\nHowever, the sequence formula_18 (for natural formula_19) converges to formula_20, rendering the function as discontinuous at formula_21 Approaching the origin not along parallels to the \"x\"- and \"y\"-axis reveals this discontinuity.\n\nThe partial derivative generalizes the notion of the derivative to higher dimensions. A partial derivative of a multivariable function is a derivative with respect to one variable with all other variables held constant.\n\nPartial derivatives may be combined in interesting ways to create more complicated expressions of the derivative. In vector calculus, the del operator (formula_22) is used to define the concepts of gradient, divergence, and curl in terms of partial derivatives. A matrix of partial derivatives, the Jacobian matrix, may be used to represent the derivative of a function between two spaces of arbitrary dimension. The derivative can thus be understood as a linear transformation which directly varies from point to point in the domain of the function.\n\nDifferential equations containing partial derivatives are called partial differential equations or PDEs. These equations are generally more difficult to solve than ordinary differential equations, which contain derivatives with respect to only one variable.\n\nThe multiple integral expands the concept of the integral to functions of any number of variables. Double and triple integrals may be used to calculate areas and volumes of regions in the plane and in space. Fubini's theorem guarantees that a multiple integral may be evaluated as a \"repeated integral\" or \"iterated integral\" as long as the integrand is continuous throughout the domain of integration.\n\nThe surface integral and the line integral are used to integrate over curved manifolds such as surfaces and curves.\n\nIn single-variable calculus, the fundamental theorem of calculus establishes a link between the derivative and the integral. The link between the derivative and the integral in multivariable calculus is embodied by the integral theorems of vector calculus:\n\nIn a more advanced study of multivariable calculus, it is seen that these four theorems are specific incarnations of a more general theorem, the generalized Stokes' theorem, which applies to the integration of differential forms over manifolds.\n\nTechniques of multivariable calculus are used to study many objects of interest in the material world. In particular,\n\nMultivariable calculus can be applied to analyze deterministic systems that have multiple degrees of freedom. Functions with independent variables corresponding to each of the degrees of freedom are often used to model these systems, and multivariable calculus provides tools for characterizing the system dynamics.\n\nMultivariate calculus is used in the optimal control of continuous time dynamic systems. It is used in regression analysis to derive formulas for estimating relationships among various sets of empirical data.\n\nMultivariable calculus is used in many fields of natural and social science and engineering to model and study high-dimensional systems that exhibit deterministic behavior. In economics, for example, consumer choice over a variety of goods, and producer choice over various inputs to use and outputs to produce, are modeled with multivariate calculus. Quantitative analysts in finance also often use multivariate calculus to predict future trends in the stock market.\n\nNon-deterministic, or stochastic systems can be studied using a different kind of mathematics, such as stochastic calculus.\n\n\n"}
{"id": "2399097", "url": "https://en.wikipedia.org/wiki?curid=2399097", "title": "Numbering (computability theory)", "text": "Numbering (computability theory)\n\nIn computability theory a numbering is the assignment of natural numbers to a set of objects such as functions, rational numbers, graphs, or words in some language. A numbering can be used to transfer the idea of computability and related concepts, which are originally defined on the natural numbers using computable functions, to these different types of objects.\n\nCommon examples of numberings include Gödel numberings in first-order logic and admissible numberings of the set of partial computable functions.\n\nA numbering of a set formula_1 is a surjective partial function from formula_2 to \"S\" (Ershov 1999:477). The value of a numbering ν at a number \"i\" (if defined) is often written ν instead of the usual formula_3.\n\nExamples of numberings include:\n\nA numbering is total if it is a total function. If the domain of a partial numbering is recursively enumerable then there always exists an equivalent total numbering (equivalence of numberings is defined below).\n\nA numbering η is decidable if the set formula_10 is a decidable set.\n\nA numbering η is single-valued if η(\"x\") = η(\"y\") if and only if \"x\"=\"y\"; in other words if η is an injective function. A single-valued numbering of the set of partial computable functions is called a Friedberg numbering.\n\nThere is a partial ordering on the set of all numberings. Let formula_11 and formula_12 be two numberings. Then formula_13 is reducible to formula_14, written formula_15, \nif \n\nIf formula_15 and formula_18 then formula_13 is equivalent to formula_14; this is written formula_21.\n\nWhen the objects of the set \"S\" are sufficiently \"constructive\", it is common to look at numberings that can be effectively decoded (Ershov 1999:486). For example, if \"S\" consists of recursively enumerable sets, the numbering η is computable if the set of pairs (\"x\",\"y\") where \"y\"∈η(\"x\") is recursively enumerable. Similarly, a numbering \"g\" of partial functions is computable if the relation \"R\"(\"x\",\"y\",\"z\") = \"[\"g\"(\"x\")](\"y\") = \"z\"\" is partial recursive (Ershov 1999:487).\n\nA computable numbering is called principal if every computable numbering of the same set is reducible to it. Both the set of all r.e. subsets of formula_2 and the set of all partial computable functions have principle numberings (Ershov 1999:487). A principle numbering of the set of partial recursive functions is known as an admissible numbering in the literature.\n\n\n"}
{"id": "3709199", "url": "https://en.wikipedia.org/wiki?curid=3709199", "title": "Observable variable", "text": "Observable variable\n\nIn statistics, observable variable or observable quantity (also manifest variables), as opposed to \"latent variable\", is a variable that can be observed and directly measured.\n\n"}
{"id": "22035175", "url": "https://en.wikipedia.org/wiki?curid=22035175", "title": "Pace count beads", "text": "Pace count beads\n\nPace count beads or ranger beads is a manual counting tool used to keep track of distance traveled through a pace count. It is used in military land navigation or orienteering.\n\nThe tool is usually constructed using a set of 13 beads on a length of cord. The beads are divided into two sections, separated by a knot. Nine beads are used in the lower section, and four or more beads are used in the upper section. There is often a loop in the upper end, making it possible to attach the tool to the user's gear with a simple Larks head hitch.\n\nThere are two ways to use the beads. One is to represent the paces the user has walked, while the other is to represent the distance walked. Both methods requires the user to know the relationship between the paces walked and the distance travelled.\n\nAs users walk, they typically slide one bead on the cord for every ten paces taken. On the tenth pace, the user slides a bead in the lower section towards the knot. After the 90th pace, all nine beads are against the knot. On the 100th pace, all nine beads in the lower section are returned away from the knot, and a bead from the upper section is slid upwards, away from the knot.\n\nIn this manner, the user calculates distance traveled by keeping track of paces taken. To use this method, the user must know the length of his pace to accurately calculate distance traveled. Also, the number of paces to be walked must be precalculated (example: 2,112 paces= one mile, based on 30 inch pace) and then the distance traveled has to be calculated from the walked paces.\n\nFor every 100 meters the user walks, one of the lower beads is pulled down. When the ninth of the lower beads is pulled, the user has walked 900 meters. When the user has walked 1000 meters, one of the upper beads is pulled down, and all the lower beads are pulled back up.\n\nUsing this method the user must know the number of paces walked in 100 meters. An experienced user can also adapt the pace count for each hundred meters depending on the terrain. When using this method the user does not have to calculate distance from number of paces.\n\nThis method can of course be used for non-metric distances as well, though with the beads arranged in a different manner.\n"}
{"id": "5866008", "url": "https://en.wikipedia.org/wiki?curid=5866008", "title": "Privilege revocation (computing)", "text": "Privilege revocation (computing)\n\nPrivilege revocation is the act of an entity giving up some, or all of, the privileges they possess, or some authority taking those (privileged) rights away.\n\nHonoring the Principle of least privilege at a granularity provided by the base system such as sandboxing of (to that point successful) attacks to an unprivileged user account helps in reliability of computing services provided by the system. As the chances of restarting such a process are better, and other services on the same machine aren't affected (or at least probably not as much as in the alternative case: i.e. a privileged process gone haywire instead).\n\nIn computing security \"privilege revocation\" is a measure taken by a program to protect the system against misuse of itself.\n\nPrivilege revocation is a variant of privilege separation whereby the program terminates the privileged part immediately after it has served its purpose. If a program doesn't revoke privileges, it risks the escalation of privileges.\n\nRevocation of privileges is a technique of defensive programming.\n\n"}
{"id": "3473949", "url": "https://en.wikipedia.org/wiki?curid=3473949", "title": "Repetition code", "text": "Repetition code\n\nIn coding theory, the repetition code is one of the most basic error-correcting codes. In order to transmit a message over a noisy channel that may corrupt the transmission in a few places, the idea of the repetition code is to just repeat the message several times. The hope is that the channel corrupts only a minority of these repetitions. This way the receiver will notice that a transmission error occurred since the received data stream is not the repetition of a single message, and moreover, the receiver can recover the original message by looking at the received message in the data stream that occurs most often.\n\nBecause of the bad error correcting performance and the low ratio between information symbols and actually transmitted symbols, other error correction codes are preferred in most cases. The chief attraction of the repetition code is the ease of implementation.\n\nIn the case of a binary repetition code, there exist two code words - all ones and all zeros - which have a length of formula_1. Therefore, the minimum Hamming distance of the code equals its length formula_1. This gives the repetition code an error correcting capacity of formula_3 (i.e. it will correct up to formula_3 errors in any code word).\n\nIf the length of a binary repetition code is odd, then it's a perfect code. The binary repetition code of length 3 is equivalent to the (3,1)-Hamming code.\n\nLet's consider a binary repetition code of length 3. The user wants to transmit the information bits codice_1. Then the encoding maps each bit either to the all ones or all zeros code word, so we get the codice_2, which will be transmitted.\n\nLet's say three errors corrupt the transmitted bits and the received sequence is codice_3. Decoding is usually done by a simple majority decision for each code word. That lead us to codice_4 as the decoded information bits, because in the first and second code word occurred less than two errors, so the majority of the bits are correct. But in the third code word two bits are corrupted, which results in an erroneous information bit, since two errors lie above the error correcting capacity.\n\nDespite their poor performance as stand-alone codes, use in Turbo code-like iteratively decoded concatenated coding schemes, such as repeat-accumulate (RA) and accumulate-repeat-accumulate (ARA) codes, allows for surprisingly good error correction performance.\n\nRepetition codes are one of the few known codes whose code rate can be automatically adjusted to varying channel capacity, by sending more or less parity information as required to overcome the channel noise, and it is the only such code known for non-erasure channels. Practical adaptive codes for erasure channels have been invented only recently, and are known as fountain codes.\n\nSome UARTs, such as the ones used in the FlexRay protocol, use a majority filter to ignore brief noise spikes. This spike-rejection filter can be seen as a kind of repetition decoder.\n"}
{"id": "169945", "url": "https://en.wikipedia.org/wiki?curid=169945", "title": "Rounding", "text": "Rounding\n\nRounding a numerical value means replacing it by another value that is approximately equal but has a shorter, simpler, or more explicit representation; for example, replacing $ with $, or the fraction 312/941 with 1/3, or the expression with .\n\nRounding is often done to obtain a value that is easier to report and communicate than the original. Rounding can also be important to avoid misleadingly precise reporting of a computed number, measurement or estimate; for example, a quantity that was computed as but is known to be accurate only to within a few hundred units is better stated as \"about \".\n\nOn the other hand, rounding of exact numbers will introduce some round-off error in the reported result. Rounding is almost unavoidable when reporting many computations – especially when dividing two numbers in integer or fixed-point arithmetic; when computing mathematical functions such as square roots, logarithms, and sines; or when using a floating-point representation with a fixed number of significant digits. In a sequence of calculations, these rounding errors generally accumulate, and in certain ill-conditioned cases they may make the result meaningless.\n\nAccurate rounding of transcendental mathematical functions is difficult because the number of extra digits that need to be calculated to resolve whether to round up or down cannot be known in advance. This problem is known as \"the table-maker's dilemma\".\n\nRounding has many similarities to the quantization that occurs when physical quantities must be encoded by numbers or digital signals.\n\nA wavy equals sign (≈: \"approximately equal to\") is sometimes used to indicate rounding of exact numbers, e.g., 0.75 ≈ 1. This sign was introduced by Alfred George Greenhill in 1892.\n\nIdeal characteristics of rounding methods include:\n\n\nBut, because it is not usually possible for a method to satisfy all ideal characteristics, many methods exist.\n\nAs a general rule, rounding is idempotent, i.e., once a number has been rounded, rounding it again will not change its value. In practice, rounding functions are also monotonic.\n\nTypical rounding problems include:\nThe most basic form of rounding is to replace an arbitrary number by an integer. All the following rounding modes are concrete implementations of an abstract single-argument \"round()\" procedure. These are true functions with the exception of those that are random-based.\n\nThese four methods are called directed rounding, as the displacements from the original number to the rounded value are all directed towards or away from the same limiting value (0, +∞, or −∞). Directed rounding is used in Interval arithmetic and is often required in financial calculations.\n\nIf is positive, round-down is the same as round-towards-zero, and round-up is the same as round-away-from-zero. If is negative, round-down is the same as round-away-from-zero, and round-up is the same as round-towards-zero. In any case, if is integer, is just .\n\nWhere many calculations are done in sequence, the choice of rounding method can have a very significant effect on the result. A famous instance involved a new index set up by the Vancouver Stock Exchange in 1982. It was initially set at 1000.000 (three decimal places of accuracy), and after 22 months had fallen to about 520 — whereas stock prices had generally increased in the period. The problem was caused by the index being recalculated thousands of times daily, and always being rounded down to 3 decimal places, in such a way that the rounding errors accumulated. Recalculating with better rounding gave an index value of 1098.892 at the end of the same period.\n\nFor the examples below, refers to the sign function applied to the original number, .\n\n\n\n\n\nRounding a number to the nearest integer requires some tie-breaking rule for those cases when is exactly half-way between two integers — that is, when the fraction part of is exactly 0.5.\n\nIf it were not for the 0.5 fractional parts, the round-off errors introduced by the round to nearest method would be symmetric: for every fraction that gets rounded up (such as 0.268), there is a complementary fraction (namely, 0.732) that gets rounded down by the same amount.\n\nWhen rounding a large set of fixed point numbers with uniformly distributed fractional parts, the rounding errors by all values, with the omission of those having 0.5 fractional part, would statistically compensate each other. This means that the expected (average) value of the rounded numbers is equal to the expected value of the original numbers when we remove numbers with fractional part 0.5 from the set. \n\nIn practice floating point numbers are typically used which have even more computational nuances because they are not equally spaced.\n\nThe following tie-breaking rule, called round half up (or round half towards positive infinity), is widely used in many disciplines. That is, half-way values of are always rounded up.\n\nFor example, by this rule the value 23.5 gets rounded to 24, but −23.5 gets rounded to −23.\n\nHowever, some programming languages (such as Java, Python) define their \"half up\" as \"round half away from zero\" here.\n\nThis method only requires checking one digit to determine rounding direction in 2's complement and similar representations.\n\nOne may also use round half down (or round half towards negative infinity) as opposed to the more common \"round half up\".\n\nFor example, 23.5 gets rounded to 23, and −23.5 gets rounded to −24.\n\nOne may also round half towards zero (or round half away from infinity) as opposed to the conventional \"round half away from zero\".\n\nFor example, 23.5 gets rounded to 23, and −23.5 gets rounded to −23.\n\nThis method treats positive and negative values symmetrically, and therefore is free of overall positive/negative bias if the original numbers are positive or negative with equal probability. It does, however, still have bias towards zero.\n\nThe other tie-breaking method commonly taught and used is the round half away from zero (or round half towards infinity), namely:\n\nFor example, 23.5 gets rounded to 24, and −23.5 gets rounded to −24.\n\nThis can be more efficient on binary computers because only the first omitted bit needs to be considered to determine if it rounds up (on a 1) or down (on a 0). This is one method used when rounding to significant figures due to its simplicity.\n\nThis method, also known as commercial rounding, treats positive and negative values symmetrically, and therefore is free of overall positive/negative bias if the original numbers are positive or negative with equal probability. It does, however, still have bias away from zero.\n\nIt is often used for currency conversions and price roundings (when the amount is first converted into the smallest significant subdivision of the currency, such as cents of a euro) as it is easy to explain by just considering the first fractional digit, independently of supplementary precision digits or sign of the amount (for strict equivalence between the paying and recipient of the amount).\n\nA tie-breaking rule without positive/negative bias \"and\" without bias toward/away from zero is round half to even. By this convention, if the fractional part of is 0.5, then is the even integer nearest to . Thus, for example, +23.5 becomes +24, as does +24.5; while −23.5 becomes −24, as does −24.5. This function minimizes the expected error when summing over rounded figures, even when the inputs are mostly positive or mostly negative.\n\nThis variant of the round-to-nearest method is also called convergent rounding, statistician's rounding, Dutch rounding, Gaussian rounding, odd–even rounding, or bankers' rounding.\n\nThis is the default rounding mode used in IEEE 754 computing functions and operators (see also Nearest integer function), and the more sophisticated mode used when rounding to significant figures.\n\nBy eliminating bias, repeated rounded addition or subtraction of independent numbers will give a result with an error that tends to grow in proportion to the square root of the number of operations rather than linearly. See random walk for more.\n\nHowever, this rule distorts the distribution by increasing the probability of evens relative to odds. Typically this is less important than the biases that are eliminated by this method.\n\nA similar tie-breaking rule is round half to odd. In this approach, if the fraction of is 0.5, then is the odd integer nearest to . Thus, for example, +23.5 becomes +23, as does +22.5; while −23.5 becomes −23, as does −22.5.\n\nThis method is also free from positive/negative bias and bias toward/away from zero.\n\nThis variant is almost never used in computations, except in situations where one wants to avoid increasing the scale of floating-point numbers, which have a limited exponent range. With \"round half to even\", a non-infinite number would round to infinity, and a small value would round to a normal non-zero value. Effectively, this mode prefers preserving the existing scale of tie numbers, avoiding out-of-range results when possible for even-based number systems (such as binary and decimal).\n\nOne method, more obscure than most, is to alternate direction when rounding a number with 0.5 fractional part. All others are rounded to the closest integer.\n\n\nIf occurrences of 0.5 fractional parts occur significantly more than a restart of the occurrence \"counting\" then it is effectively bias free. With guaranteed zero bias, it is useful if the numbers are to be summed or averaged.\n\n\nLike round-half-to-even and round-half-to-odd, this rule is essentially free of overall bias; but it is also fair among even and odd values. The advantage over alternate tie-breaking is that the last direction of rounding on 0.5 fractional part does not have to be \"remembered\".\nRounding as follows to one of the closest straddling integers with a probability dependent on the proximity is called stochastic rounding and will give an unbiased result on average.\n\nFor example, 1.6 would be rounded to 1 with probability 0.4 and to 2 with probability 0.6.\n\nStochastic rounding is accurate in a way that a rounding function can never be. For example, say you started with 0 and added 0.3 to that one hundred times while rounding the running total between every addition. The result would be 0 with regular rounding, but with stochastic rounding, the expected result would be 30, which is the same value obtained without rounding. This can be useful in machine learning where the training may use low precision arithmetic iteratively. Stochastic rounding is a way to achieve 1-dimensional dithering.\n\nThe most common type of rounding is to round to an integer; or, more generally, to an integer multiple of some increment — such as rounding to whole tenths of seconds, hundredths of a dollar, to whole multiples of 1/2 or 1/8 inch, to whole dozens or thousands, etc.\n\nIn general, rounding a number to a multiple of some specified positive value entails the following steps:\n\nFor example, rounding  = 2.1784 dollars to whole cents (i.e., to a multiple of 0.01) entails computing 2.1784/0.01 = 217.84, then rounding that to 218, and finally computing 218 × 0.01 = 2.18.\n\nWhen rounding to a predetermined number of significant digits, the increment depends on the magnitude of the number to be rounded (or of the rounded result).\n\nThe increment is normally a finite fraction in whatever number system is used to represent the numbers. For display to humans, that usually means the decimal number system (that is, is an integer times a power of 10, like 1/1000 or 25/100). For intermediate values stored in digital computers, it often means the binary number system ( is an integer times a power of 2).\n\nThe abstract single-argument \"round()\" function that returns an integer from an arbitrary real value has at least a dozen distinct concrete definitions presented in the rounding to integer section. The abstract two-argument \"roundToMultiple()\" function is formally defined here, but in many cases it is used with the implicit value  = 1 for the increment and then reduces to the equivalent abstract single-argument function, with also the same dozen distinct concrete definitions.\n\nRounding to a specified \"power\" is very different from rounding to a specified \"multiple\"; for example, it is common in computing to need to round a number to a whole power of 2. The steps, in general, to round a positive number to a power of some specified integer greater than 1, are:\n\nMany of the caveats applicable to rounding to a multiple are applicable to rounding to a power.\n\nThis type of rounding, which is also named rounding to a logarithmic scale, is a variant of rounding to a specified power. Rounding on a logarithmic scale is accomplished by taking the log of the amount and doing normal rounding to the nearest value on the log scale.\n\nFor example, resistors are supplied with preferred numbers on a logarithmic scale. For example, for resistors with 10% accuracy they are supplied with nominal values 100, 121, 147, 178, 215 etc. If a calculation indicates a resistor of 165 ohms is required then log(147)=2.167, log(165)=2.217 and log(178)=2.250. The logarithm of 165 is closer to the logarithm of 178 therefore a 178 ohm resistor would be the first choice if there are no other considerations.\n\nWhether a value rounds to or depends upon whether the squared value is greater than or less than the product . The value 165 rounds to 178 in the resistors example because is greater than .\n\nIn floating-point arithmetic, rounding aims to turn a given value into a value with a specified number of digits. In other words, should be a multiple of a number that depends on the magnitude of . The number is a power of the base (usually 2 or 10) of the floating-point representation.\n\nApart from this detail, all the variants of rounding discussed above apply to the rounding of floating-point numbers as well. The algorithm for such rounding is presented in the Scaled rounding section above, but with a constant scaling factor  = 1, and an integer base  > 1.\n\nWhere the rounded result would overflow the result for a directed rounding is either the appropriate signed infinity when \"rounding away from zero\", or the highest representable positive finite number (or the lowest representable negative finite number if is negative), when \"rounding towards zero\". The result of an overflow for the usual case of \"round to nearest\" is always the appropriate infinity.\n\nIn some contexts it is desirable to round a given number to a \"neat\" fraction — that is, the nearest fraction  = / whose numerator and denominator do not exceed a given maximum. This problem is fairly distinct from that of rounding a value to a fixed number of decimal or binary digits, or to a multiple of a given unit . This problem is related to Farey sequences, the Stern–Brocot tree, and continued fractions.\n\nFinished lumber, writing paper, capacitors, and many other products are usually sold in only a few standard sizes.\n\nMany design procedures describe how to calculate an approximate value, and then \"round\" to some standard size using phrases such as \"round down to nearest standard value\", \"round up to nearest standard value\", or \"round to nearest standard value\".\n\nWhen a set of preferred values is equally spaced on a logarithmic scale, choosing the closest preferred value to any given value can be seen as a form of scaled rounding. Such rounded values can be directly calculated.\n\nWhen digitizing continuous signals, such as sound waves, the overall effect of a number of measurements is more important than the accuracy of each individual measurement. In these circumstances, dithering, and a related technique, error diffusion, are normally used. A related technique called pulse-width modulation is used to achieve analog type output from an inertial device by rapidly pulsing the power with a variable duty cycle.\n\nError diffusion tries to ensure the error, on average, is minimized. When dealing with a gentle slope from one to zero, the output would be zero for the first few terms until the sum of the error and the current value becomes greater than 0.5, in which case a 1 is output and the difference subtracted from the error so far. Floyd–Steinberg dithering is a popular error diffusion procedure when digitizing images.\n\nAs a one-dimensional example, suppose the numbers , , , and occur in order and each is to be rounded to a multiple of . In this case the cumulative sums, , , , and , are each rounded to a multiple of : , , , and . The first of these and the differences of adjacent values give the desired rounded values: , , , and .\n\nMonte Carlo arithmetic is a technique in Monte Carlo methods where the rounding is randomly up or down. Stochastic rounding can be used for Monte Carlo arithmetic, but in general, just rounding up or down with equal probability is more often used. Repeated runs will give a random distribution of results which can indicate how stable the computation is.\n\nIt is possible to use rounded arithmetic to evaluate the exact value of a function with integer domain and range. For example, if we know that an integer is a perfect square, we can compute its square root by converting to a floating-point value , computing the approximate square root of with floating point, and then rounding to the nearest integer . If is not too big, the floating-point round-off error in will be less than 0.5, so the rounded value will be the exact square root of . This is essentially why slide rules could be used for exact arithmetic.\n\nRounding a number twice in succession to different levels of precision, with the latter precision being coarser, is not guaranteed to give the same result as rounding once to the final precision except in the case of directed rounding. For instance rounding 9.46 to one decimal gives 9.5, and then 10 when rounding to integer using rounding half to even, but would give 9 when rounded to integer directly. Borman and Chatfield discuss the implications of double rounding when comparing data rounded to one decimal place to specification limits expressed using integers.\n\nIn \"Martinez v. Allstate\" and \"Sendejo v. Farmers\", litigated between 1995 and 1997, the insurance companies argued that double rounding premiums was permissible and in fact required. The US courts ruled against the insurance companies and ordered them to adopt rules to ensure single rounding.\n\nSome computer languages and the IEEE 754-2008 standard dictate that in straightforward calculations the result should not be rounded twice. This has been a particular problem with Java as it is designed to be run identically on different machines, special programming tricks have had to be used to achieve this with x87 floating point.\nThe Java language was changed to allow different results where the difference does not matter and require a strictfp qualifier to be used when the results have to conform accurately.\n\nIn some algorithms, an intermediate result is computed and rounded and then, after more computation, must be rounded to the final precision. Double rounding error can be avoided by choosing an adequate rounding precision for the intermediate computation and/or by using a proven rounding method for intermediate and final rounding.\n\nWilliam Kahan coined the term \"The Table-Maker's Dilemma\" for the unknown cost of rounding transcendental functions:\n\"Nobody knows how much it would cost to compute correctly rounded for two floating-point arguments at which it does not over/underflow. Instead, reputable math libraries compute elementary transcendental functions mostly within slightly more than half an ulp and almost always well within one ulp. Why can't be rounded within half an ulp like SQRT? Because nobody knows how much computation it would cost... No general way exists to predict how many extra digits will have to be carried to compute a transcendental expression and round it to some preassigned number of digits. Even the fact (if true) that a finite number of extra digits will ultimately suffice may be a deep theorem.\"\n\nThe IEEE floating-point standard guarantees that add, subtract, multiply, divide, fused multiply–add, square root, and floating-point remainder will give the correctly rounded result of the infinite precision operation. No such guarantee was given in the 1985 standard for more complex functions and they are typically only accurate to within the last bit at best. However, the 2008 standard guarantees that conforming implementations will give correctly rounded results which respect the active rounding mode; implementation of the functions, however, is optional.\n\nUsing the Gelfond–Schneider theorem and Lindemann–Weierstrass theorem many of the standard elementary functions can be proved to return transcendental results when given rational non-zero arguments; therefore it is always possible to correctly round such functions. However, determining a limit for a given precision on how accurate results need to be computed, before a correctly rounded result can be guaranteed, may demand a lot of computation time.\n\nSome programming packages offer correct rounding. The GNU MPFR package gives correctly rounded arbitrary precision results. Some other libraries implement elementary functions with correct rounding in double precision:\n\nThere exist computable numbers for which a rounded value can never be determined no matter how many digits are calculated. Specific instances cannot be given but this follows from the undecidability of the halting problem. For instance, if Goldbach's conjecture is true but unprovable, then the result of rounding the following value up to the next integer cannot be determined: 10 where is the first even number greater than 4 which is not the sum of two primes, or 0 if there is no such number. The result is 1 if such a number exists and 0 if no such number exists. The value before rounding can however be approximated to any given precision even if the conjecture is unprovable.\n\nRounding can adversely affect a string search for a number. For example, rounded to four digits is \"3.1416\" but a simple search for this string will not discover \"3.14159\" or any other value of rounded to more than four digits. In contrast, truncation does not suffer from this problem; for example, a simple string search for \"3.1415\", which is truncated to four digits, will discover values of truncated to more than four digits.\n\nThe concept of rounding is very old, perhaps older even than the concept of division. Some ancient clay tablets found in Mesopotamia contain tables with rounded values of reciprocals and square roots in base 60.\nRounded approximations to π, the length of the year, and the length of the month are also ancient—see base 60 examples.\n\nThe \"round-to-even\" method has served as the ASTM (E-29) standard since 1940. The origin of the terms \"unbiased rounding\" and \"statistician's rounding\" are fairly self-explanatory. In the 1906 fourth edition of \"Probability and Theory of Errors\" Robert Simpson Woodward called this \"the computer's rule\" indicating that it was then in common use by human computers who calculated mathematical tables. Churchill Eisenhart indicated the practice was already \"well established\" in data analysis by the 1940s.\n\nThe origin of the term \"bankers' rounding\" remains more obscure. If this rounding method was ever a standard in banking, the evidence has proved extremely difficult to find. To the contrary, section 2 of the European Commission report \"The Introduction of the Euro and the Rounding of Currency Amounts\" suggests that there had previously been no standard approach to rounding in banking; and it specifies that \"half-way\" amounts should be rounded up.\n\nUntil the 1980s, the rounding method used in floating-point computer arithmetic was usually fixed by the hardware, poorly documented, inconsistent, and different for each brand and model of computer. This situation changed after the IEEE 754 floating-point standard was adopted by most computer manufacturers. The standard allows the user to choose among several rounding modes, and in each case specifies precisely how the results should be rounded. These features made numerical computations more predictable and machine-independent, and made possible the efficient and consistent implementation of interval arithmetic.\n\nCurrently, tendency to round around 5 or 2 is used in the number of research. To be more precise age heaping was used in the number of studies by Baten, to evaluate the numeracy level of the ancient populations. He came up with ABCC index, which enables the comparison of the numeracy among regions possible without any historical sources where the population literacy was measured.\n\nMost programming languages provide functions or special syntax to round fractional numbers in various ways. The earliest numeric languages, such as FORTRAN and C, would provide only one method, usually truncation (towards zero). This default method could be implied in certain contexts, such as when assigning a fractional number to an integer variable, or using a fractional number as an index of an array. Other kinds of rounding had to be programmed explicitly; for example, rounding a positive number to the nearest integer could be implemented by adding 0.5 and truncating.\n\nIn the last decades, however, the syntax and/or the standard libraries of most languages have commonly provided at least the four basic rounding functions (up, down, to nearest, and towards zero). The tie-breaking method may vary depending the language and version, and/or may be selectable by the programmer. Several languages follow the lead of the IEEE 754 floating-point standard, and define these functions as taking a double precision float argument and returning the result of the same type, which then may be converted to an integer if necessary. This approach may avoid spurious overflows since floating-point types have a larger range than integer types. Some languages, such as PHP, provide functions that round a value to a specified number of decimal digits, e.g. from 4321.5678 to 4321.57 or 4300. In addition, many languages provide a printf or similar string formatting function, which allows one to convert a fractional number to a string, rounded to a user-specified number of decimal places (the \"precision\"). On the other hand, truncation (round to zero) is still the default rounding method used by many languages, especially for the division of two integer values.\n\nOn the opposite, CSS and SVG do not define any specific maximum precision for numbers and measurements, that are treated and exposed in their DOM and in their IDL interface as strings as if they had infinite precision, and do not discriminate between integers and floating-point values; however, the implementations of these languages will typically convert these numbers into IEEE 754 double-precision floating-point values before exposing the computed digits with a limited precision (notably within standard JavaScript or ECMAScript interface bindings).\n\nSome disciplines or institutions have issued standards or directives for rounding.\n\nIn a guideline issued in mid-1966, the U.S. Office of the Federal Coordinator for Meteorology determined that weather data should be rounded to the nearest round number, with the \"round half up\" tie-breaking rule. For example, 1.5 rounded to integer should become 2, and −1.5 should become −1. Prior to that date, the tie-breaking rule was \"round half away from zero\".\n\nSome meteorologists may write \"−0\" to indicate a temperature between 0.0 and −0.5 degrees (exclusive) that was rounded to integer. This notation is used when the negative sign is considered important, no matter how small is the magnitude; for example, when rounding temperatures in the Celsius scale, where below zero indicates freezing.\n\nBecause digital information is stored in binary format, computer memory is usually produced in sizes which are a power of 2. So a common size is 1024 bytes (2 bytes), and this can be referred to as 1 kilobyte. Similarly, (2 bytes) can be referred to as 1 megabyte. This may or may not be considered rounding. More information on the different representations is on the entry for kilobyte.\n\n\n"}
{"id": "37707389", "url": "https://en.wikipedia.org/wiki?curid=37707389", "title": "Seances (film)", "text": "Seances (film)\n\nSeances is a 2016 interactive project by filmmaker and installation artist Guy Maddin, co-creators Evan and Galen Johnson, and the National Film Board of Canada, combining Maddin's recreations of lost films with an algorithmic film generator that allows for multiple storytelling permutations. Maddin began the project in 2012 in Paris, France, shooting footage for 18 films at the Centre Georges Pompidou (this installation was titled \"Spiritismes\", the French word for “seances,” leading to press confusion about the project title) and continued shooting footage for an additional 12 films at the Phi Centre in Montreal, Quebec, Canada. The Paris and Montreal shoots each took three weeks, with Maddin completing one short film of approximately 15-20 minutes each day. The shoots were also presented as art installation projects, during which Maddin, along with the cast and crew, held a “séance” during which Maddin \"invite[d] the spirit of a lost photoplay to possess them.\"\n\n\"Seances\" grew out of Maddin’s \"Hauntings\" project. Noah Cowan, a former director of the Toronto International Film Festival, told Maddin “he didn’t think it was possible to make art on the Internet,” which “reminded [Maddin] of what people said about cinema when it was starting out, when the moviolas and kinetoscopes were considered artless novelties.”\n\nMaddin began with the idea of “shooting adaptations of lost films” and originally conceived the project as making “title-for-title remakes of specific lost films” but altered this plan in favour of producing original material as the project developed. Maddin completed 11 films to show as installation loops for Noah Cowan and the Toronto International Film Festival’s Bell Lightbox theatre for this 2010 \"Hauntings\" project.\n\nAt the SXSW 2012 festival, Maddin announced that he had begun production on the \"Seances\" project, for which he would shoot one hundred short films within a hundred-day span, at locations in Canada, France, and the United States. However, Maddin abandoned this approach to the project to focus more fully on original script creation, partnering with writers Evan Johnson and Robert Kotyk, with additional writing by Maddin’s wife Kim Morgan and American poet John Ashbery.\n\nMaddin and Johnson also co-directed and shot, concurrently, a feature film titled \"The Forbidden Room\", with the same writers. Although often misreported as the same project, \"The Forbidden Room\" “is a feature film with its own separate story and stars” while “\"Seances\" will be an interactive Internet project.” Many of the actors in \"Seances\" also appear in \"The Forbidden Room\".\n\nEach viewer sees a unique film. Software designed by Halifax-based Nickel Media utilizes an algorithm to create the narrative from scenes shot by Maddin, to form a 10- to 13-minute film, each with a unique title. The number of films ensures \"hundreds of billions of unique permutations.\"\n\n\"Seances\" was launched on April 14, online and as part of Tribeca Film Festival’s Storyscapes program. \n\nIn addition to reimagining lost films, Maddin is also \"resurrecting\" projects that were planned but never filmed. Maddin has stated that he will not be parodying or otherwise mimicking the approach of the directors whose films he is reenvisioning, but rather tried to capture the imagined \"spirits of the films, rather than of their directors.\" Films will not be shown in their entirety, but rather, offered as fragments in order to be recombined online.\n\nThe following films were filmed at Centre Pompidou, Paris, February 22 - March 12, 2012. \n\nAn additional film, \"How to Take a Bath\" (lost Dwain Esper sexploitation film, 1937, USA) was scripted by American poet John Ashbery and completed in 2010.\" Footage from this film appears in \"The Forbidden Room\". In addition, Ashbery has given [Maddin] a copy of his collage-play \"The Inn of the Guardian Angel\", which was produced from \"New York Times\" obituaries and 1930 Hollywood fanzines, to \"strip-mine for dialogue for the lost films.\"\n\nThe following works were refilmed at Centre PHI, Montreal, July 7–20, 2013.\n\n\nIn April 2017, \"Seances\" received a Webby Award nomination in the Art & Experimental/Film & Video category. \n\n"}
{"id": "15875029", "url": "https://en.wikipedia.org/wiki?curid=15875029", "title": "Shmuel Safra", "text": "Shmuel Safra\n\nShmuel Safra () is an Israeli computer scientist. He is a Professor of Computer Science at Tel Aviv University, Israel. He was born in Jerusalem.\n\nSafra's research areas include complexity theory and automata theory. His work in Complexity Theory includes the classification of approximation problems—showing them NP-hard even for weak factors of approximation—and the theory of probabilistically checkable proofs (PCP) and the PCP theorem, which gives stronger characterizations of the class NP, via a membership proof that can be verified reading only a constant number of its bits.\n\nHis work on automata theory investigates determinization and complementation of finite automata over infinite strings, in particular, the complexity of such translation for Büchi automata, Streett automata and Rabin automata.\n\nIn 2001, Safra won the Gödel Prize in theoretical computer science for his papers \"Interactive Proofs and the Hardness of Approximating Cliques\" and \"Probabilistic Checking of Proofs: A New Characterization of NP\".\n\n\n"}
{"id": "57463279", "url": "https://en.wikipedia.org/wiki?curid=57463279", "title": "Siobhan Roberts", "text": "Siobhan Roberts\n\nSiobhan Roberts is a Canadian science journalist, biographer, and historian of mathematics. Roberts did her undergraduate studies in history at Queen's University, and earned a graduate degree in journalism from Ryerson University. She has won a number of Canadian National Magazine Awards, and she is the winner of the Communications Award of the Joint Policy Board for Mathematics \"for her engaging biographies of eminent mathematicians and articles about mathematics\".\n\nRoberts is the author of:\n\n"}
{"id": "45200516", "url": "https://en.wikipedia.org/wiki?curid=45200516", "title": "Strong measure zero set", "text": "Strong measure zero set\n\nIn mathematical analysis, a strong measure zero set is a subset \"A\" of the real line with the following property:\n\nEvery countable set is a strong measure zero set, and so is every union of countably many strong measure zero sets. Every strong measure zero set has Lebesgue measure 0. The Cantor set is an example of an uncountable set of Lebesgue measure 0 which is not of strong measure zero.\n\nBorel's conjecture states that every strong measure zero set is countable. It is now known that this statement is independent of ZFC (the Zermelo–Fraenkel axioms of set theory, which is the standard axiom system assumed in mathematics). This means that Borel's conjecture can neither be proven nor disproven in ZFC (assuming ZFC is consistent).\nSierpiński proved in 1928 that the continuum hypothesis (which is now also known to be independent of ZFC) implies the existence of uncountable strong measure zero sets. In 1976 Laver used a method of forcing to construct a model of ZFC in which Borel's conjecture holds. These two results together establish the independence of Borel's conjecture.\n\nThe following characterization of strong measure zero sets was proved in 1973:\nThis result establishes a connection to the notion of strongly meagre set, defined as follows: \nThe dual Borel conjecture states that every strongly meagre set is countable. This statement is also independent of ZFC.\n"}
{"id": "15036487", "url": "https://en.wikipedia.org/wiki?curid=15036487", "title": "Subcountability", "text": "Subcountability\n\nIn constructive mathematics, a collection is subcountable if there exists a partial surjection from the natural numbers onto it. The name derives from the intuitive sense that such a collection is \"no bigger\" than the counting numbers. The concept is trivial in classical set theory, where a set is subcountable if and only if it is finite or countably infinite. Constructively it is consistent to assert the subcountability of some uncountable collections such as the real numbers. Indeed, there are models of the constructive set theory CZF in which \"all\" sets are subcountable and models of IZF in which all sets with apartness relations are subcountable.\n\nFor example Cantor's diagonal argument establishes that the real numbers can not be countable but in the constructive interpretation they may still be subcountable.\n"}
{"id": "6486874", "url": "https://en.wikipedia.org/wiki?curid=6486874", "title": "Telelogic", "text": "Telelogic\n\nTelelogic AB was a software business headquartered in Malmö, Sweden. Telelogic was founded in 1983 as a research and development arm of Televerket, the Swedish department of telecom (now part of TeliaSonera). It was later acquired by IBM Rational, and exists under the IBM software group.\n\nTelelogic had operations in 22 countries and had been publicly traded since 1999. On June 11, 2007, IBM announced that it had made a cash offer to acquire Telelogic. On August 29, 2007, the European Union opened an investigation into the acquisition. On March 5, 2008, European regulators approved the acquisition of Telelogic by the Swedish IBM subsidiary Watchtower AB. On April 28, 2008, IBM completed its purchase of Telelogic.\n\n\nAll of these products have been continued under IBM's Rational Software division in the systems engineering and Product lifecycle management (PLM) \"solutions\" software line.\n\nTelelogic acquired the following companies between 1999 and 2007: \n"}
{"id": "176420", "url": "https://en.wikipedia.org/wiki?curid=176420", "title": "Underwood Dudley", "text": "Underwood Dudley\n\nUnderwood Dudley (born January 6, 1937) is a mathematician, formerly of DePauw University, who has written a number of research works and textbooks but is best known for his popular writing. Most notable are several books describing crank mathematics by people who think they have squared the circle or done other impossible things. \nThese books, which alternate between appreciation and exasperation, include \"The Trisectors\" (MAA 1996, ), \"Mathematical Cranks\" (MAA 1992, ), and \"Numerology: Or, What Pythagoras Wrought\" (MAA 1997, ). They helped him win the Trevor Evans Award for expository writing from the Mathematical Association of America (MAA) in 1996.\n\nDudley has also written and edited straightforward mathematical works such as \"Readings for Calculus\" (MAA 1993, ) and \"Elementary Number Theory\" (W.H. Freeman 1978, ). He is the discoverer of the Dudley triangle.\n\nDudley is a native of New York City. He received bachelor's and master's degrees from the Carnegie Institute of Technology and a Ph.D. from the University of Michigan. His academic career consisted of two years at Ohio State University followed by thirty-seven at DePauw University, from which he retired in 2004. He has edited the \"College Mathematics Journal\" and the \"Pi Mu Epsilon Journal\", and was a Polya Lecturer for the MAA for two years.\n\nIn 1995, Dudley was one of several people sued by William Dilworth for defamation because \"Mathematical Cranks\" included an analysis of Dilworth's \"A correction in set theory\", an attempted refutation of Cantor's diagonal method. The suit was dismissed in 1996 due to failure to state a claim.\n\nThe dismissal was upheld on appeal in a decision written by Richard Posner. From the decision: \"A crank is a person inexplicably obsessed by an obviously unsound idea—a person with a bee in his bonnet. To call a person a crank is to say that because of some quirk of temperament he is wasting his time pursuing a line of thought that is plainly without merit or promise ... To call a person a crank is basically just a colorful and insulting way of expressing disagreement with his master idea, and it therefore belongs to the language of controversy rather than to the language of defamation.\".\n\n\n"}
{"id": "1296085", "url": "https://en.wikipedia.org/wiki?curid=1296085", "title": "Virtual finite-state machine", "text": "Virtual finite-state machine\n\nA virtual finite state machine (VFSM) is a finite state machine (FSM) defined in a Virtual Environment. The VFSM concept provides a software specification method to describe the behaviour of a control system using assigned names of input control properties and of output actions.\n\nThe VFSM method introduces an execution model and facilitates the idea of an executable specification. This technology is mainly used in complex machine control, instrumentation and telecommunication applications.\n\nThe major problem by implementing of a state machine is the generation of logical conditions (State transition conditions and Action conditions). In the hardware environment, where state machines have found their original use, this problem does not exist: all signals are Boolean. In contrary the state machines specified and implemented in the software require logical conditions that are per se multivalued ones.\n\nExamples:<br>\n- Temperature can be at least Low , OK, High<br>\n- Commands may have several values: Init, Start, Stop, Break, Continue<br>\n- In a (hierarchical) system of state machines the Slaves state machines have many states that are used in conditions of a Master state machine.<br>\nIn addition many input signals can be not known due for instance to a broken cable which means that even a digital input signals (considered as classical Boolean values) are in fact 3 values signals: Low, High, Unknown. The Temperature example needs probably also the value Unknown.\n\nA Positive Logical Algebra solves this problem by creating a Virtual Environment which allows specification of state machines for software using multivalued variables.\n\nA variable in the VFSM environment may have one or more values which are relevant for the control – in such a case it is an input variable. Those values are the control properties of this variable. Control properties are not necessarily specific data values but are rather certain states of the variable. For instance, a digital variable could provide three control properties: TRUE, FALSE and UNKNOWN according to its possible boolean values. A numerical (analog) input variable has control properties such as: LOW, HIGH, OK, BAD, UNKNOWN according to its range of desired values. A timer can have its OVER state (time-out occurred) as its most significant control value; other values could be STOPPED, RUNNING etc...\n\nA variable in the VFSM environment may be activated by actions - in such a case it is an output variable. For instance, a digital output has two actions: True and False. A numerical (analog) output variable has an action: Set. A timer which is both: an input and output variable can be triggered by actions like: Start, Stop or Reset.\n\nThe virtual environment characterises the environment in which a VFSM operates. It is defined by three sets of names:\nThe input names build virtual conditions to perform state transitions or input actions. The virtual conditions are built using the positive logic algebra. The output names trigger actions (entry actions, exit actions, input actions or transition actions).\n\nA virtual finite state machine is a finite state machine (FSM) defined in a virtual environment. The VFSM concept provides a software specification method to describe the behaviour of a control system using assigned names of input control properties and of output actions.\n\nThe VFSM method introduces an execution model and facilitates the idea of an executable specification. This technology is mainly used in complex machine control, instrumentation and telecommunication applications.\n\nA variable in the VFSM environment may have one or more values which are relevant for the control – in such a case it is an input variable. Those values are the control properties of this variable. Control properties are not necessarily specific data values but are rather certain states of the variable. For instance, a digital variable could provide three control properties: TRUE, FALSE and UNKNOWN according to its possible boolean values. A numerical (analog) input variable has control properties such as: LOW, HIGH, OK, BAD, UNKNOWN according to its range of desired values. A timer can have its OVER state (time-out occurred) as its most significant control value; other values could be STOPPED, RUNNING etc...\n\nA variable in the VFSM environment may be activated by actions - in such a case it is an output variable. For instance, a digital output has two actions: True and False. A numerical (analog) output variable has an action: Set. A timer which is both: an input and output variable can be triggered by actions like: Start, Stop or Reset.\n\nThe virtual environment characterises the environment in which a VFSM operates. It is defined by three sets of names:\nThe input names build virtual conditions to perform state transitions or input actions. The virtual conditions are built using the positive logic algebra. The output names trigger actions (entry actions, exit actions, input actions or transition actions).\n\nThe rules to build a virtual condition are as follows:\n\nA state of an input is described by Input Names which create a set.\n\nFor instance:<br>\n- for the input A: Anames = {\"A1\", \"A2\", \"A3\"}<br>\n- for the input B: Bnames = {\"B1\", \"B2\"}<br>\n- for the input C: Cnames = {\"C1\", \"C2\", \"C3\", \"C4\", \"C5\"}<br>\netc.\n\nVirtual Input VI is a set of mutually exclusive elements of input names. A VI contains always the element always.\n\nExamples:<br>\nVI = {\"always\"}<br>\nVI = {\"always\", \"A1\"}<br>\n\n& (AND) operation is a set of input names.\n\nFor instance<br>\n\nFor instance<br>\n\"A1\" & \"B3\" & \"C2\" => formula_1\n\n~ (Complement) is a compliment of a set of input names.\n\nFor instance<br>\n\nA logical expression is an OR-table of AND-sets (corresponds to disjunctive form of a boolean expression).\n\nFor instance:<br>\n\"A1\" & \"B3\" | \"A1\" & \"B2\" & \"C4\" | \"C2\" => formula_2<br>\nLogical expressions are used to express any logical function.\n\nThe logical value (true, false) of a logical expression is calculated by testing whether any of the AND-sets in the OR-table is a subset of VI.\n\nA state of an output is described by Output Names which create a set.\n\nFor instance:<br>\nfor an output Xnames = {\"X1\", \"X2\"}<br>\nfor an output Ynames = {\"Y1\", \"Y2\", \"Y3\"}<br>\nVirtual output VO is a set of mutually exclusive elements of output names.\n\nThe Virtual Name and Virtual Output completed by State Names create a Virtual Environment VE where the behavior is specified.\n\nA subset of all defined input names, which can exist only in a certain situation, is called virtual input (VI). For instance temperature can be either \"too low\", \"good\" or \"too high\". Although there are three input names defined, only one of them can exist in a real situation. This one builds the VI.\n\nA subset of all defined output names, which can exist only in a certain situation is called virtual output (VO). VO is built by the current action(s) of the VFSM.\n\nThe behaviour specification is built by a state table which describes all details of all states of the VFSM.\n\nThe VFSM executor is triggered by VI and the current state of the VFSM. In consideration of the behaviour specification of the current state, the VO is set.\n\nFigure 2 shows one possible implementation of a VFSM executor. Based on this implementation a typical behaviour characteristics must be considered.\n\nA \"state table\" defines all details of the behaviour of a state of a VFSM. It consists of three columns: in the first column state names are used, in the second the virtual conditions built out of input names using the positive logic algebra are placed and in the third column the output names appear:\n\nRead the table as following: the first two lines define the entry and exit actions of the current state. The following lines which do not provide the next state represent the input actions. Finally the lines providing the next state represent the state transition conditions and transition actions. All fields are optional. A pure combinatorial VFSM is possible in case only where input actions are used, but no state transitions are defined. The transition action can be replaced by the proper use of other actions.\n\n\n"}
{"id": "34521", "url": "https://en.wikipedia.org/wiki?curid=34521", "title": "Z notation", "text": "Z notation\n\nThe Z notation is a formal specification language used for describing and modelling computing systems. It is targeted at the clear specification of computer programs and computer-based systems in general.\n\nIn 1974, Jean-Raymond Abrial published \"Data Semantics\". He used a notation that would later be taught in the University of Grenoble until the end of the 1980s. While at EDF (Électricité de France), Abrial wrote internal notes on Z. The Z notation is used in the 1980 book \"Méthodes de programmation\".\n\nZ was originally proposed by Abrial in 1977 with the help of Steve Schuman and Bertrand Meyer. It was developed further at the Programming Research Group at Oxford University, where Abrial worked in the early 1980s, having arrived at Oxford in September 1979.\n\nAbrial has said that Z is so named \"Because it is the ultimate language!\" although the name \"Zermelo\" is also associated with the Z notation through its use of Zermelo–Fraenkel set theory.\n\nZ is based on the standard mathematical notation used in axiomatic set theory, lambda calculus, and first-order predicate logic. All expressions in Z notation are typed, thereby avoiding some of the paradoxes of naive set theory. Z contains a standardized catalogue (called the \"mathematical toolkit\") of commonly used mathematical functions and predicates, defined using Z itself.\n\nAlthough Z notation (just like the APL language, long before it) uses many non-ASCII symbols, the specification includes suggestions for rendering the Z notation symbols in ASCII and in LaTeX. There are also Unicode encodings for all standard Z symbols.\n\nISO completed a Z standardization effort in 2002. This standard and a technical corrigendum are available from ISO for free:\n\n\n\n\n"}
