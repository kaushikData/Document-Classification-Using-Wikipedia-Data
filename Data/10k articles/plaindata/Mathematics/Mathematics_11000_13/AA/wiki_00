{"id": "667332", "url": "https://en.wikipedia.org/wiki?curid=667332", "title": "125 (number)", "text": "125 (number)\n\n125 (one hundred [and] twenty-five) is the natural number following 124 and preceding 126.\n\n125 is the cube of 5.\nIt can be expressed as a sum of two squares in two different ways, 125 = 10² + 5² = 11² + 2².\n\n125 and 126 form a Ruth-Aaron pair under the second definition in which repeated prime factors are counted as often as they occur.\n\nLike many other powers of 5, it is a Friedman number in base 10 since 125 = 5.\n\n\n\n125 is also:\n\n"}
{"id": "1226666", "url": "https://en.wikipedia.org/wiki?curid=1226666", "title": "Adams operation", "text": "Adams operation\n\nIn mathematics, an Adams operation, denoted ψ for natural numbers \"k\", is a cohomology operation in topological K-theory, or any allied operation in algebraic K-theory or other types of algebraic construction, defined on a pattern introduced by Frank Adams. The basic idea is to implement some fundamental identities in symmetric function theory, at the level of vector bundles or other representing object in more abstract theories.\n\nAdams operations can be defined more generally in any λ-ring over the rational numbers.\n\nAdams operations ψ on K theory (algebraic or topological) are characterized by the following properties.\n\nThe fundamental idea is that for a vector bundle \"V\" on a topological space \"X\", there is an analogy between Adams operators and exterior powers, in which\n\nas\n\nof the roots α of a polynomial \"P\"(\"t\"). (Cf. Newton's identities.) Here Λ denotes the \"k\"-th exterior power. From classical algebra it is known that the power sums are certain integral polynomials \"Q\" in the σ. The idea is to apply the same polynomials to the Λ(\"V\"), taking the place of σ. This calculation can be defined in a \"K\"-group, in which vector bundles may be formally combined by addition, subtraction and multiplication (tensor product). The polynomials here are called Newton polynomials (not, however, the Newton polynomials of interpolation theory).\n\nJustification of the expected properties comes from the line bundle case, where \"V\" is a Whitney sum of line bundles. In this special case the result of any Adams operation is naturally a vector bundle, not a linear combination of ones in \"K\"-theory. Treating the line bundle direct factors formally as roots is something rather standard in algebraic topology (cf. the Leray–Hirsch theorem). In general a mechanism for reducing to that case comes from the splitting principle for vector bundles.\n\nThe Adams operation has a simple expression in group representation theory. Let \"G\" be a group and ρ a representation of \"G\" with character χ. The representation ψ(ρ) has character\n"}
{"id": "7740302", "url": "https://en.wikipedia.org/wiki?curid=7740302", "title": "Automatic sequence", "text": "Automatic sequence\n\nIn mathematics and theoretical computer science, an automatic sequence (also called a k\"-automatic sequence or a k\"-recognizable sequence when one wants to indicate that the base of the numerals used is \"k\") is an infinite sequence of terms characterized by a finite automaton. The \"n\"-th term of an automatic sequence \"a\"(\"n\") is a mapping of the final state reached in a finite automaton accepting the digits of the number \"n\" in some fixed base \"k\".\n\nAn automatic set is a set of non-negative integers \"S\" for which the sequence of values of its characteristic function χ is an automatic sequence; that is, \"S\" is \"k\"-automatic if χ(\"n\") is \"k\"-automatic, where χ(\"n\") = 1 if \"n\" formula_1 \"S\" and 0 otherwise.\n\nAutomatic sequences may be defined in a number of ways, all of which are equivalent. Four common definitions are as follows.\n\nLet \"k\" be a positive integer, and let \"D\" = (\"Q\", Σ, δ, \"q\", Δ, τ) be a deterministic finite automaton \"with output\", where\n\nExtend the transition function δ from acting on single digits to acting on strings of digits by defining the action of δ on a string \"s\" consisting of digits \"s\"\"s\"...\"s\" as:\n\nDefine a function \"a\" from the set of positive integers to the output alphabet Δ as follows:\n\nwhere \"s\"(\"n\") is \"n\" written in base \"k\". Then the sequence \"a\" = \"a\"(1)\"a\"(2)\"a\"(3)... is a \"k\"-automatic sequence.\n\nAn automaton reading the base \"k\" digits of \"s\"(\"n\") starting with the most significant digit is said to be \"direct reading\", while an automaton starting with the least significant digit is \"reverse reading\". The above definition holds whether \"s\"(\"n\") is direct or reverse reading.\n\nLet φ be a \"k\"-uniform morphism of a free monoid Σ and let τ be defined as in the automata-theoretic case. If \"w\" is a fixed point of φ—that is, if \"w\" = φ(\"w\")—then \"s\" = τ(\"w\") is a \"k\"-automatic sequence. This result is due to Cobham, and it is referred to in the literature as \"Cobham's theorem\". Conversely, every \"k\"-automatic sequence is obtainable in this way.\n\nLet \"k\" ≥ 2. The \"k-kernel\" of the sequence \"s\"(\"n\") is the set of subsequences\nIn most cases, the \"k\"-kernel of a sequence is infinite. However, if the \"k\"-kernel is finite, then the sequence \"s\"(\"n\") is \"k\"-automatic.\n\nIt follows that a \"k\"-automatic sequence is necessarily a sequence on a finite alphabet.\n\nLet \"u\"(\"n\") be a sequence over an alphabet Σ and suppose that there is an injective function β from Σ to the finite field F, where \"q\" = \"p\" for some prime \"p\". The associated formal power series is \nThen the sequence \"u\" is \"q\"-automatic if and only if this formal power series is algebraic over F(\"X\"). This result is due to Christol, and it is referred to in the literature as \"Christol's theorem\".\n\nAutomatic sequences were introduced by Büchi in 1960, although his paper took a more logico-theoretic approach to the matter and did not use the terminology found in this article. The notion of automatic sequences was further studied by Cobham in 1972, who called these sequences \"uniform tag sequences\".\n\nThe term \"automatic sequence\" first appeared in a paper of Deshouillers.\n\nThe following sequences are automatic:\n\nThe Thue–Morse sequence \"t\"(\"n\") () is the fixed point of the morphism 0 → 01, 1 → 10. Since the \"n\"-th term of the Thue–Morse sequence counts the number of ones modulo 2 in the base-2 representation of \"n\", it is generated by the two-state deterministic finite automaton with output pictured here, where being in state \"q\" indicates there are an even number of ones in the representation of \"n\" and being in state \"q\" indicates there are an odd number of ones.\nHence, the Thue–Morse sequence is 2-automatic.\n\nThe \"n\"-th term of the period-doubling sequence \"d\"(\"n\") () is determined by the parity of the exponent of the highest power of 2 dividing \"n\". It is also the fixed point of the morphism 0 → 01, 1 → 00. Starting with the initial term \"w\" = 0 and iterating the 2-uniform morphism φ on \"w\" where φ(0) = 01 and φ(1) = 00, it is evident that the period-doubling sequence is the fixed-point of φ(\"w\") and thus it is 2-automatic.\n\nThe \"n\"-th term of the Rudin–Shapiro sequence \"r\"(\"n\") () is determined by the number of consecutive ones in the base-2 representation of \"n\". The 2-kernel of the Rudin–Shapiro sequence is \nSince the 2-kernel consists only of \"r\"(\"n\"), \"r\"(2\"n\" + 1), \"r\"(4\"n\" + 3), and \"r\"(8\"n\" + 3), it is finite and thus the Rudin–Shapiro sequence is 2-automatic.\n\nBoth the Baum–Sweet sequence () and the regular paperfolding sequence () are automatic. In addition, the general paperfolding sequence with a periodic sequence of folds is also automatic.\n\nAutomatic sequences exhibit a number of interesting properties. A non-exhaustive list of these properties is presented below.\n\n\n\"k\"-automatic sequences are normally only defined for \"k\" ≥ 2. The concept can be extended to \"k\" = 1 by defining a 1-automatic sequence to be a sequence whose \"n\"-th term depends on the unary notation for \"n\"; that is, (1). Since a finite state automaton must eventually return to a previously visited state, all 1-automatic sequences are ultimately periodic.\n\nAutomatic sequences are robust against variations to either the definition or the input sequence. For instance, as noted in the automata-theoretic definition, a given sequence remains automatic under both direct and reverse reading of the input sequence. A sequence also remains automatic when an alternate set of digits is used or when the base is negated; that is, when the input sequence is represented in base −\"k\" instead of in base \"k\". However, in contrast to using an alternate set of digits, a change of base may affect the automaticity of a sequence.\n\nThe domain of an automatic sequence can be extended from the natural numbers to the integers via \"two-sided\" automatic sequences. This stems from the fact that, given \"k\" ≥ 2, every integer can be represented uniquely in the form formula_5 where formula_6. Then a two-sided infinite sequence \"a\"(\"n\") is (−\"k\")-automatic if and only if its subsequences \"a\"(\"n\") and \"a\"(−\"n\") are \"k\"-automatic.\n\nThe alphabet of a \"k\"-automatic sequence can be extended from finite size to infinite size via \"k\"-regular sequences.\n\n\n\n"}
{"id": "21663799", "url": "https://en.wikipedia.org/wiki?curid=21663799", "title": "Behnke–Stein theorem", "text": "Behnke–Stein theorem\n\nIn mathematics, especially several complex variables, the Behnke–Stein theorem states that a union of an increasing sequence formula_1 (i.e., formula_2) of domains of holomorphy is again a domain of holomorphy.\n\nThis is related to the fact that an increasing union of pseudoconvex domains is pseudoconvex and so it can be proven using that fact and the solution of the Levi problem. Though historically this theorem was in fact used to solve the Levi problem.\n"}
{"id": "3205927", "url": "https://en.wikipedia.org/wiki?curid=3205927", "title": "Bergman space", "text": "Bergman space\n\nIn complex analysis, functional analysis and operator theory, a Bergman space is a function space of holomorphic functions in a domain \"D\" of the complex plane that are sufficiently well-behaved at the boundary that they are absolutely integrable. Specifically, for , the Bergman space is the space of all holomorphic functions formula_1 in \"D\" for which the p-norm is finite:\n\nThe quantity formula_3 is called the \"norm\" of the function ; it is a true norm if formula_4. Thus is the subspace of holomorphic functions that are in the space L(\"D\"). The Bergman spaces are Banach spaces, which is a consequence of the estimate, valid on compact subsets \"K\" of \"D\":\n\nThus convergence of a sequence of holomorphic functions in implies also compact convergence, and so the limit function is also holomorphic.\n\nIf , then is a reproducing kernel Hilbert space, whose kernel is given by the Bergman kernel.\n\nIf the domain is bounded, then the norm is often given by\n\nwhere formula_6 is a normalised Lebesgue measure of the complex plane, i.e. . Alternatively is used, regardless of the area of .\nThe Bergman space is usually defined on the open unit disk formula_7 of the complex plane, in which case formula_8. In the Hilbert space case, given formula_9, we have\n\nthat is, is isometrically isomorphic to the weighted \"ℓ(1/(n+1))\" space. In particular the polynomials are dense in . Similarly, if , the right (or the upper) complex half-plane, then\n\nwhere formula_12, that is, is isometrically isomorphic to the weighted \"L (0,∞)\" space (via the Laplace transform).\n\nThe weighted Bergman space is defined in an analogous way, i.e.\n\nprovided that is chosen in such way, that formula_14 is a Banach space (or a Hilbert space, if ). In case where formula_15, by a weighted Bergman space formula_16 we mean the space of all analytic functions such that\n\nand similarly on the right half-plane (i.e. formula_18) we have\n\nand this space is isometrically isomorphic, via the Laplace transform, to the space formula_20, where\n\n(here denotes the Gamma function).\n\nFurther generalisations are sometimes considered, for example formula_22 denotes a weighted Bergman space (often called a Zen space) with respect to a translation-invariant positive regular Borel measure formula_23 on the closed right complex half-plane formula_24, that is\n\nThe reproducing kernel formula_26 of at point formula_27 is given by\n\nand similarly for formula_29 we have\n\nIn general, if formula_31 maps a domain formula_32 conformally onto a domain formula_33, then\n\nIn weighted case we have\n\nand\n\n\n"}
{"id": "2832170", "url": "https://en.wikipedia.org/wiki?curid=2832170", "title": "Bit manipulation", "text": "Bit manipulation\n\nBit manipulation is the act of algorithmically manipulating bits or other pieces of data shorter than a word. Computer programming tasks that require bit manipulation include low-level device control, error detection and correction algorithms, data compression, encryption algorithms, and optimization. For most other tasks, modern programming languages allow the programmer to work directly with abstractions instead of bits that represent those abstractions. Source code that does bit manipulation makes use of the bitwise operations: AND, OR, XOR, NOT, and bit shifts.\n\nBit manipulation, in some cases, can obviate or reduce the need to loop over a data structure and can give many-fold speed ups, as bit manipulations are processed in parallel, but the code can become more difficult to write and maintain.\n\n\"Bit twiddling\" and \"bit bashing\" are often used interchangeably with bit manipulation, but sometimes exclusively refer to clever or non-obvious ways or uses of bit manipulation, or tedious or challenging low-level device control data manipulation tasks.\n\nThe term \"bit twiddling\" dates from early computing hardware, where computer operators would make adjustments by tweaking or \"twiddling\" computer controls. As computer programming languages evolved, programmers adopted the term to mean any handling of data that involved bit-level computation.\n\nA bitwise operation operates on one or more bit patterns or binary numerals at the level of their individual bits. It is a fast, primitive action directly supported by the central processing unit (CPU), and is used to manipulate values for comparisons and calculations.\nOn simple low-cost processors, typically, bitwise operations are substantially faster than division, several times faster than multiplication, and sometimes significantly faster than addition. While modern processors usually perform addition and multiplication just as fast as bitwise operations due to their longer instruction pipelines and other architectural design choices, bitwise operations do commonly use less power because of the reduced use of resources.\n\nA mask is data that is used for bitwise operations, particularly in a bit field.\n\nUsing a mask, multiple bits in a Byte, nibble, word (etc.) can be set either on, off or inverted from on to off (or vice versa) in a single bitwise operation.\n\nThe following two code samples, written in the programming language C++, both determine if the given unsigned integer x is a power of two.\n\nThe second method uses the fact that powers of two have one and only one bit set in their binary representation:\n\nIf the number is neither zero nor a power of two, it will have '1' in more than one place:\n\nIf inline assembly language code is used, then an instruction that counts the number of 1's or 0's might be available; for example, the POPCNT instruction from the x86 instruction set. Some compilers provide predefined function for that, e.g. GNU Compiler Collection has __builtin_popcount(). Such instructions may have greater latency, however, than the bit-twiddling solution.\n\nThe programming language C has direct support for bitwise operations that can be used for bit manipulation. In the following examples, codice_1 is the index of the bit to be manipulated within the variable codice_2, which is an codice_3 being used as a bit field. Bit indexing begins at 0, not 1. Bit 0 is the least significant bit.\n\nWhen using an array of bytes to represent a set of bits, i.e., a bit array or bitset, the index of the byte in the array associated with a bit codice_1 can be calculated using division:\n\nwhere codice_1 is the index of the given bit and codice_6 gives the number of bits in a C codice_7.\n\nThe index of the bit within the byte indexed by the above can be calculated via a modulo operation:\nNote: codice_3 is typically used in C to represent a byte and codice_6 is most often 8 on modern processors. codice_10 is the C modulo operator.\n\n\n\n"}
{"id": "12561793", "url": "https://en.wikipedia.org/wiki?curid=12561793", "title": "CAPICOM", "text": "CAPICOM\n\nCAPICOM is a discontinued ActiveX control created by Microsoft to help expose a select set of Microsoft Cryptographic Application Programming Interface (CryptoAPI) functions through Microsoft Component Object Model (COM). It was intended to enable every environment that supports ActiveX to use Microsoft Cryptographic technologies, including web pages that are opened with Microsoft Internet Explorer or any other web browser that supports ActiveX.\n\nCAPICOM can be used to digitally sign data, inspect, verify and display their digital signature or digital certificate, add or remove certificates to or from the certificate stores, and finally, to encrypt or decrypt data.\n\nCAPICOM Version 2.1.0.3, the latest and last version of CAPICOM, is officially supported on Windows Vista. However, Microsoft has announced that CAPICOM is discontinued and is no longer being developed. Microsoft suggests replacing CAPICOM with .NET Framework's X509 Cryptographic Classes and a number of other alternatives.\n\nCAPICOM was not included in Windows 7. The linked Microsoft article goes into detail. \n"}
{"id": "62382", "url": "https://en.wikipedia.org/wiki?curid=62382", "title": "Catalan's conjecture", "text": "Catalan's conjecture\n\nCatalan's conjecture (or Mihăilescu's theorem) is a theorem in number theory that was conjectured by the mathematician Eugène Charles Catalan in 1844 and proven in 2002 by Preda Mihăilescu. The integers 2 and 3 are two powers of natural numbers whose values (8 and 9, respectively) are consecutive. The theorem states that this is the \"only\" case of two consecutive powers. That is to say, that the only solution in the natural numbers of\nfor \"a\", \"b\" > 1, \"x\", \"y\" > 0 is \"x\" = 3, \"a\" = 2, \"y\" = 2, \"b\" = 3.\n\nThe history of the problem dates back at least to Gersonides, who proved a special case of the conjecture in 1343 where (\"x\", \"y\") was restricted to be (2, 3) or (3, 2). The first significant progress after Catalan made his conjecture came in 1850 when Victor-Amédée Lebesgue dealt with the case \"b\" = 2.\n\nIn 1976, Robert Tijdeman applied Baker's method in transcendence theory to establish a bound on a,b and used existing results bounding \"x\",\"y\" in terms of \"a\", \"b\" to give an effective upper bound for \"x\",\"y\",\"a\",\"b\". Michel Langevin computed a value of exp exp exp exp 730 for the bound. This resolved Catalan's conjecture for all but a finite number of cases. Nonetheless, the finite calculation required to complete the proof of the theorem was too time-consuming to perform.\n\nCatalan's conjecture was proven by Preda Mihăilescu in April 2002. The proof was published in the \"Journal für die reine und angewandte Mathematik\", 2004. It makes extensive use of the theory of cyclotomic fields and Galois modules. An exposition of the proof was given by Yuri Bilu in the Séminaire Bourbaki.\n\nIt is a conjecture that for every natural number \"n\", there are only finitely many pairs of perfect powers with difference \"n\". The list below shows, for n ≤ 64, all solutions for perfect powers less than 10, as per .\n\nSee for the smallest solution (> 0), and for number of solutions (except 0) for a given \"n\".\n\nPillai's conjecture concerns a general difference of perfect powers : it is an open problem initially proposed by S. S. Pillai, who conjectured that the gaps in the sequence of perfect powers tend to infinity. This is equivalent to saying that each positive integer occurs only finitely many times as a difference of perfect powers: more generally, in 1931 Pillai conjectured that for fixed positive integers \"A\", \"B\", \"C\" the equation formula_2 has only finitely many solutions (\"x\",\"y\",\"m\",\"n\") with (\"m\",\"n\") ≠ (2,2). Pillai proved that the difference formula_3 for any λ less than 1, uniformly in \"m\" and \"n\".\n\nThe general conjecture would follow from the ABC conjecture.\n\nPaul Erdős conjectured that there is some positive constant \"c\" such that if \"d\" is the difference of a perfect power \"n\", then \"d\">\"n\" for sufficiently large \"n\".\n\n\n\n"}
{"id": "21697672", "url": "https://en.wikipedia.org/wiki?curid=21697672", "title": "Cavalieri's principle", "text": "Cavalieri's principle\n\nIn geometry, Cavalieri's principle, a modern implementation of the method of indivisibles, named after Bonaventura Cavalieri, is as follows:\n\n\nToday Cavalieri's principle is seen as an early step towards integral calculus, and while it is used in some forms, such as its generalization in Fubini's theorem, results using Cavalieri's principle can often be shown more directly via integration. In the other direction, Cavalieri's principle grew out of the ancient Greek method of exhaustion, which used limits but did not use infinitesimals.\n\nCavalieri's principle was originally called the method of indivisibles, the name it was known by in Renaissance Europe. Cavalieri developed a complete theory of indivisibles, elaborated in his \"Geometria indivisibilibus continuorum nova quadam ratione promota\" (\"Geometry, advanced in a new way by the indivisibles of the continua\", 1635) and his \"Exercitationes geometricae sex\" (\"Six geometrical exercises\", 1647).\n\nIn the 3rd century BC, Archimedes, using a method resembling Cavalieri's principle, was able to find the volume of a sphere given the volumes of a cone and cylinder in his work \"The Method of Mechanical Theorems\". In the 5th century AD, Zu Chongzhi and his son Zu Gengzhi established a similar method to find a sphere's volume. The transition from Cavalieri's indivisibles to Evangelista Torricelli's and John Wallis's infinitesimals was a major advance in the history of the calculus. The indivisibles were entities of codimension 1, so that a plane figure was thought as made out of an infinity of 1-dimensional lines. Meanwhile, infinitesimals were entities of the same dimension as the figure they make up; thus, a plane figure would be made out of \"parallelograms\" of infinitesimal width. Applying the formula for the sum of an arithmetic progression, Wallis computed the area of a triangle by partitioning it into infinitesimal parallelograms of width 1/∞.\n\nIf one knows that the volume of a cone is formula_1, then one can use Cavalieri's principle to derive the fact that the volume of a sphere is formula_2, where formula_3 is the radius.\n\nThat is done as follows: Consider a sphere of radius formula_3 and a cylinder of radius formula_3 and height formula_3. Within the cylinder is the cone whose apex is at the center of one base of the cylinder and whose base is the other base of the cylinder. By the Pythagorean theorem, the plane located formula_7 units above the \"equator\" intersects the sphere in a circle of area formula_8. The area of the plane's intersection with the part of the cylinder that is \"outside\" of the cone is also formula_8. As we can see, the area of every intersection of the circle with the horizontal plane located at any height formula_7 equals the area of the intersection of the plane with the part of the cylinder that is \"outside\" of the cone; thus, applying Cavalieri's principle, we could say that the volume of the half sphere equals the volume of the part of the cylinder that is \"outside\" the cone. The aforementioned volume of the cone is formula_11 of the volume of the cylinder, thus the volume \"outside\" of the cone is formula_12 the volume of the cylinder. Therefore the volume of the upper half of the sphere is formula_12 of the volume of the cylinder. The volume of the cylinder is\n\nTherefore the volume of the upper half-sphere is formula_15 and that of the whole sphere is formula_16.\n\nThe fact that the volume of any pyramid, regardless of the shape of the base, whether circular as in the case of a cone, or square as in the case of the Egyptian pyramids, or any other shape, is (1/3) × base × height, can be established by Cavalieri's principle if one knows only that it is true in one case. One may initially establish it in a single case by partitioning the interior of a triangular prism into three pyramidal components of equal volumes. One may show the equality of those three volumes by means of Cavalieri's principle.\n\nIn fact, Cavalieri's principle or similar infinitesimal argument is \"necessary\" to compute the volume of cones and even pyramids, which is essentially the content of Hilbert's third problem – polyhedral pyramids and cones cannot be cut and rearranged into a standard shape, and instead must be compared by infinite (infinitesimal) means. The ancient Greeks used various precursor techniques such as Archimedes's mechanical arguments or method of exhaustion to compute these volumes.\n\nIn what is called the napkin ring problem, one shows by Cavalieri's principle that when a hole is drilled straight through the centre of a sphere where the remaining band has height \"h\", the volume of the remaining material surprisingly does not depend on the size of the sphere. The cross-section of the remaining ring is a plane annulus, whose area is the difference between the areas of two circles. By the Pythagorean theorem, the area of one of the two circles is \"π\" times \"r\" − \"y\", where \"r\" is the sphere's radius and \"y\" is the distance from the plane of the equator to the cutting plane, and that of the other is \"π\" times \"r\" − (\"h\"/2). When these are subtracted, the \"r\" cancels; hence the lack of dependence of the bottom-line answer upon \"r\".\n\nN. Reed has shown how to find the area bounded by a cycloid by using Cavalieri's principle. A circle of radius \"r\" can roll in a clockwise direction upon a line below it, or in a counterclockwise direction upon a line above it. A point on the circle thereby traces out two cycloids. When the circle has rolled any particular distance, the angle through which it would have turned clockwise and that through which it would have turned counterclockwise are the same. The two points tracing the cycloids are therefore at equal heights. The line through them is therefore horizontal (i.e. parallel to the two lines on which the circle rolls). Consequently each horizontal cross-section of the circle has the same length as the corresponding horizontal cross-section of the region bounded by the two arcs of cyloids. By Cavalieri's principle, the circle therefore has the same area as that region.\n\nConsider the rectangle bounding a single cycloid arch. From the definition of a cycloid, it has width and height , so its area is four times the area of the circle. Calculate the area within this rectangle that lies above the cycloid arch by bisecting the rectangle at the midpoint where the arch meets the rectangle, rotate one piece by 180° and overlay the other half of the rectangle with it. The new rectangle, of area twice that of the circle, consists of the \"lens\" region between two cycloids, whose area was calculated above to be the same as that of the circle, and the two regions that formed the region above the cycloid arch in the original rectangle. Thus, the area bounded by a rectangle above a single complete arch of the cycloid has area equal to the area of the circle, and so, the area bounded by the arch is three times the area of the circle.\n\n\n"}
{"id": "27358159", "url": "https://en.wikipedia.org/wiki?curid=27358159", "title": "Cheung–Marks theorem", "text": "Cheung–Marks theorem\n\nIn information theory, the Cheung–Marks theorem, named after K. F. Cheung and Robert J. Marks II, specifies conditions where restoration of a signal by the sampling theorem can become ill-posed. It offers conditions whereby \"reconstruction error with unbounded variance [results] when a bounded variance noise is added to the samples.\"\n\nIn the sampling theorem, the uncertainty of the interpolation as measured by noise variance is the same as the uncertainty of the sample data when the noise is i.i.d. In his classic 1948 paper founding information theory, Claude Shannon offered the following generalization of the sampling theorem:\n\nAlthough true in the absence of noise, many of the expansions proposed by Shannon become ill-posed. An arbitrarily small amount of noise on the data renders restoration unstable. Such sampling expansions are not useful in practice since sampling noise, such as quantization noise, rules out stable interpolation and therefore any practical use.\n\nShannon's suggestion of simultaneous sampling of the signal and its derivative at half the Nyquist rate results in well behaved interpolation. The Cheung–Marks theorem shows counter-intuitively that interlacing signal and derivative samples makes the restoration problem ill-posed.\n\nThe theorem also shows sensitivity increases with derivative order.\n\nGenerally, the Cheung–Marks theorem shows the sampling theorem becomes ill-posed when the area (integral) of the squared magnitude of the interpolation function over all time is not finite.\n\"While the generalized sampling concept is relatively straightforward, the reconstruction is not always feasible because of potential instabilities.\"\n"}
{"id": "993328", "url": "https://en.wikipedia.org/wiki?curid=993328", "title": "Contingent cooperator", "text": "Contingent cooperator\n\nIn game theory, a contingent cooperator is a person or agent who is willing to act in the collective interest, rather than his short-term selfish interest, if he observes a majority of the other agents in the collective doing the same. The apparent contradiction in this stance is resolved by game theory, which shows that in the right circumstances, cooperation with a sufficient number of other participants will have a better outcome for cooperators than pursuing short-term selfish interests.\n\n\n"}
{"id": "506383", "url": "https://en.wikipedia.org/wiki?curid=506383", "title": "Cryptosystem", "text": "Cryptosystem\n\nIn cryptography, a cryptosystem is a suite of cryptographic algorithms needed to implement a particular security service, most commonly for achieving confidentiality (encryption).\n\nTypically, a cryptosystem consists of three algorithms: one for key generation, one for encryption, and one for decryption. The term \"cipher\" (sometimes \"cypher\") is often used to refer to a pair of algorithms, one for encryption and one for decryption. Therefore, the term \"cryptosystem\" is most often used when the key generation algorithm is important. For this reason, the term \"cryptosystem\" is commonly used to refer to public key techniques; however both \"cipher\" and \"cryptosystem\" are used for symmetric key techniques.\n\nMathematically, a cryptosystem or encryption scheme can be defined as a tuple formula_1 with the following properties.\n\n\nFor each formula_9, there is formula_10 such that formula_11 for all formula_12.\n\nNote; typically this definition is modified in order to distinguish an encryption scheme as being either a symmetric-key or public-key type of cryptosystem.\n\nA classical example of a cryptosystem is the Caesar cipher. A more contemporary example is the RSA cryptosystem.\n"}
{"id": "57326", "url": "https://en.wikipedia.org/wiki?curid=57326", "title": "De Moivre's formula", "text": "De Moivre's formula\n\nIn mathematics, de Moivre's formula (also known as de Moivre's theorem and de Moivre's identity), named after Abraham de Moivre, states that for any real number and integer it holds that\n\nwhere is the imaginary unit (). While the formula was named after de Moivre, he never stated it in his works. The expression is sometimes abbreviated to .\n\nThe formula is important because it connects complex numbers and trigonometry. By expanding the left hand side and then comparing the real and imaginary parts under the assumption that is real, it is possible to derive useful expressions for and in terms of and .\n\nAs written, the formula is not valid for non-integer powers . However, there are generalizations of this formula valid for other exponents. These can be used to give explicit expressions for the th roots of unity, that is, complex numbers such that .\n\nAlthough historically proven earlier, de Moivre's formula can easily be derived from Euler's formula\n\nand the exponential law for integer powers\n\nThen, by Euler's formula,\n\nThe truth of de Moivre's theorem can be established by using mathematical induction for natural numbers, and extended to all integers from there. For an integer , call the following statement :\n\nFor , we proceed by mathematical induction. is clearly true. For our hypothesis, we assume is true for some natural . That is, we assume\n\nNow, considering :\n\nSee angle sum and difference identities.\n\nWe deduce that implies . By the principle of mathematical induction it follows that the result is true for all natural numbers. Now, is clearly true since . Finally, for the negative integer cases, we consider an exponent of for natural .\n\nThe equation (*) is a result of the identity\nfor . Hence, holds for all integers .\n\nBeing an equality of complex numbers, one necessarily has equality both of the real parts and of the imaginary parts of both members of the equation. If , and therefore also and , are real numbers, then the identity of these parts can be written using binomial coefficients. This formula was given by 16th century French mathematician François Viète:\n\nIn each of these two equations, the final trigonometric function equals one or minus one or zero, thus removing half the entries in each of the sums. These equations are in fact even valid for complex values of , because both sides are entire (that is, holomorphic on the whole complex plane) functions of , and two such functions that coincide on the real axis necessarily coincide everywhere. Here are the concrete instances of these equations for and :\n\nThe right hand side of the formula for is in fact the value of the Chebyshev polynomial at .\n\nDe Moivre's formula does not hold for non-integer powers. The derivation of de Moivre's formula above involves a complex number raised to the integer power . If a complex number is raised to a non-integer power, the result is multiple-valued (see failure of power and logarithm identities). For example, when , de Moivre's formula gives the following results:\n\nThis assigns two different values for the same expression 1, so the formula is not consistent in this case.\n\nOn the other hand, the values 1 and −1 are both square roots of 1. More generally, if and are complex numbers, then\n\nis multi-valued while\n\nis not. However, it is always the case that\n\nis one value of\n\nA modest extension of the version of de Moivre's formula given in this article can be used to find the th roots of a complex number (equivalently, the power of ).\n\nIf is a complex number, written in polar form as\n\nthen the th roots of are given by\n\nwhere varies over the integer values from 0 to .\n\nThis formula is also sometimes known as de Moivre's formula.\n\nSince , an analog to de Moivre's formula also applies to the hyperbolic trigonometry. For all ,\nAlso, if , then one value of will be .\n\nThe formula holds for any complex number formula_19\nwhere\n\nTo find the roots of a quaternion there is an analogous form of de Moivre's formula. A quaternion in the form\ncan be represented in the form\nIn this representation,\nand the trigonometric functions are defined as\nIn the case that ,\nthat is, the unit vector. This leads to the variation of De Moivre's formula:\n\nTo find the cube roots of\nwrite the quaternion in the form\nThen the cube roots are given by:\nConsider the following matrix\nformula_31. Then formula_32. This fact (although it can be proven in the very same way as for complex numbers) is a direct consequence of the fact that the space of matrices of type formula_33 is isomorphic to the space of complex numbers.\n\n"}
{"id": "15938739", "url": "https://en.wikipedia.org/wiki?curid=15938739", "title": "Edward Burger", "text": "Edward Burger\n\nEdward Bruce Burger (born December 10, 1964) is a mathematician who is the president of Southwestern University in Georgetown, Texas. Previously he was the Francis Christopher Oakley Third Century Professor of Mathematics at Williams College, and the Robert Foster Cherry Professor for Great Teaching at Baylor University. He also had been named to a single-year-appointment as Vice Provost of Strategic Educational Initiatives at Baylor University in February 2011.\n\nBurger has been honored as a leader in education and for his innovative work in developing educational and entertaining mathematics electronic textbooks. He has been a keynote speaker, invited special session speaker, or the conference chair at a number of American Mathematical Society, Mathematical Association of America, and the National Council of Teachers of Mathematics conferences.\n\nDuring the late 1980s Burger was featured at a stand-up comedy club in Austin, Texas and also was an 'independent contractor', writing for Jay Leno. Humor is an integral tool among his teaching methods. Today he has a weekly, lively program on higher education, thinking, and learning produced by NPR's Austin affiliate KUT. The series is aptly called Higher ED, and the episodes are also available on iTunes.\n\nGraduated from Connecticut College in 1985, where he had earned B.A. Summa Cum Laude with Distinction in Mathematics, in 1990, he was awarded his Ph.D. in mathematics from The University of Texas at Austin, where his thesis advisor was Professor Jeffrey Vaaler. He did his postdoctoral work at the University of Waterloo in Canada. In 2013 he was awarded an LL.D. from Williams College.\n\nHis research interests include algebraic number theory, Diophantine analysis, \"p\"-adic analysis, geometry of numbers, and the theory of continued fractions. He teaches abstract algebra, \"The Art of Creating Mathematics\", and Diophantine analysis.\n\nHe has taught or has been a visiting scholar at The University of Texas at Austin, Westminster College, James Madison University, the University of Colorado at Boulder, Concordia University Texas, Baylor University, and the Macquarie University in Australia.\n\nBurger is a pioneer in rich, multimedia Internet lectures that, together with written material, form an electronic textbook. By providing students with the presence of a recorded instructor, the material is much more interactive and engaging compared to traditional text books. Together with Thinkwell, Burger \"crafted the first-ever virtual, CD-ROM video, interactive, mathematics texts/courses\" published over the World Wide Web; he is featured in several of the entertaining multimedia math lessons. Additionally, his lesson tutorial videos earned publisher Holt, Rinehart and Winston one of the 2007 \"Awards of Excellence\" from \"Technology & Learning\", an academic publication.\n\nBurger has written and starred in number of educational videos, including the 24-lecture video series \"Zero to Infinity: A History of Numbers\" and \"An Introduction to Number Theory\". He has delivered more than 400 lectures worldwide and has appeared on more than 40 radio and TV programs including \"ABC News Now\" on WABC-TV in New York and National Public Radio. He starred in the \"Mathletes\" episode of NBC's \"Science of the Winter Olympics\" series shown on the Today Show and throughout the 2010 Winter Olympic Games. That episode won a Telly Award.\n\nIn recognition for his work in multimedia education technology, The Association of Educational Publishers awarded Burger with the 2007 \"Distinguished Achievement Award\" for Educational Video Technology\n\nIt should come as no surprise that someone so visually focused would embrace creativity as one of the cornerstones of his teaching methods:\nBurger feels that \"math trauma\" is commonly inflicted upon America's elementary and middle-school students, particularly girls, having received a seventh-grade report card stating: \"Eddie is a nice boy, but he'll never do well in arithmetic.\" The problem is not with math, but with the way in which it is taught. \"When we teach mathematics, we are not sensitive to the audience. Teachers are performers in front of an audience. Some teachers don't realize they have to reach their audience.\"\n\nHe offers his students \"challenging questions for which the solution is by no means apparent\". For example, when teaching students about topology, he asked students if it is \"possible to take a cord of rope long and tie it snugly around your right ankle and your left ankle, take off your pants, turn them inside out, and put your pants back on without ever cutting the rope?\" He proceeded to demonstrate the solution to that challenge, wearing huge Boston Red Sox boxer shorts under his trousers, at the Boston Public Library in the summer of 2005. He believes that failure is closely tied to creativity:\n\"As a professor, I'm basically performing three times a week. I like to be funny and to make my students laugh,\" he said. In addition to his math courses, Burger teaches a short course in comedy writing during the winter study program at Williams. Combining math with comedy comes from his days as a stand-up comic at the Laff Stop Comedy Club in Austin in the late-1980s.\n\nBurger states that \"mathematics is basically an approach to solving difficult problems. The techniques and strategies that we can learn in mathematics are techniques and strategies we can use to solve any difficult problem.\" In the textbook he co-authored with Michael Starbird, \"The Heart of Mathematics\", they offer their favorite top ten mathematical ways of thinking:\n\nRecently, Burger and Starbird expanded their scope beyond mathematics while distilling their \"top ten\" list down to just five in their book, \"The 5 Elements of Effective Thinking\"\n\nBurger has written 12 books and has had more than 30 papers published in scholarly journals. With Michael Starbird, he coauthored \"The Heart of Mathematics: An invitation to effective thinking\", for which they won a 2001 Robert W. Hamilton Book Award, and \"Coincidences, Chaos, and All That Math Jazz,\", a humorous look at mathematics filed under both \"math\" and \"humor\" in the Library of Congress catalog. Burger is also an associate editor for the American Mathematical Monthly and a member of the Editorial Board for [AK Peters Publishing].\n\nSome of the books and papers he has authored or co-authored include:\n\nAdditionally, Burger has created virtual video textbooks on CD-ROM and on the web for Thinkwell on the topics of \"College Algebra\", 2000; \"Pre-Calculus\", 2000; \"Calculus\", 2001; \"Intermediate Algebra\", 2001; \"Beginning Algebra\", 2004; \"Trigonometry\", 2006; \"Prealgebra\", 2007; and \"Algebra II\", 2011.\n\nBurger has held the following professional positions:\n\nSome of the honors and awards Burger has received include:\n\nBurger is the Francis Christopher Oakley Third Century Professor of Mathematics and was the Lissack Professor for Social Responsibility and Personal Ethics (2010–2012) and the Gaudino Scholar (2008–2010) at Williams College, where he was also awarded the 2007 \"Nelson Bushnell Prize\" for Scholarship and Teaching.\n\nBurger has been honored by The Mathematical Association of America on several occasions: 2001, Deborah and Franklin Tepper Haimo Award for Distinguished College or University Teaching of Mathematics; 2001–2003, George Pólya Lecturer; 2004, \"Chauvenet Prize\"; and 2006, \"Lester R. Ford Award\"\n"}
{"id": "35208721", "url": "https://en.wikipedia.org/wiki?curid=35208721", "title": "Euler substitution", "text": "Euler substitution\n\nEuler substitution is a method for evaluating integrals of the form\n\nwhere formula_2 is a rational function of formula_3 and formula_4. In such cases, the integrand can be changed to a rational function by using the substitutions of Euler.\n\nThe first substitution of Euler is used when formula_5. We substitute\nformula_6\nand solve the resulting expression for formula_3. We have that formula_8 and that the formula_9 term is expressible rationally in formula_10.\n\nIn this substitution, either the positive sign or the negative sign can be chosen.\n\nIf formula_11, we take\nformula_12\nWe solve for formula_3 similarly as above and find\nformula_14\n\nAgain, either the positive or the negative sign can be chosen.\n\nIf the polynomial formula_15 has real roots formula_16 and formula_17, we may choose\nformula_18. This yields \nformula_19\nand as in the preceding cases, we can express the entire integrand rationally in formula_10.\n\nIn the integral formula_21 we can use the first substitution and set formula_22, thus\nAccordingly, we obtain:\nThe cases formula_27, give the formulas\n\nFor finding the value of formula_30, we find formula_10 using the first substitution of Euler: formula_32. Squaring both sides of the equation gives us formula_33, from which the formula_34 terms will cancel out; solving for formula_3 yields formula_36. From there, we can take derivatives of both sides of the equation, find formula_37, and substitute formula_3 and formula_37 in the integral to find the answer.\n\nformula_40; formula_41 and since formula_42, the value of the integral is formula_43\n\nIn the integral formula_44 we can use the second substitution and set formula_45, thus\nAccordingly, we obtain:\n\nIn the integral formula_51 we can use the third substitution and set formula_52, thus\nAccordingly, we obtain:\nAs we can see this is a rational function which can be solved using partial fractions.\n\nThe substitutions of Euler can be generalized by allowing the use of imaginary numbers. For example, in the integral formula_56, the substitution formula_57 can be used. Extensions to the complex numbers allows us to use every type of Euler substitution regardless of the coefficients on the quadratic.\n\nThe substitutions of Euler can be generalized to a larger class of functions. Consider integrals of the form\n\nwhere formula_59 and formula_60 are rational functions of formula_3 and formula_4. This integral can be transformed by the substitution formula_63 into another integral\n\nwhere formula_65 and formula_66 are now simply rational functions of formula_10. In principle, factorization and partial fraction decomposition can be employed to break the integral down into simple terms, which can be integrated analytically through use of the dilogarithm function.\n"}
{"id": "6338601", "url": "https://en.wikipedia.org/wiki?curid=6338601", "title": "F-FCSR", "text": "F-FCSR\n\nIn cryptography, F-FCSR is a stream cipher developed by Thierry Berger, François Arnault, and Cédric Lauradoux. The core of the cipher is a Feedback with Carry Shift Register (FCSR) automaton, which is similar to a LFSR, but they perform operations with carries so their transition function is nonlinear.\n\nF-FCSR was one of the eight algorithms selected for the eCRYPT network's eSTREAM Portfolio, but it was later removed because further analysis showed weaknesses.\n"}
{"id": "2617872", "url": "https://en.wikipedia.org/wiki?curid=2617872", "title": "Faithful representation", "text": "Faithful representation\n\nIn mathematics, especially in an area of abstract algebra known as representation theory, a faithful representation ρ of a group formula_1 on a vector space formula_2 is a linear representation in which different elements formula_3 of formula_1 are represented by distinct linear mappings formula_5. \n\nIn more abstract language, this means that the group homomorphism\n\nformula_6\n\nis injective (or one-to-one).\n\n\"Caveat:\" While representations of formula_1 over a field formula_8 are \"de facto\" the same as formula_9-modules (with formula_9 denoting the group algebra of the group formula_1), a faithful representation of formula_1 is not necessarily a faithful module for the group algebra. In fact each faithful formula_9-module is a faithful representation of formula_1, but the converse does not hold. Consider for example the natural representation of the symmetric group formula_15 in formula_16 dimensions by permutation matrices, which is certainly faithful. Here the order of the group is formula_16! while the formula_18 matrices form a vector space of dimension formula_19. As soon as formula_16 is at least 4, dimension counting means that some linear dependence must occur between permutation matrices (since formula_21); this relation means that the module for the group algebra is not faithful.\n\nA representation formula_2 of a finite group formula_1 over an algebraically closed field formula_8 of characteristic zero is faithful (as a representation) if and only if every irreducible representation of formula_1 occurs as a subrepresentation of formula_26 (the formula_16-th symmetric power of the representation formula_2) for a sufficiently high formula_16. Also, formula_2 is faithful (as a representation) if and only if every irreducible representation of formula_1 occurs as a subrepresentation of\n(the formula_16-th tensor power of the representation formula_2) for a sufficiently high formula_16.\n"}
{"id": "16415701", "url": "https://en.wikipedia.org/wiki?curid=16415701", "title": "Fast Library for Number Theory", "text": "Fast Library for Number Theory\n\nThe Fast Library for Number Theory (FLINT) is a C library for number theory applications. The two major areas of functionality currently implemented in FLINT are polynomial arithmetic over the integers and a quadratic sieve. The library is designed to be compiled with the GNU Multi-Precision Library (GMP) and is released under the GNU General Public License. It is developed by William Hart of the University of Kaiserslautern (formerly University of Warwick) and David Harvey of University of New South Wales (formerly Harvard University) to address the speed limitations of the PARI and NTL libraries.\n\n\n\n\n"}
{"id": "22506723", "url": "https://en.wikipedia.org/wiki?curid=22506723", "title": "Fuzzy number", "text": "Fuzzy number\n\nA fuzzy number is a generalization of a regular, real number in the sense that it does not refer to one single value but rather to a connected set of possible values, where each possible value has its own weight between 0 and 1. This weight is called the membership function. A fuzzy number is thus a special case of a convex, normalized fuzzy set of the real line. Just like Fuzzy logic is an extension of Boolean logic (which uses absolute truth and falsehood only, and nothing in between), fuzzy numbers are an extension of real numbers. Calculations with fuzzy numbers allow the incorporation of uncertainty on parameters, properties, geometry, initial conditions, etc.\n\n\n"}
{"id": "2949102", "url": "https://en.wikipedia.org/wiki?curid=2949102", "title": "Gauge covariant derivative", "text": "Gauge covariant derivative\n\nThe gauge covariant derivative is a variation of the covariant derivative used in general relativity. If a theory has gauge transformations, it means that some physical properties of certain equations are preserved under those transformations. Likewise, the gauge covariant derivative is the ordinary derivative modified in such a way as to make it behave like a true vector operator, so that equations written using the covariant derivative preserve their physical properties under gauge transformations.\n\nThere are many ways in which to understand the gauge covariant derivative. The approach taken in this article is based on the historically traditional notation used in many physics textbooks. Another approach is to understand the gauge covariant derivative as a kind of connection, and more specifically, an affine connection. The affine connection is interesting because it does not require any concept of a metric tensor to be defined; the curvature of an affine connection can be understood as the field strength of the gauge potential. When a metric is available, then one can go in a different direction, and define a connection on a frame bundle. This path leads directly to general relativity; however, it requires a metric, which particle physics gauge theories do not have.\n\nRather than being generalizations of one-another, affine and metric geometry go off in different directions: the gauge group of (pseudo-)Riemannian geometry \"must\" be the indefinite orthogonal group O(s,r) in general, or the Lorentz group O(3,1) for space-time. This is because the fibers of the frame bundle must necessarily, by definition, connect the tangent and cotangent spaces of space-time. By contrast, the gauge groups employed in particle physics could be (in principle) any Lie group at all (and, in practice, being only U(1), SU(2) or SU(3) in the Standard Model). Note that Lie groups do not come equipped with a metric.\n\nA yet more complicated, yet more accurate and geometrically enlightening, approach is to understand that the gauge covariant derivative is (exactly) the same thing as the exterior covariant derivative on a section of an associated bundle for the principal fiber bundle of the gauge theory; and, for the case of spinors, the associated bundle would be a spin bundle of the spin structure. Although conceptually the same, this approach uses a very different set of notation, and requires a far more advanced background in multiple areas of differential geometry.\n\nThe final step in the geometrization of gauge invariance is to recognize that, in quantum theory, one needs only to compare neighboring fibers of the principal fiber bundle, and that the fibers themselves provide a superfluous extra description. This leads to the idea of modding out the gauge group to obtain the gauge groupoid as the closest description of the gauge connection in quantum field theory.\n\nFor ordinary Lie algebras, the gauge covariant derivative on the space symmetries (those of the pseudo-Riemannian manifold and general relativity) cannot be intertwined with the internal gauge symmetries; that is, metric geometry and affine geometry are necessarily distinct mathematical subjects: this is the content of the Coleman–Mandula theorem. However, a premise of this theorem is violated by the Lie superalgebras (which are \"not\" Lie algebras!) thus offering hope that a single unified symmetry can describe both spatial and internal symmetries: this is the foundation of supersymmetry.\n\nThe more mathematical approach uses an index-free notation, emphasizing the geometric and algebraic structure of the gauge theory and its relationship to Lie algebras and Riemannian manifolds; for example, treating gauge covariance as equivariance on fibers of a fiber bundle. The index notation used in physics makes it far more convenient for practical calculations, although it makes the overall geometric structure of the theory more opaque. The physics approach also has a pedagogical advantage: the general structure of a gauge theory can be exposed after a minimal background in multivariate calculus, whereas the geometric approach requires a large investment of time in the general theory of differential geometry, Riemannian manifolds, Lie algebras, representations of Lie algebras and principle bundles before a general understanding can be developed. In more advanced discussions, both notations are commonly intermixed.\n\nThis article attempts to hew most closely to the notation and language commonly employed in physics curriculum, touching only briefly on the more abstract connections.\n\nIn fluid dynamics, the gauge covariant derivative of a fluid may be defined as\nwhere formula_2 is a velocity vector field of a fluid.\n\nIn gauge theory, which studies a particular class of fields which are of importance in quantum field theory, the minimally-coupled gauge covariant derivative is defined as\nwhere formula_4 is the electromagnetic four potential.\n\nConsider a generic (possibly non-Abelian) Gauge transformation, defined by a symmetry operator formula_7, acting on a field formula_8, such that\nwhere formula_11 is an element of the Lie algebra associated with the Lie group of symmetry transformations, and can be expressed in terms of the generators of the group, formula_12, as formula_13.\n\nThe partial derivative formula_14 transforms, accordingly, as\nand a kinetic term of the form formula_16 is thus not invariant under this transformation.\n\nWe can introduce the covariant derivative formula_17 in this context as a generalization of the partial derivative formula_14 which transforms covariantly under the Gauge transformation, i.e. an object satisfying\nwhich in operatorial form takes the form\nWe thus compute (omitting the explicit formula_21 dependencies for brevity)\nwhere\n\nThe requirement for formula_17 to transform covariantly is now translated in the condition\nTo obtain an explicit expression, we follow QED and make the Ansatz\nwhere the vector field formula_4 satisfies,\n\nfrom which it follows that\nand\nwhich, using formula_31, takes the form\n\nWe have thus found an object formula_17 such that\n\nIf a gauge transformation is given by\nand for the gauge potential \nthen formula_37 transforms as\nand formula_39 transforms as\nand formula_41 transforms as\nso that\nand formula_44 in the QED Lagrangian is therefore gauge invariant, and the gauge covariant derivative is thus named aptly.\n\nOn the other hand, the non-covariant derivative formula_45 would not preserve the Lagrangian's gauge symmetry, since\n\nIn quantum chromodynamics, the gauge covariant derivative is\nwhere formula_48 is the coupling constant, formula_49 is the gluon gauge field, for eight different gluons formula_50, formula_51 is a four-component Dirac spinor, and where formula_52 is one of the eight Gell-Mann matrices, formula_50. The Gell-Mann matricies give a representation of the color symmetry group SU(3). For quarks, the representation is the fundamental representation, for gluons, the representation is the adjoint representation.\n\nThe covariant derivative in the Standard Model can be expressed in the following form:\n\nThe gauge fields here belong to the fundamental representations of the electroweak Lie group formula_55 times the color symmetry Lie group SU(3).\n\nIn general relativity, the gauge covariant derivative is defined as\nwhere formula_57 is the Christoffel symbol. More formally, this derivative can be understood as the Riemannian connection on a frame bundle. The \"gauge freedom\" here is the arbitrary choice of a coordinate frame at each point in space-time.\n\n\n"}
{"id": "17201962", "url": "https://en.wikipedia.org/wiki?curid=17201962", "title": "Helen Abbot Merrill", "text": "Helen Abbot Merrill\n\nHelen Abbot Merrill (1864 – 1949) was an American mathematician, educator and textbook author.\n\nBorn March 30, 1876 to a New Jersey insurance claims adjuster and a housewife, and raised in Massachusetts, her family tree included colonial settlers. Young Helen's formal education started at a high school in Massachusetts, and after graduating she went to Wellesley College, where she intended to major in Greek and Latin. Unusually, the mathematics faculty at the college consisted mostly of women, including Ellen Hayes, and before completing her first years, Helen Merrill had decided to major in mathematics instead of languages. In 1893 she began teaching at Wellesley while also studying and guest lecturing abroad. In 1903 she earned a PhD in mathematics at Yale under the direction of James Pierpont. In 1920 she was appointed vice-president of the Mathematical Association of America. Upon her retirement from Wellesley, she was given the title Professor Emerita.\n\nAt Wellesley, Merrill wrote two textbooks with Clara Eliza Smith, \"Selected Topics in Higher Algebra\" (Norwood, 1914) and \"A First Course in Higher Algebra\" (Macmillan, 1917).\nShe also wrote as a popularizer a book titled \"Mathematical Excursions\" in 1933.\n\n\n"}
{"id": "38023412", "url": "https://en.wikipedia.org/wiki?curid=38023412", "title": "Hua's identity", "text": "Hua's identity\n\nIn algebra, Hua's identity states that for any elements \"a\", \"b\" in a division ring,\nwhenever formula_2. Replacing formula_3 with formula_4 gives another equivalent form of the identity:\n\nAn important application of the identity is a proof of Hua's theorem. The theorem says that if formula_6 is a function between division rings and if formula_7 satisfies:\nthen formula_7 is either a homomorphism or an antihomomorphism. The theorem is important because of the connection to the fundamental theorem of projective geometry.\n\nformula_10\n\n"}
{"id": "302178", "url": "https://en.wikipedia.org/wiki?curid=302178", "title": "Index of logic articles", "text": "Index of logic articles\n\nA System of Logic --\nA priori and a posteriori --\nAbacus logic --\nAbduction (logic) --\nAbductive validation --\nAcademia Analitica --\nAccuracy and precision --\nAd captandum --\nAd hoc hypothesis --\nAd hominem --\nAffine logic --\nAffirming the antecedent --\nAffirming the consequent --\nAlgebraic logic --\nAmbiguity --\nAnalysis --\nAnalysis (journal) --\nAnalytic reasoning --\nAnalytic–synthetic distinction --\nAnangeon --\nAnecdotal evidence --\nAntecedent (logic) --\nAntepredicament --\nAnti-psychologism --\nAntinomy --\nApophasis --\nAppeal to probability --\nAppeal to ridicule --\nArchive for Mathematical Logic --\nArché --\nArgument --\nArgument by example --\nArgument form --\nArgument from authority --\nArgument map --\nArgumentation ethics --\nArgumentation theory --\nArgumentum ad baculum --\nArgumentum e contrario --\nAriadne's thread (logic) --\nAristotelian logic --\nAristotle --\nAssociation for Informal Logic and Critical Thinking --\nAssociation for Logic, Language and Information --\nAssociation for Symbolic Logic --\nAttacking Faulty Reasoning --\nAustralasian Association for Logic --\nAxiom --\nAxiom independence --\nAxiom of reducibility --\nAxiomatic system --\nAxiomatization --\n\nBackward chaining --\nBarcan formula --\nBegging the question --\nBegriffsschrift --\nBelief --\nBelief bias --\nBelief revision --\nBenson Mates --\nBertrand Russell Society --\nBiconditional elimination --\nBiconditional introduction --\nBivalence and related laws --\nBlue and Brown Books --\nBoole's syllogistic --\nBoolean algebra (logic) --\nBoolean algebra (structure) --\nBoolean network --\n\nCanon (basic principle) --\nCanonical form --\nCanonical form (Boolean algebra) --\nCartesian circle --\nCase-based reasoning --\nCategorical logic --\nCategories (Aristotle) --\nCategories (Peirce) --\nCategory mistake --\nCatuṣkoṭi --\nCircular definition --\nCircular reasoning --\nCircular reference --\nCircular reporting --\nCircumscription (logic) --\nCircumscription (taxonomy) --\nClassical logic --\nClocked logic --\nCognitive bias --\nCointerpretability --\nColorless green ideas sleep furiously --\nCombinational logic --\nCombinatory logic --\nCombs method --\nCommon knowledge (logic) --\nCommutativity of conjunction --\nCompleteness (logic) --\nComposition of Causes --\nCompossibility --\nComprehension (logic) --\nComputability logic --\nConcept --\nConceptualism --\nCondensed detachment --\nConditional disjunction --\nConditional probability --\nConditional proof --\nConditional quantifier --\nConfirmation bias --\nConflation --\nConfusion of the inverse --\nConjunction elimination --\nConjunction fallacy --\nConjunction introduction --\nConjunctive normal form --\nConnexive logic --\nConnotation --\nConsequent --\nConsistency --\nConstructive dilemma --\nContra principia negantem non est disputandum --\nContradiction --\nContrapositive --\nControl logic --\nConventionalism --\nConverse (logic) --\nConverse Barcan formula --\nCorrelative-based fallacies --\nCounterexample --\nCounterfactual conditional --\nCounterintuitive --\nCratylism --\nCredibility --\nCriteria of truth --\nCritical-Creative Thinking and Behavioral Research Laboratory --\nCritical pedagogy --\nCritical reading --\nCritical thinking --\nCritique of Pure Reason --\nCurry's paradox --\nCyclic negation --\n\nDagfinn Føllesdal --\nDe Interpretatione --\nDe Morgan's laws --\nDecidability (logic) --\nDecidophobia --\nDecision making --\nDecisional balance sheet --\nDeductive closure --\nDeduction theorem --\nDeductive fallacy --\nDeductive reasoning --\nDefault logic --\nDefeasible logic --\nDefeasible reasoning --\nDefinable set --\nDefinist fallacy --\nDefinition --\nDefinitions of logic --\nDegree of truth --\nDenying the antecedent --\nDenying the correlative --\nDeontic logic --\nDescription --\nDescription logic --\nDescriptive fallacy --\nDeviant logic --\nDharmakirti --\nDiagrammatic reasoning --\nDialectica --\nDialectica space --\nDialetheism --\nDichotomy --\nDifference (philosophy) --\nDigital timing diagram --\nDignāga --\nDilemma --\nDisjunction elimination --\nDisjunction introduction --\nDisjunctive normal form --\nDisjunctive syllogism --\nDispositional and occurrent belief --\nDisquotational principle --\nDissoi logoi --\nDivision of Logic, Methodology, and Philosophy of Science --\nDon't-care term --\nDonald Davidson (philosopher) --\nDouble counting (fallacy) --\nDouble negation --\nDouble negative --\nDouble negative elimination --\nDoxa --\nDrinking the Kool-Aid --\n\nEL++ --\nEcological fallacy --\nEffective method --\nElimination rule --\nEmotional reasoning --\nEmotions in decision-making --\nEmpty name --\nEncyclopedia of the Philosophical Sciences --\nEnd term --\nEngineered language --\nEntailment --\nEntitative graph --\nEnumerative definition --\nEpicureanism --\nEpilogism --\nEpistemic closure --\nEquisatisfiability --\nErotetics --\nEternal statement --\nEtymological fallacy --\nEuropean Summer School in Logic, Language and Information --\nEvidence --\nExclusive nor --\nExclusive or --\nExistential fallacy --\nExistential graph --\nExistential quantification --\nExpert --\nExplanandum --\nExplanation --\nExplanatory power --\nExtension (semantics) --\nExtensional context --\nExtensional definition --\n\nFa (concept) --\nFact --\nFallacies of definition --\nFallacy --\nFallacy of distribution --\nFallacy of four terms --\nFallacy of quoting out of context --\nFallacy of the four terms --\nFalse attribution --\nFalse dilemma --\nFalse equivalence --\nFalse premise --\nFictionalism --\nFinitary relation --\nFinite model property --\nFirst-order logic --\nFirst-order predicate --\nFirst-order predicate calculus --\nFirst-order resolution --\nFitch-style calculus --\nFluidic logic --\nFluidics --\nFormal fallacy --\nFormal ontology --\nFormal system --\nFormalism (philosophy) --\nForward chaining --\nFree logic --\nFree variables and bound variables --\nFunction and Concept --\nFuzzy logic --\n\nGame semantics --\nGanto's Ax --\nGeometry of interaction --\nGilles-Gaston Granger --\nGongsun Long --\nGrammaticality --\nGreedy reductionism --\nGrundlagen der Mathematik --\n\nHPO formalism --\nHalo effect --\nHandbook of Automated Reasoning --\nHanlon's razor --\nHasty generalization --\nHerbrandization --\nHetucakra --\nHeyting algebra --\nHigher-order predicate --\nHigher-order thinking --\nHistorian's fallacy --\nHistorical fallacy --\nHistory of logic --\nHistory of the function concept --\nHold come what may --\nHomunculus argument --\nHorn clause --\nHume's fork --\nHume's principle --\nHypothetical syllogism --\n\nIdentity (philosophy) --\nIdentity of indiscernibles --\nIdola fori --\nIdola specus --\nIdola theatri --\nIdola tribus --\nIf-by-whiskey --\nIff --\nIllicit major --\nIllicit minor --\nIlluminationism --\nImmutable truth --\nImperative logic --\nImplicant --\nInclusion (logic) --\nIncomplete comparison --\nInconsistent comparison --\nInconsistent triad --\nIndependence-friendly logic --\nIndian logic --\nInductive logic --\nInductive logic programming --\nInference --\nInference procedure --\nInference rule --\nInferential role semantics --\nInfinitary logic --\nInfinite regress --\nInfinity --\nInformal fallacy --\nInformal logic --\nInquiry --\nInquiry (philosophy journal) --\nInsolubilia --\nInstitute for Logic, Language and Computation --\nIntellectual responsibility --\nIntended interpretation --\nIntension --\nIntensional fallacy --\nIntensional logic --\nIntensional statement --\nIntentional Logic --\nIntermediate logic --\nInterpretability --\nInterpretability logic --\nInterpretive discussion --\nIntroduction rule --\nIntroduction to Mathematical Philosophy --\nIntuitionistic linear logic --\nIntuitionistic logic --\nInvalid proof --\nInventor's paradox --\nInverse (logic) --\nInverse consequences --\nIrreducibility --\nIs Logic Empirical? --\nIsagoge --\nIvor Grattan-Guinness --\n\nJacobus Naveros --\nJayanta Bhatta --\nJingle-jangle fallacies --\nJohn Corcoran (logician) --\nJohn W. Dawson, Jr --\nJournal of Applied Non-Classical Logics --\nJournal of Automated Reasoning --\nJournal of Logic, Language and Information --\nJournal of Logic and Computation --\nJournal of Mathematical Logic --\nJournal of Philosophical Logic --\nJournal of Symbolic Logic --\nJudgment (mathematical logic) --\nJudgmental language --\nJust-so story --\n\nKarnaugh map --\nKinetic logic --\nKnowing and the Known --\nKripke semantics --\nKurt Gödel Society --\n\nLanguage --\nLanguage, Proof and Logic --\nLateral thinking --\nLaw of excluded middle --\nLaw of identity --\nLaw of non-contradiction --\nLaw of noncontradiction --\nLaw of thought --\nLaws of Form --\nLaws of logic --\nLeap of faith --\nLemma (logic) --\nLexical definition --\nLinear logic --\nLinguistic and Philosophical Investigations --\nLinguistics and Philosophy --\nList of fallacies --\nList of incomplete proofs --\nList of logic journals --\nList of paradoxes --\nLogic --\nLogic Lane --\nLogic Spectacles --\nLogic gate --\nLogic in China --\nLogic in Islamic philosophy --\nLogic of class --\nLogic of information --\nLogic programming --\nLogica Universalis --\nLogica nova --\nLogical Analysis and History of Philosophy --\nLogical Investigations (Husserl) --\nLogical Methods in Computer Science --\nLogical abacus --\nLogical argument --\nLogical assertion --\nLogical atomism --\nLogical biconditional --\nLogical conditional --\nLogical conjunction --\nLogical constant --\nLogical disjunction --\nLogical equality --\nLogical equivalence --\nLogical extreme --\nLogical form --\nLogical harmony --\nLogical holism --\nLogical nand --\nLogical nor --\nLogical operator --\nLogical quality --\nLogical truth --\nLogicism --\nLogico-linguistic modeling --\nLogos --\nLoosely associated statements --\nŁoś–Tarski preservation theorem --\nLudic fallacy --\nLwów–Warsaw school of logic --\n\nMain contention --\nMajor term --\nMarkov's principle --\nMartin Gardner bibliography --\nMasked man fallacy --\nMaterial conditional --\nMathematical fallacy --\nMathematical logic --\nMeaning (linguistics) --\nMeaning (non-linguistic) --\nMeaning (philosophy of language) --\nMeaningless statement --\nMegarian school --\nMental model theory of reasoning --\nMereology --\nMeta-communication --\nMetalanguage --\nMetalogic --\nMetamathematics --\nMetasyntactic variable --\nMetatheorem --\nMetavariable --\nMiddle term --\nMinimal axioms for Boolean algebra --\nMinimal logic --\nMinor premise --\nMiscellanea Logica --\nMissing dollar riddle --\nModal fallacy --\nModal fictionalism --\nModal logic --\nModel theory --\nModus ponens --\nModus tollens --\nMoral reasoning --\nMotivated reasoning --\nMoving the goalposts --\nMultigrade predicate --\nMulti-valued logic --\nMultiple-conclusion logic --\nMutatis mutandis --\nMutual knowledge (logic) --\nMutually exclusive events --\nMünchhausen trilemma --\n\nNaive set theory --\nName --\nNarrative logic --\nNatural deduction --\nNatural kind --\nNatural language --\nNecessary and sufficient --\nNecessity and sufficiency --\nNegation --\nNeutrality (philosophy) --\nNirvana fallacy --\nNixon diamond --\nNo true Scotsman --\nNominal identity --\nNon-Aristotelian logic --\nNon-classical logic --\nNon-monotonic logic --\nNon-rigid designator --\nNon sequitur (logic) --\nNoneism --\nNonfirstorderizability --\nNordic Journal of Philosophical Logic --\nNormal form (natural deduction) --\nNovum Organum --\nNyaya --\nNyāya Sūtras --\n\nObject language --\nObject of the mind --\nObject theory --\nOccam's razor --\nOn Formally Undecidable Propositions of Principia Mathematica and Related Systems --\nOne-sided argument --\nOntological commitment --\nOpen sentence --\nOpinion --\nOpposing Viewpoints series --\nOrdered logic --\nOrganon --\nOriginal proof of Gödel's completeness theorem --\nOsmund Lewry --\nOstensive definition --\nOutline of logic --\nOverbelief --\n\nPackage-deal fallacy --\nPanlogism --\nParaconsistent logic --\nParaconsistent logics --\nParade of horribles --\nParadox --\nPars destruens/pars construens --\nPathetic fallacy --\nPer fas et nefas --\nPersuasive definition --\nPeter Simons (academic) --\nPhilosophia Mathematica --\nPhilosophical logic --\nPhilosophy of logic --\nPierce's law --\nPlural quantification --\nPoisoning the well --\nPolarity item --\nPolish Logic --\nPolish notation --\nPolitician's syllogism --\nPolychotomous key --\nPolylogism --\nPolysyllogism --\nPort-Royal Logic --\nPossible world --\nPost's lattice --\nPost disputation argument --\nPost hoc ergo propter hoc --\nPosterior Analytics --\nPractical syllogism --\nPragmatic mapping --\nPragmatic maxim --\nPragmatic theory of truth --\nPramāṇa --\nPramāṇa-samuccaya --\nPrecising definition --\nPrecision questioning --\nPredicable --\nPredicate (logic) --\nPredicate abstraction --\nPredicate logic --\nPreferential entailment --\nPreintuitionism --\nPrescriptivity --\nPresentism (literary and historical analysis) --\nPresupposition --\nPrincipia Mathematica --\nPrinciple of bivalence --\nPrinciple of explosion --\nPrinciple of nonvacuous contrast --\nPrinciple of sufficient reason --\nPrinciples of Mathematical Logic --\nPrior Analytics --\nPrivate Eye Project --\nPro hominem --\nProbabilistic logic --\nProbabilistic logic network --\nProblem of future contingents --\nProblem of induction --\nProcess of elimination --\nProject Reason --\nProof-theoretic semantics --\nProof (truth) --\nProof by assertion --\nProof theory --\nPropaganda techniques --\nProposition --\nPropositional calculus --\nPropositional function --\nPropositional representation --\nPropositional variable --\nProsecutor's fallacy --\nProvability logic --\nProving too much --\nPrudence --\nPseudophilosophy --\nPsychologism --\nPsychologist's fallacy --\n\nQ.E.D. --\nQuantification --\nQuantization (linguistics) --\nQuantum logic --\n\nRamism --\nRationality --\nRazor (philosophy) --\nReason --\nReductio ad absurdum --\nReference --\nReflective equilibrium --\nRegression fallacy --\nRegular modal logic --\nReification (fallacy) --\nRelativist fallacy --\nRelevance --\nRelevance logic --\nRelevant logic --\nRemarks on the Foundations of Mathematics --\nRetroduction --\nRetrospective determinism --\nRevolutions in Mathematics --\nRhetoric --\nRigour --\nRolandas Pavilionis --\nRound square copula --\nRudolf Carnap --\nRule of inference --\nRvachev function --\n\nSEE-I --\nSalva congruitate --\nSalva veritate --\nSatisfiability --\nScholastic logic --\nSchool of Names --\nScience of Logic --\nScientific temper --\nSecond-order predicate --\nSegment addition postulate --\nSelf-reference --\nSelf-refuting idea --\nSelf-verifying theories --\nSemantic theory of truth --\nSemantics --\nSense and reference --\nSequent --\nSequent calculus --\nSequential logic --\nSet (mathematics) --\nSeven Types of Ambiguity (Empson) --\nSheffer stroke --\nShip of Theseus --\nSimple non-inferential passage --\nSingular term --\nSituation --\nSituational analysis --\nSkeptic's Toolbox --\nSlingshot argument --\nSocial software (social procedure) --\nSocratic questioning --\nSoku hi --\nSome Remarks on Logical Form --\nSophism --\nSophistical Refutations --\nSoundness --\nSource credibility --\nSource criticism --\nSpecial case --\nSpecialization (logic) --\nSpeculative reason --\nSpurious relationship --\nSquare of opposition --\nState of affairs (philosophy) --\nStatement (logic) --\nStraight and Crooked Thinking --\nStraight face test --\nStraw man --\nStrength (mathematical logic) --\nStrict conditional --\nStrict implication --\nStrict logic --\nStructural rule --\nStudia Logica --\nStudies in Logic, Grammar and Rhetoric --\nSubjective logic --\nSubstitution (logic) --\nSubstructural logic --\nSufficient condition --\nSum of Logic --\nSunk costs --\nSupertask --\nSupervaluationism --\nSupposition theory --\nSurvivorship bias --\nSyllogism --\nSyllogistic fallacy --\nSymbol (formal) --\nSyntactic Structures --\nSyntax (logic) --\nSynthese --\nSystems of Logic Based on Ordinals --\n\nT-schema --\nTacit assumption --\nTarski's undefinability theorem --\nTautology (logic) --\nTemporal logic --\nTemporal parts --\nTeorema (journal) --\nTerm (argumentation) --\nTerm logic --\nTernary logic --\nTestability --\nTetralemma --\nTextual case based reasoning --\nThe False Subtlety of the Four Syllogistic Figures --\nThe Foundations of Arithmetic --\nThe Geography of Thought --\nThe Laws of Thought --\nThe Paradoxes of the Infinite --\nTheorem --\nTheoretical definition --\nTheory and Decision --\nTheory of justification --\nTheory of obligationes --\nThird-cause fallacy --\nThree men make a tiger --\nTolerance (in logic) --\nTopical logic --\nTopics (Aristotle) --\nTractatus Logico-Philosophicus --\nTrain of thought --\nTrairūpya --\nTransferable belief model --\nTransparent Intensional Logic --\nTregoED --\nTrikonic --\nTrilemma --\nTrivial objections --\nTrivialism --\nTruth --\nTruth-bearer --\nTruth claim --\nTruth condition --\nTruth function --\nTruth value --\nTruthiness --\nTruthmaker --\nType (model theory) --\nType theory --\nType–token distinction --\n\nUltrafinitism --\nUnification (computer science) --\nUnifying theories in mathematics --\nUniqueness quantification --\nUniversal logic --\nUniversal quantification --\nUnivocity --\nUnspoken rule --\nUse–mention distinction --\n\nVacuous truth --\nVagrant predicate --\nVagueness --\nValidity --\nValuation-based system --\nVan Gogh fallacy --\nVenn diagram --\nVicious circle principle --\n\nWarnier/Orr diagram --\nWell-formed formula --\nWhat the Tortoise Said to Achilles --\nWillard Van Orman Quine --\nWilliam Kneale --\nWindow operator --\nWisdom of repugnance --\nWitness (mathematics) --\nWord sense --\n\nZhegalkin polynomial --\n\n"}
{"id": "379845", "url": "https://en.wikipedia.org/wiki?curid=379845", "title": "Inflection point", "text": "Inflection point\n\nIn differential calculus, an inflection point, point of inflection, flex, or inflection (British English: inflexion) is a point on a continuously differentiable plane curve at which the curve crosses its tangent, that is, the curve changes from being concave (concave downward) to convex (concave upward), or vice versa.\n\nIf the curve is the graph of a function , of differentiability class , this means that the second derivative of vanishes and changes sign at the point. A point where the second derivative vanishes but does not change sign is sometimes called a point of undulation or undulation point.\n\nIn algebraic geometry an inflection point is defined slightly more generally, as a regular point where the tangent meets the curve to order at least 3, and an undulation point or hyperflex is defined as a point where the tangent meets the curve to order at least 4.\n\nInflection points are the points of the curve where the curvature changes its sign while a tangent exists.\n\nA differentiable function has an inflection point at (\"x\", \"f\"(\"x\")) if and only if its first derivative, \"f′\", has an isolated extremum at \"x\". (This is not the same as saying that \"f\" has an extremum). That is, in some neighborhood, \"x\" is the one and only point at which \"f′\" has a (local) minimum or maximum. If all extrema of \"f′\" are isolated, then an inflection point is a point on the graph of \"f\" at which the tangent crosses the curve.\n\nA \"rising point of inflection\" is an inflection point where the derivative has a local minimum, and a \"falling point of inflection\" is a point where the derivative has a local maximum.\n\nFor an algebraic curve, a non singular point is an inflection point if and only if the multiplicity of the intersection of the tangent line and the curve (at the point of tangency) is odd and greater than 2.\n\nFor a curve given by parametric equations, a point is an inflection point if its signed curvature changes from plus to minus or from minus to plus, i.e., changes sign.\n\nFor a twice differentiable function, an inflection point is a point on the graph at which the second derivative has an isolated zero and changes sign.\n\nIf \"x\" is an inflection point for \"f\" then the second derivative, \"f″\"(\"x\"), is equal to zero if it exists, but this condition does not provide a sufficient definition of a point of inflection. One also needs the lowest-order (above the second) non-zero derivative to be of odd order (third, fifth, etc.). If the lowest-order non-zero derivative is of even order, the point is not a point of inflection, but an \"undulation point\". However, in algebraic geometry, both inflection points and undulation points are usually called \"inflection points\". An example of such an undulation point is \"x\" = 0 for the function \"f\" given by \"f\"(\"x\") = \"x\".\n\nThis definition assumes that \"f\" has some higher-order non-zero derivative at \"x\", which is not necessarily the case, But if it has one, it follows from the definition that the sign of \"f′\"(\"x\") is the same on either side of \"x\" in a neighborhood of \"x\". If this is positive, the point is a \"rising point of inflection\"; if it is negative, the point is a \"falling point of inflection\".\n\nInflection points sufficient conditions:\n\n1) A sufficient existence condition for a point of inflection is:\n\n2) Another sufficient existence condition requires \"f′′\"(\"x\" + ε) and \"f′′\"(\"x\" − \"ε\") to have opposite signs in the neighborhood of \"x\" , if also a tangent exists here. (Bronshtein and Semendyayev 2004, p. 231).\n\nPoints of inflection can also be categorized according to whether \"f′\"(\"x\") is zero or not zero.\n\nA stationary point of inflection is not a local extremum. More generally, in the context of functions of several real variables, a stationary point that is not a local extremum is called a saddle point.\n\nAn example of a stationary point of inflection is the point (0,0) on the graph of \"y\" = \"x\". The tangent is the \"x\"-axis, which cuts the graph at this point.\n\nAn example of a non-stationary point of inflection is the point (0,0) on the graph of \"y\" = \"x\" + \"ax\", for any nonzero \"a\". The tangent at the origin is the line \"y\" = \"ax\", which cuts the graph at this point.\n\nSome functions change concavity without having points of inflection. Instead, they can change concavity around vertical asymptotes or discontinuities. For example, the function formula_1 is concave for negative and convex for positive , but it has no points of inflection because 0 is not in the domain of the function.\n\n\n\n"}
{"id": "19777721", "url": "https://en.wikipedia.org/wiki?curid=19777721", "title": "Introductio in analysin infinitorum", "text": "Introductio in analysin infinitorum\n\nIntroductio in analysin infinitorum (Introduction to the Analysis of the Infinite) is a two-volume work by Leonhard Euler which lays the foundations of mathematical analysis. Written in Latin and published in 1748, the \"Introductio\" contains 18 chapters in the first part and 22 chapters in the second. It has Eneström numbers E101 and E102.\n\nCarl Boyer's lectures at the 1950 International Congress of Mathematicians compared the influence of Euler's \"Introductio\" to that of Euclid's \"Elements\", calling the \"Elements\" the foremost textbook of ancient times, and the \"Introductio\" \"the foremost textbook of modern times\". Boyer also wrote:\n\nThe first translation into English was that by John D. Blanton, published in 1988. The second, by Ian Bruce, is available online. A list of the editions of \"Introductio\" has been assembled by V. Frederick Rickey.\n\nChapter 1 is on the concepts of variables and functions. Chapter 4 introduces infinite series through rational functions.\n\nAccording to Henk Bos,\n\nEuler accomplished this feat by introducing exponentiation \"a\" for arbitrary constant \"a\" in the positive real numbers. He noted that mapping \"x\" this way is \"not\" an algebraic function, but rather a transcendental function. For \"a\" > 1 these functions are monotonic increasing and form bijections of the real line with positive real numbers. Then each base \"a\" corresponds to an inverse function called the logarithm to base \"a\", in chapter 6. In chapter 7, Euler introduces e as the number whose hyperbolic logarithm is 1. The reference here is to Gregoire de Saint-Vincent who performed a quadrature of the hyperbola \"y\" = 1/\"x\" through description of the hyperbolic logarithm. Section 122 labels the logarithm to base e the \"natural or hyperbolic logarithm...since the quadrature of the hyperbola can be expressed through these logarithms\". Here he also gives the exponential series:\n\nThen in chapter 8 Euler is prepared to address the classical trigonometric functions as \"transcendental quantities that arise from the circle.\" He uses the unit circle and presents Euler's formula. Chapter 9 considers trinomial factors in polynomials. Chapter 16 is concerned with partitions, a topic in number theory. Continued fractions are the topic of chapter 18.\n\n\n"}
{"id": "20988589", "url": "https://en.wikipedia.org/wiki?curid=20988589", "title": "Jordan's inequality", "text": "Jordan's inequality\n\nIn mathematics, Jordan's inequality, named after Camille Jordan, states that\n\nIt can be proven through the geometry of circles (see drawing).\n\n\n"}
{"id": "4534426", "url": "https://en.wikipedia.org/wiki?curid=4534426", "title": "Leaky integrator", "text": "Leaky integrator\n\nIn mathematics, a leaky integrator equation is a specific differential equation, used to describe a component or system that takes the integral of an input, but gradually leaks a small amount of input over time. It appears commonly in hydraulics, electronics, and neuroscience where it can represent either a single neuron or a local population of neurons. \n\nThis is equivalent to a 1st-order lowpass filter with cutoff frequency far below the frequencies of interest.\n\nThe equation is of the form\n\nwhere C is the input and A is the rate of the 'leak'.\n\nAs the equation is a nonhomogeneous first-order linear differential equation, its general solution is \n\nwhere formula_3 is a constant, and formula_4 is an arbitrary solution of the equation.\n"}
{"id": "410712", "url": "https://en.wikipedia.org/wiki?curid=410712", "title": "Leibniz's notation", "text": "Leibniz's notation\n\nIn calculus, Leibniz's notation, named in honor of the 17th-century German philosopher and mathematician Gottfried Wilhelm Leibniz, uses the symbols and to represent infinitely small (or infinitesimal) increments of and , respectively, just as and represent finite increments of and , respectively.\n\nConsider as a function of a variable , or = . If this is the case, then the derivative of with respect to , which later came to be viewed as the limit\n\nwas, according to Leibniz, the quotient of an infinitesimal increment of by an infinitesimal increment of , or\n\nwhere the right hand side is Joseph-Louis Lagrange's notation for the derivative of at .\n\nLeibniz's concept of infinitesimals, long considered to be too imprecise to be used as a foundation of calculus, was eventually replaced by rigorous concepts developed by Weierstrass and others. Consequently, Leibniz's quotient notation was re-interpreted to stand for the limit of the modern definition. However, in many instances, the symbol did seem to act as an actual quotient would and its usefulness kept it popular even in the face of several competing notations. In the modern rigorous treatment of non-standard calculus, justification can be found to again consider the notation as representing an actual quotient.\n\nThe Newton–Leibniz approach to infinitesimal calculus was introduced in the 17th century. While Newton worked with fluxions and fluents, Leibniz based his approach on generalizations of sums and differences. Leibniz was the first to use the formula_3 character. He based the character on the Latin word \"summa\" (\"sum\"), which he wrote with the elongated s commonly used in Germany at the time. Viewing differences as the inverse operation of summation, he used the symbol , the first letter of the Latin \"differentia\", to indicate this inverse operation. Leibniz was fastidious about notation; spending years experimenting, adjusting, rejecting and corresponding with other mathematicians about them. Notations he used for the differential of ranged successively from , , and until he finally settled on . His integral sign first appeared publicly in the article \"De Geometria Recondita et analysi indivisibilium atque infinitorum\" (On a hidden geometry and analysis of indivisibles and infinites), published in \"Acta Eruditorum\" in June 1686, but he had been using it in private manuscripts at least since 1675. Leibniz first used in the article \"Nova Methodus pro Maximis et Minimis\" also published in \"Acta Eruditorum\" in 1684. While the symbol does appear in private manuscripts of 1675, it does not appear in this form in either of the above-mentioned published works. Leibniz did, however, use forms such as and in print.\n\nEnglish mathematicians were encumbered by Newton's dot notation until 1803 when Robert Woodhouse published a description of the continental notation. Later the Analytical Society at Cambridge University promoted the adoption of Leibniz's notation.\n\nAt the end of the 19th century, Weierstrass's followers ceased to take Leibniz's notation for derivatives and integrals literally. That is, mathematicians felt that the concept of infinitesimals contained logical contradictions in its development. A number of 19th century mathematicians (Weierstrass and others) found logically rigorous ways to treat derivatives and integrals without infinitesimals using limits as shown above, while Cauchy exploited both infinitesimals and limits (see \"Cours d'Analyse\"). Nonetheless, Leibniz's notation is still in general use. Although the notation need not be taken literally, it is usually simpler than alternatives when the technique of separation of variables is used in the solution of differential equations. In physical applications, one may for example regard \"f\"(\"x\") as measured in meters per second, and d\"x\" in seconds, so that \"f\"(\"x\") d\"x\" is in meters, and so is the value of its definite integral. In that way the Leibniz notation is in harmony with dimensional analysis.\n\nSuppose a dependent variable represents a function of an independent variable , that is,\n\nThen the derivative of the function , in Leibniz's notation for differentiation, can be written as\n\nThe Leibniz expression, also, at times, written , is one of several notations used for derivatives and derived functions. A common alternative is Lagrange's notation\n\nAnother alternative is Newton's notation, often used for derivatives with respect to time (like velocity), which requires placing a dot over the dependent variable (in this case, ):\n\nLagrange's \"prime\" notation is especially useful in discussions of derived functions and has the advantage of having a natural way of denoting the value of the derived function at a specific value. However, the Leibniz notation has other virtues that have kept it popular through the years.\n\nIn its modern interpretation, the expression should not be read as the division of two quantities and (as Leibniz had envisioned it); rather, the whole expression should be seen as a single symbol that is shorthand for\n\n(note vs. , where indicates a finite difference).\n\nThe expression may also be thought of as the application of the differential operator (again, a single symbol) to , regarded as a function of . This operator is written in Euler's notation. Leibniz did not use this form, but his use of the symbol corresponds fairly closely to this modern concept.\n\nWhile there is no division implied by the notation, the division-like notation is useful since in many situations, the derivative operator does behave like a division, making some results about derivatives easy to obtain and remember.\nThis notation owes its longevity to the fact that it seems to reach to the very heart of the geometrical and mechanical applications of the calculus.\n\nIf , the th derivative of in Leibniz notation is given by,\n\nThis notation, for the second derivative, is obtained by using as an operator in the following way,\n\nA third derivative, which might be written as,\n\ncan be obtained from\n\nSimilarly, the higher derivatives may be obtained inductively.\nWhile it is possible, with carefully chosen definitions, to interpret as a quotient of differentials, this should not be done with the higher order forms.\n\nThis notation was, however, not used by Leibniz. In print he did not use multi-tiered notation nor numerical exponents (before 1695). To write for instance, he would write , as was common in his time. The square of a differential, as it might appear in an arc length formula for instance, was written as . However, Leibniz did use his notation as we would today use operators, namely he would write a second derivative as and a third derivative as . In 1695 Leibniz started to write and for and respectively, but l'Hôpital, in his textbook on calculus written around the same time, used Leibniz's original forms.\n\nOne reason that Leibniz's notations in calculus have endured so long is that they permit the easy recall of the appropriate formulas used for differentiation and integration. For instance, the chain rule—suppose that the function is differentiable at and is differentiable at . Then the composite function is differentiable at and its derivative can be expressed in Leibniz notation as,\nThis can be generalized to deal with the composites of several appropriately defined and related functions, and would be expressed as,\n\nAlso, the integration by substitution formula may be expressed by\n\nwhere is thought of as a function of a new variable and the function on the left is expressed in terms of while on the right it is expressed in terms of .\n\nIf where is a differentiable function that is invertible, the derivative of the inverse function, when it exists, can be given by,\nwhere the parentheses are added to emphasize the fact that the derivative is not a fraction.\n\nOne of the simplest types of differential equations is \nwhere and are continuous functions. Solving (implicitly) such an equation can be done by examining the equation in its differential form,\nand integrating to obtain\nRewriting, when possible, a differential equation into this form and applying the above argument is known as the \"separation of variables\" technique for solving such equations.\n\nIn each of these instances the Leibniz notation for a derivative appears to act like a fraction, even though, in its modern interpretation, it isn't one.\n\nIn the 1960s, building upon earlier work by Edwin Hewitt and Jerzy Łoś, Abraham Robinson developed mathematical explanations for Leibniz's infinitesimals that were acceptable by contemporary standards of rigor, and developed non-standard analysis based on these ideas. Robinson's methods are used by only a minority of mathematicians. Jerome Keisler wrote a first-year calculus textbook, \"\", based on Robinson's approach.\n\nFrom the point of view of modern infinitesimal theory, is an infinitesimal -increment, is the corresponding -increment, and the derivative is the standard part of the infinitesimal ratio:\nThen one sets formula_21, formula_22, so that by definition, formula_23 is the ratio of by .\n\nSimilarly, although most mathematicians now view an integral\n\nas a limit\n\nwhere is an interval containing , Leibniz viewed it as the sum (the integral sign denoted summation for him) of infinitely many infinitesimal quantities . From the viewpoint of nonstandard analysis, it is correct to view the integral as the standard part of such an infinite sum.\n\nThe trade-off needed to gain the precision of these concepts is that the set of real numbers must be extended to the set of hyperreal numbers.\n\nLeibniz experimented with many different notations in various areas of mathematics. He felt that good notation was fundamental in the pursuit of mathematics. In a letter to l'Hôpital in 1693 he says:\n\nMany of the over 200 new symbols introduced by Leibniz are still in use today. Besides the differentials , and the integral sign ( ∫ ) already mentioned, he also introduced the colon (:) for division, the dot (⋅) for multiplication, the geometric signs for similar (~) and congruence (≅), the use of Recorde's equal sign (=) for proportions (replacing Oughtred's :: notation) and the double-suffix notation for determinants.\n\n\n"}
{"id": "47859629", "url": "https://en.wikipedia.org/wiki?curid=47859629", "title": "Leslie Ann Goldberg", "text": "Leslie Ann Goldberg\n\nLeslie Ann Goldberg is a professor of computer science at the University of Oxford and a Fellow of St Edmund Hall. Her research concerns the design and analysis of algorithms for random sampling and approximate combinatorial enumeration.\n\nGoldberg did her undergraduate studies at Rice University and completed her doctorate from the University of Edinburgh in 1992 under the joint supervision of Mark Jerrum and Alistair Sinclair. Her dissertation, on algorithms for listing structures with polynomial delay, won the UK Distinguished Dissertations in Computer Science prize. Prior to working at Oxford, her employers have included Sandia National Laboratories, the University of Warwick, and the University of Liverpool.\n\nGoldberg is an editor-in-chief of the Elsevier \"Journal of Discrete Algorithms\", and has served as program chair of the algorithms track of the International Colloquium on Automata, Languages and Programming in 2008.\n\nShe is a member of the Academia Europaea.\n\n"}
{"id": "5027268", "url": "https://en.wikipedia.org/wiki?curid=5027268", "title": "Macdonald identities", "text": "Macdonald identities\n\nIn mathematics, the Macdonald identities are some infinite product identities associated to affine root systems, introduced by . They include as special cases the Jacobi triple product identity, Watson's quintuple product identity, several identities found by , and a 10-fold product identity found by .\n\n"}
{"id": "11269780", "url": "https://en.wikipedia.org/wiki?curid=11269780", "title": "Matrix determinant lemma", "text": "Matrix determinant lemma\n\nIn mathematics, in particular linear algebra, the matrix determinant lemma computes the determinant of the sum of an invertible matrix A and the dyadic product, uv, of a column vector u and a row vector v. \n\nSuppose A is an invertible square matrix and u, v are column vectors. Then the matrix determinant lemma states that\n\nHere, uv is the outer product of two vectors u and v.\n\nThe theorem can also be stated in terms of the adjugate matrix of A:\n\nin which case it applies whether or not the square matrix A is invertible.\n\nFirst the proof of the special case A = I follows from the equality:\n\nThe determinant of the left hand side is the product of the determinants of the three matrices. Since the first and third matrix are triangle matrices with unit diagonal, their determinants are just 1. The determinant of the middle matrix is our desired value. The determinant of the right hand side is simply (1 + vu). So we have the result:\n\nThen the general case can be found as:\n\nIf the determinant and inverse of A are already known, the formula provides a numerically cheap way to compute the determinant of A corrected by the matrix uv. The computation is relatively cheap because the determinant of A + uv does not have to be computed from scratch (which in general is expensive). Using unit vectors for u and/or v, individual columns, rows or elements of A may be manipulated and a correspondingly updated determinant computed relatively cheaply in this way. \n\nWhen the matrix determinant lemma is used in conjunction with the Sherman-Morrison formula, both the inverse and determinant may be conveniently updated together.\n\nSuppose A is an invertible \"n\"-by-\"n\" matrix and U, V are \"n\"-by-\"m\" matrices. Then\n\nIn the special case formula_7 this is Sylvester's theorem for determinants.\n\nGiven additionally an invertible \"m\"-by-\"m\" matrix W, the relationship can also be expressed as\n\n"}
{"id": "44403623", "url": "https://en.wikipedia.org/wiki?curid=44403623", "title": "Meshedness coefficient", "text": "Meshedness coefficient\n\nIn graph theory, the meshedness coefficient is a graph invariant of planar graphs that measures the number of bounded faces of the graph, as a fraction of the possible number of faces for other planar graphs with the same number of vertices. It ranges from 0 for trees to 1 for maximal planar graphs.\nThe meshedness coefficient is used to compare the general cycle structure of a connected planar graph to two extreme relevant references. In one end, there are trees, planar graphs with no cycle. The other extreme is represented by maximal planar graphs, planar graphs with the highest possible number of edges and faces for a given number of vertices. The normalized meshedness coefficient is the ratio of available face cycles to the maximum possible number of face cycles in the graph. This ratio is 0 for a tree and 1 for any maximal planar graph.\n\nMore generally, it can be shown using the Euler characteristic that all \"n\"-vertex planar graphs have at most 2\"n\" − 5 bounded faces (not counting the one unbounded face)\nand that if there are \"m\" edges then the number of bounded faces is \"m\" − \"n\" + 1 (the same as the circuit rank of the graph).\nTherefore, a normalized meshedness coefficient can be defined as the ratio of these two numbers:\nIt varies from 0 for trees to 1 for maximal planar graphs.\n\nThe meshedness coefficient can be used to estimate the redundancy of a network. This parameter along with the algebraic connectivity which measures the robustness of the network, may be used to quantify the topological aspect of network resilience in water distribution networks. It has also been used to characterize the network structure of streets in urban areas.\n\nUsing the definition of the average degree formula_2, one can see that in the limit of large graphs (number of edges the meshedness tends to\nThus, for large graphs, the meshedness does not carry more information than the average degree.\n"}
{"id": "35763931", "url": "https://en.wikipedia.org/wiki?curid=35763931", "title": "Multipartition", "text": "Multipartition\n\nIn number theory and combinatorics, a multipartition of a positive integer \"n\" is a way of writing \"n\" as a sum, each element of which is in turn a partition. The concept is also found in the theory of Lie algebras.\n\nAn \"r\"-component multipartition of an integer \"n\" is an \"r\"-tuple of partitions λ...,λ where each λ is a partition of some \"a\" and the \"a\" sum to \"n\". The number of \"r\"-component multipartitions of \"n\" is denoted \"P\"(\"n\"). Congruences for the function \"P\"(\"n\") have been studied by A. O. L. Atkin.\n\n"}
{"id": "618076", "url": "https://en.wikipedia.org/wiki?curid=618076", "title": "Opticks", "text": "Opticks\n\nOpticks: or, A Treatise of the Reflexions, Refractions, Inflexions and Colours of Light is a book by English natural philosopher Isaac Newton that was published in English in 1704. (A scholarly Latin translation appeared in 1706.) The book analyzes the fundamental nature of light by means of the refraction of light with prisms and lenses, the diffraction of light by closely spaced sheets of glass, and the behaviour of color mixtures with spectral lights or pigment powders. It is considered one of the great works of science in history. \"Opticks\" was Newton's second major book on physical science. Newton's name did not appear on the title page of the first edition of \"Opticks\".\n\nThe publication of \"Opticks\" represented a major contribution to science, different from but in some ways rivalling the \"Principia\". \"Opticks\" is largely a record of experiments and the deductions made from them, covering a wide range of topics in what was later to be known as physical optics. That is, this work is not a geometric discussion of catoptrics or dioptrics, the traditional subjects of reflection of light by mirrors of different shapes and the exploration of how light is \"bent\" as it passes from one medium, such as air, into another, such as water or glass. Rather, the \"Opticks\" is a study of the nature of light and colour and the various phenomena of diffraction, which Newton called the \"inflexion\" of light.\n\nIn this book Newton sets forth in full his experiments, first reported to the Royal Society of London in 1672, on dispersion, or the separation of light into a spectrum of its component colours. He demonstrates how the appearance of color arises from selective absorption, reflection, or transmission of the various component parts of the incident light.\n\nThe major significance of Newton's work is that it overturned the dogma, attributed to Aristotle or Theophrastus and accepted by scholars in Newton's time, that \"pure\" light (such as the light attributed to the Sun) is fundamentally white or colourless, and is altered into color by mixture with darkness caused by interactions with matter. Newton showed just the opposite was true: light is composed of different spectral hues (he describes seven — red, orange, yellow, green, blue, indigo and violet), and all colours, including white, are formed by various mixtures of these hues. He demonstrates that color arises from a physical property of light — each hue is refracted at a characteristic angle by a prism or lens — but he clearly states that color is a sensation within the mind and not an inherent property of material objects or of light itself. For example, he demonstrates that a red violet (magenta) color can be mixed by overlapping the red and violet ends of two spectra, although this color does not appear in the spectrum and therefore is not a \"color of light\". By connecting the red and violet ends of the spectrum, he organised all colours as a color circle that both quantitatively predicts color mixtures and qualitatively describes the perceived similarity among hues.\n\nNewton's visionary contribution to prismatic dispersion was the first to outlined multiple-prism arrays. Multiple-prism configurations, as beam expanders, became central to the design of the tunable laser more than 275 years later and set the stage for the development of the multiple-prism dispersion theory.\n\n\"Opticks\" differs in many respects from the \"Principia\". It was first published in English rather than in the Latin used by European philosophers, contributing to the development of a vernacular science literature. This marks a significant transition in the history of the English Language. With Britain's growing confidence and world influence, due at least in part to people like Newton, the English language was rapidly becoming the language of science and business. The book is a model of popular science exposition: although Newton's English is somewhat dated—he shows a fondness for lengthy sentences with much embedded qualifications—the book can still be easily understood by a modern reader. In contrast, few readers of Newton's time found the \"Principia\" accessible or even comprehensible. His formal but flexible style shows colloquialisms and metaphorical word choice.\n\nUnlike the \"Principia\", \"Opticks\" is not developed using the geometric convention of propositions proved by deduction from either previous propositions, lemmas or first principles (or axioms). Instead, axioms define the meaning of technical terms or fundamental properties of matter and light, and the stated propositions are demonstrated by means of specific, carefully described experiments. The first sentence of the book declares \"My Design in this Book is not to explain the Properties of Light by Hypotheses, but to propose and prove them by Reason and Experiments.\" In an \"Experimentum crucis\" or \"critical experiment\" (Book I, Part II, Theorem ii), Newton showed that the color of light corresponded to its \"degree of refrangibility\" (angle of refraction), and that this angle cannot be changed by additional reflection or refraction or by passing the light through a coloured filter.\n\nThe work is a \"vade mecum\" of the experimenter's art, displaying in many examples how to use observation to propose factual generalisations about the physical world and then exclude competing explanations by specific experimental tests. However, unlike the \"Principia\", which vowed \"Non fingo hypotheses\" or \"I make no hypotheses\" outside the deductive method, the \"Opticks\" develops conjectures about light that go beyond the experimental evidence: for example, that the physical behaviour of light was due its \"corpuscular\" nature as small particles, or that perceived colours were harmonically proportioned like the tones of a diatonic musical scale.\n\n\"Opticks\" concludes with a set of \"Queries.\" In the first edition, these were sixteen such Queries; that number was increased in the Latin edition, published in 1706, and then in the revised English edition, published in 1717/18. The first set of Queries were brief, but the later ones became short essays, filling many pages. In the fourth edition of 1730, there were 31 Queries, and it was the famous \"31st Query\" that, over the next two hundred years, stimulated a great deal of speculation and development on theories of chemical affinity.\n\nThese Queries, especially the later ones, deal with a wide range of physical phenomena, far transcending any narrow interpretation of the subject matter of \"optics.\" They concern the nature and transmission of heat; the possible cause of gravity; electrical phenomena; the nature of chemical action; the way in which God created matter in \"the Beginning;\" the proper way to do science; and even the ethical conduct of human beings. These Queries are not really questions in the ordinary sense. They are almost all posed in the negative, as rhetorical questions. That is, Newton does not ask whether light \"is\" or \"may be\" a \"body.\" Rather, he declares: \"Is not Light a Body?\" Not only does this form indicate that Newton had an answer, but that it may go on for many pages. Clearly, as Stephen Hales (a firm Newtonian of the early eighteenth century) declared, this was Newton's mode of explaining \"by Query.\"\n\nNewton suggests the idea of a multiverse in this passage:And since Space is divisible in infinitum, and Matter is not necessarily in all places, it may be also allow'd that God is able to create Particles of Matter of several Sizes and Figures, and in several Proportions to Space, and perhaps of different Densities and Forces, and thereby to vary the Laws of Nature, and make Worlds of several sorts in several Parts of the Universe. At least, I see nothing of Contradiction in all this.\n\nThe \"Opticks\" was widely read and debated in England and on the Continent. The early presentation of the work to the Royal Society stimulated a bitter dispute between Newton and Robert Hooke over the \"corpuscular\" or particle theory of light, which prompted Newton to postpone publication of the work until after Hooke's death in 1703. On the Continent, and in France in particular, both the \"Principia\" and the \"Opticks\" were initially rejected by many natural philosophers, who continued to defend Cartesian natural philosophy and the Aristotelian version of color, and claimed to find Newton's prism experiments difficult to replicate. Indeed, the Aristotelian theory of the fundamental nature of white light was defended into the 19th century, for example by the German writer Johann Wolfgang von Goethe in his \"Farbenlehre\".\n\nNewtonian science became a central issue in the assault waged by the philosophes in the Age of Enlightenment against a natural philosophy based on the authority of ancient Greek or Roman naturalists or on deductive reasoning from first principles (the method advocated by French philosopher René Descartes), rather than on the application of mathematical reasoning to experience or experiment. Voltaire popularised Newtonian science, including the content of both the \"Principia\" and the \"Opticks\", in his \"Elements de la philosophie de Newton\" (1738), and after about 1750 the combination of the experimental methods exemplified by the \"Opticks\" and the mathematical methods exemplified by the \"Principia\" were established as a unified and comprehensive model of Newtonian science. Some of the primary adepts in this new philosophy were such prominent figures as Benjamin Franklin, Antoine-Laurent Lavoisier, and James Black.\n\nSubsequent to Newton, much has been amended. Young and Fresnel combined Newton's particle theory with Huygens' wave theory to show that colour is the visible manifestation of light's wavelength. Science also slowly came to realise the difference between perception of colour and mathematisable optics. The German poet Goethe, with his epic diatribe \"Theory of Colours\", could not shake the Newtonian foundation - but \"one hole Goethe did find in Newton's armour.. Newton had committed himself to the doctrine that refraction without colour was impossible. He therefore thought that the object-glasses of telescopes must for ever remain imperfect, achromatism and refraction being incompatible. This inference was proved by Dollond to be wrong.\" (John Tyndall, 1880)\n\n\n\nFull and free online editions of Newton's \"Opticks\"\n"}
{"id": "23916899", "url": "https://en.wikipedia.org/wiki?curid=23916899", "title": "Order isomorphism", "text": "Order isomorphism\n\nIn the mathematical field of order theory an order isomorphism is a special kind of monotone function that constitutes a suitable notion of isomorphism for partially ordered sets (posets). Whenever two posets are order isomorphic, they can be considered to be \"essentially the same\" in the sense that one of the orders can be obtained from the other just by renaming of elements. Two strictly weaker notions that relate to order isomorphisms are order embeddings and Galois connections.\n\nFormally, given two posets formula_1 and formula_2, an order isomorphism from formula_1 to formula_2 is a bijective function formula_5 from formula_6 to formula_7 with the property that, for every formula_8 and formula_9 in formula_6, formula_11 if and only if formula_12. That is, it is a bijective order-embedding.\n\nIt is also possible to define an order isomorphism to be a surjective order-embedding. The two assumptions that formula_5 cover all the elements of formula_7 and that it preserve orderings, are enough to ensure that formula_5 is also one-to-one, for if formula_16 then (by the assumption that formula_5 preserves the order) it would follow that formula_18 and formula_19, implying by the definition of a partial order that formula_20.\n\nYet another characterization of order isomorphisms is that they are exactly the monotone bijections that have a monotone inverse.\n\nAn order isomorphism from a partially ordered set to itself is called an order automorphism.\n\nWhen an additional algebraic structure is imposed on the posets formula_1 and formula_2, a function from formula_1 to formula_2 must satisfy additional properties to be regarded as an isomorphism. For example, given two partially ordered groups (po-groups) formula_25 and formula_26, an isomorphism of po-groups from formula_27 to formula_28 is an order isomorphism that is also a group isomorphism, not merely a bijection that is an order embedding.\n\n\nIf formula_5 is an order isomorphism, then so is its inverse function.\nAlso, if formula_5 is an order isomorphism from formula_1 to formula_2 and formula_39 is an order isomorphism from formula_2 to formula_41, then the function composition of formula_5 and formula_39 is itself an order isomorphism, from formula_1 to formula_41.\n\nTwo partially ordered sets are said to be order isomorphic when there exists an order isomorphism from one to the other. Identity functions, function inverses, and compositions of functions correspond, respectively, to the three defining characteristics of an equivalence relation: reflexivity, symmetry, and transitivity. Therefore, order isomorphism is an equivalence relation. The class of partially ordered sets can be partitioned by it into equivalence classes, families of partially ordered sets that are all isomorphic to each other. These equivalence classes are called order types.\n\n\n"}
{"id": "23253", "url": "https://en.wikipedia.org/wiki?curid=23253", "title": "Parallax", "text": "Parallax\n\nParallax () is a displacement or difference in the apparent position of an object viewed along two different lines of sight, and is measured by the angle or semi-angle of inclination between those two lines. Due to foreshortening, nearby objects show a larger parallax than farther objects when observed from different positions, so parallax can be used to determine distances.\n\nTo measure large distances, such as the distance of a planet or a star from Earth, astronomers use the principle of parallax. Here, the term \"parallax\" is the semi-angle of inclination between two sight-lines to the star, as observed when Earth is on opposite sides of the Sun in its orbit. These distances form the lowest rung of what is called \"the cosmic distance ladder\", the first in a succession of methods by which astronomers determine the distances to celestial objects, serving as a basis for other distance measurements in astronomy forming the higher rungs of the ladder.\n\nParallax also affects optical instruments such as rifle scopes, binoculars, microscopes, and twin-lens reflex cameras that view objects from slightly different angles. Many animals, including humans, have two eyes with overlapping visual fields that use parallax to gain depth perception; this process is known as stereopsis. In computer vision the effect is used for computer stereo vision, and there is a device called a parallax rangefinder that uses it to find range, and in some variations also altitude to a target.\n\nA simple everyday example of parallax can be seen in the dashboard of motor vehicles that use a needle-style speedometer gauge. When viewed from directly in front, the speed may show exactly 60; but when viewed from the passenger seat the needle may appear to show a slightly different speed, due to the angle of viewing.\n\nAs the eyes of humans and other animals are in different positions on the head, they present different views simultaneously. This is the basis of stereopsis, the process by which the brain exploits the parallax due to the different views from the eye to gain depth perception and estimate distances to objects. Animals also use \"motion parallax\", in which the animals (or just the head) move to gain different viewpoints. For example, pigeons (whose eyes do not have overlapping fields of view and thus cannot use stereopsis) bob their heads up and down to see depth.\n\nThe motion parallax is exploited also in wiggle stereoscopy, computer graphics which provide depth cues through viewpoint-shifting animation rather than through binocular vision.\n\nParallax arises due to change in viewpoint occurring due to motion of the observer, of the observed, or of both. What is essential is relative motion. By observing parallax, measuring angles, and using geometry, one can determine distance. Astronomers also use the word \"parallax\" as a synonym for \"distance measurement\" by other methods: see parallax (disambiguation)#Astronomy.\n\nStellar parallax created by the relative motion between the Earth and a star can be seen, in the Copernican model, as arising from the orbit of the Earth around the Sun: the star only \"appears\" to move relative to more distant objects in the sky. In a geostatic model, the movement of the star would have to be taken as \"real\" with the star oscillating across the sky with respect to the background stars.\n\nStellar parallax is most often measured using annual parallax, defined as the difference in position of a star as seen from the Earth and Sun, i. e. the angle subtended at a star by the mean radius of the Earth's orbit around the Sun. The parsec (3.26 light-years) is defined as the distance for which the annual parallax is 1 arcsecond. Annual parallax is normally measured by observing the position of a star at different times of the year as the Earth moves through its orbit. Measurement of annual parallax was the first reliable way to determine the distances to the closest stars. The first successful measurements of stellar parallax were made by Friedrich Bessel in 1838 for the star 61 Cygni using a heliometer. Stellar parallax remains the standard for calibrating other measurement methods. Accurate calculations of distance based on stellar parallax require a measurement of the distance from the Earth to the Sun, now based on radar reflection off the surfaces of planets.\n\nThe angles involved in these calculations are very small and thus difficult to measure. The nearest star to the Sun (and thus the star with the largest parallax), Proxima Centauri, has a parallax of 0.7687 ± 0.0003 arcsec. This angle is approximately that subtended by an object 2 centimeters in diameter located 5.3 kilometers away.\n\nThe fact that stellar parallax was so small that it was unobservable at the time was used as the main scientific argument against heliocentrism during the early modern age. It is clear from Euclid's geometry that the effect would be undetectable if the stars were far enough away, but for various reasons such gigantic distances involved seemed entirely implausible: it was one of Tycho's principal objections to Copernican heliocentrism that in order for it to be compatible with the lack of observable stellar parallax, there would have to be an enormous and unlikely void between the orbit of Saturn (then the most distant known planet) and the eighth sphere (the fixed stars).\n\nIn 1989, the satellite Hipparcos was launched primarily for obtaining improved parallaxes and proper motions for over 100,000 nearby stars, increasing the reach of the method tenfold. Even so, Hipparcos is only able to measure parallax angles for stars up to about 1,600 light-years away, a little more than one percent of the diameter of the Milky Way Galaxy. The European Space Agency's Gaia mission, launched in December 2013, will be able to measure parallax angles to an accuracy of 10 microarcseconds, thus mapping nearby stars (and potentially planets) up to a distance of tens of thousands of light-years from Earth. In April 2014, NASA astronomers reported that the Hubble Space Telescope, by using spatial scanning, can now precisely measure distances up to 10,000 light-years away, a ten-fold improvement over earlier measurements.\n\nDistance measurement by parallax is a special case of the principle of triangulation, which states that one can solve for all the sides and angles in a network of triangles if, in addition to all the angles in the network, the length of at least one side has been measured. Thus, the careful measurement of the length of one baseline can fix the scale of an entire triangulation network. In parallax, the triangle is extremely long and narrow, and by measuring both its shortest side (the motion of the observer) and the small top angle (always less than 1 arcsecond, leaving the other two close to 90 degrees), the length of the long sides (in practice considered to be equal) can be determined.\n\nAssuming the angle is small (see derivation below), the distance to an object (measured in parsecs) is the reciprocal of the parallax (measured in arcseconds): formula_1 For example, the distance to Proxima Centauri is 1/0.7687=.\n\n\"Diurnal parallax\" is a parallax that varies with rotation of the Earth or with difference of location on the Earth. The Moon and to a smaller extent the terrestrial planets or asteroids seen from different viewing positions on the Earth (at one given moment) can appear differently placed against the background of fixed stars.\n\n\"Lunar parallax\" (often short for \"lunar horizontal parallax\" or \"lunar equatorial horizontal parallax\"), is a special case of (diurnal) parallax: the Moon, being the nearest celestial body, has by far the largest maximum parallax of any celestial body, it can exceed 1 degree.\n\nThe diagram (above) for stellar parallax can illustrate lunar parallax as well, if the diagram is taken to be scaled right down and slightly modified. Instead of 'near star', read 'Moon', and instead of taking the circle at the bottom of the diagram to represent the size of the Earth's orbit around the Sun, take it to be the size of the Earth's globe, and of a circle around the Earth's surface. Then, the lunar (horizontal) parallax amounts to the difference in angular position, relative to the background of distant stars, of the Moon as seen from two different viewing positions on the Earth: one of the viewing positions is the place from which the Moon can be seen directly overhead at a given moment (that is, viewed along the vertical line in the diagram); and the other viewing position is a place from which the Moon can be seen on the horizon at the same moment (that is, viewed along one of the diagonal lines, from an Earth-surface position corresponding roughly to one of the blue dots on the modified diagram).\n\nThe lunar (horizontal) parallax can alternatively be defined as the angle subtended at the distance of the Moon by the radius of the Earth—equal to angle p in the diagram when scaled-down and modified as mentioned above.\n\nThe lunar horizontal parallax at any time depends on the linear distance of the Moon from the Earth. The Earth–Moon linear distance varies continuously as the Moon follows its perturbed and approximately elliptical orbit around the Earth. The range of the variation in linear distance is from about 56 to 63.7 Earth radii, corresponding to horizontal parallax of about a degree of arc, but ranging from about 61.4' to about 54'. The Astronomical Almanac and similar publications tabulate the lunar horizontal parallax and/or the linear distance of the Moon from the Earth on a periodical e.g. daily basis for the convenience of astronomers (and formerly, of navigators), and the study of the way in which this coordinate varies with time forms part of lunar theory.\n\nParallax can also be used to determine the distance to the Moon.\n\nOne way to determine the lunar parallax from one location is by using a lunar eclipse. A full shadow of the Earth on the Moon has an apparent radius of curvature equal to the difference between the apparent radii of the Earth and the Sun as seen from the Moon. This radius can be seen to be equal to 0.75 degree, from which (with the solar apparent radius 0.25 degree) we get an Earth apparent radius of 1 degree. This yields for the Earth–Moon distance 60.27 Earth radii or This procedure was first used by Aristarchus of Samos and Hipparchus, and later found its way into the work of Ptolemy. The diagram at the right shows how daily lunar parallax arises on the geocentric and geostatic planetary model in which the Earth is at the centre of the planetary system and does not rotate. It also illustrates the important point that parallax need not be caused by any motion of the observer, contrary to some definitions of parallax that say it is, but may arise purely from motion of the observed.\n\nAnother method is to take two pictures of the Moon at exactly the same time from two locations on Earth and compare the positions of the Moon relative to the stars. Using the orientation of the Earth, those two position measurements, and the distance between the two locations on the Earth, the distance to the Moon can be triangulated:\n\nThis is the method referred to by Jules Verne in \"From the Earth to the Moon\": Until then, many people had no idea how one could calculate the distance separating the Moon from the Earth. The circumstance was exploited to teach them that this distance was obtained by measuring the parallax of the Moon. If the word parallax appeared to amaze them, they were told that it was the angle subtended by two straight lines running from both ends of the Earth's radius to the Moon. If they had doubts on the perfection of this method, they were immediately shown that not only did this mean distance amount to a whole two hundred thirty-four thousand three hundred and forty-seven miles (94,330 leagues), but also that the astronomers were not in error by more than seventy miles (≈ 30 leagues).\n\nAfter Copernicus proposed his heliocentric system, with the Earth in revolution around the Sun, it was possible to build a model of the whole Solar System without scale. To ascertain the scale, it is necessary only to measure one distance within the Solar System, e.g., the mean distance from the Earth to the Sun (now called an astronomical unit, or AU). When found by triangulation, this is referred to as the \"solar parallax\", the difference in position of the Sun as seen from the Earth's centre and a point one Earth radius away, i. e., the angle subtended at the Sun by the Earth's mean radius. Knowing the solar parallax and the mean Earth radius allows one to calculate the AU, the first, small step on the long road of establishing the size and expansion age of the visible Universe.\n\nA primitive way to determine the distance to the Sun in terms of the distance to the Moon was already proposed by Aristarchus of Samos in his book \"On the Sizes and Distances of the Sun and Moon\". He noted that the Sun, Moon, and Earth form a right triangle (with the right angle at the Moon) at the moment of first or last quarter moon. He then estimated that the Moon, Earth, Sun angle was 87°. Using correct geometry but inaccurate observational data, Aristarchus concluded that the Sun was slightly less than 20 times farther away than the Moon. The true value of this angle is close to 89° 50', and the Sun is actually about 390 times farther away. He pointed out that the Moon and Sun have nearly equal apparent angular sizes and therefore their diameters must be in proportion to their distances from Earth. He thus concluded that the Sun was around 20 times larger than the Moon; this conclusion, although incorrect, follows logically from his incorrect data. It does suggest that the Sun is clearly larger than the Earth, which could be taken to support the heliocentric model.\n\nAlthough Aristarchus' results were incorrect due to observational errors, they were based on correct geometric principles of parallax, and became the basis for estimates of the size of the Solar System for almost 2000 years, until the transit of Venus was correctly observed in 1761 and 1769. This method was proposed by Edmond Halley in 1716, although he did not live to see the results. The use of Venus transits was less successful than had been hoped due to the black drop effect, but the resulting estimate, 153 million kilometers, is just 2% above the currently accepted value, 149.6 million kilometers.\n\nMuch later, the Solar System was \"scaled\" using the parallax of asteroids, some of which, such as Eros, pass much closer to Earth than Venus. In a favourable opposition, Eros can approach the Earth to within 22 million kilometres. Both the opposition of 1901 and that of 1930/1931 were used for this purpose, the calculations of the latter determination being completed by Astronomer Royal Sir Harold Spencer Jones.\n\nAlso radar reflections, both off Venus (1958) and off asteroids, like Icarus, have been used for solar parallax determination. Today, use of spacecraft telemetry links has solved this old problem. The currently accepted value of solar parallax is 8\".794 143.\n\nThe open stellar cluster Hyades in Taurus extends over such a large part of the sky, 20 degrees, that the proper motions as derived from astrometry appear to converge with some precision to a perspective point north of Orion. Combining the observed apparent (angular) proper motion in seconds of arc with the also observed true (absolute) receding motion as witnessed by the Doppler redshift of the stellar spectral lines, allows estimation of the distance to the cluster (151 light-years) and its member stars in much the same way as using annual parallax.\n\nDynamical parallax has sometimes also been used to determine the distance to a supernova, when the optical wave front of the outburst is seen to propagate through the surrounding dust clouds at an apparent angular velocity, while its true propagation velocity is known to be the speed of light.\n\nFor a right triangle,\nwhere formula_4 is the parallax, is approximately the average distance from the Sun to Earth, and formula_5 is the distance to the star.\nUsing small-angle approximations (valid when the angle is small compared to 1 radian),\nso the parallax, measured in arcseconds, is\nIf the parallax is 1\", then the distance is\nThis \"defines\" the parsec, a convenient unit for measuring distance using parallax. Therefore, the distance, measured in parsecs, is simply formula_9, when the parallax is given in arcseconds.\n\nPrecise parallax measurements of distance have an associated error. However this error in the measured parallax angle does not translate directly into an error for the distance, except for relatively small errors. The reason for this is that an error toward a smaller angle results in a greater error in distance than an error toward a larger angle.\n\nHowever, an approximation of the distance error can be computed by\nwhere \"d\" is the distance and \"p\" is the parallax. The approximation is far more accurate for parallax errors that are small relative to the parallax than for relatively large errors. For meaningful results in stellar astronomy, Dutch astronomer Floor van Leeuwen recommends that the parallax error be no more than 10% of the total parallax when computing this error estimate.\n\nFrom enhanced \"relativistic positioning systems\", spatio-temporal parallax generalizing the usual notion of parallax in space only has been developed. Then, eventfields in spacetime can be deduced directly without intermediate models of light bending by massive bodies such as the one used in the PPN formalism for instance.\n\nMeasurements made by viewing the position of some marker relative to something to be measured are subject to parallax error if the marker is some distance away from the object of measurement and not viewed from the correct position. For example, if measuring the distance between two ticks on a line with a ruler marked on its top surface, the thickness of the ruler will separate its markings from the ticks. If viewed from a position not exactly perpendicular to the ruler, the apparent position will shift and the reading will be less accurate than the ruler is capable of.\n\nA similar error occurs when reading the position of a pointer against a scale in an instrument such as an analog multimeter. To help the user avoid this problem, the scale is sometimes printed above a narrow strip of mirror, and the user's eye is positioned so that the pointer obscures its own reflection, guaranteeing that the user's line of sight is perpendicular to the mirror and therefore to the scale. The same effect alters the speed read on a car's speedometer by a driver in front of it and a passenger off to the side, values read from a graticule not in actual contact with the display on an oscilloscope, etc.\n\nAerial picture pairs, when viewed through a stereo viewer, offer a pronounced stereo effect of landscape and buildings. High buildings appear to 'keel over' in the direction away from the centre of the photograph. Measurements of this parallax are used to deduce the height of the buildings, provided that flying height and baseline distances are known. This is a key component to the process of photogrammetry.\n\nParallax error can be seen when taking photos with many types of cameras, such as twin-lens reflex cameras and those including viewfinders (such as rangefinder cameras). In such cameras, the eye sees the subject through different optics (the viewfinder, or a second lens) than the one through which the photo is taken. As the viewfinder is often found above the lens of the camera, photos with parallax error are often slightly lower than intended, the classic example being the image of person with his or her head cropped off. This problem is addressed in single-lens reflex cameras, in which the viewfinder sees through the same lens through which the photo is taken (with the aid of a movable mirror), thus avoiding parallax error.\n\nParallax is also an issue in image stitching, such as for panoramas.\n\nParallax affects sighting devices of ranged weapons in many ways. On sights fitted on small arms and bows, etc. the perpendicular distance between the sight and the weapon's launch axis (e.g. the bore axis of a gun) — generally referred to as \"sight height\" — can induce significant aiming errors when shooting at close range, particularly when shooting at small targets. This parallax error is compensated for (when needed) via calculations that also take in other variables such as bullet drop, windage, and the distance at which the target is expected to be. Sight height can be used to advantage when \"sighting-in\" rifles for field use. A typical hunting rifle (.222 with telescopic sights) sighted-in at 75m will still be useful from 50m to 200m without needing further adjustment.\n\nIn some reticled optical instruments such as telescopes, microscopes or in telescopic sights (\"scopes\") used on small arms and theodolites, parallax can create problems with aiming when the reticle is not coincident with the focal plane of the target image. This is because when the reticle and the target are not at the same focus, the optically corresponded distances being projected through the eyepiece are also different, and the user's eye will register that parallax difference between the reticle and the target whenever eye position changes. The term \"parallax shift\" refers to the resultant apparent \"floating\" movements of the reticle in relationship to the target when the user moves his/her head laterally (up/down or left/right) behind the sight, i.e. it is an error where the reticle does not stay aligned with the sight's own optical axis.\n\nSome firearm scopes are equipped with a parallax compensation mechanism, which basically consists of a movable optical element that enables the optical system to shift the focus of the target image at varying distances into the exact same optical plane of the reticle. Many low-tier telescopic sights may have no parallax compensation because in practice they can still perform very acceptably without eliminating parallax shift, in which case the scope is often set fixed at a designated parallax-free distance that best suits their intended usage. Typical standard factory parallax-free distances for hunting scopes are 100 yd or 100 m to make them suited for hunting shots that rarely exceed 300 yd/m. Some target and military style scopes without parallax compensation may be adjusted to be parallax free at ranges up to 300 yd/m to make them better suited for aiming at longer ranges. Scopes for rimfires, shotguns, and muzzleloaders will have shorter parallax settings, commonly 50 yd/m for rimfire scopes and 100 yd/m for shotguns and muzzleloaders. Scopes for airguns are very often found with adjustable parallax, usually in the form of an adjustable objective (or \"AO\" for short)design. These may adjust down as far as .\n\nNon-magnifying reflector or \"reflex\" sights have the ability to be theoretically \"parallax free.\" But since these sights use parallel collimated light this is only true when the target is at infinity. At finite distances eye movement perpendicular to the device will cause parallax movement in the reticle image in exact relationship to eye position in the cylindrical column of light created by the collimating optics. Firearm sights, such as some red dot sights, try to correct for this via not focusing the reticle at infinity, but instead at some finite distance, a designed target range where the reticle will show very little movement due to parallax. Some manufactures market reflector sight models they call \"parallax free,\" but this refers to an optical system that compensates for off axis spherical aberration, an optical error induced by the spherical mirror used in the sight that can cause the reticle position to diverge off the sight's optical axis with change in eye position.\n\nBecause of the positioning of field or naval artillery guns, each one has a slightly different perspective of the target relative to the location of the fire-control system itself. Therefore, when aiming its guns at the target, the fire control system must compensate for parallax in order to assure that fire from each gun converges on the target.\n\nA coincidence rangefinder or parallax rangefinder can be used to find distance to a target.\n\nIn a philosophic/geometric sense: an apparent change in the direction of an object, caused by a change in observational position that provides a new line of sight. The apparent displacement, or difference of position, of an object, as seen from two different stations, or points of view. In contemporary writing parallax can also be the same story, or a similar story from approximately the same time line, from one book told from a different perspective in another book. The word and concept feature prominently in James Joyce's 1922 novel, \"Ulysses\". Orson Scott Card also used the term when referring to Ender's Shadow as compared to Ender's Game.\n\nThe metaphor is invoked by Slovenian philosopher Slavoj Žižek in his work \"The Parallax View\", borrowing the concept of \"parallax view\" from the Japanese philosopher and literary critic Kojin Karatani. Žižek notes, \n\n\n"}
{"id": "4824664", "url": "https://en.wikipedia.org/wiki?curid=4824664", "title": "Patrick Suppes", "text": "Patrick Suppes\n\nPatrick Colonel Suppes (; March 17, 1922 – November 17, 2014) was an American philosopher who made significant contributions to philosophy of science, the theory of measurement, the foundations of quantum mechanics, decision theory, psychology and educational technology. He was the Lucie Stern Professor of Philosophy Emeritus at Stanford University and until January 2010 was the Director of the Education Program for Gifted Youth also at Stanford.\n\nSuppes was born on March 17, 1922, in Tulsa, Oklahoma. He grew up as an only child, later with a half brother George who was born in 1943 after Patrick had entered the army. His grandfather, C.E. Suppes, had moved to Oklahoma from Ohio. Suppes' father and grandfather were independent oil men. His mother died when he was a young boy. He was raised by his stepmother, who married his father before he was six years old. His parents did not have much formal education.\n\nSuppes began college at the University of Oklahoma in 1939, but transferred to the University of Chicago in his second year, citing boredom with intellectual life in Oklahoma as his primary motivation. In his third year, at the insistence of his family, Suppes attended the University of Tulsa, majoring in physics, before entering the Army Reserves in 1942. In 1943 he returned to the University of Chicago and graduated with a B.S. in meteorology, and was stationed shortly thereafter at the Solomon Islands to serve during World War II.\n\nSuppes was discharged from the Army Air Force in 1946. In January 1947 he entered Columbia University as a graduate student in philosophy as a student of Ernest Nagel and received a PhD in 1950. In 1952 he went to Stanford University, and from 1959 to 1992 he was the director of the Institute for Mathematical Studies in the Social Sciences (IMSSS). He would subsequently become the Lucie Stern Professor of Philosophy, Emeritus, at Stanford.\n\nIn the 1960s Suppes and Richard C. Atkinson (the future president of the University of California) conducted experiments in using computers to teach math and reading to school children in the Palo Alto area. Stanford's Education Program for Gifted Youth and Computer Curriculum Corporation (CCC, now named Pearson Education Technologies) are indirect descendants of those early experiments. At Stanford, Suppes was instrumental in encouraging the development of high-technology companies that were springing up in the field of educational software up into the 1990s, (such as Bien Logic).\n\nOne computer used in Suppes and Atkinson's Computer-assisted Instruction (CAI) experiments was the specialized IBM 1500 Instructional System. Seeded by a research grant in 1964 from the U.S. Department of Education to the Institute for Mathematical Studies in the Social Sciences at Stanford University, the IBM 1500 CAI system was initially prototyped at the Brentwood Elementary School (Ravenswood City School District) in East Palo Alto, California by Suppes. The students first used the system in 1966.\n\nSuppes' Dial-a-Drill program was a touchtone phone interface for CAI. Ten schools around Manhattan were involved in the program which delivered three lessons per week by telephone. Dial-a-Drill adjusted the routine for students who answered two questions incorrectly. The system went online in March 1969. Touchtone telephones were installed in the homes of children participating in the program. Field workers educated parents on the benefits of the program and collected feedback.\n\nDuring the 1950s and 1960s Suppes collaborated with Donald Davidson on decision theory, at Stanford. Their initial work followed lines of thinking which had been anticipated in 1926 by Frank P. Ramsey, and involved experimental testing of their theories, culminating in the 1957 monograph \"\". Such commentators as Kirk Ludwig trace the origins of Davidson's theory of radical interpretation to his formative work with Suppes.\n\n\n\n\n"}
{"id": "26611936", "url": "https://en.wikipedia.org/wiki?curid=26611936", "title": "Penrose tiling", "text": "Penrose tiling\n\nA Penrose tiling is an example of non-periodic tiling generated by an aperiodic set of prototiles. Penrose tilings are named after mathematician and physicist Sir Roger Penrose, who investigated these sets in the 1970s. The aperiodicity of prototiles implies that a shifted copy of a tiling will never match the original. A Penrose tiling may be constructed so as to exhibit both reflection symmetry and fivefold rotational symmetry, as in the diagram at the right.\n\nPenrose tiling is non-periodic, which means that it lacks any translational symmetry. It is self-similar, so the same patterns occur at larger and larger scales. Thus, the tiling can be obtained through \"inflation\" (or \"deflation\") and every finite patch from the tiling occurs infinitely many times. It is a quasicrystal: implemented as a physical structure a Penrose tiling will produce Bragg diffraction and its diffractogram reveals both the fivefold symmetry and the underlying long range order.\n\nVarious methods to construct Penrose tilings have been discovered, including matching rules, substitutions or subdivision rules, cut and project schemes and coverings.\n\nPenrose tilings are simple examples of aperiodic tilings of the plane. A tiling is a covering of the plane by tiles with no overlaps or gaps; the tiles normally have a finite number of shapes, called \"prototiles\", and a set of prototiles is said to \"admit a tiling\" or \"tile the plane\" if there is a tiling of the plane using only tiles congruent to these prototiles. The most familiar tilings (e.g., by squares or triangles) are periodic: a perfect copy of the tiling can be obtained by translating all of the tiles by a fixed distance in a given direction. Such a translation is called a \"period\" of the tiling; more informally, this means that a finite region of the tiling repeats itself in periodic intervals. If a tiling has no periods it is said to be \"non-periodic\". A set of prototiles is said to be \"aperiodic\" if it tiles the plane but every such tiling is non-periodic; tilings by aperiodic sets of prototiles are called aperiodic tilings.\n\nThe subject of aperiodic tilings received new interest in the 1960s when logician Hao Wang noted connections between decision problems and tilings. In particular, he introduced tilings by square plates with colored edges, now known as \"Wang dominoes\" or \"tiles\", and posed the \"Domino Problem\": to determine whether a given set of Wang dominoes could tile the plane with matching colors on adjacent domino edges. He observed that if this problem were undecidable, then there would have to exist an aperiodic set of Wang dominoes. At the time, this seemed implausible, so Wang conjectured no such set could exist.\nWang's student Robert Berger proved that the Domino Problem was undecidable (so Wang's conjecture was incorrect) in his 1964 thesis, and obtained an aperiodic set of 20426 Wang dominoes. He also described a reduction to 104 such prototiles; the latter did not appear in his published monograph, but in 1968, Donald Knuth detailed a modification of Berger's set requiring only 92 dominoes.\n\nThe color matching required in a tiling by Wang dominoes can easily be achieved by modifying the edges of the tiles like jigsaw puzzle pieces so that they can fit together only as prescribed by the edge colorings. Raphael Robinson, in a 1971 paper which simplified Berger's techniques and undecidability proof, used this technique to obtain an aperiodic set of just six prototiles.\n\nThe first Penrose tiling (tiling P1 below) is an aperiodic set of six prototiles, introduced by Roger Penrose in a 1974 paper, but it is based on pentagons rather than squares. Any attempt to tile the plane with regular pentagons necessarily leaves gaps, but Johannes Kepler showed, in his 1619 work \"Harmonices Mundi\", that these gaps can be filled using pentagrams (star polygons), decagons and related shapes. Traces of these ideas can also be found in the work of Albrecht Dürer. Acknowledging inspiration from Kepler, Penrose found matching rules (which can be imposed by decorations of the edges) for these shapes, obtaining an aperiodic set. His tiling can be viewed as a completion of Kepler's finite \"Aa\" pattern.\n\nPenrose subsequently reduced the number of prototiles to two, discovering the kite and dart tiling (tiling P2 below) and the rhombus tiling (tiling P3 below). The rhombus tiling was independently discovered by Robert Ammann in 1976. Penrose and John H. Conway investigated the properties of Penrose tilings, and discovered that a substitution property explained their hierarchical nature; their findings were publicized by Martin Gardner in his January 1977 \"Mathematical Games column\" in \"Scientific American\".\n\nIn 1981, De Bruijn explained a method to construct Penrose tilings from five families of parallel lines as well as a \"cut and project method\", in which Penrose tilings are obtained as two-dimensional projections from a five-dimensional cubic structure. In this approach, the Penrose tiling is viewed as a set of points, its vertices, while the tiles are geometrical shapes obtained by connecting vertices with edges.\n\nThe three types of Penrose tiling, P1–P3, are described individually below. They have many common features: in each case, the tiles are constructed from shapes related to the pentagon (and hence to the golden ratio), but the basic tile shapes need to be supplemented by \"matching rules\" in order to tile aperiodically; these rules may be described using labeled vertices or edges, or patterns on the tile faces – alternatively the edge profile can be modified (e.g. by indentations and protrusions) to obtain an aperiodic set of prototiles.\n\nPenrose's first tiling uses pentagons and three other shapes: a five-pointed \"star\" (a pentagram), a \"boat\" (roughly 3/5 of a star) and a \"diamond\" (a thin rhombus). To ensure that all tilings are non-periodic, there are matching rules that specify how tiles may meet each other, and there are three different types of matching rule for the pentagonal tiles. It is common to indicate the three different types of pentagonal tiles using three different colors, as in the figure above right.\n\nPenrose's second tiling uses quadrilaterals called the \"kite\" and \"dart\", which may be combined to make a rhombus. However, the matching rules prohibit such a combination. Both the kite and dart are composed of two triangles, called \"Robinson triangles\", after 1975 notes by Robinson.\n\n\nThe matching rules can be described in several ways. One approach is to color the vertices (with two colors, e.g., black and white) and require that adjacent tiles have matching vertices. Another is to use a pattern of circular arcs (as shown above left in green and red) to constrain the placement of tiles: when two tiles share an edge in a tiling, the patterns must match at these edges.\n\nThese rules often force the placement of certain tiles: for example, the concave vertex of any dart is necessarily filled by two kites. The corresponding figure (center of the top row in the lower image on the left) is called an \"ace\" by Conway; although it looks like an enlarged kite, it does not tile in the same way. Similarly the concave vertex formed when two kites meet along a short edge is necessarily filled by two darts (bottom right). In fact, there are only seven possible ways for the tiles to meet at a vertex; two of these figures – namely, the \"star\" (top left) and the \"sun\" (top right) – have 5-fold dihedral symmetry (by rotations and reflections), while the remainder have a single axis of reflection (vertical in the image). Apart from the ace and the sun, all of these vertex figures force the placement of additional tiles.\n\nThe third tiling uses a pair of rhombuses (often referred to as \"rhombs\" in this context) with equal sides but different angles. Ordinary rhombus-shaped tiles can be used to tile the plane periodically, so restrictions must be made on how tiles can be assembled: no two tiles may form a parallelogram, as this would allow a periodic tiling, but this constraint is not sufficient to force aperiodicity, as figure 1 above shows.\n\nThere are two kinds of tile, both of which can be decomposed into Robinson triangles.\n\nThe matching rules distinguish sides of the tiles, and entail that tiles may be juxtaposed in certain particular ways but not in others. Two ways to describe these matching rules are shown in the image on the right. In one form, tiles must be assembled such that the curves on the faces match in color and position across an edge. In the other, tiles must be assembled such that the bumps on their edges fit together.\n\nThere are 54 cyclically ordered combinations of such angles that add up to 360 degrees at a vertex, but the rules of the tiling allow only seven of these combinations to appear (although one of these arises in two ways).\n\nThe various combinations of angles and facial curvature allow construction of arbitrarily complex tiles, such as the \"Penrose chickens\".\n\nSeveral properties and common features of the Penrose tilings involve the golden ratio = (1+)/2 (approximately 1.618). This is the ratio of chord lengths to side lengths in a regular pentagon, and satisfies = 1 + 1/.\n\nConsequently, the ratio of the lengths of long sides to short sides in the (isosceles) Robinson triangles is :1. It follows that the ratio of long side lengths to short in both kite and dart tiles is also :1, as are the length ratios of sides to the short diagonal in the thin rhomb t, and of long diagonal to sides in the thick rhomb T. In both the P2 and P3 tilings, the ratio of the area of the larger Robinson triangle to the smaller one is :1, hence so are the ratios of the areas of the kite to the dart, and of the thick rhomb to the thin rhomb. (Both larger and smaller obtuse Robinson triangles can be found in the pentagon on the left: the larger triangles at the top – the halves of the thick rhomb – have linear dimensions scaled up by compared to the small shaded triangle at the base, and so the ratio of areas is :1.)\n\nAny Penrose tiling has local pentagonal symmetry, in the sense that there are points in the tiling surrounded by a symmetric configuration of tiles: such configurations have fivefold rotational symmetry about the center point, as well as five mirror lines of reflection symmetry passing through the point, a dihedral symmetry group. This symmetry will generally preserve only a patch of tiles around the center point, but the patch can be very large: Conway and Penrose proved that whenever the colored curves on the P2 or P3 tilings close in a loop, the region within the loop has pentagonal symmetry, and furthermore, in any tiling, there are at most two such curves of each color that do not close up.\n\nThere can be at most one center point of global fivefold symmetry: if there were more than one, then rotating each about the other would yield two closer centers of fivefold symmetry, which leads to a mathematical contradiction. There are only two Penrose tilings (of each type) with global pentagonal symmetry: for the P2 tiling by kites and darts, the center point is either a \"sun\" or \"star\" vertex.\n\nMany of the common features of Penrose tilings follow from a hierarchical pentagonal structure given by \"substitution rules\": this is often referred to as \"inflation\" and \"deflation\", or \"composition\" and \"decomposition\", of tilings or (collections of) tiles. The substitution rules decompose each tile into smaller tiles of the same shape as those used in the tiling (and thus allow larger tiles to be \"composed\" from smaller ones). This shows that the Penrose tiling has a scaling self-similarity, and so can be thought of as a fractal.\n\nPenrose originally discovered the P1 tiling in this way, by decomposing a pentagon into six smaller pentagons (one half of a net of a dodecahedron) and five half-diamonds; he then observed that when he repeated this process the gaps between pentagons could all be filled by stars, diamonds, boats and other pentagons. By iterating this process indefinitely he obtained one of the two P1 tilings with pentagonal symmetry.\n\nThe substitution method for both P2 and P3 tilings can be described using Robinson triangles of different sizes. The Robinson triangles arising in P2 tilings (by bisecting kites and darts) are called A-tiles, while those arising in the P3 tilings (by bisecting rhombs) are called B-tiles. The smaller A-tile, denoted A, is an obtuse Robinson triangle, while the larger A-tile, A, is acute; in contrast, a smaller B-tile, denoted B, is an acute Robinson triangle, while the larger B-tile, B, is obtuse.\n\nConcretely, if A has side lengths (1, 1, ), then A has side lengths (, , 1). B-tiles can be related to such A-tiles in two ways:\n\nIn these decompositions, there appears to be an ambiguity: Robinson triangles may be decomposed in two ways, which are mirror images of each other in the (isosceles) axis of symmetry of the triangle. In a Penrose tiling, this choice is fixed by the matching rules – furthermore, the matching rules \"also\" determine how the smaller triangles in the tiling compose to give larger ones.\nIt follows that the P2 and P3 tilings are \"mutually locally derivable\": a tiling by one set of tiles can be used to generate a tiling by another – for example a tiling by kites and darts may be subdivided into A-tiles, and these can be composed in a canonical way to form B-tiles and hence rhombs. The P2 and P3 tilings are also both mutually locally derivable with the P1 tiling (see figure 2 above).\n\nThe decomposition of B-tiles into A-tiles may be written\n(assuming the larger size convention for the B-tiles), which can be summarized in a \"substitution matrix\" equation:\nCombining this with the decomposition of enlarged A-tiles into B-tiles yields the substitution\nso that the enlarged tile A decomposes into two A tiles and one A tiles. The matching rules force a particular substitution: the two A tiles in a A tile must form a kite – thus a kite decomposes into two kites and a two half-darts, and a dart decomposes into a kite and two half-darts. Enlarged B-tiles decompose into B-tiles in a similar way (via A-tiles).\n\nComposition and decomposition can be iterated, so that, for example\nThe number of kites and darts in the \"n\"th iteration of the construction is determined by the \"n\"th power of the substitution matrix:\nwhere \"F\" is the \"n\"th Fibonacci number. The ratio of numbers of kites to darts in any sufficiently large P2 Penrose tiling pattern therefore approximates to the golden ratio . A similar result holds for the ratio of the number of thick rhombs to thin rhombs in the P3 Penrose tiling.\n\nStarting with a collection of tiles from a given tiling (which might be a single tile, a tiling of the plane, or any other collection), deflation proceeds with a sequence of steps called generations. In one generation of deflation, each tile is replaced with two or more new tiles that are scaled-down versions of tiles used in the original tiling. The substitution rules guarantee that the new tiles will be arranged in accordance with the matching rules. Repeated generations of deflation produce a tiling of the original axiom shape with smaller and smaller tiles. \nThis rule for dividing the tiles is a subdivision rule.\n\nInflation and deflation yield a method for constructing kite and dart (P2) tilings, or rhombus (P3) tilings, known as \"up-down generation\".\n\nThe Penrose tilings, being non-periodic, have no translational symmetry – the pattern cannot be shifted to match itself over the entire plane. However, any bounded region, no matter how large, will be repeated an infinite number of times within the tiling. Therefore, a finite patch cannot differentiate between the uncountably many Penrose tilings, nor even determine which position within the tiling is being shown.\n\nThis shows in particular that the number of distinct Penrose tilings (of any type) is uncountably infinite. Up-down generation yields one method to parameterize the tilings, but other methods use Ammann bars, pentagrids, or cut and project schemes.\n\nIn 1996, German mathematician Petra Gummelt demonstrated that a covering (so called to distinguish it from a non-overlapping tiling) equivalent to the Penrose tiling can be constructed using a single decagonal tile if two kinds of overlapping regions are allowed. The decagonal tile is decorated with colored patches, and the covering rule allows only those overlaps compatible with the coloring. A suitable decomposition of the decagonal tile into kites and darts transforms such a covering into a Penrose (P2) tiling. Similarly, a P3 tiling can be obtained by inscribing a thick rhomb into each decagon; the remaining space is filled by thin rhombs.\n\nThese coverings have been considered as a realistic model for the growth of quasicrystals: the overlapping decagons are 'quasi-unit cells' analogous to the unit cells from which crystals are constructed, and the matching rules maximize the density of certain atomic clusters.\n\nThe three variants of the Penrose tiling are mutually locally derivable. Selecting some subsets from the vertices of a P1 tiling allows to produce other non-periodic tilings. If the corners of one pentagon in P1 are labeled in succession by \"1,3,5,2,4\" an unambiguous tagging in all the pentagons is established, the order being either clockwise or counterclockwise.\nPoints with the same label define a tiling by Robinson triangles while points with the numbers 3 and 4 on them define the vertices of a Tie-and-Navette tiling.\n\nThere are also other related unequivalent tilings, such as the hexagon-boat-star and Mikulla–Roth tilings. For instance, if the matching rules for the rhombus tiling are reduced to a specific restriction on the angles permitted at each vertex, a binary tiling is obtained. Its underlying symmetry is also fivefold but it is not a quasicrystal. It can be obtained either by decorating the rhombs of the original tiling with smaller ones, or by applying substitution rules, but not by de Bruijn's cut-and-project method.\n\nThe aesthetic value of tilings has long been appreciated, and remains a source of interest in them; here the visual appearance (rather than the formal defining properties) of Penrose tilings has attracted attention. The similarity with some decorative patterns used in the Middle East has been noted; the physicists Peter J. Lu and Paul Steinhardt have presented evidence that a Penrose tiling underlies some examples of medieval Islamic geometric patterns, such as the girih (strapwork) tilings at the Darb-e Imam shrine in Isfahan.\n\nDrop City artist Clark Richert used Penrose rhombs in artwork in 1970 - derived by projecting the rhombic triacontahedron shadow onto a plane observing the embedded \"fat\" rhombi and \"skinny\" rhombi which tile together to produce the non-periodic tessellation. Art historian Martin Kemp has observed that Albrecht Dürer sketched similar motifs of a rhombus tiling.\n\nSan Francisco's new $2.2 billion Transbay Transit Center features perforations in its exterior's undulating white metal skin in the Penrose pattern.\n\nThe floor of the atrium of the Bayliss Building at The University of Western Australia is tiled with Penrose tiles.\n\nIn 1979 Miami University used a Penrose tiling executed in terrazzo to decorate the Bachelor Hall courtyard in their Department of Mathematics and Statistics.\n\nThe Andrew Wiles Building, the location of the Mathematics Department at the University of Oxford as of October 2013, includes a section of Penrose tiling as the paving of its entrance.\nThe pedestrian part of the street Keskuskatu in central Helsinki is paved using a form of Penrose tiling. The work was finished in 2014.\n\n\n\n\n"}
{"id": "666832", "url": "https://en.wikipedia.org/wiki?curid=666832", "title": "Philip Herbert Cowell", "text": "Philip Herbert Cowell\n\nPhilip Herbert Cowell FRS (7 August 1870, Calcutta – 6 June 1949) was a British astronomer.\n\nPhilip Herbert Cowell was born in Calcutta, India and educated at Eton and Trinity College, Cambridge. He became second chief assistant at the Royal Greenwich Observatory in 1896 and later became the Superintendent of HM Nautical Almanac Office during 1910–1930. He worked on celestial mechanics, and orbits of comets and minor planets in particular. He also carefully studied the discrepancy that then existed between the theory and observation of the position of the Moon.\n\nHe was elected a Fellow of the Royal Society on 3 May 1906. He won the Gold Medal of the Royal Astronomical Society in 1911.\n\nIn 1909, he discovered 4358 Lynn, a 10-kilometer sized main-belt asteroid and member of the Eunomia.\n\nIn 1910, Cowell and Andrew Crommelin jointly received the Prix Jules Janssen, the highest award of the Société astronomique de France, the French astronomical society.\n\nHe died in Aldeburgh, Suffolk. The main-belt asteroid 1898 Cowell is named after him.\n"}
{"id": "26605226", "url": "https://en.wikipedia.org/wiki?curid=26605226", "title": "Post-quantum cryptography", "text": "Post-quantum cryptography\n\nPost-quantum cryptography (sometimes referred to as quantum-proof, quantum-safe or quantum-resistant) refers to cryptographic algorithms (usually public-key algorithms) that are thought to be secure against an attack by a quantum computer. , this is not true for the most popular public-key algorithms, which can be efficiently broken by a sufficiently strong hypothetical quantum computer. The problem with currently popular algorithms is that their security relies on one of three hard mathematical problems: the integer factorization problem, the discrete logarithm problem or the elliptic-curve discrete logarithm problem. All of these problems can be easily solved on a sufficiently powerful quantum computer running Shor's algorithm. Even though current, publicly known, experimental quantum computers lack processing power to break any real cryptographic algorithm, many cryptographers are designing new algorithms to prepare for a time when quantum computing becomes a threat. This work has gained greater attention from academics and industry through the PQCrypto conference series since 2006 and more recently by several workshops on Quantum Safe Cryptography hosted by the European Telecommunications Standards Institute (ETSI) and the Institute for Quantum Computing.\n\nIn contrast to the threat quantum computing poses to current public-key algorithms, most current symmetric cryptographic algorithms and hash functions are considered to be relatively secure against attacks by quantum computers. While the quantum Grover's algorithm does speed up attacks against symmetric ciphers, doubling the key size can effectively block these attacks. Thus post-quantum symmetric cryptography does not need to differ significantly from current symmetric cryptography. See section on symmetric-key approach below.\n\nCurrently post-quantum cryptography research is mostly focused on six different approaches:\n\nThis approach includes cryptographic systems such as learning with errors, ring learning with errors (ring-LWE), the ring learning with errors key exchange and the ring learning with errors signature, the older NTRU or GGH encryption schemes, and the newer NTRU signature and BLISS signatures. Some of these schemes like NTRU encryption have been studied for many years without anyone finding a feasible attack. Others like the ring-LWE algorithms have proofs that their security reduces to a worst-case problem. The Post Quantum Cryptography Study Group sponsored by the European Commission suggested that the Stehle–Steinfeld variant of NTRU be studied for standardization rather than the NTRU algorithm. At that time, NTRU was still patented.\n\nThis includes cryptographic systems such as the Rainbow (Unbalanced Oil and Vinegar) scheme which is based on the difficulty of solving systems of multivariate equations. Various attempts to build secure multivariate equation encryption schemes have failed. However, multivariate signature schemes like Rainbow could provide the basis for a quantum secure digital signature. There is a patent on the Rainbow Signature Scheme.\n\nThis includes cryptographic systems such as Lamport signatures and the Merkle signature scheme and the newer XMSS and SPHINCS schemes. Hash based digital signatures were invented in the late 1970s by Ralph Merkle and have been studied ever since as an interesting alternative to number-theoretic digital signatures like RSA and DSA. Their primary drawback is that for any hash-based public key, there is a limit on the number of signatures that can be signed using the corresponding set of private keys. This fact had reduced interest in these signatures until interest was revived due to the desire for cryptography that was resistant to attack by quantum computers. There appear to be no patents on the Merkle signature scheme and there exist many non-patented hash functions that could be used with these schemes. The stateful hash-based signature scheme XMSS is described in RFC 8391.\nNote that all the above schemes are one-time or bounded-time signatures, Moni Naor and Moti Yung invented UOWHF hashing in 1989 and designed a signature based on hashing (the Naor-Yung scheme) which can be unlimited-time in use (the first such signature that does not require trapdoor properties).\nThis includes cryptographic systems which rely on error-correcting codes, such as the McEliece and Niederreiter encryption algorithms and the related Courtois, Finiasz and Sendrier Signature scheme. The original McEliece signature using random Goppa codes has withstood scrutiny for over 30 years. However, many variants of the McEliece scheme, which seek to introduce more structure into the code used in order to reduce the size of the keys, have been shown to be insecure. The Post Quantum Cryptography Study Group sponsored by the European Commission has recommended the McEliece public key encryption system as a candidate for long term protection against attacks by quantum computers.\n\nThis cryptographic system relies on the properties of supersingular elliptic curves and supersingular isogeny graphs to create a Diffie-Hellman replacement with forward secrecy. This cryptographic system uses the well studied mathematics of supersingular elliptic curves to create a Diffie-Hellman like key exchange that can serve as a straightforward quantum computing resistant replacement for the Diffie-Hellman and elliptic curve Diffie–Hellman key exchange methods that are in widespread use today. Because it works much like existing Diffie–Hellman implementations, it offers forward secrecy which is viewed as important both to prevent mass surveillance by governments but also to protect against the compromise of long term keys through failures. In 2012, researchers Sun, Tian and Wang of the Chinese State Key Lab for Integrated Service Networks and Xidian University, extended the work of De Feo, Jao, and Plut to create quantum secure digital signatures based on supersingular elliptic curve isogenies. There are no patents covering this cryptographic system.\n\nProvided one uses sufficiently large key sizes, the symmetric key cryptographic systems like AES and SNOW 3G are already resistant to attack by a quantum computer. Further, key management systems and protocols that use symmetric key cryptography instead of public key cryptography like Kerberos and the 3GPP Mobile Network Authentication Structure are also inherently secure against attack by a quantum computer. Given its widespread deployment in the world already, some researchers recommend expanded use of Kerberos-like symmetric key management as an efficient and effective way to get Post Quantum cryptography today.\n\nIn cryptography research, it is desirable to prove the equivalence of a cryptographic algorithm and a known hard mathematical problem. These proofs are often called \"security reductions\", and are used to demonstrate the difficulty of cracking the encryption algorithm. In other words, the security of a given cryptographic algorithm is reduced to the security of a known hard problem. Researchers are actively looking for security reductions in the prospects for post quantum cryptography. Current results are given here:\n\nIn some versions of Ring-LWE there is a security reduction to the shortest-vector problem (SVP) in a lattice as a lower bound on the security. The SVP is known to be NP-hard. Specific ring-LWE systems that have provable security reductions include a variant of Lyubashevsky's ring-LWE signatures defined in a paper by Guneysu, Lyubashevsky, and Poppelmann. The GLYPH signature scheme is a variant of the Gunesyu, Lyubashevsky, and Poppelmann (GLP) signature which takes into account research results that have come after the publication of the GLP signature in 2012. Another Ring-LWE signature is Ring-TESLA.\n\nThe security of the NTRU encryption scheme and the BLISS signature is believed to be related to, but not provably reducible to, the Closest Vector Problem (CVP) in a Lattice. The CVP is known to be NP-hard. The Post Quantum Cryptography Study Group sponsored by the European Commission suggested that the Stehle–Steinfeld variant of NTRU which does have a security reduction be studied for long term use instead of the original NTRU algorithm.\n\nThe Rainbow Multivariate Equation Signature Scheme is a member of a class of multivariate quadratic equation cryptosystems called \"Unbalanced Oil and Vinegar Cryptosystems\" (UOV Cryptosystems) Bulygin, Petzoldt and Buchmann have shown a reduction of generic multivariate quadratic UOV systems to the NP-Hard Multivariate Quadratic Equation Solving problem.\n\nIn 2005, Luis Garcia proved that there was a security reduction of Merkle Hash Tree signatures to the security of the underlying hash function. Garcia showed in his paper that if computationally one-way hash functions exist then the Merkle Hash Tree signature is provably secure. \n\nTherefore, if one used a hash function with a provable reduction of security to a known hard problem one would have a provable security reduction of the Merkle tree signature to that known hard problem. \n\nThe Post Quantum Cryptography Study Group sponsored by the European Commission has recommended use of Merkle signature scheme for long term security protection against quantum computers.\n\nThe McEliece Encryption System has a security reduction to the Syndrome Decoding Problem (SDP). The SDP is known to be NP-hard The Post Quantum Cryptography Study Group sponsored by the European Commission has recommended the use of this cryptography for long term protection against attack by a quantum computer.\n\nIn 2016, Wang proposed a random linear code encryption scheme RLCE\n\nSecurity is related to the problem of constructing an isogeny between two supersingular curves with the same number of points. The most recent investigation of the difficulty of this problem is by Delfs and Galbraith indicates that this problem is as hard as the inventors of the key exchange suggest that it is. There is no security reduction to a known NP-hard problem.\n\nOne common characteristic of many post-quantum cryptography algorithms is that they require larger key sizes than commonly used \"pre-quantum\" public key algorithms. There are often tradeoffs to be made in key size, computational efficiency and ciphertext or signature size. The table lists some values for different schemes at a 128 bit post-quantum security level.\nA practical consideration on a choice among post-quantum cryptographic algorithms is the effort required to send public keys over the internet. From this point of view, the Ring-LWE, NTRU, and SIDH algorithms provide key sizes conveniently under 1KB, hash-signature public keys come in under 5KB, and MDPC-based McEliece takes about 1KB. On the other hand, Rainbow schemes require about 125KB and Goppa-based McEliece requires a nearly 1MB key.\n\nThe fundamental idea of using LWE and Ring LWE for key exchange was proposed and filed at the University of Cincinnati in 2011 by Jintai Ding. The basic idea comes from the associativity of matrix multiplications, and the errors are used to provide the security. The paper appeared in 2012 after a provisional patent application was filed in 2012.\n\nIn 2014, Peikert presented a key transport scheme following the same basic idea of Ding's, where the new idea of sending additional 1 bit signal for rounding in Ding's construction is also utilized. For somewhat greater than 128 bits of security, Singh presents a set of parameters which have 6956-bit public keys for the Peikert's scheme. The corresponding private key would be roughly 14,000 bits.\n\nIn 2015, an authenticated key exchange with provable forward security following the same basic idea of Ding's was presented at Eurocrypt 2015, which is an extension of the HMQV construction in Crypto2005. The parameters for different security levels from 80 bits to 350 bits, along with the corresponding key sizes are provided in the paper.\n\nFor 128 bits of security in NTRU, Hirschhorn, Hoffstein, Howgrave-Graham and Whyte, recommend using a public key represented as a degree 613 polynomial with coefficients formula_1. This results in a public key of 6130 bits. The corresponding private key would be 6743 bits.\n\nFor 128 bits of security and the smallest signature size in a Rainbow multivariate quadratic equation signature scheme, Petzoldt, Bulygin and Buchmann, recommend using equations in formula_2 with a public key size of just over 991,000 bits, a private key of just over 740,000 bits and digital signatures which are 424 bits in length.\n\nIn order to get 128 bits of security for hash based signatures to sign 1 million messages using the fractal Merkle tree method of Naor Shenhav and Wool the public and private key sizes are roughly 36,000 bits in length.\n\nFor 128 bits of security in a McEliece scheme, The European Commissions Post Quantum Cryptography Study group recommends using a binary Goppa code of length at least formula_3 and dimension at least formula_4, and capable of correcting formula_5 errors. With these parameters the public key for the McEliece system will be a systematic generator matrix whose non-identity part takes formula_6 bits. The corresponding private key, which consists of the code support with formula_3 elements from formula_8 and a generator polynomial of with formula_5 coefficients from formula_8, will be 92,027 bits in length\n\nThe group is also investigating the use of Quasi-cyclic MDPC codes of length at least formula_11 and dimension at least formula_12, and capable of correcting formula_13 errors. With these parameters the public key for the McEliece system will be the first row of a systematic generator matrix whose non-identity part takes formula_14 bits. The private key, a quasi-cyclic parity-check matrix with formula_15 nonzero entries on a column (or twice as much on a row), takes no more than formula_16 bits when represented as the coordinates of the nonzero entries on the first row.\n\nBarreto et al. recommend using a binary Goppa code of length at least formula_17 and dimension at least formula_18, and capable of correcting formula_19 errors. With these parameters the public key for the McEliece system will be a systematic generator matrix whose non-identity part takes formula_20 bits. The corresponding private key, which consists of the code support with formula_17 elements from formula_22 and a generator polynomial of with formula_19 coefficients from formula_22, will be 40,476 bits in length.\n\nFor 128 bits of security in the supersingular isogeny Diffie-Hellman (SIDH) method, De Feo, Jao and Plut recommend using a supersingular curve modulo a 768-bit prime. If one uses elliptic curve point compression the public key will need to be no more than 8x768 or 6144 bits in length. A March 2016 paper by authors Azarderakhsh, Jao, Kalach, Koziel, and Leonardi showed how to cut the number of bits transmitted in half, which was further improved by authors Costello, Jao, Longa, Naehrig, Renes and Urbanik resulting in a compressed-key version of the SIDH protocol with public keys only 2640 bits in size. This makes the number of bits transmitted roughly equivalent to the non-quantum secure RSA and Diffie-Hellman at the same classical security level.\n\nAs a general rule, for 128 bits of security in a symmetric-key-based system, one can safely use key sizes of 256 bits. The best quantum attack against generic symmetric-key systems is an application of Grover's algorithm, which requires work proportional to the square root of the size of the key space. To transmit an encrypted key to a device that possesses the symmetric key necessary to decrypt that key requires roughly 256 bits as well. It is clear that symmetric-key systems offer the smallest key sizes for post-quantum cryptography.\n\nA public-key system demonstrates a property referred to as perfect forward secrecy when it generates random public keys per session for the purposes of key agreement. This means that the compromise of one message cannot lead to the compromise of others, and also that there is not a single secret value which can lead to the compromise of multiple messages. Security experts recommend using cryptographic algorithms that support forward secrecy over those that do not. The reason for this is that forward secrecy can protect against the compromise of long term private keys associated with public/private key pairs. This is viewed as a means of preventing mass surveillance by intelligence agencies.\n\nBoth the Ring-LWE key exchange and supersingular isogeny Diffie-Hellman (SIDH) key exchange can support forward secrecy in one exchange with the other party. Both the Ring-LWE and SIDH can also be used without forward secrecy by creating a variant of the classic ElGamal encryption variant of Diffie-Hellman.\n\nThe other algorithms in this article, such as NTRU, do not support forward secrecy as is.\n\nAny authenticated public key encryption system can be used to build a key exchange with forward secrecy.\n\nOpen Quantum Safe (OQS) project was started in late 2016 and has the goal of developing and prototyping quantum-resistant cryptography. It aims to integrate current post-quantum schemes in one library: liboqs. liboqs is an open source C library for quantum-resistant cryptographic algorithms. liboqs initially focuses on key exchange algorithms. liboqs provides a common API suitable for post-quantum key exchange algorithms, and will collect together various implementations. liboqs will also include a test harness and benchmarking routines to compare performance of post-quantum implementations. Furthermore, OQS also provides integration of liboqs into OpenSSL.\n\nAs of April 2017, the following key exchange algorithms are supported:\n\nOne of the main challenges in Post-quantum cryptography is considered to be the implementation of potentially quantum safe algorithms into existing systems. There are tests done, for example by Microsoft Research implementing PICNIC in a PKI using Hardware security modules. Test implementations for Google's NewHope algorithm have also been done by HSM vendors. \n\n\n\n"}
{"id": "3454616", "url": "https://en.wikipedia.org/wiki?curid=3454616", "title": "Pseudorandom function family", "text": "Pseudorandom function family\n\nIn cryptography, a pseudorandom function family, abbreviated PRF, is a collection of efficiently-computable functions which emulate a random oracle in the following way: no efficient algorithm can distinguish (with significant advantage) between a function chosen randomly from the PRF family and a random oracle (a function whose outputs are fixed completely at random). Pseudorandom functions are vital tools in the construction of cryptographic primitives, especially secure encryption schemes.\n\nPseudorandom functions are not to be confused with pseudorandom generators (PRGs). The guarantee of a PRG is that a \"single\" output appears random if the input was chosen at random. On the other hand, the guarantee of a PRF is that \"all its outputs\" appear random, regardless of how the corresponding inputs were chosen, as long as the \"function\" was drawn at random from the PRF family.\n\nA pseudorandom function family can be constructed from any pseudorandom generator, using, for example, the \"GGM\" construction given by Goldreich, Goldwasser, and Micali.. While in practice, block ciphers are used in most instances where a pseudorandom function is needed, they do not, in general, constitute a pseudorandom function family, as block ciphers such as AES are defined for only limited numbers of input and key sizes.\n\nA PRF is an efficient (i.e. computable in polynomial time), deterministic function that maps two distinct sets (domain and range) and looks like a truly random function.\n\nEssentially, a truly random function would just be composed of a lookup table filled with uniformly distributed random entries. However, in practice, a PRF is given an input string in the domain and a hidden random seed and runs multiple times with the same input string and seed, always returning the same value. Nonetheless, given an arbitrary input string, the output looks random if the seed is taken from a uniform distribution.\n\nA PRF is considered to be good if its behavior is indistinguishable from a truly random function. Therefore, given an output from either the truly random function or a PRF, there should be no efficient method to correctly determine whether the output was produced by the truly random function or the PRF.\n\nA family of functions, \n\nis pseudorandom if the following conditions are satisfied:\n\n\nIn an oblivious pseudorandom function, information is concealed from two parties that are involved in a PRF. That is, if Alice gives the input for a pseudorandom function to Bob, and Bob computes a PRF and gives the output to Alice, Bob is not able to see either the input or the output, and Alice is not able to see the secret key Bob uses with the pseudorandom function. This enables transactions of sensitive cryptographic information to be secure even between untrusted parties.\n\nPRFs can be used for:\n\n\n"}
{"id": "4025531", "url": "https://en.wikipedia.org/wiki?curid=4025531", "title": "Reflection formula", "text": "Reflection formula\n\nIn mathematics, a reflection formula or reflection relation for a function \"f\" is a relationship between \"f\"(\"a\" − \"x\") and \"f\"(\"x\"). It is a special case of a functional equation, and it is very common in the literature to use the term \"functional equation\" when \"reflection formula\" is meant.\n\nReflection formulas are useful for numerical computation of special functions. In effect, an approximation that has greater accuracy or only converges on one side of a reflection point (typically in the positive half of the complex plane) can be employed for all arguments.\n\nThe even and odd functions satisfy simple reflection relations around \"a\" = 0. For all even functions,\n\nand for all odd functions,\n\nA famous relationship is Euler's reflection formula\n\nfor the Gamma function formula_4, due to Leonhard Euler.\n\nThere is also a reflection formula for the general \"n\"-th order polygamma function ψ(\"z\"),\n\nwhich springs trivially from the fact that the polygamma functions are defined as the derivatives of formula_6 and thus inherit the reflection formula.\n\nThe Riemann zeta function ζ(\"z\") satisfies\n\nand the Riemann Xi function ξ(\"z\") satisfies\n\n"}
{"id": "1701075", "url": "https://en.wikipedia.org/wiki?curid=1701075", "title": "Sequence space", "text": "Sequence space\n\nIn functional analysis and related areas of mathematics, a sequence space is a vector space whose elements are infinite sequences of real or complex numbers. Equivalently, it is a function space whose elements are functions from the natural numbers to the field K of real or complex numbers. The set of all such functions is naturally identified with the set of all possible infinite sequences with elements in K, and can be turned into a vector space under the operations of pointwise addition of functions and pointwise scalar multiplication. All sequence spaces are linear subspaces of this space. Sequence spaces are typically equipped with a norm, or at least the structure of a topological vector space.\n\nThe most important sequence spaces in analysis are the ℓ spaces, consisting of the \"p\"-power summable sequences, with the \"p\"-norm. These are special cases of L spaces for the counting measure on the set of natural numbers. Other important classes of sequences like convergent sequences or null sequences form sequence spaces, respectively denoted \"c\" and \"c\", with the sup norm. Any sequence space can also be equipped with the topology of pointwise convergence, under which it becomes a special kind of Fréchet space called FK-space.\n\nLet K denote the field either of real or complex numbers. Denote by K the set of all sequences of scalars\nThis can be turned into a vector space by defining vector addition as\nand the scalar multiplication as\nA sequence space is any linear subspace of K.\n\nFor 0 < \"p\" < ∞, ℓ is the subspace of K consisting of all sequences \"x\" = (x) satisfying\nIf \"p\" ≥ 1, then the real-valued operation formula_5 defined by\ndefines a norm on ℓ. In fact, ℓ is a complete metric space with respect to this norm, and therefore is a Banach space.\n\nIf 0 < \"p\" < 1, then ℓ does not carry a norm, but rather a metric defined by\n\nIf \"p\" = ∞, then ℓ is defined to be the space of all bounded sequences. With respect to the norm\nℓ is also a Banach space.\n\nThe space of convergent sequences \"c\" is a sequence space. This consists of all \"x\" ∈ K such that lim\"x\" exists. Since every convergent sequence is bounded, \"c\" is a linear subspace of ℓ. It is, moreover, a closed subspace with respect to the infinity norm, and so a Banach space in its own right.\n\nThe subspace of null sequences \"c\" consists of all sequences whose limit is zero. This is a closed subspace of \"c\", and so again a Banach space.\n\nThe subspace of eventually zero sequences \"c\" consists of all sequences which have only finitely many nonzero elements. This is not a closed subspace and therefore is not a Banach space with respect to the infinity norm. For example, the sequence (\"x\") where \"x\" has 1/k for the first n entries and is zero everywhere else (i.e. \"x\" = (1, 1/2, ..., 1/n, 0, ...)) is Cauchy w.r.t. infinity norm but not convergent (to a sequence in \"c\").\n\nThe space of bounded series, denote by bs, is the space of sequences \"x\" for which\nThis space, when equipped with the norm\nis a Banach space isometrically isomorphic to ℓ, via the linear mapping\nThe subspace \"cs\" consisting of all convergent series is a subspace that goes over to the space \"c\" under this isomorphism.\n\nThe space Φ or formula_12 is defined to be the space of all infinite sequences with only a finite number of non-zero terms (sequences with finite support). This set is dense in many sequence spaces.\n\nThe space ℓ is the only ℓ space that is a Hilbert space, since any norm that is induced by an inner product should satisfy the parallelogram law\n\nSubstituting two distinct unit vectors for \"x\" and \"y\" directly shows that the identity is not true unless \"p\" = 2.\n\nEach ℓ is distinct, in that ℓ is a strict subset of ℓ whenever \"p\" < \"s\"; furthermore, ℓ is not linearly isomorphic to ℓ when \"p\" ≠ \"s\". In fact, by Pitt's theorem , every bounded linear operator from ℓ to ℓ is compact when \"p\" < \"s\". No such operator can be an isomorphism; and further, it cannot be an isomorphism on any infinite-dimensional subspace of ℓ, and is thus said to be strictly singular.\n\nIf 1 < \"p\" < ∞, then the (continuous) dual space of ℓ is isometrically isomorphic to ℓ, where \"q\" is the Hölder conjugate of \"p\": 1/\"p\" + 1/\"q\" = 1. The specific isomorphism associates to an element \"x\" of ℓ the functional\nfor \"y\" in ℓ. Hölder's inequality implies that \"L\" is a bounded linear functional on ℓ, and in fact\nso that the operator norm satisfies\nIn fact, taking \"y\" to be the element of ℓ with\ngives \"L\"(\"y\") = ||\"x\"||, so that in fact\nConversely, given a bounded linear functional \"L\" on ℓ, the sequence defined by \"x\" = \"L\"(\"e\") lies in ℓ. Thus the mapping formula_19 gives an isometry\n\nThe map\nobtained by composing κ with the inverse of its transpose coincides with the canonical injection of ℓ into its double dual. As a consequence ℓ is a reflexive space. By abuse of notation, it is typical to identify ℓ with the dual of ℓ: (ℓ) = ℓ. Then reflexivity is understood by the sequence of identifications (ℓ) = (ℓ) = ℓ.\n\nThe space \"c\" is defined as the space of all sequences converging to zero, with norm identical to ||\"x\"||. It is a closed subspace of ℓ, hence a Banach space. The dual of \"c\" is ℓ; the dual of ℓ is ℓ. For the case of natural numbers index set, the ℓ and \"c\" are separable, with the sole exception of ℓ. The dual of ℓ is the ba space.\n\nThe spaces \"c\" and ℓ (for 1 ≤ \"p\" < ∞) have a canonical unconditional Schauder basis {\"e\" | \"i\" = 1, 2,…}, where \"e\" is the sequence which is zero but for a 1 in the \"i\" entry.\n\nThe space ℓ has the Schur property: In ℓ, any sequence that is weakly convergent is also strongly convergent . However, since the weak topology on infinite-dimensional spaces is strictly weaker than the strong topology, there are nets in ℓ that are weak convergent but not strong convergent.\n\nThe ℓ spaces can be embedded into many Banach spaces. The question of whether every infinite-dimensional Banach space contains an isomorph of some ℓ or of \"c\", was answered negatively by B. S. Tsirelson's construction of Tsirelson space in 1974. The dual statement, that every separable Banach space is linearly isometric to a quotient space of ℓ, was answered in the affirmative by . That is, for every separable Banach space \"X\", there exists a quotient map formula_22, so that \"X\" is isomorphic to formula_23. In general, ker \"Q\" is not complemented in ℓ, that is, there does not exist a subspace \"Y\" of ℓ such that formula_24. In fact, ℓ has uncountably many uncomplemented subspaces that are not isomorphic to one another (for example, take formula_25; since there are uncountably many such \"X\" 's, and since no ℓ is isomorphic to any other, there are thus uncountably many ker \"Q\" 's).\n\nExcept for the trivial finite-dimensional case, an unusual feature of ℓ is that it is not polynomially reflexive.\n\nFor formula_26, the spaces formula_27 are increasing in formula_28, with the inclusion operator being continuous: for formula_29, one has formula_30. \n\nThis follows from defining formula_31 for formula_32, and noting that formula_33 for all formula_34, which can be shown to imply formula_35.\n\n\n"}
{"id": "1456863", "url": "https://en.wikipedia.org/wiki?curid=1456863", "title": "Setpoint (control system)", "text": "Setpoint (control system)\n\nIn cybernetics and control theory, a setpoint (also set point or set-point) is the desired or target value for an essential variable, or process value of a system. Departure of such a variable from its setpoint is one basis for error-controlled regulation using negative feedback for automatic control. . The set point is usually abbreviated to SP, and the process value is usually abbreviated to PV.\n\nCruise control\n\nThe SP-PV error can be used to return a system to its norm. An everyday example is the cruise control on a road vehicle; where external influences such as gradients cause speed changes (PV), and the driver also alters the desired set speed (SP). The automatic control algorithm restores the actual speed to the desired speed in the optimum way, without delay or overshoot, by altering the power output of the vehicle's engine. In this way the SP-PV error is used to control the PV so that it equals the SP. A widespread use of SP-PV error control is the PID controller.\n\nIndustrial applications\n\nSpecial consideration must be given for engineering applications. In industrial systems, physical or process restraints may limit the determined set point. For example, a reactor which operates more efficiently at higher temperatures may be rated to withstand 500°C. However, for safety reasons, the set point for the reactor temperature control loop would be well below this limit, even if this means the reactor is running less efficiently.\n"}
{"id": "7853805", "url": "https://en.wikipedia.org/wiki?curid=7853805", "title": "Social trap", "text": "Social trap\n\nIn psychology, a social trap is a situation in which a group of people act to obtain short-term individual gains, which in the long run leads to a loss for the group as a whole. Examples of social traps include overfishing, energy \"brownout\" and \"blackout\" power outages during periods of extreme temperatures, the overgrazing of cattle on the Sahelian Desert, and the destruction of the rainforest by logging interests and agriculture.\n\nThe term \"social trap\" was first introduced to the scientific community by John Platt's 1973 paper in \"American Psychologist\", and in a book developed in an interdisciplinary symposium held at the University of Michigan. Building upon the concept of the \"tragedy of the commons\" in Garrett Hardin's pivotal article in \"Science\" (1968), Platt and others in the seminar applied behavioral psychology concepts to actions of people operating in social traps. By applying the findings of basic research on \"schedules of operant reinforcement\" (B.F. Skinner 1938, 1948, 1953, 1957; Keller and Schoenfeld, 1950), Platt recognized that individuals operating for short-term positive gain (\"reinforcement\") had a tendency to over-exploit a resource, which led to a long-term overall loss to society.\n\nThe application of behavioral psychology terms to behaviors in the tragedy of the commons led to the realization that the same short-term/long-term cause-effect relationship also applied to other human traps, in addition to the exploitation of commonly held resources. Platt et al. also introduced the terms \"social fence\" and \"individual trap\". \"Social fence\" refers to a short-term avoidance behavior by individuals that leads to a long-term loss to the entire group. An example is the anecdote of a mattress that falls from a vehicle on a two lane highway. Motorists tend to back up in a traffic jam behind the mattress, waiting for a break in the oncoming traffic to pass around the mattress. Each individual motorist avoids the opportunity to exit their stopped car and pull the mattress to the side of the road. The long-term consequence of this avoidance behavior is that all of the motorists (except for perhaps one) arrived at their destinations later than they would have if an individual had removed the mattress barrier.\n\nAn individual trap is similar to a social trap except that it involves the behavior of only a single person rather than a group of people. The basic concept is that an individual's behavior for short-term reinforcers leads to a long-term loss for the individual. Examples of individual traps are tobacco smoking leading to lung cancer or alcohol ingestion leading to cirrhosis of the liver.\n\nThe first empirical test of the concept of social traps was by Brechner at Arizona State University, who operationalized the concepts underlying Platt et al.'s theoretical analysis. By creating a laboratory game, Brechner had groups of college students playing a game where they could accumulate points by pressing buttons for the individual short-term positive rewards of experimental credit in their introductory psychology classes. Players could see a lighted display that indicated the total quantity of points available at any given time in the experiment. Players were told that if they completely drained the pool of points, the game was over and they could not accumulate more points. By responding for points at a moderate rate all the players in the group could accumulate enough points to fulfill their entire semester's experimental requirements. But if one or more players took points for themselves at too fast a rate, the pool would be drained of points and none of the players would achieve the maximum potential experimental credit.\n\nIn building the laboratory analogy of social traps, Brechner introduced the concept of \"superimposed schedules of reinforcement\". Skinner and Ferster (1957) had demonstrated that reinforcers could be delivered on schedules (schedule of reinforcement), and further that organisms behaved differently under different schedules. Rather than a reinforcer, such as food or water, being delivered every time as a consequence of some behavior, a reinforcer could be delivered after more than one instance of the behavior. For example, a pigeon may be required to peck a button switch five times before food is made available to the pigeon. This is called a \"ratio schedule\". Also, a reinforcer could be delivered after an interval of time passed following a target behavior. An example is a rat that is given a food pellet one minute after the rat pressed a lever. This is called an \"interval schedule\". In addition, ratio schedules can deliver reinforcement following fixed or variable number of behaviors by the individual organism. Likewise, interval schedules can deliver reinforcement following fixed or variable intervals of time following a single response by the organism. Individual behaviors tend to generate response rates that differ based upon how the reinforcement schedule is created. Much subsequent research in many labs examined the effects on behaviors of scheduling reinforcers.\n\nWhen an organism is offered the opportunity to choose between or among two or more simple schedules of reinforcement at the same time, the reinforcement structures are called \"concurrent schedules of reinforcement\". In creating the laboratory analogy of social traps, Brechner created a situation where simple reinforcement schedules were superimposed upon each other. In other words, a single response or group of responses by an organism led to multiple consequences. Concurrent schedules of reinforcement can be thought of as \"or\" schedules, and superimposed schedules of reinforcement can be thought of as \"and\" schedules.\n\nTo simulate social traps a short-term positive reward is superimposed upon a long-term negative consequence. In the specific experiment, the short-term positive reinforcer was earning points that applied to class credits. The long-term negative consequence was that each point earned by a player also drained the pool of available points. Responding too rapidly for short-term gains led to the long-term loss of draining the resource pool. What makes the traps social is that any individual can respond in a way that the long-term consequence also comes to bear on the other individuals in the environment.\n\nSuperimposed schedules of reinforcement have many real-world applications in addition to generating social traps (Brechner and Linder, 1981; Brechner, 1987; Brechner, 2010). Many different human individual and social situations can be created by superimposing simple reinforcement schedules. For example, a human being could have simultaneous tobacco and alcohol addictions. Even more complex situations can be created or simulated by superimposing two or more concurrent schedules. For example, a high school senior could have a choice between going to Stanford University or UCLA, and at the same time have the choice of going into the Army or the Air Force, and simultaneously the choice of taking a job with an internet company or a job with a software company. That would be a reinforcement structure of three superimposed concurrent schedules of reinforcement. An example of the use of superimposed schedules as a tool in the analysis of the contingencies of rent control can be found online in the website \"Economic and Game Theory Forum\", (Brechner, 2003).\n\nSubsequent empirical studies by other researchers explored aspects of social traps other than the underlying reinforcement structure. Studies tended to concentrate on manipulating social and cognitive variables. Cass and Edney (1978) created a simpler game using a bowl of nuts to simulate a commonly held resource. The Nuts Game as they called it had some distinct advantages over Brechner's electronically wired laboratory simulation. The Nuts Game could be transported easily to any environment in or out of the laboratory. It was simple and required no electronics. The reinforcers used were primary food rewards rather than the secondary conditioned reinforcers of class credit used in the earlier study.\n\nFrom Platt's and others' initial concept, social trap research has spread to laboratories all over the world and has expanded into the fields of sociology, economics, institutional design, and the nuclear arms race. Summaries of the many other diverse studies of social traps can be found in Messick and McClelland (1983), Costanza (1984), Komorita and Parks (1996), and Rothstein (2005).\n\nSocial trap research continues to be an active area. Urlacher (2008) devised an iterated version of the prisoner's dilemma game using groups of people, or \"agents\", pitted against other groups of agents, in a variation he termed a \"two-level social trap\". He reported that when using a democratic decision rule, larger groups behaved more cooperatively than smaller groups. Chuang, Rivoire, and Liebler (2009) constructed a non-mammalian commons dilemma using colonies of the bacteria \"Escherichia coli\" composed of strains of producer and nonproducer microbes that contribute (or do not contribute) to the common resource in an examination of the statistical concept of Simpson's paradox.\n\nIn 2010, Shaimaa Lazem and Denis Gračanin, in the Department of Computer Science at Virginia Tech, took social traps to a new level: Into cyberspace. They performed a replication of the original social trap experiment, but created the social trap in the internet virtual world known as Second Life (Lazem and Gračanin, 2010). They constructed a virtual experimental laboratory with the subjects responding through avatars. The findings mirrored the original study, by finding that the ability to communicate led to greater replenishment of common resources.\n\n\n"}
{"id": "28680558", "url": "https://en.wikipedia.org/wiki?curid=28680558", "title": "Square-integrable function", "text": "Square-integrable function\n\nIn mathematics, a square-integrable function, also called a quadratically integrable function, is a real- or complex-valued measurable function for which the integral of the square of the absolute value is finite. Thus, if\n\nthen \"ƒ\" is square integrable on the real line formula_2. One may also speak of quadratic integrability over bounded intervals such as [0, 1].\n\nAn equivalent definition is to say that the square of the function itself (rather than of its absolute value) is Lebesgue integrable. For this to be true, the integrals of the positive and negative portions of the real part must both be finite, as well as those for the imaginary part. The vector space of square integrable functions (with respect to Lebesque measure) form the L -space with p = 2.\n\nOften the term is used not to refer to a specific function, but to a set of functions that are equal almost everywhere.\n\nThe square integrable functions (in the sense mentioned in which a \"function\" actually means a set of functions that are equal almost everywhere) form an inner product space with inner product given by\n\nwhere\n\nSince |\"a\"| = \"a \", square integrability is the same as saying\n\nIt can be shown that square integrable functions form a complete metric space under the metric induced by the inner product defined above.\nA complete metric space is also called a Cauchy space, because sequences in such metric spaces converge if and only if they are Cauchy.\nA space which is complete under the metric induced by a norm is a Banach space.\nTherefore, the space of square integrable functions is a Banach space, under the metric induced by the norm, which in turn is induced by the inner product.\nAs we have the additional property of the inner product, this is specifically a Hilbert space, because the space is complete under the metric induced by the inner product.\n\nThis inner product space is conventionally denoted by formula_6 and many times abbreviated as formula_7.\nNote that formula_7 denotes the set of square integrable functions, but no selection of metric, norm or inner product are specified by this notation.\nThe set, together with the specific inner product formula_9 specify the inner product space.\n\nThe space of square integrable functions is the \"L\" space in which \"p\" = 2.\n\n"}
{"id": "22453058", "url": "https://en.wikipedia.org/wiki?curid=22453058", "title": "Sun-Ni law", "text": "Sun-Ni law\n\nSun-Ni's Law (or Sun and Ni's Law, also known as memory-bounded speedup), is a memory-bounded speedup model which states that as computing power increases the corresponding increase in problem size is constrained by the system’s memory capacity. In general, as a system grows in computational power, the problems run on the system increase in size. Analogous to Amdahl's law, which says that the problem size remains constant as system sizes grow, and Gustafson's law, which proposes that the problem size should scale but be bound by a fixed amount of time, Sun-Ni's Law states the problem size should scale but be bound by the memory capacity of the system. Sun-Ni's Law was initially proposed by Xian-He Sun and Lionel Ni at the Proceedings of IEEE Supercomputing Conference 1990.\n\nWith the increasing disparity between CPU speed and memory data access latency, application execution time often depends on the memory speed of the system. As predicted by Sun and Ni, data access has become the premier performance bottleneck for high-end computing. From this fact one can see the intuition behind Sun-Ni's Law, as system resources increase, applications are often bottlenecked by memory speed and bandwidth, thus an application can achieve a larger speedup by utilizing all the memory capacity in the system. Sun-Ni's Law can be applied to different layers of a memory hierarchy system, from L1 cache to main memory. Through its memory-bounded function,\"W=G(M)\", it reveals the trade-off between computing and memory in algorithm and system architecture design.\n\nAll three speedup models, Sun-Ni, Gustafson, and Amdahl, provide a metric to analyze speedup for Parallel computing. Amdahl’s law focuses on the time reduction for a given fixed-size problem. Amdahl’s law states that the sequential portion of the problem (algorithm) limits the total speedup that can be achieved as system resources increase. Gustafson’s law suggests that it is beneficial to build a large-scale parallel system as the speedup can grow linearly with the system size if the problem size is scaled up to maintain a fixed execution time. Yet as memory access latency often becomes the dominant factor in an application’s execution time, applications may not scale up to meet the time bound constraint. Sun-Ni's Law, instead of constraining the problem size by time, constrains the problem by the memory capacity of the system, or in other words bounds based on memory. Sun-Ni's Law is a generalization of Amdahl's Law and Gustafson's Law. When the memory-bounded function \"G(M)=1\", it resolves to Amdahl's law, when the memory-bounded function \"G(M)=m\",the number of processors, it resolves to Gustafson's Law.\n\nLet formula_1 be the scaled workload under a memory space constraint. The memory bounded speedup can be defined as:\n\nSuppose formula_2 is the portion of the workload that can be parallelized and formula_3 is the sequential portion of the workload.\n\nLet formula_4 be the function that reflects the parallel workload increase factor as the memory capacity increases m times.\n\nLet: formula_5 and: formula_6 where formula_7 is the memory capacity of one node.\n\nThus, formula_8\n\nThe memory bounded speedup is then:\n\nformula_9\n\nFor any power function formula_10 and for any rational numbers \"a\" and \"b\", we have:\n\nformula_11\n\nwhere formula_12 is the power function with the coefficient as 1.\n\nThus by taking the highest degree term to determine the complexity of the algorithm, we can rewrite memory bounded speedup as:\n\nformula_13\n\nIn this equation, formula_14 represents the influence of memory change on the change in problem size.\n\nSuppose formula_15, Then the memory-bounded speedup model reduces to Amdahl's law, since problem size is fixed or independent of resource increase.\n\nSuppose formula_16, Then the memory-bounded speedup model reduces to Gustafson's law, which means when memory capacity increases \"m\" times and the workload also increases \"m\" times all the data needed is local to every node in the system.\n\nOften, for simplicity and for matching the notation of Amdahl's Law and Gustafson's Law, the letter \"G\" is used to represent the memory bound function formula_14, and \"n\" replaces \"m\". Using this notation we get:\n\nformula_18\n\nSuppose one would like to determine the memory-bounded speedup of matrix multiplication. The memory requirement of matrix multiplication is roughly \nformula_19 where \"N\" is the dimension of the two \"N X N\" source matrices. And the computation requirement is formula_20\n\nThus we have:\n\nformula_21 and formula_22\n\nThus the memory-bounded speedup is for matrix multiplication is:\n\nformula_23\n\nThe following is another matrix multiplication example which illustrates the rapid increase in parallel execution time. \nThe execution time of a \"N X N\" matrix for a uniprocessor is:formula_24. While the memory usage is: formula_25\n\nSuppose a \"10000\"-by-\"10000\" matrix takes \"800 MB\" of memory and can be factorized in \"1\" hour on a uniprocessor. Now for the scaled workload suppose is possible to factorize a \"320,000\"-by-\"320,000\" matrix in \"32\" hours. The time increase is quite large, but the increase in problem size may be more valuable for someones whose premier goal is accuracy. For example, an astrophysicist may be more interested in simulating an N-body problem with as the number of particles as large as possible. This example shows for computation intensive applications, memory capacity does not need to proportionally scale up with computing power.\n\nThe memory-bounded speedup model is the first work to reveal that memory is the performance constraint for high-end computing and presents a quantitative mathematical formulation for the trade-off between memory and computing. It is based on the memory-bounded function,\"W=G(n)\", where W is the work and thus also the computation for most applications. \"M\" is the memory requirement in terms of capacity, and \"G\" is the reuse rate. \"W=G(M)\" gives a very simple, but effective, description of the relation between computation and memory requirement. From an architecture viewpoint, the memory-bounded model suggests the size, as well as speed, of the cache(s) should match the CPU performance. Today, modern microprocessors such as the Pentium Pro, Alpha 21164, Strong Arm SA110, and Longson-3A use 80% or more of their transistors for the on-chip cache rather than computing components. From an algorithm design viewpoint, we should reduce the number of memory accesses. That is, reuse the data when it is possible. The function \"G()\" gives the reuse rate. Today, the term memory bound functions has become a general term which refers to functions which involve extensive memory access. Memory complexity analysis has become a discipline of computer algorithm analysis.\n"}
{"id": "50336602", "url": "https://en.wikipedia.org/wiki?curid=50336602", "title": "Tadpole graph", "text": "Tadpole graph\n\nIn the mathematical discipline of graph theory, the (\"m\",\"n\")-tadpole graph is a special type of graph consisting of a cycle graph on \"m\" (at least 3) vertices and a path graph on \"n\" vertices, connected with a bridge.\n\n"}
{"id": "24511700", "url": "https://en.wikipedia.org/wiki?curid=24511700", "title": "Tutte graph", "text": "Tutte graph\n\nIn the mathematical field of graph theory, the Tutte graph is a 3-regular graph with 46 vertices and 69 edges named after W. T. Tutte. It has chromatic number 3, chromatic index 3, girth 4 and diameter 8.\n\nThe Tutte graph is a cubic polyhedral graph, but is non-hamiltonian. Therefore, it is a counterexample to Tait's conjecture that every 3-regular polyhedron has a Hamiltonian cycle.\n\nPublished by Tutte in 1946, it is the first counterexample constructed for this conjecture. Other counterexamples were found later, in many cases based on Grinberg's theorem.\n\nFrom a small planar graph called the Tutte fragment, W. T. Tutte constructed a non-Hamiltonian polyhedron, by putting together three such fragments. The \"compulsory\" edges of the fragments, that must be part of any Hamiltonian path through the fragment, are connected at the central vertex; because any cycle can use only two of these three edges, there can be no Hamiltonian cycle.\n\nThe resulting graph is 3-connected and planar, so by Steinitz' theorem it is the graph of a polyhedron. It has 25 faces.\n\nIt can be realized geometrically from a tetrahedron (the faces of which correspond to the four large nine-sided faces in the drawing, three of which are between pairs of fragments and the fourth of which forms the exterior) by multiply truncating three of its vertices.\n\nThe automorphism group of the Tutte graph is Z/3Z, the cyclic group of order 3.\n\nThe characteristic polynomial of the Tutte graph is :\n\nAlthough the Tutte graph is the first 3-regular non-Hamiltonian polyhedral graph to be discovered, it is not the smallest such graph.\n\nIn 1965 Lederberg found the Barnette–Bosák–Lederberg graph on 38 vertices. In 1968, Grinberg constructed additional small counterexamples to the Tait's conjecture – the Grinberg graphs on 42, 44 and 46 vertices. In 1974 Faulkner and Younger published two more graphs – the Faulkner–Younger graphs on 42 and 44 vertices.\n\nFinally Holton and McKay showed there are exactly six 38-vertex non-Hamiltonian polyhedra that have nontrivial three-edge cuts. They are formed by replacing two of the vertices of a pentagonal prism by the same fragment used in Tutte's example.\n"}
{"id": "49253", "url": "https://en.wikipedia.org/wiki?curid=49253", "title": "Urysohn's lemma", "text": "Urysohn's lemma\n\nIn topology, Urysohn's lemma is a lemma that states that a topological space is normal if and only if any two disjoint closed subsets can be separated by a continuous function.\n\nUrysohn's lemma is commonly used to construct continuous functions with various properties on normal spaces. It is widely applicable since all metric spaces and all compact Hausdorff spaces are normal. The lemma is generalized by (and usually used in the proof of) the Tietze extension theorem.\n\nThe lemma is named after the mathematician Pavel Samuilovich Urysohn.\n\nTwo subsets \"A\" and \"B\" of a topological space \"X\" are said to be separated by neighbourhoods if there are neighbourhoods \"U\" of \"A\" and \"V\" of \"B\" that are disjoint. In particular \"A\" and \"B\" are necessarily disjoint.\n\nTwo plain subsets \"A\" and \"B\" are said to be separated by a function if there exists a continuous function \"f\" from \"X\" into the unit interval [0,1] such that \"f\"(\"a\") = 0 for all \"a\" in \"A\" and \"f\"(\"b\") = 1 for all \"b\" in \"B\". Any such function is called a Urysohn function for \"A\" and \"B\". In particular \"A\" and \"B\" are necessarily disjoint.\n\nIt follows that if two subsets \"A\" and \"B\" are separated by a function then so are their closures.<br>\nAlso it follows that if two subsets \"A\" and \"B\" are separated by a function then \"A\" and \"B\" are separated by neighbourhoods.\n\nA normal space is a topological space in which any two disjoint closed sets can be separated by neighbourhoods. Urysohn's lemma states that a topological space is normal if and only if any two disjoint closed sets can be separated by a continuous function.\n\nThe sets \"A\" and \"B\" need not be precisely separated by \"f\", i.e., we do not, and in general cannot, require that \"f\"(\"x\") ≠ 0 and ≠ 1 for \"x\" outside of \"A\" and \"B\". The spaces in which this property holds are the perfectly normal spaces.\n\nUrysohn's lemma has led to the formulation of other topological properties such as the 'Tychonoff property' and 'completely Hausdorff spaces'. For example, a corollary of the lemma is that normal \"T\" spaces are Tychonoff.\n\nThe procedure is an entirely straightforward application of the definition of normality (once one draws some figures representing the first few steps in the induction described below to see what is going on), beginning with two disjoint closed sets. The \"clever\" part of the proof is the indexing the open sets thus constructed by dyadic fractions.\n\nFor every dyadic fraction \"r\" ∈ (0,1), we are going to construct an open subset \"U\"(\"r\") of \"X\" such that:\nOnce we have these sets, we define \"f\"(\"x\") = 1 if \"x\" ∉ \"U\"(\"r\") for any \"r\"; otherwise \"f\"(\"x\") = inf { \"r\" : \"x\" ∈ \"U\"(\"r\") } for every \"x\" ∈ \"X\". Using the fact that the dyadic rationals are dense, it is then not too hard to show that \"f\" is continuous and has the property \"f\"(\"A\") ⊆ {0} and \"f\"(\"B\") ⊆ {1}.\n\nIn order to construct the sets \"U\"(\"r\"), we actually do a little bit more: we construct sets \"U\"(\"r\") and \"V\"(\"r\") such that\nSince the complement of \"V\"(\"r\") is closed and contains \"U\"(\"r\"), the latter condition then implies condition (2) from above.\n\nThis construction proceeds by mathematical induction. First define \"U\"(1) = \"X\" \\ \"B\" and \"V\"(0) = \"X\" \\ \"A\". Since \"X\" is normal, we can find two disjoint open sets \"U\"(1/2) and \"V\"(1/2) which contain \"A\" and \"B\", respectively. Now assume that \"n\"≥1 and the sets \"U\"(\"k\"/2) and \"V\"(\"k\"/2) have already been constructed for \"k\" = 1...,2-1. Since \"X\" is normal, for any \"a\" ∈ { 0,1...,2-1 }, we can find two disjoint open sets which contain \"X\" \\ \"V\"(\"a\"/2) and \"X\" \\ \"U\"((\"a\"+1)/2), respectively. Call these two open sets \"U\"((2\"a\"+1)/2) and \"V\"((2\"a\"+1)/2), and verify the above three conditions.\n\nThe Mizar project has completely formalized and automatically checked a proof of Urysohn's lemma in the URYSOHN3 file.\n\n\n"}
