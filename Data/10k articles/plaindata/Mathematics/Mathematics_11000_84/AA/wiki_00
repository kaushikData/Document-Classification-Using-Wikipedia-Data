{"id": "5182124", "url": "https://en.wikipedia.org/wiki?curid=5182124", "title": "1701 (number)", "text": "1701 (number)\n\n1701 is the natural number preceding 1702 and following 1700.\n\n1701 is an odd number and a Stirling number of the second kind.\n\nThe number 1701 also has unusual properties as it:\n\n"}
{"id": "30625393", "url": "https://en.wikipedia.org/wiki?curid=30625393", "title": "Abhandlungen aus dem Mathematischen Seminar der Universität Hamburg", "text": "Abhandlungen aus dem Mathematischen Seminar der Universität Hamburg\n\n"}
{"id": "54171755", "url": "https://en.wikipedia.org/wiki?curid=54171755", "title": "Analysis of Boolean functions", "text": "Analysis of Boolean functions\n\nIn mathematics and theoretical computer science, analysis of Boolean functions is the study of real-valued functions on formula_1 or formula_2 from a spectral perspective (such functions are sometimes known as pseudo-Boolean functions). The functions studied are often, but not always, Boolean-valued, making them Boolean functions. The area has found many applications in combinatorics, social choice theory, random graphs, and theoretical computer science, especially in hardness of approximation, property testing and PAC learning.\n\nWe will mostly consider functions defined on the domain formula_2. Sometimes it is more convenient to work with the domain formula_1 instead. If formula_5 is defined on formula_2, then the corresponding function defined on formula_1 is\n\nSimilarly, for us a Boolean function is a formula_9-valued function, though often it is more convenient to consider formula_10-valued functions instead.\n\nEvery real-valued function formula_11 has a unique expansion as a multilinear polynomial:\n\nThis is the Hadamard transform of the function formula_5, which is the Fourier transform in the group formula_14. The coefficients formula_15 are known as \"Fourier coefficients\", and the entire sum is known as the \"Fourier expansion\" of formula_5. The functions formula_17 are known as \"Fourier characters\", and they form an orthonormal basis for the space of all functions over formula_2, with respect to the inner product formula_19.\n\nThe Fourier coefficients can be calculated using an inner product:\n\nIn particular, this shows that formula_21 Parseval's identity states that\n\nIf we skip formula_23, then we get the variance of formula_5:\n\nThe \"degree\" of a function formula_11 is the maximum formula_27 such that formula_28 for some set formula_29 of size formula_27. In other words, the degree of formula_5 is its degree as a multilinear polynomial.\n\nIt is convenient to decompose the Fourier expansion into \"levels\": the Fourier coefficient formula_15 is on level formula_33.\n\nThe \"degree formula_27\" part of formula_5 is\n\nUsing this we can define the noise stability and the noise sensitivity, as before.\n\nThe Russo–Margulis formula states that for monotone Boolean functions formula_37,\n\nBoth the influence and the probabilities are taken with respect to formula_39, and on the right-hand side we have the average sensitivity of formula_5. If we think of formula_5 as a property, then the formula states that as formula_42 varies, the derivative of the probability that formula_5 occurs at formula_42 equals the average sensitivity at formula_42.\n\nThe Russo–Margulis formula is key for proving sharp threshold theorems such as Friedgut's.\n\nOne of the deepest results in the area, the invariance principle, connects the distribution of functions on the Boolean cube formula_2 to their distribution on \"Gaussian space\", which is the space formula_47 endowed with the standard formula_48-dimensional Gaussian measure.\n\nMany of the basic concepts of Fourier analysis on the Boolean cube have counterparts in Gaussian space:\n\n\nGaussian space is more symmetric than the Boolean cube (for example, it is rotation invariant), and supports continuous arguments which may be harder to get through in the discrete setting of the Boolean cube. The invariance principle links the two settings, and allows deducing results on the Boolean cube from results on Gaussian space.\n\nIf formula_54 has degree at most 1, then formula_5 is either constant, equal to a coordinate, or equal to the negation of a coordinate. In particular, formula_5 is a \"dictatorship\": a function depending on at most one coordinate.\n\nThe Friedgut–Kalai–Naor theorem, also known as the \"FKN theorem\", states that if formula_5 \"almost\" has degree 1 then it is \"close\" to a dictatorship. Quantitatively, if formula_54 and formula_59, then formula_5 is formula_61-close to a dictatorship, that is, formula_62 for some Boolean dictatorship formula_63, or equivalently, formula_64 for some Boolean dictatorship formula_63.\n\nSimilarly, a Boolean function of degree at most formula_27 depends on at most formula_67 coordinates, making it a \"junta\" (a function depending on a constant number of coordinates). The Kindler–Safra theorem generalizes the Friedgut–Kalai–Naor theorem to this setting. It states that if formula_54 satisfies formula_69 then formula_5 is formula_61-close to a Boolean function of degree at most formula_27.\n\nThe Poincaré inequality for the Boolean cube (which follows from formulas appearing above) states that for a function formula_11,\n\nThis implies that formula_75.\n\nThe Kahn–Kalai–Linial theorem, also known as the \"KKL theorem\", states that if formula_5 is Boolean then formula_77.\n\nThe bound given by the Kahn–Kalai–Linial theorem is tight, and is achieved by the \"Tribes\" function of Ben-Or and Linial:\n\nThe Kahn–Kalai–Linial theorem was one of the first results in the area, and was the one introducing hypercontractivity into the context of Boolean functions.\n\nIf formula_54 is an formula_80-junta (a function depending on at most formula_80 coordinates) then formula_82 according to the Poincaré inequality.\n\nFriedgut's theorem is a converse to this result. It states that for any formula_83, the function formula_5 is formula_85-close to a Boolean junta depending on formula_86 coordinates.\n\nCombined with the Russo–Margulis lemma, Friedgut's junta theorem implies that for every formula_42, every monotone function is close to a junta with respect to formula_88 for some formula_89.\n\nThe invariance principle generalizes the Berry–Esseen theorem to non-linear functions.\n\nThe Berry–Esseen theorem states (among else) that if formula_90 and no formula_91 is too large compared to the rest, then the distribution of formula_5 over formula_2 is close to a normal distribution with the same mean and variance.\n\nThe invariance principle (in a special case) informally states that if formula_5 is a multilinear polynomial of bounded degree over formula_95 and all influences of formula_5 are small, then the distribution of formula_5 under the uniform measure over formula_2 is close to its distribution in Gaussian space.\n\nMore formally, let formula_99 be a univariate Lipschitz function, let formula_100, let formula_101, and let\nformula_102. Suppose that formula_103. Then\n\nBy choosing appropriate formula_99, this implies that the distributions of formula_5 under both measures are close in CDF distance, which is given by formula_107.\n\nThe invariance principle was the key ingredient in the original proof of the \"Majority is Stablest\" theorem.\n\nA Boolean function formula_54 is \"linear\" if it satisfies formula_109, where formula_110. It is not hard to show that the Boolean linear functions are exactly the characters formula_17.\n\nIn property testing we want to test whether a given function is linear. It is natural to try the following test: choose formula_112 uniformly at random, and check that formula_109. If formula_5 is linear then it always passes the test. Blum, Luby and Rubinfeld showed that if the test passes with probability formula_115 then formula_5 is formula_61-close to a Fourier character. Their proof was combinatorial.\n\nBellare et al. gave an extremely simple Fourier-analytic proof, that also shows that if the test succeeds with probability formula_118, then formula_5 is correlated with a Fourier character. Their proof relies on the following formula for the success probability of the test:\n\nArrow's impossibility theorem states that for three and more candidates, the only unanimous voting rule for which there is always a Condorcet winner is a dictatorship.\n\nThe usual proof of Arrow's theorem is combinatorial. Kalai gave an alternative proof of this result in the case of three candidates using Fourier analysis. If formula_54 is the rule that assigns a winner among two candidates given their relative orders in the votes, then the probability that there is a Condorcet winner given a uniformly random vote is formula_122, from which the theorem easily follows.\n\nThe FKN theorem implies that if formula_5 is a rule for which there is almost always a Condorcet winner, then formula_5 is close to a dictatorship.\n\nA classical result in the theory of random graphs states that the probability that a formula_125 random graph is connected tends to formula_126 if formula_127. This is an example of a \"sharp threshold\": the width of the \"threshold window\", which is formula_128, is asymptotically smaller than the threshold itself, which is roughly formula_129. In contrast, the probability that a formula_125 graph contains a triangle tends to formula_131 when formula_132. Here both the threshold window and the threshold itself are formula_133, and so this is a \"coarse threshold\".\n\nFriedgut's sharp threshold theorem states, roughly speaking, that a monotone graph property (a graph property is a property which doesn't depend on the names of the vertices) has a sharp threshold unless it is correlated with the appearance of small subgraphs. This theorem has been widely applied to analyze random graphs and percolation.\n\nOn a related note, the KKL theorem implies that the width of threshold window is always at most formula_134.\n\nLet formula_135 denote the majority function on formula_48 coordinates. Sheppard's formula gives the asymptotic noise stability of majority:\n\nThis is related to the probability that if we choose formula_138 uniformly at random and form formula_139 by flipping each bit of formula_140 with probability formula_141, then the majority stays the same:\n\nThere are Boolean functions with larger noise stability. For example, a dictatorship formula_143 has noise stability formula_53.\n\nThe Majority is Stablest theorem states, informally, then the only functions having noise stability larger than majority have influential coordinates. Formally, for every formula_83 there exists formula_146 such that if formula_54 has expectation zero and formula_148, then formula_149.\n\nThe first proof of this theorem used the invariance principle in conjunction with an isoperimetric theorem of Borell in Gaussian space; since then more direct proofs were devised.\n\nMajority is Stablest implies that the Goemans–Williamson approximation algorithm for MAX-CUT is optimal, assuming the unique games conjecture. This implication, due to Khot et al., was the impetus behind proving the theorem.\n"}
{"id": "3443011", "url": "https://en.wikipedia.org/wiki?curid=3443011", "title": "Antiunitary operator", "text": "Antiunitary operator\n\nIn mathematics, an antiunitary transformation, is a bijective antilinear map\n\nbetween two complex Hilbert spaces such that \n\nfor all formula_3 and formula_4 in formula_5, where the horizontal bar represents the complex conjugate. If additionally one has formula_6 then U is called an antiunitary operator.\n\nAntiunitary operators are important in Quantum Theory because they are used to represent certain symmetries, such as time-reversal symmetry. Their fundamental importance in quantum physics is further demonstrated by Wigner's Theorem. \n\nIn Quantum mechanics, the invariance transformations of complex Hilbert space formula_7 leave the absolute value of scalar product invariant:\n\nfor all formula_3 and formula_4 in formula_11.\nDue to Wigner's Theorem these transformations fall into two categories, they can be unitary or antiunitary. \n\nCongruences of the plane form two distinct classes. The first conserves the orientation and is generated by translations and rotations. The second does not conserve the orientation and is obtained from the first class by applying a reflection. On the complex plane these two classes corresponds (up to translation) to unitaries and antiunitaries, respectively.\n\n\nwhere formula_31 is the second Pauli matrix and formula_20 is the complex conjugate operator, is antiunitary. It satisfies formula_33.\n\nAn antiunitary operator on a finite-dimensional space may be decomposed as a direct sum of elementary Wigner antiunitaries formula_34, formula_35. The operator formula_36 is just simple complex conjugation on formula_37\n\nFor formula_39, the operator formula_34 acts on two-dimensional complex Hilbert space. It is defined by \n\nNote that for formula_39\n\nso such formula_34 may not be further decomposed into formula_45's, which square to the identity map.\n\nNote that the above decomposition of antiunitary operators contrasts with the spectral decomposition of unitary operators. In particular, a unitary operator on a complex Hilbert space may be decomposed into a direct sum of unitaries acting on 1-dimensional complex spaces (eigenspaces), but an antiunitary operator may only be decomposed into a direct sum of elementary operators on 1- and 2-dimensional complex spaces.\n\n\n"}
{"id": "2328909", "url": "https://en.wikipedia.org/wiki?curid=2328909", "title": "Architectural rendering", "text": "Architectural rendering\n\nArchitectural rendering, or architectural illustration, is the art of creating two-dimensional images or animations showing the attributes of a proposed architectural design.\n\nImages that are generated by a computer using three-dimensional modeling software or other computer software for presentation purposes are commonly termed \"Computer Generated Renderings\". Rendering techniques vary. Some methods create simple flat images or images with basic shadows. A popular technique uses sophisticated software to approximate accurate lighting and materials. This technique is often referred to as a \"Photo Real\" rendering. Renderings are usually created for presentation, marketing and design analysis purposes.\n\n\n3D renderings play a major role in real estate marketing and sales. It also makes it possible to make design related decisions well before the building is actually built. Thus it helps experimenting with building design and its visual aspects.\n\nArchitectural renderings are often categorized into 3 sub-types: Exterior Renderings, Interior Renderings, and Aerial Renderings. \n\nExterior renderings are defined as images where the vantage point or viewing angle is located outside of the building, while interior renderings are defined as images where the vantage point or viewing angle is located inside of the building. Aerial renderings are similar to exterior renderings however their viewing angle is located outside and above the building, looking down, usually at an angle.\n\nUntil 3D computer modeling became common, most architectural renderings were created by hand. There are still many architectural illustrators who create renderings entirely by hand. Some hand illustrators use a combination of hand and computer generated linework. Common mediums for hand architectural renderings include:\n\n\n\nTraditionally rendering techniques were taught in a \"master class\" practice (such as the École des Beaux-Arts), where a student works creatively with a mentor in the study of fine arts. Contemporary architects use hand-drawn sketches, pen and ink drawings, and watercolor renderings to represent their design with the vision of an artist. Computer generated graphics is the newest medium to be utilized by architectural illustrators.\n\n"}
{"id": "34948394", "url": "https://en.wikipedia.org/wiki?curid=34948394", "title": "Beez's theorem", "text": "Beez's theorem\n\nIn mathematics, Beez's theorem, introduced by Richard Beez in 1875, implies that if \"n\" > 3 then an (\"n\" – 1)-dimensional hypersurface in R cannot be deformed.\n"}
{"id": "43948116", "url": "https://en.wikipedia.org/wiki?curid=43948116", "title": "Borel's theorem", "text": "Borel's theorem\n\nIn topology, a branch of mathematics, Borel's theorem, due to Armand Borel, says the cohomology ring of a classifying space or a classifying stack is a polynomial ring.\n\n\n"}
{"id": "162265", "url": "https://en.wikipedia.org/wiki?curid=162265", "title": "Chris Freiling", "text": "Chris Freiling\n\nChristopher Francis Freiling is a mathematician responsible for Freiling's axiom of symmetry in set theory. He has also made significant contributions to coding theory, in the process establishing connections between that field and matroid theory.\n\nFreiling obtained his Ph.D. in 1981 from the University of California, Los Angeles under the supervision of Donald A. Martin. He is a member of the faculty of the Department of Mathematics at California State University, San Bernardino.\n\n\n"}
{"id": "3237201", "url": "https://en.wikipedia.org/wiki?curid=3237201", "title": "Convex analysis", "text": "Convex analysis\n\nConvex analysis is the branch of mathematics devoted to the study of properties of convex functions and convex sets, often with applications in convex minimization, a subdomain of optimization theory.\n\nA convex set is a set \"C\" ⊆ \"X\", for some vector space \"X\", such that for any \"x\", \"y\" ∈ \"C\" and λ ∈ [0, 1] then\n\nA convex function is any extended real-valued function \"f\" : \"X\" → R ∪ {±∞} which satisfies Jensen's inequality, i.e. for any \"x\", \"y\" ∈ \"X\" and any λ ∈ [0, 1] then\n\nEquivalently, a convex function is any (extended) real valued function such that its epigraph\nis a convex set.\n\nThe convex conjugate of an extended real-valued (not necessarily convex) function \"f\" : \"X\" → R ∪ {±∞} is \"f*\" : \"X*\" → R ∪ {±∞} where \"X*\" is the dual space of \"X\", and\n\nThe \"biconjugate\" of a function \"f\" : \"X\" → R ∪ {±∞} is the conjugate of the conjugate, typically written as \"f**\" : \"X\" → R ∪ {±∞}. The biconjugate is useful for showing when strong or weak duality hold (via the perturbation function).\n\nFor any \"x\" ∈ \"X\" the inequality \"f**\"(\"x\") ≤ \"f\"(\"x\") follows from the \"Fenchel–Young inequality\". For proper functions, \"f\" = \"f**\" if and only if \"f\" is convex and lower semi-continuous by Fenchel–Moreau theorem.\n\nA convex minimization (primal) problem is one of the form\n\nsuch that \"f\" : \"X\" → R ∪ {±∞} is a convex function and \"M\" ⊆ \"X\" is a convex set.\n\nIn optimization theory, the \"duality principle\" states that optimization problems may be viewed from either of two perspectives, the primal problem or the dual problem.\n\nIn general given two dual pairs separated locally convex spaces (\"X\", \"X*\") and (\"Y\", \"Y*\"). Then given the function \"f\" : \"X\" → R ∪ {+∞}, we can define the primal problem as finding \"x\" such that\n\nIf there are constraint conditions, these can be built into the function \"f\" by letting formula_7 where \"I\" is the indicator function. Then let \"F\" : \"X\" × \"Y\" → R ∪ {±∞} be a perturbation function such that \"F\"(\"x\", 0) = \"f\"(\"x\").\n\nThe \"dual problem\" with respect to the chosen perturbation function is given by\n\nwhere \"F*\" is the convex conjugate in both variables of \"F\".\n\nThe duality gap is the difference of the right and left hand sides of the inequality\n\nThis principle is the same as weak duality. If the two sides are equal to each other, then the problem is said to satisfy strong duality.\n\nThere are many conditions for strong duality to hold such as:\n\nFor a convex minimization problem with inequality constraints,\n\nthe Lagrangian dual problem is\n\nwhere the objective function \"L\"(\"x\", \"u\") is the Lagrange dual function defined as follows:\n\n\n"}
{"id": "755300", "url": "https://en.wikipedia.org/wiki?curid=755300", "title": "Curvilinear coordinates", "text": "Curvilinear coordinates\n\nIn geometry, curvilinear coordinates are a coordinate system for Euclidean space in which the coordinate lines may be curved. Commonly used curvilinear coordinate systems include: rectangular, spherical, and cylindrical coordinate systems. These coordinates may be derived from a set of Cartesian coordinates by using a transformation that is locally invertible (a one-to-one map) at each point. This means that one can convert a point given in a Cartesian coordinate system to its curvilinear coordinates and back. The name \"curvilinear coordinates\", coined by the French mathematician Lamé, derives from the fact that the coordinate surfaces of the curvilinear systems are curved.\n\nWell-known examples of curvilinear coordinate systems in three-dimensional Euclidean space (R) are Cartesian, cylindrical and spherical polar coordinates. A Cartesian coordinate surface in this space is a coordinate plane; for example \"z\" = 0 defines the \"x\"-\"y\" plane. In the same space, the coordinate surface \"r\" = 1 in spherical polar coordinates is the surface of a unit sphere, which is curved. The formalism of curvilinear coordinates provides a unified and general description of the standard coordinate systems.\n\nCurvilinear coordinates are often used to define the location or distribution of physical quantities which may be, for example, scalars, vectors, or tensors. Mathematical expressions involving these quantities in vector calculus and tensor analysis (such as the gradient, divergence, curl, and Laplacian) can be transformed from one coordinate system to another, according to transformation rules for scalars, vectors, and tensors. Such expressions then become valid for any curvilinear coordinate system.\n\nDepending on the application, a curvilinear coordinate system may be simpler to use than the Cartesian coordinate system. For instance, a physical problem with spherical symmetry defined in R (for example, motion of particles under the influence of central forces) is usually easier to solve in spherical polar coordinates than in Cartesian coordinates. Equations with boundary conditions that follow coordinate surfaces for a particular curvilinear coordinate system may be easier to solve in that system. One would for instance describe the motion of a particle in a rectangular box in Cartesian coordinates, whereas one would prefer spherical coordinates for a particle in a sphere. Spherical coordinates are one of the most used curvilinear coordinate systems in such fields as Earth sciences, cartography, and physics (in particular quantum mechanics, relativity), and engineering.\n\nFor now, consider 3d space. A point \"P\" in 3d space (or its position vector r) can be defined using Cartesian coordinates (\"x\", \"y\", \"z\") [equivalently written (\"x\", \"x\", \"x\")], by formula_1, where e, e, e are the \"standard basis vectors\".\n\nIt can also be defined by its curvilinear coordinates (\"q\", \"q\", \"q\") if this triplet of numbers defines a single point in an unambiguous way. The relation between the coordinates is then given by the invertible transformation functions:\n\nThe surfaces \"q\" = constant, \"q\" = constant, \"q\" = constant are called the coordinate surfaces; and the space curves formed by their intersection in pairs are called the coordinate curves. The coordinate axes are determined by the tangents to the coordinate curves at the intersection of three surfaces. They are not in general fixed directions in space, which happens to be the case for simple Cartesian coordinates, and thus there is generally no natural global basis for curvilinear coordinates.\n\nIn the Cartesian system, the standard basis vectors can be derived from the derivative of the location of point \"P\" with respect to the local coordinate\n\nApplying the same derivatives to the curvilinear system locally at point \"P\" defines the natural basis vectors:\n\nSuch a basis, whose vectors change their direction and/or magnitude from point to point is called a local basis. All bases associated with curvilinear coordinates are necessarily local. Basis vectors that are the same at all points are global bases, and can be associated only with linear or affine coordinate systems.\n\nNote: for this article e is reserved for the standard basis (Cartesian) and h or b is for the curvilinear basis.\n\nThese may not have unit length, and may also not be orthogonal. In the case that they \"are\" orthogonal at all points where the derivatives are well-defined, we define the Lamé coefficients (after Gabriel Lamé) by\n\nand the curvilinear orthonormal basis vectors by\n\nIt is important to note that these basis vectors may well depend upon the position of \"P\"; it is therefore necessary that they are not assumed to be constant over a region. (They technically form a basis for the tangent bundle of formula_8 at \"P\", and so are local to \"P\".)\n\nIn general, curvilinear coordinates allow the natural basis vectors h not all mutually perpendicular to each other, and not required to be of unit length: they can be of arbitrary magnitude and direction. The use of an orthogonal basis makes vector manipulations simpler than for non-orthogonal. However, some areas of physics and engineering, particularly fluid mechanics and continuum mechanics, require non-orthogonal bases to describe deformations and fluid transport to account for complicated directional dependences of physical quantities. A discussion of the general case appears later on this page.\n\nIn orthogonal curvilinear coordinates, since the total differential change in r is\n\nso scale factors are formula_10\n\nIn non-orthogonal coordinates the length of formula_11 is the positive square root of formula_12 (with Einstein summation convention). The six independent scalar products \"g\"=h.h of the natural basis vectors generalize the three scale factors defined above for orthogonal coordinates. The nine \"g\" are the components of the metric tensor, which has only three non zero components in orthogonal coordinates: \"g\"=\"hh\", \"g\"=\"hh\", \"g\"=\"hh\".\n\nSpatial gradients, distances, time derivatives and scale factors are interrelated within a coordinate system by two groups of basis vectors:\n\nwhich transforms like covariant vectors (denoted by lowered indices), or\n\nwhich transforms like contravariant vectors (denoted by raised indices), ∇ is the del operator.\n\nConsequently, a general curvilinear coordinate system has two sets of basis vectors for every point: {b, b, b} is the covariant basis, and {b, b, b} is the contravariant (a.k.a. reciprocal) basis. The covariant and contravariant basis vectors types have identical direction for orthogonal curvilinear coordinate systems, but as usual have inverted units with respect to each other.\n\nNote the following important equality:\nwherein formula_15 denotes the generalized Kronecker delta.\n\nA vector v can be specified in terms either basis, i.e.,\n\nUsing the Einstein summation convention, the basis vectors relate to the components by\nand\nwhere \"g\" is the metric tensor (see below).\n\nA vector can be specified with covariant coordinates (lowered indices, written \"v\") or contravariant coordinates (raised indices, written \"v\"). From the above vector sums, it can be seen that contravariant coordinates are associated with covariant basis vectors, and covariant coordinates are associated with contravariant basis vectors.\n\nA key feature of the representation of vectors and tensors in terms of indexed components and basis vectors is \"invariance\" in the sense that vector components which transform in a covariant manner (or contravariant manner) are paired with basis vectors that transform in a contravariant manner (or covariant manner).\n\nConsider the one-dimensional curve shown in Fig. 3. At point \"P\", taken as an origin, \"x\" is one of the Cartesian coordinates, and \"q\" is one of the curvilinear coordinates. The local (non-unit) basis vector is b (notated h above, with b reserved for unit vectors) and it is built on the \"q\" axis which is a tangent to that coordinate line at the point \"P\". The axis \"q\" and thus the vector b form an angle formula_21 with the Cartesian \"x\" axis and the Cartesian basis vector e.\n\nIt can be seen from triangle \"PAB\" that\nwhere |e|, |b| are the magnitudes of the two basis vectors, i.e., the scalar intercepts \"PB\" and \"PA\". Note that \"PA\" is also the projection of b on the \"x\" axis.\n\nHowever, this method for basis vector transformations using \"directional cosines\" is inapplicable to curvilinear coordinates for the following reasons:\n\nThe angles that the \"q\" line and that axis form with the \"x\" axis become closer in value the closer one moves towards point \"P\" and become exactly equal at \"P\".\n\nLet point \"E\" be located very close to \"P\", so close that the distance \"PE\" is infinitesimally small. Then \"PE\" measured on the \"q\" axis almost coincides with \"PE\" measured on the \"q\" line. At the same time, the ratio \"PD/PE\" (\"PD\" being the projection of \"PE\" on the \"x\" axis) becomes almost exactly equal to formula_25.\n\nLet the infinitesimally small intercepts \"PD\" and \"PE\" be labelled, respectively, as \"dx\" and d\"q\". Then\n\nThus, the directional cosines can be substituted in transformations with the more exact ratios between infinitesimally small coordinate intercepts. It follows that the component (projection) of b on the \"x\" axis is\n\nIf \"q\" = \"q\"(\"x\", \"x\", \"x\") and \"x\" = \"x\"(\"q\", \"q\", \"q\") are smooth (continuously differentiable) functions the transformation ratios can be written as formula_28 and formula_29. That is, those ratios are partial derivatives of coordinates belonging to one system with respect to coordinates belonging to the other system.\n\nDoing the same for the coordinates in the other 2 dimensions, b can be expressed as:\nSimilar equations hold for b and b so that the standard basis {e, e, e} is transformed to a local (ordered and normalised) basis {b, b, b} by the following system of equations:\n\nBy analogous reasoning, one can obtain the inverse transformation from local basis to standard basis:\n\nThe above systems of linear equations can be written in matrix form using the Einstein summation convention as\n\nThis coefficient matrix of the linear system is the Jacobian matrix (and its inverse) of the transformation. These are the equations that can be used to transform a Cartesian basis into a curvilinear basis, and vice versa.\n\nIn three dimensions, the expanded forms of these matrices are\n\nIn the inverse transformation (second equation system), the unknowns are the curvilinear basis vectors. For any specific location there can only exist one and only one set of basis vectors (else the basis is not well defined at that point). This condition is satisfied if and only if the equation system has a single solution, from linear algebra, a linear equation system has a single solution (non-trivial) only if the determinant of its system matrix is non-zero:\nwhich shows the rationale behind the above requirement concerning the inverse Jacobian determinant.\n\nThe formalism extends to any finite dimension as follows.\n\nConsider the real Euclidean \"n\"-dimensional space, that is R = R × R × ... × R (\"n\" times) where R is the set of real numbers and × denotes the Cartesian product, which is a vector space.\n\nThe coordinates of this space can be denoted by: x = (\"x\", \"x\"...,\"x\"). Since this is a vector (an element of the vector space), it can be written as:\n\nwhere e = (1,0,0...,0), e = (0,1,0...,0), e = (0,0,1...,0)...,e = (0,0,0...,1) is the \"standard basis set of vectors\" for the space R, and \"i\" = 1, 2...\"n\" is an index labelling components. Each vector has exactly one component in each dimension (or \"axis\") and they are mutually orthogonal (perpendicular) and normalized (has unit magnitude).\n\nMore generally, we can define basis vectors b so that they depend on q = (\"q\", \"q\"...,\"q\"), i.e. they change from point to point: b = b(q). In which case to define the same point x in terms of this alternative basis: the \"coordinates\" with respect to this basis \"v\" also necessarily depend on x also, that is \"v\" = \"v\"(x). Then a vector v in this space, with respect to these alternative coordinates and basis vectors, can be expanded as a linear combination in this basis (which simply means to multiply each basis vector e by a number \"v\" – scalar multiplication):\n\nThe vector sum that describes v in the new basis is composed of different vectors, although the sum itself remains the same.\n\nFrom a more general and abstract perspective, a curvilinear coordinate system is simply a coordinate patch on the differentiable manifold E (n-dimensional Euclidean space) that is diffeomorphic to the Cartesian coordinate patch on the manifold. Note that two diffeomorphic coordinate patches on a differential manifold need not overlap differentiably. With this simple definition of a curvilinear coordinate system, all the results that follow below are simply applications of standard theorems in differential topology.\n\nThe transformation functions are such that there's a one-to-one relationship between points in the \"old\" and \"new\" coordinates, that is, those functions are bijections, and fulfil the following requirements within their domains:\n\nElementary vector and tensor algebra in curvilinear coordinates is used in some of the older scientific literature in mechanics and physics and can be indispensable to understanding work from the early and mid-1900s, for example the text by Green and Zerna. Some useful relations in the algebra of vectors and second-order tensors in curvilinear coordinates are given in this section. The notation and contents are primarily from Ogden, Naghdi, Simmonds, Green and Zerna, Basar and Weichert, and Ciarlet.\n\nA second-order tensor can be expressed as\nwhere formula_39 denotes the tensor product. The components \"S\" are called the contravariant components, \"S \" the mixed right-covariant components, \"S \" the mixed left-covariant components, and \"S\" the covariant components of the second-order tensor. The components of the second-order tensor are related by\n\nAt each point, one can construct a small line element , so the square of the length of the line element is the scalar product dx • dx and is called the metric of the space, given by:\n\nThe following portion of the above equation\nis a \"symmetric\" tensor called the fundamental (or metric) tensor of the Euclidean space in curvilinear coordinates.\n\nIndices can be raised and lowered by the metric:\n\nDefining the scale factors \"h\" by\n\ngives a relation between the metric tensor and the Lamé coefficients. Note also that\n\nwhere \"h\" are the Lamé coefficients. For an orthogonal basis we also have:\n\nIf we consider polar coordinates for R, note that\n(r, θ) are the curvilinear coordinates, and the Jacobian determinant of the transformation (\"r\",θ) → (\"r\" cos θ, \"r\" sin θ) is \"r\".\n\nThe orthogonal basis vectors are b = (cos θ, sin θ), b = (−sin θ, cos θ). The scale factors are \"h\" = 1 and \"h\"= \"r\". The fundamental tensor is \"g\" =1, \"g\" =\"r\", \"g\" = \"g\" =0.\n\nIn an orthonormal right-handed basis, the third-order alternating tensor is defined as\n\nIn a general curvilinear basis the same tensor may be expressed as\n\nIt can also be shown that\n\n\nwhere the comma denotes a partial derivative (see Ricci calculus). To express Γ in terms of \"g\" we note that\n\nSince\nusing these to rearrange the above relations gives\n\n\nThis implies that\n\nOther relations that follow are\n\nAdjustments need to be made in the calculation of line, surface and volume integrals. For simplicity, the following restricts to three dimensions and orthogonal curvilinear coordinates. However, the same arguments apply for \"n\"-dimensional spaces. When the coordinate system is not orthogonal, there are some additional terms in the expressions.\n\nSimmonds, in his book on tensor analysis, quotes Albert Einstein saying\n\nThe magic of this theory will hardly fail to impose itself on anybody who has truly understood it; it represents a genuine triumph of the method of absolute differential calculus, founded by Gauss, Riemann, Ricci, and Levi-Civita.\n\nVector and tensor calculus in general curvilinear coordinates is used in tensor analysis on four-dimensional curvilinear manifolds in general relativity, in the mechanics of curved shells, in examining the invariance properties of Maxwell's equations which has been of interest in metamaterials and in many other fields.\n\nSome useful relations in the calculus of vectors and second-order tensors in curvilinear coordinates are given in this section. The notation and contents are primarily from Ogden, Simmonds, Green and Zerna, Basar and Weichert, and Ciarlet.\n\nLet φ = φ(x) be a well defined scalar field and v = v(x) a well-defined vector field, and \"λ\", \"λ\"... be parameters of the coordinates\n\n = \\sqrt{ g_{ij}\\cfrac{\\partial q^i}{\\partial \\lambda}\\cfrac{\\partial q^j}{\\partial \\lambda}} = \\sqrt{h_{i}^2\\left(\\cfrac{\\partial q^i}{\\partial \\lambda}\\right)^2} </math>\n\nwhere formula_59 is the permutation symbol. In determinant form:\n\nThe expressions for the gradient, divergence, and Laplacian can be directly extended to \"n\"-dimensions, however the curl is only defined in 3d.\n\nThe vector field b is tangent to the \"q\" coordinate curve and forms a natural basis at each point on the curve. This basis, as discussed at the beginning of this article, is also called the covariant curvilinear basis. We can also define a reciprocal basis, or contravariant curvilinear basis, b. All the algebraic relations between the basis vectors, as discussed in the section on tensor algebra, apply for the natural basis and its reciprocal at each point x.\n\nBy definition, if a particle with no forces acting on it has its position expressed in an inertial coordinate system, (\"x\", \"x\", \"x\", \"t\"), then there it will have no acceleration (d\"x\"/d\"t\" = 0). In this context, a coordinate system can fail to be “inertial” either due to non-straight time axis or non-straight space axes (or both). In other words, the basis vectors of the coordinates may vary in time at fixed positions, or they may vary with position at fixed times, or both. When equations of motion are expressed in terms of any non-inertial coordinate system (in this sense), extra terms appear, called Christoffel symbols. Strictly speaking, these terms represent components of the absolute acceleration (in classical mechanics), but we may also choose to continue to regard d\"x\"/d\"t\" as the acceleration (as if the coordinates were inertial) and treat the extra terms as if they were forces, in which case they are called fictitious forces. The component of any such fictitious force normal to the path of the particle and in the plane of the path’s curvature is then called centrifugal force.\n\nThis more general context makes clear the correspondence between the concepts of centrifugal force in rotating coordinate systems and in stationary curvilinear coordinate systems. (Both of these concepts appear frequently in the literature.) For a simple example, consider a particle of mass \"m\" moving in a circle of radius \"r\" with angular speed \"w\" relative to a system of polar coordinates rotating with angular speed \"W\". The radial equation of motion is \"mr\"” = \"F\" + \"mr\"(\"w\" + \"W\"). Thus the centrifugal force is \"mr\" times the square of the absolute rotational speed \"A\" = \"w\" + \"W\" of the particle. If we choose a coordinate system rotating at the speed of the particle, then \"W\" = \"A\" and \"w\" = 0, in which case the centrifugal force is \"mrA\", whereas if we choose a stationary coordinate system we have \"W\" = 0 and \"w\" = \"A\", in which case the centrifugal force is again \"mrA\". The reason for this equality of results is that in both cases the basis vectors at the particle’s location are changing in time in exactly the same way. Hence these are really just two different ways of describing exactly the same thing, one description being in terms of rotating coordinates and the other being in terms of stationary curvilinear coordinates, both of which are non-inertial according to the more abstract meaning of that term.\n\nWhen describing general motion, the actual forces acting on a particle are often referred to the instantaneous osculating circle tangent to the path of motion, and this circle in the general case is not centered at a fixed location, and so the decomposition into centrifugal and Coriolis components is constantly changing. This is true regardless of whether the motion is described in terms of stationary or rotating coordinates.\n\n\n"}
{"id": "32940598", "url": "https://en.wikipedia.org/wiki?curid=32940598", "title": "Decimal data type", "text": "Decimal data type\n\nFractional numbers are supported on most programming languages as floating-point numbers or fixed-point numbers. However, such representations typically restrict the denominator to a power of two. Most decimal fractions (or most fractions in general) cannot be represented exactly as a fraction with a denominator that is a power of two. For example, the simple decimal fraction 0.3 (3/10) might be represented as 5404319552844595/18014398509481984 (0.299999999999999988897769...). This inexactness causes many problems that are familiar to experienced programmers. For example, the expression codice_1 might counterintuitively evaluate to false in some systems, due to the inexactness of the representation of decimals.\n\nAlthough all decimal fractions are fractions, and thus it is possible to use a rational data type to represent it exactly, it may be more convenient in many situations to consider only non-repeating decimal fractions (fractions whose denominator is a power of ten). For example, fractional units of currency worldwide are mostly based on a denominator that is a power of ten. Also, most fractional measurements in science are reported as decimal fractions, as opposed to fractions with any other system of denominators.\n\nA decimal data type could be implemented as either a floating-point number or as a fixed-point number. In the fixed-point case, the denominator would be set to a fixed power of ten. In the floating-point case, a variable exponent would represent the power of ten to which the mantissa of the number is multiplied.\n\nLanguages that support a rational data type usually allow the construction of such a value from a string, instead of a base-2 floating-point number, due to the loss of exactness the latter would cause. Usually the basic arithmetic operations ('+', '−', '×', '/', integer powers) and comparisons ('=', '<', '>', '≤') would be extended to act on them — either natively or through operator overloading facilities provided by the language. These operations may be translated by the compiler into a sequence of integer machine instructions, or into library calls. Support may also extend to other operations, such as formatting, rounding to an integer or floating point value, etc..\nAn example of this is 123.456\n\nIEEE 754 specifies three standard floating-point decimal data types of different precision:\n\n"}
{"id": "11814754", "url": "https://en.wikipedia.org/wiki?curid=11814754", "title": "Desmond Paul Henry", "text": "Desmond Paul Henry\n\nDesmond Paul Henry (1921–2004) was a Manchester University Lecturer and Reader in Philosophy (1949–82). He was one of the first British artists to experiment with machine-generated visual effects at the time of the emerging global computer art movement of the 1960s (The Cambridge Encyclopaedia 1990 p. 289; Levy 2006 pp. 178–180). During this period, Henry constructed a succession of three drawing machines from modified bombsight analogue computers which were employed in World War II bombers to calculate the accurate release of bombs onto their targets (O'Hanrahan 2005). Henry's machine-generated effects resemble complex versions of the abstract, curvilinear graphics which accompany Microsoft's Windows Media Player. Henry's machine-generated effects may therefore also be said to represent early examples of computer graphics: \"the making of line drawings with the aid of computers and drawing machines\". (Franke 1971, p. 41)\n\nDuring the 1970s Henry focussed on further developing his own unique photo-chemical techniques for the production of original visual effects. He went on to make a fourth and a fifth drawing machine in 1984 and 2002 respectively. These later machines however, were based on a mechanical pendulum design and not bombsight computers. (O'Hanrahan 2005)\n\nIt was thanks to artist L. S. Lowry, working in collaboration with the then director of Salford Art Gallery, A. Frape, that Henry's artistic career was launched in 1961 when he won a local competition at Salford Art gallery, entitled London Opportunity. The prize for winning this competition was a one-man exhibition show in London at the Reid Gallery. Lowry knew how crucial such a London show could be in bringing an artist to public attention. As one of the competition judges, Lowry visited Henry's home in Burford Drive, Manchester, to view his range of artistic work. (O'Hanrahan 2005)\n\nIt was at this London show of 1962, entitled Ideographs, that Henry's machine-generated effects were exhibited for the first time, along with pictures based upon Henry's photo-chemical techniques which had originally won him the competition prize. (O'Hanrahan 2005) It was this first exhibition of machine-produced effects which led to Henry and his first drawing machine being included in the first ever programme in the BBC's \"North at Six\" series and to his being approached by the American magazine Life. (O'Hanrahan 2005) Henry and his first drawing machine were to be featured in this magazine, but the article was scrapped following the assassination of US President John F. Kennedy. The generally positive response his pictures received reflects the zeitgeist of technological optimism of the 1960s. (O'Hanrahan 2005) The Guardian of 17/9/62 described the images produced by this first machine as being \"quite out of this world\" and \"almost impossible to produce by human hands\".\n\nHenry's machine-generated effects went on to be exhibited at various venues during the 1960s, the most major being Cybernetic Serendipity (1968) held at the Institute of Contemporary Arts (I.C.A) in London. This represented one of the most significant art and technology exhibitions of the decade. (Goodman 1987) In this exhibition not only the effects but also the drawing machine itself was included as an interactive exhibit. Cybernetic Serendipity then went on to tour the United States, where exhibition venues included the Corcoran Gallery in Washington and San Francisco's Palace of Fine Arts. (O'Hanrahan 2005)\n\nThis second machine returned from its tour of the United States in 1972 in a complete state of disrepair. (O'Hanrahan 2005) Such technical failures were not unusual in electric and motor-driven exhibition items. (Rosenburg 1972) More recently, frequent mechanical and/or electronic computer breakdowns contributed to the decision to close Artworks, (The Lowry, Salford Quays, Manchester, U.K) in March 2003 after only three years in operation as a permanent, technology-based, interactive exhibition. (O'Hanrahan 2005)\n\nThe main component of each Henry drawing machine was the bombsight computer. These mechanical analogue computers represented some of the most important technological advancements of World War Two. However, by the 1960s they already represented \"old\" technology when compared to the more modern digital computers then available. (O'Hanrahan 2005)\n\nThe mechanical analogue bombsight computer was employed in World War Two bomber aircraft to determine the exact moment bombs were to be released to hit their target. The bombardier entered information on air and wind speed, wind direction, altitude, angle of drift and bomb weight into the computer which then calculated the bomb release point, using a complex arrangement of gyros, motors, gears and a telescope. (Jacobs 1996)\nIt was in the early 1950s that Henry purchased his very first Sperry bombsight computer, in mint condition, from an army surplus warehouse in Shude Hill, Manchester. This purchase was inspired by Henry's lifelong passion for all things mechanical, which had been further fuelled by seven years serving as a technical clerk with the Royal Electrical and Mechanical Engineers during World War Two. (O'Hanrahan 2005) Henry so marvelled at the mechanical inner workings of this bombsight computer in motion, that he eventually decided to capture its \"peerless parabolas\" (as Henry termed its inner workings), on paper. He then modified the bombsight to create the first drawing machine of 1960. A second was constructed in 1963 and a third in 1967. (O'Hanrahan 2005) These machines created complex, abstract, asymmetrical, curvilinear, repetitive line drawings which were either left untouched as completed drawings or embellished by the artist's hand in response to the suggestive machine-generated effects. None of Henry's machines now remains in operational order. (O'Hanrahan 2005)\n\nEach Henry drawing machine was based around an analogue bombsight computer in combination with other components which Henry happened to have acquired for his home-based workshop in Whalley Range, Manchester. (O'Hanrahan 2005) Each machine took up to six weeks to construct and each drawing from between two hours to two days to complete. The drawing machines relied upon an external electric power source to operate either one or two servo motors which powered the synchronisation of suspended drawing implement(s) acting upon either a stationary or moving drawing table. (O'Hanrahan 2005) With the first drawing machine Henry employed biros as the mark-making implement; however with the machines that followed he preferred to use Indian ink in technical tube pens, since these effects, in contrast to biro ink, do not risk fading upon prolonged exposure to sunlight. (O'Hanrahan 2005)\n\nHenry's drawing machines were quite unlike the conventional computers of the 1960s since they could not be pre-programmed nor store information. (O'Hanrahan 2005) His machines relied instead, as did those of artist Jean Tinguely, upon a \"mechanics of chance\". (Pontus Hulten in Peiry 1997, p. 237) That is to say, they relied upon the chance relationship in the arrangement of each machine's mechanical components, the slightest alteration to which, (for example, a loosened screw), could dramatically impinge on the final result. In the words of Henry, he let each machine \"do its own thing\" in accordance with its \"sui generis\" mechanical features, with often surprising and unpredictable results. The imprecise way Henry's machines were both constructed and operated ensured that their effects could not be mass-produced and would be infinitely varied. (O'Hanrahan 2005)\n\nSuch imprecise tools as Henry's machines, have been judged by some to enhance artistic creativity as opposed to modern computer imaging software which leaves no scope for artistic intuition. (Reffin-Smith 1997) Nor could Henry's machines have been accused of preventing the artist from exercising aesthetic choice. They were truly interactive, like modern computer graphic manipulation software. With a Henry drawing machine, the artist had general overall control and was free to exercise personal and artistic intuition at any given moment of his choosing during the drawing production process. (O'Hanrahan 2005)\n\nBoth these elements of chance and interaction were in contrast to most other computer artists or graphic designers of the period, for whom the first stage in producing a digital computer graphic was to conceive the end product. The next stage was one where, \"mathematical formulae or geometric pattern manipulations (were) found to represent the desired lines. These were then programmed into a computer language, punched onto cards, and read into the computer\". (Sumner 1968 p. 11)\n\nIn 2001 Henry's machine-generated work was discussed in terms of the use made, since earliest times, of a range of tools for producing similar abstract, visual effects. (O'Hanrahan 2001) Once Henry himself had beheld the visual effects produced by his first machine, he then strove to find possible precursors such as the organic forms described in natural form mathematics. (D'Arcy-Thompson 1917; Cook 1914). Henry also compared his machine-generated effects to those produced using earlier scientific and mathematical instruments such as: Suardi's Geometric Pen of 1750 (Adams 1813), Pendulum Harmonographs (Goold et al., 1909) and the Geometric Lathe as used in ornamental and bank-note engraving. (Holtzapffel 1973 [1894])\n\nHis inclusion in 1968 in Cybernetic Serendipity enabled him to further contrast his machine-generated effects with similar though less complex and varied ones produced using a variety of tools. These included effects displayed on a visual display screen using a cathode-ray oscilloscope (Ben F. Laposky in Cybernetic Serendipity 1968) and those produced using a mechanical plotter linked to either a digital (Lloyd Sumner in Cybernetic Serendipity 1968) or analogue computer (Maughan S. Mason in Cybernetic Serendipity 1968). However Henry's drawing machines, in contrast to other precision mark-making instruments like the lathe and mechanical plotter, relied heavily upon the element of chance both in their construction and function. (O'Hanrahan 2005)\n\nHenry's introduction in 2001 to the aesthetic application of fractal mathematics (Briggs 1994[1992]) provided Henry with the necessary terms of reference for describing the chance-based operational aspects of his machines. Fractal mathematics could also help describe the aesthetic appreciation of his machine-generated effects or \"mechanical fractals\" (Henry 2002) as he came to term them. (O'Hanrahan 2005)\n\nFractal systems are produced by a dynamic, non-linear system of interdependent and interacting elements; in Henry's case, this is represented by the mechanisms and motions of the drawing machine itself. (O'Hanrahan 2005) In a fractal system, as in each Henry drawing machine, very small changes or adjustments to initial influences can have far-reaching effects.\n\nFractal images appeal to our intuitive aesthetic appreciation of order and chaos combined. Each Henry machine-produced drawing bears all the hallmarks of a fractal image since they embody regularity and repetition coupled with abrupt changes and discontinuities. (Briggs 1994[1992]) In other words, they exhibit self-similarity (similar details on different scales) and simultaneous order and chaos. These images also resemble fractal \"strange attractors\", since groups of curves present in the machine-generated effects tend to form clusters creating suggestive patterns. (Briggs 1994[1992])\n\nFractal patterns, similar to Henry's machine-generated effects, have been found to exist when plotting volcanic tremors, weather systems, the ECG of heart beats and the electroencephalographic data of brain activity. (Briggs 1994[1992])\n\nHenry found in fractals a means of both classifying his artistic activity and describing the aesthetic appreciation of his visual effects. Among the many artists who have previously employed what are now recognised as fractal images, are: \"Vincent van Gogh's dense swirls of energy around objects; the recursive geometries of Maritus Escher; the drip-paint, tangled abstractions of Jackson Pollock\". (Briggs 1994[1992] p. 166)\n\nSome would argue that scientific and technological advances have always influenced art in terms of its inspiration, tools and visual effects. In the words of Douglas Davis: \"Art can no more reject either technology or science than it can the world itself\". (Davis 1973, introduction) In his writings Henry himself often expressed his lifelong enthusiasm for fruitful collaborations between art and technology.(Henry: 1962, 1964, 1969, 1972)\n\nDuring the First Machine Age, prior to World War Two, enthusiasm for technological advances was expressed by the Machine Aesthetic which heralded the Modern Movement. (Banham 1960) Affiliated art movements of this time which shared aspects of the Machine Aesthetic included: Purism in France, Futurism in Italy (both of which celebrated the glories of modern machines and the excitement of speed), Suprematism, Productivism in Russia, Constructivism, Precisionism in North America and kinetic sculpture. (Meecham and Sheldon 2000)\n\nBy the 1960s, in the Second Machine Age, technology provided not only the inspiration for art production but above all its tools (Popper 1993), as reflected by the Art and Technology movement in the United States. Adherents to this movement employed only the very latest available computer equipment. In this early phase of computer art, programmers became artists and artists became programmers to experiment with the computer's creative possibilities (Darley, 1990). Since Henry worked in comparative artistic and scientific isolation, he did not have access to the latest technological innovations, in contrast to those working, for example, at the Massachusetts Institute of Technology. (O'Hanrahan 2005)\n\nBy the 1970s, the earlier enthusiasm for technology witnessed in the 60s gave way to the post-modern loss of faith in technology as its destructive effects, both in war and on the environment, became more apparent. (Lucie-Smith 1980) Goodman (1987) suggests that it is since 1978 that a second generation of computer artists may be recognised; a generation which no longer needs to be electronically knowledgeable or adept because the \"software does it for them\". (Goodman 1987, p. 47) This is in contrast to Henry who had to acquire the necessary knowledge and skills to manipulate and modify the components of the bombsight computers to construct the drawing machines. (O'Hanrahan 2005)\n\nDuring the 1980s, the application in computers of the microchip (developed by 1972) increased the affordability of a home computer and led to the development of interactive computer graphics programmes like Sketchpad and various Paintbox systems. (Darley 1991) During this period, computer art gave way almost completely to computer graphics as the computer's imaging capabilities became exploited both industrially and commercially and moved into entertainment related spheres, e.g.: Pixar, Lucas Films. (Goodman 1987) The computer once again became, for some, an undisputed artistic tool in its own right. (Goodman 1987)\n\nThis renewed enthusiasm in the computer's artistic possibilities has been further reflected by the emergence towards the end of the twentieth century of various forms of cyber, virtual, or digital art, examples of which include algorithmic art and fractal art. By the twenty-first century, digitally produced and/or manipulated images were exhibited in galleries as veritable works of art in their own right (O'Hanrahan 2005).\n\nHenry's drawing machines of the 1960s represented a remarkable innovation in the field of art and technology for a variety of reasons. Firstly, the bombsight analogue computer provided not only the inspiration but also the main tool for producing highly original visual effects. (O'Hanrahan 2005) Secondly, his machines' reliance on a mechanics of chance, as opposed to pre-determined computer programmes, ensured the unrepeatable and unique quality of his infinitely varied machine-generated effects or \"mechanical fractals\". (O'Hanrahan 2005) Thirdly, the spontaneous, interactive potential of his drawing machines' modus operandi pre-empted by some twenty years this particular aspect of later computer graphic manipulation software. (O'Hanrahan 2005)\n\nFinally, Henry was never artistically inspired by the graphic potential of the modern digital computer. (O'Hanrahan 2005) He much preferred the direct interaction afforded by the clearly visible interconnecting mechanical components of the earlier analogue computer and as a consequence of his drawing machines also. This was in stark contrast to the invisible and indirect workings of the later digital computer: \"the mechanical analogue computer, was a work of art in itself, involving a most beautiful arrangement of gears, belts, cams differentials and so on- it still retained in its working a visual attractiveness which has now vanished in the modern electronic counterpart; … I enjoyed seeing the machine work…\". (Henry, 1972)\n\nIn view of these considerations, Henry’s drawing machines may be said to not only reflect the early experimental phase of Computer Art and computer graphics but to also provide an important artistic and technological link between two distinct ages of the twentieth century: the earlier Mechanical/Industrial Age and the later Electronic/Digital Age. (O'Hanrahan 2005)\n\n\nAdams, George (1813), \"Geometrical and Graphical Essays\", W & S. Jones, London. (Courtesy of the Science Museum Library, London).\n\nBanham, Reyner (1996 [1960]), \"Theory and Design in the First Machine Age\", Architectural Press, Oxford.\n\nBriggs, John (1994[1992]), \"Fractals: the Patterns of Chaos\", Thames and Hudson, London.\n\n\"Cambridge Encyclopaedia\" (1990), Crystal, D. (ed.), \"Computer Art\" by David Manning, Cambridge University Press, Cambridge, p. 289.\n\nCook, Theodore (1979[1914]), \"The Curves of Life: An account of spiral formations and their application to the growth in nature, science and art\", Dover, New York.\n\n\"Cybernetic Serendipity\",[exh.cat] (1968). Reichardt, Jasia (ed.), Studio International, Special Issue, London.\n\nDarley, Andy (1990), \"From Abstraction to Simulation\" in Philip Hayward (ed.)(1994[1990]) \"Culture, Technology and Creativity in the Late Twentieth Century\", John Libbey & Company. London, pp. 39–64.\n\nDarley, Andy (1991), 'Big Screen, Little Screen' in Ten-8:vol.2, no.2: Digital Dialogues, (ed. Bishton), pp. 80–84.\n\nDavis, Douglas (1973) \"Art and The Future\", Praeger, New York.\n\nFranke, H.W (1971), \"Computer graphics, Computer Art\", Phaedon, Oxford, p. 41.\n\nGoodman, Cynthia (1987), Digital Visions: Computers and Art, Abrams, New York. \nGoold, J., Benham, C.E., Kerr, R., Wilberforce, L.R., (1909), \"Harmonic Vibrations\", Newton & Co., London.\n\nHenry, D.P. (1962), \"A New Project for Art\". Unpublished article submitted to Today 04/03/62.\n\nHenry, D.P. (1964), \"Art and Technology\", in Bulletin of the Philosophy of Science Group, Newman Association, No. 53.\n\nHenry, D.P (1969), \"The End or the Beginning?\" in Solem (Manchester Students' Union Magazine) pp. 25–27.\n\nHenry, D.P (1972), \"Computer graphics: a case study\". (lecture given to Aberdeen University art students).\n\nHoltzapffel, John Jacob (1973[1894]), \"The Principles and Practice of Ornamental or Complex Turning\", Dover, New York.\n\nJacobs, Peter (1996), \"The Lancaster Story\", Silverdale Books, Leicester.\n\nLevy, David (2006) \"Robots Unlimited-Life in a Virtual Age\", A.K.Peters, Wellesley, USA, pp. 178–180.\n\nLucie-Smith, Edward (1980), \"Art in the Seventies\", Phaedon, Oxford.\n\nMeecham, Pam and Sheldon, Julie (2000), \"Modern Art: A Critical Introduction\", Routledge, London.\n\nO’Hanrahan, Elaine (2001)(interview) \"Intercultural Drawing Practice: the Art School Response\" In Jagjit Chuhan, (ed.) (2001), \"Responses: Intercultural Drawing Practice\", Cornerhouse Publications, Manchester, pp.: 40–47.\n\nO’Hanrahan, Elaine (2005), \"Drawing Machines: The machine produced drawings of Dr. D. P. Henry in relation to conceptual and technological developments in machine-generated art (UK 1960–1968)\". Unpublished MPhil. Thesis. John Moores University, Liverpool.\n\nPeiry, Lucienne (1997), \"Art Brut- The Origins of Outsider Art\", Flammarion, Paris.\n\nPopper, Frank (1993), \"Art of the Electronic Age\", Thames and Hudson, London.\n\nReffin-Smith, Brian (1997), \"Post-modem Art, or: Virtual Reality as Trojan Donkey, or: horsetail tartan literature groin art\" in Stuart Mealing (ed.) (1997) Computers and Art, Intellect Books, Bristol, pp. 97–117.\n\nRosenberg, Harold (1972), \"The De-definition of Art\", University of Chicago Press, Chicago.\n\nSumner, Lloyd (1968), \"Computer Art and Human Response\", Paul B. Victorius, Charlottesville, Virginia.\n\nThompson, D'Arcy Wentworth (1961[1917]), \"On Growth and Form\", Cambridge University Press, Cambridge .\n\n"}
{"id": "16627059", "url": "https://en.wikipedia.org/wiki?curid=16627059", "title": "Dual impedance", "text": "Dual impedance\n\nDual impedance and dual network are terms used in electronic network analysis. The dual of an impedance formula_1 is its reciprocal, or algebraic inverse formula_2. For this reason the dual impedance is also called the inverse impedance. Another way of stating this is that the dual of formula_1 is the admittance formula_4.\n\nThe dual of a network is the network whose impedances are the duals of the original impedances. In the case of a black-box network with multiple ports, the impedance looking into each port must be the dual of the impedance of the corresponding port of the dual network.\n\nThis is consistent with the general notion duality of electric circuits, where the voltage and current are interchanged, etc., since formula_5 yields formula_6\n\nIn physical units, the dual is taken with respect to some nominal or characteristic impedance. To do this, Z and Z' are scaled to the nominal impedance Z so that\n\nZ is usually taken to be a purely real number R, so Z' is changed by a real factor of R. In other words, the dual circuit is qualitatively the same circuit but all the component values are scaled by R. The scaling factor R has the dimensions of Ω, so the constant 1 in the unitless expression would actually be assigned the dimensions Ω in a dimensional analysis.\n\nThere is a graphical method of obtaining the dual of a network which is often easier to use than the mathematical expression for the impedance. Starting with a circuit diagram of the network in question, Z, the following steps are drawn on the diagram to produce Z' superimposed on top of Z. Typically, Z' will be drawn in a different colour to help distinguish it from the original, or, if using computer-aided design, Z' can be drawn on a different layer.\n\n\nThis completes the drawing of Z'. This method also serves to demonstrate that the dual of a mesh transforms into a node and the dual of a node transforms into a mesh. Two examples are given below.\n\nIt is now clear that the dual of a star network of inductors is a delta network of capacitors. This dual circuit is not the same thing as a star-delta (Y-Δ) transformation. A Y-Δ transform results in an \"equivalent\" circuit, not a dual circuit.\n\nFilters designed using Cauer's topology of the first form are low-pass filters consisting of a ladder network of series inductors and shunt capacitors.\n\nIt can now be seen that the dual of a Cauer low-pass filter is still a Cauer low-pass filter. It does not transform into a high-pass filter as might have been expected. Note, however, that the first element is now a shunt component instead of a series component.\n\n\n"}
{"id": "44807292", "url": "https://en.wikipedia.org/wiki?curid=44807292", "title": "Ellen Kirkman", "text": "Ellen Kirkman\n\nEllen Elizabeth Kirkman is professor of mathematics at Wake Forest University. Her research interests include noncommutative algebra, representation theory, and homological algebra.\n\nShe received her Ph.D. in Mathematics and M.A. in Statistics from Michigan State University in 1975.\nHer doctoral dissertation, \"On the Characterization of Inertial Coefficient Rings\", was supervised by Edward C. Ingraham.\n\nSince 2012 she has been treasurer of the Association for Women in Mathematics.\n\nKirkman's professional activities include serving on the American Mathematical Society (AMS) Nominating Committee 2009–11, as an Mathematical Association of America (MAA) Governor 2006–8, on the Joint Data Committee of AMS-ASA-MAA-IMS-SIAM (2000– 2007 and 2009–present), directing the CBMS 2010 survey of undergraduate mathematical sciences programs, and involvement in several EDGE programs. She is an associate editor of Communications in Algebra.\n\nIn 2012, Kirkman became a fellow of the American Mathematical Society.\nShe is part of the 2019 class of fellows of the Association for Women in Mathematics.\nShe has also received service awards from Wake Forest University and the Southeastern Section of the MAA.\n"}
{"id": "715365", "url": "https://en.wikipedia.org/wiki?curid=715365", "title": "Euclid and his Modern Rivals", "text": "Euclid and his Modern Rivals\n\nEuclid and his Modern Rivals is a mathematical book published in 1879 by the English mathematician Charles Lutwidge Dodgson (1832–1898), better known under his literary pseudnym \"Lewis Carroll\". It considers the pedagogic merit of thirteen contemporary geometry textbooks, demonstrating how each in turn is either inferior to or functionally identical to Euclid's \"Elements\".\n\nIn it Dodgson supports using Euclid's geometry textbook \"The Elements\" as the geometry textbook in schools against more modern geometry textbooks that were replacing it, advocated by the Association for the Improvement of Geometrical Teaching, satirized in the book as the \"Association for the Improvement of Things in General\". Euclid's ghost returns in the play to defend his book against its modern rivals and tries to demonstrate how all of them are inferior to his book.\n\nDespite its scholarly subject and content, the work takes the form of a whimsical dialogue, principally between a mathematician named Minos (taken from Minos, judge of the underworld in Greek mythology) and a \"devil's advocate\" named Professor Niemand (German for 'nobody') who represents the \"Modern Rivals\" of the title.\n\nA quote from the preface of this book was used in the , which was kept in use for eight months, during the course of 2001. Due to the fisheye effect, only part of the text can be read:\n\n"}
{"id": "2795027", "url": "https://en.wikipedia.org/wiki?curid=2795027", "title": "Exploded-view drawing", "text": "Exploded-view drawing\n\nAn exploded view drawing is a diagram, picture, schematic or technical drawing of an object, that shows the relationship or order of assembly of various parts.\n\nIt shows the components of an object slightly separated by distance, or suspended in surrounding space in the case of a three-dimensional exploded diagram. An object is represented as if there had been a small controlled explosion emanating from the middle of the object, causing the object's parts to be separated an equal distance away from their original locations.\n\nThe exploded view drawing is used in parts catalogs, assembly and maintenance manuals and other instructional material.\n\nThe projection of an exploded view is usually shown from above and slightly in diagonal from the left or right side of the drawing. (See exploded view drawing of a gear pump to the right: it is slightly from above and shown from the left side of the drawing in diagonal.)\n\nAn exploded view drawing is a type of drawing, that shows the intended assembly of mechanical or other parts. It shows all parts of the assembly and how they fit together. In mechanical systems usually the component closest to the center are assembled first, or is the main part in which the other parts get assembled. This drawing can also help to represent the disassembly of parts, where the parts on the outside normally get removed first.\n\nExploded diagrams are common in descriptive manuals showing parts placement, or parts contained in an assembly or sub-assembly. Usually such diagrams have the part identification number and a label indicating which part fills the particular position in the diagram. Many spreadsheet applications can automatically create exploded diagrams, such as exploded pie charts.\n\nIn patent drawings in an exploded views the separated parts should be embraced by a bracket, to show the relationship or order of assembly of various parts are permissible, see image. When an exploded view is shown in a figure that is on the same sheet as another figure, the exploded view should be placed in brackets.\n\nExploded views can also be used in architectural drawing, for example in the presentation of landscape design. An exploded view can create an image in which the elements are flying through the air above the architectural plan, almost like a cubist painting. The locations can be shadowed or dotted in the siteplan of the elements.\nTogether with the cutaway view the exploded view was among the many graphic inventions of the Renaissance, which were developed to clarify pictorial representation in a renewed naturalistic way. The exploded view can be traced back to the early fifteenth century notebooks of Marino Taccola (1382–1453), and were perfected by Francesco di Giorgio (1439–1502) and Leonardo da Vinci (1452–1519).\n\nOne of the first clearer examples of an exploded view was created by Leonardo in his design drawing of a reciprocating motion machine. Leonardo applied this method of presentation in several other studies, including those on human anatomy.\n\nThe term \"Exploded View Drawing\" emerged in the 1940s, and is one of the first times defined in 1965 as \"Three-dimensional (isometric) illustration that shows the mating relationships of parts, subassemblies, and higher assemblies. May also show the sequence of assembling or disassembling the detail parts.\"\n"}
{"id": "352709", "url": "https://en.wikipedia.org/wiki?curid=352709", "title": "Feistel cipher", "text": "Feistel cipher\n\nIn cryptography, a Feistel cipher is a symmetric structure used in the construction of block ciphers, named after the German-born physicist and cryptographer Horst Feistel who did pioneering research while working for IBM (USA); it is also commonly known as a Feistel network. A large proportion of block ciphers use the scheme, including the Data Encryption Standard (DES). The Feistel structure has the advantage that encryption and decryption operations are very similar, even identical in some cases, requiring only a reversal of the key schedule. Therefore, the size of the code or circuitry required to implement such a cipher is nearly halved.\n\nA Feistel network is an iterated cipher with an internal function called a round function.\n\nFeistel networks were first seen commercially in IBM's Lucifer cipher, designed by Horst Feistel and Don Coppersmith in 1973. Feistel networks gained respectability when the U.S. Federal Government adopted the DES (a cipher based on Lucifer, with changes made by the NSA). Like other components of the DES, the iterative nature of the Feistel construction makes implementing the cryptosystem in hardware easier (particularly on the hardware available at the time of DES's design).\n\nMany modern and also some old symmetric block ciphers are based on Feistel networks (e.g. GOST 28147-89 block cipher), and the structure and properties of Feistel ciphers have been extensively explored by cryptographers. Specifically, Michael Luby and Charles Rackoff analyzed the Feistel cipher construction, and proved that if the round function is a cryptographically secure pseudorandom function, with K used as the seed, then 3 rounds are sufficient to make the block cipher a pseudorandom permutation, while 4 rounds are sufficient to make it a \"strong\" pseudorandom permutation (which means that it remains pseudorandom even to an adversary who gets oracle access to its inverse permutation).\n\nBecause of this very important result of Luby and Rackoff, Feistel ciphers are sometimes called Luby–Rackoff block ciphers. Further theoretical work has generalized the construction somewhat, and given more precise bounds for security.\n\nLet formula_1 be the round function and let\nformula_2 be the sub-keys for the rounds formula_3 respectively.\n\nThen the basic operation is as follows:\n\nSplit the plaintext block into two equal pieces, (formula_4, formula_5)\n\nFor each round formula_6, compute\n\nThen the ciphertext is formula_9.\n\nDecryption of a ciphertext formula_9 is accomplished by computing for formula_11\n\nThen formula_14 is the plaintext again.\n\nOne advantage of the Feistel model compared to a substitution–permutation network is that the round function formula_15 does not have to be invertible.\n\nThe diagram illustrates both encryption and decryption. Note the reversal of the subkey order for decryption; this is the only difference between encryption and decryption.\n\nUnbalanced Feistel ciphers use a modified structure where formula_4 and formula_5 are not of equal lengths. The Skipjack cipher is an example of such a cipher. The Texas Instruments digital signature transponder uses a proprietary unbalanced Feistel cipher to perform challenge–response authentication.\n\nThe Thorp shuffle is an extreme case of an unbalanced Feistel cipher in which one side is a single bit. This has better provable security than a balanced Feistel cipher but requires more rounds.\n\nThe Feistel construction is also used in cryptographic algorithms other than block ciphers. For example, the optimal asymmetric encryption padding (OAEP) scheme uses a simple Feistel network to randomize ciphertexts in certain asymmetric key encryption schemes.\n\nA generalized Feistel algorithm can be used to create strong permutations on small domains of size not a power of two (see format-preserving encryption).\n\nWhether the entire cipher is a Feistel cipher or not, Feistel-like networks can be used as a component of a cipher's design. For example, MISTY1 is a Feistel cipher using a three-round Feistel network in its round function, Skipjack is a modified Feistel cipher using a Feistel network in its G permutation, and Threefish (part of Skein) is a non-Feistel block cipher that uses a Feistel-like MIX function.\n\nFeistel or modified Feistel:\n\n\n\nGeneralised Feistel:\n\n\n"}
{"id": "28268647", "url": "https://en.wikipedia.org/wiki?curid=28268647", "title": "Fundamental normality test", "text": "Fundamental normality test\n\nIn complex analysis, a mathematical discipline, the fundamental normality test gives sufficient conditions to test the normality of a family of analytic functions. It is another name for the stronger version of Montel's Theorem.\n\nLet formula_1 be a family of analytic functions defined on a domain formula_2. If there are two fixed complex numbers \"a\" and \"b\" that are omitted from the range of every \"ƒ\" ∈ formula_3, then formula_4 is a normal family on formula_2.\n\nThe proof relies on properties of the elliptic modular function and can be found here: \n"}
{"id": "17604036", "url": "https://en.wikipedia.org/wiki?curid=17604036", "title": "German Statutory Accident Insurance", "text": "German Statutory Accident Insurance\n\nGerman Statutory Accident Insurance is among the oldest branches of German social insurance. Occupational accident insurance was established in Germany by statute in 1884. \n\nIn 1871, the German Empire was founded again, at the end of the Franco-Prussian War. Formerly Chancellor of Prussia, Otto von Bismarck, now Chancellor of the new German Empire, introduced highly progressive welfare legislation by the standards of Europe at the time. (5, 6)\n\nOccupational accident insurance was established in Germany by statute in 1884. The agencies in charge of providing this form of insurance are the industrial and agricultural employers' liability funds as well as public sector accident insurance funds, which include both municipal accident insurance association and other accident funds. While employers'liability funds are organized according to industry, the public sector accident insurance funds are the most part organized regionally. (1, 2, 3)\n\nThe accident insurance funds govern themselves (self-administration) with equal representation divided between employers, entrepreneurs and employees. The organs of self-administration are the member’s assembly and executive board. This arrangement ensures that the interests of all participants are represented.\n\nThe legal basis for occupational accident insurance is formed by the German Social Code, in particular Book VII (SGB VII). (1, 2, 3)\n\nStatutory occupational accident insurance is among the oldest branches of German social insurance. By contrast with health, long-term care, pension and unemployment insurance, statutory occupational accident insurance is contribution-free for those insured. The costs for comprehensive insurance coverage for prevention, rehabilitation are borne by employers. For public-sector jobs, the federal, state and municipal governments carry the costs.\n\nThe contribution rates are determinates according to the pay-as-you-go principle, based on expenditures in prior years. This means that at the end of each fiscal year the statutory accident insurance funds allocate their expenditures among the member companies. The calculation basis is thus formed by actual financing needs, i.e. the allocation amount to be put aside, the wages and salaries of the insured and the hazard class of the particular industry concerned. For the municipal accident insurance associations and accidents funds, the contributions are based on the population, the number of insured persons, or wages and salaries. (1, 2, 3)\n\nEvery year some 1,400,000 accidents in the Federal Republic of Germany involving employees who are either working or on their way to or from work. These are joined by around 18,000 cases of recognized occupational illnesses and some 1, 5 million school accidents. For those affected, the consequences often entail wide-ranging changes in their way of life. Restoring these people’s health and, as far as possible, their ability to work is the task of statutory accident insurance.\n\nEvery employee and trainee is covered by statutory occupational accident insurance. In industry and agriculture the employer’s liability insurance fund (Berufsgenossenschaften) are responsible for accident insurance. Providing coverage in the public sector are the municipal accident insurance associations (Gemeindeunfallversicherungsverbände) and other public-sector accident funds.\n\nCoverage is provided for accidents at work or school or on the way to or from work or school- as well as for occupational illnesses. (1, 2, 3)\n\nStatutory occupational accident insurance has the task of undertaking measures to prevent job-related accidents and illnesses, as well as to protect workers from on-the-job hazards. In cases where occupational accidents or illnesses do occur, accident insurance provides assistance toward restoring the health and working ability of the persons involved and compensation to the insured persons or their persons or their survivors through the provision of cash benefits.\n\nThe primary mission of statutory occupational accident insurance according to the legislation is the use all means at its disposal to prevent occupational accidents and illness from occurring in the first place and to minimize potential job-related hazards. The focus is placed on advising companies in all matters having to do with industrial safety and health. This includes providing employers and employees with comprehensive instructions and guideline, as well as international media. The accident insurance agencies also hold free informational, motivational events on the subject of safety at work. (1, 3)\n\nIf an insured person has an accident at work or suffers from an occupational illness, statutory occupational accident insurance covers the resulting costs. This means that the insurance fund provides the best possible medical, occupational and social rehabilitation, as well as financial compensation if applicable.\n\nIn the event of an occupational accident or illness, statutory occupational accident insurance provides:\npayment for full medical treatment,\noccupational integration assistance (for example, retraining),\nsocial integration assistance and supplementary assistance, and\ncash benefits to the insured and their surviving independents.\n\nThe top priority of the accident insurance fund is to restore the health and ability to work of the insured person. Pensions are paid to fund members only when it is not possible to fully restore their ability to work, i.e. for those whose earning capacity is reduced by at least 20 percent. (1, 3)\n\n1. German Accident Insurance Organisation \n2. Federal association of agricultural Accident Insurance Funds (Bundesverband der landwirtschaftlichen Berufsgenossenschaften)\n3. German Social Security\n5. German Travel Agency\n6. Fact about Germany \n7. German International Culture \n8. Germany\n"}
{"id": "29407693", "url": "https://en.wikipedia.org/wiki?curid=29407693", "title": "Herbert Enderton", "text": "Herbert Enderton\n\nHerbert Bruce Enderton (April 15, 1936 – October 20, 2010) was a Professor Emeritus of Mathematics at UCLA and a former member of the faculties of Mathematics and of Logic and the Methodology of Science at the University of California, Berkeley.\n\nEnderton also contributed to recursion theory, the theory of definability, models of analysis, computational complexity, and the history of logic.\n\nHe earned his Ph.D. at Harvard in 1962. He was a member of the American Mathematical Society from 1961 until his death.\n\nHe lived in Santa Monica. He married his wife, Cathy, in 1961 and they had two sons; Eric and Bert.\n\nFrom 1980 to 2002 he was coordinating editor of the reviews section of the Association for Symbolic Logic's Journal of Symbolic Logic.\n\nHe died from leukemia in 2010.\n\n\n"}
{"id": "55620821", "url": "https://en.wikipedia.org/wiki?curid=55620821", "title": "Homersham Cox (mathematician)", "text": "Homersham Cox (mathematician)\n\nHomersham Cox (1857–1918) was an English mathematician.\n\nHe was the son of Homersham Cox (1821–1897) and brother of Harold Cox and was educated at Tonbridge School (1870-75). At Trinity College, Cambridge, he graduated B.A. as 4th wrangler in 1880, and M.A. in 1883. He became a fellow in 1881. \n\nCox wrote four papers applying algebra to physics, and then turned to mathematics education with a book on arithmetic in 1885. His \"Principles of Arithmetic\" included binary numbers, prime numbers, and permutations.\n\nContracted to teach mathematics at Muir Central College, Cox became a resident of Allahabad, Uttar Pradesh from 1891 to 1918.\n\n1881–1883 he published papers on non-Euclidean geometry.\n\nFor instance, in his 1881 paper (which was published in two parts in 1881 and 1882) he described homogeneous coordinates for hyperbolic geometry, now called Weierstrass coordinates of the hyperboloid model introduced by Wilhelm Killing (1879) and Henri Poincaré (1881)). Like Poincaré in 1881, Cox wrote the general Lorentz transformations leaving invariant the quadratic form formula_1, and in addition also for formula_2 (see History of Lorentz transformations#Cox). He also formulated the Lorentz boost which he described as a transfer of the origin in the hyperbolic plane, on page 194:\n\nSimilar formulas have been used by Gustav von Escherich in 1874, whom Cox mentions on page 186. In his 1882/1883 paper, which deals with Non-Euclidean geometry, quaternions and exterior algebra, he provided the following formula describing a transfer of point P to point Q in the hyperbolic plane, on page 86\n\ntogether with formula_5 with formula_6 for elliptic space, and formula_7 with formula_8 for parabolic space. On page 88, he identified all these cases as quaternion multiplications. The variant formula_9 is now called a hyperbolic number, the whole expression on the left can be used as a hyperbolic versor. Subsequently, that paper was described by Alfred North Whitehead (1898) as follows: \n\nIn 1891 Cox published a chain of theorems in Euclidean geometry of three dimensions:\n\n(i) In space of three dimensions take a point 0 through which pass sundry planes \"a, b, c, d, e\"...\n\n(ii) Each two planes intersect in a line through 0. On each such line a point is taken at random. The point on the line of intersection of the planes \"a\" and \"b\" will be called the point \"ab\".\n\n(iii) Three planes \"a, b, c\", give three points \"be, ac, ab\". These determine a plane. It will be called the plane \"abc\". Thus the planes \"a, b, c, abc\", form a tetrahedron with vertices \"be, ac, ab\", 0.\n\n(iv) Four planes \"a, b, c, d\", give four planes \"abc, abd, acd, bed\". It can be proved that these meet in a point. Call it the point \"abed\".\n\n(v) Five planes \"a, b, c, d, e\", give five points such as \"abed\". It can be proved that these lie in a plane. Call it the plane \"abede\".\n\n(vi) Six planes \"a, b, c, d, e, f\", give six planes such as \"abcde\". It can be proved that these lie in a plane. Call it the plane \"abcdef\".\nAnd so on indefinitely.\n\nThe theorem has been compared to Clifford's circle theorems since they both are an infinite chain of theorems. In 1941 Richmond argued that Cox's chain was superior:\n\nH. S. M. Coxeter derived Clifford's theorem by exchanging the arbitrary point on a line \"ab\" with an arbitrary sphere about 0 which then intersects \"ab\". The planes \"a, b, c\", ... intersect this sphere in circles which can be projected stereographically into a plane. The planar language of Cox then translates to the circles of Clifford.\n\nIn 1965 Cox's first three theorems were proven in Coxeter's textbook \"Introduction to Geometry\".\n"}
{"id": "27497399", "url": "https://en.wikipedia.org/wiki?curid=27497399", "title": "Jayme Luiz Szwarcfiter", "text": "Jayme Luiz Szwarcfiter\n\nJayme Luiz Szwarcfiter (born July 5, 1942 in Rio de Janeiro) is a computer scientist in Brazil.\n\nSzwarcfiter graduated in 1967 in electronic engineering from the Federal University of Rio de Janeiro (UFRJ). He received his MA in 1971 from COPPE. In 1975 he obtained his PhD in computer science from the University of Newcastle Upon Tyne, England, under supervision of Leslie Blackett Wilson. He is currently a professor emeritus at UFRJ. The Journal of the Brazilian Computer Society dedicated a special edition in 2001 to Szwarcfiter's major publications. Among others, he has written joint articles with Donald E. Knuth and Christos Papadimitriou.\n\nHe received the Award of Scientific Merit from the Brazilian Computer Society in 2005. In April 2006 he won the Almirante Álvaro Alberto prize in computer science, one of the most important academic recognitions in Brazil. Szwarcfiter is also one of the recipients of the Ordem Nacional do Mérito Científico (National Order of Scientific Merit).\nIn 2011, Prof. Szwarcfiter was elected a Member of the Brazilian Academy of Sciences.\n\n"}
{"id": "44012730", "url": "https://en.wikipedia.org/wiki?curid=44012730", "title": "Kosnita's theorem", "text": "Kosnita's theorem\n\nIn Euclidean geometry, Kosnita's theorem is a property of certain circles associated with an arbitrary triangle.\n\nLet formula_1 be an arbitrary triangle, formula_2 its circumcenter and formula_3 are the circumcenters of three triangles formula_4, formula_5, and formula_6 respectively. The theorem claims that the three straight lines formula_7, formula_8, and formula_9 are concurrent. This result was established by the Romanian mathematician Cezar Coşniţă (1910-1962).\n\nTheir point of concurrence is known as the triangle's Kosnita point (named by Rigby in 1997). It is the isogonal conjugate of the nine-point center. It is triangle center formula_10 in Clark Kimberling's list. This theorem is special case of Dao's theorem on six circumcenters associated with a cyclic hexagon in.\n"}
{"id": "20395096", "url": "https://en.wikipedia.org/wiki?curid=20395096", "title": "Lane (hash function)", "text": "Lane (hash function)\n\nLane is a cryptographic hash function submitted to the NIST hash function competition; it was designed by Sebastiaan Indesteege with contributions by Elena Andreeva, Christophe De Cannière, Orr Dunkelman, Emilia Käsper, Svetla Nikova, Bart Preneel and Elmar Tischhauser. It re-uses many components from AES in a custom construction. The authors claim performance of up to 25.66 cycles per byte on an Intel Core 2 Duo.\n\n"}
{"id": "58992", "url": "https://en.wikipedia.org/wiki?curid=58992", "title": "Linear-feedback shift register", "text": "Linear-feedback shift register\n\nIn computing, a linear-feedback shift register (LFSR) is a shift register whose input bit is a linear function of its previous state.\n\nThe most commonly used linear function of single bits is exclusive-or (XOR). Thus, an LFSR is most often a shift register whose input bit is driven by the XOR of some bits of the overall shift register value.\n\nThe initial value of the LFSR is called the seed, and because the operation of the register is deterministic, the stream of values produced by the register is completely determined by its current (or previous) state. Likewise, because the register has a finite number of possible states, it must eventually enter a repeating cycle. However, an LFSR with a well-chosen feedback function can produce a sequence of bits that appears random and has a very long cycle.\n\nApplications of LFSRs include generating pseudo-random numbers, pseudo-noise sequences, fast digital counters, and whitening sequences. Both hardware and software implementations of LFSRs are common.\n\nThe mathematics of a cyclic redundancy check, used to provide a quick check against transmission errors, are closely related to those of an LFSR.\n\nThe bit positions that affect the next state are called the taps. In the diagram the taps are [16,14,13,11]. The rightmost bit of the LFSR is called the output bit. The taps are XOR'd sequentially with the output bit and then fed back into the leftmost bit. The sequence of bits in the rightmost position is called the output stream.\n\nThe sequence of numbers generated by an LFSR or its XNOR counterpart can be considered a binary numeral system just as valid as Gray code or the natural binary code.\nThe arrangement of taps for feedback in an LFSR can be expressed in finite field arithmetic as a polynomial mod 2. This means that the coefficients of the polynomial must be 1s or 0s. This is called the feedback polynomial or reciprocal characteristic polynomial. For example, if the taps are at the 16th, 14th, 13th and 11th bits (as shown), the feedback polynomial is\n\nThe \"one\" in the polynomial does not correspond to a tap – it corresponds to the input to the first bit (i.e. \"x\", which is equivalent to 1). The powers of the terms represent the tapped bits, counting from the left. The first and last bits are always connected as an input and output tap respectively.\n\nThe LFSR is maximal-length if and only if the corresponding feedback polynomial is primitive. This means that the following conditions are necessary (but not sufficient):\n\nTables of primitive polynomials from which maximum-length LFSRs can be constructed are given below and in the references.\n\nThere can be more than one maximum-length tap sequence for a given LFSR length. Also, once one maximum-length tap sequence has been found, another automatically follows. If the tap sequence in an \"n\"-bit LFSR is , where the 0 corresponds to the \"x\" = 1 term, then the corresponding \"mirror\" sequence is . So the tap sequence has as its counterpart . Both give a maximum-length sequence.\n\nAn example in C is below:\n\nIf a fast parity or popcount operation is available, the feedback bit can be computed more efficiently as codice_1 or codice_2, effectively computing the dot product of the register with the characteristic polynomial.\n\nThis LFSR configuration is also known as standard, many-to-one or external XOR gates. The alternative Galois configuration is described in the next section.\n\nNamed after the French mathematician Évariste Galois, an LFSR in Galois configuration, which is also known as modular, internal XORs, or one-to-many LFSR, is an alternate structure that can generate the same output stream as a conventional LFSR (but offset in time). In the Galois configuration, when the system is clocked, bits that are not taps are shifted one position to the right unchanged. The taps, on the other hand, are XORed with the output bit before they are stored in the next position. The new output bit is the next input bit. The effect of this is that when the output bit is zero, all the bits in the register shift to the right unchanged, and the input bit becomes zero. When the output bit is one, the bits in the tap positions all flip (if they are 0, they become 1, and if they are 1, they become 0), and then the entire register is shifted to the right and the input bit becomes 1.\n\nTo generate the same output stream, the order of the taps is the \"counterpart\" (see above) of the order for the conventional LFSR, otherwise the stream will be in reverse. Note that the internal state of the LFSR is not necessarily the same. The Galois register shown has the same output stream as the Fibonacci register in the first section. A time offset exists between the streams, so a different startpoint will be needed to get the same output each cycle.\n\nBelow is a C code example for the 16-bit maximal-period Galois LFSR example in the figure:\nNote that\n\ncan also be written as\n\nwhich may produce more efficient code on some compilers.\nBinary Galois LFSRs like the ones shown above can be generalized to any \"q\"-ary alphabet {0, 1, ..., \"q\" − 1} (e.g., for binary, \"q\" = 2, and the alphabet is simply {0, 1}). In this case, the exclusive-or component is generalized to addition modulo-\"q\" (note that XOR is addition modulo 2), and the feedback bit (output bit) is multiplied (modulo-\"q\") by a \"q\"-ary value, which is constant for each specific tap point. Note that this is also a generalization of the binary case, where the feedback is multiplied by either 0 (no feedback, i.e., no tap) or 1 (feedback is present). Given an appropriate tap configuration, such LFSRs can be used to generate Galois fields for arbitrary prime values of \"q\".\n\nBinary LFSRs of both Fibonacci and Galois configurations can be expressed as linear functions using matrices in formula_2. Using the companion matrix of the characteristic polynomial of the LFSR and denoting the seed as a column vector formula_3, the state of the register in Fibonacci configuration after formula_4 steps is given by\n\nFor the Galois form, we have\n\nThese forms generalize naturally to arbitrary fields.\n\nThe following table lists maximal-length polynomials for shift-register lengths up to 24. Note that more than one maximal-length polynomial may exist for any given shift-register length. A list of alternative maximal-length polynomials for shift-register lengths 4–32 (beyond which it becomes unfeasible to store or transfer them) can be found here: http://www.ece.cmu.edu/~koopman/lfsr/index.html.\n\n\nLFSRs can be implemented in hardware, and this makes them useful in applications that require very fast generation of a pseudo-random sequence, such as direct-sequence spread spectrum radio. LFSRs have also been used for generating an approximation of white noise in various programmable sound generators.\n\nThe repeating sequence of states of an LFSR allows it to be used as a clock divider or as a counter when a non-binary sequence is acceptable, as is often the case where computer index or framing locations need to be machine-readable. LFSR counters have simpler feedback logic than natural binary counters or Gray-code counters, and therefore can operate at higher clock rates. However, it is necessary to ensure that the LFSR never enters an all-zeros state, for example by presetting it at start-up to any other state in the sequence.\nThe table of primitive polynomials shows how LFSRs can be arranged in Fibonacci or Galois form to give maximal periods. One can obtain any other period by adding to an LFSR that has a longer period some logic that shortens the sequence by skipping some states.\n\nLFSRs have long been used as pseudo-random number generators for use in stream ciphers (especially in military cryptography), due to the ease of construction from simple electromechanical or electronic circuits, long periods, and very uniformly distributed output streams. However, an LFSR is a linear system, leading to fairly easy cryptanalysis. For example, given a stretch of known plaintext and corresponding ciphertext, an attacker can intercept and recover a stretch of LFSR output stream used in the system described, and from that stretch of the output stream can construct an LFSR of minimal size that simulates the intended receiver by using the Berlekamp-Massey algorithm. This LFSR can then be fed the intercepted stretch of output stream to recover the remaining plaintext.\n\nThree general methods are employed to reduce this problem in LFSR-based stream ciphers:\n\nImportant LFSR-based stream ciphers include A5/1 and A5/2, used in GSM cell phones, E0, used in Bluetooth, and the shrinking generator. The A5/2 cipher has been broken and both A5/1 and E0 have serious weaknesses.\n\nThe linear feedback shift register has a strong relationship to linear congruential generators.\n\nLFSRs are used in circuit testing for test-pattern generation (for exhaustive testing, pseudo-random testing or pseudo-exhaustive testing) and for signature analysis.\n\nComplete LFSR are commonly used as pattern generators for exhaustive testing, since they cover all possible inputs for an \"n\"-input circuit. Maximal-length LFSRs and weighted LFSRs are widely used as pseudo-random test-pattern generators for pseudo-random test applications.\n\nIn built-in self-test (BIST) techniques, storing all the circuit outputs on chip is not possible, but the circuit output can be compressed to form a signature that will later be compared to the golden signature (of the good circuit) to detect faults. Since this compression is lossy, there is always a possibility that a faulty output also generates the same signature as the golden signature and the faults cannot be detected. This condition is called error masking or aliasing. BIST is accomplished with a multiple-input signature register (MISR or MSR), which is a type of LFSR. A standard LFSR has a single XOR or XNOR gate, where the input of the gate is connected to several \"taps\" and the output is connected to the input of the first flip-flop. A MISR has the same structure, but the input to every flip-flop is fed through an XOR/XNOR gate. For example, a 4-bit MISR has a 4-bit parallel output and a 4-bit parallel input. The input of the first flip-flop is XOR/XNORd with parallel input bit zero and the \"taps\". Every other flip-flop input is XOR/XNORd with the preceding flip-flop output and the corresponding parallel input bit. Consequently, the next state of the MISR depends on the last several states opposed to just the current state. Therefore, a MISR will always generate the same golden signature given that the input sequence is the same every time.\n\nTo prevent short repeating sequences (e.g., runs of 0s or 1s) from forming spectral lines that may complicate symbol tracking at the\nreceiver or interfere with other transmissions, the data bit sequence is combined with the output of a linear-feedback register before modulation and transmission. This scrambling is removed at the receiver after demodulation. When the LFSR runs at the same bit rate as the transmitted symbol stream, this technique is referred to as scrambling. When the LFSR runs considerably faster than the symbol stream, the LFSR-generated bit sequence is called \"chipping code\". The chipping code is combined with the data using exclusive or before transmitting using binary phase-shift keying or a similar modulation method. The resulting signal has a higher bandwidth than the data, and therefore this is a method of spread-spectrum communication. When used only for the spread-spectrum property, this technique is called direct-sequence spread spectrum; when used to distinguish several signals transmitted in the same channel at the same time and frequency, it is called code division multiple access.\n\nNeither scheme should be confused with encryption or encipherment; scrambling and spreading with LFSRs do \"not\" protect the information from eavesdropping. They are instead used to produce equivalent streams that possess convenient engineering properties to allow robust and efficient modulation and demodulation.\n\nDigital broadcasting systems that use linear-feedback registers:\n\nOther digital communications systems using LFSRs:\n\nLFSRs are also used in radio jamming systems to generate pseudo-random noise to raise the noise floor of a target communication system.\n\nThe German time signal DCF77, in addition to amplitude keying, employs phase-shift keying driven by a 9-stage LFSR to increase the accuracy of received time and the robustness of the data stream in the presence of noise.\n\n\n"}
{"id": "18263842", "url": "https://en.wikipedia.org/wiki?curid=18263842", "title": "Locally regular space", "text": "Locally regular space\n\nIn mathematics, particularly topology, a topological space \"X\" is locally regular if intuitively it looks locally like a regular space. More precisely, a locally regular space satisfies the property that each point of the space belongs to an open subset of the space that is regular under the subspace topology. \n\nA topological space \"X\" is said to be locally regular if and only if each point, \"x\", of \"X\" has a neighbourhood that is regular under the subspace topology. Equivalently, a space \"X\" is locally regular if and only if the collection of all open sets that are regular under the subspace topology forms a base for the topology on \"X\".\n\n\n"}
{"id": "13650583", "url": "https://en.wikipedia.org/wiki?curid=13650583", "title": "Malliavin's absolute continuity lemma", "text": "Malliavin's absolute continuity lemma\n\nIn mathematics — specifically, in measure theory — Malliavin's absolute continuity lemma is a result due to the French mathematician Paul Malliavin that plays a foundational rôle in the regularity (smoothness) theorems of the Malliavin calculus. Malliavin's lemma gives a sufficient condition for a finite Borel measure to be absolutely continuous with respect to Lebesgue measure.\n\nLet \"μ\" be a finite Borel measure on \"n\"-dimensional Euclidean space R. Suppose that, for every \"x\" ∈ R, there exists a constant \"C\" = \"C\"(\"x\") such that\n\nfor every \"C\" function \"φ\" : R → R with compact support. Then \"μ\" is absolutely continuous with respect to \"n\"-dimensional Lebesgue measure \"λ\" on R. In the above, D\"φ\"(\"y\") denotes the Fréchet derivative of \"φ\" at \"y\" and ||\"φ\"|| denotes the supremum norm of \"φ\".\n\n"}
{"id": "40203396", "url": "https://en.wikipedia.org/wiki?curid=40203396", "title": "Material inference", "text": "Material inference\n\nIn logic, inference is the process of deriving logical conclusions from premises known or assumed to be true. In checking a logical inference for formal and material validity, the meaning of only its logical vocabulary and of both its logical and extra-logical vocabulary\nis considered, respectively.\n\nFor example, the inference \"\"Socrates is a human, and each human must eventually die, therefore Socrates must eventually die\" is a formally valid inference; it remains valid if the nonlogical vocabulary \"Socrates\", \"is human\", and \"must eventually die\"\" is arbitrarily, but consistently replaced.\n\nIn contrast, the inference \"Montreal is north of New York, therefore New York is south of Montreal\" is materially valid only; its validity relies on the extra-logical relations \"is north of\" and \"is south of\"\" being converse to each other.\n\nClassical formal logic considers the above \"north/south\" inference as an enthymeme, that is, as an incomplete inference; it can be made formally valid by supplementing the tacitly used conversity relationship explicitly: \"Montreal is north of New York, and whenever a location x is north of a location y, then y is south of x; therefore New York is south of Montreal\".\n\nIn contrast, the notion of a material inference has been developed by Wilfrid Sellars in order to emphasize his view that such supplements are not necessary to obtain a correct argument.\n\nRobert Brandom adopted Sellars' view, arguing that everyday (practical) reasoning is usually non-monotonic, i.e. additional premises can turn a practically valid inference into an invalid one, e.g.\nTherefore, practically valid inference is different from formally valid inference (which is monotonic - the above argument that \"Socrates must eventually die\" cannot be challenged by whatever additional information), and should better be modelled by materially valid inference. While a classical logician could add a ceteris paribus clause to 1. to make it usable in formally valid inferences:\nHowever, Brandom doubts that the meaning of such a clause can be made explicit, and prefers to consider it as a hint to non-monotony rather than a miracle drug to establish monotony.\n\nMoreover, the \"match\" example shows that a typical everyday inference can hardly be ever made formally complete. In a similar way, Lewis Carroll's dialogue \"What the Tortoise Said to Achilles\" demonstrates that the attempt to make every inference fully complete can lead to an infinite regression.\n\nMaterial inference should not be confused with the following concepts, which refer to \"formal\", not material validity:\n\n"}
{"id": "22074859", "url": "https://en.wikipedia.org/wiki?curid=22074859", "title": "Maze solving algorithm", "text": "Maze solving algorithm\n\nThere are a number of different maze solving algorithms, that is, automated methods for the solving of mazes. The random mouse, wall follower, Pledge, and Trémaux's algorithms are designed to be used inside the maze by a traveler with no prior knowledge of the maze, whereas the dead-end filling and shortest path algorithms are designed to be used by a person or computer program that can see the whole maze at once.\n\nMazes containing no loops are known as \"simply connected\", or \"perfect\" mazes, and are equivalent to a \"tree\" in graph theory. Thus many maze solving algorithms are closely related to graph theory. Intuitively, if one pulled and stretched out the paths in the maze in the proper way, the result could be made to resemble a tree.\n\nThis is a trivial method that can be implemented by a very unintelligent robot or perhaps a mouse. It is simply to proceed following the current passage until a junction is reached, and then to make a random decision about the next direction to follow. Although such a method would always eventually find the right solution, this algorithm can be extremely slow.\n\nThe wall follower, the best-known rule for traversing mazes, is also known as either the \"left-hand rule\" or the \"right-hand rule\". If the maze is \"simply connected\", that is, all its walls are connected together or to the maze's outer boundary, then by keeping one hand in contact with one wall of the maze the solver is guaranteed not to get lost and will reach a different exit if there is one; otherwise, he or she will return to the entrance having traversed every corridor next to that connected section of walls at least once.\n\nAnother perspective into why wall following works is topological. If the walls are connected, then they may be deformed into a loop or circle. Then wall following reduces to walking around a circle from start to finish. To further this idea, notice that by grouping together connected components of the maze walls, the boundaries between these are precisely the solutions, even if there is more than one solution (see figures on the right).\n\nIf the maze is not simply-connected (i.e. if the start or endpoints are in the center of the structure surrounded by passage loops, or the pathways cross over and under each other and such parts of the solution path are surrounded by passage loops), this method will not reach the goal.\n\nAnother concern is that care should be taken to begin wall-following at the entrance to the maze. If the maze is not simply-connected and one begins wall-following at an arbitrary point inside the maze, one could find themselves trapped along a separate wall that loops around on itself and containing no entrances or exits. Should it be the case that wall-following begins late, attempt to mark the position in which wall-following began. Because wall-following will always lead you back to where you started, if you come across your starting point a second time, you can conclude the maze is not simply-connected, and you should switch to an alternative wall not yet followed. See the \"Pledge Algorithm\", below, for an alternative methodology.\n\nWall-following can be done in 3D or higher-dimensional mazes if its higher-dimensional passages can be projected onto the 2D plane in a deterministic manner. For example, if in a 3D maze \"up\" passages can be assumed to lead northwest, and \"down\" passages can be assumed to lead southeast, then standard wall following rules can apply. However, unlike in 2D, this requires that the current orientation be known, to determine which direction is the first on the left or right.\n\nDisjoint mazes can be solved with the wall follower method, so long as the entrance and exit to the maze are on the outer walls of the maze. If however, the solver starts inside the maze, it might be on a section disjoint from the exit, and wall followers will continually go around their ring. The Pledge algorithm (named after Jon Pledge of Exeter) can solve this problem.\n\nThe Pledge algorithm, designed to circumvent obstacles, requires an arbitrarily chosen direction to go toward, which will be preferential. When an obstacle is met, one hand (say the right hand) is kept along the obstacle while the angles turned are counted (clockwise turn is positive, counter-clockwise turn is negative). When the solver is facing the original preferential direction again, and the angular sum of the turns made is 0, the solver leaves the obstacle and continues moving in its original direction.\n\nThe hand is removed from the wall only when both \"sum of turns made\" and \"current heading\" are at zero. This allows the algorithm to avoid traps shaped like an upper case letter \"G\". Assuming the algorithm turns left at the first wall, one gets turned around a full 360 degrees by the walls. An algorithm that only keeps track of \"current heading\" leads into an infinite loop as it leaves the lower rightmost wall heading left and runs into the curved section on the left hand side again. The Pledge algorithm does not leave the rightmost wall due to the \"sum of turns made\" not being zero at that point (note 360 degrees is not equal to 0 degrees). It follows the wall all the way around, finally leaving it heading left outside and just underneath the letter shape.\n\nThis algorithm allows a person with a compass to find their way from any point inside to an outer exit of any finite two-dimensional maze, regardless of the initial position of the solver. However, this algorithm will not work in doing the reverse, namely finding the way from an entrance on the outside of a maze to some end goal within it.\n\nTrémaux's algorithm, invented by Charles Pierre Trémaux, is an efficient method to find the way out of a maze that requires drawing lines on the floor to mark a path, and is guaranteed to work for all mazes that have well-defined passages.\nA path from a junction is either unvisited, marked once or marked twice. The algorithm works according to the following rules:\nWhen you finally reach the solution, paths marked exactly once will indicate a way back to the start. If there is no exit, this method will take you back to the start where all paths are marked twice.\nIn this case each path is walked down exactly twice, once in each direction. The resulting walk is called a bidirectional double-tracing.\n\nEssentially, this algorithm, which was discovered in the 19th century, has been used about a hundred years later as depth-first search.\n\nDead-end filling is an algorithm for solving mazes that fills all dead ends, leaving only the correct ways unfilled. It can be used for solving mazes on paper or with a computer program, but it is not useful to a person inside an unknown maze since this method looks at the entire maze at once. The method is to 1) find all of the dead-ends in the maze, and then 2) \"fill in\" the path from each dead-end until the first junction is met. Note that some passages won't become parts of dead end passages until other dead ends are removed first. A video of dead-end filling in action can be seen here: .\n\nDead-end filling cannot accidentally \"cut off\" the start from the finish since each step of the process preserves the topology of the maze. Furthermore, the process won't stop \"too soon\" since the end result cannot contain any dead-ends. Thus if dead-end filling is done on a perfect maze (maze with no loops), then only the solution will remain. If it is done on a partially braid maze (maze with some loops), then every possible solution will remain but nothing more. \n\nIf given an omniscient view of the maze, a simple recursive algorithm can tell one how to get to the end. The algorithm will be given a starting X and Y value. If the X and Y values are not on a wall, the method will call itself with all adjacent X and Y values, making sure that it did not already use those X and Y values before. If the X and Y values are those of the end location, it will save all the previous instances of the method as the correct path. Here is a sample code in Java:\nThe maze-routing algorithm is a low overhead method to find the way between any two locations of the maze. The algorithm is initially proposed for chip multiprocessors (CMPs) domain and guarantees to work for any grid-based maze. In addition to finding paths between two location of the grid (maze), the algorithm can detect when there is no path between the source and destination. Also, the algorithm is to be used by an inside traveler with no prior knowledge of the maze with fixed memory complexity regardless of the maze size; requiring 4 variables in total for finding the path and detecting the unreachable locations. Nevertheless, the algorithm is not to find the shortest path.\n\nMaze-routing algorithm uses the notion of Manhattan distance (MD) and relies on the property of grids that the MD increments/decrements \"exactly\" by 1 when moving from one location to any 4 neighboring locations. Here is the pseudocode without the capability to detect unreachable locations.\n\nWhen a maze has multiple solutions, the solver may want to find the shortest path from start to finish. There are several algorithms to find shortest paths, most of them coming from graph theory. One possible algorithm finds the shortest path by implementing a breadth-first search, while another, the A* algorithm, uses a heuristic technique. The breadth-first search algorithm uses a queue to visit cells in increasing distance order from the start until the finish is reached. Each visited cell needs to keep track of its distance from the start or which adjacent cell nearer to the start caused it to be added to the queue. When the finish location is found, follow the path of cells backwards to the start, which is the shortest path. The breadth-first search in its simplest form has its limitations, like finding the shortest path in weighted graphs.\n\n\n"}
{"id": "1155738", "url": "https://en.wikipedia.org/wiki?curid=1155738", "title": "Menger's theorem", "text": "Menger's theorem\n\nIn the mathematical discipline of graph theory, Menger's theorem says that in a finite graph, the size of a minimum cut set is equal to the maximum number of disjoint paths that can be found between any pair of vertices.\nProved by Karl Menger in 1927, it characterizes the connectivity of a graph.\nIt is generalized by the max-flow min-cut theorem, which is a weighted, edge version, and which in turn is a special case of the strong duality theorem for linear programs.\n\nThe edge-connectivity version of Menger's theorem is as follows:\n\nThe vertex-connectivity statement of Menger's theorem is as follows:\n\nAll these statements (in both edge and vertex versions) remain true in directed graphs (when considering directed paths).\n\nMost direct proofs consider a more general statement to allow proving it by induction. It is also convenient to use definitions that include some degenerate cases.\nThe following proof for undirected graphs works without change for directed graphs or multi-graphs, provided we take \"path\" to mean directed path.\n\nFor sets of vertices \"A,B ⊂ G\" (not necessarily disjoint), an \"AB-path\" is a path in \"G\" with a starting vertex in \"A\", a final vertex in \"B\", and no internal vertices in \"A\" or \"B\". We allow a path with a single vertex in \"A ∩ B\" and zero edges.\nAn \"AB-separator\" of size \"k\" is a set \"S\" of \"k\" vertices (which may intersect \"A\" and \"B\") such that \"G−S\" contains no \"AB\"-path.\nAn \"AB-connector\" of size \"k\" is a union of \"k\" vertex-disjoint \"AB\"-paths.\n\nIn other words, if no \"k\"−1 vertices disconnect \"A\" from \"B\", then there exist \"k\" disjoint paths from \"A\" to \"B\".\nThis variant implies the above vertex-connectivity statement: for \"x,y ∈ G\" in the previous section, apply the current theorem to \"G\"−{\"x,y\"} with \"A = N(x)\", \"B = N(y)\", the neighboring vertices of \"x,y\". Then a set of vertices disconnecting \"x\" and \"y\" is the same thing as an\n\"AB\"-separator, and removing the end vertices in a set of independent \"xy\"-paths gives an \"AB\"-connector.\n\n\"Proof of the Theorem:\"\nInduction on the number of edges in \"G\".\nFor \"G\" with no edges, the minimum \"AB\"-separator is \"A ∩ B\",\nwhich is itself an \"AB\"-connector consisting of single-vertex paths.\n\nFor \"G\" having an edge \"e\", we may assume by induction that the Theorem holds for \"G−e\". If \"G−e\" has a minimal \"AB\"-separator of size \"k\", then there is an \"AB\"-connector of size \"k\" in \"G−e\", and hence in \"G\".\n\nOtherwise, let \"S\" be a \"AB\"-separator of \"G−e\" of size less than \"k\",\nso that every \"AB\"-path in \"G\" contains a vertex of \"S\" or the edge \"e\".\nThe size of \"S\" must be \"k-1\", since if it was less, \"S\" together with either endpoint of \"e\" would be a better \"AB\"-separator of \"G\".\nIn \"G−S\" there is an \"AB\"-path through \"e\", since \"S\" alone is too small to be an \"AB\"-separator of \"G\".\nLet \"v\" be the earlier and \"v\" be the later vertex of \"e\" on such a path.\nThen \"v\" is reachable from \"A\" but not from \"B\" in \"G−S−e\", while \"v\" is reachable from \"B\" but not from \"A\".\n\nNow, let \"S = S ∪ {v}\", and consider a minimum \"AS\"-separator \"T\" in \"G−e\".\nSince \"v\" is not reachable from \"A\" in \"G−S\", \"T\" is also an \"AS\"-separator in \"G\".\nThen \"T\" is also an \"AB\"-separator in \"G\" (because every \"AB\"-path intersects \"S\").\nHence it has size at least \"k\".\nBy induction, \"G−e\" contains an \"AS\"-connector \"C\" of size \"k\".\nBecause of its size, the endpoints of the paths in it must be exactly \"S\".\n\nSimilarly, letting \"S = S ∪ {v}\", a minimum \"SB\"-separator has size \"k\", and there is \nan \"SB\"-connector \"C\" of size \"k\", with paths whose starting points are exactly \"S\".\n\nFurthermore, since \"S\" disconnects \"G\", every path in \"C\" is internally disjoint from \nevery path in \"C\", and we can define an \"AB\"-connector of size \"k\" in \"G\" by concatenating paths (\"k−1\" paths through \"S\" and one path going through \"e=vv\"). Q.E.D.\n\nThe directed edge version of the theorem easily implies the other versions.\nTo infer the directed graph vertex version, it suffices to split each vertex \"v\" into two vertices \"v\", \"v\", with all ingoing edges going to \"v\", all outgoing edges going from \"v\", and an additional edge from \"v\" to \"v\".\nThe directed versions of the theorem immediately imply undirected versions: it suffices to replace each edge of an undirected graph with a pair of directed edges (a digon).\n\nThe directed edge version in turn follows from its weighted variant, the max-flow min-cut theorem.\nIts proofs are often correctness proofs for max flow algorithms.\nIt is also a special case of the still more general (strong) duality theorem for linear programs.\n\nA formulation that for finite digraphs is equivalent to the above formulation is:\n\nIn this version the theorem follows in fairly easily from König's theorem: in a bipartite graph, the minimal size of a cover is equal to the maximal size of a matching.\n\nThis is done as follows: replace every vertex \"v\" in the original digraph \"D\" by two vertices \"v' \", \"v<nowiki>\"</nowiki>\", and every edge \"uv\" by the edge \"u'v<nowiki>\"</nowiki>\". This results in a bipartite graph, whose one side consists of the vertices \"v' \", and the other of the vertices \"v<nowiki>\"</nowiki>\".\n\nApplying König's theorem we obtain a matching \"M\" and a cover \"C\" of the same size. In particular, exactly one endpoint of each edge of \"M\" is in \"C\". Add to \"C\" all vertices \"a<nowiki>\"</nowiki>\", for \"a\" in \"A,\" and all vertices \"b' \", for \"b\" in \"B\". Let \"P\" be the set of all \"AB\"-paths composed of edges \"uv\" in \"D\" such that \"u'v<nowiki>\"</nowiki>\" belongs to M. Let \"Q\" in the original graph consist of all vertices \"v\" such that both \"v' \" and \"v<nowiki>\"</nowiki>\" belong to \"C\". It is straightforward to check that \"Q\" is an \"AB\"-separating set, and that every path in the family \"P\" contains precisely one vertex from \"Q\", as desired.\n\nMenger's theorem holds for infinite graphs, and in that context it applies to the minimum cut between any two elements that are either vertices or ends of the graph . The following result of Ron Aharoni and Eli Berger was originally a conjecture proposed by Paul Erdős, and before being proved was known as the Erdős–Menger conjecture.\nIt is equivalent to Menger's theorem when the graph is finite.\n\n\n\n"}
{"id": "1590804", "url": "https://en.wikipedia.org/wiki?curid=1590804", "title": "Method of distinguished element", "text": "Method of distinguished element\n\nIn the mathematical field of enumerative combinatorics, identities are sometimes established by arguments that rely on singling out one \"distinguished element\" of a set.\n\nLet formula_1 be a family of subsets of the set formula_2 and let formula_3 be a distinguished element of set formula_2. Then suppose there is a predicate formula_5 that relates a subset formula_6 to formula_7. Denote formula_8 to be the set of subsets formula_9 from formula_1 for which formula_5 is true and formula_12 to be the set of subsets formula_9 from formula_1 for which formula_5 is false, Then formula_8 and formula_12 are disjoint sets, so by the method of summation, the cardinalities are additive\n\nThus the distinguished element allows for a decomposition according to a predicate that is a simple form of a divide and conquer algorithm. In combinatorics, this allows for the construction of recurrence relations. Examples are in the next section.\n\n\n\n\n"}
{"id": "47522601", "url": "https://en.wikipedia.org/wiki?curid=47522601", "title": "Michael Brin Prize in Dynamical Systems", "text": "Michael Brin Prize in Dynamical Systems\n\nThe Michael Brin Prize in Dynamical Systems, abbreviated as the Brin Prize, is awarded to mathematicians who have made outstanding advances in the field of dynamical systems and are within 14 years of their PhD. The prize is endowed by and named after Michael Brin, whose son Sergey Brin, is a co-founder of Google. Michael Brin is a retired mathematician at the University of Maryland and a specialist in dynamical systems.\n\nThe first prize was awarded in 2008, and since 2009, it has been awarded bi-annually. Artur Avila, the 2011 awardee, went on to win the Fields Medal in 2014.\n\n"}
{"id": "1089079", "url": "https://en.wikipedia.org/wiki?curid=1089079", "title": "Miller index", "text": "Miller index\n\nMiller indices form a notation system in crystallography for planes in crystal (Bravais) lattices.\n\nIn particular, a family of lattice planes is determined by three integers \"h\", \"k\", and \"ℓ\", the \"Miller indices\". They are written (hkℓ), and denote the family of planes orthogonal to formula_1, where formula_2 are the basis of the reciprocal lattice vectors. (Note that the plane is not always orthogonal to the linear combination of direct lattice vectors formula_3 because the reciprocal lattice vectors need not be mutually orthogonal.) By convention, negative integers are written with a bar, as in for −3. The integers are usually written in lowest terms, i.e. their greatest common divisor should be 1.\nm \nThere are also several related notations:\nIn the context of crystal \"directions\" (not planes), the corresponding notations are:\n\nMiller indices were introduced in 1839 by the British mineralogist William Hallowes Miller, although an almost identical system (\"Weiss parameters\") had already been used by German mineralogist Christian Samuel Weiss since 1817 . The method was also historically known as the Millerian system, and the indices as Millerian, although this is now rare.\n\nThe Miller indices are defined with respect to any choice of unit cell and not only with respect to primitive basis vectors, as is sometimes stated.\n\nThere are two equivalent ways to define the meaning of the Miller indices: via a point in the reciprocal lattice, or as the inverse intercepts along the lattice vectors. Both definitions are given below. In either case, one needs to choose the three lattice vectors a, a, and a that define the unit cell (note that the conventional unit cell may be larger than the primitive cell of the Bravais lattice, as the examples below illustrate). Given these, the three primitive reciprocal lattice vectors are also determined (denoted b, b, and b).\n\nThen, given the three Miller indices h, k, ℓ, (hkℓ) denotes planes orthogonal to the reciprocal lattice vector:\nThat is, (hkℓ) simply indicates a normal to the planes in the basis of the primitive reciprocal lattice vectors. Because the coordinates are integers, this normal is itself always a reciprocal lattice vector. The requirement of lowest terms means that it is the \"shortest\" reciprocal lattice vector in the given direction.\n\nEquivalently, (hkℓ) denotes a plane that intercepts the three points a/\"h\", a/\"k\", and a/\"ℓ\", or some multiple thereof. That is, the Miller indices are proportional to the \"inverses\" of the intercepts of the plane, in the basis of the lattice vectors. If one of the indices is zero, it means that the planes do not intersect that axis (the intercept is \"at infinity\").\n\nConsidering only (hkℓ) planes intersecting one or more lattice points (the \"lattice planes\"), the perpendicular distance \"d\" between adjacent lattice planes is related to the (shortest) reciprocal lattice vector orthogonal to the planes by the formula: formula_5.\n\nThe related notation [hkℓ] denotes the \"direction\":\nThat is, it uses the direct lattice basis instead of the reciprocal lattice. Note that [hkℓ] is \"not\" generally normal to the (hkℓ) planes, except in a cubic lattice as described below.\n\nFor the special case of simple cubic crystals, the lattice vectors are orthogonal and of equal length (usually denoted \"a\"), as are those of the reciprocal lattice. Thus, in this common case, the Miller indices (hkℓ) and [hkℓ] both simply denote normals/directions in Cartesian coordinates.\n\nFor cubic crystals with lattice constant \"a\", the spacing \"d\" between adjacent (hkℓ) lattice planes is (from above)\n\nBecause of the symmetry of cubic crystals, it is possible to change the place and sign of the integers and have equivalent directions and planes:\n\nFor face-centered cubic and body-centered cubic lattices, the primitive lattice vectors are not orthogonal. However, in these cases the Miller indices are conventionally defined relative to the lattice vectors of the cubic supercell and hence are again simply the Cartesian directions.\n\nWith hexagonal and rhombohedral lattice systems, it is possible to use the Bravais-Miller system, which uses four indices (\"h\" \"k\" \"i\" \"ℓ\") that obey the constraint\nHere \"h\", \"k\" and \"ℓ\" are identical to the corresponding Miller indices, and \"i\" is a redundant index.\n\nThis four-index scheme for labeling planes in a hexagonal lattice makes permutation symmetries apparent. For example, the similarity between (110) ≡ (110) and (10) ≡ (110) is more obvious when the redundant index is shown.\n\nIn the figure at right, the (001) plane has a 3-fold symmetry: it remains unchanged by a rotation of 1/3 (2π/3 rad, 120°). The [100], [010] and the [0] directions are really similar. If \"S\" is the intercept of the plane with the [0] axis, then\n\nThere are also \"ad hoc\" schemes (e.g. in the transmission electron microscopy literature) for indexing hexagonal \"lattice vectors\" (rather than reciprocal lattice vectors or planes) with four indices. However they don't operate by similarly adding a redundant index to the regular three-index set.\n\nFor example, the reciprocal lattice vector (hkℓ) as suggested above can be written in terms of reciprocal lattice vectors as formula_1. For hexagonal crystals this may be expressed in terms of direct-lattice basis-vectors a, a and a as\n\nHence zone indices of the direction perpendicular to plane (hkℓ) are, in suitably normalized triplet form, simply formula_10. When \"four indices\" are used for the zone normal to plane (hkℓ), however, the literature often uses formula_11 instead. Thus as you can see, four-index zone indices in square or angle brackets sometimes mix a single direct-lattice index on the right with reciprocal-lattice indices (normally in round or curly brackets) on the left.\n\nThe crystallographic directions are fictitious lines linking nodes (atoms, ions or molecules) of a crystal. Similarly, the crystallographic planes are fictitious \"planes\" linking nodes. Some directions and planes have a higher density of nodes; these dense planes have an influence on the behaviour of the crystal:\nFor all these reasons, it is important to determine the planes and thus to have a notation system.\n\nOrdinarily, Miller indices are always integers by definition, and this constraint is physically significant. To understand this, suppose that we allow a plane (abc) where the Miller \"indices\" \"a\", \"b\" and \"c\" (defined as above) are not necessarily integers.\n\nIf \"a\", \"b\" and \"c\" have rational ratios, then the same family of planes can be written in terms of integer indices (hkℓ) by scaling \"a\", \"b\" and \"c\" appropriately: divide by the largest of the three numbers, and then multiply by the least common denominator. Thus, integer Miller indices implicitly include indices with all rational ratios. The reason why planes where the components (in the reciprocal-lattice basis) have rational ratios are of special interest is that these are the lattice planes: they are the only planes whose intersections with the crystal are 2d-periodic.\n\nFor a plane (abc) where \"a\", \"b\" and \"c\" have irrational ratios, on the other hand, the intersection of the plane with the crystal is \"not\" periodic. It forms an aperiodic pattern known as a quasicrystal. This construction corresponds precisely to the standard \"cut-and-project\" method of defining a quasicrystal, using a plane with irrational-ratio Miller indices. (Although many quasicrystals, such as the Penrose tiling, are formed by \"cuts\" of periodic lattices in more than three dimensions, involving the intersection of more than one such hyperplane.)\n\n\n"}
{"id": "23629444", "url": "https://en.wikipedia.org/wiki?curid=23629444", "title": "Mixed linear complementarity problem", "text": "Mixed linear complementarity problem\n\nIn mathematical optimization theory, the mixed linear complementarity problem, often abbreviated as MLCP or LMCP, is a generalization of the linear complementarity problem to include free variables.\n\n"}
{"id": "312648", "url": "https://en.wikipedia.org/wiki?curid=312648", "title": "Mutual exclusivity", "text": "Mutual exclusivity\n\nIn logic and probability theory, two events (or propositions) are mutually exclusive or disjoint if they cannot both occur at the same time(be true). A clear example is the set of outcomes of a single coin toss, which can result in either heads or tails, but not both.\n\nIn the coin-tossing example, both outcomes are, in theory, collectively exhaustive, which means that at least one of the outcomes must happen, so these two possibilities together exhaust all the possibilities. However, not all mutually exclusive events are collectively exhaustive. For example, the outcomes 1 and 4 of a single roll of a six-sided die are mutually exclusive (both cannot happen at the same time) but not collectively exhaustive (there are other possible outcomes; 2,3,5,6).\n\nIn logic, two mutually exclusive propositions are propositions that logically cannot be true in the same sense at the same time. To say that more than two propositions are mutually exclusive, depending on context, means that one cannot be true if the other one is true, or at least one of them cannot be true. The term \"pairwise mutually exclusive\" always means that two of them cannot be true simultaneously.\n\nIn probability theory, events \"E\", \"E\", ..., \"E\" are said to be mutually exclusive if the occurrence of any one of them implies the non-occurrence of the remaining \"n\" − 1 events. Therefore, two mutually exclusive events cannot both occur. Formally said, the intersection of each two of them is empty (the null event): \"A\" ∩ \"B\" = ∅. In consequence, mutually exclusive events have the property: P(\"A\" ∩ \"B\") = 0.\n\nFor example, it is impossible to draw a card that is both red and a club because clubs are always black. If just one card is drawn from the deck, either a red card (heart or diamond) or a black card (club or spade) will be drawn. When \"A\" and \"B\" are mutually exclusive, P(\"A\" ∪ \"B\") = P(\"A\") + P(\"B\"). To find the probability of drawing a red card or a club, for example, add together the probability of drawing a red card and the probability of drawing a club. In a standard 52-card deck, there are twenty-six red cards and thirteen clubs: 26/52 + 13/52 = 39/52 or 3/4.\n\nOne would have to draw at least two cards in order to draw both a red card and a club. The probability of doing so in two draws depends on whether the first card drawn were replaced before the second drawing, since without replacement there is one fewer card after the first card was drawn. The probabilities of the individual events (red, and club) are multiplied rather than added. The probability of drawing a red and a club in two drawings without replacement is then 26/52 × 13/51 × 2 = 676/2652, or 13/51. With replacement, the probability would be 26/52 × 13/52 × 2 = 676/2704, or 13/52.\n\nIn probability theory, the word \"or\" allows for the possibility of both events happening. The probability of one or both events occurring is denoted P(\"A\" ∪ \"B\") and in general it equals P(\"A\") + P(\"B\") – P(\"A\" ∩ \"B\"). Therefore, in the case of drawing a red card or a king, drawing any of a red king, a red non-king, or a black king is considered a success. In a standard 52-card deck, there are twenty-six red cards and four kings, two of which are red, so the probability of drawing a red or a king is 26/52 + 4/52 – 2/52 = 28/52.\n\nEvents are collectively exhaustive if all the possibilities for outcomes are exhausted by those possible events, so at least one of those outcomes must occur. The probability that at least one of the events will occur is equal to one. For example, there are theoretically only two possibilities for flipping a coin. Flipping a head and flipping a tail are collectively exhaustive events, and there is a probability of one of flipping either a head or a tail. Events can be both mutually exclusive and collectively exhaustive. In the case of flipping a coin, flipping a head and flipping a tail are also mutually exclusive events. Both outcomes cannot occur for a single trial (i.e., when a coin is flipped only once). The probability of flipping a head and the probability of flipping a tail can be added to yield a probability of 1: 1/2 + 1/2 =1.\n\nIn statistics and regression analysis, an independent variable that can take on only two possible values is called a dummy variable. For example, it may take on the value 0 if an observation is of a male subject or 1 if the observation is of a female subject. The two possible categories associated with the two possible values are mutually exclusive, so that no observation falls into more than one category, and the categories are exhaustive, so that every observation falls into some category. Sometimes there are three or more possible categories, which are pairwise mutually exclusive and are collectively exhaustive — for example, under 18 years of age, 18 to 64 years of age, and age 65 or above. In this case a set of dummy variables is constructed, each dummy variable having two mutually exclusive and jointly exhaustive categories — in this example, one dummy variable (called D) would equal 1 if age is less than 18, and would equal 0 \"otherwise\"; a second dummy variable (called D) would equal 1 if age is in the range 18-64, and 0 otherwise. In this set-up, the dummy variable pairs (D, D) can have the values (1,0) (under 18), (0,1) (between 18 and 64), or (0,0) (65 or older) (but not (1,1), which would nonsensically imply that an observed subject is both under 18 and between 18 and 64). Then the dummy variables can be included as independent (explanatory) variables in a regression. Note that the number of dummy variables is always one less than the number of categories: with the two categories male and female there is a single dummy variable to distinguish them, while with the three age categories two dummy variables are needed to distinguish them.\n\nSuch qualitative data can also be used for dependent variables. For example, a researcher might want to predict whether someone goes to college or not, using family income, a gender dummy variable, and so forth as explanatory variables. Here the variable to be explained is a dummy variable that equals 0 if the observed subject does not go to college and equals 1 if the subject does go to college. In such a situation, ordinary least squares (the basic regression technique) is widely seen as inadequate; instead probit regression or logistic regression is used. Further, sometimes there are three or more categories for the dependent variable — for example, no college, community college, and four-year college. In this case, the multinomial probit or multinomial logit technique is used.\n\n\n"}
{"id": "42928696", "url": "https://en.wikipedia.org/wiki?curid=42928696", "title": "Planar cover", "text": "Planar cover\n\nIn graph theory, a planar cover of a finite graph \"G\" is a finite covering graph of \"G\" that is itself a planar graph. Every graph that can be embedded into the projective plane has a planar cover; an unsolved conjecture of Seiya Negami states that these are the only graphs with planar covers.\n\nThe existence of a planar cover is a minor-closed graph property, and so can be characterized by finitely many forbidden minors, but the exact set of forbidden minors is not known. For the same reason, there exists a polynomial time algorithm for testing whether a given graph has a planar cover, but an explicit description of this algorithm is not known.\n\nA \"covering map\" from one graph \"C\" to another graph \"H\" may be described by a function \"f\" from the vertices of \"C\" onto the vertices of \"H\" that, for each vertex \"v\" of \"C\", gives a bijection between the neighbors of \"v\" and the neighbors of \"f\"(\"v\"). If \"H\" is a connected graph, each vertex of \"H\" must have the same number of pre-images in \"C\"; this number is called the \"ply\" of the map, and \"C\" is called a covering graph of \"G\". If \"C\" and \"H\" are both finite, and \"C\" is a planar graph, then \"C\" is called a planar cover of \"H\".\n\nThe graph of the dodecahedron has a symmetry that maps each vertex to the antipodal vertex. The set of antipodal pairs of vertices and their adjacencies can itself be viewed as a graph, the Petersen graph. The dodecahedron forms a planar cover of this nonplanar graph. As this example shows, not every graph with a planar cover is itself planar. However, when a planar graph covers a non-planar one, the ply must be an even number.\nThe graph of a \"k\"-gonal prism has 2\"k\" vertices, and is planar with two \"k\"-gon faces and \"k\" quadrilateral faces. If \"k\" = \"ab\", with \"a\" ≥ 2 and \"b\" ≥ 3, then it has an \"a\"-ply covering map to a \"b\"-fonal prism, in which two vertices of the \"k\"-prism are mapped to the same vertex of the \"b\"-prism if they both belong to the same \"k\"-gonal face and the distance from one to the other is a multiple of \"b\". For instance, the dodecagonal prism can form a 2-ply cover of the hexagonal prism, a 3-ply cover of the cube, or a 4-ply cover of the triangular prism. These examples show that a graph may have many different planar covers, and may be the planar cover for many other graphs. Additionally they show that the ply of a planar cover may be arbitrarily large.\nThey are not the only covers involving prisms: for instance, the hexagonal prism can also cover a non-planar graph, the utility graph \"K\", by identifying antipodal pairs of vertices.\n\nIf a graph \"H\" has a planar cover, so does every graph minor of \"H\". A minor \"G\" of \"H\" may be formed by deleting edges and vertices from \"H\", and by contracting edges of \"H\". The covering graph \"C\" can be transformed in the same way: for each deleted edge or vertex in \"H\", delete its preimage in \"C\", and for each contracted edge or vertex in \"H\", contract its preimage in \"C\". The result of applying these operations to \"C\" is a minor of \"C\" that covers \"G\". Since every minor of a planar graph is itself planar, this gives a planar cover of the minor \"G\".\n\nBecause the graphs with planar covers are closed under the operation of taking minors, it follows from the Robertson–Seymour theorem that they may be characterized by a finite set of forbidden minors. A graph is a forbidden minor for this property if it has no planar cover, but all of its minors do have planar covers. This characterization can be used to prove the existence of a polynomial time algorithm that tests for the existence of a planar cover, by searching for each of the forbidden minors and returning that a planar cover exists only if this search fails to find any of them. However, because the exact set of forbidden minors for this property is not known, this proof of existence is non-constructive, and does not lead to an explicit description of the set of forbidden minors or of the algorithm based on them.\n\nAnother graph operation that preserves the existence of a planar cover is the Y-Δ transform, which replaces any degree-three vertex of a graph \"H\" by a triangle connecting its three neighbors. However, the reverse of this transformation, a Δ-Y transform, does not necessarily preserve planar covers.\n\nAdditionally, a disjoint union of two graphs that have covers will also have a cover, formed as the disjoint union of the covering graphs. If the two covers have the same ply as each other, then this will also be the ply of their union.\n\nIf a graph \"H\" has an embedding into the projective plane, then it necessarily has a planar cover, given by the preimage of \"H\" in the orientable double cover of the projective plane, which is a sphere.\n\nA \"regular cover\" of a graph \"H\" is one that comes from a group of symmetries of its covering graph: the preimages of each vertex in \"H\" are an orbit of the group. proved that every connected graph with a planar regular cover can be embedded into the projective plane. Based on these two results, he conjectured that in fact every connected graph with a planar cover is projective.\nAs of 2013, this conjecture remains unsolved. It is also known as Negami's \"1-2-∞ conjecture\", because it can be reformulated as stating that the minimum ply of a planar cover, if it exists, must be either 1 or 2.\nLike the graphs with planar covers, the graphs with projective plane embeddings can be characterized by forbidden minors. In this case the exact set of forbidden minors is known: there are 35 of them. 32 of these are connected, and one of these 32 graphs necessarily appears as a minor in any connected non-projective-planar graph. Since Negami made his conjecture, it has been proven that 31 of these 32 forbidden minors either do not have planar covers, or can be reduced by Y-Δ transforms to a simpler graph on this list. The one remaining graph for which this has not yet been done is \"K\", a seven-vertex apex graph that forms the skeleton of a four-dimensional octahedral pyramid. If \"K\" could be shown not to have any planar covers, this would complete a proof of the conjecture. On the other hand, if the conjecture is false, \"K\" would necessarily be its smallest counterexample.\n\nA related conjecture by Michael Fellows, now solved, concerns planar \"emulators\", a generalization of planar covers that maps graph neighborhoods surjectively rather than bijectively. The graphs with planar emulators, like those with planar covers, are closed under minors and Y-Δ transforms. In 1988, independently of Negami, Fellows conjectured that the graphs with planar emulators are exactly the graphs that can be embedded into the projective plane. The conjecture is true for \"regular\" emulators, coming from automomorphisms of the underlying covering graph, by a result analogous to the result of for regular planar covers.\nHowever, several of the 32 connected forbidden minors for projective-planar graphs turn out to have planar emulators. Therefore, Fellows' conjecture is false. Finding a full set of forbidden minors for the existence of planar emulators remains an open problem.\n\n\n\n"}
{"id": "25182", "url": "https://en.wikipedia.org/wiki?curid=25182", "title": "Quantization (physics)", "text": "Quantization (physics)\n\nIn physics, quantization is the process of transition from a classical understanding of physical phenomena to a newer understanding known as quantum mechanics. (It is a procedure for constructing a quantum field theory starting from a classical field theory.) This is a generalization of the procedure for building quantum mechanics from classical mechanics. One also speaks of field quantization, as in the \"quantization of the electromagnetic field\", where one refers to photons as field \"quanta\" (for instance as light quanta). This procedure is basic to theories of particle physics, nuclear physics, condensed matter physics, and quantum optics.\n\nQuantization converts classical fields into operators acting on quantum states of the field theory. The lowest energy state is called the vacuum state. The reason for quantizing a theory is to deduce properties of materials, objects or particles through the computation of quantum amplitudes, which may be very complicated. Such computations have to deal with certain subtleties called renormalization, which, if neglected, can often lead to nonsense results, such as the appearance of infinities in various amplitudes. The full specification of a quantization procedure requires methods of performing renormalization.\n\nThe first method to be developed for quantization of field theories was canonical quantization. While this is extremely easy to implement on sufficiently simple theories, there are many situations where other methods of quantization yield more efficient procedures for computing quantum amplitudes. However, the use of canonical quantization has left its mark on the language and interpretation of quantum field theory.\n\nCanonical quantization of a field theory is analogous to the construction of quantum mechanics from classical mechanics. The classical field is treated as a dynamical variable called the canonical coordinate, and its time-derivative is the canonical momentum. One introduces a commutation relation between these which is exactly the same as the commutation relation between a particle's position and momentum in quantum mechanics. Technically, one converts the field to an operator, through combinations of creation and annihilation operators. The field operator acts on quantum states of the theory. The lowest energy state is called the vacuum state. The procedure is also called second quantization.\n\nThis procedure can be applied to the quantization of any field theory: whether of fermions or bosons, and with any internal symmetry. However, it leads to a fairly simple picture of the vacuum state and is not easily amenable to use in some quantum field theories, such as quantum chromodynamics which is known to have a complicated vacuum characterized by many different condensates.\n\nEven within the setting of canonical quantization, there is difficulty associated to quantizing arbitrary observables on the classical phase space. This is the \"ordering ambiguity\": Classically the position and momentum variables \"x\" and \"p\" commute, but their quantum mechanical counterparts do not. Various \"quantization schemes\" have been proposed to resolve this ambiguity, of which the most popular is the Weyl quantization scheme. Nevertheless, the \"Groenewold–van Hove theorem\" says that no perfect quantization scheme exists. Specifically, if the quantizations of \"x\" and \"p\" are taken to be the usual position and momentum operators, then no quantization scheme can perfectly reproduce the Poisson bracket relations among the classical observables. See Groenewold's theorem for one version of this result.\n\nThere is a way to perform a canonical quantization without having to resort to the non covariant approach of foliating spacetime and choosing a Hamiltonian. This method is based upon a classical action, but is different from the functional integral approach.\n\nThe method does not apply to all possible actions (for instance, actions with a noncausal structure or actions with gauge \"flows\"). It starts with the classical algebra of all (smooth) functionals over the configuration space. This algebra is quotiented over by the ideal generated by the Euler–Lagrange equations. Then, this quotient algebra is converted into a Poisson algebra by introducing a Poisson bracket derivable from the action, called the Peierls bracket. This Poisson algebra is then formula_1-deformed in the same way as in canonical quantization.\n\nThere is also a way to quantize actions with gauge \"flows\". It involves the Batalin–Vilkovisky formalism, an extension of the BRST formalism.\n\nIn mathematical physics, geometric quantization is a mathematical approach to defining a quantum theory corresponding to a given classical theory. It attempts to carry out quantization, for which there is in general no exact recipe, in such a way that certain analogies between the classical theory and the quantum theory remain manifest. For example, the similarity between the Heisenberg equation in the Heisenberg picture of quantum mechanics and the Hamilton equation in classical physics should be built in.\n\nOne of the earliest attempts at a natural quantization was Weyl quantization, proposed by Hermann Weyl in 1927. Here, an attempt is made to associate a quantum-mechanical observable (a self-adjoint operator on a Hilbert space) with a real-valued function on classical phase space. The position and momentum in this phase space are mapped to the generators of the Heisenberg group, and the Hilbert space appears as a group representation of the Heisenberg group. In 1946, H. J. Groenewold considered the product of a pair of such observables and asked what the corresponding function would be on the classical phase space. This led him to discover the phase-space star-product of a pair of functions.\nMore generally, this technique leads to deformation quantization, where the ★-product is taken to be a deformation of the algebra of functions on a symplectic manifold or Poisson manifold. However, as a natural quantization scheme (a functor), Weyl's map is not satisfactory. For example, the Weyl map of the classical angular-momentum-squared is not just the quantum angular momentum squared operator, but it further contains a constant term 3ħ2/2. (This extra term is actually physically significant, since it accounts for the nonvanishing angular momentum of the ground-state Bohr orbit in the hydrogen atom. As a mere representation change, however, Weyl's map underlies the alternate Phase space formulation of conventional quantum mechanics.\n\nA more geometric approach to quantization, in which the classical phase space can be a general symplectic manifold, was developed in the 1970s by Bertram Kostant and Jean-Marie Souriau. The method proceeds in two stages. First, once constructs a \"prequantum Hilbert space\" consisting of square-integrable functions (or, more properly, sections of a line bundle) over the phase space. Here one can construct operators satisfying commutation relations corresponding exactly to the classical Poisson-bracket relations. On the other hand, this prequantum Hilbert space is too big to be physically meaningful. One then restricts to functions (or sections) depending on half the variables on the phase space, yielding the quantum Hilbert space.\n\nSee Loop quantum gravity.\n\nA classical mechanical theory is given by an action with the permissible configurations being the ones which are extremal with respect to functional variations of the action. A quantum-mechanical description of the classical system can also be constructed from the action of the system by means of the path integral formulation.\n\nSee Uncertainty principle\n\nSee Schwinger's quantum action principle\n\n\n"}
{"id": "16989288", "url": "https://en.wikipedia.org/wiki?curid=16989288", "title": "Rajeev Motwani", "text": "Rajeev Motwani\n\nRajeev Motwani (; March 26, 1962 – June 5, 2009) was a professor of Computer Science at Stanford University whose research focused on theoretical computer science. He was an early advisor and supporter of companies including Google and PayPal, and a special advisor to Sequoia Capital. He was a winner of the Gödel Prize in 2001.\n\nRajeev Motwani was born in Jammu and grew up in New Delhi. His father was in the Indian Army. He had two brothers. As a child, inspired by luminaries like Gauss, he wanted to become a mathematician.\nMotwani went to St Columba's School, New Delhi. He completed his B.Tech in Computer Science from the Indian Institute of Technology Kanpur in 1983 and got his Ph.D. in Computer Science from the University of California, Berkeley in 1988 under the supervision of Richard M. Karp.\n\nMotwani joined Stanford soon after U.C. Berkeley. \nHe founded the Mining Data at Stanford project (MIDAS), an umbrella organization for several groups looking into new and innovative data management concepts. His research included data privacy, web search, robotics, and computational drug design. He is also one of the originators of the Locality-sensitive hashing algorithm.\n\nMotwani was one of the co-authors (with Larry Page and Sergey Brin, and Terry Winograd) of an influential early paper on the PageRank algorithm. He also co-authored another seminal search paper \"What Can You Do With A Web In Your Pocket\" with those same authors.\nPageRank was the basis for search techniques of Google (founded by Page and Brin), and Motwani advised or taught many of Google's developers and researchers, including the first employee, Craig Silverstein.\n\nHe was an author of two widely used theoretical computer science textbooks: \"Randomized Algorithms\" with Prabhakar Raghavan and \"Introduction to Automata Theory, Languages, and Computation\" with John Hopcroft and Jeffrey Ullman.\n\nHe was an avid angel investor and helped fund a number of startups to emerge from Stanford. He sat on boards including Google, Kaboodle, Mimosa Systems (acquired by Iron Mountain Incorporated), Adchemy, Baynote, Vuclip, NeoPath Networks (acquired by Cisco Systems in 2007), Tapulous and Stanford Student Enterprises. He was active in the Business Association of Stanford Entrepreneurial Students (BASES).\n\nHe was a winner of the Gödel Prize in 2001 for his work on the PCP theorem and its applications to hardness of approximation.\n\nHe served on the editorial boards of SIAM Journal on Computing, Journal of Computer and System Sciences, ACM Transactions on Knowledge Discovery from Data, and IEEE Transactions on Knowledge and Data Engineering.\n\nMotwani was found dead in his pool in the backyard of his Atherton home on June 5, 2009. The San Mateo County coroner, Robert Foucrault, ruled the death an accidental drowning. Toxicology tests showed that Motwani's blood alcohol content was 0.26 percent.\nHe could not swim, but was planning on taking lessons, according to his friends.\n\nMotwani, and his wife Asha Jadeja Motwani, had two daughters named Naitri and Anya.\nAfter his death his family donated US$1.5 million in 2011, a building was named in his honor at IIT Kanpur.\n\n\n"}
{"id": "16993965", "url": "https://en.wikipedia.org/wiki?curid=16993965", "title": "Reduced derivative", "text": "Reduced derivative\n\nIn mathematics, the reduced derivative is a generalization of the notion of derivative that is well-suited to the study of functions of bounded variation. Although functions of bounded variation have derivatives in the sense of Radon measures, it is desirable to have a derivative that takes values in the same space as the functions themselves. Although the precise definition of the reduced derivative is quite involved, its key properties are quite easy to remember:\n\n\nThe notion of reduced derivative appears to have been introduced by Alexander Mielke and Florian Theil in 2004.\n\nLet \"X\" be a separable, reflexive Banach space with norm || || and fix \"T\" > 0. Let BV([0, \"T\"]; \"X\") denote the space of all left-continuous functions \"z\" : [0, \"T\"] → \"X\" with bounded variation on [0, \"T\"].\n\nFor any function of time \"f\", use subscripts +/− to denote the right/left continuous versions of \"f\", i.e.\n\nFor any sub-interval [\"a\", \"b\"] of [0, \"T\"], let Var(\"z\", [\"a\", \"b\"]) denote the variation of \"z\" over [\"a\", \"b\"], i.e., the supremum\n\nThe first step in the construction of the reduced derivative is the “stretch” time so that \"z\" can be linearly interpolated at its jump points. To this end, define\n\nThe “stretched time” function \"τ̂\" is left-continuous (i.e. \"τ̂\" = \"τ̂\"); moreover, \"τ̂\" and \"τ̂\" are strictly increasing and agree except at the (at most countable) jump points of \"z\". Setting \"T̂\" = \"τ̂\"(\"T\"), this “stretch” can be inverted by\n\nUsing this, the stretched version of \"z\" is defined by\n\nwhere \"θ\" ∈ [0, 1] and\n\nThe effect of this definition is to create a new function \"ẑ\" which “stretches out” the jumps of \"z\" by linear interpolation. A quick calculation shows that \"ẑ\" is not just continuous, but also lies in a Sobolev space:\n\nThe derivative of \"ẑ\"(\"τ\") with respect to \"τ\" is defined almost everywhere with respect to Lebesgue measure. The reduced derivative of \"z\" is the pull-back of this derivative by the stretching function \"τ̂\" : [0, \"T\"] → [0, \"T̂\"]. In other words,\n\nAssociated with this pull-back of the derivative is the pull-back of Lebesgue measure on [0, \"T̂\"], which defines the differential measure \"μ\":\n\n\n\n"}
{"id": "30640685", "url": "https://en.wikipedia.org/wiki?curid=30640685", "title": "Rocky Mountain Journal of Mathematics", "text": "Rocky Mountain Journal of Mathematics\n\nFounded in 1971, the journal publishes both research and expository articles on mathematics, with an emphasis on survey articles. \nThe journal is indexed by \"Mathematical Reviews\" and Zentralblatt MATH.\nIts 2009 MCQ was 0.25. According to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 0.250.\n"}
{"id": "33490570", "url": "https://en.wikipedia.org/wiki?curid=33490570", "title": "Schröder–Hipparchus number", "text": "Schröder–Hipparchus number\n\nIn number theory, the Schröder–Hipparchus numbers form an integer sequence that can be used to count the number of plane trees with a given set of leaves, the number of ways of inserting parentheses into a sequence, and the number of ways of dissecting a convex polygon into smaller polygons by inserting diagonals. These numbers begin\nThey are also called the super-Catalan numbers, the little Schröder numbers, or the Hipparchus numbers, after Eugène Charles Catalan and his Catalan numbers, Ernst Schröder and the closely related Schröder numbers, and the ancient Greek mathematician Hipparchus who appears from evidence in Plutarch to have known of these numbers.\n\nThe Schröder–Hipparchus numbers may be used to count several closely related combinatorial objects:\nAs the figure shows, there is a simple combinatorial equivalence between these objects: a polygon subdivision has a plane tree as a form of its dual graph, the leaves of the tree correspond to the symbols in a parenthesized sequence, and the internal nodes of the tree other than the root correspond to parenthesized groups. The parenthesized sequence itself may be written around the perimeter of the polygon with its symbols on the sides of the polygon and with parentheses at the endpoints of the selected diagonals. This equivalence provides a bijective proof that all of these kinds of objects are counted by a single integer sequence.\n\nThe same numbers also count the number of double permutations (sequences of the numbers from 1 to \"n\", each number appearing twice, with the first occurrences of each number in sorted order) that avoid the permutation patterns 12312 and 121323.\n\nThe closely related large Schröder numbers are equal to twice the Schröder–Hipparchus numbers, and may also be used to count several types of combinatorial objects including certain kinds of lattice paths, partitions of a rectangle into smaller rectangles by recursive slicing, and parenthesizations in which a pair of parentheses surrounding the whole sequence of elements is also allowed. The Catalan numbers also count closely related sets of objects including subdivisions of a polygon into triangles, plane trees in which all internal nodes have exactly two children, and parenthesizations in which each pair of parentheses surrounds exactly two symbols or parenthesized groups.\n\nThe sequence of Catalan numbers and the sequence of Schröder–Hipparchus numbers, viewed as infinite-dimensional vectors, are the unique eigenvectors for the first two in a sequence of naturally defined linear operators on number sequences. More generally, the \"k\"th sequence in this sequence of integer sequences is (\"x\", \"x\", \"x\", ...) where the numbers \"x\" are calculated as the sums of Narayana numbers multiplied by powers of \"k\":\nSubstituting \"k\" = 1 into this formula gives the Catalan numbers and substituting \"k\" = 2 into this formula gives the Schröder–Hipparchus numbers.\n\nIn connection with the property of Schröder–Hipparchus numbers of counting faces of an associahedron, the number of vertices of the associahedron is given by the Catalan numbers. The corresponding numbers for the permutohedron are respectively the ordered Bell numbers and the factorials.\n\nAs well as the summation formula above, the Schröder–Hipparchus numbers may be defined by a recurrence relation:\nStanley proves this fact using generating functions while Foata and Zeilberger provide a direct combinatorial proof.\n\nAccording to a line in Plutarch's \"Table Talk\", Hipparchus showed that the number of \"affirmative compound propositions\" that can be made from ten simple propositions is 103049 and that the number of negative compound propositions that can be made from ten simple propositions is 310952. This statement went unexplained until 1994, when David Hough, a graduate student at George Washington University, observed that there are 103049 ways of inserting parentheses into a sequence of ten items. A similar explanation can be provided for the other number: it is very close to the average of the tenth and eleventh Schröder–Hipparchus numbers, 310954, and counts bracketings of ten terms together with a negative particle.\n\nThe problem of counting parenthesizations was introduced to modern mathematics by .\n\n"}
{"id": "7820200", "url": "https://en.wikipedia.org/wiki?curid=7820200", "title": "Singular distribution", "text": "Singular distribution\n\nIn probability, a singular distribution is a probability distribution concentrated on a set of Lebesgue measure zero, where the probability of each point in that set is zero. \n\nThese distributions are sometimes called singular continuous distributions, since their cumulative distribution functions are singular and continuous. \n\nSuch distributions are not absolutely continuous with respect to Lebesgue measure.\n\nA singular distribution is not a discrete probability distribution because each discrete point has a zero probability. On the other hand, neither does it have a probability density function, since the Lebesgue integral of any such function would be zero.\n\nIn general, distributions can be described as a discrete distribution (with a probability mass function), an absolutely continuous distribution (with a probability density), a singular distribution (with neither), or can be decomposed into a mixture of these.\n\nAn example is the Cantor distribution; its cumulative distribution function is a devil's staircase.\n\n\n"}
{"id": "2256109", "url": "https://en.wikipedia.org/wiki?curid=2256109", "title": "Situation calculus", "text": "Situation calculus\n\nThe situation calculus is a logic formalism designed for representing and reasoning about dynamical domains. It was first introduced by John McCarthy in 1963. The main version of the situational calculus that is presented in this article is based on that introduced by Ray Reiter in 1991. It is followed by sections about McCarthy's 1986 version and a logic programming formulation.\n\nThe situation calculus represents changing scenarios as a set of first-order logic formulae. The basic elements of the calculus are:\n\n\nA domain is formalized by a number of formulae, namely:\n\n\nA simple robot world will be modeled as a running example. In this world there is a single robot and several inanimate objects. The world is laid out according to a grid so that locations can be specified in terms of formula_1 coordinate points. It is possible for the robot to move around the world, and to pick up and drop items. Some items may be too heavy for the robot to pick up, or fragile so that they break when they are dropped. The robot also has the ability to repair any broken items that it is holding.\n\nThe main elements of the situation calculus are the actions, fluents and the situations. A number of objects are also typically involved in the description of the world. The situation calculus is based on a sorted domain with three sorts: actions, situations, and objects, where the objects include everything that is not an action or a situation. Variables of each sort can be used. While actions, situations, and objects are elements of the domain, the fluents are modeled as either predicates or functions.\n\nThe actions form a sort of the domain. Variables of sort action can be used. Actions can be quantified. A special predicate formula_2 is used to indicate when an action is executable.\n\nIn the example robot world, possible action terms would be formula_3 to model the robot moving to a new location formula_1, and formula_5 to model the robot picking up an object formula_6.\n\nIn the situation calculus, a dynamic world is modeled as progressing through a series of situations as a result of various actions being performed within the world. A situation represents a history of action occurrences. In the Reiter version of the situation calculus described here, a situation does not represent a state, contrarily to the literal meaning of the term and contrarily to the original definition by McCarthy and Hayes. This point has been summarized by Reiter as follows:\n\nThe situation before any actions have been performed is typically denoted formula_7 and called the initial situation. The new situation resulting from the performance of an action is denoted using the function symbol formula_8 (Some other references also use formula_9). This function symbol has a situation and an action as arguments, and a situation as a result, the latter being the situation that results from performing the given action in the given situation.\n\nThe fact that situations are sequences of actions and not states is enforced by an axiom stating that formula_10 is equal to formula_11 if and only if formula_12 and formula_13. This condition makes no sense if situations were states, as two different actions executed in two different states can result in the same state.\n\nIn the example robot world, if the robot's first action is to move to location formula_14, the first action is formula_15 and the resulting situation is formula_16. If its next action is to pick up the ball, the resulting situation is formula_17. Situations terms like formula_16 and formula_17 denote the sequences of executed actions, and not the description of the state that result from execution.\n\nStatements whose truth value may change are modeled by \"relational fluents\", predicates which take a situation as their final argument. Also possible are \"functional fluents\", functions which take a situation as their final argument and return a situation-dependent value. Fluents may be thought of as \"properties of the world\"'.\nIn the example, the fluent formula_20 can be used to indicate that the robot is carrying a particular object in a particular situation. If the robot initially carries nothing, formula_21 is false while formula_22 is true. The location of the robot can be modeled using a functional fluent formula_23 which returns the location formula_1 of the robot in a particular situation.\n\nThe description of a dynamic world is encoded in second order logics using three kinds of formulae: formulae about actions (preconditions and effects), formulae about the state of the world, and foundational axioms.\n\nSome actions may not be executable in a given situation. For example, it is impossible to put down an object unless one is in fact carrying it. The restrictions on the performance of actions are modeled by literals of the form formula_25, where formula_26 is an action, formula_27 a situation, and formula_2 is a special binary predicate denoting executability of actions. In the example, the condition that dropping an object is only possible when one is carrying it is modeled by:\n\nformula_29\n\nAs a more complex example, the following models that the robot can carry only one object at a time, and that some objects are too heavy for the robot to lift (indicated by the predicate formula_30):\n\nformula_31\n\nGiven that an action is possible in a situation, one must specify the effects of that action on the fluents. This is done by the effect axioms. For example, the fact that picking up an object causes the robot to be carrying it can be modeled as:\n\nformula_32\n\nIt is also possible to specify conditional effects, which are effects that depend on the current state. The following models that some objects are fragile (indicated by the predicate formula_33) and dropping them causes them to be broken (indicated by the fluent formula_34):\n\nformula_35\n\nWhile this formula correctly describes the effect of the actions, it is not sufficient to correctly describe the action in logic, because of the frame problem.\n\nWhile the above formulae seem suitable for reasoning about the effects of actions, they have a critical weakness - they cannot be used to derive the \"non-effects\" of actions. For example, it is not possible to deduce that after picking up an object, the robot's location remains unchanged. This requires a so-called frame axiom, a formula like:\n\nformula_36\n\nThe need to specify frame axioms has long been recognised as a problem in axiomatizing dynamic worlds, and is known as the frame problem. As there are generally a very large number of such axioms, it is very easy for the designer to leave out a necessary frame axiom, or to forget to modify all appropriate axioms when a change to the world description is made.\n\nThe successor state axioms \"solve\" the frame problem in the situation calculus. According to this solution, the designer must enumerate as effect axioms all the ways in which the value of a particular fluent can be changed. The effect axioms affecting the value of fluent formula_37 can be written in generalised form as a positive and a negative effect axiom:\n\nformula_38\n\nformula_39\n\nThe formula formula_40 describes the conditions under which action formula_26 in situation formula_27 makes the fluent formula_43 become true in the successor situation formula_10. Likewise, formula_45 describes the conditions under which performing action formula_26 in situation formula_27 makes fluent formula_43 false in the successor situation.\n\nIf this pair of axioms describe all the ways in which fluent formula_43 can change value, they can be rewritten as a single axiom:\n\nformula_50\n\nIn words, this formula states: \"given that it is possible to perform action formula_26 in situation formula_27, the fluent formula_43 would be true in the resulting situation formula_10 if and only if performing formula_26 in formula_27 would make it true, or it is true in situation formula_27 and performing formula_26 in formula_27 would not make it false.\"\n\nBy way of example, the value of the fluent formula_34 introduced above is given by the following successor state axiom:\n\nformula_61\n\nThe properties of the initial or any other situation can be specified by simply stating them as formulae. For example, a fact about the initial state is formalized by making assertions about formula_62 (which is not a state, but a \"situation\"). The following statements model that initially, the robot carries nothing, is at\nlocation formula_63, and there are no broken objects:\n\nformula_64\n\nformula_65\n\nformula_66\n\nThe foundational axioms of the situation calculus formalize the idea that situations are histories by having formula_67. They also include other properties such as the second order induction on situations.\n\nRegression is a mechanism for proving consequences in the situation calculus. It is based on expressing a formula containing the situation formula_10 in terms of a formula containing the action formula_26 and the situation formula_27, but not the situation formula_10. By iterating this procedure, one can end up with an equivalent formula containing only the initial situation formula_7. Proving consequences is supposedly simpler from this formula than from the original one.\n\nGOLOG is a logic programming language based on the situation calculus.\n\nThe main difference between the original situation calculus by McCarthy and Hayes and the one in use today is the interpretation of situations. In the modern version of the situational calculus, a situation is a sequence of actions. Originally, situations were defined as “the complete state of the universe at an instant of time”. It was clear from the beginning that such situations could not be completely described; the idea was simply to give some statements about situations, and derive consequences from them. This is also different from the approach that is taken by the fluent calculus, where a state can be a collection of known facts, that is, a possibly \"incomplete\" description of the universe.\n\nIn the original version of the situation calculus, fluents are not reified. In other words, conditions that can change are represented by predicates and not by functions. Actually, McCarthy and Hayes defined a fluent as a function that depends on the situation, but they then proceeded always using predicates to represent fluents. For example, the fact that it is raining at place formula_73 in the situation formula_27 is represented by the literal formula_75. In the 1986 version of the situation calculus by McCarthy, functional fluents are used. For example, the position of an object formula_73 in the situation formula_27 is represented by the value of formula_78, where formula_79 is a function. Statements about such functions can be given using equality: formula_80 means that the location of the object formula_73 is the same in the two situations formula_27 and formula_83.\n\nThe execution of actions is represented by the function formula_9: the execution of the action formula_26 in the situation formula_27 is the situation formula_87. The effects of actions are expressed by formulae relating fluents in situation formula_27 and fluents in situations formula_87. For example, that the action of opening the door results in the door being open if not locked is represented by: \n\nThe predicates formula_91 and formula_92 represent the conditions of a door being locked and open, respectively. Since these condition may vary, they are represented by predicates with a situation argument. The formula says that if the door is not locked in a situation, then the door is open after executing the action of opening, this action being represented by the constant formula_93.\n\nThese formulae are not sufficient to derive everything that is considered plausible. Indeed, fluents at different situations are only related if they are preconditions and effects of actions; if a fluent is not affected by an action, there is no way to deduce it did not change. For example, the formula above does not imply that formula_94 follows from formula_95, which is what one would expect (the door is not made locked by opening it). In order for inertia to hold, formulae called \"frame axioms\" are needed. These formulae specify all non-effects of actions:\n\nIn the original formulation of the situation calculus, the initial situation, later denoted by formula_7, is not explicitly identified. The initial situation is not needed if situations are taken to be descriptions of the world. For example, to represent the scenario in which the door was closed but not locked and the action of opening it is performed is formalized by taking a constant formula_27 to mean the initial situation and making statements about it (e.g., formula_95). That the door is open after the change is reflected by formula formula_100 being entailed. The initial situation is instead necessary if, like in the modern situation calculus, a situation is taken to be a history of actions, as the initial situation represents the empty sequence of actions.\nThe version of the situation calculus introduced by McCarthy in 1986 differs to the original one for the use of functional fluents (e.g., formula_78 is a term representing the position of formula_73 in the situation formula_27) and for an attempt to use circumscription to replace the frame axioms.\n\nIt is also possible (e.g. Kowalski 1979, Apt and Bezem 1990, Shanahan 1997) to write the situation calculus as a logic program:\n\nformula_104\n\nformula_105\n\nHere formula_106 is a meta-predicate and the variable formula_107 ranges over fluents. The predicates formula_2, formula_109 and formula_110 correspond to the predicates formula_2, formula_112, and formula_113 respectively. The left arrow formula_114 is half of the equivalence formula_115. The other half is implicit in the completion of the program, in which negation is interpreted as negation as failure. Induction axioms are also implicit, and are needed only to prove program properties. Backward reasoning as in SLD resolution, which is the usual mechanism used to execute logic programs, implements regression implicitly.\n\n\n"}
{"id": "699718", "url": "https://en.wikipedia.org/wiki?curid=699718", "title": "Software verification and validation", "text": "Software verification and validation\n\nIn software project management, software testing, and software engineering, verification and validation (V&V) is the process of checking that a software system meets specifications and that it fulfills its intended purpose. It may also be referred to as software quality control. It is normally the responsibility of software testers as part of the software development lifecycle. In simple terms, software verification is: \"Assuming we should build X, does our software achieve its goals without any bugs or gaps?\" On the other hand, software validation is: \"Was X what we should have built? Does X meet the high level requirements?\"\n\nVerification and validation are not the same things, although they are often confused. Boehm succinctly expressed the difference as\n\nBuilding the product right implies the use of the Requirements Specification as input for the next phase of the development process, the design process, the output of which is the Design Specification. Then, it also implies the use of the Design Specification to feed the construction process. Every time the output of a process correctly implements its input specification, the software product is one step closer to final verification. If the output of a process is incorrect, the developers are not building the product the stakeholders want correctly. This kind of verification is called \"artifact or specification verification\".\n\nBuilding the right product implies creating a Requirements Specification that contains the needs and goals of the stakeholders of the software product. If such artifact is incomplete or wrong, the developers will not be able to build the product the stakeholders want. This is a form of \"artifact or specification validation\".\n\nNote: Verification begins before Validation and then they run in parallel until the software product is released. \n\nIt would imply to verify if the specifications are met by running the software but this is not possible (e. g., how can anyone know if the architecture/design/etc. are correctly implemented by running the software?). Only by reviewing its associated artifacts, someone can conclude if the specifications are met.\n\nThe output of each software development process stage can also be subject to verification when checked against its input specification (see the definition by CMMI below).\n\nExamples of artifact verification:\n\nSoftware validation checks that the software product satisfies or fits the intended use (high-level checking), i.e., the software meets the user requirements, not as specification artifacts or as needs of those who will operate the software only; but, as the needs of all the stakeholders (such as users, operators, administrators, managers, investors, etc.). There are two ways to perform software validation: internal and external. During internal software validation, it is assumed that the goals of the stakeholders were correctly understood and that they were expressed in the requirement artifacts precise and comprehensively. If the software meets the requirement specification, it has been internally validated. External validation happens when it is performed by asking the stakeholders if the software meets their needs. Different software development methodologies call for different levels of user and stakeholder involvement and feedback; so, external validation can be a discrete or a continuous event. Successful final external validation occurs when all the stakeholders accept the software product and express that it satisfies their needs. Such final external validation requires the use of an acceptance test which is a dynamic test.\n\nHowever, it is also possible to perform internal static tests to find out if it meets the requirements specification but that falls into the scope of static verification because the software is not running.\n\nRequirements should be validated before the software product as a whole is ready (the waterfall development process requires them to be perfectly defined before design starts; but, iterative development processes do not require this to be so and allow their continual improvement).\n\nExamples of artifact validation:\n\n\nAccording to the Capability Maturity Model (CMMI-SW v1.1),\n\nValidation during the software development process can be seen as a form of User Requirements Specification validation; and, that at the end of the development process is equivalent to Internal and/or External Software validation. Verification, from CMMI's point of view, is evidently of the artifact kind.\n\nIn other words, software verification ensures that the output of each phase of the software development process effectively carry out what its corresponding input artifact specifies (requirement -> design -> software product), while software validation ensures that the software product meets the needs of all the stakeholders (therefore, the requirement specification was correctly and accurately expressed in the first place). Software verification ensures that \"you built it right\" and confirms that the product, as provided, fulfills the plans of the developers. Software validation ensures that \"you built the right thing\" and confirms that the product, as provided, fulfills the intended use and goals of the stakeholders.\n\nThis article has used the strict or narrow definition of verification.\n\nFrom a testing perspective:\n\nBoth verification and validation are related to the concepts of quality and of software quality assurance. By themselves, verification and validation do not guarantee software quality; planning, traceability, configuration management and other aspects of software engineering are required.\n\nWithin the modeling and simulation (M&S) community, the definitions of verification, validation and accreditation are similar:\n\nThe definition of M&S validation focuses on the accuracy with which the M&S represents the real-world intended use(s). Determining the degree of M&S accuracy is required because all M&S are approximations of reality, and it is usually critical to determine if the degree of approximation is acceptable for the intended use(s). This stands in contrast to software validation.\n\nIn mission-critical software systems, where flawless performance is absolutely necessary, formal methods may be used to ensure the correct operation of a system. These formal methods can prove costly, however, representing as much as 80 percent of total software design cost.\n\nA test case is a tool used in the process. Test cases may be prepared for software verification and software validation to determine if the product was built according to the requirements of the user. Other methods, such as reviews, may be used early in the life cycle to provide for software validation.\n\nISVV stands for Independent Software Verification and Validation. ISVV is targeted at safety-critical software systems and aims to increase the quality of software products, thereby reducing risks and costs through the operational life of the software. ISVV provides assurance that software performs to the specified level of confidence and within its designed parameters and defined requirements.\n\nISVV activities are performed by independent engineering teams, not involved in the software development process, to assess the processes and the resulting products. The ISVV team independency is performed at three different levels: financial, managerial and technical.\n\nISVV goes far beyond “traditional” verification and validation techniques, applied by development teams. While the latter aim to ensure that the software performs well against the nominal requirements, ISVV is focused on non-functional requirements such as robustness and reliability, and on conditions that can lead the software to fail. ISVV results and findings are fed back to the development teams for correction and improvement.\n\nISVV derives from the application of IV&V (Independent Verification and Validation) to the software. Early ISVV application (as known today) dates back to the early 1970s when the U.S. Army sponsored the first significant program related to IV&V for the Safeguard Anti-Ballistic Missile System.\n\nBy the end of the 1970s IV&V was rapidly becoming popular. The constant increase in complexity, size and importance of the software lead to an increasing demand on IV&V applied to software (ISVV).\n\nMeanwhile, IV&V (and ISVV for software systems) gets consolidated and is now widely used by organisations such as the DoD, FAA, NASA and ESA. IV&V is mentioned in [DO-178B], [ISO/IEC 12207] and formalised in [IEEE 1012].\n\nInitially in 2004-2005, a European consortium led by the European Space Agency, and composed by DNV(N), Critical Software SA(P), Terma(DK) and CODA Scisys(UK) created the first version of a guide devoted to ISVV, called \"ESA Guide for Independent Verification and Validation\" with support from other organizations, e.g. SoftWcare SL (E) (), etc.\n\nIn 2008 the European Space Agency released a second version, being SoftWcare SL was the supporting editor having received inputs from many different European Space ISVV stakeholders. This guide covers the methodologies applicable to all the software engineering phases in what concerns ISVV.\n\nISVV is usually composed by five principal phases, these phases can be executed sequentially or as results of a tailoring process.\n\n\"ISVV Planning\n\n\"Requirements Verification\n\n\"Design Verification\n\n\"Code Verification\n\n\"Validation\n\nVerification and validation must meet the compliance requirements of law regulated industries, which is often guided by government agencies or industrial administrative authorities. For instance, the FDA requires software versions and patches to be validated.\n\n"}
{"id": "24612539", "url": "https://en.wikipedia.org/wiki?curid=24612539", "title": "Somos sequence", "text": "Somos sequence\n\nIn mathematics, a Somos sequence is a sequence of numbers defined by a certain recurrence relation, described below. They were discovered by mathematician Michael Somos. From the form of their defining recurrence (which involves division), one would expect the terms of the sequence to be fractions, but nevertheless many Somos sequences have the property that all of their members are integers.\n\nFor an integer number \"k\" larger than 1, the Somos-\"k\" sequence formula_1 is defined by the equation\nwhen \"k\" is odd, or by the analogous equation\nwhen \"k\" is even, together with the initial values\n\nFor \"k\" = 2 or 3, these recursions are very simple (there is no addition on the right-hand side) and they define the all-ones sequence (1, 1, 1, 1, 1, 1, ...). In the first nontrivial case, \"k\" = 4, the defining equation is\nwhile for \"k\" = 5 the equation is\n\nThese equations can be rearranged into the form of a recurrence relation, in which the value \"a\" on the left hand side of the recurrence is defined by a formula on the right hand side, by dividing the formula by \"a\". For \"k\" = 4, this yields the recurrence\nwhile for \"k\" = 5 it gives the recurrence\n\nWhile in the usual definition of the Somos sequences, the values of \"a\" for \"i\" < \"k\" are all set equal to 1, it is also possible to define other sequences by using the same recurrences with different initial values.\n\nThe values in the Somos-4 sequence are\nThe values in the Somos-5 sequence are\nThe values in the Somos-6 sequence are\nThe values in the Somos-7 sequence are\n\nThe form of the recurrences describing the Somos sequences involves divisions, making it appear likely that the sequences defined by these recurrence will contain fractional values. Nevertheless, for \"k\" ≤ 7 the Somos sequences contain only integer values. Several mathematicians have studied the problem of proving and explaining this integer property of the Somos sequences; it is closely related to the combinatorics of cluster algebras.\n\nFor \"k\" ≥ 8 the analogously defined sequences eventually contain fractional values. For \"k\" < 7, changing the initial values (but using the same recurrence relation) also typically results in fractional values.\n\n"}
{"id": "53828785", "url": "https://en.wikipedia.org/wiki?curid=53828785", "title": "Starlike tree", "text": "Starlike tree\n\nIn the area of mathematics known as graph theory, a tree is said to be starlike if it has exactly one vertex of degree greater than 2. This high-degree vertex is the root and a starlike tree is obtained by attaching at least three linear graphs to this central vertex.\n\nTwo finite starlike trees are isospectral, i.e. their graph Laplacians have the same spectra, if and only if they are isomorphic. \n"}
{"id": "567391", "url": "https://en.wikipedia.org/wiki?curid=567391", "title": "Trachtenberg system", "text": "Trachtenberg system\n\nThe Trachtenberg system is a system of rapid mental calculation. The system consists of a number of readily memorized operations that allow one to perform arithmetic computations very quickly. It was developed by the Russian Jewish engineer Jakow Trachtenberg in order to keep his mind occupied while being in a Nazi concentration camp.\n\nThe rest of this article presents some methods devised by Trachtenberg. Some of the algorithms Trachtenberg developed are ones for general multiplication, division and addition. Also, the Trachtenberg system includes some specialised methods for multiplying small numbers between 5 and 13.\n\nThe section on addition demonstrates an effective method of checking calculations that can also be applied to multiplication.\n\nThe method for general multiplication is a method to achieve multiplications formula_1 with low space complexity, i.e. as few temporary results as possible to be kept in memory. This is achieved by noting that the final digit is completely determined by multiplying the last digit of the multiplicands. This is held as a temporary result. To find the next to last digit, we need everything that influences this digit: The temporary result, the last digit of formula_2 times the next-to-last digit of formula_3, as well as the next-to-last digit of formula_2 times the last digit of formula_3. This calculation is performed, and we have a temporary result that is correct in the final two digits.\n\nIn general, for each position formula_6 in the final result, we sum for all formula_7:\n\nPeople can learn this algorithm and thus multiply four digit numbers in their head – writing down only the final result. They would write it out starting with the rightmost digit and finishing with the leftmost.\n\nTrachtenberg defined this algorithm with a kind of pairwise multiplication where two digits are multiplied by one digit, essentially only keeping the middle digit of the result. By performing the above algorithm with this pairwise multiplication, even fewer temporary results need to be held.\n\nExample: formula_9\n\nTo find the first digit of the answer, start at the first digit of the multiplicand:\n\nTo find the second digit of the answer, start at the second digit of the multiplicand:\n\nTo find the third digit of the answer, start at the third digit of the multiplicand:\n\nTo find the fourth digit of the answer, start at the fourth digit of the multiplicand:\n\nContinue with the same method to obtain the remaining digits.\nTrachtenberg called this the 2 Finger Method. The calculations for finding the fourth digit from the example above are illustrated at right. The arrow from the nine will always point to the digit of the multiplicand directly above the digit of the answer you wish to find, with the other arrows each pointing one digit to the right. Each arrow head points to a UT Pair, or Product Pair. The vertical arrow points to the product where we will get the Units digit, and the sloping arrow points to the product where we will get the Tens digits of the Product Pair. If an arrow points to a space with no digit there is no calculation for that arrow. As you solve for each digit you will move each of the arrows over the multiplicand one digit to the left until all of the arrows point to prefixed zeros.\n\nAn example of the algorithm multiplying numbers represented as variable length ASCII strings is shown below in C++.\nclass Trachtenberg\npublic:\n\nprivate:\n\n};\n\nDivision in the Trachtenberg System is done much the same as in multiplication but with subtraction instead of addition. Splitting the dividend into smaller Partial Dividends, then dividing this Partial Dividend by only the left-most digit of the divisor will provide the answer one digit at a time. As you solve each digit of the answer you then subtract Product Pairs (UT pairs) and also NT pairs (Number-Tens) from the Partial Dividend to find the next Partial Dividend. The Product Pairs are found between the digits of the answer so far and the divisor. If a subtraction results in a negative number you have to back up one digit and reduce that digit of the answer by one. With enough practice this method can be done in your head.\nA method of adding columns of numbers and accurately checking the result without repeating the first operation. An intermediate sum, in the form of two rows of digits, is produced. The answer is obtained by taking the sum of the intermediate results with an L-shaped algorithm. As a final step, the checking method that is advocated removes both the risk of repeating any original errors and allows the precise column in which an error occurs to be identified at once. It is based on a check (or digit) sums, such as the nines-remainder method.\n\nFor the procedure to be effective, the different operations used in each stage must be kept distinct, otherwise there is a risk of interference.\n\nWhen performing any of these multiplication algorithms the following \"steps\" should be applied.\n\nThe answer must be found one digit at a time starting at the least significant digit and moving left. The last calculation is on the leading zero of the multiplicand.\n\nEach digit has a \"neighbor\", i.e., the digit on its right. The rightmost digit's neighbor is the trailing zero.\n\nThe 'halve' operation has a particular meaning to the Trachtenberg system. It is intended to mean \"half the digit, rounded down\" but for speed reasons people following the Trachtenberg system are encouraged to make this halving process instantaneous. So instead of thinking \"half of seven is three and a half, so three\" it's suggested that one thinks \"seven, three\". This speeds up calculation considerably. In this same way the tables for subtracting digits from 10 or 9 are to be memorized.\n\nAnd whenever the rule calls for adding half of the neighbor, always add 5 if the current digit is odd. This makes up for dropping 0.5 in the next digit's calculation.\n\n\nRule:\n\nExample: 492 × 3 = 1476\n\nWorking from right to left:\n\nRule:\n\nExample: 346×4 = 1384\n\nWorking from right to left:\n\nExample:\n42×5=210\n\n\nExample: 357 × 6 = 2142\n\nWorking right to left,\n\nRule:\n\nExample: 523 × 7 = 3,661.\n\n3661.\n\nRule:\n\nExample: 456 × 8 = 3648\n\nWorking from right to left:\nRules:\nFor rules 9, 8, 4, and 3 only the first digit is subtracted from 10. After that each digit is subtracted from nine instead.\n\nExample: 2,130 × 9 = 19,170\n\nWorking from right to left:\n\nRule: Add the digit to its neighbor. (By \"neighbor\" we mean the digit on the right.)\n\nExample: formula_36\n\nTo illustrate:\n\nThus,\n\nRule: to multiply by 12:\nStarting from the rightmost digit,\ndouble each digit and add the neighbor. (The \"neighbor\" is the digit on the right.)\n\nIf the answer is greater than a single digit, simply carry over the extra digit (which will be a 1 or 2) to the next operation.\nThe remaining digit is one digit of the final result.\n\nExample: formula_40\n\nDetermine neighbors in the multiplicand 0316:\n\n\nThe book contains specific algebraic explanations for each of the above operations.\n\nMost of the information in this article is from the original book.\n\nThe algorithms/operations for multiplication, etc., can be expressed in other more compact ways that the book does not specify, despite the chapter on algebraic description.\nThe 2017 American film \"Gifted\" revolves around a child prodigy who at the age of 7 impresses her teacher by doing calculations in her head using the Trachtenberg system.\n\nThere are many other methods of calculation in mental mathematics. The list below shows a few other methods of calculating, though they may not be entirely mental.\n\n\nFollowing are known programs and sources available as teaching tools\n\nWeb\n\niPhone\n\nAndroid\n\nBlackBerry\n\n\n"}
{"id": "53941", "url": "https://en.wikipedia.org/wiki?curid=53941", "title": "Triangle inequality", "text": "Triangle inequality\n\nIn mathematics, the triangle inequality states that for any triangle, the sum of the lengths of any two sides must be greater than or equal to the length of the remaining side. This statement permits the inclusion of degenerate triangles, but some authors, especially those writing about elementary geometry, will exclude this possibility, thus leaving out the possibility of equality. If , , and are the lengths of the sides of the triangle, with no side being greater than , then the triangle inequality states that\nwith equality only in the degenerate case of a triangle with zero area.\nIn Euclidean geometry and some other geometries, the triangle inequality is a theorem about distances, and it is written using vectors and vector lengths (norms):\nwhere the length of the third side has been replaced by the vector sum . When and are real numbers, they can be viewed as vectors in , and the triangle inequality expresses a relationship between absolute values.\n\nIn Euclidean geometry, for right triangles the triangle inequality is a consequence of the Pythagorean theorem, and for general triangles a consequence of the law of cosines, although it may be proven without these theorems. The inequality can be viewed intuitively in either or . The figure at the right shows three examples beginning with clear inequality (top) and approaching equality (bottom). In the Euclidean case, equality occurs only if the triangle has a angle and two angles, making the three vertices collinear, as shown in the bottom example. Thus, in Euclidean geometry, the shortest distance between two points is a straight line.\n\nIn spherical geometry, the shortest distance between two points is an arc of a great circle, but the triangle inequality holds provided the restriction is made that the distance between two points on a sphere is the length of a minor spherical line segment (that is, one with central angle in ) with those endpoints.\n\nThe triangle inequality is a \"defining property\" of norms and measures of distance. This property must be established as a theorem for any function proposed for such purposes for each particular space: for example, spaces such as the real numbers, Euclidean spaces, the L spaces (), and inner product spaces.\n\nEuclid proved the triangle inequality for distances in plane geometry using the construction in the figure. Beginning with triangle , an isosceles triangle is constructed with one side taken as and the other equal leg along the extension of side . It then is argued that angle , so side . But so the sum of sides . This proof appears in Euclid's Elements, Book 1, Proposition 20.\n\nFor a proper triangle, the triangle inequality, as stated in words, literally translates into three inequalities (given that a proper triangle has side lengths , , that are all positive and excludes the degenerate case of zero area):\nA more succinct form of this inequality system can be shown to be\nAnother way to state it is\nimplying\nand thus that the longest side length is less than the semiperimeter.\n\nA mathematically equivalent formulation is that the area of a triangle with sides \"a\", \"b\", \"c\" must be a real number greater than zero. Heron's formula for the area is\n\nIn terms of either area expression, the triangle inequality imposed on all sides is equivalent to the condition that the expression under the square root sign be real and greater than zero (so the area expression is real and greater than zero).\n\nThe triangle inequality provides two more interesting constraints for triangles whose sides are \"a, b, c\", where \"a ≥ b ≥ c\" and \"formula_8\" is the golden ratio, as\n\nIn the case of right triangles, the triangle inequality specializes to the statement that the hypotenuse is greater than either of the two sides, and less than their sum.\n\nThe second part of this theorem is already established above for any side of any triangle. The first part is established using the lower figure. In the figure, consider the right triangle . An isosceles triangle is constructed with equal sides . From the triangle postulate, the angles in the right triangle satisfy:\nLikewise, in the isosceles triangle , the angles satisfy:\nTherefore,\nand so, in particular,\nThat means side opposite angle is shorter than side opposite the larger angle . But . Hence:\nA similar construction shows , establishing the theorem.\n\nAn alternative proof (also based upon the triangle postulate) proceeds by considering three positions for point : (i) as depicted (which is to be proven), or (ii) coincident with (which would mean the isosceles triangle had two right angles as base angles plus the vertex angle , which would violate the triangle postulate), or lastly, (iii) interior to the right triangle between points and (in which case angle is an exterior angle of a right triangle and therefore larger than , meaning the other base angle of the isosceles triangle also is greater than and their sum exceeds in violation of the triangle postulate).\n\nThis theorem establishing inequalities is sharpened by Pythagoras' theorem to the equality that the square of the length of the hypotenuse equals the sum of the squares of the other two sides.\n\nConsider a triangle whose sides are in an arithmetic progression and let the sides be , , . Then the triangle inequality requires that\n\nThe first of these quadratic inequalities requires to range in the region beyond the value of the positive root of the quadratic equation , i.e. where is the golden ratio. The second quadratic inequality requires to range between 0 and the positive root of the quadratic equation , i.e. . The combined requirements result in being confined to the range\n\nWhen the common ratio is chosen such that it generates a right triangle that is always similar to the Kepler triangle.\n\nThe triangle inequality can be extended by mathematical induction to arbitrary polygonal paths, showing that the total length of such a path is no less than the length of the straight line between its endpoints. Consequently, the length of any polygon side is always less than the sum of the other polygon side lengths.\n\nConsider a quadrilateral whose sides are in a geometric progression and let the sides be , , , . Then the generalized polygon inequality requires that\n\nThe left-hand side polynomials of these two inequalities have roots that are the tribonacci constant and its reciprocal. Consequently, is limited to the range where is the tribonacci constant.\n\nThis generalization can be used to prove that the shortest curve between two points in Euclidean geometry is a straight line.\n\nNo polygonal path between two points is shorter than the line between them. This implies that no curve can have an arc length less than the distance between its endpoints. By definition, the arc length of a curve is the least upper bound of the lengths of all polygonal approximations of the curve. The result for polygonal paths shows that the straight line between the endpoints is shortest of all the polygonal approximations. Because the arc length of the curve is greater than or equal to the length of every polygonal approximation, the curve itself cannot be shorter than the straight line path.\n\nThe converse of the triangle inequality theorem is also true: if three real numbers are such that each is less than the sum of the others, then there exists a triangle with these numbers as its side lengths and with positive area; and if one number equals the sum of the other two, there exists a degenerate triangle (i.e., with zero area) with these numbers as its side lengths.\n\nIn either case, if the side lengths are \"a, b, c\" we can attempt to place a triangle in the Euclidean plane as shown in the diagram. We need to prove that there exists a real number \"h\" consistent with the values \"a, b,\" and \"c\", in which case this triangle exists.\n\nBy the Pythagorean theorem we have and according to the figure at the right. Subtracting these yields . This equation allows us to express in terms of the sides of the triangle:\nFor the height of the triangle we have that . By replacing with the formula given above, we have\n\nFor a real number \"h\" to satisfy this, formula_22 must be non-negative:\nwhich holds if the triangle inequality is satisfied for all sides. Therefore there does exist a real number \"h\" consistent with the sides \"a, b, c\", and the triangle exists. If each triangle inequality holds strictly, \"h\" > 0 and the triangle is non-degenerate (has positive area); but if one of the inequalities holds with equality, so \"h\" = 0, the triangle is degenerate.\n\nIn Euclidean space, the hypervolume of an -facet of an -simplex is less than or equal to the sum of the hypervolumes of the other facets. In particular, the area of a triangular face of a tetrahedron is less than or equal to the sum of the areas of the other three sides.\n\nIn a normed vector space , one of the defining properties of the norm is the triangle inequality:\n\nthat is, the norm of the sum of two vectors is at most as large as the sum of the norms of the two vectors. This is also referred to as subadditivity. For any proposed function to behave as a norm, it must satisfy this requirement.\n\nIf the normed space is euclidean, or, more generally, strictly convex, then formula_29 if and\nonly if the triangle formed by , , and , is degenerate, that is,\nthe spaces with . However, there are normed spaces in which this is\nnot true. For instance, consider the plane with the norm (the Manhattan distance) and\ndenote and . Then the triangle formed by\n, , and , is non-degenerate but\n\n\nProof:\n\nAfter adding, \nUse the fact that formula_35\n(with \"b\" replaced by \"x\"+\"y\" and \"a\" by formula_36), we have\n\nThe triangle inequality is useful in mathematical analysis for determining the best upper estimate on the size of the sum of two numbers, in terms of the sizes of the individual numbers.\n\nThere is also a lower estimate, which can be found using the \"reverse triangle inequality\" which states that for any real numbers and :\n\n\nThe Cauchy–Schwarz inequality turns into an equality if and only if and \nare linearly dependent. The inequality\nformula_43\nturns into an equality for linearly dependent formula_39 and formula_40\nif and only if one of the vectors or is a \"nonnegative\" scalar of the other.\n\n\nIn a metric space with metric , the triangle inequality is a requirement upon distance:\n\nfor all , , in . That is, the distance from to is at most as large as the sum of the distance from to and the distance from to .\n\nThe triangle inequality is responsible for most of the interesting structure on a metric space, namely, convergence. This is because the remaining requirements for a metric are rather simplistic in comparison. For example, the fact that any convergent sequence in a metric space is a Cauchy sequence is a direct consequence of the triangle inequality, because if we choose any and such that and , where is given and arbitrary (as in the definition of a limit in a metric space), then by the triangle inequality, , so that the sequence is a Cauchy sequence, by definition.\n\nThis version of the triangle inequality reduces to the one stated above in case of normed vector spaces where a metric is induced via , with being the vector pointing from point to .\n\nThe reverse triangle inequality is an elementary consequence of the triangle inequality that gives lower bounds instead of upper bounds. For plane geometry the statement is:\n\nIn the case of a normed vector space, the statement is:\nor for metric spaces, .\nThis implies that the norm formula_51 as well as the distance function formula_52 are Lipschitz continuous with Lipschitz constant , and therefore are in particular uniformly continuous.\n\nThe proof for the reverse triangle uses the regular triangle inequality, and formula_53:\n\nCombining these two statements gives:\n\nIn Minkowski space, if \"x\" and \"y\" are both timelike vectors lying in the future light cone, the triangle inequality is reversed:\n\nA physical example of this inequality is the twin paradox in special relativity. The same reversed form of the inequality holds if both vectors lie in the past light cone, and if one or both are null vectors. The result holds in \"n\"+1 dimensions for any \"n\"≥1. If the plane defined by \"x\" and \"y\" is spacelike (and therefore a euclidean subspace) then the usual triangle inequality holds.\n\n\n"}
{"id": "13048500", "url": "https://en.wikipedia.org/wiki?curid=13048500", "title": "Vantieghems theorem", "text": "Vantieghems theorem\n\nIn number theory, Vantieghems theorem is a primality criterion. It states that a natural number \"n\" is prime if and only if\n\nSimilarly, \"n\" is prime, if and only if the following congruence for polynomials in \"X\" holds:\n\nor:\n\nLet n=7 forming the product 1*3*7*15*31*63 = 615195. 615195 = 7 mod 127 and so 7 is prime<br>\nLet n=9 forming the product 1*3*7*15*31*63*127*255 = 19923090075. 19923090075 = 301 mod 511 and so 9 is composite\n\n"}
{"id": "41918550", "url": "https://en.wikipedia.org/wiki?curid=41918550", "title": "Zemor's decoding algorithm", "text": "Zemor's decoding algorithm\n\nIn coding theory, Zemor's algorithm, designed and developed by Gilles Zemor, is a recursive low-complexity approach to code construction. It is an improvement over the algorithm of Sipser and Spielman.\n\nZemor considered a typical class of Sipser–Spielman construction of expander codes, where the underlying graph is bipartite graph. Sipser and Spielman introduced a constructive family of asymptotically good linear-error codes together with a simple parallel algorithm that will always remove a constant fraction of errors. The article is based on Dr. Venkatesan Guruswami's course notes \n\nZemor's algorithm is based on a type of expander graphs called Tanner graph. The construction of code was first proposed by Tanner. The codes are based on double cover formula_1, regular expander formula_2, which is a bipartite graph. formula_2 =formula_4, where formula_5 is the set of vertices and formula_6 is the set of edges and formula_5 = formula_8 formula_9 formula_10 and formula_8 formula_12 formula_10 = formula_14, where formula_8 and formula_10 denotes the set of 2 vertices. Let formula_17 be the number of vertices in each group, \"i.e\", formula_18. The edge set formula_6 be of size formula_20 =formula_21 and every edge in formula_6 has one endpoint in both formula_8 and formula_10. formula_25 denotes the set of edges containing formula_26.\n\nAssume an ordering on formula_5, therefore ordering will be done on every edges of formula_25 for every formula_29. Let finite field formula_30, and for a word formula_31 in formula_32, let the subword of the word will be indexed by formula_25. Let that word be denoted by formula_34. The subset of vertices formula_8 and formula_10 induces every word formula_37 a partition into formula_17 non-overlapping sub-words formula_39, where formula_26 ranges over the elements of formula_8. \nFor constructing a code formula_42, consider a linear subcode formula_43, which is a formula_44 code, where formula_45, the size of the alphabet is formula_46. For any vertex formula_29, let formula_48 be some ordering of the formula_1 vertices of formula_6 adjacent to formula_26. In this code, each bit formula_52 is linked with an edge formula_53 of formula_6. \n\nWe can define the code formula_42 to be the set of binary vectors formula_56 of formula_57 such that, for every vertex formula_26 of formula_5, formula_60 is a code word of formula_43. In this case, we can consider a special case when every vertex of formula_6 is adjacent to exactly formula_46 vertices of formula_5. It means that formula_5 and formula_6 make up, respectively, the vertex set and edge set of formula_1 regular graph formula_2.\n\nLet us call the code formula_42 constructed in this way as formula_70 code. For a given graph formula_2 and a given code formula_43, there are several formula_70 codes as there are different ways of ordering edges incident to a given vertex formula_26, i.e., formula_75. In fact our code formula_42 consist of all codewords such that formula_77 for all formula_78. The code formula_79 is linear formula_80 in formula_81 as it is generated from a subcode formula_43, which is linear. The code formula_42 is defined as formula_84 for every formula_85.\n\nIn this figure, formula_86. It shows the graph formula_2 and code formula_42.\n\nIn matrix formula_2, let formula_90 is equal to the second largest eigen value of adjacency matrix of formula_2. Here the largest eigen value is formula_92. \nTwo important claims are made:\n\nformula_93<br>\n\". Let formula_94 be the rate of a linear code constructed from a bipartite graph whose digit nodes have degree formula_95 and whose subcode nodes have degree formula_17. If a single linear code with parameters formula_97 and rate formula_98 is associated with each of the subcode nodes, then formula_99\".\n\nLet formula_94 be the rate of the linear code, which is equal to formula_101\nLet there are formula_102 subcode nodes in the graph. If the degree of the subcode is formula_17, then the code must have formula_104 digits, as each digit node is connected to formula_95 of the formula_106 edges in the graph. Each subcode node contributes formula_107 equations to parity check matrix for a total of formula_108. These equations may not be linearly independent. \nTherefore, formula_109<br>\nformula_110<br>\nformula_111, Since the value of formula_95, i.e., the digit node of this bipartite graph is formula_46 and here formula_114, we can write as:<br>\nformula_115\n\n\"If formula_119 is linear code of rate formula_120, block code length formula_92, and minimum relative distance formula_122, and if formula_10 is the edge vertex incidence graph of a formula_1 – regular graph with second largest eigen value formula_90, then the code formula_126 has rate at least formula_127 and minimum relative distance at least formula_128.\n\nLet formula_129 be derived from the formula_1 regular graph formula_2. So, the number of variables of formula_126 is formula_133 and the number of constraints is formula_17. According to Alon - Chung, if formula_135 is a subset of vertices of formula_2 of size formula_137, then the number of edges contained in the subgraph is induced by formula_135 in formula_2 is at most formula_140. \n\nAs a result, any set of formula_141 variables will be having at least formula_137 constraints as neighbours. So the average number of variables per constraint is : formula_143\nformula_144 formula_145\n\nSo if formula_146, then a word of relative weight formula_147, cannot be a codeword of formula_126. The inequality formula_149 is satisfied for formula_150. Therefore, formula_126 cannot have a non zero codeword of relative weight formula_152 or less.\n\nIn matrix formula_2, we can assume that formula_154 is bounded away from formula_155. For those values of formula_92 in which formula_157 is odd prime, there are explicit constructions of sequences of formula_92 - regular bipartite graphs with arbitrarily large number of vertices such that each graph formula_2 in the sequence is a Ramanujan graph. It is called Ramanujan graph as it satisfies the inequality formula_160. Certain expansion properties are visible in graph formula_2 as the separation between the eigen values formula_1 and formula_163. If the graph formula_164 is Ramanujan graph, then that expression formula_165 will become formula_166 eventually as formula_167 becomes large.\n\nThe iterative decoding algorithm written below alternates between the vertices formula_8 and formula_10 in formula_2 and corrects the codeword of formula_43 in formula_8 and then it switches to correct the codeword formula_43 in formula_10. Here edges associated with a vertex on one side of a graph are not incident to other vertex on that side. In fact, it doesn't matter in which order, the set of nodes formula_8 and formula_10 are processed. The vertex processing can also be done in parallel. \n\nThe decoder formula_177stands for a decoder for formula_43 that recovers correctly with any codewords with less than formula_179 errors.\n\nReceived word : formula_180<br> \ncodice_1\nOutput: formula_192\n\nSince formula_2 is bipartite, the set formula_8 of vertices induces the partition of the edge set formula_6 = formula_196 . The set formula_10 induces another partition, formula_6 = formula_199 .\n\nLet formula_200 be the received vector, and recall that formula_201. The first iteration of the algorithm consists of applying the complete decoding for the code induced by formula_202 for every formula_203 . This means that for replacing, for every formula_203, the vector formula_205 by one of the closest codewords of formula_43. Since the subsets of edges formula_202 are disjoint for formula_203, the decoding of these formula_17 subvectors of formula_210 may be done in parallel.\n\nThe iteration will yield a new vector formula_192. The next iteration consists of applying the preceding procedure to formula_192 but with formula_8 replaced by formula_10. In other words, it consists of decoding all the subvectors induced by the vertices of formula_10. The coming iterations repeat those two steps alternately applying parallel decoding to the subvectors induced by the vertices of formula_8 and to the subvectors induced by the vertices of formula_10. <br>\nNote: [If formula_218 and formula_2 is the complete bipartite graph, then formula_42 is a product code of formula_43 with itself and the above algorithm reduces to the natural hard iterative decoding of product codes].\n\nHere, the number of iterations, formula_95 is formula_223. \nIn general, the above algorithm can correct a code word whose Hamming weight is no more than formula_224 for values of formula_225. Here, the decoding algorithm is implemented as a circuit of size formula_226 and depth formula_227 that returns the codeword given that error vector has weight less than formula_228 .\n\n\"If formula_2 is a Ramanujan graph of sufficiently high degree, for any formula_225, the decoding algorithm can correct formula_231 errors, in formula_232 rounds ( where the big- formula_233 notation hides a dependence on formula_234). This can be implemented in linear time on a single processor; on formula_17 processors each round can be implemented in constant time.\"\n\nSince the decoding algorithm is insensitive to the value of the edges and by linearity, we can assume that the transmitted codeword is the all zeros - vector. Let the received codeword be formula_210. The set of edges which has an incorrect value while decoding is considered. Here by incorrect value, we mean formula_155 in any of the bits. Let formula_238 be the initial value of the codeword, formula_239 be the values after first, second . . . formula_185 stages of decoding. \nHere, formula_241, and formula_242. Here formula_243 corresponds to those set of vertices that was not able to successfully decode their codeword in the formula_244 round. From the above algorithm formula_245is a decreasing sequence.\nIn fact, formula_246. As we are assuming, formula_247, the above equation is in a geometric decreasing sequence. \nSo, when formula_248, more than formula_249 rounds are necessary. Furthermore, formula_250, and if we implement the formula_244 round in formula_252 time, then the total sequential running time will be linear.\n\ngiven in.\n\n"}
