{"id": "3334535", "url": "https://en.wikipedia.org/wiki?curid=3334535", "title": "186 (number)", "text": "186 (number)\n\n186 (one hundred [and] eighty-six) is the natural number following 185 and preceding 187.\n\n\n\n\n\n\n186 is also:\n\n"}
{"id": "2139612", "url": "https://en.wikipedia.org/wiki?curid=2139612", "title": "A Course of Modern Analysis", "text": "A Course of Modern Analysis\n\n\"A Course of Modern Analysis: an introduction to the general theory of infinite processes and of analytic functions; with an account of the principal transcendental functions\" (colloquially known as \"Whittaker and Watson\") is a landmark textbook on mathematical analysis written by E. T. Whittaker and G. N. Watson, first published by Cambridge University Press in 1902. (The first edition was Whittaker's alone; it was in later editions with Watson that this book is best known.)\n\nIts first, second, third, and the fourth, last edition were published in 1902, 1915, 1920, and 1927, respectively. Since then, it has continuously been reprinted and still in print today.\n\nThe book is notable for being the standard reference and textbook for a generation of Cambridge mathematicians including Littlewood and G. H. Hardy. Mary Cartwright studied it as preparation for her final honours on the advice of fellow student V.C. Morton, later Professor of Mathematics at Aberystwyth University. But its reach was much further than just the Cambridge school; André Weil in his obituary of the French mathematician Jean Delsarte noted that Delsarte always had a copy on his desk.\n\nToday, the book retains much of its original appeal. Some idiosyncratic but interesting problems from the salad days of the Cambridge Mathematical Tripos are to be found in the exercises. It is terse, yet readable by the motivated student. It conforms to high standards of mathematical rigour, while compressing much actual formulaic information also.\n\nThe book was one of the earliest to use decimal numbering for its sections, an innovation the authors attribute to Giuseppe Peano.\n\nBelow is the contens of the fourth edition:\n\n\n\n"}
{"id": "55866076", "url": "https://en.wikipedia.org/wiki?curid=55866076", "title": "Acylindrically hyperbolic group", "text": "Acylindrically hyperbolic group\n\nIn the mathematical subject of geometric group theory, an acylindrically hyperbolic group is a group admitting a non-elementary 'acylindrical' isometric action on some geodesic hyperbolic metric space. This notion generalizes the notions of a hyperbolic group and of a relatively hyperbolic group and includes a significantly wider class of examples, such as mapping class groups and Out(\"F\").\n\nLet \"G\" be a group with an isometric action on some geodesic hyperbolic metric space \"X\". This action is called acylindrical if for every formula_1 there exist formula_2 such that for every formula_3 with formula_4 one has\n\nIf the above property holds for a specific formula_1, the action of \"G\" on \"X\" is called \"R\"-acylindrical. The notion of acylindricity provides a suitable substitute for being a proper action in the more general context where non-proper actions are allowed. \n\nAn acylindrical isometric action of a group \"G\" on a geodesic hyperbolic metric space \"X\" is non-elementary if G admits two independent hyperbolic isometries of \"X\", that is, two loxodromic elements formula_7 such that their fixed point sets formula_8 and formula_9 are disjoint. \n\nIt is known (Theorem 1.1 in ) that an acylindrical action of a group \"G\" on a geodesic hyperbolic metric space \"X\" is non-elementary if and only if this action has unbounded orbits in \"X\" and the group \"G\" is not a finite extension of a cyclic group generated by loxodromic isometry of \"X\".\n\nA group \"G\" is called acylindrically hyperbolic if \"G\" admits a non-elementary acylindrical isometric action on some geodesic hyperbolic metric space \"X\".\n\nIt is known (Theorem 1.2 in ) that for a group \"G\" the following conditions are equivalent:\n\n\n"}
{"id": "31962097", "url": "https://en.wikipedia.org/wiki?curid=31962097", "title": "An Introduction to the Theory of Numbers", "text": "An Introduction to the Theory of Numbers\n\nAn Introduction to the Theory of Numbers is a classic book in the field of number theory, by G. H. Hardy and E. M. Wright.\n\nThe book grew out of a series of lectures by Hardy and Wright and was first published in 1938.\n\nThe third edition added an elementary proof of the prime number theorem, and the sixth edition added a chapter on elliptic curves.\n\n\n"}
{"id": "21257247", "url": "https://en.wikipedia.org/wiki?curid=21257247", "title": "Arrival theorem", "text": "Arrival theorem\n\nIn queueing theory, a discipline within the mathematical theory of probability, the arrival theorem (also referred to as the random observer property, ROP or job observer property) states that \"upon arrival at a station, a job observes the system as if in steady state at an arbitrary instant for the system without that job.\"\n\nThe arrival theorem always holds in open product-form networks with unbounded queues at each node, but it also holds in more general networks. A necessary and sufficient condition for the arrival theorem to be satisfied in product-form networks is given in terms of Palm probabilities in Boucherie & Dijk, 1997. A similar result also holds in some closed networks. Examples of product-form networks where the arrival theorem does not hold include reversible Kingman networks and networks with a delay protocol.\n\nMitrani offers the intuition that \"The state of node \"i\" as seen by an incoming job has a different distribution from the state seen by a random observer. For instance, an incoming job can never see all \"'k\" jobs present at node \"i\", because it itself cannot be among the jobs already present.\"\n\nFor Poisson processes the property is often referred to as the PASTA property (Poisson Arrivals See Time Averages) and states that the probability of the state as seen by an outside random observer is the same as the probability of the state seen by an arriving customer. The property also holds for the case of a doubly stochastic Poisson process where the rate parameter is allowed to vary depending on the state.\n\nIn an open Jackson network with \"m\" queues, write formula_1 for the state of the network. Suppose formula_2 is the equilibrium probability that the network is in state formula_3. Then the probability that the network is in state formula_3 immediately before an arrival to any node is also formula_2.\n\nNote that this theorem does not follow from Jackson's theorem, where the steady state in continuous time is considered. Here we are concerned with particular points in time, namely arrival times. This theorem first published by Sevcik and Mitrani in 1981.\n\nIn a closed Gordon–Newell network with \"m\" queues, write formula_6 for the state of the network. For a customer in transit to state formula_7, let formula_8 denote the probability that immediately before arrival the customer 'sees' the state of the system to be\n\nThis probability, formula_8, is the same as the steady state probability for state formula_11 for a network of the same type with \"one customer less\". It was published independently by Sevcik and Mitrani, and Reiser and Lavenberg, where the result was used to develop mean value analysis.\n"}
{"id": "16728132", "url": "https://en.wikipedia.org/wiki?curid=16728132", "title": "Basis (universal algebra)", "text": "Basis (universal algebra)\n\nIn universal algebra a basis is a structure inside of some (universal) algebras, which are called free algebras. It generates all algebra elements from its own elements by the algebra operations in an independent manner. It also represents the endomorphisms of an algebra by certain indexings of algebra elements, which can correspond to the usual matrices when the free algebra is a vector space.\n\nThe basis (or reference frame) of a (universal) algebra is a function \"b\" that takes some algebra elements as values formula_1 and satisfies either one of the following two equivalent conditions. Here, the set of all formula_1 is called basis set, whereas several authors call it the \"basis\". The set formula_3 of its arguments \"i\" is called dimension set. Any function, with all its arguments in the whole formula_3, that takes algebra elements as values (even outside the basis set) will be denoted by \"m\". Then, \"b\" will be an \"m\".\n\nThis condition will define bases by the set \"L\" of the formula_3-ary elementary functions of the algebra, which are certain functions formula_6 that take every \"m\" as argument to get some algebra element as value formula_7. In fact, they consist of all the projections formula_8 with \"i\" in formula_3, which are the functions such that formula_10 for each \"m\", and of all functions that rise from them by repeated \"multiple compositions\" with operations of the algebra. \n\n(When an algebra operation has a single algebra element as argument, the value of such a composed function is the one that the operation takes from the value of a single previously computed formula_3-ary function as in composition. When it does not, such compositions require that many (or none for a nullary operation) formula_3-ary functions are evaluated before the algebra operation: one for each possible algebra element in that argument. In case formula_3 and the numbers of elements in the arguments, or “arity”, of the operations are finite, this is the finitary multiple composition .) \n\nThen, according to the \"outer condition\" a basis has to \"generate\" the algebra (namely when formula_6 ranges over the whole \"L\", formula_15 gets every algebra element) and must be \"independent\" (namely whenever any two formula_3-ary elementary functions coincide at \"b\", they will do everywhere: formula_17 implies formula_18). This is the same as to require that there exists a \"single\" function formula_19 that takes every algebra element as argument to get an formula_3-ary elementary function as value and satisfies formula_21 for all formula_6 in \"L\".\n\nThis other condition will define bases by the set \"E\" of the endomorphisms of the algebra, which are the homomorphisms from the algebra into itself, through its analytic representation formula_23 by a basis. The latter is a function that takes every endomorphism \"e\" as argument to get a function \"m\" as value: formula_24, where this \"m\" is the \"sample\" of the values of \"e\" at \"b\", namely formula_25 for all \"i\" in the dimension set.\n\nThen, according to the \"inner condition\" \"b\" is a basis, when formula_23 is a bijection from \"E\" onto the set of all \"m\", namely for each \"m\" there is one and only one endomorphism \"e\" such that formula_27. This is the same as to require that there exists an extension function, namely a function formula_28 that takes every (sample) \"m\" as argument to extend it onto an endomorphism formula_29 such that formula_30.\n\nThe link between these two conditions is given by the identity formula_31, which holds for all \"m\" and all algebra elements \"a\". Several other conditions that characterize bases for universal algebras are omitted.\n\nAs the next example will show, present bases are a generalization of the bases of vector spaces. Then, the name \"reference frame\" can well replace \"basis\". Yet, contrary to the vector space case, a universal algebra might lack bases and, when it has them, their dimension sets might have different finite positive cardinalities.\n\nIn the universal algebra corresponding to a vector space with positive dimension the bases essentially are the ordered bases of this vector space. Yet, this will come after several details.\n\nWhen the vector space is finite-dimensional, for instance formula_32 with formula_33, the functions formula_6 in the set \"L\" of the \"outer condition\" exactly are the ones that provide the spanning and linear independence properties with linear combinations formula_35 and present generator property becomes the spanning one. On the contrary, linear independence is a mere instance of present independence, which becomes equivalent to it in such vector spaces. (Also, several other generalizations of linear independence for universal algebras do not imply present independence.)\n\nThe functions \"m\" for the \"inner condition\" correspond to the square arrays of field numbers (namely, usual vector-space square matrices) that serve to build the endomorphisms of vector spaces (namely, linear maps into themselves). Then, the \"inner condition\" requires a bijection property from endomorphisms also to arrays. In fact, each column of such an array represents a vector formula_36 as its \"n\"-tuple of coordinates with respect to the basis \"b\". For instance, when the vectors are \"n\"-tuples of numbers from the underlying field and \"b\" is the Kronecker basis, \"m\" is such an array \"seen by columns\", formula_23 is the sample of such a linear map at the reference vectors and formula_28 extends this sample to this map as below.\n\nformula_39 \n\nWhen the vector space is not finite-dimensional, further distinctions are needed. In fact, though the functions formula_6 formally have an infinity of vectors in every argument, the linear combinations they evaluate never require infinitely many addenda formula_41 and each formula_6 determines a finite subset \"J\" of formula_3 that contains all required \"i\". Then, every value formula_7 equals an formula_45, where formula_46 is the restriction of \"m\" to \"J\" and formula_47 is the \"J\"-ary elementary function corresponding to formula_6. When the formula_47 replace the formula_6, both the linear independence and spanning properties for infinite basis sets follow from present \"outer condition\" and conversely. \n\nTherefore, as far as vector spaces of a positive dimension are concerned, the only difference between present bases for universal algebras and the ordered bases of vector spaces is that here no order on formula_3 is required. Still it is allowed, in case it serves some purpose. \n\nWhen the space is zero-dimensional, its ordered basis is empty. Then, being the empty function, it is a present basis. Yet, since this space only contains the null vector and its only endomorphism is the identity, any function \"b\" from any set formula_3 (even a nonempty one) to this singleton space work as a present basis. This is not so strange from the point of view of Universal Algebra, where singleton algebras, which are called \"trivial\", enjoy a lot of other seeming strange properties.\n\nLet formula_53 be an \"alphabet\", namely a (usually finite) set of objects called \"letters\". Let \"W\" denote the corresponding set of words or \"strings\", which will be denoted as in strings, namely either by writing their letters in sequence or by formula_54 in case of the empty word (Formal Language notation). Accordingly, the juxtaposition \"formula_55\" will denote the concatenation of two words \"v\" and \"w\", namely the word that begins with \"v\" and is followed by \"w\".\n\nConcatenation is a binary operation on \"W\" that together with the empty word formula_54 defines a free monoid, the monoid of the words on formula_3, which is one of the simplest universal algebras. Then, the \"inner condition\" will immediately prove that one of its bases is the function \"b\" that makes a single-letter word formula_58 of each letter formula_59, formula_60.\n\n(Depending on the set-theoretical implementation of sequences, \"b\" may not be an identity function, namely formula_61 may not be formula_59, rather an object like formula_63, namely a singleton function, or a pair like formula_64 or formula_65.) \n\nIn fact, in the theory of D0L systems (Rozemberg & Salomaa 1980) such formula_27 are the tables of \"productions\", which such systems use to define the simultaneous substitutions of every formula_61 by a single word formula_68 in any word \"u\" in \"W\": if formula_69, then formula_70. Then, \"b\" satisfies the \"inner condition\", since the function formula_23 is the well-known bijection that identifies every word endomorphism with any such table. (The repeated applications of such an endomorphism starting from a given \"seed\" word are able to model many growth processes, where words and concatenation serve to build fairly heterogeneous structures as in L-system, not just \"sequences\".)\n\n"}
{"id": "37834267", "url": "https://en.wikipedia.org/wiki?curid=37834267", "title": "Binary Goppa code", "text": "Binary Goppa code\n\nIn mathematics and computer science, the binary Goppa code is an error-correcting code that belongs to the class of general Goppa codes originally described by Valerii Denisovich Goppa, but the binary structure gives it several mathematical advantages over non-binary variants, also providing a better fit for common usage in computers and telecommunication. Binary Goppa codes have interesting properties suitable for cryptography in McEliece-like cryptosystems and similar setups.\n\nA binary Goppa code is defined by a polynomial formula_1 of degree formula_2 over a finite field formula_3 without multiple zeros, and a sequence formula_4 of formula_5 distinct elements from formula_3 that aren't roots of the polynomial:\n\nCodewords belong to the kernel of syndrome function, forming a subspace of formula_8:\n\nCode defined by a tuple formula_10 has minimum distance formula_11, thus it can correct formula_12 errors in a word of size formula_13 using codewords of size formula_5. It also possesses a convenient parity-check matrix formula_15 in form\n\nNote that this form of the parity-check matrix, being composed of a Vandermonde matrix formula_17 and diagonal matrix formula_18, shares the form with check matrices of alternant codes, thus alternant decoders can be used on this form. Such decoders usually provide only limited error-correcting capability (in most cases formula_19).\n\nFor practical purposes, parity-check matrix of a binary Goppa code is usually converted to a more computer-friendly binary form by a trace construction, that converts the formula_2-by-formula_5 matrix over formula_3 to a formula_23-by-formula_5 binary matrix by writing polynomial coefficients of formula_3 elements on formula_26 successive rows.\n\nDecoding of binary Goppa codes is traditionally done by Patterson algorithm, which gives good error-correcting capability (it corrects all formula_2 design errors), and is also fairly simple to implement.\n\nPatterson algorithm converts a syndrome to a vector of errors. The syndrome of a word formula_28 is expected to take a form of\n\nAlternative form of a parity-check matrix based on formula for formula_30 can be used to produce such syndrome with a simple matrix multiplication.\n\nThe algorithm then computes formula_31. That fails when formula_32, but that is the case when the input word is a codeword, so no error correction is necessary.\n\nformula_33 is reduced to polynomials formula_34 and formula_35 using the extended euclidean algorithm, so that formula_36, while formula_37 and formula_38.\n\nFinally, the \"error locator polynomial\" is computed as formula_39. Note that in binary case, locating the errors is sufficient to correct them, as there's only one other value possible. In non-binary cases a separate error correction polynomial has to be computed as well.\n\nIf the original codeword was decodable and the formula_40 was the error vector, then\n\nFactoring or evaluating all roots of formula_42 therefore gives enough information to recover the error vector and fix the errors.\n\nBinary Goppa codes viewed as a special case of Goppa codes have the interesting property that they correct full formula_43 errors, while only formula_44 errors in ternary and all other cases. Asymptotically, this error correcting capability meets the famous Gilbert–Varshamov bound.\n\nBecause of the high error correction capacity compared to code rate and form of parity-check matrix (which is usually hardly distinguishable from a random binary matrix of full rank), the binary Goppa codes are used in several post-quantum cryptosystems, notably McEliece cryptosystem and Niederreiter cryptosystem.\n\n\n"}
{"id": "41819221", "url": "https://en.wikipedia.org/wiki?curid=41819221", "title": "Bogdan Suceavă", "text": "Bogdan Suceavă\n\nBogdan Suceavă (born September 27, 1969 in Curtea de Argeș) is a Romanian-born U.S. mathematician and writer.\n\nBogdan Suceavă was born in Curtea de Argeș, Romania, on September 27, 1969. Growing up, Suceavă spent his holidays with his maternal grandparents at Nucșoara, a remote community that maintained its traditions, unbroken by the collectivisation elsewhere of Ceaușescu regime. There he absorbed Balkan folk-tales and myths, which would inform some of his literary works.\n\nSuceavă attended the University of Bucharest, where he obtained his undergraduate and master's degree in mathematics. He then moved to the United States to study at the Michigan State University for his doctorate. His thesis, titled \"New Riemannian and Kählerian Curvature Invariants and Strongly Minimal Submanifolds\", was written under the supervision of Bang-Yen Chen.\n\nFollowing his doctorate in 2002, Suceavă was hired by California State University, Fullerton.\n\nAt the age of 13, Suceavă won a prize at the Romanian National Mathematical Olympiad, following which he was encouraged to pursue mathematics as a viable career.\n\nSuceavă is a Professor of Mathematics at the California State University, Fullerton. He specialises in Differential geometry, the foundations of geometry, and the history of mathematics.\n\nSuceavă is active in the encouragement of mathematical research among young students in California. He has established a mathematics circle involving undergraduates, and extensively published in gazettes of mathematical problems aimed at high school students.\n\nHis mathematical works appeared in \"Houston Journal of Mathematics, Taiwanese Journal of Mathematics, American Mathematical Monthly, Mathematical Intelligencer, Beiträge zur Algebra und Geometrie, Differential Geometry and Its Applications, Czechoslovak Mathematical Journal, Publicationes Mathematicae, Results in Mathematics, Tsukuba Journal of Mathematics, Notices of the American Mathematical Society, Contemporary Mathematics, Historia Mathematica\", and other mathematical journals.\n\nSuceavă served as editor, together with Alfonso Carriazo, Yun Myung Oh and Joeri Van Der Veken, of the volume \"Recent Advances in the Geometry of Submanifolds. Dedicated to the Memory of Franki Dillen (1963-2013),\" American Mathematical Society, 2016.\n\nSuceavă began his writing career in 1990 with a volume of prose and essays published by Topaz, \"Teama de amurg\" (\"Fear of twilight\"). He has also published various volumes of novels and short stories.\n\nWhile Suceavă writes predominantly in Romanian, his short fiction in English has appeared in \"Review of Contemporary Fiction\", \"Absinthe: New European Writing\", and \"Red Mountain Review\".\n\nIn 1989, Suceavă was a student in Bucharest during the downfall of the Ceaușescu dictatorship. Its impact on his country's social and cultural life motivated him to write his novel \"Venea din timpul diez\" in 2004.\nIn 2007, Suceavă received the Fiction Award of the Association of Bucharest Writers for his novel, \"Miruna, A Tale.\"\n\nTwo of his books (\"Coming from an Off-Key Time\", and \"Miruna, A Tale\") have been translated into English, and received positive reviews.\n\nIn 2015, the Czech version of the novel \"Coming from an Off-Key Time\", in Jiří Našinec's translation, was presented with the Josef Jungmann Award.\n\n\n"}
{"id": "634780", "url": "https://en.wikipedia.org/wiki?curid=634780", "title": "Bohr compactification", "text": "Bohr compactification\n\nIn mathematics, the Bohr compactification of a topological group \"G\" is a compact Hausdorff topological group \"H\" that may be canonically associated to \"G\". Its importance lies in the reduction of the theory of uniformly almost periodic functions on \"G\" to the theory of continuous functions on \"H\". The concept is named after Harald Bohr who pioneered the study of almost periodic functions, on the real line.\n\nGiven a topological group \"G\", the Bohr compactification of \"G\" is a compact \"Hausdorff\" topological group Bohr(\"G\") and a continuous homomorphism \n\nwhich is universal with respect to homomorphisms into compact Hausdorff groups; this means that if \"K\" is another compact Hausdorff topological group and\n\nis a continuous homomorphism, then there is a unique continuous homomorphism\n\nsuch that \"f\" = Bohr(\"f\") ∘ b.\n\nTheorem. The Bohr compactification exists and is unique up to isomorphism.\n\nWe will denote the Bohr compactification of \"G\" by Bohr(\"G\") and the canonical map by\n\nThe correspondence \"G\" ↦ Bohr(\"G\") defines a covariant functor on the category of topological groups and continuous homomorphisms.\n\nThe Bohr compactification is intimately connected to the finite-dimensional unitary representation theory of a topological group. The kernel of b consists exactly of those elements of \"G\" which cannot be separated from the identity of \"G\" by finite-dimensional \"unitary\" representations.\n\nThe Bohr compactification also reduces many problems in the theory of almost periodic functions on topological groups to that of functions on compact groups.\n\nA bounded continuous complex-valued function \"f\" on a topological group \"G\" is uniformly almost periodic if and only if the set of right translates \"f\" where\n\nis relatively compact in the uniform topology as \"g\" varies through \"G\".\n\nTheorem. A bounded continuous complex-valued function \"f\" on \"G\" is uniformly almost periodic if and only if there is a continuous function \"f\" on Bohr(\"G\") (which is uniquely determined) such that\n\nTopological groups for which the Bohr compactification mapping is injective are called \"maximally almost periodic\" (or MAP groups). In the case \"G\" is a locally compact connected group, MAP groups are completely characterized: They are precisely products of compact groups with vector groups\nof finite dimension.\n"}
{"id": "16823137", "url": "https://en.wikipedia.org/wiki?curid=16823137", "title": "Branch-decomposition", "text": "Branch-decomposition\n\nIn graph theory, a branch-decomposition of an undirected graph \"G\" is a hierarchical clustering of the edges of \"G\", represented by an unrooted binary tree \"T\" with the edges of \"G\" as its leaves. Removing any edge from \"T\" partitions the edges of \"G\" into two subgraphs, and the width of the decomposition is the maximum number of shared vertices of any pair of subgraphs formed in this way. \nThe branchwidth of \"G\" is the minimum width of any branch-decomposition of \"G\".\n\nBranchwidth is closely related to tree-width: for all graphs, both of these numbers are within a constant factor of each other, and both quantities may be characterized by forbidden minors. And as with treewidth, many graph optimization problems may be solved efficiently for graphs of small branchwidth. However, unlike treewidth, the branchwidth of planar graphs may be computed exactly, in polynomial time. Branch-decompositions and branchwidth may also be generalized from graphs to matroids.\n\nAn unrooted binary tree is a connected undirected graph with no cycles in which each non-leaf node has exactly three neighbors. A branch-decomposition may be represented by an unrooted binary tree \"T\", together with a bijection between the leaves of \"T\" and the edges of the given graph \"G\" = (\"V\",\"E\").\nIf \"e\" is any edge of the tree \"T\", then removing \"e\" from \"T\" partitions it into two subtrees \"T\" and \"T\". This partition of \"T\" into subtrees induces a partition of the edges associated with the leaves of \"T\" into two subgraphs \"G\" and \"G\" of \"G\". This partition of \"G\" into two subgraphs is called an e-separation.\n\nThe width of an e-separation is the number of vertices of \"G\" that are incident both to an edge of \"E\" and to an edge of \"E\"; that is, it is the number of vertices that are shared by the two subgraphs \"G\" and \"G\". The width of the branch-decomposition is the maximum width of any of its e-separations. The branchwidth of \"G\" is the minimum width of a branch-decomposition of \"G\".\n\nBranch-decompositions of graphs are closely related to tree decompositions, and branch-width is closely related to tree-width: the two quantities are always within a constant factor of each other. In particular, in the paper in which they introduced branch-width, Neil Robertson and Paul Seymour showed that for a graph \"G\"\nwith tree-width \"k\" and branchwidth \n\nCarving width is a concept defined similarly to branch width, except with edges replaced by vertices and vice versa. A carving decomposition is an unrooted binary tree with each leaf representing a vertex in the original graph, and the width of a cut is the number (or total weight in a weighted graph) of edges that are incident to a vertex in both subtrees.\n\nBranch width algorithms typically work by reducing to an equivalent carving width problem. In particular, the carving width of the medial graph of a planar graph is exactly twice the branch width of the original graph.\n\nIt is NP-complete to determine whether a graph \"G\" has a branch-decomposition of width at most \"k\", when \"G\" and \"k\" are both considered as inputs to the problem. However, the graphs with branchwidth at most \"k\" form a minor-closed family of graphs, from which it follows that computing the branchwidth is fixed-parameter tractable: there is an algorithm for computing optimal branch-decompositions whose running time, on graphs of branchwidth \"k\" for any fixed constant \"k\", is linear in the size of the input graph.\n\nFor planar graphs, the branchwidth can be computed exactly in polynomial time. This in contrast to treewidth for which the complexity on planar graphs is a well known open problem. The original algorithm for planar branchwidth, by Paul Seymour and Robin Thomas, took time O(\"n\") on graphs with \"n\" vertices, and their algorithm for constructing a branch decomposition of this width took time O(\"n\"). This was later sped up to O(\"n\").\n\nAs with treewidth, branchwidth can be used as the basis of dynamic programming algorithms for many NP-hard optimization problems, using an amount of time that is exponential in the width of the input graph or matroid. For instance, apply branchwidth-based dynamic programming to a problem of merging multiple partial solutions to the travelling salesman problem into a single global solution, by forming a sparse graph from the union of the partial solutions, using a spectral clustering heuristic to find a good branch-decomposition of this graph, and applying dynamic programming to the decomposition. argue that branchwidth works better than treewidth in the development of fixed-parameter-tractable algorithms on planar graphs, for multiple reasons: branchwidth may be more tightly bounded by a function of the parameter of interest than the bounds on treewidth, it can be computed exactly in polynomial time rather than merely approximated, and the algorithm for computing it has no large hidden constants.\n\nIt is also possible to define a notion of branch-decomposition for matroids that generalizes branch-decompositions of graphs. A branch-decomposition of a matroid is a hierarchical clustering of the matroid elements, represented as an unrooted binary tree with the elements of the matroid at its leaves. An e-separation may be defined in the same way as for graphs, and results in a partition of the set \"M\" of matroid elements into two subsets \"A\" and \"B\". If ρ denotes the rank function of the matroid, then the width of an e-separation is defined as , and the width of the decomposition and the branchwidth of the matroid are defined analogously. The branchwidth of a graph and the branchwidth of the corresponding graphic matroid may differ: for instance, the three-edge path graph and the three-edge star have different branchwidths, 2 and 1 respectively, but they both induce the same graphic matroid with branchwidth 1. However, for graphs that are not trees, the branchwidth of the graph is equal to the branchwidth of its associated graphic matroid. The branchwidth of a matroid is equal to the branchwidth of its dual matroid, and in particular this implies that the branchwidth of any planar graph that is not a tree is equal to that of its dual.\n\nBranchwidth is an important component of attempts to extend the theory of graph minors to matroid minors: although treewidth can also be generalized to matroids, and plays a bigger role than branchwidth in the theory of graph minors, branchwidth has more convenient properties in the matroid setting. Robertson and Seymour conjectured that the matroids representable over any particular finite field are well-quasi-ordered, analogously to the Robertson–Seymour theorem for graphs, but so far this has been proven only for the matroids of bounded branchwidth. Additionally, if a minor-closed family of matroids representable over a finite field does not include the graphic matroids of all planar graphs, then there is a constant bound on the branchwidth of the matroids in the family, generalizing similar results for minor-closed graph families.\n\nFor any fixed constant \"k\", the matroids with branchwidth at most \"k\" can be recognized in polynomial time by an algorithm that has access to the matroid via an independence oracle.\n\nBy the Robertson–Seymour theorem, the graphs of branchwidth \"k\" can be characterized by a finite set of forbidden minors. The graphs of branchwidth 0 are the matchings; the minimal forbidden minors are a two-edge path graph and a triangle graph (or the two-edge cycle, if multigraphs rather than simple graphs are considered). The graphs of branchwidth 1 are the graphs in which each connected component is a star; the minimal forbidden minors for branchwidth 1 are the triangle graph (or the two-edge cycle, if multigraphs rather than simple graphs are considered) and the three-edge path graph. The graphs of branchwidth 2 are the graphs in which each biconnected component is a series-parallel graph; the only minimal forbidden minor is the complete graph \"K\" on four vertices. A graph has branchwidth three if and only if it has treewidth three and does not have the cube graph as a minor; therefore, the four minimal forbidden minors are three of the four forbidden minors for treewidth three (the graph of the octahedron, the complete graph \"K\", and the Wagner graph) together with the cube graph.\n\nForbidden minors have also been studied for matroid branchwidth, despite the lack of a full analogue to the Robertson–Seymour theorem in this case. A matroid has branchwidth one if and only if every element is either a loop or a coloop, so the unique minimal forbidden minor is the uniform matroid U(2,3), the graphic matroid of the triangle graph. A matroid has branchwidth two if and only if it is the graphic matroid of a graph of branchwidth two, so its minimal forbidden minors are the graphic matroid of \"K\" and the non-graphic matroid U(2,4). The matroids of branchwidth three are not well-quasi-ordered without the additional assumption of representability over a finite field, but nevertheless the matroids with any finite bound on their branchwidth have finitely many minimal forbidden minors, all of which have a number of elements that is at most exponential in the branchwidth.\n\n"}
{"id": "39474050", "url": "https://en.wikipedia.org/wiki?curid=39474050", "title": "Consumer network", "text": "Consumer network\n\nThe notion of consumer networks expresses the idea that people’s embeddedness in social networks affects their behavior as consumers. Interactions within consumer networks such as information exchange and imitation can affect demand and market outcomes in ways not considered in the neoclassical theory of consumer choice.\n\nEconomic research on the topic is not ample. In attempts to incorporate consumer networks into standard microeconomic models, some interesting implications have been found concerning market structure, market dynamics and the firm’s profit maximizating decision.\n\nIt has been shown that under certain assumptions the structure of the consumer network can affect market structure. In certain scenarios, where consumers have a higher inclination to compare their habitually consumed product to that of their acquaintances, the equilibrium market structure can switch from oligopoly to monopoly.\n\nIn an other model, which incorporates small world consumer networks into the profit function of the firm, it has been demonstrated that the density of the network significantly affects the optimal price the firm should charge and the optimal referral fee (paid to consumers who can convince an other one to buy). On the other hand, the size of the network does not have an important effect on these.\n\nA 2007 laboratory experiment found that increased density of consumer networks can reduce market inefficiencies caused by moral hazard. The ability of consumers to exchange information with more neighbors increases firms’ incentives to build a reputation through selling high quality products. Even a low level of density was found to isolated consumers who can rely only on their own experience.\n\nExploiting consumer networks for marketing purposes, through techniques such as viral marketing, word-of-mouth marketing, or network marketing, is increasingly experimented with by marketers, to the extent that \"some developments in customer networking are ahead of empirical research, and a few seem ahead even of accepted theory\". These might often be more effective than more traditional forms of advertising. A key task of such forms of marketing is to target the people who are opinion leaders regarding consumption, having many contacts and positive reputation. They are, in network science language, the hubs of consumer networks.\n\n"}
{"id": "41583056", "url": "https://en.wikipedia.org/wiki?curid=41583056", "title": "Cypher Query Language", "text": "Cypher Query Language\n\nCypher is a declarative graph query language that allows for expressive and efficient querying and updating of a property graph. Cypher is a relatively simple but still very powerful language. Very complicated database queries can easily be expressed through Cypher. This allows users to focus on their domain instead of getting lost in database access.\n\nCypher was largely an invention of Andrés Taylor while working for Neo4j, Inc.(formerly Neo Technology) in 2011. Cypher was originally intended to be used with the graph database Neo4j, but was opened up through the openCypher project in October 2015.\n\nCypher is based on the Property Graph Model, which in addition to the standard graph elements of nodes and edges (which are called \"relationships\" in Cypher) adds labels and properties as concepts. Nodes may have zero or more labels, while each relationship has exactly one relationship type. Nodes and relationships also have zero or more properties, where a property is a key-value binding of a string key and some value from the Cypher type system.\n\nThe Cypher type system includes nodes, relationships, paths, maps, lists, integers, floating-point numbers, booleans, and strings.\n\nCypher contains a variety of clauses. Among the most common are: MATCH and WHERE. These functions are slightly different than in SQL. MATCH is used for describing the structure of the pattern searched for, primarily based on relationships. WHERE is used to add additional constraints to patterns. For example, the below query will return all movies where an actor named 'Nicole Kidman' has acted, and that were produced before a certain year (sent by parameter):\n\nCypher additionally contains clauses for writing, updating, and deleting data. CREATE and DELETE are used to create and delete nodes and relationships. SET and REMOVE are used to set values to properties and add labels on nodes. Nodes can only be deleted when they have no other relationships still existing. For example:\n\nWith the openCypher project, an effort was started to standardize Cypher as the query language for graph processing. One part of this process is the First openCypher Implementers Meeting (oCIM), which was first announced in December 2016.\n\n\n"}
{"id": "39447416", "url": "https://en.wikipedia.org/wiki?curid=39447416", "title": "DEAP (software)", "text": "DEAP (software)\n\nDistributed Evolutionary Algorithms in Python (DEAP) is an evolutionary computation framework for rapid prototyping and testing of ideas. It incorporates the data structures and tools required to implement most common evolutionary computation techniques such as genetic algorithm, genetic programming, evolution strategies, particle swarm optimization, differential evolution, traffic flow and estimation of distribution algorithm. It is developed at Université Laval since 2009.\n\nThe following code gives a quick overview how the Onemax problem optimization with genetic algorithm can be implemented with DEAP.\n\n"}
{"id": "37892981", "url": "https://en.wikipedia.org/wiki?curid=37892981", "title": "DataStax", "text": "DataStax\n\nDataStax, Inc. is a data management company based in Santa Clara, California. The company was built on Apache Cassandra. As of October 2017, the company has roughly 400 customers distributed in over 50 countries.\n\nDataStax was built on the open source NoSQL database Apache Cassandra. Cassandra was initially developed internally at Facebook to handle large data sets across multiple servers, and was released as an Apache open source project in 2008. In 2010, Jonathan Ellis and Matt Pfeil left Rackspace, where they had worked with Cassandra, to launch Riptano in Austin, Texas. Ellis and Pfeil later renamed the company DataStax, and moved its headquarters to Santa Clara, California.\n\nThe company went on to create its own proprietary version of Cassandra, a NoSQL database called DataStax Enterprise (DSE). Version 1.0, released in October 2011, was the first commercial distribution of the Cassandra database, designed to provide real-time application performance and heavy analytics on the same physical infrastructure. It grew to include advanced security controls, graph database models, operational analytics and advanced search capabilities.\n\nIn September 2014, DataStax raised $106 million in a Series E funding round, raising the total investment in the company to $190 million.\n\nIn April 2016, the company announced the release of DataStax Enterprise Graph, adding graph data model functionality to DSE.\n\nIn March 2017, DataStax announced the release of its DSE platform 5.1, which included improved search capabilities, improved security control, improvements to its Graph data management and improvements to operational analytics performance. DataStax also announced a shift in strategy, with an added focus on customer experience applications. Rather than a new set of technologies, the company started to offer advice on best practice to users of its core DSE platform. \n\nIn April 2018, DataStax released DSE 6, with the new version focused on businesses using a hybrid cloud computing model, with all the benefits of a distributed cloud database on any public cloud or on-premise, twice the responsiveness and ability to handle twice the throughput.\n\n"}
{"id": "1157370", "url": "https://en.wikipedia.org/wiki?curid=1157370", "title": "Depolarization ratio", "text": "Depolarization ratio\n\nIn Raman spectroscopy, the depolarization ratio is the intensity ratio between the perpendicular component and the parallel component of Raman scattered light.\n\nEarly work in this field was carried out by George Placzek, who developed the theoretical treatment of bond polarizability\n\nThe Raman scattered light is emitted by the stimulation of the electric field of the incident light. Therefore, the direction of the vibration of the electric field, or polarization direction, of the scattered light might be expected to be the same as that of the incident light. In reality, however, some fraction of the Raman scattered light has a polarization direction that is perpendicular to that of the incident light. This component is called the perpendicular component. Naturally, the component of the Raman scattered light whose polarization direction is parallel to that of the incident light is called the parallel component, and the Raman scattered light consists of the parallel component and the perpendicular component. \n\nThe ratio of the peak intensity of the parallel and perpendicular component is known as the depolarization ratio (ρ), defined in equation 1.\n\nFor example, a spectral band with a peak of intensity 10 units when the polarizers are parallel, and intensity 1 unit when the polarizers are perpendicular, would have a depolarization ratio of 1/10 = 0.1, which corresponds to a highly polarized band. \n\nThe value of the depolarization ratio of a Raman band depends on the symmetry of the molecule and the normal vibrational mode, in other words, the point group of the molecule and its irreducible representation to which the normal mode belongs. Under Placzek’s polarizability approximation, it is known that the depolarization ratio of a totally symmetric vibrational mode is less than 0.75, and that of the other modes equals 0.75. A Raman band whose depolarization ratio is less than 0.75 is called a polarized band, and a band with a depolarization ratio equal to or greater than 0.75 is called a depolarized band.\n\nFor a spherical top molecule in which all three axes are equivalent, symmetric vibrations have Raman spectral bands which are completely polarized (ρ = 0). An example is the symmetric stretching or \"breathing\" mode of methane (CH) in which all 4 C–H bonds vibrate in phase. However for the asymmetric mode in which one C–H bond stretches while the other three contract, the Raman scattered radiation is depolarized.\n\nFor molecules of lower symmetry (symmetric tops or asymmetric tops), a vibration with the full symmetry of the molecule leads to a polarized or partially polarized Raman band (ρ < 0.75), while a less symmetric vibration yields a depolarized band (ρ ≥ 0.75).\n"}
{"id": "58117270", "url": "https://en.wikipedia.org/wiki?curid=58117270", "title": "Distribution function (measure theory)", "text": "Distribution function (measure theory)\n\nIn mathematics, a distribution function is a real function in measure theory. From every measure on the algebra of Borel sets of real numbers, a distribution function can be constructed, which reflects some of the properties of this measure. Distribution functions (in the sense of measure theory) are a generalization of distribution functions (in the sense of probability theory).\n\nLet formula_1 be a measure on the real numbers, equipped with the Borel formula_2-algebra. Then the function\n\ndefined by\n\nis called the (right continuous) distribution function of the measure formula_1.\n\nAs the measure, choose the Lebesgue measure formula_6. Then by Definition of formula_6\n\nTherefore, the distribution function of the Lebesgue measure is\n\nfor all formula_10\n\nThe definition of the distribution function (in the sense of measure theory) differs slightly from the definition of the distribution function (in the sense of probability theory). The latter has the boundary conditions\n\nThis makes this distribution function well defined for all probability measures.\nHowever, in the case of an unbounded measure formula_1, defining the distribution function as in probability theory by\ncan be without meaning. This is since many measures take on the value formula_14 on all intervals formula_15, making their distribution function a constant function with value infinity. This is for example the case for the Lebesgue measure. To avoid this pathological case, the distribution function is defined to be zero at the origin. This makes sure that even for unbounded measures, the distribution function is well defined and finite close to the origin.\n"}
{"id": "839358", "url": "https://en.wikipedia.org/wiki?curid=839358", "title": "EFF DES cracker", "text": "EFF DES cracker\n\nIn cryptography, the EFF DES cracker (nicknamed \"Deep Crack\") is a machine built by the Electronic Frontier Foundation (EFF) in 1998, to perform a brute force search of the Data Encryption Standard (DES) cipher's key space – that is, to decrypt an encrypted message by trying every possible key. The aim in doing this was to prove that the key size of DES was not sufficient to be secure.\n\nDES uses a 56-bit key, meaning that there are 2 possible keys under which a message can be encrypted. This is exactly 72,057,594,037,927,936, or approximately 72 quadrillion possible keys. One of the major criticisms of DES, when proposed in 1975, was that the key size was too short. Martin Hellman and Whitfield Diffie of Stanford University estimated that a machine fast enough to test that many keys in a day would have cost about $20 million in 1976, an affordable sum to national intelligence agencies such as the US National Security Agency. Subsequent advances in the price/performance of chips kept reducing that cost until twenty years later it became affordable to even a small nonprofit organization such as the EFF.\n\nDES was a federal standard, and the US government encouraged the use of DES for all non-classified data. RSA Security wished to demonstrate that DES's key length was not enough to ensure security, so they set up the DES Challenges in 1997, offering a monetary prize. The first DES Challenge was solved in 96 days by the DESCHALL Project led by Rocke Verser in Loveland, Colorado. RSA Security set up DES Challenge II-1, which was solved by distributed.net in 39 days in January and February 1998.\n\nIn 1998, the EFF built Deep Crack (named in reference to IBM's Deep Blue chess computer) for less than $250,000. In response to DES Challenge II-2, on July 15, 1998, Deep Crack decrypted a DES-encrypted message after only 56 hours of work, winning $10,000. The brute force attack showed that cracking DES was actually a very practical proposition. Most governments and large corporations could reasonably build a machine like Deep Crack.\n\nSix months later, in response to RSA Security's DES Challenge III, and in collaboration with distributed.net, the EFF used Deep Crack to decrypt another DES-encrypted message, winning another $10,000. This time, the operation took less than a day – 22 hours and 15 minutes. The decryption was completed on January 19, 1999. In October of that year, DES was reaffirmed as a federal standard, but this time the standard recommended Triple DES.\n\nThe small key-space of DES, and relatively high computational costs of Triple DES resulted in its replacement by AES as a Federal standard, effective May 26, 2002.\n\nDeep Crack was designed by Cryptography Research, Inc., Advanced Wireless Technologies, and the EFF. The principal designer was Paul Kocher, president of Cryptography Research. Advanced Wireless Technologies built 1856 custom ASIC DES chips (called \"Deep Crack\" or \"AWT-4500\"), housed on 29 circuit boards of 64 chips each. The boards were then fitted in six cabinets and mounted in a Sun-4/470 chassis.\n\nThe search was coordinated by a single PC which assigned ranges of keys to the chips. The entire machine was capable of testing over 90 billion keys per second. It would take about 9 days to test every possible key at that rate. On average, the correct key would be found in half that time.\n\nIn 2006, another custom hardware attack machine was designed based on FPGAs. COPACOBANA (COst-optimized PArallel COdeBreaker) is able to crack DES at considerably lower cost. This advantage is mainly due to progress in integrated circuit technology. \n\nIn July 2012, security researchers David Hulton and Moxie Marlinspike unveiled a cloud computing tool for breaking the MS-CHAPv2 protocol by recovering the protocol's DES encryption keys by brute force. This tool effectively allows members of the general public to recover a DES key from a known plaintext-ciphertext pair in about 24 hours.\n\n"}
{"id": "1844527", "url": "https://en.wikipedia.org/wiki?curid=1844527", "title": "Exponential dichotomy", "text": "Exponential dichotomy\n\nIn the mathematical theory of dynamical systems, an exponential dichotomy is a property of an equilibrium point that extends the idea of hyperbolicity to non-autonomous systems.\n\nIf\n\nis a linear non-autonomous dynamical system in R with fundamental solution matrix Φ(\"t\"), Φ(0) = \"I\", then the equilibrium point 0 is said to have an \"exponential dichotomy\" if there exists a (constant) matrix \"P\" such that \"P\" = \"P\" and positive constants \"K\", \"L\", α, and β such that\n\nand\n\nIf furthermore, \"L\" = 1/\"K\" and β = α, then 0 is said to have a \"uniform exponential dichotomy\".\n\nThe constants α and β allow us to define the \"spectral window\" of the equilibrium point, (−α, β).\n\nThe matrix \"P\" is a projection onto the stable subspace and \"I\" − \"P\" is a projection onto the unstable subspace. What the exponential dichotomy says is that the norm of the projection onto the stable subspace of any orbit in the system decays exponentially as \"t\" → ∞ and the norm of the projection onto the unstable subspace of any orbit decays exponentially as \"t\" → −∞, and furthermore that the stable and unstable subspaces are conjugate (because formula_4).\n\nAn equilibrium point with an exponential dichotomy has many of the properties of a hyperbolic equilibrium point in autonomous systems. In fact, it can be shown that a hyperbolic point has an exponential dichotomy.\n\n"}
{"id": "312008", "url": "https://en.wikipedia.org/wiki?curid=312008", "title": "Fourier optics", "text": "Fourier optics\n\nFourier optics is the study of classical optics using Fourier transforms (FTs), in which the waveform being considered is regarded as made up of a combination, or \"superposition\", of plane waves. It has some parallels to the Huygens–Fresnel principle, in which the wavefront is regarded as being made up of a combination of spherical wavefronts whose sum is the wavefront being studied. A key difference is that Fourier optics considers the plane waves to be natural modes of the propagation medium, as opposed to Huygens–Fresnel, where the spherical waves originate in the physical medium. \n\nA curved phasefront may be synthesized from an infinite number of these \"natural modes\" i.e., from plane wave phasefronts oriented in different directions in space. Far from its sources, an expanding spherical wave is locally tangent to a planar phase front (a single plane wave out of the infinite spectrum), which is transverse to the radial direction of propagation. In this case, a Fraunhofer diffraction pattern is created, which emanates from a single spherical wave phase center. In the near field, no single well-defined spherical wave phase center exists, so the wavefront isn't locally tangent to a spherical ball. In this case, a Fresnel diffraction pattern would be created, which emanates from an \"extended\" source, consisting of a distribution of (physically identifiable) spherical wave sources in space. In the near field, a full spectrum of plane waves is necessary to represent the Fresnel near-field wave, \"even locally\". A \"wide\" wave moving forward (like an expanding ocean wave coming toward the shore) can be regarded as an infinite number of \"plane wave modes\", all of which could (when they collide with something in the way) scatter independently of one other. These mathematical simplifications and calculations are the realm of Fourier analysis and synthesis – together, they can describe what happens when light passes through various slits, lenses or mirrors curved one way or the other, or is fully or partially reflected.\n\nFourier optics forms much of the theory behind image processing techniques, as well as finding applications where information needs to be extracted from optical sources such as in quantum optics. To put it in a slightly more complex way, similar to the concept of \"frequency\" and \"time\" used in traditional Fourier transform theory, Fourier optics makes use of the spatial frequency domain (\"k\", \"k\") as the conjugate of the spatial (\"x\", \"y\") domain. Terms and concepts such as transform theory, spectrum, bandwidth, window functions and sampling from one-dimensional signal processing are commonly used.\n\nLight can be described as a waveform propagating through free space (vacuum) or a material medium (such as air or glass). Mathematically, the (real valued) amplitude of one wave component is represented by a scalar wave function \"u\" that depends on both space and time:\n\nwhere\n\nrepresents position in three dimensional space, and \"t\" represents time.\n\nFourier optics begins with the homogeneous, scalar wave equation (valid in source-free regions):\n\nwhere \"u\"(r,\"t\") is a real valued Cartesian component of an electromagnetic wave propagating through free space.\n\nIf light of a fixed frequency/wavelength/color (as from a laser) is assumed, then the time-harmonic form of the optical field is given as:\n\nwhere formula_5 is the imaginary unit,\nis the angular frequency (in radians per unit time) of the light waves, and\nis, in general, a complex quantity, with separate amplitude formula_8 and phase formula_9.\n\nSubstituting this expression into the wave equation yields the time-independent form of the wave equation, also known as the Helmholtz equation:\n\nwhere \n\nis the wave number, ψ(r) is the time-independent, complex-valued component of the propagating wave. Note that the propagation constant, k, and the frequency, formula_12, are linearly related to one another, a typical characteristic of transverse electromagnetic (TEM) waves in homogeneous media.\n\nSolutions to the Helmholtz equation may readily be found in rectangular coordinates via the principle of separation of variables for partial differential equations. This principle says that in separable orthogonal coordinates, an \"elementary product solution\" to this wave equation may be constructed of the following form:\n\ni.e., as the product of a function of \"x\", times a function of \"y\", times a function of \"z\". If this \"elementary product solution\" is substituted into the wave equation (2.0), using the scalar Laplacian in rectangular coordinates:\n\nthen the following equation for the 3 individual functions is obtained\n\nwhich is readliy rearranged into the form:\n\nIt may now be argued that each of the quotients in the equation above must, of necessity, be constant. For, say the first quotient is not constant, and is a function of \"x\". None of the other terms in the equation has any dependence on the variable x. Therefore, the first term may not have any \"x\"-dependence either; it must be constant. The constant is denoted as -\"k\"². Reasoning in a similar way for the \"y\" and \"z\" quotients, three ordinary differential equations are obtained for the \"f\", \"f\" and \"f\", along with one \"separation condition\":\n\nEach of these 3 differential equations has the same solution: sines, cosines or complex exponentials. We'll go with the complex exponential for notational simplicity, compatibility with usual FT notation, and the fact that a two-sided integral of complex exponentials picks up both the sine and cosine contributions. As a result, the elementary product solution for \"E\" is:\n\nwhich represents a propagating or exponentially decaying uniform plane wave solution to the homogeneous wave equation. The - sign is used for a wave propagating/decaying in the +z direction and the + sign is used for a wave propagating/decaying in the -z direction (this follows the engineering time convention, which assumes an e time dependence). This field represents a propagating plane wave when the quantity under the radical is positive, and an exponentially decaying wave when it is negative (in passive media, the root with a non-positive imaginary part must always be chosen, to represent uniform propagation or decay, but not amplification).\n\nProduct solutions to the Helmholtz equation are also readily obtained in cylindrical and spherical coordinates, yielding cylindrical and spherical harmonics (with the remaining separable coordinate systems being used much less frequently).\n\nA general solution to the homogeneous electromagnetic wave equation in rectangular coordinates may be formed as a weighted superposition of all possible elementary plane wave solutions as:\n\nNext, let\n\nThen:\n\nThis plane wave spectrum representation of the electromagnetic field is the basic foundation of Fourier optics (this point cannot be emphasized strongly enough), because when \"z\"=0, the equation above simply becomes a Fourier transform (FT) relationship between the field and its plane wave content (hence the name, \"Fourier optics\").\n\nThus:\n\nand\n\nAll spatial dependence of the individual plane wave components is described explicitly via the exponential functions. The coefficients of the exponentials are only functions of spatial wavenumber \"k\", \"k\", just as in ordinary Fourier analysis and Fourier transforms.\n\nWhen\nthe plane waves are evanescent (decaying), so that any spatial frequency content in an object plane transparency which is finer than one wavelength will not be transferred over to the image plane, simply because the plane waves corresponding to that content cannot propagate. In connection with lithography of electronic components, this phenomenon is known as the diffraction limit and is the reason why light of progressively higher frequency (smaller wavelength, thus larger \"k\") is required for etching progressively finer features in integrated circuits.\n\nAs shown above, an elementary product solution to the Helmholtz equation takes the form:\n\nwhere\n\nis the wave vector, and\n\nis the wave number. Next, using the paraxial approximation, it is assumed that\n\nor equivalently,\n\nwhere θ is the angle between the wave vector k and the z-axis.\n\nAs a result,\nand\n\nSubstituting this expression into the Helmholtz equation, the paraxial wave equation is derived:\nwhere\nis the transverse Laplace operator, shown here in Cartesian coordinates.\n\nThe equation above may be evaluated asymptotically in the far field (using the stationary phase method) to show that the field at the distant point (\"x\",\"y\",\"z\") is indeed due solely to the plane wave component (\"k\", \"k\", \"k\") which propagates parallel to the vector (\"x\",\"y\",\"z\"), and whose plane is tangent to the phasefront at (\"x\",\"y\",\"z\"). The mathematical details of this process may be found in Scott [1998] or Scott [1990]. The result of performing a stationary phase integration on the expression above is the following expression,\n\nwhich clearly indicates that the field at (x,y,z) is directly proportional to the spectral component in the direction of (x,y,z), where,\n\nand\n\nStated another way, the radiation pattern of any planar field distribution is the FT of that source distribution (see Huygens–Fresnel principle, wherein the same equation is developed using a Green's function approach). Note that this is NOT a plane wave. The formula_46 radial dependence is a spherical wave - both in magnitude and phase - whose local amplitude is the FT of the source plane distribution at that far field angle. The plane wave spectrum has nothing to do with saying that the field behaves something like a plane wave for far distances.\n\nEquation (2.2) above is critical to making the connection between \"spatial bandwidth\" (on the one hand) and \"angular bandwidth\" (on the other), in the far field. Note that the term \"far field\" usually means we're talking about a converging or diverging spherical wave with a pretty well defined phase center. The connection between spatial and angular bandwidth in the far field is essential in understanding the low pass filtering property of thin lenses. See section 5.1.3 for the condition defining the far field region.\n\nOnce the concept of angular bandwidth is understood, the optical scientist can \"jump back and forth\" between the spatial and spectral domains to quickly gain insights which would ordinarily not be so readily available just through spatial domain or ray optics considerations alone. For example, any source bandwidth which lies past the edge angle to the first lens (this edge angle sets the bandwidth of the optical system) will not be captured by the system to be processed.\n\nAs a side note, electromagnetics scientists have devised an alternative means for calculating the far zone electric field which does not involve stationary phase integration. They have devised a concept known as \"fictitious magnetic currents\" usually denoted by M, and defined as\nIn this equation, it is assumed that the unit vector in the z-direction points into the half-space where the far field calculations will be made. These equivalent magnetic currents are obtained using equivalence principles which, in the case of an infinite planar interface, allow any electric currents, J to be \"imaged away\" while the fictitious magnetic currents are obtained from twice the aperture electric field (see Scott [1998]). Then the radiated electric field is calculated from the magnetic currents using an equation similar to the equation for the magnetic field radiated by an electric current. In this way, a vector equation is obtained for the radiated electric field in terms of the aperture electric field and the derivation requires no use of stationary phase ideas.\n\nFourier optics is somewhat different from ordinary ray optics typically used in the analysis and design of focused imaging systems such as cameras, telescopes and microscopes. Ray optics is the very first type of optics most of us encounter in our lives; it's simple to conceptualize and understand, and works very well in gaining a baseline understanding of common optical devices. Unfortunately, ray optics does not explain the operation of Fourier optical systems, which are in general not focused systems. Ray optics is a subset of wave optics (in the jargon, it is \"the asymptotic zero-wavelength limit\" of wave optics) and therefore has limited applicability. We have to know when it is valid and when it is not - and this is one of those times when it is not. For our current task, we must expand our understanding of optical phenomena to encompass wave optics, in which the optical field is seen as a solution to Maxwell's equations. This more general \"wave optics\" accurately explains the operation of Fourier optics devices.\n\nIn this section, we won't go all the way back to Maxwell's equations, but will start instead with the homogeneous Helmholtz equation (valid in source-free media), which is one level of refinement up from Maxwell's equations (Scott [1998]). From this equation, we'll show how infinite uniform plane waves comprise one field solution (out of many possible) in free space. These uniform plane waves form the basis for understanding Fourier optics.\n\nThe plane wave spectrum concept is the basic foundation of Fourier Optics. The plane wave spectrum is a continuous spectrum of \"uniform\" plane waves, and there is one plane wave component in the spectrum for every tangent point on the far-field phase front. The amplitude of that plane wave component would be the amplitude of the optical field at that tangent point. Again, this is true only in the far field, defined as: Range = 2 D / λ where D is the maximum linear extent of the optical sources and λ is the wavelength (Scott [1998]). The plane wave spectrum is often regarded as being discrete for certain types of periodic gratings, though in reality, the spectra from gratings are continuous as well, since no physical device can have the infinite extent required to produce a true line spectrum.\n\nAs in the case of electrical signals, bandwidth is a measure of how finely detailed an image is; the finer the detail, the greater the bandwidth required to represent it. A DC electrical signal is constant and has no oscillations; a plane wave propagating parallel to the optic (formula_48) axis has constant value in any \"x\"-\"y\" plane, and therefore is analogous to the (constant) DC component of an electrical signal. Bandwidth in electrical signals relates to the difference between the highest and lowest frequencies present in the spectrum of the signal. For \"optical\" systems, bandwidth also relates to spatial frequency content (spatial bandwidth), but it also has a secondary meaning. It also measures how far from the optic axis the corresponding plane waves are tilted, and so this type of bandwidth is often referred to also as angular bandwidth. It takes more frequency bandwidth to produce a short pulse in an electrical circuit, and more angular (or, spatial frequency) bandwidth to produce a sharp spot in an optical system (see discussion related to Point spread function).\n\nThe plane wave spectrum arises naturally as the eigenfunction or \"natural mode\" solution to the homogeneous electromagnetic wave equation in rectangular coordinates (see also Electromagnetic radiation, which derives the wave equation from Maxwell's equations in source-free media, or Scott [1998]). In the frequency domain, with an assumed (engineering) time convention of formula_49, the homogeneous electromagnetic wave equation is known as the Helmholtz equation and takes the form:\n\nwhere \"u\" = \"x\", \"y\", \"z\" and \"k\" = 2π/λ is the wavenumber of the medium.\n\nIn the case of differential equations, as in the case of matrix equations, whenever the right-hand side of an equation is zero (i.e., the forcing function / forcing vector is zero), the equation may still admit a non-trivial solution, known in applied mathematics as an eigenfunction solution, in physics as a \"natural mode\" solution and in electrical circuit theory as the \"zero-input response.\" This is a concept that spans a wide range of physical disciplines. Common physical examples of \"resonant\" natural modes would include the resonant vibrational modes of stringed instruments (1D), percussion instruments (2D) or the former Tacoma Narrows Bridge (3D). Examples of \"propagating\" natural modes would include waveguide modes, optical fiber modes, solitons and Bloch waves. Infinite homogeneous media admit the rectangular, circular and spherical harmonic solutions to the Helmholtz equation, depending on the coordinate system under consideration. The propagating plane waves we'll study in this article are perhaps the simplest type of propagating waves found in any type of media.\n\nThere is a striking similarity between the Helmholtz equation (2.0) above, which may be written\n\nand the usual equation for the eigenvalues/eigenvectors of a square matrix, A,\n\nparticularly since both the scalar Laplacian, formula_53 and the matrix, A are linear operators on their respective function/vector spaces (the minus sign in the second equation is, for all intents and purposes, immaterial; the plus sign in the first equation however is significant). It is perhaps worthwhile to note that both the eigenfunction and eigenvector solutions to these two equations respectively, often yield an orthogonal set of functions/vectors which span (i.e., form a basis set for) the function/vector spaces under consideration. The interested reader may investigate other functional linear operators which give rise to different kinds of orthogonal eigenfunctions such as Legendre polynomials, Chebyshev polynomials and Hermite polynomials.\n\nIn the matrix case, eigenvalues formula_54 may be found by setting the determinant of the matrix equal to zero, i.e. finding where the matrix has no inverse. Finite matrices have only a finite number of eigenvalues/eigenvectors, whereas linear operators can have a countably infinite number of eigenvalues/eigenfunctions (in confined regions) or uncountably infinite (continuous) spectra of solutions, as in unbounded regions.\n\nIn certain physics applications such as in the computation of bands in a periodic volume, it is often the case that the elements of a matrix will be very complicated functions of frequency and wavenumber, and the matrix will be non-singular for most combinations of frequency and wavenumber, but will also be singular for certain specific combinations. By finding which combinations of frequency and wavenumber drive the determinant of the matrix to zero, the propagation characteristics of the medium may be determined. Relations of this type, between frequency and wavenumber, are known as dispersion relations and some physical systems may admit many different kinds of dispersion relations. An example from electromagnetics is the ordinary waveguide, which may admit numerous dispersion relations, each associated with a unique mode of the waveguide. Each propagation mode of the waveguide is known as an eigenfunction solution (or eigenmode solution) to Maxwell's equations in the waveguide. Free space also admits eigenmode (natural mode) solutions (known more commonly as plane waves), but with the distinction that for any given frequency, free space admits a continuous modal spectrum, whereas waveguides have a discrete mode spectrum. In this case the dispersion relation is linear, as in section 1.2.\n\nThe separation condition,\n\nwhich is identical to the equation for the Euclidean metric in three-dimensional configuration space, suggests the notion of a k-vector in three-dimensional \"k-space\", defined (for propagating plane waves) in rectangular coordinates as:\n\nand in the spherical coordinate system as\n\nUse will be made of these spherical coordinate system relations in the next section.\n\nThe notion of k-space is central to many disciplines in engineering and physics, especially in the study of periodic volumes, such as in crystallography and the band theory of semiconductor materials.\n\nAnalysis Equation (calculating the spectrum of the function):\n\nSynthesis Equation (reconstructing the function from its spectrum):\n\n\"Note\": the normalizing factor of:formula_62 is present whenever angular frequency (radians) is used, but not when ordinary frequency (cycles) is used.\n\nAn optical system consists of an input plane, and output plane, and a set of components that transforms the image \"f\" formed at the input into a different image \"g\" formed at the output. The output image is related to the input image by convolving the input image with the optical impulse response, \"h\" (known as the \"point-spread function\", for focused optical systems). The impulse response uniquely defines the input-output behavior of the optical system. By convention, the optical axis of the system is taken as the \"z\"-axis. As a result, the two images and the impulse response are all functions of the transverse coordinates, \"x\" and \"y\".\n\nThe impulse response of an optical imaging system is the output plane field which is produced when an ideal mathematical point source of light is placed in the input plane (usually on-axis). In practice, it is not necessary to have an ideal point source in order to determine an exact impulse response. This is because any source bandwidth which lies outside the bandwidth of the system won't matter anyway (since it cannot even be captured by the optical system), so therefore it's not necessary in determining the impulse response. The source only needs to have at least as much (angular) bandwidth as the optical system.\n\nOptical systems typically fall into one of two different categories. The first is the ordinary focused optical imaging system, wherein the input plane is called the object plane and the output plane is called the image plane. The field in the image plane is desired to be a high-quality reproduction of the field in the object plane. In this case, the impulse response of the optical system is desired to approximate a 2D delta function, at the same location (or a linearly scaled location) in the output plane corresponding to the location of the impulse in the input plane. The \"actual\" impulse response typically resembles an Airy function, whose radius is on the order of the wavelength of the light used. In this case, the impulse response is typically referred to as a point spread function, since the mathematical point of light in the object plane has been spread out into an Airy function in the image plane.\n\nThe second type is the optical image processing system, in which a significant feature in the input plane field is to be located and isolated. In this case, the impulse response of the system is desired to be a close replica (picture) of that feature which is being searched for in the input plane field, so that a convolution of the impulse response (an image of the desired feature) against the input plane field will produce a bright spot at the feature location in the output plane. It is this latter type of optical \"image processing\" system that is the subject of this section. Section 5.2 presents one hardware implementation of the optical image processing operations described in this section.\n\nThe input plane is defined as the locus of all points such that \"z\" = 0. The input image \"f\" is therefore\n\nThe output plane is defined as the locus of all points such that \"z\" = \"d\". The output image \"g\" is therefore\n\ni.e.,\n\nThe alert reader will note that the integral above tacitly assumes that the impulse response is NOT a function of the position (x',y') of the impulse of light in the input plane (if this were not the case, this type of convolution would not be possible). This property is known as \"shift invariance\" (Scott [1998]). No optical system is perfectly shift invariant: as the ideal, mathematical point of light is scanned away from the optic axis, aberrations will eventually degrade the impulse response (known as a coma in focused imaging systems). However, high quality optical systems are often \"shift invariant enough\" over certain regions of the input plane that we may regard the impulse response as being a function of only the difference between input and output plane coordinates, and thereby use the equation above with impunity.\n\nAlso, this equation assumes unit magnification. If magnification is present, then eqn. (4.1) becomes\n\nwhich basically translates the impulse response function, h(), from x' to x=Mx'. In (4.2), h() will be a magnified version of the impulse response function h() of a similar, unmagnified system, so that h(x,y) =h(x/M,y/M).\n\nThe extension to two dimensions is trivial, except for the difference that causality exists in the time domain, but not in the spatial domain. Causality means that the impulse response \"h\"(\"t\" - t') of an electrical system, due to an impulse applied at time t', must of necessity be zero for all times t such that t - t' < 0.\n\nObtaining the convolution representation of the system response requires representing the input signal as a weighted superposition over a train of impulse functions by using the \"shifting property\" of Dirac delta functions.\n\nIt is then presumed that the system under consideration is \"linear\", that is to say that the output of the system due to two different inputs (possibly at two different times) is the sum of the individual outputs of the system to the two inputs, when introduced individually. Thus the optical system may contain no nonlinear materials nor active devices (except possibly, extremely linear active devices). The output of the system, for a single delta function input is defined as the \"impulse response\" of the system, h(t - t'). And, by our linearity assumption (i.e., that the output of system to a pulse train input is the sum of the outputs due to each individual pulse), we can now say that the general input function \"f\"(\"t\") produces the output:\n\nwhere \"h\"(t - t') is the (impulse) response of the linear system to the delta function input δ(t - t'), applied at time t'. This is where the convolution equation above comes from. The convolution equation is useful because it is often much easier to find the response of a system to a delta function input - and then perform the convolution above to find the response to an arbitrary input - than it is to try to find the response to the arbitrary input directly. Also, the impulse response (in either time or frequency domains) usually yields insight to relevant figures of merit of the system. In the case of most lenses, the point spread function (PSF) is a pretty common figure of merit for evaluation purposes.\n\nThe same logic is used in connection with the Huygens–Fresnel principle, or Stratton-Chu formulation, wherein the \"impulse response\" is referred to as the Green's function of the system. So the spatial domain operation of a linear optical system is analogous in this way to the Huygens–Fresnel principle.\n\nIf the last equation above is Fourier transformed, it becomes:\n\nwhere\n\nIn like fashion, (4.1) may be Fourier transformed to yield:\n\nThe system transfer function, formula_76. In optical imaging this function is better known as the optical transfer function \"(Goodman)\".\n\nOnce again it may be noted from the discussion on the Abbe sine condition, that this equation assumes unit magnification.\n\nThis equation takes on its real meaning when the Fourier transform, formula_77 is associated with the coefficient of the plane wave whose transverse wavenumbers are formula_78. Thus, the input-plane plane wave spectrum is transformed into the output-plane plane wave spectrum through the multiplicative action of the system transfer function. It is at this stage of understanding that the previous background on the plane wave spectrum becomes invaluable to the conceptualization of Fourier optical systems.\n\nFourier optics is used in the field of optical information processing, the staple of which is the classical 4F processor.\n\nThe Fourier transform properties of a lens provide numerous applications in optical signal processing such as spatial filtering, optical correlation and computer generated holograms.\n\nFourier optical theory is used in interferometry, optical tweezers, atom traps, and quantum computing. Concepts of Fourier optics are used to reconstruct the phase of light intensity in the spatial frequency plane (see adaptive-additive algorithm).\n\nIf a transmissive object is placed one focal length in front of a lens, then its Fourier transform will be formed one focal length behind the lens. Consider the figure to the right (click to enlarge)\n\nIn this figure, a plane wave incident from the left is assumed. The transmittance function in the front focal plane (i.e., Plane 1) \"spatially modulates the incident plane wave\" in magnitude and phase, \"like on the left-hand side of eqn. (2.1)\" (specified to \"z\"=0), and \"in so doing, produces a spectrum of plane waves\" corresponding to the FT of the transmittance function, \"like on the right-hand side of eqn. (2.1)\" (for \"z\">0). The various plane wave components propagate at different tilt angles with respect to the optic axis of the lens (i.e., the horizontal axis). The finer the features in the transparency, the broader the angular bandwidth of the plane wave spectrum. We'll consider one such plane wave component, propagating at angle θ with respect to the optic axis. It is assumed that θ is small (paraxial approximation), so that\n\nand\n\nand\n\nIn the figure, the \"plane wave\" phase, moving horizontally from the front focal plane to the lens plane, is\n\nand the \"spherical wave\" phase from the lens to the spot in the back focal plane is:\n\nand the sum of the two path lengths is \"f\" (1 + θ/2 + 1 - θ/2) = 2\"f\" i.e., it is a constant value, independent of tilt angle, θ, for paraxial plane waves. Each paraxial plane wave component of the field in the front focal plane appears as a point spread function spot in the back focal plane, with an intensity and phase equal to the intensity and phase of the original plane wave component in the front focal plane. In other words, the field in the back focal plane is the Fourier transform of the field in the front focal plane.\n\nAll FT components are computed simultaneously - in parallel - at the speed of light. As an example, light travels at a speed of roughly . / ns, so if a lens has a . focal length, an entire 2D FT can be computed in about 2 ns (2 x 10 seconds). If the focal length is 1 in., then the time is under 200 ps. No electronic computer can compete with these kinds of numbers or perhaps ever hope to, although new supercomputers such as the petaflop IBM Roadrunner may actually prove faster than optics, as improbable as that may seem. However, their speed is obtained by combining numerous computers which, individually, are still slower than optics. The disadvantage of the optical FT is that, as the derivation shows, the FT relationship only holds for paraxial plane waves, so this FT \"computer\" is inherently bandlimited. On the other hand, since the wavelength of visible light is so minute in relation to even the smallest visible feature dimensions in the image i.e.,\n\n(for all \"k\", \"k\" within the spatial bandwidth of the image, so that \"k\" is nearly equal to \"k\"), the paraxial approximation is not terribly limiting in practice. And, of course, this is an analog - not a digital - computer, so precision is limited. Also, phase can be challenging to extract; often it is inferred interferometrically.\n\nOptical processing is especially useful in real time applications where rapid processing of massive amounts of 2D data is required, particularly in relation to pattern recognition.\n\nThe spatially modulated electric field, shown on the left-hand side of eqn. (2.1), typically only occupies a finite (usually rectangular) aperture in the x,y plane. The rectangular aperture function acts like a 2D square-top filter, where the field is assumed to be zero outside this 2D rectangle. The spatial domain integrals for calculating the FT coefficients on the right-hand side of eqn. (2.1) are truncated at the boundary of this aperture. This step truncation can introduce inaccuracies in both theoretical calculations and measured values of the plane wave coefficients on the RHS of eqn. (2.1).\n\nWhenever a function is discontinuously truncated in one FT domain, broadening and rippling are introduced in the other FT domain. A perfect example from optics is in connection with the point spread function, which for on-axis plane wave illumination of a quadratic lens (with circular aperture), is an Airy function, \"J\"(\"x\")/\"x\". Literally, the point source has been \"spread out\" (with ripples added), to form the Airy point spread function (as the result of truncation of the plane wave spectrum by the finite aperture of the lens). This source of error is known as Gibbs phenomenon and it may be mitigated by simply ensuring that all significant content lies near the center of the transparency, or through the use of window functions which smoothly taper the field to zero at the frame boundaries. By the convolution theorem, the FT of an arbitrary transparency function - multiplied (or truncated) by an aperture function - is equal to the FT of the non-truncated transparency function convolved against the FT of the aperture function, which in this case becomes a type of \"Greens function\" or \"impulse response function\" in the spectral domain. Therefore, the image of a circular lens is equal to the object plane function convolved against the Airy function (the FT of a circular aperture function is \"J\"(\"x\")/\"x\" and the FT of a rectangular aperture function is a product of sinc functions, sin \"x\"/\"x\").\n\nEven though the input transparency only occupies a finite portion of the \"x\"-\"y\" plane (Plane 1), the uniform plane waves comprising the plane wave spectrum occupy the entire \"x\"-\"y\" plane, which is why (for this purpose) only the longitudinal plane wave phase (in the \"z\"-direction, from Plane 1 to Plane 2) must be considered, and not the phase transverse to the \"z\"-direction. It is of course, very tempting to think that if a plane wave emanating from the finite aperture of the transparency is tilted too far from horizontal, it will somehow \"miss\" the lens altogether but again, since the uniform plane wave extends infinitely far in all directions in the transverse (\"x\"-\"y\") plane, the planar wave components cannot miss the lens.\n\nThis issue brings up perhaps the predominant difficulty with Fourier analysis, namely that the input-plane function, defined over a finite support (i.e., over its own finite aperture), is being approximated with other functions (sinusiods) which have infinite support (\"i\".\"e\"., they are defined over the entire infinite \"x\"-\"y\" plane). This is unbelievably inefficient computationally, and is the principal reason why wavelets were conceived, that is to represent a function (defined on a finite interval or area) in terms of oscillatory functions which are also defined over finite intervals or areas. Thus, instead of getting the frequency content of the entire image all at once (along with the frequency content of the entire rest of the \"x\"-\"y\" plane, over which the image has zero value), the result is instead the frequency content of different parts of the image, which is usually much simpler. Unfortunately, wavelets in the \"x\"-\"y\" plane don't correspond to any known type of propagating wave function, in the same way that Fourier's sinusoids (in the \"x\"-\"y\" plane) correspond to plane wave functions in three dimensions. However, the FTs of most wavelets are well known and could possibly be shown to be equivalent to some useful type of propagating field.\n\nOn the other hand, Sinc functions and Airy functions - which are not only the point spread functions of rectangular and circular apertures, respectively, but are also cardinal functions commonly used for functional decomposition in interpolation/sampling theory [Scott 1990] - do correspond to converging or diverging spherical waves, and therefore could potentially be implemented as a whole new functional decomposition of the object plane function, thereby leading to another point of view similar in nature to Fourier optics. This would basically be the same as conventional ray optics, but with diffraction effects included. In this case, each point spread function would be a type of \"smooth pixel,\" in much the same way that a soliton on a fiber is a \"smooth pulse.\"\n\nPerhaps a lens figure-of-merit in this \"point spread function\" viewpoint would be to ask how well a lens transforms an Airy function in the object plane into an Airy function in the image plane, as a function of radial distance from the optic axis, or as a function of the size of the object plane Airy function. This is somewhat like the point spread function, except now we're really looking at it as a kind of input-to-output plane transfer function (like MTF), and not so much in absolute terms, relative to a perfect point. Similarly, Gaussian wavelets, which would correspond to the waist of a propagating Gaussian beam, could also potentially be used in still another functional decomposition of the object plane field.\n\nIn the figure above, illustrating the Fourier transforming property of lenses, the lens is in the near field of the object plane transparency, therefore the object plane field at the lens may be regarded as a superposition of plane waves, each one of which propagates at some angle with respect to the z-axis. In this regard, the far-field criterion is loosely defined as: Range = 2 \"D\" / λ where \"D\" is the maximum linear extent of the optical sources and λ is the wavelength (Scott [1998]). The \"D\" of the transparency is on the order of cm (10 m) and the wavelength of light is on the order of 10 m, therefore \"D\"/λ for the whole transparency is on the order of 10. This times \"D\" is on the order of 10 m, or hundreds of meters. On the other hand, the far field distance from a PSF spot is on the order of λ. This is because D for the spot is on the order of λ, so that \"D\"/λ is on the order of unity; this times \"D\" (i.e., λ) is on the order of λ (10 m).\n\nSince the lens is in the far field of any PSF spot, the field incident on the lens from the spot may be regarded as being a spherical wave, as in eqn. (2.2), not as a plane wave spectrum, as in eqn. (2.1). On the other hand, the lens is in the near field of the entire input plane transparency, therefore eqn. (2.1) - the full plane wave spectrum - accurately represents the field incident on the lens from that larger, extended source.\n\nA lens is basically a low-pass plane wave filter (see Low-pass filter). Consider a \"small\" light source located on-axis in the object plane of the lens. It is assumed that the source is small enough that, by the far-field criterion, the lens is in the far field of the \"small\" source. Then, the field radiated by the small source is a spherical wave which is modulated by the FT of the source distribution, as in eqn. (2.2), Then, the lens passes - from the object plane over onto the image plane - only that portion of the radiated spherical wave which lies inside the edge angle of the lens. In this far-field case, truncation of the radiated spherical wave is equivalent to truncation of the plane wave spectrum of the small source. So, the plane wave components in this far-field spherical wave, which lie beyond the edge angle of the lens, are not captured by the lens and are not transferred over to the image plane. Note: this logic is valid only for small sources, such that the lens is in the far field region of the source, according to the 2 \"D\" / λ criterion mentioned previously. If an object plane transparency is imagined as a summation over small sources (as in the Whittaker–Shannon interpolation formula, Scott [1990]), each of which has its spectrum truncated in this fashion, then every point of the entire object plane transparency suffers the same effects of this low pass filtering.\n\nLoss of the high (spatial) frequency content causes blurring and loss of sharpness (see discussion related to point spread function). Bandwidth truncation causes a (fictitious, mathematical, ideal) point source in the object plane to be blurred (or, spread out) in the image plane, giving rise to the term, \"point spread function.\" Whenever bandwidth is expanded or contracted, image size is typically contracted or expanded accordingly, in such a way that the space-bandwidth product remains constant, by Heisenberg's principle (Scott [1998] and Abbe sine condition).\n\nWhile working in the frequency domain, with an assumed e (engineering) time dependence, coherent (laser) light is implicitly assumed, which has a delta function dependence in the frequency domain. Light at different (delta function) frequencies will \"spray\" the plane wave spectrum out at different angles, and as a result these plane wave components will be focused at different places in the output plane. The Fourier transforming property of lenses works best with coherent light, unless there is some special reason to combine light of different frequencies, to achieve some special purpose.\n\nThe theory on optical transfer functions presented in section 4 is somewhat abstract. However, there is one very well known device which implements the system transfer function H in hardware using only 2 identical lenses and a transparency plate - the 4F correlator. Although one important application of this device would certainly be to implement the mathematical operations of cross-correlation and convolution, this device - 4 focal lengths long - actually serves a wide variety of image processing operations that go well beyond what its name implies. A diagram of a typical 4F correlator is shown in the figure below (click to enlarge). This device may be readily understood by combining the plane wave spectrum representation of the electric field (\"section 2\") with the Fourier transforming property of quadratic lenses (\"section 5.1\") to yield the optical image processing operations described in section 4.\n\nThe 4F correlator is based on the convolution theorem from Fourier transform theory, which states that convolution in the spatial (\"x\",\"y\") domain is equivalent to direct multiplication in the spatial frequency (\"k\", \"k\") domain (aka: \"spectral domain\"). Once again, a plane wave is assumed incident from the left and a transparency containing one 2D function, \"f\"(\"x\",\"y\"), is placed in the input plane of the correlator, located one focal length in front of the first lens. The transparency spatially modulates the incident plane wave in magnitude and phase, like on the left-hand side of eqn. (2.1), and in so doing, produces a spectrum of plane waves corresponding to the FT of the transmittance function, like on the right-hand side of eqn. (2.1). That spectrum is then formed as an \"image\" one focal length behind the first lens, as shown. A transmission mask containing the FT of the second function, \"g\"(\"x\",\"y\"), is placed in this same plane, one focal length behind the first lens, causing the transmission through the mask to be equal to the product, \"F\"(\"k\",\"k\") x \"G\"(\"k\",\"k\"). This product now lies in the \"input plane\" of the second lens (one focal length in front), so that the FT of this product (i.e., the convolution of \"f\"(\"x\",\"y\") and \"g\"(\"x\",\"y\")), is formed in the back focal plane of the second lens.\n\nIf an ideal, mathematical point source of light is placed on-axis in the input plane of the first lens, then there will be a uniform, collimated field produced in the output plane of the first lens. When this uniform, collimated field is multiplied by the FT plane mask, and then Fourier transformed by the second lens, the output plane field (which in this case is the \"impulse response\" of the correlator) is just our correlating function, \"g\"(\"x\",\"y\"). In practical applications, \"g\"(\"x\",\"y\") will be some type of feature which must be identified and located within the input plane field (see Scott [1998]). In military applications, this feature may be a tank, ship or airplane which must be quickly identified within some more complex scene.\n\nThe 4F correlator is an excellent device for illustrating the \"systems\" aspects of optical instruments, alluded to in \"section 4\" above. The FT plane mask function, \"G\"(\"k\",\"k\") is the system transfer function of the correlator, which we'd in general denote as \"H\"(\"k\",\"k\"), and it is the FT of the impulse response function of the correlator, \"h\"(\"x\",\"y\") which is just our correlating function \"g\"(\"x\",\"y\"). And, as mentioned above, the impulse response of the correlator is just a picture of the feature we're trying to find in the input image. In the 4F correlator, the system transfer function \"H\"(\"k\",\"k\") is directly multiplied against the spectrum \"F\"(\"k\",\"k\") of the input function, to produce the spectrum of the output function. This is how electrical signal processing systems operate on 1D temporal signals.\n\nElectrical fields can be represented mathematically in many different ways. In the Huygens–Fresnel or Stratton-Chu viewpoints, the electric field is represented as a superposition of point sources, each one of which gives rise to a Green's function field. The total field is then the weighted sum of all of the individual Green's function fields. That seems to be the most natural way of viewing the electric field for most people - no doubt because most of us have, at one time or another, drawn out the circles with protractor and paper, much the same way Thomas Young did in his classic paper on the double-slit experiment. However, it is by no means the only way to represent the electric field, which may also be represented as a spectrum of sinusoidally varying plane waves. In addition, Frits Zernike proposed still another functional decomposition based on his Zernike polynomials, defined on the unit disc. The third-order (and lower) Zernike polynomials correspond to the normal lens aberrations. And still another functional decomposition could be made in terms of Sinc functions and Airy functions, as in the Whittaker–Shannon interpolation formula and the Nyquist–Shannon sampling theorem. All of these functional decompositions have utility in different circumstances. The optical scientist having access to these various representational forms has available a richer insight to the nature of these marvelous fields and their properties. These different ways of looking at the field are not conflicting or contradictory, rather, by exploring their connections, one can often gain deeper insight into the nature of wave fields.\n\nThe twin subjects of eigenfunction expansions and functional decomposition, both briefly alluded to here, are not completely independent. The eigenfunction expansions to certain linear operators defined over a given domain, will often yield a countably infinite set of orthogonal functions which will span that domain. Depending on the operator and the dimensionality (and shape, and boundary conditions) of its domain, many different types of functional decompositions are, in principle, possible.\n\n\n\n"}
{"id": "11168505", "url": "https://en.wikipedia.org/wiki?curid=11168505", "title": "G. W. Peck", "text": "G. W. Peck\n\nG. W. Peck is a pseudonymous attribution used as the author or co-author of a number of published mathematics academic papers. Peck is sometimes humorously identified with George Wilbur Peck, a former governor of the US state of Wisconsin.\n\nPeck first appeared as the official author of a 1979 paper entitled \"Maximum antichains of rectangular arrays\". The name \"G. W. Peck\" is derived from the initials of the actual writers of this paper: Ronald Graham, Douglas West, George B. Purdy, Paul Erdős, Fan Chung, and Daniel Kleitman. The paper initially listed Peck's affiliation as Xanadu, but the editor of the journal objected, so Ron Graham gave him a job at Bell Labs. Since then, Peck's name has appeared on some sixteen publications, primarily as a pseudonym of Daniel Kleitman.\n\nIn reference to \"G. W. Peck\", Richard P. Stanley defined a Peck poset to be a graded partially ordered set that is rank symmetric, rank unimodal, and strongly Sperner. The posets in the original paper by G. W. Peck are not quite Peck posets, as they lack the property of being rank symmetric.\n\n\n"}
{"id": "297446", "url": "https://en.wikipedia.org/wiki?curid=297446", "title": "Gelfand–Naimark–Segal construction", "text": "Gelfand–Naimark–Segal construction\n\nIn functional analysis, a discipline within mathematics, given a C*-algebra \"A\", the Gelfand–Naimark–Segal construction establishes a correspondence between cyclic *-representations of \"A\" and certain linear functionals on \"A\" (called \"states\"). The correspondence is shown by an explicit construction of the *-representation from the state. It is named for Israel Gelfand, Mark Naimark, and Irving Segal.\n\nA *-representation of a C*-algebra \"A\" on a Hilbert space \"H\" is a mapping\nπ from \"A\" into the algebra of bounded operators on \"H\" such that\n\nA state on C*-algebra \"A\" is a positive linear functional \"f\" of norm 1. If \"A\" has a multiplicative unit element this condition is equivalent to \"f\"(1) = 1.\n\nFor a representation π of a C*-algebra \"A\" on a Hilbert space \"H\", an element ξ is called a cyclic vector if the set of vectors\nis norm dense in \"H\", in which case π is called a cyclic representation. Any non-zero vector of an irreducible representation is cyclic. However, non-zero vectors in a cyclic representation may fail to be cyclic.\n\nLet π be a *-representation of a C*-algebra \"A\" on the Hilbert space \"H\" and ξ be a unit norm cyclic vector for π. Then\nis a state of \"A\".\n\nIn fact, every state of \"A\" may be viewed as a vector state as above, under a suitable canonical representation.\n\nThe method used to produce a *-representation from a state of \"A\" in the proof of the above theorem is called the GNS construction.\nFor a state of a C*-algebra \"A\", the corresponding GNS representation is essentially uniquely determined by the condition, formula_6 as seen in the theorem below.\n\nThe GNS construction is at the heart of the proof of the Gelfand–Naimark theorem characterizing C*-algebras as algebras of operators. A C*-algebra has sufficiently many pure states (see below) so that the direct sum of corresponding irreducible GNS representations is faithful.\n\nThe direct sum of the corresponding GNS representations of all states is called the universal representation of \"A\". The universal representation of \"A\" contains every cyclic representation. As every *-representation is a direct sum of cyclic representations, it follows that every *-representation of \"A\" is a direct summand of some sum of copies of the universal representation.\n\nIf Φ is the universal representation of a C*-algebra \"A\", the closure of Φ(\"A\") in the weak operator topology is called the enveloping von Neumann algebra of \"A\". It can be identified with the double dual \"A**\".\n\nAlso of significance is the relation between irreducible *-representations and extreme points of the convex set of states. A representation π on \"H\" is irreducible if and only if there are no closed subspaces of \"H\" which are invariant under all the operators π(\"x\") other than \"H\" itself and the trivial subspace {0}.\n\nBoth of these results follow immediately from the Banach–Alaoglu theorem.\n\nIn the unital commutative case, for the C*-algebra \"C\"(\"X\") of continuous functions on some compact \"X\", Riesz–Markov–Kakutani representation theorem says that the positive functionals of norm ≤ 1 are precisely the Borel positive measures on \"X\" with total mass ≤ 1. It follows from Krein–Milman theorem that the extremal states are the Dirac point-mass measures.\n\nOn the other hand, a representation of \"C\"(\"X\") is irreducible if and only if it is one-dimensional. Therefore the GNS representation of \"C\"(\"X\") corresponding to a measure μ is irreducible if and only if μ is an extremal state. This is in fact true for C*-algebras in general.\n\nTo prove this result one notes first that a representation is irreducible if and only if the commutant of π(\"A\"), denoted by π(\"A\")', consists of scalar multiples of the identity.\n\nAny positive linear functionals \"g\" on \"A\" dominated by \"f\" is of the form\n\nfor some positive operator \"T\" in π(\"A\")' with 0 ≤ \"T\" ≤ 1 in the operator order. This is a version of the Radon–Nikodym theorem.\n\nFor such \"g\", one can write \"f\" as a sum of positive linear functionals: \"f\" = \"g\" + \"g' \". So π is unitarily equivalent to a subrepresentation of π ⊕ π. This shows that π is irreducible if and only if any such π is unitarily equivalent to π, i.e. \"g\" is a scalar multiple of \"f\", which proves the theorem.\n\nExtremal states are usually called pure states. Note that a state is a pure state if and only if it is extremal in the convex set of states.\n\nThe theorems above for C*-algebras are valid more generally in the context of B*-algebras with approximate identity.\n\nThe Stinespring factorization theorem characterizing completely positive maps is an important generalization of the GNS construction.\n\nGelfand and Naimark's paper on the Gelfand–Naimark theorem was published in 1943. Segal recognized the construction that was implicit in this work and presented it in sharpened form.\n\nIn his paper of 1947 Segal showed that it is sufficient, for any physical system that can be described by an algebra of operators on a Hilbert space, to consider the \"irreducible\" representations of a C*-algebra. In quantum theory this means that the C*-algebra is generated by the observables. This, as Segal pointed out, had been shown earlier by John von Neumann only for the specific case of the non-relativistic Schrödinger-Heisenberg theory.\n\n\n"}
{"id": "35841633", "url": "https://en.wikipedia.org/wiki?curid=35841633", "title": "Goudreau Museum of Mathematics in Art and Science", "text": "Goudreau Museum of Mathematics in Art and Science\n\nThe Goudreau Museum of Mathematics in Art and Science was a museum of math that was open from 1980–2006 in Long Island, New York. The museum was named after mathematics teacher Bernhard Goudreau, who had died in 1985, and featured many of the 3-dimensional solid models, oversized wooden math games, and puzzles built by Goudreau and his former students. After the museum closed, Glen Whitney, a former math professor, decided to open the Museum of Mathematics in Manhattan (New York City), which opened in December 2012.\n"}
{"id": "1063408", "url": "https://en.wikipedia.org/wiki?curid=1063408", "title": "HAS-160", "text": "HAS-160\n\nHAS-160 is a cryptographic hash function designed for use with the Korean KCDSA digital signature algorithm. It is derived from SHA-1, with assorted changes intended to increase its security. It produces a 160-bit output.\n\nHAS-160 is used in the same way as SHA-1. First it divides input in blocks of 512 bits each and pads the final block. A digest function updates the intermediate hash value by processing the input blocks in turn.\n\nThe message digest algorithm consists of 80 rounds.\n\n"}
{"id": "37797", "url": "https://en.wikipedia.org/wiki?curid=37797", "title": "Hilbert's paradox of the Grand Hotel", "text": "Hilbert's paradox of the Grand Hotel\n\nHilbert's paradox of the Grand Hotel (colloquial: Infinite Hotel Paradox or Hilbert's Hotel) is a thought experiment which illustrates a counterintuitive property of infinite sets. It is demonstrated that a fully occupied hotel with infinitely many rooms may still accommodate additional guests, even infinitely many of them, and this process may be repeated infinitely often. The idea was introduced by David Hilbert in a 1924 lecture \"Über das Unendliche\", reprinted in , and was popularized through George Gamow's 1947 book \"One Two Three... Infinity\".\n\nConsider a hypothetical hotel with a countably infinite number of rooms, all of which are occupied. One might be tempted to think that the hotel would not be able to accommodate any newly arriving guests, as would be the case with a finite number of rooms, where the pigeonhole principle would apply.\n\nSuppose a new guest arrives and wishes to be accommodated in the hotel. We can (simultaneously) move the guest currently in room 1 to room 2, the guest currently in room 2 to room 3, and so on, moving every guest from his current room \"n\" to room \"n\"+1. After this, room 1 is empty and the new guest can be moved into that room. By repeating this procedure, it is possible to make room for any finite number of new guests.\n\nIt is also possible to accommodate a \"countably infinite\" number of new guests: just move the person occupying room 1 to room 2, the guest occupying room 2 to room 4, and, in general, the guest occupying room \"n\" to room 2\"n\" (2 times \"n\"), and all the odd-numbered rooms (which are countably infinite) will be free for the new guests.\n\nIt is possible to accommodate countably infinitely many coachloads of countably infinite passengers each, by several different methods. Most methods depend on the seats in the coaches being already numbered (or use the axiom of countable choice). In general any pairing function can be used to solve this problem. For each of these methods, consider a passenger's seat number on a coach to be formula_1, and their coach number to be formula_2, and the numbers formula_1 and formula_2 are then fed into the two arguments of the pairing function.\n\nEmpty the odd numbered rooms by sending the guest in room formula_5 to room formula_6, then put the first coach's load in rooms formula_7, the second coach's load in rooms formula_8; for coach number formula_2 we use the rooms formula_10 where formula_11 is the formula_2th odd prime number. This solution leaves certain rooms empty (which may or may not be useful to the hotel); specifically, all odd numbers that are not prime powers, such as 15 or 847, will no longer be occupied. (So, strictly speaking, this shows that the number of arrivals is \"less than or equal to\" the number of vacancies created. It is easier to show, by an independent means, that the number of arrivals is also \"greater than or equal to\" the number of vacancies, and thus that they are \"equal\", than to modify the algorithm to an exact fit.) (The algorithm works equally well if one interchanges formula_1 and formula_2, but whichever choice is made, it must be applied uniformly throughout.)\n\nYou can put each person of a certain seat formula_15 and coach formula_2 into room formula_17 (presuming \"c\"=0 for the people already in the hotel, 1 for the first coach, etc. ...). Because every number has a unique prime factorization, it's easy to see all people will have a room, while no two people will end up in the same room. For example, the person in room 2592 (formula_18) was sitting in on the 4th coach, on the 5th seat. Like the prime powers method, this solution leaves certain rooms empty.\n\nThis method can also easily be expanded for infinite nights, infinite entrances, etc. ... ( formula_19 )\n\nFor each passenger, compare the lengths of formula_1 and formula_2 as written in any positional numeral system, such as decimal. (Treat each hotel resident as being in coach #0.) If either number is shorter, add leading zeroes to it until both values have the same number of digits. Interleave the digits to produce a room number: its digits will be [first digit of coach number]-[first digit of seat number]-[second digit of coach number]-[second digit of seat number]-etc. The hotel (coach #0) guest in room number 1729 moves to room 01070209 (i.e., room 1,070,209.) The passenger on seat 1234 of coach 789 goes to room 01728394 (or just 1728394).\n\nUnlike the prime powers solution, this one fills the hotel completely, and we can reconstruct a guest's original coach and seat by reversing the interleaving process. First add a leading zero if the room has an odd number of digits. Then de-interleave the number into two numbers: the seat number consists of the odd-numbered digits and the coach number is the even-numbered ones. Of course, the original encoding is arbitrary, and the roles of the two numbers can be reversed (seat-odd and coach-even), so long as it is applied consistently.\n\nThose already in the hotel will be moved to room formula_22, or the formula_1th triangular number. Those in a coach will be in room formula_24, or the formula_25 triangular number plus formula_1. In this way all the rooms will be filled by one, and only one, guest.\n\nThis pairing function can be demonstrated visually by structuring the hotel as a one-room-deep, infinitely tall pyramid. The pyramid's topmost row is a single room: room 1; its second row is rooms 2 and 3; and so on. The column formed by the set of rightmost rooms will correspond to the triangular numbers. Once they are filled (by the hotel's redistributed occupants), the remaining empty rooms form the shape of a pyramid exactly identical to the original shape. Thus, the process can be repeated for each infinite set. Doing this one at a time for each coach would require an infinite number of steps, but by using the prior formulas, a guest can determine what his room \"will be\" once his coach has been reached in the process, and can simply go there immediately.\n\nLet formula_27. formula_28 is countable since formula_29 is countable, hence we may enumerate its elements formula_30. Now if formula_31, assign the formula_32th guest of the formula_33th coach to the formula_1th room (consider the guests already in the hotel as guests of the formula_35th coach). Thus we have a function assigning each person to a room; furthermore, this assignment does not skip over any rooms.\n\nSuppose the hotel is next to an ocean, and an infinite number of car ferries arrive, each bearing an infinite number of coaches, each with an infinite number of passengers. This is a situation involving three \"levels\" of infinity, and it can be solved by extensions of any of the previous solutions.\n\nThe prime factorization method can be applied by adding a new prime number for every additional layer of infinity ( formula_36, with formula_37 the ferry).\n\nThe prime power solution can be applied with further exponentiation of prime numbers, resulting in very large room numbers even given small inputs. For example, the passenger in the second seat of the third bus on the second ferry (address 2-3-2) would raise the 2nd odd prime (5) to 49, which is the result of the 3rd odd prime (7) being raised to the power of his seat number (2). This room number would have over thirty decimal digits.\n\nThe interleaving method can be used with three interleaved \"strands\" instead of two. The passenger with the address 2-3-2 would go to room 232, while the one with the address 4935-198-82217 would go to room #008,402,912,391,587 (the leading zeroes can be removed).\n\nAnticipating the possibility of any number of layers of infinite guests, the hotel may wish to assign rooms such that no guest will need to move, no matter how many guests arrive afterward. One solution is to convert each arrival's address into a binary number in which ones are used as separators at the start of each layer, while a number within a given layer (such as a guests' coach number) is represented with that many zeroes. Thus, a guest with the prior address 2-5-1-3-1 (five infinite layers) would go to room 10010000010100010 (decimal 295458).\n\nAs an added step in this process, one zero can be removed from each section of the number; in this example, the guest's new room is 101000011001 (decimal 2585). This ensures that every room could be filled by a hypothetical guest. If no infinite sets of guests arrive, then only rooms that are a power of two will be occupied.\n\nAlthough a room can be found for any finite number of nested infinities of people, the same is not always true for an infinite number of layers, even if a finite number of elements exists at each layer.\n\nThe set of real numbers, and the set of guests in this example, is uncountably infinite. Because no one-to-one pairing can be made between countable and uncountable sets, rooms at the hotel cannot be made for all of these guests, although any countably infinite subset of them can still be accommodated.\n\nIf this variant is modified in certain ways, then the set of people is countable again. For example, suppose there \"were\" a largest ship, directly containing a finite (or countably infinite) number of both ships and people, and each of these ships in turn contained both ships and people, and so forth. This time, any given person is a finite number of levels \"down\" from the top, and thus can be identified with a unique finite address. The set of people is countable again, even if the total number of layers is infinite, because we do not have to consider an \"infinitieth layer\" in either direction.\n\nHilbert's paradox is a veridical paradox: it leads to a counter-intuitive result that is provably true. The statements \"there is a guest to every room\" and \"no more guests can be accommodated\" are not equivalent when there are infinitely many rooms.\n\nInitially, this state of affairs might seem to be counter-intuitive. The properties of \"infinite collections of things\" are quite different from those of \"finite collections of things\". The paradox of Hilbert's Grand Hotel can be understood by using Cantor's theory of transfinite numbers. Thus, while in an ordinary (finite) hotel with more than one room, the number of odd-numbered rooms is obviously smaller than the total number of rooms. However, in Hilbert's aptly named Grand Hotel, the quantity of odd-numbered rooms is not smaller than the total \"number\" of rooms. In mathematical terms, the cardinality of the subset containing the odd-numbered rooms is the same as the cardinality of the set of all rooms. Indeed, infinite sets are characterized as sets that have proper subsets of the same cardinality. For countable sets (sets with the same cardinality as the natural numbers) this cardinality is formula_38.\n\nRephrased, for any countably infinite set, there exists a bijective function which maps the countably infinite set to the set of natural numbers, even if the countably infinite set contains the natural numbers. For example, the set of rational numbers—those numbers which can be written as a quotient of integers—contains the natural numbers as a subset, but is no bigger than the set of natural numbers since the rationals are countable: there is a bijection from the naturals to the rationals.\n\n\n\n"}
{"id": "32612385", "url": "https://en.wikipedia.org/wiki?curid=32612385", "title": "Hindley–Milner type system", "text": "Hindley–Milner type system\n\nA Hindley–Milner (HM) type system is a classical type system for the lambda calculus with parametric polymorphism. It is also known as Damas–Milner or Damas–Hindley–Milner. It was first described by J. Roger Hindley and later rediscovered by Robin Milner. Luis Damas contributed a close formal analysis and proof of the method in his PhD thesis.\n\nAmong HM's more notable properties are its completeness and its ability to infer the most general type of a given program without programmer-supplied type annotations or other hints. Algorithm W is an efficient type inference method that performs in almost linear time with respect to the size of the source, making it practically useful to type large programs. HM is preferably used for functional languages. It was first implemented as part of the type system of the programming language ML. Since then, HM has been extended in various ways, most notably with type class constraints like those in Haskell.\n\nOne and the same thing can be used for many purposes. A chair might be used to support a sitting person but also as a ladder to stand on while changing a light bulb or as a clothes valet. Beside having particular material qualities, which make a chair usable as such, it also has the particular designation for its use. When no chair is at hand, other things might be used as a seat, and so the designation of a thing can be changed as fast as one can turn an empty bottle crate upside down to change its purpose from a container to that of a support.\n\nDifferent uses of physically near-identical things are usually accompanied by giving those things different names to emphasize the intended purpose. Depending on the use, seamen have a dozen or more words for a rope though it might materially be the same thing. The same in everyday language, where a leash indicates a use different to a line.\n\nIn computer science, this practice of naming things by its intended use is put to an extreme called \"typing\" and the names or expressions called \"types\":\n\n\nBeside structuring objects, (data) types serve as means to validate that these objects are used as intended. Much like a crate that could only be used as a support or a container at a time, a particular arrangement of bytes designated for one purpose might exclude other possible uses.\n\nIn programming, these uses are expressed as \"functions\" or \"procedures\" which serve the role of verbs in natural language. As an example for typing verbs, an English dictionary might define \"gift\" as \"to give so. sth.\", indicating that the object must be a person and the indirect object a physical thing. In programming, \"so.\" (someone) and \"sth.\" (something) would be called types and using a thing into the place of \"so.\" would be indicated as a programming error by a type checker.\n\nBeside checking, one can use the types in this example to gain knowledge about an unknown word. Reading the sentence \"Mary gifts John a bilber\" the types could be used to conclude that a \"bilber\" is likely a physical thing. This activity and conclusion is called \"type inference\". As the story unfolds, more and more information about the unknown \"bilber\" may be gained, and eventually enough details become known to form a complete image of that kind of thing.\n\nThe type inference method designed by Hindley and Milner does just this for programming languages. The advantage of type inference over type checking is that it allows a more natural and dense style of programming. Instead of starting a program text with a glossary defining what a bilber and everything else is, one can distribute this information over the text simply by using the yet undefined words and let a program collect all the details about them. The method works for both nouns (data types) and for verbs (functions types). As a consequence, a programmer can proceed without ever mentioning types at all, while still having the full support of a type checker that validates their writing. When reading a program, the programmer can use type inference to query the full definition of anything named in the program whenever needed.\n\nHistorically, type inference to this extent was developed for a particular group of programming languages, called functional languages. These started in 1958 with Lisp, a programming language based on the lambda calculus and that compares well with modern scripting languages like Python or Lua. Lisp was mainly used for computer science research, often for symbol manipulation purposes where large, tree-like data structures were common.\n\nData in Lisp is dynamically typed and the types are only available to some degree while running a program. Debugging type errors was no less of a concern than it is with modern script languages. But, being completely untyped, i.e. written without any explicit type information, maintaining large programs written in Lisp soon became a problem because the many complicated types of the data were mentioned only in the program documentation and comments at best.\n\nThus, the need to have a Lisp-like language with machine-checkable types became more and more pressing. At some point, programming language development faced two challenges:\n\n\nAs an example, polymorphically constructing the list \"(1 2)\" of two numbers would mean writing:\n\nThis example was quite typical. Every third word a type, monotonously serving the type checker in every step. This worsens when the types become more complex. Then, the methods to be expressed in code become buried in types.\n\nTo handle this issue, effective methods for type inference were the subject of research, and Hindley-Milner's method was one of them. Their method was first used in ML (1973) and is also used in an extended form in Haskell (1990). The HM type inference method is strong enough to infer types not only for expressions, but for whole programs including the procedures and local definitions, providing a type-less style of programming.\n\nThe following text gives an impression of the resulting programming style for the quicksort procedure in Haskell:\n\nThough all of the functions in the above example need type parameters, types are nowhere mentioned. The code is statically type-checked even though the type of the function defined is unknown and must be inferred to type-check the applications in the body.\n\nOver the years, other programming languages added their own version of parametric types. C++ templates were introduced in 1998 and Java introduced generics in 2004. As programming with type parameters became more common, problems similar to the ones sketched for Lisp surfaced in imperative languages too, perhaps not as pressing as it was for the functional languages. As a consequence, these languages obtained support for some type inference techniques, for instance \"auto\" in C++11 (2014). Unfortunately, the stronger type inference methods developed for functional programming cannot simply be integrated in the imperative languages, as their type systems' features are in part incompatible, and a programming language must be designed from the ground up when continuous type inference is wanted.\n\nBefore presenting the HM type system and related algorithms, the following sections make some features of HM more formal and precise.\n\nIn a typing, an expression E is opposed to a type T, formally written as E : T. Usually a typing only makes sense within some context, which is omitted here.\n\nIn this setting, the following questions are of particular interest:\n\n\nFor the simply typed lambda calculus, all three questions are decidable. The situation is not as comfortable when more expressive types are allowed. Additionally, the simply typed lambda calculus makes the types of the parameters of each function explicit, while they are not needed in HM. While HM is a method for type inference, it can be used also for type checking and answer the first question. To do that, a type is first inferred from E and then compared with the type wanted. The third question becomes of interest when looking at recursively-defined functions at the end of this article.\n\nIn the simply typed lambda calculus, types formula_1 are either atomic type constants or function types of form formula_2. Such types are \"monomorphic\". Typical examples are the types used in arithmetic values:\n\nContrary to this, the untyped lambda calculus is neutral to typing at all, and many of its functions can be meaningfully applied to all type of arguments. The trivial example is the identity function\n\nwhich simply returns whatever value it is applied to. Less trivial examples include parametric types like lists.\n\nWhile polymorphism in general means that operations accept values of more than one type, the polymorphism used here is parametric. One finds the notation of \"type schemes\" in the literature, too, emphasizing the parametric nature of the polymorphism. Additionally, constants may be typed with (quantified) type variables. E.g.:\n\nPolymorphic types can become monomorphic by consistent substitution of their variables. Examples of monomorphic \"instances\" are:\n\nMore generally, types are polymorphic when they contain type variables, while types without them are monomorphic.\n\nContrary to the type systems used for example in Pascal (1970) or C (1972), which only support monomorphic types, HM is designed with emphasis on parametric polymorphism. The successors of the languages mentioned, like C++ (1985), focused on different types of polymorphism, namely subtyping in connection with object-oriented programming and overloading. While subtyping is incompatible with HM, a variant of systematic overloading is available in the HM-based type system of Haskell.\n\nWhen extending the simply-typed lambda calculus towards polymorphism, one has to define when deriving an instance of a value is admissible. Ideally, this would be allowed with any use of a bound variable, as in:\n\nUnfortunately, type inference in such a system is not decidable. Instead, HM provides \"let-polymorphism\" of the form\n\nrestricting the binding mechanism in an extension of the expression syntax. Only values bound in a let construct are subject to instantiation, i.e. are polymorphic, while the parameters in lambda-abstractions are treated as being monomorphic.\n\nThe remainder of the article is more technical as it has to present the HM method as it is handled in the literature. It proceeds as follows:\n\n\nThe same description of the deduction system is used throughout, even for the two algorithms, to make the various forms in which the HM method is presented directly comparable.\n\nThe type system can be formally described by syntax rules that fix a language for the expressions, types, etc. The presentation here of such a syntax is not too formal, in that it is written down not to study the surface grammar, but rather the depth grammar, and leaves some syntactical details open. This form of presentation is usual. Building on this, type rules are used to define how expressions and types are related. As before, the form used is a bit liberal.\n\nThe expressions to be typed are exactly those of the lambda calculus extended with a let-expression as shown in the adjacent table. Parentheses can be used to disambiguate an expression. The application is left-binding and binds stronger than abstraction or the let-in construct.\n\nTypes are syntactically split into two groups, monotypes and polytypes.\n\nMonotypes always designate a particular type. Monotypes formula_4 are syntactically represented as terms.\n\nExamples of monotypes include type constants like formula_5 or formula_6, and parametric types like formula_7. The later types are examples of \"applications\" of type functions, for example, from the set\nformula_8, \nwhere the superscript indicates the number of type parameters. The complete set of type functions formula_9 is arbitrary in HM, except that it \"must\" contain at least formula_10, the type of functions. It is often written in infix notation for convenience. For example, a function mapping integers to strings has type formula_11. Again, parentheses can be used to disambiguate a type expression. The application binds stronger than the infix arrow, which is right-binding.\n\nType variables are admitted as monotypes. Monotypes are not to be confused with monomorphic types, which exclude variables and allow only ground terms.\n\nTwo monotypes are equal if they have identical terms.\n\n\"Polytypes\" (or \"type schemes\") are types containing variables bound by one or more for-all quantifiers, e.g. formula_12.\n\nA function with polytype formula_12 can map \"any\" value of the same type to itself,\nand the identity function is a value for this type.\n\nAs another example formula_14 is the type of a function mapping all finite sets to integers. A function which returns the cardinality of a set would be a value of this type.\n\nNote that quantifiers can only appear top level, i.e. a type formula_15 for instance, is excluded by the syntax of types. Note also that monotypes are included in the polytypes, thus a type has the general form formula_16, where formula_4 is a monotype.\n\nEquality of polytypes is up to reordering the quantification and renaming the quantified variables (formula_18-conversion). Further, quantified variables not occurring in the monotype can be dropped.\n\nTo meaningfully bring together the still disjoint parts (syntax expressions and types) a third part is needed: context. Syntactically, a context is a list of pairs formula_19, called assignments, assumptions or bindings, each pair stating that value variable formula_20has type formula_21. All three parts combined give a \"typing judgment\" of the form formula_22,\nstating that under assumptions formula_23, the expression formula_24 has type formula_25.\n\nIn a type formula_16, the symbol formula_27 is the quantifier binding the type variables formula_28 in the monotype formula_4. The variables formula_28 are called \"quantified\" and any occurrence of a quantified type variable in formula_4 is called \"bound\" and all unbound type variables in formula_4 are called \"free\". Additionally to the quantification formula_27 in polytypes, type variables can also be bound by occurring in the context, but with the inverse effect on the right hand side of the formula_34. Such variables then behave like type constants there. Finally, a type variable may legally occur unbound in a typing, in which case they are implicitly all-quantified.\n\nThe presence of both bound and unbound type variables is a bit uncommon in programming languages. Often, all type variables are implicitly treated all-quantified. For instance, one does not have clauses with free variables in Prolog. Likely in Haskell, in the absence of the ScopedTypeVariables language extension, all type variables implicitly occur quantified, i.e. a Haskell type codice_4 means formula_12 here.\n\nPolymorphism means that one and the same expression can have (perhaps\ninfinitely) many types. But in this type system, these types are not completely\nunrelated, but rather orchestrated by the parametric polymorphism.\n\nAs an example, the identity formula_36 can have formula_37 as its type as well as\nformula_38 or formula_39 and many others, but not formula_40. The most general type for this function is\nformula_41, while the\nothers are more specific and can be derived from the general one by consistently\nreplacing another type for the \"type parameter\", i.e. the quantified\nvariable formula_18. The counter-example fails because the\nreplacement is not consistent.\n\nThe consistent replacement can be made formal by applying a substitution formula_43 to the term of a type formula_4, written formula_45. As the example suggests, substitution is not only strongly related to an order, that expresses that a type is more or less special, but also with the all-quantification which allows the substitution to be applied.\nFormally, in HM, a type formula_25 is \nmore general than formula_47, formally formula_48 if some quantified variable in formula_25 is\nconsistently substituted such that one gains formula_47 as shown in the side bar.\nThis order is part of the type definition of the type system.\n\nWhile substituting a monomorphic (ground) type for a quantified variable is\nstraight forward, substituting a polytype has some pitfalls caused by the\npresence of free variables. Most particularly, unbound variables must not be\nreplaced. They are treated as constants here. Additionally, note\nthat quantifications can only occur top-level. Substituting a parametric type,\none has to lift its quantors. The table on the right makes the rule precise.\n\nAlternatively, consider an equivalent notation for the polytypes without\nquantors in which quantified variables are represented by a different set of\nsymbols. In such a notation, the specialization reduces to plain consistent\nreplacement of such variables.\n\nThe relation formula_51 is a partial order\nand formula_52 is its smallest element.\n\nWhile specialization of a type scheme is one use of the order, it plays a\ncrucial second role in the type system. Type inference with polymorphism\nfaces the challenge of summarizing all possible types an expression may have.\nThe order guarantees that such a summary exists as the most general type\nof the expression.\n\nThe type order defined above can be extended to typings because the implied all-quantification of typings enables consistent replacement:\nContrary to the specialisation rule, this is not part of the definition, but like the implicit all-quantification rather a consequence of the type rules defined next.\nFree type variables in a typing serve as placeholders for possible refinement. The binding effect of the environment to free type\nvariables on the right hand side of formula_34 that prohibits their substitution in the specialisation rule is again\nthat a replacement has to be consistent and would need to include the whole typing.\n\nThe syntax of HM is carried forward to the syntax of the inference rules that form the body of the formal system, by using the typings as judgments. Each of the rules define what conclusion could be drawn from what premises. Additionally to the judgments, some extra conditions introduced above might be used as premises, too.\n\nA proof using the rules is a sequence of judgments such that all premises are listed before a conclusion. The examples below show a possible format of proofs. From left to right, each line shows the conclusion, the formula_55 of the rule applied and the premises, either by referring to an earlier line (number) if the premise is a judgment or by making the predicate explicit.\n\nThe side box shows the deduction rules of the HM type system. One can roughly divide the rules into two groups:\n\nThe first four rules formula_56 (variable or function access), formula_57 (\"application\", i.e. function call with one parameter), formula_58 (\"abstraction\", i.e. function declaration) and formula_59 (variable declaration) are centered around the syntax, presenting one rule for each of the expression forms. Their meaning is obvious at the first glance, as they decompose each expression, prove their sub-expressions and finally combine the individual types found in the premises to the type in the conclusion.\n\nThe second group is formed by the remaining two rules formula_60 and formula_61.\nThey handle specialization and generalization of types. While the rule formula_60 should be clear from the section on specialization above, formula_61 complements the former, working in the opposite direction. It allows generalization, i.e. to quantify monotype variables not bound in the context.\nThe following two examples exercise the rule system in action. Since both the expression and the type are given, they are a type-checking use of the rules.\n\nExample: A proof for formula_64 where formula_65,\ncould be written\n\nExample: To demonstrate generalization,\nformula_67\nis shown below:\n\nNot visible immediately, the rule set encodes a regulation under which circumstances a type might be generalized or not by a slightly varying use of mono- and polytypes in the rules formula_58 and formula_59. Remember that formula_25 and formula_4 denote poly- and monotypes respectively.\n\nIn rule formula_58, the value variable of the parameter of the function formula_74 is added to the context with a monomorphic type through the premise formula_75, while in the rule formula_59, the variable enters the environment in polymorphic form formula_77. Though in both cases the presence of formula_78 in the context prevents the use of the generalisation rule for any free variable in the assignment, this regulation forces the type of parameter formula_78 in a formula_80-expression to remain monomorphic, while in a let-expression, the variable could be introduced polymorphic, making specializations possible.\n\nAs a consequence of this regulation, formula_81 cannot be typed,\nsince the parameter formula_82 is in a monomorphic position, while formula_83 has type formula_84, because formula_82 has been introduced in a let-expression and is treated polymorphic therefore.\n\nThe generalisation rule is also worth for closer look. Here, the all-quantification implicit in the premise formula_86 is simply moved to the right hand side of formula_87 in the conclusion. This is possible, since formula_18 does not occur free in the context. Again, while this makes the generalisation rule plausible, it is not really a consequence. Vis versa, the generalisation rule is part of the definition of HM's type system and the implicit all-quantification a consequence.\n\nNow that the deduction system of HM is at hand, one could present an algorithm and validate it with respect to the rules.\nAlternatively, it might be possible to derive it by taking a closer look on how the rules interact and proof are\nformed. This is done in the remainder of this article focusing on the possible decisions one can make while proving a typing.\n\nIsolating the points in a proof, where no decision is possible at all,\nthe first group of rules centered around the syntax leaves no choice since\nto each syntactical rule corresponds a unique typing rule, which determines\na part of the proof, while between the conclusion and the premises of these\nfixed parts chains of formula_60 and formula_61\ncould occur. Such a chain could also exist between the conclusion of the\nproof and the rule for topmost expression. All proofs must have\nthe so sketched shape.\n\nBecause the only choice in a proof with respect of rule selection are the\nformula_60 and formula_61 chains, the\nform of the proof suggests the question whether it can be made more precise,\nwhere these chains might be needed. This is in fact possible and leads to a\nvariant of the rules system with no such rules.\n\nA contemporary treatment of HM uses a purely syntax-directed rule system due to\nClement\nas an intermediate step. In this system, the specialization is located directly after the original formula_56 rule\nand merged into it, while the generalization becomes part of the formula_59 rule. There the generalization is\nalso determined to always produce the most general type by introducing the function formula_95, which quantifies\nall monotype variables not bound in formula_23.\n\nFormally, to validate, that this new rule system formula_97 is equivalent to the original formula_87, one has\nto show that formula_99, which falls apart into two sub-proofs:\n\n\nWhile consistency can be seen by decomposing the rules formula_59 and formula_56\nof formula_97 into proofs in formula_87, it is likely visible that formula_97 is incomplete, as\none cannot show formula_107 in formula_97, for instance, but only\nformula_109. An only slightly weaker version of completeness is provable\n\n\nimplying, one can derive the principal type for an expression in formula_97 allowing us to generalize the proof in the end.\n\nComparing formula_87 and formula_97, note that now only monotypes appear in the judgments of all rules. Additionally, the shape of any possible proof with the deduction system is now identical to the shape of the expression (both seen as trees). Thus the expression fully determines the shape of the proof. In formula_87 the shape would likely be determined with respect to all rules except formula_60 and formula_61, which allow building arbitrarily long branches (chains) between the other nodes.\n\nNow that the shape of the proof is known, one is already close to formulating a type inference algorithm.\nBecause any proof for a given expression must have the same shape, one can assume the monotypes in the\nproof's judgements to be undetermined and consider how to determine them.\n\nHere, the substitution (specialisation) order comes into play. Although at the first glance one cannot determine the types locally, the hope is that it is possible to refine them with the help of the order while traversing the proof tree, additionally assuming, because the resulting algorithm is to become an inference method, that the type in any premise will be determined as the best possible. And in fact, one can, as looking at the rules of formula_97 suggests:\n\n\nThe first premise forces the outcome of the inference to be of the form formula_129.\n\nThe second premise requires that the inferred type is equal to formula_4 of the first premise. Now there are two possibly different types, perhaps with open type variables, at hand to compare and to make equal if it is possible. If it is, a refinement is found, and if not, a type error is detected again. An effective method is known to \"make two terms equal\" by substitution, Robinson's Unification in combination with the so-called Union-Find algorithm.\n\nTo briefly summarize the union-find algorithm, given the set of all types in a proof, it allows one to group them together into equivalence classes by means of a formula_132\nprocedure and to pick a representative for each such class using a formula_133 procedure. Emphasizing the word procedure in the sense of side effect, we're clearly leaving the realm of logic in order to prepare an effective algorithm. The representative of a formula_134 is determined such that, if both formula_135 and formula_136 are type variables then the representative is arbitrarily one of them, but while uniting a variable and a term, the term becomes the representative. Assuming an implementation of union-find at hand, one can formulate the unification of two monotypes as follows:\n\nNow having a sketch of an inference algorithm at hand, a more formal presentation is given in the next section. It is described in Milner P. 370 ff. as algorithm J.\n\nThe presentation of Algorithm J is a misuse of the notation of logical rules, since it includes side effects but allows a direct comparison with formula_97 while expressing an efficient implementation at the same time. The rules now specify a procedure with parameters formula_138 yielding formula_4 in the conclusion where the execution of the premises proceeds from left to right.\n\nThe procedure formula_140 specializes the polytype formula_25 by copying the term and replacing the bound type variables consistently by new monotype variables. 'formula_142' produces a new monotype variable. Likely, formula_95 has to copy the type introducing new variables for the quantification to avoid unwanted captures. Overall, the algorithm now proceeds by always making the most general choice leaving the specialization to the unification, which by itself produces the most general result. As noted above, the final result formula_4 has to be generalized to formula_95 in the end, to gain the most general type for a given expression.\n\nBecause the procedures used in the algorithm have nearly O(1) cost, the overall cost of the algorithm is close to linear in the size of the expression for which a type is to be inferred. This is in strong contrast to many other attempts to derive type inference algorithms, which often came out to be NP-hard, if not undecidable with respect to termination. Thus the HM performs as well as the best fully informed type-checking algorithms can. Type-checking here means that an algorithm does not have to find a proof, but only to validate a given one.\n\nEfficiency is slightly reduced because the binding of type variables in the context has to be maintained to allow computation of formula_95 and enable an occurs check to prevent the building of recursive types during formula_147.\nAn example of such a case is formula_148, for which no type can be derived using HM. Practically, types are only small terms and do not build up expanding structures. Thus, in complexity analysis, one can treat comparing them as a constant, retaining O(1) costs.\n\nIn the previous section, while sketching the algorithm its proof was hinted at with metalogical argumentation. While this leads to an efficient algorithm J, it is\nnot clear whether the algorithm properly reflects the deduction systems D or S\nwhich serve as a semantic base line.\n\nThe most critical point in the above argumentation is the refinement of monotype\nvariables bound by the context. For instance, the algorithm boldly changes the\ncontext while inferring e.g. formula_149,\nbecause the monotype variable added to the context for the parameter formula_82 later needs to be refined\nto formula_151 when handling application.\nThe problem is that the deduction rules do not allow such a refinement.\nArguing that the refined type could have been added earlier instead of the\nmonotype variable is an expedient at best.\n\nThe key to reaching a formally satisfying argument is to properly include\nthe context within the refinement. Formally,\ntyping is compatible with substitution of free type variables.\n\nTo refine the free variables thus means to refine the whole typing.\n\nFrom there, a proof of algorithm J leads to algorithm W, which only makes the\nside effects imposed by the procedure formula_153 explicit by\nexpressing its serial composition by means of the substitutions\nformula_154. The presentation of algorithm W in the sidebar still makes use of side effects\nin the operations set in italic, but these are now limited to generating\nfresh symbols. The form of judgement is formula_155,\ndenoting a function with a context and expression as parameter producing a monotype together with\na substitution. formula_156 is a side-effect free version\nof formula_153 producing a substitution which is the most general unifier.\n\nWhile algorithm W is normally considered to be \"the\" HM algorithm and is\noften directly presented after the rule system in literature, its purpose is\ndescribed by Milner on P. 369 as follows:\n\nWhile he considered W more complicated and less efficient, he presented it \nin his publication before J. It has its merits when side effects are unavailable or unwanted.\nBy the way, W is also needed to prove completeness, which is factored by him into the soundness proof.\n\nBefore formulating the proof obligations, a deviation between the rules systems\nD and S and the algorithms presented needs to be emphasized.\n\nWhile the development above sort of misused the monotypes as \"open\" proof variables, the possibility that proper monotype variables might be harmed was sidestepped by introducing fresh variables and hoping for the best. But there's a catch: One of the promises made was that these fresh variables would be \"kept in mind\" as such. This promise is not fulfilled by the algorithm.\n\nHaving a context formula_158, the expression formula_159\ncannot be typed in either formula_87 or formula_97, but the algorithms come up with\nthe type formula_162, where W additionally delivers the substitution formula_163,\nmeaning that the algorithm fails to detect all type errors. This omission can easily be fixed by more carefully distinguishing proof\nvariables and monotype variables.\n\nThe authors were well aware of the problem but decided not to fix it. One might assume a pragmatic reason behind this.\nWhile more properly implementing the type inference would have enabled the algorithm to deal with abstract monotypes,\nthey were not needed for the intended application where none of the items in a preexisting context have free\nvariables. In this light, the unneeded complication was dropped in favor of a simpler algorithm.\nThe remaining downside is that the proof of the algorithm with respect to the rule system is less general and can only be made\nfor contexts with formula_164 as a side condition.\n\nformula_165\n\nThe side condition in the completeness obligation addresses how the deduction may give many types, while the algorithm always produces one. At the same time, the side condition demands that the type inferred is actually the most general.\n\nTo properly prove the obligations one needs to strengthen them first to allow activating the substitution lemma threading the substitution formula_166 through formula_97 and formula_168. From there, the proofs are by induction over the expression.\n\nAnother proof obligation is the substitution lemma itself, i.e. the substitution of the typing, which finally establishes the all-quantification. The later cannot formally be proven, since no such syntax is at hand.\n\nTo make programming practical recursive functions are needed.\nA central property of the lambda calculus is that recursive definitions\nare not directly available, but can instead be expressed with a fixed point combinator.\nBut unfortunately, the fixpoint combinator cannot be formulated in a typed version\nof the lambda calculus without having a disastrous effect on the system as outlined\nbelow.\n\nThe original paper notes that recursion can be realized by a combinator\nformula_169. A possible recursive definition could thus be formulated as\nformula_170.\n\nAlternatively an extension of the expression syntax and an extra typing rule is possible:\n\nwhere\nbasically merging formula_58 and formula_59 while including the recursively defined\nvariables in monotype positions where they occur to the left of the formula_176 but as polytypes to the right of it. This\nformulation perhaps best summarizes the essence of let-polymorphism.\n\nWhile the above is straightforward it does come at a price.\n\nType theory connects lambda calculus computation and logic.\nThe easy modification above has effects on both:\n\n\nPrograms in simply typed lambda calculus are guaranteed to always terminate. Moreover, they\nare even guaranteed to terminate under any evaluation strategy, be it top down, bottom up, breadth first, whatever. The same is true for expressions that have types in HM. It is well-known that separating\nterminating from non-terminating programs is most difficult, and especially in lambda calculus,\nwhich is so expressive that it can formulate recursion with just a few symbols. Thus the initial\ninability of HM to provide recursive functions was not an omission, but a feature. Adding\nrecursion enables normal programming but the guarantee is not longer valid.\n\nAnother reading of the typing is given by the Curry–Howard isomorphism. Here\nthe types are interpreted as logical expressions. Let's look at the type of the fixpoint combinator from this\nperspective, assuming the variables to have logical value:\nBut this is invalid.\nAdding an invalid axiom will break the logic in the sense that\nevery formula can then be shown to be true in it, e.g. formula_178.\nThus the ability to distinguish even two simple things is no longer given. Everything is the same and\ncollapses into 42. The fixpoint\ncombinator that came in so handy above also plays a role in Curry's paradox.\n\nLogic aside, does this matter for typing programs? It does. Since one is now able to formulate\nnon-terminating functions, one can make a function that would return whatever one wants but never really returns:\nIn practical programming such a function can come in handy when breaking out of a computation,\nlike with codice_5 in C, while silencing the type checker in\nthe current branch by returning essentially nothing but with a suitable type.\n\nLess desirable is that the type checker (type inferencer) now succeeds with a type for a function that in fact never returns any value, like formula_180. The function \"would\" return a value of this type, but it \"cannot\" because no terminating function with this type exists. The type checker's claim that everything is ok thus has to be taken with a grain of salt. The types might only be \"claimed\" to be checked, but the program can still be typed wrong. Only if all functions are terminating does formula_18 in the logic above have a \"true\" value, and the assertions of the type checker become strong again.\n\nOverloading means, that different functions still can be defined and used with the same name. Most programming languages at least provide overloading with the built-in arithmetic operations (+,<,etc.), to allow the programmer to write arithmetic expressions in the same form, even for different numerical types like codice_6 or codice_7. Because a mixture of these different types within the same expression also demands for implicit conversion, overloading especially for these operations is often built into the programming language itself. In some languages, this feature is generalized and made available to the user, e.g. in C++.\n\nWhile ad-hoc overloading has been avoided in functional programming for the computation costs both in type checking and inference, a means to systematise overloading has been introduced that resembles both in form and naming to object oriented programming, but works one level upwards. \"Instances\" in this systematic are not objects (i.e. on value level), but rather types.\nThe quicksort example mentioned in the introduction uses the overloading in the orders, having the following type annotation in Haskell:\n\nHerein, the type codice_8 is not only polymorphic, but also restricted to be an instance of some type class codice_9, that provides the order predicates codice_10 and codice_11 used in the functions body. The proper implementations of these predicates are then passed to quicksorts as additional parameters, as soon as quicksort is used on more concrete types providing a single implementation of the overloaded function quickSort.\n\nBecause the \"classes\" only allow a single type as their argument, the resulting type system can still provide inference. Additionally, the type classes can then be equipped with some kind of overloading order allowing one to arrange the classes as a lattice.\n\nParametric polymorphism implies that types themselves are passed as parameters as if they were proper values. Passed as arguments into a proper functions as in the introduction, but also into \"type functions\" as in the \"parametric\" type constants, leads to the question how to more properly type types themselves. A meta type, the \"type of types\" would be useful to create an even more expressive type system.\n\nThough this would be a straight forward extension, unfortunately, only unification is not longer decidable in the presence of meta types, rendering type inference impossible in this extend of generality.\nAdditionally, assuming a type of all types that includes itself as type leads into a paradox, as in the set of all sets, so one must proceed in steps of levels of abstraction.\nResearch in second order lambda calculus, one step upwards, showed, that type inference is undecidable in this generality.\n\nParts of one extra level has been introduced into Haskell named kind, where it is used helping to type monads. Kinds are left implicit, working behind scene in the inner mechanics of the extended type system.\n\nAttempts to combine subtyping and type inference have caused quite some frustration. While type inference is needed in object-oriented programming for the same reason as in functional languages, methods like HM cannot be made going for this purpose. It is not difficult to setup a type system with subtyping enabling object-oriented style, as\ne.g. Cardelli\n\n\nSuch objects would be immutual in a functional language context, but the type system would enable object-oriented programming style and the type inference method could be reused in imperative languages.\n\nThe subtyping rule for the record types is:\nSyntatically, record expressions would have form\nand have a type rule leading to the above type.\nSuch record values could then be used the same way as objects in object oriented programming.\n\n\n"}
{"id": "15682063", "url": "https://en.wikipedia.org/wiki?curid=15682063", "title": "Homological dimension", "text": "Homological dimension\n\nHomological dimension may refer to the global dimension of a ring. It may also refer to any other concept of dimension that is defined in terms of homological algebra, which includes:\n"}
{"id": "32160914", "url": "https://en.wikipedia.org/wiki?curid=32160914", "title": "Infinity Laplacian", "text": "Infinity Laplacian\n\nIn mathematics, the infinity Laplace (or formula_1-Laplace) operator is a 2nd-order partial differential operator, commonly abbreviated formula_2. It is alternately defined by\n\nor\n\nThe first version avoids the singularity which occurs when the gradient vanishes, while the second version is homogeneous of order zero in the gradient. Verbally, the second version is the second derivative in the direction of the gradient. In the case of the infinity Laplace equation formula_5, the two definitions are equivalent.\n\nWhile the equation involves second derivatives, usually (generalized) solutions are not twice differentiable, as evidenced by the well-known Aronsson solution formula_6. For this reason the correct notion of solutions is that given by the viscosity solutions.\n\nViscosity solutions to the equation formula_5 are also known as infinity harmonic functions. This terminology arises from the fact that the infinity Laplace operator first arose in the study of absolute minimizers for formula_8, and it can be viewed in a certain sense as the limit of the p-Laplacian as formula_9. More recently, viscosity solutions to the infinity Laplace equation have been identified with the payoff functions from randomized tug-of-war games. The game theory point of view has significantly improved the understanding of the partial differential equation itself.\n\nA defining property of the usual formula_10-harmonic functions is the mean value property. That has a natural and important discrete version: a real-valued function formula_11 on a finite or infinite graph formula_12 is discrete harmonic on a subset formula_13 if \nfor all formula_15. Similarly, the vanishing second derivative in the direction of the gradient has a natural discrete version:\n\nIn this equation, we used sup and inf instead of max and min because the graph formula_12 does not have to be locally finite (i.e., to have finite degrees): a key example is when formula_18 is the set of points in a domain in formula_19, and formula_20 if their Euclidean distance is at most formula_21. The importance of this example lies in the following.\n\nConsider a bounded open set formula_22 with smooth boundary formula_23, and a continuous function formula_24. In the formula_10-case, an approximation of the harmonic extension of \"f\" to \"D\" is given by taking a lattice formula_26 with small mesh size formula_21, letting formula_28 and formula_29 be the set of vertices with degree less than \"2d\", taking a natural approximation formula_30, and then taking the unique discrete harmonic extension of formula_31 to \"V\". However, it is easy to see by examples that this does not work for the formula_1-case. Instead, as it turns out, one should take the \"continuum graph\" with all edges of length at most formula_21, mentioned above.\n\nNow, a probabilistic way of looking at the formula_10-harmonic extension of formula_31 from formula_36 to formula_37 is that \nwhere formula_39 is the simple random walk on formula_40 started at formula_41, and formula_42 is the hitting time of formula_36.\n\nFor the formula_1-case, we need game theory. A token is started at location formula_45, and formula_46 is given. There are two players, in each turn they flip a fair coin, and the winner can move the token to any neighbour of the current location. The game ends when the token reaches formula_36 at some time formula_42 and location formula_49, at which point the first player gets the amount formula_50 from the second player. Therefore, the first player wants to maximize formula_50, while the second player wants to minimize it. If both players play optimally (which has a well-defined meaning in game theory), the expected payoff formula_52 to the first player is a discrete infinity harmonic function, as defined above.\n\nThere is a game theory approach to the p-Laplacian, too, interpolating between simple random walk and the above random tug-of-war game.\n\n"}
{"id": "25606481", "url": "https://en.wikipedia.org/wiki?curid=25606481", "title": "International Journal of Computational Geometry and Applications", "text": "International Journal of Computational Geometry and Applications\n\nThe \"International Journal of Computational Geometry and Applications\" (IJCGA) is a bimonthly journal published since 1991, by World Scientific. It covers the application of computational geometry in design and analysis of algorithms, focusing on problems arising in various fields of science and engineering such as computer-aided geometry design (CAGD), operations research, and others.\n\nThe current editors-in-chief are D.-T. Lee of the Institute of Information Science in Taiwan, and Joseph S. B. Mitchell from the Department of Applied Mathematics and Statistics\nin the State University of New York at Stony Brook.\n\n\n"}
{"id": "35256327", "url": "https://en.wikipedia.org/wiki?curid=35256327", "title": "Jacqueline Ferrand", "text": "Jacqueline Ferrand\n\nJacqueline Lelong-Ferrand (17 February 1918, Alès, France – 26 April 2014, Sceaux, France) was a French mathematician who worked on conformal representation theory, potential theory, and Riemannian manifolds. She taught at universities in Caen, Lille, and Paris.\n\nFerrand was born in Alès, the daughter of a classics teacher, and went to secondary school in Nîmes.\nIn 1936 the École Normale Supérieure began admitting women, and she was one of the first to apply and be admitted. In 1939 she and Roger Apéry placed first in the mathematics agrégation; she began teaching at a girls' school in Sèvres, while continuing to do mathematics research under the supervision of Arnaud Denjoy, publishing three papers in 1941 and defending a doctoral thesis in 1942. In 1943 she won the Girbal-Baral Prize of the French Academy of Sciences, and obtained a faculty position at the University of Bordeaux. She moved to the University of Caen in 1945, was given a chair at the University of Lille in 1948, and in 1956 moved to the University of Paris as a full professor. She retired in 1984.\n\nFerrand had nearly 100 mathematical publications, including ten books, and was active in mathematical research into her late 70s. \nOne of her accomplishments, in 1971, was to prove the compactness of the group of conformal mappings of a non-spherical compact Riemannian manifold, resolving a conjecture of André Lichnerowicz, and on the basis of this work she became an invited speaker at the 1974 International Congress of Mathematicians in Vancouver.\n\nShe married mathematician Pierre Lelong in 1947, taking his surname alongside hers in her subsequent publications until their separation in 1977.\n\n"}
{"id": "8102238", "url": "https://en.wikipedia.org/wiki?curid=8102238", "title": "Jedediah Buxton", "text": "Jedediah Buxton\n\nJedediah Buxton (1707–1772) was a noted English mental calculator, born at Elmton, near Creswell, in Derbyshire. He was one of the earliest people referred to as an autistic savant.\n\nBuxton was born in 1707 and although his father was schoolmaster of Elmton, and his grandfather had been the vicar, he could not write; and his knowledge, except of numbers, was extremely limited. How he came to understand the relative proportions of numbers, and their progressive denominations, he did not remember. However this was his interest. He frequently took no notice of objects, and when he did, it was only with reference to their numbers. He measured the lands of Elmton, consisting of some thousand acres (4 km²), simply by striding over it. He gave the area not only in acres, roods and perches, but even in square inches. After this, he reduced them into square hairs'-breadths, reckoning forty-eight to each side of the inch. His memory was so great, that in resolving a question he could leave off and resume the operation again at the same point after the lapse of several months. His perpetual application to figures prevented the acquisition of other knowledge. Among the examples of Buxton's arithmetical feats which are given are his calculation of the product of a farthing doubled 139 times. The result, expressed in pounds, extends to thirty-nine figures, and is correct so far as it can be readily verified by the use of logarithms. Buxton afterwards multiplied this enormous number by itself. It appears that he had invented an original nomenclature for large numbers, a 'tribe' being the cube of a million, and a 'cramp' (if Mr. Holliday's statement can be trusted) a thousand 'tribes of tribes'.\n\nHis mental acuity was tested in 1754 by the Royal Society when he walked to London, who acknowledged their satisfaction by presenting him with a handsome gratuity. During his visit to the metropolis he was taken to see the tragedy of \"Richard III.\" performed at Drury Lane theatre, but his whole mind was given to the counting of the words uttered by David Garrick. Similarly, he set himself to count the steps of the dancers; and he declared that the innumerable sounds produced by the musical instruments had perplexed him beyond measure.\n\nA memoir appeared in the Gentleman's Magazine for June 1754, to which (probably through the medium of a Mr Holliday, of Haughton Hall, Nottinghamshire), Buxton had contributed several letters. In this memoir, his age is given as forty-nine, which points to his birth in 1705; the date adopted above is on the authority of Daniel and Samuel Lysons' \"Magna Britannia\" (Derbyshire).\n\nHis image can be seen online in the New York Library. A portrait by Miss Maria Hartley in 1764 hangs in Elmton Church.\n\nJedediah Buxton was the son of William Buxton, a farmer and also the schoolmaster at Elmton. However, the Vicar of Elmton was not Jedediah's biological grandfather. John Davenport, the Vicar of Elmton, 1689–1709, was the second husband of Ann (William Buxton's mother). She had been previously married to Jedidiah's paternal grandfather, Edward Buxton of Chelmorton.\n\nA blue plaque was erected in Jedediah's honour in Elmton in 2011 after a public poll.\nd\n\n"}
{"id": "53552649", "url": "https://en.wikipedia.org/wiki?curid=53552649", "title": "Juhani Karhumäki", "text": "Juhani Karhumäki\n\nEero Urho Juhani Karhumäki (born 1949) is a Finnish mathematician\nand theoretical computer scientist\nknown for his contributions to automata theory.\nHe is a professor at the University of Turku.\n\nKarhumäki earned his doctorate from the University of Turku in 1976.\nIn 1980–1985, he was a junior researcher of Academy of Finland.\nSince 1986, he has held teaching positions at the University of Turku,\nattaining full professorship in 1998.\nIn 1998–2015,\nKarhumäki was the head of the mathematics department at the University of Turku.\nHe has authored altogether around 200 research papers.\n\nKarhumäki is a member of the Finnish Academy of Science and Letters since 2000\nand of Academia Europaea since 2006.\nA festschrift in his honour was published in 2009\nas a special issue of Theoretical Computer Science.\n\nKarhumäki has been a member of the Lothaire group of mathematicians\nthat developed the foundations of combinatorics of words. In 1991, jointly with Tero Harju, he solved the long-standing equivalence problem for multitape finite automata in automata theory.\nKarhumäki contributed to different areas of formal language theory, such as word equations,\nlanguage equations\nand descriptional complexity of finite automata.\n\n"}
{"id": "2180593", "url": "https://en.wikipedia.org/wiki?curid=2180593", "title": "Kirby–Siebenmann class", "text": "Kirby–Siebenmann class\n\nIn mathematics, the Kirby–Siebenmann class is an element of the fourth cohomology group\nwhich must vanish if a topological manifold \"M\" is to have a piecewise linear structure. It is named for Robion Kirby and Larry Siebenmann.\n\n\n"}
{"id": "4827706", "url": "https://en.wikipedia.org/wiki?curid=4827706", "title": "Knuth Prize", "text": "Knuth Prize\n\nThe Donald E. Knuth Prize is a prize for outstanding contributions to the foundations of computer science, named after Donald E. Knuth.\n\nThe Knuth Prize has been awarded since 1996 and includes an award of $5000. The prize is awarded by ACM SIGACT and by IEEE Computer Society's Technical Committee on the Mathematical Foundations of Computing. Prizes are awarded in alternation at the ACM Symposium on Theory of Computing and at the IEEE Symposium on Foundations of Computer Science, which are among the most prestigious conferences in theoretical computer science.\n\nIn contrast with the Gödel Prize, which recognizes outstanding papers, the Knuth Prize is awarded to individuals for their overall impact in the field.\n\nSince the prize was instituted in 1996, it has been awarded to:\n\n\n"}
{"id": "2557590", "url": "https://en.wikipedia.org/wiki?curid=2557590", "title": "Lagrange's identity", "text": "Lagrange's identity\n\nIn algebra, Lagrange's identity, named after Joseph Louis Lagrange, is:\n\nwhich applies to any two sets {\"a\", \"a\", . . ., \"a\"} and {\"b\", \"b\", . . ., \"b\"} of real or complex numbers (or more generally, elements of a commutative ring). This identity is a generalisation of the Brahmagupta-Fibonacci identity and a special form of the Binet–Cauchy identity.\n\nIn a more compact vector notation, Lagrange's identity is expressed as:\n\nwhere a and b are \"n\"-dimensional vectors with components that are real numbers. The extension to complex numbers requires the interpretation of the dot product as an inner product or Hermitian dot product. Explicitly, for complex numbers, Lagrange's identity can be written in the form:\n\ninvolving the absolute value.\n\nSince the right-hand side of the identity is clearly non-negative, it implies Cauchy's inequality in the finite-dimensional real coordinate space ℝ and its complex counterpart ℂ.\n\nGeometrically, the identity asserts that the square of the volume of the parallelepiped spanned by a set of vectors is the Gram determinant of the vectors.\n\nIn terms of the wedge product, Lagrange's identity can be written\n\nHence, it can be seen as a formula which gives the length of the wedge product of two vectors, which is the area of the parallelogram they define, in terms of the dot products of the two vectors, as\n\nIn three dimensions, Lagrange's identity asserts that if a and b are vectors in ℝ with lengths |a| and |b|, then Lagrange's identity can be written in terms of the cross product and dot product:\n\nUsing the definition of angle based upon the dot product (see also Cauchy–Schwarz inequality), the left-hand side is\nwhere θ is the angle formed by the vectors a and b. The area of a parallelogram with sides |a| and |b| and angle θ is known in elementary geometry to be\nso the left-hand side of Lagrange's identity is the squared area of the parallelogram. The cross product appearing on the right-hand side is defined by\nwhich is a vector whose components are equal in magnitude to the areas of the projections of the parallelogram onto the \"yz\", \"zx\", and \"xy\" planes, respectively.\n\nFor a and b as vectors in ℝ, Lagrange's identity takes on the same form as in the case of ℝ \n\nHowever, the cross product in 7 dimensions does not share all the properties of the cross product in 3 dimensions. For example, the direction of a × b in 7-dimensions may be the same as c × d even though c and d are linearly independent of a and b. Also the seven-dimensional cross product is not compatible with the Jacobi identity.\n\nA quaternion \"p\" is defined as the sum of a scalar \"t\" and a vector v:\n\nThe product of two quaternions and is defined by\n\nThe quaternionic conjugate of \"q\" is defined by\n\nand the norm squared is\n\nThe multiplicativity of the norm in the quaternion algebra provides, for quaternions \"p\" and \"q\":\n\nThe quaternions \"p\" and \"q\" are called imaginary if their scalar part is zero; equivalently, if\n\nLagrange's identity is just the multiplicativity of the norm of imaginary quaternions,\n\nsince, by definition,\n\nThe vector form follows from the Binet-Cauchy identity by setting \"c\" = \"a\" and \"d\" = \"b\". The second version follows by letting \"c\" and \"d\" denote the complex conjugates of \"a\" and \"b\", respectively,\n\nHere is also a direct proof. The expansion of the first term on the left side is:\n\nwhich means that the product of a column of \"a\" and a row of \"b\" yields (a sum of elements of) a square of \"ab\", which can be broken up into a diagonal and a pair of triangles on either side of the diagonal.\n\nThe second term on the left side of Lagrange's identity can be expanded as:\n\nwhich means that a symmetric square can be broken up into its diagonal and a pair of equal triangles on either side of the diagonal.\n\nTo expand the summation on the right side of Lagrange's identity, first expand the square within the summation:\n\nDistribute the summation on the right side,\n\nNow exchange the indices \"i\" and \"j\" of the second term on the right side, and permute the \"b\" factors of the third term, yielding:\n\nBack to the left side of Lagrange's identity: it has two terms, given in expanded form by Equations (') and ('). The first term on the right side of Equation (') ends up canceling out the first term on the right side of Equation ('), yielding\nwhich is the same as Equation (), so Lagrange's identity is indeed an identity, \"Q.E.D.\".\n\nNormed division algebras require that the norm of the product is equal\nto the product of the norms. Lagrange's identity exhibits this equality.\nThe product identity used as a starting point here, is a consequence of the norm of the product equality with the product of the norm for scator algebras. This proposal, originally presented in the context of a deformed Lorentz metric, is based on a transformation stemming from the product operation and magnitude definition in hyperbolic scator algebra.\nLagrange's identity can be proved in a variety of ways.\nMost derivations use the identity as a starting point and prove in one way or another that the equality is true. In the present approach, Lagrange's identity is actually derived without assuming it \"a priori\". An extended version of these results are available in an open source journal.\n\nLet formula_25 be complex numbers and the overbar\nrepresents complex conjugate.\n\nThe product identity formula_26\nreduces to the complex Lagrange's identity when fourth order terms, in a series expansion, are considered.\n\nIn order to prove it, expand the product on the LHS of the product identity in terms of\nseries up to fourth order. To this end, recall that products of the form formula_27 can be expanded\nin terms of sums as\nformula_28\nwhere formula_29 means terms with order three or higher\nin formula_30.\n\nformula_31\n\nThe two factors on the RHS are also written in terms of series\nformula_32\n\nThe product of this expression up to fourth order is \nformula_33\nSubstitution of these two results in the product identity give \nformula_34\n\nThe product of two conjugates series can be expressed as series involving the product of conjugate terms. The conjugate series product is formula_35, thus\n\nformula_36\n\nThe terms of the last two series on the LHS are grouped as \nformula_37\nin order to obtain the complex Lagrange's identity:\n\nformula_38\n\nIn terms of the modulii,\nformula_39\n\nLagrange's identity for complex numbers has been obtained from a straightforward\nproduct identity. A derivation for the reals is obviously even more succinct. Since the Cauchy–Schwarz inequality is a particular case of Lagrange's identity, this\nproof is yet another way to obtain the CS inequality. Higher order terms in the series produce novel identities.\n\n"}
{"id": "1519594", "url": "https://en.wikipedia.org/wiki?curid=1519594", "title": "Lebesgue point", "text": "Lebesgue point\n\nIn mathematics, given a locally Lebesgue integrable function formula_1 on formula_2, a point formula_3 in the domain of formula_1 is a Lebesgue point if\n\nHere, formula_6 is a ball centered at formula_3 with radius formula_8, and formula_9 is its Lebesgue measure. The Lebesgue points of formula_1 are thus points where formula_1 does not oscillate too much, in an average sense.\n\nThe Lebesgue differentiation theorem states that, given any formula_12, almost every formula_3 is a Lebesgue point of formula_1.\n"}
{"id": "3084295", "url": "https://en.wikipedia.org/wiki?curid=3084295", "title": "Linear network coding", "text": "Linear network coding\n\nNetwork coding is a field of research founded in a series of papers from the late 1990s to the early 2000s. However, the concept of network coding, in particular linear network coding, appeared much earlier. In a 1978 paper, a scheme for improving the throughput of a two-way communication through a satellite was proposed. In this scheme, two users trying to communicate with each other transmit their data streams to a satellite, which combines the two streams by summing them modulo 2 and then broadcasts the combined stream. Each of the two users, upon receiving the broadcast stream, can decode the other stream by using the information of their own stream.\n\nThe 2000 paper gave the butterfly network example (discussed below) that illustrates how linear network coding can outperform routing. This example is equivalent to the scheme for satellite communication described above. The same paper gave an optimal coding scheme for a network with one source node and three destination nodes. This is the first example illustrating the optimality of convolutional network coding (a more general form of linear network coding) over a cyclic network.\n\nLinear network coding may be used to improve a network's throughput, efficiency and scalability, as well as resilience to attacks and eavesdropping. Instead of simply relaying the packets of information they receive, the nodes of a network take \"several\" packets and combine them together for transmission. This may be used to attain the maximum possible information flow in a network.\n\nIt has been mathematically proven in theory that linear coding is enough to achieve the upper bound in multicast problems with one source. However linear coding is not sufficient in general (e.g. multisource, multisink with arbitrary demands), even for more general versions of linearity such as convolutional coding and filter-bank coding. Finding optimal coding solutions for general network problems with arbitrary demands remains an open problem.\n\nIn a linear network coding problem, a group of nodes formula_1 are involved in moving the data from formula_2 source nodes to formula_3 sink nodes. Each node generates new packets which are linear combinations of earlier received packets, multiplying them by coefficients chosen from a finite field, typically of size formula_4.\n\nEach node, formula_5 with indegree, formula_6, generates a message formula_7 from the linear combination of received messages formula_8 by the relation:\nwhere the values formula_10 are the coefficients selected from formula_4. Note that, since operations are computed in a finite field, the generated message is of the same length as the original messages. Each node forwards the computed value formula_7 along with the coefficients, formula_10, used in the formula_14 level, formula_10.\n\nSink nodes receive these network coded messages, and collect them in a matrix. The original messages can be recovered by performing Gaussian elimination on the matrix. In reduced row echelon form, decoded packets correspond to the rows of the form formula_16.\n\nA network is represented by a directed graph formula_17. formula_18 is the set of nodes or vertices, formula_19 is the set of directed links (or edges), and formula_20 gives the capacity of each link of formula_19. Let formula_22 be the maximum possible throughput from node formula_23 to node formula_24. By the max-flow min-cut theorem, formula_22 is upper bounded by the minimum capacity of all cuts, which is the sum of the capacities of the edges on a cut, between these two nodes.\n\nKarl Menger proved that there is always a set of edge-disjoint paths achieving the upper bound in a unicast scenario, known as the max-flow min-cut theorem. Later, the Ford–Fulkerson algorithm was proposed to find such paths in polynomial time. Then, Edmonds proved in the paper \"Edge-Disjoint Branchings\" the upper bound in the broadcast scenario is also achievable, and proposed a polynomial time algorithm.\n\nHowever, the situation in the multicast scenario is more complicated, and in fact, such an upper bound can't be reached using traditional routing ideas. Ahlswede, et al. proved that it can be achieved if additional computing tasks (incoming packets are combined into one or several outgoing packets) can be done in the intermediate nodes.\n\nThe butterfly network is often used to illustrate how linear network coding can outperform routing. Two source nodes (at the top of the picture) have information A and B that must be transmitted to the two destination nodes (at the bottom), which each want to know both A and B. Each edge can carry only a single value (we can think of an edge transmitting a bit in each time slot).\n\nIf only routing were allowed, then the central link would be only able to carry A or B, but not both. Suppose we send A through the center; then the left destination would receive A twice and not know B at all. Sending B poses a similar problem for the right destination. We say that routing is insufficient because no routing scheme can transmit both A and B simultaneously to both destinations.\n\nUsing a simple code, as shown, A and B can be transmitted to both destinations simultaneously by sending the sum of the symbols through the center – in other words, we encode A and B using the formula \"A+B\". The left destination receives A and A + B, and can calculate B by subtracting the two values. Similarly, the right destination will receive B and A + B, and will also be able to determine both A and B.\n\nRandom linear network coding is a simple yet powerful encoding scheme, which in broadcast transmission schemes allows close to optimal throughput using a decentralized algorithm. Nodes transmit random linear combinations of the packets they receive, with coefficients chosen from a Galois field. If the field size is sufficiently large, the probability that the receiver(s) will obtain linearly independent combinations (and therefore obtain innovative information) approaches 1. It should however be noted that, although random linear network coding has excellent throughput performance, if a receiver obtains an insufficient number of packets, it is extremely unlikely that they can recover any of the original packets. This can be addressed by sending additional random linear combinations until the receiver obtains the appropriate number of packets.\n\nLinear network coding is still a relatively new subject. Based on previous studies, there are three important open issues in RLNC:\n\nThe broadcast nature of wireless (coupled with network topology) determines the nature of interference. Simultaneous transmissions in a wireless network typically result in all of the packets being lost (i.e., collision, see Multiple Access with Collision Avoidance for Wireless). A wireless network therefore requires a scheduler (as part of the MAC functionality) to minimize such interference. Hence any gains from network coding are strongly impacted by the underlying scheduler and will deviate from the gains seen in wired networks. Further, wireless links are typically half-duplex due to hardware constraints; i.e., a node can not simultaneously transmit and receive due to the lack of sufficient isolation between the two paths.\n\nAlthough, originally network coding was proposed to be used at Network layer (see OSI model), in wireless networks, network coding has been widely used in either MAC layer or PHY layer. It has been shown that network coding when used in wireless mesh networks need attentive design and thoughts to exploit the advantages of packet mixing, else advantages cannot be realized. There are also a variety of factors influencing throughput performance, such as media access layer protocol, congestion control algorithms, etc. It is not evident how network coding can co-exist and not jeopardize what existing congestion and flow control algorithms are doing for our Internet.\n\nSince linear network coding is a relatively new subject, its adoption in industries \nis still pending. Unlike other coding, linear network coding is not entirely applicable\nin a system due to its narrow specific usage scenario. Theorists are trying to connect\nto real world applications. In fact, it was found that BitTorrent approach is far superior than network coding.\n\nIt is envisaged that network coding is useful in the following areas:\n\nSince this area is relatively new and the mathematical treatment of this subject is\ncurrently limited to a handful of people, network coding has yet found its way to\ncommercialization in products and services. It is unclear at this stage if this \nsubject will prevail, or cease as a good mathematical exercise.\n\nResearchers have clearly pointed out that special care is needed to explore how network coding can co-exist with existing routing, media access, congestion, flow control algorithms and TCP protocol. If not, network coding may not offer much advantages and can increase computation complexity and memory requirements.\n\n\n\nAli Farzamnia, Sharifah K. Syed-Yusof, Norsheila Fisa \"Multicasting Multiple Description Coding Using p-Cycle Network Coding\", KSII Transactions on Internet and Information Systems, Vol 7, No 12, 2013.\n\n"}
{"id": "5971815", "url": "https://en.wikipedia.org/wiki?curid=5971815", "title": "List of mathematicians (K)", "text": "List of mathematicians (K)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "51291103", "url": "https://en.wikipedia.org/wiki?curid=51291103", "title": "Mivar-based approach", "text": "Mivar-based approach\n\nThe Mivar-based approach is a mathematical tool for designing artificial intelligence (AI) systems. Mivar (\"Multidimensional Informational Variable Adaptive Reality\") was developed by combining production and Petri nets. The Mivar-based approach was developed for semantic analysis and adequate representation of humanitarian epistemological and axiological principles in the process of developing artificial intelligence. The Mivar-based approach incorporates computer science, informatics and discrete mathematics, databases, expert systems, graph theory, matrices and inference systems. The Mivar-based approach involves two technologies:\nMivar networks allow us to develop cause-effect dependencies (“If-then”) and create an automated, trained, logical reasoning system.\n\nRepresentatives of Russian association for artificial intelligence (RAAI) – for example, V. I. Gorodecki, doctor of technical science, professor at SPIIRAS and V. N. Vagin, doctor of technical science, professor at MPEI declared that the term is incorrect and suggested that the author should use standard terminology.\n\nWhile working in the Russian Ministry of Defense, O. O. Varlamov started developing the theory of “rapid logical inference” in 1985. He was analyzing Petri nets and productions to construct algorithms. Generally, mivar-based theory represents an attempt to combine entity-relationship models and their problem instance – semantic networks and Petri networks.\n\nThe abbreviation MIVAR was introduced as a technical term by O. O. Varlamov, Doctor of Technical Science, professor at Bauman MSTU in 1993 to designate a “semantic unit” in the process of mathematical modeling. The term has been established and used in all of his further works.\n\nThe first experimental systems operating according to mivar-based principles were developed in 2000. Applied mivar systems were introduced in 2015.\n\nMivar is the smallest structural element of discrete information space.\n\nObject-Property-Relation (VSO) is a graph, the nodes of which are concepts and arcs are connections between concepts.\n\nMivar space represents a set of axes, a set of elements, a set of points of space and a set of values of points.\n\nformula_1\n\nwhere:\n\n\nThen: <math>\\forall a_n \\exists F_n =\\{ f_\n\n"}
{"id": "167394", "url": "https://en.wikipedia.org/wiki?curid=167394", "title": "Nicolas Bourbaki", "text": "Nicolas Bourbaki\n\nNicolas Bourbaki was the collective pseudonym of a group of (mainly French) mathematicians. Their aim was to reformulate mathematics on an extremely abstract and formal but self-contained basis in a series of books beginning in 1935. With the goal of grounding all of mathematics on set theory, the group strove for rigour and generality. Their work led to the discovery of several concepts and terminologies still used, and influenced modern branches of mathematics.\n\nWhile there is no one person named Nicolas Bourbaki, the Bourbaki group, officially known as the \"Association des collaborateurs de Nicolas Bourbaki\" (Association of Collaborators of Nicolas Bourbaki), has an office at the École Normale Supérieure in Paris.\n\nIn 1934, young French mathematicians from various French universities felt the need to form a group to jointly produce textbooks that they could all use for teaching. André Weil organized the first meeting on 10 December 1934 in the basement of a Parisian grill room, while all participants were attending a conference in Paris.\n\nAccounts of the early days vary, but original documents have now come to light. The founding members were all connected to the École Normale Supérieure in Paris and included Henri Cartan, Claude Chevalley, Jean Coulomb, Jean Delsarte, Jean Dieudonné, Charles Ehresmann, René de Possel, Szolem Mandelbrojt and André Weil. There was a preliminary meeting, towards the end of 1934. Jean Leray and Paul Dubreil were present at the preliminary meeting but dropped out before the group actually formed. Other notable participants in later days were Hyman Bass, Laurent Schwartz, Jean-Pierre Serre, Alexander Grothendieck, Jean-Louis Koszul, Samuel Eilenberg, Serge Lang and Roger Godement.\n\nThe original goal of the group had been to compile an improved mathematical analysis text; it was soon decided that a more comprehensive treatment of all of mathematics was necessary. There was no official status of membership, and at the time the group was quite secretive and also fond of supplying disinformation. Regular meetings were scheduled (totalling about 4 weeks a year), during which the group would discuss vigorously every proposed line of every book. Members had to resign by age 50, which allegedly resulted in a complete change of personnel by 1958. However, historian Liliane Beaulieu was quoted as never having found written affirmation of this rule.\n\nThe atmosphere in the group can be illustrated by an anecdote told by Laurent Schwartz. Dieudonné regularly and spectacularly threatened to resign unless topics were treated in their logical order, and after a while others played on this for a joke. Godement's wife wanted to see Dieudonné announcing his resignation, and so on one occasion while she was there Schwartz deliberately brought up again the question of permuting the order in which measure theory and topological vector spaces were to be handled, to precipitate a guaranteed crisis.\n\nThe name \"Bourbaki\" refers to a French general, Charles Denis Bourbaki; it was adopted by the group as a reference to a student anecdote about a hoax mathematical lecture, and also possibly to a statue. It is said that Weil's wife Evelyne supplied \"Nicolas\". This is more or less confirmed by Robert Mainard.\n\nThe Bourbaki group released a few humorous hoaxes related to the fake life of Nicolas Bourbaki. For example, the group released a wedding announcement, relating the marriage of Betti Bourbaki (daughter of Nicolas) with a certain \"Hector Pétard\" (Hector Firecrackers in English). In November 1968, a mock obituary of Nicolas Bourbaki was released during one of the seminars, containing a few mathematical puns. The group is however still active as of 2018, organizing seminars and having released a book in 2016.\n\nBourbaki's main work is the \"Elements of Mathematics (Éléments de mathématique)\" series. This series aims to be a completely self-contained treatment of the core areas of modern mathematics. Assuming no special knowledge of mathematics, it takes up mathematics from the very beginning, proceeds axiomatically and gives complete proofs.\n\nThe dates indicated below are for the first edition of the first chapter of each book. Most of the books were reedited several times (with significant changes between editions), and the books were released in several parts containing different chapters (e.g. Book II, \"Algebra\", was released in five parts, the first in 1942 with chapters 1-2-3 and the last in 1980 containing chapter 10).\n\nThe book \"Variétés différentielles et analytiques\" was a \"fascicule de résultats\", that is, a summary of results, on the theory of manifolds, rather than a worked-out exposition. The (still incomplete) volume on spectral theory (\"Théories spectrales\") from 1967 was for almost four decades the last new book to be added to the series. After that several new chapters to existing books as well as revised editions of existing chapters appeared until the publication of chapters 8-9 of Commutative Algebra in 1983. A long break in publishing activity followed, leading many to suspect the end of the publishing project. However, chapter 10 of Commutative Algebra appeared in 1998, and after another long break a completely re-written and expanded chapter 8 of Algèbre was published in 2012. More importantly, the first four chapters of a completely new book on algebraic topology were published in 2016. The new material from 2012 and 2014 address some references to forthcoming books in the book on Lie Groups and Algebras; there remain other such references (some very precise) to expected additional chapters of the book spectral theory.\n\nBesides the \"Éléments de mathématique\" series, lectures from the Séminaire Bourbaki also have been periodically published in monograph form since 1948.\n\nNotations introduced by Bourbaki include the symbol formula_1 for the empty set and a dangerous bend symbol ☡, and the terms \"injective\", \"surjective\", and \"bijective\".\n\nThe emphasis on rigour may be seen as a reaction to the work of Henri Poincaré, who stressed the importance of free-flowing mathematical intuition, at a cost of completeness in presentation. The impact of Bourbaki's work initially was great on many active research mathematicians worldwide. For example:\nIt provoked some hostility, too, mostly on the side of classical analysts; they approved of rigour but not of high abstraction. Around 1950, also, some parts of geometry were still not fully axiomatic — in less prominent developments, one way or another, these were brought into line with the new foundational standards, or quietly dropped. This led to a gulf with the way theoretical physics was practiced.\n\nBourbaki's direct influence has decreased over time. This is partly because certain concepts which are now important, such as the machinery of category theory, are not covered in the treatise. The completely uniform and essentially linear referential structure of the books became difficult to apply to areas closer to current research than the already mature ones treated in the published books, and thus publishing activity diminished significantly from the 1970s. It also mattered that, while especially algebraic structures can be naturally defined in Bourbaki's terms, there are areas where the Bourbaki approach was less straightforward to apply.\n\nOn the other hand, the approach and rigour advocated by Bourbaki have permeated the current mathematical practices to such extent that the task undertaken was completed. This is particularly true for the less applied parts of mathematics.\n\nThe Bourbaki seminar series founded in post-WWII Paris continues; it has been going on since 1948, and contains more than 1000 items. It is an important source of survey articles, with sketches (or sometimes improvements) of proofs. The topics range through all branches of mathematics, including sometimes theoretical physics. The idea is that the presentation should be on the level of specialists, but should be tailored to an audience which is \"not\" specialized in the particular field.\n\nThe underlying drive, in Weil and Chevalley at least, was the perceived need for French mathematics to absorb the best ideas of the Göttingen school, particularly Hilbert and the modern algebra school of Noether, Artin and van der Waerden. It is fairly clear that the Bourbaki point of view, while \"encyclopedic\", was never intended as \"neutral\". Quite the opposite: it was more a question of trying to make a consistent whole out of some enthusiasms, for example for Hilbert's legacy, with emphasis on formalism and axiomatics. But always through a transforming process of reception and selection—their ability to sustain this collective, critical approach has been described as \"something unusual\".\n\nThe following is a list of some of the criticisms commonly made of the Bourbaki approach. Pierre Cartier, a Bourbaki member between 1955 and 1983, said that:\n\nIn addition, algorithms are considered off-topic and almost completely omitted. Analysis is treated 'softly', without 'hard' estimates. Measure theory is developed from a functional analytic perspective. Taking the case of locally compact measure spaces as fundamental focuses the presentation on Radon measures and leads to an approach to measurable functions that is cumbersome, especially from the viewpoint of probability theory. However, the last chapter of the book addresses limitations, especially for use in probability theory, of the restriction to locally compact spaces. Logic is treated minimally.\n\nFurthermore, Bourbaki makes only limited use of pictures in their presentation. Pierre Cartier is quoted as later saying: \"The Bourbaki were Puritans, and Puritans are strongly opposed to pictorial representations of truths of their faith.\" In general, Bourbaki has been criticized for reducing geometry as a whole to abstract algebra and soft analysis.\n\nWhile several of Bourbaki's books have become standard references in their fields, some have felt that the austere presentation makes them unsuitable as textbooks. The books' influence may have been at its strongest when few other graduate-level texts in current pure mathematics were available, between 1950 and 1960.\n\nIn the longer term, the manifesto of Bourbaki has had a definite and deep influence. In secondary education the new math movement corresponded to teachers influenced by Bourbaki. In France the change was secured by the Lichnerowicz Commission.\n\nPublic discussion of, and justification for, Bourbaki's thoughts has in general been through Jean Dieudonné (who initially was the 'scribe' of the group) writing under his own name. In a survey of \"le choix bourbachique\" written in 1977, he did not shy away from a hierarchical development of the 'important' mathematics of the time.\n\nHe also wrote extensively under his own name: nine volumes on analysis, perhaps in belated fulfillment of the original project or pretext; and also on other topics mostly connected with algebraic geometry. While Dieudonné could reasonably speak on Bourbaki's encyclopedic tendency and tradition, it may be doubted—after innumerable frank \"tais-toi, Dieudonné!\" (\"Hush, Dieudonné!\") remarks at the meetings—whether all others agreed with him about mathematical writing and research. In particular Serre has often championed greater attention to problem-solving, within number theory especially, not an area treated in the main Bourbaki texts.\n\nDieudonné stated the view that most workers in mathematics were doing ground-clearing work, in order that a future Riemann could find the way ahead intuitively open. He pointed to the way the axiomatic method can be used as a tool for problem-solving, for example by Alexander Grothendieck. Others found him too close to Grothendieck to be an unbiased observer. Comments in Pál Turán's 1970 speech on the award of a Fields Medal to Alan Baker about theory-building and problem-solving were a reply from the traditionalist camp at the next opportunity, Grothendieck having received the previous Fields Medal \"in absentia\" in 1966.\n\n\n\n\n"}
{"id": "48643701", "url": "https://en.wikipedia.org/wiki?curid=48643701", "title": "Out-of-bag error", "text": "Out-of-bag error\n\nOut-of-bag (OOB) error, also called out-of-bag estimate, is a method of measuring the prediction error of random forests, boosted decision trees, and other machine learning models utilizing bootstrap aggregating (bagging) to sub-sample data samples used for training. OOB is the mean prediction error on each training sample , using only the trees that did not have in their bootstrap sample.\n\nSubsampling allows one to define an out-of-bag estimate of the prediction performance improvement by evaluating predictions on those observations which were not used in the building of the next base learner. Out-of-bag estimates help avoid the need for an independent validation dataset, but often underestimates actual performance improvement and the optimal number of iterations.\n\n"}
{"id": "182837", "url": "https://en.wikipedia.org/wiki?curid=182837", "title": "Pattern language", "text": "Pattern language\n\nA pattern language is a method of describing good design practices or patterns of useful organization within a field of expertise. The term was coined by architect Christopher Alexander and popularized by his 1977 book \"A Pattern Language\".\n\nA pattern language can also be an attempt to express the deeper wisdom of what brings aliveness within a particular field of human endeavor, through a set of interconnected patterns. Aliveness is one placeholder term for \"the quality that has no name\": a sense of wholeness, spirit, or grace, that while of varying form, is precise and empirically verifiable. Some advocates of this design approach claim that ordinary people can use it to successfully solve very large, complex design problems.\n\nWhen a designer designs something – whether a house, computer program, or lamp – they must make many decisions about how to solve problems. A single problem is documented with its typical place (the syntax), and use (the grammar) with the most common and recognized good solution seen in the wild, like the examples seen in dictionaries. Each such entry is a single design pattern. Each pattern has a name, a descriptive entry, and some cross-references, much like a dictionary entry. A documented pattern should explain why that solution is good in the pattern's contexts.\n\nElemental or universal \"patterns\" such as \"door\" or \"partnership\" are versatile ideals of design, either as found in experience or for use as components in practice, explicitly described as holistic resolutions of the forces in recurrent contexts and circumstances, whether in architecture, medicine, software development or governance, etc. Patterns might be invented or found and studied, such as the naturally occurring patterns of design that characterize human environments.\n\nLike all languages, a pattern language has vocabulary, syntax, and grammar – but a pattern language applies to some complex activity other than communication. In pattern languages for design, the parts break down in this way:\n\nThis simplifies the design work because designers can start the process from any part of the problem they understand and work toward the unknown parts. At the same time, if the pattern language has worked well for many projects, there is reason to believe that even a designer who does not completely understand the design problem at first will complete the design process, and the result will be usable. For example, skiers coming inside must shed snow and store equipment. The messy snow and boot cleaners should stay outside. The equipment needs care, so the racks should be inside.\n\nJust as words must have grammatical and semantic relationships to each other in order to make a spoken language useful, design patterns must be related to each other in position and utility order to form a pattern language. Christopher Alexander's work describes a process of decomposition, in which the designer has a problem (perhaps a commercial assignment), selects a solution, then discovers new, smaller problems resulting from the larger solution. Occasionally, the smaller problems have no solution, and a different larger solution must be selected. Eventually all of the remaining design problems are small enough or routine enough to be solved by improvisation by the builders, and the \"design\" is done.\n\nThe actual organizational structure (hierarchical, iterative, etc.) is left to the discretion of the designer, depending on the problem. This explicitly lets a designer explore a design, starting from some small part. When this happens, it's common for a designer to realize that the problem is actually part of a larger solution. At this point, the design almost always becomes a better design.\n\nIn the language, therefore, each pattern has to indicate its relationships to other patterns and to the language as a whole. This gives the designer using the language a great deal of guidance about the related problems that must be solved.\n\nThe most difficult part of having an outside expert apply a pattern language is in fact to get a reliable, complete list of the problems to be solved. Of course, the people most familiar with the problems are the people that need a design. So, Alexander famously advocated on-site improvisation by concerned, empowered users, as a powerful way to form very workable large-scale initial solutions, maximizing the utility of a design, and minimizing the design rework. The desire to empower users of architecture was, in fact, what led Alexander to undertake a pattern language project for architecture in the first place.\n\nAn important aspect of design patterns is to identify and document the key ideas that make a good system different from a poor system (that may be a house, a computer program or an object of daily use), and to assist in the design of future systems. The idea expressed in a pattern should be general enough to be applied in very different systems within its context, but still specific enough to give constructive guidance.\n\nThe range of situations in which the problems and solutions addressed in a pattern apply is called its context. An important part in each pattern is to describe this context. Examples can further illustrate how the pattern applies to very different situation.\n\nFor instance, Alexander's pattern \"A PLACE TO WAIT\" addresses bus stops in the same way as waiting rooms in a surgery, while still proposing helpful and constructive solutions. The \"Gang-of-Four\" book \"Design Patterns\" by Gamma et al. proposes solutions that are independent of the programming language, and the program's application domain.\n\nStill, the problems and solutions described in a pattern can vary in their level of abstraction and generality on the one side, and specificity on the other side. In the end this depends on the author's preferences. However, even a very abstract pattern will usually contain examples that are, by nature, absolutely concrete and specific.\n\nPatterns can also vary in how far they are proven in the real world. Alexander gives each pattern a rating by zero, one or two stars, indicating how well they are proven in real-world examples. It is generally claimed that all patterns need at least some existing real-world examples. It is, however, conceivable to document yet unimplemented ideas in a pattern-like format.\n\nThe patterns in Alexander's book also vary in their level of scale – some describing how to build a town or neighbourhood, others dealing with individual buildings and the interior of rooms. Alexander sees the low-scale artifacts as constructive elements of the large-scale world, so they can be connected to a hierarchic network.\n\nA pattern must characterize the problems that it is meant to solve, the context or situation where these problems arise, and the conditions under which the proposed solutions can be recommended.\n\nOften these problems arise from a conflict of different interests or \"forces\". A pattern emerges as a dialogue that will then help to balance the forces and finally make a decision.\n\nFor instance, there could be a pattern suggesting a wireless telephone. The forces would be the need to communicate, and the need to get other things done at the same time (cooking, inspecting the bookshelf). A very specific pattern would be just \"WIRELESS TELEPHONE\". More general patterns would be \"WIRELESS DEVICE\" or \"SECONDARY ACTIVITY\", suggesting that a secondary activity (such as talking on the phone, or inspecting the pockets of your jeans) should not interfere with other activities.\n\nThough quite unspecific in its context, the forces in the \"SECONDARY ACTIVITY\" pattern are very similar to those in \"WIRELESS TELEPHONE\". Thus, the competing forces can be seen as part of the essence of a design concept expressed in a pattern.\n\nUsually a pattern contains a rationale referring to some given values. For Christopher Alexander, it is most important to think about the people who will come in contact with a piece of architecture. One of his key values is making these people feel more alive. He talks about the \"quality without a name\" (QWAN).\n\nMore generally, we could say that a good system should be accepted, welcomed and happily embraced as an enrichment of daily life by those who are meant to use it, or – even better – by all people it affects. For instance, when discussing a street café, Alexander discusses the possible desires of a guest, but also mentions people who just walk by.\n\nThe same thinking can be applied to technical devices such as telephones and cars, to social structures like a team working on a project, or to the user interface of a computer program. The qualities of a software system, for instance, could be rated by observing whether users spend their time enjoying or struggling with the system.\n\nBy focusing on the impacts on human life, we can identify patterns that are independent from changing technology, and thus find \"timeless quality\" (Alexander).\n\nUsually the author of a pattern language or collection chooses a generic structure for all the patterns it contains, breaking each into generic sections like context, problem statement, solution etc.\n\nChristopher Alexander's patterns, for instance, each consist of a short name, a rating (up to two '*' symbols), a sensitizing picture, the context description, the problem statement, a longer part of text with examples and explanations, a solution statement, a sketch and further references. This structure and layout is sometimes referred to as the \"Alexandrian form\".\n\nAlexander uses a special text layout to mark the different sections of his patterns. For instance, the problem statement and the solution statement are printed in bold font, the latter is always preceded by the \"Therefore:\" keyword. Some authors instead use explicit labels, which creates some degree of redundancy.\n\nWhen design is done by a team, pattern names will form a vocabulary they can share. This makes it necessary for pattern names to be easy to remember and highly descriptive. Some examples from Alexander's works are WINDOW PLACE (helps define where windows should go in a room) and A PLACE TO WAIT (helps define the characteristics of bus stops and hospital waiting rooms, for example).\n\nA pattern language, as conceived by Alexander, contains links from one pattern to another, so when trying to apply one pattern in a project, a designer is pushed to other patterns that are considered helpful in its context.\n\nIn Alexander's book, such links are collected in the \"references\" part, and echoed in the linked pattern's \"context\" part – thus the overall structure is a directed graph. A pattern that is linked to in the \"references\" usually addresses a problem of lower scale, that is suggested as a part of the higher-scale problem. For instance, the \"PUBLIC OUTDOOR ROOM\" pattern has a reference to \"STAIR SEATS\".\n\nEven without the pattern description, these links, along with meaningful names, carry a message: When building a place outside where people can spend time (\"PUBLIC OUTDOOR ROOM\"), consider to surround it by stairs where people can sit (\"STAIR SEATS\"). If you are planning an office (\"WORKSHOPS AND OFFICES\"), consider to arrange workspaces in small groups (\"SMALL WORKING GROUPS\"). Alexander argues that the connections in the network can be considered even more meaningful than the text of the patterns themselves.\n\nThe links in Alexander's book clearly result in a hierarchic network. Alexander draws a parallel to the hierarchy of a grammar – that is one argument for him to speak of a pattern \"language\".\n\nThe idea of linking is generally accepted among pattern authors, though the semantic rationale behind the links may vary. Some authors, however, like Gamma et al. in \"Design Patterns\", make only little use of pattern linking – possibly because it did not make that much sense for their collection of patterns. In such a case we would speak of a \"pattern catalogue\" rather than a \"pattern language\".\n\nAlexander encouraged people who used his system to expand his language with patterns of their own. In order to enable this, his books do not focus strictly on architecture or civil engineering; he also explains the general method of pattern languages. The original concept for the book \"A Pattern Language\" was that it would be published in the form of a 3-ring binder, so that pages could easily be added later; this proved impractical in publishing. The pattern language approach has been used to document expertise in diverse fields. Some examples are architectural patterns, computer science patterns, interaction design patterns, pedagogical patterns, social action patterns, and group facilitation patterns. The pattern language approach has also been recommended as a way to promote civic intelligence by helping to coordinate actions for diverse people and communities who are working together on significant shared problems. Alexander's specifications for using pattern languages as well as creating new ones remain influential, and his books are referenced for style by experts in unrelated fields.\n\nIt is important to note that notations such as UML or the flowchart symbol collection are not pattern languages. They could more closely be compared to an alphabet: their symbols could be used to document a pattern language, but they are not a language by themselves. A recipe or other sequential set of steps to be followed, with only one correct path from start to finish, is also not a pattern language. However, the process of designing a new recipe might benefit from the use of a pattern language.\n\n\nChristopher Alexander, an architect and author, coined the term pattern language. He used it to refer to common problems of the design and construction of buildings and towns and how they should be solved. The solutions proposed in the book include suggestions ranging from how cities and towns should be structured to where windows should be placed in a room.\n\nThe framework and philosophy of the \"pattern language\" approach was initially popularized in the book \"A Pattern Language\" that was written by Christopher Alexander and five colleagues at the Center for Environmental Structure in Berkeley, California in the late 1970s. While \"A Pattern Language\" contains 253 \"patterns\" from the first pattern, \"Independent Regions\" (the most general) to the last, \"Things from Your Life\", Alexander's book \"The Timeless Way of Building\" goes into more depth about the motivation and purpose of the work. The following definitions of \"pattern\" and \"pattern language\" are paraphrased from \"A Pattern Language\":\n\n\"A \"pattern\" is a careful description of a perennial solution to a recurring problem within a building context, describing one of the configurations that brings life to a building. Each pattern describes a problem that occurs over and over again in our environment, and then describes the core solution to that problem, in such a way that you can use the solution a million times over, without ever doing it the same way twice.\"\n\nA \"pattern language\" is a network of patterns that call upon one another. Patterns help us remember insights and knowledge about design and can be used in combination to create solutions.\n\nChristopher Alexander's idea has been adopted in other disciplines, often much more heavily than the original application of patterns to architecture as depicted in the book \"A Pattern Language\". Recent examples include software design patterns in software engineering and, more generally, architectural patterns in computer science, as well as interaction design patterns. Pedagogical patterns are used to document good practices in teaching. The book \"Liberating Voices: A Pattern Language for Communication Revolution\", containing 136 patterns for using information and communication to promote sustainability, democracy and positive social change, was published in 2008 along with a website containing even more patterns. The deck \"Group Works: A Pattern Language for Bringing Life to Meetings and Other Gatherings\" was published in 2011. Recently, patterns were also introduced into systems architecture design. Chess strategy and tactics involve many patterns from opening to checkmate.\n\nWard Cunningham, the inventor of wiki, coauthored a paper with Michael Mehaffy arguing that there are deep relationships between wikis and pattern languages, and that wikis \"were in fact developed as tools to facilitate efficient sharing and modifying of patterns\".\n\n\n\n\n"}
{"id": "1267288", "url": "https://en.wikipedia.org/wiki?curid=1267288", "title": "Poincaré–Hopf theorem", "text": "Poincaré–Hopf theorem\n\nIn mathematics, the Poincaré–Hopf theorem (also known as the Poincaré–Hopf index formula, Poincaré–Hopf index theorem, or Hopf index theorem) is an important theorem that is used in differential topology. It is named after Henri Poincaré and Heinz Hopf.\n\nThe Poincaré–Hopf theorem is often \nillustrated by the special case of the hairy ball theorem, which simply states that there is no smooth vector field on a sphere having no sources or sinks.\n\nLet \"M\" be a differentiable manifold, of dimension \"n\", and \"v\" a vector field on \"M\". Suppose that \"x\" is an isolated zero of \"v\", and fix some local coordinates near \"x\". Pick a closed ball \"D\" centered at \"x\", so that \"x\" is the only zero of \"v\" in \"D\". Then we define the index of \"v\" at \"x\", index(\"v\"), to be the degree of the map \"u\":∂\"D\"→\"S\" from the boundary of \"D\" to the (\"n\"-1)-sphere given by \"u\"(\"z\")=\"v\"(\"z\")/| \"v\"(\"z\") |.\n\nTheorem. Let \"M\" be a compact differentiable manifold. Let \"v\" be a vector field on \"M\" with isolated zeroes. If \"M\" has boundary, then we insist that \"v\" be pointing in the outward normal direction along the boundary. Then we have the formula\n\nwhere the sum of the indices is over all the isolated zeroes of \"v\" and formula_2 is the Euler characteristic of \"M\". A particularly useful corollary is when there is a non-vanishing vector field implying Euler characteristic 0.\n\nThe theorem was proven for two dimensions by Henri Poincaré and later generalized to higher dimensions by Heinz Hopf.\n\nThe Euler characteristic of a closed surface is a purely topological concept, whereas the index of a vector field is purely analytic. Thus, this theorem establishes a deep link between two seemingly unrelated areas of mathematics. It is perhaps as interesting that the proof of this theorem relies heavily on integration, and, in particular, Stokes' theorem, which states that the integral of the exterior derivative of a differential form is equal to the integral of that form over the boundary. In the special case of a manifold without boundary, this amounts to saying that the integral is 0. But by examining vector fields in a sufficiently small neighborhood of a source or sink, we see that sources and sinks contribute integer amounts (known as the index) to the total, and they must all sum to 0. This result may be considered one of the earliest of a whole series of theorems establishing deep relationships between geometric and analytical or physical concepts. They play an important role in the modern study of both fields.\n\n1. Embed \"M\" in some high-dimensional Euclidean space. (Use the Whitney embedding theorem.)\n\n2. Take a small neighborhood of \"M\" in that Euclidean space, \"N\". Extend the vector field to this neighborhood so that it still has the same zeroes and the zeroes have the same indices. In addition, make sure that the extended vector field at the boundary of \"N\" is directed outwards.\n\n3. The sum of indices of the zeroes of the old (and new) vector field is equal to the degree of the Gauss map from the boundary of \"N\" to the sphere. Thus, the sum of the indices is independent of the actual vector field, and depends only on the manifold \"M\".\nTechnique: cut away all zeroes of the vector field with small neighborhoods. Then use the fact that the degree of a map from the boundary of an n-dimensional manifold to an sphere, that can be extended to the whole n-dimensional manifold, is zero.\n\n4. Finally, identify this sum of indices as the Euler characteristic of \"M\". To do that, construct a very specific vector field on \"M\" using a triangulation of \"M\" for which it is clear that the sum of indices is equal to the Euler characteristic.\n\nIt is still possible to define the index for a vector field with nonisolated zeroes. A construction of this index and the extension of Poincaré–Hopf theorem for vector fields with nonisolated zeroes is outlined in Section 1.1.2 of .\n\n\n"}
{"id": "14076693", "url": "https://en.wikipedia.org/wiki?curid=14076693", "title": "Relative dimension", "text": "Relative dimension\n\nIn mathematics, specifically linear algebra and geometry, relative dimension is the dual notion to codimension.\n\nIn linear algebra, given a quotient map formula_1, the difference dim \"V\" − dim \"Q\" is the relative dimension; this equals the dimension of the kernel.\n\nIn fiber bundles, the relative dimension of the map is the dimension of the fiber.\n\nMore abstractly, the codimension of a map is the dimension of the cokernel, while the relative dimension of a map is the dimension of the kernel.\n\nThese are dual in that the inclusion of a subspace formula_2 of codimension \"k\" dualizes to yield a quotient map formula_3 of relative dimension \"k\", and conversely.\n\nThe additivity of codimension under intersection corresponds to the additivity of relative dimension in a fiber product.\n\nJust as codimension is mostly used for injective maps, relative dimension is mostly used for surjective maps.\n"}
{"id": "2067581", "url": "https://en.wikipedia.org/wiki?curid=2067581", "title": "Routh–Hurwitz theorem", "text": "Routh–Hurwitz theorem\n\nIn mathematics, the Routh–Hurwitz theorem gives a test to determine whether all roots of a given polynomial lie in the left half-plane. Polynomials with this property are called Hurwitz-stable. The Routh–Hurwitz theorem was proved in 1895, and it was named after Edward John Routh and Adolf Hurwitz.\n\nLet \"f\"(\"z\") be a polynomial (with complex coefficients) of degree \"n\" with no roots on the imaginary line (i.e. the line \"Z\" = \"ic\" where \"i\" is the imaginary unit and \"c\" is a real number). Let us define formula_1 (a polynomial of degree \"n\") and formula_2 (a nonzero polynomial of degree strictly less than \"n\") by formula_3, respectively the real and imaginary parts of \"f\" on the imaginary line.\n\nFurthermore, let us denote by:\n\nWith the notations introduced above, the Routh–Hurwitz theorem states that:\n\nFrom the first equality we can for instance conclude that when the variation of the argument of \"f\"(\"iy\") is positive, then \"f\"(\"z\") will have more roots to the left of the imaginary axis than to its right.\nThe equality \"p\" − \"q\" = \"w\"(+∞) − \"w\"(−∞) can be viewed as the complex counterpart of Sturm's theorem. Note the differences: in Sturm's theorem, the left member is \"p\" + \"q\" and the \"w\" from the right member is the number of variations of a Sturm chain (while \"w\" refers to a generalized Sturm chain in the present theorem).\n\nWe can easily determine a stability criterion using this theorem as it is trivial that \"f\"(\"z\") is Hurwitz-stable iff \"p\" − \"q\" = \"n\". We thus obtain conditions on the coefficients of \"f\"(\"z\") by imposing \"w\"(+∞) = \"n\" and \"w\"(−∞) = 0.\n\n\n"}
{"id": "2451294", "url": "https://en.wikipedia.org/wiki?curid=2451294", "title": "Simple set", "text": "Simple set\n\nIn recursion theory a subset of the natural numbers is called a simple set if it is co-infinite and recursively enumerable, but every infinite subset of its complement fails to be enumerated recursively. Simple sets are examples of recursively enumerable sets that are not recursive.\n\nSimple sets were devised by Emil Leon Post in the search for a non-Turing-complete recursively enumerable set. Whether such sets exist is known as Post's problem. Post had to prove two things in order to obtain his result, one is that the simple set, say \"A\", does not Turing-reduce to the empty set, and that the \"K\", the halting problem, does not Turing-reduce to \"A\". He succeeded in the first part (which is obvious by definition), but for the other part, he managed only to prove a many-one reduction.\n\nIt was affirmed by Friedberg and Muchnik in the 1950s using a novel technique called the priority method. They give a construction for a set that is simple (and thus non-recursive), but fails to compute the halting problem.\n\n\n"}
{"id": "21878925", "url": "https://en.wikipedia.org/wiki?curid=21878925", "title": "Stiffness matrix", "text": "Stiffness matrix\n\nIn the finite element method for the numerical solution of elliptic partial differential equations, the stiffness matrix represents the system of linear equations that must be solved in order to ascertain an approximate solution to the differential equation.\n\nFor simplicity, we will first consider the Poisson problem\n\non some domain Ω, subject to the boundary condition \"u\" = 0 on the boundary of Ω. To discretize this equation by the finite element method, one chooses a set of \"basis functions\" {\"φ\", ..., \"φ\"} defined on Ω which also vanish on the boundary. One then approximates\n\nThe coefficients \"u\", ..., \"u\" are determined so that the error in the approximation is orthogonal to each basis function \"φ\":\n\nThe stiffness matrix is the n-element square matrix A defined by\n\nBy defining the vector \"F\" with components \"F\" = formula_5, the coefficients \"u\" are determined by the linear system \"AU\" = \"F\". The stiffness matrix is symmetric, i.e. \"A\" = \"A\", so all its eigenvalues are real. Moreover, it is a strictly positive-definite matrix, so that the system \"AU\" = \"F\" always has a unique solution. (For other problems, these nice properties will be lost.)\n\nNote that the stiffness matrix will be different depending on the computational grid used for the domain and what type of finite element is used. For example, the stiffness matrix when piecewise quadratic finite elements are used will have more degrees of freedom than piecewise linear elements.\n\nDetermining the stiffness matrix for other PDE follows essentially the same procedure, but it can be complicated by the choice of boundary conditions. As a more complex example, consider the elliptic equation\n\nwhere \"A\"(\"x\") = \"a\"(\"x\") is a positive-definite matrix defined for each point \"x\" in the domain. We impose the Robin boundary condition\n\nwhere \"ν\" is the component of the unit outward normal vector \"ν\" in the \"k\"-th direction. The system to be solved is\n\nas can be shown using an analogue of Green's identity. The coefficients \"u\" are still found by solving a system of linear equations, but the matrix representing the system is markedly different from that for the ordinary Poisson problem.\n\nIn general, to each scalar elliptic operator \"L\" of order 2\"k\", there is associated a bilinear form \"B\" on the Sobolev space \"H\", so that the weak formulation of the equation \"Lu\" = \"f\" is\n\nfor all functions \"v\" in \"H\". Then the stiffness matrix for this problem is\n\nIn order to implement the finite element method on a computer, one must first choose a set of basis functions and then compute the integrals defining the stiffness matrix. Usually, the domain Ω is discretized by some form of mesh generation, wherein it is divided into non-overlapping triangles or quadrilaterals, which are generally referred to as elements. The basis functions are then chosen to be polynomials of some order within each element, and continuous across element boundaries. The simplest choices are piecewise linear for triangular elements and piecewise bilinear for rectangular elements.\n\nThe element stiffness matrix \"A\" for element \"T\" is the matrix\n\nThe element stiffness matrix is zero for most values of i and j, for which the corresponding basis functions are zero within \"T\". The full stiffness matrix \"A\" is the sum of the element stiffness matrices. In particular, for basis functions that are only supported locally, the stiffness matrix is sparse.\n\nFor many standard choices of basis functions, i.e. piecewise linear basis functions on triangles, there are simple formulas for the element stiffness matrices. For example, for piecewise linear elements, consider a triangle with vertices (\"x\",\"y\"), (\"x\",\"y\"), (\"x\",\"y\"), and define the 2×3 matrix\n\nThen the element stiffness matrix is\n\nWhen the differential equation is more complicated, say by having an inhomogeneous diffusion coefficient, the integral defining the element stiffness matrix can be evaluated by Gaussian quadrature.\n\nThe condition number of the stiffness matrix depends strongly on the quality of the numerical grid. In particular, triangles with small angles in the finite element mesh induce large eigenvalues of the stiffness matrix, degrading the solution quality.\n\n"}
{"id": "1479058", "url": "https://en.wikipedia.org/wiki?curid=1479058", "title": "Supercombinator", "text": "Supercombinator\n\nA supercombinator is a mathematical expression which is fully bound and self-contained. It may be either a constant or a combinator where all the subexpressions are supercombinators. Supercombinators are used in the implementation of functional languages.\n\nIn mathematical terms, a lambda expression \"S\" is a supercombinator of arity \"n\" if it has no free variables and is of the form λx.λx...λx.\"E\" (with \"n\" ≥ 0, so that lambdas are not required) such that \"E\" itself is not a lambda abstraction and any lambda abstraction in \"E\" is again a supercombinator.\n\n\n"}
{"id": "286534", "url": "https://en.wikipedia.org/wiki?curid=286534", "title": "Truncated icosidodecahedron", "text": "Truncated icosidodecahedron\n\nIn geometry, the truncated icosidodecahedron is an Archimedean solid, one of thirteen convex isogonal nonprismatic solids constructed by two or more types of regular polygon faces.\n\nIt has 62 faces: 30 squares, 20 regular hexagons, and 12 regular decagons. It has more vertices (120) and edges (180) than any other convex nonprismatic uniform polyhedron. Since each of its faces has point symmetry (equivalently, 180° rotational symmetry), the truncated icosidodecahedron is a zonohedron.\n\nThe name \"great rhombicosidodecahedron\" refers to the relationship with the (small) rhombicosidodecahedron (compare section Dissection).<br>\nThere is a nonconvex uniform polyhedron with a similar name, the nonconvex great rhombicosidodecahedron.\n\nThe surface area \"A\" and the volume \"V\" of the truncated icosidodecahedron of edge length \"a\" are:\n\nIf a set of all 13 Archimedean solids were constructed with all edge lengths equal, the truncated icosidodecahedron would be the largest.\n\nCartesian coordinates for the vertices of a truncated icosidodecahedron with edge length 2\"φ\" − 2, centered at the origin, are all the even permutations of:\nwhere \"φ\" =  is the golden ratio.\n\nThe truncated icosidodecahedron is the convex hull of a rhombicosidodecahedron with cuboids above its 30 squares whose height to base ratio is the golden ratio. The rest of its space can be dissected into 12 nonuniform pentagonal cupolas below the decagons and 20 nonuniform triangular cupolas below the hexagons.\n\nThe truncated icosidodecahedron has seven special orthogonal projections, centered on a vertex, on three types of edges, and three types of faces: square, hexagonal and decagonal. The last two correspond to the A and H Coxeter planes.\nThe truncated icosidodecahedron can also be represented as a spherical tiling, and projected onto the plane via a stereographic projection. This projection is conformal, preserving angles but not areas or lengths. Straight lines on the sphere are projected as circular arcs on the plane.\n\nSchlegel diagrams are similar, with a perspective projection and straight edges.\nWithin Icosahedral symmetry there are unlimited geometric variations of the \"truncated icosidodecahedron\" with isogonal faces. The truncated dodecahedron, rhombicosidodecahedron, and truncated icosahedron as degenerate limiting cases.\n\nIn the mathematical field of graph theory, a truncated icosidodecahedral graph (or great rhombicosidodecahedral graph) is the graph of vertices and edges of the truncated icosidodecahedron, one of the Archimedean solids. It has 120 vertices and 180 edges, and is a zero-symmetric and cubic Archimedean graph.\n\nThis polyhedron can be considered a member of a sequence of uniform patterns with vertex figure (4.6.2\"p\") and Coxeter-Dynkin diagram . For \"p\" < 6, the members of the sequence are omnitruncated polyhedra (zonohedrons), shown below as spherical tilings. For \"p\" > 6, they are tilings of the hyperbolic plane, starting with the truncated triheptagonal tiling.\n\n\n"}
{"id": "14989249", "url": "https://en.wikipedia.org/wiki?curid=14989249", "title": "Turán's inequalities", "text": "Turán's inequalities\n\nIn mathematics, Turán's inequalities are some inequalities for Legendre polynomials found by (and first published by ). There are many generalizations to other polynomials, often called Turán's inequalities, given by and other authors.\n\nIf is the th Legendre polynomial, Turán's inequalities state that\nFor \"H\", the \"n\"th Hermite polynomial, Turán's inequalities are\n\nwhilst for Chebyshev polynomials they are\n\n\n"}
{"id": "46879546", "url": "https://en.wikipedia.org/wiki?curid=46879546", "title": "Uncomputation", "text": "Uncomputation\n\nUncomputation is a technique, used in reversible circuits, for cleaning up temporary side effects on ancilla bits so they can be re-used.\n\nUncomputation is important to quantum computing. Whether or not intermediate effects have been uncomputed affects how states interfere with each other when measuring results.\n"}
