{"id": "399148", "url": "https://en.wikipedia.org/wiki?curid=399148", "title": "54 (number)", "text": "54 (number)\n\n54 (fifty-four) is the natural number following 53 and preceding 55.\n\n54 is a 19-gonal number. Twice the third power of three, 54 is a Leyland number. 54 can be written as the sum of three squares in three different ways: = = = 54. It is the smallest number with this property. Like all other multiples of 6, it is a semiperfect number.\n\nIn base 10, 54 is a Harshad number.\n\nThe Holt graph has 54 edges.\n\nThe sine of an angle of 54 degrees is half the golden ratio.\n\n\n\n\n54 is also:\n"}
{"id": "8198761", "url": "https://en.wikipedia.org/wiki?curid=8198761", "title": "Adleman–Pomerance–Rumely primality test", "text": "Adleman–Pomerance–Rumely primality test\n\nIn computational number theory, the Adleman–Pomerance–Rumely primality test is an algorithm for determining whether a number is prime. Unlike other, more efficient algorithms for this purpose, it avoids the use of random numbers, so it is a deterministic primality test. It is named after its discoverers, Leonard Adleman, Carl Pomerance, and Robert Rumely. The test involves arithmetic in cyclotomic fields.\n\nIt was later improved by Henri Cohen and Hendrik Willem Lenstra, commonly referred to as APR-CL. It can test primality of an integer \"n\" in time:\n\n\n"}
{"id": "22523598", "url": "https://en.wikipedia.org/wiki?curid=22523598", "title": "Alan Turing Year", "text": "Alan Turing Year\n\nThe Alan Turing Year, 2012, marked the celebration of the life and scientific influence of Alan Turing during the centenary of his birth on 23 June 1912. Turing had an important influence on computing, computer science, artificial intelligence, developmental biology, and the mathematical theory of computability and made important contributions to code-breaking during the Second World War. The Alan Turing Centenary Advisory committee (TCAC) was originally set up by Professor Barry Cooper\n\nThe international impact of Turing's work is reflected in the list of countries in which Alan Turing Year was celebrated, including: Bolivia, Brazil, Canada, China, Czech Republic, France, Germany, India, Israel, Italy, Netherlands, Mexico, New Zealand, Norway, Philippines, Portugal, Spain, Switzerland, U.K. and the U.S.A. 41+ countries were involved.\n\nA number of major events took place throughout the year. Some of these were linked to places with special significance in Turing’s life, such as Cambridge University, the University of Manchester, Bletchley Park, Princeton University. The ACM was involved from June to September 2012. Twelve museums were involved including in Germany and Brazil. Artists, musicians and poets took part in the celebrations internationally.\n\nEvents included the 2012 Computability in Europe conference, as well as Turing Centenary activities organized or sponsored by the British Computer Society, the Association for Symbolic Logic, British Colloquium for Theoretical Computer Science, the British Society for the History of Mathematics, the Association for Computing Machinery, British Logic Colloquium, Society for the Study of Artificial Intelligence and the Simulation of Behaviour, the Computer Conservation Society, the Computer Society of India, the Bletchley Park Trust, the European Association for Computer Science Logic, the European Association for Theoretical Computer Science, International Association for Computing and Philosophy, the Department of Philosophy at De La Salle University-Manila, the John Templeton Foundation, the Kurt Gödel Society, the IEEE Symposium on Logic in Computer Science, the Science Museum, and Turing100in2012. The Alan Turing Centenary Conference was held at the University of Manchester during June 2012.\n\nAlan Turing Year is known on Twitter as Alan Turing Years. @alanturingyear.\n\nThe Turing Year was coordinated by the Turing Centenary Advisory Committee (TCAC), representing a range of expertise and organisational involvement in the 2012 celebrations. Members of TCAC include Honorary President, Sir John Dermot Turing; The Chair and founder of the committee, mathematician and author of Alan Turing - His Work and Impact S. Barry Cooper; Turing's biographer Andrew Hodges; Wendy Hall, first person from outside North America elected President of the Association for Computing Machinery (ACM) in July 2008; Simon Singh; Hugh Loebner sponsor of the Loebner Prize for Artificial Intelligence (annual science contest based on the famous Turing test) cyberneticist Kevin Warwick, author of 'March of the Machines' and 'I, Cyborg', and committee member Daniela Derbyshire, who is also handling international co-ordination of marketing and publicity.\n\nExamples include:\n\nThe Royal Mail issued a UK commemorative stamp for the Turing Centenary.\n\nThe Imitation Game: how Benedict Cumberbatch brought Turing to life The Guardian, Tuesday 7 October 2014\n\nDe-coding the Turing family Professor Barry Cooper The Guardian, Tue 17 April 2012\n\nAlan Turing: Centenary accolades keep coming - Yahoo News 3 April 2013\n\nAlan Turing Year - the Establishment still doesn't get it Barry Cooper The Guardian, Tue 22 January 2013\n\nAlan Turing and the bullying of Britain's geeks S Barry Cooper The Guardian, Wed 20 June 2012\n\nPlaying Monopoly with Alan Turing S Barry Cooper The Guardian, Mon 24 September 2012\n\nAlan Turing: \"I am building a brain.\" Half a century later, its successor beat Kasparov S Barry Cooper The Guardian, Mon 14 May 2012\n\nGoogle doodle becomes an enigma in honour of Alan Turing The Daily Telegraph, Sat 23 June 2012\n\nTribute to computing's forefather BBC News, Sat 27 October 2012\n\nThe other Turing test: Codebreaker's beloved Monopoly pays him the ultimate compliment The Independent, Sat 8 September 2012\n\nGCHQ Director Iain Lobban pays tribute to Alan Turing Centenary - \"Oddballs Wanted\" piece in Daily Mail 5 October 2012\n\nSunflower maths theory is tested BBC News\n\nHow did the leopard get its spots? Codebreaker Alan Turing was right all along The Daily Telegraph\n\nThe Queen hails 'genius' of Alan Turing on visit to WWII codebreaking HQ at Bletchley Park menmedia.co.uk, Fri 15 July 2011\n\nBarry Cooper was interviewed on BBC 3 Counties Radio and Sky News in relation to the pardon of Alan Turing on December 24, 2013.\n\n\n"}
{"id": "151912", "url": "https://en.wikipedia.org/wiki?curid=151912", "title": "All horses are the same color", "text": "All horses are the same color\n\nThe horse paradox is a falsidical paradox that arises from flawed demonstrations, which purport to use mathematical induction, of the statement \"All horses are the same color\". There is no actual contradiction, as these arguments have a crucial flaw that makes them incorrect. This example was originally raised by George Pólya. The paradox was also used by Joel E. Cohen as an example of the subtle errors that can occur in attempts to prove statements by induction.\n\nThe argument is proof by induction. First we establish a base case for one horse (formula_1). We then prove that if formula_2 horses have the same color, then formula_3 horses must also have the same color.\n\nThe case with just one horse is trivial. If there is only one horse in the \"group\", then clearly all horses in that group have the same color.\n\nAssume that formula_2 horses always are the same color. Let us consider a group consisting of formula_3 horses.\n\nFirst, exclude the last horse and look only at the first formula_2 horses; all these are the same color since formula_2 horses always are the same color. Likewise, exclude the first horse and look only at the last formula_2 horses. By the same reasoning, these too, must also be of the same color. Therefore, the first horse in the group is of the same color as the horses in the middle, who in turn are of the same color as the last horse. Hence the first horse, middle horses, and last horse are all of the same color, and we have proven that:\n\nThus, by the principle of mathematical induction, in any group of horses, all horses must be the same color.\n\nThe argument above makes the implicit assumption that the two subsets of horses to which the induction assumption is applied have a common element. This is not true when the original set (prior to either removal) only contains two horses.\n\nLet the two horses be horse A and horse B. When horse A is removed, it is true that the remaining horses in the set are the same color (only horse B remains). The same is true when horse B is removed. However the statement \"the first horse in the group is of the same color as the horses in the middle\" is meaningless, because there are no \"horses in the middle\" (common elements (horses) in the two sets). Therefore the above proof has a logical link broken. The proof forms a falsidical paradox; it seems to show by valid reasoning something that is manifestly false, but in fact the reasoning is flawed.\n\n\n"}
{"id": "339496", "url": "https://en.wikipedia.org/wiki?curid=339496", "title": "As I was going to St Ives", "text": "As I was going to St Ives\n\n\"As I was going to St Ives\" is a traditional English-language nursery rhyme in the form of a riddle. Its Roud Folk Song Index number is 19772.\n\nThe most common modern version is:\n\nThe following version is found in a manuscript (Harley MS 7316) dating from approximately 1730: \n\nA version very similar to that accepted today was published in the \"Weekly Magazine\" of August 4, 1779:\n\nThe suggestion of polygamy (implicit in the line \"I met a man with seven wives\") is generally absent from the earliest publications, but is present by 1837.\n\nThere are a number of places called St Ives in England and elsewhere. It is generally thought\nthat the rhyme refers to St Ives, Cornwall, when it was a busy fishing port and had many cats to stop the rats and mice destroying the fishing gear, although some people argue it was St Ives, Cambridgeshire as this is an ancient market town and therefore an equally plausible destination.\n\nAll potential answers to this riddle are based on its ambiguity because the riddle only tells us the group has been \"met\" on the journey to St. Ives and gives no further information about its intentions, only those of the narrator. In modern usage, 'to meet someone on the road' may include the sense of 'passed' or 'overtook'; while the older usage may have referred exclusively to those going in opposite directions. As such, the 'correct' answer could be stated as \"at least one, the narrator plus anyone who happens to be travelling in the same direction as him or her\".\n\nIf the group that the narrator meets is assumed \"not\" to be travelling to St. Ives the answer could be \"one\" person going to St. Ives: the narrator. This is the most common assumption, as the purpose of the riddle was most likely to trick the listener into making long winded calculations only to be surprised by the simplicity of the answer.\n\nIf one disregards the 'trick' answer and assumes the narrator overtook the group as they were \"also\" travelling to St. Ives, the most common mathematical answer is 2802: 1 man, 7 wives, 49 sacks, 343 cats, and 2401 kits, plus the narrator (the sum of a geometric series, plus one).\n\nAfter the riddle was published in the August 4, 1779 issue of \"The Weekly Magazine\", as described above, a subsequent edition of that journal contained the following solution, submitted by reader \"Philo-Rhithmus\" of Edinburgh:\n\nA similar problem is found in the Rhind Mathematical Papyrus (Problem 79), dated to around 1650 BC.\nThe papyrus is translated as follows:\nThe problem appears to be an illustration of an algorithm for multiplying numbers. The sequence 7, 7, 7, 7, 7 appears in the right-hand column, and the terms 2,801, 2×2,801, 4×2,801 appear in the left; the sum on the left is 7×2,801 = 19,607, the same as the sum of the terms on the right. The equality of the two geometric sequences can be stated as the equation (2+2+2)(7+7+7+7+7) = 7+7+7+7+7, which relies on the coincidence 2+2+2=7.\n\nNote that the author of the papyrus listed a wrong value for the fourth power of 7; it should be 2,401, not 2,301. However, the sum of the powers (19,607) is correct.\n\nThe problem has been paraphrased by modern commentators as a story problem involving houses, cats, mice, and grain, although in the Rhind Mathematical Papyrus there is no discussion beyond the bare outline stated above. The hekat was of a cubic cubit (approximately ).\n\n"}
{"id": "22578403", "url": "https://en.wikipedia.org/wiki?curid=22578403", "title": "Bender–Knuth involution", "text": "Bender–Knuth involution\n\nIn algebraic combinatorics, a Bender–Knuth involution is an involution on the set of semistandard tableaux, introduced by in their study of plane partitions.\n\nThe Bender–Knuth involutions σ are defined for integers \"k\", and act on the set of semistandard skew Young tableaux of some fixed shape μ/ν, where μ and ν are partitions. It acts by changing some of the elements \"k\" of the tableau to \"k\" + 1, and some of the entries \"k\" + 1 to \"k\", in such a way that the numbers of elements with values \"k\" or \"k\" + 1 are exchanged. Call an entry of the tableau free if it is \"k\" or \"k\" + 1 and there is no other element with value \"k\" or \"k\" + 1 in the same column. For any \"i\", the free entries of row \"i\" are all in consecutive columns, and consist of \"a\" copies of \"k\" followed by \"b\" copies of \"k\" + 1, for some \"a\" and \"b\". The Bender–Knuth involution σ replaces them by \n\"b\" copies of \"k\" followed by \"a\" copies of \"k\" + 1.\n\nBender–Knuth involutions can be used to show that the number of semistandard skew tableaux of given shape and weight is unchanged under permutations of the weight. In turn this implies that the Schur function of a partition is a symmetric function.\n\nBender–Knuth involutions were used by to give a short proof of the Littlewood–Richardson rule.\n\n"}
{"id": "24103948", "url": "https://en.wikipedia.org/wiki?curid=24103948", "title": "Bounded quantification", "text": "Bounded quantification\n\nIn type theory, bounded quantification (also bounded polymorphism or constrained genericity) refers to universal or existential quantifiers which are restricted (\"bounded\") to range only over the subtypes of a particular type. Bounded quantification is an interaction of parametric polymorphism with subtyping. Bounded quantification has traditionally been studied in the functional setting of System F, but is available in modern object-oriented languages supporting parametric polymorphism (generics) such as Java, C# and Scala.\n\nThe purpose of bounded quantification is to allow for polymorphic functions to depend on some specific behaviour of objects instead of type inheritance. It assumes a record-based model for object classes, where every class member is a record element and all class members are named functions. Object attributes are represented as functions that take no argument and return an object. The specific behaviour is then some function name along with the types of the arguments and the return type. Bounded quantification allows to considers all objects with such a function. An example would be a polymorphic codice_1 function that considers all objects that are comparable to each other.\n\n\"F\"-bounded quantification or recursively bounded quantification, introduced in 1989, allows for more precise typing of functions that are applied on recursive types. A recursive type is one that includes a function that uses it as a type for some argument or its return value.\n\nThis kind of type constraint can be expressed in Java with a generic interface. The following example demonstrates how to describe types that can be compared to each other and use this as typing information in polymorphic functions. The codice_2 function uses simple bounded quantification and does not preserve the type of the assigned types, in contrast with the codice_3 function which uses F-bounded quantification.\n\nIn mathematical notation, the types of the two functions are\n\nwhere\n\n\n\n"}
{"id": "11376440", "url": "https://en.wikipedia.org/wiki?curid=11376440", "title": "CIP-Tool", "text": "CIP-Tool\n\nCIP-Tool (Communicating Interacting Processes) is a software tool for the modelling and implementation of event-driven applications. It is especially relevant for the development of software components of embedded systems.\n\nThe underlying mathematical formalisms of CIP were first proposed by the physicist, Prof. Dr. Hugo Fierz. The tool was subsequently developed at the Swiss Federal Institute of Technology (Zurich) in a series of research projects during the 1990s. Development and distribution has since been transferred to a commercially operating spin-off company, CIP-Tool, based in Solothurn, Switzerland.\n\nCIP Tool has been over taken by Actifsource GmbH in summer 2011. Actifsource has integrated the CIP Tool into the Actifsource workbench. \n\nThe CIP-model is basically a finite state machine, or more precisely, an extended finite state machine (processes can store and modify variables and can use these to enable or disable transitions).\n\nIn CIP, a desired system behaviour is broken down into distinct \"processes\", each of which is a set of \"states\" interconnected by \"transitions\". One state in every process is tagged as \"active\" state. This active status can be transferred to another state through the execution of a transition. Such transitions are triggered by \"events\" (from external sources, e.g. sensors) or \"in-pulses\" (from other processes). Transitions can in turn send one or several \"out-pulses\" (to other processes) or \"actions\" (to external receivers, e.g. effectors).\n\nThe CIP-model is sometimes confused with petri nets. This may be because to beginners, the notation looks similar. The similarities should not be over-stressed, however. For example, CIP allows only (and exactly) one active state per process and processes are neither started nor terminated during run-time.\n\nCIP-Tool permits models to be automatically converted to executable code. This greatly facilitates testing, documentation and final implementation. Currently the languages C/C++ and Java are supported as output formats.\n\n"}
{"id": "15433374", "url": "https://en.wikipedia.org/wiki?curid=15433374", "title": "Classical Hamiltonian quaternions", "text": "Classical Hamiltonian quaternions\n\nWilliam Rowan Hamilton invented quaternions, a mathematical entity in 1843. This article describes Hamilton's original treatment of quaternions, using his notation and terms. Hamilton's treatment is more geometric than the modern approach, which emphasizes quaternions' algebraic properties. Mathematically, quaternions discussed differ from the modern definition only by the terminology which is used.\n\nHamilton defined a quaternion as the quotient of two directed lines in tridimensional space; or, more generally, as the quotient of two vectors.\n\nA quaternion can be represented as the sum of a \"scalar\" and a \"vector\". It can also be represented as the product of its \"tensor\" and its \"versor\".\n\nHamilton invented the term \"scalars\" for the real numbers, because they span the \"scale of progression from positive to negative infinity\" or because they represent the \"comparison of positions upon one common scale\". Hamilton regarded ordinary scalar algebra as the science of pure time.\n\nHamilton defined a vector as \"a right line ... having not only length but also direction\". Hamilton derived the word \"vector\" from the Latin \"vehere\", to carry.\n\nHamilton conceived a vector as the \"difference of its two extreme points.\" For Hamilton, a vector was always a three-dimensional entity, having three co-ordinates relative to any given co-ordinate system, including but not limited to both polar and rectangular systems. He therefore referred to vectors as \"triplets\".\n\nHamilton defined addition of vectors in geometric terms, by placing the origin of the second vector at the end of the first. He went on to define vector subtraction.\n\nBy adding a vector to itself multiple times, he defined multiplication of a vector by an integer, then extended this to division by an integer, and multiplication (and division) of a vector by a rational number. Finally, by taking limits, he defined the result of multiplying a vector α by any scalar \"x\" as a vector β with the same direction as α if \"x\" is positive; the opposite direction to α if \"x\" is negative; and a length that is |\"x\"| times the length of α.\n\nThe quotient of two parallel or anti-parallel vectors is therefore a scalar with absolute value equal to the ratio of the lengths of the two vectors; the scalar is positive if the vectors are parallel and negative if they are anti-parallel.\n\nA unit vector is a vector of length one. Examples of unit vectors include i, j and k.\n\nHamilton defined \"tensor\" as a positive numerical quantity, or, more properly, signless number. A tensor can be thought of as a positive scalar. The \"tensor\" can be thought of as representing a \"stretching factor.\"\n\nHamilton introduced the term tensor in his first book, Lectures on Quaternions, based on lectures he gave shortly after his invention of the quaternions:\n\nEach quaternion has a tensor, which is a measure of its magnitude (in the same way as the length of a vector is a measure of a vectors' magnitude). When a quaternion is defined as the quotient of two vectors, its tensor is the ratio of the lengths of these vectors.\n\nA versor is a quaternion with a tensor of 1. Alternatively, a versor can be defined as the quotient of two equal-length vectors.\n\nIn general a versor defines all of the following: a directional axis; the plane normal to that axis; and an angle of rotation.\n\nWhen a versor and a vector which lies in the plane of the versor are multiplied, the result is a new vector of the same length but turned by the angle of the versor.\n\nSince every unit vector can be thought of as a point on a unit sphere, and since a versor can be thought of as the quotient of two vectors, a versor has a representative great circle arc, called a vector arc, connecting these two points, drawn from the divisor or lower part of quotient, to the dividend or upper part of the quotient.\n\nWhen the arc of a versor has the magnitude of a right angle, then it is called a right versor, a \"right radial\" or \"quadrantal versor\".\n\nTwo special degenerate versor cases, called the unit-scalars These two scalars, negative and positive unity can be thought of as scalar quaternions. These two scalars are special limiting cases, corresponding to versors with angles of either zero or π.\n\nUnlike other versors, these two cannot be represented by a unique arc. The arc of one is a single point, and minus one can be represented by an infinite number of arcs, because there are an infinite number of shortest lines between antipodal points of a sphere.\n\nEvery quaternion can be decomposed into a scalar and a vector.\n\nThese two operations S and V are called \"take the Scalar of\" and \"take the vector of\" a quaternion. The vector part of a quaternion is also called the right part.\n\nEvery quaternion is equal to a versor multiplied by the tensor of the quaternion. Denoting the versor of a quaternion by\n\nand the tensor of a quaternion by\n\nwe have\n\nA right quaternion is a quaternion whose scalar component is zero,\n\nThe angle of a right quaternion is 90 degrees. A right quaternion can also be thought of as a vector plus a zero scalar. Right quaternions may be put in what was called the standard trinomial form. For example, if Q is a right quaternion, it may be written as:\n\nFour operations are of fundamental importance in quaternion notation.\nIn particular it is important to understand that there is a single operation of multiplication, a single operation of division, and a single operations of addition and subtraction. This single multiplication operator can operate on any of the types of mathematical entities. Likewise every kind of entity can be divided, added or subtracted from any other type of entity. Understanding the meaning of the subtraction symbol is critical in quaternion theory, because it leads to an understanding of the concept of a vector.\n\nThe two ordinal operations in classical quaternion notation were addition and subtraction or + and −.\n\nThese marks are:\n\n\"...characteristics of synthesis and analysis of a state of progression, according as this state is considered as being derived from, or compared with, some other state of that progression.\"\n\nSubtraction is a type of analysis called ordinal analysis\n\n...let space be now regarded as the field of progression which is to be studied, and POINTS as \"states\" of that progression. ...I am led to regard the word \"Minus,\" or the mark −, in geometry, as the sign or characteristic of analysis of one geometric position (in space), as compared with another (such) position. The comparison of one mathematical point with another with a view to the determination of what may be called their ordinal relation, or their relative position in space...\n\nThe first example of subtraction is to take the point A to represent the earth, and the point B to represent the sun, then an arrow drawn from A to B represents the act of moving or vection from A to B.\n\nthis represents the first example in Hamilton's lectures of a vector. In this case the act of traveling from the earth to the moon.\n\nAddition is a type of analysis called ordinal synthesis.\n\nVectors and scalars can be added. When a vector is added to a scalar, a completely different entity, a quaternion is created.\n\nA vector plus a scalar is always a quaternion even if the scalar is zero. If the scalar added to the vector is zero then the new quaternion produced is called a right quaternion. It has an angle characteristic of 90 degrees.\n\nThe two Cardinal operations in quaternion notation are geometric multiplication and geometric division and can be written:\nIt is not required to learn the following more advanced terms in order to use division and multiplication.\n\nDivision is a kind of analysis called cardinal analysis. Multiplication is a kind of synthesis called cardinal synthesis\n\nClassically, the quaternion was viewed as the ratio of two vectors, sometimes called a geometric fraction.\n\nIf OA and OB represent two vectors drawn from the origin O to two other points A and B, then the geometric fraction was written as\n\nAlternately if the two vectors are represented by α and β the quotient was written as\n\nor\n\nHamilton asserts: \"The quotient of two vectors is generally a quaternion\". \"Lectures on Quaternions\" also first introduces the concept of a quaternion as the quotient of two vectors:\n\nLogically and by definition,\n\nif formula_10\n\nthen formula_11.\n\nIn Hamilton's calculus the product is not commutative, i.e., the order of the variables is of great importance. If the order of q and β were to be reversed the result would not in general be α. The quaternion q can be thought of as an operator that changes β into α, by first rotating it, formerly an act of \"version\" and then changing the length of it, formerly call an act of \"tension\".\n\nAlso by definition the quotient of two vectors is equal to the numerator times the reciprocal of the denominator. Since multiplication of vectors is not commutative, the order cannot be changed in the following expression.\n\nAgain the order of the two quantities on the right hand side is significant.\n\nHardy presents the definition of division in terms of mnemonic cancellation rules. \"Canceling being performed by an upward right hand stroke\".\n\nIf alpha and beta are vectors and q is a quaternion such that\n\nthen formula_14\n\nand formula_15\n\nand\n\nAn important way to think of q is as an operator that changes β into α, by first rotating it (\"version\") and then changing its length (tension).\n\nThe results of the using the division operator on i, j and k was as follows.\n\nThe reciprocal of a unit vector is the vector reversed.\n\nBecause a unit vector and its reciprocal are parallel to each other but point in opposite directions, the product of a unit vector and its reciprocal have a special case commutative property, for example if a is any unit vector then:\n\nHowever, in the more general case involving more than one vector (whether or not it is a unit vector) the commutative property does not hold. For example:\n\nThis is because k/i is carefully defined as:\n\nSo that:\n\nhowever\n\nWhile in general the quotient of two vectors is a quaternion, If α and β are two parallel vectors then the quotient of these two vectors is a scalar. For example, if\n\nformula_29,\n\nand formula_30 then\n\nWhere a/b is a scalar.\n\nThe quotient of two vectors is in general the quaternion:\n\nWhere α and β are two non-parallel vectors, φ is that angle between them, and e is a unit vector perpendicular to the plane of the vectors α and β, with its direction given by the standard right hand rule.\n\nClassical quaternion notation had only one concept of multiplication. Multiplication of two real numbers, two imaginary numbers or a real number by an imaginary number in the classical notation system was the same operation.\n\nMultiplication of a scalar and a vector was accomplished with the same single multiplication operator; multiplication of two vectors of quaternions used this same operation as did multiplication of a quaternion and a vector or of two quaternions.\n\nWhen two quantities are multiplied the first quantity is called the factor, the second quantity is called the faciend and the result is called the factum.\n\nIn classical notation, multiplication was distributive. Understanding this makes it simple to see why the product of two vectors in classical notation produced a quaternion.\n\nUsing the quaternion multiplication table we have:\n\nThen collecting terms:\n\nThe first three terms are a scalar.\n\nLetting\n\nSo that the product of two vectors is a quaternion, and can be written in the form:\n\nThe product of two right quaternions is generally a quaternion.\n\nLet α and β be the right quaternions that result from taking the vectors of two quaternions:\n\nTheir product in general is a new quaternion represented here by r. This product is not ambiguous because classical notation has only one product.\n\nLike all quaternions r may now be decomposed into its vector and scalar parts.\n\nThe terms on the right are called \"scalar of the product\", and the \"vector of the product\" of two right quaternions.\n\nTwo important operations in two the classical quaternion notation system were S(q) and V(q) which meant take the scalar part of, and take the imaginary part, what Hamilton called the vector part of the quaternion. Here S and V are operators acting on q. Parenthesis can be omitted in these kinds of expressions without ambiguity. Classical notation:\n\nHere, \"q\" is a quaternion. \"Sq\" is the scalar of the quaternion while Vq is the vector of the quaternion.\n\nK is the conjugate operator. The conjugate of a quaternion is a quaternion obtained by multiplying the vector part of the first quaternion by minus one.\n\nIf\n\nthen\n\nThe expression\n\nmeans, assign the quaternion r the value of the conjugate of the quaternion q.\n\nT is the tensor operator. It returns a kind of number called a \"tensor\".\n\nThe tensor of a positive scalar is that scalar. The tensor of a negative scalar is the absolute value of the scalar (i.e., without the negative sign). For example:\n\nThe tensor of a vector is by definition the length of the vector. For example, if:\n\nThen\n\nThe tensor of a unit vector is one. Since the versor of a vector is a unit vector, the tensor of the versor of any vector is always equal to unity. Symbolically:\n\nA quaternion is by definition the quotient of two vectors and the tensor of a quaternion is by definition the quotient of the tensors of these two vectors. In symbols:\n\nFrom this definition it can be shown that a useful formula for the tensor of a quaternion is:\n\nIt can also be proven from this definition that another formula to obtain the tensor of a quaternion is from the common norm, defined as the product of a quaternion and its conjugate. The square root of the common norm of a quaternion is equal to its tensor.\n\nA useful identity is that the square of the tensor of a quaternion is equal to the tensor of the square of a quaternion, so that parenthesis may be omitted.\n\nAlso, the tensors of conjugate quaternions are equal.\n\nThe tensor of a quaternion is now called its norm.\n\nTaking the angle of a non-scalar quaternion, resulted in a value greater than zero and less than π.\n\nWhen a non-scalar quaternion is viewed as the quotient of two vectors, then the axis of the quaternion is a unit vector perpendicular to the plane of the two vectors in this original quotient, in a direction specified by the right hand rule. The angle is the angle between the two vectors.\n\nIn symbols,\n\nIf\n\nthen its reciprocal is defined as\n\nformula_65\n\nThe expression:\n\nReciprocals have many important applications, for example rotations, particularly when q is a versor. A versor has an easy formula for its reciprocal.\n\nIn words the reciprocal of a versor is equal to its conjugate. The dots between operators show the order of the operations, and also help to indicate that S and U for example, are two different operations rather than a single operation named SU.\n\nThe product of a quaternion with its conjugate is its common norm.\n\nThe operation of taking the common norm of a quaternion is represented with the letter N. By definition the common norm is the product of a quaternion with its conjugate. It can be proven that common norm is equal to the square of the tensor of a quaternion. However this proof does not constitute a definition. Hamilton gives exact, independent definitions of both the common norm and the tensor. This norm was adopted as suggested from the theory of numbers, however to quote Hamilton \"they will not often be wanted\". The tensor is generally of greater utility. The word norm does not appear in \"Lectures on Quaternions\", and only twice in the table of contents of \"Elements of Quaternions\".\n\nIn symbols:\n\nThe common norm of a versor is always equal to positive unity.\n\nIn classical quaternion literature the equation\n\nwas thought to have infinitely many solutions that were called geometrically real.\nThese solutions are the unit vectors that form the surface of a unit sphere.\n\nA geometrically real quaternion is one that can be written as a linear combination of \"i\", \"j\" and \"k\", such that the squares of the coefficients add up to one. Hamilton demonstrated that there had to be additional roots of this equation in addition to the geometrically real roots. Given the existence of the imaginary scalar, a number of expressions can be written and given proper names. All of these were part of Hamilton's original quaternion calculus. In symbols:\n\nwhere q and q′ are real quaternions, and the square root of minus one is the imaginary of ordinary algebra, and are called an imaginary or symbolical roots and not a geometrically real vector quantity.\n\nGeometrically Imaginary quantities are additional roots of the above equation of a purely symbolic nature. In article 214 of \"Elements\" Hamilton proves that if there is an i, j and k there also has to be another quantity h which is an imaginary scalar, which he observes should have already occurred to anyone who had read the preceding articles with attention. Article 149 of \"Elements\" is about Geometrically Imaginary numbers and includes a footnote introducing the term \"biquaternion\". The terms \"imaginary of ordinary algebra\" and \"scalar imaginary\" are sometimes used for these geometrically imaginary quantities.\n\n\"Geometrically Imaginary\" roots to an equation were interpreted in classical thinking as geometrically impossible situations. Article 214 of elements of quaternions explores the example of the equation of a line and a circle that do not intersect, as being indicated by the equation having only a geometrically imaginary root.\n\nIn Hamilton's later writings he proposed using the letter h to denote the imaginary scalar\n\nOn page 665 of \"Elements of Quaternions\" Hamilton defines a biquaternion to be a quaternion with complex number coefficients. The scalar part of a biquaternion is then a complex number called a biscalar. The vector part of a biquaternion is a bivector consisting of three complex components. The biquaternions are then the complexification of the original (real) quaternions.\n\nHamilton invented the term \"associative\" to distinguish between the imaginary scalar (known by now as a complex number) which is both commutative and associative, and four other possible roots of negative unity which he designated L, M, N and O, mentioning them briefly in appendix B of \"Lectures on Quaternions\" and in private letters. However, non-associative roots of minus one do not appear in \"Elements of Quaternions\". Hamilton died before he worked on these strange entities. His son claimed it to be a \"bow for another Ulysses\".\n\n\n"}
{"id": "40929", "url": "https://en.wikipedia.org/wiki?curid=40929", "title": "Comparator", "text": "Comparator\n\nIn electronics, a comparator is a device that compares two voltages or currents and outputs a digital signal indicating which is larger. It has two analog input terminals formula_1 and formula_2 and one binary digital output formula_3. The output is ideally \nA comparator consists of a specialized high-gain differential amplifier. They are commonly used in devices that measure and digitize analog signals, such as analog-to-digital converters (ADCs), as well as relaxation oscillators.\n\nThe differential voltages must stay within the limits specified by the manufacturer. Early integrated comparators, like the LM111 family, and certain high-speed comparators like the LM119 family, require differential voltage ranges substantially lower than the power supply voltages (±15 V vs. 36 V). \"Rail-to-rail\" comparators allow any differential voltages within the power supply range. When powered from a bipolar (dual rail) supply, \n\nor, when powered from a unipolar TTL/CMOS power supply:\n\nSpecific rail-to-rail comparators with p-n-p input transistors, like the LM139 family, allow the input potential to drop 0.3 volts \"below\" the negative supply rail, but do not allow it to rise above the positive rail. Specific ultra-fast comparators, like the LMH7322, allow input signal to swing below the negative rail \"and\" above the positive rail, although by a narrow margin of only 0.2 V. Differential input voltage (the voltage between two inputs) of a modern rail-to-rail comparator is usually limited only by the full swing of power supply.\n\nAn operational amplifier (op-amp) has a well balanced difference input and a very high gain. This parallels the characteristics of comparators and can be substituted in applications with low-performance requirements.\n\nA comparator circuit compares two voltages and outputs either a 1 (the voltage at the plus side; VDD in the illustration) or a 0 (the voltage at the negative side) to indicate which is larger. Comparators are often used, for example, to check whether an input has reached some predetermined value. In most cases a comparator is implemented using a dedicated comparator IC, but op-amps may be used as an alternative. Comparator diagrams and op-amp diagrams use the same symbols.\n\nFigure 4 shows a comparator circuit. Note first that the circuit does not use feedback. The circuit amplifies the voltage difference between Vin and VREF, and outputs the result at Vout. If Vin is greater than VREF, then voltage at Vout will rise to its positive saturation level; that is, to the voltage at the positive side. If Vin is lower than VREF, then Vout, will fall to its negative saturation level, equal to the voltage at the negative side.\n\nIn practice, this circuit can be improved by incorporating a hysteresis voltage range to reduce its sensitivity to noise. The circuit shown in Fig. 5, for example, will provide stable operation even when the Vin signal is somewhat noisy.\n\nIn practice, using an operational amplifier as a comparator presents several disadvantages as compared to using a dedicated comparator:\n\n\nA dedicated voltage comparator will generally be faster than a general-purpose operational amplifier pressed into service as a comparator. A dedicated voltage comparator may also contain additional features such as an accurate, internal voltage reference, an adjustable hysteresis and a clock gated input.\n\nA dedicated voltage comparator chip such as LM339 is designed to interface with a digital logic interface (to a TTL or a CMOS). The output is a binary state often used to interface real world signals to digital circuitry (see analog to digital converter). If there is a fixed voltage source from, for example, a DC adjustable device in the signal path, a comparator is just the equivalent of a cascade of amplifiers. When the voltages are nearly equal, the output voltage will not fall into one of the logic levels, thus analog signals will enter the digital domain with unpredictable results. To make this range as small as possible, the amplifier cascade is high gain. The circuit consists of mainly Bipolar transistors. For very high frequencies, the input impedance of the stages is low. This reduces the saturation of the slow, large P-N junction bipolar transistors that would otherwise lead to long recovery times. Fast small Schottky diodes, like those found in binary logic designs, improve the performance significantly though the performance still lags that of circuits with amplifiers using analog signals. Slew rate has no meaning for these devices. For applications in flash ADCs the distributed signal across eight ports matches the voltage and current gain after each amplifier, and resistors then behave as level-shifters.\n\nThe LM339 accomplishes this with an open collector output. When the inverting input is at a higher voltage than the non inverting input, the output of the comparator connects to the negative power supply. When the non inverting input is higher than the inverting input, the output is 'floating' (has a very high impedance to ground).\nThe gain of op amp as comparator is given by this equation\nV(out)=V(in)\n\nWhile it is easy to understand the basic task of a comparator, that is, comparing two voltages or currents, several parameters must be considered while selecting a suitable comparator:\n\nWhile in general comparators are \"fast,\" their circuits are not immune to the classic speed-power tradeoff. High speed comparators use transistors with larger aspect ratios and hence also consume more power. Depending on the application, select either a comparator with high speed or one that saves power. For example, nano-powered comparators in space-saving chip-scale packages (UCSP), DFN or SC70 packages such as MAX9027, LTC1540, LPV7215, MAX9060 and MCP6541 are ideal for ultra-low-power, portable applications. Likewise if a comparator is needed to implement a relaxation oscillator circuit to create a high speed clock signal then comparators having few nano seconds of propagation delay may be suitable. ADCMP572 (CML output), LMH7220 (LVDS Output), MAX999 (CMOS output / TTL output), LT1719 (CMOS output / TTL output), MAX9010 (TTL output), and MAX9601 (PECL output) are examples of some good high speed comparators.\n\nA comparator normally changes its output state when the voltage between its inputs crosses through approximately zero volts. Small voltage fluctuations due to noise, always present on the inputs, can cause undesirable rapid changes between the two output states when the input voltage difference is near zero volts. To prevent this output oscillation, a small hysteresis of a few millivolts is integrated into many modern comparators. \nFor example, the LTC6702, MAX9021 and MAX9031 have internal hysteresis desensitizing them from input noise. In place of one switching point, hysteresis introduces two: one for rising voltages, and one for falling voltages. The difference between the higher-level trip value (VTRIP+) and the lower-level trip value (VTRIP-) equals the hysteresis voltage (VHYST).\n\nIf the comparator does not have internal hysteresis or if the input noise is greater than the internal hysteresis then an external hysteresis network can be built using positive feedback from the output to the non-inverting input of the comparator. The resulting Schmitt trigger circuit gives additional noise immunity and a cleaner output signal. Some comparators such as LMP7300, LTC1540, MAX931, MAX971 and ADCMP341 also provide the hysteresis control through a separate hysteresis pin. These comparators make it possible to add a programmable hysteresis without feedback or complicated equations. Using a dedicated hysteresis pin is also convenient if the source impedance is high since the inputs are isolated from the hysteresis network. When hysteresis is added then a comparator cannot resolve signals within the hysteresis band.\n\nBecause comparators have only two output states, their outputs are near zero or near the supply voltage. Bipolar rail-to-rail comparators have a common-emitter output that produces a small voltage drop between the output and each rail. That drop is equal to the collector-to-emitter voltage of a saturated transistor. When output currents are light, output voltages of CMOS rail-to-rail comparators, which rely on a saturated MOSFET, range closer to the rails than their bipolar counterparts.\n\nOn the basis of outputs, comparators can also be classified as open drain or push–pull. Comparators with an open-drain output stage use an external pull up resistor to a positive supply that defines the logic high level. Open drain comparators are more suitable for mixed-voltage system design. Since the output is high impedance for logic level high, open drain comparators can also be used to connect multiple comparators on to a single bus. Push pull output does not need a pull up resistor and can also source current unlike an open drain output.\n\nThe most frequent application for comparators is the comparison between a voltage and a stable reference. Most comparator manufacturers also offer comparators in which a reference voltage is integrated on to the chip. Combining the reference and comparator in one chip not only saves space, but also draws less supply current than a comparator with an external reference. ICs with wide range of references are available such as MAX9062 (200 mV reference), LT6700 (400 mV reference), ADCMP350 (600 mV reference), MAX9025 (1.236 V reference), MAX9040 (2.048 V reference), TLV3012 (1.24 V reference) and TSM109 (2.5 V reference).\n\nA continuous comparator will output either a \"1\" or a \"0\" any time a high or low signal is applied to its input and will change quickly when the inputs are updated. However, many applications only require comparator outputs at certain instances, such as in A/D converters and memory. By only strobing a comparator at certain intervals, higher accuracy and lower power can be achieved with a clocked (or dynamic) comparator structure, also called a latched comparator. Often latched comparators employ strong positive feedback for a \"regeneration phase\" when a clock is high, and have a \"reset phase\" when the clock is low. \nThis is in contrast to a continuous comparator, which can only employ weak positive feedback since there is no reset period.\n\nA null detector is one that functions to identify when a given value is zero. Comparators can be a type of amplifier distinctively for null comparison measurements. It is the equivalent to a very high gain amplifier with well-balanced inputs and controlled output limits. The circuit compares the two input voltages, determining the larger. The inputs are an unknown voltage and a reference voltage, usually referred to as v and v. A reference voltage is generally on the non-inverting input (+), while v is usually on the inverting input (−). (A circuit diagram would display the inputs according to their sign with respect to the output when a particular input is greater than the other.) The output is either positive or negative, for example ±12 V. In this case, the idea is to detect when there is no difference between in the input voltages. This gives the identity of the unknown voltage since the reference voltage is known.\n\nWhen using a comparator as a null detector, there are limits as to the accuracy of the zero value measurable. Zero output is given when the magnitude of the difference in the voltages multiplied by the gain of the amplifier is less than the voltage limits. For example, if the gain of the amplifier is 10, and the voltage limits are ±6 V, then no output will be given if the difference in the voltages is less than 6 μV. One could refer to this as a sort of uncertainty in the measurement.\n\nFor this type of detector, a comparator detects each time an ac pulse changes polarity. The output of the comparator changes state each time the pulse changes its polarity, that is the output is HI (high) for a positive pulse and LO (low) for a negative pulse squares the input signal.\n\nA comparator can be used to build a relaxation oscillator. It uses both positive and negative feedback. The positive feedback is a Schmitt trigger configuration. Alone, the trigger is a bistable multivibrator. However, the slow negative feedback added to the trigger by the RC circuit causes the circuit to oscillate automatically. That is, the addition of the RC circuit turns the hysteretic bistable multivibrator into an astable multivibrator.\n\nThis circuit requires only a single comparator with an open-drain output as in the LM393, TLV3011 or MAX9028. The circuit provides great flexibility in choosing the voltages to be translated by using a suitable pull up voltage. It also allows the translation of bipolar ±5 V logic to unipolar 3 V logic by using a comparator like the MAX972.\n\nWhen a comparator performs the function of telling if an input voltage is above or below a given threshold, it is essentially performing a 1-bit quantization. This function is used in nearly all analog to digital converters (such as flash, pipeline, successive approximation, delta-sigma modulation, folding, interpolating, dual-slope and others) in combination with other devices to achieve a multi-bit quantization.\n\nComparators can also be used as window detectors. In a window detector, a comparator is used to compare two voltages and determine whether a given input voltage is under voltage or over voltage.\n\nComparators can be used to create absolute value detectors. In an absolute value detector, two comparators and a digital logic gate are used to compare the absolute values of two voltages.\n\n\n"}
{"id": "34709019", "url": "https://en.wikipedia.org/wiki?curid=34709019", "title": "Denjoy–Luzin theorem", "text": "Denjoy–Luzin theorem\n\nIn mathematics, the Denjoy–Luzin theorem, introduced independently by and \nstates that if a trigonometric series converges absolutely on a set of positive measure, then the sum of its coefficients converges absolutely, and in particular the trigonometric series converges absolutely everywhere.\n\n"}
{"id": "5907688", "url": "https://en.wikipedia.org/wiki?curid=5907688", "title": "Digital topology", "text": "Digital topology\n\nDigital topology deals with properties and features of two-dimensional (2D) or three-dimensional (3D) digital images\nthat correspond to topological properties (e.g., connectedness) or topological features (e.g., boundaries) of objects.\n\nConcepts and results of digital topology are used to specify and justify important (low-level) image analysis algorithms, \nincluding algorithms for thinning, border or surface tracing, counting of components or tunnels, or region-filling.\n\nDigital topology was first studied in the late 1960s by the computer image analysis researcher Azriel Rosenfeld (1931–2004), whose publications on the subject played a major role in establishing and developing the field. The term \"digital topology\" was itself invented by Rosenfeld, who used it in a 1973 publication for the first time.\n\nA related work called the grid cell topology appeared in Alexandrov-Hopf's book Topologie I (1935) can be considered as a link to classic combinatorial topology. Rosenfeld \"et al.\" proposed digital connectivity such as 4-connectivity and 8-connectivity in two dimensions as well as 6-connectivity and 26-connectivity in three dimensions. The labeling method for inferring a connected component was studied in the 1970s. T. Pavlidis (1982) suggested the use of graph-theoretic algorithms such as the depth-first search method for finding connected components. V. Kovalevsky (1989) extended Alexandrov-Hopf's 2D grid cell topology to three and higher dimensions. He also proposed (2008) a more general axiomatic theory of locally finite topological spaces and abstract cell complexes formerly suggested by Steinitz (1908). It is the Alexandrov topology. The book of 2008 contains new definitions of topological balls and spheres independent of a metric and numerous applications to digital image analysis.\n\nIn the early 1980s, digital surfaces were studied. Morgenthaler and Rosenfeld (1981) gave a mathematical definition of surfaces in three-dimensional digital space. This definition contains a total of nine types of digital surfaces. The digital manifold was studied in the 1990s. A recursive definition of the digital k-manifold was proposed intuitively by Chen and Zhang in 1993. Many applications were found in image processing and computer vision.\n\nA basic (early) result in digital topology says that 2D binary images require the alternative use of 4- or 8-adjacency or \"pixel connectivity\" (for \"object\" or \"non-object\"\npixels) to ensure the basic topological duality of separation and connectedness. This alternative use corresponds to open or closed\nsets in the 2D grid cell topology, and the result generalizes to 3D: the alternative use of 6- or 26-adjacency corresponds\nto open or closed sets in the 3D grid cell topology. Grid cell topology also applies to multilevel (e.g., color) 2D or 3D images,\nfor example based on a total order of possible image values and applying a 'maximum-label rule' (see book by Klette and Rosenfeld, 2004).\n\nDigital topology is highly related to combinatorial topology. The main differences between them are: (1) digital topology mainly studies digital objects that are formed by grid cells, and (2) digital topology also deals with non-Jordan manifolds.\n\nA combinatorial manifold is a kind of manifold which is discretization of a manifold. It usually means a piecewise linear manifold made by simplicial complexes. A digital manifold is a special kind of combinatorial manifold which is defined in digital space i.e. grid cell space.\n\nA digital form of the Gauss–Bonnet theorem is: Let \"M\" be a closed digital 2D manifold in direct adjacency (i.e. a (6,26)-surface in 3D). \nThe formula for genus is\nwhere \"M\" indicates the set of surface-points each of which has \"i\" adjacent points on the surface (Chen and Rong, ICPR 2008).\nIf \"M\" is simply connected, i.e. \"g\" = 0, then \"M\" = 8 + \"M\" + 2\"M\". (See also Euler characteristic.)\n\n\n"}
{"id": "3132981", "url": "https://en.wikipedia.org/wiki?curid=3132981", "title": "Ehud Hrushovski", "text": "Ehud Hrushovski\n\nEhud Hrushovski (; born 1959) is a mathematical logician. He is a Merton Professor of Mathematical Logic at the University of Oxford and a Fellows Merton College, Oxford. He is also Professor of Mathematics at the Hebrew University of Jerusalem.\nHrushovski's father, Benjamin Harshav (Hebrew: בנימין הרשב, né Hruszowski; 1928-2015), was a literary theorist, a Yiddish and Hebrew poet and a translator, Professor at Yale University and Tel Aviv University in comparative literature. Ehud Hrushovski earned his PhD from the University of California, Berkeley in 1986 under Leo Harrington. He was Professor of Mathematics at the Massachusetts Institute of Technology until 1998 before he went to Jerusalem.\n\nHrushovski is well known for several fundamental contributions to model theory, in particular in the branch that has become known as geometric model theory, and its applications. His PhD thesis revolutionized stable model theory (a part of model theory arising from the stability theory introduced by Saharon Shelah). Shortly afterwards he found counterexamples to the Trichotomy Conjecture of Boris Zilber and his method of proof has become well known as Hrushovski constructions and found many other applications since.\n\nOne of his most famous results is his proof of the geometric Mordell–Lang conjecture in all characteristics using model theory in 1996. This deep proof was a landmark in logic and geometry. He has had many other famous and notable results in model theory and its applications to geometry, algebra, and combinatorics.\n\nHrushovski is a fellow of the American Academy of Arts and Sciences (2007), and Israel Academy of Sciences and Humanities (2008). He is a recipient of the Erdős Prize of \nthe Israel Mathematical Union in 1994, the Rothschild Prize in 1998, the Carol Karp Prize of the Association of Symbolic Logic in 1993 (jointly with Alex Wilkie), and the Carol Karp Prize in 1998. He was a Plenary speaker of the ICM in 1998.\n\n\n"}
{"id": "39171295", "url": "https://en.wikipedia.org/wiki?curid=39171295", "title": "F. D. C. Willard", "text": "F. D. C. Willard\n\nF. D. C. Willard (ca. 1968–1982) was the pen name of a Siamese cat named Chester, who internationally published under this name on physics in scientific journals, once as a co-author and another time as the sole author.\n\nThe American physicist and mathematician Jack H. Hetherington, of Michigan State University, in 1975 wanted to publish some of his research results in the field of low–temperature physics in the scientific journal \"Physical Review Letters\". A colleague, to whom he had given his paper for review, pointed out that Hetherington had used the first person plural in his text, and that the journal would reject this form on submissions with a sole author. Rather than take the time to retype the article to use the singular form, or to bring in a co-author, Hetherington decided to invent one.\n\nHetherington had a Siamese cat named Chester, who had been sired by a Siamese named Willard. Fearing that colleagues might recognize his pet's name, he thought it better to use the pet's initial. Aware that most Americans have at least two given names, he invented two more given names based on the scientific name for a house cat, \"Felis domesticus\", and abbreviated them accordingly as F. D. C. His article, entitled \"Two-, Three-, and Four-Atom Exchange Effects in bcc ³He\" and written by J. H. Hetherington and F. D. C. Willard, was accepted by the Physical Review and published in number 35 (November 1975).\n\nAt the 15th International Conference on Low Temperature Physics in 1978 in Grenoble, Hetherington's co-author was exposed: Hetherington had sent some signed copies of his article to friends and colleagues and included the \"signature\" (paw prints) of his co-author in them. Later, another essay appeared, this time solely authored by F. D. C. Willard, entitled \"L'hélium 3 solide. Un antiferromagnétique nucléaire\", published (in French) in September 1980 in the French popular science magazine \"La Recherche\". Subsequently, Willard disappeared as an author from the professional world.\n\nThe unmasking of Hetherington's co-author on the \"Physical Review\" essay, which was frequently referenced, caused the co-authorship to become world-famous. The story goes that when inquiries were made to Hetherington's office at Michigan State University, and Hetherington was absent, the callers would ask to speak to the co-author instead. F. D. C. Willard appeared henceforth repeatedly in footnotes, where he was thanked for \"useful contributions to the discussion\" or oral communications, and even offered a position as a professor. F. D. C. Willard is sometimes included in lists of \"Famous Cats\" or \"Historical Cats\". As an April Fool's joke, in 2014 the American Physical Society announced that cat-authored papers, including the Hetherington/Willard paper, would henceforth be open-access (papers of the APS usually require subscription or membership for web access).\n\n"}
{"id": "79150", "url": "https://en.wikipedia.org/wiki?curid=79150", "title": "Feigenbaum constants", "text": "Feigenbaum constants\n\nIn mathematics, specifically bifurcation theory, the Feigenbaum constants are two mathematical constants which both express ratios in a bifurcation diagram for a non-linear map. They are named after the mathematician Mitchell Feigenbaum.\n\nFeigenbaum originally related the first constant to the period-doubling bifurcations in the logistic map, but also showed it to hold for all one-dimensional maps with a single quadratic maximum. As a consequence of this generality, every chaotic system that corresponds to this description will bifurcate at the same rate. It was discovered in 1978.\n\nThe first Feigenbaum constant is the limiting ratio of each bifurcation interval to the next between every period doubling, of a one-parameter map\nwhere is a function parameterized by the bifurcation parameter .\n\nIt is given by the limit\nwhere are discrete values of at the -th period doubling.\n\nHere is this number to 30 decimal places : = \n\nTo see how this number arises, consider the real one-parameter map\nHere is the bifurcation parameter, is the variable. The values of for which the period doubles (e.g. the largest value for with no period-2 orbit, or the largest with no period-4 orbit), are , etc. These are tabulated below:\n\nThe ratio in the last column converges to the first Feigenbaum constant. The same number arises for the logistic map\nwith real parameter and variable . Tabulating the bifurcation values again:\n\nIn the case of the Mandelbrot set for complex quadratic polynomial\nthe Feigenbaum constant is the ratio between the diameters of successive circles on the real axis in the complex plane (see animation on the right).\n\nBifurcation parameter is a root point of period- component. This series converges to the Feigenbaum point = −1.401155... The ratio in the last column converges to the first Feigenbaum constant.\n\nOther maps also reproduce this ratio, in this sense the Feigenbaum constant in bifurcation theory is analogous to in geometry and in calculus.\n\nThe second Feigenbaum constant ,\nis the ratio between the width of a tine and the width of one of its two subtines (except the tine closest to the fold). A negative sign is applied to when the ratio between the lower subtine and the width of the tine is measured.\n\nThese numbers apply to a large class of dynamical systems (for example, dripping faucets to population growth).\n\nBoth numbers are believed to be transcendental, although they have not been proven to be so.\n\nThe first proof of the universality of the Feigenbaum constants carried out by Lanford (with a small correction by Eckmann and Wittwer,) was computer-assisted. Over the years, non-numerical methods were discovered for different parts of the proof, aiding Lyubich in producing the first complete non-numerical proof.\n\n\n\n"}
{"id": "26993580", "url": "https://en.wikipedia.org/wiki?curid=26993580", "title": "François Labourie", "text": "François Labourie\n\nFrançois Labourie (born 15 December 1960) is a French mathematician who has made various contributions to geometry, including pseudoholomorphic curves, Anosov diffeomorphism, and convex geometry. In a series of papers with Yves Benoist and Patrick Foulon, he solved a conjecture on Anosov's flows in compact contact manifolds.\n\nHe was educated at the École Normale Supérieure and Paris Diderot University, where he earned his Ph.D. under supervision of Mikhail Gromov. In 1992 he was awarded one of the inaugural prizes of the European Mathematical Society.\n\n"}
{"id": "48779269", "url": "https://en.wikipedia.org/wiki?curid=48779269", "title": "Geometric set cover problem", "text": "Geometric set cover problem\n\nThe geometric set cover problem is the special case of the set cover problem in geometric settings. The input is a range space formula_1 where formula_2 is a universe of points in formula_3 and formula_4 is a family of subsets of formula_2 called \"ranges\", defined by the intersection of formula_2 and geometric shapes such as disks and axis-parallel rectangles. The goal is to select a \"minimum-size\" subset formula_7 of ranges such that every point in the universe formula_2 is covered by some range in formula_9.\n\nGiven the same range space formula_10, a closely related problem is the geometric hitting set problem, where the goal is to select a \"minimum-size\" subset formula_11 of points such that every range of formula_4 has nonempty intersection with formula_13, i.e., is \"hit\" by formula_13.\n\nIn the one-dimensional case, where formula_2 contains points on the real line and formula_4 is defined by intervals, both the geometric set cover and hitting set problems can be solved in polynomial time using a simple greedy algorithm. However, in higher dimensions, they are known to be NP-complete even for simple shapes, i.e., when formula_4 is induced by unit disks or unit squares. The discrete unit disc cover problem is a geometric version of the general set cover problem which is NP-hard.\n\nMany approximation algorithms have been devised for these problems. Due to the geometric nature, the approximation ratios for these problems can be much better than the general set cover/hitting set problems. Moreover, these approximate solutions can even be computed in near-linear time.\n\nThe greedy algorithm for the general set cover problem gives formula_18 approximation, where formula_19. This approximation is known to be tight up to constant factor. However, in geometric settings, better approximations can be obtained. Using a multiplicative weight algorithm, Brönnimann and Goodrich showed that an formula_20-approximate set cover/hitting set for a range space formula_10 with constant VC-dimension can be computed in polynomial time, where formula_22 denotes the size of the optimal solution. The approximation ratio can be further improved to formula_23 or formula_24 when formula_4 is induced by axis-parallel rectangles or disks in formula_26, respectively.\n\nBased on the iterative-reweighting technique of Clarkson and Brönnimann and Goodrich, Agarwal and Pan gave algorithms that computes an approximate set cover/hitting set of a geometric range space in formula_27 time. For example, their algorithms computes an formula_23-approximate hitting set in formula_29 time for range spaces induced by 2D axis-parallel rectangles; and it computes an formula_24-approximate set cover in formula_31 time for range spaces induced by 2D disks.\n\n"}
{"id": "3564294", "url": "https://en.wikipedia.org/wiki?curid=3564294", "title": "George Thibaut", "text": "George Thibaut\n\nGeorge Frederick William Thibaut CIE (March 20, 1848 – 1914) was an Indologist notable for his contributions to the understanding of ancient Indian mathematics and astronomy.\n\nThibaut was born in Germany, worked briefly in England, and then in 1875 was appointed Professor at the Government Sanskrit College, Varanasi in northern India. From 1888 to 1895 he was professor at Muir Central College in Allahabad.\n\nOn 6 November 2014 in its column \"100 Years Ago\" \"The Statesman\" reprinted the following obituary on the late Dr. Thibaut:\nThe death is reported at Heidelberg Hospital, Germany of Dr George Thibaut, C.I.E., Ph.D., D.Sc., who recently retired from the Education Service as Registrar of the Calcutta University. Dr. Thibaut who took part in Franco-German War of 1870 as a noncommissioned officer joined the Muir Central College, Allahabad some 22 years ago as Professor of Philosophy. He rose to be the Principal of the College and was appointed Registrar of the Allahabad University, afterwards being transferred to Calcutta. Besides being a well-known student of philosophy Eastern and Western, the late Dr. Thibaut was an eminent Sanskrit scholar.\n\nHe was appointed CIE in the 1906 New Year Honours.\n\nBetween 1875 and 1878 Thibaut published a detailed essay on the \"Śulba sūtras\", together with a translation of the \"Baudhāyana Śulba sūtra\"; he later translated the \"Pañca Siddhāntikā\" which he co-edited with Pandit Sudhakar Dwivedi (the latter added a Sanskrit commentary). He also edited and translated the following volumes in Max Müller's \"Sacred Books of the East\":\n\nIt was one of the earliest translation of the Brahma Sutras along with the work of Paul Deussen. \n\nThibaut contributed a number of Sanskrit manuscripts to the Department of Oriental Collections, Bodleian Library, University of Oxford, where they are archived today.\n\n"}
{"id": "961668", "url": "https://en.wikipedia.org/wiki?curid=961668", "title": "Goal difference", "text": "Goal difference\n\nGoal difference or points difference is a form of tiebreaker used to rank sport teams which finish on equal points in a league competition. Either \"goal difference\" or \"points difference\" is used, depending on whether matches are scored by goals (as in ice hockey and association football) or by points (as in rugby union and basketball).\n\nGoal difference (or points difference) is calculated as the number of goals (or points) scored in all league matches minus the number of goals or points conceded. Goal difference was first introduced as a tiebreaker in association football, at the 1970 FIFA World Cup, and was adopted by the Football League in England five years later. It has since spread to many other competitions, where it is typically used as either the first or, after tying teams' head-to-head records, second tiebreaker.\n\nGoal difference has often replaced the older goal average, or goal ratio. Goal average means the number of goals scored divided by the number of goals conceded. It was replaced by goal difference, which was thought to encourage more attacking play, encouraging teams to score more goals (or points) as opposed to defending against conceding. However goal average is still used as the tiebreaker in Australian rules football where it is referred to as \"percentage\". This is calculated as points scored divided by points conceded, and then multiplied by 100.\n\nIf two or more teams' total points scored and goal differences are both equal, then often goals scored is used as a further tiebreaker, with the team scoring the most goals winning. After this a variety of other tiebreakers may be used.\n\nThe different schemes can lead to strikingly different results. With the following matches:\n\nUnder goal average, Team A would win:\n\nUnder goal difference, Team B would win:\n\nGoal average was replaced by goal difference due to the former's encouragement of lower-scoring games. For example, a team that scores 70 while conceding 40 would have a lesser goal average (1.750) than another team that scores 69 while conceding 39 (1.769). Or, for the team that has scored 70 while conceding 40, conceding another would reduce the goal average by 0.043 (to 1.707), whereas scoring another would increase it by only 0.025 (to 1.775), making not conceding much more important than scoring again.\n\nAnother issue with goal average is that, if a team has conceded no goals (e.g. Group 1 of the 1966 World Cup), the value cannot be calculated, as division by zero is undefined.\n\nHeading into the final day of the 2006–07 \"Eredivisie\" season, three teams were still in contention to win the title, and with it a guaranteed place in the 2007–08 UEFA Champions League. PSV, looking to win their third straight league title, was the only one of the three to play its final match at home, against Vitesse Arnhem. Ajax, looking to win their first title since 2004, traveled to Willem II, while AZ faced Excelsior looking to win its first league title since 1981, after finishing in the top three in the previous two seasons.\n\nThese final matches were played on April 29, 2007. AZ struggled against Excelsior (who would have to go through a relegation play-off after the end of the game) as they played almost 72 minutes of the match with only 10 men, as goalkeeper Boy Waterman was red-carded in the 18th minute. AZ came from behind twice, with Danny Koevermans tying the match in the 70th minute with his 22nd goal of the season. AZ had a chance to take the lead after its numerical disadvantage was leveled as Excelsior's Rene van Dieren was sent off for yellow card accumulation. AZ never took advantage and a goal from Johan Voskamp in the 90th minute gave Excelsior a shock 3–2 win.\n\nMeanwhile, in Tilburg, Ajax took the lead in the 18th minute with a goal from Urby Emanuelson. Ajax added a second goal in the 69th minute as Klaas-Jan Huntelaar scored his 21st goal of the season.\n\nMeanwhile, PSV scored twice in the first 10 minutes, but gave up a goal three minutes later and led only 2–1 at half-time. In the second half, Ibrahim Afellay scored in the 58th minute before another goal from Jefferson Farfan made the score 4–1 to PSV.\n\nFollowing Huntelaar's 69th-minute goal, PSV and Ajax were level on points, but Ajax had a superior goal difference, by one goal. But in the 77th minute, Philip Cocu put PSV up 5–1 and the team was up on goal difference (+50 to Ajax's +49). The scores stayed that way at full time, and so PSV won the 2006–07 \"Eredivisie\" in one of the most exciting finishes to a season in recent memory.\n\nThe 2010 Úrvalsdeild season concluded on September 25, 2010, and three teams were still in contention to win the league title. Leading the table was Breiðablik, based in Kópavogur, who knew that a win would give them their first ever league title. Trailing one point behind were ÍBV from Vestmannaeyjar, who were looking to win their fourth league title, but its first since 1998. In third place was two-time defending champions FH, looking to win the league title, but trailing Breiðablik by only two points.\n\nBreiðablik traveled to Stjarnan and were held to a scoreless draw, but would get encouraging news. Playing their final game at Keflavík, ÍBV were losing 2–0 with 16 minutes remaining when Denis Sytnik scored for ÍBV to cut the deficit to 2–1. But two late goals from Keflavík's Magnús Þorsteinsson and Bojan Ljubicic denied ÍBV a chance to overtake Breiðablik, as ÍBV lost to Keflavík by 4–1.\n\nMeanwhile, a draw opened the door for FH as they traveled to Reykjavík to face Fram needing to overturn an 11-goal difference. FH got two goals from Gunnar Kristjansson and a third from Atli Viðar Björnsson (which would tie him with two players for the league lead with 14 goals). However, the 3–0 victory was not enough to deny Breiðablik their first ever league title.\n\nThe 2011–12 Premier League was generally a two-horse race contested between Manchester City and Manchester United for most of the season, with both clubs finishing 19 points ahead of third-placed Arsenal. City and United went into their final matches of the season level on points, but with City in first-place due to a goal difference superior by +8. The final matches were relegation threatened Queens Park Rangers at home for City, and Sunderland away for United. City were strong favourites, with United's manager Alex Ferguson stating City would have to do 'something stupid' not to beat QPR.\n\nA Manchester City win would guarantee the title due to a realistically unassailable superior goal difference. If not a win, then City just needed to match United's result at the Stadium of Light against Sunderland. United scored in the 20th minute, winning 1–0. City scored two goals in injury time to come from behind and win 3–2.\n\nArsenal won the league championship on goals-scored, after finishing level on points and goal-difference with Liverpool in the 1988–89 season. Arsenal defeated Liverpool 2–0 in the final game of the season to win the championship.\n\nAhead of the final day of the 2013–14 Nemzeti Bajnokság I season, Debrecen was on course to win its 7th league title since 2005 as its closest competitor Gyor had to overturn a 14-goal swing on the final matchday. Despite losing its season-finale 2–0 to Budapest Honved FC, Debrecen won the title as Gyor only won 5–0 against already-relegated Mezőkövesd-Zsóry SE.\n\nChelsea 88 points and goal difference 50, Sheffield Wednesday 88 points and goal difference 38.\n\nLeeds United 85 points and goal difference 27, Sheffield United 85 points and goal difference 20.\n\nBurnley 80 points and goal difference 21, Carlisle United 80 points and goal difference 15.\n\nPortsmouth 87 points and goal difference 39, Plymouth 87 points and goal difference 25.\n\nIn 1986, Hearts lost 2–0 at Dundee on the final day of the season, which allowed Celtic to win the league championship on goal difference. Had the first tie-breaker been goal average, Hearts would have won the championship.\n\nRangers won the Scottish Premier League in 2003 on goal difference. In the final round of matches, Rangers played Dunfermline, while second-placed Celtic were playing at Kilmarnock. With Celtic and Rangers level on 94 points going into these matches, the Championship would be decided by which team, Celtic or Rangers, performed best during the final round of matches. If both teams won they would each finish on 97 points, and the League would be decided on goal difference. Rangers won 6–1 and Celtic won 4–0, which left Rangers with a goal difference of 73 (101 for and 28 against), and Celtic a goal difference of 72 (98 scored and 26 against) giving Rangers the title.\n\nIn the 1952–53 Football League Championship, Arsenal and Preston North End both finished on 54 points. Arsenal won the title with 97 goals for to 64 against, for an average of 1.516. Preston's 85 to 60 gave 1.417.\n\nGoing into the last game of the 1949–50 season, Sheffield Wednesday needed a result against Tottenham Hotspur to secure second place and clinch promotion at the expense of their local rivals Sheffield United. The resulting 0–0 draw meant Wednesday won promotion by a goal average difference of just 0.008 – a 1–1 draw would have left the two level on points and goal average, and a unique play-off match would have had to be played.\n\nRangers drew their last match of the 1952–53 season, against Queen of the South, 1–1, to finish level with Hibernian on 43 points. They won the title with a goal average of 80–39 to 93–51 (2.051 to 1.824).\n\nEntering the final day of the 1964–65 season, Hearts were two points ahead of nearest rivals Kilmarnock, with two points awarded for a win. Hearts played Kilmarnock at Tynecastle in the last game, with Kilmarnock needing a 2–0 victory to win the league championship on goal average. Hearts could afford to lose 1–0 or 2–1, but lost 2–0 and Kilmarnock won the championship by a goal average of 1.88 to 1.84. Had goal difference been in use, Hearts would have been champions.\n\nRed Star Belgrade won the 1951 Yugoslav First League championship ahead of Dinamo Zagreb with a 0.013 better goal average. Dinamo's final match against BSK Belgrade ended in a 2–2 draw, and the following day Red Star defeated Partizan 2–0, meaning that both teams finished on 35 points. Red Star's 50 goals for and 21 against gave a goal average of 2.381, while Dinamo's 45 to 19 gave 2.368.\n\nIn the 1957–58 Yugoslav First League championship, RNK Split and Budućnost finished the season leveled on points and goal average. Both teams had 25 points, with Budućnost's 30 goals for and 36 against giving a goal average of 0.833, the same as RNK Split's 35 goals for and 42 against. A two-legged play-off match between the two was needed to decide who will enter relegation play-offs. The match in Split ended in a goalless draw, while in the return leg Budućnost defeated RNK Split 4–0. RNK Split entered the relegation play-offs and was relegated in their first season in the top flight.\n"}
{"id": "349051", "url": "https://en.wikipedia.org/wiki?curid=349051", "title": "Guido Fubini", "text": "Guido Fubini\n\nGuido Fubini (19 January 1879 – 6 June 1943) was an Italian mathematician, known for Fubini's theorem and the Fubini–Study metric.\n\nBorn in Venice, he was steered towards mathematics at an early age by his teachers and his father, who was himself a teacher of mathematics. In 1896 he entered the Scuola Normale Superiore di Pisa, where he studied under the notable mathematicians Ulisse Dini and Luigi Bianchi. He gained some early fame when his 1900 doctoral thesis, entitled \"Clifford's parallelism in elliptic spaces\", was discussed in a widely read work on differential geometry published by Bianchi in 1902.\n\nAfter earning his doctorate, he took up a series of professorships. In 1901 he began teaching at the University of Catania in Sicily; shortly afterwards he moved to the University of Genoa; and in 1908 he moved to the Politecnico in Turin and then the University of Turin, where he would stay for a few decades.\n\nDuring this time his research focused primarily on topics in mathematical analysis, especially differential equations, functional analysis, and complex analysis; but he also studied the calculus of variations, group theory, non-Euclidean geometry, and projective geometry, among other topics. With the outbreak of World War I, he shifted his work towards more applied topics, studying the accuracy of artillery fire; after the war, he continued in an applied direction, applying results from this work to problems in electrical circuits and acoustics.\n\nIn 1939, when Fubini at the age of 60 was nearing retirement, Benito Mussolini's Fascists adopted the anti-Jewish policies advocated for several years by Adolf Hitler's Nazis. As a Jew, Fubini feared for the safety of his family, and so accepted an invitation by Princeton University to teach there; he died in New York City four years later.\n\nA main belt asteroid, 22495 Fubini, was named in his honour.\n\n\n\n"}
{"id": "32490744", "url": "https://en.wikipedia.org/wiki?curid=32490744", "title": "Hajós construction", "text": "Hajós construction\n\nIn graph theory, a branch of mathematics, the Hajós construction is an operation on graphs named after that may be used to construct any critical graph or any graph whose chromatic number is at least some given threshold.\n\nLet and be two undirected graphs, be an edge of , and be an edge of . Then the Hajós construction forms a new graph that combines the two graphs by identifying vertices and into a single vertex, removing the two edges and , and adding a new edge .\n\nFor example, let and each be a complete graph on four vertices; because of the symmetry of these graphs, the choice of which edge to select from each of them is unimportant. In this case, the result of applying the Hajós construction is the Moser spindle, a seven-vertex unit distance graph that requires four colors.\n\nAs another example, if and are cycle graphs of length and respectively, then the result of applying the Hajós construction is itself a cycle graph, of length .\n\nA graph is said to be -constructible (or Hajós--constructible) when it formed in one of the following three ways:\n\nIt is straightforward to verify that every -constructible graph requires at least colors in any proper graph coloring. Indeed, this is clear for the complete graph , and the effect of identifying two nonadjacent vertices is to force them to have the same color as each other in any coloring, something that does not reduce the number of colors. In the Hajós construction itself, the new edge forces at least one of the two vertices and to have a different color than the combined vertex for and , so any proper coloring of the combined graph leads to a proper coloring of one of the two smaller graphs from which it was formed, which again causes it to require colors.\n\nHajós proved more strongly that a graph requires at least colors, in any proper coloring, if and only if it contains a -constructible graph as a subgraph. Equivalently, every -critical graph (a graph that requires colors but for which every proper subgraph requires fewer colors) is -constructible. Alternatively, every graph that requires colors may be formed by combining the Hajós construction, the operation of identifying any two nonadjacent vertices, and the operations of adding a vertex or edge to the given graph, starting from the complete graph .\n\nA similar construction may be used for list coloring in place of coloring.\n\nFor , every -critical graph (that is, every odd cycle) can be generated as a -constructible graph such that all of the graphs formed in its construction are also -critical. For , this is not true: a graph found by as a counterexample to Hajós's conjecture that -chromatic graphs contain a subdivision of , also serves as a counterexample to this problem. Subsequently, -critical but not -constructible graphs solely through -critical graphs were found for all . For , one such example is the graph obtained from the dodecahedron graph by adding a new edge between each pair of antipodal vertices\n\nBecause merging two non-adjacent vertices reduces the number of vertices in the resulting graph, the number of operations needed to represent a given graph using the operations defined by Hajós may exceed the number of vertices in .\n\nMore specifically, define the Hajós number of a -chromatic graph to be the minimum number of steps needed to construct from , where each step forms a new graph by combining two previously formed graphs, merging two nonadjacent vertices of a previously formed graph, or adding a vertex or edge to a previously formed graph. They showed that, for an -vertex graph with edges, . If every graph has a polynomial Hajós number, this would imply that it is possible to prove non-colorability in nondeterministic polynomial time, and therefore imply that NP = co-NP, a conclusion considered unlikely by complexity theorists. However, it is not known how to prove non-polynomial lower bounds on the Hajós number without making some complexity-theoretic assumption, and if such a bound could be proven it would also imply the existence of non-polynomial bounds on certain types of Frege system in mathematical logic.\n\nThe minimum size of an expression tree describing a Hajós construction for a given graph may be significantly larger than the Hajós number of , because a shortest expression for may re-use the same graphs multiple times, an economy not permitted in an expression tree. There exist 3-chromatic graphs for which the smallest such expression tree has exponential size.\n\n used the Hajós construction to generate an infinite set of 4-critical polyhedral graphs, each having more than twice as many edges as vertices. Similarly, used the construction, starting with the Grötzsch graph, to generate many 4-critical triangle-free graphs, which they showed to be difficult to color using traditional backtracking algorithms.\n\nIn polyhedral combinatorics, used the Hajós construction to generate facets of the stable set polytope.\n\n"}
{"id": "14463", "url": "https://en.wikipedia.org/wiki?curid=14463", "title": "Harmonic mean", "text": "Harmonic mean\n\nIn mathematics, the harmonic mean (sometimes called the subcontrary mean) is one of several kinds of average, and in particular one of the Pythagorean means. Typically, it is appropriate for situations when the average of rates is desired.\n\nThe harmonic mean can be expressed as the reciprocal of the arithmetic mean of the reciprocals of the given set of observations. As a simple example, the harmonic mean of 1, 4, and 4 is\n\nThe harmonic mean \"H\" of the positive real numbers\nformula_2 is defined to be\n\nThe third formula in the above equation expresses the harmonic mean as the reciprocal of the arithmetic mean of the reciprocals.\n\nFrom the following formula:\n\nit is more apparent that the harmonic mean is related to the arithmetic and geometric means. It is the reciprocal dual of the arithmetic mean for positive inputs:\n\nThe harmonic mean is a Schur-concave function, and dominated by the minimum of its arguments, in the sense that for any positive set of arguments, formula_6. Thus, the harmonic mean cannot be made arbitrarily large by changing some values to bigger ones (while having at least one value unchanged).\n\nThe harmonic mean is one of the three Pythagorean means. For all \"positive\" data sets \"containing at least one pair of nonequal values\", the harmonic mean is always the least of the three means, while the arithmetic mean is always the greatest of the three and the geometric mean is always in between. (If all values in a nonempty dataset are equal, the three means are always equal to one another; e.g., the harmonic, geometric, and arithmetic means of {2, 2, 2} are all 2.)\n\nIt is the special case \"M\" of the power mean:\n\nSince the harmonic mean of a list of numbers tends strongly toward the least elements of the list, it tends (compared to the arithmetic mean) to mitigate the impact of large outliers and aggravate the impact of small ones.\n\nThe arithmetic mean is often mistakenly used in places calling for the harmonic mean. In the speed example below for instance, the arithmetic mean of 50 is incorrect, and too big.\n\nThe harmonic mean is related to the other Pythagorean means, as seen in the equation below. This can be seen by interpreting the denominator to be the arithmetic mean of the product of numbers \"n\" times but each time omitting the \"j\"-th term. That is, for the first term, we multiply all \"n\" numbers except the first; for the second, we multiply all \"n\" numbers except the second; and so on. The numerator, excluding the \"n\", which goes with the arithmetic mean, is the geometric mean to the power \"n\". Thus the \"n\"-th harmonic mean is related to the \"n\"-th geometric and arithmetic means. The general formula is\n\nIf a set of non-identical numbers is subjected to a mean-preserving spread — that is, two or more elements of the set are \"spread apart\" from each other while leaving the arithmetic mean unchanged — then the harmonic mean always decreases.\n\nFor the special case of just two numbers, formula_9 and formula_10, the harmonic mean can be written\n\nIn this special case, the harmonic mean is related to the arithmetic mean formula_12 and the geometric mean formula_13 by\n\nSince formula_15 by the inequality of arithmetic and geometric means, this shows for the \"n\" = 2 case that \"H\" ≤ \"G\" (a property that in fact holds for all \"n\"). It also follows that formula_16, meaning the two numbers' geometric mean equals the geometric mean of their arithmetic and harmonic means.\n\nFor the special case of three numbers, formula_9, formula_10 and formula_19, the harmonic mean can be written\n\nThree positive numbers \"H\", \"G\", and \"A\" are respectively the harmonic, geometric, and arithmetic means of three positive numbers if and only if the following inequality holds\n\nIf a set of weights formula_22, ..., formula_23 is associated to the dataset formula_9, ..., formula_25, the weighted harmonic mean is defined by\n\nThe unweighted harmonic mean can be regarded as the special case where all of the weights are equal.\n\nIn many situations involving rates and ratios, the harmonic mean provides the truest average. For instance, if a vehicle travels a certain distance \"d\" outbound at a speed \"x\" (e.g. 60 km/h) and returns the same distance at a speed \"y\" (e.g. 20 km/h), then its average speed is the harmonic mean of \"x\" and \"y\" (30 km/h) – not the arithmetic mean (40 km/h). The total travel time is the same as if it had traveled the whole distance at that average speed. This can be proven as follows:\n\nAverage speed for the entire journey\n\nHowever, if the vehicle travels for a certain amount of \"time\" at a speed \"x\" and then the same amount of time at a speed \"y\", then its average speed is the arithmetic mean of \"x\" and \"y\", which in the above example is 40 km/h. The same principle applies to more than two segments: given a series of sub-trips at different speeds, if each sub-trip covers the same \"distance\", then the average speed is the \"harmonic\" mean of all the sub-trip speeds; and if each sub-trip takes the same amount of \"time\", then the average speed is the \"arithmetic\" mean of all the sub-trip speeds. (If neither is the case, then a weighted harmonic mean or weighted arithmetic mean is needed. For the arithmetic mean, the speed of each portion of the trip is weighted by the duration of that portion, while for the harmonic mean, the corresponding weight is the distance. In both cases, the resulting formula reduces to dividing the total distance by the total time.)\n\nHowever one may avoid the use of the harmonic mean for the case of \"weighting by distance\". Pose the problem as finding \"slowness\" of the trip where \"slowness\" (in hours per kilometre) is the inverse of speed. When trip slowness is found, invert it so as to find the \"true\" average trip speed. For each trip segment i, the slowness s = 1/speed. Then take the weighted arithmetic mean of the s's weighted by their respective distances (optionally with the weights normalized so they sum to 1 by dividing them by trip length). This gives the true average slowness (in time per kilometre). It turns out that this procedure, which can be done with no knowledge of the harmonic mean, amounts to the same mathematical operations as one would use in solving this problem by using the harmonic mean. Thus it illustrates why the harmonic mean works in this case.\n\nSimilarly, if one wishes to estimate the density of an alloy given the densities of its constituent elements and their mass fractions (or, equivalently, percentages by mass), then the predicted density of the alloy (exclusive of typically minor volume changes due to atom packing effects) is the weighted harmonic mean of the individual densities, weighted by mass, rather than the weighted arithmetic mean as one might at first expect. To use the weighted arithmetic mean, the densities would have to be weighted by volume. Applying dimensional analysis to the problem while labelling the mass units by element and making sure that only like element-masses cancel, makes this clear.\n\nIf one connects two electrical resistors in parallel, one having resistance \"x\" (e.g., 60 Ω) and one having resistance \"y\" (e.g., 40 Ω), then the effect is the same as if one had used two resistors with the same resistance, both equal to the harmonic mean of \"x\" and \"y\" (48 Ω): the equivalent resistance, in either case, is 24 Ω (one-half of the harmonic mean). This same principle applies to capacitors in series or to inductors in parallel.\n\nHowever, if one connects the resistors in series, then the average resistance is the arithmetic mean of \"x\" and \"y\" (with total resistance equal to the sum of x and y). As with the previous example, the same principle applies when more than two resistors are connected, provided that all are in parallel or all are in series.\n\nThe \"conductivity effective mass\" of a semiconductor is also defined as the harmonic mean of the effective masses along the three crystallographic directions.\n\nThe weighted harmonic mean is the preferable method for averaging multiples, such as the price–earnings ratio (P/E), in which price is in the numerator. If these ratios are averaged using a weighted arithmetic mean (a common error), high data points are given greater weights than low data points. The weighted harmonic mean, on the other hand, gives equal weight to each data point. The simple weighted arithmetic mean when applied to non-price normalized ratios such as the P/E is biased upwards and cannot be numerically justified, since it is based on equalized earnings; just as vehicles speeds cannot be averaged for a roundtrip journey.\n\nFor example, consider two firms, one with a market capitalization of $150 billion and earnings of $5 billion (P/E of 30) and one with a market capitalization of $1 billion and earnings of $1 million (P/E of 1000). Consider an index made of the two stocks, with 30% invested in the first and 70% invested in the second. We want to calculate the P/E ratio of this index.\n\nUsing the weighted arithmetic mean (incorrect):\n\nUsing the weighted harmonic mean (correct):\n\nThus, the correct P/E of 93.46 of this index can only be found using the weighted harmonic mean, while the weighted arithmetic mean will significantly overestimate it.\n\nIn any triangle, the radius of the incircle is one-third of the harmonic mean of the altitudes.\n\nFor any point P on the minor arc BC of the circumcircle of an equilateral triangle ABC, with distances \"q\" and \"t\" from B and C respectively, and with the intersection of PA and BC being at a distance \"y\" from point P, we have that \"y\" is half the harmonic mean of \"q\" and \"t\".\n\nIn a right triangle with legs \"a\" and \"b\" and altitude \"h\" from the hypotenuse to the right angle, is half the harmonic mean of and .\n\nLet \"t\" and \"s\" (\"t\" > \"s\") be the sides of the two inscribed squares in a right triangle with hypotenuse \"c\". Then equals half the harmonic mean of and .\n\nLet a trapezoid have vertices A, B, C, and D in sequence and have parallel sides AB and CD. Let E be the intersection of the diagonals, and let F be on side DA and G be on side BC such that FEG is parallel to AB and CD. Then FG is the harmonic mean of AB and DC. (This is provable using similar triangles.)\nOne application of this trapezoid result is in the crossed ladders problem, where two ladders lie oppositely across an alley, each with feet at the base of one sidewall, with one leaning against a wall at height \"A\" and the other leaning against the opposite wall at height \"B\", as shown. The ladders cross at a height of \"h\" above the alley floor. Then \"h\" is half the harmonic mean of \"A\" and \"B\". This result still holds if the walls are slanted but still parallel and the \"heights\" \"A\", \"B\", and \"h\" are measured as distances from the floor along lines parallel to the walls.\n\nIn an ellipse, the semi-latus rectum (the distance from a focus to the ellipse along a line parallel to the minor axis) is the harmonic mean of the maximum and minimum distances of the ellipse from a focus.\n\nIn computer science, specifically information retrieval and machine learning, the harmonic mean of the precision (true positives per predicted positive) and the recall (true positives per real positive) is often used as an aggregated performance score for the evaluation of algorithms and systems: the F-score (or F-measure). This is used in information retrieval because only the positive class is of relevance, while number of negatives, in general, is large and unknown. It is thus a trade-off as to whether the correct positive predictions should be measured in relation to the number of predicted positives or the number of real positives, so it is measured versus a putative number of positives that is an arithmetic mean of the two possible denominators.\n\nA consequence arises from basic algebra in problems where people or systems work together. As an example, if a gas-powered pump can drain a pool in 4 hours and a battery-powered pump can drain the same pool in 6 hours, then it will take both pumps , which is equal to 2.4 hours, to drain the pool together. This is one-half of the harmonic mean of 6 and 4: . That is the appropriate average for the two types of pump is the harmonic mean, and with one pair of pumps (two pumps), it takes half this harmonic mean time, while with two pairs of pumps (four pumps) it would take a quarter of this harmonic mean time.\n\nIn hydrology, the harmonic mean is similarly used to average hydraulic conductivity values for a flow that is perpendicular to layers (e.g., geologic or soil) - flow parallel to layers uses the arithmetic mean. This apparent difference in averaging is explained by the fact that hydrology uses conductivity, which is the inverse of resistivity.\n\nIn sabermetrics, a player's Power–speed number is the harmonic mean of their home run and stolen base totals.\n\nIn population genetics, the harmonic mean is used when calculating the effects of fluctuations in generation size on the effective breeding population. This is to take into account the fact that a very small generation is effectively like a bottleneck and means that a very small number of individuals are contributing disproportionately to the gene pool, which can result in higher levels of inbreeding.\n\nWhen considering fuel economy in automobiles two measures are commonly used – miles per gallon (mpg), and litres per 100 km. As the dimensions of these quantities are the inverse of each other (one is distance per volume, the other volume per distance) when taking the mean value of the fuel economy of a range of cars one measure will produce the harmonic mean of the other – i.e., converting the mean value of fuel economy expressed in litres per 100 km to miles per gallon will produce the harmonic mean of the fuel economy expressed in miles-per-gallon.\n\nIn chemistry and nuclear physics the average mass per particle of a mixture consisting of different species (e.g., molecules or isotopes) is given by the harmonic mean of the individual species' masses weighted by their respective mass fraction.\n\nThe harmonic mean of a beta distribution with shape parameters \"α\" and \"β\" is:\n\nThe harmonic mean with \"α\" < 1 is undefined because its defining expression is not bounded in [0, 1].\n\nLetting \"α\" = \"β\"\n\nshowing that for \"α\" = \"β\" the harmonic mean ranges from 0 for \"α\" = \"β\" = 1, to 1/2 for \"α\" = \"β\" → ∞.\n\nThe following are the limits with one parameter finite (non-zero) and the other parameter approaching these limits:\n\nWith the geometric mean the harmonic mean may be useful in maximum likelihood estimation in the four parameter case.\n\nA second harmonic mean (\"H\") also exists for this distribution\n\nThis harmonic mean with \"β\" < 1 is undefined because its defining expression is not bounded in [ 0, 1 ].\n\nLetting \"α\" = \"β\" in the above expression\n\nshowing that for \"α\" = \"β\" the harmonic mean ranges from 0, for \"α\" = \"β\" = 1, to 1/2, for \"α\" = \"β\" → ∞.\n\nThe following are the limits with one parameter finite (non zero) and the other approaching these limits:\n\nAlthough both harmonic means are asymmetric, when \"α\" = \"β\" the two means are equal.\n\nThe harmonic mean ( \"H\" ) of a lognormal distribution is\n\nwhere \"μ\" is the arithmetic mean and \"σ\" is the variance of the distribution.\n\nThe harmonic and arithmetic means are related by\n\nwhere \"C\" is the coefficient of variation.\n\nThe geometric (\"G\"), arithmetic and harmonic means are related by\n\nThe harmonic mean of type 1 Pareto distribution is\n\nwhere \"k\" is the scale parameter and \"α\" is the shape parameter.\n\nFor a random sample, the harmonic mean is calculated as above. Both the mean and the variance may be infinite (if it includes at least one term of the form 1/0).\n\nThe mean of the sample \"m\" is asymptotically distributed normally with variance \"s\".\n\nThe variance of the mean itself is\n\nwhere \"m\" is the arithmetic mean of the reciprocals, \"x\" are the variates, \"n\" is the population size and \"E\" is the expectation operator.\n\nAssuming that the variance is not infinite and that the central limit theorem applies to the sample then using the delta method, the variance is\n\nwhere \"H\" is the harmonic mean, \"m\" is the arithmetic mean of the reciprocals\n\n\"s\" is the variance of the reciprocals of the data\n\nand \"n\" is the number of data points in the sample.\n\nA jackknife method of estimating the variance is possible if the mean is known. This method is the usual 'delete 1' rather than the 'delete m' version.\n\nThis method first requires the computation of the mean of the sample (\"m\")\n\nwhere \"x\" are the sample values.\n\nA series of value \"w\" is then computed where\n\nThe mean (\"h\") of the \"w\" is then taken:\n\nThe variance of the mean is\n\nSignificance testing and confidence intervals for the mean can then be estimated with the t test.\n\nAssume a random variate has a distribution \"f\"( \"x\" ). Assume also that the likelihood of a variate being chosen is proportional to its value. This is known as length based or size biased sampling.\n\nLet \"μ\" be the mean of the population. Then the probability density function \"f\"*( \"x\" ) of the size biased population is\n\nThe expectation of this length biased distribution E( \"x\" ) is\n\nwhere \"σ\" is the variance.\n\nThe expectation of the harmonic mean is the same as the non-length biased version E( \"x\" )\n\nThe problem of length biased sampling arises in a number of areas including textile manufacture pedigree analysis and survival analysis\n\nAkman \"et al\" have developed a test for the detection of length based bias in samples.\n\nIf \"X\" is a positive random variable and \"q\" > 0 then for all \"ε\" > 0\n\nAssuming that \"X\" and E(\"X\") are > 0 then\n\nThis follows from Jensen's inequality.\n\nGurland has shown that for a distribution that takes only positive values, for any \"n\" > 0\n\nUnder some conditions\n\nwhere ~ means approximately.\n\nAssuming that the variates (\"x\") are drawn from a lognormal distribution there are several possible estimators for \"H\":\n\nwhere\n\nOf these \"H\" is probably the best estimator for samples of 25 or more.\n\nA first order approximation to the bias and variance of \"H\" are\n\nwhere \"C\" is the coefficient of variation.\n\nSimilarly a first order approximation to the bias and variance of \"H\" are\n\nIt has been found in numerical experiments that \"H\" is generally a superior estimator of the harmonic mean than \"H\". \"H\" produces estimates that are largely similar to \"H\".\n\nThe Environmental Protection Agency recommend the use of the harmonic mean in setting maximum toxin levels in water.\n\nIn geophysical reservoir engineering studies, the harmonic mean is widely used.\n\n\n"}
{"id": "14609233", "url": "https://en.wikipedia.org/wiki?curid=14609233", "title": "Holographic algorithm", "text": "Holographic algorithm\n\nIn computer science, a holographic algorithm is an algorithm that uses a holographic reduction. A holographic reduction is a constant-time reduction that maps solution fragments many-to-many such that the sum of the solution fragments remains unchanged. These concepts were introduced by Leslie Valiant, who called them \"holographic\" because \"their effect can be viewed as that of producing interference patterns among the solution fragments\". The algorithms are unrelated to laser holography, except metaphorically. Their power comes from the mutual cancellation of many contributions to a sum, analogous to the interference patterns in a hologram.\n\nHolographic algorithms have been used to find polynomial-time solutions to problems without such previously known solutions for special cases of satisfiability, vertex cover, and other graph problems. They have received notable coverage due to speculation that they are relevant to the P versus NP problem and their impact on computational complexity theory. Although some of the general problems are #P-hard problems, the special cases solved are not themselves #P-hard, and thus do not prove FP = #P.\n\nHolographic algorithms have some similarities with quantum computation, but are completely classical.\n\nHolographic algorithms exist in the context of Holant problems, which generalize counting constraint satisfaction problems (#CSP). A #CSP instance is a hypergraph \"G\"=(\"V\",\"E\") called the constraint graph. Each hyperedge represents a variable and each vertex formula_1 is assigned a constraint formula_2 A vertex is connected to an hyperedge if the constraint on the vertex involves the variable on the hyperedge. The counting problem is to compute\nwhich is a sum over all variable assignments, the product of every constraint, where the inputs to the constrain formula_4 are the variables on the incident hyperedges of formula_1.\n\nA Holant problem is like a #CSP except the input must be a graph, not a hypergraph. Restricting the class of input graphs in this way is indeed a generalization. Given a #CSP instance, replace each hyperedge \"e\" of size \"s\" with a vertex \"v\" of degree \"s\" with edges incident to the vertices contained in \"e\". The constraint on \"v\" is the equality function of arity \"s\". This identifies all of the variables on the edges incident to \"v\", which is the same effect as the single variable on the hyperedge \"e\".\n\nIn the context of Holant problems, the expression in (1) is called the Holant after a related exponential sum introduced by Valiant.\n\nA standard technique in complexity theory is a many-one reduction, where an instance of one problem is reduced to an instance of another (hopefully simpler) problem.\nHowever, holographic reductions between two computational problems preserve the sum of solutions without necessarily preserving correspondences between solutions. For instance, the total number of solutions in both sets can be preserved, even though individual problems do not have matching solutions. The sum can also be weighted, rather than simply counting the number of solutions, using linear basis vectors.\n\nIt is convenient to consider holographic reductions on bipartite graphs. A general graph can always be transformed it into a bipartite graph while preserving the Holant value. This is done by replacing each edge in the graph by a path of length 2, which is also known as the 2-stretch of the graph. To keep the same Holant value, each new vertex is assigned the binary equality constraint.\n\nConsider a bipartite graph \"G\"=(\"U\",\"V\",\"E\") where the constraint assigned to every vertex formula_6 is formula_7 and the constraint assigned to every vertex formula_8 is formula_4. Denote this counting problem by formula_10 If the vertices in \"U\" are viewed as one large vertex of degree |\"E\"|, then the constraint of this vertex is the tensor product of formula_7 with itself |\"U\"| times, which is denoted by formula_12 Likewise, if the vertices in \"V\" are viewed as one large vertex of degree |\"E\"|, then the constraint of this vertex is formula_13 Let the constraint formula_7 be represented by its weighted truth table as a row vector and the constraint formula_4 be represented by its weighted truth table as a column vector. Then the Holant of this constraint graph is simply formula_16\n\nNow for any complex 2-by-2 invertible matrix \"T\" (the columns of which are the linear basis vectors mentioned above), there is a holographic reduction between formula_17 and formula_18 To see this, insert the identity matrix formula_19 in between formula_20 to get\nThus, formula_17 and formula_25 have exactly the same Holant value for every constraint graph. They essentially define the same counting problem.\n\nLet \"G\" be a graph. There is a 1-to-1 correspondence between the vertex covers of \"G\" and the independent sets of \"G\". For any set \"S\" of vertices of \"G\", \"S\" is a vertex cover in \"G\" if and only if the complement of \"S\" is an independent set in \"G\". Thus, the number of vertex covers in \"G\" is exactly the same as the number of independent sets in \"G\".\n\nThe equivalence of these two counting problems can also be proved using a holographic reduction. For simplicity, let \"G\" be a 3-regular graph. The 2-stretch of \"G\" gives a bipartite graph \"H\"=(\"U\",\"V\",\"E\"), where \"U\" corresponds to the edges in \"G\" and \"V\" corresponds to the vertices in \"G\". The Holant problem that naturally corresponds to counting the number of vertex covers in \"G\" is formula_26 The truth table of OR as a row vector is (0,1,1,1). The truth table of EQUAL as a column vector is formula_27. Then under a holographic transformation by formula_28\nwhich is formula_35 the Holant problem that naturally corresponds to counting the number of independent sets in \"G\".\n\nAs with any type of reduction, a holographic reduction does not, by itself, yield a polynomial time algorithm. In order to get a polynomial time algorithm, the problem being reduced to must also have a polynomial time algorithm. Valiant's original application of holographic algorithms used a holographic reduction to a problem where every constraint is realizable by matchgates, which he had just proved is tractable by a further reduction to counting the number of perfect matchings in a planar graph. The latter problem is tractable by the FKT algorithm, which dates to the 1960s.\n\nSoon after, Valiant found holographic algorithms with reductions to matchgates for #Pl-Rtw-Mon-3CNF and #Pl-3/2Bip-VC. These problems may appear somewhat contrived, especially with respect to the modulus. Both problems were already known to be #P-hard when ignoring the modulus and Valiant supplied proofs of #P-hardness modulo 2, which also used holographic reductions. Valiant found these two problems by a computer search that looked for problems with holographic reductions to matchgates. He called their algorithms \"accidental algorithms\", saying \"when applying the term accidental to an algorithm we intend to point out that the algorithm arises from satisfying an apparently onerous set of constraints.\" The \"onerous\" set of constraints in question are polynomial equations that, if satisfied, imply the existence of a holographic reduction to matchgate realizable constraints.\n\nAfter several years of developing (what is known as) matchgate signature theory, Jin-Yi Cai and Pinyan Lu were able to explain the existence of Valiant's two accidental algorithms. These two problems are just special cases of two much larger families of problems: #Pl-Rtw-Mon-kCNF and #Pl-k/2Bip-VC for any positive integer \"k\". The modulus 7 is just the third Mersenne number and Cai and Lu showed that these types of problems with parameter \"k\" can be solved in polynomial time exactly when the modulus is the \"k\"th Mersenne number by using holographic reductions to matchgates and the Chinese remainder theorem.\n\nAround the same time, Jin-Yi Cai, Pinyan Lu and Mingji Xia gave the first holographic algorithm that did not reduce to a problem that is tractable by matchgates. Instead, they reduced to a problem that is tractable by Fibonacci gates, which are symmetric constraints whose truth tables satisfy a recurrence relation similar to one that defines the Fibonacci numbers. They also used holographic reductions to prove that certain counting problems are #P-hard. Since then, holographic reductions have been used extensively as ingredients in both polynomial time algorithms and proofs of #P-hardness.\n"}
{"id": "22885538", "url": "https://en.wikipedia.org/wiki?curid=22885538", "title": "Imre Bárány", "text": "Imre Bárány\n\nImre Bárány (Mátyásföld, 7 December 1947) is a Hungarian mathematician, working in combinatorics and discrete geometry. He works at the Rényi Mathematical Institute of the Hungarian Academy of Sciences, and has a part-time job at University College London.\n\n\nBárány received the Mathematical Prize (now Paul Erdős Prize) of the Hungarian Academy of Sciences in 1985. He was an invited speaker at the Combinatorics session of the International Congress of Mathematicians, in Beijing, 2002. He was an Erdős Lecturer at Hebrew University of Jerusalem in 2004. He was elected a corresponding member of the Hungarian Academy of Sciences (2010). In 2012 he became a fellow of the American Mathematical Society.\n\nHe is an Editorial Board member for the journals \"Combinatorica\", \"Mathematika\", and the \"Online Journal of Analytic Combinatorics\".\nHe is area editor of the journal \"Mathematics of Operations Research\".\n\n"}
{"id": "7952833", "url": "https://en.wikipedia.org/wiki?curid=7952833", "title": "Institute for Quantum Computing", "text": "Institute for Quantum Computing\n\nThe Institute for Quantum Computing (IQC) is an affiliate scientific research institute of the University of Waterloo in located in Waterloo, Ontario with a multidisciplinary approach to the field of quantum information processing. IQC was founded in 2002 primarily through a donation made by Mike Lazaridis and his wife Ophelia whose substantial donations have continued over the years. The institute is now located in the Mike & Ophelia Lazaridis Quantum-Nano Centre and the Research Advancement Centre at the University of Waterloo.\n\nIt is led by founder and physicist, Raymond Laflamme with researchers based in 6 departments across 3 faculties at the University of Waterloo. In addition to theoretical and experimental research on quantum computing, IQC also hosts academic conferences and workshops, short courses for undergraduate and high school students, and scientific outreach events including open houses and tours for the public.\n\nThe IQC seeks to harness quantum mechanics to develop transformational technologies that will benefit society and become a new engine of economic development in the 21st century. It aims to develop and advance quantum information science and technology at the highest international level through the collaboration of computer scientists, engineers, mathematicians and physical scientists.\n\nThe institute's three strategic objectives have been stated as:\n\nThe Institute for Quantum Computing was officially created in 2002, sparked by Research In Motion co-founder Mike Lazaridis and then-president of the University of Waterloo, David Johnston, for research into quantum information. Since inception, Lazaridis has provided more than $100 million in private funding for IQC. The institute is a collaboration between academia, the private sector, and the federal and provincial governments. Raymond Laflamme is the founding executive director.\n\nAt its establishment, the institute was composed of only a handful of researchers from the Departments of Computer Science and Physics. Ten years later, there are more than 200 researchers across six departments within the Faculties of Science, Mathematics, and Engineering at the University of Waterloo.\n\nIn 2008, IQC moved into the Research Advancement Centre 1 (RAC I) in the University of Waterloo's Research & Technology Park. In 2010, research operations expanded into the adjacent building, Research Advancement Centre 2 (RAC II).\n\nIn 2012, IQC expanded into the Mike & Ophelia Lazaridis Quantum-Nano Centre. The 285,000-square-foot facility is shared with the Waterloo Institute for Nanotechnology, and is built to stringent standards (controls for vibration, humidity, temperature, and electromagnetic radiation) for quantum and nanotechnology experiments. The building was designed by Toronto-based firm Kuwabara Payne McKenna Blumberg Architects (KPMB).\n\nResearch at IQC focuses on three main applications of quantum information science and technology using the physical sciences, mathematics and engineering from both theoretical and experimental perspectives. The three applications are quantum computing, which encompasses the manipulation and storage of quantum information; quantum communication, which is related to the transmission of quantum information; and quantum sensing, which is used to detect signals or stimuli that are present in the nanoscopic world.\n\nAreas of research currently studied at IQC include:\n\nIn collaboration with the University of Waterloo, IQC offers research positions and advanced courses in the foundations, applications, and implementation of quantum information processing for graduate students. In addition, IQC also offers an interdisciplinary graduate program in Quantum Information which leads to MMath, MSc, MASc, and PhD degrees.\n\nIQC researchers have frequently made novel theoretical discoveries or performed novel experiments within their respective fields.\n\n\n\n\n\n\n\n\n\n\n\n\n\nIQC's scientific outreach activities include annual workshops, short courses, public lectures, tours, open houses and science centre and museum exhibits. IQC shares many of these special events, including lectures and special interviews, with the online public through its YouTube channel, Instagram feed, and Twitter feed.\n\nQUANTUM: The Exhibition is the first-ever travelling show on quantum information science and technology. Throughout 2017, visitors to science centres and museums across the country can explore how researchers are merging quantum mechanics and information technology to create the technologies that will revolutionize and redefine the 21st century—and how many Canadian researchers are leading the way. This exhibition was selected as part of the Signature Initiative of the Government of Canada's sesquicentennial celebration, INNOVATION150, which celebrates 150 years of Canadian innovation.\n\nThe Exhibition began its cross-Canada tour with an invitation-only premiere on October 13, 2016 at THEMUSEUM in downtown Kitchener, Ontario, and will continue on to at least five other cities on its journey including Vancouver, Saskatoon, Calgary, Halifax and Ottawa.\n\nThe United Nations (UN) General Assembly named 2015 as the International Year of Light and Light-based Technologies (IYL 2015). In order to celebrate and educate the public on the physics of light, a group of graduate students from the University of Waterloo Student Chapter of the Optical Society (OSA) created LIGHT Illuminated, an exhibition featured at THEMUSEUM in downtown Kitchener, Ontario, from October 2015 to March 2016. PhD students from the Institute for Quantum Computing along with a Master’s student from the University of Waterloo Department of Physics and Astronomy created and curated the exhibition. Over 40,000 visitors passed through THEMUSEUM during the exhibit’s display.\n\nIQC has played host to many notable conferences and workshops including:\n\nIQC has presented public lectures by notable researchers including David Cory, Joseph Emerson, Raymond Laflamme, Michele Mosca and Bill Unruh.\n\nThe Undergraduate School on Experimental Quantum Information Processing (USEQIP) is an annual two-week program held in May and June designed for undergraduate students completing the third year of their undergraduate education. The program aims to introduce 20 students to the field of quantum information processing through lectures on quantum information theory and experimental approaches to quantum devices, followed by hands-on exploration using the experimental facilities of IQC.\n\nThe Quantum Cryptography School for Young Students (QCSYS) is an annual one-week summer program for 40 high school students aged 15 and older. The program is run by IQC in conjunction with the University of Waterloo. The selected students attend specialized lectures on quantum physics and cryptography, visit local research institutes, meet renowned researchers in these fields, and take a tour of quantum computing and quantum cryptography experiments.\n\nIQC currently has offices and laboratories in both Research Advancement Centre I and II, located in the University of Waterloo’s David Johnston Research & Technology Park.\n\nOn 9 June 2008, Mike and Ophelia Lazaridis, together with Ontario Premier Dalton McGuinty, University of Waterloo President David Johnston, and other guests officially broke ground on the project which will consist of three areas: one to house IQC, one for the Waterloo Institute for Nanotechnology, and a clean fabrication and metrology suite to be shared between the two institutes. It will house offices, laboratory space, and areas for collaboration among researchers. The QNC opened September 21, 2012.\n\nAs of 2017, IQC’s research team consists of 27 faculty members, 2 research assistant professors, over 30 postdoctoral fellows, and more than 120 students. The institute has expressed intentions to expand to include 33 faculty members, 50 post-doctoral fellows, and 125 students.\n\nIQC faculty members have appointments in the departments of Physics & Astronomy, Combinatorics & Optimization, Applied Mathematics, Electrical & Computer Engineering, Chemistry, and the David R. Cheriton School of Computer Science at the University of Waterloo. IQC faculty and postdoctoral fellows account for 10 of the 31 members of the Canadian Institute for Advanced Research’s Quantum Information Processing Program. In addition, 3 faculty members have associate membership at the Perimeter Institute for Theoretical Physics and 11 are affiliate members.\n\nCurrently, 2 IQC faculty members hold Canada Research Chairs in various aspects of quantum information and 1 faculty member holds a Canada Excellence Research Chair.\n\n\nThe following major awards have been won by IQC researchers for significant contributions to their fields:\n\nAlfred P. Sloan Foundation Fellowship\n\nAmerican Physical Society\n\nCanada Excellence Research Chair (CERC)\n\nCanada Research Chair (CRC)\n\nQueen Elizabeth II Diamond Jubilee\n\nRoyal Society of Canada Fellowship\n\nPolanyi Prize\n\nNatural Sciences and Engineering Research Council (NSERC) Vanier Canada Graduate Scholarship\n\n\n\n\n"}
{"id": "345023", "url": "https://en.wikipedia.org/wiki?curid=345023", "title": "Intuitionistic type theory", "text": "Intuitionistic type theory\n\nIntuitionistic type theory (also known as constructive type theory, or Martin-Löf type theory) is a type theory and an alternative foundation of mathematics.\nIntuitionistic type theory was created by Per Martin-Löf, a Swedish mathematician and philosopher, who first published it in 1972. There are multiple versions of the type theory: Martin-Löf proposed both intensional and extensional variants of the theory and early impredicative versions, shown to be inconsistent by Girard's paradox, gave way to predicative versions. However, all versions keep the core design of constructive logic using dependent types. \n\nMartin-Löf designed the type theory on the principles of mathematical constructivism. Constructivism requires any existence proof to contain a \"witness\". So, any proof of \"there exists a prime greater than 1000\" must identify a specific number that is both prime and greater than 1000. Intuitionistic type theory accomplished this design goal by internalizing the BHK interpretation. An interesting consequence is that proofs become mathematical objects that can be examined, compared, and manipulated.\n\nIntuitionistic type theory's type constructors were built to follow a one-to-one correspondence with logical connectives. For example, the logical connective called implication (formula_1) corresponds to the type of a function (formula_2). This correspondence is called the Curry–Howard isomorphism. Previous type theories had also followed this isomorphism, but Martin-Löf's was the first to extend it to predicate logic by introducing dependent types.\n\nIntuitionistic type theory has 3 finite types, which are then composed using 5 different type constructors. Unlike set theories, type theories are not built on top of a logic like Frege's. So, each feature of the type theory does double duty as a feature of both math and logic.\n\n\"If you are unfamiliar with type theory and know set theory, a quick summary is: Types contain terms just like sets contain elements. Terms belong to one and only one type. Terms like formula_3 and formula_4 compute (\"reduce\") down to canonical terms like 4. For more, see the article on Type theory.\"\n\nThere are 3 finite types: The 0 type contains 0 terms. The 1 type contains 1 canonical term. And the 2 type contains 2 canonical terms.\n\nBecause the 0 type contains 0 terms, it is also called the empty type. It is used to represent anything that cannot exist. It is also written formula_5 and represents anything unprovable. (That is, a proof of it cannot exist.) As a result, negation is defined as a function to it: formula_6.\n\nLikewise, the 1 type contains 1 canonical term and represents existence. It also is called the unit type. It often represents propositions that can be proven and is, therefore, sometimes written formula_7. \n\nFinally, the 2 type contains 2 canonical terms. It represents a definite choice between two values. It is used for Boolean values but \"not\" propositions. Propositions may be proven (the 1 type), may be proven to never have a proof (the 0 type), or may not be proven either way. (The Law of Excluded Middle does not hold for propositions in intuitionistic type theory.)\n\nΣ-types contain ordered pairs. As with typical ordered pair (or 2-tuple) types, a Σ-type can describe the cartesian product, formula_8, of two other types, formula_9 and formula_10. Logically, such an ordered pair would hold a proof of formula_9 and a proof of formula_10, so one may see such a type written as formula_13.\n\nΣ-types are more powerful than typical ordered pair types because of dependent typing. In the ordered pair, the type of the second term can depend on the value of the first term. For example, the first term of the pair might be a natural number and the second term's type might be a vector of length equal to the first term. Such a type would be written:\n\nUsing set-theory terminology, this is similar to an indexed disjoint unions of sets. In the case of usual ordered pairs, the type of the second term does not depend on the value of the first term. Thus the type describing the cartesian product formula_15 is written:\n\nIt is important to note here that the value of the first term, formula_17, is not depended on by the type of the second term, formula_18.\n\nObviously, Σ-types can be used to build up longer dependently-typed tuples used in mathematics and the records or structs used in most programming languages. An example of a dependently-typed 3-tuple is two integers and a proof that the first integer is smaller than the second integer, described by the type:\n\nDependent typing allows Σ-types to serve the role of existential quantifier. The statement \"there exists an formula_17 of type formula_21, such that formula_22 is proven\" becomes the type of ordered pairs where the first item is the value formula_17 of type formula_21 and the second item is a proof of formula_22. Notice that the type of the second item (proofs of formula_22) depends on the value in the first part of the ordered pair (formula_17). Its type would be: \n\nΠ-types contain functions. As with typical function types, they consist of an input type and an output type. They are more powerful than typical function types however, in that the return type can depend on the input value. Functions in type theory are different from set theory. In set theory, you look up the argument's value in a set of ordered pairs. In type theory, the argument is substituted into a term and then computation (\"reduction\") is applied to the term. \nAs an example, the type of a function that, given a natural number formula_17, returns a vector containing formula_17 real numbers is written:\n\nWhen the output type does not depend on the input value, the function type is often simply written with a formula_32. Thus, formula_33 is the type of functions from natural numbers to real numbers. Such Π-types correspond to logical implication. The logical proposition formula_1 corresponds to the type formula_2, containing functions that take proofs-of-A and return proofs-of-B. This type could be written more consistently as:\n\nΠ-types are also used in logic for universal quantification. The statement \"for every formula_17 of type formula_21, formula_22 is proven\" becomes a function from formula_17 of type formula_21 to proofs of formula_22. Thus, given the value for formula_17 the function generates a proof that formula_44 holds for that value. The type would be \n\n=-types are created from two terms. Given two terms like formula_3 and formula_47, you can create a new type formula_48. The terms of that new type represent proofs that the pair reduce to the same canonical term. Thus, since both formula_3 and formula_4 compute to the canonical term formula_51, there will be a term of the type formula_48. In intuitionistic type theory, there is a single way to make terms of =-types and that is by reflexivity:\n\nIt is possible to create =-types such as formula_54 where the terms do not reduce to the same canonical term, but you will be unable to create terms of that new type. In fact, if you were able to create a term of formula_54, you could create a term of formula_5. Putting that into a function would generating a function of type formula_57. Since formula_58 is how intuitionistic type theory defines negation, you would have formula_59 or, finally, formula_60.\n\nEquality of proofs is an area of active research in proof theory and has led to the development of homotopy type theory and other type theories.\n\nInductive types allow the creation of complex, self-referential types. For example, a linked list of natural numbers is either an empty list or a pair of a natural number and another linked list. Inductive types can be used to define unbounded mathematical structures like trees, graphs, etc.. In fact, the natural numbers type may be defined as an inductive type, either being formula_61 or the successor of another natural number. \n\nInductive types define new constants, such as zero formula_62 and the successor function formula_63. Since formula_64 does not have a definition and cannot be evaluated using substitution, terms like formula_65\nand formula_66 become the canonical terms of the natural numbers. \n\nProofs on inductive types are made possible by induction. Each new inductive type comes with its own inductive rule. To prove a predicate formula_44 for every natural number, you use the following rule:\n\nInductive types in intuitionistic type theory are defined in terms of W-types, the type of well-founded trees. Later work in type theory generated coinductive types, induction-recursion, and induction-induction for working on types with more obscure kinds of self-referentiality. Higher inductive types allow equality to be defined between terms.\n\nThe universe types allow proofs to be written about all the types created with the other type constructors. Every term in the universe type formula_69 can be mapped to a type created with any combination of formula_70 and the inductive type constructor. However, to avoid paradoxes, there is no term in formula_69 that maps to formula_69. \n\nTo write proofs about all \"the small types\" and formula_69, you must use formula_74, which does contain a term for formula_69, but not for itself formula_74. Similarly, for formula_77. There is a predicative hierarchy of universes, so to quantify a proof over any fixed constant formula_78 universes, you can use formula_79.\n\nUniverse types are a tricky feature of type theories. Martin-Löf's original type theory had to be changed to account for Girard's paradox. Later research covered topics such as \"super universes\", \"Mahlo universes\", and impredicative universes.\n\nThe formal definition of intuitionistic type theory is written using judgements. For example, in the statement \"if formula_9 is a type and formula_10 is a type then formula_82 is a type\" there are judgements of \"is a type\", \"and\", and \"if ... then ...\". The expression formula_82 is not a judgement; it is the type being defined. \n\nThis second level of the type theory can be confusing, particularly where it comes to equality. There is a judgement of term equality, which might say formula_84. It is a statement that two terms reduce to the same canonical term. There is also a judgement of type equality, say that formula_85, which means every element of formula_9 is an element of the type formula_10 and vice versa. At the type level, there is a type formula_84 and it contains terms if there is a proof that formula_51 and formula_3 reduce to the same value. (Obviously, terms of this type are generated using the term-equality judgement.) Lastly, there is an English-language level of equality, because we use the word \"four\" and symbol \"formula_51\" to refer to the canonical term formula_92. Synonyms like these are called \"definitionally equal\" by Martin-Löf.\n\nThe description of judgements below is based on the discussion in Nordström, Petersson, and Smith.\nThe formal theory works with \"types\" and \"objects\".\n\nA type is declared by:\nAn object exists and is in a type if:\nObjects can be equal\nand types can be equal\nA type that depends on an object from another type is declared\nand removed by substitution\nAn object that depends on an object from another type can be done two ways.\nIf the object is \"abstracted\", then it is written\nand removed by substitution\nThe object-depending-on-object can also be declared as a constant as part of a recursive type. An example of a recursive type is:\nHere, formula_64 is a constant object-depending-on-object. It is not associated with an abstraction.\nConstants like formula_64 can be removed by defining equality. Here the relationship with addition is defined using equality and using pattern matching to handle the recursive aspect of formula_64:\nformula_64 is manipulated as an opaque constant - it has no internal structure for substitution.\n\nSo, objects and types and these relations are used to express formulae in the theory. The following styles of judgements are used to create new objects, types and relations from existing ones:\n\nBy convention, there is a type that represents all other types. It is called formula_114 (or formula_115). Since formula_114 is a type, the member of it are objects. There is a dependent type formula_117 that maps each object to its corresponding type. \"In most texts formula_117 is never written.\" From the context of the statement, a reader can almost always tell whether formula_9 refers to a type, or whether it refers to the object in formula_114 that corresponds to the type.\n\nThis is the complete foundation of the theory. Everything else is derived.\n\nTo implement logic, each proposition is given its own type. The objects in those types represent the different possible ways to prove the proposition. Obviously, if there is no proof for the proposition, then the type has no objects in it. Operators like \"and\" and \"or\" that work on propositions introduce new types and new objects. So formula_121 is a type that depends on the type formula_9 and the type formula_10. The objects in that dependent type are defined to exist for every pair of objects in formula_9 and formula_10. Obviously, if formula_9 or formula_10 has no proof and is an empty type, then the new type representing formula_128 is also empty.\n\nThis can be done for other types (booleans, natural numbers, etc.) and their operators.\n\nUsing the language of category theory, R.A.G. Seely introduced the notion of a locally cartesian closed category (LCCC) as the basic model of type theory. This has been refined by Hofmann and Dybjer to \"Categories with Families\" or \"Categories with Attributes\" based on earlier work by Cartmell.\n\nA category with families is a category \"C\" of contexts (in which the objects are contexts, and\nthe context morphisms are substitutions), together with a functor \"T\" : \"C\" → \"Fam(Set)\".\n\n\"Fam(Set)\" is the category of families of Sets, in which objects are pairs \"(A,B)\" of an \"index set\" \"A\" and a function \"B\": \"X\" → \"A\", and morphisms are pairs of functions \"f\" : \"A\" → \"A' \" and \"g\" : \"X\" → \"X' \", such that \"B' \" \"g\" = \"f\" \"B\" - in other words, \"f\" maps \"B\" to \"B'\".\n\nThe functor \"T\" assigns to a context \"G\" a set \"Ty(G)\" of types, and for each \"A\" : \"Ty(G)\", a set \"Tm(G,A)\" of terms.\nThe axioms for a functor require that these play harmoniously with substitution. Substitution is usually\nwritten in the form \"Af\" or \"af\", where \"A\" is a type in \"Ty(G)\" and \"a\" is a term in \"Tm(G,A)\", and \"f\" is a substitution\nfrom \"D\" to \"G\". Here \"Af\" : \"Ty(D)\" and \"af\" : \"Tm(D,Af)\".\n\nThe category \"C\" must contain a terminal object (the empty context), and a final object for a form\nof product called comprehension, or context extension, in which the right element is a type in the context of the left element.\nIf \"G\" is a context, and \"A\" : \"Ty(G)\", then there should be an object \"(G,A)\" final among\ncontexts \"D\" with mappings \"p\" : \"D → \"G\", \"q\" : \"Tm(D,Ap)\".\n\nA logical framework, such as Martin-Löf's takes the form of\nclosure conditions on the context dependent sets of types and terms: that there should be a type called\nSet, and for each set a type, that the types should be closed under forms of dependent sum and\nproduct, and so forth.\n\nA theory such as that of predicative set theory expresses closure conditions on the types of sets and\ntheir elements: that they should be closed under operations that reflect dependent sum and product,\nand under various forms of inductive definition.\n\nA fundamental distinction is extensional vs intensional type theory. In extensional type theory definitional (i.e., computational) equality is not distinguished from propositional equality, which requires proof. As a consequence type checking becomes undecidable in extensional type theory because programs in the theory might not terminate. For example, such a theory allows one to give a type to the Y-combinator, a detailed example of this can be found in Nordstöm and Petersson \"Programming in Martin-Löf's Type Theory\". However, this doesn't prevent extensional type theory from being a basis for a practical tool, for example, NuPRL is based on extensional type theory. From a practical standpoint there's no difference between a program which doesn't terminate and a program which takes a million years to terminate.\n\nIn contrast in intensional type theory type checking is decidable, but the representation of standard mathematical concepts is somewhat more cumbersome, since intensional reasoning requires using setoids or similar constructions. There are many common mathematical objects, which are hard to work with or can't be represented without this, for example, integer numbers, rational numbers, and real numbers. Integers and rational numbers can be represented without setoids, but this representation isn't easy to work with. Real numbers can't be represented without this.\n\nHomotopy type theory works on resolving this problem. It allows one to define higher inductive types, which not only define first order constructors (values or points), but higher order constructors, i.e. equalities between elements (paths), equalities between equalities (homotopies), \"ad infinitum\".\n\nDifferent forms of type theory have been implemented as the formal systems underlying of a number of proof assistants. While many are based off Per Martin-Löf's ideas, many have added features, more axioms, or different philosophical background. For instance, the NuPRL system is based on computational type theory and Coq is based on the calculus of (co)inductive constructions. Dependent types also feature in the design of programming languages such as ATS, Cayenne, Epigram, Agda, and Idris.\n\nPer Martin-Löf constructed several type theories that were published at various times, some of them much later than the preprints with their description became accessible to the specialists. The list below attempts to list all the theories that have been described in a printed form and to sketch the key features that distinguished them from each other. All of these theories had dependent products, dependent sums, disjoint unions, finite types and natural numbers. All the theories had the same reduction rules that did not include η-reduction either for dependent products or for dependent sums except for MLTT79 where the η-reduction for dependent products is added.\n\nMLTT71 was the first of type theories created by Per Martin-Löf. It appeared in a preprint in 1971. It had one universe but this universe had a name in itself, i.e. it was a type theory with, as it is called today, \"Type in Type\". Jean-Yves Girard has shown that this system was inconsistent and the preprint was never published.\n\nMLTT72 was presented in a 1972 preprint that has now been published. That theory had one universe V and no identity types. The universe was \"predicative\" in the sense that the dependent product of a family of objects from V over an object that was not in V such as, for example, V itself, was not assumed to be in V. The universe was a-la Russell, i.e., one would write directly \"T∈V\" and \"t∈T\" (Martin-Löf uses the sign \"∈\" instead of modern \":\") without the additional constructor such as \"El\".\n\nMLTT73 It was the first definition of a type theory that Per Martin-Löf published (it was presented at the Logic Colloquium 73 and published in 1975). There are identity types which he calls \"propositions\" but since no real distinction between propositions and the rest of the types is introduced the meaning of this is unclear. There is what later acquires the name of J-eliminator but yet without a name (see pp. 94–95). There is in this theory an infinite sequence of universes V...,V... . The universes are predicative, a-la Russell and \"non-cumulative\"! In fact, Corollary 3.10 on p. 115 says that if A∈V and B∈V are such that A and B are convertible then m=n. This means, for example, that it would be difficult to formulate univalence in this theory—there are contractible types in each of the V but it is unclear how to declare them to be equal since there are no identity types connecting V and V for i≠j.\n\nMLTT79 It was presented in 1979 and published in 1982. In this paper, Martin-Löf introduced the four basic types of judgement for the dependent type theory that has since became fundamental in the study of the meta-theory of such systems. He also introduced contexts as a separate concept in it (see p. 161). There are identity types with the J-eliminator (which already appeared in MLTT73 but did not have this name there) but also with the rule that makes the theory \"extensional\" (p. 169). There are W-types. There is an infinite sequence of predicative universes that \"are cumulative\".\n\nBibliopolis There is a discussion of a type theory in the Bibliopolis book from 1984 but it is somewhat open-ended and does not seem to represent a particular set of choices and so there is no specific type theory associated with it.\n\n\n\n\n\n"}
{"id": "45108074", "url": "https://en.wikipedia.org/wiki?curid=45108074", "title": "Irina Mitrea", "text": "Irina Mitrea\n\nIrina Mitrea is a Romanian-American mathematician who works as a professor of mathematics at Temple University. She is known for her research on partial differential equations as well as for promoting mathematics to schoolgirls.\n\nMitrea earned a master's degree from the University of Bucharest in 1993, and completed her doctorate in 2000 at the University of Minnesota under the supervision of Carlos Kenig and Mikhail Safonov. After temporary positions at the Institute for Advanced Study and Cornell University, she joined the faculty of the University of Virginia in 2004, and earned tenure there in 2007. She also taught at the Worcester Polytechnic Institute before moving to Temple. She is the founder of the Girls and Mathematics Program at Temple University, a week-long summer camp in mathematics for middle-school girls.\n\nIn 2008, Mitrea won the Ruth I. Michler Memorial Prize of the Association for Women in Mathematics. In 2014, she was elected as a fellow of the American Mathematical Society \"for contributions to partial differential equations and related fields as well as outreach to women and under-represented minorities at all educational levels.\" She is part of the 2019 class of fellows of the Association for Women in Mathematics.\n"}
{"id": "47874333", "url": "https://en.wikipedia.org/wiki?curid=47874333", "title": "Karim Adiprasito", "text": "Karim Adiprasito\n\nKarim Alexander Adiprasito is a mathematician at the Hebrew University of Jerusalem who works in combinatorics.\n\nHe was awarded the 2015 European Prize in Combinatorics for his work in discrete geometry, in particular on realization spaces of polytopes citing \"his wide-ranging and deep contributions to discrete geometry using analytic methods particularly for his solution of old problems of Perles and Shephard (going back to Legendre and Steinitz) on projectively unique polyhedra.\" \n\nIn joint work with June Huh and Eric Katz, he resolved the Heron–Rota–Welsh conjecture on the log-concavity of the characteristic polynomial of matroids.\nWith Huh, he is one of five winners of the 2019 New Horizons Prize for Early-Career Achievement in Mathematics, associated with the Breakthrough Prize in Mathematics.\n\nHe completed his Ph.D. in 2013 at Free University Berlin under the supervision of Günter Ziegler.\n\n"}
{"id": "30662489", "url": "https://en.wikipedia.org/wiki?curid=30662489", "title": "Key-recovery attack", "text": "Key-recovery attack\n\nA key-recovery attack is an adversary's attempt to recover the cryptographic key of an encryption scheme. Historically, cryptanalysis of block ciphers has focused on key-recovery, but security against these sorts of attacks is a very weak guarantee since it may not be necessary to recover the key to obtain partial information about the message or decrypt message entirely. Modern cryptography uses more robust notions of security. Recently, indistinguishability under adaptive chosen-ciphertext attack (IND-CCA2 security) has become the \"golden standard\" of security. The most obvious key-recovery attack is the exhaustive key-search attack. But modern ciphers often have a key space of size formula_1 or greater, making such attacks infeasible with current technology.\n"}
{"id": "1536947", "url": "https://en.wikipedia.org/wiki?curid=1536947", "title": "Kruskal–Katona theorem", "text": "Kruskal–Katona theorem\n\nIn algebraic combinatorics, the Kruskal–Katona theorem gives a complete characterization of the \"f\"-vectors of abstract simplicial complexes. It includes as a special case the Erdős–Ko–Rado theorem and can be restated in terms of uniform hypergraphs. The theorem is named after Joseph Kruskal and Gyula O. H. Katona. It was independently proved by Marcel-Paul Schützenberger, but his contribution escaped notice for several years.\n\nGiven two positive integers \"N\" and \"i\", there is a unique way to expand \"N\" as a sum of binomial coefficients as follows:\n\nThis expansion can be constructed by applying the greedy algorithm: set \"n\" to be the maximal \"n\" such that formula_2 replace \"N\" with the difference, \"i\" with \"i\" − 1, and repeat until the difference becomes zero. Define\n\nAn integral vector formula_4 is the \"f\"-vector of some formula_5-dimensional simplicial complex if and only if\n\nLet \"A\" be a set consisting of \"N\" distinct \"i\"-element subsets of a fixed set \"U\" (\"the universe\") and \"B\" be the set of all formula_7-element subsets of the sets in \"A\". Expand \"N\" as above. Then the cardinality of \"B\" is bounded below as follows:\n\nThe following weaker but useful form is due to Lovász. Let \"A\" be a set of \"i\"-element subsets of a fixed set \"U\" (\"the universe\") and \"B\" be the set of all formula_7-element subsets of the sets in \"A\". If formula_10 then formula_11.\n\nIn this formulation, \"x\" need not be an integer. The value of the binomial expression is formula_12.\n\nFor every positive \"i\", list all \"i\"-element subsets \"a\" < \"a\" < … \"a\" of the set N of natural numbers in the colexicographical order. For example, for \"i\" = 3, the list begins\n\nGiven a vector formula_14 with positive integer components, let \"Δ\" be the subset of the power set 2 consisting of the empty set together with the first formula_15 \"i\"-element subsets of N in the list for \"i\" = 1, …, \"d\". Then the following conditions are equivalent:\n\n\nThe difficult implication is 1 ⇒ 2.\n\n\n\n\n"}
{"id": "26532764", "url": "https://en.wikipedia.org/wiki?curid=26532764", "title": "Lazare Carnot", "text": "Lazare Carnot\n\nLazare Nicolas Marguerite, Count Carnot (13 May 1753 – 2 August 1823) was a French mathematician, physicist and politician. He was known as the \"Organizer of Victory\" in the French Revolutionary Wars.\n\nBorn on May 13, 1753 in the village of Nolay, Côte-d'Or, Carnot was the son of local judge and royal notary, Claude Carnot and his wife, Marguerite Pothier. He was the second oldest of seven children. At the age of fourteen, Lazare and his brother were enrolled at the Collège d’Autun, in Burgundy where he focused on the study of philosophy and the classics. He held a strong belief in stoic philosophy and was deeply influenced by Roman civilization. When he turned fifteen, he left the Collège d’Autun to strengthen his philosophical knowledge and study under the Society of the Priests of Saint Sulpice. During his short time with them, he studied logic, mathematics and theology under the Abbe Bison. After being impressed with Lazare’s work as a scholar, the Duke D’Aumont (Marquis de Nolay) recommended a military career for the young one and was soon sent by his father to the Aumont residence to further his education. Here, he was enrolled in M. de Longpres pension school in 1770 until he was ready to enter one of two prestigious engineering and artillery schools in Paris. A year later, in February 1771, he was ranked the third highest among twelve who were chosen out of his class of more than one hundred who took the entrance exams. It was at this point when he entered the Mézières School of Engineering appointed as second lieutenant. Studies at the Mézières included geometry, mechanics, geometrical designing, geography, hydraulics and material preparation. On January 1, 1773, he graduated the school ranked as first lieutenant. He was eighteen years old.\n\nIt was here where he met and studied with Benjamin Franklin and at the age of twenty and obtained commission as a lieutenant in the Prince of Condé’s engineer corps. At this moment, he made a name for himself both in the line of (physics) theoretical engineering and in his work in the field of fortifications. While in the army, he continued his study of mathematics. In 1784 he published his first work \"Essay on Machines,\" which contained a statement that foreshadowed the principle of energy as applied to a falling weight, and the earliest proof that kinetic energy is lost in the collision of imperfectly elastic bodies. This publication earned him the honor of admittance to a literary society. Another turning point was his essay on Vauban in which he praises the engineer on his works while at the same time developing his own career as a writer/engineer. Vauban's work had a profound effect on his work as a general and engineer. In that same year, he also received a promotion to the rank of captain.\n\nAt the outbreak of the French Revolution in 1789, Carnot entered political life. He became a delegate to the Legislature in 1791. While a member of the Legislative Assembly, Carnot was elected to the Committee for Public Instruction. He believed that all citizens should be educated and as a member of that committee, he wrote a series of reforms for the teaching and educational systems, but they were not implemented due to the violent social and economic climate of the Revolution.\n\nAfter the Legislative Assembly was dissolved, Carnot was elected to the National Convention in 1792. He spent the last few months of 1792 on a mission to Bayonne, organizing the military defense effort in an attempt to ward off any possible attacks from Spain. Upon returning to Paris, Carnot voted for the death of King Louis XVI, although he had been absent for the debates surrounding the king’s trial.\n\nOn 14 August 1793 Carnot was elected to the Committee of Public Safety, where he took charge of the military situation as one of the Ministers of War.\n\nWith the establishment of the Directory in 1795, Carnot became one of the five initial directors. For the first year, the Directors did well working harmoniously together as well as with the Councils. However, difference of political views led to a schism between Carnot and Étienne-François Letourneur, followed by François de Barthélemy, on the one side, and the triumvirate of Paul François Jean Nicolas, vicomte de Barras, Jean-François Rewbell, and Louis Marie de La Révellière-Lépeaux on the other side. Carnot and Barthélemy supported concessions to end the war, and hoped to oust the triumvirate and replace them with more conservative men. After Letourneur had been replaced by another close collaborator of Carnot, François de Barthélemy, both of them, alongside many deputies in the Council of Five Hundred, were ousted in the Coup of 18 Fructidor (4 September 1797), engineered by Generals Napoleon Bonaparte (originally, Carnot's \"protégé\") and Pierre François Charles Augereau. Carnot took refuge in Geneva, and there in 1797 issued his \"La métaphysique du calcul infinitésimal\".\n\nThe creation of the French Revolutionary Army was largely due to his powers of organization and enforcing discipline. In order to raise more troops for the war, Carnot introduced conscription: the \"levée en masse\" approved by the National Convention was able to raise France’s army from 645,000 troops in mid-1793 to 1,500,000 in September 1794. He was the first to execute the modern waging of war with mass armies and strategic planning realized by the Revolution. As a military engineer, Carnot favored fortresses and defensive strategies. He developed innovative defensive designs for forts, including the Carnot wall, called after him. However, with the constant invasions he decided to take his strategic planning to an offensive strike. From his intellect sprang the maneuvers and organization that turned the tides of war from 1793 to 1794. The basic idea was to have a massive army separated into several units that could move more quickly than the enemy and attack from the flanks rather than head on, which had led to resounding defeats before Carnot was elected to the Committee of Public Safety. This tactic was extremely successful against the more traditional tactics of existing European armies. It was his initiative to train the conscripts in the art of war and to place new recruits with experienced soldiers rather than having a massive volunteer army without any real idea of how to wage battle.\n\nOnce the problem of troop numbers had been solved, Carnot turned his administrative skills to the supplies that this massive army would need. Many of the munitions and supplies were in short supply: copper was lacking for guns so he ordered church bells seized in order to melt them down; saltpeter was lacking and he called chemistry to his aid; leather for boots was scarce so he demanded and secured new methods for tanning. He quickly organized the army and helped to turn the tide of the war. It added significantly to discontent with the course of the Revolution in still Bourbon-loyalist areas – such as the Vendée, which had broken out in open revolt 5 months earlier – but the government of the time considered it a success, and Carnot became known as the \"Organizer of Victory\". In autumn 1793, he took charge of French columns on the Northern Front, and contributed to Jean-Baptiste Jourdan's victory in the Battle of Wattignies.\n\nCarnot met Robespierre for the first time in Arras where he was assigned for military duty and shortly after Robespierre finished his legal studies. Both of them were members of the literary and singing \"Societe des Rosati.\" The group was founded in 1778 and was inspired by the works of Chapelle, La Fontaine, and Chanlieu. It was here where they became acquaintances and eventually friends. Robespierre preceded Carnot into the Academy of Arras entering in April 1784 while he entered three years later in 1787.\n\nWhile being an active member of the Committee of Public Safety in 1794, tensions between Carnot and Robespierre began to rise massively. During his time on the committee, which was heavily radical, Carnot signed a total of 43 decrees and drafted 18 of them. Most of them regarding military tactics and education. Despite leaning on Jacobin beliefs, Carnot was considered the \"conservative\" of his half. He was not an official member of the radical group and therefore took on his own independent beliefs in regards to many issues. One of these issues included Robespierre's proposal on an egalitarian social system in which he feverishly disagreed with.\n\nAlthough he had taken no steps to oppose the Reign of Terror, he and some other technocrats on the committee, including Robert Lindet and Louis-Bernard Guyton de Morveau, turned on Maximilien Robespierre and his allies during the Thermidorian Reaction by having him arrested. Robespierre was later killed along with 21 of his followers. Shortly after Robespierre's fall, Carnot was charged for his role during the time but the charges were quickly dismissed when he became a member of the Directory.\n\nIn 1795, Lazare appointed Napoleon Bonaparte as general in chief of the Army of Italy. He is known to be the only member of the Directory to have supported Napoleon during this time.\n\nIn 1800 Bonaparte appointed Carnot as Minister of War, and he served in that office at the time of the Battle of Marengo. In 1802 he voted against the establishment of Napoleon's Consular powers for life and the passing of the title to his children, for as Carnot said when speaking of the power necessary to govern a state \"If this power is the appendage of a hereditary family it becomes despotic.\"\n\nAfter Napoleon crowned himself emperor on 2 December 1804, Carnot's republican convictions precluded his acceptance of high office under the First French Empire, and he resigned from public life. Probably in response to the fall of the fortress of Vlissingen to the British during the Walcheren Campaign in 1809, Napoleon employed Carnot to write a treatise describing how fortifications could be improved, for the use of the École militaire de Metz. Building on the theories of the controversial engineer Montalembert, Carnot advanced ideas on how the long established bastioned system of fortification could be modified for close defense and to allow for counter attack by the besieged garrison.\n\nIn 1812, Carnot returned to office in defense of Napoleon during the disastrous invasion of Russia; he was assigned the defense of Antwerp against the Sixth Coalition – he only surrendered on the demand of the Count of Artois, who was the younger brother of Louis XVIII and later Charles X. He was later made a Count of the Empire by Napoleon as Lazare Nicolas Marguerite, Comte Carnot. During the Hundred Days, Carnot served as Minister of the Interior for Napoleon, and was exiled as a \"regicide\" during the White Terror after the Second Restoration during the reign of Louis XVIII.\n\nIn 1803 Carnot produced his \"Géométrie de position\". This work deals with projective rather than descriptive geometry.\nCarnot is responsible for initiating the use of cross-ratios:\n\"He was the first to introduce the cross (anharmonic) ratio of four points of a line taking account of its sign, thereby sharpening Pappus' concept. He then proved that this ratio is invariant for the four points obtained by cutting four lines of a pencil of lines with different secants. In this way, he established the harmonic properties of the complete quadrilateral.\" This approach to geometry was used by Karl von Staudt four decades later to set a new foundation to mathematics.\n\nThe Borda–Carnot equation of fluid dynamics and Carnot's theorem in plane geometry are named after him.\n\nPublished in 1810 under the title \"Traité de la Défense des Places Fortes\", his ideas on fortification were further developed in the third edition which was published in 1812. An English translation, \"A Treatise on the Defence of Fortified Places\" was published in 1814. Although few of his proposals were accepted by mainstream engineers, the Carnot wall, a detached wall at the foot of the , became a common feature in fortifications built in the mid-19th century.\n\nHe lived in Warsaw, and moved to Prussia, where he died in the city of Magdeburg. Carnot's remains were interred at the Panthéon in 1889, at the same time as those of Marie Victor de La Tour-Maubourg, Jean-Baptiste Baudin, and François Séverin Marceau-Desgraviers.\n\nCarnot survived all the phases of the French Revolution, from its beginnings in 1789 until the fall of Napoleon in 1815. On the social and political front, Carnot was the author of many reforms sought to improve the country. One of these was the proposal for compulsory public education for all citizens. He also penned a proposal for the new Constitution which included the \"Declaration of the Duties of the Citizens\" that held that there should be not only education but military service for all citizens of France between the ages of twenty and twenty-five. These proposals were in accordance with the Revolutionaries' thinking at the time, which held that men and women should be honored through ability and intelligence rather than through birthright, even though Carnot himself was nobly born.\n\n\nHe also published essays about engineering theory. \n\"Essai sur les machines en général\" won honorable mention from the Academie sur Science of Paris in 1780. It was revised and published in 1783. In this he outlined a mathematical theory of power transmission in mechanical systems. \nHis essay \"Principes fondamentaux de l'équilibre et du mouvement\" 1803 was a further revision and expansion of the earlier work. This was \"the first theoretical analysis of engineering mechanics\". In this \"he went on to analyze the movement of energy from one part of the system to another; he found that power is transmitted most efficiently, and the largest amount of useful work done, when friction, turbulence, and other energy wasting factors are kept to a minimum. This was an early and incomplete approach to the general law of conservation of energy.\"\nCarnot's son Nicolas Léonard Sadi Carnot was influenced by his father's work when he undertook his research into the thermal efficiency of steam engines.\n\nLazare Carnot's name is one of the 72 names inscribed on the Eiffel Tower.\n\n\n\n\n"}
{"id": "1252550", "url": "https://en.wikipedia.org/wiki?curid=1252550", "title": "Leroy P. Steele Prize", "text": "Leroy P. Steele Prize\n\nThe Leroy P. Steele Prizes are awarded every year by the American Mathematical Society, for distinguished research work and writing in the field of mathematics. Since 1993 there has been a formal division into three categories.\n\nThe prizes have been given since 1970, from a bequest of Leroy P. Steele, and were set up in honor of George David Birkhoff, William Fogg Osgood and William Caspar Graustein. The way the prizes are awarded was changed in 1976 and 1993, but the initial aim of honoring expository writing as well as research has been retained. The prizes of $5,000 are not given on a strict national basis, but relate to mathematical activity in the USA, and writing in English (originally, or in translation).\n\n\n\n\n"}
{"id": "24465861", "url": "https://en.wikipedia.org/wiki?curid=24465861", "title": "Littlewood–Paley theory", "text": "Littlewood–Paley theory\n\nIn harmonic analysis, a field within mathematics, Littlewood–Paley theory is a theoretical framework used to extend certain results about \"L\" functions to \"L\" functions for 1 < \"p\" < ∞. It is typically used as a substitute for orthogonality arguments which only apply to \"L\" functions when \"p\" = 2. One implementation involves studying a function by decomposing it in terms of functions with localized frequencies, and using the Littlewood–Paley \"g\"-function to compare it with its Poisson integral. The 1-variable case was originated by and developed further by Polish mathematicians A. Zygmund and J. Marcinkiewicz in the 1930s using complex function theory . E. M. Stein later extended the theory to higher dimensions using real variable techniques.\n\nLittlewood–Paley theory uses a decomposition of a function \"f\" into a sum of functions \"f\" with localized frequencies. There are several ways to construct such a decomposition; a typical method is as follows.\n\nIf \"f\" is a function on R, and \"ρ\" is a measurable set with characteristic function \"χ\", then \"f\" is defined to be given by\nwhere the \"hat\" is used to represent the Fourier transform. Informally, \"f\" is the piece of \"f\" whose frequencies lie in \"ρ\".\n\nIf Δ is a collection of measurable sets which (up to measure 0) are disjoint and have union the real line, then a well behaved function \"f\" can be written as a sum of functions \"f\" for \"ρ\" ∈ Δ.\n\nWhen Δ consists of the sets of the form\n\nfor \"k\" an integer, this gives a so-called \"dyadic decomposition\" of \"f\" : Σ \"f\".\n\nThere are many variations of this construction; for example, the characteristic function of a set used in the definition of \"f\" can be replaced by a smoother function.\n\nA key estimate of Littlewood–Paley theory is the Littlewood–Paley theorem, which bounds the size of the functions \"f\" in terms of the size of \"f\". There are many versions of this theorem corresponding to the different ways of decomposing \"f\". A typical estimate is to bound the \"L\" norm of (Σ |\"f\"|) by a multiple of the \"L\" norm of \"f\".\n\nIn higher dimensions it is possible to generalize this construction by replacing intervals with rectangles with sides parallel to the coordinate axes. Unfortunately these are rather special sets, which limits the applications to higher dimensions.\n\nThe \"g\" function is a non-linear operator on \"L\"(R) that can be used to control the \"L\" norm of a function \"f\" in terms of its Poisson integral.\nThe Poisson integral \"u\"(\"x\",\"y\") of \"f\" is defined for \"y\" > 0 by\n\nwhere the Poisson kernel \"P\" is given by \n\nThe Littlewood–Paley \"g\" function \"g\"(\"f\") is defined by \n\nA basic property of \"g\" is that it approximately preserves norms. More precisely, for 1 < \"p\" < ∞, the ratio of the \"L\" norms of \"f\" and \"g\"(\"f\") is bounded above and below by fixed positive constants depending on \"n\" and \"p\" but not on \"f\".\n\nOne early application of Littlewood–Paley theory was the proof that if \"S\" are the partial sums of the Fourier series of a periodic \"L\" function (\"p\" > 1) and \"n\" is a sequence satisfying \"n\"/\"n\" > \"q\" for some fixed \"q\" > 1, then the sequence \"S\" converges almost everywhere. This was later superseded by the Carleson–Hunt theorem showing that \"S\" itself converges almost everywhere.\n\nLittlewood–Paley theory can also be used to prove the Marcinkiewicz multiplier theorem.\n\n"}
{"id": "30775584", "url": "https://en.wikipedia.org/wiki?curid=30775584", "title": "Louis Leithold", "text": "Louis Leithold\n\nLouis Leithold (San Francisco, United States, 16 November 1924 – Los Angeles, 29 April 2005) was an American mathematician and teacher. He is best known for authoring \"The Calculus\", a classic textbook about calculus that changed the teaching methods for calculus in world high schools and universities. Known as \"a legend in AP calculus circles,\" Leithold was the mentor of Jaime Escalante, the Los Angeles high-school teacher whose story is the subject of the 1988 movie \"Stand and Deliver\".\n\nLeithold attained master's and doctorate degrees from the University of California, Berkeley. He went on to teach at Phoenix College (Arizona) (which has a math scholarship in his name), California State University, Los Angeles, the University of Southern California, Pepperdine University, and The Open University (UK). In 1968, Leithold published \"The Calculus\", a \"blockbuster best-seller\" which simplified the teaching of calculus.\n\nAt age 72, after his retirement from Pepperdine, he began teaching calculus at Malibu High School, in Malibu, California, drilling his students for the Advanced Placement Calculus, and achieving considerable success. He regularly assigned two hours of homework per night, and had two training sessions at his own house that ran Saturdays or Sundays from 9AM to 4PM before the AP test. His teaching methods were praised for their liveliness, and his love for the topic was well known. He also taught workshops for calculus teachers. One of the people he influenced was Jaime Escalante, who taught math to minority students at Garfield High School in East Los Angeles. Escalante's subsequent success as a teacher is portrayed in the 1988 film \"Stand and Deliver\".\n\nLeithold died of natural causes the week before his class (which he had been \"relentlessly drilling\" for eight months) was to take the AP exam; his students went on to receive top scores. A memorial service was held in Glendale, and a scholarship established in his name.\n"}
{"id": "516931", "url": "https://en.wikipedia.org/wiki?curid=516931", "title": "Map (mathematics)", "text": "Map (mathematics)\n\nIn mathematics, the term mapping, sometimes shortened to map, refers to either a function, often with some sort of special structure, or a morphism in category theory, which generalizes the idea of a function. There are also a few, less common uses in logic and graph theory.\n\nIn many branches of mathematics, the term map is used to mean a function, sometimes with a specific property of particular importance to that branch. For instance, a \"map\" is a \"continuous function\" in topology, a \"linear transformation\" in linear algebra, etc.\n\nSome authors, such as Serge Lang, use \"function\" only to refer to maps in which the codomain is a set of numbers (i.e. a subset of the fields R or C) and the term \"mapping\" for more general functions.\n\nSets of maps of special kinds are the subjects of many important theories: see for instance Lie group, mapping class group, permutation group.\n\nIn the theory of dynamical systems, a map denotes an evolution function used to create discrete dynamical systems. See also Poincaré map.\n\nA \"partial map\" is a \"partial function\", and a \"total map\" is a \"total function\". Related terms like \"domain\", \"codomain\", \"injective\", \"continuous\", etc. can be applied equally to maps and functions, with the same meaning. All these usages can be applied to \"maps\" as general functions or as functions with special properties.\n\nIn category theory, \"map\" is often used as a synonym for morphism or arrow, thus for something more general than a function. For example, morphisms formula_1, in a concrete category, in other words morphisms that can be viewed as functions, carry with them the information of both its domain (the source formula_2 of the morphism), but also its co-domain (the target formula_3). In the widely used definition of function formula_4, this is a subset of formula_5 consisting of all the pairs formula_6 for formula_7. In this sense, the function doesn't capture the information of which set formula_3 is used as the co-domain. Only the range formula_9 is determined by the function.\n\nIn formal logic, the term map is sometimes used for a \"functional predicate\", whereas a function is a model of such a predicate in set theory.\n\nIn graph theory, a map is a drawing of a graph on a surface without overlapping edges (an embedding). If the surface is a plane then a map is a planar graph, similar to a political map.\n\nIn the communities surrounding programming languages that treat functions as first-class citizens, a map often refers to the binary higher-order function that takes a function \"f\" and a list as arguments and returns , where .\n"}
{"id": "1921357", "url": "https://en.wikipedia.org/wiki?curid=1921357", "title": "Mcrypt", "text": "Mcrypt\n\nmcrypt is a replacement for the popular Unix crypt command.\ncrypt was a file encryption tool that used an algorithm very close to the\nWorld War II Enigma cipher. Mcrypt provides the same functionality but uses several modern algorithms such as AES. Libmcrypt, Mcrypt's companion, is a library of code which contains the actual encryption functions and provides an easy method for use. The last update to libmcrypt was in 2007, despite years of unmerged patches. These facts have led security experts to declare mcrypt abandonware and discourage its use in new development. Prefer ccrypt, libressl, and others. \n\nExamples of mcrypt usage in a Linux command line environment:\n\nIt implements numerous cryptographic algorithms, mostly block ciphers and stream ciphers, some of which falls under export restrictions in the United States. Algorithms include DES, Blowfish, ARCFOUR, Enigma, GOST, LOKI97, RC2, Serpent, Threeway, Twofish, WAKE, and XTEA.\n\n"}
{"id": "41244624", "url": "https://en.wikipedia.org/wiki?curid=41244624", "title": "Ocean general circulation model", "text": "Ocean general circulation model\n\nOcean general circulation models (OGCMs) are a particular kind of general circulation model to describe physical and thermodynamical processes in oceans. The oceanic general circulation is defined as the horizontal space scale and time scale larger than mesoscale (of order 100 km and 6 months). They depict oceans using a three-dimensional grid that include active thermodynamics and hence are most directly applicable to climate studies. They are the most advanced tools currently available for simulating the response of the global ocean system to increasing greenhouse gas concentrations. A hierarchy of OGCMs have been developed that include varying degrees of spatial coverage, resolution, geographical realism, process detail, etc.\n\nThe first generation of OGCMs assumed “rigid lid” to eliminate high-speed external gravity waves. According to CFL criteria without those fast waves, we can use a bigger time step, which is not so computationally expensive. But it also filtered those ocean tides and other waves having the speed of tsunamis. Within this assumption Bryan and co-worker Cox developed a 2D model, a 3D box model, and then a model of full circulation in GFDL, with variable density as well, for the world ocean with its complex coastline and bottom topography. The first application with specified global geometry was done in the early 1970s. Cox designed a 2° latitude-longitude grid with up to 12 vertical levels at each point.\n\nWith more and more research on ocean model, mesoscale phenomenon, e.g. most ocean currents have crossstream dimensions equal to Rossby radius of deformation, started to get more awareness. However, in order to analyze those eddies and currents in numerical models, we need grid spacing to be approximately 20 km in middle latitudes. Thanks to those faster computers and further filtering the equations in advance to remove internal gravity waves, those major currents and low-frequency eddies then can be resolved, one example is the three-layer quasi-geostrophic models designed by Holland. Meanwhile there are some model retaining internal gravity wave, for example one adiabatic layered model by O'Brien and his students, which did retain internal gravity waves so that equatorial and coastal problems involving these waves could be treated, led to an initial understanding of El Niño in terms of those waves.\n\nIn the late 1980s, simulations could finally be undertaken using the GFDL formulation with eddies marginally resolved over extensive domains and with observed winds and some atmospheric influence on density. Further more these simulation with high enough resolution such as the Southern Ocean south of latitude 25°, the North Atlantic, and the World Ocean without the Arctic provided first side-by-side comparison with data.\nEarly in the 1990s, for those large scale and eddies resolvable models the computer requirement for the 2D ancillary problem associated with the rigid lid approximation was becoming excessive. Further more, in order to predict tidal effects or compare height data from satellites, methods were developed to predict the height and pressure of the ocean surface directly. For example, one method is to treat the free surface and the vertically averaged velocity using many small steps in time for each single step of the full 3D model. Another method developed at Los Alamos National Laboratory solves the same 2D equations using an implicit method for the free surface. Both methods are quite efficient.\n\nOGCMs have many important applications: dynamical coupling with the atmosphere, sea ice, and land run-off that in reality jointly determine the oceanic boundary fluxes; transpire of biogeochemical materials; interpretation of the paleoclimate record;climate prediction for both natural variability and anthropogenic chafes; data assimilation and fisheries and other biospheric management. OGCMs play a critical role in Earth system model. They maintain the thermal balance as they transport energy from tropical to the polar latitudes. To analyze the feedback between ocean and atmosphere we need ocean model, which can initiate and amplify climate change on many different time scales, for instance, the interannual variability of El Niño and the potential modification of the major patterns for oceanic heat transport as a result of increasing greenhouse gases. Oceans are a kind of undersampled nature fluid system, so by using OGCMs we can fill in those data blank and improve understanding of basic processes and their interconnectedness, as well as to help interpret sparse observations. Even though, simpler models can be used to estimate climate response, only OGCM can be used conjunction with atmospheric general circulation model to estimate global climate change.\n\nMolecular friction rarely upsets the dominant balances (geostrophic and hydrostatic) in the ocean. With kinematic viscosities of v=10m s the Ekman number is several orders of magnitude smaller than unity; therefore, molecular frictional forces are certainly negligible for large-scale oceanic motions. Similar argument holds for the tracer equations, where the molecular thermodiffusivity and salt diffusivity lead to Reynolds number of negligible magnitude, which means the molecular diffusive time scales are much longer than advective time scale. So we can thus safely conclude that the direct effects of molecular processes are insignificant for large-scale. Yet the molecular friction is essential somewhere. The point is that large-scale motions in the ocean interacted with other scales by the nonlinearities in primitive equation. We can show that by Reynolds approach, which will leads to the closure problem. That means new variables arise at each level in the Reynolds averaging procedure. This leads to the need of parameterization scheme to account for those sub grid scale effects.\n\nHere is a schematic “family tree” of subgridscale (SGS) mixing schemes. Although there is considerable degree of overlap and inter relatedness among the huge variety of schemes in use today, several branch points maybe defined. Most importantly, the approaches for lateral and vertical subgridscale closure vary considerably. Filters and higher-order operators are used to remove small-scale noise that is numerically necessary. Those special dynamical parameterizations (topographic stress, eddy thickness diffusion and convection) are becoming available for certain processes. \nIn the vertical, the surface mixed layer (sml) has historically received special attention because of its important role in air-sea exchange. Now there are so many schemes can be chose from: Price-Weller-Pinkel, Pacanowksi and Philander, bulk, Mellor-Yamada and KPP (k-profile parameterization) schemes.\n\nAdaptive (non-constant) mixing length schemes are widely used for parameterization of both lateral and vertical mixing. In the horizontal, parameterizations dependent on the rates of stress and strain (Smagroinsky), grid spacing and Reynolds number (Re) have been advocated. In the vertical, vertical mixing as a function stability frequency (N^2) and/or Richardson number are historically prevalent. The rotated mixing tensors scheme is the one considering the angle of the principle direction of mixing, as for in the main thermocline, mixing along isopycnals dominates diapycnal mixing. There for the principle direction of mixing is neither strictly vertical nor purely horizontal, but a spatially variable mixture of the two.\n\nOGCMs and AGCMs have much in common, such as, the equations of motion and the numerical techniques. However, OGCMs have some unique features. For example, the atmosphere is forced thermally throughout its volume, the ocean is forced both thermally and mechanically primarily at its surface, in addition, the geometry of ocean basins is very complex. The boundary conditions are totally different. For ocean models, we need to consider those narrow but important boundary layers on nearly all bounding surfaces as well as within the oceanic interior. These boundary conditions on ocean flows are difficult to define and to parameterize, which results in a high computationally demand.\n\nOcean modeling is also strongly constrained by the existence in much of the world’s oceans of mesoscale eddies with time and space scales, respectively, of weeks to months and tens to hundreds of kilometers. Dynamically, these nearly geostrophic turbulent eddies are the oceanographic counterparts of the atmospheric synoptic scale. Nevertheless, there are important differences. First, ocean eddies are not perturbations on an energetic mean flow. They may play an important role in the poleward transport of heat. Second, they are relatively small in horizontal extent so that ocean climate models, which must have the same overall exterior dimensions as AGCMs, may require as much as 20 times the resolution as AGCM if the eddies are to be explicitly resolved.\n\nMost of the difference between OGCMs and AGCMs is that the data are sparser for OGCMs. Also, the data are not only sparse but also nonuniform and indirect.\n\nWe can classify ocean models according to different standards. For example, according to vertical ordinates we have geo-potential, isopycnal and topography-following models. According to horizontal discretizations we have unstaggered or staggered grids. According to methods of approximation we have finite difference and finite element models. There are three basic types of OGCMs: \n\n1. Idealized geometry models \n\nModels with idealized basin geometry have been used extensively in ocean modeling and have played a major role in the development of new modeling methodologies. They use a simplified geometry, offering a basin itself, while the distribution of winds and buoyancy force are generally chosen as simple functions of latitude.\n\n2. Basin-scale models\n\nTo compare OGCM results with observations we need realistic basin information instead of idealized data. However, if we only pay attention to local observation data, we don’t need to run whole global simulation, and by doing that we can save a lot of computational resources. \n\n3. Global models \n\nThis kind of model is the most computationally costly one. More experiments are needed as a preliminary step in constructing coupled Earth system models.\n\n"}
{"id": "2924960", "url": "https://en.wikipedia.org/wiki?curid=2924960", "title": "Odious number", "text": "Odious number\n\nIn number theory, an odious number is a positive integer that has an odd number of 1s in its binary expansion.\n\nThe first odious numbers are:\nThese numbers give the positions of the nonzero values in the Thue–Morse sequence.\n\nNon-negative integers that are not odious are called evil numbers.\n"}
{"id": "1509729", "url": "https://en.wikipedia.org/wiki?curid=1509729", "title": "Order of a kernel", "text": "Order of a kernel\n\nIn statistics, the order of a kernel is the degree of the first non-zero moment of a kernel.\n"}
{"id": "19696519", "url": "https://en.wikipedia.org/wiki?curid=19696519", "title": "Polyhedral combinatorics", "text": "Polyhedral combinatorics\n\nPolyhedral combinatorics is a branch of mathematics, within combinatorics and discrete geometry, that studies the problems of counting and describing the faces of convex polyhedra and higher-dimensional convex polytopes.\n\nResearch in polyhedral combinatorics falls into two distinct areas. Mathematicians in this area study the combinatorics of polytopes; for instance, they seek inequalities that describe the relations between the numbers of vertices, edges, and faces of higher dimensions in arbitrary polytopes or in certain important subclasses of polytopes, and study other combinatorial properties of polytopes such as their connectivity and diameter (number of steps needed to reach any vertex from any other vertex). Additionally, many computer scientists use the phrase “polyhedral combinatorics” to describe research into precise descriptions of the faces of certain specific polytopes (especially 0-1 polytopes, whose vertices are subsets of a hypercube) arising from integer programming problems.\n\nA \"face\" of a convex polytope \"P\" may be defined as the intersection of \"P\" and a closed halfspace \"H\" such that the boundary of \"H\" contains no interior point of \"P\". The dimension of a face is the dimension of this hull. The 0-dimensional faces are the vertices themselves, and the 1-dimensional faces (called \"edges\") are line segments connecting pairs of vertices. Note that this definition also includes as faces the empty set and the whole polytope \"P\". If \"P\" itself has dimension \"d\", the faces of \"P\" with dimension \"d\" − 1 are called \"facets\" of \"P\" and the faces with dimension \"d\" − 2 are called \"ridges\". The faces of \"P\" may be partially ordered by inclusion, forming a face lattice that has as its top element \"P\" itself and as its bottom element the empty set.\n\nA key tool in polyhedral combinatorics is the \"ƒ-vector\" of a polytope, the vector (\"f\", \"f\", ..., \"f\") where \"f\" is the number of \"i\"-dimensional features of the polytope. For instance, a cube has eight vertices, twelve edges, and six facets, so its ƒ-vector is (8,12,6). The dual polytope has a ƒ-vector with the same numbers in the reverse order; thus, for instance, the regular octahedron, the dual to a cube, has the ƒ-vector (6,12,8). Configuration matrices include the f-vectors of regular polytopes as diagonal elements.\n\nThe \"extended ƒ-vector\" is formed by concatenating the number one at each end of the ƒ-vector, counting the number of objects at all levels of the face lattice; on the left side of the vector, \"f\" = 1 counts the empty set as a face, while on the right side, \"f\" = 1 counts \"P\" itself.\nFor the cube the extended ƒ-vector is (1,8,12,6,1) and for the octahedron it is (1,6,12,8,1). Although the vectors for these example polyhedra are unimodal (the coefficients, taken in left to right order, increase to a maximum and then decrease), there are higher-dimensional polytopes for which this is not true.\n\nFor simplicial polytopes (polytopes in which every facet is a simplex), it is often convenient to transform these vectors, producing a different vector called the \"h\"-vector. If we interpret the terms of the ƒ-vector (omitting the final 1) as coefficients of a polynomial ƒ(\"x\") = Σ\"fx\" (for instance, for the octahedron this gives the polynomial ƒ(\"x\") = \"x\" + 6\"x\" + 12\"x\" + 8), then the \"h\"-vector lists the coefficients of the polynomial \"h\"(\"x\") = ƒ(\"x\" − 1) (again, for the octahedron, \"h\"(\"x\") = \"x\" + 3\"x\" + 3\"x\" + 1). As Ziegler writes, “for various problems about simplicial polytopes, \"h\"-vectors are a much more convenient and concise way to encode the information about the face numbers than ƒ-vectors.”\n\nThe most important relation among the coefficients of the ƒ-vector of a polytope is Euler's formula Σ(−1)\"f\" = 0, where the terms of the sum range over the coefficients of the extended ƒ-vector. In three dimensions, moving the two 1's at the left and right ends of the extended ƒ-vector (1, \"v\", \"e\", \"f\", 1) to the right hand side of the equation transforms this identity into the more familiar form \"v\" − \"e\" + \"f\" = 2. From the fact that each facet of a three-dimensional polyhedron has at least three edges, it follows by double counting that 2\"e\" ≥ 3\"f\", and using this inequality to eliminate \"e\" and \"f\" from Euler's formula leads to the further inequalities \"e\" ≤ 3\"v\" − 6 and \"f\" ≤ 2\"v\" − 4. By duality, \"e\" ≤ 3\"f\" − 6 and \"v\" ≤ 2\"f\" − 4. It follows from Steinitz's theorem that any 3-dimensional integer vector satisfying these equalities and inequalities is the ƒ-vector of a convex polyhedron.\n\nIn higher dimensions, other relations among the numbers of faces of a polytope become important as well, including the Dehn–Sommerville equations which, expressed in terms of \"h\"-vectors of simplicial polytopes, take the simple form \"h\" = \"h\" for all \"k\". The instance of these equations with \"k\" = 0 is equivalent to Euler's formula but for \"d\" > 3 the other instances of these equations are linearly independent of each other and constrain the \"h\"-vectors (and therefore also the ƒ-vectors) in additional ways.\n\nAnother important inequality on polytope face counts is given by the upper bound theorem, first proven by , which states that a \"d\"-dimensional polytope with \"n\" vertices can have at most as many faces of any other dimension as a neighborly polytope with the same number of vertices:\nwhere the asterisk means that the final term of the sum should be halved when \"d\" is even. Asymptotically, this implies that there are at most formula_2 faces of all dimensions.\n\nEven in four dimensions, the set of possible ƒ-vectors of convex polytopes does not form a convex subset of the four-dimensional integer lattice, and much remains unknown about the possible values of these vectors.\n\nAlong with investigating the numbers of faces of polytopes, researchers have studied other combinatorial properties of them, such as descriptions of the graphs obtained from the vertices and edges of polytopes (their 1-skeleta).\n\nBalinski's theorem states that the graph obtained in this way from any \"d\"-dimensional convex polytope is \"d\"-vertex-connected. In the case of three-dimensional polyhedra, this property and planarity may be used to exactly characterize the graphs of polyhedra: Steinitz's theorem states that \"G\" is the skeleton of a three-dimensional polyhedron if and only if \"G\" is a 3-vertex-connected planar graph.\n\nA theorem of (previously conjectured by Micha Perles) states that one can reconstruct the face structure of a simple polytope from its graph. That is, if a given undirected graph is the skeleton of a simple polytope, there is only one polytope (up to combinatorial equivalence) for which this is true. This is in sharp contrast with (non-simple) neighborly polytopes whose graph is a complete graph; there can be many different neighborly polytopes for the same graph. Another proof of this theorem based on unique sink orientations was given by , and showed how to use this theorem to derive a polynomial time algorithm for reconstructing the face lattices of simple polytopes from their graphs. However, testing whether a given graph or lattice can be realized as the face lattice of a simple polytope is equivalent (by polarity) to realization of simplicial polytopes, which was shown to be complete for the existential theory of the reals by .\n\nIn the context of the simplex method for linear programming, it is important to understand the diameter of a polytope, the minimum number of edges needed to reach any vertex by a path from any other vertex. The system of linear inequalities of a linear program define facets of a polytope representing all feasible solutions to the program, and the simplex method finds the optimal solution by following a path in this polytope. Thus, the diameter provides a lower bound on the number of steps this method requires. The Hirsch conjecture, now disproved, suggested a strong bound on how large the diameter could be. Weaker (quasi-polynomial) upper bounds on the diameter are known, as well as proofs of the Hirsch conjecture for special classes of polytope.\n\nDeciding whether the number of vertices of a given polytope is bounded by some natural number \"k\" is a computationally difficult problem and complete for the complexity class PP.\n\nIt is important in the context of cutting-plane methods for integer programming to be able to describe accurately the facets of polytopes that have vertices corresponding to the solutions of combinatorial optimization problems. Often, these problems have solutions that can be described by binary vectors, and the corresponding polytopes have vertex coordinates that are all zero or one.\n\nAs an example, consider the Birkhoff polytope, the set of \"n\" × \"n\" matrices that can be formed from convex combinations of permutation matrices. Equivalently, its vertices can be thought of as describing all perfect matchings in a complete bipartite graph, and a linear optimization problem on this polytope can be interpreted as a bipartite minimum weight perfect matching problem. The \"Birkhoff–von Neumann theorem\" states that this polytope can be described by two types of linear inequality or equality. First, for each matrix cell, there is a constraint that this cell has a non-negative value. And second, for each row or column of the matrix, there is a constraint that the sum of the cells in that row or column equal one. The row and column constraints define a linear subspace of dimension \"n\" − 2\"n\" + 1 in which the Birkhoff polytope lies, and the non-negativity constraints define facets of the Birkhoff polytope within that subspace.\n\nHowever, the Birkhoff polytope is unusual in that a complete description of its facets is available. For many other 0-1 polytopes, there are exponentially many or superexponentially many facets, and only partial descriptions of their facets are available.\n\n"}
{"id": "59217", "url": "https://en.wikipedia.org/wiki?curid=59217", "title": "Quadratic formula", "text": "Quadratic formula\n\nIn elementary algebra, the quadratic formula is the solution of the quadratic equation. There are other ways to solve the quadratic equation instead of using the quadratic formula, such as factoring, completing the square, or graphing. Using the quadratic formula is often the most convenient way.\n\nThe general quadratic equation is\n\nHere represents an unknown, while , , and are constants with not equal to 0. One can verify that the quadratic formula satisfies the quadratic equation by inserting the former into the latter. With the above parameterization, the quadratic formula is: \n\nEach of the solutions given by the quadratic formula is called a root of the quadratic equation. Geometrically, these roots represent the values at which \"any\" parabola, explicitly given as , crosses the -axis. As well as being a formula that will yield the zeros of any parabola, the quadratic formula will give the axis of symmetry of the parabola, and it can be used to immediately determine how many real zeros the quadratic equation has.\n\nThe earliest methods for solving quadratic equations were geometric. Babylonian cuneiform tablets contain problems reducible to solving quadratic equations. The Egyptian Berlin Papyrus, dating back to the Middle Kingdom (2050 BC to 1650 BC), contains the solution to a two-term quadratic equation.\n\nThe Greek mathematician Euclid (circa 300 BC) used geometric methods to solve quadratic equations in Book 2 of his \"Elements\", an influential mathematical treatise. Rules for quadratic equations appear in the Chinese \"The Nine Chapters on the Mathematical Art\" circa 200 BC. In his work \"Arithmetica\", the Greek mathematician Diophantus (circa 250 BC) solved quadratic equations with a method more recognizably algebraic than the geometric algebra of Euclid. His solution gives only one root, even when both roots are positive.\n\nThe Indian mathematician Brahmagupta (597–668 AD) explicitly described the quadratic formula in his treatise \"Brāhmasphuṭasiddhānta\" published in 628 AD, but written in words instead of symbols. His solution of the quadratic equation was as follows: \"To the absolute number multiplied by four times the [coefficient of the] square, add the square of the [coefficient of the] middle term; the square root of the same, less the [coefficient of the] middle term, being divided by twice the [coefficient of the] square is the value.\"\nThis is equivalent to:\n\nThe 9th-century Persian mathematician Muḥammad ibn Mūsā al-Khwārizmī solved quadratic equations algebraically. The quadratic formula covering all cases was first obtained by Simon Stevin in 1594. In 1637 René Descartes published \"La Géométrie\" containing special cases of the quadratic formula in the form we know today. The first appearance of the general solution in the modern mathematical literature appeared in an 1896 paper by Henry Heaton.\n\nThe quadratic formula can be derived with a simple application of technique of completing the square. The two derivations are as follows:\n\nDivide the quadratic equation by , which is allowed because is non-zero:\n\nSubtract from both sides of the equation, yielding:\n\nThe quadratic equation is now in a form to which the method of completing the square can be applied. Thus, add a constant to both sides of the equation such that the left hand side becomes a complete square.\n\nwhich produces:\n\nAccordingly, after rearranging the terms on the right hand side to have a common denominator, we obtain:\n\nThe square has thus been completed. Taking the square root of both sides yields the following equation:\n\nIsolating gives the quadratic formula:\n\nThe plus-minus symbol \"±\" indicates that both\n\nare solutions of the quadratic equation. There are many alternatives of this derivation with minor differences, mostly concerning the manipulation of .\n\nAn alternative way to write the quadratic formula is:\n\nwhich may be simplified to:\n\nThis version of the formula is convenient when complex roots are acceptable. Then the expression outside the square root will be the real part and the square root expression will be the imaginary part. The expression inside the square root is a discriminant.\n\nSome sources, particularly older ones, use alternative parameterizations of the quadratic equation such as or , where has a magnitude one half of the more common one. These result in slightly different forms for the solution, but are otherwise equivalent.\n\nA lesser known quadratic formula, as used in Muller's method, and which can be found from Vieta's formulas, provides the same roots via the equation:\n\nThe majority of algebra texts published over the last several decades teach completing the square using the sequence presented earlier: (1) divide each side by to make the equation monic, (2) rearrange, (3) then add to both sides to complete the square.\n\nAs pointed out by Larry Hoehn in 1975, completing the square can be accomplished by a different sequence that leads to a simpler sequence of intermediate terms: (1) multiply each side by , (2) rearrange, (3) then add .\n\nIn other words, the quadratic formula can be derived as follows: \n\nThis actually represents an ancient derivation of the quadratic formula and was known to the Hindus at least as far back as 1025. Compared with the derivation in standard usage, this alternate derivation is shorter, involves fewer computations with literal coefficients, avoids fractions until the last step, has simpler expressions, and uses simpler mathematics. As Hoehn states, \"it is easier 'to add the square of ' than it is 'to add the square of half the coefficient of the term'\".\n\nMany \"alternative derivations\" of the quadratic formula are in the literature. These derivations may be simpler than the standard completing the square method and represent interesting applications of other algebraic techniques or may offer insight into other areas of mathematics.\n\nAnother technique is solution by substitution. In this technique, we substitute into the quadratic to get:\n\nExpanding the result and then collecting the powers of produces:\n\nWe have not yet imposed a second condition on and , so we now choose so that the middle term vanishes. That is, or . Subtracting the constant term from both sides of the equation (to move it to the right hand side) and then dividing by gives:\n\nSubstituting for gives:\n\nTherefore,\n\nsubstituting provides the quadratic formula\n\nThe following method was used by many historical mathematicians:\n\nLet the roots of the standard quadratic equation be and . The derivation starts by recalling the identity:\n\nTaking the square root on both sides, we get:\n\nSince the coefficient , we can divide the standard equation by to obtain a quadratic polynomial having the same roots. Namely,\n\nFrom this we can see that the sum of the roots of the standard quadratic equation is given by , and the product of those roots is given by .\n\nHence the identity can be rewritten as:\n\nNow, \n\nSince , if we take\n\nthen we obtain\n\nand if we instead take\n\nthen we calculate that\n\nCombining these results by using the standard shorthand ±, we have that the solutions of the quadratic equation are given by:\n\nAn alternative way of deriving the quadratic formula is via the method of Lagrange resolvents, which is an early part of Galois theory.\nThis method can be generalized to give the roots of cubic polynomials and quartic polynomials, and leads to Galois theory, which allows one to understand the solution of algebraic equations of any degree in terms of the symmetry group of their roots, the Galois group.\n\nThis approach focuses on the \"roots\" more than on rearranging the original equation. Given a monic quadratic polynomial\n\nassume that it factors as\n\nExpanding yields\n\nwhere and .\n\nSince the order of multiplication does not matter, one can switch and and the values of and will not change: one can say that and are symmetric polynomials in and . In fact, they are the elementary symmetric polynomials – any symmetric polynomial in and can be expressed in terms of and The Galois theory approach to analyzing and solving polynomials is: given the coefficients of a polynomial, which are symmetric functions in the roots, can one \"break the symmetry\" and recover the roots? Thus solving a polynomial of degree is related to the ways of rearranging (\"permuting\") terms, which is called the symmetric group on letters, and denoted . For the quadratic polynomial, the only way to rearrange two terms is to swap them (\"transpose\" them), and thus solving a quadratic polynomial is simple.\n\nTo find the roots and , consider their sum and difference:\n\nThese are called the \"Lagrange resolvents\" of the polynomial; notice that one of these depends on the order of the roots, which is the key point. One can recover the roots from the resolvents by inverting the above equations:\n\nThus, solving for the resolvents gives the original roots.\n\nNow is a symmetric function in and , so it can be expressed in terms of and , and in fact as noted above. But is not symmetric, since switching and yields (formally, this is termed a group action of the symmetric group of the roots). Since is not symmetric, it cannot be expressed in terms of the coefficients and , as these are symmetric in the roots and thus so is any polynomial expression involving them. Changing the order of the roots only changes by a factor of −1, and thus the square is symmetric in the roots, and thus expressible in terms of and . Using the equation\n\nyields\n\nand thus\n\nIf one takes the positive root, breaking symmetry, one obtains:\n\nand thus\nThus the roots are\nwhich is the quadratic formula. Substituting yields the usual form for when a quadratic is not monic. The resolvents can be recognized as being the vertex, and is the discriminant (of a monic polynomial).\n\nA similar but more complicated method works for cubic equations, where one has three resolvents and a quadratic equation (the \"resolving polynomial\") relating and , which one can solve by the quadratic equation, and similarly for a quartic equation (degree 4), whose resolving polynomial is a cubic, which can in turn be solved. The same method for a quintic equation yields a polynomial of degree 24, which does not simplify the problem, and, in fact, solutions to quintic equations in general cannot be expressed using only roots.\n\nKnowing the value of in the functional extreme point makes it possible to solve only for the increase (or decrease) needed in to solve the quadratic equation. This method first uses differentiation to find the value at the extremum, called . We then solve for the value, , that ensures that . While this may not be the most intuitive method, it ensures that the mathematics is straightforward.\n\nSetting the above differential to zero will give us the extrema of the quadratic function\n\nWe define as follows:\n\nHere is the value of that solves the quadratic equation. The sum of and the variable of interest, , is plugged into the quadratic equation\n\nThe value of in the extreme point is then added to both sides of the equation\n\nThis gives the quadratic formula. This way one avoids the technique of completing the square and much more complicated math is not needed. Note this solution is very similar to solving deriving the formula by substitution.\n\nConsider the equation\nwhere formula_49 is a complex number and where \"a\", \"b\", and \"c\" are real numbers. Then\nThis splits into two equations, the real part:\nand the imaginary part:\n\nAssuming that formula_53 then divide the second equation by \"y\":\nand solve for \"x\":\nSubstitute this value for \"x\" into the first equation and solve for \"y\":\n\nSince formula_49, then\nEven though \"y\" was assumed to be non-zero, this last formula works for any roots of the original equation, whereas assuming that formula_59 turns out to be of not much help (trivial and circular).\n\nIn terms of coordinate geometry, a parabola is a curve whose -coordinates are described by a second-degree polynomial, i.e. any equation of the form:\n\nwhere represents the polynomial of degree 2 and and are constant coefficients whose subscripts correspond to their respective term's degree. The geometrical interpretation of the quadratic formula is that it defines the points on the -axis where the parabola will cross the axis. Additionally, if the quadratic formula was looked at as two terms,\n\nthe axis of symmetry appears as the line . The other term, , gives the distance the zeros are away from the axis of symmetry, where the plus sign represents the distance to the right, and the minus sign represents the distance to the left. \n\nIf this distance term were to decrease to zero, the value of the axis of symmetry would be the value of the only zero, that is, there is only one possible solution to the quadratic equation. Algebraically, this means that , or simply (where the left-hand side is referred to as the \"discriminant\"). This is one of three cases, where the discriminant indicates how many zeros the parabola will have. If the discriminant is positive, the distance would be non-zero, and there will be two solutions. However, there is also the case where the discriminant is less than zero, and this indicates the distance will be \"imaginary\" or some multiple of the complex unit , where and the parabola's zeros will be complex numbers. The complex roots will be complex conjugates, where the real part of the complex roots will be the value of the axis of symmetry. There will be no real values of where the parabola crosses the -axis.\n\nIf the constants , , and/or are not unitless, then the units of must be equal to the units of , due to the requirement that and agree on their units. Furthermore, by the same logic, the units of must be equal to the units of , which can be verified without solving for . This can be a powerful tool for verifying that a quadratic expression of physical quantities has been set up correctly, prior to solving it.\n\n"}
{"id": "5884024", "url": "https://en.wikipedia.org/wiki?curid=5884024", "title": "Ramanujan's sum", "text": "Ramanujan's sum\n\nIn number theory, a branch of mathematics, Ramanujan's sum, usually denoted \"c\"(\"n\"), is a function of two positive integer variables \"q\" and \"n\" defined by the formula:\n\nwhere (\"a\", \"q\") = 1 means that \"a\" only takes on values coprime to \"q\".\n\nSrinivasa Ramanujan mentioned the sums in a 1918 paper. In addition to the expansions discussed in this article, Ramanujan's sums are used in the proof of Vinogradov's theorem that every sufficiently-large odd number is the sum of three primes.\n\nFor integers \"a\" and \"b\", formula_2 is read \"\"a\" divides \"b\" and means that there is an integer \"c\" such that \"b\" = \"ac\". Similarly, formula_3 is read \"a\" does not divide \"b\"\". The summation symbol\n\nmeans that \"d\" goes through all the positive divisors of \"m\", e.g.\n\nformula_6 is the greatest common divisor,\n\nformula_7 is Euler's totient function,\n\nformula_8 is the Möbius function, and\n\nformula_9 is the Riemann zeta function.\n\nThese formulas come from the definition, Euler's formula formula_10 and elementary trigonometric identities.\n\nand so on (, , , .., ...) They show that \"c\"(\"n\") is always real.\n\nLet formula_12 Then is a root of the equation . Each of its powers,\n\nis also a root. Therefore, since there are \"q\" of them, they are all of the roots. The numbers formula_14 where 1 ≤ \"n\" ≤ \"q\" are called the \"q\"-th roots of unity. is called a primitive \"q\"-th root of unity because the smallest value of \"n\" that makes formula_15 is \"q\". The other primitive \"q\"-th roots of unity are the numbers formula_16 where (\"a\", \"q\") = 1. Therefore, there are φ(\"q\") primitive \"q\"-th roots of unity.\n\nThus, the Ramanujan sum \"c\"(\"n\") is the sum of the \"n\"-th powers of the primitive \"q\"-th roots of unity.\n\nIt is a fact that the powers of are precisely the primitive roots for all the divisors of \"q\".\n\nExample. Let \"q\" = 12. Then\n\nTherefore, if\n\nis the sum of the \"n\"-th powers of all the roots, primitive and imprimitive,\n\nand by Möbius inversion,\n\nIt follows from the identity \"x\" − 1 = (\"x\" − 1)(\"x\" + \"x\" + ... + \"x\" + 1) that\n\nand this leads to the formula\n\npublished by Kluyver in 1906.\n\nThis shows that \"c\"(\"n\") is always an integer. Compare it with the formula\n\nIt is easily shown from the definition that \"c\"(\"n\") is multiplicative when considered as a function of \"q\" for a fixed value of \"n\": i.e.\n\nFrom the definition (or Kluyver's formula) it is straightforward to prove that, if \"p\" is a prime number,\n\nand if \"p\" is a prime power where \"k\" > 1,\n\nThis result and the multiplicative property can be used to prove\n\nThis is called von Sterneck's arithmetic function. The equivalence of it and Ramanujan's sum is due to Hölder.\n\nFor all positive integers \"q\",\n\nFor a fixed value of \"q\" the absolute value of the sequence \n\nfor a fixed value of \"n\" the absolute value of the sequence \n\nIf \"q\" > 1\n\nLet \"m\", \"m\" > 0, \"m\" = lcm(\"m\", \"m\"). Then Ramanujan's sums satisfy an orthogonality property:\n\nLet \"n\", \"k\" > 0. Then\nknown as the Brauer - Rademacher identity.\n\nIf \"n\" > 0 and \"a\" is any integer, we also have\ndue to Cohen.\n\nIf \"f\"(\"n\") is an arithmetic function (i.e. a complex-valued function of the integers or natural numbers), then a convergent infinite series of the form:\n\nor of the form:\n\nwhere the , is called a Ramanujan expansion of \"f\"(\"n\").\n\nRamanujan found expansions of some of the well-known functions of number theory. All of these results are proved in an \"elementary\" manner (i.e. only using formal manipulations of series and the simplest results about convergence).\n\nThe expansion of the zero function depends on a result from the analytic theory of prime numbers, namely that the series\n\nconverges to 0, and the results for \"r\"(\"n\") and \"r\"′(\"n\") depend on theorems in an earlier paper.\n\nAll the formulas in this section are from Ramanujan's 1918 paper. \nThe generating functions of the Ramanujan sums are Dirichlet series:\n\nis a generating function for the sequence \"c\"(1), \"c\"(2), ... where \"q\" is kept constant, and\n\nis a generating function for the sequence \"c\"(\"n\"), \"c\"(\"n\"), ... where \"n\" is kept constant.\n\nThere is also the double Dirichlet series\n\nσ(\"n\") is the divisor function (i.e. the sum of the \"k\"-th powers of the divisors of \"n\", including 1 and \"n\"). σ(\"n\"), the number of divisors of \"n\", is usually written \"d\"(\"n\") and σ(\"n\"), the sum of the divisors of \"n\", is usually written σ(\"n\").\n\nIf \"s\" > 0,\n\nand\n\nSetting \"s\" = 1 gives\n\nIf the Riemann hypothesis is true, and formula_52\n\n\"d\"(\"n\") = σ(\"n\") is the number of divisors of \"n\", including 1 and \"n\" itself.\n\nwhere γ = 0.5772... is the Euler–Mascheroni constant.\n\nEuler's totient function φ(\"n\") is the number of positive integers less than \"n\" and coprime to \"n\". Ramanujan defines a generalization of it, if\n\nis the prime factorization of \"n\", and \"s\" is a complex number, let\n\nso that \"φ\"(\"n\") = \"φ\"(\"n\") is Euler's function.\n\nHe proves that\n\nand uses this to show that\n\nLetting \"s\" = 1,\n\nNote that the constant is the inverse of the one in the formula for σ(\"n\").\n\nVon Mangoldt's function unless \"n\" = \"p\" is a power of a prime number, in which case it is the natural logarithm log \"p\".\n\nFor all \"n\" > 0,\n\nThis is equivalent to the prime number theorem.\n\n\"r\"(\"n\") is the number of way of representing \"n\" as the sum of 2\"s\" squares, counting different orders and signs as different (e.g., \"r\"(13) = 8, as 13 = (±2) + (±3) = (±3) + (±2).)\n\nRamanujan defines a function δ(\"n\") and references a paper in which he proved that \"r\"(\"n\") = δ(\"n\") for \"s\" = 1, 2, 3, and 4. For \"s\" > 4 he shows that δ(\"n\") is a good approximation to \"r\"(\"n\").\n\n\"s\" = 1 has a special formula:\n\nIn the following formulas the signs repeat with a period of 4.\n\nIf \"s\" ≡ 0 (mod 4),\n\nIf \"s\" ≡ 2 (mod 4),\n\nIf \"s\" ≡ 1 (mod 4) and \"s\" > 1,\n\nIf \"s\" ≡ 3 (mod 4),\n\nand therefore,\n\n\"r\"′(\"n\") is the number of ways \"n\" can be represented as the sum of 2\"s\" triangular numbers (i.e. the numbers 1, 3 = 1 + 2, 6 = 1 + 2 + 3, 10 = 1 + 2 + 3 + 4, 15, ...; the \"n\"-th triangular number is given by the formula \"n\"(\"n\" + 1)/2.)\n\nThe analysis here is similar to that for squares. Ramanujan refers to the same paper as he did for the squares, where he showed that there is a function δ′(\"n\") such that \"r\"′(\"n\") = δ′(\"n\") for \"s\" = 1, 2, 3, and 4, and that for \"s\" > 4, δ′(\"n\") is a good approximation to \"r\"′(\"n\").\n\nAgain, \"s\" = 1 requires a special formula:\n\nIf \"s\" is a multiple of 4,\n\nIf \"s\" is twice an odd number,\n\nIf \"s\" is an odd number and \"s\" > 1,\n\nTherefore,\n\nLet\n\nThen for ,\n\n\n\n\n"}
{"id": "334320", "url": "https://en.wikipedia.org/wiki?curid=334320", "title": "Rule of 72", "text": "Rule of 72\n\nIn finance, the rule of 72, the rule of 70 and the rule of 69.3 are methods for estimating an investment's doubling time. The rule number (e.g., 72) is divided by the interest percentage per period (usually years) to obtain the approximate number of periods required for doubling. Although scientific calculators and spreadsheet programs have functions to find the accurate doubling time, the rules are useful for mental calculations and when only a basic calculator is available.\n\nThese rules apply to exponential growth and are therefore used for compound interest as opposed to simple interest calculations. They can also be used for decay to obtain a halving time. The choice of number is mostly a matter of preference: 69 is more accurate for continuous compounding, while 72 works well in common interest situations and is more easily divisible.\nThere are a number of variations to the rules that improve accuracy. For periodic compounding, the \"exact\" doubling time for an interest rate of \"r\" percent per period is\nwhere \"t\" is the number of periods required. The formula above can be used for more than calculating the doubling time. If one wants to know the tripling time, for example, simply replace the constant 2 in the numerator with 3. As another example, if one wants to know the number of periods it takes for the initial value to rise by 50%, replace the constant 2 with 1.5.\n\nTo estimate the number of periods required to double an original investment, divide the most convenient \"rule-quantity\" by the expected growth rate, expressed as a percentage.\n\nSimilarly, to determine the time it takes for the value of money to halve at a given rate, divide the rule quantity by that rate.\n\n\nThe value 72 is a convenient choice of numerator, since it has many small divisors: 1, 2, 3, 4, 6, 8, 9, and 12. It provides a good approximation for annual compounding, and for compounding at typical rates (from 6% to 10%). The approximations are less accurate at higher interest rates.\n\nFor continuous compounding, 69 gives accurate results for any rate. This is because ln(2) is about 69.3%; see derivation below. Since daily compounding is close enough to continuous compounding, for most purposes 69, 69.3 or 70 are better than 72 for daily compounding. For lower annual rates than those above, 69.3 would also be more accurate than 72.\n\nAn early reference to the rule is in the \"Summa de arithmetica\" (Venice, 1494. Fol. 181, n. 44) of Luca Pacioli (1445–1514). He presents the rule in a discussion regarding the estimation of the doubling time of an investment, but does not derive or explain the rule, and it is thus assumed that the rule predates Pacioli by some time.\n\nRoughly translated:\nFor higher rates, a bigger numerator would be better (e.g., for 20%, using 76 to get 3.8 years would be only about 0.002 off, where using 72 to get 3.6 would be about 0.2 off). This is because, as above, the rule of 72 is only an approximation that is accurate for interest rates from 6% to 10%. For every three percentage points away from 8% the value 72 could be adjusted by 1.\n\nor for the same result, but simpler:\n\nThe Eckart–McHale second-order rule (the E-M rule) provides a multiplicative correction for the rule of 69.3 that is very accurate for rates from 0% to 20%. The rule of 69.3 is normally only accurate at the lowest end of interest rates, from 0% to about 5%. To compute the E-M approximation, simply multiply the rule of 69.3 result by 200/(200−\"r\") as follows:\n\nFor example, if the interest rate is 18%, the rule of 69.3 says \"t\" = 3.85 years. The E-M rule multiplies this by 200/(200−18), giving a doubling time of 4.23 years, where the actual doubling time at this rate is 4.19 years. (The E-M rule thus gives a closer approximation than the rule of 72.)\n\nNote that the numerator here is simply 69.3 times 200. As long as the product stays constant, the factors can be modified arbitrarily. The E-M rule could thus be written also as\n\nin order to keep the product mostly unchanged. In these variants, the multiplicative correction becomes 1 respectively for r=2 and r=8, the values for which the rule of 70 (respectively 72) is most precise.\n\nSimilarly, the third-order Padé approximant gives a more accurate answer over an even larger range of \"r\", but it has a slightly more complicated formula:\n\nFor periodic compounding, future value is given by:\nwhere formula_10 is the present value, formula_11 is the number of time periods, and formula_12 stands for the interest rate per time period.\n\nThe future value is double the present value when the following condition is met:\nThis equation is easily solved for formula_11:\nA simple rearrangement shows:\n\nIf \"r\" is small, then ln(1 + \"r\") approximately equals \"r\" (this is the first term in the Taylor series). That is, the latter term grows slowly when formula_12 is close to zero. \n\nCalling this latter term formula_18, the function formula_18 is shown to be accurate in the approximation of formula_11 for a small, positive interest rate when formula_21 (see derivation below). formula_22, and we therefore approximate time formula_11 as:\n\nWritten as a percentage:\n\nThis approximation increases in accuracy as the compounding of interest becomes continuous (see derivation below). formula_26 is formula_12 written as a percentage.\n\nIn order to derive the more precise adjustments presented above, it is noted that formula_28 is more closely approximated by formula_29 (using the second term in the Taylor series). formula_30 can then be further simplified by Taylor approximations:\n\nReplacing the \"R\" in \"R/200\" on the third line with 7.79 gives 72 on the numerator. This shows that the rule of 72 is most precise for periodically composed interests around 8%.\n\nAlternatively, the E-M rule is obtained if the second-order Taylor approximation is used directly.\n\nFor continuous compounding, the derivation is simpler and yields a more accurate rule:\n\n\n"}
{"id": "1487910", "url": "https://en.wikipedia.org/wiki?curid=1487910", "title": "Schwarz–Ahlfors–Pick theorem", "text": "Schwarz–Ahlfors–Pick theorem\n\nIn mathematics, the Schwarz–Ahlfors–Pick theorem is an extension of the Schwarz lemma for hyperbolic geometry, such as the Poincaré half-plane model.\n\nThe Schwarz–Pick lemma states that every holomorphic function from the unit disk \"U\" to itself, or from the upper half-plane \"H\" to itself, will not increase the Poincaré distance between points. The unit disk \"U\" with the Poincaré metric has negative Gaussian curvature −1. In 1938, Lars Ahlfors generalised the lemma to maps from the unit disk to other negatively curved surfaces:\n\nTheorem (Schwarz–Ahlfors–Pick). Let \"U\" be the unit disk with Poincaré metric formula_1; let \"S\" be a Riemann surface endowed with a Hermitian metric formula_2 whose Gaussian curvature is ≤ −1; let formula_3 be a holomorphic function. Then \nfor all formula_5\n\nA generalization of this theorem was proved by Shing-Tung Yau in 1973.\n"}
{"id": "25911217", "url": "https://en.wikipedia.org/wiki?curid=25911217", "title": "Set function", "text": "Set function\n\nIn mathematics, a set function is a function whose input is a set. The output is usually a number. Often the input is a set of real numbers, a set of points in Euclidean space, or a set of points in some measure space. \n\nExamples of set functions include:\n\n\n"}
{"id": "173196", "url": "https://en.wikipedia.org/wiki?curid=173196", "title": "Spin network", "text": "Spin network\n\nIn physics, a spin network is a type of diagram which can be used to represent states and interactions between particles and fields in quantum mechanics. From a mathematical perspective, the diagrams are a concise way to represent multilinear functions and functions between representations of matrix groups. The diagrammatic notation often simplifies calculation because simple diagrams may be used to represent complicated functions. \n\nRoger Penrose is credited with the invention of spin networks in 1971, although similar diagrammatic techniques existed before his time. Spin networks have been applied to the theory of quantum gravity by Carlo Rovelli, Lee Smolin, Jorge Pullin, Rodolfo Gambini and others. \n\nSpin networks can also be used to construct a particular functional on the space of connections which is invariant under local gauge transformations.\n\nA spin network, as described in Penrose (1971), is a kind of diagram in which each line segment represents the world line of a \"unit\" (either an elementary particle or a compound system of particles). Three line segments join at each vertex. A vertex may be interpreted as an event in which either a single unit splits into two or two units collide and join into a single unit. Diagrams whose line segments are all joined at vertices are called \"closed spin networks\". Time may be viewed as going in one direction, such as from the bottom to the top of the diagram, but for closed spin networks the direction of time is irrelevant to calculations.\n\nEach line segment is labelled with an integer called a spin number. A unit with spin number \"n\" is called an \"n\"-unit and has angular momentum \"nħ/2\", where \"ħ\" is the reduced Planck constant. For bosons, such as photons and gluons, \"n\" is an even number. For fermions, such as electrons and quarks, \"n\" is odd.\n\nGiven any closed spin network, a non-negative integer can be calculated which is called the \"norm\" of the spin network. Norms can be used to calculate the probabilities of various spin values. A network whose norm is zero has zero probability of occurrence. The rules for calculating norms and probabilities are beyond the scope of this article. However, they imply that for a spin network to have nonzero norm, two requirements must be met at each vertex. Suppose a vertex joins three units with spin numbers \"a\", \"b\", and \"c\". Then, these requirements are stated as:\nFor example, \"a\" = 3, \"b\" = 4, \"c\" = 6 is impossible since 3 + 4 + 6 = 13 is odd, and \"a\" = 3, \"b\" = 4, \"c\" = 9 is impossible since 9 > 3 + 4. However, \"a\" = 3, \"b\" = 4, \"c\" = 5 is possible since 3 + 4 + 5 = 12 is even, and the triangle inequality is satisfied. \nSome conventions use labellings by half-integers, with the condition that the sum \"a\" + \"b\" + \"c\" must be a whole number.\n\nMore formally, a spin network is a (directed) graph whose edges are associated with irreducible representations of a compact Lie group and whose vertices are associated with intertwiners of the edge representations adjacent to it.\n\nA spin network, immersed into a manifold, can be used to define a functional on the space of connections on this manifold. One computes holonomies of the connection along every link (closed path) of the graph, determines representation matrices corresponding to every link, multiplies all matrices and intertwiners together, and contracts indices in a prescribed way. A remarkable feature of the resulting functional is that it is invariant under local gauge transformations.\n\nIn loop quantum gravity (LQG), a spin network represents a \"quantum state\" of the gravitational field on a 3-dimensional hypersurface. The set of all possible spin networks (or, more accurately, \"s-knots\" - that is, equivalence classes of spin networks under diffeomorphisms) is countable; it constitutes a basis of LQG Hilbert space.\n\nOne of the key results of loop quantum gravity is quantization of areas: the operator of the area \"A\" of a two-dimensional surface Σ should have a discrete spectrum. Every spin network is an eigenstate of each such operator, and the area eigenvalue equals\n\nwhere the sum goes over all intersections \"i\" of Σ with the spin network. In this formula,\n\nAccording to this formula, the lowest possible non-zero eigenvalue of the area operator corresponds to a link that carries spin 1/2 representation. Assuming an Immirzi parameter on the order of 1, this gives the smallest possible measurable area of ~10 cm.\n\nThe formula for area eigenvalues becomes somewhat more complicated if the surface is allowed to pass through the vertices, as with anomalous diffusion models. Also, the eigenvalues of the area operator \"A\" are constrained by ladder symmetry.\n\nSimilar quantization applies to the volume operator. The volume of 3D submanifold that contains part of spin network is given by a sum of contributions from each node inside it. One can think that every node in a spin network is an elementary \"quantum of volume\" and every link is a \"quantum of area\" surrounding this volume.\n\nSimilar constructions can be made for general gauge theories with a compact Lie group G and a connection form. This is actually an exact duality over a lattice. Over a manifold however, assumptions like diffeomorphism invariance are needed to make the duality exact (smearing Wilson loops is tricky). Later, it was generalized by Robert Oeckl to representations of quantum groups in 2 and 3 dimensions using the Tannaka–Krein duality.\n\nMichael A. Levin and Xiao-Gang Wen have also defined string-nets using tensor categories that are objects very similar to spin networks. However the exact connection with spin networks is not clear yet. String-net condensation produces topologically ordered states in condensed matter.\n\nIn mathematics, spin networks have been used to study skein modules and character varieties, which correspond to spaces of connections.\n\n\n\n\n"}
{"id": "36549256", "url": "https://en.wikipedia.org/wiki?curid=36549256", "title": "Sumario Compendioso", "text": "Sumario Compendioso\n\nThe Sumario Compendioso was the first mathematics book published in the New World. The book was published in Mexico City in 1556 by a clergyman Juan Diez.\n\nThe book has been digitized and is available on the Internet.\n\nBefore the Digital Age the only four known surviving copies were preserved at the Huntington Library, San Marino, CA, the British Library, London, Duke University Library, and the University of Salamanca in Spain.\n\nIn his book \"The Math Book\", Clifford A. Pickover provided the following information about \"Sumario Compendioso\":\n"}
{"id": "3958696", "url": "https://en.wikipedia.org/wiki?curid=3958696", "title": "Tatyana Afanasyeva", "text": "Tatyana Afanasyeva\n\nTatyana Alexeyevna Afanasyeva () (Kiev, 19 November 1876 – Leiden, 14 April 1964) (also known as Tatiana Ehrenfest-Afanaseva or spelled Afanassjewa) was a Russian/Dutch mathematician and physicist who made contributions to the fields of statistical mechanics and statistical thermodynamics. On 21 December 1904, she married Austrian physicist Paul Ehrenfest (1880–1933). They had two daughters and two sons; one daughter, Tatyana Pavlovna Ehrenfest, also became a mathematician.\n\nAfanasyeva was born in Kiev, Ukraine, then part of the Russian Empire. Her father was Alexander Afanassjev, a chief engineer on the Imperial Railways, who would bring Tatyana on his travels around the Russian Empire. Her father died while she was still young, so she moved to St Petersburg in Russia to live with her aunt Sonya, and uncle Peter Afanassjev, a professor at the St Petersburg Polytechnic Institute.\n\nTatyana attended normal school in St Petersburg with a specialty in mathematics and science. At the time, women were not allowed to attend universities in Russian territory, so after graduating from normal school, Tatyana began studying mathematics and physics at the Women's University in St Petersburg under Orest Chvolson. In 1902, she transferred to University of Göttingen in Germany to continue her studies with Felix Klein and David Hilbert.\n\nAt the University of Gottingen, Tatyana met Paul Ehrenfest. When Ehrenfest discovered that Tatyana could not attend a mathematics club meeting, he argued with the school to have the rule changed. A friendship developed between the two, and they married in 1904, later returned to St Petersburg in 1907. Under Russian law, marriage was not allowed between two people of different religions. Since Tatyana was a Russian Orthodox and Ehrenfest was Jewish, they both decided to officially renounce their religions in order to remain married.\n\nIn 1912 they moved to Leiden in the Netherlands, where Paul Ehrenfest was appointed to succeed H.A. Lorentz as professor at the University of Leiden, and where the couple lived throughout their career.\n\nTatyana collaborated closely with her husband, most famously on their classic review of the statistical mechanics of Boltzmann. \"The Conceptual Foundations of the Statistical Approach in Mechanics,\" by Paul and Tatyana Ehrenfest was originally published in 1911 as an article for the German \"Encyclopedia of Mathematical Sciences\", and has since been translated and republished.\n\nShe published many papers on various topics such as randomness and entropy, and teaching geometry to children.\n\n\n\n"}
{"id": "6553354", "url": "https://en.wikipedia.org/wiki?curid=6553354", "title": "Tightness of measures", "text": "Tightness of measures\n\nIn mathematics, tightness is a concept in measure theory. The intuitive idea is that a given collection of measures does not \"escape to infinity.\"\n\nLet formula_1 be a topological space, and let formula_2 be a σ-algebra on formula_3 that contains the topology formula_4. (Thus, every open subset of formula_3 is a measurable set and formula_2 is at least as fine as the Borel σ-algebra on formula_3.) Let formula_8 be a collection of (possibly signed or complex) measures defined on formula_2. The collection formula_8 is called tight (or sometimes uniformly tight) if, for any formula_11, there is a compact subset formula_12 of formula_3 such that, for all measures formula_14,\n\nwhere formula_16 is the total variation measure of formula_17. Very often, the measures in question are probability measures, so the last part can be written as\n\nIf a tight collection formula_8 consists of a single measure formula_17, then (depending upon the author) formula_17 may either be said to be a tight measure or to be an inner regular measure.\n\nIf formula_22 is an formula_3-valued random variable whose probability distribution on formula_3 is a tight measure then formula_22 is said to be a separable random variable or a Radon random variable.\n\nIf formula_3 is a metrisable compact space, then every collection of (possibly complex) measures on formula_3 is tight. This is not necessarily so for non-metrisable compact spaces. If we take formula_28 with its order topology, then there exists a measure formula_17 on it that is not inner regular. Therefore, the singleton formula_30 is not tight.\n\nIf formula_3 is a compact Polish space, then every probability measure on formula_3 is tight. Furthermore, by Prokhorov's theorem, a collection of probability measures on formula_3 is tight if and only if\nit is precompact in the topology of weak convergence.\n\nConsider the real line formula_34 with its usual Borel topology. Let formula_35 denote the Dirac measure, a unit mass at the point formula_36 in formula_34. The collection\n\nis not tight, since the compact subsets of formula_34 are precisely the closed and bounded subsets, and any such set, since it is bounded, has formula_40-measure zero for large enough formula_41. On the other hand, the collection\n\nis tight: the compact interval formula_43 will work as formula_12 for any formula_11. In general, a collection of Dirac delta measures on formula_46 is tight if, and only if, the collection of their supports is bounded.\n\nConsider formula_41-dimensional Euclidean space formula_46 with its usual Borel topology and σ-algebra. Consider a collection of Gaussian measures\n\nwhere the measure formula_50 has expected value (mean) formula_51 and covariance matrix formula_52. Then the collection formula_53 is tight if, and only if, the collections formula_54 and formula_55 are both bounded.\n\nTightness is often a necessary criterion for proving the weak convergence of a sequence of probability measures, especially when the measure space has infinite dimension. See\n\n\nA strengthening of tightness is the concept of exponential tightness, which has applications in large deviations theory. A family of probability measures formula_56 on a Hausdorff topological space formula_3 is said to be exponentially tight if, for any formula_11, there is a compact subset formula_12 of formula_3 such that\n\n"}
{"id": "383208", "url": "https://en.wikipedia.org/wiki?curid=383208", "title": "Time Cube", "text": "Time Cube\n\nTime Cube was a personal web page, founded in 1997 by the self-proclaimed \"wisest man on earth\", the late Otis Eugene \"Gene\" Ray. It was a self-published outlet for Ray's theory of everything, called \"Time Cube\", which claims that all modern sciences are participating in a worldwide conspiracy to teach lies, by omitting his theory's alleged truth that each day actually consists of four days. Alongside these statements, Ray described himself as a \"godlike being with superior intelligence who has absolute evidence and proof\" for his views. Ray asserted repeatedly and variously that \"academia\" had not taken Time Cube seriously.\n\nOtis Eugene Ray died at the age of 87, on March 18, 2015. Ray's website domain names expired in August 2015, and Time Cube was last archived by the Wayback Machine on January 12, 2016. (January 10–14) An accurate mirror of the site is currently being hosted and maintained as of 2018.\n\nThe Time Cube website had no home page or separate sections. It consisted of a single web page that contained a vertical column of centered body text of various sizes and colors.\n\nRay used cryptic language that included insults and \"non-sequitur\" lines such as \"Belly-Button Logic© Works. When Does Teenager Die? Adults Eat Teenagers Alive, No Record Of Their Death.\" The narrative weaved in and out of his metaphysical ideas with numerous unique digressions. In one paragraph he claimed that because his own wisdom \"so antiquates known knowledge\", a psychiatrist examining his behavior diagnosed him with schizophrenia.\n\nSome have claimed it is futile to analyze the text rationally, locate meaningful proofs in the text, or verify any evidence.\n\nRay's personal model of reality, called \"Time Cube\", states that all of modern physics and education is wrong, and argues that, among many other things, Greenwich Time is a global conspiracy. He utilizes various graphs (along with pictures of himself) that purport to show how each day is really four separate days—sunup, midday, sundown, and midnight (formerly morning, early afternoon, late afternoon, and evening)—occurring simultaneously.\n\nThe following quotation from the website illustrates the recurring theme:\nRay offered $1,000 or $10,000 to anyone who could prove his views wrong. Mike Hartwell of \"The Maine Campus\" wrote that any attempt to claim the prize would require convincing Ray that his theory was invalid. The proof would need to be framed in terms of his own model, thus deviating from any form of modern science. \"Even if you could pull that off\", Harwell said, \"Ray is probably broke\".\n\nRay spoke about Time Cube at the Massachusetts Institute of Technology in January 2002 as part of a student-organized extra-curricular event during the independent activities period. He repeated his $10,000 offer for professors to disprove his notions at the event; none attempted it. John C. Dvorak wrote in \"PC Magazine\" that \"Metasites that track crackpot sites often say this is the number one nutty site.\" He also characterized the site's content as \"endless blather\". Asked by Martin Sargent in 2003 how it felt to be an Internet celebrity, Ray stated that it was not a position he wanted, but something he felt he had to do as \"no writer or speaker understands the Time Cube\". Ray also spoke about Time Cube at the Georgia Institute of Technology in April 2005, in a speech in which he attacked the instruction offered by academics.\n\nA 2004 editorial in \"The Maine Campus\" student newspaper remarked upon what it called the site's \"subtle little racist ideologies\" which culminate in Ray describing racial integration as \"destroying all of the races\".\n\nIn 2005, Brett Hanover made \"Above God\" (also the name of one of Ray's websites which criticized the idea that God exists), a short documentary film about Ray and Time Cube, which won awards for Best Documentary at the Indie Memphis Film Festival and the Atlanta Underground Film Festival.\n\nRichard Janczarski, also known online as \"Cubehead\", was a self-professed disciple of Ray who styled himself the \n\"second-wisest human\". From 2004, Janczarski created the Time Cube fansite Cubic Awareness Online, and an accompanying forum. In 2007, he travelled from his native Australia to Florida to meet Ray. The pair had several disagreements after going their separate ways. Ray denounced Janczarski on his website after the latter’s visit. Janczarski took his own life on February 12, 2008.\n\n"}
