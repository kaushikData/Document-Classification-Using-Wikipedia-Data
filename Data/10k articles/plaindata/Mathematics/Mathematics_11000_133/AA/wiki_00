{"id": "867612", "url": "https://en.wikipedia.org/wiki?curid=867612", "title": "163 (number)", "text": "163 (number)\n\n163 (one hundred [and] sixty-three) is the natural number following 162 and preceding 164.\n\n163 is a strong prime in the sense that it is greater than the arithmetic mean of its two neighboring primes.\n\n163 is a lucky prime and a fortunate number.\n\n163 is a strictly non-palindromic number, since it is not palindromic in any base between base 2 and base 161.\n\nGiven 163, the Mertens function returns 0, it is the fourth prime with this property, the first three such primes are 2, 101 and 149.\n\n163 figures in an approximation of π, in which formula_1.\n\n163 figures in an approximation of \"e\", in which formula_2.\n\n163 is a Heegner number, the largest of the nine such numbers. That is, the ring of integers of the field formula_3 has unique factorization for formula_4. The only other such integers are\nformula_5. \n\n163 is the number of -independent McKay-Thompson series for the monster group. This fact about 163 might be a clue for understanding monstrous moonshine.\n\n163 is a permutable prime in base 12, which it is written as 117, the permutations of its digits are 171 and 711, the two numbers in base 12 is 229 and 1021 in base 10, both of them are primes.\n\nThe function formula_6 gives prime values for all values of formula_7 between 0 and 39, and for formula_8 approximately half of all values are prime. 163 appears as a result of solving formula_9, which gives formula_10.\n\nformula_11\nappears in the Ramanujan constant, since -163 is a quadratic nonresidue to modulo all the primes 3, 5, 7, ..., 37. In which formula_12 almost equals the integer 262537412640768744 = 640320 + 744. Martin Gardner famously asserted that this identity was exact in a 1975 April Fools' hoax in \"Scientific American\"; in fact the value is 262537412640768743.99999999999925007259...\n\n\n\n\n\n163 is also:\n\n\n\n"}
{"id": "454446", "url": "https://en.wikipedia.org/wiki?curid=454446", "title": "Abstraction (mathematics)", "text": "Abstraction (mathematics)\n\nAbstraction in mathematics is the process of extracting the underlying essence of a mathematical concept, removing any dependence on real world objects with which it might originally have been connected, and generalizing it so that it has wider applications or matching among other abstract descriptions of equivalent phenomena. Two of the most highly abstract areas of modern mathematics are category theory and model theory.\n\nMany areas of mathematics began with the study of real world problems, before the underlying rules and concepts were identified and defined as abstract structures. For example, geometry has its origins in the calculation of distances and areas in the real world; algebra started with methods of solving problems in arithmetic.\n\nAbstraction is an ongoing process in mathematics and the historical development of many mathematical topics exhibits a progression from the concrete to the abstract. Take the historical development of geometry as an example; the first steps in the abstraction of geometry were made by the ancient Greeks, with Euclid's Elements being the earliest extant documentation of the axioms of plane geometry—though Proclus tells of an earlier axiomatisation by Hippocrates of Chios. In the 17th century Descartes introduced Cartesian co-ordinates which allowed the development of analytic geometry. Further steps in abstraction were taken by Lobachevsky, Bolyai, Riemann, and Gauss who generalised the concepts of geometry to develop non-Euclidean geometries. Later in the 19th century mathematicians generalised geometry even further, developing such areas as geometry in n dimensions, projective geometry, affine geometry and finite geometry. Finally Felix Klein's \"Erlangen program\" identified the underlying theme of all of these geometries, defining each of them as the study of properties invariant under a given group of symmetries. This level of abstraction revealed connections between geometry and abstract algebra.\n\nThe advantages of abstraction are :\n\n\nOne disadvantage of abstraction is that highly abstract concepts can be difficult to learn. A degree of mathematical maturity and experience may be needed for conceptual assimilation of abstractions. One of the underlying principles of the Montessori approach to mathematics education is encouraging children to move from concrete examples to abstract thinking.\n\nBertrand Russell, in \"The Scientific Outlook\" (1931), writes that \"Ordinary language is totally unsuited for expressing what physics really asserts, since the words of everyday life are not sufficiently abstract. Only mathematics and mathematical logic can say as little as the physicist means to say.\"\n\n"}
{"id": "40138", "url": "https://en.wikipedia.org/wiki?curid=40138", "title": "Bacterial growth", "text": "Bacterial growth\n\nBacterial growth is the asexual reproduction, or cell division, of a bacterium into two daughter cells, in a process called binary fission. Providing no mutational event occurs, the resulting daughter cells are genetically identical to the original cell. Hence, bacterial growth occurs. Both daughter cells from the division do not necessarily survive. However, if the number surviving exceeds unity on average, the bacterial population undergoes exponential growth. The measurement of an exponential bacterial growth curve in batch culture was traditionally a part of the training of all microbiologists; the basic means requires bacterial enumeration (cell counting) by direct and individual (microscopic, flow cytometry), direct and bulk (biomass), indirect and individual (colony counting), or indirect and bulk (most probable number, turbidity, nutrient uptake) methods. Models reconcile theory with the measurements.\n\nIn autecological studies, the growth of bacteria (or other microorganisms, as protozoa, microalgae or yeasts) in batch culture can be modeled with four different phases: lag phase (A), log phase or exponential phase (B), stationary phase (C), and death phase (D).\nThis basic batch culture growth model draws out and emphasizes aspects of bacterial growth which may differ from the growth of macrofauna. It emphasizes clonality, asexual binary division, the short development time relative to replication itself, the seemingly low death rate, the need to move from a dormant state to a reproductive state or to condition the media, and finally, the tendency of lab adapted strains to exhaust their nutrients. In reality, even in batch culture, the four phases are not well defined. The cells do not reproduce in synchrony without explicit and continual prompting (as in experiments with stalked bacteria ) and their exponential phase growth is often not ever a constant rate, but instead a slowly decaying rate, a constant stochastic response to pressures both to reproduce and to go dormant in the face of declining nutrient concentrations and increasing waste concentrations.\n\nNear the end of the logarithmic phase of a batch culture, competence for natural genetic transformation may be induced, as in \"Bacillus subtilis\" and in other bacteria. Natural genetic transformation is a form of DNA transfer that appears to be an adaptation for repairing DNA damages.\n\nBatch culture is the most common laboratory growth method in which bacterial growth is studied, but it is only one of many. It is ideally spatially unstructured and temporally structured. The bacterial culture is incubated in a closed vessel with a single batch of medium. In some experimental regimes, some of the bacterial culture is periodically removed and added to fresh sterile medium. In the extreme case, this leads to the continual renewal of the nutrients. This is a chemostat, also known as continuous culture. It is ideally spatially unstructured and temporally unstructured, in a steady state defined by the rates of nutrient supply and bacterial growth. In comparison to batch culture, bacteria are maintained in exponential growth phase, and the growth rate of the bacteria is known. Related devices include turbidostats and auxostats. When \"Escherichia coli\" is growing very slowly with a doubling time of 16 hours in a chemostat most cells have a single chromosome.\n\nBacterial growth can be suppressed with bacteriostats, without necessarily killing the bacteria. In a synecological, true-to-nature situation in which more than one bacterial species is present, the growth of microbes is more dynamic and continual.\n\nLiquid is not the only laboratory environment for bacterial growth. Spatially structured environments such as biofilms or agar surfaces present additional complex growth models.\n\nEnvironmental factors influence rate of bacterial growth such as acidity (pH), temperature, water activity, macro and micro nutrients, oxygen levels, and toxins. Conditions tend to be relatively consistent between bacteria with the exception of extremophiles. Bacterium have optimal growth conditions under which they thrive, but once outside of those conditions the stress can result in either reduced or stalled growth, dormancy (such as formation spores), or death.\nMaintaining sub-optimal growth conditions is a key principle to food preservation.\n\nLow temperatures tend to reduce growth rates which has led to refrigeration being instrumental in food preservation. Depending on temperature, bacteria can be classified as:\nPsychrophiles are extremophilic cold-loving bacteria or archaea with an optimal temperature for growth at about 15 °C or lower (maximal temperature for growth at 20 °C, minimal temperature for growth at 0 °C or lower). Psychrophiles are typically found in Earth's extremely cold ecosystems, such as polar ice-cap regions, permafrost, polar surface, and deep oceans.\nMesophiles are bacteria that thrive at moderate temperatures, growing best between 20° and 45 °C. These temperatures align with the natural body temperatures of humans, which is why many human pathogens are mesophiles.\nSurvive under temperatures of 45° - 60 °C\n\nOptimal acidity for bacteria tends to be around pH 6.5 to 7.0 with the exception of acidophiles. Some bacteria can change the pH such as by excreting acid resulting in sub-optimal conditions.\n\nBacteria can be aerobes or anaerobes.\n\nAmple nutrients\n\nToxins such as ethanol can hinder or kill bacterial growth. This is used beneficially for disinfection and in food preservation.\n\n\n\"This article includes material from an article posted on 26 April 2003 on Nupedia; written by Nagina Parmar; reviewed and approved by the Biology group; editor, Gaytha Langlois; lead reviewer, Gaytha Langlois ; lead copyeditors, Ruth Ifcher. and Jan Hogle.\"\n"}
{"id": "15374375", "url": "https://en.wikipedia.org/wiki?curid=15374375", "title": "Bartell mechanism", "text": "Bartell mechanism\n\nThe Bartell mechanism is a pseudorotational mechanism similar to the Berry mechanism. It occurs only in molecules with a pentagonal bipyramidal molecular geometry, such as IF. This mechanism was first predicted by H. B. Bartell. The mechanism exchanges the axial atoms with one pair of the equatorial atoms with an energy requirement of about 2.7 kcal/mol. Similarly to the Berry mechanism in square planar molecules, the symmetry of the intermediary phase of the vibrational mode is \"chimeric\" of other mechanisms; it displays characteristics of the Berry mechanism, a \"lever\" mechanism seen in pseudorotation of disphenoidal molecules, and a \"turnstile\" mechanism (which can be seen in trigonal bipyramidal molecules under certain conditions).\n"}
{"id": "13193620", "url": "https://en.wikipedia.org/wiki?curid=13193620", "title": "Bound graph", "text": "Bound graph\n\nIn graph theory, a bound graph expresses which pairs of elements of some partially ordered set have an upper bound. Rigorously, any graph \"G\" is a bound graph if there exists a partial order ≤ on the vertices of \"G\" with the property that for any vertices \"u\" and \"v\" of \"G\", \"uv\" is an edge of \"G\" if and only if \"u\" ≠ \"v\" and there is a vertex \"w\" such that \"u\" ≤ \"w\" and \"v\" ≤ \"w\".\n\nBound graphs are sometimes referred to as \"upper bound graphs\", but the analogously defined lower bound graphs comprise exactly the same class—any lower bound for ≤ is easily seen to be an upper bound for the dual partial order ≥.\n"}
{"id": "39561038", "url": "https://en.wikipedia.org/wiki?curid=39561038", "title": "Categorical quotient", "text": "Categorical quotient\n\nIn algebraic geometry, given a category \"C\", a categorical quotient of an object \"X\" with action of a group \"G\" is a morphism formula_1 that\nOne of the main motivations for the development of geometric invariant theory was the construction of a categorical quotient for varieties or schemes.\n\nNote formula_5 need not be surjective. Also, if it exists, a categorical quotient is unique up to a canonical isomorphism. In practice, one takes \"C\" to be the category of varieties or the category of schemes over a fixed scheme. A categorical quotient formula_5 is a universal categorical quotient if it is stable under base change: for any formula_8, formula_9 is a categorical quotient.\n\nA basic result is that geometric quotients (e.g., formula_10) and GIT quotients (e.g., formula_11) are categorical quotients.\n\n\n"}
{"id": "2068445", "url": "https://en.wikipedia.org/wiki?curid=2068445", "title": "Costas array", "text": "Costas array\n\nIn mathematics, a Costas array can be regarded geometrically as a set of \"n\" points lying on the squares of a \"n\"×\"n\" checkerboard, such that each row or column contains only one point, and that all of the \"n\"(\"n\" − 1)/2 displacement vectors between each pair of dots are distinct. This results in an ideal \"thumbtack\" auto-ambiguity function, making the arrays useful in applications such as sonar and radar. Costas arrays can be regarded as two-dimensional cousins of the one-dimensional Golomb ruler construction, and, as well as being of mathematical interest, have similar applications in experimental design and phased array radar engineering.\n\nCostas arrays are named after John P. Costas, who first wrote about them in a 1965 technical report. Independently, Edgar Gilbert also wrote about them in the same year, publishing what is now known as the logarithmic Welch method of constructing Costas arrays.\n\nA Costas array may be represented numerically as an \"n\"×\"n\" array of numbers, where each entry is either 1, for a point, or 0, for the absence of a point. When interpreted as binary matrices, these arrays of numbers have the property that, since each row and column has the constraint that it only has one point on it, they are therefore also permutation matrices. Thus, the Costas arrays for any given \"n\" are a subset of the permutation matrices of order \"n\".\n\nArrays are usually described as a series of indices specifying the column for any row. Since it is given that any column has only one point, it is possible to represent an array one-dimensionally. For instance, the following is a valid Costas array of order \"N\" = 4:\n\nThere are dots at coordinates: (1,2), (2,1), (3,3), (4,4)\n\nSince the \"x\"-coordinate increases linearly, we can write this in shorthand as the set of all \"y\"-coordinates. The position in the set would then be the \"x\"-coordinate. Observe: {2,1,3,4} would describe the aforementioned array. This makes it easy to communicate the arrays for a given order of \"N\".\n\nAll Costas array orders are known for orders 1 through 29 Enumeration is as in the following table.\n\nEnumeration of known Costas arrays to order 200, order 500 and to order 1030 are available. Although these lists and databases of these Costas arrays are likely near complete, other Costas arrays with orders above 29 that are not in these lists may exist.\n\nA Welch–Costas array, or just Welch array, is a Costas array generated using the following method, first discovered by Edgar Gilbert in 1965 and rediscovered in 1982 by Lloyd R. Welch.\nThe Welch–Costas array is constructed by taking a primitive root \"g\" of a prime number \"p\" and defining the array \"A\" by formula_1 if formula_2, otherwise 0. The result is a Costas array of size \"p\" − 1.\n\nExample:\n\n3 is a primitive element modulo 5.\n\nTherefore, [3 4 2 1] is a Costas permutation. More specifically, this is an exponential Welch array. The transposition of the array is a logarithmic Welch array.\n\nThe number of Welch–Costas arrays which exist for a given size depends on the totient function.\n\nThe Lempel–Golomb construction takes α and β to be primitive elements of the finite field GF(\"q\") and similarly defines formula_1 if formula_4, otherwise 0. The result is a Costas array of size \"q\" − 2. If \"α\" + \"β\" = 1 then the first row and column may be deleted to form another Costas array of size \"q\" − 3: such a pair of primitive elements exists for every prime power \"q>2\".\n\nGeneration of new Costas arrays by adding or subtracting a row/column or two with a 1 or a pair of 1's in a corner were published in a paper focused on generation methods and in Golomb and Taylor's landmark 1984 paper\n\nMore sophisticated methods of generating new Costas arrays by deleting rows and columns of existing Costas arrays that were generated by the Welch, Lempel or Golomb generators were published in 1992. There is no upper limit on the order for which these generators will produce Costas arrays.\n\nTwo methods that found Costas arrays up to order 52 using more complicated methods of adding or deleting rows and columns were published in 2004 and 2007\n\n\n\n"}
{"id": "38017423", "url": "https://en.wikipedia.org/wiki?curid=38017423", "title": "David Francis Barrow", "text": "David Francis Barrow\n\nDavid Francis Barrow (Athens, Georgia, November 14, 1888 – 1970) was an American mathematician who introduced Barrow's inequality in 1937.\n\nBarrow's father, David Crenshaw Barrow Jr., was also a mathematician, and served as chancellor of the University of Georgia from 1906 to 1925. His son, David F. Barrow, did his undergraduate studies at the University of Georgia and then studied at Harvard University, where he earned his Ph.D. in 1913. After a year abroad, he taught for two years at the University of Texas, and then at the Sheffield Scientific School. After a brief stint in the U.S. armed services, he joined the faculty of his father's university in 1920. He became a full professor in 1923, and chaired the mathematics department in 1944–1945.\n\n"}
{"id": "2567707", "url": "https://en.wikipedia.org/wiki?curid=2567707", "title": "Formal specification", "text": "Formal specification\n\nIn computer science, formal specifications are mathematically based techniques whose purpose are to help with the implementation of systems and software. They are used to describe a system, to analyze its behavior, and to aid in its design by verifying key properties of interest through rigorous and effective reasoning tools. These specifications are \"formal\" in the sense that they have a syntax, their semantics fall within one domain, and they are able to be used to infer useful information.\n\nIn each passing decade computer systems have become increasingly more powerful and as a result they have become more impactful to society. Because of this, better techniques are needed to assist in the design and implementation of reliable software. Established engineering disciplines use mathematical analysis as the foundation of creating and validating product design. Formal specifications are one such way to achieve this in software engineering reliability as once predicted. Other methods such as testing are more commonly used to enhance code quality.\n\nGiven such a specification, it is possible to use formal verification techniques to demonstrate that a system design is correct with respect to its specification. This allows incorrect system designs to be revised before any major investments have been made into an actual implementation. Another approach is to use probably correct refinement steps to transform a specification into a design, which is ultimately transformed into an implementation that is \"correct by construction\".\n\nIt is important to note that a formal specification is \"not\" an implementation, but rather it may be used to develop an implementation. Formal specifications describe \"what\" a system should do, not \"how\" the system should do it.\n\nA good specification must have some of the following attributes: adequate, internally consistent, unambiguous, complete, satisfied, minimal\n\nA good specification will have:\n\nOne of the main reasons there is interest in formal specifications is that they will provide an ability to perform proofs on software implementations. These proofs may be used to validate a specification, verify correctness of design, or to prove that a program satisfies a specification.\n\nA design (or implementation) cannot ever be declared “correct” on its own. It can only ever be “correct with respect to a given specification”. Whether the formal specification correctly describes the problem to be solved is a separate issue. It is also a difficult issue to address, since it ultimately concerns the problem constructing abstracted formal representations of an informal concrete problem domain, and such an abstraction step is not amenable to formal proof. However, it is possible to validate a specification by proving “challenge” theorems concerning properties that the specification is expected to exhibit. If correct, these theorems reinforce the specifier's understanding of the specification and its relationship with the underlying problem domain. If not, the specification probably needs to be changed to better reflect the domain understanding of those involved with producing (and implementing) the specification.\n\nFormal methods of software development are not widely used in industry. Most companies do not consider it cost-effective to apply them in their software development processes. This may be for a variety of reasons, some of which are:\n\n\nOther limitations:\n\nFormal specification techniques have existed in various domains and on various scales for quite some time. Implementations of formal specifications will differ depending on what kind of system they are attempting to model, how they are applied and at what point in the software life cycle they have been introduced. These types of models can be categorized into the following specification paradigms:\n\n\nIn addition to the above paradigms there are ways to apply certain heuristics to help improve the creation of these specifications. The paper referenced here best discusses heuristics to use when designing a specification. They do so by applying a divide-and-conquer approach.\n\nThe Z notation is an example of a leading formal specification language. Others include the Specification Language(VDM-SL) of the Vienna Development Method and the Abstract Machine Notation (AMN) of the B-Method. In the Web services area, formal specification is often used to describe non-functional properties (Web services Quality of Service).\n\nSome tools are: \n\nFor implementation examples, refer to the links in Software Tools section.\n\n\n"}
{"id": "45170672", "url": "https://en.wikipedia.org/wiki?curid=45170672", "title": "Fractional graph isomorphism", "text": "Fractional graph isomorphism\n\nIn graph theory, a fractional isomorphism of graphs whose adjacency matrices are denoted \"A\" and \"B\" is a doubly stochastic matrix \"D\" such that \"DA\" = \"BD\". If the doubly stochastic matrix is a permutation matrix, then it constitutes a graph isomorphism.\n\nWhereas the graph isomorphism problem is not known to be decidable in polynomial time and not known to be NP-complete, the fractional graph isomorphism problem is decidable in polynomial time because it is a special case of the linear programming problem, for which there is an efficient solution.\n\nTwo graphs are also fractionally isomorphic if they have a common coarsest equitable partition. A partition of a graph is a collection of pairwise disjoint sets of vertices whose union is the vertex set of the graph. A partition is equitable if for any pair of vertices \"u\" and \"v\" in the same block of the partition and any block \"B\" of the partition, both \"u\" and \"v\" have the same number of neighbors in \"B\". An equitable partition \"P\" is coarsest if each block in any other equitable partition is a subset of a block in \"P\". Two coarsest equitable partitions \"P\" and \"Q\" are common if there is a bijection \"f\" from the blocks of \"P\" to the blocks of \"Q\" such for any blocks \"B\" and \"C\" in \"P\", the number of neighbors in \"C\" of any vertex in \"B\" equals the number of neighbors in \"f(C)\" of any vertex in \"f(B)\".\n\n"}
{"id": "41853858", "url": "https://en.wikipedia.org/wiki?curid=41853858", "title": "Generalized iterative scaling", "text": "Generalized iterative scaling\n\nIn statistics, generalized iterative scaling (GIS) and improved iterative scaling (IIS) are two early algorithms used to fit log-linear models, notably multinomial logistic regression (MaxEnt) classifiers and extensions of it such as MaxEnt Markov models and conditional random fields. These algorithms have been largely surpassed by gradient-based methods such as L-BFGS and coordinate descent algorithms.\n\n"}
{"id": "22018940", "url": "https://en.wikipedia.org/wiki?curid=22018940", "title": "Graph equation", "text": "Graph equation\n\nIn graph theory, Graph equations are equations in which the unknowns are graphs. One of the central questions of graph theory concerns the notion of isomorphism. We ask: When are two graphs the same (i.e., graph isomorphism)? The graphs in question may be expressed differently in terms of graph equations.\n\nWhat are the graphs (solutions) \"G\" and \"H\" such that the line graph of \"G\" is same as the total graph of \"H\"? (What are \"G\" and \"H\" such that \"L\"(\"G\") = \" T\"( \"H\") ? ).\n\nFor example, \"G\" = \"K\", and \"H\" = \"K\" are the solutions of the graph equation \"L\"(\"K\") = \"T\"(\"K\") and \"G\" = \"K\", and \"H\" = \"K\" are the solutions of the graph equation \"L\"(\"K\") = \"T\"(\"K\").\n\nNote that \"T\"(\"K\") is a 4-regular graph on 6 vertices.\n\n"}
{"id": "4000394", "url": "https://en.wikipedia.org/wiki?curid=4000394", "title": "Grigore Moisil", "text": "Grigore Moisil\n\nGrigore Constantin Moisil (; 10 January 1906 – 21 May 1973) was a Romanian mathematician, computer pioneer, and member of the Romanian Academy. His research was mainly in the fields of mathematical logic, (Łukasiewicz–Moisil algebra), algebraic logic, MV-algebra, and differential equations. He is viewed as the father of computer science in Romania.\n\nMoisil was also a member of the Academy of Sciences of Bologna and of the International Institute of Philosophy. In 1996, the IEEE Computer Society awarded him posthumously the \"Computer Pioneer\" Award.\n\nGrigore Moisil was born in 1906 in Tulcea into an intellectual family. His great-grandfather, Grigore Moisil (1814–1891), a clergyman, was one of the founders of the first Romanian high school in Năsăud. His father, Constantin Moisil (1876–1958), was a history professor, archaeologist and numismatist; as a member of the Romanian Academy, he filled the position of Director of the Numismatics Office of the Academy. His mother, Elena (1863–1949), was a teacher in Tulcea, later the director of \"Maidanul Dulapului\" school in Bucharest (now \"Enăchiţă Văcărescu\" school).\n\nGrigore Moisil attended primary school in Bucharest, then high school in Vaslui and Bucharest (at \"Spiru Haret\" High School) between 1916 and 1922. In 1924 he was admitted to the Civil Engineering School of the Polytechnic University of Bucharest, and also the Mathematics School of the University of Bucharest. He showed a stronger interest in mathematics, so he quit the Polytechnic University in 1929, despite already having passed all the third-year exams. In 1929 he defended his Ph.D. thesis, \"La mécanique analytique des systemes continus\" (Analytical mechanics of continuous systems), before a commission led by Gheorghe Ţiţeica, with Dimitrie Pompeiu and Anton Davidoglu as members. The thesis was published the same year by the Gauthier-Villars publishing house in Paris, and received favourable comments from Vito Volterra, Tullio Levi-Civita, and Paul Lévy.\n\nIn 1930 Moisil went to the University of Paris for further study in mathematics, which he finalized the next year with the paper \"On a class of systems of equations with partial derivatives from mathematical physics\". In 1931 he returned to Romania, where he was appointed in a teaching position at the Mathematics School of the University of Iaşi. Shortly after, he left for a one-year Rockefeller Foundation scholarship to study in Rome.\nIn 1932 he returned to Iaşi, where he remained for almost 10 years, developing a close relationship with professor Alexandru Myller. He taught the first modern algebra course in Romania, named \"Logic and theory of proof\", at the University of Iaşi. During that time, he started writing a series of papers based on the works of Jan Łukasiewicz in multi-valued logic. His research in mathematical logic laid the foundation for significant work done afterwards in Romania, as well as Argentina, Yugoslavia, Czechoslovakia, and Hungary. While in Iaşi, he completed research remarkable for the many new ideas and for his way of finding and using new connections between concepts from different areas of mathematics. He was promoted to Full Professor in November 1939.\n\nIn 1941, a position of professor at the University of Bucharest opened up, and Moisil applied for it. However, Gheorghe Vrânceanu, Dan Barbilian, and Miron Nicolescu also applied for the position, and Vrânceanu got it. Moisil approached the Ministry of Education, arguing that it would be a great opportunity for mathematics in Romania if all four could be appointed. As a result of his appeal, all four mathematicians were hired. Moisil moved to Bucharest, where he became a Professor in the School of Mathematics (later the School of Mathematics and Computer Science) at the University of Bucharest, on 30 December 1941.\n\nFrom 1946 to 1948, Moisil took a leave of absence, being named plenipotentiary envoy to Ankara. While in Turkey, he gave several series of mathematics lectures at Istanbul University and Istanbul Technical University.\n\nIn 1948, he resumed teaching at the University of Bucharest. That same year, he was elected to the Romanian Academy, and a member of the Institute of Mathematics of the Romanian Academy. After 1965, one of his outstanding students – \"George Georgescu\" – worked closely with him on multi-valued logics, and after the emergence of Romania from dictatorship in 1989, he became a Professor of Mathematics and Logic at the same university and department as Moisil in 1991. His student also published extensive, original work on algebraic logic, MV-algebra, algebra, algebraic topology, categories of MV-algebras, category theory and Łukasiewicz–Moisil algebra.\n\nMoisil published papers on mechanics, mathematical analysis, geometry, algebra and mathematical logic. He developed a multi-dimensional extension of Pompeiu's areolar derivative, and studied monogenic functions of one hypercomplex variable with applications to mechanics. Moisil also introduced some many-valued algebras, which he called Łukasiewicz algebras (now also named Łukasiewicz–Moisil algebras), and used them in logic and the study of automata theory. He created new methods to analyze finite automata, and had many contributions to the field of automata theory in algebra.\n\nMoisil had important contributions in the creation of the first Romanian computers. He played a fundamental role in the development of computer science in Romania, and in raising the first generations of Romanian computer scientists. In 1996, he was awarded by exception posthumously the Computer Pioneer Award by the Institute of Electrical and Electronics Engineers Computer Society.\n\nŁukasiewicz logic\nŁukasiewicz–Moisil algebras\nQuantum computers\nMV-algebra\nMathematical logic\nCategorical logic,\nAdjoint functors\n\n\n\n"}
{"id": "6791579", "url": "https://en.wikipedia.org/wiki?curid=6791579", "title": "Guard digit", "text": "Guard digit\n\nIn numerical analysis, one or more guard digits can be used to reduce the amount of roundoff error.\n\nFor example, suppose that the final result of a long, multi-step calculation can be safely rounded off to \"N\" decimal places. That is to say, the roundoff error introduced by this final roundoff makes a negligible contribution to the overall uncertainty.\n\nHowever, it is quite likely that it is \"not\" safe to round off the intermediate steps in the calculation to the same number of digits. Be aware that roundoff errors can accumulate. If \"M\" decimal places are used in the intermediate calculation, we say there are \"M−N\" guard digits.\n\nGuard digits are also used in floating point operations in most computer systems. Given formula_1 we have to line up the binary points. This means we must add an extra digit to the first operand—a guard digit. This gives us formula_2. Performing this operation gives us formula_3 or formula_4. Without using a guard digit we have formula_5, yielding formula_6 or formula_7. This gives us a relative error of 1. Therefore, we can see how important guard digits can be.\n\nAn example of the error caused by floating point roundoff is illustrated in the following C code.\nint main(){\n\nIt appears that the program should not terminate. Yet the output is : \n\nAnother example is:\n\nTake 2 numbers: <br>\n\nformula_8 and formula_9<br>\n\nwe bring the first number to the same power of formula_10 as the second one: <br>\n\nformula_11\n\nThe addition of the 2 numbers is:<br>\n\nAfter padding the second number (i.e., formula_9) with two formula_13s, the bit after formula_14 is the guard digit, and the bit after is the round digit. The result after rounding is formula_15 as opposed to formula_16, without the extra bits (guard and round bits), i.e., by considering only formula_17. The error therefore is formula_18.\n\n"}
{"id": "21753192", "url": "https://en.wikipedia.org/wiki?curid=21753192", "title": "ISO 128", "text": "ISO 128\n\nISO 128 is an international standard (ISO), about the general principles of presentation in technical drawings, specifically the graphical representation of objects on technical drawings.\n\nSince 2003 the ISO 128 standard contains twelve parts, which were initiated between 1996 and 2003. It starts with a summary of the general rules for the execution and structure of technical drawings. Further it describes basic conventions for lines, views, cuts and sections, and different types of engineering drawings, such as those for mechanical engineering, architecture, civil engineering, and shipbuilding. It is applicable to both manual and computer-based drawings, but it is not applicable to three-dimensional CAD models.\n\nThe ISO 128 replaced the previous DIN 6 standard for drawings, projections and views, which was first published in 1922 and updated in 1950 and 1968. ISO 128 itself was first published in 1982, contained 15 pages and \"specified the general principles of presentation to be applied to technical drawings following the orthographic projection methods\". Several parts of this standard have been updated individually. The last parts and the standard as a whole were withdrawn by the ISO in 2001.\n\nA thirteenth part was added in 2013.\n\nThe 15 parts of the ISO 128 standard are:\n\n\n"}
{"id": "23705618", "url": "https://en.wikipedia.org/wiki?curid=23705618", "title": "Invariant set postulate", "text": "Invariant set postulate\n\nThe invariant set postulate concerns the possible relationship between fractal geometry and quantum mechanics and in particular the hypothesis that the former can assist in resolving some of the challenges posed by the latter. It is underpinned by nonlinear dynamical systems theory and black hole thermodynamics.\n\nThe proposer of the postulate is climate scientist and physicist Tim Palmer. Palmer completed a PhD at the University of Oxford under Dennis Sciama, the same supervisor that Stephen Hawking had and then worked with Hawking himself at the University of Cambridge on supergravity theory. He later switched to meteorology and has established a reputation pioneering ensemble forecasting. He now works at the European Centre for Medium-Range Weather Forecasts in Reading, England.\n\nPalmer argues that the postulate may help to resolve some of the paradoxes of quantum mechanics that have been discussed since the Bohr–Einstein debates of the 1920s and 30s and which remain unresolved. The idea backs Einstein's view that quantum theory is incomplete, but also agrees with Bohr's contention that quantum systems are not independent of the observer.\nThe key idea involved is that there exists a state space for the Universe, and that the state of the entire Universe can be expressed as a point in this state space. This state space can then be divided into \"real\" and \"unreal\" sets (parts), where, for example, the states where the Nazis lost WW2 are in the \"real\" set, and the states where the Nazis won WW2 are in the \"unreal\" set of points. The partition of state space into these two sets is unchanging, making the sets invariant.\n\nIf the Universe is a complex system affected by chaos then its invariant set (a fixed state of rest) is likely to be a fractal. According to Palmer this could resolve problems posed by the Kochen–Specker theorem, which appears to indicate that physics may have to abandon the idea of any kind of objective reality, and the apparent paradox of action at a distance. In a paper submitted to the \"Proceedings of the Royal Society\" he indicates how the idea can account for quantum uncertainty and problems of \"contextuality\". For example, exploring the quantum problem of wave-particle duality, one of the central mysteries of quantum theory, the author claims that \"in terms of the Invariant Set Postulate, the paradox is easily resolved, in principle at least\". The paper and related talks given at the Perimeter Institute and University of Oxford also explores the role of gravity in quantum physics.\n\n\"New Scientist\" quotes Bob Coeke of Oxford University as stating \"What makes this really interesting is that it gets away from the usual debates over multiple universes and hidden variables and so on. It suggests there might be an underlying physical geometry that physics has just missed, which is radical and very positive\". He added that \"Palmer manages to explain some quantum phenomena, but he hasn't yet derived the whole rigid structure of the theory. This is really necessary.\"\n\nRobert Spekkens of the Perimeter Institute has said: \"I think his approach is really interesting and novel. Other physicists have shown how you can find a way out of the Kochen–Specker theorem, but this work actually provides a mechanism to explain the theorem.\"\n\n"}
{"id": "98912", "url": "https://en.wikipedia.org/wiki?curid=98912", "title": "János Bolyai", "text": "János Bolyai\n\nJános Bolyai (; 15 December 1802 – 27 January 1860) or Johann Bolyai, was a Hungarian mathematician, one of the founders of non-Euclidean geometry — a geometry that differs from Euclidean geometry in its definition of parallel lines. The discovery of a consistent alternative geometry that might correspond to the structure of the universe helped to free mathematicians to study abstract concepts irrespective of any possible connection with the physical world.\n\nBolyai was born in the Hungarian town of Kolozsvár, Grand Principality of Transylvania (now Cluj-Napoca in Romania), the son of Zsuzsanna Benkő and the well-known mathematician Farkas Bolyai.\nBy the age of 13, he had mastered calculus and other forms of analytical mechanics, receiving instruction from his father. He studied at the Imperial and Royal Military Academy (TherMilAk) in Vienna from 1818 to 1822.\n\nBolyai became so obsessed with Euclid's parallel postulate that his father, who had pursued the same subject for many years, wrote to him in 1820: \"You must not attempt this approach to parallels. I know this way to the very end. I have traversed this bottomless night, which extinguished all light and joy in my life. I entreat you, leave the science of parallels alone...Learn from my example.\" \n\nJános, however, persisted in his quest and eventually came to the conclusion that the postulate is independent of the other axioms of geometry and that different consistent geometries can be constructed on its negation. In 1823, he wrote to his father: \"I have discovered such wonderful things that I was amazed...out of nothing I have created a strange new universe.\" Between 1820 and 1823 he had prepared a treatise on a complete system of non-Euclidean geometry. Bolyai's work was published in 1832 as an appendix to a mathematics textbook by his father.\n\nCarl Friedrich Gauss, on reading the Appendix, wrote to a friend saying \"I regard this young geometer Bolyai as a genius of the first order.\" To Bolyai, however, Gauss wrote: \"To praise it would amount to praising myself. For the entire content of the work...coincides almost exactly with my own meditations which have occupied my mind for the past thirty or thirty-five years.\" In 1848 Bolyai discovered that Nikolai Ivanovich Lobachevsky had published a similar piece of work in 1829. Though Lobachevsky published his work a few years earlier than Bolyai, it contained only hyperbolic geometry. Bolyai and Lobachevsky did not know each other or each other's works.\n\nIn addition to his work in geometry, Bolyai developed a rigorous geometric concept of complex numbers as ordered pairs of real numbers. Although he never published more than the 24 pages of the Appendix, he left more than 20,000 pages of mathematical manuscripts when he died. These can now be found in the Teleki-Bolyai Library in Marosvásárhely (today Târgu Mureş), where Bolyai died.\n\nHe was an accomplished polyglot speaking nine foreign languages, including Chinese and Tibetan. He learned the violin and performed in Vienna. \nNo original portrait of Bolyai survives. An unauthentic picture appears in some encyclopedias and on a Hungarian postage stamp.\n\nThe Babeş-Bolyai University in Cluj-Napoca, that was established in 1959, bears his name, as does the crater Bolyai on the Moon and the János Bolyai Mathematical Institute at the University of Szeged. Furthermore, 1441 Bolyai, a minor planet discovered in 1937, is named after him; and many primary and secondary schools in the Carpathian Basin bear his name, e.g. Bolyai János Műszaki Szakközépiskola in Budapest, Bolyai János Gyakorló Általános Iskola és Gimnázium in Szombathely, Bolyai János Általános Iskola in Debrecen, etc. A street in Budapest, Hungary and another one in Temesvár (now Timișoara), Romania is also named after him. The professional society of Hungarian mathematicians also bears his name. Bolyai is a minor character in the 1969 science-fiction/fantasy story \"Operation Changeling\", where his unique abilities allow the protagonists to navigate the non-Euclidean geometry of Hell.\n\nThere is also a mathematical award given out every five years named the Bolyai Prize.\n\n\n\n"}
{"id": "155407", "url": "https://en.wikipedia.org/wiki?curid=155407", "title": "Kleene's recursion theorem", "text": "Kleene's recursion theorem\n\nIn computability theory, Kleene's recursion theorems are a pair of fundamental results about the application of computable functions to their own descriptions. The theorems were first proved by Stephen Kleene in 1938 and appear in his 1952 book \"Introduction to Metamathematics\". A related theorem which constructs fixed points of a computable function is known as Rogers's theorem and is due to Hartley Rogers, Jr. (1967).\n\nThe recursion theorems can be applied to construct fixed points of certain operations on computable functions, to generate quines, and to construct functions defined via recursive definitions.\n\nThe statement of the theorems refers to an admissible numbering formula_1 of the partial recursive functions, such that the function corresponding to index formula_2 is formula_3. In programming terms, formula_2 represents a program and formula_3 represents the function computed by this program.\n\nIf formula_6 and formula_7 are partial functions on the natural numbers, the notation formula_8 indicates that, for each \"n\", either formula_9 and formula_10 are both defined and are equal, or else formula_9 and formula_10 are both undefined.\n\nGiven a function formula_6, a fixed point of formula_6 is an index formula_2 such that formula_16. Rogers (Rogers 1967: §11.2) describes the following result as \"a simpler version\" of Kleene's (second) recursion theorem.\n\nThe proof uses a particular total computable function formula_18, defined as follows. Given a natural number formula_19, the function formula_18 outputs the index of the partial computable function that performs the following computation:\nThus, for all indices formula_19 of partial computable functions, if formula_26 is defined, then formula_27. If formula_26 is not defined, then formula_29 is a function that is nowhere defined. The function formula_18 can be constructed from the partial computable function formula_31 described above and the s-m-n theorem: for each formula_19, formula_33 is the index of a program which computes the function formula_34.\n\nTo complete the proof, let formula_6 be any total computable function, and construct formula_18 as above. Let formula_2 be an index of the composition formula_38, which is a total computable function. Then formula_39 by the definition of formula_18.\nBut, because formula_2 is an index of formula_38, formula_43, and thus formula_44. By the transitivity of formula_45, this means formula_46. Hence formula_47 for formula_48.\n\nThis proof is a construction of a partial recursive function which implements the Y combinator.\n\nA function formula_6 such that formula_50 for all formula_2 is called fixed point free. The fixed-point theorem shows that no computable function is fixed point free, but there are many non-computable fixed-point free functions. Arslanov's completeness criterion states that the only recursively enumerable Turing degree that computes a fixed point free function is 0′, the degree of the halting problem (Soare 1987, p. 88)\n\nThe second recursion theorem is a generalization of Rogers's theorem with a second input in the function. One informal interpretation of the second recursion theorem is that it is possible to construct self-referential programs; see \"Application to quines\" below.\n\nThe theorem can be proved from Rogers's theorem by letting formula_55 be a function such that formula_56 (a construction described by the S-m-n theorem). One can then verify that a fixed-point of this formula_6 is an index formula_53 as required. The theorem is constructive in the sense that a fixed computable function maps an index for \"Q\" into the index \"p\".\n\nKleene's second recursion theorem and Rogers's theorem can both be proved, rather simply, from each other (Jones, 1997: p. 229-230). However, a direct proof of Kleene's theorem (Kleene 1952: p. 352-353) does not make use of a universal program, which means that the theorem holds for certain subrecursive programming systems that do not have a universal program.\n\nA classic example using the second recursion theorem is the function formula_59. The corresponding index formula_53 in this case yields a computable function that outputs its own index when applied to any value (Cutland 1980, p. 204). When expressed as computer programs, such indices are known as quines.\n\nThe following example in Lisp illustrates how the formula_53 in the corollary can be effectively produced from the function formula_62. The function codice_1 in the code is the function of that name produced by the S-m-n theorem.\n\ncodice_2 can be changed to any two-argument function.\nThe results of the following expressions should be the same. formula_1 codice_3\n\ncodice_4\nSuppose that formula_64 and formula_18 are total computable functions that are used in a recursive definition for a function formula_66:\n\nThe second recursion theorem can be used to show that such equations define a computable function, where the notion of computability does not have to allow, a priori, for recursive definitions (for example, it may be defined by μ-recursion, or by Turing machines). This recursive definition can be converted into a computable function formula_69 that assumes formula_2 is an index to itself, to simulate recursion:\n\nThe recursion theorem establishes the existence of a computable function formula_73 such that formula_74. Thus formula_66 satisfies the given recursive definition.\n\nReflexive, or reflective, programming refers to the usage of self-reference in programs. Jones (1997) presents a view of the second recursion theorem based on a reflexive language.\nIt is shown that the reflexive language defined is not stronger than a language without reflection (because an interpreter for the reflexive language can be implemented without using reflection); then, it is shown that the recursion theorem is almost trivial in the reflexive language.\n\nWhile the second recursion theorem is about fixed points of computable functions, the first recursion theorem is related to fixed points determined by enumeration operators, which are a computable analogue of inductive definitions. An enumeration operator is a set of pairs (\"A\",\"n\") where \"A\" is a (code for a) finite set of numbers and \"n\" is a single natural number. Often, \"n\" will be viewed as a code for an ordered pair of natural numbers, particularly when functions are defined via enumeration operators. Enumeration operators are of central importance in the study of enumeration reducibility.\n\nEach enumeration operator Φ determines a function from sets of naturals to sets of naturals given by\nA recursive operator is an enumeration operator that, when given the graph of a partial recursive function, always returns the graph of a partial recursive function.\n\nA fixed point of an enumeration operator Φ is a set \"F\" such that Φ(\"F\") = \"F\". The first enumeration theorem shows that fixed points can be effectively obtained if the enumeration operator itself is computable.\n\nLike the second recursion theorem, the first recursion theorem can be used to obtain functions satisfying systems of recursion equations. To apply the first recursion theorem, the recursion equations must first be recast as a recursive operator.\n\nConsider the recursion equations for the factorial function \"f\":\n\nformula_77\n\nformula_78\n\nThe corresponding recursive operator Φ will have information that tells how to get to the next value of \"f\" from the previous value. However, the recursive operator will actually define the graph of \"f\". First, Φ will contain the pair formula_79. This indicates that \"f\"(0) is unequivocally 1, and thus the pair (0,1) is in the graph of \"f\".\n\nNext, for each \"n\" and \"m\", Φ will contain the pair formula_80. This indicates that, if \"f\"(\"n\") is \"m\", then \"f\"(\"n\" + 1) is (\"n\" + 1)\"m\", so that the pair (\"n\" + 1, (\"n\" + 1)\"m\") is in the graph of \"f\". Unlike the base case \"f\"(0) = 1, the recursive operator requires some information about \"f\"(\"n\") before it defines a value of \"f\"(\"n\" + 1).\n\nThe first recursion theorem (in particular, part 1) states that there is a set \"F\" such that Φ(\"F\") = F. The set \"F\" will consist entirely of ordered pairs of natural numbers, and will be the graph of the factorial function \"f\", as desired.\n\nThe restriction to recursion equations that can be recast as recursive operators ensures that the recursion equations actually define a least fixed point. For example, consider the set of recursion equations:\n\nformula_81\n\nformula_82\n\nformula_83\n\nThere is no function \"g\" satisfying these equations, because they imply \"g\"(2) = 1 and also imply \"g\"(2) = 0. Thus there is no fixed point \"g\" satisfying these recursion equations. It is possible to make an enumeration operator corresponding to these equations, but it will not be a recursive operator.\n\nThe proof of part 1 of the first recursion theorem is obtained by iterating the enumeration operator Φ beginning with the empty set. First, a sequence \"F\" is constructed, for formula_84. Let \"F\" be the empty set. Proceeding inductively, for each \"k\", let \"F\" be formula_85. Finally, \"F\" is taken to be formula_86. The remainder of the proof consists of a verification that \"F\" is recursively enumerable and is the least fixed point of Φ. The sequence \"F\" used in this proof corresponds to the Kleene chain in the proof of the Kleene fixed-point theorem.\n\nThe second part of the first recursion theorem follows from the first part. The assumption that Φ is a recursive operator is used to show that the fixed point of Φ is the graph of a partial function. The key point is that if the fixed point \"F\" is not the graph of a function, then there is some \"k\" such that \"F\" is not the graph of a function.\n\nCompared to the second recursion theorem, the first recursion theorem produces a stronger conclusion but only when narrower hypotheses are satisfied. Rogers (1967) uses the term weak recursion theorem for the first recursion theorem and strong recursion theorem for the second recursion theorem.\n\nOne difference between the first and second recursion theorems is that the fixed points obtained by the first recursion theorem are guaranteed to be least fixed points, while those obtained from the second recursion theorem may not be least fixed points.\n\nA second difference is that the first recursion theorem only applies to systems of equations that can be recast as recursive operators. This restriction is similar to the restriction to continuous operators in the Kleene fixed-point theorem of order theory. The second recursion theorem can be applied to any total recursive function.\n\nAnatoly Maltsev proved a generalized version of the recursion theorem for any set with a precomplete numbering. A Gödel numbering is a precomplete numbering on the set of computable functions so the generalized theorem yields the Kleene recursion theorem as a special case.\n\nGiven a precomplete numbering formula_87 then for any partial computable function formula_66 with two parameters there exists a total computable function formula_89 with one parameter such that\n\n\n"}
{"id": "5361510", "url": "https://en.wikipedia.org/wiki?curid=5361510", "title": "Leibniz formula for determinants", "text": "Leibniz formula for determinants\n\nIn algebra, the Leibniz formula, named in honor of Gottfried Leibniz, expresses the determinant of a square matrix in terms of permutations of the matrix elements. If \"A\" is an \"n\"×\"n\" matrix, where \"a\" is the entry in the \"i\"th row and \"j\"th column of \"A\", the formula is\nwhere sgn is the sign function of permutations in the permutation group \"S\", which returns +1 and −1 for even and odd permutations, respectively.\n\nAnother common notation used for the formula is in terms of the Levi-Civita symbol and makes use of the Einstein summation notation, where it becomes\nwhich may be more familiar to physicists.\n\nDirectly evaluating the Leibniz formula from the definition requires formula_3 operations in general—that is, a number of operations asymptotically proportional to \"n\" factorial—because \"n\"! is the number of order-\"n\" permutations. This is impractically difficult for large \"n\". Instead, the determinant can be evaluated in O(\"n\") operations by forming the LU decomposition formula_4 (typically via Gaussian elimination or similar methods), in which case formula_5 and the determinants of the triangular matrices \"L\" and \"U\" are simply the products of their diagonal entries. (In practical applications of numerical linear algebra, however, explicit computation of the determinant is rarely required.) See, for example, .\n\nTheorem.\nThere exists exactly one function\nwhich is alternating multilinear w.r.t. columns and such that formula_7.\n\nProof.\n\nUniqueness: Let formula_8 be such a function, and let formula_9 be an formula_10 matrix. Call formula_11 the formula_12-th column of formula_13, i.e. formula_14, so that formula_15\n\nAlso, let formula_16 denote the formula_17-th column vector of the identity matrix.\n\nNow one writes each of the formula_11's in terms of the formula_16, i.e.\n\nAs formula_8 is multilinear, one has\n\nFrom alternation it follows that any term with repeated indices is zero. The sum can therefore be restricted to tuples with non-repeating indices, i.e. permutations:\n\nBecause F is alternating, the columns formula_24 can be swapped until it becomes the identity. The sign function formula_25 is defined to count the number of swaps necessary and account for the resulting sign change. One finally gets:\n\nas formula_27 is required to be equal to formula_28.\n\nTherefore no function besides the function defined by the Leibniz Formula can be a multilinear alternating function with formula_29. \n\nExistence: We now show that F, where F is the function defined by the Leibniz formula, has these three properties.\n\nMultilinear:\nAlternating:\nFor any formula_32 let formula_33 be the tuple equal to formula_34 with the formula_35 and formula_36 indices switched.\nThus if formula_38 then formula_39.\n\nFinally, formula_40:\n\nThus the only alternating multilinear functions with formula_40 are restricted to the function defined by the Leibniz formula, and it in fact also has these three properties. Hence the determinant can be defined as the only function\n\nwith these three properties.\n\n\n"}
{"id": "10814713", "url": "https://en.wikipedia.org/wiki?curid=10814713", "title": "Levi's lemma", "text": "Levi's lemma\n\nIn theoretical computer science and mathematics, especially in the area of combinatorics on words, the Levi lemma states that, for all strings \"u\", \"v\", \"x\" and \"y\", if \"uv\" = \"xy\", then there exists a string \"w\" such that either\n\nor \n\nThat is, there is a string \"w\" that is \"in the middle\", and can be grouped to one side or the other. Levi's lemma is named after Friedrich Wilhelm Levi, who published it in 1944.\n\nLevi's lemma can be applied repeatedly in order to solve word equations; in this context it is sometimes called the Nielsen transformation by analogy with the Nielsen transformation for groups. For example, starting with an equation \"xα\" = \"yβ\" where \"x\" and \"y\" are the unknowns, we can transform it (assuming \"|x| ≥ |y|\", so there exists \"t\" such that \"x\"=\"yt\") to \"ytα\" = \"yβ\", thus to \"tα\" = \"β\". This approach results in a graph of substitutions generated by repeatedly applying Levi's lemma. If each unknown appears at most twice, then word equation is called quadratic; in a quadratic word equation the graph obtained by repeatedly applying Levi's lemma is finite, so it is decidable if a quadratic word equation has a solution. (A more general method for solving word equations is Makanin's algorithm.)\n\nThe above is known as the Levi lemma for strings; the lemma can occur in a more general form in graph theory and in monoid theory; for example, there is a more general Levi lemma for traces.\n\nA monoid in which Levi's lemma holds is said to have the equidivisibility property. The free monoid of strings and string concatenation has this property (by Levi's lemma for strings), but by itself equidivisibility is not enough to guarantee that a monoid is free. However an equidivisibile monoid \"M\" is free if additionally there exists a homomorphism \"f\" from \"M\" to the monoid of natural numbers (free monoid on one generator) with the property that the preimage of 0 contains only the identity element of \"M\", i.e. formula_1. (Note that \"f\" simply being a homomorhism does not guarantee this latter property, as there could be multiple elements of \"M\" mapped to 0.) A monoid for which such a homorphims exists is also called \"graded\" (and the \"f\" is called a gradation).\n\n"}
{"id": "1168506", "url": "https://en.wikipedia.org/wiki?curid=1168506", "title": "List of algebraic number theory topics", "text": "List of algebraic number theory topics\n\nThis is a list of algebraic number theory topics.\n\nThese topics are basic to the field, either as prototypical examples, or as basic objects of study.\n\n\n\n\n\n"}
{"id": "53802271", "url": "https://en.wikipedia.org/wiki?curid=53802271", "title": "Machine learning control", "text": "Machine learning control\n\nMachine learning control (MLC) is a subfield of machine learning, intelligent control and control theory\nwhich solves optimal control problems with methods of machine learning.\nKey applications are complex nonlinear systems\nfor which linear control theory methods are not applicable.\n\nFour types of problems are commonly encountered.\n\nMLC comprises, for instance, neural network control, \ngenetic algorithm based control, \ngenetic programming control,\nreinforcement learning control, \nand has methodological overlaps with other data-driven control,\nlike artificial intelligence and robot control.\n\nMLC has been successfully applied\nto many nonlinear control problems,\nexploring unknown and often unexpected actuation mechanisms.\nExample applications include\n\n\nAs for all general nonlinear methods,\nMLC comes with no guaranteed convergence, \noptimality or robustness for a range of operating conditions.\n\n"}
{"id": "276734", "url": "https://en.wikipedia.org/wiki?curid=276734", "title": "Minimal surface", "text": "Minimal surface\n\nIn mathematics, a minimal surface is a surface that locally minimizes its area. This is equivalent to having zero mean curvature (see definitions below).\n\nThe term \"minimal surface\" is used because these surfaces originally arose as surfaces that minimized total surface area subject to some constraint. Physical models of area-minimizing minimal surfaces can be made by dipping a wire frame into a soap solution, forming a soap film, which is a minimal surface whose boundary is the wire frame. However, the term is used for more general surfaces that may self-intersect or do not have constraints. For a given constraint there may also exist several minimal surfaces with different areas (for example, see minimal surface of revolution): the standard definitions only relate to a local optimum, not a global optimum.\n\nMinimal surfaces can be defined in several equivalent ways in R. The fact that they are equivalent serves to demonstrate how minimal surface theory lies at the crossroads of several mathematical disciplines, especially differential geometry, calculus of variations, potential theory, complex analysis and mathematical physics.\n\nNote that this property is local: there might exist other surfaces that minimize area better with the same global boundary.\n\nThis definition makes minimal surfaces a 2-dimensional analogue to geodesics.\n\nBy the Young–Laplace equation the curvature of a soap film is proportional to the difference in pressure between the sides: if it is zero, the membrane has zero mean curvature. Note that spherical bubbles are \"not\" minimal surfaces as per this definition: while they minimize total area subject to a constraint on internal volume, they have a positive pressure.\n\nA direct implication of this definition is that every point on the surface is a saddle point with equal and opposite principal curvatures.\n\nThe partial differential equation in this definition was originally found in 1762 by Lagrange, and Jean Baptiste Meusnier discovered in 1776 that it implied a vanishing mean curvature.\n\nThis definition ties minimal surfaces to harmonic functions and potential theory.\n\nA direct implication of this definition and the maximum principle for harmonic functions is that there are no compact complete minimal surfaces in R.\n\nThis definition uses that the mean curvature is half of the trace of the shape operator, which is linked to the derivatives of the Gauss map. If the projected Gauss map obeys the Cauchy–Riemann equations then either the trace vanishes or every point of \"M\" is umbilic, in which case it is a piece of a sphere.\n\nThe local least area and variational definitions allow extending minimal surfaces to other Riemannian manifolds than R.\n\nMinimal surface theory originates with Lagrange who in 1762 considered the variational problem of finding the surface \"z\" = \"z\"(\"x\", \"y\") of least area stretched across a given closed contour. He derived the Euler–Lagrange equation for the solution\n\nHe did not succeed in finding any solution beyond the plane. In 1776 Jean Baptiste Marie Meusnier discovered that the helicoid and catenoid satisfy the equation and that the differential expression corresponds to twice the mean curvature of the surface, concluding that surfaces with zero mean curvature are area-minimizing.\n\nBy expanding Lagrange's equation to\n\nGaspard Monge and Legendre in 1795 derived representation formulas for the solution surfaces. While these were successfully used by Heinrich Scherk in 1830 to derive his surfaces, they were generally regarded as practically unusable. Catalan proved in 1842/43 that the helicoid is the only ruled minimal surface.\n\nProgress had been fairly slow until the middle of the century when the Björling problem was solved using complex methods. The \"first golden age\" of minimal surfaces began. Schwarz found the solution of the Plateau problem for a regular quadrilateral in 1865 and for a general quadrilateral in 1867 (allowing the construction of his periodic surface families) using complex methods. Weierstrass and Enneper developed more useful representation formulas, firmly linking minimal surfaces to complex analysis and harmonic functions. Other important contributions came from Beltrami, Bonnet, Darboux, Lie, Riemann, Serret and Weingarten.\n\nBetween 1925 and 1950 minimal surface theory revived, now mainly aimed at nonparametric minimal surfaces. The complete solution of the Plateau problem by Jesse Douglas and Tibor Radó was a major milestone. Bernstein's problem and Robert Osserman's work on complete minimal surfaces of finite total curvature were also important.\n\nAnother revival began in the 1980s. One cause was the discovery in 1982 by Celso Costa of a surface that disproved the conjecture that the plane, the catenoid, and the helicoid are the only complete embedded minimal surfaces in R of finite topological type. This not only stimulated new work on using the old parametric methods, but also demonstrated the importance of computer graphics to visualise the studied surfaces and numerical methods to solve the \"period problem\" (when using the conjugate surface method to determine surface patches that can be assembled into a larger symmetric surface, certain parameters need to be numerically matched to produce an embedded surface). Another cause was the verification by H. Karcher that the triply periodic minimal surfaces originally described empirically by Alan Schoen in 1970 actually exist. This has led to a rich menagerie of surface families and methods of deriving new surfaces from old, for example by adding handles or distorting them.\n\nCurrently the theory of minimal surfaces has diversified to minimal submanifolds in other ambient geometries, becoming relevant to mathematical physics (e.g. the positive mass conjecture, the Penrose conjecture) and three-manifold geometry (e.g. the Smith conjecture, the Poincaré conjecture, the Thurston Geometrization Conjecture).\n\nClassical examples of minimal surfaces include:\n\nSurfaces from the 19th century golden age include:\n\nModern surfaces include:\n\nMinimal surfaces can be defined in other manifolds than R, such as hyperbolic space, higher-dimensional spaces or Riemannian manifolds.\n\nThe definition of minimal surfaces can be generalized/extended to cover constant-mean-curvature surfaces: surfaces with a constant mean curvature, which need not equal zero.\n\nIn discrete differential geometry discrete minimal surfaces are studied: simplicial complexes of triangles that minimize their area under small perturbations of their vertex positions. Such discretizations are often used to approximate minimal surfaces numerically, even if no closed form expressions are known.\n\nBrownian motion on a minimal surface leads to probabilistic proofs of several theorems on minimal surfaces.\n\nMinimal surfaces have become an area of intense scientific study, especially in the areas of molecular engineering and materials science, due to their anticipated applications in self-assembly of complex materials.\n\nMinimal surfaces play a role in general relativity. The apparent horizon (marginally outer trapped surface) is a minimal hypersurface, linking the theory of black holes to minimal surfaces and the Plateau problem.\n\nMinimal surfaces are part of the generative design toolbox used by modern designers. In architecture there has been much interest in tensile structures, which are closely related to minimal surfaces. A famous example is the Olympiapark in Münich by Frei Otto, inspired by soap surfaces.\n\nIn the art world, minimal surfaces have been extensively explored in the sculpture of Robert Engman (1927– ), Robert Longhurst (1949– ), and Charles O. Perry (1929–2011), among others.\n\n\n\n"}
{"id": "54248486", "url": "https://en.wikipedia.org/wiki?curid=54248486", "title": "Multivalued treatment", "text": "Multivalued treatment\n\nIn statistics, in particular in the design of experiments, a multi-valued treatment is a treatment that can take on more than two values. It is related to the dose-response model in the medical literature. Generally speaking, treatment levels may be finite or infinite as well as ordinal or cardinal, which leads to a large collection of possible treatment effects to be studied in applications. One example is the effect of different levels of program participation (e.g. full-time and part-time) in a job training program.\n\nAssume there exists a finite collection of multi-valued treatment status formula_1 with J some fixed integer. As in the potential outcomes framework, denote formula_2 the collection of potential outcomes under the treatment \"J\", and formula_3 denotes the observed outcome and formula_4 is an indicator that equals 1 when the treatment equals \"j\" and 0 when it does not equal j, leading to a fundamental problem of causal inference. A general framework that analyzes ordered choice models in terms of marginal treatment effects and average treatment effects has been extensively discussed by Heckman and Vytlacil.\n\nRecent work in the econometrics and statistics literature has focused on estimation and inference for multivalued treatments and ignorability conditions for identifying the treatment effects. In the context of program evaluation, the propensity score has been generalized to allow for multi-valued treatments, while other work has also focused on the role of the conditional mean independence assumption. Other recent work has focused more on the large sample properties of an estimator of the marginal mean treatment effect conditional on a treatment level in the context of a difference-in-differences model, and on the efficient estimation of multi-valued treatment effects in a semiparametric framework.\n"}
{"id": "32890500", "url": "https://en.wikipedia.org/wiki?curid=32890500", "title": "Nemenyi test", "text": "Nemenyi test\n\nIn statistics, the Nemenyi test is a post-hoc test intended to find the groups of data that differ after a statistical test of multiple comparisons (such as the Friedman test) has rejected the null hypothesis that the performance of the comparisons on the groups of data is similar. The test makes pair-wise tests of performance.\n\nThe test is named after Peter Nemenyi.\n\nThe test is sometimes referred to as the \"Nemenyi–Damico–Wolfe–Dunn test\".\n\n"}
{"id": "55345", "url": "https://en.wikipedia.org/wiki?curid=55345", "title": "Net present value", "text": "Net present value\n\nIn finance, the net present value (NPV) or net present worth (NPW) is the summation of the present (now) value of a series of present and future cash flows. Because NPV accounts for the time value of money NPV provides a method for evaluating and comparing products with cash flows spread over many years, as in loans, investments, payouts from insurance contracts plus many other applications. \n\nTime value of money dictates that time affects the value of cash flows. For example, a lender may offer 99 cents for the promise of receiving $1.00 a month from now, but the promise to receive that same dollar 20 years in the future would be worth much less today to that same person (lender), even if the payback in both cases was equally certain. This decrease in the current value of future cash flows is based on a chosen rate of return (or discount rate). If for example there exists a time series of identical cash flows, the cash flow in the present is the most valuable, with each future cash flow becoming less valuable than the previous cash flow. A cash flow today is more valuable than an identical cash flow in the future because a present flow can be invested immediately and begin earning returns, while a future flow cannot.\n\nNet present value (NPV) is determined by calculating the costs (negative cash flows) and benefits (positive cash flows) for each period of an investment. The period is typically one year, but could be measured in quarter-years, half-years or months. After the cash flow for each period is calculated, the present value (PV) of each one is achieved by discounting its future value (see Formula) at a periodic rate of return (the rate of return dictated by the market). NPV is the sum of all the discounted future cash flows. Because of its simplicity, NPV is a useful tool to determine whether a project or investment will result in a net profit or a loss. A positive NPV results in profit, while a negative NPV results in a loss. The NPV measures the excess or shortfall of cash flows, in present value terms, above the cost of funds. In a theoretical situation of unlimited capital budgeting a company should pursue every investment with a positive NPV. However, in practical terms a company's capital constraints limit investments to projects with the highest NPV whose cost cash flows, or initial cash investment, do not exceed the company's capital. NPV is a central tool in discounted cash flow (DCF) analysis and is a standard method for using the time value of money to appraise long-term projects. It is widely used throughout economics, finance, and accounting.\n\nIn the case when all future cash flows are positive, or incoming (such as the principal and coupon payment of a bond) the only outflow of cash is the purchase price, the NPV is simply the PV of future cash flows minus the purchase price (which is its own PV). NPV can be described as the \"difference amount\" between the sums of discounted cash inflows and cash outflows. It compares the present value of money today to the present value of money in the future, taking inflation and returns into account.\n\nThe NPV of a sequence of cash flows takes as input the cash flows and a discount rate or discount curve and outputs a price. The converse process in DCF analysis—taking a sequence of cash flows and a price as input and inferring as output a discount rate (the discount rate which would yield the given price as NPV)—is called the yield and is more widely used in bond trading.\n\nMany computer-based spreadsheet programs have built-in formulae for PV and NPV.\n\nEach cash inflow/outflow is discounted back to its present value (PV). Then all are summed. Therefore, NPV is the sum of all terms,\nwhere\n\nThe result of this formula is multiplied with the Annual Net cash in-flows and reduced by Initial Cash outlay the present value but in cases where the cash flows are not equal in amount, then the previous formula will be used to determine the present value of each cash flow separately. Any cash flow within 12 months will not be discounted for NPV purpose, nevertheless the usual initial investments during the first year \"R\" are summed up a negative cash flow.\n\nGiven the (period, cash flow) pairs (formula_2, formula_4) where formula_8 is the total number of periods, the net present value formula_9 is given by:\n\nThe rate used to discount future cash flows to the present value is a key variable of this process.\n\nA firm's weighted average cost of capital (after tax) is often used, but many people believe that it is appropriate to use higher discount rates to adjust for risk, opportunity cost, or other factors. A variable discount rate with higher rates applied to cash flows occurring further along the time span might be used to reflect the yield curve premium for long-term debt.\n\nAnother approach to choosing the discount rate factor is to decide the rate which the capital needed for the project could return if invested in an alternative venture. If, for example, the capital required for Project A can earn 5% elsewhere, use this discount rate in the NPV calculation to allow a direct comparison to be made between Project A and the alternative. Related to this concept is to use the firm's reinvestment rate. Re-investment rate can be defined as the rate of return for the firm's investments on average. When analyzing projects in a capital constrained environment, it may be appropriate to use the reinvestment rate rather than the firm's weighted average cost of capital as the discount factor. It reflects opportunity cost of investment, rather than the possibly lower cost of capital.\n\nAn NPV calculated using variable discount rates (if they are known for the duration of the investment) may better reflect the situation than one calculated from a constant discount rate for the entire investment duration. Refer to the tutorial article written by Samuel Baker for more detailed relationship between the NPV value and the discount rate.\n\nFor some professional investors, their investment funds are committed to target a specified rate of return. In such cases, that rate of return should be selected as the discount rate for the NPV calculation. In this way, a direct comparison can be made between the profitability of the project and the desired rate of return.\n\nTo some extent, the selection of the discount rate is dependent on the use to which it will be put. If the intent is simply to determine whether a project will add value to the company, using the firm's weighted average cost of capital may be appropriate. If trying to decide between alternative investments in order to maximize the value of the firm, the corporate reinvestment rate would probably be a better choice.\n\nUsing variable rates over time, or discounting \"guaranteed\" cash flows differently from \"at risk\" cash flows, may be a superior methodology but is seldom used in practice. Using the discount rate to adjust for risk is often difficult to do in practice (especially internationally) and is difficult to do well. An alternative to using discount factor to adjust for risk is to explicitly correct the cash flows for the risk elements using rNPV or a similar method, then discount at the firm's rate.\n\nNPV is an indicator of how much value an investment or project adds to the firm. With a particular project, if formula_4 is a positive value, the project is in the status of positive cash inflow in the time of \"t\". If formula_4 is a negative value, the project is in the status of discounted cash outflow in the time of \"t\". Appropriately risked projects with a positive NPV could be accepted. This does not necessarily mean that they should be undertaken since NPV at the cost of capital may not account for opportunity cost, i.e., comparison with other available investments. In financial theory, if there is a choice between two mutually exclusive alternatives, the one yielding the higher NPV should be selected. A positive net present value indicates that the projected earnings generated by a project or investment (in present dollars) exceeds the anticipated costs (also in present dollars). Generally, an investment with a positive NPV will be a profitable one and one with a negative NPV will result in a net loss. This concept is the basis for the Net Present Value Rule, which dictates that the only investments that should be made are those with positive NPV values.\n\nAn alternative way of looking at Net Present Value is that at the given rate of Cost of Capital, whether the project can meet the cost of capital. For example, if the NPV is -$2.5 million (i.e. negative NPV) for a given project, it may mean that at the given Weighted Average Cost of Capital (WACC), the project fails to meet the expectations of the suppliers of capital for the project. On the other hand, the NPV of $2.5 million would add $2.5 million to the wealth of the suppliers of funds over and above their expected returns.\n\nThe time-discrete formula of the net present value\n\ncan also be written in a continuous variation\nwhere\n\nNet present value can be regarded as Laplace- respectively Z-transformed cash flow with the integral operator including the complex number \"s\" which resembles to the interest rate \"i\" from the real number space or more precisely \"s\" = ln(1 + \"i\").\n\nFrom this follow simplifications known from cybernetics, control theory and system dynamics. Imaginary parts of the complex number \"s\" describe the oscillating behaviour (compare with the pork cycle, cobweb theorem, and phase shift between commodity price and supply offer) whereas real parts are responsible for representing the effect of compound interest (compare with damping).\n\nA corporation must decide whether to introduce a new product line. The company will have immediate costs of 100,000 at \"t = 0\". Recall, a cost is a negative for outgoing cash flow, thus this cash flow is represented as -100,000. The company assumes the product will provide equal benefits of 10,000 for each of 12 years beginning at \"t = 1\". For simplicity, assume the company will have no outgoing cash flows after the initial 100,000 cost. This also makes the simplifying assumption that the net cash received or paid is lumped into a single transaction occurring \"on the last day\" of each year. At the end of the 12 years the product no longer provides any cash flow and is discontinued without any additional costs. Assume that the effective annual discount rate is 10%.\n\nThe present value (value at \"t = 0\") can be calculated for each year:\nThe total present value of the incoming cash flows is 68,136.91. The total present value of the outgoing cash flows is simply the 100,000 at time \"t = 0\".\nThus:\n\nformula_16\n\nIn this example:\n\nformula_17\n\nformula_18\n\nObserve that as \"t\" increases the present value of each cash flow at \"t\" decreases. For example, the final incoming cash flow has a future value of 10,000 at \"t = 12\" but has a present value (at \"t = 0\") of 3,186.31. The opposite of discounting is compounding. Taking the example in reverse, it is the equivalent of investing 3,186.31 at \"t = 0\" (the present value) at an interest rate of 10% compounded for 12 years, which results in a cash flow of 10,000 at \"t = 12\" (the future value).\n\nThe importance of NPV becomes clear in this instance. Although the incoming cash flows (10,000 x 12 = 120,000) appear to exceed the outgoing cash flow (100,000), the future cash flows are not adjusted using the discount rate. Thus, the project appears misleadingly profitable. When the cash flows are discounted however, it indicates the project would result in a net loss of 31,863.09. Thus, the NPV calculation indicates that this project should be disregarded because investing in this project is the equivalent of a loss of 31,863.09 at \"t = 0\". The concept of time value of money indicates that cash flows in different periods of time cannot be accurately compared unless they have been adjusted to reflect their value at the same period of time (in this instance, \"t = 0\"). It is the present value of each future cash flow that must be determined in order to provide any meaningful comparison between cash flows at different periods of time. There are a few inherent assumptions in this type of analysis:\n\n\nMore realistic problems would also need to consider other factors, generally including: smaller time buckets, the calculation of taxes (including the cash flow timing), inflation, currency exchange fluctuations, hedged or unhedged commodity costs, risks of technical obsolescence, potential future competitive factors, uneven or unpredictable cash flows, and a more realistic salvage value assumption, as well as many others.\n\nA more simple example of the net present value of incoming cash flow over a set period of time, would be winning a Powerball lottery of $500 million. If one does not select the \"CASH\" option they will be paid $25,000,000 per year for 20 years, a total of $500,000,000, however, if one does select the \"CASH\" option, they will receive a one-time lump sum payment of approximately $285 million, the NPV of $500,000,000 paid over time. See \"other factors\" above that could affect the payment amount. Both scenarios are before taxes.\n\n\nNet present value as a valuation methodology dates at least to the 19th century. Karl Marx refers to NPV as fictitious capital, and the calculation as \"capitalising,\" writing:\n\nIn mainstream neo-classical economics, NPV was formalized and popularized by Irving Fisher, in his 1907 \"The Rate of Interest\" and became included in textbooks from the 1950s onwards, starting in finance texts.\n\n\n"}
{"id": "2975225", "url": "https://en.wikipedia.org/wiki?curid=2975225", "title": "Nilpotent cone", "text": "Nilpotent cone\n\nIn mathematics, the nilpotent cone formula_1 of a finite-dimensional semisimple Lie algebra formula_2 is the set of elements that act nilpotently in all representations of formula_3 In other words, \n\nThe nilpotent cone is an irreducible subvariety of formula_2 (considered as a formula_6-vector space).\n\nThe nilpotent cone of formula_7, the Lie algebra of 2×2 matrices with vanishing trace, is the variety of all 2×2 traceless matrices with rank less than or equal to formula_8\n\n"}
{"id": "26991095", "url": "https://en.wikipedia.org/wiki?curid=26991095", "title": "Norman L. Biggs", "text": "Norman L. Biggs\n\nNorman Linstead Biggs (born 2 January 1941) is a leading British mathematician focusing on discrete mathematics and in particular algebraic combinatorics.\n\nBiggs was educated at Harrow County Grammar School and then studied mathematics at Selwyn College, Cambridge. In 1962, Biggs gained first-class honours in his third year of the University's undergraduate degree in mathematics.\n\n\nHe was a lecturer at University of Southampton, lecturer then reader at Royal Holloway, University of London, and Professor of Mathematics at the London School of Economics. He has been on the editorial board of a number of journals, including the \"Journal of Algebraic Combinatorics\". He has been a member of the Council of the London Mathematical Society.\n\nHe has written 12 books and over 100 papers on mathematical topics, many of them in algebraic combinatorics and its applications. He became Emeritus Professor in 2006 and continue to teach History of Mathematics in Finance and Economics for undergraduates. He is also Vice-President of the British Society for the History of Mathematics.\n\nBiggs married Christine Mary Farmer in 1975 and has one daughter Clare Juliet born in 1980.\n\nBiggs' interests include computational learning theory, the history of mathematics and historical metrology. Since 2006, he has been an Emeritus Professor at the London School of Economics.\n\nBiggs hobbies consist of writing about the history of weights and scales. He currently holds the position of Chair of the International Society of Antique Scale Collectors (Europe), and a member of the British Numismatic Society.\n\nIn 2002, Biggs wrote the second edition of \"Discrete Mathematics\" breaking down a wide range of topics into a clear and organised style. Biggs organised the book into four major sections; The Language of Mathematics, Techniques, Algorithms and Graphs, and Algebraic Methods. This book was an accumulation of \"Discrete Mathematics\", first edition, textbook published in 1985 which dealt with calculations involving a finite number of steps rather than limiting processes. The second edition added nine new introductory chapters; Fundamental language of mathematicians, statements and proofs, the logical framework, sets and functions, and number system. This book stresses the significance of simple logical reasoning, shown by the exercises and examples given in the book. Each chapter contains modelled solutions, examples, exercises including hints and answers.\n\nIn 1974, Biggs published \"Algebraic Graph Theory\" which articulates properties of graphs in algebraic terms, then works out theorems regarding them. In the first section, he tackles the applications of linear algebra and matrix theory; algebraic constructions such as adjacency matrix and the incidence matrix and their applications are discussed in depth. Next, there is and wide-ranging description of the theory of chromatic polynomials. The last section discusses symmetry and regularity properties. Biggs makes important connections with other branches of algebraic combinatorics and group theory.\n\nIn 1997, N. Biggs and M. Anthony wrote a book titled \"Computational Learning Theory: an Introduction\". Both Biggs and Anthony focused on the necessary background material from logic, probability, and complex theory. This book is an introduction to computational learning.\n\nBiggs contributed to thirteen journals and books developing topics such as the four-colour conjecture, the roots/history of combinatorics, calculus, Topology on the 19th century, and mathematicians. In addition, Biggs examined the ideas of William Ludlam, Thomas Harriot, John Arbuthnot, and Leonhard Euler.\n\nThe chip-firing game has been around for less than 20 years. It has become an important part of the study of structural combinatorics. The set of configurations that are stable and recurrent for this game can be given the structure of an abelian group. In addition, the order of the group is equal to the tree number of the graph.\n\n\n2000\n\n2001\n\n2002\n\n2004\n\n2005\n\n2007\n\n2008\n\n2009\n\n2010\n\n2011\n\nFor other published work on the history of mathematics, please see.\n\n\n"}
{"id": "2608829", "url": "https://en.wikipedia.org/wiki?curid=2608829", "title": "Peetre's inequality", "text": "Peetre's inequality\n\nIn mathematics, Peetre's inequality, named after Jaak Peetre, says that for any real number \"t\" and any vectors \"x\" and \"y\" in R, the following inequality holds:\n\n"}
{"id": "3509077", "url": "https://en.wikipedia.org/wiki?curid=3509077", "title": "Ply (game theory)", "text": "Ply (game theory)\n\nIn two-player sequential games, a ply is one turn taken by one of the players. The word is used to clarify what is meant when one might otherwise say \"turn\".\n\nThe word \"turn\" can be a problem since it means different things in different traditions. For example, in standard chess terminology, one \"move\" consists of a turn by each player; therefore a ply in chess is a \"half-move\". Thus, after 20 moves in a chess game, 40 plies have been completed—20 by white and 20 by black. In the game of Go, by contrast, a ply is the normal unit of counting moves; so for example to say that a game is \"250 moves long\" is to imply 250 plies.\n\nThe word \"ply\" used as a synonym for \"layer\" goes back to the 15th century. Arthur Samuel first used the term in its game-theoretic sense in his seminal paper on machine learning in checkers in 1959, but with a slighty different meaning: the \"ply\", in Samuel's terminology, is actually the depth of analysis (\"Certain expressions were introduced which we will find useful. These are: Ply, defined as the number of moves ahead, where a ply of two consists of one proposed move by the machine and one anticipated reply by the opponent\").\n\nIn computing, the concept of ply is important because one ply corresponds to one level of the game tree. The Deep Blue chess computer which defeated Kasparov in 1997 would typically search to a depth of between six and sixteen plies to a maximum of forty plies in some situations.\n\n"}
{"id": "24107159", "url": "https://en.wikipedia.org/wiki?curid=24107159", "title": "Popoviciu's inequality on variances", "text": "Popoviciu's inequality on variances\n\nIn probability theory, Popoviciu's inequality, named after Tiberiu Popoviciu, is an upper bound on the variance \"σ²\" of any bounded probability distribution. Let \"M\" and \"m\" be upper and lower bounds on the values of any random variable with a particular probability distribution. Then Popoviciu's inequality states:\n\nThis equality holds precisely when half of the probability is concentrated at each of the two bounds.\n\nSharma \"et al\". have sharpened Popoviciu's inequality:\n\nIf the sample size is finite then the von Szokefalvi Nagy inequality gives a lower bound to the variance\n\nwhere \"n\" is the sample size.\n\nPopoviciu's inequality is weaker than the Bhatia–Davis inequality which states\n\nwhere \"μ\" is the expectation of the random variable. \n\nA lower bound for the variance based on the Bhatia–Davis inequality has been found by Agarwal \"et al\"\n"}
{"id": "14979971", "url": "https://en.wikipedia.org/wiki?curid=14979971", "title": "Quantum lithography", "text": "Quantum lithography\n\nQuantum lithography is a type of photolithography, which exploits non-classical properties of the photons, such as quantum entanglement, in order to achieve superior performance over ordinary classical lithography. Quantum lithography is closely related to the fields of quantum imaging, quantum metrology, and quantum sensing. The effect exploits the quantum mechanical state of light called the NOON state. Quantum lithography was invented at Jonathan P. Dowling's group at JPL, and has been studied by a number of groups.\n\nOf particular importance, quantum lithography can beat the classical Rayleigh criterion for the diffraction limit. Classical photolithography has an optical imaging resolution that cannot be smaller than the wavelength of light used. For example, in the use of photolithography to mass-produce computer chips, it is desirable to produce smaller and smaller features on the chip, which classically requires moving to smaller and smaller wavelengths (ultraviolet and x-ray), which entails exponentially greater cost to produce the optical imaging systems at these extremely short optical wavelengths.\n\nQuantum lithography exploits the quantum entanglement between specially prepared photons in the NOON state and special photoresists, that display multi-photon absorption processes to achieve the smaller resolution without the requirement of shorter wavelengths. For example, a beam of red photons, entangled 50 at a time in the NOON state, would have the same resolving power as a beam of x-ray photons.\n\nThe field of quantum lithography is in its infancy, and although experimental proofs of principal have been carried out using the Hong–Ou–Mandel effect, it is still a long way from commercial application.\n\n"}
{"id": "23779912", "url": "https://en.wikipedia.org/wiki?curid=23779912", "title": "Quaternionic analysis", "text": "Quaternionic analysis\n\nIn mathematics, quaternionic analysis is the study of functions with quaternions as the domain and/or range. Such functions can be called functions of a quaternion variable just as functions of a real variable or a complex variable are called.\n\nAs with complex and real analysis, it is possible to study the concepts of analyticity, holomorphy, harmonicity and conformality in the context of quaternions. Unlike the complex numbers and like the reals, the four notions do not coincide.\n\nThe projections of a quaternion onto its scalar part or onto its vector part, as well as the modulus and versor functions, are examples that are basic to understanding quaternion structure.\n\nAn important example of a function of a quaternion variable is\nwhich rotates the vector part of \"q\" by twice the angle represented by \"u\".\n\nThe quaternion multiplicative inverse formula_2 is another fundamental function, but as with other number systems, formula_3 and related problems are generally discluded due to the nature of dividing by zero.\n\nAffine transformations of quaternions have the form\nLinear fractional transformations of quaternions can be represented by elements of the matrix ring formula_5 operating on the projective line over formula_6. For instance, the mappings formula_7 where formula_8 and formula_9 are fixed versors serve to produce the motions of elliptic space.\n\nQuaternion variable theory differs in some respects from complex variable theory. For example: The complex conjugate mapping of the complex plane is a central tool but requires the introduction of a non-arithmetic, non-analytic operation. Indeed, conjugation changes the orientation of plane figures, something that arithmetic functions do not change.\n\nIn contrast to the complex conjugate, the quaternion conjugation can be expressed arithmetically, as formula_10\n\nThis equation can be proven, starting with the basis {1, i, j, k}:\nConsequently, since formula_12 is linear,\n\nThe success of complex analysis in providing a rich family of holomorphic functions for scientific work has engaged some workers in efforts to extend the planar theory, based on complex numbers, to a 4-space study with functions of a quaternion variable. These efforts were summarized in .\n\nThough formula_6 appears as a union of complex planes, the following proposition shows that extending complex functions requires special care:\n\nLet formula_15 be a function of a complex variable, formula_16. Suppose also that formula_8 is an even function of formula_18 and that formula_9 is an odd function of formula_18. Then formula_21 is an extension of formula_22 to a quaternion variable formula_23 where formula_24 and formula_25.\nThen, let formula_26 represent the conjugate of formula_27, so that formula_28. The extension to formula_6 will be complete when it is shown that formula_30. Indeed, by hypothesis\n\nThe rotation about axis \"r\" is a classical application of quaternions to space mapping.\nIn terms of a homography, the rotation is expressed\nwhere formula_34 is a versor. If \"p\" * = −\"p\", then the translation formula_35 is expressed by\nRotation and translation \"xr\" along the axis of rotation is given by\nSuch a mapping is called a screw displacement. In classical kinematics, Chasles' theorem states that any rigid body motion can be displayed as a screw displacement. Just as the representation of a Euclidean plane isometry as a rotation is a matter of complex number arithmetic, so Chasles' theorem, and the screw axis required, is a matter of quaternion arithmetic with homographies: Let \"s\" be a right versor, or square root of minus one, perpendicular to \"r\", with \"t\" = \"rs\".\n\nConsider the axis passing through \"s\" and parallel to \"r\". Rotation about it is expressed by the homography composition\nwhere formula_39\n\nNow in the (\"s,t\")-plane the parameter θ traces out a circle formula_40 in the half-plane formula_41 \n\nAny \"p\" in this half-plane lies on a ray from the origin through the circle formula_42 and can be written formula_43\n\nThen up = az, with formula_44 as the homography expressing conjugation of a rotation by a translation p.\n\nThe map formula_45\nof quaternion algebra is called linear, if following equalities hold\nwhere formula_49 is real field.\nSince formula_50 is linear map of quaternion algebra,\nthen, for any formula_51, the map\nis linear map.\nIf formula_50 is identity map (formula_54),\nthen, for any formula_51,\nwe identify tensor product formula_56 and the map\nFor any linear map\nformula_45\nthere exists a tensor formula_59,\nformula_60,\nsuch that\nSo we can identify the linear map formula_50\nand the tensor formula_63.\n\nSince the time of Hamilton, it has been realized that requiring the independence of the derivative from the path that a differential follows toward zero is too restrictive: it excludes even formula_64 from differentiation. Therefore, a direction-dependent derivative is necessary for functions of a quaternion variable.\nConsidering of the increment of polynomial function of quaternionic argument shows that the increment is linear map of increment of the argument. From this, a definition can be made:\n\nContinuous map\nformula_45\nis called differentiable\non the set formula_66,\nif, at every point formula_67,\nthe increment of the map formula_50 can be represented as\nwhere\nis linear map of quaternion algebra formula_71 and\nformula_72\nis such continuous map that\nLinear map\nformula_74\nis called derivative of the map formula_50.\n\nOn the quaternions, the derivative may be expressed as\nTherefore, the differential of the map formula_50 may be expressed as\n/math With Brackets on either side.\n\nThe number of terms in the sum will depend on the function \"f\". The expressions \nformula_79 are called\ncomponents of derivative.\n\nThe derivative of a quaternionic function holds the following equalities\n\nFor the function \"f\"(\"x\") = \"axb\", the derivative is\nand so the components are:\nSimilarly, for the function \"f\"(\"x\") = \"x^2\", the derivative is\nand the components are:\nFinally, for the function \"f\"(\"x\") = \"x\", the derivative is\nand the components are: \n\n"}
{"id": "41445293", "url": "https://en.wikipedia.org/wiki?curid=41445293", "title": "Quillen's theorems A and B", "text": "Quillen's theorems A and B\n\nIn topology, a branch of mathematics, Quillen's Theorem A gives a sufficient condition for the classifying spaces of two categories to be homotopy equivalent. Quillen's Theorem B gives a sufficient condition for a square consisting of classifying spaces of categories to be homotopy Cartesian. The two theorems play central roles in Quillen's Q-construction in algebraic K-theory and are named after Daniel Quillen.\n\nThe precise statements of the theorems are as follows.\n\nIn general, the homotopy fiber of formula_1 is not naturally the classifying space of a category: there is no natural category formula_2 such that formula_3. Theorem B constructs formula_2 in a case when formula_5 is especially nice.\n\n"}
{"id": "43198110", "url": "https://en.wikipedia.org/wiki?curid=43198110", "title": "Random algebra", "text": "Random algebra\n\nIn set theory, the random algebra or random real algebra is the Boolean algebra of Borel sets of the unit interval modulo the ideal of measure zero sets. It is used in random forcing to add random reals to a model of set theory. The random algebra was studied by John von Neumann in 1935 (in work later published as ) who showed that it is not isomorphic to the Cantor algebra of Borel sets modulo meager sets. Random forcing was introduced by .\n\n"}
{"id": "16458571", "url": "https://en.wikipedia.org/wiki?curid=16458571", "title": "Rationalisation (mathematics)", "text": "Rationalisation (mathematics)\n\nIn elementary algebra, root rationalisation is a process by which radicals in the denominator of an algebraic fraction are eliminated. \n\nIf the denominator is a monomial in some radical, say formula_1 with , rationalisation consists of multiplying the numerator and the denominator by formula_2 and replacing formula_3 by formula_4 if is even or by if is odd (if , the same replacement allows us to reduce until it becomes lower than .\n\nIf the denominator is linear in some square root, say formula_5 rationalisation consists of multiplying the numerator and the denominator by formula_6 and expanding the product in the denominator.\n\nThis technique may be extended to any algebraic denominator, by multiplying the numerator and the denominator by all algebraic conjugates of the denominator, and expanding the new denominator into the norm of the old denominator. However, except in special cases, the resulting fractions may have huge numerators and denominators, and, therefore, the technique is generally used only in the above elementary cases.\n\nFor the fundamental technique, the numerator and denominator must be multiplied by the same factor.\n\nExample 1:\n\nTo rationalise this kind of monomial, bring in the factor formula_8:\n\nThe square root disappears from the denominator, because it is squared:\n\nThis gives the result, after simplification:\n\nExample 2:\n\nTo rationalise this radical, bring in the factor formula_13:\n\nThe cube root disappears from the denominator, because it is cubed:\n\nThis gives the result, after simplification:\n\nFor a denominator that is: \n\nRationalisation can be achieved by multiplying by the \"Conjugate\":\n\nand applying the difference of two squares identity, which here will yield −1. To get this result, the entire fraction should be multiplied by \n\nThis technique works much more generally. It can easily be adapted to remove one square root at a time, i.e. to rationalise\n\nby multiplication by\n\nExample:\n\nThe fraction must be multiplied by a quotient containing formula_23.\n\nNow, we can proceed to remove the square roots in the denominator:\n\nExample 2:\n\nThis process also works with complex numbers with formula_26\n\nThe fraction must be multiplied by a quotient containing formula_28.\n\nRationalisation can be extended to all algebraic numbers and algebraic functions (as an application of norm forms). For example, to rationalise a cube root, two linear factors involving cube roots of unity should be used, or equivalently a quadratic factor.\n\n\nThis material is carried in classic algebra texts. For example:\n\n"}
{"id": "695101", "url": "https://en.wikipedia.org/wiki?curid=695101", "title": "Real representation", "text": "Real representation\n\nIn the mathematical field of representation theory a real representation is usually a representation on a real vector space \"U\", but it can also mean a representation on a complex vector space \"V\" with an invariant real structure, i.e., an antilinear equivariant map\nwhich satisfies\nThe two viewpoints are equivalent because if \"U\" is a real vector space acted on by a group \"G\" (say), then \"V\" = \"U\"⊗C is a representation on a complex vector space with an antilinear equivariant map given by complex conjugation. Conversely, if \"V\" is such a complex representation, then \"U\" can be recovered as the fixed point set of \"j\" (the eigenspace with eigenvalue 1).\n\nIn physics, where representations are often viewed concretely in terms of matrices, a real representation is one in which the entries of the matrices representing the group elements are real numbers. These matrices can act either on real or complex column vectors.\n\nA real representation on a complex vector space is isomorphic to its complex conjugate representation, but the converse is not true: a representation which is isomorphic to its complex conjugate but which is not real is called a pseudoreal representation. An irreducible pseudoreal representation \"V\" is necessarily a quaternionic representation: it admits an invariant quaternionic structure, i.e., an antilinear equivariant map\nwhich satisfies\nA direct sum of real and quaternionic representations is neither real nor quaternionic in general.\n\nA representation on a complex vector space can also be isomorphic to the dual representation of its complex conjugate. This happens precisely when the representation admits a nondegenerate invariant sesquilinear form, e.g. a hermitian form. Such representations are sometimes said to be complex or (pseudo-)hermitian.\n\nA criterion (for compact groups \"G\") for reality of irreducible representations in terms of character theory is based on the Frobenius-Schur indicator defined by\nwhere \"χ\" is the character of the representation and \"μ\" is the Haar measure with μ(\"G\") = 1. For a finite group, this is given by\nThe indicator may take the values 1, 0 or −1. If the indicator is 1, then the representation is real. If the indicator is zero, the representation is complex (hermitian), and if the indicator is −1, the representation is quaternionic.\n\nAll representation of the symmetric groups are real (and in fact rational), since we can build a complete set of irreducible representations using Young tableaux.\n\nAll representations of the rotation groups are real, since they all appear as subrepresentations of tensor products of copies of the fundamental representation, which is real.\n\nFurther examples of real representations are the spinor representations of the spin groups in 8\"k\"−1, 8\"k\", and 1 + 8\"k\" dimensions for \"k\" = 1, 2, 3 ... . This periodicity \"modulo\" 8 is known in mathematics not only in the theory of Clifford algebras, but also in algebraic topology, in KO-theory; see spin representation.\n\n"}
{"id": "655974", "url": "https://en.wikipedia.org/wiki?curid=655974", "title": "Rigour", "text": "Rigour\n\nRigour (British English) or rigor (American English; see spelling differences) describes a condition of stiffness or strictness. Rigour frequently refers to a process of adhering absolutely to certain constraints, or the practice of maintaining strict consistency with certain predefined parameters. These constraints may be environmentally imposed, such as \"the rigours of famine\"; logically imposed, such as mathematical proofs which must maintain consistent answers; or socially imposed, such as the process of defining ethics and law.\n\n\"Rigour\" comes to English through old French (13th c., Modern French \"rigueur\") meaning \"stiffness\", which itself is based on the Latin \"rigorem\" (nominative \"rigor\") \"numbness, stiffness, hardness, firmness; roughness, rudeness\", from the verb \"rigere\" \"to be stiff\". The noun was frequently used to describe a condition of strictness or stiffness, which arises from a situation or constraint either chosen or experienced passively. For example, the title of the book \"Theologia Moralis Inter Rigorem et Laxitatem Medi\" roughly translates as \"mediating theological morality between rigour and laxness\". The book details, for the clergy, situations in which they are obligated to follow church law exactly, and in which situations they can be more forgiving yet still considered moral. \"Rigor mortis\" translates directly as the stiffness (\"rigor\") of death (\"mortis\"), again describing a condition which arises from a certain constraint (death).\n\nIntellectual rigour is a process of thought which is consistent, does not contain self-contradiction, and takes into account the entire scope of available knowledge on the topic. It actively avoids logical fallacy. Furthermore, it requires a sceptical assessment of the available knowledge. If a topic or case is dealt with in a rigorous way, it means that it is dealt with in a comprehensive, thorough and complete way, leaving no room for inconsistencies.\n\nScholarly method describes the different approaches or methods which may be taken to apply intellectual rigour on an institutional level to ensure the quality of information published. An example of intellectual rigour assisted by a methodical approach is the scientific method, in which a person will produce a hypothesis based on what they believe to be true, then construct experiments in order to prove that hypothesis wrong. This method, when followed correctly, helps to prevent against circular reasoning and other fallacies which frequently plague conclusions within academia. Other disciplines, such as philosophy and mathematics, employ their own structures to ensure intellectual rigour. Each method requires close attention to criteria for logical consistency, as well as to all relevant evidence and possible differences of interpretation. At an institutional level, Peer review is used to validate intellectual rigour.\n\nIntellectual rigour is a subset of intellectual honesty—a practice of thought in which ones convictions are kept in proportion to valid evidence. Intellectual honesty is an unbiased approach to the acquisition, analysis, and transmission of ideas. A person is being intellectually honest when he or she, knowing the truth, states that truth, regardless of outside social/environmental pressures. It is possible to doubt whether complete intellectual honesty exists—on the grounds that no one can entirely master his or her own presuppositions—without doubting that certain kinds of intellectual rigour are potentially available. The distinction certainly matters greatly in debate, if one wishes to say that an argument is flawed in its premises.\n\nThe setting for intellectual rigour does tend to assume a principled position from which to advance or argue. An opportunistic tendency to use any argument at hand is not very rigorous, although very common in politics, for example. Arguing one way one day, and another later, can be defended by casuistry, i.e. by saying the cases are different.\n\nIn the legal context, for practical purposes, the facts of cases do always differ. Case law can therefore be at odds with a principled approach; and intellectual rigour can seem to be defeated. This defines a judge's problem with uncodified law. Codified law poses a different problem, of interpretation and adaptation of definite principles without losing the point; here applying the letter of the law, with all due rigour, may on occasion seem to undermine the \"principled approach\".\n\nMathematical rigour can refer both to rigorous methods of mathematical proof and to rigorous methods of mathematical practice (thus relating to other interpretations of rigour).\n\n<div id=\"mathematical\">\nMathematical rigour is often cited as a kind of gold standard for mathematical proof. Its history traces back to Greek mathematics, especially to Euclid's \"Elements\".\n\nUntil the 19th century, the treatise was seen as extremely rigorous and profound, but, during that century, it was realized the work assumed some conditions that were not stated and also could not be proved (e.g. two circles can intersect in a point, some point is within an angle, and figures can be superimposed on each other). This was contrary to the idea of rigorous proof where all conditions need to be stated and nothing can be assumed. New foundations were developed using the axiomatic method to address the lack of rigour in the \"Elements\".\n\nDuring the 19th century, the term 'rigorous' began to be used to describe decreasing levels of abstraction when dealing with calculus which eventually became known as analysis. The works of Cauchy added rigour to the older works of Euler and Gauss. The works of Riemann added rigour to the works of Cauchy. The works of Weierstrass added rigour to the works of Riemann, eventually culminating in the arithmetization of analysis. Starting in the 1870s, the term gradually came to be associated with Cantorian set theory.\n\nMathematical rigour can be defined as amenability to algorithmic proof checking. Indeed, with the aid of computers, it is possible to check some proofs mechanically. Formal rigour is the introduction of high degrees of completeness by means of a formal language where such proofs can be codified using set theories such as ZFC (see automated theorem proving).\n\nMost mathematical arguments are presented as prototypes of formally rigorous proofs. The reason often cited for this is that completely rigorous proofs, which tend to be longer and more unwieldy, may obscure what is being demonstrated. Steps which are obvious to a human mind may have fairly long formal derivations from the axioms. Under this argument, there is a trade-off between rigour and comprehension. Some argue that the use of formal languages to institute complete mathematical rigour might make theories which are commonly disputed or misinterpreted completely unambiguous by revealing flaws in reasoning.\n\nThe role of mathematical rigour in relation to physics is twofold:\n\n\nBoth aspects of mathematical rigour in physics have attracted considerable attention in philosophy of science. (See, for example, ref. and works quoted therein.)\n\nRigour in the classroom is a hotly debated topic amongst educators. Generally speaking, however, classroom rigour consists of multi-faceted, challenging instruction and correct placement of the student. Students excelling in formal operational thought tend to excel in classes for gifted students. Students who have not reached that final stage of cognitive development, according to developmental psychologist Jean Piaget, can build upon those skills with the help of a properly trained teacher.\n\nRigour in the classroom is commonly referred to as \"rigorous instruction\". It is instruction that requires students to construct meaning for themselves, impose structure on information, integrate individual skills into processes, operate within but at the outer edge of their abilities, and apply what they learn in more than one context and to unpredictable situations \n\n"}
{"id": "11186496", "url": "https://en.wikipedia.org/wiki?curid=11186496", "title": "Self-avoiding walk", "text": "Self-avoiding walk\n\nIn mathematics, a self-avoiding walk (SAW) is a sequence of moves on a lattice (a lattice path) that does not visit the same point more than once. This is a special case of the graph theoretical notion of a path. A self-avoiding polygon (SAP) is a closed self-avoiding walk on a lattice. SAWs were first introduced by the chemist Paul Flory in order to model the real-life behavior of chain-like entities such as solvents and polymers, whose physical volume prohibits multiple occupation of the same spatial point. Very little is known rigorously about the self-avoiding walk from a mathematical perspective, although physicists have provided numerous conjectures that are believed to be true and are strongly supported by numerical simulations.\n\nIn computational physics a self-avoiding walk is a chain-like path in or with a certain number of nodes, typically a fixed step length and has the imperative property that it doesn't cross itself or another walk. A system of self-avoiding walks satisfies the so-called excluded volume condition. In higher dimensions, the self-avoiding walk is believed to behave much like the ordinary random walk. SAWs and SAPs play a central role in the modelling of the topological and knot-theoretic behaviour of thread- and loop-like molecules such as proteins. SAW is a fractal. For example, in the fractal dimension is , for it is close to while for the fractal dimension is . The dimension is called the upper critical dimension above which excluded volume is negligible. A SAW that does not satisfy the excluded volume condition was recently studied to model explicit surface geometry resulting from expansion of a SAW.\n\nThe properties of SAWs cannot be calculated analytically, so numerical simulations are employed. The pivot algorithm is a common method for Markov chain Monte Carlo simulations for the uniform measure on -step self-avoiding walks. The pivot algorithm works by taking a self-avoiding walk and randomly choosing a point on this walk, and then applying a symmetry operation (rotations and reflections) on the walk after the nth step to create a new walk. Calculating the number of self-avoiding walks in any given lattice is a common computational problem. There is currently no known formula for determining the number of self-avoiding walks, although there are rigorous methods for approximating them. Finding the number of such paths is conjectured to be an NP-hard problem. For self-avoiding walks from one end of a diagonal to the other, with only moves in the positive direction, there are exactly\n\npaths for an rectangular lattice.\n\nOne of the phenomena associated with self-avoiding walks and -dimensional statistical physics models in general is the notion of universality, that is, independence of macroscopic observables from microscopic details, such as the choice of the lattice. One important quantity that appears in conjectures for universal laws is the connective constant, defined as follows. Let denote the number of -step self-avoiding walks. Since every -step self avoiding walk can be decomposed into an -step self-avoiding walk and an -step self-avoiding walk, it follows that . Therefore, the sequence is subadditive and we can apply Fekete's lemma to show that the following limit exists:\n\nFor other lattices, has only been approximated numerically, and is believed to not even be an algebraic number. It is conjectured that\n\nas , where depends on the lattice, but the power law correction formula_5 does not; in other words, this law is believed to be universal.\n\nSelf-avoiding walks have also been studied in the context of network theory. In this context, it is customary to treat the SAW as a dynamical process, such that in every time-step a walker randomly hops between neighboring nodes of the network. The walk ends when the walker reaches a dead-end state, such that it can no longer progress to newly un-visited nodes. It was recently found that on Erdős–Rényi networks, the distribution of path lengths of such dynamically grown SAWs can be calculated analytically, and follows the Gompertz distribution.\n\nConsider the uniform measure on -step self-avoiding walks in the full plane. It is currently unknown whether the limit of the uniform measure as induces a measure on infinite full-plane walks. However, Harry Kesten has shown that such a measure exists for self-avoiding walks in the half-plane. One important question involving self-avoiding walks is the existence and conformal invariance of the scaling limit, that is, the limit as the length of the walk goes to infinity and the mesh of the lattice goes to zero. The scaling limit of the self-avoiding walk is conjectured to be described by Schramm–Loewner evolution with parameter \n\n\n"}
{"id": "3571350", "url": "https://en.wikipedia.org/wiki?curid=3571350", "title": "Socle (mathematics)", "text": "Socle (mathematics)\n\nIn mathematics, the term socle has several related meanings.\n\nIn the context of group theory, the socle of a group \"G\", denoted soc(\"G\"), is the subgroup generated by the minimal normal subgroups of \"G\". It can happen that a group has no minimal non-trivial normal subgroup (that is, every non-trivial normal subgroup properly contains another such subgroup) and in that case the socle is defined to be the subgroup generated by the identity. The socle is a direct product of minimal normal subgroups.\n\nAs an example, consider the cyclic group Z with generator \"u\", which has two minimal normal subgroups, one generated by \"u\" (which gives a normal subgroup with 3 elements) and the other by \"u\" (which gives a normal subgroup with 2 elements). Thus the socle of Z is the group generated by \"u\" and \"u\", which is just the group generated by \"u\".\n\nThe socle is a characteristic subgroup, and hence a normal subgroup. It is not necessarily transitively normal, however.\n\nIf a group G is a finite solvable group, then the socle can be expressed as a product of elementary abelian p-groups. Thus, in this case, it is just a product of copies of Z/pZ for various p where the same p may occur multiple times in the product.\n\nIn the context of module theory and ring theory the socle of a module \"M\" over a ring \"R\" is defined to be the sum of the minimal nonzero submodules of \"M\". It can be considered as a dual notion to that of the radical of a module. In set notation,\n\nEquivalently,\n\nThe socle of a ring \"R\" can refer to one of two sets in the ring. Considering \"R\" as a right \"R\" module, soc(\"R\") is defined, and considering \"R\" as a left \"R\" module, soc(\"R\") is defined. Both of these socles are ring ideals, and it is known they are not necessarily equal.\n\n\nIn the context of Lie algebras, a socle of a symmetric Lie algebra is the eigenspace of its structural automorphism which corresponds to the eigenvalue −1. (A symmetric Lie algebra decomposes into the direct sum of its socle and cosocle.)\n\n\n"}
{"id": "2335732", "url": "https://en.wikipedia.org/wiki?curid=2335732", "title": "Spherical angle", "text": "Spherical angle\n\nA spherical angle is a particular dihedral angle; it is the angle between two intersecting arcs of great circles on a sphere. It is measured by the angle between the planes containing the arcs (which naturally also contain the centre of the sphere).\n\n"}
{"id": "6334535", "url": "https://en.wikipedia.org/wiki?curid=6334535", "title": "Spin-weighted spherical harmonics", "text": "Spin-weighted spherical harmonics\n\nIn special functions, a topic in mathematics, spin-weighted spherical harmonics are generalizations of the standard spherical harmonics and—like the usual spherical harmonics—are functions on the sphere. Unlike ordinary spherical harmonics, the spin-weighted harmonics are gauge fields rather than scalar fields: mathematically, they take values in a complex line bundle. The spin-weighted harmonics are organized by degree , just like ordinary spherical harmonics, but have an additional spin weight that reflects the additional symmetry. A special basis of harmonics can be derived from the Laplace spherical harmonics , and are typically denoted by , where and are the usual parameters familiar from the standard Laplace spherical harmonics. In this special basis, the spin-weighted spherical harmonics appear as actual functions, because the choice of a polar axis fixes the gauge ambiguity. The spin-weighted spherical harmonics can be obtained from the standard spherical harmonics by application of spin raising and lowering operators. In particular, the spin-weighted spherical harmonics of spin weight are simply the standard spherical harmonics:\n\nSpaces of spin-weighted spherical harmonics were first identified in connection with the representation theory of the Lorentz group . They were subsequently and independently rediscovered by and applied to describe gravitational radiation, and again by as so-called \"monopole harmonics\" in the study of Dirac monopoles.\n\nRegard the sphere as embedded into the three-dimensional Euclidean space . At a point on the sphere, a positively oriented orthonormal basis of tangent vectors at is a pair of vectors such that\nwhere the first pair of equations states that and are tangent at , the second pair states that and are unit vectors, the penultimate equation that and are orthogonal, and the final equation that is a right-handed basis of .\n\nA spin-weight function is a function accepting as input a point of and a positively oriented orthonormal basis of tangent vectors at , such that\nfor every rotation angle .\n\nFollowing , denote the collection of all spin-weight functions by . Concretely, these are understood as functions on } satisfying the following homogeneity law under complex scaling\nThis makes sense provided is a half-integer.\n\nAbstractly, is isomorphic to the smooth vector bundle underlying the antiholomorphic vector bundle of the Serre twist on the complex projective line . A section of the latter bundle is a function on } satisfying\nGiven such a , we may produce a spin-weight function by multiplying by a suitable power of the hermitian form\nSpecifically, is a spin-weight function. The association of a spin-weighted function to an ordinary homogeneous function is an isomorphism.\n\nThe spin weight bundles are equipped with a differential operator (eth). This operator is essentially the Dolbeault operator, after suitable identifications have been made,\nThus for ,\ndefines a function of spin-weight .\n\nJust as conventional spherical harmonics are the eigenfunctions of the Laplace-Beltrami operator on the sphere, the spin-weight harmonics are the eigensections for the Laplace-Beltrami operator acting on the bundles of spin-weight functions.\nThe spin-weighted harmonics can be represented as functions on a sphere once a point on the sphere has been selected to serve as the North pole. By definition, a function with \"spin weight \" transforms under rotation about the pole via\n\nWorking in standard spherical coordinates, we can define a particular operator acting on a function as:\nThis gives us another function of and . (The operator is effectively a covariant derivative operator in the sphere.)\n\nAn important property of the new function is that if had spin weight , has spin weight . Thus, the operator raises the spin weight of a function by 1. Similarly, we can define an operator which will lower the spin weight of a function by 1:\n\nThe spin-weighted spherical harmonics are then defined in terms of the usual spherical harmonics as:\nThe functions then have the property of transforming with spin weight .\n\nOther important properties include the following:\n\nThe harmonics are orthogonal over the entire sphere:\nand satisfy the completeness relation\n\nThese harmonics can be explicitly calculated by several methods. The obvious recursion relation results from repeatedly applying the raising or lowering operators. Formulae for direct calculation were derived by . Note that their formulae use an old choice for the Condon–Shortley phase. The convention chosen below is in agreement with Mathematica, for instance.\n\nThe more useful of the Goldberg, et al., formulae is the following:\n\nA Mathematica notebook using this formula to calculate arbitrary spin-weighted spherical harmonics can be found here.\n\nWith the phase convention here:\n\nAnalytic expressions for the first few orthonormalized spin-weighted spherical harmonics:\n\nThis relation allows the spin harmonics to be calculated using recursion relations for the -matrices.\n\nThe triple integral in the case that is given in terms of the 3- symbol:\n\n\n"}
{"id": "37571050", "url": "https://en.wikipedia.org/wiki?curid=37571050", "title": "Splicing rule", "text": "Splicing rule\n\nIn mathematics and computer science, a splicing rule is a transformation on formal languages which formalises the action of gene splicing in molecular biology. A splicing language is a language generated by iterated application of a splicing rule: the splicing languages form a proper subset of the regular languages.\n\nLet \"A\" be an alphabet and \"L\" a language, that is, a subset of the free monoid \"A\". A splicing rule is a quadruple \"r\" = (\"a\",\"b\",\"c\",\"d\") of elements of \"A\", and the action of the rule \"r\" on \"L\" is to produce the language\n\nIf \"R\" is a set of rules then \"R\"(\"L\") is the union of the languages produced by the rules of \"R\". We say that \"R\" \"respects\" \"L\" if \"R\"(\"L\") is a subset of \"L\". The \"R\"-closure of \"L\" is the union of \"L\" and all iterates of \"R\" on \"L\": clearly it is respected by \"R\". A splicing language is the \"R\"-closure of a finite language.\n\nA rule set \"R\" is reflexive if (\"a\",\"b\",\"c\",\"d\") in \"R\" implies that (\"a\",\"b\",\"a\",\"b\") and (\"c\",\"d\",\"c\",\"d\") are in \"R\". A splicing language is reflexive if it is defined by a reflexive rule set.\n\n\n"}
{"id": "1053995", "url": "https://en.wikipedia.org/wiki?curid=1053995", "title": "Stevens's power law", "text": "Stevens's power law\n\nStevens's power law is an empirical relationship in psychophysics between an increased intensity or strength in a physical stimulus and the perceived magnitude increase in the sensation created by the stimulus. It is often considered to supersede the Weber–Fechner law, based on a logarithmic relationship between stimulus and sensation, because the power law describes a wider range of sensory comparisons.\n\nThe theory is named after psychophysicist Stanley Smith Stevens (1906–1973). Although the idea of a power law had been suggested by 19th-century researchers, Stevens is credited with reviving the law and publishing a body of psychophysical data to support it in 1957.\n\nThe general form of the law is\nwhere \"I\" is the intensity or strength of the stimulus in physical units (energy, weight, pressure, mixture proportions, etc.), ψ(\"I\") is the magnitude of the sensation evoked by the stimulus, \"a\" is an exponent that depends on the type of stimulation or sensory modality, and \"k\" is a proportionality constant that depends on the units used.\n\nA distinction has been made between local psychophysics, where stimuli can only be discriminated with a probability around 50%, and global psychophysics, where the stimuli can be discriminated correctly with near certainty (Luce & Krumhansl, 1988). The Weber–Fechner law and methods described by L. L. Thurstone are generally applied in local psychophysics, whereas Stevens's methods are usually applied in global psychophysics.\n\nThe table to the right lists the exponents reported by Stevens.\n\nThe principal methods used by Stevens to measure the perceived intensity of a stimulus were \"magnitude estimation\" and \"magnitude production\". In magnitude estimation with a standard, the experimenter presents a stimulus called a \"standard\" and assigns it a number called the \"modulus\". For subsequent stimuli, subjects report numerically their perceived intensity relative to the standard so as to preserve the ratio between the sensations and the numerical estimates (e.g., a sound perceived twice as loud as the standard should be given a number twice the modulus). In magnitude estimation without a standard (usually just \"magnitude estimation\"), subjects are free to choose their own standard, assigning any number to the first stimulus and all subsequent ones with the only requirement being that the ratio between sensations and numbers is preserved. In magnitude production a number and a reference stimulus is given and subjects produce a stimulus that is perceived as that number times the reference. Also used is \"cross-modality matching\", which generally involves subjects altering the magnitude of one physical quantity, such as the brightness of a light, so that its perceived intensity is equal to the perceived intensity of another type of quantity, such as warmth or pressure.\n\nStevens generally collected magnitude estimation data from multiple observers, averaged the data across subjects, and then fitted a power function to the data. Because the fit was generally reasonable, he concluded the power law was correct.\n\nA principal criticism has been that Stevens's approach provides neither a direct test of the power law itself nor the underlying assumptions of the magnitude estimation/production method: it simply fits curves to data points. In addition, the power law can be deduced mathematically from the Weber-Fechner logarithmic function (Mackay, 1963), and the relation makes predictions consistent with data (Staddon, 1978). As with all psychometric studies, Stevens's approach ignores individual differences in the stimulus-sensation relationship, and there are generally large individual differences in this relationship that averaging the data will obscure .\n\nStevens's main assertion was that using magnitude estimations/productions respondents were able to make judgements on a ratio scale (i.e., if \"x\" and \"y\" are values on a given ratio scale, then there exists a constant \"k\" such that \"x\" = \"ky\"). In the context of axiomatic psychophysics, formulated a testable property capturing the implicit underlying assumption this assertion entailed. Specifically, for two proportions \"p\" and \"q\", and three stimuli, \"x\", \"y\", \"z\", if \"y\" is judged \"p\" times \"x\", \"z\" is judged \"q\" times \"y\", then \"t\" = \"pq\" times \"x\" should be equal to \"z\". This amounts to assuming that respondents interpret numbers in a veridical way. This property was unambiguously rejected (, ). Without assuming veridical interpretation of numbers, formulated another property that, if sustained, meant that respondents could make ratio scaled judgments, namely, if \"y\" is judged \"p\" times \"x\", \"z\" is judged \"q\" times \"y\", and if \"y\" is judged \"q\" times \"x\", \"z\" is judged \"p\" times \"y\", then \"z\" should equal \"z\". This property has been sustained in a variety of situations (, ).\n\nCritics of the power law also point out that the validity of the law is contingent on the measurement of perceived stimulus intensity that is employed in the relevant experiments. , under the condition that respondents' numerical distortion function and the psychophysical functions could be separated, formulated a behavioral condition equivalent to the psychophysical function being a power function. This condition was confirmed for just over half the respondents, and the power form was found to be a reasonable approximation for the rest .\n\nIt has also been questioned, particularly in terms of signal detection theory, whether any given stimulus is actually associated with a particular and \"absolute\" perceived intensity; i.e. one that is independent of contextual factors and conditions. Consistent with this, Luce (1990, p. 73) observed that \"by introducing contexts such as background noise in loudness judgements, the shape of the magnitude estimation functions certainly deviates sharply from a power function\". Indeed, nearly all sensory judgments can be changed by the context in which a stimulus is perceived.\n\n\n"}
{"id": "30123945", "url": "https://en.wikipedia.org/wiki?curid=30123945", "title": "Stochastic volatility jump", "text": "Stochastic volatility jump\n\nIn mathematical finance, the stochastic volatility jump (SVJ) model is suggested by Bates. This model fits the observed implied volatility surface well. The model is a Heston process with an added Merton log-normal jump.\n\nThe model assumes the following correlated processes:\n\n[where \"S\" = Price of Security, \"μ\" = constant drift (i.e. expected return), \"t\" = time, \"Z\" = Standard Brownian Motion etc.]\n"}
{"id": "28305", "url": "https://en.wikipedia.org/wiki?curid=28305", "title": "String theory", "text": "String theory\n\nIn physics, string theory is a theoretical framework in which the point-like particles of particle physics are replaced by one-dimensional objects called strings. It describes how these strings propagate through space and interact with each other. On distance scales larger than the string scale, a string looks just like an ordinary particle, with its mass, charge, and other properties determined by the vibrational state of the string. In string theory, one of the many vibrational states of the string corresponds to the graviton, a quantum mechanical particle that carries gravitational force. Thus string theory is a theory of quantum gravity.\n\nString theory is a broad and varied subject that attempts to address a number of deep questions of fundamental physics. String theory has been applied to a variety of problems in black hole physics, early universe cosmology, nuclear physics, and condensed matter physics, and it has stimulated a number of major developments in pure mathematics. Because string theory potentially provides a unified description of gravity and particle physics, it is a candidate for a theory of everything, a self-contained mathematical model that describes all fundamental forces and forms of matter. Despite much work on these problems, it is not known to what extent string theory describes the real world or how much freedom the theory allows in the choice of its details. \n\nString theory was first studied in the late 1960s as a theory of the strong nuclear force, before being abandoned in favor of quantum chromodynamics. Subsequently, it was realized that the very properties that made string theory unsuitable as a theory of nuclear physics made it a promising candidate for a quantum theory of gravity. The earliest version of string theory, bosonic string theory, incorporated only the class of particles known as bosons. It later developed into superstring theory, which posits a connection called supersymmetry between bosons and the class of particles called fermions. Five consistent versions of superstring theory were developed before it was conjectured in the mid-1990s that they were all different limiting cases of a single theory in eleven dimensions known as M-theory. In late 1997, theorists discovered an important relationship called the AdS/CFT correspondence, which relates string theory to another type of physical theory called a quantum field theory.\n\nOne of the challenges of string theory is that the full theory does not have a satisfactory definition in all circumstances. Another issue is that the theory is thought to describe an enormous landscape of possible universes, and this has complicated efforts to develop theories of particle physics based on string theory. These issues have led some in the community to criticize these approaches to physics and question the value of continued research on string theory unification.\n\nIn the twentieth century, two theoretical frameworks emerged for formulating the laws of physics. The first is Albert Einstein's general theory of relativity, a theory that explains the force of gravity and the structure of space and time. The other is quantum mechanics which is a completely different formulation to describe physical phenomena using the known probability principles. By the late 1970s, these two frameworks had proven to be sufficient to explain most of the observed features of the universe, from elementary particles to atoms to the evolution of stars and the universe as a whole.\n\nIn spite of these successes, there are still many problems that remain to be solved. One of the deepest problems in modern physics is the problem of quantum gravity. The general theory of relativity is formulated within the framework of classical physics, whereas the other fundamental forces are described within the framework of quantum mechanics. A quantum theory of gravity is needed in order to reconcile general relativity with the principles of quantum mechanics, but difficulties arise when one attempts to apply the usual prescriptions of quantum theory to the force of gravity. In addition to the problem of developing a consistent theory of quantum gravity, there are many other fundamental problems in the physics of atomic nuclei, black holes, and the early universe.\n\nString theory is a theoretical framework that attempts to address these questions and many others. The starting point for string theory is the idea that the point-like particles of particle physics can also be modeled as one-dimensional objects called strings. String theory describes how strings propagate through space and interact with each other. In a given version of string theory, there is only one kind of string, which may look like a small loop or segment of ordinary string, and it can vibrate in different ways. On distance scales larger than the string scale, a string will look just like an ordinary particle, with its mass, charge, and other properties determined by the vibrational state of the string. In this way, all of the different elementary particles may be viewed as vibrating strings. In string theory, one of the vibrational states of the string gives rise to the graviton, a quantum mechanical particle that carries gravitational force. Thus string theory is a theory of quantum gravity.\n\nOne of the main developments of the past several decades in string theory was the discovery of certain \"dualities\", mathematical transformations that identify one physical theory with another. Physicists studying string theory have discovered a number of these dualities between different versions of string theory, and this has led to the conjecture that all consistent versions of string theory are subsumed in a single framework known as M-theory.\n\nStudies of string theory have also yielded a number of results on the nature of black holes and the gravitational interaction. There are certain paradoxes that arise when one attempts to understand the quantum aspects of black holes, and work on string theory has attempted to clarify these issues. In late 1997 this line of work culminated in the discovery of the anti-de Sitter/conformal field theory correspondence or AdS/CFT. This is a theoretical result which relates string theory to other physical theories which are better understood theoretically. The AdS/CFT correspondence has implications for the study of black holes and quantum gravity, and it has been applied to other subjects, including nuclear and condensed matter physics.\n\nSince string theory incorporates all of the fundamental interactions, including gravity, many physicists hope that it fully describes our universe, making it a theory of everything. One of the goals of current research in string theory is to find a solution of the theory that reproduces the observed spectrum of elementary particles, with a small cosmological constant, containing dark matter and a plausible mechanism for cosmic inflation. While there has been progress toward these goals, it is not known to what extent string theory describes the real world or how much freedom the theory allows in the choice of details.\n\nOne of the challenges of string theory is that the full theory does not have a satisfactory definition in all circumstances. The scattering of strings is most straightforwardly defined using the techniques of perturbation theory, but it is not known in general how to define string theory nonperturbatively. It is also not clear whether there is any principle by which string theory selects its vacuum state, the physical state that determines the properties of our universe. These problems have led some in the community to criticize these approaches to the unification of physics and question the value of continued research on these problems.\n\nThe application of quantum mechanics to physical objects such as the electromagnetic field, which are extended in space and time, is known as quantum field theory. In particle physics, quantum field theories form the basis for our understanding of elementary particles, which are modeled as excitations in the fundamental fields.\n\nIn quantum field theory, one typically computes the probabilities of various physical events using the techniques of perturbation theory. Developed by Richard Feynman and others in the first half of the twentieth century, perturbative quantum field theory uses special diagrams called Feynman diagrams to organize computations. One imagines that these diagrams depict the paths of point-like particles and their interactions.\n\nThe starting point for string theory is the idea that the point-like particles of quantum field theory can also be modeled as one-dimensional objects called strings. The interaction of strings is most straightforwardly defined by generalizing the perturbation theory used in ordinary quantum field theory. At the level of Feynman diagrams, this means replacing the one-dimensional diagram representing the path of a point particle by a two-dimensional surface representing the motion of a string. Unlike in quantum field theory, string theory does not have a full non-perturbative definition, so many of the theoretical questions that physicists would like to answer remain out of reach.\n\nIn theories of particle physics based on string theory, the characteristic length scale of strings is assumed to be on the order of the Planck length, or meters, the scale at which the effects of quantum gravity are believed to become significant. On much larger length scales, such as the scales visible in physics laboratories, such objects would be indistinguishable from zero-dimensional point particles, and the vibrational state of the string would determine the type of particle. One of the vibrational states of a string corresponds to the graviton, a quantum mechanical particle that carries the gravitational force.\n\nThe original version of string theory was bosonic string theory, but this version described only bosons, a class of particles which transmit forces between the matter particles, or fermions. Bosonic string theory was eventually superseded by theories called superstring theories. These theories describe both bosons and fermions, and they incorporate a theoretical idea called supersymmetry. This is a mathematical relation that exists in certain physical theories between the bosons and fermions. In theories with supersymmetry, each boson has a counterpart which is a fermion, and vice versa.\n\nThere are several versions of superstring theory: type I, type IIA, type IIB, and two flavors of heterotic string theory ( and ). The different theories allow different types of strings, and the particles that arise at low energies exhibit different symmetries. For example, the type I theory includes both open strings (which are segments with endpoints) and closed strings (which form closed loops), while types IIA, IIB and heterotic include only closed strings.\n\nIn everyday life, there are three familiar dimensions of space: height, width and length. Einstein's general theory of relativity treats time as a dimension on par with the three spatial dimensions; in general relativity, space and time are not modeled as separate entities but are instead unified to a four-dimensional spacetime. In this framework, the phenomenon of gravity is viewed as a consequence of the geometry of spacetime.\n\nIn spite of the fact that the universe is well described by four-dimensional spacetime, there are several reasons why physicists consider theories in other dimensions. In some cases, by modeling spacetime in a different number of dimensions, a theory becomes more mathematically tractable, and one can perform calculations and gain general insights more easily. There are also situations where theories in two or three spacetime dimensions are useful for describing phenomena in condensed matter physics. Finally, there exist scenarios in which there could actually be more than four dimensions of spacetime which have nonetheless managed to escape detection.\n\nOne notable feature of string theories is that these theories require extra dimensions of spacetime for their mathematical consistency. In bosonic string theory, spacetime is 26-dimensional, while in superstring theory it is 10-dimensional, and in M-theory it is 11-dimensional. In order to describe real physical phenomena using string theory, one must therefore imagine scenarios in which these extra dimensions would not be observed in experiments.\n\nCompactification is one way of modifying the number of dimensions in a physical theory. In compactification, some of the extra dimensions are assumed to \"close up\" on themselves to form circles. In the limit where these curled up dimensions become very small, one obtains a theory in which spacetime has effectively a lower number of dimensions. A standard analogy for this is to consider a multidimensional object such as a garden hose. If the hose is viewed from a sufficient distance, it appears to have only one dimension, its length. However, as one approaches the hose, one discovers that it contains a second dimension, its circumference. Thus, an ant crawling on the surface of the hose would move in two dimensions.\n\nCompactification can be used to construct models in which spacetime is effectively four-dimensional. However, not every way of compactifying the extra dimensions produces a model with the right properties to describe nature. In a viable model of particle physics, the compact extra dimensions must be shaped like a Calabi–Yau manifold. A Calabi–Yau manifold is a special space which is typically taken to be six-dimensional in applications to string theory. It is named after mathematicians Eugenio Calabi and Shing-Tung Yau.\n\nAnother approach to reducing the number of dimensions is the so-called brane-world scenario. In this approach, physicists assume that the observable universe is a four-dimensional subspace of a higher dimensional space. In such models, the force-carrying bosons of particle physics arise from open strings with endpoints attached to the four-dimensional subspace, while gravity arises from closed strings propagating through the larger ambient space. This idea plays an important role in attempts to develop models of real world physics based on string theory, and it provides a natural explanation for the weakness of gravity compared to the other fundamental forces.\n\nOne notable fact about string theory is that the different versions of the theory all turn out to be related in highly nontrivial ways. One of the relationships that can exist between different string theories is called S-duality. This is a relationship which says that a collection of strongly interacting particles in one theory can, in some cases, be viewed as a collection of weakly interacting particles in a completely different theory. Roughly speaking, a collection of particles is said to be strongly interacting if they combine and decay often and weakly interacting if they do so infrequently. Type I string theory turns out to be equivalent by S-duality to the heterotic string theory. Similarly, type IIB string theory is related to itself in a nontrivial way by S-duality.\n\nAnother relationship between different string theories is T-duality. Here one considers strings propagating around a circular extra dimension. T-duality states that a string propagating around a circle of radius is equivalent to a string propagating around a circle of radius in the sense that all observable quantities in one description are identified with quantities in the dual description. For example, a string has momentum as it propagates around a circle, and it can also wind around the circle one or more times. The number of times the string winds around a circle is called the winding number. If a string has momentum and winding number in one description, it will have momentum and winding number in the dual description. For example, type IIA string theory is equivalent to type IIB string theory via T-duality, and the two versions of heterotic string theory are also related by T-duality.\n\nIn general, the term \"duality\" refers to a situation where two seemingly different physical systems turn out to be equivalent in a nontrivial way. Two theories related by a duality need not be string theories. For example, Montonen–Olive duality is example of an S-duality relationship between quantum field theories. The AdS/CFT correspondence is example of a duality which relates string theory to a quantum field theory. If two theories are related by a duality, it means that one theory can be transformed in some way so that it ends up looking just like the other theory. The two theories are then said to be \"dual\" to one another under the transformation. Put differently, the two theories are mathematically different descriptions of the same phenomena.\n\nIn string theory and other related theories, a brane is a physical object that generalizes the notion of a point particle to higher dimensions. For instance, a point particle can be viewed as a brane of dimension zero, while a string can be viewed as a brane of dimension one. It is also possible to consider higher-dimensional branes. In dimension \"p\", these are called \"p\"-branes. The word brane comes from the word \"membrane\" which refers to a two-dimensional brane.\n\nBranes are dynamical objects which can propagate through spacetime according to the rules of quantum mechanics. They have mass and can have other attributes such as charge. A \"p\"-brane sweeps out a (\"p\"+1)-dimensional volume in spacetime called its \"worldvolume\". Physicists often study fields analogous to the electromagnetic field which live on the worldvolume of a brane.\n\nIn string theory, D-branes are an important class of branes that arise when one considers open strings. As an open string propagates through spacetime, its endpoints are required to lie on a D-brane. The letter \"D\" in D-brane refers to a certain mathematical condition on the system known as the Dirichlet boundary condition. The study of D-branes in string theory has led to important results such as the AdS/CFT correspondence, which has shed light on many problems in quantum field theory.\n\nBranes are frequently studied from a purely mathematical point of view, and they are described as objects of certain categories, such as the derived category of coherent sheaves on a complex algebraic variety, or the Fukaya category of a symplectic manifold. The connection between the physical notion of a brane and the mathematical notion of a category has led to important mathematical insights in the fields of algebraic and symplectic geometry and representation theory.\n\nPrior to 1995, theorists believed that there were five consistent versions of superstring theory (type I, type IIA, type IIB, and two versions of heterotic string theory). This understanding changed in 1995 when Edward Witten suggested that the five theories were just special limiting cases of an eleven-dimensional theory called M-theory. Witten's conjecture was based on the work of a number of other physicists, including Ashoke Sen, Chris Hull, Paul Townsend, and Michael Duff. His announcement led to a flurry of research activity now known as the second superstring revolution.\n\nIn the 1970s, many physicists became interested in supergravity theories, which combine general relativity with supersymmetry. Whereas general relativity makes sense in any number of dimensions, supergravity places an upper limit on the number of dimensions. In 1978, work by Werner Nahm showed that the maximum spacetime dimension in which one can formulate a consistent supersymmetric theory is eleven. In the same year, Eugene Cremmer, Bernard Julia, and Joel Scherk of the École Normale Supérieure showed that supergravity not only permits up to eleven dimensions but is in fact most elegant in this maximal number of dimensions.\n\nInitially, many physicists hoped that by compactifying eleven-dimensional supergravity, it might be possible to construct realistic models of our four-dimensional world. The hope was that such models would provide a unified description of the four fundamental forces of nature: electromagnetism, the strong and weak nuclear forces, and gravity. Interest in eleven-dimensional supergravity soon waned as various flaws in this scheme were discovered. One of the problems was that the laws of physics appear to distinguish between clockwise and counterclockwise, a phenomenon known as chirality. Edward Witten and others observed this chirality property cannot be readily derived by compactifying from eleven dimensions.\n\nIn the first superstring revolution in 1984, many physicists turned to string theory as a unified theory of particle physics and quantum gravity. Unlike supergravity theory, string theory was able to accommodate the chirality of the standard model, and it provided a theory of gravity consistent with quantum effects. Another feature of string theory that many physicists were drawn to in the 1980s and 1990s was its high degree of uniqueness. In ordinary particle theories, one can consider any collection of elementary particles whose classical behavior is described by an arbitrary Lagrangian. In string theory, the possibilities are much more constrained: by the 1990s, physicists had argued that there were only five consistent supersymmetric versions of the theory.\n\nAlthough there were only a handful of consistent superstring theories, it remained a mystery why there was not just one consistent formulation. However, as physicists began to examine string theory more closely, they realized that these theories are related in intricate and nontrivial ways. They found that a system of strongly interacting strings can, in some cases, be viewed as a system of weakly interacting strings. This phenomenon is known as S-duality. It was studied by Ashoke Sen in the context of heterotic strings in four dimensions and by Chris Hull and Paul Townsend in the context of the type IIB theory. Theorists also found that different string theories may be related by T-duality. This duality implies that strings propagating on completely different spacetime geometries may be physically equivalent.\n\nAt around the same time, as many physicists were studying the properties of strings, a small group of physicists was examining the possible applications of higher dimensional objects. In 1987, Eric Bergshoeff, Ergin Sezgin, and Paul Townsend showed that eleven-dimensional supergravity includes two-dimensional branes. Intuitively, these objects look like sheets or membranes propagating through the eleven-dimensional spacetime. Shortly after this discovery, Michael Duff, Paul Howe, Takeo Inami, and Kellogg Stelle considered a particular compactification of eleven-dimensional supergravity with one of the dimensions curled up into a circle. In this setting, one can imagine the membrane wrapping around the circular dimension. If the radius of the circle is sufficiently small, then this membrane looks just like a string in ten-dimensional spacetime. In fact, Duff and his collaborators showed that this construction reproduces exactly the strings appearing in type IIA superstring theory.\n\nSpeaking at a string theory conference in 1995, Edward Witten made the surprising suggestion that all five superstring theories were in fact just different limiting cases of a single theory in eleven spacetime dimensions. Witten's announcement drew together all of the previous results on S- and T-duality and the appearance of higher dimensional branes in string theory. In the months following Witten's announcement, hundreds of new papers appeared on the Internet confirming different parts of his proposal. Today this flurry of work is known as the second superstring revolution.\n\nInitially, some physicists suggested that the new theory was a fundamental theory of membranes, but Witten was skeptical of the role of membranes in the theory. In a paper from 1996, Hořava and Witten wrote \"As it has been proposed that the eleven-dimensional theory is a supermembrane theory but there are some reasons to doubt that interpretation, we will non-committally call it the M-theory, leaving to the future the relation of M to membranes.\" In the absence of an understanding of the true meaning and structure of M-theory, Witten has suggested that the \"M\" should stand for \"magic\", \"mystery\", or \"membrane\" according to taste, and the true meaning of the title should be decided when a more fundamental formulation of the theory is known.\n\nIn mathematics, a matrix is a rectangular array of numbers or other data. In physics, a matrix model is a particular kind of physical theory whose mathematical formulation involves the notion of a matrix in an important way. A matrix model describes the behavior of a set of matrices within the framework of quantum mechanics.\n\nOne important example of a matrix model is the BFSS matrix model proposed by Tom Banks, Willy Fischler, Stephen Shenker, and Leonard Susskind in 1997. This theory describes the behavior of a set of nine large matrices. In their original paper, these authors showed, among other things, that the low energy limit of this matrix model is described by eleven-dimensional supergravity. These calculations led them to propose that the BFSS matrix model is exactly equivalent to M-theory. The BFSS matrix model can therefore be used as a prototype for a correct formulation of M-theory and a tool for investigating the properties of M-theory in a relatively simple setting.\n\nThe development of the matrix model formulation of M-theory has led physicists to consider various connections between string theory and a branch of mathematics called noncommutative geometry. This subject is a generalization of ordinary geometry in which mathematicians define new geometric notions using tools from noncommutative algebra. In a paper from 1998, Alain Connes, Michael R. Douglas, and Albert Schwarz showed that some aspects of matrix models and M-theory are described by a noncommutative quantum field theory, a special kind of physical theory in which spacetime is described mathematically using noncommutative geometry. This established a link between matrix models and M-theory on the one hand, and noncommutative geometry on the other hand. It quickly led to the discovery of other important links between noncommutative geometry and various physical theories.\n\nIn general relativity, a black hole is defined as a region of spacetime in which the gravitational field is so strong that no particle or radiation can escape. In the currently accepted models of stellar evolution, black holes are thought to arise when massive stars undergo gravitational collapse, and many galaxies are thought to contain supermassive black holes at their centers. Black holes are also important for theoretical reasons, as they present profound challenges for theorists attempting to understand the quantum aspects of gravity. String theory has proved to be an important tool for investigating the theoretical properties of black holes because it provides a framework in which theorists can study their thermodynamics.\n\nIn the branch of physics called statistical mechanics, entropy is a measure of the randomness or disorder of a physical system. This concept was studied in the 1870s by the Austrian physicist Ludwig Boltzmann, who showed that the thermodynamic properties of a gas could be derived from the combined properties of its many constituent molecules. Boltzmann argued that by averaging the behaviors of all the different molecules in a gas, one can understand macroscopic properties such as volume, temperature, and pressure. In addition, this perspective led him to give a precise definition of entropy as the natural logarithm of the number of different states of the molecules (also called \"microstates\") that give rise to the same macroscopic features.\n\nIn the twentieth century, physicists began to apply the same concepts to black holes. In most systems such as gases, the entropy scales with the volume. In the 1970s, the physicist Jacob Bekenstein suggested that the entropy of a black hole is instead proportional to the \"surface area\" of its event horizon, the boundary beyond which matter and radiation is lost to its gravitational attraction. When combined with ideas of the physicist Stephen Hawking, Bekenstein's work yielded a precise formula for the entropy of a black hole. The Bekenstein–Hawking formula expresses the entropy as\n\nwhere is the speed of light, is Boltzmann's constant, is the reduced Planck constant, is Newton's constant, and is the surface area of the event horizon.\n\nLike any physical system, a black hole has an entropy defined in terms of the number of different microstates that lead to the same macroscopic features. The Bekenstein–Hawking entropy formula gives the expected value of the entropy of a black hole, but by the 1990s, physicists still lacked a derivation of this formula by counting microstates in a theory of quantum gravity. Finding such a derivation of this formula was considered an important test of the viability of any theory of quantum gravity such as string theory.\n\nIn a paper from 1996, Andrew Strominger and Cumrun Vafa showed how to derive the Beckenstein–Hawking formula for certain black holes in string theory. Their calculation was based on the observation that D-branes—which look like fluctuating membranes when they are weakly interacting—become dense, massive objects with event horizons when the interactions are strong. In other words, a system of strongly interacting D-branes in string theory is indistinguishable from a black hole. Strominger and Vafa analyzed such D-brane systems and calculated the number of different ways of placing D-branes in spacetime so that their combined mass and charge is equal to a given mass and charge for the resulting black hole. Their calculation reproduced the Bekenstein–Hawking formula exactly, including the factor of . Subsequent work by Strominger, Vafa, and others refined the original calculations and gave the precise values of the \"quantum corrections\" needed to describe very small black holes.\n\nThe black holes that Strominger and Vafa considered in their original work were quite different from real astrophysical black holes. One difference was that Strominger and Vafa considered only extremal black holes in order to make the calculation tractable. These are defined as black holes with the lowest possible mass compatible with a given charge. Strominger and Vafa also restricted attention to black holes in five-dimensional spacetime with unphysical supersymmetry.\n\nAlthough it was originally developed in this very particular and physically unrealistic context in string theory, the entropy calculation of Strominger and Vafa has led to a qualitative understanding of how black hole entropy can be accounted for in any theory of quantum gravity. Indeed, in 1998, Strominger argued that the original result could be generalized to an arbitrary consistent theory of quantum gravity without relying on strings or supersymmetry. In collaboration with several other authors in 2010, he showed that some results on black hole entropy could be extended to non-extremal astrophysical black holes.\n\nOne approach to formulating string theory and studying its properties is provided by the anti-de Sitter/conformal field theory (AdS/CFT) correspondence. This is a theoretical result which implies that string theory is in some cases equivalent to a quantum field theory. In addition to providing insights into the mathematical structure of string theory, the AdS/CFT correspondence has shed light on many aspects of quantum field theory in regimes where traditional calculational techniques are ineffective. The AdS/CFT correspondence was first proposed by Juan Maldacena in late 1997. Important aspects of the correspondence were elaborated in articles by Steven Gubser, Igor Klebanov, and Alexander Markovich Polyakov, and by Edward Witten. By 2010, Maldacena's article had over 7000 citations, becoming the most highly cited article in the field of high energy physics.\n\nIn the AdS/CFT correspondence, the geometry of spacetime is described in terms of a certain vacuum solution of Einstein's equation called anti-de Sitter space. In very elementary terms, anti-de Sitter space is a mathematical model of spacetime in which the notion of distance between points (the metric) is different from the notion of distance in ordinary Euclidean geometry. It is closely related to hyperbolic space, which can be viewed as a disk as illustrated on the left. This image shows a tessellation of a disk by triangles and squares. One can define the distance between points of this disk in such a way that all the triangles and squares are the same size and the circular outer boundary is infinitely far from any point in the interior.\n\nOne can imagine a stack of hyperbolic disks where each disk represents the state of the universe at a given time. The resulting geometric object is three-dimensional anti-de Sitter space. It looks like a solid cylinder in which any cross section is a copy of the hyperbolic disk. Time runs along the vertical direction in this picture. The surface of this cylinder plays an important role in the AdS/CFT correspondence. As with the hyperbolic plane, anti-de Sitter space is curved in such a way that any point in the interior is actually infinitely far from this boundary surface.\n\nThis construction describes a hypothetical universe with only two space dimensions and one time dimension, but it can be generalized to any number of dimensions. Indeed, hyperbolic space can have more than two dimensions and one can \"stack up\" copies of hyperbolic space to get higher-dimensional models of anti-de Sitter space.\n\nAn important feature of anti-de Sitter space is its boundary (which looks like a cylinder in the case of three-dimensional anti-de Sitter space). One property of this boundary is that, within a small region on the surface around any given point, it looks just like Minkowski space, the model of spacetime used in nongravitational physics. One can therefore consider an auxiliary theory in which \"spacetime\" is given by the boundary of anti-de Sitter space. This observation is the starting point for AdS/CFT correspondence, which states that the boundary of anti-de Sitter space can be regarded as the \"spacetime\" for a quantum field theory. The claim is that this quantum field theory is equivalent to a gravitational theory, such as string theory, in the bulk anti-de Sitter space in the sense that there is a \"dictionary\" for translating entities and calculations in one theory into their counterparts in the other theory. For example, a single particle in the gravitational theory might correspond to some collection of particles in the boundary theory. In addition, the predictions in the two theories are quantitatively identical so that if two particles have a 40 percent chance of colliding in the gravitational theory, then the corresponding collections in the boundary theory would also have a 40 percent chance of colliding.\n\nThe discovery of the AdS/CFT correspondence was a major advance in physicists' understanding of string theory and quantum gravity. One reason for this is that the correspondence provides a formulation of string theory in terms of quantum field theory, which is well understood by comparison. Another reason is that it provides a general framework in which physicists can study and attempt to resolve the paradoxes of black holes.\n\nIn 1975, Stephen Hawking published a calculation which suggested that black holes are not completely black but emit a dim radiation due to quantum effects near the event horizon. At first, Hawking's result posed a problem for theorists because it suggested that black holes destroy information. More precisely, Hawking's calculation seemed to conflict with one of the basic postulates of quantum mechanics, which states that physical systems evolve in time according to the Schrödinger equation. This property is usually referred to as unitarity of time evolution. The apparent contradiction between Hawking's calculation and the unitarity postulate of quantum mechanics came to be known as the black hole information paradox.\n\nThe AdS/CFT correspondence resolves the black hole information paradox, at least to some extent, because it shows how a black hole can evolve in a manner consistent with quantum mechanics in some contexts. Indeed, one can consider black holes in the context of the AdS/CFT correspondence, and any such black hole corresponds to a configuration of particles on the boundary of anti-de Sitter space. These particles obey the usual rules of quantum mechanics and in particular evolve in a unitary fashion, so the black hole must also evolve in a unitary fashion, respecting the principles of quantum mechanics. In 2005, Hawking announced that the paradox had been settled in favor of information conservation by the AdS/CFT correspondence, and he suggested a concrete mechanism by which black holes might preserve information.\n\nIn addition to its applications to theoretical problems in quantum gravity, the AdS/CFT correspondence has been applied to a variety of problems in quantum field theory. One physical system that has been studied using the AdS/CFT correspondence is the quark–gluon plasma, an exotic state of matter produced in particle accelerators. This state of matter arises for brief instants when heavy ions such as gold or lead nuclei are collided at high energies. Such collisions cause the quarks that make up atomic nuclei to deconfine at temperatures of approximately two trillion kelvins, conditions similar to those present at around seconds after the Big Bang.\n\nThe physics of the quark–gluon plasma is governed by a theory called quantum chromodynamics, but this theory is mathematically intractable in problems involving the quark–gluon plasma. In an article appearing in 2005, Đàm Thanh Sơn and his collaborators showed that the AdS/CFT correspondence could be used to understand some aspects of the quark–gluon plasma by describing it in the language of string theory. By applying the AdS/CFT correspondence, Sơn and his collaborators were able to describe the quark gluon plasma in terms of black holes in five-dimensional spacetime. The calculation showed that the ratio of two quantities associated with the quark–gluon plasma, the shear viscosity and volume density of entropy, should be approximately equal to a certain universal constant. In 2008, the predicted value of this ratio for the quark–gluon plasma was confirmed at the Relativistic Heavy Ion Collider at Brookhaven National Laboratory.\n\nThe AdS/CFT correspondence has also been used to study aspects of condensed matter physics. Over the decades, experimental condensed matter physicists have discovered a number of exotic states of matter, including superconductors and superfluids. These states are described using the formalism of quantum field theory, but some phenomena are difficult to explain using standard field theoretic techniques. Some condensed matter theorists including Subir Sachdev hope that the AdS/CFT correspondence will make it possible to describe these systems in the language of string theory and learn more about their behavior.\n\nSo far some success has been achieved in using string theory methods to describe the transition of a superfluid to an insulator. A superfluid is a system of electrically neutral atoms that flows without any friction. Such systems are often produced in the laboratory using liquid helium, but recently experimentalists have developed new ways of producing artificial superfluids by pouring trillions of cold atoms into a lattice of criss-crossing lasers. These atoms initially behave as a superfluid, but as experimentalists increase the intensity of the lasers, they become less mobile and then suddenly transition to an insulating state. During the transition, the atoms behave in an unusual way. For example, the atoms slow to a halt at a rate that depends on the temperature and on Planck's constant, the fundamental parameter of quantum mechanics, which does not enter into the description of the other phases. This behavior has recently been understood by considering a dual description where properties of the fluid are described in terms of a higher dimensional black hole.\n\nIn addition to being an idea of considerable theoretical interest, string theory provides a framework for constructing models of real world physics that combine general relativity and particle physics. Phenomenology is the branch of theoretical physics in which physicists construct realistic models of nature from more abstract theoretical ideas. String phenomenology is the part of string theory that attempts to construct realistic or semi-realistic models based on string theory.\n\nPartly because of theoretical and mathematical difficulties and partly because of the extremely high energies needed to test these theories experimentally, there is so far no experimental evidence that would unambiguously point to any of these models being a correct fundamental description of nature. This has led some in the community to criticize these approaches to unification and question the value of continued research on these problems.\n\nThe currently accepted theory describing elementary particles and their interactions is known as the standard model of particle physics. This theory provides a unified description of three of the fundamental forces of nature: electromagnetism and the strong and weak nuclear forces. Despite its remarkable success in explaining a wide range of physical phenomena, the standard model cannot be a complete description of reality. This is because the standard model fails to incorporate the force of gravity and because of problems such as the hierarchy problem and the inability to explain the structure of fermion masses or dark matter.\n\nString theory has been used to construct a variety of models of particle physics going beyond the standard model. Typically, such models are based on the idea of compactification. Starting with the ten- or eleven-dimensional spacetime of string or M-theory, physicists postulate a shape for the extra dimensions. By choosing this shape appropriately, they can construct models roughly similar to the standard model of particle physics, together with additional undiscovered particles. One popular way of deriving realistic physics from string theory is to start with the heterotic theory in ten dimensions and assume that the six extra dimensions of spacetime are shaped like a six-dimensional Calabi–Yau manifold. Such compactifications offer many ways of extracting realistic physics from string theory. Other similar methods can be used to construct realistic or semi-realistic models of our four-dimensional world based on M-theory.\n\nThe Big Bang theory is the prevailing cosmological model for the universe from the earliest known periods through its subsequent large-scale evolution. Despite its success in explaining many observed features of the universe including galactic redshifts, the relative abundance of light elements such as hydrogen and helium, and the existence of a cosmic microwave background, there are several questions that remain unanswered. For example, the standard Big Bang model does not explain why the universe appears to be same in all directions, why it appears flat on very large distance scales, or why certain hypothesized particles such as magnetic monopoles are not observed in experiments.\n\nCurrently, the leading candidate for a theory going beyond the Big Bang is the theory of cosmic inflation. Developed by Alan Guth and others in the 1980s, inflation postulates a period of extremely rapid accelerated expansion of the universe prior to the expansion described by the standard Big Bang theory. The theory of cosmic inflation preserves the successes of the Big Bang while providing a natural explanation for some of the mysterious features of the universe. The theory has also received striking support from observations of the cosmic microwave background, the radiation that has filled the sky since around 380,000 years after the Big Bang.\n\nIn the theory of inflation, the rapid initial expansion of the universe is caused by a hypothetical particle called the inflaton. The exact properties of this particle are not fixed by the theory but should ultimately be derived from a more fundamental theory such as string theory. Indeed, there have been a number of attempts to identify an inflaton within the spectrum of particles described by string theory, and to study inflation using string theory. While these approaches might eventually find support in observational data such as measurements of the cosmic microwave background, the application of string theory to cosmology is still in its early stages.\n\nIn addition to influencing research in theoretical physics, string theory has stimulated a number of major developments in pure mathematics. Like many developing ideas in theoretical physics, string theory does not at present have a mathematically rigorous formulation in which all of its concepts can be defined precisely. As a result, physicists who study string theory are often guided by physical intuition to conjecture relationships between the seemingly different mathematical structures that are used to formalize different parts of the theory. These conjectures are later proved by mathematicians, and in this way, string theory serves as a source of new ideas in pure mathematics.\n\nAfter Calabi–Yau manifolds had entered physics as a way to compactify extra dimensions in string theory, many physicists began studying these manifolds. In the late 1980s, several physicists noticed that given such a compactification of string theory, it is not possible to reconstruct uniquely a corresponding Calabi–Yau manifold. Instead, two different versions of string theory, type IIA and type IIB, can be compactified on completely different Calabi–Yau manifolds giving rise to the same physics. In this situation, the manifolds are called mirror manifolds, and the relationship between the two physical theories is called mirror symmetry.\n\nRegardless of whether Calabi–Yau compactifications of string theory provide a correct description of nature, the existence of the mirror duality between different string theories has significant mathematical consequences. The Calabi–Yau manifolds used in string theory are of interest in pure mathematics, and mirror symmetry allows mathematicians to solve problems in enumerative geometry, a branch of mathematics concerned with counting the numbers of solutions to geometric questions.\n\nEnumerative geometry studies a class of geometric objects called algebraic varieties which are defined by the vanishing of polynomials. For example, the Clebsch cubic illustrated on the right is an algebraic variety defined using a certain polynomial of degree three in four variables. A celebrated result of nineteenth-century mathematicians Arthur Cayley and George Salmon states that there are exactly 27 straight lines that lie entirely on such a surface.\n\nGeneralizing this problem, one can ask how many lines can be drawn on a quintic Calabi–Yau manifold, such as the one illustrated above, which is defined by a polynomial of degree five. This problem was solved by the nineteenth-century German mathematician Hermann Schubert, who found that there are exactly 2,875 such lines. In 1986, geometer Sheldon Katz proved that the number of curves, such as circles, that are defined by polynomials of degree two and lie entirely in the quintic is 609,250.\n\nBy the year 1991, most of the classical problems of enumerative geometry had been solved and interest in enumerative geometry had begun to diminish. The field was reinvigorated in May 1991 when physicists Philip Candelas, Xenia de la Ossa, Paul Green, and Linda Parks showed that mirror symmetry could be used to translate difficult mathematical questions about one Calabi–Yau manifold into easier questions about its mirror. In particular, they used mirror symmetry to show that a six-dimensional Calabi–Yau manifold can contain exactly 317,206,375 curves of degree three. In addition to counting degree-three curves, Candelas and his collaborators obtained a number of more general results for counting rational curves which went far beyond the results obtained by mathematicians.\n\nOriginally, these results of Candelas were justified on physical grounds. However, mathematicians generally prefer rigorous proofs that do not require an appeal to physical intuition. Inspired by physicists' work on mirror symmetry, mathematicians have therefore constructed their own arguments proving the enumerative predictions of mirror symmetry. Today mirror symmetry is an active area of research in mathematics, and mathematicians are working to develop a more complete mathematical understanding of mirror symmetry based on physicists' intuition. Major approaches to mirror symmetry include the homological mirror symmetry program of Maxim Kontsevich and the SYZ conjecture of Andrew Strominger, Shing-Tung Yau, and Eric Zaslow.\n\nGroup theory is the branch of mathematics that studies the concept of symmetry. For example, one can consider a geometric shape such as an equilateral triangle. There are various operations that one can perform on this triangle without changing its shape. One can rotate it through 120°, 240°, or 360°, or one can reflect in any of the lines labeled , , or in the picture. Each of these operations is called a \"symmetry\", and the collection of these symmetries satisfies certain technical properties making it into what mathematicians call a group. In this particular example, the group is known as the dihedral group of order 6 because it has six elements. A general group may describe finitely many or infinitely many symmetries; if there are only finitely many symmetries, it is called a finite group.\n\nMathematicians often strive for a classification (or list) of all mathematical objects of a given type. It is generally believed that finite groups are too diverse to admit a useful classification. A more modest but still challenging problem is to classify all finite \"simple\" groups. These are finite groups which may be used as building blocks for constructing arbitrary finite groups in the same way that prime numbers can be used to construct arbitrary whole numbers by taking products. One of the major achievements of contemporary group theory is the classification of finite simple groups, a mathematical theorem which provides a list of all possible finite simple groups.\n\nThis classification theorem identifies several infinite families of groups as well as 26 additional groups which do not fit into any family. The latter groups are called the \"sporadic\" groups, and each one owes its existence to a remarkable combination of circumstances. The largest sporadic group, the so-called monster group, has over elements, more than a thousand times the number of atoms in the Earth.\n\nA seemingly unrelated construction is the -function of number theory. This object belongs to a special class of functions called modular functions, whose graphs form a certain kind of repeating pattern. Although this function appears in a branch of mathematics which seems very different from the theory of finite groups, the two subjects turn out to be intimately related. In the late 1970s, mathematicians John McKay and John Thompson noticed that certain numbers arising in the analysis of the monster group (namely, the dimensions of its irreducible representations) are related to numbers that appear in a formula for the -function (namely, the coefficients of its Fourier series). This relationship was further developed by John Horton Conway and Simon Norton who called it monstrous moonshine because it seemed so far fetched.\n\nIn 1992, Richard Borcherds constructed a bridge between the theory of modular functions and finite groups and, in the process, explained the observations of McKay and Thompson. Borcherds' work used ideas from string theory in an essential way, extending earlier results of Igor Frenkel, James Lepowsky, and Arne Meurman, who had realized the monster group as the symmetries of a particular version of string theory. In 1998, Borcherds was awarded the Fields medal for his work.\n\nSince the 1990s, the connection between string theory and moonshine has led to further results in mathematics and physics. In 2010, physicists Tohru Eguchi, Hirosi Ooguri, and Yuji Tachikawa discovered connections between a different sporadic group, the Mathieu group, and a certain version of string theory. Miranda Cheng, John Duncan, and Jeffrey A. Harvey proposed a generalization of this moonshine phenomenon called umbral moonshine, and their conjecture was proved mathematically by Duncan, Michael Griffin, and Ken Ono. Witten has also speculated that the version of string theory appearing in monstrous moonshine might be related to a certain simplified model of gravity in three spacetime dimensions.\n\nSome of the structures reintroduced by string theory arose for the first time much earlier as part of the program of classical unification started by Albert Einstein. The first person to add a fifth dimension to a theory of gravity was Gunnar Nordström in 1914, who noted that gravity in five dimensions describes both gravity and electromagnetism in four. Nordström attempted to unify electromagnetism with his theory of gravitation, which was however superseded by Einstein's general relativity in 1919. Thereafter, German mathematician Theodor Kaluza combined the fifth dimension with general relativity, and only Kaluza is usually credited with the idea. In 1926, the Swedish physicist Oskar Klein gave a physical interpretation of the unobservable extra dimension—it is wrapped into a small circle. Einstein introduced a non-symmetric metric tensor, while much later Brans and Dicke added a scalar component to gravity. These ideas would be revived within string theory, where they are demanded by consistency conditions.\n\nString theory was originally developed during the late 1960s and early 1970s as a never completely successful theory of hadrons, the subatomic particles like the proton and neutron that feel the strong interaction. In the 1960s, Geoffrey Chew and Steven Frautschi discovered that the mesons make families called Regge trajectories with masses related to spins in a way that was later understood by Yoichiro Nambu, Holger Bech Nielsen and Leonard Susskind to be the relationship expected from rotating strings. Chew advocated making a theory for the interactions of these trajectories that did not presume that they were composed of any fundamental particles, but would construct their interactions from self-consistency conditions on the S-matrix. The S-matrix approach was started by Werner Heisenberg in the 1940s as a way of constructing a theory that did not rely on the local notions of space and time, which Heisenberg believed break down at the nuclear scale. While the scale was off by many orders of magnitude, the approach he advocated was ideally suited for a theory of quantum gravity.\n\nWorking with experimental data, R. Dolen, D. Horn and C. Schmid developed some sum rules for hadron exchange. When a particle and antiparticle scatter, virtual particles can be exchanged in two qualitatively different ways. In the s-channel, the two particles annihilate to make temporary intermediate states that fall apart into the final state particles. In the t-channel, the particles exchange intermediate states by emission and absorption. In field theory, the two contributions add together, one giving a continuous background contribution, the other giving peaks at certain energies. In the data, it was clear that the peaks were stealing from the background—the authors interpreted this as saying that the t-channel contribution was dual to the s-channel one, meaning both described the whole amplitude and included the other.\n\nThe result was widely advertised by Murray Gell-Mann, leading Gabriele Veneziano to construct a scattering amplitude that had the property of Dolen–Horn–Schmid duality, later renamed world-sheet duality. The amplitude needed poles where the particles appear, on straight line trajectories, and there is a special mathematical function whose poles are evenly spaced on half the real line—the gamma function— which was widely used in Regge theory. By manipulating combinations of gamma functions, Veneziano was able to find a consistent scattering amplitude with poles on straight lines, with mostly positive residues, which obeyed duality and had the appropriate Regge scaling at high energy. The amplitude could fit near-beam scattering data as well as other Regge type fits, and had a suggestive integral representation that could be used for generalization.\n\nOver the next years, hundreds of physicists worked to complete the bootstrap program for this model, with many surprises. Veneziano himself discovered that for the scattering amplitude to describe the scattering of a particle that appears in the theory, an obvious self-consistency condition, the lightest particle must be a tachyon. Miguel Virasoro and Joel Shapiro found a different amplitude now understood to be that of closed strings, while Ziro Koba and Holger Nielsen generalized Veneziano's integral representation to multiparticle scattering. Veneziano and Sergio Fubini introduced an operator formalism for computing the scattering amplitudes that was a forerunner of world-sheet conformal theory, while Virasoro understood how to remove the poles with wrong-sign residues using a constraint on the states. Claud Lovelace calculated a loop amplitude, and noted that there is an inconsistency unless the dimension of the theory is 26. Charles Thorn, Peter Goddard and Richard Brower went on to prove that there are no wrong-sign propagating states in dimensions less than or equal to 26.\n\nIn 1969–70, Yoichiro Nambu, Holger Bech Nielsen, and Leonard Susskind recognized that the theory could be given a description in space and time in terms of strings. The scattering amplitudes were derived systematically from the action principle by Peter Goddard, Jeffrey Goldstone, Claudio Rebbi, and Charles Thorn, giving a space-time picture to the vertex operators introduced by Veneziano and Fubini and a geometrical interpretation to the Virasoro conditions.\n\nIn 1971, Pierre Ramond added fermions to the model, which led him to formulate a two-dimensional supersymmetry to cancel the wrong-sign states. John Schwarz and André Neveu added another sector to the fermi theory a short time later. In the fermion theories, the critical dimension was 10. Stanley Mandelstam formulated a world sheet conformal theory for both the bose and fermi case, giving a two-dimensional field theoretic path-integral to generate the operator formalism. Michio Kaku and Keiji Kikkawa gave a different formulation of the bosonic string, as a string field theory, with infinitely many particle types and with fields taking values not on points, but on loops and curves.\n\nIn 1974, Tamiaki Yoneya discovered that all the known string theories included a massless spin-two particle that obeyed the correct Ward identities to be a graviton. John Schwarz and Joel Scherk came to the same conclusion and made the bold leap to suggest that string theory was a theory of gravity, not a theory of hadrons. They reintroduced Kaluza–Klein theory as a way of making sense of the extra dimensions. At the same time, quantum chromodynamics was recognized as the correct theory of hadrons, shifting the attention of physicists and apparently leaving the bootstrap program in the dustbin of history.\n\nString theory eventually made it out of the dustbin, but for the following decade all work on the theory was completely ignored. Still, the theory continued to develop at a steady pace thanks to the work of a handful of devotees. Ferdinando Gliozzi, Joel Scherk, and David Olive realized in 1977 that the original Ramond and Neveu Schwarz-strings were separately inconsistent and needed to be combined. The resulting theory did not have a tachyon, and was proven to have space-time supersymmetry by John Schwarz and Michael Green in 1984. The same year, Alexander Polyakov gave the theory a modern path integral formulation, and went on to develop conformal field theory extensively. In 1979, Daniel Friedan showed that the equations of motions of string theory, which are generalizations of the Einstein equations of general relativity, emerge from the renormalization group equations for the two-dimensional field theory. Schwarz and Green discovered T-duality, and constructed two superstring theories—IIA and IIB related by T-duality, and type I theories with open strings. The consistency conditions had been so strong, that the entire theory was nearly uniquely determined, with only a few discrete choices.\n\nIn the early 1980s, Edward Witten discovered that most theories of quantum gravity could not accommodate chiral fermions like the neutrino. This led him, in collaboration with Luis Álvarez-Gaumé, to study violations of the conservation laws in gravity theories with anomalies, concluding that type I string theories were inconsistent. Green and Schwarz discovered a contribution to the anomaly that Witten and Alvarez-Gaumé had missed, which restricted the gauge group of the type I string theory to be SO(32). In coming to understand this calculation, Edward Witten became convinced that string theory was truly a consistent theory of gravity, and he became a high-profile advocate. Following Witten's lead, between 1984 and 1986, hundreds of physicists started to work in this field, and this is sometimes called the first superstring revolution.\n\nDuring this period, David Gross, Jeffrey Harvey, Emil Martinec, and Ryan Rohm discovered heterotic strings. The gauge group of these closed strings was two copies of E8, and either copy could easily and naturally include the standard model. Philip Candelas, Gary Horowitz, Andrew Strominger and Edward Witten found that the Calabi–Yau manifolds are the compactifications that preserve a realistic amount of supersymmetry, while Lance Dixon and others worked out the physical properties of orbifolds, distinctive geometrical singularities allowed in string theory. Cumrun Vafa generalized T-duality from circles to arbitrary manifolds, creating the mathematical field of mirror symmetry. Daniel Friedan, Emil Martinec and Stephen Shenker further developed the covariant quantization of the superstring using conformal field theory techniques. David Gross and Vipul Periwal discovered that string perturbation theory was divergent. Stephen Shenker showed it diverged much faster than in field theory suggesting that new non-perturbative objects were missing.\n\nIn the 1990s, Joseph Polchinski discovered that the theory requires higher-dimensional objects, called D-branes and identified these with the black-hole solutions of supergravity. These were understood to be the new objects suggested by the perturbative divergences, and they opened up a new field with rich mathematical structure. It quickly became clear that D-branes and other p-branes, not just strings, formed the matter content of the string theories, and the physical interpretation of the strings and branes was revealed—they are a type of black hole. Leonard Susskind had incorporated the holographic principle of Gerardus 't Hooft into string theory, identifying the long highly excited string states with ordinary thermal black hole states. As suggested by 't Hooft, the fluctuations of the black hole horizon, the world-sheet or world-volume theory, describes not only the degrees of freedom of the black hole, but all nearby objects too.\n\nIn 1995, at the annual conference of string theorists at the University of Southern California (USC), Edward Witten gave a speech on string theory that in essence united the five string theories that existed at the time, and giving birth to a new 11-dimensional theory called M-theory. M-theory was also foreshadowed in the work of Paul Townsend at approximately the same time. The flurry of activity that began at this time is sometimes called the second superstring revolution.\n\nDuring this period, Tom Banks, Willy Fischler, Stephen Shenker and Leonard Susskind formulated matrix theory, a full holographic description of M-theory using IIA D0 branes. This was the first definition of string theory that was fully non-perturbative and a concrete mathematical realization of the holographic principle. It is an example of a gauge-gravity duality and is now understood to be a special case of the AdS/CFT correspondence. Andrew Strominger and Cumrun Vafa calculated the entropy of certain configurations of D-branes and found agreement with the semi-classical answer for extreme charged black holes. Petr Hořava and Witten found the eleven-dimensional formulation of the heterotic string theories, showing that orbifolds solve the chirality problem. Witten noted that the effective description of the physics of D-branes at low energies is by a supersymmetric gauge theory, and found geometrical interpretations of mathematical structures in gauge theory that he and Nathan Seiberg had earlier discovered in terms of the location of the branes.\n\nIn 1997, Juan Maldacena noted that the low energy excitations of a theory near a black hole consist of objects close to the horizon, which for extreme charged black holes looks like an anti-de Sitter space. He noted that in this limit the gauge theory describes the string excitations near the branes. So he hypothesized that string theory on a near-horizon extreme-charged black-hole geometry, an anti-de Sitter space times a sphere with flux, is equally well described by the low-energy limiting gauge theory, the N = 4 supersymmetric Yang–Mills theory. This hypothesis, which is called the AdS/CFT correspondence, was further developed by Steven Gubser, Igor Klebanov and Alexander Polyakov, and by Edward Witten, and it is now well-accepted. It is a concrete realization of the holographic principle, which has far-reaching implications for black holes, locality and information in physics, as well as the nature of the gravitational interaction. Through this relationship, string theory has been shown to be related to gauge theories like quantum chromodynamics and this has led to more quantitative understanding of the behavior of hadrons, bringing string theory back to its roots.\n\nTo construct models of particle physics based on string theory, physicists typically begin by specifying a shape for the extra dimensions of spacetime. Each of these different shapes corresponds to a different possible universe, or \"vacuum state\", with a different collection of particles and forces. String theory as it is currently understood has an enormous number of vacuum states, typically estimated to be around , and these might be sufficiently diverse to accommodate almost any phenomena that might be observed at low energies.\n\nMany critics of string theory have expressed concerns about the large number of possible universes described by string theory. In his book \"Not Even Wrong\", Peter Woit, a lecturer in the mathematics department at Columbia University, has argued that the large number of different physical scenarios renders string theory vacuous as a framework for constructing models of particle physics. According to Woit,\n\nSome physicists believe this large number of solutions is actually a virtue because it may allow a natural anthropic explanation of the observed values of physical constants, in particular the small value of the cosmological constant. The anthropic principle is the idea that some of the numbers appearing in the laws of physics are not fixed by any fundamental principle but must be compatible with the evolution of intelligent life. In 1987, Steven Weinberg published an article in which he argued that the cosmological constant could not have been too large, or else galaxies and intelligent life would not have been able to develop. Weinberg suggested that there might be a huge number of possible consistent universes, each with a different value of the cosmological constant, and observations indicate a small value of the cosmological constant only because humans happen to live in a universe that has allowed intelligent life, and hence observers, to exist.\n\nString theorist Leonard Susskind has argued that string theory provides a natural anthropic explanation of the small value of the cosmological constant. According to Susskind, the different vacuum states of string theory might be realized as different universes within a larger multiverse. The fact that the observed universe has a small cosmological constant is just a tautological consequence of the fact that a small value is required for life to exist. Many prominent theorists and critics have disagreed with Susskind's conclusions. According to Woit, \"in this case [anthropic reasoning] is nothing more than an excuse for failure. Speculative scientific ideas fail not just when they make incorrect predictions, but also when they turn out to be vacuous and incapable of predicting anything.\"\n\nOne of the fundamental properties of Einstein's general theory of relativity is that it is background independent, meaning that the formulation of the theory does not in any way privilege a particular spacetime geometry.\n\nOne of the main criticisms of string theory from early on is that it is not manifestly background independent. In string theory, one must typically specify a fixed reference geometry for spacetime, and all other possible geometries are described as perturbations of this fixed one. In his book \"The Trouble With Physics\", physicist Lee Smolin of the Perimeter Institute for Theoretical Physics claims that this is the principal weakness of string theory as a theory of quantum gravity, saying that string theory has failed to incorporate this important insight from general relativity.\n\nOthers have disagreed with Smolin's characterization of string theory. In a review of Smolin's book, string theorist Joseph Polchinski writes\n\nPolchinski notes that an important open problem in quantum gravity is to develop holographic descriptions of gravity which do not require the gravitational field \nto be asymptotically anti-de Sitter. Smolin has responded by saying that the AdS/CFT correspondence, as it is currently understood, may not be strong enough to resolve all concerns about background independence.\n\nSince the superstring revolutions of the 1980s and 1990s, string theory has become the dominant paradigm of high energy theoretical physics. Some string theorists have expressed the view that there does not exist an equally successful alternative theory addressing the deep questions of fundamental physics. In an interview from 1987, Nobel laureate David Gross made the following controversial comments about the reasons for the popularity of string theory:\n\nSeveral other high-profile theorists and commentators have expressed similar views, suggesting that there are no viable alternatives to string theory.\n\nMany critics of string theory have commented on this state of affairs. In his book criticizing string theory, Peter Woit views the status of string theory research as unhealthy and detrimental to the future of fundamental physics. He argues that the extreme popularity of string theory among theoretical physicists is partly a consequence of the financial structure of academia and the fierce competition for scarce resources. In his book \"The Road to Reality\", mathematical physicist Roger Penrose expresses similar views, stating \"The often frantic competitiveness that this ease of communication engenders leads to bandwagon effects, where researchers fear to be left behind if they do not join in.\" Penrose also claims that the technical difficulty of modern physics forces young scientists to rely on the preferences of established researchers, rather than forging new paths of their own. Lee Smolin expresses a slightly different position in his critique, claiming that string theory grew out of a tradition of particle physics which discourages speculation about the foundations of physics, while his preferred approach, loop quantum gravity, encourages more radical thinking. According to Smolin,\n\nSmolin goes on to offer a number of prescriptions for how scientists might encourage a greater diversity of approaches to quantum gravity research.\n\n\n\n\n"}
{"id": "24419775", "url": "https://en.wikipedia.org/wiki?curid=24419775", "title": "Theodore von Kármán Prize", "text": "Theodore von Kármán Prize\n\nThe Theodore von Kármán Prize in Applied Mathematics is awarded every fifth year to an individual in recognition of his or her notable application of mathematics to mechanics and/or the engineering sciences. This award was established and endowed in 1968 in honor of Theodore von Kármán by the Society for Industrial and Applied Mathematics (SIAM).\n\n"}
{"id": "7954712", "url": "https://en.wikipedia.org/wiki?curid=7954712", "title": "Turnstile (symbol)", "text": "Turnstile (symbol)\n\nIn mathematical logic and computer science the symbol formula_1 has taken the name turnstile because of its resemblance to a typical turnstile if viewed from above. It is also referred to as tee and is often read as \"yields\", \"proves\", \"satisfies\" or \"entails\". The symbol was first used by Gottlob Frege in his 1879 book on logic, \"Begriffsschrift\".\n\nPer Martin-Löf (1996) analyzes the formula_1 symbol thus: \"...[T]he combination of Frege's , judgement stroke [ | ], and , content stroke [—], came to be called the assertion sign.\" Frege's notation for a judgement of some content \ncan then be read\nIn the same vein, a conditional assertion\ncan be read as:\nIn TeX, the turnstile symbol formula_1 is obtained from the command \\vdash. In Unicode, the turnstile symbol (⊢) is called right tack and is at code point U+22A2. (Code point U+22A6 is named \"assertion sign\" (⊦).) On a typewriter, a turnstile can be composed from a vertical bar (|) and a dash (–). In LaTeX there is a turnstile package which issues this sign in many ways, and is capable of putting labels below or above it, in the correct places.\n\nThe turnstile represents a binary relation. It has several different interpretations in different contexts:\n\n\n\n"}
{"id": "38864413", "url": "https://en.wikipedia.org/wiki?curid=38864413", "title": "Universal point set", "text": "Universal point set\n\nIn graph drawing, a universal point set of order \"n\" is a set \"S\" of points in the Euclidean plane with the property that every \"n\"-vertex planar graph has a straight-line drawing in which the vertices are all placed at points of \"S\".\n\nWhen \"n\" is ten or less, there exist universal point sets with exactly \"n\" points, but for all \"n\" ≥ 15 additional points are required.\n\nSeveral authors have shown that subsets of the integer lattice of size \"O\"(\"n\") × \"O\"(\"n\") are universal. In particular, showed that a grid of (2\"n\" − 3) × (\"n\" − 1) points is universal, and reduced this to a triangular subset of an (\"n\" − 1) × (\"n\" − 1) grid, with \"n\"/2 − \"O\"(\"n\") points. By modifying the method of de Fraysseix et al., found an embedding of any planar graph into a triangular subset of the grid consisting of 4\"n\"/9 points. A universal point set in the form of a rectangular grid must have size at least \"n\"/3 × \"n\"/3 but this does not rule out the possibility of smaller universal point sets of other types. The smallest known universal point sets are not based on grids, but are instead constructed from superpatterns (permutations that contain all permutation patterns of a given size); the universal point sets constructed in this way have size \"n\"/4 − \"O\"(\"n\").\n\nClosing the gap between the known linear lower bounds and quadratic upper bounds remains an open problem.\n\nSubclasses of the planar graphs may, in general, have smaller universal sets (sets of points that allow straight-line drawings of all \"n\"-vertex graphs in the subclass) than the full class of planar graphs, and in many cases universal point sets of exactly \"n\" points are possible. For instance, it is not hard to see that every set of \"n\" points in convex position (forming the vertices of a convex polygon) is universal for the \"n\"-vertex outerplanar graphs, and in particular for trees. Less obviously, every set of \"n\" points in general position (no three collinear) remains universal for outerplanar graphs.\n\nPlanar graphs that can be partitioned into nested cycles, 2-outerplanar graphs and planar graphs of bounded pathwidth, have universal point sets of nearly-linear size. Planar 3-trees have universal point sets of size \"O\"(\"n\"); the same bound also applies to series-parallel graphs.\n\nAs well as for straight-line graph drawing, universal point sets have been studied for other drawing styles; in many of these cases, universal point sets with exactly \"n\" points exist, based on a topological book embedding in which the vertices are placed along a line in the plane and the edges are drawn as curves that cross this line at most once. For instance, every set of \"n\" collinear points is universal for an arc diagram in which each edge is represented as either a single semicircle or a smooth curve formed from two semicircles.\n\nBy using a similar layout, every convex curve in the plane can be shown to contain an \"n\"-point subset that is universal for polyline drawing with at most one bend per edge. This set contains only the vertices of the drawing, not the bends; larger sets are known that can be used for polyline drawing with all vertices and all bends placed within the set.\n\n"}
