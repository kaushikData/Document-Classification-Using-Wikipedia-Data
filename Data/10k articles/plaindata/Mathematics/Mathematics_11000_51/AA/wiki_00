{"id": "9032663", "url": "https://en.wikipedia.org/wiki?curid=9032663", "title": "Airway resistance", "text": "Airway resistance\n\nIn respiratory physiology, airway resistance is the resistance of the respiratory tract to airflow during inhalation and expiration. Airway resistance can be measured using body plethysmography.\n\nAnalogously to Ohm's Law:\n\nWhere:\n\nN.B. P and formula_6 change constantly during the respiratory cycle.\n\nThere are several important determinants of airway resistance including:\n\nIn fluid dynamics, the Hagen–Poiseuille equation is a physical law that gives the pressure drop in a fluid flowing through a long cylindrical pipe. The assumptions of the equation are that the flow is laminar viscous and incompressible and the flow is through a constant circular cross-section that is substantially longer than its diameter. The equation is also known as the \"Hagen–Poiseuille law\", \"Poiseuille law\" and \"Poiseuille equation\".\n\nWhere:\n\nDividing both sides by formula_6 and given the above definition shows:-\n\nWhile the assumptions of the Hagen–Poiseuille equation are not strictly true of the respiratory tract it serves to show that, because of the fourth power, relatively small changes in the radius of the airways causes large changes in airway resistance.\n\nAn individual small airway has much greater resistance than a large airway, however there are many more small airways than large ones. Therefore, resistance is greatest at the bronchi of intermediate size, in between the fourth and eighth bifurcation.\n\nWhere air is flowing in a laminar manner it has less resistance than when it is flowing in a turbulent manner. If flow becomes turbulent, and the pressure difference is increased to maintain flow, this response itself increases resistance. This means that a large increase in pressure difference is required to maintain flow if it becomes turbulent.\n\nWhether flow is laminar or turbulent is complicated, however generally flow within a pipe will be laminar as long as the Reynolds number is less than 2300.\n\nAlso called volumic airway resistance. Due to the elastic nature of the tissue that supports the small airways airway resistance changes with lung volume. It is not practically possible to measure airway resistance at a set absolute lung volume, therefore specific airway resistance attempts to correct for differences in lung volume at which different measurements of airway resistance were made.\n\nSpecific airway resistance is often measured at FRC, in which case:\n\nAlso called volumic airway conductance. Similarly to specific airway resistance, specific airway conductance attempts to correct for differences in lung volume.\n\nSpecific airway conductance is often measured at FRC, in which case:\n\n\n"}
{"id": "20888637", "url": "https://en.wikipedia.org/wiki?curid=20888637", "title": "Babenko–Beckner inequality", "text": "Babenko–Beckner inequality\n\nIn mathematics, the Babenko–Beckner inequality (after K. Ivan Babenko and William E. Beckner) is a sharpened form of the Hausdorff–Young inequality having applications to uncertainty principles in the Fourier analysis of L spaces. The (\"q\", \"p\")-norm of the \"n\"-dimensional Fourier transform is defined to be\n\nIn 1961, Babenko found this norm for \"even\" integer values of \"q\". Finally, in 1975,\nusing Hermite functions as eigenfunctions of the Fourier transform, Beckner proved that the value of this norm for all formula_2 is\n\nThus we have the Babenko–Beckner inequality that\n\nTo write this out explicitly, (in the case of one dimension,) if the Fourier transform is normalized so that\n\nthen we have\n\nor more simply\n\nThroughout this sketch of a proof, let\n\nLet formula_9 be the discrete measure with weight formula_10 at the points formula_11 Then the operator\nmaps formula_13 to formula_14 with norm 1; that is,\nor more explicitly,\nfor any complex \"a\", \"b\". (See Beckner's paper for the proof of his \"two-point lemma\".)\n\nThe measure formula_17 that was introduced above is actually a fair Bernoulli trial with mean 0 and variance 1. Consider the sum of a sequence of \"n\" such Bernoulli trials, independent and normalized so that the standard deviation remains 1. We obtain the measure formula_18 which is the \"n\"-fold convolution of formula_19 with itself. The next step is to extend the operator \"C\" defined on the two-point space above to an operator defined on the (\"n\" + 1)-point space of formula_18 with respect to the elementary symmetric polynomials.\n\nThe sequence formula_18 converges weakly to the standard normal probability distribution formula_22 with respect to functions of polynomial growth. In the limit, the extension of the operator \"C\" above in terms of the elementary symmetric polynomials with respect to the measure formula_18 is expressed as an operator \"T\" in terms of the Hermite polynomials with respect to the standard normal distribution. These Hermite functions are the eigenfunctions of the Fourier transform, and the (\"q\", \"p\")-norm of the Fourier transform is obtained as a result after some renormalization.\n\n"}
{"id": "21564294", "url": "https://en.wikipedia.org/wiki?curid=21564294", "title": "Bcrypt", "text": "Bcrypt\n\nbcrypt is a password hashing function designed by Niels Provos and David Mazières, based on the Blowfish cipher, and presented at USENIX in 1999. Besides incorporating a salt to protect against rainbow table attacks, bcrypt is an adaptive function: over time, the iteration count can be increased to make it slower, so it remains resistant to brute-force search attacks even with increasing computation power.\n\nThe bcrypt function is the default password hash algorithm for OpenBSD and other systems including some Linux distributions such as SUSE Linux.\n\nThere are implementations of bcrypt for C, C++, C#, Go, Java, JavaScript, Perl, PHP, Python, Ruby and other languages.\n\nBlowfish is notable among block ciphers for its expensive key setup phase. It starts off with subkeys in a standard state, then uses this state to perform a block encryption using part of the key, and uses the result of that encryption (which is more accurately a hashing) to replace some of the subkeys. Then it uses this modified state to encrypt another part of the key, and uses the result to replace more of the subkeys. It proceeds in this fashion, using a progressively modified state to hash the key and replace bits of state, until all subkeys have been set.\n\nProvos and Mazières took advantage of this, and took it further. They developed a new key setup algorithm for Blowfish, dubbing the resulting cipher \"Eksblowfish\" (\"expensive key schedule Blowfish\"). The key setup begins with a modified form of the standard Blowfish key setup, in which both the salt and password are used to set all subkeys. There are then a number of rounds in which the standard Blowfish keying algorithm is applied, using alternatively the salt and the password as the key, each round starting with the subkey state from the previous round. In theory, this is no stronger than the standard Blowfish key schedule, but the number of rekeying rounds is configurable; this process can therefore be made arbitrarily slow, which helps deter brute-force attacks upon the hash or salt.\n\nThe prefix \"$2a$\" or \"$2b$\" (or \"$2y$\") in a hash string in a shadow password file indicates that hash string is a bcrypt hash in modular crypt format.\nThe rest of the hash string includes the cost parameter, a 128-bit salt (Radix-64 encoded as 22 characters), and 184 bits of the resulting hash value (Radix-64 encoded as 31 characters). The Radix-64 encoding uses the unix/crypt alphabet, and is not 'standard' Base-64. The cost parameter specifies a key expansion iteration count as a power of two, which is an input to the crypt algorithm.\n\nFor example, the shadow password record codice_1 specifies a cost parameter of 10, indicating 2 key expansion rounds. The salt is codice_2 and the resulting hash is codice_3. Per standard practice, the user's password itself is not stored.\n\n $2$ (1999) \n\nThe original Bcrypt specification defined a prefix of codice_4. This follows the Modular Crypt Format format used when storing passwords in the OpenBSD password file:\n\n\nThe original specification did not define how to handle non-ASCII character, nor how to handle a null terminator. The specification was revised to specify that when hashing strings:\n\n\nWith this change, the version was changed to codice_10\n\nIn June 2011, a bug was discovered in crypt_blowfish, a PHP implementation of BCrypt. It was mis-handling characters with the 8th bit set. They suggested that system administrators update their existing password database, replacing codice_10 with codice_12, to indicate that those hashes are bad (and need to use the old broken algorithm). They also suggested the idea of having crypt_blowfish emit codice_13 for hashes generated by the fixed algorithm.\n\nNobody else, including canonical OpenBSD, adopted the idea of 2x/2y. This version marker change was limited to crypt_blowfish.\n\nA bug was discovered in the OpenBSD implementation of bcrypt. They were storing the length of their strings in an unsigned char (\"i.e.\" 8-bit codice_14). If a password was longer than 255 characters, it would overflow and wrap at 255. \n\nBCrypt was created for OpenBSD. When they had a bug in their library, they decided to bump the version number.\n\nThe bcrypt algorithm is the result of encrypting the text \"OrpheanBeholderScryDoubt\" 64 times using Blowfish. In bcrypt the usual Blowfish key setup function is replaced with an \"expensive\" key setup (EksBlowfishSetup) function:\n\nThe bcrypt algorithm depends heavily on its \"Eksblowfish\" key setup algorithm, which runs as follows:\n\nInitialState works as in the original Blowfish algorithm, populating the P-array and S-box entries with the fractional part of formula_8 in hexadecimal.\n\nThe ExpandKey function does the following:\n\nHence, codice_15 is the same as regular Blowfish key schedule since all XORs with the all-zero salt value are ineffectual. codice_16 is similar, but uses the salt as a 128-bit key.\n\nMany implementations of bcrypt truncate the password to the first 72 bytes.\n\nThe mathematical algorithm itself requires initialization with 18 32-bit subkeys (equivalent to 72 octets/bytes). The original specification of bcrypt does not mandate any one particular method for mapping text-based passwords from userland into numeric values for the algorithm. One brief comment in the text mentions, but does not mandate, the possibility of simply using the ASCII encoded value of a character string, \"Finally, the key argument is a secret encryption key, which can be a user-chosen password of up to 56 bytes (including a terminating zero byte when the key is an ASCII string).\"\n\nNote that the quote above mentions passwords \"up to 56 bytes\" even though the algorithm itself makes use of a 72 byte initial value. Although Provos and Mazières do not state the reason for the shorter restriction, they may have been motivated by the following statement from Bruce Schneier's original specification of Blowfish, \"The 448 [bit] limit on the key size ensures that the every bit of every subkey depends on every bit of the key.\"\n\nImplementations have varied in their approach of converting passwords into initial numeric values, including sometimes reducing the strength of passwords containing non-ASCII characters.\n\n"}
{"id": "5539282", "url": "https://en.wikipedia.org/wiki?curid=5539282", "title": "Borel hierarchy", "text": "Borel hierarchy\n\nIn mathematical logic, the Borel hierarchy is a stratification of the Borel algebra generated by the open subsets of a Polish space; elements of this algebra are called Borel sets. Each Borel set is assigned a unique countable ordinal number called the rank of the Borel set. The Borel hierarchy is of particular interest in descriptive set theory.\n\nOne common use of the Borel hierarchy is to prove facts about the Borel sets using transfinite induction on rank. Properties of sets of small finite ranks are important in measure theory and analysis.\n\nThe Borel algebra in an arbitrary topological space is the smallest collection of subsets of the space that contains the open sets and is closed under countable unions and complementation. It can be shown that the Borel algebra is closed under countable intersections as well.\n\nA short proof that the Borel algebra is well defined proceeds by showing that the entire powerset of the space is closed under complements and countable unions, and thus the Borel algebra is the intersection of all families of subsets of the space that have these closure properties. This proof does not give a simple procedure for determining whether a set is Borel. A motivation for the Borel hierarchy is to provide a more explicit characterization of the Borel sets.\n\nThe Borel hierarchy or boldface Borel hierarchy on a space \"X\" consists of classes formula_1, formula_2, and formula_3 for every countable ordinal formula_4 greater than zero. Each of these classes consists of subsets of \"X\". The classes are defined inductively from the following rules:\n\nThe motivation for the hierarchy is to follow the way in which a Borel set could be constructed from open sets using complementation and countable unions. \nA Borel set is said to have finite rank if it is in formula_1 for some finite ordinal formula_4; otherwise it has infinite rank.\n\nThe hierarchy can be shown to have the following properties:\n\nThe classes of small rank are known by alternate names in classical descriptive set theory.\n\nThe lightface Borel hierarchy is an effective version of the boldface Borel hierarchy. It is important in effective descriptive set theory and recursion theory. The lightface Borel hierarchy extends the arithmetical hierarchy of subsets of an effective Polish space. It is closely related to the hyperarithmetical hierarchy.\n\nThe lightface Borel hierarchy can be defined on any effective Polish space. It consists of classes formula_33, formula_34 and formula_35 for each nonzero countable ordinal formula_4 less than the Church-Kleene ordinal formula_37. Each class consists of subsets of the space. The classes, and codes for elements of the classes, are inductively defined as follows:\n\nA code for a lightface Borel set gives complete information about how to recover the set from sets of smaller rank. This contrasts with the boldface hierarchy, where no such effectivity is required. Each lightface Borel set has infinitely many distinct codes. Other coding systems are possible; the crucial idea is that a code must effectively distinguish between effectively open sets, complements of sets represented by previous codes, and computable enumerations of sequences of codes.\n\nIt can be shown that for each formula_49 there are sets in formula_50, and thus the hierarchy does not collapse. No new sets would be added at stage formula_37, however.\n\nA famous theorem due to Spector and Kleene states that a set is in the lightface Borel hierarchy if and only if it is at level formula_52 of the analytical hierarchy. These sets are also called hyperarithmetic.\n\nThe code for a lightface Borel set \"A\" can be used to inductively define a tree whose nodes are labeled by codes. The root of the tree is labeled by the code for \"A\". If a node is labeled by a code of the form \"(1,c)\" then it has a child node whose code is \"c\". If a node is labeled by a code of the form \"(2,e)\" then it has one child for each code enumerated by the program with index \"e\". If a node is labeled with a code of the form \"(0,e)\" then it has no children. This tree describes how \"A\" is built from sets of smaller rank. The ordinals used in the construction of \"A\" ensure that this tree has no infinite path, because any infinite path through the tree would have to include infinitely many codes starting with \"2\", and thus would give an infinite decreasing sequence of ordinals. Conversely, if an arbitrary subtree of formula_53 has its nodes labeled by codes in a consistent way, and the tree has no infinite paths, then the code at the root of the tree is a code for a lightface Borel set. The rank of this set is bounded by the order type of the tree in the Kleene–Brouwer order. Because the tree is arithmetically definable, this rank must be less than formula_37. This is the origin of the Church-Kleene ordinal in the definition of the lightface hierarchy.\n\n\n"}
{"id": "483173", "url": "https://en.wikipedia.org/wiki?curid=483173", "title": "Branch point", "text": "Branch point\n\nIn the mathematical field of complex analysis, a branch point of a multi-valued function (usually referred to as a \"multifunction\" in the context of complex analysis) is a point such that the function is discontinuous when going around an arbitrarily small circuit around this point. Multi-valued functions are rigorously studied using Riemann surfaces, and the formal definition of branch points employs this concept.\n\nBranch points fall into three broad categories: algebraic branch points, transcendental branch points, and logarithmic branch points. Algebraic branch points most commonly arise from functions in which there is an ambiguity in the extraction of a root, such as solving the equation \"w\"  = \"z\" for \"w\" as a function of \"z\". Here the branch point is the origin, because the analytic continuation of any solution around a closed loop containing the origin will result in a different function: there is non-trivial monodromy. Despite the algebraic branch point, the function \"w\" is well-defined as a multiple-valued function and, in an appropriate sense, is continuous at the origin. This is in contrast to transcendental and logarithmic branch points, that is, points at which a multiple-valued function has nontrivial monodromy and an essential singularity. In geometric function theory, unqualified use of the term \"branch point\" typically means the former more restrictive kind: the algebraic branch points. In other areas of complex analysis, the unqualified term may also refer to the more general branch points of transcendental type.\n\nLet Ω be a connected open set in the complex plane C and \"ƒ\":Ω → C a holomorphic function. If \"ƒ\" is not constant, then the set of the critical points of \"ƒ\", that is, the zeros of the derivative \"ƒ\"<nowiki>'</nowiki>(\"z\"), has no limit point in Ω. So each critical point \"z\" of \"ƒ\" lies at the center of a disc \"B\"(\"z\",\"r\") containing no other critical point of \"ƒ\" in its closure.\n\nLet γ be the boundary of \"B\"(\"z\",\"r\"), taken with its positive orientation. The winding number of \"ƒ\"(\"γ\") with respect to the point \"ƒ\"(\"z\") is a positive integer called the ramification index of \"z\". If the ramification index is greater than 1, then \"z\" is called a ramification point of \"ƒ\", and the corresponding critical value \"ƒ\"(\"z\") is called an (algebraic) branch point. Equivalently, \"z\" is a ramification point if there exists a holomorphic function φ defined in a neighborhood of \"z\" such that \"ƒ\"(\"z\") = φ(\"z\")(\"z\" − \"z\") for some positive integer \"k\" > 1.\n\nTypically, one is not interested in \"ƒ\" itself, but in its inverse function. However, the inverse of a holomorphic function in the neighborhood of a ramification point does not properly exist, and so one is forced to define it in a multiple-valued sense as a global analytic function. It is common to abuse language and refers to a branch point \"w\" = \"ƒ\"(\"z\") of \"ƒ\" as a branch point of the global analytic function \"ƒ\". More general definitions of branch points are possible for other kinds of multiple-valued global analytic functions, such as those that are defined implicitly. A unifying framework for dealing with such examples is supplied in the language of Riemann surfaces below. In particular, in this more general picture, poles of order greater than 1 can also be considered ramification points.\n\nIn terms of the inverse global analytic function \"ƒ\", branch points are those points around which there is nontrivial monodromy. For example, the function \"ƒ\"(\"z\") = \"z\" has a ramification point at \"z\" = 0. The inverse function is the square root \"ƒ\"(\"w\") = \"w\", which has a branch point at \"w\" = 0. Indeed, going around the closed loop \"w\" = \"e\", one starts at θ = 0 and \"e\" = 1. But after going around the loop to θ = 2π, one has \"e\" = −1. Thus there is monodromy around this loop enclosing the origin.\n\nSuppose that \"g\" is a global analytic function defined on a punctured disc around \"z\". Then \"g\" has a transcendental branch point if \"z\" is an essential singularity of \"g\" such that analytic continuation of a function element once around some simple closed curve surrounding the point \"z\" produces a different function element. An example of a transcendental branch point is the origin for the multi-valued function\n\nfor some integer \"k\" > 1. Here the monodromy group for a circuit around the origin is finite. Analytic continuation around \"k\" full circuits brings the function back to the original.\n\nBy contrast, the point \"z\" is called a logarithmic branch point if it is impossible to return to the original function element by analytic continuation along a curve with nonzero winding number about \"z\". This is so called because the typical example of this phenomenon is the branch point of the complex logarithm at the origin. Going once counterclockwise around a simple closed curve encircling the origin, the complex logarithm is incremented by 2π\"i\". Encircling a loop with winding number \"w\", the logarithm is incremented by 2π\"i w\" and the monodromy group is the infinite cyclic group formula_2.\n\nThere is no corresponding notion of ramification for transcendental and logarithmic branch points since the associated covering Riemann surface cannot be analytically continued to a cover of the branch point itself. Such covers are therefore always unramified.\n\n\nRoughly speaking, branch points are the points where the various sheets of a multiple valued function come together. The branches of the function are the various sheets of the function. For example, the function \"w\" = \"z\" has two branches: one where the square root comes in with a plus sign, and the other with a minus sign. A branch cut is a curve in the complex plane such that it is possible to define a single analytic branch of a multi-valued function on the plane minus that curve. Branch cuts are usually, but not always, taken between pairs of branch points.\n\nBranch cuts allow one to work with a collection of single-valued functions, \"glued\" together along the branch cut instead of a multivalued function. For example, to make the function\n\nsingle-valued, one makes a branch cut along the interval [0, 1] on the real axis, connecting the two branch points of the function. The same idea can be applied to the function ; but in that case one has to perceive that the \"point at infinity\" is the appropriate 'other' branch point to connect to from 0, for example along the whole negative real axis.\n\nThe branch cut device may appear arbitrary (and it is); but it is very useful, for example in the theory of special functions. An invariant explanation of the branch phenomenon is developed in Riemann surface theory (of which it is historically the origin), and more generally in the ramification and monodromy theory of algebraic functions and differential equations.\n\nThe typical example of a branch cut is the complex logarithm. If a complex number is represented in polar form \"z\" = \"r\"e, then the logarithm of \"z\" is\nHowever, there is an obvious ambiguity in defining the angle \"θ\": adding to \"θ\" any integer multiple of 2\"π\" will yield another possible angle. A branch of the logarithm is a continuous function \"L\"(\"z\") giving a logarithm of \"z\" for all \"z\" in a connected open set in the complex plane. In particular, a branch of the logarithm exists in the complement of any ray from the origin to infinity: a \"branch cut\". A common choice of branch cut is the negative real axis, although the choice is largely a matter of convenience.\n\nThe logarithm has a jump discontinuity of 2\"π\"i when crossing the branch cut. The logarithm can be made continuous by gluing together countably many copies, called \"sheets\", of the complex plane along the branch cut. On each sheet, the value of the log differs from its principal value by a multiple of 2\"π\"i. These surfaces are glued to each other along the branch cut in the unique way to make the logarithm continuous. Each time the variable goes around the origin, the logarithm moves to a different branch.\n\nOne reason that branch cuts are common features of complex analysis is that a branch cut can be thought of as a sum of infinitely many poles arranged along a line in the complex plane with infinitesimal residues. For example,\n\nis a function with a simple pole at \"z\" = \"a\". Integrating over the location of the pole:\n\ndefines a function \"u\"(\"z\") with a cut from −1 to 1. The branch cut can be moved around, since the integration line can be shifted without altering the value of the integral so long as the line does not pass across the point \"z\".\n\nThe concept of a branch point is defined for a holomorphic function ƒ:\"X\" → \"Y\" from a compact connected Riemann surface \"X\" to a compact Riemann surface \"Y\" (usually the Riemann sphere). Unless it is constant, the function ƒ will be a covering map onto its image at all but a finite number of points. The points of \"X\" where ƒ fails to be a cover are the ramification points of ƒ, and the image of a ramification point under ƒ is called a branch point.\n\nFor any point \"P\" ∈ \"X\" and \"Q\" = ƒ(\"P\") ∈ \"Y\", there are holomorphic local coordinates \"z\" for \"X\" near \"P\" and \"w\" for \"Y\" near \"Q\" in terms of which the function ƒ(\"z\") is given by\nfor some integer \"k\". This integer is called the ramification index of \"P\". Usually the ramification index is one. But if the ramification index is not equal to one, then \"P\" is by definition a ramification point, and \"Q\" is a branch point.\n\nIf \"Y\" is just the Riemann sphere, and \"Q\" is in the finite part of \"Y\", then there is no need to select special coordinates. The ramification index can be calculated explicitly from Cauchy's integral formula. Let γ be a simple rectifiable loop in \"X\" around \"P\". The ramification index of ƒ at \"P\" is\nThis integral is the number of times ƒ(γ) winds around the point \"Q\". As above, \"P\" is a ramification point and \"Q\" is a branch point if \"e\" > 1.\n\nIn the context of algebraic geometry, the notion of branch points can be generalized to mappings between arbitrary algebraic curves. Let ƒ:\"X\" → \"Y\" be a morphism of algebraic curves. By pulling back rational functions on \"Y\" to rational functions on \"X\", \"K\"(\"X\") is a field extension of \"K\"(\"Y\"). The degree of ƒ is defined to be the degree of this field extension [\"K\"(\"X\"):\"K\"(\"Y\")], and ƒ is said to be finite if the degree is finite.\n\nAssume that ƒ is finite. For a point \"P\" ∈ \"X\", the ramification index \"e\" is defined as follows. Let \"Q\" = ƒ(\"P\") and let \"t\" be a local uniformizing parameter at \"P\"; that is, \"t\" is a regular function defined in a neighborhood of \"Q\" with \"t\"(\"Q\") = 0 whose differential is nonzero. Pulling back \"t\" by ƒ defines a regular function on \"X\". Then\nwhere \"v\" is the valuation in the local ring of regular functions at \"P\". That is, \"e\" is the order to which formula_10 vanishes at \"P\". If \"e\" > 1, then ƒ is said to be ramified at \"P\". In that case, \"Q\" is called a branch point.\n\n"}
{"id": "4264157", "url": "https://en.wikipedia.org/wiki?curid=4264157", "title": "Burning Ship fractal", "text": "Burning Ship fractal\n\nThe Burning Ship fractal, first described and created by Michael Michelitsch and Otto E. Rössler in 1992, is generated by iterating the function:\n\nin the complex plane formula_2 which will either escape or remain bounded. The difference between this calculation and that for the Mandelbrot set is that the real and imaginary components are set to their respective absolute values before squaring at each iteration. The mapping is non-analytic because its real and imaginary parts do not obey the Cauchy–Riemann equations.\n\nThe below pseudocode implementation hardcodes the complex operations for Z. Consider implementing complex number operations to allow for more dynamic and reusable code. Note that the typical images of the Burning Ship fractal display the ship upright: the actual fractal, and that produced by the below pseudocode, is inverted along the x-axis.\nFor each pixel (x, y) on the screen, do:\n\n"}
{"id": "4638484", "url": "https://en.wikipedia.org/wiki?curid=4638484", "title": "Clause (logic)", "text": "Clause (logic)\n\nIn logic, a clause is an expression formed from a finite collection of literals (atoms or their negations) that is true either whenever at least one of the literals that form it is true (a disjunctive clause, the most common use of the term), or when all of the literals that form it are true (a conjunctive clause, a less common use of the term). That is, it is a finite disjunction or conjunction of literals, depending on the context. Clauses are usually written as follows, where the symbols formula_1 are literals:\n\nA clause can be empty (defined from an empty set of literals).\nThe empty clause is denoted by various symbols such as formula_3,\nformula_4, or formula_5. The truth evaluation of an empty disjunctive\nclause is always formula_6. This is justified by considering that formula_6 is the neutral element of the monoid formula_8.\n\nThe truth evaluation of an empty conjunctive clause is always formula_9. This is related to the concept of a vacuous truth.\n\nEvery nonempty clause is logically equivalent to an implication of a head from a body, where the head is an arbitrary literal of the clause and the body is the conjunction of the negations of the other literals. That is, if a truth assignment causes a clause to be true, and none of the literals of the body satisfy the clause, then the head must also be true.\n\nThis equivalence is commonly used in logic programming, where clauses are usually written as an implication in this form. More generally, the head may be a disjunction of literals. If formula_10 are the literals in the body of a clause and formula_11 are those of its head, the clause is usually written as follows:\n\n\n\n"}
{"id": "51203", "url": "https://en.wikipedia.org/wiki?curid=51203", "title": "Coefficient", "text": "Coefficient\n\nIn mathematics, a coefficient is a multiplicative factor in some term of a polynomial, a series, or any expression; it is usually a number, but may be any expression. In the latter case, the variables appearing in the coefficients are often called parameters, and must be clearly distinguished from the other variables. \n\nFor example, in\nthe first two terms respectively have the coefficients 7 and −3. The third term 1.5 is a constant coefficient. The final term does not have any explicitly written coefficient, but is considered to have coefficient 1, since multiplying by that factor would not change the term. \n\nOften coefficients are numbers as in this example, although they could be parameters of the problem or any expression in these parameters. In such a case one must clearly distinguish between symbols representing variables and symbols representing parameters. Following René Descartes, the variables are often denoted by , , ..., and the parameters by , , , ..., but it is not always the case. For example, if is considered as a parameter in the above expression, the coefficient of is , and the constant coefficient is .\n\nWhen one writes \nit is generally supposed that is the only variable and that , and are parameters; thus the constant coefficient is in this case.\n\nSimilarly, any polynomial in one variable can be written as\nfor some positive integer formula_4, where formula_5 are coefficients; to allow this kind of expression in all cases one must allow introducing terms with 0 as coefficient.\nFor the largest formula_6 with formula_7 (if any), formula_8 is called the leading coefficient of the polynomial. So for example the leading coefficient of the polynomial\n\nis 4.\n\nSome specific coefficients that occur frequently in mathematics have received a name. This is the case of the binomial coefficients, the coefficients which occur in the expanded form of formula_10, and are tabulated in Pascal's triangle.\n\nIn linear algebra, the leading coefficient (also leading entry) of a row in a matrix is the first nonzero entry in that row. So, for example, given\n\nThe leading coefficient of the first row is 1; 2 is the leading coefficient of the second row; 4 is the leading coefficient of the third row, and the last row does not have a leading coefficient.\n\nThough coefficients are frequently viewed as constants in elementary algebra, they can be variables more generally. For example, the coordinates formula_12 of a vector formula_13 in a vector space with basis formula_14, are the coefficients of the basis vectors in the expression \n\n\n"}
{"id": "632224", "url": "https://en.wikipedia.org/wiki?curid=632224", "title": "Compare-and-swap", "text": "Compare-and-swap\n\nIn computer science, compare-and-swap (CAS) is an atomic instruction used in multithreading to achieve synchronization. It compares the contents of a memory location with a given value and, only if they are the same, modifies the contents of that memory location to a new given value. This is done as a single atomic operation. The atomicity guarantees that the new value is calculated based on up-to-date information; if the value had been updated by another thread in the meantime, the write would fail. The result of the operation must indicate whether it performed the substitution; this can be done either with a simple boolean response (this variant is often called compare-and-set), or by returning the value read from the memory location (\"not\" the value written to it).\n\nAlgorithms built around CAS typically read some key memory location and remember the old value. Based on that old value, they compute some new value. Then they try to swap in the new value using CAS, where the comparison checks for the location still being equal to the old value. If CAS indicates that the attempt has failed, it has to be repeated from the beginning: the location is re-read, a new value is re-computed and the CAS is tried again.\n\nA compare-and-swap operation is an atomic version of the following pseudocode, where denotes access through a pointer:\n\nThis operation is used to implement synchronization primitives like semaphores and mutexes, as well as more sophisticated lock-free and wait-free algorithms. Maurice Herlihy (1991) proved that CAS can implement more of these algorithms than atomic read, write, or fetch-and-add, and assuming a fairly large amount of memory, that it can implement all of them. CAS is equivalent to load-link/store-conditional, in the sense that a constant number of invocations of either primitive can be used to implement the other one in a wait-free manner.\n\nAlgorithms built around CAS typically read some key memory location and remember the old value. Based on that old value, they compute some new value. Then they try to swap in the new value using CAS, where the comparison checks for the location still being equal to the old value. If CAS indicates that the attempt has failed, it has to be repeated from the beginning: the location is re-read, a new value is re-computed and the CAS is tried again. Instead of immediately retrying after a CAS operation fails, researchers have found that total system performance can be improved in multiprocessor systems—where many threads constantly update some particular shared variable—if threads that see their CAS fail use exponential backoff—in other words, wait a little before retrying the CAS.\n\nAs an example use case of compare-and-swap, here is an algorithm for atomically incrementing or decrementing an integer. This is useful in a variety of applications that use counters. The function performs the action , atomically (again denoting pointer indirection by , as in C) and returns the final value stored in the counter. Unlike in the pseudocode above, there is no requirement that any sequence of operations is atomic except for .\n\nIn this algorithm, if the value of changes after (or while!) it is fetched and before the CAS does the store, CAS will notice and report this fact, causing the algorithm to retry.\n\nSome CAS-based algorithms are affected by and must handle the problem of a false positive match, or the ABA problem. It is possible that between the time the old value is read and the time CAS is attempted, some other processors or threads change the memory location two or more times such that it acquires a bit pattern which matches the old value. The problem arises if this new bit pattern, which looks exactly like the old value, has a different meaning: for instance, it could be a recycled address, or a wrapped version counter.\n\nA general solution to this is to use a double-length CAS (e.g. on a 32-bit system, a 64-bit CAS). The second half is used to hold a counter. The compare part of the operation compares the previously read value of the pointer \"and\" the counter, with the current pointer and counter. If they match, the swap occurs - the new value is written - but the new value has an incremented counter. This means that if ABA has occurred, although the pointer value will be the same, the counter is exceedingly unlikely to be the same (for a 32-bit value, a multiple of 2 operations would have to have occurred, causing the counter to wrap and at that moment, the pointer value would have to also by chance be the same).\n\nAn alternative form of this (useful on CPUs which lack DCAS) is to use an index into a freelist, rather than a full pointer, e.g. with a 32-bit CAS, use a 16-bit index and a 16-bit counter. However, the reduced counter lengths begin to make ABA - especially at modern CPU speeds - likely.\n\nOne simple technique which helps alleviate this problem is to store an ABA counter in each data structure element, rather than using a single ABA counter for the whole data structure.\n\nA more complicated but more effective solution is to implement safe memory reclamation (SMR). This is in effect lock-free garbage collection. The advantage of using SMR is the assurance a given pointer will exist only once at any one time in the data structure, thus the ABA problem is completely solved. (Without SMR, something like a freelist will be in use, to ensure that all data elements can be accessed safely (no memory access violations) even when they are no longer present in the data structure. With SMR, only elements actually currently in the data structure will be accessed).\n\nCAS, and other atomic instructions, are sometimes thought to be unnecessary in uniprocessor systems, because the atomicity of any sequence of instructions can be achieved by disabling interrupts while executing it. However, disabling interrupts has numerous downsides. For example, code that is allowed to do so must be trusted not to be malicious and monopolize the CPU, as well as to be correct and not accidentally hang the machine in an infinite loop or page fault. Further, disabling interrupts is often deemed too expensive to be practical. Thus, even programs only intended to run on uniprocessor machines will benefit from atomic instructions, as in the case of Linux's futexes.\n\nIn multiprocessor systems, it is usually impossible to disable interrupts on all processors at the same time. Even if it were possible, two or more processors could be attempting to access the same semaphore's memory at the same time, and thus atomicity would not be achieved. The compare-and-swap instruction allows any processor to atomically test and modify a memory location, preventing such multiple-processor collisions.\n\nOn server-grade multi-processor architectures of the 2010s, compare-and-swap is cheap relative to a simple load that is not served from cache. A 2013 paper points out that a CAS is only 1.15 times more expensive than a non-cached load on Intel Xeon (Westmere-EX) and 1.35 times on AMD Opteron (Magny-Cours).\n\nCompare-and-swap (and compare-and-swap-double) has been an integral part of the IBM 370 (and all successor) architectures since 1970. The operating systems that run on these architectures make extensive use of this instruction to facilitate process (i.e., system and user tasks) and processor (i.e., central processors) parallelism while eliminating, to the greatest degree possible, the \"disabled spin locks\" which had been employed in earlier IBM operating systems. Similarly, the use of test-and-set was also eliminated. In these operating systems, new units of work may be instantiated \"globally\", into the global service priority list, or \"locally\", into the local service priority list, by the execution of a single compare-and-swap instruction. This substantially improved the responsiveness of these operating systems.\n\nIn the x86 (since 80486) and Itanium architectures this is implemented as the compare and exchange (CMPXCHG) instruction (on a multiprocessor the LOCK prefix must be used).\n\nAs of 2013, most multiprocessor architectures support CAS in hardware, and the compare-and-swap operation is the most popular synchronization primitive for implementing both lock-based and non-blocking concurrent data structures.\n\nThe atomic counter and atomic bitmask operations in the Linux kernel typically use a compare-and-swap instruction in their implementation.\nThe SPARC 32 and PA-RISC architectures are two of the very few recent architectures that do not support CAS in hardware; the Linux port to these architectures uses a spinlock.\n\nMany C compilers support using compare-and-swap either with the C11\ncodice_1 functions,\nor some non-standard C extension of that particular C compiler,\nor by calling a function written directly in assembly language using the compare-and-swap instruction.\n\nThe following C function shows the basic behavior of a compare-and-swap variant that returns the old value of the specified memory location; however, this version does not provide the crucial guarantees of atomicity that a real compare-and-swap operation would:\n\ncodice_2 is always returned, but it can be tested following the codice_3 operation to see if it matches codice_4, as it may be different, meaning that another process has managed to succeed in a competing codice_3 to change the reg value from codice_4.\n\nFor example, an election protocol can be implemented such that every process checks the result of codice_3 against its own PID (= newval). The winning process finds the codice_3 returning the initial non-PID value (e.g., zero). For the losers it will return the winning PID.\n\nThis is the logic in the Intel Software Manual Vol 2A.\n\nSince CAS operates on a single pointer-sized memory location, while most lock-free and wait-free algorithms need to modify multiple locations, several extensions have been implemented.\n\n\n\n\n"}
{"id": "16261979", "url": "https://en.wikipedia.org/wiki?curid=16261979", "title": "Crypto-1", "text": "Crypto-1\n\nCrypto1 is a proprietary encryption algorithm created by NXP Semiconductors specifically for Mifare RFID tags, including Oyster card, CharlieCard and OV-chipkaart.\n\nRecent cryptographic research has shown that, \"the security of this cipher is ... close to zero\".\nCrypto1 is a stream cipher very similar in its structure to its successor, Hitag2. Crypto1 consists of\n\nIt can operate as an NLFSR and as an LFSR, depending on its input parameters. Outputs of one or both linear and nonlinear functions can be fed back into the cipher state or used as its output filters. The usual operation of Crypto1 and Hitag2 ciphers uses nonlinear feedback only during the initialization/authentication stage, switching to operation as LFSR with a nonlinear output filter for encrypting the tag's communications in both directions.\nPositiveID\n\n"}
{"id": "571341", "url": "https://en.wikipedia.org/wiki?curid=571341", "title": "DOT (graph description language)", "text": "DOT (graph description language)\n\nDOT is a graph description language. DOT graphs are typically files with the file extension \"gv\" or \"dot\". The extension \"gv\" is preferred to avoid confusion with the extension \"dot\" used by early (pre-2007) versions of Microsoft Word.\n\nVarious programs can process DOT files. Some, such as \"dot\", \"neato\", \"twopi\", \"circo\", \"fdp\", and \"sfdp\", can read a DOT file and render it in graphical form. Others, such as \"gvpr\", \"gc\", \"acyclic\", \"ccomps\", \"sccmap\", and \"tred\", read DOT files and perform calculations on the represented graph. Finally, others, such as \"lefty\", \"dotty\", and \"grappa\", provide an interactive interface. The \"GVedit\" tool combines a text editor with noninteractive image viewer. Most programs are part of the Graphviz package or use it internally.\n\nAt its simplest, DOT can be used to describe an undirected graph. An undirected graph shows simple relations between objects, such as friendship between people. The \"graph\" keyword is used to begin a new graph, and nodes are described within curly braces. A double-hyphen (--) is used to show relations between the nodes.\n\nSimilar to undirected graphs, DOT can describe directed graphs, such as flowcharts and dependency trees. The syntax is the same as for undirected graphs, except the \"digraph\" keyword is used to begin the graph, and an arrow (->) is used to show relationships between nodes.\n\nVarious attributes can be applied to graphs, nodes and edges in DOT files. These attributes can control aspects such as color, shape, and line styles. For nodes and edges, one or more attribute-value pairs are placed in square brackets ([]) after a statement and before the semicolon (which is optional). Graph attributes are specified as direct attribute-value pairs under the graph element, where multiple attributes are separated by a comma or using multiple sets of square brackets, while node attributes are placed after a statement containing only the name of the node, but not the relations between the dots.\n\nHTML-like labels are only available on versions of Graphviz that are newer than mid-November 2003, in particular, they are not considered as part of release 1.10.\n\nDot supports C and C++ style single line and multiple line comments. In addition, it ignores lines with a number sign symbol (#) as their first character.\n\nFollowing is an example script that describes the bonding structure of an ethane molecule. This is an undirected graph and contains node attributes as explained above.\n\nThe DOT language defines a graph, but does not provide facilities for rendering the graph. There are several programs that can be used to render, view, and manipulate graphs in the DOT language:\n\nIt is possible to specify layout details with DOT, although not all tools that implement the DOT language pay attention to the position attributes. Thus, depending on the tools used, users must rely on automated layout algorithms (potentially resulting in unexpected output) or tediously hand-positioned nodes.\n\nFor example:\nThere are two problems in the image above. The square on the right is not a perfect square and the labels are in the wrong place.\n\nThis can be fixed with Inkscape or other SVG editors. In some cases, this can also be fixed by using the \"pos\" attribute to specify a position, and the \"weight\" attribute to square the graph.\n\n"}
{"id": "30638875", "url": "https://en.wikipedia.org/wiki?curid=30638875", "title": "Damping matrix", "text": "Damping matrix\n\nIn applied mathematics, a damping matrix is a matrix corresponding to any of certain systems of linear ordinary differential equations. \n\nA damping matrix is defined as follows. If the system has \"n\" degrees of freedom \"u\" and is under application of \"m\" damping forces.\n\nEach force can be expressed as follows:\n\nIt yields in matrix form;\n\nwhere C is the damping matrix composed by the damping coefficients:\n\n"}
{"id": "1198479", "url": "https://en.wikipedia.org/wiki?curid=1198479", "title": "Degree symbol", "text": "Degree symbol\n\nThe degree symbol (°) is a typographical symbol that is used, among other things, to represent degrees of arc (e.g. in geographic coordinate systems), hours (in the medical field), degrees of temperature, alcohol proof, or diminished quality in musical harmony. The symbol consists of a small raised circle, historically a zero glyph.\n\nIn Unicode it is encoded at .\n\nThe first known recorded modern use of the degree symbol in mathematics is from 1657\nwhere the usage seems to show that the symbol is a small raised zero, to match the prime symbol notation of sexagesimal subdivisions of degree such as minute (′), second (″), and third (‴), which originate as small raised Roman numerals.\n\nIn the case of degrees of angular arc, the degree symbol follows the number without any intervening space (e.g., 30°). The addition of minute and second of arc units follow the degree units, with intervening spaces between the units but no spaces between the numbers and arc symbols (e.g., 30° 12′ 5″).\n\nIn the case of degrees of temperature, three scientific and engineering standards bodies (BIPM, ISO and the U.S. Government Printing Office) prescribe printing temperatures with a space between the number and the degree symbol (e.g., 10 °C). However, in many works with professional typesetting, including scientific works published by the University of Chicago Press or Oxford University Press, the degree symbol is printed with no spaces between the number, the symbol, and the Latin letters \"C\" or \"F\" representing Celsius or Fahrenheit, respectively (e.g., 10°C). This is also the practice of the University Corporation for Atmospheric Research, which operates the National Center for Atmospheric Research. Though not recommended, use of the degree symbol without a following Latin letter is done so without a space between the number and symbol (e.g., 10°); this is considered more acceptable if the standard of temperature is not known, but it is recommended in this case that the full word be used rather than the symbol (e.g., 10 degrees). Use of the degree symbol to refer to temperatures measured in kelvins (symbol: K) was abolished in 1967 by the 13th General Conference on Weights and Measures (CGPM). Therefore, the triple point of water, for instance, is correctly written simply as 273.16 K. The name of the SI unit of temperature is now \"kelvin\" (note the lower case), and no longer \"degrees Kelvin\".\n\nThe degree sign is included in Unicode as .\n\nFor use with Chinese characters there are also code points for and .\n\nThe degree sign was missing from the basic 7-bit ASCII set of 1963, but in 1987 the ISO/IEC 8859 standard introduced it at position 0xB0 (176 decimal) in the Latin-1 variant. In 1991 the Unicode standard incorporated all of the Latin-1 code points, including the degree sign.\n\nThe Windows Code Page 1252 was also an extension of the Latin-1 standard, so it had the degree sign at the same code point.\nThe code point in the older DOS Code Page 437 was 0xF8 (248 decimal); therefore, the Alt code used to enter the symbol directly from the keyboard is .\n\nOther characters with similar appearance but different meanings include:\n\nSome computer keyboard layouts, such as the QWERTY layout as used in Italy, the QWERTZ layout as used in Germany, Austria and Switzerland, and the AZERTY layout as used in France and Belgium, have the degree symbol available directly on a key. But the common keyboard layouts in English-speaking countries do not include the degree sign, which then has to be input some other way. The method of inputting depends on the operating system being used.\n\nOn the Colemak keyboard layout (Windows/Mac), one can press + followed by to insert a degree sign. On Linux, one can press + twice to insert a degree sign.\n\nWith Microsoft Windows, there are several ways to make the degree symbol:\n\nIn Microsoft Office and similar programs, there is often also an \"Insert\" menu with an \"Insert Symbol\" or \"Symbol\" command that brings up a graphical palette of symbols to insert, including the degree symbol. In WordPerfect, pressing brings up lists of special characters. The character map is usually sorted as per the unicode tables, so sixteen characters horizontally (0–F); this may vary from system to system, though. An easier way is to simply enter the hexadecimal value and press ; if the characters touching the Unicode number is any digit or the letters A–F, make sure there is a space before pressing . Example: ‘At 71[insert space]b0[remove space before symbol] N, temperatures can get frigid.’\n\nIn the classic Mac OS and macOS operating systems, the degree symbol can be entered by typing . One can also use the Mac OS character palette, which is available in many programs by selecting \"Special Characters\" from the Edit Menu, or from the Input Menu (flag) icon on the menu bar (enabled in the International section of the System Preferences).\n\nIn iOS, the degree symbol is accessed by pressing and holding and dragging your finger to the degree symbol. This procedure is the same as entering diacritics on other characters.\n\nIn LaTeX, the packages codice_1 and codice_2 provide the commands codice_3 and codice_4, respectively. In the absence of these packages one can write the degree symbol as codice_5 in math mode. In other words, it is written as the empty circle glyph codice_6 as a superscript.\n\nIn Linux operating systems such as Ubuntu, this symbol may be entered via the Compose key followed by , . Some keyboard layouts print this symbol upon pressing (once or twice, depending on specific keyboard layout), and, in programs created by GTK+, one can enter Unicode characters in any text entry field by first pressing , regardless of keyboard layout. For the degree symbol, this is done by entering .\n\n\n"}
{"id": "33005523", "url": "https://en.wikipedia.org/wiki?curid=33005523", "title": "Denisyuk polynomials", "text": "Denisyuk polynomials\n\nIn mathematics, Denisyuk polynomials De(\"x\") or \"M\"(\"x\") are generalizations of the Laguerre polynomials introduced by given by the generating function\n\n"}
{"id": "20565885", "url": "https://en.wikipedia.org/wiki?curid=20565885", "title": "Early numeracy", "text": "Early numeracy\n\nEarly numeracy is a branch of numeracy that aims to enhance numeracy learning for younger learners, particularly those at-risk in the area of mathematics. Usually the mathematical learning begins with simply learning the digits, being 1-10. This is done because it acts as an entry way to the expansion of counting. One can keep track of the digits using any of the phalanges\n\n"}
{"id": "31583410", "url": "https://en.wikipedia.org/wiki?curid=31583410", "title": "Elitzur's theorem", "text": "Elitzur's theorem\n\nElitzur's theorem is a theorem in quantum and statistical field theory stating that local gauge symmetries cannot be spontaneously broken. The theorem was proposed in 1975 by Shmuel Elitzur, who proved it for Abelian gauge fields on a lattice. It is nonetheless possible to spontaneously break a global symmetry within a theory that has a local gauge symmetry, as in the Higgs mechanism.\n\n\n"}
{"id": "236020", "url": "https://en.wikipedia.org/wiki?curid=236020", "title": "Elliptic geometry", "text": "Elliptic geometry\n\nElliptic geometry is a geometry in which Euclid's parallel postulate does not hold. Elliptic geometry is studied in two, three, or more dimensions. The appearance of this geometry in the nineteenth century stimulated the development of non-Euclidean geometry generally, including hyperbolic geometry.\n\nElliptic geometry has a variety of properties that differ from those of classical Euclidean plane geometry. For example, the sum of the interior angles of any triangle is always greater than 180°.\n\nIn elliptic geometry, two lines perpendicular to a given line must intersect. In fact, the perpendiculars on one side all intersect at the \"absolute pole\" of the given line. The perpendiculars on the other side also intersect at a point, which is different from the other absolute pole only in spherical geometry, for in elliptic geometry the poles on either side are the same. There are no antipodal points in elliptic geometry. Every point corresponds to an \"absolute polar line\" of which it is the absolute pole. Any point on this polar line forms an \"absolute conjugate pair\" with the pole. Such a pair of points is \"orthogonal\", and the distance between them is a \"quadrant\".\n\nThe distance between a pair of points is proportional to the angle between their absolute polars.\n\nAs explained by H. S. M. Coxeter\n\nThe elliptic plane is the real projective plane provided with a metric: Kepler and Desargues used the gnomonic projection to relate a plane σ to points on a hemisphere tangent to it. With O the center of the hemisphere, a point \"P\" in σ determines a line \"OP\" intersecting the hemisphere, and any line \"L\" ⊂ σ determines a plane \"OL\" which intersects the hemisphere in half of a great circle. The hemisphere is bounded by a plane through O and parallel to σ. No ordinary line of σ corresponds to this plane; instead a line at infinity is appended to σ. As any line in this extension of σ corresponds to a plane through O, and since any pair of such planes intersects in a line through O, one can conclude that any pair of lines in the extension intersect: the point of intersection lies where the plane intersection meets σ or the line at infinity. Thus the axiom of projective geometry, requiring all pairs of lines in a plane to intersect, is confirmed.\n\nGiven \"P\" and \"Q\" in σ, the elliptic distance between them is the measure of the angle \"POQ\", usually taken in radians. Arthur Cayley initiated the study of elliptic geometry when he wrote \"On the definition of distance\". This venture into abstraction in geometry was followed by Felix Klein and Bernhard Riemann leading to non-Euclidean geometry and Riemannian geometry.\n\nIn Euclidean geometry, a figure can be scaled up or scaled down indefinitely, and the resulting figures are similar, i.e., they have the same angles and the same internal proportions. In elliptic geometry this is not the case. For example, in the spherical model we can see that the distance between any two points must be strictly less than half the circumference of the sphere (because antipodal points are identified). A line segment therefore cannot be scaled up indefinitely. A geometer measuring the geometrical properties of the space he or she inhabits can detect, via measurements, that there is a certain distance scale that is a property of the space. On scales much smaller than this one, the space is approximately flat, geometry is approximately Euclidean, and figures can be scaled up and down while remaining approximately similar.\n\nA great deal of Euclidean geometry carries over directly to elliptic geometry. For example, the first and fourth of Euclid's postulates, that there is a unique line between any two points and that all right angles are equal, hold in elliptic geometry. Postulate 3, that one can construct a circle with any given center and radius, fails if \"any radius\" is taken to mean \"any real number\", but holds if it is taken to mean \"the length of any given line segment\". Therefore any result in Euclidean geometry that follows from these three postulates will hold in elliptic geometry, such as proposition 1 from book I of the \"Elements\", which states that given any line segment, an equilateral triangle can be constructed with the segment as its base.\n\nElliptic geometry is also like Euclidean geometry in that space is continuous, homogeneous, isotropic, and without boundaries. Isotropy is guaranteed by the fourth postulate, that all right angles are equal. For an example of homogeneity, note that Euclid's proposition I.1 implies that the same equilateral triangle can be constructed at any location, not just in locations that are special in some way. The lack of boundaries follows from the second postulate, extensibility of a line segment.\n\nOne way in which elliptic geometry differs from Euclidean geometry is that the sum of the interior angles of a triangle is greater than 180 degrees. In the spherical model, for example, a triangle can be constructed with vertices at the locations where the three positive Cartesian coordinate axes intersect the sphere, and all three of its internal angles are 90 degrees, summing to 270 degrees. For sufficiently small triangles, the excess over 180 degrees can be made arbitrarily small.\n\nThe Pythagorean theorem fails in elliptic geometry. In the 90°–90°–90° triangle described above, all three sides have the same length, and consequently do not satisfy formula_1. The Pythagorean result is recovered in the limit of small triangles.\n\nThe ratio of a circle's circumference to its area is smaller than in Euclidean geometry. In general, area and volume do not scale as the second and third powers of linear dimensions.\n\nElliptic space can be constructed in a way similar to the construction of three-dimensional vector space: with equivalence classes. One uses directed arcs on great circles of the sphere. As directed line segments are equipollent when they are parallel, of the same length, and similarly oriented, so directed arcs found on great circles are equipollent when they are of the same length, orientation, and great circle. These relations of equipollence produce 3D vector space and elliptic space, respectively.\n\nAccess to elliptic space structure is provided through the vector algebra of William Rowan Hamilton: he envisioned a sphere as a domain of square roots of minus one. Then Euler's formula formula_2 (where \"r\" is on the sphere) represents the great circle in the plane perpendicular to \"r\". Opposite points \"r\" and –\"r\" correspond to oppositely directed circles. An arc between θ and φ is equipollent with one between 0 and φ – θ. In elliptic space, arc length is less than π, so arcs may be parametrized with θ in [0, π) or (–π/2, π/2].\n\nFor formula_3 It is said that the modulus or norm of \"z\" is one (Hamilton called it the tensor of z). But since \"r\" ranges over a sphere in 3-space, exp(θ r) ranges over a sphere in 4-space, now called the 3-sphere, as its surface has three dimensions. Hamilton called his algebra quaternions and it quickly became a useful and celebrated tool of mathematics. Its space of four dimensions is evolved in polar co-ordinates formula_4 with \"t\" in the positive real numbers.\n\nWhen doing trigonometry on Earth or the celestial sphere, the sides of the triangles are great circle arcs. The first success of quaternions was a rendering of spherical trigonometry to algebra. Hamilton called a quaternion of norm one a versor, and these are the points of elliptic space.\n\nWith fixed, the versors \nform an \"elliptic line\". The distance from formula_6 to 1 is . For an arbitrary versor , the distance will be that θ for which since this is the formula for the scalar part of any quaternion.\n\nAn \"elliptic motion\" is described by the quaternion mapping\nDistances between points are the same as between image points of an elliptic motion. In the case that and are quaternion conjugates of one another, the motion is a spatial rotation, and their vector part is the axis of rotation. In the case the elliptic motion is called a right \"Clifford translation\", or a \"parataxy\". The case corresponds to left Clifford translation.\n\n\"Elliptic lines\" through versor  may be of the form\nThey are the right and left Clifford translations of  along an elliptic line through 1.\nThe \"elliptic space\" is formed by from by identifying antipodal points.\n\nElliptic space has special structures called Clifford parallels and Clifford surfaces.\n\nThe versor points of elliptic space are mapped by the Cayley transform to ℝ for an alternative representation of the space.\n\nThe hyperspherical model is the generalization of the spherical model to higher dimensions. The points of \"n\"-dimensional elliptic space are the pairs of unit vectors in R, that is, pairs of opposite points on the surface of the unit ball in -dimensional space (the \"n\"-dimensional hypersphere). Lines in this model are great circles, i.e., intersections of the hypersphere with flat hypersurfaces of dimension \"n\" passing through the origin.\n\nIn the projective model of elliptic geometry, the points of \"n\"-dimensional real projective space are used as points of the model. This models an abstract elliptic geometry that is also known as projective geometry.\n\nThe points of \"n\"-dimensional projective space can be identified with lines through the origin in -dimensional space, and can be represented non-uniquely by nonzero vectors in R, with the understanding that and , for any non-zero scalar , represent the same point. Distance is defined using the metric\nthat is, the distance between two points is the angle between their corresponding lines in R. The distance formula is homogeneous in each variable, with if and are non-zero scalars, so it does define a distance on the points of projective space.\n\nA notable property of the projective elliptic geometry is that for even dimensions, such as the plane, the geometry is non-orientable. It erases the distinction between clockwise and counterclockwise rotation by identifying them.\n\nA model representing the same space as the hyperspherical model can be obtained by means of stereographic projection. Let E represent that is, -dimensional real space extended by a single point at infinity. We may define a metric, the \"chordal metric\", on\nE by\nwhere and are any two vectors in R and formula_12 is the usual Euclidean norm. We also define\nThe result is a metric space on E, which represents the distance along a chord of the corresponding points on the hyperspherical model, to which it maps bijectively by stereographic projection. We obtain a model of spherical geometry if we use the metric\nElliptic geometry is obtained from this by identifying the points and , and taking the distance from to this pair to be the minimum of the distances from to each of these two points.\n\nBecause spherical elliptic geometry can be modeled as, for example, a spherical subspace of a Euclidean space, it follows that if Euclidean geometry is self-consistent, so is spherical elliptic geometry. Therefore it is not possible to prove the parallel postulate based on the other four postulates of Euclidean geometry.\n\nTarski proved that elementary Euclidean geometry is complete: there is an algorithm which, for every proposition, can show it to be either true or false. (This does not violate Gödel's theorem, because Euclidean geometry cannot describe a sufficient amount of arithmetic for the theorem to apply.) It therefore follows that elementary elliptic geometry is also self-consistent and complete.\n\n\n"}
{"id": "878327", "url": "https://en.wikipedia.org/wiki?curid=878327", "title": "Engineering notation", "text": "Engineering notation\n\nEngineering notation or engineering form is a version of scientific notation in which the exponent of ten must be divisible by three (i.e., they are powers of a thousand, but written as, for example, 10 instead of 1000). As an alternative to writing powers of 10, SI prefixes can be used, which also usually provide steps of a factor of a thousand.\n\nOn most calculators, engineering notation is called \"ENG\" mode.\n\nAn early implementation of engineering notation in form of range selection and number display with SI prefixes was introduced in the computerized HP 5360A frequency counter by Hewlett-Packard in 1969.\n\nBased on an idea by Peter D. Dickinson the first calculator to support engineering notation displaying the power-of-ten exponent values was the HP-25 in 1975. It was implemented as a dedicated display mode in addition to scientific notation.\n\nIn 1975 Commodore introduced a number of scientific calculators (like the SR4148/SR4148R and SR4190R) providing a \"variable scientific notation\", where pressing the and keys shifted the exponent and decimal point by ±1 in \"scientific\" notation. Between 1976 and 1980 the same \"exponent shift\" facility was also available on some Texas Instruments calculators of the pre-LCD era such as early SR-40, TI-30 and TI-45 model variants utilizing () instead. This can be seen as a precursor to a feature implemented on many Casio calculators since about 1978/1979 (f.e. in the FX-501P/FX-502P), where number display in \"engineering\" notation is available on demand by the single press of a () button (instead of having to activate a dedicated display mode as on most other calculators), and subsequent button presses would shift the exponent and decimal point of the number displayed by ±3 in order to easily let results match a desired prefix. Some graphical calculators (for example the fx-9860G) in the 2000s also support the display of some SI prefixes (f, p, n, µ, m, k, M, G, T, P, E) as suffixes in engineering mode.\n\nCompared to normalized scientific notation, one disadvantage of using SI prefixes and engineering notation is that significant figures are not always readily apparent. For example, 500 µm and 500 × 10 m cannot express the uncertainty distinctions between 5 × 10 m, 5.0 × 10 m, and 5.00 × 10 m. This can be solved by changing the range of the coefficient in front of the power from the common 1–1000 to 0.001–1.0. In some cases this may be suitable; in others it may be impractical. In the previous example, 0.5 mm, 0.50 mm, or 0.500 mm would have been used to show uncertainty and significant figures. It is also common to state the precision explicitly, such as \"47 kΩ ±5%\"\n\nAnother example: when the speed of light (exactly by the definition of the meter and second) is expressed as 3.00 × 10 m/s or 3.00 × 10 km/s then it is clear that it is between 299 500 km/s and 300 500 km/s, but when using 300 × 10 m/s, or 300 × 10 km/s, 300 000 km/s, or the unusual but short 300 Mm/s, this is not clear. A possibility is using 0.300 Gm/s, convenient to write, but somewhat impractical in understanding (writing something large as a fraction of something even larger; in a context of larger numbers expressed in the same unit this could be convenient, but that is not applicable here).\n\nOn the other hand, engineering notation allows the numbers to explicitly match their corresponding SI prefixes, which facilitates reading and oral communication. For example, 12.5 × 10 m can be read as \"twelve-point-five nanometers\" and written as 12.5 nm, while its scientific notation equivalent 1.25 × 10 m would likely be read out as \"one-point-two-five times ten-to-the-negative-eight meters\".\n\nEngineering notation, like scientific notation generally, can use the E-notation, such that\n\ncan be written as\n\nThe \"E\" (or \"e\") should not be confused with the exponential \"e\" which holds a completely different significance. In the latter case, it would be shown that 3e ≈ 0.000 370 23.\n\nJust like decimal engineering notation can be viewed as a base-1000 scientific notation (10 = 1000), binary engineering notation relates to a base-1024 scientific notation (2 = 1024), where the exponent of two must be divisible by ten. This is closely related to the base-2 floating-point representation commonly used in computer arithmetic, and the usage of IEC binary prefixes (e.g. 1B10 for 1 × 2, 1B20 for 1 × 2, 1B30 for 1 × 2, 1B40 for 1 × 2 etc.).\n\n\n"}
{"id": "2616413", "url": "https://en.wikipedia.org/wiki?curid=2616413", "title": "Erdős–Woods number", "text": "Erdős–Woods number\n\nIn number theory, a positive integer is said to be an Erdős–Woods number if it has the following property:\nthere exists a positive integer such that in the sequence of consecutive integers, each of the elements has a non-trivial common factor with one of the endpoints. In other words, is an Erdős–Woods number if there exists a positive integer such that for each integer between and , at least one of the greatest common divisors and is greater than .\n\nThe first few Erdős–Woods numbers are\n\nInvestigation of such numbers stemmed from the following prior conjecture by Paul Erdős:\n\nAlan R. Woods investigated this question for his 1981 thesis. Woods conjectured that whenever , the interval always includes a number coprime to both endpoints. It was only later that he found the first counterexample, , with . The existence of this counterexample shows that 16 is an Erdős–Woods number.\n"}
{"id": "58083234", "url": "https://en.wikipedia.org/wiki?curid=58083234", "title": "Exterior calculus identities", "text": "Exterior calculus identities\n\nThis article summarizes important identities in exterior calculus.\n\nThe following summarizes short definitions and notations that are used in this article.\n\nformula_1, formula_2 are formula_3-dimensional smooth manifolds, where formula_4. That is, differentiable manifolds that can be differentiated enough times for the purposes on this page.\n\nformula_5, formula_6 denote two points on the manifolds.\n\nformula_7 is the tangent bundle of the smooth manifold formula_1.\n\nformula_9, formula_10 denote the tangent spaces of formula_1, formula_2 at the points formula_13, formula_14, respectively.\n\nSections of the tangent bundles, also known as vector fields, are typically denoted as formula_15 such that at a point formula_5 we have formula_17.\n\nGiven an inner product formula_18 on each formula_9, the manifold becomes a Riemannian manifold.\n\nThe boundary of a manifold formula_20 is a manifold formula_21, which has dimension formula_22. An orientation on formula_20 induces an orientation on formula_21.\n\nWe usually denote a submanifold by formula_25.\n\nformula_26-forms are differential forms defined on formula_7. We denote the set of all formula_26-forms as formula_29. For formula_30 we usually write formula_31, formula_32, formula_33.\n\nformula_34-forms formula_35 are just scalar functions formula_36 on formula_1. formula_38 denotes the constant formula_34-form equal to formula_40 everywhere.\n\nWhen we are given formula_41 inputs formula_42 and a formula_26-form formula_31 we omit the formula_45th entry by writing\n\nThe exterior product is also known as the \"wedge product\". It is denoted by formula_47. The exterior product of a formula_26-form formula_31 and an formula_50-form formula_32 produce a formula_52-form formula_53. It can be written using the set formula_54 of all permutations formula_55 of formula_56 such that formula_57 as\n\nThe Lie bracket of sections formula_59 is defined as the unique section formula_60 that satisfies\n\nThe exterior derivative formula_62 is defined for all formula_63. We generally omit the subscript when it is clear from the context.\nFor a formula_34-form formula_65 we have formula_66 as the directional derivative formula_40-form. i.e. in the direction formula_68 we have formula_69.\n\nFor formula_70,\n\nIf formula_72 is a smooth map, then formula_73 defines a tangent map from formula_1 to formula_2. It is defined through curves formula_76 on formula_1 with derivative formula_78 such that\n\nNote that formula_80 is a formula_34-form with values in formula_2.\n\nIf formula_72 is a smooth map, then the pull-back of a formula_26-form formula_85 is defined such that for any formula_26 dimensional submanifold formula_87\n\nThe pull-back can also be expressed as\n\nGiven a section formula_90 there exists a formula_40-form formula_92 such that on each formula_93\n\nWe call this mapping the flat operator formula_95.\n\nGiven a formula_40-form formula_97 there exists a section formula_98 such that on each formula_99\n\nWe call this mapping the sharp operator formula_101. formula_101 and formula_95 constitute the musical isomorphisms.\n\nAlso known as the interior derivative, the interior product given a section formula_104 is a map formula_105 that effectively substitutes the first input of a formula_41-form with formula_107. If formula_108 and formula_109 then\n\nThe Hodge star operator formula_111 is defined as such that it maps formula_26-forms formula_113 to their dual formula_114-form formula_115.\n\nFor example, if formula_116 is a positively oriented frame for formula_7 according to the given metric formula_118, then\n\nWe omit to write the dimension formula_26 or inversion symbol formula_121 with the Hodge star operator as it is evident in the context. \n\nWe call formula_122 the signature of the metric formula_118. For example in Minkowski space formula_124 and in Riemannian manifolds formula_125.\n\nThe co-differential operator formula_126 on an formula_3 dimensional manifold formula_1 is defined by\n\nAn formula_3-dimensional orientable manifold formula_1 is a manifold that can be equipped with a choice of a non-zero formula_3-form formula_133.\n\nOn a orientable manifold formula_1 the canonical choice of a volume form given a metric formula_118 is formula_136 for any positively oriented basis formula_137.\n\nGiven a volume form formula_138 and a unit normal vector formula_2 we can also define an area form formula_140 on the \n\nThe inner product between two formula_26-forms formula_142 is defined pointwise on formula_1 by\n\nThe formula_145-inner product for the space of formula_26-forms formula_29 is defined by\n\nWe define the Lie derivative formula_149 through Cartan's magic formula for a given section formula_150 as\n\nIt describes the change of a formula_26-form along a flow map formula_153 associated to the section formula_154.\n\nThe Laplacian formula_155 is defined as formula_156.\n\nformula_31 is called...\n\n\nThe formula_26-th cohomology of a manifold formula_1 and its exterior derivative operators formula_164 is given by\n\nTwo closed formula_26-forms formula_142 are in the same cohomology class if their difference is an exact form i.e.\n\nA closed surface of genus formula_118 will have formula_170 generators which are harmonic.\n\nGiven formula_31\n\nIf formula_247\n\nIf formula_252 is a basis, then a basis of formula_29 is\n\nIf formula_264, then\n\n\n\n\nGiven the boundary formula_271 with unit normal vector formula_2\n\n\n\nIf formula_280, formula_281 such that\n\nIf formula_1 has only one cohomology class formula_284 and no boundary formula_285, then for any closed formula_286 such that\n\nLet Euclidean metric formula_288.\n\nWe use formula_289 differential operator formula_290\n\n"}
{"id": "4316185", "url": "https://en.wikipedia.org/wiki?curid=4316185", "title": "Finite measure", "text": "Finite measure\n\nIn measure theory, a branch of mathematics, a finite measure or totally finite measure is a special measure that always takes on finite values. Among finite measures are probability measures. The finite measures are often easier to handle than more general measures and show a variety of different properties depending on the sets they are defined on.\n\nA measure formula_1 on measurable space formula_2 is called a finite measure iff it satisfies\n\nBy the monotonicity of measures, this implies\n\nIf formula_1 is a finite measure, the measure space formula_6 is called a finite measure space or a totally finite measure space.\n\nFor any measurable space, the finite measures form a convex cone in the Banach space of signed measures with the total variation norm. Important subsets of the finite measures are the sub-probability measures, that form a convex subset, and the probability measures, which are the intersection of the unit sphere in the normed space of signed measures and the finite measures.\n\nIf formula_7 is a Hausdorff space and formula_8 contains the Borel formula_9-algebra then every finite measure is also a locally finite Borel measure.\n\nIf formula_7 is a metric space and the formula_8 is again the Borel formula_12-algebra, the weak convergence of measures can be defined. The corresponding topology is called weak topology and is the initial topology of all bounded continuous functions on formula_7. The weak topology corresponds to the weak* topology in functional analysis. If formula_7 is also separable, the weak convergence is metricized by the Lévy–Prokhorov metric.\n\nIf formula_7 is a polish space and formula_8 is the Borel formula_12-algebra, then every finite measure is a regular measure and therefore a Radon measure.\nIf formula_7 is polish then the set of all finite measures with the weak topology is polish too.\n"}
{"id": "152205", "url": "https://en.wikipedia.org/wiki?curid=152205", "title": "Forcing (mathematics)", "text": "Forcing (mathematics)\n\nIn the mathematical discipline of set theory, forcing is a technique for proving consistency and independence results. It was first used by Paul Cohen in 1963, to prove the independence of the axiom of choice and the continuum hypothesis from Zermelo–Fraenkel set theory.\n\nForcing has been considerably reworked and simplified in the following years, and has since served as a powerful technique, both in set theory and in areas of mathematical logic such as recursion theory. Descriptive set theory uses the notion of forcing from both recursion theory and set theory. Forcing has also been used in model theory, but it is common in model theory to define genericity directly without mention of forcing.\n\nIntuitively, forcing consists of expanding the set theoretical universe formula_1 to a larger universe formula_2. In this bigger universe, for example, one might have many new subsets of that were not there in the old universe, and thereby violate the continuum hypothesis.\n\nWhile impossible when dealing with finite sets, this is just another version of Cantor's paradox about infinity. In principle, one could consider:\n\nidentify formula_4 with formula_5, and then introduce an expanded membership relation involving \"new\" sets of the form formula_6. Forcing is a more elaborate version of this idea, reducing the expansion to the existence of one new set, and allowing for fine control over the properties of the expanded universe.\n\nCohen's original technique, now called ramified forcing, is slightly different from the unramified forcing expounded here. Forcing is also equivalent to the method of Boolean-valued models, which some feel is conceptually more natural and intuitive, but usually much more difficult to apply.\n\nA forcing poset is an ordered triple, formula_7, where formula_8 is a preorder on formula_9 that is atomless, meaning that it satisfies the following condition:\n\nMembers of formula_9 are called forcing conditions or just conditions. One reads formula_20 as \"formula_21 is stronger than formula_22\". Intuitively, the \"smaller\" condition provides \"more\" information, just as the smaller interval formula_23 provides more information about the number formula_24 than the interval formula_25 does.\n\nThere are various conventions in use. Some authors require formula_8 to also be antisymmetric, so that the relation is a partial order. Some use the term partial order anyway, conflicting with standard terminology, while some use the term preorder. The largest element can be dispensed with. The reverse ordering is also used, most notably by Saharon Shelah and his co-authors.\n\nAssociated with a forcing poset formula_9 is the class formula_28 of formula_9-names. A formula_9-name is a set formula_31 of the form\n\nThis is actually a definition by transfinite recursion. More precisely, one first uses transfinite recursion to define the following hierarchy:\n\nThen the class of formula_9-names is defined as\n\nThe formula_9-names are, in fact, an expansion of the universe. Given formula_4, one defines formula_38 to be the formula_9-name\n\nAgain, this is really a definition by transfinite recursion.\n\nGiven any subset formula_41 of formula_9, one next defines the interpretation or valuation map from formula_9-names by\n\nThis is again a definition by transfinite recursion. Note that if formula_45, then formula_46. One then defines\n\nso that formula_48.\n\nA good example of a forcing poset is formula_49, where formula_50 and formula_51 is the collection of Borel subsets of formula_52 having non-zero Lebesgue measure. In this case, one can talk about the conditions as being probabilities, and a formula_51-name assigns membership in a probabilistic sense. Due to the ready intuition this example can provide, probabilistic language is sometimes used with other forcing posets.\n\nThe key step in forcing is, given a formula_54 universe formula_1, to find an appropriate object formula_41 not in formula_1. The resulting class of all interpretations of formula_9-names will be a model of formula_54 that properly extends the original formula_1 (since formula_61).\n\nInstead of working with formula_1, it is useful to consider a countable transitive model formula_63 with formula_64. \"Model\" refers to a model of set theory, either of all of formula_54, or a model of a large but finite subset of formula_54, or some variant thereof. \"Transitivity\" means that if formula_67, then formula_68. The Mostowski collapse lemma states that this can be assumed if the membership relation is well-founded. The effect of transitivity is that membership and other elementary notions can be handled intuitively. Countability of the model relies on the Löwenheim–Skolem theorem.\n\nAs formula_63 is a set, there are sets not in formula_63 – this follows from Russell's paradox. The appropriate set formula_41 to pick and adjoin to formula_63 is a generic filter on formula_9. The \"filter\" condition means that:\n\n\nFor formula_41 to be \"generic\" means:\n\n\nThe existence of a generic filter formula_41 follows from the Rasiowa–Sikorski lemma. In fact, slightly more is true: Given a condition formula_10, one can find a generic filter formula_41 such that formula_91. Due to the splitting condition, if formula_41 is a filter, then formula_93 is dense. If formula_94, then formula_95 because formula_63 is a model of formula_54. For this reason, a generic filter is never in formula_63.\n\nGiven a generic filter formula_99, one proceeds as follows. The subclass of formula_9-names in formula_63 is denoted formula_102. Let \n\nTo reduce the study of the set theory of formula_104 to that of formula_63, one works with the \"forcing language\", which is built up like ordinary first-order logic, with membership as the binary relation and all the formula_9-names as constants.\n\nDefine formula_107 (to be read as \"formula_108 forces formula_109 in the model formula_63 with poset formula_9\"), where formula_21 is a condition, formula_109 is a formula in the forcing language, and the formula_114's are formula_9-names, to mean that if formula_41 is a generic filter containing formula_21, then formula_118. The special case formula_119 is often written as \"formula_120\" or simply \"formula_121\". Such statements are true in formula_104, no matter what formula_41 is.\n\nWhat is important is that this external definition of the forcing relation formula_124 is equivalent to an internal definition within formula_63, defined by transfinite induction over the formula_9-names on instances of formula_127 and formula_128, and then by ordinary induction over the complexity of formulae. This has the effect that all the properties of formula_104 are really properties of formula_63, and the verification of formula_54 in formula_104 becomes straightforward. This is usually summarized as the following three key properties:\n\n\nWe define the forcing relation formula_142 in formula_63 by induction on the complexity of formulas, in which we first define the relation for atomic formulas by formula_144-induction and then define it for arbitrary formulas by induction on their complexity.\n\nDefine we forcing relation on atomic formulas. We define forcing of both types of formulas formula_145 and formula_146 simultaneously. This means that we define one relation formula_147 where formula_148 denotes type of formula as follows:\n\n1. formula_149 means formula_150.\n\n2. formula_151 means formula_152.\n\n3. formula_153 means formula_154.\n\nHere formula_108 is condition and formula_156 and formula_157 are formula_158-names. Let formula_147 is formula defined by formula_160-induction:\n\nR1. formula_149 if and only if formula_162.\n\nR2. formula_151 if and only if formula_164.\n\nR3. formula_153 if and only if formula_166.\n\nMore formally, we use following binary relation formula_158-names: Let formula_168 holds for names formula_156 and formula_157 if and only if formula_171 for at least one condition formula_108. This relation is well founded, which means that for any name formula_156 the class of all names formula_157, such that formula_168 holds, is a set and there is no function formula_176 such that formula_177.\n\nIn general a well founded relation is not a preorder, because it might not be transitive. But, if we consider it as an \"ordering\", it is a relation without infinite decreasing sequences and where for any element the class of elements below it is a set.\n\nIt is easy to close any binary relation for transitivity. For names formula_156 and formula_157, formula_180 (as a map with domain formula_181) for some formula_182 such that formula_183, formula_184 and for any formula_185, formula_186 holds. Such an ordering is well founded too.\n\nWe define the following well defined ordering on pairs of names: formula_187 if one of the following holds:\n\n1. formula_188,\n\n2. formula_189 and formula_190,\n\n3. formula_189 and formula_192 and formula_193.\n\nThe relation formula_147 is defined by recursion on pairs formula_195 of names. For any pair it is defined by the same relation on \"simpler\" pairs. Actually, by the recursion theorem there is a formula formula_147 such that R1, R2 and R3 are theorems because its truth value at some point is defined by its truth values in \"smaller\" points relative to the some well founded relation used as an \"ordering\". Now, we are ready to define forcing relation:\n\n1. formula_197 means formula_198.\n\n2. formula_199 means formula_200.\n\n3. formula_201 means formula_202.\n\n4. formula_203 means formula_204.\n\n5. formula_205 means formula_206.\n\nActually, this is a transformation of an arbitrary formula formula_207 to the formula formula_208 where formula_108 and formula_158 are additional variables. This is the definition of the forcing relation in the universe formula_211 of all sets regardless to any countable transitive model. However, there is a relation between this \"syntactic\" formulation of forcing and the \"semantic\" formulation of forcing over some countable transitive model formula_212.\n\n1. For any formula formula_207 there is a theorem formula_214 of the theory formula_215 (for example conjunction of finite number of axioms) such that for any countable transitive model formula_212 such that formula_217 and any atomless partial order formula_218 and any formula_158-generic filter formula_220 over formula_212\n\nThis is called the property of definability of the forcing relation.\n\nThe discussion above can be summarized by the fundamental consistency result that, given a forcing poset formula_9, we may assume the existence of a generic filter formula_41, not belonging to the universe formula_1, such that formula_226 is again a set-theoretic universe that models formula_54. Furthermore, all truths in formula_226 may be reduced to truths in formula_1 involving the forcing relation.\n\nBoth styles, adjoining formula_41 to either a countable transitive model formula_63 or the whole universe formula_1, are commonly used. Less commonly seen is the approach using the \"internal\" definition of forcing, in which no mention of set or class models is made. This was Cohen's original method, and in one elaboration, it becomes the method of Boolean-valued analysis.\n\nThe simplest nontrivial forcing poset is formula_233, the finite partial functions from formula_234 to formula_235 under \"reverse\" inclusion. That is, a condition formula_21 is essentially two disjoint finite subsets formula_237 and formula_238 of formula_234, to be thought of as the \"yes\" and \"no\" parts of with no information provided on values outside the domain of formula_21. \"formula_22 is stronger than formula_21\" means that formula_243, in other words, the \"yes\" and \"no\" parts of formula_22 are supersets of the \"yes\" and \"no\" parts of formula_21, and in that sense, provide more information.\n\nLet formula_41 be a generic filter for this poset. If formula_21 and formula_22 are both in formula_41, then formula_250 is a condition because formula_41 is a filter. This means that formula_252 is a well-defined partial function from formula_234 to formula_254 because any two conditions in formula_41 agree on their common domain.\n\nIn fact, formula_256 is a total function. Given formula_257, let formula_258. Then formula_259 is dense. (Given any formula_21, if formula_261 is not in formula_21's domain, adjoin a value for formula_261 — the result is in formula_259.) A condition formula_265 has formula_261 in its domain, and since formula_267, we find that formula_268 is defined.\n\nLet formula_269, the set of all \"yes\" members of the generic conditions. It is possible to give a name for formula_270 directly. Let \n\nThen formula_272 Now suppose that formula_273 in formula_1. We claim that formula_275. Let \n\nThen formula_277 is dense. (Given any formula_21, if formula_261 is not in its domain, adjoin a value for formula_261 contrary to the status of \"formula_281\".) Then any formula_282 witnesses formula_275. To summarize, formula_270 is a \"new\" subset of formula_234, necessarily infinite.\n\nReplacing formula_234 with formula_287, that is, consider instead finite partial functions whose inputs are of the form formula_288, with formula_289 and formula_290, and whose outputs are formula_291 or formula_292, one gets formula_293 new subsets of formula_234. They are all distinct, by a density argument: Given formula_295, let \n\nthen each formula_297 is dense, and a generic condition in it proves that the αth new set disagrees somewhere with the formula_298-th new set.\n\nThis is not yet the falsification of the continuum hypothesis. One must prove that no new maps have been introduced which map formula_234 onto formula_300, or formula_300 onto formula_293. For example, if one considers instead formula_303, finite partial functions from formula_234 to formula_300, the first uncountable ordinal, one gets in formula_226 a bijection from formula_234 to formula_300. In other words, formula_300 has \"collapsed\", and in the forcing extension, is a countable ordinal.\n\nThe last step in showing the independence of the continuum hypothesis, then, is to show that Cohen forcing does not collapse cardinals. For this, a sufficient combinatorial property is that all of the antichains of this poset are countable.\n\nAn antichain formula_31 of formula_21 is a subset such that if formula_312, then formula_21 and formula_22 are incompatible (written formula_315), meaning there is no formula_316 in formula_9 such that formula_318 and formula_319. In the example on Borel sets, incompatibility means that formula_320 has zero measure. In the example on finite partial functions, incompatibility means that formula_250 is not a function, in other words, formula_21 and formula_22 assign different values to some domain input.\n\nformula_9 satisfies the countable chain condition (c.c.c.) if and only if every antichain in formula_9 is countable. (The name, which is obviously inappropriate, is a holdover from older terminology. Some mathematicians write \"c.a.c.\" for \"countable antichain condition\".)\n\nIt is easy to see that formula_51 satisfies the c.c.c. because the measures add up to at most formula_292. Also, formula_328 is c.c.c., but the proof is more difficult.\n\nGiven an uncountable subfamily formula_329, shrink formula_330 to an uncountable subfamily formula_331 of sets of size formula_261, for some formula_333. If formula_334 for uncountably many formula_335, shrink this to an uncountable subfamily formula_336 and repeat, getting a finite set formula_337 and an uncountable family formula_338 of incompatible conditions of size formula_339 such that every formula_340 is in formula_341 for at most countable many formula_342. Now, pick an arbitrary formula_342, and pick from formula_338 any formula_22 that is not one of the countably many members that have a domain member in common with formula_21. Then formula_347 and formula_348 are compatible, so formula_330 is not an antichain. In other words, formula_328-antichains are countable.\n\nThe importance of antichains in forcing is that for most purposes, dense sets and maximal antichains are equivalent. A \"maximal\" antichain formula_31 is one that cannot be extended to a larger antichain. This means that every element formula_10 is compatible with some member of formula_31. The existence of a maximal antichain follows from Zorn's Lemma. Given a maximal antichain formula_31, let \n\nThen formula_356 is dense, and formula_87 if and only if formula_358. Conversely, given a dense set formula_356, Zorn's Lemma shows that there exists a maximal antichain formula_360, and then formula_87 if and only if formula_358.\n\nAssume that formula_9 is c.c.c. Given formula_364, with formula_365 a function in formula_226, one can approximate formula_367 inside formula_1 as follows. Let formula_369 be a name for formula_367 (by the definition of formula_226) and let formula_21 be a condition that forces formula_369 to be a function from formula_374 to formula_375. Define a function formula_376, whose domain is formula_374, by \n\nBy the definability of forcing, this definition makes sense within formula_1. By the coherence of forcing, a different formula_380 come from an incompatible formula_21. By c.c.c., formula_382 is countable.\n\nIn summary, formula_367 is unknown in formula_1 as it depends on formula_41, but it is not wildly unknown for a c.c.c.-forcing. One can identify a countable set of guesses for what the value of formula_367 is at any input, independent of formula_41.\n\nThis has the following very important consequence. If in formula_226, formula_389 is a surjection from one infinite ordinal onto another, then there is a surjection formula_390 in formula_1, and consequently, a surjection formula_392 in formula_1. In particular, cardinals cannot collapse. The conclusion is that formula_394 in formula_226.\n\nThe exact value of the continuum in the above Cohen model, and variants like formula_396 for cardinals formula_397 in general, was worked out by Robert M. Solovay, who also worked out how to violate formula_398 (the generalized continuum hypothesis), for regular cardinals only, a finite number of times. For example, in the above Cohen model, if formula_399 holds in formula_1, then formula_401 holds in formula_226.\n\nWilliam B. Easton worked out the proper class version of violating the formula_398 for regular cardinals, basically showing that the known restrictions, (monotonicity, Cantor's Theorem and König's Theorem), were the only formula_54-provable restrictions (see Easton's Theorem).\n\nEaston's work was notable in that it involved forcing with a proper class of conditions. In general, the method of forcing with a proper class of conditions fails to give a model of formula_54. For example, forcing with formula_406, where formula_407 is the proper class of all ordinals, makes the continuum a proper class. On the other hand, forcing with formula_408 introduces a countable enumeration of the ordinals. In both cases, the resulting formula_226 is visibly not a model of formula_54.\n\nAt one time, it was thought that more sophisticated forcing would also allow an arbitrary variation in the powers of singular cardinals. However, this has turned out to be a difficult, subtle and even surprising problem, with several more restrictions provable in formula_54 and with the forcing models depending on the consistency of various large-cardinal properties. Many open problems remain.\n\nRandom forcing can be defined as forcing over set formula_412 of all compact subsets of formula_413 of positive measure ordered by relation formula_414 (smaller set in context of inclusion is smaller set in ordering and represents condition with more information). There are two types of important dense sets:\n\n1. For any positive integer formula_415 the set \n\nis dense, where formula_417 is diameter of the set formula_108.\n\n2. For any Borel subset formula_419 of measure 1, the set \n\nis dense.\n\nFor any filter formula_220 and for any finitely many elements formula_422 there is formula_423 such that holds formula_424. In case of this ordering, this means that any filter is set of compact sets with finite intersection property. For this reason, intersection of all elements of any filter is nonempty. Let formula_220 is filter intersecting dense set formula_426 for any positive integer formula_415, then filter formula_220 contains conditions of arbitrary small positive diameter. Therefore, intersection of all conditions from formula_220 has diameter 0. Only nonempty sets of diameter 0 are singletons. Finally, there is exactly one real number formula_430 such that formula_431.\n\nLet formula_432 is any Borel set of measure 1. If formula_220 intersects formula_434, then formula_435.\n\nHowever, generic filter over countable transitive model formula_211 is not in formula_211. Real formula_430 is defined by formula_220. One can also prove that it is not in formula_211. The problem is that if formula_441, then formula_442\"formula_108 is compact\", but from the viewpoint of universe formula_108 can be non-compact and the intersection of all conditions from generic filter is actually empty. For this reason, we consider set formula_445 of closures (in topological sense) of conditions from generic filter. Due to formula_446 and finite intersection property of formula_220, the set formula_448 also has finite intersection property. Elements of the set formula_448 are bounded closed sets as closures of bounded sets. Therefore, formula_448\nis set of compacts with finite intersection property and for this reason has nonempty intersection. Due to formula_451 and model formula_211 inherits metric from universe, the set formula_448 has elements of arbitrary small diameter. Finally, there is exactly one real which belongs to all members of the set formula_448. The generic filter formula_220 can be reconstructed from formula_430 as formula_457.\n\nIf formula_156 is name of formula_430, and for formula_460 holds formula_442\"formula_462 is Borel set of measure 1\", then holds \n\nfor some formula_464. There is name formula_156 such that for any generic filter formula_220 holds \n\nThen \n\nholds for any condition formula_108.\n\nEvery Borel set can, non-uniquely, be built up, starting from intervals with rational endpoints and applying the operations of complement and countable unions, a countable number of times. The record of such a construction is called a \"Borel code\". Given a Borel set formula_462 in formula_211, one recovers a Borel code, and then applies the same construction sequence in formula_472, getting a Borel set formula_473. It can be proven that one gets the same set independent of the construction of formula_474, and that basic properties are preserved. For example, if formula_475, then formula_476. If formula_462 has measure zero, then formula_473 has measure zero. This mapping formula_479 is injective.\n\nFor any set formula_432 such that formula_460 and formula_442\"formula_462 is Borel set of measure 1\" holds formula_484.\n\nThis means that formula_430 is \"infinite random sequence of 0s and 1s\" from the viewpoint of formula_211 which means that it satisfies all statistical tests from the ground model formula_211.\n\nSo given formula_430, a random real, one can show that \n\nBecause of the mutual inter-definability between formula_430 and formula_41, one generally writes formula_492 for formula_472.\n\nA different interpretation of reals in formula_472 was provided by Dana Scott. Rational numbers in formula_226 have names that correspond to countably-many distinct rational values assigned to a maximal antichain of Borel sets – in other words, a certain rational-valued function on formula_496. Real numbers in formula_472 then correspond to Dedekind cuts of such functions, that is, measurable functions.\n\nPerhaps more clearly, the method can be explained in terms of Boolean-valued models. In these, any statement is assigned a truth value from some complete atomless Boolean algebra, rather than just a true/false value. Then an ultrafilter is picked in this Boolean algebra, which assigns values true/false to statements of our theory. The point is that the resulting theory has a model which contains this ultrafilter, which can be understood as a new model obtained by extending the old one with this ultrafilter. By picking a Boolean-valued model in an appropriate way, we can get a model that has the desired property. In it, only statements which must be true (are \"forced\" to be true) will be true, in a sense (since it has this extension/minimality property).\n\nIn forcing, we usually seek to show that some sentence is consistent with formula_54 (or optionally some extension of formula_54). One way to interpret the argument is to assume that formula_54 is consistent and then prove that formula_54 combined with the new sentence is also consistent.\n\nEach \"condition\" is a finite piece of information – the idea is that only finite pieces are relevant for consistency, since, by the Compactness Theorem, a theory is satisfiable if and only if every finite subset of its axioms is satisfiable. Then we can pick an infinite set of consistent conditions to extend our model. Therefore, assuming the consistency of formula_54, we prove the consistency of formula_54 extended by this infinite set.\n\nBy Gödel's second incompleteness theorem, one cannot prove the consistency of any sufficiently strong formal theory, such as formula_54, using only the axioms of the theory itself, unless the theory is inconsistent. Consequently, mathematicians do not attempt to prove the consistency of formula_54 using only the axioms of formula_54, or to prove that formula_507 is consistent for any hypothesis formula_508 using only formula_507. For this reason, the aim of a consistency proof is to prove the consistency of formula_507 relative to the consistency of formula_54. Such problems are known as problems of relative consistency, one of which proves\n\n(*) formula_512\n\nThe general schema of relative consistency proofs follows. As any proof is finite, it uses only a finite number of axioms:\n\nFor any given proof, formula_54 can verify the validity of this proof. This is provable by induction on the length of the proof.\n\nThen resolve\n\nBy proving the following\n\n(**) formula_517\n\nit can be concluded that\n\nwhich is equivalent to\n\nwhich gives (*). The core of the relative consistency proof is proving (**). A formula_54 proof of formula_521 can be constructed for any given finite subset formula_522 of the formula_54 axioms (by formula_54 instruments of course). (No universal proof of formula_521 of course.)\n\nIn formula_54, it is provable that for any condition formula_21, the set of formulas (evaluated by names) forced by formula_21 is deductively closed. Furthermore, for any formula_54 axiom, formula_54 proves that this axiom is forced by formula_16. Then it suffices to prove that there is at least one condition that forces formula_508.\n\nIn the case of Boolean-valued forcing, the procedure is similar: proving that the Boolean value of formula_508 is not formula_534.\n\nAnother approach uses the Reflection Theorem. For any given finite set of formula_54 axioms, there is a formula_54 proof that this set of axioms has a countable transitive model. For any given finite set formula_522 of formula_54 axioms, there is a finite set formula_539 of formula_54 axioms such that formula_54 proves that if a countable transitive model formula_63 satisfies formula_539, then formula_104 satisfies formula_522. By proving that there is finite set formula_546 of formula_54 axioms such that if a countable transitive model formula_63 satisfies formula_546, then formula_104 satisfies the hypothesis formula_508. Then, for any given finite set formula_522 of formula_54 axioms, formula_54 proves formula_521.\n\nSometimes in (**), a stronger theory formula_556 than formula_54 is used for proving formula_521. Then we have proof of the consistency of formula_507 relative to the consistency of formula_556. Note that formula_561, where formula_562 is formula_563 (the axiom of constructibility).\n\n\n\n"}
{"id": "169358", "url": "https://en.wikipedia.org/wiki?curid=169358", "title": "Foundations of mathematics", "text": "Foundations of mathematics\n\nFoundations of mathematics is the study of the philosophical and logical and/or algorithmic basis of mathematics, or, in a broader sense, the mathematical investigation of what underlies the philosophical theories concerning the nature of mathematics. In this latter sense, the distinction between foundations of mathematics and philosophy of mathematics turns out to be quite vague.\nFoundations of mathematics can be conceived as the study of the basic mathematical concepts (set, function, geometrical figure, number, etc.) and how they form hierarchies of more complex structures and concepts, especially the fundamentally important structures that form the language of mathematics (formulas, theories and their models giving a meaning to formulas, definitions, proofs, algorithms, etc.) also called metamathematical concepts, with an eye to the philosophical aspects and the unity of mathematics. The search for foundations of mathematics is a central question of the philosophy of mathematics; the abstract nature of mathematical objects presents special philosophical challenges.\n\nThe foundations of mathematics as a whole does not aim to contain the foundations of every mathematical topic.\nGenerally, the \"foundations\" of a field of study refers to a more-or-less systematic analysis of its most basic or fundamental concepts, its conceptual unity and its natural ordering or hierarchy of concepts, which may help to connect it with the rest of human knowledge. The development, emergence and clarification of the foundations can come late in the history of a field, and may not be viewed by everyone as its most interesting part.\n\nMathematics always played a special role in scientific thought, serving since ancient times as a model of truth and rigor for rational inquiry, and giving tools or even a foundation for other sciences (especially physics). Mathematics' many developments towards higher abstractions in the 19th century brought new challenges and paradoxes, urging for a deeper and more systematic examination of the nature and criteria of mathematical truth, as well as a unification of the diverse branches of mathematics into a coherent whole.\n\nThe systematic search for the foundations of mathematics started at the end of the 19th century and formed a new mathematical discipline called mathematical logic, with strong links to theoretical computer science.\nIt went through a series of crises with paradoxical results, until the discoveries stabilized during the 20th century as a large and coherent body of mathematical knowledge with several aspects or components (set theory, model theory, proof theory, etc.), whose detailed properties and possible variants are still an active research field.\nIts high level of technical sophistication inspired many philosophers to conjecture that it can serve as a model or pattern for the foundations of other sciences.\n\nWhile the practice of mathematics had previously developed in other civilizations, special interest in its theoretical and foundational aspects was clearly evident in the work of the Ancient Greeks.\n\nEarly Greek philosophers disputed as to which is more basic, arithmetic or geometry.\nZeno of Elea (490 c. 430 BC) produced four paradoxes that seem to show the impossibility of change. The Pythagorean school of mathematics originally insisted that only natural and rational numbers exist. The discovery of the irrationality of , the ratio of the diagonal of a square to its side (around 5th century BC), was a shock to them which they only reluctantly accepted. The discrepancy between rationals and reals was finally resolved by Eudoxus of Cnidus (408–355 BC), a student of Plato, who reduced the comparison of irrational ratios to comparisons of multiples (rational ratios), thus anticipating the definition of real numbers by Richard Dedekind (1831–1916).\n\nIn the \"Posterior Analytics\", Aristotle (384–322 BC) laid down the axiomatic method for organizing a field of knowledge logically by means of primitive concepts, axioms, postulates, definitions, and theorems. Aristotle took a majority of his examples for this from arithmetic and from geometry.\nThis method reached its high point with Euclid's \"Elements\" (300 BC), a treatise on mathematics structured with very high standards of rigor: Euclid justifies each proposition by a demonstration in the form of chains of syllogisms (though they do not always conform strictly to Aristotelian templates).\nAristotle's syllogistic logic, together with the axiomatic method exemplified by Euclid's \"Elements\", are recognized as scientific achievements of ancient Greece.\n\nStarting from the end of the 19th century, a Platonist view of mathematics became common among practicing mathematicians.\n\nThe \"concepts\" or, as Platonists would have it, the \"objects\" of mathematics are abstract and remote from everyday perceptual experience: geometrical figures are conceived as idealities to be distinguished from effective drawings and shapes of objects, and numbers are not confused with the counting of concrete objects. Their existence and nature present special philosophical challenges: How do mathematical objects differ from their concrete representation? Are they located in their representation, or in our minds, or somewhere else? How can we know them?\n\nThe ancient Greek philosophers took such questions very seriously. Indeed, many of their general philosophical discussions were carried on with extensive reference to geometry and arithmetic. Plato (424/423 BC 348/347 BC) insisted that mathematical objects, like other platonic \"Ideas\" (forms or essences), must be perfectly abstract and have a separate, non-material kind of existence, in a world of mathematical objects independent of humans. He believed that the truths about these objects also exist independently of the human mind, but is \"discovered\" by humans. In the \"Meno\" Plato's teacher Socrates asserts that it is possible to come to know this truth by a process akin to memory retrieval.\n\nAbove the gateway to Plato's academy appeared a famous inscription: \"Let no one who is ignorant of geometry enter here\". In this way Plato indicated his high opinion of geometry. He regarded geometry as \"the first essential in the training of philosophers\", because of its abstract character.\n\nThis philosophy of \"Platonist mathematical realism\" is shared by many mathematicians. It can be argued that Platonism somehow comes as a necessary assumption underlying any mathematical work.\n\nIn this view, the laws of nature and the laws of mathematics have a similar status, and the effectiveness ceases to be unreasonable. Not our axioms, but the very real world of mathematical objects forms the foundation.\n\nAristotle dissected and rejected this view in his Metaphysics. These questions provide much fuel for philosophical analysis and debate.\n\nFor over 2,000 years, Euclid's Elements stood as a perfectly solid foundation for mathematics, as its methodology of rational exploration guided mathematicians, philosophers, and scientists well into the 19th century.\n\nThe Middle Ages saw a dispute over the ontological status of the universals (platonic Ideas): Realism asserted their existence independently of perception; conceptualism asserted their existence within the mind only; nominalism denied either, only seeing universals as names of collections of individual objects (following older speculations that they are words, \"logoi\").\n\nRené Descartes published \"La Géométrie\" (1637), aimed at reducing geometry to algebra by means of coordinate systems, giving algebra a more foundational role (while the Greeks embedded arithmetic into geometry by identifying whole numbers with evenly spaced points on a line). Descartes' book became famous after 1649 and paved the way to infinitesimal calculus.\n\nIsaac Newton (1642–1727) in England and Leibniz (1646–1716) in Germany independently developed the infinitesimal calculus based on heuristic methods greatly efficient, but direly lacking rigorous justifications. Leibniz even went on to explicitly describe infinitesimals as actual infinitely small numbers (close to zero). Leibniz also worked on formal logic but most of his writings on it remained unpublished until 1903.\n\nThe Protestant philosopher George Berkeley (1685–1753), in his campaign against the religious implications of Newtonian mechanics, wrote a pamphlet on the lack of rational justifications of infinitesimal calculus: \"They are neither finite quantities, nor quantities infinitely small, nor yet nothing. May we not call them the ghosts of departed quantities?\"\n\nThen mathematics developed very rapidly and successfully in physical applications, but with little attention to logical foundations.\n\nIn the 19th century, mathematics became increasingly abstract. Concerns about logical gaps and inconsistencies in different fields led to the development of axiomatic systems.\n\nCauchy (1789–1857) started the project of formulating and proving the theorems of infinitesimal calculus in a rigorous manner, rejecting the heuristic principle of the generality of algebra exploited by earlier authors. In his 1821 work \"Cours d'Analyse\" he defines infinitely small quantities in terms of decreasing sequences that converge to 0, which he then used to define continuity. But he did not formalize his notion of convergence.\n\nThe modern (ε, δ)-definition of limit and continuous functions was first developed by Bolzano in 1817, but remained relatively unknown. It gives a rigorous foundation of infinitesimal calculus based on the set of real numbers, arguably resolving the Zeno paradoxes and Berkeley's arguments.\n\nMathematicians such as Karl Weierstrass (1815–1897) discovered pathological functions such as continuous, nowhere-differentiable functions. Previous conceptions of a function as a rule for computation, or a smooth graph, were no longer adequate. Weierstrass began to advocate the arithmetization of analysis, to axiomatize analysis using properties of the natural numbers.\n\nIn 1858, Dedekind proposed a definition of the real numbers as cuts of rational numbers. This reduction of real numbers and continuous functions in terms of rational numbers, and thus of natural numbers, was later integrated by Cantor in his set theory, and axiomatized in terms of second order arithmetic by Hilbert and Bernays.\n\nFor the first time, the limits of mathematics were explored. Niels Henrik Abel (1802–1829), a Norwegian, and Évariste Galois, (1811–1832) a Frenchman, investigated the solutions of various polynomial equations, and proved that there is no general algebraic solution to equations of degree greater than four (Abel–Ruffini theorem). With these concepts, Pierre Wantzel (1837) proved that straightedge and compass alone cannot trisect an arbitrary angle nor double a cube. In 1882, Lindemann building on the work of Hermite showed that a straightedge and compass quadrature of the circle (construction of a square equal in area to a given circle) was also impossible by proving that is a transcendental number. Mathematicians had attempted to solve all of these problems in vain since the time of the ancient Greeks.\n\nAbel and Galois's works opened the way for the developments of group theory (which would later be used to study symmetry in physics and other fields), and abstract algebra. Concepts of vector spaces emerged from the conception of barycentric coordinates by Möbius in 1827, to the modern definition of vector spaces and linear maps by Peano in 1888. Geometry was no more limited to three dimensions.\nThese concepts did not generalize numbers but combined notions of functions and sets which were not yet formalized, breaking away from familiar mathematical objects.\n\nAfter many failed attempts to derive the parallel postulate from other axioms, the study of the still hypothetical hyperbolic geometry by Johann Heinrich Lambert (1728–1777) led him to introduce the hyperbolic functions and compute the area of a hyperbolic triangle (where the sum of angles is less than 180°). Then the Russian mathematician Nikolai Lobachevsky (1792–1856) established in 1826 (and published in 1829) the coherence of this geometry (thus the independence of the parallel postulate), in parallel with the Hungarian mathematician János Bolyai (1802–1860) in 1832, and with Gauss.\nLater in the 19th century, the German mathematician Bernhard Riemann developed Elliptic geometry, another non-Euclidean geometry where no parallel can be found and the sum of angles in a triangle is more than 180°. It was proved consistent by defining point to mean a pair of antipodal points on a fixed sphere and line to mean a great circle on the sphere. At that time, the main method for proving the consistency of a set of axioms was to provide a model for it.\n\nOne of the traps in a deductive system is circular reasoning, a problem that seemed to befall projective geometry until it was resolved by Karl von Staudt. As explained by Russian historians:\n\nThe purely geometric approach of von Staudt was based on the complete quadrilateral to express the relation of projective harmonic conjugates. Then he created a means of expressing the familiar numeric properties with his Algebra of Throws. English language versions of this process of deducing the properties of a field can be found in either the book by Oswald Veblen and John Young, \"Projective Geometry\" (1938), or more recently in John Stillwell's \"Four Pillars of Geometry\" (2005). Stillwell writes on page 120\n\nThe algebra of throws is commonly seen as a feature of cross-ratios since students ordinarily rely upon numbers without worry about their basis. However, cross-ratio calculations use metric features of geometry, features not admitted by purists. For instance, in 1961 Coxeter wrote \"Introduction to Geometry\" without mention of cross-ratio.\n\nAttempts of formal treatment of mathematics had started with Leibniz and Lambert (1728–1777), and continued with works by algebraists such as George Peacock (1791–1858).\nSystematic mathematical treatments of logic came with the British mathematician George Boole (1847) who devised an algebra that soon evolved into what is now called Boolean algebra, in which the only numbers were 0 and 1 and logical combinations (conjunction, disjunction, implication and negation) are operations similar to the addition and multiplication of integers. Additionally, De Morgan published his laws in 1847. Logic thus became a branch of mathematics. Boolean algebra is the starting point of mathematical logic and has important applications in computer science.\n\nCharles Sanders Peirce built upon the work of Boole to develop a logical system for relations and quantifiers, which he published in several papers from 1870 to 1885.\n\nThe German mathematician Gottlob Frege (1848–1925) presented an independent development of logic with quantifiers in his Begriffsschrift (formula language) published in 1879, a work generally considered as marking a turning point in the history of logic. He exposed deficiencies in Aristotle's \"Logic\", and pointed out the three expected properties of a mathematical theory:\n\n\nHe then showed in \"Grundgesetze der Arithmetik (Basic Laws of Arithmetic)\" how arithmetic could be formalised in his new logic.\n\nFrege's work was popularized by Bertrand Russell near the turn of the century. But Frege's two-dimensional notation had no success. Popular notations were (x) for universal and (∃x) for existential quantifiers, coming from Giuseppe Peano and William Ernest Johnson until the ∀ symbol was introduced by Gerhard Gentzen in 1935 and became canonical in the 1960s.\n\nFrom 1890 to 1905, Ernst Schröder published \"Vorlesungen über die Algebra der Logik\" in three volumes. This work summarized and extended the work of Boole, De Morgan, and Peirce, and was a comprehensive reference to symbolic logic as it was understood at the end of the 19th century.\n\nThe formalization of arithmetic (the theory of natural numbers) as an axiomatic theory started with Peirce in 1881 and continued with Richard Dedekind and Giuseppe Peano in 1888. This was still a second-order axiomatization (expressing induction in terms of arbitrary subsets, thus with an implicit use of set theory) as concerns for expressing theories in first-order logic were not yet understood. In Dedekind's work, this approach appears as completely characterizing natural numbers and providing recursive definitions of addition and multiplication from the successor function and mathematical induction.\n\nThe foundational crisis of mathematics (in German \"Grundlagenkrise der Mathematik\") was the early 20th century's term for the search for proper foundations of mathematics.\n\nSeveral schools of the philosophy of mathematics ran into difficulties one after the other in the 20th century, as the assumption that mathematics had any foundation that could be consistently stated within mathematics itself was heavily challenged by the discovery of various paradoxes (such as Russell's paradox).\n\nThe name \"paradox\" should not be confused with \"contradiction\". A contradiction in a formal theory is a formal proof of an absurdity inside the theory (such as ), showing that this theory is inconsistent and must be rejected. But a paradox may be either a surprising but true result in a given formal theory, or an informal argument leading to a contradiction, so that a candidate theory, if it is to be formalized, must disallow at least one of its steps; in this case the problem is to find a satisfying theory without contradiction. Both meanings may apply if the formalized version of the argument forms the proof of a surprising truth. For instance, Russell's paradox may be expressed as \"there is no set of all sets\" (except in some marginal axiomatic set theories).\n\nVarious schools of thought opposed each other. The leading school was that of the formalist approach, of which David Hilbert was the foremost proponent, culminating in what is known as Hilbert's program, which thought to ground mathematics on a small basis of a logical system proved sound by metamathematical finitistic means. The main opponent was the intuitionist school, led by L. E. J. Brouwer, which resolutely discarded formalism as a meaningless game with symbols (van Dalen, 2008). The fight was acrimonious. In 1920 Hilbert succeeded in having Brouwer, whom he considered a threat to mathematics, removed from the editorial board of \"Mathematische Annalen\", the leading mathematical journal of the time.\n\nAt the beginning of the 20th century, three schools of philosophy of mathematics opposed each other: Formalism, Intuitionism and Logicism.\n\nIt has been claimed that formalists, such as David Hilbert (1862–1943), hold that mathematics is only a language and a series of games. Indeed, he used the words \"formula game\" in his 1927 response to L. E. J. Brouwer's criticisms:\n\nThus Hilbert is insisting that mathematics is not an \"arbitrary\" game with \"arbitrary\" rules; rather it must agree with how our thinking, and then our speaking and writing, proceeds.\n\nThe foundational philosophy of formalism, as exemplified by David Hilbert, is a response to the paradoxes of set theory, and is based on formal logic. Virtually all mathematical theorems today can be formulated as theorems of set theory. The truth of a mathematical statement, in this view, is represented by the fact that the statement can be derived from the axioms of set theory using the rules of formal logic.\n\nMerely the use of formalism alone does not explain several issues: why we should use the axioms we do and not some others, why we should employ the logical rules we do and not some others, why do \"true\" mathematical statements (e.g., the laws of arithmetic) appear to be true, and so on. Hermann Weyl would ask these very questions of Hilbert:\n\nIn some cases these questions may be sufficiently answered through the study of formal theories, in disciplines such as reverse mathematics and computational complexity theory. As noted by Weyl, formal logical systems also run the risk of inconsistency; in Peano arithmetic, this arguably has already been settled with several proofs of consistency, but there is debate over whether or not they are sufficiently finitary to be meaningful. Gödel's second incompleteness theorem establishes that logical systems of arithmetic can never contain a valid proof of their own consistency. What Hilbert wanted to do was prove a logical system \"S\" was consistent, based on principles \"P\" that only made up a small part of \"S\". But Gödel proved that the principles \"P\" could not even prove \"P\" to be consistent, let alone \"S\".\n\nIntuitionists, such as L. E. J. Brouwer (1882–1966), hold that mathematics is a creation of the human mind. Numbers, like fairy tale characters, are merely mental entities, which would not exist if there were never any human minds to think about them.\n\nThe foundational philosophy of \"intuitionism\" or \"constructivism\", as exemplified in the extreme by Brouwer and Stephen Kleene, requires proofs to be \"constructive\" in nature the existence of an object must be demonstrated rather than inferred from a demonstration of the impossibility of its non-existence. For example, as a consequence of this the form of proof known as reductio ad absurdum is suspect.\n\nSome modern theories in the philosophy of mathematics deny the existence of foundations in the original sense. Some theories tend to focus on mathematical practice, and aim to describe and analyze the actual working of mathematicians as a social group. Others try to create a cognitive science of mathematics, focusing on human cognition as the origin of the reliability of mathematics when applied to the real world. These theories would propose to find foundations only in human thought, not in any objective outside construct. The matter remains controversial.\n\nLogicism is a school of thought, and research programme, in the philosophy of mathematics, based on the thesis that mathematics is an extension of a logic or that some or all mathematics may be derived in a suitable formal system whose axioms and rules of inference are 'logical' in nature . Bertrand Russell and Alfred North Whitehead championed this theory initiated by Gottlob Frege and influenced by Richard Dedekind\n\nMany researchers in axiomatic set theory have subscribed to what is known as set-theoretic Platonism, exemplified by Kurt Gödel.\n\nSeveral set theorists followed this approach and actively searched for axioms that may be considered as true for heuristic reasons and that would decide the continuum hypothesis. Many large cardinal axioms were studied, but the hypothesis always remained independent from them and it is now considered unlikely that CH can be resolved by a new large cardinal axiom. Other types of axioms were considered, but none of them has reached consensus on the continuum hypothesis yet. Recent work by Hamkins proposes a more flexible alternative: a set-theoretic multiverse allowing free passage between set-theoretic universes that satisfy the continuum hypothesis and other universes that do not.\n\nThis argument by Willard Quine and Hilary Putnam says (in Putnam's shorter words),\n\nHowever Putnam was not a Platonist.\n\nFew mathematicians are typically concerned on a daily, working basis over logicism, formalism or any other philosophical position. Instead, their primary concern is that the mathematical enterprise as a whole always remains productive. Typically, they see this as insured by remaining open-minded, practical and busy; as potentially threatened by becoming overly-ideological, fanatically reductionistic or lazy. \n\nSuch a view was has also been expressed by some well-known physicists.\n\nFor example, the Physics Nobel Prize laureate Richard Feynman said\n\nAnd Steven Weinberg:\n\nWeinberg believed that any undecidability in mathematics, such as the continuum hypothesis, could be potentially resolved despite the incompleteness theorem, by finding suitable further axioms to add to set theory.\n\nGödel's completeness theorem establishes an equivalence in first-order logic between the formal provability of a formula and its truth in all possible models. Precisely, for any consistent first-order theory it gives an \"explicit construction\" of a model described by the theory; this model will be countable if the language of the theory is countable. However this \"explicit construction\" is not algorithmic. It is based on an iterative process of completion of the theory, where each step of the iteration consists in adding a formula to the axioms if it keeps the theory consistent; but this consistency question is only semi-decidable (an algorithm is available to find any contradiction but if there is none this consistency fact can remain unprovable).\n\nThis can be seen as a giving a sort of justification to the Platonist view that the objects of our mathematical theories are real. More precisely, it shows that the mere assumption of the existence of the set of natural numbers as a totality (an actual infinity) suffices to imply the existence of a model (a world of objects) of any consistent theory. However several difficulties remain:\n\n\nAnother consequence of the completeness theorem is that it justifies the conception of infinitesimals as actual infinitely small nonzero quantities, based on the existence of non-standard models as equally legitimate to standard ones. This idea was formalized by Abraham Robinson into the theory of nonstandard analysis.\n\n\nStarting in 1935, the Bourbaki group of French mathematicians started publishing a series of books to formalize many areas of mathematics on the new foundation of set theory.\n\nThe intuitionistic school did not attract many adherents, and it was not until Bishop's work in 1967 that constructive mathematics was placed on a sounder footing.\n\nOne may consider that Hilbert's program has been partially completed, so that the crisis is essentially resolved, satisfying ourselves with lower requirements than Hilbert's original ambitions. His ambitions were expressed in a time when nothing was clear: it was not clear whether mathematics could have a rigorous foundation at all.\n\nThere are many possible variants of set theory, which differ in consistency strength, where stronger versions (postulating higher types of infinities) contain formal proofs of the consistency of weaker versions, but none contains a formal proof of its own consistency. Thus the only thing we don't have is a formal proof of consistency of whatever version of set theory we may prefer, such as ZF.\n\nIn practice, most mathematicians either do not work from axiomatic systems, or if they do, do not doubt the consistency of ZFC, generally their preferred axiomatic system. In most of mathematics as it is practiced, the incompleteness and paradoxes of the underlying formal theories never played a role anyway, and in those branches in which they do or whose formalization attempts would run the risk of forming inconsistent theories (such as logic and category theory), they may be treated carefully.\n\nThe development of category theory in the middle of the 20th century showed the usefulness of set theories guaranteeing the existence of larger classes than does ZFC, such as Von Neumann–Bernays–Gödel set theory or Tarski–Grothendieck set theory, albeit that in very many cases the use of large cardinal axioms or Grothendieck Universes is formally eliminable.\n\nOne goal of the Reverse Mathematics program is to identify whether there are areas of 'core mathematics' in which foundational issues may again provoke a crisis.\n\n\n\n"}
{"id": "56075256", "url": "https://en.wikipedia.org/wiki?curid=56075256", "title": "Graph-encoded map", "text": "Graph-encoded map\n\nIn topological graph theory, a graph-encoded map or gem is a method of encoding a cellular embedding of a graph using a different graph with four vertices per edge of the original graph. It is the topological analogue of runcination, a geometric operation on polyhedra. Graph-encoded maps were formulated and named by .\nAlternative and equivalent systems for representing cellular embeddings include signed rotation systems and ribbon graphs.\n\nThe graph-encoded map for an embedded graph formula_1 is another cubic graph formula_2 together with a 3-edge-coloring of formula_2. Each edge formula_4 of formula_1 is expanded into exactly four vertices in formula_2, one for each choice of a side and endpoint of the edge. An edge in formula_2 connects each such vertex to the vertex representing the opposite side and same endpoint of formula_4; these edges are by convention colored red. Another edge in formula_2 connects each vertex to the vertex representing the opposite endpoint and same side of formula_4; these edges are by convention colored blue. An edge in formula_2 of the third color, yellow, connects each vertex to the vertex representing another edge formula_12 that meets formula_4 at the same side and endpoint.\n\nAn alternative description of formula_2 is that it has a vertex for each flag of formula_1 (a mutually incident triple of a vertex, edge, and face). If formula_16 is a flag,\nthen there is exactly one vertex formula_17, edge formula_12, and face formula_19 such that formula_20, formula_21, and formula_22 are also flags. The three colors of edges in formula_2 represent each of these three types of flags that differ by one of their three elements. However, interpreting a graph-encoded map in this way requires more care. When the same face appears on both sides of an edge, as can happen for instance for a planar embedding of a tree, the two sides give rise to different gem vertices. And when the same vertex appears at both endpoints of a self-loop, the two ends of the edge again give rise to different gem vertices. In this way, each triple formula_16 may be associated with up to four different vertices of the gem.\n\nWhenever a cubic graph formula_2 can be 3-edge-colored so that the red-blue cycles of the coloring all have length four, the colored graph can be interpreted as a graph-encoded map, and represents an embedding of another graph formula_1.\nTo recover formula_1 and its embedding, interpret each 2-colored cycle of formula_2 as the face of an embedding of formula_2 onto a surface,\ncontract each red--yellow cycle into a single vertex of formula_1, and replace each pair of parallel blue edges left by the contraction with a single edge of formula_1.\n\nThe dual graph of a graph-encoded map may be obtained from the map by recoloring it so that the red edges of the gem become blue and the blue edges become red.\n"}
{"id": "56951340", "url": "https://en.wikipedia.org/wiki?curid=56951340", "title": "Group functor", "text": "Group functor\n\nIn mathematics, a group functor is a group-valued functor on the category of commutative rings. Although it is typically viewed as a generalization of a group scheme, the notion itself involves no scheme theory. Because of this feature, some authors, notably Waterhouse and Milne (who followed Waterhouse), develop the theory of group schemes based on the notion of group functor instead of scheme theory.\n\nA formal group is usually defined as a particular kind of a group functor.\n\nA scheme may be thought of as a contravariant functor from the category formula_1 of \"S\"-schemes to the category of sets satisfying the gluing axiom; the perspective known as the functor of points. Under this perspective, a group scheme is a contravariant functor from formula_1 to the category of groups that is a Zariski sheaf (i.e., satisfying the gluing axiom for the Zariski topology).\n\nFor example, if Γ is a finite group, then consider the functor that sends Spec(\"R\") to the set of locally constant functions on it. For example, the group scheme\ncan be described as the functor\nIf we take a ring, for example, formula_5, then\n\nIt is useful to consider a group functor that respects a topology (if any) of the underlying category; namely, one that is a sheaf and a group functor that is a sheaf is called a group sheaf. The notion appears in particular in the discussion of a torsor (where a choice of topology is an important matter).\n\nFor example, a \"p\"-divisible group is an example of a fppf group sheaf (a group sheaf with respect to the fppf topology).\n"}
{"id": "12967138", "url": "https://en.wikipedia.org/wiki?curid=12967138", "title": "ISO/IEC 18014", "text": "ISO/IEC 18014\n\nISO/IEC 18014 \"Information technology — Security techniques — Time-stamping services\" is an international standard that specifies time-stamping techniques. It comprises four parts:\n\nIn this first part of ISO/IEC 18014, several things are explained and developed:\n\nKey words: audit, non-repudiation, security, time-stamp\n\nA time-stamping service provides evidence that a data item existed before a certain point in time. Time-stamp services produce time-stamp tokens, which are data structures containing a verifiable cryptographic binding between a data item's representation and a time-value. This part of ISO/IEC 18014 defines time-stamping mechanisms that produce independent tokens, which can be verified one by one.\n\nThis part of ISO/IEC 18014:\n\nISO/IEC 18014-3:2009 describes time-stamping services producing linked tokens, that is, tokens that are cryptographically bound to other tokens produced by these time-stamping services. It describes a general model for time-stamping services of this type and the basic components used to construct a time-stamping service of this type, it defines the data structures and protocols used to interact with a time-stamping service of this type, and it describes specific instances of such time-stamping services.\n"}
{"id": "9525221", "url": "https://en.wikipedia.org/wiki?curid=9525221", "title": "James Arthur (mathematician)", "text": "James Arthur (mathematician)\n\nJames Greig Arthur (born May 18, 1944) is a Canadian mathematician working on harmonic analysis, and former President of the American Mathematical Society. He is currently in the Mathematics Department of the University of Toronto.\n\nBorn in Hamilton, Ontario, Arthur received a B.Sc. from the University of Toronto in 1966, and a M.Sc. from the same institution in 1967. He received his Ph.D. from Yale University in 1970. Arthur taught at Yale from 1970 until 1976. He joined the faculty of Duke University in 1976. He has been a professor at the University of Toronto since 1978. He was four times a visiting scholar at the Institute for Advanced Study between 1976 and 2002.\n\nA student of Robert Langlands, he is known for the Arthur–Selberg trace formula, generalizing the Selberg trace formula from the rank-one case (due to Selberg himself) to general reductive groups, one of the most important tools for research on the Langlands program. He also introduced the Arthur conjectures.\n\nArthur was elected a Fellow of the Royal Society of Canada in 1981 and a Fellow of the Royal Society in 1992. He was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 2003. In 2012 he became a fellow of the American Mathematical Society.\n\n"}
{"id": "32084423", "url": "https://en.wikipedia.org/wiki?curid=32084423", "title": "Jantzen filtration", "text": "Jantzen filtration\n\nIn algebra, a Jantzen filtration is a filtration of a Verma module of a semisimple Lie algebra, or a Weyl module of a reductive algebraic group of positive characteristic. Jantzen filtrations were introduced by .\n\nIf \"M\"(λ) is a Verma module of a semisimple Lie algebra with highest weight λ, then the Janzen filtration is a decreasing filtration\nIt has the following properties:\n\n"}
{"id": "19618101", "url": "https://en.wikipedia.org/wiki?curid=19618101", "title": "K-Poincaré group", "text": "K-Poincaré group\n\nIn physics and mathematics, the κ-Poincaré group, named after Henri Poincaré, is a quantum group, obtained by deformation of the Poincaré group into an Hopf algebra.\nIt is generated by the elements formula_1 and formula_2 with the usual constraint:\n\nwhere formula_4 is the Minkowskian metric:\n\nThe commutation rules reads:\n\nIn the (1 + 1)-dimensional case the commutation rules between formula_1 and formula_2 are particularly simple. The Lorentz generator in this case is:\n\nand the commutation rules reads:\n\n\nThe coproducts are classical, and encode the group composition law:\n\nAlso the antipodes and the counits are classical, and represent the group inversion law and the map to the identity:\n\nThe κ-Poincaré group is the dual Hopf algebra to the K-Poincaré algebra, and can be interpreted as its “finite” version.\n"}
{"id": "1884319", "url": "https://en.wikipedia.org/wiki?curid=1884319", "title": "Lenore Blum", "text": "Lenore Blum\n\nLenore Blum (December 18, 1942, New York) is a former distinguished professor of Computer Science at Carnegie Mellon.\n\nBlum grew up in New York City and Venezuela. Her mother was a science teacher in a New York City school.\n\nAfter high school graduation, she studied architecture at Carnegie Institute of Technology from 1959 to 1961 before transferring to Simmons College in Boston to study mathematics, graduating with a B.S. in 1963.\n\nShe received her Ph.D. in mathematics from the Massachusetts Institute of Technology in 1968. Her dissertation was on Generalized Algebraic Structures and her advisor was Gerald Sacks. She then went to the University of California at Berkeley as a Postdoctoral Fellow and Lecturer in Mathematics.\n\nIn 1973 she joined the faculty of Mills College where in 1974 she founded the Mathematics and Computer Science Department (serving as its Head or co-Head for 13 years). In 1979 she was awarded the first Letts-Villard Chair at Mills.\n\nIn 1983 Blum won a National Science Foundation CAREER award to work with Michael Shub for two years at the CUNY Graduate Center. They worked on secure random number generators and evaluating rational functions. See Blum Blum Shub. In 1987 she spent a year at IBM. In 1989 she published a paper with Michael Shub and Stephen Smale on NP completeness, recursive functions and universal Turing machines. See Blum–Shub–Smale machine. In 1990 she gave an address at the International Congress of Mathematicians on computational complexity theory and real computation.\n\nIn 1992 Blum became the deputy director of the Mathematical Sciences Research Institute, otherwise known as MSRI. After visiting the City University of Hong Kong for a year, she became a Computer Science professor at Carnegie Mellon in 1999. In 2002 she was selected to be a Noether Lecturer. In 2012 she became a fellow of the American Mathematical Society. In 2017 she was selected as a fellow of the Association for Women in Mathematics in the inaugural class.\n\nShe resigned from CMU in 2018 due to the change in management structure of Project Olympus. She found that her role was noticeably marginalized, and sexism became blatant.\n\nLenore Blum is married to Manuel Blum and is the mother of Avrim Blum. All three are MIT alumni and professors of Computer Science at Carnegie Mellon. Lenore has a sister, Harriet Epstein.\n\n\n"}
{"id": "26736091", "url": "https://en.wikipedia.org/wiki?curid=26736091", "title": "List of prime knots", "text": "List of prime knots\n\nIn knot theory, prime knots are those knots that are indecomposable under the operation of knot sum. The prime knots with ten or fewer crossings are listed here for quick comparison of their properties and varied naming schemes.\n\n\n\n"}
{"id": "37338762", "url": "https://en.wikipedia.org/wiki?curid=37338762", "title": "Mathematical physiology", "text": "Mathematical physiology\n\nMathematical physiology is an interdisciplinary science. Primarily, it investigates ways in which mathematics may be used to give insight into physiological questions. In turn, it also describes how physiological questions can lead to new mathematical problems. The field may be broadly grouped into two physiological application areas: cell physiology – including mathematical treatments of biochemical reactions, ionic flow and regulation of function – and systems physiology – including electrocardiology, circulation and digestion.\n\n"}
{"id": "24318162", "url": "https://en.wikipedia.org/wiki?curid=24318162", "title": "Mnëv's universality theorem", "text": "Mnëv's universality theorem\n\nIn algebraic geometry, Mnëv's universality theorem is a result which can be used to represent algebraic (or semi algebraic) varieties as realizations of oriented matroids, a notion of combinatorics.\n\nFor the purposes of Mnëv's universality, an oriented matroid of a finite subset formula_1 is a list of all partitions of points in \"S\" induced by hyperplanes in formula_2. In particular, the structure of oriented matroid contains full information on the incidence relations in \"S\", inducing on \"S\" a matroid structure.\n\nThe realization space of an oriented matroid is the space of all configurations of points formula_1 inducing the same oriented matroid structure on \"S\".\n\nFor the purposes of Mnëv's Universality, the stable equivalence of semialgebraic sets is defined as follows.\n\nLet \"U\", \"V\" be semialgebraic sets, obtained as a disconnected union of connected semialgebraic sets\n\nWe say that \"U\" and \"V\" are \"rationally equivalent\" if there exist homeomorphisms formula_6 defined by rational maps.\n\nLet formula_7 be semialgebraic sets,\n\nwith formula_10 mapping to formula_11 under the natural projection formula_12 deleting last \"d\" coordinates. We say that formula_13 is a \"stable projection\" if there exist integer polynomial maps\n\nsuch that\n\nThe \"stable equivalence\" is an equivalence relation on semialgebraic subsets generated by stable projections and rational equivalence.\n\nTHEOREM (\"Mnëv's universality theorem\")\n\nLet \"V\" be a semialgebraic subset in formula_2 defined over integers. Then \"V\" is stably equivalent to a realization space of a certain oriented matroid.\n\nMnëv's universality theorem was discovered by Nikolai Mnëv in his 1986 Ph.D. thesis. It has numerous applications in algebraic geometry, due to Laurent Lafforgue, Ravi Vakil and others, allowing one to construct moduli spaces with arbitrarily bad behaviour.\n\n\n"}
{"id": "51430790", "url": "https://en.wikipedia.org/wiki?curid=51430790", "title": "Möbius–Kantor polygon", "text": "Möbius–Kantor polygon\n\nIn geometry, the Möbius–Kantor polygon is a regular complex polygon {3}, , in formula_1. {3} has 8 vertices, and 8 edges. It is self-dual. Every vertex is shared by 3 triangular edges. Coxeter named it a \"Möbius–Kantor polygon\" for sharing the complex configuration structure as the Möbius–Kantor configuration, (8).\n\nDiscovered by G.C. Shephard in 1952, he represented it as 3(24)3, with its symmetry, Coxeter called as [3], isomorphic to the binary tetrahedral group, order 24.\n\nThe 8 vertex coordinates of this polygon can be given in formula_2, as: \n\nwhere formula_3.\n\nThe configuation matrix for {3} is: formula_4\n\nIt has a real representation as the 16-cell, , in 4-dimensional space, sharing the same 8 vertices. The 24 edges in the 16-cell are seen in the Möbius–Kantor polygon when the 8 triangular edges are drawn as 3-separate edges. The triangles are represented 2 sets of 4 red or blue outlines. The B projections are given in two different symmetry orientations between the two color sets.\nIt can also be seen as an alternation of , represented as . has 16 vertices, and 24 edges. A compound of two, in dual positions, and , can be represented as , contains all 16 vertices of .\n\nThe truncation , is the same as the regular polygon, {6}, . Its edge-diagram is the cayley diagram for [3].\n\nThe regular Hessian polyhedron {3}{3}, has this polygon as a facet and vertex figure.\n\n"}
{"id": "372090", "url": "https://en.wikipedia.org/wiki?curid=372090", "title": "Open and closed maps", "text": "Open and closed maps\n\nIn topology, an open map is a function between two topological spaces which maps open sets to open sets. That is, a function formula_1 is open if for any open set \"U\" in \"X\", the image formula_2 is open in \"Y\". Likewise, a closed map is a function which maps closed sets to closed sets. A map may be open, closed, both, or neither; in particular, an open map need not be closed and vice versa.\n\nOpen and closed maps are not necessarily continuous. Further, continuity is independent of openness and closedness in the general case and a continuous function may have one, both, or neither property; this fact remains true even if one restricts oneself to metric spaces. Although their definitions seem more natural, open and closed maps are much less important than continuous maps. Recall that, by definition, a function formula_1 is continuous if the preimage of every open set of \"Y\" is open in \"X\". (Equivalently, if the preimage of every closed set of \"Y\" is closed in \"X\").\n\nEarly study of open maps was pioneered by Simion Stoilow and Gordon Thomas Whyburn.\n\nEvery homeomorphism is open, closed, and continuous. In fact, a bijective continuous map is a homeomorphism if and only if it is open, or equivalently, if and only if it is closed.\n\nIf \"Y\" has the discrete topology (i.e. all subsets are open and closed) then every function formula_1 is both open and closed (but not necessarily continuous). For example, the floor function from R to Z is open and closed, but not continuous. This example shows that the image of a connected space under an open or closed map need not be connected.\n\nWhenever we have a product of topological spaces formula_5, the natural projections formula_6 are open (as well as continuous). Since the projections of fiber bundles and covering maps are locally natural projections of products, these are also open maps. Projections need not be closed however. Consider for instance the projection formula_7 on the first component; then the set formula_8 is closed in formula_9, but formula_10 is not closed in formula_11. However, for a compact space \"Y\", the projection formula_12 is closed. This is essentially the tube lemma.\n\nTo every point on the unit circle we can associate the angle of the positive \"x\"-axis with the ray connecting the point with the origin. This function from the unit circle to the half-open interval <nowiki>[0,2π)</nowiki> is bijective, open, and closed, but not continuous. It shows that the image of a compact space under an open or closed map need not be compact. Also note that if we consider this as a function from the unit circle to the real numbers, then it is neither open nor closed. Specifying the codomain is essential.\n\nThe function \"f\" : R → R with \"f\"(\"x\") = \"x\" is continuous and closed, but not open.\n\nA function \"f\" : \"X\" → \"Y\" is open if and only if for every \"x\" in \"X\" and every neighborhood \"U\" of \"x\" (however small), there exists a neighborhood \"V\" of \"f\"(\"x\") such that \"V\" ⊂ \"f\"(\"U\").\n\nIt suffices to check openness on a basis for \"X\". That is, a function \"f\" : \"X\" → \"Y\" is open if and only if it maps basic open sets to open sets.\n\nOpen and closed maps can also be characterized by the interior and closure operators. Let \"f\" : \"X\" → \"Y\" be a function. Then\n\nThe composition of two open maps is again open; the composition of two closed maps is again closed.\n\nThe categorical sum of two open maps is open, or of two closed maps is closed.\n\nThe categorical product of two open maps is open, however, the categorical product of two closed maps need not be closed.\n\nA bijective map is open if and only if it is closed. The inverse of a bijective continuous map is a bijective open/closed map (and vice versa).\n\nA surjective open map is not necessarily a closed map, and likewise, a surjective closed map is not necessarily an open map. \n\nLet \"f\" : \"X\" → \"Y\" be a \"continuous\" map which is either open or closed. Then\nIn the first two cases, being open or closed is merely a sufficient condition for the result to follow. In the third case, it is necessary as well.\n\nIt is useful to have conditions for determining when a map is open or closed. The following are some results along these lines.\n\nThe closed map lemma states that every continuous function \"f\" : \"X\" → \"Y\" from a compact space \"X\" to a Hausdorff space \"Y\" is closed and proper (i.e. preimages of compact sets are compact). A variant of this result states that if a continuous function between locally compact Hausdorff spaces is proper, then it is also closed.\n\nIn functional analysis, the open mapping theorem states that every surjective continuous linear operator between Banach spaces is an open map.\n\nIn complex analysis, the identically named open mapping theorem states that every non-constant holomorphic function defined on a connected open subset of the complex plane is an open map.\n\nThe invariance of domain theorem states that a continuous and locally injective function between two \"n\"-dimensional topological manifolds must be open.\n\n"}
{"id": "46951059", "url": "https://en.wikipedia.org/wiki?curid=46951059", "title": "PCDitch", "text": "PCDitch\n\nPCDitch is a dynamic aquatic ecosystem model used to study eutrophication effects in ditches. PCDitch models the nutrient fluxes in the water, the sediment and the vegetation, as well as the competition between different groups of vegetation. PCDitch is used both by scientists and water quality managers.\n\nAs a result of intensive agriculture in catchment areas, many polder ditches have turned from a clear water state with submerged plant dominance into a state where the water is completely covered with duckweed. Dominance of free-floating plants poses a threat to biodiversity by causing dark an anoxic conditions in the water column, and is generally associated with poor ecological quality. \n\nPCDitch predicts the existence of a 'critical nutrient load' above which one may expect duckweed coverage.\nIt can be used by water resource managers to estimate the critical nutrient loading for ditch systems, and to evaluate the effectiveness of restoration measures such as nutrient loading reductions. A meta-model has also been developed for use by water managers to derive an estimate of the critical nutrient loading based on only a few key parameters, without any need to run the full dynamic model. PCDitch is also used by scientists to investigate the more general effects of eutrophication in ditch ecosystems, or to study the competition between different aquatic plant species.\n\nIn essence PCDitch is a set of coupled ordinary differential equations. With more than 40 state variables and more than 400 parameters the model is considered fairly complex. The model describes a completely mixed water body comprising the water column and the upper sediment layer. The overall nutrient cycles for nitrogen and phosphorus are described as completely closed (except for in- and outflow, denitrification and burial). Inputs to the model are: water inflow, evaporation, nutrient loading, light intensity, water temperature, sediment characteristics and depth of the ditch. Six functional groups of water plants are modelled: floating-leaved plants, emerged plants, non-rooted floating plants, non-rooted submerged flowering plants, rooted submerged flowering plants, Charophytes, and one phytoplankton group.\nThe default configuration of PCDitch does not take spatial heterogeneity into account. However, PCDitch can be coupled with spatial explicit hydrodynamical models to model networks of ditches.\nAlthough PCDitch is primarily used for Dutch ditches, the model can be applied to other non-stratifying freshwater ecosystems where competition between primary producers is a key determinant of ecosystem functioning, if parameters are adjusted or some small changes to the model are made.\nPCDitch was calibrated with data of experimental ditches with sand and clay sediments that were exposed to different nutrient loading treatments. As phosphorus in PCDitch is related with aluminium, iron, lutum content (sediment particles smaller than 2 µm), porosity and organic matter content in de sediment, it is assumed that the model can also be used to describe peat ditches.\n\nPCDitch is the twin-model of PCLake, which is an ecosystem model for shallow lakes. Both models were developed by Dr. Jan H. Janse and colleagues at the Netherlands Environmental Assessment Agency (PBL), formerly part of the Netherlands National Institute for Public Health and the Environment (RIVM). Since 2009, the model has been jointly owned by PBL and Wageningen University & Research Centre, where further development and application of the model is taking place.\n\n"}
{"id": "3921324", "url": "https://en.wikipedia.org/wiki?curid=3921324", "title": "Q-derivative", "text": "Q-derivative\n\nIn mathematics, in the area of combinatorics, the \"q\"-derivative, or Jackson derivative, is a \"q\"-analog of the ordinary derivative, introduced by Frank Hilton Jackson. It is the inverse of Jackson's \"q\"-integration. For other forms of q-derivative, see ().\n\nThe \"q\"-derivative of a function \"f\"(\"x\") is defined as\n\nIt is also often written as formula_2. The \"q\"-derivative is also known as the Jackson derivative.\n\nFormally, in terms of Lagrange's shift operator in logarithmic variables, it amounts to the operator\nwhich goes to the plain derivative formula_4 as formula_5.\n\nIt is manifestly linear,\n\nIt has product rule analogous to the ordinary derivative product rule, with two equivalent forms\n\nSimilarly, it satisfies a quotient rule,\n\nThere is also a rule similar to the chain rule for ordinary derivatives. Let formula_9. Then\n\nThe eigenfunction of the \"q\"-derivative is the \"q\"-exponential \"e\"(\"x\").\n\n\"Q\"-differentiation resembles ordinary differentiation, with curious differences. For example, the \"q\"-derivative of the monomial is:\n\nwhere formula_12 is the \"q\"-bracket of \"n\". Note that formula_13 so the ordinary derivative is regained in this limit.\n\nThe \"n\"-th \"q\"-derivative of a function may be given as:\n\nprovided that the ordinary \"n\"-th derivative of \"f\" exists at \"x\" = 0. Here, formula_15 is the \"q\"-Pochhammer symbol, and formula_16 is the \"q\"-factorial. If formula_17 is analytic we can apply the Taylor formula to the definition of formula_18 to get\n\nA \"q\"-analog of the Taylor expansion of a function about zero follows:\n\n\n\n"}
{"id": "1088331", "url": "https://en.wikipedia.org/wiki?curid=1088331", "title": "Radical of an algebraic group", "text": "Radical of an algebraic group\n\nThe radical of an algebraic group is the identity component of its maximal normal solvable subgroup.\nFor example, the radical of the general linear group formula_1 (for a field \"K\") is the subgroup consisting of scalar matrices, i.e. matrices formula_2 with formula_3 and formula_4 for formula_5.\n\nAn algebraic group is called semisimple if its radical is trivial, i.e., consists of the identity element only. The group formula_6 is semi-simple, for example.\n\nThe subgroup of unipotent elements in the radical is called the unipotent radical, it serves to define reductive groups.\n\n"}
{"id": "40355051", "url": "https://en.wikipedia.org/wiki?curid=40355051", "title": "Ribbon (mathematics)", "text": "Ribbon (mathematics)\n\nIn mathematics (differential geometry) by a ribbon (or strip) formula_1 is meant a smooth space curve formula_2 given by a three-dimensional vector formula_3, depending continuously on the curve arc-length formula_4 (formula_5), together with a smoothly varying unit vector formula_6 perpendicular to formula_2 at each point (Blaschke 1950).\n\nThe ribbon formula_1 is called \"simple\" and \"closed\" if formula_2 is simple (i.e. without self-intersections) and closed and if formula_10 and all its derivatives agree at formula_11 and formula_12. \nFor any simple closed ribbon the curves formula_13 given parametrically by formula_14 are, for all sufficiently small positive formula_15, simple closed curves disjoint from formula_2.\n\nThe ribbon concept plays an important role in the Cǎlugǎreǎnu-White-Fuller \nformula (Fuller 1971), that states that\n\nwhere formula_18 is the asymptotic (Gauss) linking number (a topological quantity), formula_19 denotes the total writhing number (or simply writhe) and formula_20 is the total twist number (or simply twist).\n\nRibbon theory investigates geometric and topological aspects of a mathematical reference ribbon associated with physical and biological properties, such as those arising in topological fluid dynamics, DNA modeling and in material science.\n\n"}
{"id": "6706108", "url": "https://en.wikipedia.org/wiki?curid=6706108", "title": "Schramm–Loewner evolution", "text": "Schramm–Loewner evolution\n\nIn probability theory, the Schramm–Loewner evolution with parameter \"κ\", also known as stochastic Loewner evolution (SLE), is a family of random planar curves that have been proven to be the scaling limit of a variety of two-dimensional lattice models in statistical mechanics. Given a parameter \"κ\" and a domain in the complex plane \"U\", it gives a family of random curves in \"U\", with \"κ\" controlling how much the curve turns. There are two main variants of SLE, \"chordal SLE\" which gives a family of random curves from two fixed boundary points, and \"radial SLE\", which gives a family of random curves from a fixed boundary point to a fixed interior point. These curves are defined to satisfy conformal invariance and a domain Markov property.\n\nIt was discovered by as a conjectured scaling limit of the planar uniform spanning tree (UST) and the planar loop-erased random walk (LERW) probabilistic processes, and developed by him together with Greg Lawler and Wendelin Werner in a series of joint papers.\n\nBesides UST and LERW, the Schramm–Loewner evolution is conjectured or proven to describe the scaling limit of various stochastic processes in the plane, such as critical percolation, the critical Ising model, the double-dimer model, self-avoiding walks, and other critical statistical mechanics models that exhibit conformal invariance. The SLE curves are the scaling limits of interfaces and other non-self-intersecting random curves in these models. The main idea is that the conformal invariance and a certain Markov property inherent in such stochastic processes together make it possible to encode these planar curves into a one-dimensional Brownian motion running on the boundary of the domain (the driving function in Loewner's differential equation). This way, many important questions about the planar models can be translated into exercises in Itō calculus. Indeed, several mathematically non-rigorous predictions made by physicists using conformal field theory have been proven using this strategy.\n\nIf \"D\" is a simply connected, open complex domain not equal to C, and γ is a simple curve in \"D\" starting on the boundary (a continuous function with γ(0) on the boundary of \"D\" and γ((0, ∞)) a subset of \"D\"), then for each \"t\" ≥ 0, the complement \"D\" of γ([0, \"t\"]) is simply connected and therefore conformally isomorphic to \"D\" by the Riemann mapping theorem. If \"ƒ\" is a suitable normalized isomorphism from \"D\" to \"D\", then it satisfies a differential equation found by in his work on the Bieberbach conjecture.\nSometimes it is more convenient to use the inverse function \"g\" of \"ƒ\", which is a conformal mapping from \"D\" to \"D\".\n\nIn Loewner's equation, \"z\" is in the domain \"D\", \"t\" ≥ 0, and the boundary values at time \"t\"=0 are \"ƒ\"(\"z\") = \"z\" or \"g\"(\"z\") = \"z\". The equation depends on a driving function ζ(\"t\") taking values in the boundary of \"D\". If \"D\" is the unit disk and the curve γ is parameterized by \"capacity\", then Loewner's equation is\n\nWhen \"D\" is the upper half plane the Loewner equation differs from this by changes of variable and is\n\nThe driving function ζ and the curve γ are related by\nwhere \"ƒ\" and \"g\" are extended by continuity.\n\nLet \"D\" be the upper half plane and consider an SLE, so the driving function ζ is a Brownian motion of diffusivity zero. The function ζ is thus identically zero almost surely and\n\nSchramm–Loewner evolution is the random curve γ given by the Loewner equation as in the previous section, for the driving function\nwhere B(\"t\") is Brownian motion on the boundary of \"D\", scaled by some real κ. In other words Schramm–Loewner evolution is a probability measure on planar curves, given as the image of Wiener measure under this map.\n\nIn general the curve γ need not be simple, and the domain \"D\" is not the complement of γ([0,\"t\"]) in \"D\", but is instead the unbounded component of the complement.\n\nThere are two versions of SLE, using two families of curves, each depending on a non-negative real parameter κ:\n\nSLE depends on a choice of Brownian motion on the boundary of the domain, and there are several variations depending on what sort of Brownian motion is used: for example it might start at a fixed point, or start at a uniformly distributed point on the unit circle, or might have a built in drift, and so on. The parameter κ controls the rate of diffusion of the Brownian motion, and the behavior of SLE depends critically on its value.\n\nThe two domains most commonly used in Schramm–Loewner evolution are the upper half plane and the unit circle. Although the Loewner differential equation in these two cases look different, they are equivalent up to changes of variables as the unit circle and the upper half plane are conformally equivalent. However a conformal equivalence between them does not preserve the Brownian motion on their boundaries used to drive Schramm–Loewner evolution.\n\n\nWhen SLE corresponds to some conformal field theory, the parameter \"κ\" is related to the central charge \"c\"\nof the conformal field theory by\nEach value of \"c\" < 1 corresponds to two values of \"κ\", one value \"κ\" between 0 and 4, and a \"dual\" value 16/\"κ\" greater than 4.\n\nThe probability of chordal SLE(\"κ\") \"γ\" being on the left of fixed point formula_14 was computed by \nwhere formula_16 is the Gamma function and formula_17 is the hypergeometric function. This was derived by using the martingale property of formula_18 and Itô's lemma to obtain the following pde for formula_19\nFor \"κ\"=4, the RHS is formula_21, which was used in the construction of the , and for \"κ\"=6, we obtain Cardy's formula, which was used by to prove conformal invariance in percolation.\n\n used SLE to prove the conjecture of that the boundary of planar Brownian motion has fractal dimension 4/3.\n\nCritical percolation on the triangular lattice was proved to be related to SLE with κ=6 by Stanislav Smirnov. Combined with earlier work of Harry Kesten, this led to the determination of many of the critical exponents for percolation. This breakthrough, in turn, allowed further analysis of many aspects of this model.\n\nLoop-erased random walk was shown to converge to SLE with κ=2 by Lawler, Schramm and Werner. This allowed derivation of many quantitative properties of loop-erased random walk (some of which were derived earlier by Richard Kenyon). The related random Peano curve outlining the uniform spanning tree was shown to converge to SLE with κ=8.\n\nRohde and Schramm showed that κ is related to the fractal dimension of a curve by the following relation\n\nComputer programs (Matlab) are presented in this github repository to simulate Schramm Loewner Evolution planar curves.\n\n\n"}
{"id": "29511652", "url": "https://en.wikipedia.org/wiki?curid=29511652", "title": "Semi-infinite", "text": "Semi-infinite\n\nIn mathematics, semi-infinite objects are objects which are infinite or unbounded in some but not all possible ways.\n\nGenerally, a semi-infinite set is bounded in one direction, and unbounded in another. For instance, the natural numbers are semi-infinite considered as a subset of the integers; similarly, the intervals formula_1 and formula_2 and their closed counterparts are semi-infinite subsets of formula_3. Half-spaces are sometimes described as semi-infinite regions.\n\nSemi-infinite regions occur frequently in the study of differential equations. For instance, one might study solutions of the heat equation in an idealised semi-infinite metal bar.\n\nA semi-infinite integral is an improper integral over a semi-infinite interval. More generally, objects indexed or parametrised by semi-infinite sets may be described as semi-infinite.\n\nMost forms of semi-infiniteness are boundedness properties, not cardinality or measure properties: semi-infinite sets are typically infinite in cardinality and measure.\n\nMany optimization problems involve some set of variables and some set of constraints. A problem is called semi-infinite if one (but not both) of these sets is finite. The study of such problems is known as semi-infinite programming.\n"}
{"id": "13972359", "url": "https://en.wikipedia.org/wiki?curid=13972359", "title": "Separating set", "text": "Separating set\n\nIn mathematics a set of functions \"S\" from a set \"D\" to a set \"C\" is called a separating set for \"D\" or said to separate the points of \"D\" if for any two distinct elements \"x\" and \"y\" of \"D\", there exists a function \"f\" in \"S\" so that \"f\"(\"x\") ≠ \"f\"(\"y\").\n\nSeparating sets can be used to formulate a version of the Stone-Weierstrass theorem for real-valued functions on a compact Hausdorff space \"X\", with the topology of uniform convergence. It states that any subalgebra of this space of functions is dense if and only if it separates points. This is the version of the theorem originally proved by Marshall H. Stone.\n\n"}
{"id": "58901856", "url": "https://en.wikipedia.org/wiki?curid=58901856", "title": "Steinmetz curve", "text": "Steinmetz curve\n\nA Steinmetz curve is the curve of intersection of two right circular cylinders of radii formula_1 and formula_2 whose axes intersect perpendicularly. In case of formula_3 the Steimetz curves are the edges of a Steinmetz solid. If the cylinder axes are the x- and y-axes and formula_4, then the Steinmetz curves are given by the parametric equations:\n\nIt is named after mathematician Charles Proteus Steinmetz, along with Steinmetz's equation, Steinmetz solids, and Steinmetz equivalent circuit theory.\n\nIn the case when the two cylinders have equal radii the curve degenerates to two intersecting ellipses.\n\n"}
{"id": "745789", "url": "https://en.wikipedia.org/wiki?curid=745789", "title": "Subadditivity", "text": "Subadditivity\n\nIn mathematics, subadditivity is a property of a function that states, roughly, that evaluating the function for the sum of two elements of the domain always returns something less than or equal to the sum of the function's values at each element. There are numerous examples of subadditive functions in various areas of mathematics, particularly norms and square roots. Additive maps are special cases of subadditive functions.\n\nA subadditive function is a function formula_1, having a domain \"A\" and an ordered codomain \"B\" that are both closed under addition, with the following property:\n\nAn example is the square root function, having the non-negative real numbers as domain and codomain,\nsince formula_3 we have:\n\nA sequence formula_5, is called subadditive if it satisfies the inequality\nfor all \"m\" and \"n\". This is a special case of subadditive function, if a sequence is interpreted as a function on the set of natural numbers.\n\nA useful result pertaining to subadditive sequences is the following lemma due to Michael Fekete.\n\nThe analogue of Fekete's lemma holds for superadditive sequences as well, that is:\nformula_11 (The limit then may be positive infinity: consider the sequence formula_12.)\n\nThere are extensions of Fekete's lemma that do not require the inequality (1) to hold for all \"m\" and \"n\", but only for \"m\" and \"n\" such that formula_13 Moreover, the condition formula_14 may be weakened as follows: formula_15 provided that formula_16 is an increasing function such that the integral formula_17 converges (near the infinity).\n\nThere are also results that allow one to deduce the rate of convergence to the limit whose existence is stated in Fekete's lemma if some kind of both superadditivity and subadditivity is present.\n\nBesides, analogues of Fekete's lemma have been proved for subadditive real maps (with additional assumptions) from finite subsets of an amenable group \nand further, of a cancellative left-amenable semigroup.\n\nIf \"f\" is a subadditive function, and if 0 is in its domain, then \"f\"(0) ≥ 0. To see this, take the inequality at the top. formula_22. Hence formula_23\n\nA concave function formula_24 with formula_25 is also subadditive.\nTo see this, one first observes that formula_26.\nThen looking at the sum of this bound for formula_27 and formula_28, will finally verify that \"f\" is subadditive.\n\nThe negative of a subadditive function is superadditive.\n\nSubadditivity is an essential property of some particular cost functions. It is, generally, a necessary and sufficient condition for the verification of a natural monopoly. It implies that production from only one firm is socially less expensive (in terms of average costs) than production of a fraction of the original quantity by an equal number of firms.\n\nEconomies of scale are represented by subadditive average cost functions.\n\nExcept in the case of complementary goods, the price of goods (as a function of quantity) must be subadditive. Otherwise, if the sum of the cost of two items is cheaper than the cost of the bundle of two of them together, then nobody would ever buy the bundle, effectively causing the price of the bundle to \"become\" the sum of the prices of the two separate items. Thus proving that it is not a sufficient condition for a natural monopoly; since the unit of exchange may not be the actual cost of an item. This situation is familiar to everyone in the political arena where some minority asserts that the loss of some particular freedom at some particular level of government means that many governments are better; whereas the majority assert that there is some other correct unit of cost.\n\nSubadditivity occurs in the thermodynamic properties of non-ideal solutions and mixtures like the excess molar volume and heat of mixing or excess enthalpy.\n\n\n"}
{"id": "18012776", "url": "https://en.wikipedia.org/wiki?curid=18012776", "title": "Symmetric convolution", "text": "Symmetric convolution\n\nIn mathematics, symmetric convolution is a special subset of convolution operations in which the convolution kernel is symmetric across its zero point. Many common convolution-based processes such as Gaussian blur and taking the derivative of a signal in frequency-space are symmetric and this property can be exploited to make these convolutions easier to evaluate.\n\nThe convolution theorem states that a convolution in the real domain can be represented as a pointwise multiplication across the frequency domain of a Fourier transform. Since sine and cosine transforms are related transforms a modified version of the convolution theorem can be applied, in which the concept of circular convolution is replaced with symmetric convolution. Using these transforms to compute discrete symmetric convolutions is non-trivial since discrete sine transforms (DSTs) and discrete cosine transforms (DCTs) can be counter-intuitively incompatible for computing symmetric convolution, i.e. symmetric convolution can only be computed between a fixed set of compatible transforms.\n\nIn order to compute symmetric convolution effectively, one must know which particular frequency domains (which are reachable by transforming real data through DSTs or DCTs) the inputs and outputs to the convolution can be and then tailor the symmetries of the transforms to the required symmetries of the convolution.\n\nThe following table documents which combinations of the domains from the main eight commonly used DST I-IV and DCT I-IV satisfy formula_1 where formula_2 represents the symmetric convolution operator. Convolution is a commutative operator, and so formula_3 and formula_4 are interchangeable.\n\nForward transforms of formula_3, formula_4 and formula_7, through the transforms specified should allow the symmetric convolution to be computed as a pointwise multiplication, with any excess undefined frequency amplitudes set to zero. Possibilities for symmetric convolutions involving DSTs and DCTs V-VIII derived from the discrete Fourier transforms (DFTs) of odd logical order can be determined by adding four to each type in the above tables.\n\nThere are a number of advantages to computing symmetric convolutions in DSTs and DCTs in comparison with the more common circular convolution with the Fourier transform.\n\nMost notably the implicit symmetry of the transforms involved is such that only data unable to be inferred through symmetry is required. For instance using a DCT-II, a symmetric signal need only have the positive half DCT-II transformed, since the frequency domain will implicitly construct the mirrored data comprising the other half. This enables larger convolution kernels to be used with the same cost as smaller kernels circularly convolved on the DFT. Also the boundary conditions implicit in DSTs and DCTs create edge effects that are often more in keeping with neighbouring data than the periodic effects introduced by using the Fourier transform.\n"}
{"id": "555390", "url": "https://en.wikipedia.org/wiki?curid=555390", "title": "Type conversion", "text": "Type conversion\n\nIn computer science, type conversion, type casting, and type coercion are different ways of changing an entity of one data type into another. An example would be the conversion of an integer value into a floating point value or its textual representation as a string, and vice versa. Type conversions can take advantage of certain features of type hierarchies or data representations. Two important aspects of a type conversion is whether it happens \"implicitly\" or \"explicitly\", and whether the underlying data representation is converted from one representation into another, or a given representation is merely \"reinterpreted\" as the representation of another data type. In general, both primitive and compound data types can be converted.\n\nEach programming language has its own rules on how types can be converted. Languages with strong typing typically do little implicit conversion and discourage the reinterpretation of representations, while languages with weak typing perform many implicit conversions between data types. Weak typing language often allow forcing the compiler to arbitrarily interpret a data item as having different representations—this can be a non-obvious programming error, or a technical method to directly deal with underlying hardware.\n\nIn most languages, the word \"coercion\" is used to denote an \"implicit\" conversion, either during compilation or during run time. For example, in an expression mixing integer and floating point numbers (like 5 + 0.1), the compiler will automatically convert integer representation into floating point representation so fractions are not lost. Explicit type conversions are either indicated by writing additional code (e.g. adding type identifiers or calling built-in routines) or by coding conversion routines for the compiler to use when it otherwise would halt with a type mismatch.\n\nIn most ALGOL-like languages, such as Pascal, Modula-2, Ada and Delphi, \"conversion\" and \"casting\" are distinctly different concepts. In these languages, \"conversion\" refers to either implicitly or explicitly changing a value from one data type storage format to another, e.g. a 16-bit integer to a 32-bit integer. The storage needs may change as a result of the conversion, including a possible loss of precision or truncation. The word \"cast\", on the other hand, refers to explicitly changing the \"interpretation\" of the \"bit pattern\" representing a value from one type to another. For example, 32 contiguous bits may be treated as an array of 32 booleans, a 4-byte string, an unsigned 32-bit integer or an IEEE single precision floating point value. Because the stored bits are never changed, the programmer must know low level details such as representation format, byte order, and alignment needs, to meaningfully cast.\n\nIn the C family of languages and ALGOL 68, the word \"cast\" typically refers to an \"explicit\" type conversion (as opposed to an implicit conversion), causing some ambiguity about whether this is a re-interpretation of a bit-pattern or a real data representation conversion. More important is the multitude of ways and rules that apply to what data type (or class) is located by a pointer and how a pointer may be adjusted by the compiler in cases like object (class) inheritance.\n\nImplicit type conversion, also known as \"coercion\", is an automatic type conversion by the compiler. Some programming languages allow compilers to provide coercion; others require it.\n\nIn a mixed-type expression, data of one or more subtypes can be converted to a supertype as needed at runtime so that the program will run correctly. For example, the following is legal C language code:\n\nAlthough , , and belong to different data types, they will be automatically converted to equal data types each time a comparison or assignment is executed. This behavior should be used with caution, as unintended consequences can arise. Data can be lost when converting representations from floating-point to integer, as the fractional components of the floating-point values will be truncated (rounded toward zero). Conversely, precision can be lost when converting representations from integer to floating-point, since a floating-point type may be unable to exactly represent an integer type. For example, might be an IEEE 754 single precision type, which cannot represent the integer 16777217 exactly, while a 32-bit integer type can. This can lead to unintuitive behavior, as demonstrated by the following code:\n\nOn compilers that implement floats as IEEE single precision, and ints as at least 32 bits, this code will give this peculiar print-out:\n\nNote that 1 represents equality in the last line above. This odd behavior is caused by an implicit conversion of to float when it is compared with . The conversion causes loss of precision, which makes the values equal before the comparison.\n\nImportant takeaways:\n\n\nOne special case of implicit type conversion is type promotion, where the compiler automatically expands the binary representation of objects of integer or floating-point types. Promotions are commonly used with types smaller than the native type of the target platform's arithmetic logic unit (ALU), before arithmetic and logical operations, to make such operations possible, or more efficient if the ALU can work with more than one type. C and C++ perform such promotion for objects of boolean, character, wide character, enumeration, and short integer types which are promoted to int, and for objects of type float, which are promoted to double. Unlike some other type conversions, promotions never lose precision or modify the value stored in the object.\n\nIn Java:\nExplicit type conversion is a type conversion which is explicitly defined within a program (instead of being done by a compiler for implicit type conversion). It is defined by the user in the program.\n\nThere are several kinds of explicit conversion.\n\n\nIn object-oriented programming languages, objects can also be downcast : a reference of a base class is cast to one of its derived classes.\n\nIn C#, type conversion can be made in a safe or unsafe (i.e., C-like) manner, the former called \"checked type cast\".\n\nIn C++ the similar effect can be achieved using \"C++-style cast syntax\".\n\nIn Eiffel the notion of type conversion is integrated into the rules of the type system. The Assignment Rule says that an assignment, such as:\n\nis valid if and only if the type of its source expression, codice_1 in this case, is \"compatible with\" the type of its target entity, codice_2 in this case. In this rule, \"compatible with\" means that the type of the source expression either \"conforms to\" or \"converts to\" that of the target. Conformance of types is defined by the familiar rules for polymorphism in object-oriented programming. For example, in the assignment above, the type of codice_1 conforms to the type of codice_2 if the class upon which codice_1 is based is a descendant of that upon which codice_2 is based.\n\nThe actions of type conversion in Eiffel, specifically \"converts to\" and \"converts from\" are defined as:\nA type based on a class CU \"converts to\" a type T based on a class CT (and T \"converts from\" U) if either\n\nEiffel is a fully compliant language for Microsoft .NET Framework. Before development of .NET, Eiffel already had extensive class libraries. Using the .NET type libraries, particularly with commonly used types such as strings, poses a conversion problem. Existing Eiffel software uses the string classes (such as codice_7) from the Eiffel libraries, but Eiffel software written for .NET must use the .NET string class (codice_8) in many cases, for example when calling .NET methods which expect items of the .NET type to be passed as arguments. So, the conversion of these types back and forth needs to be as seamless as possible.\n\nIn the code above, two strings are declared, one of each different type (codice_9 is the Eiffel compliant alias for System.String). Because codice_8 does not conform to codice_7, then the assignment above is valid only if codice_8 converts to codice_7.\n\nThe Eiffel class codice_7 has a conversion procedure codice_15 for objects of type codice_8. Conversion procedures are also always designated as creation procedures (similar to constructors). The following is an excerpt from the codice_7 class:\n\nThe presence of the conversion procedure makes the assignment:\nsemantically equivalent to:\nin which codice_18 is constructed as a new object of type codice_7 with content equivalent to that of codice_20.\n\nTo handle an assignment with original source and target reversed:\nthe class codice_7 also contains a conversion query codice_22 which will produce a codice_8 from an instance of codice_7.\n\nThe assignment:\nthen, becomes equivalent to:\nIn Eiffel, the setup for type conversion is included in the class code, but then appears to happen as automatically as explicit type conversion in client code. The includes not just assignments but other types of attachments as well, such as argument (parameter) substitution.\n\nIn hacking, typecasting is the misuse of type conversion to temporarily change a variable's data type from how it was originally defined. This provides opportunities for hackers since in type conversion after a variable is \"typecast\" to become a different data type, the compiler will treat that hacked variable as the new data type for that specific operation.\n\n\n"}
{"id": "18549880", "url": "https://en.wikipedia.org/wiki?curid=18549880", "title": "Victor Shestakov", "text": "Victor Shestakov\n\nVictor Ivanovich Shestakov (1907–1987) was a Russian/Soviet logician and theoretician of electrical engineering. In 1935 he discovered the possible interpretation of Boolean algebra of logic in electro-mechanical relay circuits. He graduated from Moscow State University (1934) and worked there in the General Physics Department almost until his death.\n\nShestakov proposed a theory of electric switches based on Boolean logic earlier than Claude Shannon (according to certification of Soviet logicians and mathematicians Sofya Yanovskaya, M.G. Gaaze-Rapoport, Roland Dobrushin, Oleg Lupanov, Yu. A. Gastev, Yu. T. Medvedev, and Vladimir Andreevich Uspensky), though Shestakov and Shannon defended Theses the same year (1938) and the first publication of Shestakov's result took place only in 1941 (in Russian).\n\nIn the early 20th century, relay circuits began to be more widely used in automatics, defense of electric and communications systems. Every relay circuit schema for practical use was a distinct invention, because the general principle of simulation of these systems was not known. Shestakov's credit (and independently later Claude Shannon's) is the general theory of logical simulation, inspired by the rapidly increasing complexity of technical demands. Logical simulation requires solid mathematical foundations. Namely these foundations were originally established by Shestakov.\n\nShestakov set forth an algebraic logic model of electrical two-pole switches (later three- and four-pole switches) with series and parallel connections of schematic elements (resistors, capacitors, magnets, inductive coils, etc.). Resistance of these elements could take arbitrary values on the real-number line, and upon the two-element set {0, ∞} this degenerates into the bivalent Boolean algebra of logic.\n\nShestakov may be considered as a forerunner of combinatorial logic and its application (and, hence, Boolean algebra of logic as well) in electric engineering, the 'language' of which is broad enough to simulate non-electrical objects of any conceivable physical nature. He was a pioneer of study of merged continual algebraic logic (parametrical) and topological (structural) models.\n\n\n"}
{"id": "444091", "url": "https://en.wikipedia.org/wiki?curid=444091", "title": "Weierstrass function", "text": "Weierstrass function\n\nIn mathematics, the Weierstrass function is an example of a pathological real-valued function on the real line. The function has the property of being continuous everywhere but differentiable nowhere. It is named after its discoverer Karl Weierstrass.\n\nHistorically, the Weierstrass function is important because it was the first published example (1872) to challenge the notion that every continuous function is differentiable except on a set of isolated points.\n\nIn Weierstrass's original paper, the function was defined as a Fourier series:\n\nwhere formula_2 is a positive odd integer, and\n\nThe minimum value of formula_4 which satisfies these constraints is formula_5. This construction, along with the proof that the function is not differentiable over any interval was first delivered by Weierstrass in a paper presented to the Königliche Akademie der Wissenschaften on 18 July 1872.\n\nDespite never being differentiable, the function is continuous: Since the terms of the infinite series which defines it are bounded by ±\"a\" and this has finite sum for 0 < \"a\" < 1, convergence of the sum of the terms is uniform by the Weierstrass M-test with \"M\" = \"a\". Since each partial sum is continuous, by the uniform limit theorem, it follows that \"f\" is continuous. Additionally, since each partial sum is uniformly continuous, it follows that \"f\" is also uniformly continuous.\n\nNaïvely it might be expected that a continuous function must have a derivative, or that the set of points where it is not differentiable should be \"small\" in some sense. According to Weierstrass in his paper, earlier mathematicians including Gauss had often assumed that this was true. This might be because it is difficult to draw or visualise a continuous function whose set of nondifferentiable points is something other than a countable set of points. Analogous results for better behaved classes of continuous functions do exist, for example the Lipschitz functions, whose set of non-differentiability points must be a Lebesgue null set (Rademacher's theorem). When we try to draw a general continuous function, we usually draw the graph of a function which is Lipschitz or otherwise well-behaved.\n\nThe Weierstrass function could perhaps be described as one of the very first fractals studied, although this term was not used until much later. The function has detail at every level, so zooming in on a piece of the curve does not show it getting progressively closer and closer to a straight line. Rather between any two points no matter how close, the function will not be monotone. The Hausdorff dimension \"D\" of the graph of the classical Weierstrass function is bounded above by 2 + log\"a\", (where \"a\" and \"b\" are the constants in the construction above) and is generally believed to be exactly that value, but this has not been proven rigorously.\nNotice that 1 < \"D\" < 2 if \"ab\" > 1.\n\nThe term Weierstrass function is often used in real analysis to refer to any function with similar properties and construction to Weierstrass's original example. For example, the cosine function can be replaced in the infinite series by a piecewise linear \"zigzag\" function. G. H. Hardy showed that the function of the above construction is nowhere differentiable with the assumptions 0 < \"a\" < 1, \"ab\" ≥ 1.\n\nIt is convenient to write the Weierstrass function equivalently as\n\nfor formula_7. Then \"W\"(\"x\") is Hölder continuous of exponent α, which is to say that there is a constant \"C\" such that\n\nfor all \"x\" and \"y\". Moreover, \"W\" is Hölder continuous of all orders but not Lipschitz continuous.\n\nIt turns out that the Weierstrass function is far from being an isolated example: although it is \"pathological\", it is also \"typical\" of continuous functions:\n\n\n\n"}
{"id": "44969299", "url": "https://en.wikipedia.org/wiki?curid=44969299", "title": "Zero-symmetric graph", "text": "Zero-symmetric graph\n\nIn the mathematical field of graph theory, a zero-symmetric graph is a connected graph in which all vertices are symmetric to each other, each vertex has exactly three incident edges, and these three edges are not symmetric to each other. More precisely, it is a connected vertex-transitive cubic graph whose edges are partitioned into three different orbits by the automorphism group. In these graphs, for every two vertices \"u\" and \"v\", there is exactly one graph automorphism that takes \"u\" into \"v\".\n\nThe name for this class of graphs was coined by R. M. Foster in a 1966 letter to H. S. M. Coxeter.\n\nThe smallest zero-symmetric graph is a nonplanar graph with 18 vertices. Its LCF notation is [5,−5]. \n\nAmong planar graphs, the truncated cuboctahedral and truncated icosidodecahedral graphs are also zero-symmetric.\n\nThese examples are all bipartite graphs. However, there exist larger examples of zero-symmetric graphs that are not bipartite.\n\nEvery finite zero-symmetric graph is a Cayley graph, a property that does not always hold for cubic vertex-transitive graphs more generally and that helps in the solution of combinatorial enumeration tasks concerning zero-symmetric graphs. There are 97687 zero-symmetric graphs on up to 1280 vertices. These graphs form 89% of the cubic Cayley graphs and 88% of all connected vertex-transitive cubic graphs on the same number of vertices.\nAll known finite connected zero-symmetric graphs contain a Hamiltonian cycle, but it is unknown whether every finite connected zero-symmetric graph is necessarily Hamiltonian. This is a special case of the Lovász conjecture that (with five known exceptions, none of which is zero-symmetric) every finite connected vertex-transitive graph and every finite Cayley graph is Hamiltonian.\n\n"}
{"id": "1452979", "url": "https://en.wikipedia.org/wiki?curid=1452979", "title": "Znám's problem", "text": "Znám's problem\n\nIn number theory, Znám's problem asks which sets of \"k\" integers have the property that each integer in the set is a proper divisor of the product of the other integers in the set, plus 1. Znám's problem is named after the Slovak mathematician Štefan Znám, who suggested it in 1972, although other mathematicians had considered similar problems around the same time. One closely related problem drops the assumption of properness of the divisor, and will be called the improper Znám problem hereafter.\n\nOne solution to the improper Znám problem is easily provided for any \"k\": the first \"k\" terms of Sylvester's sequence have the required property. showed that there is at least one solution to the (proper) Znám problem for each \"k\" ≥ 5. Sun's solution is based on a recurrence similar to that for Sylvester's sequence, but with a different set of initial values.\n\nThe Znám problem is closely related to Egyptian fractions. It is known that there are only finitely many solutions for any fixed \"k\". It is unknown whether there are any solutions to Znám's problem using only odd numbers, and there remain several other open questions.\n\nZnám's problem asks which sets of integers have the property that each integer in the set is a proper divisor of the product of the other integers in the set, plus 1. That is, given \"k\", what sets of integers \nare there, such that, for each \"i\", \"n\" divides but is not equal to\n\nA closely related problem concerns sets of integers in which each integer in the set is a divisor, but not necessarily a proper divisor, of one plus the product of the other integers in the set. This problem does not seem to have been named in the literature, and will be referred to as the improper Znám problem. Any solution to Znám's problem is also a solution to the improper Znám problem, but not necessarily vice versa.\n\nZnám's problem is named after the Slovak mathematician Štefan Znám, who suggested it in 1972. had posed the improper Znám problem for \"k\" = 3, and , independently of Znám, found all solutions to the improper problem for \"k\" ≤ 5. showed that Znám's problem is unsolvable for \"k\" < 5, and credited J. Janák with finding the solution {2, 3, 11, 23, 31} for \"k\" = 5.\n\nOne solution to \"k\" = 5 is {2, 3, 7, 47, 395}. A few calculations will show that\n\nAn interesting \"near miss\" for \"k\" = 4 is the set {2, 3, 7, 43}, formed by taking the first four terms of Sylvester's sequence. It has the property that each integer in the set divides the product of the other integers in the set, plus 1, but the last member of this set is equal to the product of the first three members plus one, rather than being a proper divisor. Thus, it is a solution to the improper Znám problem, but not a solution to Znám's problem as it is usually defined.\n\nAny solution to the improper Znám problem is equivalent (via division by the product of the \"x\"'s) to a solution to the equation\nwhere \"y\" as well as each \"x\" must be an integer, and conversely any such solution corresponds to a solution to the improper Znám problem. However, all known solutions have \"y\" = 1, so they satisfy the equation\nThat is, they lead to an Egyptian fraction representation of the number one as a sum of unit fractions. Several of the cited papers on Znám's problem study also the solutions to this equation. describe an application of the equation in topology, to the classification of singularities on surfaces, and describe an application to the theory of nondeterministic finite automata.\n\nAs showed, the number of solutions for any \"k\" is finite, so it makes sense to count the total number of solutions for each \"k\".\n\nBrenton and Vasiliu calculated that the number of solutions for small values of \"k\", starting with \"k\" = 5, forms the sequence\nPresently, a few solutions are known for \"k\" = 9 and \"k\" = 10, but it is unclear how many solutions remain undiscovered for those values of \"k\".\nHowever, there are infinitely many solutions if \"k\" is not fixed:\n\nIt is unknown whether there are any solutions to Znám's problem using only odd numbers. With one exception, all known solutions start with 2. If all numbers in a solution to Znám's problem or the improper Znám problem are prime, their product is a primary pseudoperfect number ; it is unknown whether infinitely many solutions of this type exist.\n\n"}
