{"id": "6317372", "url": "https://en.wikipedia.org/wiki?curid=6317372", "title": "215 (number)", "text": "215 (number)\n\n215 (two hundred [and] fifteen) is the natural number following 214 and preceding 216.\n\n\n\n215 is also:\n"}
{"id": "343340", "url": "https://en.wikipedia.org/wiki?curid=343340", "title": "90 (number)", "text": "90 (number)\n\n90 (ninety) is the natural number preceded by 89 and followed by 91.\n\nIn the English language, the numbers 90 and 19 are often confused, as sounding very similar. When carefully enunciated, they differ in which syllable is stressed: 19 /naɪnˈtiːn/ vs 90 /ˈnaɪnti/. However, in dates such as 1999, and when contrasting numbers in the teens and when counting, such as 17, 18, 19, the stress shifts to the first syllable: 19 /ˈnaɪntiːn/.\n\n90 is:\n\n\nIn normal space, the interior angles of a rectangle measure 90 degrees each. Also, in a right triangle, the angle opposing the hypotenuse measures 90 degrees, with the other two angles adding up to 90 for a total of 180 degrees. Thus, an angle measuring 90 degrees is called a right angle.\n\nNinety is:\n\n\n\n"}
{"id": "53382320", "url": "https://en.wikipedia.org/wiki?curid=53382320", "title": "Andrew Browder", "text": "Andrew Browder\n\nAndrew Browder (born January 8, 1931) is an American mathematician at Brown University.\n\nAndrew Browder was born in Moscow, Russia, where his father Earl Browder, an American communist from Kansas, United States, was living and working for a period. His mother was Raisa, a Russian woman. His brothers were Felix Browder, also born in Moscow, and William Browder. Andrew, Felix and William have careers in mathematics. Their father returned to the United States in the early 1930s, bringing his family with him. The senior Browder became head of the Communist Party USA. He ran for US President in 1936 and 1940.\n\nThe analytic nature of the game of chess enthralled Andrew early on: \"My own main sport was always chess. My father taught me the game when I was six, a friend of the family gave me a chess book when I was eleven or twelve, and after that I was hooked.\"\n\nAndrew traces his interest in mathematics to 1955 when he was a private at Fort Dix and Eisenhower offered early release to servicemen entering graduate school. He studied at Massachusetts Institute of Technology. For two years he was a Miller Fellow at University of California, Berkeley and also studied at Aarhus, Denmark. As for teaching, \"I taught well over one hundred courses, some of which I found interesting and enjoyable, some pretty depressing, most somewhere in between. The students had parallel experiences.\"\n\nBrowder married and has as least one child: Laura Browder, professor of American Studies at the University of Richmond.\n\nIn 1969 Browder published \"Introduction to Function Algebras\". \"The author develops much of the general theory of function algebras and then applies it in the last two chapters to the theory of rational approximation in the plane, and to finding analytic structure in the spectrum of a Dirichlet algebra.\"\n\nIn 1996 he published an upper-level textbook \"Mathematical Analysis\" for well-motivated students having a background of calculus and linear algebra. Four topics are encompassed by the text: single-variable theory, topology and function spaces, measure theory and Lebesgue integration, and functions of several variables.\n\nIn 2000 Browder published his article \"Topology in the Complex Plane\", which described the Brouwer fixed point theorem, the Jordan curve theorem, and Alexander duality.\n\nWith Hiroshi Yamaguchi, Browder wrote \"A variation formula for harmonic modules and its application to several complex variables\".\n\n"}
{"id": "52130823", "url": "https://en.wikipedia.org/wiki?curid=52130823", "title": "Andy Lomas", "text": "Andy Lomas\n\nAndy Lomas (born 1967 in Welwyn Garden City, England) is a British artist with a mathematical background, formerly a television and film CG supervisor and more recently a contemporary digital artist, with a special interest in morphogenesis using mathematical morphology.\n\nLomas previously worked on visual effects using computer graphics (CGI) for television and films such as \"The Matrix Reloaded\" (2003), \"The Matrix Revolutions\" (2003) and \"Avatar\" (2009). before becoming a digital artist. In 2006 he appeared in \"The Tech of 'Over the Hedge\"', a short documentary. With his collaborators, in 1999 Lomas won the 51st Primetime Emmy Awards \"Outstanding Special Visual Effects for a Miniseries or a Movie\" for the 1999 film \"Alice in Wonderland\".\n\nLomas's works are displayed in the form of videos, still images, and sculptures, produced using a mathematical programming approach. Some works include collaborative music, by Max Cooper for example. His artworks are inspired by the work of Ernst Haeckel, D'Arcy Thompson, and Alan Turing.\n\nLomas won the 2014 international Lumen Prize \"Gold Award\" for digital art, the top category. He has exhibited at the Butler Institute of American Art (Youngstown, Ohio, USA), the Computing Commons Art Gallery (Arizona State University), the Los Angeles Center for Digital Art, and SIGGRAPH.\n\nIn June–July 2016, Lomas held a solo exhibition of his work at the Watermans Arts Centre in west London, which has been acquired by the Victoria and Albert Museum for its collection. His work is also held in the D'Arcy Thompson Zoology Museum art collection at the University of Dundee in Scotland, funded by the UK Art Fund.\n\nBy way of summarizing his technique, Lomas counts himself among those who have entered into a \"hybrid\" relationship with the computer, wherein the latter is used to quickly generate a series of visual images based on an original idea or algorithm.\n\n"}
{"id": "3062599", "url": "https://en.wikipedia.org/wiki?curid=3062599", "title": "Anonymous recursion", "text": "Anonymous recursion\n\nIn computer science, anonymous recursion is recursion which does not explicitly call a function by name. This can be done either explicitly, by using a higher-order function – passing in a function as an argument and calling it – or implicitly, via reflection features which allow one to access certain functions depending on the current context, especially \"the current function\" or sometimes \"the calling function of the current function\".\n\nIn programming practice, anonymous recursion is notably used in JavaScript, which provides reflection facilities to support it. In general programming practice, however, this is considered poor style, and recursion with named functions is suggested instead. Anonymous recursion via explicitly passing functions as arguments is possible in any language that supports functions as arguments, though this is rarely used in practice, as it is longer and less clear than explicitly recursing by name.\n\nIn theoretical computer science, anonymous recursion is important, as it shows that one can implement recursion without requiring named functions. This is particularly important for the lambda calculus, which has anonymous unary functions, but is able to compute any recursive function. This anonymous recursion can be produced generically via fixed-point combinators.\n\nAnonymous recursion is primarily of use in allowing recursion for anonymous functions, particularly when they form closures or are used as callbacks, to avoid having to bind the name of the function.\n\nAnonymous recursion primarily consists of calling \"the current function\", which results in direct recursion. Anonymous indirect recursion is possible, such as by calling \"the caller (the previous function)\", or, more rarely, by going further up the call stack, and this can be chained to produce mutual recursion. The self-reference of \"the current function\" is a functional equivalent of the \"this\" keyword in object-oriented programming, allowing one to refer to the current context.\n\nAnonymous recursion can also be used for named functions, rather that calling them by name, say to specify that one is recursing on the current function, or to allow one to rename the function without needing to change the name where it calls itself. However, as a matter of programming style this is generally not done.\n\nThe usual alternative is to use named functions and named recursion. Given an anonymous function, this can be done either by binding a name to the function, as in named function expressions in JavaScript, or by assigning the function to a variable and then calling the variable, as in function statements in JavaScript. Since languages that allow anonymous functions generally allow assigning these functions to variables (if not first-class functions), many languages do not provide a way to refer to the function itself, and explicitly reject anonymous recursion; examples include Go.\n\nFor example, in JavaScript the factorial function can be defined via anonymous recursion as such:\n\nRewritten to use a named function expression yields:\nEven without mechanisms to refer to the current function or calling function, anonymous recursion is possible in a language that allows functions as arguments. This is done by adding another parameter to the basic recursive function and using this parameter as the function for the recursive call. This creates a higher-order function, and passing this higher function \"itself\" allows anonymous recursion within the actual recursive function. This can be done purely anonymously by applying a fixed-point combinator to this higher order function. This is mainly of academic interest, particularly to show that the lambda calculus has recursion, as the resulting expression is significantly more complicated than the original named recursive function. Conversely, the use of fixed-pointed combinators may be generically referred to as \"anonymous recursion\", as this is a notable use of them, though they have other applications.\n\nThis is illustrated below using Python. First, a standard named recursion:\n\nUsing a higher-order function so the top-level function recurses anonymously on an argument, but still needing the standard recursive function as an argument:\n\nWe can eliminate the standard recursive function by passing the function argument into the call:\n\nThe second line can be replaced by a generic higher-order function called a \"combinator:\"\n\nWritten anonymously:\nIn the lambda calculus, which only uses functions of a single variable, this can be done via the \"Y combinator.\" First make the higher-order function of two variables be a function of a single variable, which directly returns a function, by currying:\n\nThere are two \"applying a higher order function to itself\" operations here: codice_1 in the first line and codice_2 in the second. Factoring out the second double application into a \"combinator\" yields:\n\nFactoring out the other double application yields:\n\nCombining the two combinators into one yields the Y combinator:\n\nExpanding out the Y combinator yields:\n\nCombining these yields a recursive definition of the factorial in lambda calculus (anonymous functions of a single variable):\n\nIn JavaScript, the current function is accessible via codice_3, while the calling function is accessible via codice_4. These allow anonymous recursion, such as in this implementation of the factorial:\nStarting with Perl 5.16, the current subroutine is accessible via the codice_5 token, which returns a reference to the current subroutine, or codice_6 outside a subroutine. This allows anonymous recursion, such as in the following implementation of the factorial:\nIn R, the current function can be called using codice_7. For example,\n\nIt will not work, however, if passed as an argument to another function , e.g. codice_8, inside the anonymous function definition. In this case, codice_9 can be used. For example, the code below squares a list recursively:\n"}
{"id": "1844", "url": "https://en.wikipedia.org/wiki?curid=1844", "title": "Archimedes", "text": "Archimedes\n\nArchimedes of Syracuse (; ; ) was a Greek mathematician, physicist, engineer, inventor, and astronomer. Although few details of his life are known, he is regarded as one of the leading scientists in classical antiquity. Generally considered the greatest mathematician of antiquity and one of the greatest of all time, Archimedes anticipated modern calculus and analysis by applying concepts of infinitesimals and the method of exhaustion to derive and rigorously prove a range of geometrical theorems, including the area of a circle, the surface area and volume of a sphere, and the area under a parabola.\n\nOther mathematical achievements include deriving an accurate approximation of pi, defining and investigating the spiral bearing his name, and creating a system using exponentiation for expressing very large numbers. He was also one of the first to apply mathematics to physical phenomena, founding hydrostatics and statics, including an explanation of the principle of the lever. He is credited with designing innovative machines, such as his screw pump, compound pulleys, and defensive war machines to protect his native Syracuse from invasion.\n\nArchimedes died during the Siege of Syracuse when he was killed by a Roman soldier despite orders that he should not be harmed. Cicero describes visiting the tomb of Archimedes, which was surmounted by a sphere and a cylinder, which Archimedes had requested be placed on his tomb to represent his mathematical discoveries.\n\nUnlike his inventions, the mathematical writings of Archimedes were little known in antiquity. Mathematicians from Alexandria read and quoted him, but the first comprehensive compilation was not made until by Isidore of Miletus in Byzantine Constantinople, while commentaries on the works of Archimedes written by Eutocius in the sixth century AD opened them to wider readership for the first time. The relatively few copies of Archimedes' written work that survived through the Middle Ages were an influential source of ideas for scientists during the Renaissance, while the discovery in 1906 of previously unknown works by Archimedes in the Archimedes Palimpsest has provided new insights into how he obtained mathematical results.\n\nArchimedes was born c. 287 BC in the seaport city of Syracuse, Sicily, at that time a self-governing colony in Magna Graecia, located along the coast of Southern Italy. The date of birth is based on a statement by the Byzantine Greek historian John Tzetzes that Archimedes lived for 75 years. In \"The Sand Reckoner\", Archimedes gives his father's name as Phidias, an astronomer about whom nothing else is known. Plutarch wrote in his \"Parallel Lives\" that Archimedes was related to King Hiero II, the ruler of Syracuse. A biography of Archimedes was written by his friend Heracleides but this work has been lost, leaving the details of his life obscure. It is unknown, for instance, whether he ever married or had children. During his youth, Archimedes may have studied in Alexandria, Egypt, where Conon of Samos and Eratosthenes of Cyrene were contemporaries. He referred to Conon of Samos as his friend, while two of his works (\"The Method of Mechanical Theorems\" and the \"Cattle Problem\") have introductions addressed to Eratosthenes.\n\nArchimedes died c. 212 BC during the Second Punic War, when Roman forces under General Marcus Claudius Marcellus captured the city of Syracuse after a two-year-long siege. According to the popular account given by Plutarch, Archimedes was contemplating a mathematical diagram when the city was captured. A Roman soldier commanded him to come and meet General Marcellus but he declined, saying that he had to finish working on the problem. The soldier was enraged by this, and killed Archimedes with his sword. Plutarch also gives a account of the death of Archimedes which suggests that he may have been killed while attempting to surrender to a Roman soldier. According to this story, Archimedes was carrying mathematical instruments, and was killed because the soldier thought that they were valuable items. General Marcellus was reportedly angered by the death of Archimedes, as he considered him a valuable scientific asset and had ordered that he must not be harmed. Marcellus called Archimedes \"a geometrical Briareus\".\n\nThe last words attributed to Archimedes are \"Do not disturb my circles\", a reference to the circles in the mathematical drawing that he was supposedly studying when disturbed by the Roman soldier. This quote is often given in Latin as \"\"Noli turbare circulos meos\",\" but there is no reliable evidence that Archimedes uttered these words and they do not appear in the account given by Plutarch. Valerius Maximus, writing in \"Memorable Doings and Sayings\" in the 1st century AD, gives the phrase as \"\"...sed protecto manibus puluere 'noli' inquit, 'obsecro, istum disturbare\"'\" – \"... but protecting the dust with his hands, said 'I beg of you, do not disturb this. The phrase is also given in Katharevousa Greek as \"μὴ μου τοὺς κύκλους τάραττε!\" (\"Mē mou tous kuklous taratte!\").\n\nThe tomb of Archimedes carried a sculpture illustrating his favorite mathematical proof, consisting of a sphere and a cylinder of the same height and diameter. Archimedes had proven that the volume and surface area of the sphere are two thirds that of the cylinder including its bases. In 75 BC, 137 years after his death, the Roman orator Cicero was serving as quaestor in Sicily. He had heard stories about the tomb of Archimedes, but none of the locals were able to give him the location. Eventually he found the tomb near the Agrigentine gate in Syracuse, in a neglected condition and overgrown with bushes. Cicero had the tomb cleaned up, and was able to see the carving and read some of the verses that had been added as an inscription. A tomb discovered in the courtyard of the Hotel Panorama in Syracuse in the early 1960s was claimed to be that of Archimedes, but there was no compelling evidence for this and the location of his tomb today is unknown.\n\nThe standard versions of the life of Archimedes were written long after his death by the historians of Ancient Rome. The account of the siege of Syracuse given by Polybius in his \"The Histories\" was written around seventy years after Archimedes' death, and was used subsequently as a source by Plutarch and Livy. It sheds little light on Archimedes as a person, and focuses on the war machines that he is said to have built in order to defend the city.\n\nThe most widely known anecdote about Archimedes tells of how he invented a method for determining the volume of an object with an irregular shape. According to Vitruvius, a votive crown for a temple had been made for King Hiero II of Syracuse, who had supplied the pure gold to be used, and Archimedes was asked to determine whether some silver had been substituted by the dishonest goldsmith. Archimedes had to solve the problem without damaging the crown, so he could not melt it down into a regularly shaped body in order to calculate its density.\nWhile taking a bath, he noticed that the level of the water in the tub rose as he got in, and realized that this effect could be used to determine the volume of the crown. For practical purposes water is incompressible, so the submerged crown would displace an amount of water equal to its own volume. By dividing the mass of the crown by the volume of water displaced, the density of the crown could be obtained. This density would be lower than that of gold if cheaper and less dense metals had been added. Archimedes then took to the streets naked, so excited by his discovery that he had forgotten to dress, crying \"Eureka!\" (, \"heúrēka\"!\", meaning \"I have found [it]!\"). The test was conducted successfully, proving that silver had indeed been mixed in.\n\nThe story of the golden crown does not appear in the known works of Archimedes. Moreover, the practicality of the method it describes has been called into question, due to the extreme accuracy with which one would have to measure the water displacement. Archimedes may have instead sought a solution that applied the principle known in hydrostatics as Archimedes' principle, which he describes in his treatise \"On Floating Bodies\". This principle states that a body immersed in a fluid experiences a buoyant force equal to the weight of the fluid it displaces. Using this principle, it would have been possible to compare the density of the crown to that of pure gold by balancing the crown on a scale with a pure gold reference sample of the same weight, then immersing the apparatus in water. The difference in density between the two samples would cause the scale to tip accordingly. Galileo considered it \"probable that this method is the same that Archimedes followed, since, besides being very accurate, it is based on demonstrations found by Archimedes himself.\" In a 12th-century text titled \"Mappae clavicula\" there are instructions on how to perform the weighings in the water in order to calculate the percentage of silver used, and thus solve the problem. The Latin poem \"Carmen de ponderibus et mensuris\" of the 4th or 5th century describes the use of a hydrostatic balance to solve the problem of the crown, and attributes the method to Archimedes.\n\nA large part of Archimedes' work in engineering arose from fulfilling the needs of his home city of Syracuse. The Greek writer Athenaeus of Naucratis described how King Hiero II commissioned Archimedes to design a huge ship, the \"Syracusia\", which could be used for luxury travel, carrying supplies, and as a naval warship. The \"Syracusia\" is said to have been the largest ship built in classical antiquity. According to Athenaeus, it was capable of carrying 600 people and included garden decorations, a gymnasium and a temple dedicated to the goddess Aphrodite among its facilities. Since a ship of this size would leak a considerable amount of water through the hull, the Archimedes' screw was purportedly developed in order to remove the bilge water. Archimedes' machine was a device with a revolving screw-shaped blade inside a cylinder. It was turned by hand, and could also be used to transfer water from a body of water into irrigation canals. The Archimedes' screw is still in use today for pumping liquids and granulated solids such as coal and grain. The Archimedes' screw described in Roman times by Vitruvius may have been an improvement on a screw pump that was used to irrigate the Hanging Gardens of Babylon. The world's first seagoing steamship with a screw propeller was the SS \"Archimedes\", which was launched in 1839 and named in honor of Archimedes and his work on the screw.\n\nThe Claw of Archimedes is a weapon that he is said to have designed in order to defend the city of Syracuse. Also known as \"the ship shaker,\" the claw consisted of a crane-like arm from which a large metal grappling hook was suspended. When the claw was dropped onto an attacking ship the arm would swing upwards, lifting the ship out of the water and possibly sinking it. There have been modern experiments to test the feasibility of the claw, and in 2005 a television documentary entitled \"Superweapons of the Ancient World\" built a version of the claw and concluded that it was a workable device.\n\nArchimedes may have used mirrors acting collectively as a parabolic reflector to burn ships attacking Syracuse.\nThe 2nd century AD author Lucian wrote that during the Siege of Syracuse (c. 214–212 BC), Archimedes destroyed enemy ships with fire. Centuries later, Anthemius of Tralles mentions burning-glasses as Archimedes' weapon. The device, sometimes called the \"Archimedes heat ray\", was used to focus sunlight onto approaching ships, causing them to catch fire. In the modern era, similar devices have been constructed and may be referred to as a heliostat or solar furnace.\n\nThis purported weapon has been the subject of ongoing debate about its credibility since the Renaissance. René Descartes rejected it as false, while modern researchers have attempted to recreate the effect using only the means that would have been available to Archimedes. It has been suggested that a large array of highly polished bronze or copper shields acting as mirrors could have been employed to focus sunlight onto a ship.\n\nA test of the Archimedes heat ray was carried out in 1973 by the Greek scientist Ioannis Sakkas. The experiment took place at the Skaramagas naval base outside Athens. On this occasion 70 mirrors were used, each with a copper coating and a size of around five by three feet (1.5 by 1 m). The mirrors were pointed at a plywood of a Roman warship at a distance of around 160 feet (50 m). When the mirrors were focused accurately, the ship burst into flames within a few seconds. The plywood ship had a coating of tar paint, which may have aided combustion. A coating of tar would have been commonplace on ships in the classical era.\n\nIn October 2005 a group of students from the Massachusetts Institute of Technology carried out an experiment with 127 one-foot (30 cm) square mirror tiles, focused on a wooden ship at a range of around 100 feet (30 m). Flames broke out on a patch of the ship, but only after the sky had been cloudless and the ship had remained stationary for around ten minutes. It was concluded that the device was a feasible weapon under these conditions. The MIT group repeated the experiment for the television show \"MythBusters\", using a wooden fishing boat in San Francisco as the target. Again some charring occurred, along with a small amount of flame. In order to catch fire, wood needs to reach its autoignition temperature, which is around 300 °C (570 °F).\n\nWhen \"MythBusters\" broadcast the result of the San Francisco experiment in January 2006, the claim was placed in the category of \"busted\" (or failed) because of the length of time and the ideal weather conditions required for combustion to occur. It was also pointed out that since Syracuse faces the sea towards the east, the Roman fleet would have had to attack during the morning for optimal gathering of light by the mirrors. \"MythBusters\" also pointed out that conventional weaponry, such as flaming arrows or bolts from a catapult, would have been a far easier way of setting a ship on fire at short distances.\n\nIn December 2010, \"MythBusters\" again looked at the heat ray story in a special edition entitled \"President's Challenge\". Several experiments were carried out, including a large scale test with 500 schoolchildren aiming mirrors at a of a Roman sailing ship 400 feet (120 m) away. In all of the experiments, the sail failed to reach the 210 °C (410 °F) required to catch fire, and the verdict was again \"busted\". The show concluded that a more likely effect of the mirrors would have been blinding, dazzling, or distracting the crew of the ship.\n\nWhile Archimedes did not invent the lever, he gave an explanation of the principle involved in his work \"On the Equilibrium of Planes\". Earlier descriptions of the lever are found in the Peripatetic school of the followers of Aristotle, and are sometimes attributed to Archytas. According to Pappus of Alexandria, Archimedes' work on levers caused him to remark: \"Give me a place to stand on, and I will move the Earth.\" () Plutarch describes how Archimedes designed block-and-tackle pulley systems, allowing sailors to use the principle of leverage to lift objects that would otherwise have been too heavy to move. Archimedes has also been credited with improving the power and accuracy of the catapult, and with inventing the odometer during the First Punic War. The odometer was described as a cart with a gear mechanism that dropped a ball into a container after each mile traveled.\n\nCicero (106–43 BC) mentions Archimedes briefly in his dialogue \"De re publica\", which portrays a fictional conversation taking place in 129 BC. After the capture of Syracuse c. 212 BC, General Marcus Claudius Marcellus is said to have taken back to Rome two mechanisms, constructed by Archimedes and used as aids in astronomy, which showed the motion of the Sun, Moon and five planets. Cicero mentions similar mechanisms designed by Thales of Miletus and Eudoxus of Cnidus. The dialogue says that Marcellus kept one of the devices as his only personal loot from Syracuse, and donated the other to the Temple of Virtue in Rome. Marcellus' mechanism was demonstrated, according to Cicero, by Gaius Sulpicius Gallus to Lucius Furius Philus, who described it thus:\n\nThis is a description of a planetarium or orrery. Pappus of Alexandria stated that Archimedes had written a manuscript (now lost) on the construction of these mechanisms entitled . Modern research in this area has been focused on the Antikythera mechanism, another device built  BC that was probably designed for the same purpose. Constructing mechanisms of this kind would have required a sophisticated knowledge of differential gearing. This was once thought to have been beyond the range of the technology available in ancient times, but the discovery of the Antikythera mechanism in 1902 has confirmed that devices of this kind were known to the ancient Greeks.\n\nWhile he is often regarded as a designer of mechanical devices, Archimedes also made contributions to the field of mathematics. Plutarch wrote: \"He placed his whole affection and ambition in those purer speculations where there can be no reference to the vulgar needs of life.\"\nArchimedes was able to use infinitesimals in a way that is similar to modern integral calculus. Through proof by contradiction (reductio ad absurdum), he could give answers to problems to an arbitrary degree of accuracy, while specifying the limits within which the answer lay. This technique is known as the method of exhaustion, and he employed it to approximate the value of π. In \"Measurement of a Circle\" he did this by drawing a larger regular hexagon outside a circle and a smaller regular hexagon inside the circle, and progressively doubling the number of sides of each regular polygon, calculating the length of a side of each polygon at each step. As the number of sides increases, it becomes a more accurate approximation of a circle. After four such steps, when the polygons had 96 sides each, he was able to determine that the value of π lay between 3 (approximately 3.1429) and 3 (approximately 3.1408), consistent with its actual value of approximately 3.1416. He also proved that the area of a circle was equal to π multiplied by the square of the radius of the circle (πr). In \"On the Sphere and Cylinder\", Archimedes postulates that any magnitude when added to itself enough times will exceed any given magnitude. This is the Archimedean property of real numbers.\nIn \"Measurement of a Circle\", Archimedes gives the value of the square root of 3 as lying between (approximately 1.7320261) and (approximately 1.7320512). The actual value is approximately 1.7320508, making this a very accurate estimate. He introduced this result without offering any explanation of how he had obtained it. This aspect of the work of Archimedes caused John Wallis to remark that he was: \"as it were of set purpose to have covered up the traces of his investigation as if he had grudged posterity the secret of his method of inquiry while he wished to extort from them assent to his results.\" It is possible that he used an iterative procedure to calculate these values.\n\nIn \"The Quadrature of the Parabola\", Archimedes proved that the area enclosed by a parabola and a straight line is times the area of a corresponding inscribed triangle as shown in the figure at right. He expressed the solution to the problem as an infinite geometric series with the common ratio :\n\nIf the first term in this series is the area of the triangle, then the second is the sum of the areas of two triangles whose bases are the two smaller secant lines, and so on. This proof uses a variation of the series which sums to .\n\nIn \"The Sand Reckoner\", Archimedes set out to calculate the number of grains of sand that the universe could contain. In doing so, he challenged the notion that the number of grains of sand was too large to be counted. He wrote: \"There are some, King Gelo (Gelo II, son of Hiero II), who think that the number of the sand is infinite in multitude; and I mean by the sand not only that which exists about Syracuse and the rest of Sicily but also that which is found in every region whether inhabited or uninhabited.\" To solve the problem, Archimedes devised a system of counting based on the myriad. The word is from the Greek \"murias\", for the number 10,000. He proposed a number system using powers of a myriad of myriads (100 million) and concluded that the number of grains of sand required to fill the universe would be 8 vigintillion, or 8.\n\nThe works of Archimedes were written in Doric Greek, the dialect of ancient Syracuse. The written work of Archimedes has not survived as well as that of Euclid, and seven of his treatises are known to have existed only through references made to them by other authors. Pappus of Alexandria mentions \"On Sphere-Making\" and another work on polyhedra, while Theon of Alexandria quotes a remark about refraction from the \"Catoptrica\". During his lifetime, Archimedes made his work known through correspondence with the mathematicians in Alexandria. The writings of Archimedes were first collected by the Byzantine Greek architect Isidore of Miletus (c. 530 AD), while commentaries on the works of Archimedes written by Eutocius in the sixth century AD helped to bring his work a wider audience. Archimedes' work was translated into Arabic by Thābit ibn Qurra (836–901 AD), and Latin by Gerard of Cremona (c. 1114–1187 AD). During the Renaissance, the \"Editio Princeps\" (First Edition) was published in Basel in 1544 by Johann Herwagen with the works of Archimedes in Greek and Latin. Around the year 1586 Galileo Galilei invented a hydrostatic balance for weighing metals in air and water after apparently being inspired by the work of Archimedes.\n\n\n\n\n\nArchimedes' \"Book of Lemmas\" or \"Liber Assumptorum\" is a treatise with fifteen propositions on the nature of circles. The earliest known copy of the text is in Arabic. The scholars T.L. Heath and Marshall Clagett argued that it cannot have been written by Archimedes in its current form, since it quotes Archimedes, suggesting modification by another author. The \"Lemmas\" may be based on an earlier work by Archimedes that is now lost.\n\nIt has also been claimed that Heron's formula for calculating the area of a triangle from the length of its sides was known to Archimedes. However, the first reliable reference to the formula is given by Heron of Alexandria in the 1st century AD.\n\nThe foremost document containing the work of Archimedes is the Archimedes Palimpsest. In 1906, the Danish professor Johan Ludvig Heiberg visited Constantinople and examined a 174-page goatskin parchment of prayers written in the 13th century AD. He discovered that it was a palimpsest, a document with text that had been written over an erased older work. Palimpsests were created by scraping the ink from existing works and reusing them, which was a common practice in the Middle Ages as vellum was expensive. The older works in the palimpsest were identified by scholars as 10th century AD copies of previously unknown treatises by Archimedes. The parchment spent hundreds of years in a monastery library in Constantinople before being sold to a private collector in the 1920s. On October 29, 1998 it was sold at auction to an anonymous buyer for $2 million at Christie's in New York. The palimpsest holds seven treatises, including the only surviving copy of \"On Floating Bodies\" in the original Greek. It is the only known source of \"The Method of Mechanical Theorems\", referred to by Suidas and thought to have been lost forever. \"Stomachion\" was also discovered in the palimpsest, with a more complete analysis of the puzzle than had been found in previous texts. The palimpsest is now stored at the Walters Art Museum in Baltimore, Maryland, where it has been subjected to a range of modern tests including the use of ultraviolet and light to read the overwritten text.\n\nThe treatises in the Archimedes Palimpsest are:\n\n\n\na. In the preface to \"On Spirals\" addressed to Dositheus of Pelusium, Archimedes says that \"many years have elapsed since Conon's death.\" Conon of Samos lived , suggesting that Archimedes may have been an older man when writing some of his works.\n\nb. The treatises by Archimedes known to exist only through references in the works of other authors are: \"On Sphere-Making\" and a work on polyhedra mentioned by Pappus of Alexandria; \"Catoptrica\", a work on optics mentioned by Theon of Alexandria; \"Principles\", addressed to Zeuxippus and explaining the number system used in \"The Sand Reckoner\"; \"On Balances and Levers\"; \"On Centers of Gravity\"; \"On the Calendar\". Of the surviving works by Archimedes, T.L. Heath offers the following suggestion as to the order in which they were written: \"On the Equilibrium of Planes I\", \"The Quadrature of the Parabola\", \"On the Equilibrium of Planes II\", \"On the Sphere and the Cylinder I, II\", \"On Spirals\", \"On Conoids and Spheroids\", \"On Floating Bodies I, II\", \"On the Measurement of a Circle\", \"The Sand Reckoner\".\n\nc. Boyer, Carl Benjamin \"A History of Mathematics\" (1991) \"Arabic scholars inform us that the familiar area formula for a triangle in terms of its three sides, usually known as Heron's formula — \"k\" = , where \"s\" is the semiperimeter — was known to Archimedes several centuries before Heron lived. Arabic scholars also attribute to Archimedes the 'theorem on the broken chord' ... Archimedes is reported by the Arabs to have given several proofs of the theorem.\"\n\nd. \"It was usual to smear the seams or even the whole hull with pitch or with pitch and wax\". In Νεκρικοὶ Διάλογοι (\"Dialogues of the Dead\"), Lucian refers to coating the seams of a skiff with wax, a reference to pitch (tar) or wax.\n\n\n\n"}
{"id": "10591072", "url": "https://en.wikipedia.org/wiki?curid=10591072", "title": "Axiom of reducibility", "text": "Axiom of reducibility\n\nThe axiom of reducibility was introduced by Bertrand Russell in the early 20th century as part of his ramified theory of types. Russell devised and introduced the axiom in an attempt to manage the contradictions he had discovered in his analysis of set theory.\n\nWith Russell's discovery (1901, 1902) of a paradox in Gottlob Frege's 1879 \"Begriffsschrift\" and Frege's acknowledgment of the same (1902), Russell tentatively introduced his solution as \"Appendix B: Doctrine of Types\" in his 1903 \"The Principles of Mathematics\". This contradiction can be stated as \"the class of all classes that do not contain themselves as elements\". At the end of this appendix Russell asserts that his \"doctrine\" would solve the immediate problem posed by Frege, but \"there is at least one closely analogous contradiction which is probably not soluble by this doctrine. The totality of all logical objects, or of all propositions, involves, it would seem a fundamental logical difficulty. What the complete solution of the difficulty may be, I have not succeeded in discovering; but as it affects the very foundations of reasoning...\"\n\nBy the time of his 1908 \"Mathematical logic as based on the theory of types\" Russell had studied \"the contradictions\" (among them the Epimenides paradox, the Burali-Forti paradox, and Richard's paradox) and concluded that \"In all the contradictions there is a common characteristic, which we may describe as self-reference or reflexiveness\". \n\nIn 1903, Russell defined \"predicative\" functions as those whose order is one more than the highest-order function occurring in the expression of the function. While these were fine for the situation, \"impredicative\" functions had to be disallowed:\n\nHe repeats this definition in a slightly different way later in the paper (together with a subtle prohibition that they would express more clearly in 1913):\n\nThis usage carries over to Alfred North Whitehead and Russell's 1913 \"Principia Mathematica\" wherein the authors devote an entire subsection of their Chapter II: \"The Theory of Logical Types\" to subchapter I. \"The Vicious-Circle Principle\": \"We will define a function of one variable as \"predicative\" when it is of the next order above that of its argument, i.e. of the lowest order compatible with its having that argument. . . A function of several arguments is predicative if there is one of its arguments such that, when the other arguments have values assigned to them, we obtain a predicative function of the one undetermined argument.\"\n\nThey again propose the definition of a \"predicative function\" as one that does not violate The Theory of Logical Types. Indeed the authors assert such violations are \"incapable [to achieve]\" and \"impossible\":\n\nThe authors stress the word \"impossible\":\n\nThe axiom of reducibility states that any truth function (i.e. propositional function) can be expressed by a formally equivalent \"predicative\" truth function. It made its first appearance in Bertrand Russell's (1908) \"Mathematical logic as based on the theory of types\", but only after some five years of trial and error. In his words: \nFor relations (functions of two variables such as \"For all x and for all y, those values for which f(x,y) is true\" i.e. ∀x∀y: f(x,y)), Russell assumed an \"axiom of relations\", or [the same] axiom of reducibility.\n\nIn 1903, he proposed a possible process of evaluating such a 2-place function by comparing the process to double integration: One after another, plug into \"x\" definite values \"a\" (i.e. the particular \"a\" is \"a constant\" or a parameter held constant), then evaluate f(\"a\",\"y\") across all the \"n\" instances of possible \"y\". For all \"y\" evaluate f(a, \"y\"), then for all \"y\" evaluate f(\"a\", \"y\"), etc until all the \"x\" = \"a\" are exhausted). This would create an \"m\" by \"n\" matrix of values: TRUE or UNKNOWN. (In this exposition, the use of indices is a modern convenience.) \n\nIn 1908, Russell made no mention of this matrix of \"x\", \"y\" values that render a two-place function (e.g. relation) TRUE, but by 1913 he has introduced a matrix-like concept into \"function\". In *12 of Principia Mathematica (1913) he defines \"a matrix\" as \"any function, of however many variables, which does not involve any apparent variables. Then any possible function other than a matrix is derived from a matrix by means of generalisation, i.e. by considering the proposition which asserts that the function in question is true with all possible values or with some values of one of the arguments, the other argument or arguments remaining undetermined\". For example, if one asserts that \"∀y: f(x, y) is true\", then \"x\" is the apparent variable because it is unspecified. \n\nRussell now defines a matrix of \"individuals\" as a \"first-order\" matrix, and he follows a similar process to define a \"second-order matrix\", etc. Finally, he introduces the definition of a \"predicative function\":\n\nFrom this reasoning, he then uses the same wording to propose the same \"axioms of reducibility\" as he did in his 1908.\n\nAs an aside, Russell in his 1903 considered, and then rejected, \"a temptation to regard a relation as definable in extension as a class of couples\", i.e. the modern set-theoretic notion of ordered pair. An intuitive version of this notion appeared in Frege's (1879) \"Begriffsschrift\" (translated in van Heijenoort 1967:23); Russell's 1903 followed closely the work of Frege (cf Russell 1903:505ff). Russell worried that \"it is necessary to give sense to the couple, to distinguish the referent from the relatum: thus a couple becomes essentially distinct from a class of two terms, and must itself be introduced as a primitive idea. It would seem, viewing the idea philosophically, that sense can only be derived from some relational proposition . . . it seems therefore more correct to take an intensional view of relations, and to identify them rather with class-concepts than with classes\". As shown below, Norbert Wiener (1914) reduced the notion of relation to class by his definition of an ordered pair.\n\nThe outright prohibition implied by Russell's \"axiom of reducibility\" was roundly criticised by Ernst Zermelo in his 1908 \"Investigations in the foundations of set theory I\", stung as he was by a demand similar to that of Russell that came from Poincaré:\n\nZermelo countered:\n\nIn his 1914 \"A simplification of the logic of relations\", Norbert Wiener removed the need for the axiom of reducibility as applied to relations between two variables \"x\", and \"y\" e.g. φ(\"x\",\"y\"). He did this by introducing a way to express a relation as a set of ordered pairs: \"It will be seen that what we have done is practically to revert to Schröder's treatment of a relation as a class [set] of ordered couples\". Van Heijenoort observes that \"[b]y giving a definition of the ordered pair of two-elements in terms of class operations, the note reduced the theory of relations to that of classes.\" But Wiener opined that while he had dispatched Russell and Whitehead's two-variable version of the axiom *12.11, the single-variable version of the axiom of reducibility for (axiom *12.1 in \"Principia Mathematica\") was still necessary.\n\nLudwig Wittgenstein, while imprisoned in a prison camp, finished his \"Tractatus Logico-Philosophicus\". His introduction credits \"the great works of Frege and the writings of my friend Bertrand Russell\". Not a self-effacing intellectual, he pronounced that \"the \"truth\" of the thoughts communicated here seems to me unassailable and definitive. I am, therefore, of the opinion that the problems have in essentials been finally solved.\" So given such an attitude, it is no surprise that Russell's theory of types comes under criticism:\nThis appears to support the same argument Russell uses to erase his \"paradox\". This \"using the signs\" to \"speak of the signs\" Russell criticises in his introduction that preceded the original English translation:\n\nThis problem appears later when Wittgenstein arrives at this gentle disavowal of the axiom of reducibility—one interpretation of the following is that Wittgenstein is saying that Russell has made (what is known today as) a category error; Russell has asserted (inserted into the theory) a \"further law of logic\" when \"all\" the laws (e.g. the unbounded Sheffer stroke adopted by Wittgenstein) have \"already\" been asserted:\n\nRussell in his 1919 \"Introduction to Mathematical Philosophy\", a non-mathematical companion to his first edition of \"PM\", discusses his Axiom of Reducibility in Chapter 17 \"Classes\" (pp. 146ff). He concludes that \"we cannot accept \"class\" as a primitive idea; the symbols for classes are \"mere conveniences\" and classes are \"logical fictions, or (as we say) 'incomplete symbols' ... classes cannot be regarded as part of the ultimate furniture of the world\" (p. 146). The reason for this is because of the problem of impredicativity: \"classes cannot be regarded as a species of individuals, on account of the contradiction about classes which are not members of themselves ... and because we can prove that the number of classes is greater than the number of individuals, [etc]\". What he then does is propose 5 obligations that must be satisfied with respect to a theory of classes, and the result is his axiom of reducibility. He states that this axiom is \"a generalised form of Leibniz's identity of indiscernibles\" (p. 155). But he concludes Leibniz's assumption is not necessarily true for all possible predicates in all possible worlds, so he concludes that:\n\nThe goal that he sets for himself then is \"adjustments to his theory\" of avoiding classes:\nThoralf Skolem in his 1922 \"Some remarks on axiomatised set theory\" took a less than positive attitude toward \"Russell and Whitehead\" (i.e. their work \"Principia Mathematica\"):\n\nSkolem then observes the problems of what he called \"nonpredicative definition\" in the set theory of Zermelo:\n\nWhile Skolem is mainly addressing a problem with Zermelo's set theory, he does make this observation about the \"axiom of reducibility\":\n\nIn his 1927 \"Introduction\" to the second edition of \"Principia Mathematica\" Russell criticises his own axiom:\n\nWittgenstein's 5.54ff is more centred on the notion of function:\n\nA possible interpretation of Wittgenstein's stance is that the thinker A i.e. \"p\" \"is identically\" the thought \"p\", in this way the \"soul\" remains a unit and not a composite. So to utter \"the thought thinks the thought\" is nonsense, because per 5.542 the utterance does not specify anything.\n\nJohn von Neumann in his 1925 \"An axiomatisation of set theory\" wrestled with the same issues as did Russell, Zermelo, Skolem, and Fraenkel. He summarily rejected the effort of Russell:\n\nHe then notes the work of the set theorists Zermelo, Fraenkel and Schoenflies, in which \"one understands by \"set\" nothing but an object of which one knows no more and wants to know no more than what follows about it from the postulates. The postulates [of set theory] are to be formulated in such a way that all the desired theorems of Cantor's set theory follow from them, but not the antinomies.\n\nWhile he mentions the efforts of David Hilbert to prove the consistency of his axiomatisation of mathematics von Neumann placed him in the same group as Russell. Rather, von Neumann considered his proposal to be \"in the spirit of the second group ... We must, however, avoid forming sets by collecting or separating elements [durch Zusammenfassung oder Aussonderung von Elementen], and so on, as well as eschew the unclear principle of 'definiteness' that can still be found in Zermelo. [...] We prefer, however, to axiomatise not 'set' but 'function'.\"\n\nVan Heijenoort observes that ultimately this axiomatic system of von Neumann's, \"was simplified, revised, and expanded ... and it come to be known as the von Neumann-Bernays-Gödel set theory.\"\n\nDavid Hilbert's axiomatic system that he presents in his 1925 \"The Foundations of Mathematics\" is the mature expression of a task he set about in the early 1900s but let lapse for a while (cf his 1904 \"On the foundations of logic and arithmetic\"). His system is neither set theoretic nor derived directly from Russell and Whitehead. Rather, it invokes 13 axioms of logic—four axioms of Implication, six axioms of logical AND and logical OR, 2 axioms of logical negation, and 1 ε-axiom (\"existence\" axiom)-- plus a version of the Peano axioms in 4 axioms including mathematical induction, some definitions that \"have the character of axioms, and certain \"recursion axioms\" that result from a general recursion schema\" plus some formation rules that \"govern the use of the axioms\". \n\nHilbert states that, with regard to this system, i.e. \"Russell and Whitehead's theory of foundations[,] ... the foundation that it provides for mathematics rests, first, upon the axiom of infinity and, then upon what is called the axiom of reducibility, and both of these axioms are genuine contentual assumptions that are not supported by a consistency proof; they are assumptions whose validity in fact remains dubious and that, in any case, my theory does not require ... reducibility is not presupposed in my theory ... the execution of the reduction would be required only in case a proof of a contradiction were given, and then, according to my proof theory, this reduction would always be bound to succeed.\"\n\nIt is upon this foundation that modern recursion theory rests.\n\nIn 1925, Frank Plumpton Ramsey argued that it is not needed. However in the second edition of Principia Mathematica (1927, page xiv) and in Ramsey's 1926 paper it is stated that certain theorems about real numbers could not be proved using Ramsey's approach. Most later mathematical formalisms (Hilbert's Formalism or Brower's Intuitionism for example) do not use it.\n\nRamsey showed that it is possible to reformulate the definition of \"predicative\" by using the definitions in Wittgenstein's Tractatus Logico-Philosophicus. As a result, all functions of a given order are \"predicative\", irrespective of how they are expressed. He goes on to show that his formulation still avoids the paradoxes. However, the \"Tractatus\" theory did not appear strong enough to prove some mathematical results.\n\nKurt Gödel in his 1944 \"Russell's mathematical logic\" offers in the words of his commentator Charles Parsons, \"[what] might be seen as a defense of these [realist] attitudes of Russell against the reductionism prominent in his philosophy and implicit in much of his actual logical work. It was perhaps the most robust defense of realism about mathematics and its objects since the paradoxes and come to the consciousness of the mathematical world after 1900\". \n\nIn general, Gödel is sympathetic to the notion that a propositional function can be reduced to (identified with) the \"real objects\" that satisfy it, but this causes problems with respect to the theory of real numbers, and even integers (p. 134). He observes that the first edition of \"PM\" \"abandoned\" the realist (constructivistic) \"attitude\" with his proposal of the axiom of reducibility (p. 133). However, within the introduction to the second edition of \"PM\" (1927) Gödel asserts \"the constructivistic attitude is resumed again\" (p. 133) when Russell \"dropped\" of the axiom of reducibility in favour of the matrix (truth-functional) theory; Russell \"stated explicitly that all primitive predicates belong to the lowest type and that the only purpose of variables (and evidently also of constants) is to make it possible to assert more complicated truth-functions of atomic propositions ... [i.e.] the higher types and orders are solely a \"façon de parler\"\" (p. 134). But this only works when the number of individuals and primitive predicates is finite, for one can construct finite strings of symbols such as:\nAnd from such strings one can form strings of strings to obtain the equivalent of classes of classes, with a mixture of types possible. However, from such finite strings the whole of mathematics cannot be constructed because they cannot be \"analyzed\", i.e. reducible to the law of identity or disprovable by a negations of the law:\nBut he observes that \"this procedure seems to presuppose arithmetic in some form or other\" (p. 134), and he states in the next paragraph that \"the question of whether (or to what extent) the theory of integers can be obtained on the basis of the ramified hierarchy must be considered as unsolved.\" (p. 135)\n\nGödel proposed that one should take a \"more conservative approach\":\n\nIn a critique that also discusses the pros and cons of Ramsey (1931) Quine calls Russell's formulation of \"types\" to be \"troublesome ... the confusion persists as he attempts to define \"n\"th order propositions'... the method is indeed oddly devious ... the axiom of reducibility is self-effacing\", etc.\n\nLike Kleene Quine observes that Ramsey (1926), (1931) divided the various paradoxes into two varieties (i) \"those of pure set theory\" and (ii) those derived from \"semantic concepts such as falsity and specifiability\", and Ramsey believed that the second variety should have been left out of Russell's solution. Quine ends with the opinion that \"because of the confusion of propositions with sentences, and of attributes with their expressions, Russell's purported solution of the semantic paradoxes was enigmatic anyway.\"\n\nIn his section §12. First inferences from the paradoxes, subchapter \"LOGICISM\" Kleene (1952) traces the development of Russell's theory of types:\nKleene observes that \"to exclude impredicative definitions within a type, the types above type 0 [primary objects or individuals \"not subjected to logical analysis\"] are further separated into orders. Thus for type 1 [properties of individuals, i.e. logical results of the propositional calculus ], properties defined without mentioning any totality belong to \"order\" 0, and properties defined using the totality of properties of a given order below to the next higher order)\".\n\nKleene, however, parenthetically observes that \"the logicistic definition of natural number now becomes predicative when the [property] P in it is specified to range only over properties of a given order; in [this] case the property of being a natural number is of the next higher order\". But this separation into orders makes it impossible to construct the familiar analysis, which [see Kleene's example at Impredicativity] contains impredicative definitions. To escape this outcome, Russell postulated his \"axiom of reducibility\". But, Kleene wonders, \"on what grounds should we believe in the axiom of reducibility?\" He observes that, whereas \"Principia Mathematica\" is presented as derived from \"intuitively\"-derived axioms that \"were intended to be believed about the world, or at least to be accepted as plausible hypotheses concerning the world[,] ... if properties are to be constructed, the matter should be settled on the basis of constructions, not by an axiom.\" Indeed, he quotes Whitehead and Russell (1927) questioning their own axiom: \"clearly it is not the sort of axiom with which we can rest content\".\n\nKleene references work of Ramsey 1926 but notes that \"neither Whitehead and Russell nor Ramsey succeeded in attaining the logicistic goal constructively\" and \"an interesting proposal ... by Langford 1927 and Carnap 1931-2, is also not free of difficulties.\" Kleene ends this discussion with quotes from Weyl (1946) that \"the system of \"Principia Mathematica\" ... [is founded on] a sort of logician's paradise\" and anyone \"who is ready to believe in this 'transcendental world' could also accept the system of axiomatic set theory (Zermelo, Fraenkel, etc), which, for the deduction of mathematics, has the advantage of being simpler in structure.\"\n\n"}
{"id": "25744542", "url": "https://en.wikipedia.org/wiki?curid=25744542", "title": "Berlekamp–Zassenhaus algorithm", "text": "Berlekamp–Zassenhaus algorithm\n\nIn mathematics, in particular in computational algebra, the Berlekamp–Zassenhaus algorithm is an algorithm for factoring polynomials over the integers, named after Elwyn Berlekamp and Hans Zassenhaus. As a consequence of Gauss's lemma, this amounts to solving the problem also over the rationals.\n\nThe algorithm starts by finding factorizations over suitable finite fields using Hensel's lemma to lift the solution from modulo a prime \"p\" to a convenient power of \"p\". After this the right factors are found as a subset of these. \nThe worst case of this algorithm is exponential in the number of factors.\n\n\n"}
{"id": "4064", "url": "https://en.wikipedia.org/wiki?curid=4064", "title": "Borsuk–Ulam theorem", "text": "Borsuk–Ulam theorem\n\nIn mathematics, the Borsuk–Ulam theorem states that every continuous function from an \"n\"-sphere into Euclidean \"n\"-space maps some pair of antipodal points to the same point. Here, two points on a sphere are called antipodal if they are in exactly opposite directions from the sphere's center.\n\nFormally: if formula_1 is continuous then there exists an formula_2 such that: formula_3.\n\nThe case formula_4 can be illustrated by saying that there always exist a pair of opposite points on the Earth's equator with the same temperature. The same is true for any circle. This assumes the temperature varies continuously.\n\nThe case formula_5 is often illustrated by saying that at any moment, there is always a pair of antipodal points on the Earth's surface with equal temperatures and equal barometric pressures.\n\nThe Borsuk–Ulam theorem has several equivalent statements in terms of odd functions. Recall that formula_6 is the \"n\"-sphere and formula_7 is the \"n\"-ball:\n\nAccording to , the first historical mention of the statement of the Borsuk–Ulam theorem appears in . The first proof was given by , where the formulation of the problem was attributed to Stanislaw Ulam. Since then, many alternative proofs have been found by various authors, as collected by .\n\nThe following statements are equivalent to the Borsuk–Ulam theorem.\n\nA function formula_16 is called \"odd\" (aka \"antipodal\" or \"antipode-preserving\") if for every formula_17: formula_18.\n\nThe Borsuk–Ulam theorem is equivalent to the following statement: A continuous odd function from an \"n\"-sphere into Euclidean \"n\"-space has a zero. PROOF: \n\nDefine a \"retraction\" as a function formula_25 The Borsuk–Ulam theorem is equivalent to the following claim: there is no continuous odd retraction.\n\nProof: If the theorem is correct, then every continuous odd function from formula_6 must include 0 in its range. However, formula_27 so there cannot be a continuous odd function whose range is formula_12.\n\nConversely, if it is incorrect, then there is a continuous odd function formula_29 with no zeroes. Then we can construct another odd function formula_30 by:\n\nsince formula_16 has no zeroes, formula_33 is well-defined and continuous. Thus we have a continuous odd retraction.\n\nThe 1-dimensional case can easily be proved using the intermediate value theorem (IVT).\n\nLet formula_16 be an odd real-valued continuous function on a circle. Pick an arbitrary formula_17. If formula_10 then we are done. Otherwise, without loss of generality, formula_37 But formula_38 Hence, by the IVT, there is a point formula_39 between formula_17 and formula_41 at which formula_42\n\nAssume that formula_30 is an odd continuous function with formula_44 (the case formula_45 is treated above, the case formula_46 can be handled using basic covering theory). By passing to orbits under the antipodal action, we then get an induced function formula_47 which induces an isomorphism on fundamental groups. By the Hurewicz theorem, the induced map on cohomology with formula_48 coefficients, \n\nsends formula_50 to formula_51. But then we get that formula_52 is sent to formula_53, a contradiction.\n\nOne can also show the stronger statement that any odd map formula_54 has odd degree and then deduce the theorem from this result.\n\nThe Borsuk–Ulam theorem can be proved from Tucker's lemma.\n\nLet formula_8 be a continuous odd function. Because \"g\" is continuous on a compact domain, it is uniformly continuous. Therefore, for every formula_56, there is a formula_57 such that, for every two points of formula_58 which are within formula_59 of each other, their images under \"g\" are within formula_60 of each other.\n\nDefine a triangulation of formula_58 with edges of length at most formula_59. Label each vertex formula_63 of the triangulation with a label formula_64 in the following way:\n\n\nBecause \"g\" is odd, the labeling is also odd: formula_67. Hence, by Tucker's lemma, there are two adjacent vertices formula_68 with opposite labels. Assume w.l.o.g. that the labels are formula_69. By the definition of \"l\", this means that in both formula_70 and formula_71, coordinate #1 is the largest coordinate: in formula_70 this coordinate is positive while in formula_71 it is negative. By the construction of the triangulation, the distance between formula_70 and formula_71 is at most formula_60, so in particular formula_77 (since formula_78 and formula_79 have opposite signs) and so formula_80. But since the largest coordinate of formula_70 and formula_71 is coordinate #1, this means that formula_83 for each formula_84. So formula_85, where formula_86 is some constant depending on formula_87 and the norm formula_88 which you have chosen.\n\nThe above is true for every formula_60; hence there must be a point \"u\" in which formula_90.\n\n\nAbove we showed how to prove the Borsuk–Ulam theorem from Tucker's lemma. The converse is also true: it is possible to prove Tucker's lemma from the Borsuk–Ulam theorem. Therefore, these two theorems are equivalent.\n\n\n\n"}
{"id": "26829", "url": "https://en.wikipedia.org/wiki?curid=26829", "title": "Category of sets", "text": "Category of sets\n\nIn the mathematical field of category theory, the category of sets, denoted as Set, is the category whose objects are sets. The arrows or morphisms between sets \"A\" and \"B\" are the total functions from \"A\" to \"B\", and the composition of morphisms is the composition of functions.\n\nMany other categories (such as the category of groups, with group homomorphisms as arrows) add structure to the objects of the category of sets and/or restrict the arrows to functions of a particular kind.\n\nThe axioms of a category are satisfied by Set because composition of functions is associative, and because every set \"X\" has an identity function \"id : X → X\" which serves as identity element for function composition.\n\nThe epimorphisms in Set are the surjective maps, the monomorphisms are the injective maps, and the isomorphisms are the bijective maps.\n\nThe empty set serves as the initial object in Set with empty functions as morphisms. Every singleton is a terminal object, with the functions mapping all elements of the source sets to the single target element as morphisms. There are thus no zero objects in Set. \n\nThe category Set is complete and co-complete. The product in this category is given by the cartesian product of sets. The coproduct is given by the disjoint union: given sets \"A\" where \"i\" ranges over some index set \"I\", we construct the coproduct as the union of \"A\"×{\"i\"} (the cartesian product with \"i\" serves to ensure that all the components stay disjoint).\n\nSet is the prototype of a concrete category; other categories are concrete if they are \"built on\" Set in some well-defined way.\n\nEvery two-element set serves as a subobject classifier in Set. The power object of a set \"A\" is given by its power set, and the exponential object of the sets \"A\" and \"B\" is given by the set of all functions from \"A\" to \"B\". Set is thus a topos (and in particular cartesian closed and exact in the sense of Barr).\n\nSet is not abelian, additive nor preadditive.\n\nEvery non-empty set is an injective object in Set. Every set is a projective object in Set (assuming the axiom of choice).\n\nThe finitely presentable objects in Set are the finite sets. Since every set is a direct limit of its finite subsets, the category Set is a locally finitely presentable category.\n\nIf \"C\" is an arbitrary category, the contravariant functors from \"C\" to Set are often an important object of study. If \"A\" is an object of \"C\", then the functor from \"C\" to Set that sends \"X\" to Hom(\"X\",\"A\") (the set of morphisms in \"C\" from \"X\" to \"A\") is an example of such a functor. If \"C\" is a small category (i.e. the collection of its objects forms a set), then the contravariant functors from \"C\" to Set, together with natural transformations as morphisms, form a new category, a functor category known as the category of presheaves on \"C\".\n\nIn Zermelo–Fraenkel set theory the collection of all sets is not a set; this follows from the axiom of foundation. One refers to collections that are not sets as proper classes. One cannot handle proper classes as one handles sets; in particular, one cannot write that those proper classes belong to a collection (either a set or a proper class). This is a problem: it means that the category of sets cannot be formalized straightforwardly in this setting. Categories like Set whose collection of objects forms a proper class are known as large categories, to distinguish them from the small categories whose objects form a set. \n\nOne way to resolve the problem is to work in a system that gives formal status to proper classes, such as NBG set theory. In this setting, categories formed from sets are said to be \"small\" and those (like Set) that are formed from proper classes are said to be \"large\".\n\nAnother solution is to assume the existence of Grothendieck universes. Roughly speaking, a Grothendieck universe is a set which is itself a model of ZF(C) (for instance if a set belongs to a universe, its elements and its powerset will belong to the universe). The existence of Grothendieck universes (other than the empty set and the set formula_1 of all hereditarily finite sets) is not implied by the usual ZF axioms; it is an additional, independent axiom, roughly equivalent to the existence of strongly inaccessible cardinals. Assuming this extra axiom, one can limit the objects of Set to the elements of a particular universe. (There is no \"set of all sets\" within the model, but one can still reason about the class \"U\" of all inner sets, i.e., elements of \"U\".)\n\nIn one variation of this scheme, the class of sets is the union of the entire tower of Grothendieck universes. (This is necessarily a proper class, but each Grothendieck universe is a set because it is an element of some larger Grothendieck universe.) However, one does not work directly with the \"category of all sets\". Instead, theorems are expressed in terms of the category Set whose objects are the elements of a sufficiently large Grothendieck universe \"U\", and are then shown not to depend on the particular choice of \"U\". As a foundation for category theory, this approach is well matched to a system like Tarski–Grothendieck set theory in which one cannot reason directly about proper classes; its principal disadvantage is that a theorem can be true of all Set but not of Set.\n\nVarious other solutions, and variations on the above, have been proposed.\n\nThe same issues arise with other concrete categories, such as the category of groups or the category of topological spaces.\n\n\n"}
{"id": "1656381", "url": "https://en.wikipedia.org/wiki?curid=1656381", "title": "Clifford Truesdell", "text": "Clifford Truesdell\n\nClifford Ambrose Truesdell III (February 18, 1919 – January 14, 2000) was an American mathematician, natural philosopher, and historian of science.\n\nTruesdell was born in Los Angeles, California. After high school, he spent two years in Europe learning French, German, and Italian, and improving his Latin and Greek. His linguistic skills stood him in good stead in his later historical investigations. At Caltech he was deeply influenced by the teaching of Harry Bateman. In particular, a course in partial differential equations \"taught me the difference between an ordinary good teacher and a great mathematician, and after that I never cared what grade I got in anything.\" He obtained a B.Sc. in mathematics and physics in 1941, and an MSc. in mathematics in 1942.\n\nIn 1943, he completed a Ph.D. in mathematics at Princeton University. For the rest of the decade, the U.S. Navy employed him to do mechanics research.\n\nTruesdell taught at Indiana University 1950-61, where his students included James Serrin, Jerald Ericksen, and Walter Noll. From 1961 until his retirement in 1989, Truesdell was professor of rational mechanics at Johns Hopkins University. He and Noll contributed to foundational rational mechanics, whose aim is to construct a mathematical model for treating (continuous) mechanical phenomena.\n\nTruesdell was the founder and editor-in-chief of the journals \"Archive for Rational Mechanics and Analysis\" and \"Archive for History of Exact Sciences\", which were unusual in several ways. Following Truesdell's criticisms of awkward style in scientific writing, the journal accepted papers in English, French, German, and Latin.\n\nIn addition to his original work in mechanics, Truesdell was a major historian of science and mathematics, editing or co-editing six volumes of the collected works of Leonhard Euler.\n\nIn the words of Bernard Lavenda if there is something rational in rational thermodynamics it is well-hidden. Ironically, the 'rational' theory even failed in fields where the authors assumed expertise: \"More damage was suffered by rational thermodynamics when it was found that the theory could not be applied to non-Newtonian fluids.\".\n\nTruesdell become also famous by his attacks on Lars Onsager (Nobel Prize 1968 for nonequilibrium thermodynamics) and related scientists. As Ingo Müller reports:\n\nAn article written by Müller (On the frame dependence of stress and heat flux) was later refuted by Truesdell (Correction of two errors in the kinetic theory of gases which have been used to cast unfounded doubt upon the principle of material frame-indifference).\n\n\n\n"}
{"id": "1822603", "url": "https://en.wikipedia.org/wiki?curid=1822603", "title": "Cointegration", "text": "Cointegration\n\nCointegration is a statistical property of a collection of time series variables. First, all of the series must be integrated of order \"d\" (see Order of integration). Next, if a linear combination of this collection is integrated of order zero, then the collection is said to be co-integrated. Formally, if (\"X\",\"Y\",\"Z\") are each integrated of order \"d\", and there exist coefficients \"a\",\"b\",\"c\" such that is integrated of order 0, then \"X\", \"Y\", and \"Z\" are cointegrated. Cointegration has become an important property in contemporary time series analysis. Time series often have trends—either deterministic or stochastic. In an influential paper, Charles Nelson and Charles Plosser (1982) provided statistical evidence that many US macroeconomic time series (like GNP, wages, employment, etc.) have stochastic trends—these are also called unit root processes, or processes integrated of order . They also showed that unit root processes have non-standard statistical properties, so that conventional econometric theory methods do not apply to them.\n\nIf two or more series are individually integrated (in the time series sense) but some linear combination of them has a lower order of integration, then the series are said to be cointegrated. A common example is where the individual series are first-order integrated () but some (cointegrating) vector of coefficients exists to form a stationary linear combination of them. For instance, a stock market index and the price of its associated futures contract move through time, each roughly following a random walk. Testing the hypothesis that there is a statistically significant connection between the futures price and the spot price could now be done by testing for the existence of a cointegrated combination of the two series.\n\nThe first to introduce and analyse the concept of spurious—or nonsense—correlations was Udne Yule in 1926.\nBefore the 1980s, many economists used linear regressions on non-stationary time series data, which Nobel laureate Clive Granger and Paul Newbold showed to be a dangerous approach that could produce spurious correlation, since standard detrending techniques can result in data that are still non-stationary. Granger's 1987 paper with Robert Engle formalized the cointegrating vector approach, and coined the term.\n\nFor integrated processes, Granger and Newbold showed that de-trending does not work to eliminate the problem of spurious correlation, and that the superior alternative is to check for co-integration. Two series with trends can be co-integrated only if there is a genuine relationship between the two. Thus the standard current methodology for time series regressions is to check all-time series involved for integration. If there are series on both sides of the regression relationship, then it's possible for regressions to give misleading results.\n\nThe possible presence of cointegration must be taken into account when choosing a technique to test hypotheses concerning the relationship between two variables having unit roots (i.e. integrated of at least order one). The usual procedure for testing hypotheses concerning the relationship between non-stationary variables was to run ordinary least squares (OLS) regressions on data which had been differenced. This method is biased if the non-stationary variables are cointegrated.\n\nFor example, regressing the consumption series for any country (e.g. Fiji) against the GNP for a randomly selected dissimilar country (e.g. Afghanistan) might give a high R-squared relationship (suggesting high explanatory power on Fiji's consumption from Afghanistan's GNP). This is called spurious regression: two integrated series which are not directly causally related may nonetheless show a significant correlation; this phenomenon is called spurious correlation.\n\nThe three main methods for testing for cointegration are:\n\nIf formula_1 and formula_2 are non-stationary and cointegrated, then a linear combination of them must be stationary. In other words:\n\nwhere formula_4 is stationary.\n\nIf we knew formula_4, we could just test it for stationarity with something like a Dickey–Fuller test, Phillips–Perron test and be done. But because we don't know formula_4, we must estimate this first, generally by using ordinary least squares, and then run our stationarity test on the estimated formula_4 series, often denoted formula_8.\n\nA second regression is then run on the first differenced variables from the first regression, and the lagged residuals formula_9 is included as a regressor.\n\nThe Johansen test is a test for cointegration that allows for more than one cointegrating relationship, unlike the Engle–Granger method, but this test is subject to asymptotic properties, i.e. large samples. If the sample size is too small then the results will not be reliable and one should use Auto Regressive Distributed Lags (ARDL).\n\nPeter C. B. Phillips and Sam Ouliaris (1990) show that residual-based unit root tests applied to the estimated cointegrating residuals do not have the usual Dickey–Fuller distributions under the null hypothesis of no-cointegration. Because of the spurious regression phenomenon under the null hypothesis, the distribution of these tests have asymptotic distributions that depend on (1) the number of deterministic trend terms and (2) the number of variables with which co-integration is being tested. These distributions are known as Phillips–Ouliaris distributions and critical values have been tabulated. In finite samples, a superior alternative to the use of these asymptotic critical value is to generate critical values from simulations.\n\nIn practice, cointegration is often used for two series, but it is more generally applicable and can be used for variables integrated of higher order (to detect correlated accelerations or other second-difference effects). Multicointegration extends the cointegration technique beyond two variables, and occasionally to variables integrated at different orders.\n\nTests for cointegration assume that the cointegrating vector is constant during the period of study. In reality, it is possible that the long-run relationship between the underlying variables change (shifts in the cointegrating vector can occur). The reason for this might be technological progress, economic crises, changes in the people's preferences and behaviour accordingly, policy or regime alteration, and organizational or institutional developments. This is especially likely to be the case if the sample period is long. To take this issue into account, tests have been introduced for cointegration with one unknown structural break, and tests for cointegration with two unknown breaks are also available.\n\n\n"}
{"id": "75802", "url": "https://en.wikipedia.org/wiki?curid=75802", "title": "Consistency", "text": "Consistency\n\nIn classical deductive logic, a consistent theory is one that does not contain a contradiction. The lack of contradiction can be defined in either semantic or syntactic terms. The semantic definition states that a theory is consistent if and only if it has a model, i.e., there exists an interpretation under which all formulas in the theory are true. This is the sense used in traditional Aristotelian logic, although in contemporary mathematical logic the term satisfiable is used instead. The syntactic definition states a theory formula_1 is consistent if and only if there is no formula formula_2 such that both formula_2 and its negation formula_4 are elements of the set formula_1. Let formula_6 be a set of closed sentences (informally \"axioms\") and formula_7 the set of closed sentences provable from formula_6 under some (specified, possibly implicitly) formal deductive system. The set of axioms formula_6 is consistent when formula_10 is.\n\nIf there exists a deductive system for which these semantic and syntactic definitions are equivalent for any theory formulated in a particular deductive logic, the logic is called complete. The completeness of the sentential calculus was proved by Paul Bernays in 1918 and Emil Post in 1921, while the completeness of predicate calculus was proved by Kurt Gödel in 1930, and consistency proofs for arithmetics restricted with respect to the induction axiom schema were proved by Ackermann (1924), von Neumann (1927) and Herbrand (1931). Stronger logics, such as second-order logic, are not complete.\n\nA consistency proof is a mathematical proof that a particular theory is consistent. The early development of mathematical proof theory was driven by the desire to provide finitary consistency proofs for all of mathematics as part of Hilbert's program. Hilbert's program was strongly impacted by incompleteness theorems, which showed that sufficiently strong proof theories cannot prove their own consistency (provided that they are in fact consistent).\n\nAlthough consistency can be proved by means of model theory, it is often done in a purely syntactical way, without any need to reference some model of the logic. The cut-elimination (or equivalently the normalization of the underlying calculus if there is one) implies the consistency of the calculus: since there is obviously no cut-free proof of falsity, there is no contradiction in general.\n\nIn theories of arithmetic, such as Peano arithmetic, there is an intricate relationship between the consistency of the theory and its completeness. A theory is complete if, for every formula φ in its language, at least one of φ or ¬φ is a logical consequence of the theory.\n\nPresburger arithmetic is an axiom system for the natural numbers under addition. It is both consistent and complete.\n\nGödel's incompleteness theorems show that any sufficiently strong recursively enumerable theory of arithmetic cannot be both complete and consistent. Gödel's theorem applies to the theories of Peano arithmetic (PA) and primitive recursive arithmetic (PRA), but not to Presburger arithmetic.\n\nMoreover, Gödel's second incompleteness theorem shows that the consistency of sufficiently strong recursively enumerable theories of arithmetic can be tested in a particular way. Such a theory is consistent if and only if it does \"not\" prove a particular sentence, called the Gödel sentence of the theory, which is a formalized statement of the claim that the theory is indeed consistent. Thus the consistency of a sufficiently strong, recursively enumerable, consistent theory of arithmetic can never be proven in that system itself. The same result is true for recursively enumerable theories that can describe a strong enough fragment of arithmetic—including set theories such as Zermelo–Fraenkel set theory. These set theories cannot prove their own Gödel sentence—provided that they are consistent, which is generally believed.\n\nBecause consistency of ZF is not provable in ZF, the weaker notion is interesting in set theory (and in other sufficiently expressive axiomatic systems). If \"T\" is a theory and \"A\" is an additional axiom, \"T\" + \"A\" is said to be consistent relative to \"T\" (or simply that \"A\" is consistent with \"T\") if it can be proved that\nif \"T\" is consistent then \"T\" + \"A\" is consistent. If both \"A\" and ¬\"A\" are consistent with \"T\", then \"A\" is said to be independent of \"T\".\n\nformula_11 (Turnstile symbol) in the following context of mathematical logic, means \"provable from\". That is, formula_12 reads: \"b\" is provable from \"a\" (in some specified formal system). See List of logic symbols. In other cases, the turnstile symbol may mean implies; permits the derivation of. See: List of mathematical symbols.\n\n\n\n\n\n\n\nLet formula_72 be a symbol set. Let formula_13 be a maximally consistent set of formula_72-formulas containing witnesses.\n\nDefine an equivalence relation formula_75 on the set of formula_72-terms by formula_77 if formula_78, where formula_79 denotes equality. Let formula_80 denote the equivalence class of terms containing formula_81; and let formula_82 where formula_83 is the set of terms based on the symbol set formula_84.\n\nDefine the formula_72-structure formula_86 over formula_87, also called the term-structure corresponding to formula_13, by:\n\n\nDefine a variable assignment formula_98 by formula_99 for each variable formula_38. Let formula_101 be the term interpretation associated with formula_13.\n\nThen for each formula_72-formula formula_2:\nformula_105 if and only if formula_106\nThere are several things to verify. First, that formula_75 is in fact an equivalence relation. Then, it needs to be verified that (1), (2), and (3) are well defined. This falls out of the fact that formula_75 is an equivalence relation and also requires a proof that (1) and (2) are independent of the choice of formula_109 class representatives. Finally, formula_110 can be verified by induction on formulas.\n\nIn ZFC set theory with classical first-order logic, an inconsistent theory formula_1 is one such that there exists a closed sentence formula_2 such that formula_1 contains both formula_2 and its negation formula_115. A consistent theory is one such that the following logically equivalent conditions hold\n\n\n\n\n"}
{"id": "660019", "url": "https://en.wikipedia.org/wiki?curid=660019", "title": "Convex polygon", "text": "Convex polygon\n\nA convex polygon is a simple polygon (not self-intersecting) in which no line segment between two points on the boundary ever goes outside the polygon. Equivalently, it is a simple polygon whose interior is a convex set. In a convex polygon, all interior angles are less than or equal to 180 degrees, while in a strictly convex polygon all interior angles are strictly less than 180 degrees.\n\nThe following properties of a simple polygon are all equivalent to convexity:\n\nAdditional properties of convex polygons include:\n\nEvery polygon inscribed in a circle (such that all vertices of the polygon touch the circle), if not self-intersecting, is convex. However, not every convex polygon can be inscribed in a circle.\n\nThe following properties of a simple polygon are all equivalent to strict convexity:\n\n\nEvery nondegenerate triangle is strictly convex.\n\n\n"}
{"id": "12101596", "url": "https://en.wikipedia.org/wiki?curid=12101596", "title": "Convolution power", "text": "Convolution power\n\nIn mathematics, the convolution power is the \"n\"-fold iteration of the convolution with itself. Thus if formula_1 is a function on Euclidean space R and formula_2 is a natural number, then the convolution power is defined by\n\nwhere * denotes the convolution operation of functions on R and δ is the Dirac delta distribution. This definition makes sense if \"x\" is an integrable function (in L), a rapidly decreasing distribution (in particular, a compactly supported distribution) or is a finite Borel measure.\n\nIf \"x\" is the distribution function of a random variable on the real line, then the \"n\" convolution power of \"x\" gives the distribution function of the sum of \"n\" independent random variables with identical distribution \"x\". The central limit theorem states that if \"x\" is in L and L with mean zero and variance σ, then\nwhere Φ is the cumulative standard normal distribution on the real line. Equivalently, formula_5 tends weakly to the standard normal distribution.\n\nIn some cases, it is possible to define powers \"x\" for arbitrary real \"t\" > 0. If μ is a probability measure, then μ is infinitely divisible provided there exists, for each positive integer \"n\", a probability measure μ such that\n\nThat is, a measure is infinitely divisible if it is possible to define all \"n\"th roots. Not every probability measure is infinitely divisible, and a characterization of infinitely divisible measures is of central importance in the abstract theory of stochastic processes. Intuitively, a measure should be infinitely divisible provided it has a well-defined \"convolution logarithm.\" The natural candidate for measures having such a logarithm are those of (generalized) Poisson type, given in the form\n\nIn fact, the Lévy–Khinchin theorem states that a necessary and sufficient condition for a measure to be infinitely divisible is that it must lie in the closure, with respect to the vague topology, of the class of Poisson measures .\n\nMany applications of the convolution power rely on being able to define the analog of analytic functions as formal power series with powers replaced instead by the convolution power. Thus if formula_8 is an analytic function, then one would like to be able to define\n\nIf \"x\" ∈ \"L\"(R) or more generally is a finite Borel measure on R, then the latter series converges absolutely in norm provided that the norm of \"x\" is less than the radius of convergence of the original series defining \"F\"(\"z\"). In particular, it is possible for such measures to define the convolutional exponential\n\nIt is not generally possible to extend this definition to arbitrary distributions, although a class of distributions on which this series still converges in an appropriate weak sense is identified by .\n\nIf \"x\" is itself suitably differentiable, then the properties of convolution, one has\n\nwhere formula_12 denotes the derivative operator. Specifically, this holds if \"x\" is a compactly supported distribution or lies in the Sobolev space \"W\" to ensure that the derivative is sufficiently regular for the convolution to be well-defined.\n\nIn the configuration random graph, the size distribution of connected components can be expressed via the convolution power of the excess degree distribution ():\nHere, formula_14 is the size distribution for connected components, formula_15 is the excess degree distribution, and formula_16 denotes the degree distribution.\n\nAs convolution algebras are special cases of Hopf algebras, the convolution power is a special case of the (ordinary) power in a Hopf algebra. In applications to quantum field theory, the convolution exponential, convolution logarithm, and other analytic functions based on the convolution are constructed as formal power series in the elements of the algebra . If, in addition, the algebra is a Banach algebra, then convergence of the series can be determined as above. In the formal setting, familiar identities such as\ncontinue to hold. Moreover, by the permanence of functional relations, they hold at the level of functions, provided all expressions are well-defined in an open set by convergent series.\n\n"}
{"id": "4658581", "url": "https://en.wikipedia.org/wiki?curid=4658581", "title": "Crofton formula", "text": "Crofton formula\n\nIn mathematics, the Crofton formula, named after Morgan Crofton (1826–1915), is a classic result of integral geometry relating the length of a curve to the expected number of times a \"random\" line intersects it.\n\nSuppose formula_1 is a rectifiable plane curve. Given an oriented line \"l\", let formula_2(\"l\") be the number of points at which formula_1 and \"l\" intersect. We can parametrize the general line \"l\" by the direction formula_4 in which it points and its signed distance formula_5 from the origin. The Crofton formula expresses the arc length of the curve formula_1 in terms of an integral over the space of all oriented lines:\n\nThe differential form\n\nis invariant under rigid motions, so it is a natural integration measure for speaking of an \"average\" number of intersections. The right-hand side in the Crofton formula is sometimes called the Favard length.\n\nBoth sides of the Crofton formula are additive over concatenation of curves, so it suffices to prove the formula for a single line segment. Since the right-hand side does not depend on the positioning of the line segment, it must equal some function of the segment's length. Because, again, the formula is additive over concatenation of line segments, the integral must be a constant times the length of the line segment. It remains only to determine the factor of 1/4; this is easily done by computing both sides when γ is the unit circle.\n\nThe space of oriented lines is a double cover of the space of unoriented lines. The Crofton formula is often stated in terms of the corresponding density in the latter space, in which the numerical factor is not 1/4 but 1/2. Since a convex curve intersects almost every line either twice or not at all, the unoriented Crofton formula for convex curves can be stated without numerical factors: the measure of the set of straight lines which intersect a convex curve is equal to its length.\n\nThe Crofton formula generalizes to any Riemannian surface; the integral is then performed with the natural measure on the space of geodesics.\n\nCrofton's formula yields elegant proofs of the following results, among others:\n\n\n\n"}
{"id": "401767", "url": "https://en.wikipedia.org/wiki?curid=401767", "title": "Cross-ratio", "text": "Cross-ratio\n\nIn geometry, the cross-ratio, also called the double ratio and anharmonic ratio, is a number associated with a list of four collinear points, particularly points on a projective line. Given four points \"A\", \"B\", \"C\" and \"D\" on a line, their cross ratio is defined as\n\nwhere an orientation of the line determines the sign of each distance and the distance is measured as projected into Euclidean space. (If one of the four points is the line's point at infinity, then the two distances involving that point are dropped from the formula.)\nThe point \"D\" is the harmonic conjugate of \"C\" with respect to \"A\" and \"B\" precisely if the cross-ratio of the quadruple is −1, called the \"harmonic ratio\". The cross-ratio can therefore be regarded as measuring the quadruple's deviation from this ratio; hence the name \"anharmonic ratio\".\n\nThe cross-ratio is preserved by linear fractional transformations. It is essentially the only projective invariant of a quadruple of collinear points; this underlies its importance for projective geometry. \n\nThe cross-ratio had been defined in deep antiquity, possibly already by Euclid, and was considered by Pappus, who noted its key invariance property. It was extensively studied in the 19th century. \n\nVariants of this concept exist for a quadruple of concurrent lines on the projective plane and a quadruple of points on the Riemann sphere.\nIn the Cayley–Klein model of hyperbolic geometry, the distance between points is expressed in terms of a certain cross-ratio.\n\nPappus of Alexandria made implicit use of concepts equivalent to the cross-ratio in his \"Collection: Book VII\". Early users of Pappus included Isaac Newton, Michel Chasles, and Robert Simson. In 1986 Alexander Jones made a translation of the original by Pappus, then wrote a commentary on how the lemmas of Pappus relate to modern terminology.\n\nModern use of the cross ratio in projective geometry began with Lazare Carnot in 1803 with his book \"Géométrie de Position\". The term used was \"le rapport anharmonique\" (Fr: anharmonic ratio). German geometers call it \"das Doppelverhältnis\" (Ger: double ratio).\n\nGiven three points on a line, a fourth point that makes the cross ratio equal to minus one is called the projective harmonic conjugate. In 1847 Carl von Staudt called the construction of the fourth point a Throw (Wurf), and used the construction to exhibit arithmetic implicit in geometry. His Algebra of Throws provides an approach to numerical propositions, usually taken as axioms, but proven in projective geometry.\n\nThe English term \"cross-ratio\" was introduced in 1878 by William Kingdon Clifford.\n\nThe cross-ratio of a quadruple of distinct points on the real line with coordinates \"z\", \"z\", \"z\", \"z\" is given by\n\nIt can also be written as a \"double ratio\" of two division ratios of triples of points:\n\nThe cross-ratio is normally extended to the case when one of \"z\", \"z\", \"z\", \"z\" is infinity formula_4 this is done by removing the corresponding two differences from the formula.\n\nFor example: if formula_5 the cross ratio becomes: \n\nIn geometry, if \"A\", \"B\", \"C\" and \"D\" are collinear points, then the cross ratio is defined similarly as\nwhere each of the distances is signed according to a consistent orientation of the line.\n\nThe same formulas can be applied to four different complex numbers or, more generally, to elements of any field and can also be extended to the case when one of them is the symbol ∞, by removing the corresponding two differences from the formula.\nThe formula shows that cross-ratio is a function of four points, generally four numbers formula_8 taken from a field.\n\nThe cross ratio of the four collinear points \"A\", \"B\", \"C\", \"D\" can be written as\nwhere formula_10 describes the ratio with which the point \"C\" divides the line segment \"AB\", and formula_11 describes the ratio with which the point \"D\" divides that same line segment. The cross ratio then appears as a ratio of ratios, describing how the two points \"C\", \"D\" are situated with respect to the line segment \"AB\". As long as the points \"A\", \"B\", \"C\" and \"D\" are distinct, the cross ratio (\"A\", \"B\"; \"C\", \"D\") will be a non-zero real number. We can easily deduce that\n\nFour points can be ordered in 4! = 4 x 3 x 2 x 1 = 24 ways, but there are only six ways for partitioning them into two non-ordered pairs. Thus, four points can have only six different cross-ratios, which are related as:\nformula_12\n\nformula_13\n\nformula_14\n\nformula_15\n\nformula_16\n\nformula_17\n\nCross-ratio is a projective invariant in the sense that it is preserved by the projective transformations of a projective line.\n\nIn particular, if four points lie on a straight line \"L\" in R then their cross-ratio is a well-defined quantity, because any choice of the origin and even of the scale on the line will yield the same value of the cross-ratio.\n\nFurthermore, let be four distinct lines in the plane passing through the same point \"Q\". Then any line \"L\" not passing through \"Q\" intersects these lines in four distinct points \"P\" (if \"L\" is parallel to \"L\" then the corresponding intersection point is \"at infinity\"). It turns out that the cross-ratio of these points (taken in a fixed order) does not depend on the choice of a line \"L\", and hence it is an invariant of the 4-tuple of lines {\"L\"}. \n\nThis can be understood as follows: if \"L\" and \"L\"′ are two lines not passing through \"Q\" then the perspective transformation from \"L\" to \"L\"′ with the center \"Q\" is a projective transformation that takes the quadruple {\"P\"} of points on \"L\" into the quadruple {\"P\"′} of points on \"L\"′.\n\nTherefore, the invariance of the cross-ratio under projective automorphisms of the line implies (in fact, is equivalent to) the independence of the cross-ratio of the four collinear points {\"P\"} on the lines {\"L\"} from the choice of the line that contains them.\n\nIf four collinear points are represented in homogeneous coordinates by vectors \"a\", \"b\", \"c\", \"d\" such that and , then their cross-ratio is \"k\".\n\nArthur Cayley and Felix Klein found an application of the cross-ratio to non-Euclidean geometry. Given a nonsingular conic \"C\" in the real projective plane, its stabilizer \"G\" in the projective group acts transitively on the points in the interior of \"C\". However, there is an invariant for the action of \"G\" on \"pairs\" of points. In fact, every such invariant is expressible as a function of the appropriate cross ratio.\n\nExplicitly, let the conic be the unit circle. For any two points \"P\", \"Q\", inside the unit circle . If the line connecting them intersects the circle in two points, \"X\" and \"Y\" and the points are, in order, . Then the hyperbolic distance between \"P\" and \"Q\" in the Cayley–Klein model of the hyperbolic plane can be expressed as\n\n(the factor one half is needed to make the curvature −1). Since the cross-ratio is invariant under projective transformations, it follows that the hyperbolic distance is invariant under the projective transformations that preserve the conic \"C\".\n\nConversely, the group \"G\" acts transitively on the set of pairs of points in the unit disk at a fixed hyperbolic distance.\n\nLater, partly through the influence of Henri Poincaré, the cross ratio of four complex numbers on a circle was used for hyperbolic metrics. Being on a circle means the four points are the image of four real points under a Möbius transformation, and hence the cross ratio is a real number. The Poincaré half-plane model and Poincaré disk model are two models of hyperbolic geometry in the complex projective line.\n\nThese models are instances of Cayley–Klein metrics.\n\nThe cross-ratio may be defined by any of these four expressions:\n\nThese differ by the following permutations of the variables:\n\nThese three and the identity permutation leave the cross ratio unaltered. They make up a realization of the Klein four-group, a group of order 4 in which the order of every non-identity element is 2.\n\nOther permutations of the four variables alter the cross-ratio so that it may take any of the following six values.\n\nAs functions of \"λ\", these form a non-abelian group of order 6 with the operation of composition of functions. This is the anharmonic group. It is a subgroup of the group of all Möbius transformations. The six cross-ratios listed above represent torsion elements (geometrically, elliptic transforms) of . Namely, formula_22, formula_23, and formula_24 are of order 2 in , with fixed points, respectively, −1, 1/2, and 2 (namely, the orbit of the harmonic cross-ratio). Meanwhile, elements\nformula_25 and formula_26 are of order 3 in – in (this corresponds to the subgroup A of even elements). Each of them fixes both values formula_27 of the \"most symmetric\" cross-ratio.\n\nThe anharmonic group is generated by and . Its action on gives an isomorphism with S. It may also be realised as the six Möbius transformations mentioned, which yields a projective representation of S over any field (since it is defined with integer entries), and is always faithful/injective (since no two terms differ only by 1/−1). Over the field with two elements, the projective line only has three points, so this representation is an isomorphism, and is the exceptional isomorphism formula_28. In characteristic 3, this stabilizes the point formula_29, which corresponds to the orbit of the harmonic cross-ratio being only a single point, since formula_30. Over the field with 3 elements, the projective line has only 4 points and formula_31, and thus the representation is exactly the stabilizer of the harmonic cross-ratio, yielding an embedding formula_32 equals the stabilizer of the point formula_33.\n\nIn the language of group theory, the symmetric group S acts on the cross-ratio by permuting coordinates. The kernel of this action is isomorphic to the Klein four-group K. This group consists of 2-cycle permutations of type formula_34 (in addition to the identity), which preserve the cross-ratio. The effective symmetry group is then the quotient group formula_35, which is isomorphic to S.\n\nFor certain values of \"λ\" there will be greater symmetry and therefore fewer than six possible values for the cross-ratio. These values of \"λ\" correspond to fixed points of the action of S on the Riemann sphere (given by the above six functions); or, equivalently, those points with a non-trivial stabilizer in this permutation group.\n\nThe first set of fixed points is However, the cross-ratio can never take on these values if the points \"A\", \"B\", \"C\" and \"D\" are all distinct. These values are limit values as one pair of coordinates approach each other:\n\nThe second set of fixed points is This situation is what is classically called the , and arises in projective harmonic conjugates. In the real case, there are no other exceptional orbits.\n\nIn the complex case, the most symmetric cross-ratio occurs when formula_39. These are then the only two values of the cross-ratio, and these are acted on according to the sign of the permutation.\n\nThe cross-ratio is invariant under the projective transformations of the line. In the case of a complex projective line, or the Riemann sphere, these transformations are known as Möbius transformations. A general Möbius transformation has the form\n\nThese transformations form a group acting on the Riemann sphere, the Möbius group.\n\nThe projective invariance of the cross-ratio means that\n\nThe cross-ratio is real if and only if the four points are either collinear or concyclic, reflecting the fact that every Möbius transformation maps generalized circles to generalized circles.\n\nThe action of the Möbius group is simply transitive on the set of triples of distinct points of the Riemann sphere: given any ordered triple of distinct points, , there is a unique Möbius transformation \"f\"(\"z\") that maps it to the triple . This transformation can be conveniently described using the cross-ratio: since must equal , which in turn equals \"f\"(\"z\"), we obtain\n\nAn alternative explanation for the invariance of the cross-ratio is based on the fact that the group of projective transformations of a line is generated by the translations, the homotheties, and the multiplicative inversion. The differences are invariant under the translations\n\nwhere \"a\" is a constant in the ground field \"F\". Furthermore, the division ratios are invariant under a homothety\n\nfor a non-zero constant \"b\" in \"F\". Therefore, the cross-ratio is invariant under the affine transformations.\n\nIn order to obtain a well-defined inversion mapping\n\nthe affine line needs to be augmented by the point at infinity, denoted ∞, forming the projective line \"P\"(\"F\"). Each affine mapping can be uniquely extended to a mapping of \"P\"(\"F\") into itself that fixes the point at infinity. The map \"T\" swaps 0 and ∞. The projective group is generated by \"T\" and the affine mappings extended to \"P\"(\"F\"). In the case , the complex plane, this results in the Möbius group. Since the cross-ratio is also invariant under \"T\", it is invariant under any projective mapping of \"P\"(\"F\") into itself.\n\nIf we write the complex points as vectors formula_46 and define formula_47. Let formula_48 be the dot product of formula_49 with formula_50 then the real part of the cross ratio is given by:\n\nThis is an invariant of the 2D special conformal transformation such as inversion formula_52.\n\nThe imaginary part must make use of the 2-dimensional cross product formula_53\n\nThe concept of cross ratio only depends on the ring operations of addition, multiplication, and inversion (though inversion of a given element is not certain in a ring). One approach to cross ratio interprets it as a homography that takes three designated points to 0, 1, and infinity. Under restrictions having to do with inverses, it is possible to generate such a mapping with ring operations in the projective line over a ring. The cross ratio of four points is the evaluation of this homography at the fourth point.\n\nThe theory takes on a differential calculus aspect as the four points are brought into proximity. This leads to the theory of the Schwarzian derivative, and more generally of projective connections.\n\nThe cross-ratio does not generalize in a simple manner to higher dimensions, due to other geometric properties of configurations of points, notably collinearity – configuration spaces are more complicated, and distinct \"k\"-tuples of points are not in general position.\n\nWhile the projective linear group of the plane is 3-transitive (any three distinct points can be mapped to any other three points), and indeed simply 3-transitive (there is a \"unique\" projective map taking any triple to another triple), with the cross ratio thus being the unique projective invariant of a set of four points, there are basic geometric invariants in higher dimension. The projective linear group of \"n\"-space formula_55 has (\"n\" + 1) − 1 dimensions (because it is formula_56 projectivization removing one dimension), but in other dimensions the projective linear group is only 2-transitive – because three collinear points must be mapped to three collinear points (which is not a restriction in the projective line) – and thus there is not a \"generalized cross ratio\" providing the unique invariant of \"n\" points.\n\nCollinearity is not the only geometric property of configurations of points that must be maintained – for example, five points determine a conic, but six general points do not lie on a conic, so whether any 6-tuple of points lies on a conic is also a projective invariant. One can study orbits of points in general position – in the line \"general position\" is equivalent to being distinct, while in higher dimensions it requires geometric considerations, as discussed – but, as the above indicates, this is more complicated and less informative.\n\nHowever, a generalization to Riemann surfaces of positive genus exists, using the Abel–Jacobi map and theta functions.\n\n\n\n"}
{"id": "42043747", "url": "https://en.wikipedia.org/wiki?curid=42043747", "title": "Cut-insertion theorem", "text": "Cut-insertion theorem\n\nThe Cut-insertion theorem, also known as Pellegrini's theorem, is a linear network theorem that allows transformation of a generic network N into another\nnetwork N' that makes analysis simpler and for which the main properties are more apparent.\n\nLet \"e\", \"h\", \"u\", \"w\", \"q=q\"', and \" t=t' \" be six arbitrary nodes of the network N and \"formula_1\" be an independent voltage or current source connected between \"e\" and \"h\", while \"formula_2\" is the output quantity, either a voltage or current, relative to the branch with immittance formula_3, connected between \"u\" and \"w\". Let us now cut the \" qq' \" connection and insert a three-terminal circuit (\"TTC\") between the two nodes \"q\" and \"q' \" and the node \" t=t' \", as in figure b (formula_4 and formula_5 are homogeneous quantities, voltages or currents, relative to the ports \"qt\" and \" q'q't' \" of the TTC).\n\nIn order for the two networks N and N' to be equivalent for any formula_1, the two constraints formula_7 and formula_8, where the overline indicates the dual quantity, are to be satisfied.\n\nThe above mentioned three-terminal circuit can be implemented, for example, connecting an ideal independent voltage or current source formula_5 between \"q' \" and \" t' \", and an immittance formula_10 between \"q\" and \"t\".\n\nWith reference to the network N', the following network functions can be defined:\n\nformula_11 ; formula_12 ; formula_13\n\nformula_14 ; formula_15 ; formula_16\n\nfrom which, exploiting the Superposition theorem, we obtain:\n\nformula_17\n\nformula_18.\n\nTherefore the first constraint for the equivalence of the networks is satisfied if formula_19.\n\nFurthermore,\n\nformula_20\n\nformula_21\n\ntherefore the second constraint for the equivalence of the networks holds if formula_22\n\nIf we consider the expressions for the network functions formula_23 and formula_24, the first constraint for the equivalence of the networks, and we also consider that, as a result of the superposition principle, formula_25, the transfer function formula_26 is given by\n\nformula_27.\n\nFor the particular case of a feedback amplifier, the network functions formula_28, formula_23 and formula_30 take into account the nonidealities of such amplifier. In particular:\n\n\nIf the amplifier can be considered ideal, i.e. if formula_34, formula_35 and formula_36, the transfer function reduces to the known expression deriving from classical feedback theory:\n\nformula_37.\n\nThe evaluation of the impedance (or of the admittance) between two nodes is made somewhat simpler by the cut-insertion theorem.\n\nLet us insert a generic source formula_1 between the nodes \"j=e=q\" and \"k=h\" between which we want to evaluate the impedance formula_39. By performing a cut as shown in the figure, we notice that the immittance formula_10 is in series with formula_1 and the current through it is thus the same as that provided by formula_1. If we choose an input voltage source formula_43 and, as a consequence, a current formula_44, and an impedance formula_45, we can write the following relationships:\n\nformula_46.\n\nConsidering that formula_47, where formula_48 is the impedance seen between the nodes \"k=h\" and \"t\" if remove formula_49 and short-circuit the voltage sources, we obtain the impedance formula_39 between the nodes \"j\" and \"k\" in the form:\n\nformula_51\n\nWe proceed in a way analogous to the impedance case, but this time the cut will be as shown in the figure to the right, noticing that formula_1 is now in parallel to formula_10. If we consider an input current source formula_54 (as a result we have a voltage formula_55) and an admittance formula_56, the admittance formula_57 between the nodes \"j\" and \"k\" can be computed as follows:\n\nformula_58.\n\nConsidering that formula_59, where formula_60 is the admittance seen between the nodes \"k=h\" and \"t\" if we remove formula_61 and open the current sources, we obtain the admittance formula_57 in the form:\n\nformula_63\n\nThe implementation of the TTC with an independent source formula_5 and an immittance formula_10 is useful and intuitive for the calculation of the impedance between two nodes, but involves, as in the case of the other network functions, the difficulty of the calculation of formula_10 from the equivalence equation. Such difficulty can be avoided using a dependent source formula_67 in place of formula_10 and using the Blackman formula for the evaluation of formula_69. Such an implementation of the TTC allows finding a feedback topology even in a network consisting of a voltage source and two impedances in series.\n\n\n"}
{"id": "253418", "url": "https://en.wikipedia.org/wiki?curid=253418", "title": "DNA computing", "text": "DNA computing\n\nDNA computing is a branch of computing which uses DNA, biochemistry, and molecular biology hardware, instead of the traditional silicon-based computer technologies. Research and development in this area concerns theory, experiments, and applications of DNA computing. The term \"molectronics\" has sometimes been used, but this term had already been used for an earlier technology, a then-unsuccessful rival of the first integrated circuits; this term has also been used more generally, for molecular-scale electronic technology.\n\nThis field was initially developed by Leonard Adleman of the University of Southern California, in 1994. Adleman demonstrated a proof-of-concept use of DNA as a form of computation which solved the seven-point Hamiltonian path problem. Since the initial Adleman experiments, advances have been made and various Turing machines have been proven to be constructible.\n\nWhile the initial interest was in using this novel approach to tackle NP-hard problems, it was soon realized that they may not be best suited for this type of computation, and several proposals have been made to find a \"killer application\" for this approach. In 1997, computer scientist Mitsunori Ogihara working with biologist Animesh Ray suggested one to be the evaluation of Boolean circuits and described an implementation.\n\nIn 2002, researchers from the Weizmann Institute of Science in Rehovot, Israel, unveiled a programmable molecular computing machine composed of enzymes and DNA molecules instead of silicon microchips. On April 28, 2004, Ehud Shapiro, Yaakov Benenson, Binyamin Gil, Uri Ben-Dor, and Rivka Adar at the Weizmann Institute announced in the journal Nature that they had constructed a DNA computer coupled with an input and output module which would theoretically be capable of diagnosing cancerous activity within a cell, and releasing an anti-cancer drug upon diagnosis.\n\nIn January 2013, researchers were able to store a JPEG photograph, a set of Shakespearean sonnets, and an audio file of Martin Luther King, Jr.'s speech I Have a Dream on DNA digital data storage.\n\nIn March 2013, researchers created a transcriptor (a biological transistor).\n\nIn August 2016, researchers used the CRISPR gene-editing system to insert a GIF of a galloping horse and rider into the DNA of living bacteria.\n\nThe organisation and complexity of all living beings is based on a coding system functioning with four key components of the DNA-molecule. Because of this, the DNA is very suited as a medium for data processing. According to different calculations a DNA-computer with one liter of fluid containing six grams of DNA could potentially have a memory capacity of 3072 exabytes. The theoretical maximum data transfer speed would also be enormous due to the massive parallelism of the calculations. Therefore, about 1000 petaFLOPS could be reached, while today's most powerful computers do not go above a few dozen (99 petaFLOPS being the current record).\n\nThe slow processing speed of a DNA-computer (the response time is measured in minutes, hours or days, rather than milliseconds) is compensated by its potential to make a high amount of multiple parallel computations. This allows the system to take a similar amount of time for a complex calculation as for a simple one. This is achieved by the fact that millions or billions of molecules interact with each other simultaneously. However, it is much harder to analyze the answers given by a DNA-Computer than by a digital one.\n\nIn 1994 Leonard Adleman presented the first prototype of a DNA-Computer. The was a test tube filled with 100 microliters of a DNA-solution. He managed to solve for example an instance of the directed Hamiltonian path problem.\n\nIn another experiment a simple version of the “travelling salesman problem” was “solved”. For this purpose, different DNA-fragments were created, each one of them representing a city that had to be visited. Every one of these fragments is capable of a linkage with the other fragments created. These DNA-fragments were produced and mixed in a test tube. Within seconds, the small fragments form bigger ones, representing the different travel routes. Through a chemical reaction (that lasts a few days), the DNA-fragments representing the longer routes were eliminated. The remains are the solution to the problem. However, current technical limitations prevent evaluation of the results. Therefore, the experiment isn’t suitable for application, but it is nevertheless a proof of concept.\n\nFirst results to these problems were obtained by Leonard Adleman (NASA JPL)\n\nIn 2002, J. Macdonald, D. Stefanovic and Mr. Stojanovic created a DNA computer able to play tic-tac-toe against a human player. The calculator consists of nine bins corresponding to the nine squares of the game. Each bin contains a substrate and various combinations of DNA enzymes. The substrate itself is composed of a DNA strand onto which was grafted a fluorescent chemical group at one end, and the other end, a repressor group. Fluorescence is only active if the molecules of the substrate are halved. The DNA enzyme simulate logical functions. For example, such a DNA will unfold if two specific types of DNA strand are introduced to reproduce the logic function AND.\n\nBy default, the computer is supposed to play first in the central square. The human player has then as a starter eight different types of DNA strands assigned to each of eight boxes that may be played. To indicate that box nr. i is being ticked, the human player pours into all bins the strands corresponding to input #i. These strands bind to certain DNA enzymes present in the bins, resulting in one of these two bins in the deformation of the DNA enzymes which binds to the substrate and cuts it. The corresponding bin becomes fluorescent, indicating which box is being played by the DNA computer. The various DNA enzymes are divided into various bins in such a way to ensure the victory of the DNA computer against the human player.\n\nKevin Cherry and Lulu Qian at Caltech developed a DNA-based artificial neural network that can recognize 100-bit hand-written digits. They achieve this by programming on computer in advance with appropriate set of weights represented by varying concentrations weight molecules which will later be added to the test tube that holds the input DNA strands.\n\nDNA computing is a form of parallel computing in that it takes advantage of the many different molecules of DNA to try many different possibilities at once. For certain specialized problems, DNA computers are faster and smaller than any other computer built so far. Furthermore, particular mathematical computations have been demonstrated to work on a DNA computer. As an example, DNA molecules have been utilized to tackle the assignment problem.\n\nJian-Jun Shu and colleagues built a DNA GPS system and also conduct an experiment to show that magnetic fields can enhance charge transport through DNA (or protein), which may allow organisms to sense magnetic fields.\n\nAran Nayebi has provided a general implementation of Strassen's matrix multiplication algorithm on a DNA computer, although there are problems with scaling. In addition, Caltech researchers have created a circuit made from 130 unique DNA strands, which is able to calculate the square root of numbers up to 15. Recently, Salehi et al. showed that with a new coding referred to as \"fractional coding\", chemical reactions in general and DNA reactions in particular, can compute polynomials. In the fractional coding two DNA molecules are used to represent each variable.\n\nDNA computing does not provide any new capabilities from the standpoint of computability theory, the study of which problems are computationally solvable using different models of computation.\nFor example,\nif the space required for the solution of a problem grows exponentially with the size of the problem (EXPSPACE problems) on von Neumann machines, it still grows exponentially with the size of the problem on DNA machines.\nFor very large EXPSPACE problems, the amount of DNA required is too large to be practical.\n\nThere are multiple methods for building a computing device based on DNA, each with its own advantages and disadvantages. Most of these build the basic logic gates (AND, OR, NOT) associated with digital logic from a DNA basis. Some of the different bases include DNAzymes, deoxyoligonucleotides, enzymes, toehold exchange.\n\nCatalytic DNA (deoxyribozyme or DNAzyme) catalyze a reaction when interacting with the appropriate input, such as a matching oligonucleotide. These DNAzymes are used to build logic gates analogous to digital logic in silicon; however, DNAzymes are limited to 1-, 2-, and 3-input gates with no current implementation for evaluating statements in series.\n\nThe DNAzyme logic gate changes its structure when it binds to a matching oligonucleotide and the fluorogenic substrate it is bonded to is cleaved free. While other materials can be used, most models use a fluorescence-based substrate because it is very easy to detect, even at the single molecule limit. The amount of fluorescence can then be measured to tell whether or not a reaction took place. The DNAzyme that changes is then “used,” and cannot initiate any more reactions. Because of this, these reactions take place in a device such as a continuous stirred-tank reactor, where old product is removed and new molecules added.\n\nTwo commonly used DNAzymes are named E6 and 8-17. These are popular because they allow cleaving of a substrate in any arbitrary location. Stojanovic and MacDonald have used the E6 DNAzymes to build the MAYA I and MAYA II machines, respectively; Stojanovic has also demonstrated logic gates using the 8-17 DNAzyme. While these DNAzymes have been demonstrated to be useful for constructing logic gates, they are limited by the need for a metal cofactor to function, such as Zn or Mn, and thus are not useful in vivo.\n\nA design called a \"stem loop\", consisting of a single strand of DNA which has a loop at an end, are a dynamic structure that opens and closes when a piece of DNA bonds to the loop part. This effect has been exploited to create several logic gates. These logic gates have been used to create the computers MAYA I and MAYA II which can play tic-tac-toe to some extent.\n\nEnzyme based DNA computers are usually of the form of a simple Turing machine; there is analogous hardware, in the form of an enzyme, and software, in the form of DNA.\nBenenson, Shapiro and colleagues have demonstrated a DNA computer using the FokI enzyme and expanded on their work by going on to show automata that diagnose and react to prostate cancer: under expression of the genes PPAP2B and GSTP1 and an over expression of PIM1 and HPN. Their automata evaluated the expression of each gene, one gene at a time, and on positive diagnosis then released a single strand DNA molecule (ssDNA) that is an antisense for MDM2. MDM2 is a repressor of protein 53, which itself is a tumor suppressor. On negative diagnosis it was decided to release a suppressor of the positive diagnosis drug instead of doing nothing. A limitation of this implementation is that two separate automata are required, one to administer each drug. The entire process of evaluation until drug release took around an hour to complete. This method also requires transition molecules as well as the FokI enzyme to be present. The requirement for the FokI enzyme limits application \"in vivo\", at least for use in \"cells of higher organisms\". It should also be pointed out that the 'software' molecules can be reused in this case.\n\nDNA computers have also been constructed using the concept of toehold exchange. In this system, an input DNA strand binds to a sticky end, or toehold, on another DNA molecule, which allows it to displace another strand segment from the molecule. This allows the creation of modular logic components such as AND, OR, and NOT gates and signal amplifiers, which can be linked into arbitrarily large computers. This class of DNA computers does not require enzymes or any chemical capability of the DNA.\n\nDNA nanotechnology has been applied to the related field of DNA computing. DNA tiles can be designed to contain multiple sticky ends with sequences chosen so that they act as Wang tiles. A DX array has been demonstrated whose assembly encodes an XOR operation; this allows the DNA array to implement a cellular automaton which generates a fractal called the Sierpinski gasket. This shows that computation can be incorporated into the assembly of DNA arrays, increasing its scope beyond simple periodic arrays.\n\nA partnership between IBM and Caltech was established in 2009 aiming at \"DNA chips\" production. A Caltech group is working on the manufacturing of these nucleic-acid-based integrated circuits. One of these chips can compute whole square roots. A compiler has been written in Perl.\n\n"}
{"id": "4036694", "url": "https://en.wikipedia.org/wiki?curid=4036694", "title": "Densely packed decimal", "text": "Densely packed decimal\n\nDensely packed decimal (DPD) is an efficient method for binary encoding decimal digits.\n\nThe traditional system of binary encoding for decimal digits, known as binary-coded decimal (BCD), uses four bits to encode each digit, resulting in significant wastage of binary data bandwidth (since four bits can store 16 states and are being used to store only 10). Densely packed decimal is a more efficient code that packs three digits into ten bits using a scheme that allows compression from, or expansion to, BCD with only two or three gate delays in hardware.\n\nThe densely packed decimal encoding is a refinement of Chen–Ho encoding; it gives the same compression and speed advantages, but the particular arrangement of bits used confers additional advantages:\n\nIn 1971, Tien Chi Chen and Irving Tze Ho devised a lossless prefix code (known as Chen–Ho encoding since 2000) which packed three decimal digits into ten binary bits using a scheme which allowed compression from or expansion to BCD with only two or three gate delays in hardware. Densely packed decimal is a refinement of this, devised by Mike F. Cowlishaw in 2002, which was incorporated into the IEEE 754-2008 and standards for decimal floating-point.\n\nLike Chen–Ho encoding, DPD encoding classifies each decimal digit into one of two ranges, depending on the most significant bit of the binary form: \"small\" digits have values 0 through 7 (binary 0000–0111), and \"large\" digits, 8 through 9 (binary 1000–1001). Once it is known or has been indicated that a digit is small, three more bits are still required to specify the value. If a large value has been indicated, only one bit is required to distinguish between the values 8 or 9.\n\nWhen encoding, the most significant bit of each of the three digits to be encoded select one of eight coding patterns for the remaining bits, according to the following table. The table shows how, on decoding, the ten bits of the coded form in columns \"b9\" through \"b0\" are copied into the three digits \"d2\" through \"d0\", and the remaining bits are filled in with constant zeros or ones.\n\nBits b7, b4 and b0 (codice_1, codice_2 and codice_3) are passed through the encoding unchanged, and do not affect the meaning of the other bits. The remaining seven bits can be considered a seven-bit encoding for three base-5 digits.\n\nBits b8 and b9 are not needed and ignored when decoding DPD groups with three large digits (marked as \"x\" in the last row of the table above), but are filled with zeros when encoding.\n\nThe eight decimal values whose digits are all 8s or 9s have four codings each.\nThe bits marked x in the table above are ignored on input, but will always be 0 in computed results.\n\nThis table shows some representative decimal numbers and their encodings in BCD, Chen–Ho, and densely packed decimal (DPD):\n\n\n"}
{"id": "2838322", "url": "https://en.wikipedia.org/wiki?curid=2838322", "title": "Dusa McDuff", "text": "Dusa McDuff\n\nDusa McDuff FRS CorrFRSE (born 18 October 1945) is an English mathematician who works on symplectic geometry. She was the first recipient of the Ruth Lyttle Satter Prize in Mathematics, was a Noether Lecturer, and is a Fellow of the Royal Society.\n\nMargaret Dusa Waddington was born in London, England, on 18 October 1945 to Edinburgh architect Margaret Justin Blanco White, second wife of biologist Conrad Hal Waddington, her father. Her sister is the anthropologist Caroline Humphrey, and she has an elder half-brother C. Jake Waddington by her father's first marriage. Her mother was the daughter of Amber Reeves, the noted feminist, author and lover of H. G. Wells. McDuff grew up in Scotland where her father was Professor of Genetics at the University of Edinburgh. McDuff was educated at St. George's School for Girls in Edinburgh and, although the standard was lower than at the corresponding boys' school, The Edinburgh Academy, McDuff had an exceptionally good mathematics teacher. She writes:\n\nI always wanted to be a mathematician (apart from a time when I was eleven when I wanted to be a farmer's wife), and assumed that I would have a career, but I had no idea how to go about it: I didn't realize that the choices which one made about education were important and I had no idea that I might experience real difficulties and conflicts in reconciling the demands of a career with life as a woman.\n\nTurning down a scholarship to the University of Cambridge to stay with her boyfriend in Scotland, she enrolled at the University of Edinburgh. She graduated with a BSc Hons in 1967, going on to Girton College, Cambridge as a doctoral student. Here, under the guidance of mathematician George A. Reid, McDuff worked on problems in functional analysis. She solved a problem on Von Neumann algebras, constructing infinitely many different factors of type II, and published the work in the \"Annals of Mathematics\".\n\nAfter completing her doctorate in 1971 McDuff was appointed to a two-year Science Research Council Postdoctoral Fellowship at Cambridge. Following her husband, the literary translator David McDuff, she left for a six-month visit to Moscow. Her husband was studying the Russian Symbolist poet Innokenty Annensky. Though McDuff had no specific plans it turned out to be a profitable visit for her mathematically. There, she met Israel Gelfand in Moscow who gave her a deeper appreciation of mathematics. McDuff later wrote:\n[My collaboration with him]... was not planned: it happened that his was the only name which came to mind when I had to fill out a form in the Inotdel office. The first thing that Gel'fand told me was that he was much more interested in the fact that my husband was studying the Russian Symbolist poet Innokenty Annensky than that I had found infinitely many type II-sub-one factors, but then he proceeded to open my eyes to the world of mathematics. It was a wonderful education, in which reading Pushkin, Mozart and Salieri played as important a role as learning about Lie groups or reading Cartan and Eilenberg. Gel'fand amazed me by talking of mathematics as though it were poetry. He once said about a long paper bristling with formulas that it contained the vague beginnings of an idea which he could only hint at and which he had never managed to bring out more clearly. I had always thought of mathematics as being much more straightforward: a formula is a formula, and an algebra is an algebra, but Gel'fand found hedgehogs lurking in the rows of his spectral sequences!\n\nOn returning to Cambridge McDuff started attending Frank Adams's topology lectures and was soon invited to teach at the University of York. In 1975 she separated from her husband, and was divorced in 1978. At the University of York, she \"essentially wrote a second PhD\" while working with Graeme Segal. At this time a position at Massachusetts Institute of Technology (MIT) opened up for her, reserved for visiting female mathematicians. Her career as a mathematician developed further while at MIT, and soon she was accepted to the Institute for Advanced Study where she worked with Segal on the Atiyah–Segal completion theorem. She then returned to England where she was took up a lectureship at the University of Warwick.\n\nAround this time she met mathematician John Milnor who was then based in Princeton University. To live closer to him she took up an untenured assistant professorship at the Stony Brook University. Now an independent mathematician, she began work on the relationship between diffeomorphisms and the classifying space for foliations. She has since worked on symplectic topology. In the spring of 1985, McDuff attended the Institut des Hautes Études Scientifiques in Paris to study Mikhael Gromov's work on elliptic methods. Since 2007, she has held the Helen Lyttle Kimmel chair at Barnard College.\n\nIn 1984 McDuff married John Milnor, a mathematician at Stony Brook University, and a Fields medallist, Wolf Prize winner and Abel Prize Laureate.\n\nFor the past 30 years McDuff has been a contributor to the development of the field of symplectic geometry and topology. She gave the first example of symplectic forms on a closed manifold that are cohomologous but not diffeomorphic and also classified the rational and ruled symplectic four-manifolds, completed with François Lalonde. More recently, partly in collaboration with Susan Tolman, she has studied applications of methods of symplectic topology to the theory of Hamiltonian torus actions. She has also worked on embedding capacities of 4-dimensional symplectic ellipsoids with Felix Schlenk, which gives rise to some very interesting number-theoretical questions. It also indicates a connection between the combinatorics of J-holomorphic curves in the blow up of the projective plane and the numbers that appear as indices in embedded contact homology. With Katrin Wehrheim, she has challenged the foundational rigor of a classic proof in symplectic geometry.\n\nWith , she co-authored two textbooks \"Introduction to Symplectic Topology\" and \"J-Holomorphic Curves and Symplectic Topology\".\n\nMcDuff was the first to be awarded the Satter Prize, in 1991, for her work on symplectic geometry; she is a Fellow of the Royal Society (1994), a Noether Lecturer (1998) and a member of the United States National Academy of Sciences (1999). In 2008 she was elected a Corresponding Fellow of the Royal Society of Edinburgh. She was a Plenary Lecturer at the 1998 International Congress of Mathematicians. In 2012 she became a fellow of the American Mathematical Society. In 1999, she was the first female Hardy Lecturer, an award from the London Mathematical Society. She is also a member of the Academia Europaea, \nand is part of the 2019 class of fellows of the Association for Women in Mathematics.\n\nIn 2010, she was awarded the Senior Berwick Prize of the London Mathematical Society. For 2017 she will receive the AMS Leroy P. Steele Prize for Mathematical Exposition. In 2018 she received the Sylvester Medal by the Royal Society. \n\n\n"}
{"id": "2891127", "url": "https://en.wikipedia.org/wiki?curid=2891127", "title": "Equity value", "text": "Equity value\n\nEquity value is the value of a company available to owners or shareholders. It is the enterprise value plus all cash and cash equivalents, short and long-term investments, and less all short-term debt, long-term debt and minority interests.\n\nEquity value accounts for all the ownership interest in a firm including the value of unexercised stock options and securities convertible to equity. \n\nFrom a mergers and acquisitions to an academic perspective, equity value differs from market capitalization or market value in that it incorporates all equity interests in a firm whereas market capitalization or market value only reflects those common shares currently outstanding.\n\nEquity value can be calculated two ways, either the intrinsic value method, or the fair market value method. The intrinsic value method is calculated as follows: \n\nThe fair market value method is as follows:\n\nThe fair market value method more accurately captures the value of out of the money securities.\n"}
{"id": "43834296", "url": "https://en.wikipedia.org/wiki?curid=43834296", "title": "Extension of a topological group", "text": "Extension of a topological group\n\nIn mathematics, more specifically in topological groups, an extension of topological groups, or a topological extension, is a short exact sequence formula_1 where formula_2 and formula_3 are topological groups and formula_4 and formula_5 are continuous homomorphisms which are also open onto their images. Every extension of topological groups is therefore a group extension.\n\nWe say that the topological extensions \nand \nare equivalent (or congruent) if there exists a topological isomorphism formula_8 making commutative the diagram of Figure 1.\n\nWe say that the topological extension\n\nis a \"split extension\" (or splits) if it is equivalent to the trivial extension\n\nwhere formula_11 is the natural inclusion over the first factor and formula_12 is the natural projection over the second factor.\n\nIt is easy to prove that the topological extension formula_6 splits if and only if there is a continuous homomorphism formula_14 such that formula_15 is the identity map on formula_16\n\nNote that the topological extension formula_6 splits if and only if the subgroup formula_18 is a topological direct summand of formula_19\n\n\nAn extension of topological abelian groups will be a short exact sequence formula_1 where formula_2 and formula_3 are locally compact abelian groups and formula_4 and formula_5 are relatively open continuous homomorphisms.\n"}
{"id": "223160", "url": "https://en.wikipedia.org/wiki?curid=223160", "title": "Gantt chart", "text": "Gantt chart\n\nA Gantt chart is a type of bar chart that illustrates a project schedule, named after its inventor, Henry Gantt (1861–1919), who designed such a chart around the years 1910–1915. Modern Gantt charts also show the dependency relationships between activities and current schedule status.\n\nA Gantt chart is a type of bar chart that illustrates a project schedule. This chart lists the tasks to be performed on the vertical axis, and time intervals on the horizontal axis. The width of the horizontal bars in the graph shows the duration of each activity. Gantt charts illustrate the start and finish dates of the terminal elements and summary elements of a project. Terminal elements and summary elements constitute the work breakdown structure of the project. Modern Gantt charts also show the dependency (i.e., precedence network) relationships between activities. Gantt charts can be used to show current schedule status using percent-complete shadings and a vertical \"TODAY\" line as shown here.\n\nGantt charts are sometimes equated with bar charts.\n\nGantt charts are usually created initially using an \"early start time approach\", where each task is scheduled to start immediately when its prerequisites are complete. This method maximizes the float time available for all tasks.\n\nAlthough now regarded as a common charting technique, Gantt charts were considered revolutionary when first introduced. The first known tool of this type was developed in 1896 by Karol Adamiecki, who called it a \"harmonogram\". Adamiecki did not publish his chart until 1931, however, and only in Polish, which limited both its adoption and recognition of his authorship.\n\nIn 1912, published what would be considered Gantt charts while discussing a construction project. It appears that Schürch's charts were not notable but rather routine in Germany at the time they were published. The prior development leading to Schürch's work is unknown. Unlike later Gantt charts, Schürch's charts did not display interdependencies, leaving them to be inferred by the reader. These were also static representations of a planned schedule.\n\nThe chart is named after Henry Gantt (1861–1919), who designed his chart around the years 1910–1915.\n\nOne of the first major applications of Gantt charts was by the United States during World War I, at the instigation of General William Crozier.\n\nThe earliest Gantt charts were drawn on paper and therefore had to be redrawn entirely in order to adjust to schedule changes. For many years, project managers used pieces of paper or blocks for Gantt chart bars so they could be adjusted as needed. Gantt's collaborator, Walter Polakov introduced Gantt charts to the Soviet Union in 1929 when he was working for the Supreme Soviet of the National Economy. They were used in developing the First Five Year Plan, supplying Russian translations to explain their use..\n\nIn the 1980s, personal computers allowed widespread creation of complex and elaborate Gantt charts. The first desktop applications were intended mainly for project managers and project schedulers. With the advent of the Internet and increased collaboration over networks at the end of the 1990s, Gantt charts became a common feature of web-based applications, including collaborative groupware. By 2012, almost all Gantt charts were made by software which can easily adjust to schedule changes.\n\nIn 1999, Gantt charts were identified as \"one of the most widely used management tools for project scheduling and control\".\n\nIn the following table there are seven tasks, labeled \"a\" through \"g\". Some tasks can be done concurrently (\"a\" and \"b\") while others cannot be done until their predecessor task is complete (\"c\" and \"d\" cannot begin until \"a\" is complete). Additionally, each task has three time estimates: the optimistic time estimate (\"O\"), the most likely or normal time estimate (\"M\"), and the pessimistic time estimate (\"P\"). The expected time (\"T\") is estimated using the for the time estimates, using the formula (\"O\" + 4\"M\" + \"P\") ÷ 6.\n\nOnce this step is complete, one can draw a Gantt chart or a network diagram.\n\nIn a progress Gantt chart, tasks are shaded in proportion to the degree of their completion: a task that is 60% complete would be 60% shaded, starting from the left. A vertical line is drawn at the time index when the progress Gantt chart is created, and this line can then be compared with shaded tasks. If everything is on schedule, all task portions left of the line will be shaded, and all task portions right of the line will not be shaded. This provides a visual representation of how the project and its tasks are ahead or behind schedule.\n\n\"Linked Gantt charts\" contain lines indicating the dependencies between tasks. However, linked Gantt charts quickly become cluttered in all but the simplest cases. Critical path network diagrams are superior to visually communicate the relationships between tasks. However, Gantt charts are often preferred over network diagrams because Gantt charts are easily interpreted without training, whereas critical path diagrams require training to interpret. Gantt chart software typically provides mechanisms to link task dependencies, although this data may or may not be visually represented. Gantt charts and network diagrams are often used for the same project, both being generated from the same data by a software application.\n\n\n"}
{"id": "56572131", "url": "https://en.wikipedia.org/wiki?curid=56572131", "title": "Genevieve M. Knight", "text": "Genevieve M. Knight\n\nGenevieve Madeline Knight (born June 18, 1939 in Brunswick, Georgia) is an African-American mathematics educator.\n\nKnight was the youngest of three sisters who all became mathematics and science educators,\ndaughters of a seamstress and a civil service radar specialist.\nAs a freshman at Fort Valley State College in 1957, Knight was studying home economics when the Sputnik launch created a big push for more American students to become educated in mathematics and the sciences. Knight transferred to mathematics, \"because it had fewer labs than any of the sciences\", and graduated in 1961.\n\nShe completed a master's degree in 1963 at Atlanta University, under the supervision of Abdulalim A. Shabazz,\nand took a teaching position at the Hampton Institute, also becoming an NSF fellow, a position that allowed her to travel and meet with other college mathematics teachers. In 1966, she returned to graduate school, and completed a doctorate in mathematics education in 1970 at the University of Maryland, College Park under the supervision of Henry H. Walbesser.\n\nReturning from her doctorate, Knight remained at the Hampton Institute, where she became chair of mathematics and computer science. In 1985 she moved to Coppin State College as a full professor. She retired in 2006.\n\nIn 1980, the Virginia Council of Teachers of Mathematics named Knight as their College Teacher of the Year.\nIn 1993 she was named Maryland Mathematics Teacher of the Year, and the Mathematical Association of America gave Knight a Distinguished Teaching Award.\nIn 1996 the University System of Maryland named her as that year's Wilson H. Elkins Distinguished Professor.\nThe National Council of Teachers of Mathematics gave her their 1999 lifetime achievement award for her service to mathematics education, outspoken support of equity \"regardless of ethnicity, gender, or socioeconomic background\", and distinguished teaching. She was the 2013 Cox–Talbot Lecturer of the National Association of Mathematicians, one of the member societies of the Conference Board of the Mathematical Sciences.\nIn 2018 the Association for Women in Mathematics named her as one of their inaugural Fellows.\n"}
{"id": "39071852", "url": "https://en.wikipedia.org/wiki?curid=39071852", "title": "Geometry of binary search trees", "text": "Geometry of binary search trees\n\nIn computer science, one approach to the dynamic optimality problem on online algorithms for binary search trees involves reformulating the problem geometrically, in terms of augmenting a set of points in the plane with as few additional points as possible in order to avoid rectangles with only two points on their boundary.\n\nAs typically formulated, the online binary search tree problem involves search trees defined over a fixed key set (1, 2, ..., \"n\"). An \"access sequence\" is a sequence formula_1 ... where each number \"x\" is one of the given keys.\n\nAny particular algorithm for maintaining binary search trees (such as the splay tree algorithm or Iacono's working set structure) has a \"cost\" for each access sequence that models the amount of time it would take to use the structure to search for each of the keys in the access sequence in turn. The cost of a search is modeled by assuming that the search tree algorithm has a single pointer into a binary search tree, which at the start of each search points to the root of the tree. The algorithm may then perform any sequence of the following operations:\nThe search is required, at some point within this sequence of operations to move the pointer to a node containing the key, and the cost of the search is the number of operations that are performed in the sequence. The total cost cost(\"X\") for algorithm \"A\" on access sequence \"X\" is the sum of the costs of the searches for each successive key in the sequence.\n\nAs is standard in competitive analysis, the competitive ratio of an algorithm \"A\" is defined to be the maximum, over all access sequences, of the ratio of the cost for \"A\" to the best cost that any algorithm could achieve:\n\nThe dynamic optimality conjecture states that splay trees have constant competitive ratio, but this remains unproven. The geometric view of binary search trees provides a different way of understanding the problem that has led to the development of alternative algorithms that could also (conjecturally) have a constant competitive ratio.\n\nIn the geometric view of the online binary search tree problem,\nan \"access sequence\" formula_3 (sequence of searches performed on a binary search tree (BST) with a key set formula_4) is mapped to the set of points formula_5, where X-axis represents key space and Y-axis represents time; to which a set of touched nodes is added. By touched nodes we mean the following. Consider a BST access algorithm with a single pointer to a node in the tree. At the beginning of an access to a given key formula_6, this pointer is initialized to the root of the tree. Whenever the pointer moves to or is initialized to a node, we say that the node is touched.\nWe represent a BST algorithm for a given input sequence by drawing a point for each item that gets touched.\n\nFor example, assume the following BST on 4 nodes is given: \nThe key set is {1, 2, 3, 4}.\n\nLet 3, 1, 4, 2 be the access sequence. \n\nThe touches are represented geometrically: If an item \"x\" is touched in the operations for the \"i\"th access, then a point (\"x\",\"i\") is plotted.\n\nA point set is said to be arborally satisfied if the following property holds: for any\npair of points that do not both lie on the same horizontal or vertical line, there exists a third point\nwhich lies in the rectangle spanned by the first two points (either inside or on the boundary).\n\nA point set containing the points formula_7 is arborally satisfied if and only if it corresponds to a valid BST for the input sequence formula_8.\n\nFirst, prove that the point set for any valid BST algorithm is arborally satisfied.\nConsider points formula_9 and formula_10, where is touched at time and is touched at time . Assume by symmetry that formula_11 and formula_12. It needs to be shown that there exists a third point in the rectangle\nwith corners as formula_9 and formula_10. Also let formula_15 denote the lowest common ancestor of nodes \nand right before time . There are a few cases:\n\nNext, show the other direction: given an arborally satisfied point set, a valid BST corresponding to that point set can be constructed. Organize our BST into a treap which is organized in heap-order by next-touch-time. Note that next-touch-time has ties and is thus not uniquely defined, but this isn’t a problem as long as there is a way to break ties. When time reached, the nodes touched form a connected subtree at the top, by the heap ordering property. Now, assign new next-touch-times for this subtree, and rearrange it into a new local treap.\nIf a pair of nodes, and , straddle the boundary between the touched and untouched part\nof the treap, then if is to be touched sooner than then formula_23 is an unsatisfied rectangle because the leftmost such point would be the right child of , not .\n\nFinding the best BST execution for the input sequence formula_8 is equivalent to finding the minimum cardinality superset of points (that contains the input in geometric representation) that is arborally satisfied. The more general problem of finding the minimum cardinality arborally satisfied superset of a general set of input points (not limited to one input point per coordinate), is known to be NP-complete.\n\nThe following greedy algorithm constructs arborally satisfiable sets:\n\nThe algorithm has been conjectured to be optimal within an additive term.\n\nThe geometry of binary search trees has been used to provide an algorithm which is dynamically optimal if any binary search tree algorithm is dynamically optimal.\n\n"}
{"id": "1686601", "url": "https://en.wikipedia.org/wiki?curid=1686601", "title": "Infinity plus one", "text": "Infinity plus one\n\nIn mathematics, infinity plus one has meaning for the hyperreals, and also as the number ω+1 (omega plus one) in the ordinal numbers and surreal numbers. \n\nThere are several mathematical theories which include both infinite values and addition.\n\n\n"}
{"id": "46334533", "url": "https://en.wikipedia.org/wiki?curid=46334533", "title": "James B. Carrell", "text": "James B. Carrell\n\nJames B. Carrell (1940, Seattle) is an American and Canadian mathematician, who is currently an emeritus professor of mathematics at the University of British Columbia, Vancouver, British Columbia, Canada. His areas of research are algebraic geometry, Lie theory, transformation groups and differential geometry.\n\nHe obtained his Ph.D. at the University of Washington (Seattle) under the supervision of Allendoefer. In 1971 together with Jean Dieudonné he received Leroy P. Steele Prize for his work \"Invariant theory, old and new\".\n\nHe proved theorems in Schubert calculus about singularities of Schubert varieties. The Carrell–Liebermann theorem on the zero set of a holomorphic vector field is used in complex algebraic geometry.\n\nHe is a fellow of the American Mathematical Society.\n\n"}
{"id": "43875845", "url": "https://en.wikipedia.org/wiki?curid=43875845", "title": "Jean-Pierre Jouannaud", "text": "Jean-Pierre Jouannaud\n\nJean-Pierre Jouannaud is a French computer scientist, known for his work in the area of term rewriting.\n\nHe was born on 21 May 1947 in Aix-les-Bains (France).\nFrom 1967 to 1969 he visited the Ecole Polytechnique (Paris).\nIn 1970, 1972, and 1977, he wrote his Master thesis (DEA), PhD thesis (Thèse de 3ème cycle), and Habilitation thesis (Thèse d'état), respectively, at the Université de Paris VI.\nIn 1979, he became an associate professor at the Nancy University; 1985 he changed to the Université de Paris-Sud, where he became a full professor in 1986.\n\nHe was member of the steering committee of several international computer science conferences: International Conference on Rewriting Techniques and Applications (RTA) 1989-1994, IEEE Symposium on Logic in Computer Science (LICS) 1993-1997, Conference for Computer Science Logic (CSL) 1993-1997, International Conference on Principles and Practice of Constraint Programming (CP) since 1994, and Federated Logic Conference (FLoC) 1995-1999.\nSince 1997, he is member of the EATCS council.\n\n\n"}
{"id": "35349383", "url": "https://en.wikipedia.org/wiki?curid=35349383", "title": "List of things named after Arthur Cayley", "text": "List of things named after Arthur Cayley\n\nArthur Cayley (1821 – 1895) is the eponym of all the things listed below.\n\n"}
{"id": "1062541", "url": "https://en.wikipedia.org/wiki?curid=1062541", "title": "Multiple encryption", "text": "Multiple encryption\n\nMultiple encryption is the process of encrypting an already encrypted message one or more times, either using the same or a different algorithm. It is also known as cascade encryption, cascade ciphering, multiple encryption, and superencipherment. Superencryption refers to the outer-level encryption of a multiple encryption.\n\nPicking any two ciphers, if the key used is the same for both, the second cipher could possibly undo the first cipher, partly or entirely. This is true of ciphers where the decryption process is exactly the same as the encryption process—the second cipher would completely undo the first. If an attacker were to recover the key through cryptanalysis of the first encryption layer, the attacker could possibly decrypt all the remaining layers, assuming the same key is used for all layers.\n\nTo prevent that risk, one can use keys that are statistically independent for each layer (e.g. independent RNGs).\n\nWith the exception of the one-time pad, no cipher has been theoretically proven to be unbreakable.\nFurthermore, some recurring properties may be found in the ciphertexts generated by the first cipher. Since those ciphertexts are the plaintexts used by the second cipher, the second cipher may be rendered vulnerable to attacks based on known plaintext properties (see references below).\n\nThis is the case when the first layer is a program P that always adds the same string S of characters at the beginning (or end) of all ciphertexts (commonly known as a magic number). When found in a file, the string S allows an operating system to know that the program P has to be launched in order to decrypt the file. This string should be removed before adding a second layer.\n\nTo prevent this kind of attack, one can use the method provided by Bruce Schneier in the references below: generate a random pad of the same size of the plaintext, then XOR the plaintext with the pad, resulting in a first ciphertext. Encrypt the pad and the first ciphertext with a different cipher and a different key, resulting in 2 more ciphertexts. Concatenate the last 2 ciphertexts in order to build the final ciphertext. A cryptanalyst must break both ciphers to get any information. This will, however, have the drawback of making the ciphertext twice as long as the original plaintext.\n\nNote, however, that a weak first cipher may merely make a second cipher that is vulnerable to a chosen plaintext attack also vulnerable to a known plaintext attack. However, a block cipher must not be vulnerable to a chosen plaintext attack to be considered secure. Therefore, the second cipher described above is not secure under that definition, either. Consequently, both ciphers still need to be broken. The attack illustrates why strong assumptions are made about secure block ciphers and ciphers that are even partially broken should never be used.\n\nThe Rule of Two is a data security principle from the NSA's Commercial Solutions for Classified Program (CSfC). It specifies two completely independent layers of cryptography to protect data. For example, data could be protected by both hardware encryption at its lowest level and software encryption at the application layer.\n\nThe principle is practiced in the NSA's secure mobile phone called Fishbowl. The phones use two layers of encryption protocols, IPsec and Secure Real-time Transport Protocol (SRTP), to protect voice communications.\n\nThe figure shows from inside to outside the process of how the encrypted capsule is formed in the context of Echo Protocol, used by the Software Application GoldBug Messenger. GoldBug has implemented a hybrid system for authenticity and confidentiality.\n\nFirst layer of the encryption: \nThe ciphertext of the original readable message is hashed, and subsequently the symmetric keys are encrypted via the asymmetric key - e.g. deploying the algorithm RSA.\nIn an intermediate step the ciphertext, and the hash digest of the ciphertext are combined into a capsule, and packed together.\nIt follows the approach: Encrypt-then-MAC. In order for the receiver to verify that the ciphertext has not been tampered with, the digest is computed before the ciphertext is decrypted.\n\nSecond layer of encryption:\nOptionally it is still possible, therefore to encrypt the capsule of the first layer in addition with an AES-256, - comparable to a commonly shared, 32-character long symmetric password. Hybrid Encryption is then added to multiple encryption.\n\nThird layer of the encryption:\nThen, this capsule is transmitted via a secure SSL/TLS connection to the communication partner\n"}
{"id": "22657", "url": "https://en.wikipedia.org/wiki?curid=22657", "title": "Order of magnitude", "text": "Order of magnitude\n\nAn order of magnitude is an approximate measure of the number of digits that a number has in the commonly-used base-ten number system. It is equal to the logarithm (base 10) rounded to a whole number. For example, the order of magnitude of 1500 is 3, because 1500 = 1.5 × 10.\n\nDifferences in order of magnitude can be measured on a base-10 logarithmic scale in “decades” (i.e., factors of ten). Examples of numbers of different magnitudes can be found at Orders of magnitude (numbers).\n\nGenerally, the order of magnitude of a number is the smallest power of 10 used to represent that number. To work out the order of magnitude of a number formula_1, the number is first expressed in the following form:\nwhere formula_3. Then, formula_4 represents the order of magnitude of the number. The order of magnitude can be any integer. The table below enumerates the order of magnitude of some numbers in light of this definition:\n\nThe geometric mean of formula_5 and formula_6 is formula_7, meaning that a value of exactly formula_5 (i.e., formula_9) represents a geometric \"halfway point\" within the range of possible values of formula_10.\n\nSome use a simpler definition where <math>0.5, perhaps because the arithmetic mean of formula_5 and formula_12 approaches formula_13 for increasing formula_14. This definition has the effect of lowering the values of formula_4 slightly:\n\nYet others restrict formula_10 to values where formula_17, making the order of magnitude of a number exactly equal to its exponent part in scientific notation.\n\nOrders of magnitude are used to make approximate comparisons. If numbers differ by one order of magnitude, \"x\" is \"about\" ten times different in quantity than \"y\". If values differ by two orders of magnitude, they differ by a factor of about 100. Two numbers of the same order of magnitude have roughly the same scale: the larger value is less than ten times the smaller value.\n\nThe order of magnitude of a number is, intuitively speaking, the number of powers of 10 contained in the number. More precisely, the order of magnitude of a number can be defined in terms of the common logarithm, usually as the integer part of the logarithm, obtained by truncation. For example, the number has a logarithm (in base 10) of 6.602; its order of magnitude is 6. When truncating, a number of this order of magnitude is between 10 and 10. In a similar example, with the phrase \"He had a seven-figure income\", the order of magnitude is the number of figures minus one, so it is very easily determined without a calculator to 6. An order of magnitude is an approximate position on a logarithmic scale.\n\nAn order-of-magnitude estimate of a variable, whose precise value is unknown, is an estimate rounded to the nearest power of ten. For example, an order-of-magnitude estimate for a variable between about 3 billion and 30 billion (such as the human population of the Earth) is 10 billion. To round a number to its nearest order of magnitude, one rounds its logarithm to the nearest integer. Thus , which has a logarithm (in base 10) of 6.602, has 7 as its nearest order of magnitude, because \"nearest\" implies rounding rather than truncation. For a number written in scientific notation, this logarithmic rounding scale requires rounding up to the next power of ten when the multiplier is greater than the square root of ten (about 3.162). For example, the nearest order of magnitude for is 8, whereas the nearest order of magnitude for is 9. An order-of-magnitude estimate is sometimes also called a zeroth order approximation.\n\nAn order-of-magnitude difference between two values is a factor of 10. For example, the mass of the planet Saturn is 95 times that of Earth, so Saturn is \"two orders of magnitude\" more massive than Earth. Order-of-magnitude differences are called decades when measured on a logarithmic scale.\n\nOther orders of magnitude may be calculated using bases other than 10. The ancient Greeks ranked the nighttime brightness of celestial bodies by 6 levels in which each level was the fifth root of one hundred (about 2.512) as bright as the nearest weaker level of brightness, and thus the brightest level being 5 orders of magnitude brighter than the weakest indicates that it is (100) or a factor of 100 times brighter.\n\nThe different decimal numeral systems of the world use a larger base to better envision the size of the number, and have created names for the powers of this larger base. The table shows what number the order of magnitude aim at for base 10 and for base . It can be seen that the order of magnitude is included in the number name in this example, because bi- means 2 and tri- means 3 (these make sense in the long scale only), and the suffix -illion tells that the base is . But the number names billion, trillion themselves (here with other meaning than in the first chapter) are not names of the \"orders of\" magnitudes, they are names of \"magnitudes\", that is the \"numbers\" etc.\n\nSI units in the table at right are used together with SI prefixes, which were devised with mainly base 1000 magnitudes in mind. The IEC standard prefixes with base 1024 were invented for use in electronic technology.\n\nThe ancient apparent magnitudes for the brightness of stars uses the base formula_18 and is reversed. The modernized version has however turned into a logarithmic scale with non-integer values.\n\nFor extremely large numbers, a generalized order of magnitude can be based on their double logarithm or super-logarithm. Rounding these downward to an integer gives categories between very \"round numbers\", rounding them to the nearest integer and applying the inverse function gives the \"nearest\" round number.\n\nThe double logarithm yields the categories:\n(the first two mentioned, and the extension to the left, may not be very useful, they merely demonstrate how the sequence mathematically continues to the left).\n\nThe super-logarithm yields the categories:\n\nThe \"midpoints\" which determine which round number is nearer are in the first case:\nand, depending on the interpolation method, in the second case\n\nFor extremely small numbers (in the sense of close to zero) neither method is suitable directly, but the generalized order of magnitude of the reciprocal can be considered.\n\nSimilar to the logarithmic scale one can have a double logarithmic scale (example provided here) and super-logarithmic scale. The intervals above all have the same length on them, with the \"midpoints\" actually midway. More generally, a point midway between two points corresponds to the generalised f-mean with \"f\"(\"x\") the corresponding function log log \"x\" or slog \"x\". In the case of log log \"x\", this mean of two numbers (e.g. 2 and 16 giving 4) does not depend on the base of the logarithm, just like in the case of log \"x\" (geometric mean, 2 and 8 giving 4), but unlike in the case of log log log \"x\" (4 and giving 16 if the base is 2, but not otherwise).\n\n\n\n"}
{"id": "29746108", "url": "https://en.wikipedia.org/wiki?curid=29746108", "title": "Perturbation problem beyond all orders", "text": "Perturbation problem beyond all orders\n\nIn mathematics, perturbation theory works typically by expanding unknown quantity in a power series in a small parameter. However, in a perturbation problem beyond all orders, all coefficients of the perturbation expansion vanish and the difference between the function and the constant function 0 cannot be detected by a power series.\n\nA simple example is understood by an attempt at trying to expand formula_1 in a Taylor series in formula_2 about 0. All terms in a naïve Taylor expansion are identically zero. This is because the function formula_3 possesses an essential singularity at formula_4 in the complex formula_5-plane, and therefore the function is most appropriately modeled by a Laurent series -- a Taylor series has a zero radius of convergence. Thus, if a physical problem possesses a solution of this nature, possibly in addition to an analytic part that may be modeled by a power series, the perturbative analysis fails to recover the singular part. Terms of nature similar to formula_1 are considered to be \"beyond all orders\" of the standard perturbative power series.\n\nAsymptotic expansion\n\n"}
{"id": "7723865", "url": "https://en.wikipedia.org/wiki?curid=7723865", "title": "Philosophy of mathematics education", "text": "Philosophy of mathematics education\n\nThe Philosophy of mathematics education is an interdisciplinary area of study and research based on the intersection of the fields of mathematics education and the philosophy of mathematics, the latter being understood in an inclusive sense to include multidisciplinary theorizing about mathematics incorporating philosophical, sociological, anthropological, semiotic, historical, ethnomathematical, etc., perspectives. \n\nSome of the central questions addressed by this sub-field are:\n\nFor further details see 'What is the philosophy of mathematics education?' by Paul Ernest.\n\nThere is a dedicated \"Philosophy of Mathematics Education Journal\" freely available on the web.\n\n"}
{"id": "57545783", "url": "https://en.wikipedia.org/wiki?curid=57545783", "title": "Ping Zhang (graph theorist)", "text": "Ping Zhang (graph theorist)\n\nPing Zhang is a mathematician specializing in graph theory. She is a professor of mathematics at Western Michigan University and the author of multiple textbooks on graph theory and mathematical proof.\n\nZhang earned a master's degree in 1989 from the University of Jordan, working there on ring theory with Hasan Al-Ezeh.\nShe completed her Ph.D. in 1995 at Michigan State University. Her dissertation, in algebraic combinatorics, was \"Subposets of Boolean Algebras\", and was supervised by Bruce Sagan. After a short-term position at the University of Texas at El Paso, she joined the Western Michigan faculty in 1996.\n\nZhang is the author of:\n\nShe is also the co-editor of:\n"}
{"id": "13257986", "url": "https://en.wikipedia.org/wiki?curid=13257986", "title": "Plane wave expansion method", "text": "Plane wave expansion method\n\nPlane wave expansion method (PWE) refers to a computational technique in electromagnetics to solve the Maxwell's equations by formulating an eigenvalue problem out of the equation. This method is popular among the photonic crystal community as a method of solving for the band structure (dispersion relation) of specific photonic crystal geometries. PWE is traceable to the analytical formulations, and is useful in calculating modal solutions of Maxwell's equations over an inhomogeneous or periodic geometry. It is specifically tuned to solve problems in a time-harmonic forms, with non-dispersive media.\n\nPlane waves are solutions to the homogeneous Helmholtz equation, and form a basis to represent fields in the periodic media. PWE as applied to photonic crystals as described is primarily sourced \nfrom Dr. Danner's tutorial.\n\nThe electric or magnetic fields are expanded for each field component in terms of the Fourier series components along the reciprocal lattice vector. Similarly, the dielectric permittivity (which is periodic along reciprocal lattice vector for photonic crystals) is also expanded through Fourier series components. \n\nwith the Fourier series coefficients being the K numbers subscripted by m, n respectively, and the reciprocal lattice vector given by formula_3. In real modeling, the range of components considered will be reduced to just formula_4 instead of the ideal, infinite wave.\n\nUsing these expansions in any of the curl-curl relations like,\n\nand simplifying under assumptions of a source free, linear, and non-dispersive region we obtain the eigen value relations which can be solved.\n\nFor a y-polarized z-propagating electric wave, incident on a 1D-DBR periodic in only z-direction and homogeneous along x,y, with a lattice period of a. We then have the following simplified relations:\n\nThe constitutive eigen equation we finally have to solve becomes,\n\nThis can be solved by building a matrix for the terms in the left hand side, and finding its eigen value and vectors. The eigen values correspond to the modal solutions, while the corresponding magnetic or electric fields themselves can be plotted using the Fourier expansions. The coefficients of the field harmonics are obtained from the specific eigen vectors.\n\nThe resulting band-structure obtained through the eigen modes of this structure are shown to the right.\n\nWe can use the following code in Matlab or GNU Octave to compute the same band structure,\n\nPWE expansions are rigorous solutions. PWE is extremely well suited to the modal solution problem. Large size problems can be solved using iterative techniques like Conjugate gradient method.\nFor both generalized and normal eigen value problems, just a few band-index plots in the band-structure diagrams are required, usually lying on the brillouin zone edges. This corresponds to eigen modes solutions using iterative techniques, as opposed to diagonalization of the entire matrix.\n\nThe PWEM is highly efficient for calculating modes in periodic dielectric structures. Being a Fourier space method, it suffers from the Gibbs phenomenon and slow convergence in some configuration when fast Fourier factorization is not used. It is the method of choice for calculating the band structure of photonic crystals. It is not easy to understand at first, but it is easy to implement.\n\nSometimes spurious modes appear. Large problems scaled as \"O(n)\", with the number of the plane waves (n) used in the problem. This is both time consuming and complex in memory requirements.\n\nAlternatives include Order-N spectral method, and methods using Finite-difference time-domain (FDTD) which are simpler, and model transients.\n\nIf implemented correctly, spurious solutions are avoided. It is less efficient when index contrast is high or when metals are incorporated. It cannot be used for scattering analysis.\n\nBeing a Fourier-space method, Gibbs phenomenon affects the method's accuracy. This is particularly problematic for devices with high dielectric contrast.\n\n"}
{"id": "5779282", "url": "https://en.wikipedia.org/wiki?curid=5779282", "title": "Projected dynamical system", "text": "Projected dynamical system\n\nProjected dynamical systems is a mathematical theory investigating the behaviour of dynamical systems where solutions are restricted to a constraint set. The discipline shares connections to and applications with both the static world of optimization and equilibrium problems and the dynamical world of ordinary differential equations. A projected dynamical system is given by the flow to the projected differential equation\n\nwhere \"K\" is our constraint set. Differential equations of this form are notable for having a discontinuous vector field.\n\nProjected dynamical systems have evolved out of the desire to dynamically model the behaviour of nonstatic solutions in equilibrium problems over some parameter, typically take to be time. This dynamics differs from that of ordinary differential equations in that solutions are still restricted to whatever constraint set the underlying equilibrium problem was working on, e.g. nonnegativity of investments in financial modeling, convex polyhedral sets in operations research, etc. One particularly important class of equilibrium problems which has aided in the rise of projected dynamical systems has been that of variational inequalities.\n\nThe formalization of projected dynamical systems began in the 1990s. However, similar concepts can be found in the mathematical literature which predate this, especially in connection with variational inequalities and differential inclusions.\n\nAny solution to our projected differential equation must remain inside of our constraint set \"K\" for all time. This desired result is achieved through the use of projection operators and two particular important classes of convex cones. Here we take \"K\" to be a closed, convex subset of some Hilbert space \"X\".\n\nThe \"normal cone\" to the set \"K\" at the point \"x\" in \"K\" is given by\n\nThe \"tangent cone\" (or \"contingent cone\") to the set \"K\" at the point \"x\" is given by\n\nThe \"projection operator\" (or \"closest element mapping\") of a point \"x\" in \"X\" to \"K\" is given by the point formula_4 in \"K\" such that\n\nfor every \"y\" in \"K\".\n\nThe \"vector projection operator\" of a vector \"v\" in \"X\" at a point \"x\" in \"K\" is given by\n\nGiven a closed, convex subset \"K\" of a Hilbert space \"X\" and a vector field \"-F\" which takes elements from \"K\" into \"X\", the projected differential equation associated with \"K\" and \"-F\" is defined to be\n\nOn the interior of \"K\" solutions behave as they would if the system were an unconstrained ordinary differential equation. However, since the vector field is discontinuous along the boundary of the set, projected differential equations belong to the class of discontinuous ordinary differential equations. While this makes much of ordinary differential equation theory inapplicable, it is known that when \"-F\" is a Lipschitz continuous vector field, a unique absolutely continuous solution exists through each initial point \"x(0)=x\" in \"K\" on the interval formula_8.\n\nThis differential equation can be alternately characterized by\n\nor\n\nThe convention of denoting the vector field \"-F\" with a negative sign arises from a particular connection projected dynamical systems shares with variational inequalities. The convention in the literature is to refer to the vector field as positive in the variational inequality, and negative in the corresponding projected dynamical system.\n\n\n"}
{"id": "25350", "url": "https://en.wikipedia.org/wiki?curid=25350", "title": "Quasicrystal", "text": "Quasicrystal\n\nA quasiperiodic crystal, or quasicrystal, is a structure that is ordered but not periodic. A quasicrystalline pattern can continuously fill all available space, but it lacks translational symmetry. While crystals, according to the classical crystallographic restriction theorem, can possess only two, three, four, and six-fold rotational symmetries, the Bragg diffraction pattern of quasicrystals shows sharp peaks with other symmetry orders, for instance five-fold.\n\nAperiodic tilings were discovered by mathematicians in the early 1960s, and, some twenty years later, they were found to apply to the study of quasicrystals. The discovery of these aperiodic forms in nature has produced a paradigm shift in the fields of crystallography. Quasicrystals had been investigated and observed earlier, but, until the 1980s, they were disregarded in favor of the prevailing views about the atomic structure of matter. In 2009, after a dedicated search, a mineralogical finding, icosahedrite, offered evidence for the existence of natural quasicrystals.\n\nRoughly, an ordering is non-periodic if it lacks translational symmetry, which means that a shifted copy will never match exactly with its original. The more precise mathematical definition is that there is never translational symmetry in more than \"n\" – 1 linearly independent directions, where \"n\" is the dimension of the space filled, e.g., the three-dimensional tiling displayed in a quasicrystal may have translational symmetry in two directions. Symmetrical diffraction patterns result from the existence of an indefinitely large number of elements with a regular spacing, a property loosely described as long-range order. Experimentally, the aperiodicity is revealed in the unusual symmetry of the diffraction pattern, that is, symmetry of orders other than two, three, four, or six. \nIn 1982 materials scientist Dan Shechtman observed that certain aluminium-manganese alloys produced the unusual diffractograms which today are seen as revelatory of quasicrystal structures. Due to fear of the scientific community's reaction, it took him two years to publish the results for which he was awarded the Nobel Prize in Chemistry in 2011.\n\nIn 1961, Hao Wang asked whether determining if a set of tiles admits a tiling of the plane is an algorithmically unsolvable problem or not. He conjectured that it is solvable, relying on the hypothesis that every set of tiles that can tile the plane can do it \"periodically\" (hence, it would suffice to try to tile bigger and bigger patterns until obtaining one that tiles periodically). Nevertheless, two years later, his student Robert Berger constructed a set of some 20,000 square tiles (now called Wang tiles) that can tile the plane but not in a periodic fashion. As further aperiodic sets of tiles were discovered, sets with fewer and fewer shapes were found. In 1976 Roger Penrose discovered a set of just two tiles, now referred to as Penrose tiles, that produced only non-periodic tilings of the plane. These tilings displayed instances of fivefold symmetry. One year later Alan Mackay showed experimentally that the diffraction pattern from the Penrose tiling had a two-dimensional Fourier transform consisting of sharp 'delta' peaks arranged in a fivefold symmetric pattern. Around the same time Robert Ammann created a set of aperiodic tiles that produced eightfold symmetry.\n\nMathematically, quasicrystals have been shown to be derivable from a general method that treats them as projections of a higher-dimensional lattice. Just as circles, ellipses, and hyperbolic curves in the plane can be obtained as sections from a three-dimensional double cone, so too various (aperiodic or periodic) arrangements in two and three dimensions can be obtained from postulated hyperlattices with four or more dimensions. Icosahedral quasicrystals in three dimensions were projected from a six-dimensional hypercubic lattice by Peter Kramer and Roberto Neri in 1984. The tiling is formed by two tiles with rhombohedral shape.\n\nShechtman first observed ten-fold electron diffraction patterns in 1982, as described in his notebook. The observation was made during a routine investigation, by electron microscopy, of a rapidly cooled alloy of aluminium and manganese prepared at the US National Bureau of Standards (later NIST).\n\nIn the summer of the same year Shechtman visited Ilan Blech and related his observation to him. Blech responded that such diffractions had been seen before. Around that time, Shechtman also related his finding to John Cahn of NIST who did not offer any explanation and challenged him to solve the observation. Shechtman quoted Cahn as saying: \"Danny, this material is telling us something and I challenge you to find out what it is\".\n\nThe observation of the ten-fold diffraction pattern lay unexplained for two years until the spring of 1984, when Blech asked Shechtman to show him his results again. A quick study of Shechtman's results showed that the common explanation for a ten-fold symmetrical diffraction pattern, the existence of twins, was ruled out by his experiments. Since periodicity and twins were ruled out, Blech, unaware of the two-dimensional tiling work, was looking for another possibility: a completely new structure containing cells connected to each other by defined angles and distances but without translational periodicity. Blech decided to use a computer simulation to calculate the diffraction intensity from a cluster of such a material without long-range translational order but still not random. He termed this new structure multiple polyhedral.\n\nThe idea of a new structure was the necessary paradigm shift to break the impasse. The “Eureka moment” came when the computer simulation showed sharp ten-fold diffraction patterns, similar to the observed ones, emanating from the three-dimensional structure devoid of periodicity. The multiple polyhedral structure was termed later by many researchers as icosahedral glass but in effect it embraces \"any arrangement of polyhedra connected with definite angles and distances\" (this general definition includes tiling, for example).\n\nShechtman accepted Blech's discovery of a new type of material and it gave him the courage to publish his experimental observation. Shechtman and Blech jointly wrote a paper entitled \"The Microstructure of Rapidly Solidified AlMn\" and sent it for publication around June 1984 to the \"Journal of Applied Physics\" (JAP). The JAP editor promptly rejected the paper as being better fit for a metallurgical readership. As a result, the same paper was re-submitted for publication to the \"Metallurgical Transactions A\", where it was accepted. Although not noted in the body of the published text, the published paper was slightly revised prior to publication.\n\nMeanwhile, on seeing the draft of the Shechtman–Blech paper in the summer of 1984, John Cahn suggested that Shechtman's experimental results merit a fast publication in a more appropriate scientific journal. Shechtman agreed and, in hindsight, called this fast publication \"a winning move”. This paper, published in the \"Physical Review Letters\" (PRL), repeated Shechtman's observation and used the same illustrations as the original Shechtman–Blech paper in the \"Metallurgical Transactions A\". The PRL paper, the first to appear in print, caused considerable excitement in the scientific community.\n\nNext year Ishimasa \"et al.\" reported twelvefold symmetry in Ni-Cr particles. Soon, eightfold diffraction patterns were recorded in V-Ni-Si and Cr-Ni-Si alloys. Over the years, hundreds of quasicrystals with various compositions and different symmetries have been discovered. The first quasicrystalline materials were thermodynamically unstable—when heated, they formed regular crystals. However, in 1987, the first of many stable quasicrystals were discovered, making it possible to produce large samples for study and opening the door to potential applications. In 2009, following a 10-year systematic search, scientists reported the first natural quasicrystal, a mineral found in the Khatyrka River in eastern Russia. This natural quasicrystal exhibits high crystalline quality, equalling the best artificial examples. The natural quasicrystal phase, with a composition of AlCuFe, was named icosahedrite and it was approved by the International Mineralogical Association in 2010. Furthermore, analysis indicates it may be meteoritic in origin, possibly delivered from a carbonaceous chondrite asteroid.\nA further study of Khatyrka meteorites revealed micron-sized grains of another natural quasicrystal, which has a ten-fold symmetry and a chemical formula of AlNiFe. This quasicrystal is stable in a narrow temperature range, from 1120 to 1200 K at ambient pressure, which suggests that natural quasicrystals are formed by rapid quenching of a meteorite heated during an impact-induced shock.\nIn 1972 de Wolf and van Aalst reported that the diffraction pattern produced by a crystal of sodium carbonate cannot be labeled with three indices but needed one more, which implied that the underlying structure had four dimensions in reciprocal space. Other puzzling cases have been reported, but until the concept of quasicrystal came to be established, they were explained away or denied. However, at the end of the 1980s the idea became acceptable, and in 1992 the International Union of Crystallography altered its definition of a crystal, broadening it as a result of Shechtman’s findings, reducing it to the ability to produce a clear-cut diffraction pattern and acknowledging the possibility of the ordering to be either periodic or aperiodic. Now, the symmetries compatible with translations are defined as \"crystallographic\", leaving room for other \"non-crystallographic\" symmetries. Therefore, aperiodic or quasiperiodic structures can be divided into two main classes: those with crystallographic point-group symmetry, to which the incommensurately modulated structures and composite structures belong, and those with non-crystallographic point-group symmetry, to which quasicrystal structures belong.\n\nOriginally, the new form of matter was dubbed \"Shechtmanite\". The term \"quasicrystal\" was first used in print by Steinhardt and Levine shortly after Shechtman's paper was published.\nThe adjective \"quasicrystalline\" had already been in use, but now it came to be applied to any pattern with unusual symmetry. 'Quasiperiodical' structures were claimed to be observed in some decorative tilings devised by medieval Islamic architects. For example, Girih tiles in a medieval Islamic mosque in Isfahan, Iran, are arranged in a two-dimensional quasicrystalline pattern. These claims have, however, been under some debate.\n\nShechtman was awarded the Nobel Prize in Chemistry in 2011 for his work on quasicrystals. \"His discovery of quasicrystals revealed a new principle for packing of atoms and molecules,\" stated the Nobel Committee and pointed that \"this led to a paradigm shift within chemistry.\" \n\nThere are several ways to mathematically define quasicrystalline patterns. One definition, the \"cut and project\" construction, is based on the work of Harald Bohr (mathematician brother of Niels Bohr). The concept of an almost periodic function (also called a quasiperiodic function) was studied by Bohr, including work of Bohl and Escanglon.\nHe introduced the notion of a superspace. Bohr showed that quasiperiodic functions arise as restrictions of high-dimensional periodic functions to an irrational slice (an intersection with one or more hyperplanes), and discussed their Fourier point spectrum. These functions are not exactly periodic, but they are arbitrarily close in some sense, as well as being a projection of an exactly periodic function.\n\nIn order that the quasicrystal itself be aperiodic, this slice must avoid any lattice plane of the higher-dimensional lattice. De Bruijn showed that Penrose tilings can be viewed as two-dimensional slices of five-dimensional hypercubic structures. Equivalently, the Fourier transform of such a quasicrystal is nonzero only at a dense set of points spanned by integer multiples of a finite set of basis vectors (the projections of the primitive reciprocal lattice vectors of the higher-dimensional lattice).\nThe intuitive considerations obtained from simple model aperiodic tilings are formally expressed in the concepts of Meyer and Delone sets. The mathematical counterpart of physical diffraction is the Fourier transform and the qualitative description of a diffraction picture as 'clear cut' or 'sharp' means that singularities are present in the Fourier spectrum. There are different methods to construct model quasicrystals. These are the same methods that produce aperiodic tilings with the additional constraint for the diffractive property. Thus, for a substitution tiling the eigenvalues of the substitution matrix should be Pisot numbers. The aperiodic structures obtained by the cut-and-project method are made diffractive by choosing a suitable orientation for the construction; this is a geometric approach that has also a great appeal for physicists.\n\nClassical theory of crystals reduces crystals to point lattices where each point is the center of mass of one of the identical units of the crystal. The structure of crystals can be analyzed by defining an associated group. Quasicrystals, on the other hand, are composed of more than one type of unit, so, instead of lattices, quasilattices must be used. Instead of groups, groupoids, the mathematical generalization of groups in category theory, is the appropriate tool for studying quasicrystals.\n\nUsing mathematics for construction and analysis of quasicrystal structures is a difficult task for most experimentalists. Computer modeling, based on the existing theories of quasicrystals, however, greatly facilitated this task. Advanced programs have been developed allowing one to construct, visualize and analyze quasicrystal structures and their diffraction patterns.\n\nInteracting spins were also analyzed in quasicrystals: AKLT Model and 8-vertex model were solved in quasicrystals analytically.\n\nStudy of quasicrystals may shed light on the most basic notions related to quantum critical point observed in heavy fermion metals. Experimental measurements on the gold-aluminium-ytterbium quasicrystal have revealed a quantum critical point defining the divergence of the magnetic susceptibility as temperature tends to zero. It is suggested that the electronic system of some quasicrystals is located at quantum critical point without tuning, while quasicrystals exhibit the typical scaling behaviour of their thermodynamic properties and belong to the famous family of heavy-fermion metals.\n\nSince the original discovery by Dan Shechtman, hundreds of quasicrystals have been reported and confirmed. Undoubtedly, the quasicrystals are no longer a unique form of solid; they exist\nuniversally in many metallic alloys and some polymers. Quasicrystals are found most often in aluminium alloys (Al-Li-Cu, Al-Mn-Si, Al-Ni-Co, Al-Pd-Mn, Al-Cu-Fe, Al-Cu-V, etc.), but numerous other compositions are also known (Cd-Yb, Ti-Zr-Ni, Zn-Mg-Ho, Zn-Mg-Sc, In-Ag-Yb, Pd-U-Si, etc.).\n\nTwo types of quasicrystals are known. The first type, polygonal (dihedral) quasicrystals, have an axis of 8, 10, or 12-fold local symmetry (octagonal, decagonal, or dodecagonal quasicrystals, respectively). They are periodic along this axis and quasiperiodic in planes normal to it. The second type, icosahedral quasicrystals, are aperiodic in all directions.\n\nQuasicrystals fall into three groups of different thermal stability:\n\nExcept for the Al–Li–Cu system, all the stable quasicrystals are almost free of defects and disorder, as evidenced by X-ray and electron diffraction revealing peak widths as sharp as those of perfect crystals such as Si. Diffraction patterns exhibit fivefold, threefold, and twofold symmetries, and reflections are arranged quasiperiodically in three dimensions.\n\nThe origin of the stabilization mechanism is different for the stable and metastable quasicrystals. Nevertheless, there is a common feature observed in most quasicrystal-forming liquid alloys or their undercooled liquids: a local icosahedral order. The icosahedral order is in equilibrium in the \"liquid state\" for the stable quasicrystals, whereas the icosahedral order prevails in the \"undercooled liquid state\" for the metastable quasicrystals.\n\nA nanoscale icosahedral phase was formed in Zr-, Cu- and Hf-based bulk metallic glasses alloyed with noble metals.\n\nMost quasicrystals have ceramic-like properties including high thermal and electrical resistance, hardness and brittleness, resistance to corrosion, and non-stick\nproperties. Many metallic quasicrystalline substances are impractical for most applications due to their thermal instability; the Al-Cu-Fe ternary system and the Al-Cu-Fe-Cr and Al-Co-Fe-Cr quaternary systems, thermally stable up to 700 °C, are notable exceptions.\n\nQuasicrystalline substances have potential applications in several forms.\n\nMetallic quasicrystalline coatings can be applied by plasma-coating or magnetron sputtering. A problem that must be resolved is the tendency for cracking due to the materials' extreme brittleness. The cracking could be suppressed by reducing sample dimensions or coating thickness. Recent studies show typically brittle quasicrystals can exhibit remarkable ductility of over 50% strains at room temperature and sub-micrometer scales (<500 nm).\n\nAn application was the use of low-friction Al-Cu-Fe-Cr quasicrystals as a coating for frying pans. Food did not stick to it as much as to stainless steel making the pan moderately non-stick and easy to clean; heat transfer and durability were better than PTFE non-stick cookware and the pan was free from perfluorooctanoic acid (PFOA); the surface was very hard, claimed to be ten times harder than stainless steel, and not harmed by metal utensils or cleaning in a dishwasher; and the pan could withstand temperatures of without harm. However, cooking with a lot of salt would etch the quasicrystalline coating used, and the pans were eventually withdrawn from production. Shechtman had one of these pans.\n\nThe Nobel citation said that quasicrystals, while brittle, could reinforce steel \"like armor\". When Shechtman was asked about potential applications of quasicrystals he said that a precipitation-hardened stainless steel is produced that is strengthened by small quasicrystalline particles. It does not corrode and is extremely strong, suitable for razor blades and surgery instruments. The small quasicrystalline particles impede the motion of dislocation in the material.\n\nQuasicrystals were also being used to develop heat insulation, LEDs, diesel engines, and new materials that convert heat to electricity. Shechtman suggested new applications taking advantage of the low coefficient of friction and the hardness of some quasicrystalline materials, for example embedding particles in plastic to make strong, hard-wearing, low-friction plastic gears. The low heat conductivity of some quasicrystals makes them good for heat insulating coatings.\n\nOther potential applications include selective solar absorbers for power conversion, broad-wavelength reflectors, and bone repair and prostheses applications where biocompatibility, low friction and corrosion resistance are required. Magnetron sputtering can be readily applied to other stable quasicrystalline alloys such as Al-Pd-Mn.\n\nWhile saying that the discovery of icosahedrite, the first quasicrystal found in nature, was important, Shechtman saw no practical applications.\n\n\n"}
{"id": "937598", "url": "https://en.wikipedia.org/wiki?curid=937598", "title": "Rearrangement inequality", "text": "Rearrangement inequality\n\nIn mathematics, the rearrangement inequality states that\n\nfor every choice of real numbers\n\nand every permutation\n\nof \"x\", . . ., \"x\". If the numbers are different, meaning that\n\nthen the lower bound is attained only for the permutation which reverses the order, i.e. σ(\"i\") = \"n\" − \"i\" + 1 for all \"i\" = 1, ..., \"n\", and the upper bound is attained only for the identity, i.e. σ(\"i\") = \"i\" for all \"i\" = 1, ..., \"n\".\n\nNote that the rearrangement inequality makes no assumptions on the signs of the real numbers.\n\nMany important inequalities can be proved by the rearrangement inequality, such as the arithmetic mean – geometric mean inequality, the Cauchy–Schwarz inequality, and Chebyshev's sum inequality.\n\nThe lower bound follows by applying the upper bound to\n\nTherefore, it suffices to prove the upper bound. Since there are only finitely many permutations, there exists at least one for which\n\nis maximal. In case there are several permutations with this property, let σ denote one with the highest number of fixed points.\n\nWe will now prove by contradiction, that σ has to be the identity (then we are done). Assume that σ is the identity. Then there exists a \"j\" in {1, ..., \"n\" − 1} such that σ(\"j\") ≠ \"j\" and σ(\"i\") = \"i\" for all \"i\" in {1, ..., \"j\" − 1}. Hence σ(\"j\") > \"j\" and there exists a \"k\" in {\"j\" + 1, ..., \"n\"} with σ(\"k\") = \"j\". Now\n\nTherefore,\n\nExpanding this product and rearranging gives\n\nhence the permutation\n\nwhich arises from σ by exchanging the values σ(\"j\") and σ(\"k\"), has at least one additional fixed point compared to σ, namely at \"j\", and also attains the maximum. This contradicts the choice of σ.\n\nIf\n\nthen we have strict inequalities at (1), (2), and (3), hence the maximum can only be attained by the identity, any other permutation σ cannot be optimal.\n\nObserve first that \nimplies \nhence the result is true if \"n\" = 2. \nAssume it is true at rank \"n-1\", and let \nChoose a permutation σ for which the arrangement gives rise a maximal result.\n\nIf σ(\"n\") were different from \"n\", say σ(\"n\") = \"k\", \nthere would exist \"j\" < \"n\" such that σ(\"j\") = \"n\". \nBut \nby what has just been proved.\nConsequently, it would follow that the permutation τ coinciding with σ, except at \"j\" and \"n\", where \nτ(\"j\") = \"k\" and τ(\"n\") = \"n\", gives rise a better result. This contradicts the choice of σ.\nHence σ(\"n\") = \"n\", and from the induction hypothesis, σ(\"i\") = \"i\" for every \"i\" < \"n\".\n\nThe same proof holds if one replace strict inequalities by non strict ones.\n\nA Generalization of the Rearrangement inequality states that for all real numbers formula_16 and any choice of functions formula_17 such that\n\nthe inequality\n\nholds for every permutation formula_20 of formula_21.\n\n"}
{"id": "2174269", "url": "https://en.wikipedia.org/wiki?curid=2174269", "title": "Reduced product", "text": "Reduced product\n\nIn model theory, a branch of mathematical logic, and in algebra, the reduced product is a construction that generalizes both direct product and ultraproduct.\n\nLet {\"S\" | \"i\" ∈ \"I\"} be a family of structures of the same signature σ indexed by a set \"I\", and let \"U\" be a filter on \"I\". The domain of the reduced product is the quotient of the Cartesian product\n\nby a certain equivalence relation ~: two elements (\"a\") and (\"b\") of the Cartesian product are equivalent if\n\nIf \"U\" only contains \"I\" as an element, the equivalence relation is trivial, and the reduced product is just the original Cartesian product. If \"U\" is an ultrafilter, the reduced product is an ultraproduct.\n\nOperations from σ are interpreted on the reduced product by applying the operation pointwise. Relations are interpreted by\n\nFor example, if each structure is a vector space, then the reduced product is a vector space with addition defined as (\"a\" + \"b\") = \"a\" + \"b\" and multiplication by a scalar \"c\" as (\"ca\") = \"c a\".\n\n"}
{"id": "7437222", "url": "https://en.wikipedia.org/wiki?curid=7437222", "title": "Ring of periods", "text": "Ring of periods\n\nIn mathematics, a period is a number that can be expressed as an integral of an algebraic function over an algebraic domain. Sums and products of periods remain periods, so the periods form a ring.\n\nA real number is called a period if it is the difference of volumes of regions of Euclidean space given by polynomial inequalities with rational coefficients. More generally a complex number is called a period if its real and imaginary parts are periods. \n\nThe values of absolutely convergent integrals of rational functions with algebraic coefficients, over domains in formula_1 given by polynomial inequalities with algebraic coefficients are also periods, since integrals and irrational algebraic numbers are expressible in terms of areas of suitable domains.\n\nBesides the algebraic numbers, the following numbers are known to be periods:\n\nAn example of real number that is not a period is given by Chaitin's constant Ω. Currently there are no natural examples of computable numbers that have been proved not to be periods, though it is easy to construct artificial examples using Cantor's diagonal argument. Plausible candidates for numbers that are not periods include \"e\", 1/π, and Euler–Mascheroni constant γ.\n\nThe periods are intended to bridge the gap between the algebraic numbers and the transcendental numbers. The class of algebraic numbers is too narrow to include many common mathematical constants, while the set of transcendental numbers is not countable, and its members are not generally computable. The set of all periods is countable, and all periods are computable, and in particular definable.\n\nMany of the constants known to be periods are also given by integrals of transcendental functions. Kontsevich and Zagier note that there \"seems to be no universal rule explaining why certain infinite sums or integrals of transcendental functions are periods\".\n\nKontsevich and Zagier conjectured that, if a period is given by two different integrals, then each integral can be transformed into the other using only the linearity of integrals, changes of variables, and the Newton–Leibniz formula\n\n(or, more generally, the Stokes formula).\n\nA useful property of algebraic numbers is that equality between two algebraic expressions can be determined algorithmically. And the conjecture of Kontsevich and Zagier would imply that equality of periods is also decidable: inequality of computable reals is known recursively enumerable; and conversely if two integrals agree, then an algorithm could confirm so by trying all possible ways to transform one of them into the other one.\n\nIt is not expected that Euler's number \"e\" and Euler–Mascheroni constant γ are periods. The periods can be extended to \"exponential periods\" by permitting the product of an algebraic function and the exponential function of an algebraic function as an integrand. This extension includes all algebraic powers of \"e\", the gamma function of rational arguments, and values of Bessel functions. If, further, Euler's constant is added as a new period, then according to Kontsevich and Zagier \"all classical constants are periods in the appropriate sense\".\n\n\n"}
{"id": "37485218", "url": "https://en.wikipedia.org/wiki?curid=37485218", "title": "Robert James Blattner", "text": "Robert James Blattner\n\nRobert James Blattner (6 August 1931 – 13 June 2015) was an mathematics professor at UCLA working on harmonic analysis, representation theory, and geometric quantization, who introduced Blattner's conjecture. Born in Milwaukee, Blattner received his bachelor's degree from Harvard University in 1953 and his Ph.D. from the University of Chicago in 1957. He joined the UCLA mathematics department in 1957 and remained on the staff until his retirement as professor emeritus in 1992.\n\nBlattner was a visiting scholar at the Institute for Advanced Study in 1964-65.\nIn 2012 he became a fellow of the American Mathematical Society.\n"}
{"id": "3108522", "url": "https://en.wikipedia.org/wiki?curid=3108522", "title": "Stunted projective space", "text": "Stunted projective space\n\nIn mathematics, a stunted projective space is a construction on a projective space of importance in homotopy theory, introduced by . Part of a conventional projective space is collapsed down to a point.\n\nMore concretely, in a real projective space, complex projective space or quaternionic projective space\n\nwhere \"K\" stands for the real numbers, complex numbers or quaternions, one can find (in many ways) copies of\n\nwhere \"m\" < \"n\". The corresponding stunted projective space is then\n\nwhere the notation implies that the \"KP\" has been identified to a point. This makes a topological space that is no longer a manifold. The importance of this construction was realised when it was shown that real stunted projective spaces arose as Spanier–Whitehead duals of spaces of Ioan James, so-called \"quasi-projective spaces\", constructed from Stiefel manifolds. Their properties were therefore linked to the construction of frame fields on spheres.\n\nIn this way the vector fields on spheres question was reduced to a question on stunted projective spaces: for R\"P\", is there a degree one mapping on the 'next cell up' (of the first dimension not collapsed in the 'stunting') that extends to the whole space? Frank Adams showed that this could not happen, completing the proof.\n\nIn later developments spaces \"KP\" and stunted lens spaces have also been used.\n"}
{"id": "565742", "url": "https://en.wikipedia.org/wiki?curid=565742", "title": "Symbolic method", "text": "Symbolic method\n\nIn mathematics, the symbolic method in invariant theory is an algorithm developed by , , , and in the 19th century for computing invariants of algebraic forms. It is based on treating the form as if it were a power of a degree one form, which corresponds to embedding a symmetric power of a vector space into the symmetric elements of a tensor product of copies of it.\n\nThe symbolic method uses a compact, but rather confusing and mysterious notation for invariants, depending on the introduction of new symbols \"a\", \"b\", \"c\", ... (from which the symbolic method gets its name) with apparently contradictory properties.\n\nThese symbols can be explained by the following example from . Suppose that \nis a binary quadratic form with an invariant given by the discriminant\nThe symbolic representation of the discriminant is \nwhere \"a\" and \"b\" are the symbols. The meaning of the expression (\"ab\") is as follows. First of all, (\"ab\") is a shorthand form for the determinant of a matrix whose rows are \"a\", \"a\" and \"b\", \"b\", so \nSquaring this we get\nNext we pretend that\nso that\nand we ignore the fact that this does not seem to make sense if \"f\" is not a power of a linear form. \nSubstituting these values gives\n\nMore generally if \nis a binary form of higher degree, then one introduces new variables \"a\", \"a\", \"b\", \"b\", \"c\", \"c\", with the properties\n\nWhat this means is that the following two vector spaces are naturally isomorphic:\nThe isomorphism is given by mapping \"a'a\", \"b'b\", ... to \"A\". This mapping does not preserve products of polynomials.\n\nThe extension to a form \"f\" in more than two variables \"x\", \"x\",\"x\"... is similar: one introduces symbols \"a\", \"a\",\"a\" and so on with the properties\n\nThe rather mysterious formalism of the symbolic method corresponds to embedding a symmetric product S(\"V\") of a vector space \"V\" into a tensor product of \"n\" copies of \"V\", as the elements preserved by the action of the symmetric group. In fact this is done twice, because the invariants of degree \"n\" of a quantic of degree \"m\" are the invariant elements of SS(\"V\"), which gets embedded into a tensor product of \"mn\" copies of \"V\", as the elements invariant under a wreath product of the two symmetric groups. The brackets of the symbolic method are really invariant linear forms on this tensor product, which give invariants of SS(\"V\") by restriction.\n\n\n"}
{"id": "17319580", "url": "https://en.wikipedia.org/wiki?curid=17319580", "title": "T.C. Mits", "text": "T.C. Mits\n\nT.C. Mits (acronym for \"the celebrated man in the street\"), is a term coined by Lillian Rosanoff Lieber to refer to an everyman. In Lieber's works, T.C. Mits was a character who made scientific topics more approachable to the public audience. \n\nThe phrase has enjoyed sparse use by authors in fields such as molecular biology, secondary education, and general semantics.\n\nDr. Lillian Rosanoff Lieber wrote this treatise on mathematical thinking in twenty chapters. The writing took a form that resembled free-verse poetry, though Lieber included an introduction stating that the form was meant only to facilitate rapid reading, rather than emulate free-verse. Lieber's husband, a fellow professor at Long Island University, Hugh Gray Lieber, provided illustrations for the book. The title of the book was meant to emphasize that mathematics can be understood by anyone, which was further shown when a special \"Overseas edition for the Armed Forces\" was published in 1942, and approved by the Council on Books in Wartime to be sent to American troops fighting in World War II.\n"}
{"id": "6296376", "url": "https://en.wikipedia.org/wiki?curid=6296376", "title": "Triple product rule", "text": "Triple product rule\n\nThe triple product rule, known variously as the cyclic chain rule, cyclic relation, cyclical rule or Euler's chain rule, is a formula which relates partial derivatives of three interdependent variables. The rule finds application in thermodynamics, where frequently three variables can be related by a function of the form \"f\"(\"x\", \"y\", \"z\") = 0, so each variable is given as an implicit function of the other two variables. For example, an equation of state for a fluid relates temperature, pressure, and volume in this manner. The triple product rule for such interrelated variables \"x\", \"y\", and \"z\" comes from using a reciprocity relation on the result of the implicit function theorem and is given by\n\nHere the subscripts indicate which variables are held constant when the partial derivative is taken. That is, to explicitly compute the partial derivative of \"x\" with respect to \"y\" with \"z\" held constant, one would write \"x\" as a function of \"y\" and \"z\" and take the partial derivative of this function with respect to \"y\" only.\n\nThe advantage of the triple product rule is that by rearranging terms, one can derive a number of substitution identities which allow one to replace partial derivatives which are difficult to analytically evaluate, experimentally measure, or integrate with quotients of partial derivatives which are easier to work with. For example,\n\nVarious other forms of the rule are present in the literature; these can be derived by permuting the variables {\"x\", \"y\", \"z\"}.\n\nAn informal derivation follows. Suppose that \"f\"(\"x\", \"y\", \"z\") = 0. Write \"z\" as a function of \"x\" and \"y\". Thus the total differential \"dz\" is\n\nSuppose that we move along a curve with \"dz\" = 0, where the curve is parameterized by \"x\". Thus \"y\" can be written in terms of \"x\", so on this curve\n\nTherefore, the equation for \"dz\" = 0 becomes\n\nSince this must be true for all \"dx\", rearranging terms gives\n\nDividing by the derivatives on the right hand side gives the triple product rule\n\nNote that this proof makes many implicit assumptions regarding the existence of partial derivatives, the existence of the exact differential \"dz\", the ability to construct a curve in some neighborhood with \"dz\" = 0, and the nonzero value of partial derivatives and their reciprocals. A formal proof based on mathematical analysis would eliminate these potential ambiguities.\n\nSuppose a function \"f(x,y,z)=0\", where \"x\",\"y\" and \"z\" are functions of each other. Write the total differentials of the variables\nSubstitute \"dy\" into \"dx\"\nBy using the chain rule one can show the coefficient of \"dx\" is equal to one, thus the coefficient of \"dz\" must be zero\nSubtracting the second term and multiplying by its inverse gives the triple product rule\n\nA geometric realization of the triple product rule can be found in its close ties to the velocity of a traveling wave \nshown on the right at time \"t\" (solid blue line) and at a short time later \"t+Δt\" (dashed). The wave maintains its shape as it propagates, so that a point at position \"x\" at time \"t\" will correspond to a point at position \"x+Δx\" at time \"t+Δt\",\nThis equation can only be satisfied for all \"x\" and \"t\" if \"kΔx-ωΔt=0\", resulting in the formula for the phase velocity\n\nTo elucidate the connection with the triple product rule, consider the point \"p\" at time \"t\" and its corresponding point (with the same height) \"p̄\" at \"t+Δt\". Define \"p\" as the point at time \"t\" whose x-coordinate matches that of \"p̄\", and define \"p̄\" to be the corresponding point of \"p\" as shown in the figure on the right. The distance \"Δx\" between \"p\" and \"p̄\" is the same as the distance between \"p\" and \"p̄\" (green lines), and dividing this distance by \"Δt\" yields the speed of the wave.\n\nTo compute \"Δx\", consider the two partial derivatives computed at \"p\",\nDividing these two partial derivatives and using the definition of the slope (rise divided by run) gives us the desired formula for \nwhere the negative sign accounts for the fact that \"p\" lies behind \"p\" relative to the wave's motion. Thus, the wave's velocity is given by\nFor infinitesimal \"Δt\", formula_20 and we recover the triple product rule\n\n\n"}
{"id": "26054348", "url": "https://en.wikipedia.org/wiki?curid=26054348", "title": "Vanna–Volga pricing", "text": "Vanna–Volga pricing\n\nThe Vanna–Volga method is a mathematical tool used in finance. It is a technique for pricing first-generation exotic options in foreign exchange market (FX) derivatives.\n\nIt consists of adjusting the Black–Scholes theoretical value (BSTV)\nby the cost of a portfolio which hedges three main risks\nassociated to the volatility of the option: the Vega formula_1, the Vanna\nand the Volga. \nThe Vanna is the sensitivity of the Vega with respect to a change in the spot FX rate:\n\nformula_2.\n\nSimilarly, the Volga is the sensitivity\nof the Vega with respect to a change of the implied volatility\nformula_3:\n\nformula_4.\n\nIf we consider a smile volatility term structure formula_5 with ATM strike formula_6, ATM volatility formula_7, and where formula_8 are the 25-Delta\ncall/put strikes (obtained by solving the equations formula_9 and formula_10 where formula_11 denotes the\nBlack–Scholes Delta sensitivity) then the hedging portfolio\nwill be composed of the \"at-the-money\" (ATM), \"risk-reversal\" (RR) and \"butterfly\" (BF)\nstrategies:\n\nformula_12\n\nwith formula_13 the Black–Scholes price of a call option (similarly for the put).\n\nThe simplest formulation of the Vanna–Volga method suggests that the\nVanna–Volga price formula_14 of an exotic instrument formula_15 is\ngiven by\n\nformula_16\n\nwhere by formula_17 denotes the Black–Scholes price of the\nexotic and the Greeks are calculated with ATM volatility and\n\nformula_18\n\nThese quantities represent a \"smile cost\", namely the\ndifference between the price computed with/without including the\nsmile effect.\n\nThe rationale behind the above formulation of the Vanna-Volga price is that one can extract\nthe \"smile cost\" of an exotic option by measuring the\n\"smile cost\" of a portfolio designed to hedge its Vanna and\nVolga risks. The reason why one chooses the strategies BF and RR\nto do this is because they are liquid FX instruments and they\ncarry mainly Volga, and respectively Vanna risks. The weighting\nfactors formula_19 and formula_20 represent\nrespectively the amount of RR needed to replicate the option's\nVanna, and the amount of BF needed to replicate the option's\nVolga. The above approach ignores the small (but non-zero)\nfraction of Volga carried by the RR and the small fraction of\nVanna carried by the BF. It further neglects the cost of hedging\nthe Vega risk. This has led to a more general formulation of the\nVanna-Volga method in which one considers that within the Black–Scholes\nassumptions the exotic option's Vega, Vanna and Volga can be\nreplicated by the weighted sum of three instruments:\n\nformula_21\n\nwhere the weightings are obtained by solving the system:\nformula_22\n\nwith\n\nformula_23,\nformula_24,\nformula_25\n\nGiven this replication, the Vanna–Volga method adjusts the BS\nprice of an exotic option by the \"smile cost\" of the above\nweighted sum (note that the ATM smile cost is zero by\nconstruction):\n\nformula_26\n\nwhere\n\nformula_27\n\nand\n\nformula_28\n\nThe quantities formula_29 can be interpreted as the\nmarket prices attached to a unit amount of Vega, Vanna and Volga,\nrespectively. The resulting correction, however, typically turns\nout to be too large. Market practitioners thus modify\nformula_14 to\n\nformula_31\n\nThe Vega contribution turns out to be\nseveral orders of magnitude smaller than the Vanna and Volga terms\nin all practical situations, hence one neglects it.\n\nThe terms formula_32 and formula_33 are put in by-hand and represent factors that ensure the correct behaviour of the price of an exotic option near a barrier:\nas the knock-out barrier level formula_34 of an option\nis gradually moved toward the spot level formula_35, the BSTV price of a\nknock-out option must be a monotonically decreasing function, converging\nto zero exactly at formula_36. Since the Vanna-Volga method is a\nsimple rule-of-thumb and not a rigorous model, there is no\nguarantee that this will be a priori the case. The attenuation factors are of a different from for the Vanna or the Volga\nof an instrument. This is because for barrier values close to the spot they behave differently: the Vanna becomes large while,\non the contrary, the Volga becomes small. Hence the\nattenuation factors take the form:\n\nformula_37\n\nwhere formula_38 represents some measure of the barrier(s)\nvicinity to the spot with the features\n\nformula_39\n\nThe coefficients formula_40 are found through calibration of the model to ensure that it reproduces the vanilla smile. Good candidates for formula_41 that ensure the appropriate behaviour close to the barriers are the \"survival probability\" and the \"expected first exit time\". Both of these quantities offer the desirable property that they vanish close to a barrier.\n\nThe survival probability formula_42 refers to the\nprobability that the spot does not touch one or more barrier\nlevels formula_43. For example, for a single barrier option we have\n\nformula_44 is the value of a \"no-touch\" option and formula_45 the discount factor between today and maturity. Similarly, for options with two barriers\nthe survival probability is given through the undiscounted value\nof a double-no-touch option.\n\nThe first exit time (FET) is the minimum between: (i) the time in\nthe future when the spot is expected to exit a barrier zone before\nmaturity, and (ii) maturity, if the spot has not hit any of the\nbarrier levels up to maturity. That is, if we denote the FET by\nformula_46 then formula_47minformula_48 where\nformula_49 such that formula_50 or\nformula_51 where formula_52 are the 'low' vs 'high' barrier levels and\nformula_53 the spot of today.\n\nThe first-exit time is the solution of the following PDE\n\nformula_54\n\nThis equation is solved backwards\nin time starting from the terminal condition formula_55 where formula_56 is the time to maturity and\nboundary conditions formula_57. In case of a single\nbarrier option we use the same PDE with either formula_58 or formula_59. The parameter formula_60 represents the risk-neutral drift of the underlying stochastic process.\n\n"}
{"id": "4575515", "url": "https://en.wikipedia.org/wiki?curid=4575515", "title": "Wadge hierarchy", "text": "Wadge hierarchy\n\nIn descriptive set theory, Wadge degrees are levels of complexity for sets of reals. Sets are compared by continuous reductions. The Wadge hierarchy is the structure of Wadge degrees.\n\nSuppose formula_1 and formula_2 are subsets of Baire space ω. formula_1 is Wadge reducible to formula_2 or formula_1 ≤ formula_2 if there is a continuous function formula_7 on ω with formula_8. The Wadge order is the preorder or quasiorder on the subsets of Baire space. Equivalence classes of sets under this preorder are called Wadge degrees, the degree of a set formula_1 is denoted by [formula_1]. The set of Wadge degrees ordered by the Wadge order is called the Wadge hierarchy.\n\nProperties of Wadge degrees include their consistency with measures of complexity stated in terms of definability. For example, if formula_1 ≤ formula_2 and formula_2 is a countable intersection of open sets, then so is formula_1. The same works for all levels of the Borel hierarchy and the difference hierarchy. The Wadge hierarchy plays an important role in models of the axiom of determinacy. Further interest in Wadge degrees comes from computer science, where some papers have suggested Wadge degrees are relevant to algorithmic complexity.\n\nThe Wadge game is a simple infinite game discovered by William Wadge (pronounced \"wage\"). It is used to investigate the notion of continuous reduction for subsets of Baire space. Wadge had analyzed the structure of the Wadge hierarchy for Baire space with games by 1972, but published these results only much later in his PhD thesis. In the Wadge game formula_15, player I and player II each in turn play integers which may depend on those played before. The outcome of the game is determined by checking whether the sequences x and y generated by players I and II are contained in the sets A and B, respectively. Player II wins if the outcome is the same for both players, i.e. formula_16 is in formula_1 if and only if formula_18 is in formula_2. Player I wins if the outcome is different. Sometimes this is also called the \"Lipschitz game\", and the variant where player II has the option to pass (but has to play infinitely often) is called the Wadge game.\n\nSuppose for a moment that the game is determined. If player I has a winning strategy, then this defines a continuous (even Lipschitz) map reducing formula_2 to the complement of formula_1, and if on the other hand player II has a winning strategy then you have a reduction of formula_1 to formula_2. For example, suppose that player II has a winning strategy. Map every sequence x to the sequence y that player II plays in formula_15 if player I plays the sequence x, where player II follows his or her winning strategy. This defines a is a continuous map f with the property that x is in formula_1 if and only if f(x) is in formula_2.\n\nWadge's lemma states that under the axiom of determinacy (AD), for any two subsets formula_27 of Baire space, formula_1 ≤formula_2 or formula_2 ≤ ω–formula_1. The assertion that the Wadge lemma holds for sets in Γ is the \"semilinear ordering principle\" for Γ or SLO(Γ). Any defines a linear order on the equivalence classes modulo complements. Wadge's lemma can be applied locally to any pointclass Γ, for example the Borel sets, Δ sets, Σ sets, or Π sets. It follows from determinacy of differences of sets in Γ. Since Borel determinacy is proved in ZFC, ZFC implies Wadge's lemma for Borel sets.\n\nMartin and Monk proved in 1973 that AD implies the Wadge order for Baire space is well founded. Hence under AD, the Wadge classes modulo complements form a wellorder. The Wadge rank of a set formula_1 is the order type of the set of Wadge degrees modulo complements strictly below [formula_1]. The length of the Wadge hierarchy has been shown to be Θ. Wadge also proved that the length of the Wadge hierarchy restricted to the Borel sets is φ(1) (or φ(2) depending on the notation), where φ is the γ Veblen function to the base ω (instead of the usual ω).\n\nAs for the Wadge lemma, this holds for any pointclass Γ, assuming the axiom of determinacy. If we associate with each set formula_1 the collection of all sets strictly below formula_1 on the Wadge hierarchy, this forms a pointclass. Equivalently, for each ordinal α≤θ the collection W of sets which show up before stage α is a pointclass. Conversely, every pointclass is equal to some formula_36. A pointclass is said to be \"self-dual\" if it is closed under complementation. It can be shown that W is self-dual if and only if α is either 0, an even successor ordinal, or a limit ordinal of countable cofinality.\n\nSimilar notions of reduction and degree arise by replacing the continuous functions by any class of functions \"F\" which contains the identity function and is closed under composition. Write formula_1 ≤ formula_2 if formula_8 for some function formula_7 in \"F\". Any such class of functions again determines a preorder on the subsets of Baire space. Degrees given by Lipschitz functions are called \"Lipschitz degrees\", and degrees from Borel functions \"Borel-Wadge degrees\".\n\n\n\n"}
{"id": "96017", "url": "https://en.wikipedia.org/wiki?curid=96017", "title": "Étienne Bézout", "text": "Étienne Bézout\n\nÉtienne Bézout (; 31 March 1730 – 27 September 1783) was a French mathematician who was born in Nemours, Seine-et-Marne, France, and died in Avon (near Fontainebleau), France.\n\nIn 1758 Bézout was elected an adjoint in mechanics of the French Academy of Sciences.\nBesides numerous minor works, wrote a \"Théorie générale des équations algébriques\", published at Paris in 1779, which in particular contained much new and valuable matter on the theory of elimination and symmetrical functions of the roots of an equation: he used determinants in a paper in the \"Histoire de l'académie royale\", 1764, but did not treat the general theory.\n\n\n\n"}
