{"id": "3205518", "url": "https://en.wikipedia.org/wiki?curid=3205518", "title": "ASTOS", "text": "ASTOS\n\nASTOS is a tool dedicated to mission analysis, Trajectory optimization, vehicle design and simulation for space scenarios, i.e. launch, re-entry missions, orbit transfers, Earth observation, navigation, coverage and re-entry safety assessments. It solves Aerospace problems with a data driven interface and automatic initial guesses. Since 1989, with the support of the European Space Agency, it has developed, and improved this trajectory optimization environment to compute optimal trajectories for a variety of complex multi-phase Optimal control problems. ASTOS is being extensively used at ESA and aerospace industry community to calculate mission analysis, optimal launch and entry trajectories and was one of the tools used by ESA to assess the risk due to the ATV 'Jules Verne' re-entry. ASTOS is compatible with Windows and Linux platforms and is maintained and commercialized by Astos Solutions GmbH.\n\nThe development of ASTOS (formerly named ALTOS) started in 1989 at the DLR in Oberpfaffenhofen and MBB (now Astrium).\nIn 1991 the Institute of Flight Mechanics and Control (IFR) at the University of Stuttgart under the head of Prof. Klaus Well took the responsibility for the development of ASTOS. In 1999 the commercialization of ASTOS began. In the period 2001-2006 ASTOS was sold by Technology Transfer Initiative of the University of Stuttgart (TTI). Since September 2006 the newly founded company Astos Solutions GmbH is responsible for development and sales of ASTOS.\n\nASTOS is being extensively used at aerospace agencies and industry since 1998, hereafter a not complete list of project is presented where the software was involved during the design or accomplishment of the space mission.\n\n\n"}
{"id": "16957829", "url": "https://en.wikipedia.org/wiki?curid=16957829", "title": "Applicative computing systems", "text": "Applicative computing systems\n\nApplicative computing systems, or ACS are the systems of object calculi founded on combinatory logic and lambda calculus. \nThe only essential notion which is under consideration in these systems is the representation of object. In combinatory logic the only metaoperator is application in a sense of applying one object to other. In lambda calculus two metaoperators are used: application – the same as in combinatory logic, and functional abstraction which binds the only variable in one object.\n\nThe objects generated in these systems are the functional entities with the following features:\n\n\nACS give a sound ground for applicative approach to programming.\n\nApplicative computing systems' lack of storage and history sensitivity is the basic reason they have not provided a foundation for computer design. Moreover, most applicative systems employ the substitution operation of the lambda calculus as their basic operation. This operation is one of virtually unlimited power, but its complete and efficient realization presents great difficulties to the machine designer.\n\n\n\n"}
{"id": "39484059", "url": "https://en.wikipedia.org/wiki?curid=39484059", "title": "Arason invariant", "text": "Arason invariant\n\nIn mathematics, the Arason invariant is a cohomological invariant associated to a quadratic form of even rank and trivial discriminant and Clifford invariant over a field \"k\" of characteristic not 2, taking values in H(\"k\",Z/2Z). It was introduced by .\n\nThe Rost invariant is a generalization of the Arason invariant to other algebraic groups.\n\nSuppose that \"W\"(\"k\") is the Witt ring of quadratic forms over a field \"k\" and \"I\" is the ideal of forms of even dimension. The Arason invariant is a group homomorphism from \"I\" to the Galois cohomology group H(\"k\",Z/2Z). It is determined by the property that on the 8-dimensional diagonal form with entries 1, –\"a\", –\"b\", \"ab\", -\"c\", \"ac\", \"bc\", -\"abc\" (the 3-fold Pfister form«\"a\",\"b\",\"c\"») it is given by the cup product of the classes of \"a\", \"b\", \"c\" in H(\"k\",Z/2Z) = \"k\"*/\"k\"*. The Arason invariant vanishes on \"I\", and it follows from the Milnor conjecture proved by Voevodsky that it is an isomorphism from \"I\"/\"I\" to H(\"k\",Z/2Z).\n\n"}
{"id": "5744937", "url": "https://en.wikipedia.org/wiki?curid=5744937", "title": "Arc (projective geometry)", "text": "Arc (projective geometry)\n\nAn (\"simple\") arc in finite projective geometry is a set of points which satisfies, in an intuitive way, a feature of \"curved\" figures in continuous geometries. Loosely speaking, they are sets of points that are far from \"line-like\" in a plane or far from \"plane-like\" in a three-dimensional space. In this finite setting it is typical to include the number of points in the set in the name, so these simple arcs are called -arcs. An important generalization of the -arc concept, also referred to as arcs in the literature, are the ()-arcs.\n\nIn a finite projective plane (not necessarily Desarguesian) a set of points such that no three points of are collinear (on a line) is called a . If the plane has order then , however the maximum value of can only be achieved if is even. In a plane of order , a -arc is called an oval and, if is even, a -arc is called a hyperoval.\n\nEvery conic in the Desarguesian projective plane PG(2,), i.e., the set of zeros of an irreducible homogeneous quadratic equation, is an oval. A celebrated result of Beniamino Segre states that when is odd, every -arc in PG(2,) is a conic (Segre's theorem). This is one of the pioneering results in finite geometry.\n\nIf is even and is a -arc in , then it can be shown via combinatorial arguments that there must exist a unique point in (called the nucleus of ) such that the union of and this point is a ( + 2)-arc. Thus, every oval can be uniquely extended to a hyperoval in a finite projective plane of even order.\n\nA -arc which can not be extended to a larger arc is called a complete arc. In the Desarguesian projective planes, PG(2,), no -arc is complete, so they may all be extended to ovals.\n\nIn the finite projective space PG() with , a set of points such that no points lie in a common hyperplane is called a (spatial) -arc. This definition generalizes the definition of a -arc in a plane (where ).\n\nA ()-arc () in a finite projective plane (not necessarily Desarguesian) is a set, of points of such that each line intersects in at most points, and there is at least one line that does intersect in points. A ()-arc is a -arc and may be referred to as simply an arc if the size is not a concern.\n\nThe number of points of a ()-arc in a projective plane of order is at most . When equality occurs, one calls a maximal arc.\n\nHyperovals are maximal arcs. Complete arcs need not be maximal arcs.\n\n\n"}
{"id": "57536639", "url": "https://en.wikipedia.org/wiki?curid=57536639", "title": "Asker Abiyev", "text": "Asker Abiyev\n\nAsker Ali Abiyev (born June 28, 1934), the inventor of Abiyev's Magic squares and Cubes, was born in Baku, Azarbaijan.\n\nAbiyev studied in Physics-Mathematics faculty in Baku State University from 1954 to 1957, and then, in the faculty of Physics in Moscow State University from 1957 to 1961. After some years of working, he continued his education from 1963 until 1966 as a post-graduate in Kurchatov Institute.\n\nIn 1961–1963, Aliyev worked in the Institute of Physics of Azerbaijan National Academy of Sciences. After his post-graduation from Kurchatov Institute, he returned to the National Academy in 1966.\n\nAfter 1969, he worked as a researcher in the Radiation Problems Sector of National Academy, and then, from 1976 until 1993, he worked as the head of the Laboratory of \"Radiation Physics of Semiconductors\".\n\nHe went to Ankara, Turkey, as a professor of Yavuz Sultan Private Science Lyceum in 1993 (until 2000). From 2000 to 2007, he was a professor in the Department of Mathematics at Gaziantep University in Gaziantep, Turkey.\n\nIn 1970 and 1988, Abiyev defended dissertations in the field of \"Physics of Semiconductors\" he obtained his \"Candidate of Physical-Mathematical Sciences\" title and in the field of Dielectrics, he got \"Doctor of Physical-Mathematical Sciences\" title. Later, in 1990, he obtained the title of Professor of Physical-Mathematical Sciences.\n"}
{"id": "1455603", "url": "https://en.wikipedia.org/wiki?curid=1455603", "title": "BEST theorem", "text": "BEST theorem\n\nIn graph theory, a part of discrete mathematics, the BEST theorem gives a product formula for the number of Eulerian circuits in directed (oriented) graphs. The name is an acronym of the names of people who discovered it: de Bruijn, van Aardenne-Ehrenfest, Smith and Tutte. \n\nLet \"G\" = (\"V\", \"E\") be a directed graph. An Eulerian circuit is a directed closed path which visits each edge exactly once. In 1736, Euler showed that \"G\" has an Eulerian circuit if and only if \"G\" is connected and the indegree is equal to outdegree at every vertex. In this case \"G\" is called Eulerian. We denote the indegree of a vertex \"v\" by deg(\"v\"). \n\nThe BEST theorem states that the number ec(\"G\") of Eulerian circuits in a connected Eulerian graph \"G\" is given by the formula\n\nHere \"t\"(\"G\") is the number of arborescences, which are trees directed towards the root at a fixed vertex \"w\" in \"G\". The number \"t(G)\" can be computed as a determinant, by the version of the matrix tree theorem for directed graphs. It is a property of Eulerian graphs that \"t\"(\"G\") = \"t\"(\"G\") for every two vertices \"v\" and \"w\" in a connected Eulerian graph \"G\".\n\nThe BEST theorem shows that the number of Eulerian circuits in directed graphs can be computed in polynomial time, a problem which is #P-complete for undirected graphs. It is also used in the asymptotic enumeration of Eulerian circuits of complete and complete bipartite graphs.\n\nThe BEST theorem was first stated in this form in a \"note added in proof\" to the paper of van Aardenne-Ehrenfest and de Bruijn (1951). The original proof was bijective and generalized the de Bruijn sequences. It is a variation on an earlier result by Smith and Tutte (1941).\n\n"}
{"id": "5007452", "url": "https://en.wikipedia.org/wiki?curid=5007452", "title": "Bipolar cylindrical coordinates", "text": "Bipolar cylindrical coordinates\n\nBipolar cylindrical coordinates are a three-dimensional orthogonal coordinate system that results from projecting the two-dimensional bipolar coordinate system in the\nperpendicular formula_1-direction. The two lines of foci \nformula_2 and formula_3 of the projected Apollonian circles are generally taken to be \ndefined by formula_4 and formula_5, respectively, (and by formula_6) in the Cartesian coordinate system.\n\nThe term \"bipolar\" is often used to describe other curves having two singular points (foci), such as ellipses, hyperbolas, and Cassini ovals. However, the term \"bipolar coordinates\" is never used to describe coordinates associated with those curves, e.g., elliptic coordinates.\n\nThe most common definition of bipolar cylindrical coordinates formula_7 is\n\nwhere the formula_11 coordinate of a point formula_12\nequals the angle formula_13 and the \nformula_14 coordinate equals the natural logarithm of the ratio of the distances formula_15 and formula_16 to the focal lines\n\n(Recall that the focal lines formula_2 and formula_3 are located at formula_4 and formula_5, respectively.) \n\nSurfaces of constant formula_11 correspond to cylinders of different radii\n\nthat all pass through the focal lines and are not concentric. The surfaces of constant formula_14 are non-intersecting cylinders of different radii\n\nthat surround the focal lines but again are not concentric. The focal lines and all these cylinders are parallel to the formula_1-axis (the direction of projection). In the formula_27 plane, the centers of the constant-formula_11 and constant-formula_14 cylinders lie on the formula_30 and formula_31 axes, respectively.\n\nThe scale factors for the bipolar coordinates formula_11 and formula_14 are equal\n\nwhereas the remaining scale factor formula_35. \nThus, the infinitesimal volume element equals\n\nand the Laplacian is given by \n\nOther differential operators such as formula_38 \nand formula_39 can be expressed in the coordinates formula_40 by substituting \nthe scale factors into the general formulae \nfound in orthogonal coordinates.\n\nThe classic applications of bipolar coordinates are in solving partial differential equations, \ne.g., Laplace's equation or the Helmholtz equation, for which bipolar coordinates allow a \nseparation of variables (in 2D). A typical example would be the electric field surrounding two \nparallel cylindrical conductors.\n\n\n"}
{"id": "25766973", "url": "https://en.wikipedia.org/wiki?curid=25766973", "title": "Cipolla's algorithm", "text": "Cipolla's algorithm\n\nIn computational number theory, Cipolla's algorithm is a technique for solving a congruence of the form\n\nwhere formula_2, so \"n\" is the square of \"x\", and where formula_3 is an odd prime. Here formula_4 denotes the finite field with formula_3 elements; formula_6. The algorithm is named after Michele Cipolla, an Italian mathematician who discovered it in 1907.\n\nAccording to\n\nCipolla's algorithm is also able to take square roots of powers of prime modula as well as prime modula.\n\nInputs:\n\nOutputs:\n\nStep 1 is to find an formula_11 such that formula_12 is not a square. There is no known algorithm for finding such an formula_13, except the trial and error method. Simply pick an formula_13 and by computing the Legendre symbol formula_15 one can see whether formula_13 satisfies the condition. The chance that a random formula_13 will satisfy is formula_18. With formula_3 large enough this is about formula_20. Therefore, the expected number of trials before finding a suitable \"a\" is about 2.\n\nStep 2 is to compute \"x\" by computing formula_21 within the field formula_22. This \"x\" will be the one satisfying formula_23\n\nIf formula_24, then formula_25 also holds. And since \"p\" is odd, formula_26. So whenever a solution \"x\" is found, there's always a second solution, \"-x\".\n\n(Note: All elements before step two are considered as an element of formula_27 and all elements in step two are considered as elements of formula_28).\n\nFind all \"x\" such that formula_29\n\nBefore applying the algorithm, it must be checked that formula_30 is indeed a square in formula_27. Therefore, the Legendre symbol formula_32 has to be equal to 1. This can be computed using Euler's criterion; formula_33 This confirms 10 being a square and hence the algorithm can be applied.\n\nSo formula_45 is a solution, as well as formula_46 Indeed, formula_47 and formula_48\n\nThe first part of the proof is to verify that formula_49 is indeed a field. For the sake of notation simplicity, formula_50 is defined as formula_51. Of course, formula_52 is a quadratic non-residue, so there is no square root in formula_4. This formula_50 can roughly be seen as analogous to the complex number i.\nThe field arithmetic is quite obvious. Addition is defined as\nMultiplication is also defined as usual. With keeping in mind that formula_56, it becomes\nNow the field properties have to be checked.\nThe properties of closure under addition and multiplication, associativity, commutativity and distributivity are easily seen. This is because in this case the field formula_58 is somewhat resembles the field of complex numbers (with formula_50 being the analogon of \"i\").<br>\nThe additive identity is formula_60, or more formally formula_61: Let formula_62, then\nThe multiplicative identity is formula_64, or more formally formula_65:\nThe only thing left for formula_58 being a field is the existence of additive and multiplicative inverses. It is easily seen that the additive inverse of formula_68 is formula_69, which is an element of formula_58, because formula_71. In fact, those are the additive inverse elements of \"x\" and \"y\". For showing that every non-zero element formula_72 has a multiplicative inverse, write down formula_73 and formula_74. In other words,\nSo the two equalities formula_76 and formula_77 must hold. Working out the details gives expressions for formula_78 and formula_79, namely\nThe inverse elements which are shown in the expressions of formula_78 and formula_79 do exist, because these are all elements of formula_4. This completes the first part of the proof, showing that formula_58 is a field.\n\nThe second and middle part of the proof is showing that for every element formula_86.\nBy definition, formula_87 is not a square in formula_4. Euler's criterion then says that\nThus formula_90. This, together with Fermat's little theorem (which says that formula_91 for all formula_92) and the knowledge that in fields of characteristic \"p\" the equation formula_93 holds, a relationship sometimes called the Freshman's dream, shows the desired result\n\nThe third and last part of the proof is to show that if formula_95, then formula_96.<br>\nCompute\nNote that this computation took place in formula_58, so this formula_99. But with Lagrange's theorem, stating that a non-zero polynomial of degree \"n\" has at most \"n\" roots in any field \"K\", and the knowledge that formula_100 has 2 roots in formula_4, these roots must be all of the roots in formula_58. It was just shown that formula_103 and formula_104 are roots of formula_100 in formula_58, so it must be that formula_107.\n\nAfter finding a suitable \"a\", the number of operations required for the algorithm is formula_108 multiplications, formula_109 sums, where \"m\" is the number of digits in the binary representation of \"p\" and \"k\" is the number of ones in this representation. To find \"a\" by trial and error, the expected number of computations of the Legendre symbol is 2. But one can be lucky with the first try and one may need more than 2 tries. In the field formula_58, the following two equalities hold\nwhere formula_56 is known in advance. This computation needs 4 multiplications and 4 sums.\nwhere formula_114 and formula_115. This operation needs 6 multiplications and 4 sums.\n\nAssuming that formula_116 (in the case formula_117, the direct computation formula_118 is much faster) the binary expression of formula_119 has formula_120 digits, of which \"k\" are ones. So for computing a formula_119 power of formula_122, the first formula has to be used formula_123 times and the second formula_124 times.\n\nFor this, Cipolla's algorithm is better than the Tonelli–Shanks algorithm if and only if formula_125, with formula_126 being the maximum power of 2 which divides formula_127.\n\nAccording to Dickson's \"History Of Numbers\", the following formula of Cipolla will find square roots of powers of prime modula:\n\nTaking the example in the wiki article we can see that this formula above does indeed take square roots of prime power modula.\n\nAs\n\nNow solve for formula_134 via: \n\nNow create the formula_136 and formula_137\n(See for mathematica code showing this above computation, remembering\nthat something close to complex modular arithmetic is going on here) \n\nAs such:\n\nand the final equation is:\n\n"}
{"id": "42732578", "url": "https://en.wikipedia.org/wiki?curid=42732578", "title": "Defensive expenditures", "text": "Defensive expenditures\n\nIn environmental accounting, defensive expenditures are expenditures that seek to minimise potential damage to oneself. Examples include defence and insurance.\n"}
{"id": "46891994", "url": "https://en.wikipedia.org/wiki?curid=46891994", "title": "Demiregular tiling", "text": "Demiregular tiling\n\nIn geometry, the \"demiregular tilings\" are a set of Euclidean tessellations made from 2 or more regular polygon faces. Different authors have listed different sets of tilings. A more systematic approach looking at symmetry orbits are the 2-uniform tilings of which there are 20. Some of the demiregular ones are actually 3-uniform tilings.\n\nGrünbaum and Shephard enumerated the full list of 20 2-uniform tilings in \"Tilings and Paterns\", 1987:\n\nGhyka lists 10 of them with 2 or 3 vertex types, calling them semiregular polymorph partitions.\nSteinhaus gives 5 examples of non-homogeneous tessellations of regular polygons beyond the 11 regular and semiregular ones. (All of them have 2 types of vertices, while one is 3-uniform.)\nCritchlow identifies 14 demi-regular tessellations, with 7 being 2-uniform, and 7 being 3-uniform.\n\nHe codes letter names for the vertex types, with superscripts to distinguish face orders. He recognizes A, B, C, D, F, and J can't be a part of continuous coverings of the whole plane.\n\n\n"}
{"id": "1343980", "url": "https://en.wikipedia.org/wiki?curid=1343980", "title": "Deontic logic", "text": "Deontic logic\n\nDeontic logic is the field of philosophical logic that is concerned with obligation, permission, and related concepts. Alternatively, a deontic logic is a formal system that attempts to capture the essential logical features of these concepts. Typically, a deontic logic uses \"OA\" to mean \"it is obligatory that A\", (or \"it ought to be (the case) that A\"), and \"PA\" to mean \"it is permitted (or permissible) that A\".\n\nThe term \"deontic\" is derived from the Ancient Greek δέον \"déon\" (gen.: δέοντος \"déontos\"), meaning \"that which is binding or proper.\"\n\nIn von Wright's first system, obligatoriness and permissibility were treated as features of \"acts\". It was found not much later that a deontic logic of \"propositions\" could be given a simple and elegant Kripke-style semantics, and von Wright himself joined this movement. The deontic logic so specified came to be known as \"standard deontic logic,\" often referred to as SDL, KD, or simply D. It can be axiomatized by adding the following axioms to a standard axiomatization of classical propositional logic:\n\nIn English, these axioms say, respectively:\n\n\n\"FA\", meaning it is forbidden that \"A\", can be defined (equivalently) as formula_3 or formula_4.\n\nThere are two main extensions of SDL that are usually considered. The first results by adding an alethic modal operator formula_5 in order to express the Kantian claim that \"ought implies can\":\n\nwhere formula_7. It is generally assumed that formula_5 is at least a KT operator, but most commonly it is taken to be an S5 operator.\n\nThe other main extension results by adding a \"conditional obligation\" operator O(A/B) read \"It is obligatory that A given (or conditional on) B\". Motivation for a conditional operator is given by considering the following (\"Good Samaritan\") case. It seems true that the starving and poor ought to be fed. But that the starving and poor are fed implies that there are starving and poor. By basic principles of SDL we can infer that there ought to be starving and poor! The argument is due to the basic K axiom of SDL together with the following principle valid in any normal modal logic:\n\nIf we introduce an intensional conditional operator then we can say that the starving ought to be fed \"only on the condition that there are in fact starving\": in symbols O(A/B). But then the following argument fails on the usual (e.g. Lewis 73) semantics for conditionals: from O(A/B) and that A implies B, infer OB.\n\nIndeed, one might define the unary operator O in terms of the binary conditional one O(A/B) as formula_10, where formula_11 stands for an arbitrary tautology of the underlying logic (which, in the case of SDL, is classical). Similarly Alan R. Anderson (1959) shows how to define O in terms of the alethic operator formula_5 and a deontic constant (i.e. 0-ary modal operator) s standing for some sanction (i.e. bad thing, prohibition, etc.): formula_13. Intuitively, the right side of the biconditional says that A's failing to hold necessarily (or strictly) implies a sanction.\n\nAn important problem of deontic logic is that of how to properly represent conditional obligations, e.g. \"If you smoke (s), then you ought to use an ashtray (a). \" It is not clear that either of the following representations is adequate:\n\nUnder the first representation it is vacuously true that if you commit a forbidden act, then you ought to commit any other act, regardless of whether that second act was obligatory, permitted or forbidden (Von Wright 1956, cited in Aqvist 1994). Under the second representation, we are vulnerable to the gentle murder paradox, where the plausible statements (1) \"if you murder, you ought to murder gently\", (2) \"you do commit murder\", and (3) \"to murder gently you must murder\" imply the less plausible statement: \"you ought to murder\". Others argue that \"must\" in the phrase \"to murder gently you must murder\" is a mistranslation from the ambiguous English word (meaning either \"implies\" or \"ought\"). Interpreting \"must\" as \"implies\" does not allow one to conclude \"you ought to murder\" but only a repetition of the given \"you murder\". Misinterpreting \"must\" as \"ought\" results in a perverse axiom, not a perverse logic. With use of negations one can easily check if the ambiguous word was mistranslated by considering which of the following two English statements is equivalent with the statement \"to murder gently you must murder\": is it equivalent to \"if you murder gently it is forbidden not to murder\" or \"if you murder gently it is impossible not to murder\" ?\n\nSome deontic logicians have responded to this problem by developing dyadic deontic logics, which contain binary deontic operators:\n\n(The notation is modeled on that used to represent conditional probability.) Dyadic deontic logic escapes some of the problems of standard (unary) deontic logic, but it is subject to some problems of its own.\n\nMany other varieties of deontic logic have been developed, including non-monotonic deontic logics, paraconsistent deontic logics, and dynamic deontic logics.\n\nPhilosophers from the Indian Mimamsa school to those of Ancient Greece have remarked on the formal logical relations of deontic concepts and philosophers from the late Middle Ages compared deontic concepts with alethic ones. In his \"Elementa juris naturalis\", Leibniz notes the logical relations between the \"licitum\", \"illicitum\", \"debitum\", and \"indifferens\" are equivalent to those between the \"possible\", \"impossible\", \"necessarium\", and \"contingens\" respectively.\n\nErnst Mally, a pupil of Alexius Meinong, was the first to propose a formal system of deontic logic in his \"Grundgesetze des Sollens\" and he founded it on the syntax of Whitehead's and Russell's propositional calculus. Mally's deontic vocabulary consisted of the logical constants U and ∩, unary connective !, and binary connectives f and ∞.\nMally defined f, ∞, and ∩ as follows:\nMally proposed five informal principles:\nHe formalized these principles and took them as his axioms:\nFrom these axioms Mally deduced 35 theorems, many of which he rightly considered strange. Karl Menger showed that !A ↔ A is a theorem and thus that the introduction of the ! sign is irrelevant and that A ought to be the case if A is the case. After Menger, philosophers no longer considered Mally's system viable. Gert Lokhorst lists Mally's 35 theorems and gives a proof for Menger's theorem at the Stanford Encyclopedia of Philosophy under \"Mally's Deontic Logic\".\n\nThe first plausible system of deontic logic was proposed by G. H. von Wright in his paper \"Deontic Logic\" in the philosophical journal \"Mind\" in 1951. (Von Wright was also the first to use the term \"deontic\" in English to refer to this kind of logic although Mally published the German paper \"Deontik\" in 1926.) Since the publication of von Wright's seminal paper, many philosophers and computer scientists have investigated and developed systems of deontic logic. Nevertheless, to this day deontic logic remains one of the most controversial and least agreed-upon areas of logic.\nG. H. von Wright did not base his 1951 deontic logic on the syntax of the propositional calculus as Mally had done, but was instead influenced by alethic modal logics, which Mally had not benefited from. In 1964, von Wright published \"A New System of Deontic Logic\", which was a return to the syntax of the propositional calculus and thus a significant return to Mally's system. (For more on von Wright's departure from and return to the syntax of the propositional calculus, see \"Deontic Logic: A Personal View\" and \"A New System of Deontic Logic\", both by Georg Henrik von Wright.) G. H. von Wright's adoption of the modal logic of possibility and necessity for the purposes of normative reasoning was a return to Leibniz.\n\nDeontic logic faces Jørgensen's dilemma..\nThis problem is best seen as a trilemma. \nThe following three claims are incompatible:\nResponses to this problem involve rejecting one of the three premises.\nThe input/output logics reject the first premise. \nThey provide inference mechanism on elements without presupposing that these elements have truth-values. \nAlternatively, one can deny the second premise. One way to do this is to distinguish between the norm itself and a proposition about the norm.\nAccording to this response, only the proposition about the norm has a truth-value. \nFinally, one can deny the third premise. But this is to deny that there is a logic of norms worth investigating.\n\n\n\n"}
{"id": "8586502", "url": "https://en.wikipedia.org/wiki?curid=8586502", "title": "Enrolled actuary", "text": "Enrolled actuary\n\nAn enrolled actuary is an actuary enrolled by the Joint Board for the Enrollment of Actuaries under the Employee Retirement Income Security Act of 1974 (ERISA). Enrolled actuaries, under regulations of the Department of the Treasury and the Department of Labor, perform a variety of tasks with respect to pension plans in the United States under ERISA. As of August 2016, there were approximately 4,200 enrolled actuaries.\n\nThe Joint Board for the Enrollment of Actuaries administers two examinations to prospective enrolled actuaries. Once the two examinations have been passed, and an individual has also obtained sufficient relevant professional experience, that individual becomes an enrolled actuary.\n\nThe first exam (EA-1) tests basic knowledge of the mathematics of compound interest, the mathematics of life contingencies, and practical demographic analysis. \n\nThe second (EA-2) examination consists of two segments, which are offered during separate exam sittings in either the fall or the spring. Segment F covers the selection of actuarial assumptions, actuarial cost methods, and the calculation of minimum (required) and maximum (tax-deductible) contributions to pension plans. Segment L tests knowledge of relevant federal pension laws (in particular, the provisions of ERISA) as they affect pension actuarial practice.\n\nEnrolled actuaries generally work for human resource consulting firms, investment and insurance brokers, accounting firms, government organizations, and law firms. Some firms that employ enrolled actuaries combine two or more of these practice specialties.\n\nMany enrolled actuaries belong to one or more of the following organizations: the Society of Actuaries, the American Academy of Actuaries. the Conference of Consulting Actuaries or the American Society of Pension Professionals & Actuaries.\n\n"}
{"id": "3062637", "url": "https://en.wikipedia.org/wiki?curid=3062637", "title": "Estimation of distribution algorithm", "text": "Estimation of distribution algorithm\n\nEstimation of distribution algorithms (EDAs), sometimes called probabilistic model-building genetic algorithms (PMBGAs), are stochastic optimization methods that guide the search for the optimum by building and sampling explicit probabilistic models of promising candidate solutions. Optimization is viewed as a series of incremental updates of a probabilistic model, starting with the model encoding the uniform distribution over admissible solutions and ending with the model that generates only the global optima.\n\nEDAs belong to the class of evolutionary algorithms. The main difference between EDAs and most conventional evolutionary algorithms is that evolutionary algorithms generate new candidate solutions using an \"implicit\" distribution defined by one or more variation operators, whereas EDAs use an \"explicit\" probability distribution encoded by a Bayesian network, a multivariate normal distribution, or another model class. Similarly as other evolutionary algorithms, EDAs can be used to solve optimization problems defined over a number of representations from vectors to LISP style S expressions, and the quality of candidate solutions is often evaluated using one or more objective functions.\n\nThe general procedure of an EDA is outlined in the following:\nUsing explicit probabilistic models in optimization allowed EDAs to feasibly solve optimization problems that were notoriously difficult for most conventional evolutionary algorithms and traditional optimization techniques, such as problems with high levels of epistasis. Nonetheless, the advantage of EDAs is also that these algorithms provide an optimization practitioner with a series of probabilistic models that reveal a lot of information about the problem being solved. This information can in turn be used to design problem-specific neighborhood operators for local search, to bias future runs of EDAs on a similar problem, or to create an efficient computational model of the problem.\n\nFor example, if the population is represented by bit strings of length 4, the EDA can represent the population of promising solution using a single vector of four probabilities (p1, p2, p3, p4) where each component of p defines the probability of that position being a 1. Using this probability vector it is possible to create an arbitrary number of candidate solutions.\n\nThis section describes the models built by some well known EDAs of different levels of complexity. It is always assumed a population formula_1 at the generation formula_2, a selection operator formula_3, a model-building operator formula_4 and a sampling operator formula_5.\n\nThe most simple EDAs assume that decision variables are independent, i.e. formula_6. Therefore, univariate EDAs rely only on univariate statistics and multivariate distributions must be factorized as the product of formula_7 univariate probability distributions,\n\nformula_8\n\nSuch factorizations are used in many different EDAs, next we describe some of them.\n\nThe UMDA is a simple EDA that uses an operator formula_9 to estimate marginal probabilities from a selected population formula_10. By assuming formula_10 contain formula_12 elements, formula_9 produces probabilities:\n\nformula_14\n\nEvery UMDA step can be described as follows\n\nformula_15\n\nThe PBIL, represents the population implicitly by its model, from which it samples new solutions and updates the model. At each generation, formula_16 individuals are sampled and formula_17 are selected. Such individuals are then used to update the model as follows\n\nformula_18\n\nwhere formula_19 is a parameter defining the learning rate, a small value determines that the previous model formula_20 should be only slightly modified by the new solutions sampled. PBIL can be described as\n\nformula_21\n\nThe CGA, also relies on the implicit populations defined by univariate distributions. At each generation formula_2, two individuals formula_23 are sampled, formula_24. The population formula_1 is then sort in decreasing order of fitness, formula_26, with formula_27 being the best and formula_28 being the worst solution. The CGA estimates univariate probabilities as follows\n\nformula_29\n\nwhere, formula_19 is a constant defining the learning rate, usually set to formula_31. The CGA can be defined as\n\nformula_32\n\nAlthough univariate models can be computed efficiently, in many cases they are not representative enough to provide better performance than GAs. In order to overcome such a drawback, the use of bivariate factorizations was proposed in the EDA community, in which dependencies between pairs of variables could be modeled. A bivariate factorization can be defined as follows, where formula_33 contains a possible variable dependent to formula_34, i.e. formula_35.\n\nformula_36\n\nBivariate and multivariate distributions are usually represented as Probabilistic Graphical Models (graphs), in which edges denote statistical dependencies (or conditional probabilities) and vertices denote variables. To learn the structure of a PGM from data linkage-learning is employed.\n\nThe MIMIC factorizes the joint probability distribution in a chain-like model representing successive dependencies between variables. It finds a permutation of the decision variables, formula_37, such that formula_38 minimizes the Kullback-Leibler divergence in relation to the true probability distribution, i.e. formula_39. MIMIC models a distribution\n\nformula_40\n\nNew solutions are sampled from the leftmost to the rightmost variable, the first is generated independently and the others according to conditional probabilities. Since the estimated distribution must be recomputed each generation, MIMIC uses concrete populations in the following way\n\nformula_41\n\nThe BMDA factorizes the joint probability distribution in bivariate distributions. First, a randomly chosen variable is added as a node in a graph, the most dependent variable to one of those in the graph is chosen among those not yet in the graph, this procedure is repeated until no remaining variable depends on any variable in the graph (verified according to a threshold value).\n\nThe resulting model is a forest with multiple trees rooted at nodes formula_42. Considering formula_43 the non-root variables, BMDA estimates a factorized distribution in which the root variables can be sampled independently, whereas all the others must be conditioned to the parent variable formula_33.\n\nformula_45\n\nEach step of BMDA is defined as follows\n\nformula_46\n\nThe next stage of EDAs development was the use of multivariate factorizations. In this case, the joint probability distribution is usually factorized in a number of components of limited size formula_47.\n\nformula_48\n\nThe learning of PGMs encoding multivariate distributions is a computationally expensive task, therefore, it is usual for EDAs to estimate multivariate statistics from bivariate statistics. Such relaxation allows PGM to be built in polynomial time in formula_7; however, it also limits the generality of such EDAs.\n\nThe ECGA was one of the first EDA to employ multivariate factorizations, in which high-order dependencies among decision variables can be modeled. Its approach factorizes the joint probability distribution in the product of multivariate marginal distributions. Assume formula_50 is a set of subsets, in which every formula_51 is a linkage set, containing formula_52 variables. The factorized joint probability distribution is represented as follows\n\nformula_53\n\nThe ECGA popularized the term \"linkage-learning\" as denoting procedures that identify linkage sets. Its linkage-learning procedure relies on two measures: (1) the Model Complexity (MC) and (2) the Compressed Population Complexity (CPC). The MC quantifies the model representation size in terms of number of bits required to store all the marginal probabilities\n\nformula_54\n\nThe CPC, on the other hand, quantifies the data compression in terms of entropy of the marginal distribution over all partitions, where formula_12 is the selected population size, formula_56 is the number of decision variables in the linkage set formula_57 and formula_58 is the joint entropy of the variables in formula_57\n\nformula_60\n\nThe linkage-learning in ECGA works as follows: (1) Insert each variable in a cluster, (2) compute CCC = MC + CPC of the current linkage sets, (3) verify the increase on CCC provided by joining pairs of clusters, (4) effectively joins those clusters with highest CCC improvement. This procedure is repeated until no CCC improvements are possible and produces a linkage model formula_61. The ECGA works with concrete populations, therefore, using the factorized distribution modeled by ECGA, it can be described as\n\nformula_62\n\nThe BOA uses Bayesian networks to model and sample promising solutions. Bayesian networks are directed acyclic graphs, with nodes representing variables and edges representing conditional probabilities between pair of variables. The value of a variable formula_63 can be conditioned on a maximum of formula_64 other variables, defined in formula_33. BOA builds a PGM encoding a factorized joint distribution, in which the parameters of the network, i.e. the conditional probabilities, are estimated from the selected population using the maximum likelihood estimator.\n\nformula_66\n\nThe Bayesian network structure, on the other hand, must be built iteratively (linkage-learning). It starts with a network without edges and, at each step, adds the edge which better improves some scoring metric (e.g. Bayesian information criterion (BIC) or Bayesian-Dirichlet metric with likelihood equivalence (BDe)). The scoring metric evaluates the network structure according to its accuracy in modeling the selected population. From the built network, BOA samples new promising solutions as follows: (1) it computes the ancestral ordering for each variable, each node being preceded by its parents; (2) each variable is sampled conditionally to its parents. Given such scenario, every BOA step can be defined as\n\nformula_67\n\nThe LTGA differs from most EDA in the sense it does not explicitly model a probabilisty distribution but only a linkage model, called linkage-tree. A linkage formula_68 is a set of linkage sets with no probability distribution associated, therefore, there is no way to sample new solutions directly from formula_68. The linkage model is a linkage-tree produced stored as a Family of sets (FOS).\n\nformula_70\n\nThe linkage-tree learning procedure is a hierarchical clustering algorithm, which work as follows. At each step the two \"closest\" clusters formula_71 and formula_72 are merged, this procedure repeats until only one cluster remains, each subtree is stored as a subset formula_73.\n\nThe LTGA uses formula_74 to guide an \"optimal mixing\" procedure which resembles a recombination operator but only accepts improving moves. We denote it as formula_75, where the notation formula_76 indicates the transfer of the genetic material indexed by formula_57 from formula_78 to formula_79.\nThe LTGA does not implement typical selection operators, instead, selection is performed during recombination. Similar ideas have been usually applied into local-search heuristics and, in this sense, the LTGA can be seen as an hybrid method. In summary, one step of the LTGA is defined as\n\nformula_95\n\n\n"}
{"id": "54956115", "url": "https://en.wikipedia.org/wiki?curid=54956115", "title": "Ethics in mathematics", "text": "Ethics in mathematics\n\nEthics in mathematics is a field of applied ethics, the inquiry into ethical aspects of the applications of mathematics. It deals with the professional responsibilities of mathematicians whose work influences decisions with major consequences, such as in law, finance, the military, and environmental science.\n\nMathematicians in industrial, scientific, military and intelligence roles crucially influence decisions with large consequences. For example, complex calculations were needed for the success of the Manhattan Project, while the overextended use of the Gaussian copula formula to price derivatives before the Global Financial Crisis of 2008 has been called \"the formula that killed Wall Street\", and the theory of global warming depends on the reliability of mathematical models of climate. For the same reason as in medical ethics and engineering ethics, the high impact of the consequences of decisions imposes serious ethical obligations on practitioners to consider the rights and wrongs of their advice and decisions.\n\nThese illustrate the major consequences of numerical mistakes and hence the need for ethical care.\n\nMathematicians in professional roles in finance and similar work have a particular responsibility to ensure they use the best methods and data to reach the right answer, as the prestige of mathematics is high and others rely on mathematical results which they cannot fully understand. Other ethical issues are shared with information economy professionals in general, such as duty of care, confidentiality of information, whistleblowing, and avoiding conflict of interest.\n\nMuch of mathematics as used in applications involves the drawing of conclusions from quantitative data. It is recognised that there are many difficulties in reaching and communicating such conclusions accurately, honestly and with due regard to the uncertainties that remain. It is easy for a statistician to mislead clients whose understanding of data and inference is less developed, so statisticians have professional responsibilities to act fairly.\n\nThe American Mathematical Society publishes a code of ethical guidelines for mathematical researchers. The responsibilities of researchers include being knowledgeable in the field, avoiding plagiarism and giving credit, to publish without unreasonable delay, and to correct errors. The European Mathematical Society Ethics Committee also publishes a code of practice relating to the publication, editing and refereeing of research.\n\nIt has been argued that as pure mathematical research is relatively harmless, it raises few urgent ethical issues. However, that raises the question of whether and why pure mathematics is ethically worth doing, given that it consumes the lives of many highly intelligent people who could be making more immediately useful contributions.\n\nCourses in the ethics of mathematics remain rare. The University of New South Wales taught a compulsory course on Professional Issues and Ethics in Mathematics in its mathematics degrees from 1998 to 2012.\n\n\n"}
{"id": "4389572", "url": "https://en.wikipedia.org/wiki?curid=4389572", "title": "Factorial moment generating function", "text": "Factorial moment generating function\n\nIn probability theory and statistics, the factorial moment generating function of the probability distribution of a real-valued random variable \"X\" is defined as \nfor all complex numbers \"t\" for which this expected value exists. This is the case at least for all \"t\" on the unit circle formula_2, see characteristic function. If \"X\" is a discrete random variable taking values only in the set {0,1, ...} of non-negative integers, then formula_3 is also called probability-generating function of \"X\" and formula_4 is well-defined at least for all \"t\" on the closed unit disk formula_5.\n\nThe factorial moment generating function generates the factorial moments of the probability distribution.\nProvided formula_3 exists in a neighbourhood of \"t\" = 1, the \"n\"th factorial moment is given by \nwhere the Pochhammer symbol (\"x\") is the falling factorial\n\nSuppose \"X\" has a Poisson distribution with expected value λ, then its factorial moment generating function is\n(use the definition of the exponential function) and thus we have\n\n"}
{"id": "2480226", "url": "https://en.wikipedia.org/wiki?curid=2480226", "title": "Farkas' lemma", "text": "Farkas' lemma\n\nFarkas' lemma is a solvability theorem for a finite system of linear inequalities in mathematics. It was originally proven by the Hungarian mathematician Gyula Farkas.\nFarkas' lemma is the key result underpinning the linear programming duality and has played a central role in the development of mathematical optimization (alternatively, mathematical programming). It is used amongst other things in the proof of the Karush–Kuhn–Tucker theorem in nonlinear programming.\n\nGeneralizations of the Farkas' lemma are about the solvability theorem for convex inequalities, i.e., infinite system of linear inequalities. Farkas' lemma belongs to a class of statements called \"theorems of the alternative\": a theorem stating that exactly one of two systems has a solution.\n\nThere are a number of slightly different (but equivalent) formulations of the lemma in the literature. The one given here is due to .\nHere, the notation formula_2 means that all components of the vector formula_3 are nonnegative. Let the cone generated by the columns of formula_4 denoted by formula_5. Then formula_6 is a closed convex cone. The vector formula_3 proves that formula_8 lies in formula_9 while the vector formula_10 gives a linear functional that separates formula_8 from formula_9.\n\nLet formula_13 denote the columns of formula_4. In terms of these vectors, Farkas' lemma states that exactly one of the following two statements is true:\n\nThe vectors formula_21 with nonnegative coefficients constitute the convex cone of the set formula_22 so the first statement says that formula_8 is in this cone.\n\nThe second statement says that there exists a vector formula_10 such that the angle of formula_10 with the vectors formula_26 is at most 90° while the angle of formula_10 with the vector formula_8 is more than 90°. The hyperplane normal to this vector has the vectors formula_26 on one side and the vector formula_8 on the other side. Hence, this hyperplane separates the vectors in the cone of formula_22 and the vector formula_8.\n\nFor example, let \"n,m\"=2 and \"a\" = (1,0) and \"a\" = (1,1). The convex cone spanned by \"a\" and \"a\" can be seen as a wedge-shaped slice of the first quadrant in the \"x-y\" plane. Now, suppose \"b\" = (0,1). Certainly, \"b\" is not in the convex cone \"a\"\"x\"+\"a\"\"x\". Hence, there must be a separating hyperplane. Let \"y\" = (1,−1). We can see that \"a\" · \"y\" = 1, \"a\" · \"y\" = 0, and \"b\" · \"y\" = −1. Hence, the hyperplane with normal \"y\" indeed separates the convex cone \"a\"\"x\"+\"a\"\"x\" from \"b\".\n\n =\\{ \\mathbf{z} \\in \\mathbb{R}^{n} | \\mathbf{z}^{\\mathsf{T}} \\mathbf{x} \\geq 0, \\forall \\mathbf{x} \\in \\mathbf{S} \\} </math>. If convex cone formula_33 is closed, then exactly one of the following two statements is true:\n\nGeneralized Farkas' lemma can be interpreted geometrically as follows: a vector is either in a given closed convex cone or that there exists a hyperplane separating the vector from the cone—there are no other possibilities. The closedness condition is necessary, see Separation theorem I in Hyperplane separation theorem. For original Farkas' lemma, formula_40 is the nonnegative orthant formula_41, hence the closedness condition holds automatically. Indeed, for polyhedral convex cone, i.e., there exists a formula_42 such that formula_43, the closedness condition holds automatically. In convex optimization, various kinds of constraint qualification, e.g. Slater's condition, are responsible for closedness of the underlying convex cone formula_6. \n\nBy setting formula_45 and formula_46 in Generalized Farkas' lemma, we obtain the following corollary about the solvability for a finite system of linear equalities. \nFarkas's lemma can be varied to many further theorems of alternative by simple modifications, such as Gordan's theorem: Either formula_48 has a solution \"x\", or formula_49 has a nonzero solution \"y\" with \"y\" ≥ 0.\n\nCommon applications of Farkas' lemma include proving the strong and weak duality theorem associated with linear programming, game theory at a basic level and the Kuhn–Tucker constraints. An extension of Farkas' lemma can be used to analyze the strong duality conditions for and construct the dual of a semidefinite program. It is sufficient to prove the existence of the Kuhn–Tucker constraints using the Fredholm alternative but for the condition to be necessary, one must apply Von Neumann's minimax theorem to show the equations derived by Cauchy are not violated.\n\nA particularly suggestive and easy-to-remember version is the following: if a set of inequalities has no solution, then a contradiction can be produced from it by linear combination with nonnegative coefficients. In formulas: if formula_50 ≤ formula_51 is unsolvable then formula_52, formula_53, formula_54 ≥ formula_55 has a solution. (Note that formula_56 is a combination of the left hand sides, formula_57 a combination of the right hand side of the inequalities. Since the positive combination produces a zero vector on the left and a −1 on the right, the contradiction is apparent.)\n\n\n"}
{"id": "387931", "url": "https://en.wikipedia.org/wiki?curid=387931", "title": "French curve", "text": "French curve\n\nA French curve is a template usually made from metal, wood or plastic composed of many different curves. It is used in manual drafting to draw smooth curves of varying radii. The shapes are segments of the Euler spiral or clothoid curve. The curve is placed on the drawing material, and a pencil, knife or other implement is traced around its curves to produce the desired result.\n\nAs modern computer-aided design (CAD) systems use vector-based graphics to achieve a precise radius, mechanical templates (and most mechanical drawing techniques) have become obsolete. Digital computers can also be used to generate a set of coordinates that accurately describe an arbitrary curve, and the points can be connected with line segments to approximate the curve with a high degree of accuracy. Some computer-graphics systems make use of Bézier curves, which allow a curve to be bent in real time on a display screen to follow a set of coordinates, much in the way a French curve would be placed on a set of three or four points on paper.\n\n"}
{"id": "2310971", "url": "https://en.wikipedia.org/wiki?curid=2310971", "title": "Gauss's lemma (polynomial)", "text": "Gauss's lemma (polynomial)\n\nIn algebra, in the theory of polynomials (a subfield of ring theory), Gauss's lemma is either of two related statements about polynomials with integer coefficients:\n\nThis second statement is a consequence of the first (see proof below). The first statement and proof of the lemma are in Article 42 of Carl Friedrich Gauss's \"Disquisitiones Arithmeticae\" (1801). These statements have several generalizations described below.\n\nThe notion of primitive polynomial used here (which differs from the notion with the same name in the context of finite fields) is defined in any polynomial ring \"R\"[\"X\"] where \"R\" is a commutative ring: a polynomial \"P\" in \"R\"[\"X\"] is primitive if the only elements of \"R\" that divide all coefficients of \"P\" at once are the invertible elements of \"R\". In the case where \"R\" is the ring Z of the integers, this is equivalent to the condition that no prime number divides all the coefficients of \"P\". The notion of irreducible element is defined in any integral domain: an element is irreducible if it is not invertible and cannot be written as a product of two non-invertible elements. If \"R\" is an integral domain then so is the polynomial ring \"R\"[\"X\"] because the leading coefficient of product of non-zero polynomials in \"R\"[\"X\"] is equal to the product of their leading coefficients, hence is nonzero. A non-constant irreducible polynomial in \"R\"[\"X\"] is one that is not a product of two non-constant polynomials \"and\" which is primitive (because being primitive excludes precisely non-invertible constant polynomials as factors). Note that an irreducible element of \"R\" is still irreducible when viewed as constant polynomial in \"R\"[\"X\"]; this explains the need for \"non-constant\" above, and in the irreducibility statements below.\n\nThe two properties of polynomials with integer coefficients can now be formulated formally as follows:\n\nPrimitivity statement: The set of primitive polynomials in Z[\"X\"] is closed under multiplication: if \"P\" and \"Q\" are primitive polynomials then so is their product \"PQ\".\n\nProof: Suppose the product of two primitive polynomials \"f\"(\"x\") and \"g\"(\"x\") is not primitive, so there exists a prime number \"p\" that is a common divisor of all the coefficients of the product. But since \"f\"(\"x\") and \"g\"(\"x\") are primitive, \"p\" cannot divide either all the coefficients of \"f\"(\"x\") or all those of \"g\"(\"x\"). Let \"ax\" and \"bx\" be the first (i.e., highest degree) terms with a coefficient not divisible by \"p\", respectively in \"f\"(\"x\") and in \"g\"(\"x\"). Now consider the coefficient of \"x\" in the product. Its value is given by , where the sum runs over all pairs of indices \"i,j\" such that . This sum contains a term \"a\"\"b\" which is not divisible by \"p\" (by Euclid's lemma, because \"p\" is prime), yet all the remaining ones are (because either or ), so the entire sum is not divisible by \"p\". But by assumption \"all\" coefficients in the product are divisible by \"p\", leading to a contradiction. Therefore, the coefficients of the product can have no common divisor and the product is primitive. This completes the proof.\n\nAlternatively the statement can be proved as a special case of more general results given below.\n\nIrreducibility statement: A non-constant polynomial in Z[\"X\"] is irreducible in Z[\"X\"] if and only if it is both irreducible in Q[\"X\"] and primitive in Z[\"X\"].\n\nThis is a special case for of more general irreducibility statement proved below.\n\nThe second result implies that if a polynomial with integer coefficients can be factored over the rational numbers, then there exists a factorization over the integers. This fact is often useful when combined with results such as Eisenstein's criterion.\n\nAn application is the rational root theorem.\n\nThe second result also implies that the minimal polynomial over the rational numbers of an algebraic integer has integer coefficients.\n\nGauss's lemma holds more generally over arbitrary GCD domains. There the \"content\" of a polynomial can be defined as the greatest common divisor of the coefficients of (like the gcd, the content is actually a class of associate elements).\n\nPrimitivity statement: If \"R\" is a GCD domain, then the set of primitive polynomials in is closed under multiplication. More generally, the content of a product of polynomials is the product of their contents.\n\nProof: The latter part follows from the former since is certainly a common divisor of the coefficients of the product, so one can divide by and to reduce and to primitive polynomials. For the proof of the former part we proceed by induction on the total number of nonzero terms of and combined. If one of the polynomials has at most one term, the result is obvious; this covers in particular all cases with fewer than 4 nonzero terms. So let both and have at least 2 terms, and assume the result established for any smaller combined number of terms. By dividing by and by , we reduce to the case . If the content is not invertible, it has a non-trivial divisor in common with the leading coefficient of at least one of and (since it divides their product, which is the leading coefficient of ). Suppose by symmetry that this is the case for , let be the leading term of , and let be the mentioned common divisor (here the content of is just its unique coefficient). Since is a common divisor of and , it also divides , in other words it divides its content, which by induction (since has fewer terms than ) is . As also divides , it divides , which gives a contradiction; therefore is invertible (and can be taken to be 1).\n\nIrreducibility statement: Let \"R\" be a GCD domain and \"F\" its field of fractions. A non-constant polynomial in \"R\"[\"X\"] is irreducible in \"R\"[\"X\"] if and only if it is both irreducible in \"F\"[\"X\"] and primitive in \"R\"[\"X\"].\n\nProof: As mentioned above a non-constant polynomial is irreducible in \"R\"[\"X\"] if and only if it is primitive and not a product of two non-constant polynomials in \"R\"[\"X\"]. Being irreducible in \"F\"[\"X\"] certainly excludes the latter possibility (since those non-constant polynomials would remain non-invertible in \"F\"[\"X\"]), so the essential point left to prove is that if \"P\" is non-constant and irreducible in \"R\"[\"X\"] then it is irreducible in \"F\"[\"X\"]. Note first that in \"F\"[\"X\"]\\{0} any class of associate elements (whose elements are related by multiplication by nonzero elements of the field \"F\") meets the set of primitive elements in \"R\"[\"X\"]: starting from an arbitrary element of the class, one can first (if necessary) multiply by a nonzero element of \"R\" to enter into the subset \"R\"[\"X\"] (removing denominators), then divide by the greatest common divisor of all coefficients to obtain a primitive polynomial. Now assume that \"P\" is reducible in \"F\"[\"X\"], so with \"S\",\"T\" non-constant polynomials in \"F\"[\"X\"]. One can replace \"S\" and \"T\" by associate primitive elements \"S′\", \"T′\", and obtain for some nonzero α in \"F\". But \"S′T′\" is primitive in \"R\"[\"X\"] by the primitivity statement, so α must lie in \"R\" (if α is written as a fraction \"a/b\", then \"b\" has to divide all coefficients of \"aS′T′\", so \"b\" divides \"c\"(\"aS′T′\") = \"a\", which means α = \"a/b\" is in \"R\") and the decomposition contradicts the irreducibility of \"P\" in \"R\"[\"X\"].\n\nThe condition that \"R\" is a GCD domain is not superfluous because it implies that every irreducible element of this ring is also a prime element, which in turn implies that every nonzero element of \"R\" has at most one factorization into a product of irreducible elements and a unit up to order and associate relationship. In a ring where factorization is not unique, say with \"p\" and \"q\" irreducible elements that do not divide any of the factors on the other side, the product shows the failure of the primitivity statement. For a concrete example one can take , , , , . In this example the polynomial (obtained by dividing the right hand side by ) provides an example of the failure of the irreducibility statement (it is irreducible over \"R\", but reducible over its field of fractions ). Another well known example is the polynomial , whose roots are the golden ratio and its conjugate showing that it is reducible over the field , although it is irreducible over the non-UFD which has as field of fractions. In the latter example the ring can be made into an UFD by taking its integral closure in (the ring of Dirichlet integers), over which becomes reducible, but in the former example \"R\" is already integrally closed.\n\nIn the special case when \"R\" is unique factorization domain (UFD), the primitivity statement can be proved more easily:\n\nProof: Let \"S\",\"T\" be primitive polynomials in \"R\"[\"X\"], and assume that their product \"ST\" is not primitive, so that some non-invertible element \"d\" of \"R\" divides all coefficients of \"ST\". There is some irreducible element \"p\" of \"R\" that divides \"d\" (this is where we use that \"R\" is UFD, if \"R\" was only GCD domain then such element wouldn't necessarily exist), and it is also a prime element in \"R\" (since \"R\" is GCD domain). Then the principal ideal \"pR\" generated by \"p\" is a prime ideal, so \"R\"/\"pR\" is an integral domain, and (\"R\"/\"pR\")[\"X\"] is therefore an integral domain as well (because nonzero polynomials over an integral domain cannot be zero divisors by consideration of the leading coefficient of their product). By hypothesis the projection \"R\"[\"X\"]→(\"R\"/\"pR\")[\"X\"] sends \"ST\" to 0, and since this is the product of the projections of \"S\" and \"T\" (projection is a ring homomorphism), at least one of those projections is 0 (here one uses that (\"R\"/\"pR\")[\"X\"] is an integral domain). But this means that \"p\" divides all of the coefficients either of \"S\" or of \"T\", which contradicts its assumed primitivity.\n\nThis special case is also sufficient in many applications. For example both primitivity and irreducibility statement for UFD are essential in proving that if \"R\" is UFD, then so is \"R\"[\"X\"] (and by an immediate induction, so is the polynomial ring over \"R\" in any number of indeterminates). Any factorization of a polynomial \"P\" in \"R\"[\"X\"] can be split into its irreducible factors that are contained in \"R\" (the \"constant\" factors) and those that are not. The primitivity statement implies that the product \"Q\" of the latter (non-constant) irreducible factors is primitive, so the product of the former (constant) factors give the content \"c\"(\"P\") of \"P\". This reduces proving uniqueness of factorizations to proving it individually for \"c\"(\"P\") and for \"Q\". Since the factorization of \"c\"(\"P\") takes place in \"R\", it is unique by assumption. By the irreducibility statement, the irreducible factors that occur in any factorization of \"Q\" in \"R\"[\"X\"] are primitive representatives of irreducible factors in a factorization of \"Q\" in \"F\"[\"X\"]. But the latter is unique since \"F\"[\"X\"] is a principal ideal domain and therefore a unique factorization domain. Together this shows that the factorization of \"P\" in \"R\"[\"X\"] is unique.\n\nAs explained above, neither primitivity nor irreducibility statement of Gauss's lemma is valid over general integral domains. However there is a variation of the first statement that is valid even for polynomials over any commutative ring \"R\", which replaces primitivity by the stronger property of co-maximality. Call a polynomial \"P\" in \"R\"[\"X\"] \"co-maximal\" if the ideal of \"R\" generated by the coefficients of the polynomial is the full ring \"R\". Clearly every co-maximal polynomial in \"R\"[\"X\"] is primitive. If \"R\" is a Bézout domain (so in particular if it's a principal ideal domain) then also every primitive polynomial in \"R\" is co-maximal. However co-maximality is often much more restrictive condition than primitivity even when \"R\" is a UFD. For example let \"k\" be a field and , which is a UFD as explained above. Then the polynomial is primitive but not co-maximal in \"R\"[\"X\"] because the ideal in \"R\" generated by the coefficients of \"P\" is proper (primitivity simply means that this ideal isn't contained in any proper principal ideal). We have the following variation of Gauss's lemma:\n\nCo-maximality statement: Let \"R\" be a commutative ring. Then the product of two co-maximal polynomials in \"R\"[\"X\"] is co-maximal.\n\nProof: Let \"S\",\"T\" be co-maximal polynomials in \"R\"[\"X\"], and assume that their product \"ST\" is not co-maximal. Then its coefficients generate a proper ideal \"I\", which by Krull's theorem (which depends on the axiom of choice) is contained in a maximal ideal \"m\" of \"R\". Then \"R\"/\"m\" is a field, and (\"R\"/\"m\")[\"X\"] is therefore an integral domain. By hypothesis the projection \"R\"[\"X\"]→(\"R\"/\"m\")[\"X\"] sends \"ST\" to 0, thus also at least one of \"S\",\"T\" individually, which means that its coefficients all lie in \"m\", which contradicts the fact that they generate the whole ring as an ideal.\n"}
{"id": "1353729", "url": "https://en.wikipedia.org/wiki?curid=1353729", "title": "Gaussian binomial coefficient", "text": "Gaussian binomial coefficient\n\nIn mathematics, the Gaussian binomial coefficients (also called Gaussian coefficients, Gaussian polynomials, or \"q\"-binomial coefficients) are \"q\"-analogs of the binomial coefficients. The Gaussian binomial coefficient formula_1 is a polynomial in \"q\" with integer coefficients, whose value when \"q\" is set to a prime power counts the number of subspaces of dimension \"k\" in a vector space of dimension \"n\" over a finite field with \"q\" elements.\n\nThe Gaussian binomial coefficients are defined by\n\nwhere \"m\" and \"r\" are non-negative integers. For the value is 1 since numerator and denominator are both empty products. Although the formula in the first clause appears to involve a rational function, it actually designates a polynomial, because the division is exact in Z<nowiki>[</nowiki>\"q\"<nowiki>]</nowiki>. Note that the formula can be applied for , and gives 0 due to a factor in the numerator, in accordance with the second clause (for even larger \"r\" the factor 0 remains present in the numerator, but its further factors would involve negative powers of \"q\", whence explicitly stating the second clause is preferable). All of the factors in numerator and denominator are divisible by , with as quotient a \"q\" number:\ndividing out these factors gives the equivalent formula\nwhich makes evident the fact that substituting into formula_5 gives the ordinary binomial coefficient formula_6 In terms of the \"q\" factorial formula_7, the formula can be stated as\na compact form (often given as only definition), which however hides the presence of many common factors in numerator and denominator. This form does make obvious the symmetry formula_9 for .\n\nUnlike the ordinary binomial coefficient, the Gaussian binomial coefficient has finite values for formula_10 (the limit being analytically meaningful for |\"q\"|<1):\n\nInstead of these algebraic expressions, one can also give a combinatorial definition of Gaussian binomial coefficients. The ordinary binomial coefficient formula_18 counts the -combinations chosen from an -element set. If one takes those elements to be the different character positions in a word of length , then each -combination corresponds to a word of length using an alphabet of two letters, say with copies of the letter 1 (indicating the positions in the chosen combination) and letters 0 (for the remaining positions).\n\nThe formula_19 words using 0s and 1s would be 0011, 0101, 0110, 1001, 1010, 1100.\n\nTo obtain from this model the Gaussian binomial coefficient formula_5, it suffices to count each word with a factor , where is the number of \"inversions\" of the word: the number of pairs of positions for which the leftmost position of the pair holds a letter 1 and the rightmost position holds a letter 0 in the word. For example, there is one word with 0 inversions, 0011. There is 1 with only a single inversion, 0101. There are two words with 2 inversions, 0110, and 1001. There is one with 3, 1010, and finally one word with 4 inversions, 1100. This corresponds to the coefficients in formula_21.\n\nIt can be shown that the polynomials so defined satisfy the Pascal identities given below, and therefore coincide with the polynomials given by the algebraic definitions. A visual way to view this definition is to associate to each word a path across a rectangular grid with sides of height and width , from the bottom left corner to the top right corner, taking a step right for each letter 0 and a step up for each letter 1. Then the number of inversions of the word equals the area of the part of the rectangle that is to the bottom-right of the path.\n\nLike the ordinary binomial coefficients, the Gaussian binomial coefficients are center-symmetric, i.e., invariant under the reflection formula_22:\n\nIn particular,\n\nThe name \"Gaussian binomial coefficient\" stems from the fact that their evaluation at is\n\nfor all \"m\" and \"r\".\n\nThe analogs of Pascal identities for the Gaussian binomial coefficients are\n\nand\n\nThere are analogs of the binomial formula, and of Newton's generalized version of it for negative integer exponents, although for the former the Gaussian binomial coefficients themselves do not appear as coefficients:\n\nand\n\nwhich, for formula_31 become:\n\nand\n\nThe first Pascal identity allows one to compute the Gaussian binomial coefficients recursively (with respect to \"m\" ) using the initial \"boundary\" values\n\nand also incidentally shows that the Gaussian binomial coefficients are indeed polynomials (in \"q\"). The second Pascal identity follows from the first using the substitution formula_22 and the invariance of the Gaussian binomial coefficients under the reflection formula_22. Both Pascal identities together imply\n\nwhich leads (when applied iteratively for \"m\", \"m\" − 1, \"m\" − 2...) to an expression for the Gaussian binomial coefficient as given in the definition above.\n\nGaussian binomial coefficients occur in the counting of symmetric polynomials and in the theory of partitions. The coefficient of \"q\" in\n\nis the number of partitions of \"r\" with \"m\" or fewer parts each less than or equal to \"n\". Equivalently, it is also the number of partitions of \"r\" with \"n\" or fewer parts each less than or equal to \"m\".\n\nGaussian binomial coefficients also play an important role in the enumerative theory of projective spaces defined over a finite field. In particular, for every finite field \"F\" with \"q\" elements, the Gaussian binomial coefficient\n\ncounts the number of \"k\"-dimensional vector subspaces of an \"n\"-dimensional vector space over \"F\" (a Grassmannian). When expanded as a polynomial in \"q\", it yields the well-known decomposition of the Grassmannian into Schubert cells. For example, the Gaussian binomial coefficient\n\nis the number of one-dimensional subspaces in (\"F\") (equivalently, the number of points in the associated projective space). Furthermore, when \"q\" is 1 (respectively −1), the Gaussian binomial coefficient yields the Euler characteristic of the corresponding complex (respectively real) Grassmannian.\n\nThe number of \"k\"-dimensional affine subspaces of \"F\" is equal to\n\nThis allows another interpretation of the identity\n\nas counting the (\"r\" − 1)-dimensional subspaces of (\"m\" − 1)-dimensional projective space by fixing a hyperplane, counting such subspaces contained in that hyperplane, and then counting the subspaces not contained in the hyperplane; these latter subspaces are in bijective correspondence with the (\"r\" − 1)-dimensional affine subspaces of the space obtained by treating this fixed hyperplane as the hyperplane at infinity.\n\nIn the conventions common in applications to quantum groups, a slightly different definition is used; the quantum binomial coefficient there is\nThis version of the quantum binomial coefficient is symmetric under exchange of formula_44 and formula_45.\n\nThe Gaussian binomial coefficients can be arranged in a triangle for each \"q\", which is Pascal's triangle for \"q\"=1.<br>\nRead line by line these triangles form the following sequences in the OEIS:\n\n"}
{"id": "28948360", "url": "https://en.wikipedia.org/wiki?curid=28948360", "title": "Geometry Expert", "text": "Geometry Expert\n\nGeometry Expert (GEX) is a Chinese software for dynamic diagram drawing and automated geometry theorem proving and discovering. \n\nThere's a new Chinese version of Geometry Expert, called MMP/Geometer.\n\nJava Geometry Expert is free under GNU General Public License.\n\n"}
{"id": "12894609", "url": "https://en.wikipedia.org/wiki?curid=12894609", "title": "George Pretyman Tomline", "text": "George Pretyman Tomline\n\nSir George Pretyman Tomline, 5th Baronet (born George Pretyman; 9 October 1750 – 14 November 1827) was an English clergyman, theologian, Bishop of Lincoln and then Bishop of Winchester, and confidant of William Pitt the Younger. He was an opponent of Catholic emancipation.\n\nHe was born George Pretyman in Bury St Edmunds, Suffolk to a family claiming to have been influential in the region as far back as the fourteenth century. His father, also George Pretyman (1722–1810) was a landowner and wool merchant. His mother, George's wife, was Susan \"née\" Hubbard (1720/1721 – 1807).\n\nPretyman attended Bury St Edmunds Grammar School and then Pembroke College, Cambridge, graduating in 1772 as senior wrangler and Smith's prizewinner. He was elected a fellow of Pembroke in 1773. He was ordained deacon in 1774 and priest in 1776: by Philip Yonge, Bishop of Norwich at his Palace's chapel on 14 August 1774 and by John Hinchliffe, Bishop of Peterborough at Trinity College, Cambridge on 16 June 1776.\n\nWilliam Pitt the Younger was sent to Pembroke in 1773, at the age of fourteen, and Pretyman became his tutor and gradually his friend and confidant. When Pitt unsuccessfully stood for election as Member of Parliament for Cambridge University in the British general election, 1780, Pretyman supported him.\n\nPitt became Prime Minister of Great Britain in December 1783 when the Fox-North Coalition fell but it remained for him to win the British general election, 1784. On his 1784 victory, Pitt made Pretyman his private secretary, though the title was thought inappropriate for a clergyman. Pretyman's mathematical ability was soon called upon in advising Pitt on the sinking fund and other technicalities of fiscal policy.\n\nOn 3 September 1784, Pretyman married Elizabeth Maltby (died 13 June 1826), cousin of Edward Maltby, the future Bishop of Chichester and himself eighth wrangler, and appointed Edward his domestic chaplain. George and Elizabeth were well-matched and he constantly consulted her on church and political issues.\n\nIn 1787, Pitt appointed Pretyman Bishop of Lincoln, having to overcome the opposition of George III who objected to Pretyman's youth. Having already become Dean of St Paul's (he was instituted to the Portpoole prebend by Robert Lowth, Bishop of London on 21 February 1787), his election was confirmed by John Moore, Archbishop of Canterbury, at St Mary-le-Bow on 10 March 1787 and he was consecrated a bishop by Moore (assisted by William Ashburnham, Bishop of Chichester; Shute Barrington, Bishop of Salisbury and Beilby Porteus, Bishop of Chester) at Lambeth Palace chapel on 11 March 1787.\n\nPretyman maintained on close terms with Pitt, though Lincoln duties kept him from frequent visits to London, and shared Whig attitudes. In a sermon to the House of Lords on 30 January 1789, Pretyman condemned Charles I, executed by parliament in 1649, and praised his political opponents. John Wesley wrote to Pretyman in 1790 accusing him of driving the 'people called Methodists' out of the established church. Pretyman continued to advise Pitt on finance and on Pitt's Ecclesiastical Plan. Pretyman was an opponent of Catholic emancipation and was against Pitt's 1801 decision to resign when he failed to effect the changes promised to the Irish Catholics in the compromises made over the passage of the Act of Union 1800.\n\nHenry Addington's regime was still less to Pretyman's taste and his anti-Catholic sentiments strengthened. However, he remained on good terms with Pitt and was ready to help him out of his debts.\n\nAlready wealthy, in 1803 he inherited extensive property from a distant relative, Marmaduke Tomline, and took the name Tomline. Pitt returned to government in 1804 and, much to Tomline's satisfaction, promoted Tomline as Archbishop of Canterbury, even though there was an earlier provisional agreement with the King that Charles Manners-Sutton should be appointed. However, the King was not to be manœuvred and exercised his royal prerogative to appoint Manners-Sutton.\n\nTomline was offered the post of Bishop of London in 1813 but declined because he thought the duties too onerous. He was translated to Bishop of Winchester by the confirmation of his election (by Manners-Sutton) on 15 August 1820.\n\nTomline had inherited further property before he died of apoplexy at Kingston Hall, near Wimborne, Dorset and his estate was worth £200,000 (). He was buried in Winchester Cathedral. His monument was sculpted by Richard Westmacott (the younger).\n\nTomline and his wife had three sons but they relinquished their claim to the baronetcy:\n\nTomline published the following works:\n\n\nHe was an able administrator to his diocese, conducting eleven visitations during his thirty three years tenure.\n\nThough he appeared somewhat aloof in public, Tomline was a devoted family man and genial enough given the right company. From 1806, he was conservative as to his attitudes to church and state but was well respected by someone of as different an outlook as Samuel Parr.\n\n\n\n"}
{"id": "49343041", "url": "https://en.wikipedia.org/wiki?curid=49343041", "title": "Gradshteyn and Ryzhik", "text": "Gradshteyn and Ryzhik\n\nGradshteyn and Ryzhik (GR) is the informal name of a comprehensive table of integrals originally compiled by the Russian mathematicians I. S. Gradshteyn and I. M. Ryzhik. Its full title today is Table of Integrals, Series, and Products.\n\nSince its first publication in 1943, it was considerably expanded and it soon became a \"classic\" and highly regarded reference for mathematicians, scientists and engineers. After the deaths of the original authors, the work was maintained and further expanded by other editors.\n\nAt some stage a German and English dual-language translation became available, followed by Polish, English-only and Japanese versions. After several further editions, the Russian and German-English versions went out of print and have not been updated after the fall of the Iron Curtain, but the English version is still being actively maintained and refined by new editors, and it has recently been retranslated back into Russian as well.\n\nOne of the valuable characteristics of \"Gradshteyn and Ryzhik\" compared to similar compilations is that most listed integrals are referenced. The literature list contains 92 main entries and 140 additional entries (in the eighth English edition). The integrals are classified by numbers, which haven't changed from the fourth Russian up to the seventh English edition (the numbering in older editions as well as in the eighth English edition is not fully compatible).\nThe book does not only contain the integrals, but also lists additional properties and related special functions.\nIt also includes tables for integral transforms.\nAnother advantage of \"Gradshteyn and Ryzhik\" compared to computer algebra systems is the fact that all special functions and constants used in the evaluation of the integrals are listed in a registry as well, thereby allowing reverse lookup of integrals based on special functions or constants.\n\nOn the downsides, \"Gradshteyn and Ryzhik\" has become known to contain a relatively high number of typographical errors even in newer editions, which has repeatedly led to the publication of extensive errata lists. Earlier English editions were also criticized for their poor translation of mathematical terms and mediocre print quality.\n\nThe work was originally compiled by the Russian mathematicians Iosif Moiseevich Ryzhik (Russian: , German: ) and Izrail Solomonovich Gradshteyn (Russian: , German: ). While some contents were original, significant portions were collected from other previously existing integral tables like David Bierens de Haan's ' (1867), Václav Jan Láska's ' (1888-1894) or Edwin Plimpton Adams' and Richard Lionel Hippisley's \"Smithsonian Mathematical Formulae and Tables of Elliptic Functions\" (1922).\n\nThe first edition, which contained about 5 000 formulas, was authored by Ryzhik, who had already published a book on special functions in 1936 and died during World War II in 1941. Not announcing this fact, his compilation was published posthumously in 1943, followed by a second corrected edition in his name in 1948.\n\nThe third edition (1951) was worked on by Gradshteyn, who also introduced the chapter numbering system in decimal notation. Gradshteyn planned considerable expansion for the fourth edition, a work he could not finish due to his own death. Therefore, the fourth (1962/1963) and fifth (1971) editions were continued by Yuri Veniaminovich Geronimus (Russian: , German: ) and Michail Yulyevich Tseytlin (Russian: , German: ). The fourth edition contained about 12 000 formulas already.\n\nBased on the third Russian edition, the first German-English edition with 5 400 formulas was published in 1957 by the East-German (DVW) with German translations by and and the English texts by . In wrote:\n\nIn 1963, it was followed by the second edition, a reprint edition with a four-page inlet listing corrections compiled by Eldon Robert Hansen.\n\nDerived from the 1963 edition, but considerably expanded, the third German-English edition by was finally published in 1981; it incorporated the material of the fifth Russian edition (1971) as well.\n\nPending this third German-English edition an English-only edition by Alan Jeffrey was published in 1965. Lacking a clear designation by itself it was variously known as first, third or fourth English edition, as it was based on the then-current fourth Russian edition. The formulas were photographically reproduced and the text translated. This still held true for the expanded fourth English edition in 1980, which added chapters 10 to 17.\n\nBoth of these editions saw multiple print runs each incorporating newly found corrections. Starting with the third printing, updated table entries were marked by adding a small superscript number to the entry number indicating the corresponding print run (\"3\" etc.), a convention carried over into later editions by continuing to increase the superscript number as kind of a revision number (no longer directly corresponding with actual print runs).\n\nThe fifth edition (1994), which contained close to 20 000 formulas, was electronically reset in preparation for a CD-ROM issue of the fifth edition (1996) and in anticipation of further editions. Since the sixth edition (2000), now corresponding with superscript number \"10\", Daniel Zwillinger started contributing as well. The last edition being edited by Jeffrey before his death was the seventh English edition published in 2007 (with superscript number \"11\"). This edition has been retranslated back into Russian as \"seventh Russian edition\" in 2011.\n\nFor the eighth edition (2014/2015, with superscript number \"12\") Zwillinger took over the role of the editor. He was assisted by Victor Hugo Moll. In order to make room for additional information without increasing the size of the book significantly, the former chapters 11 (on algebraic inequalities), chapters 13 to 16 (on matrices and related results, determinants, norms, ordinary differential equations) and chapter 18 (on z-transforms) worth about 50 pages in total were removed and some chapters renumbered (12 to 11, 17 to 12). This edition contains more than 10 000 entries.\n\nIn 1995, Alan Jeffrey published his \"Handbook of Mathematical Formulas and Integrals\".\nIt was partially based on the fifth English edition of Gradshteyn and Ryzhik's \"Table of Integrals, Series, and Products\" and meant as an companion, but written to be more accessible for students and practitioners. It went through four editions up to 2008. The fourth edition also took advantage of changes incorporated into the seventh English edition of Gradshteyn and Ryzhik.\n\nInspired by a 1988 paper in which Ilan Vardi proved several integrals in \"Gradshteyn and Ryzhik\" Victor Hugo Moll with George Boros started a project to prove all integrals listed in \"Gradshteyn and Ryzhik\" and add additional commentary and references. In the foreword of the book \"Irresistible Integrals\" (2004), they wrote:\n\nNevertheless, the efforts have meanwhile resulted in about 900 entries from \"Gradshteyn and Ryzhik\" discussed in a series of more than 30 articles of which papers 1 to 28 have been published in issues 14 to 26 of Scientia, Universidad Técnica Federico Santa María (UTFSM), between 2007 and 2015 and compiled into a two-volume book series \"Special Integrals of Gradshteyn and Ryzhik: the Proofs\" (2014–2015) already.\n\n\n\n\n\n\n\n"}
{"id": "325714", "url": "https://en.wikipedia.org/wiki?curid=325714", "title": "Hopf algebra", "text": "Hopf algebra\n\nIn mathematics, a Hopf algebra, named after Heinz Hopf, is a structure that is simultaneously an (unital associative) algebra and a (counital coassociative) coalgebra, with these structures' compatibility making it a bialgebra, and that moreover is equipped with an antiautomorphism satisfying a certain property. The representation theory of a Hopf algebra is particularly nice, since the existence of compatible comultiplication, counit, and antipode allows for the construction of tensor products of representations, trivial representations, and dual representations.\n\nHopf algebras occur naturally in algebraic topology, where they originated and are related to the H-space concept, in group scheme theory, in group theory (via the concept of a group ring), and in numerous other places, making them probably the most familiar type of bialgebra. Hopf algebras are also studied in their own right, with much work on specific classes of examples on the one hand and classification problems on the other. They have diverse applications ranging from Condensed-matter physics and quantum field theory to string theory and LHC phenomenology .\n\nTheorem (Hopf) Let \"A\" be a finite-dimensional, graded commutative, graded cocommutative Hopf algebra over a field of characteristic 0. Then \"A\" (as an algebra) is a free exterior algebra with generators of odd degree.\n\nFormally, a Hopf algebra is a (associative and coassociative) bialgebra \"H\" over a field \"K\" together with a \"K\"-linear map \"S\": \"H\" → \"H\" (called the antipode) such that the following diagram commutes:\n\nHere Δ is the comultiplication of the bialgebra, ∇ its multiplication, η its unit and ε its counit. In the sumless Sweedler notation, this property can also be expressed as\n\nAs for algebras, one can replace the underlying field \"K\" with a commutative ring \"R\" in the above definition.\n\nThe definition of Hopf algebra is self-dual (as reflected in the symmetry of the above diagram), so if one can define a dual of \"H\" (which is always possible if \"H\" is finite-dimensional), then it is automatically a Hopf algebra.\n\nFixing a basis formula_2 for the underlying vector space, one may define the algebra in terms of structure constants for multiplication:\nfor co-multiplication:\nand the antipode:\nAssociativity then requires that\nwhile co-associativity requires that\nThe connecting axiom requires that\n\nThe antipode \"S\" is sometimes required to have a \"K\"-linear inverse, which is automatic in the finite-dimensional case, or if \"H\" is commutative or cocommutative (or more generally quasitriangular).\n\nIn general, \"S\" is an antihomomorphism, so \"S\" is a homomorphism, which is therefore an automorphism if \"S\" was invertible (as may be required).\n\nIf \"S\" = id, then the Hopf algebra is said to be involutive (and the underlying algebra with involution is a *-algebra). If \"H\" is finite-dimensional semisimple over a field of characteristic zero, commutative, or cocommutative, then it is involutive.\n\nIf a bialgebra \"B\" admits an antipode \"S\", then \"S\" is unique (\"a bialgebra admits at most 1 Hopf algebra structure\"). Thus, the antipode does not pose any extra structure which we can choose: Being a Hopf algebra is a property of a bialgebra.\n\nThe antipode is an analog to the inversion map on a group that sends \"g\" to \"g\".\n\nA subalgebra \"A\" of a Hopf algebra \"H\" is a Hopf subalgebra if it is a subcoalgebra of \"H\" and the antipode \"S\" maps \"A\" into \"A\". In other words, a Hopf subalgebra A is a Hopf algebra in its own right when the multiplication, comultiplication, counit and antipode of \"H\" is restricted to \"A\" (and additionally the identity 1 of \"H\" is required to be in A). The Nichols–Zoeller freeness theorem established (in 1989) that the natural \"A\"-module \"H\" is free of finite rank if \"H\" is finite-dimensional: a generalization of Lagrange's theorem for subgroups. As a corollary of this and integral theory, a Hopf subalgebra of a semisimple finite-dimensional Hopf algebra is automatically semisimple.\n\nA Hopf subalgebra \"A\" is said to be right normal in a Hopf algebra \"H\" if it satisfies the condition of stability, \"ad\"(\"h\")(\"A\") ⊆ \"A\" for all \"h\" in \"H\", where the right adjoint mapping \"ad\" is defined by \"ad\"(\"h\")(\"a\") = \"S\"(\"h\")\"ah\" for all \"a\" in \"A\", \"h\" in \"H\". Similarly, a Hopf subalgebra \"A\" is left normal in \"H\" if it is stable under the left adjoint mapping defined by \"ad\"(\"h\")(\"a\") = \"h\"\"aS\"(\"h\"). The two conditions of normality are equivalent if the antipode \"S\" is bijective, in which case \"A\" is said to be a normal Hopf subalgebra.\n\nA normal Hopf subalgebra \"A\" in \"H\" satisfies the condition (of equality of subsets of H): \"HA\" = \"A\"\"H\" where \"A\" denotes the kernel of the counit on \"K\". This normality condition implies that \"HA\" is a Hopf ideal of \"H\" (i.e. an algebra ideal in the kernel of the counit, a coalgebra coideal and stable under the antipode). As a consequence one has a quotient Hopf algebra \"H\"/\"HA\" and epimorphism \"H\" → \"H\"/\"A\"\"H\", a theory analogous to that of normal subgroups and quotient groups in group theory.\n\nA Hopf order \"O\" over an integral domain \"R\" with field of fractions \"K\" is an order in a Hopf algebra \"H\" over \"K\" which is closed under the algebra and coalgebra operations: in particular, the comultiplication Δ maps \"O\" to \"O\"⊗\"O\".\n\nA group-like element is a nonzero element \"x\" such that Δ(\"x\") = \"x\"⊗\"x\". The group-like elements form a group with inverse given by the antipode. A primitive element \"x\" satisfies Δ(\"x\") = \"x\"⊗1 + 1⊗\"x\".\n\nLet \"A\" be a Hopf algebra, and let \"M\" and \"N\" be \"A\"-modules. Then, \"M\" ⊗ \"N\" is also an \"A\"-module, with\nfor \"m\" ∈ \"M\", \"n\" ∈ \"N\" and Δ(\"a\") = (\"a\", \"a\"). Furthermore, we can define the trivial representation as the base field \"K\" with\nfor \"m\" ∈ \"K\". Finally, the dual representation of \"A\" can be defined: if \"M\" is an \"A\"-module and \"M*\" is its dual space, then\nwhere \"f\" ∈ \"M*\" and \"m\" ∈ \"M\".\n\nThe relationship between Δ, ε, and \"S\" ensure that certain natural homomorphisms of vector spaces are indeed homomorphisms of \"A\"-modules. For instance, the natural isomorphisms of vector spaces \"M\" → \"M\" ⊗ \"K\" and \"M\" → \"K\" ⊗ \"M\" are also isomorphisms of \"A\"-modules. Also, the map of vector spaces \"M*\" ⊗ \"M\" → \"K\" with \"f\" ⊗ \"m\" → \"f\"(\"m\") is also a homomorphism of \"A\"-modules. However, the map \"M\" ⊗ \"M*\" → \"K\" is not necessarily a homomorphism of \"A\"-modules.\n\nNote that functions on a finite group can be identified with the group ring, though these are more naturally thought of as dual – the group ring consists of \"finite\" sums of elements, and thus pairs with functions on the group by evaluating the function on the summed elements.\n\nThe cohomology algebra (over a field formula_12) of a Lie group formula_13 is a Hopf algebra: the multiplication is provided by the cup product, and the comultiplication \nby the group multiplication formula_15. This observation was actually a source of the notion of Hopf algebra. Using this structure, Hopf proved a structure theorem for the cohomology algebra of Lie groups.\n\nTheorem (Hopf) Let formula_16 be a finite-dimensional, graded commutative, graded cocommutative Hopf algebra over a field of characteristic 0. Then formula_16 (as an algebra) is a free exterior algebra with generators of odd degree.\n\nAll examples above are either commutative (i.e. the multiplication is commutative) or co-commutative (i.e. Δ = \"T\" ∘ Δ where the \"twist map\" \"T\": \"H\" ⊗ \"H\" → \"H\" ⊗ \"H\" is defined by \"T\"(\"x\" ⊗ \"y\") = \"y\" ⊗ \"x\"). Other interesting Hopf algebras are certain \"deformations\" or \"quantizations\" of those from example 3 which are neither commutative nor co-commutative. These Hopf algebras are often called \"quantum groups\", a term that is so far only loosely defined. They are important in noncommutative geometry, the idea being the following: a standard algebraic group is well described by its standard Hopf algebra of regular functions; we can then think of the deformed version of this Hopf algebra as describing a certain \"non-standard\" or \"quantized\" algebraic group (which is not an algebraic group at all). While there does not seem to be a direct way to define or manipulate these non-standard objects, one can still work with their Hopf algebras, and indeed one \"identifies\" them with their Hopf algebras. Hence the name \"quantum group\".\n\nGraded Hopf algebras are often used in algebraic topology: they are the natural algebraic structure on the direct sum of all homology or cohomology groups of an H-space.\n\nLocally compact quantum groups generalize Hopf algebras and carry a topology. The algebra of all continuous functions on a Lie group is a locally compact quantum group.\n\nQuasi-Hopf algebras are generalizations of Hopf algebras, where coassociativity only holds up to a twist. They have been used in the study of the Knizhnik–Zamolodchikov equations.\n\nMultiplier Hopf algebras introduced by Alfons Van Daele in 1994 are generalizations of Hopf algebras where comultiplication from an algebra (with or without unit) to the multiplier algebra of tensor product algebra of the algebra with itself.\n\nHopf group-(co)algebras introduced by V. G. Turaev in 2000 are also generalizations of Hopf algebras.\n\nWeak Hopf algebras, or quantum groupoids, are generalizations of Hopf algebras. Like Hopf algebras, weak Hopf algebras form a self-dual class of algebras; i.e., if \"H\" is a (weak) Hopf algebra, so is \"H\"*, the dual space of linear forms on \"H\" (with respect to the algebra-coalgebra structure obtained from the natural pairing with \"H\" and its coalgebra-algebra structure). A weak Hopf algebra \"H\" is usually taken to be a\n\n\n\nThe axioms are partly chosen so that the category of \"H\"-modules is a rigid monoidal category. The unit \"H\"-module is the separable algebra \"H\" mentioned above. \nFor example, a finite groupoid algebra is a weak Hopf algebra. In particular, the groupoid algebra on [n] with one pair of invertible arrows \"e\" and \"e\" between \"i\" and \"j\" in [\"n\"] is isomorphic to the algebra \"H\" of \"n\" x \"n\" matrices. The weak Hopf algebra structure on this particular \"H\" is given by coproduct Δ(\"e\") = \"e\" ⊗ \"e\", counit ε(\"e\") = 1 and antipode \"S\"(\"e\") = \"e\". The separable subalgebras \"H\" and \"H\" coincide and are non-central commutative algebras in this particular case (the subalgebra of diagonal matrices).\n\nEarly theoretical contributions to weak Hopf algebras are to be found in as well as\n\nSee Hopf algebroid\n\nGroups can be axiomatized by the same diagrams (equivalently, operations) as a Hopf algebra, where \"G\" is taken to be a set instead of a module. In this case:\nIn this philosophy, a group can be thought of as a Hopf algebra over the \"field with one element\".\n\n\n"}
{"id": "542447", "url": "https://en.wikipedia.org/wiki?curid=542447", "title": "Information content", "text": "Information content\n\nIn information theory, information content, self-information, or surprisal of a random variable or signal is the amount of information gained when it is sampled. Formally, information content is a random variable defined for any event in probability theory regardless of whether a random variable is being measured or not. \n\nInformation content is expressed in a unit of information, as explained below. The expected value of self-information is information theoretic entropy, the average amount of information an observer would expect to gain about a system when sampling the random variable.\n\nGiven a random variable formula_1 with probability mass function formula_2, the self-information of measuring formula_1 as outcome formula_4 is defined as formula_5\n\nBroadly given an event formula_6 with probability formula_7, information content is defined analogously:\n\nformula_8\n\nIn general, the base of the logarithmic chosen does not matter for most information-theoretic properties; however, different units of information are assigned based on popular choices of base. \n\nIf the logarithmic base is 2, the unit is named the Shannon but \"bit\" is also used. If the base of the logarithm is the natural logarithm (logarithm to base Euler's number e ≈ 2.7182818284), the unit is called the nat, short for \"natural\". If the logarithm is to base 10, the units are called hartleys or decimal digits.\n\nThe Shannon entropy of the random variable formula_9 above is defined as\n\nformula_10\n\nby definition equal to the expected information content of measurement of formula_9.\n\nFor a given probability space, measurement of rarer events will yield more information content than more common values. Thus, self-information is antitonic in probability for events under observation.\n\n\nThe information content of two independent events is the sum of each event's information content. This property is known as additivity in mathematics, and sigma additivity in particular in measure and probability theory. Consider two independent random variables formula_12 with probability mass functions formula_13 and formula_14 respectively. The joint probability mass function is\n\nformula_15\n\nbecause formula_16 and formula_17 are independent. The information content of the outcome formula_18 isformula_19See below for an example.\n\nThis measure has also been called surprisal, as it represents the \"surprise\" of seeing the outcome (a highly improbable outcome is very surprising). This term (as a log-probability measure) was coined by Myron Tribus in his 1961 book \"Thermostatics and Thermodynamics\".\n\nWhen the event is a random realization (of a variable) the self-information of the variable is defined as the expected value of the self-information of the realization.\n\nSelf-information is an example of a proper scoring rule.\n\nConsider the Bernoulli trial of tossing a fair coin formula_1. The probabilities of the events of the coin landing as heads formula_21 and tails formula_22 (see fair coin and obverse and reverse) are one half each, formula_23. Upon measuring the variable as heads, the associated information gain isformula_24so the information gain of a fair coin landing as heads is 1 shannon. Likewise, the information gain of measuring formula_22 tails isformula_26\n\nSuppose we have a fair six-sided dice. The value of a dice roll is a discrete uniform random variable formula_27 with probability mass function formula_28The probability of rolling a 4 is formula_29, as for any other valid roll. The information content of rolling a 4 is thusformula_30of information.\n\nSuppose we have two independent, identically distributed random variables formula_31 each corresponding to an independent fair 6-sided dice roll. The joint distribution of formula_1 and formula_33 isformula_34\n\nThe information content of the random variate formula_35 is formula_36just as \n\nformula_37as explained in .\n\nIf we receive information about the value of the dice without knowledge of which die had which value, we can formalize the approach with so-called counting variables\n\nformula_38\n\nfor formula_39, then formula_40 and the counts have the multinomial distribution \n\nformula_41\n\nTo verify this, the 6 outcomes formula_42 correspond to the event formula_43 and a total probability of . These are the only events that are faithfully preserved with identity of which dice rolled which outcome because the outcomes are the same. Without knowledge to distinguish the dice rolling the other numbers, the other formula_44 combinations correspond to one die rolling one number and the other die rolling a different number, each having probability . Indeed, formula_45, as required.\n\nUnsurprisingly, the information content of learning that both dice were rolled as the same particular number is more than the information content of learning that one dice was one number and the other was a different number. Take for examples the events formula_46 and formula_47for formula_48. For example, formula_49and formula_50. \n\nThe information contents are \n\nformula_51formula_52Let formula_53 be the event that both dice rolled the same value and formula_54 be the event that the dice differed. Then formula_55 and formula_56. The information contents of the events are\n\nformula_57formula_58\n\nThe probability mass or density function (collectively probability measure) of the sum of two independent random variables is the convolution of each probability measure. In the case of independent fair 6-sided dice rolls, the random variable formula_59 has probability mass function formula_60, where formula_61 represents the discrete convolution. The outcome formula_62 has probability formula_63. Therefore, the information asserted isformula_64\n\nGeneralizing the example above, consider a general discrete uniform random variable (DURV) formula_65 For convenience, define formula_66. The p.m.f. is formula_67In general, the values of the DURV need not be integers, or for the purposes of information theory even uniformly spaced; they need only be equiprobable. The information gain of any observation formula_68isformula_69\n\nIf formula_70 above, formula_1 degenerates to a constant random variable with probability distribution deterministically given by formula_72 and probability measure the Dirac measure formula_73. The only value formula_1 can take is deterministically formula_75, so the information content of any measurement of formula_1 isformula_77In general, there is no information gained from measuring a known value.\n\nGeneralizing all of the above cases, consider a categorical discrete random variable with support formula_78 and given by \n\nformula_79\n\nFor the purposes of information theory, the values formula_80 do not even have to be numbers at all; they can just be mutually exclusive events on a measure space of finite measure that has been normalized to a probability measure formula_81. Without loss of generality, we can assume the categorical distribution is supported on the set formula_82; the mathematical structure is isomorphic in terms of probability theory and therefore information theory as well. \n\nThe information of the outcome formula_83 is given\n\nformula_84\n\nFrom these examples, it is possible to calculate the information of any set of independent DRVs with known distributions by additivity.\n\nThe entropy is the expected value of the information content of the discrete random variable, with expectation taken over the discrete values it takes. Sometimes, the entropy itself is called the \"self-information\" of the random variable, possibly because the entropy satisfies formula_85, where formula_86 is the mutual information of formula_1 with itself.\n\nBy definition, information is transferred from an originating entity possessing the information to a receiving entity only when the receiver had not known the information a priori. If the receiving entity had previously known the content of a message with certainty before receiving the message, the amount of information of the message received is zero.\n\nFor example, quoting a character (the Hippy Dippy Weatherman) of comedian George Carlin, \"“Weather forecast for tonight: dark. Continued dark overnight, with widely scattered light by morning.”\" Assuming one does not reside near the Earth's poles or polar circles, the amount of information conveyed in that forecast is zero because it is known, in advance of receiving the forecast, that darkness always comes with the night.\n\nWhen the content of a message is known a priori with certainty, with probability of 1, there is no actual information conveyed in the message. Only when the advance knowledge of the content of the message by the receiver is less than 100% certain does the message actually convey information. \n\nAccordingly, the amount of self-information contained in a message conveying content informing an occurrence of event, formula_88, depends only on the probability of that event. \n\nfor some function formula_90 to be determined below. If formula_91, then formula_92. If formula_93, then formula_94.\n\nFurther, by definition, the measure of self-information is nonnegative and additive. If a message informing of event formula_95 is the intersection of two independent events formula_96 and formula_97, then the information of event formula_95 occurring is that of the compound message of both independent events formula_96 and formula_97 occurring. The quantity of information of compound message formula_95 would be expected to equal the sum of the amounts of information of the individual component messages formula_96 and formula_97 respectively:\n\nBecause of the independence of events formula_96 and formula_97, the probability of event formula_95 is\n\nHowever, applying function formula_90 results in\n\nThe class of function formula_90 having the property such that\n\nis the logarithm function of any base. The only operational difference between logarithms of different bases is that of different scaling constants.\n\nSince the probabilities of events are always between 0 and 1 and the information associated with these events must be nonnegative, that requires that formula_114.\n\nTaking into account these properties, the self-information formula_115 associated with outcome formula_88 with probability formula_117 is defined as:\n\nThe smaller the probability of event formula_88, the larger the quantity of self-information associated with the message that the event indeed occurred. If the above logarithm is base 2, the unit of formula_120 is bits. This is the most common practice. When using the natural logarithm of base formula_121, the unit will be the nat. For the base 10 logarithm, the unit of information is the hartley.\n\nAs a quick illustration, the information content associated with an outcome of 4 heads (or any specific outcome) in 4 consecutive tosses of a coin would be 4 bits (probability 1/16), and the information content associated with getting a result other than the one specified would be ~0.09 bits (probability 15/16). See below for detailed examples.\n\n\n\n"}
{"id": "4705339", "url": "https://en.wikipedia.org/wiki?curid=4705339", "title": "Information dimension", "text": "Information dimension\n\nIn information theory, information dimension is an information measure for random vectors in Euclidean space, based on the normalized entropy of finely quantized versions of the random vectors. This concept was first introduced by Alfréd Rényi in 1959.\n\nSimply speaking, it is a measure of the fractal dimension of a probability distribution. It characterizes the growth rate of the Shannon entropy given by successively finer discretizations of the space.\n\nIn 2010, Wu and Verdú gave an operational characterization of Rényi information dimension as the fundamental limit of almost lossless data compression for analog sources under various regularity constraints of the encoder/decoder.\n\nThe entropy of a discrete random variable formula_1 is\n\nwhere formula_3 is the probability measure of formula_1 when formula_5, and the formula_6 denotes a set formula_7.\n\nLet formula_8 be an arbitrary real-valued random variable. Given a positive integer formula_9, we create a new discrete random variable\n\nwhere the formula_11 is the floor operator which converts a real number to the greatest integer less than it. Then\n\nand\n\nare called lower and upper information dimensions of formula_8 respectively. When formula_15, we call this value information dimension of formula_8,\n\nSome important properties of information dimension formula_18:\n\nIf the information dimension formula_27 exists, one can define the formula_27-dimensional entropy of this distribution by\n\nprovided the limit exists. If formula_31, the zero-dimensional entropy equals the standard Shannon entropy formula_32. For integer dimension formula_33, the formula_21-dimensional entropy is the formula_21-fold integral defining the respective differential entropy.\n\nAccording to Lebesgue decomposition theorem, a probability distribution can be uniquely represented by the mixture formula_36where formula_37 and formula_38; formula_39 is a purely atomic probability measure (discrete part), formula_40 is the absolutely continuous probability measure, and formula_41 is a probability measure singular with respect to Lebesgue measure but with no atoms (singular part).\n\nLet formula_8 be a random variable such that formula_43. Assume the distribution of formula_8 can be represented asformula_45where formula_39 is a discrete measure and formula_40 is the absolutely continuous probability measure with formula_48. Thenformula_49Moreover, given formula_50 and differential entropy formula_51, the formula_27-Dimensional Entropy is simply given byformula_53where formula_54 is the Shannon entropy of a discrete random variable formula_1 with formula_56 and formula_57 and given byformula_58\n\nConsider a signal which has a Gaussian probability distribution.\n\nWe pass the signal through a half-wave rectifier which converts all negative value to 0, and maintains all other values. The half-wave rectifier can be characterized by the functionformula_59\n\nThen, at the output of the rectifier, the signal has a rectified Gaussian distribution. It is characterized by an atomic mass of weight 0.5 and has a Gaussian PDF for all formula_60.\n\nWith this mixture distribution, we apply the formula above and get the information dimension formula_27 of the distribution and calculate the formula_27-dimensional entropy.formula_63The normalized right part of the zero-mean Gaussian distribution has entropy formula_64, hence \nformula_65\n\nIt is shown that information dimension and differential entropy are tightly connected.\n\nLet formula_8 be a positive random variable with density formula_67. Suppose we divide the range of formula_8 into bins of length formula_69. By the mean value theorem, there exists a value formula_70 within each bin such that\n\nConsider the discretized random variable formula_72 if formula_73.\n\nThe probability of each support point formula_72 is\n\nThe entropy of this variable is\n\nIf we set formula_77 and formula_78 then we are doing exactly the same quantization as the definition of information dimension. Since relabeling the events of a discrete random variable does not change its entropy, we have\n\nThis yields\n\nand when formula_9 is sufficient large,\n\nwhich is the differential entropy formula_83 of the continuous random variable. In particular, if formula_67 is Riemann integrable, then\n\nComparing this with the formula_27-dimensional entropy shows that the differential entropy is exactly the one-dimensional entropy\n\nIn fact, this can be generalized to higher dimensions. Rényi shows that, if formula_22 is a random vector in a formula_21-dimensional Euclidean space formula_90 with an absolutely continuous distribution with a probability density function formula_91 and finite entropy of the integer part (formula_92), we have\nformula_93\n\nand\n\nif the integral exist.\n\nThe information dimension of a distribution gives a theoretical upper bound on the compression rate, if one wants to compress a variable coming from this distribution. In the context of lossless data compression, we try to compress real number with less real number which both have infinite precision.\n\nThe main objective of the lossless data compression is to find efficient representations for source realizations formula_95 by formula_96. A formula_97code for formula_98 is a pair of mappings:\nThe block error probability is formula_101.\n\nDefine formula_102 to be the infimum of formula_103 such that there exists a sequence of formula_104codes such that formula_105 for all sufficiently large formula_21.\n\nSo formula_102 basically gives the ratio between the code length and the source length, it shows how good a specific encoder decoder pair is. The fundamental limits in lossless source coding are as follows.\n\nConsider a continuous encoder function formula_108 with its continuous decoder function formula_109. If we impose no regularity on formula_67 and formula_111, due to the rich structure of formula_112, we have the minimum formula_113-achievable rate formula_114 for all formula_115. It means that one can build an encoder-decoder pair with infinity compression rate.\n\nIn order to get some nontrivial and meaningful conclusions, let formula_116 the minimum formula_117achievable rate for linear encoder and Borel decoder. If random variable formula_8 has a distribution which is a mixture of discrete and continuous part. Then formula_119 for all formula_115 Suppose we restrict the decoder to be a Lipschitz continuous function and formula_121 holds, then the minimum formula_117achievable rate formula_123 for all formula_115.\n"}
{"id": "54493", "url": "https://en.wikipedia.org/wiki?curid=54493", "title": "Kuratowski's theorem", "text": "Kuratowski's theorem\n\nIn graph theory, Kuratowski's theorem is a mathematical forbidden graph characterization of planar graphs, named after Kazimierz Kuratowski. It states that a finite graph is planar if and only if it does not contain a subgraph that is a subdivision of \"K\" (the complete graph on five vertices) or of \"K\" (complete bipartite graph on six vertices, three of which connect to each of the other three, also known as the utility graph).\n\nA planar graph is a graph whose vertices can be represented by points in the Euclidean plane, and whose edges can be represented by simple curves in the same plane connecting the points representing their endpoints, such that no two curves intersect except at a common endpoint. Planar graphs are often drawn with straight line segments representing their edges, but by Fáry's theorem this makes no difference to their graph-theoretic characterization.\n\nA subdivision of a graph is a graph formed by subdividing its edges into paths of one or more edges. Kuratowski's theorem states that a finite graph \"G\" is planar, if it is not possible to subdivide the edges of \"K\" or \"K\", and then possibly add additional edges and vertices, to form a graph isomorphic to \"G\". Equivalently, a finite graph is planar if and only if it does not contain a subgraph that is homeomorphic to \"K\" or \"K\".\n\nIf \"G\" is a graph that contains a subgraph \"H\" that is a subdivision of \"K\" or \"K\", then \"H\" is known as a Kuratowski subgraph of \"G\". With this notation, Kuratowski's theorem can be expressed succinctly: a graph is planar if and only if it does not have a Kuratowski subgraph.\n\nThe two graphs \"K\" and \"K\" are nonplanar, as may be shown either by a case analysis or an argument involving Euler's formula. Additionally, subdividing a graph cannot turn a nonplanar graph into a planar graph: if a subdivision of a graph \"G\" has a planar drawing, the paths of the subdivision form curves that may be used to represent the edges of \"G\" itself. Therefore, a graph that contains a Kuratowski subgraph cannot be planar. The more difficult direction in proving Kuratowski's theorem is to show that, if a graph is nonplanar, it must contain a Kuratowski subgraph.\n\nA Kuratowski subgraph of a nonplanar graph can be found in linear time, as measured by the size of the input graph. This allows the correctness of a planarity testing algorithm to be verified for nonplanar inputs, as it is straightforward to test whether a given subgraph is or is not a Kuratowski subgraph.\nUsually, non-planar graphs contain a large number of Kuratowski-subgraphs. The extraction of these subgraphs is needed, e.g., in branch and cut algorithms for crossing minimization. It is possible to extract a large number of Kuratowski subgraphs in time dependent on their total size.\n\nKazimierz Kuratowski published his theorem in 1930. The theorem was independently proved by Orrin Frink and Paul Smith, also in 1930, but their proof was never published. The special case of cubic planar graphs (for which the only minimal forbidden subgraph is \"K\") was also independently proved by Karl Menger in 1930.\nSince then, several new proofs of the theorem have been discovered.\n\nIn the Soviet Union, Kuratowski's theorem was known as either the Pontryagin–Kuratowski theorem or the Kuratowski–Pontryagin theorem,\nas the theorem was reportedly proved independently by Lev Pontryagin around 1927.\nHowever, as Pontryagin never published his proof, this usage has not spread to other places.\n\nA closely related result, Wagner's theorem, characterizes the planar graphs by their minors in terms of the same two forbidden graphs \"K\" and \"K\". Every Kuratowski subgraph is a special case of a minor of the same type, and while the reverse is not true, it is not difficult to find a Kuratowski subgraph (of one type or the other) from one of these two forbidden minors; therefore, these two theorems are equivalent.\n\nAn extension is the Robertson-Seymour theorem.\n\n"}
{"id": "342038", "url": "https://en.wikipedia.org/wiki?curid=342038", "title": "List of Lie groups topics", "text": "List of Lie groups topics\n\nThis is a list of Lie group topics, by Wikipedia page.\n\n\"See Table of Lie groups for a list\"\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "32844480", "url": "https://en.wikipedia.org/wiki?curid=32844480", "title": "Little q-Jacobi polynomials", "text": "Little q-Jacobi polynomials\n\nIn mathematics, the little \"q\"-Jacobi polynomials \"p\"(\"x\";\"a\",\"b\";\"q\") are a family of basic hypergeometric orthogonal polynomials in the basic Askey scheme, introduced by . give a detailed list of their properties.\n\nThe little \"q\"-Jacobi polynomials are given in terms of basic hypergeometric functions and the Pochhammer symbol by \n\nThe following are a set of animation plots for Little q-Jacobi polynomials, with varying q;\nthree density plots of imaginary, real and modula in complex space; three set of complex 3D plots\nof imaginary, real and modulus of the said polynomials.\n\n"}
{"id": "3437245", "url": "https://en.wikipedia.org/wiki?curid=3437245", "title": "Malthusian growth model", "text": "Malthusian growth model\n\nA Malthusian growth model, sometimes called a simple exponential growth model, is essentially exponential growth based on the idea of the function being proportional to the speed to which the function grows. The model is named after Thomas Robert Malthus, who wrote \"An Essay on the Principle of Population\" (1798), one of the earliest and most influential books on population.\n\nMalthusian models have the following form:\nwhere\n\n\nThe model can also been written in the form of a differential equation:\n\ndP/dt = rP\n\nwith initial condition:\nP(0)= P\n\nThis model is often referred to as the \"exponential law\". It is widely regarded in the field of population ecology as the first principle of population dynamics, with Malthus as the founder. The exponential law is therefore also sometimes referred to as the \"Malthusian Law\". By now, it is a widely accepted view to analogize Malthusian growth in Ecology to Newton's First Law of uniform motion in physics.\n\nMalthus wrote that all life forms, including humans, have a propensity to exponential population growth when resources are abundant but that actual growth is limited by available resources:\nA model of population growth bounded by resource limitations was developed by Pierre Francois Verhulst in 1838, after he had read Malthus' essay. Verhulst named the model a logistic function.\n\n\n"}
{"id": "31840693", "url": "https://en.wikipedia.org/wiki?curid=31840693", "title": "Martin T. Barlow", "text": "Martin T. Barlow\n\nMartin Thomas Barlow FRS FRSC (born 16 June 1953 in London) is a British mathematician who is professor of mathematics at the University of British Columbia in Canada since 1992.\n\nBarlow is the son of Andrew Dalmahoy Barlow (1916–2006) and his wife Yvonne. He is thus the grandson of Alan Barlow, and his wife Nora (née Darwin), through whom he is a great-great-grandson of Charles Darwin. He is the nephew of Horace Barlow (also FRS and Fellow of Trinity). In 1994 he married Colleen McLaughlin.\n\nHe was educated Sussex House School, St Paul's School, London, Trinity College, Cambridge (BA 1975, Diploma 1976, ScD 1993); University College of Swansea (PhD).\n\nBarlow worked as a research fellow of the University of Liverpool 1978–1980. He was a Fellow of Trinity College, Cambridge, 1979–1992. He worked in the Statistical Laboratory, University of Cambridge 1981–1985 and was a Royal Society University Research Fellow 1985–1992.\n\nHis mathematical interests include probability, Brownian motion and fractal sets.\n\nHe was awarded the Rollo Davidson Prize in 1984. He was elected a Fellow of the Royal Society of Canada in 1998. He was elected a Fellow of the Royal Society in 2005. In 2012 he became a fellow of the American Mathematical Society. His doctoral students include Steven N. Evans.\n\n\n"}
{"id": "28831427", "url": "https://en.wikipedia.org/wiki?curid=28831427", "title": "Multivariate kernel density estimation", "text": "Multivariate kernel density estimation\n\nKernel density estimation is a nonparametric technique for density estimation i.e., estimation of probability density functions, which is one of the fundamental questions in statistics. It can be viewed as a generalisation of histogram density estimation with improved statistical properties. Apart from histograms, other types of density estimators include parametric, spline, wavelet and Fourier series. Kernel density estimators were first introduced in the scientific literature for univariate data in the 1950s and 1960s and subsequently have been widely adopted. It was soon recognised that analogous estimators for multivariate data would be an important addition to multivariate statistics. Based on research carried out in the 1990s and 2000s, multivariate kernel density estimation has reached a level of maturity comparable to its univariate counterparts.\n\nWe take an illustrative synthetic bivariate data set of 50 points to illustrate the construction of histograms. This requires the choice of an anchor point (the lower left corner of the histogram grid). For the histogram on the left, we choose (−1.5, −1.5): for the one on the right, we shift the anchor point by 0.125 in both directions to (−1.625, −1.625). Both histograms have a binwidth of 0.5, so any differences are due to the change in the anchor point only. The colour-coding indicates the number of data points which fall into a bin: 0=white, 1=pale yellow, 2=bright yellow, 3=orange, 4=red. The left histogram appears to indicate that the upper half has a higher density than the lower half, whereas the reverse is the case for the right-hand histogram, confirming that histograms are highly sensitive to the placement of the anchor point.\n\nOne possible solution to this anchor point placement problem is to remove the histogram binning grid completely. In the left figure below, a kernel (represented by the grey lines) is centred at each of the 50 data points above. The result of summing these kernels is given on the right figure, which is a kernel density estimate. The most striking difference between kernel density estimates and histograms is that the former are easier to interpret since they do not contain artifices induced by a binning grid.\nThe coloured contours correspond to the smallest region which contains the respective probability mass: red = 25%, orange + red = 50%, yellow + orange + red = 75%, thus indicating that a single central region contains the highest density.\n\nThe goal of density estimation is to take a finite sample of data and to make inferences about the underlying probability density function everywhere, including where no data are observed. In kernel density estimation, the contribution of each data point is smoothed out from a single point into a region of space surrounding it. Aggregating the individually smoothed contributions gives an overall picture of the structure of the data and its density function. In the details to follow, we show that this approach leads to a reasonable estimate of the underlying density function.\n\nThe previous figure is a graphical representation of kernel density estimate, which we now define in an exact manner. Let x, x, …, x be a sample of \"d\"-variate random vectors drawn from a common distribution described by the density function \"ƒ\". The kernel density estimate is defined to be\nwhere\n\nThe choice of the kernel function \"K\" is not crucial to the accuracy of kernel density estimators, so we use the standard multivariate normal kernel throughout: formula_3, where H plays the role of the covariance matrix. On the other hand, the choice of the bandwidth matrix H is the single most important factor affecting its accuracy since it controls the amount and orientation of smoothing induced. That the bandwidth matrix also induces an orientation is a basic difference between multivariate kernel density estimation from its univariate analogue since orientation is not defined for 1D kernels. This leads to the choice of the parametrisation of this bandwidth matrix. The three main parametrisation classes (in increasing order of complexity) are \"S\", the class of positive scalars times the identity matrix; \"D\", diagonal matrices with positive entries on the main diagonal; and \"F\", symmetric positive definite matrices. The \"S\" class kernels have the same amount of smoothing applied in all coordinate directions, \"D\" kernels allow different amounts of smoothing in each of the coordinates, and \"F\" kernels allow arbitrary amounts and orientation of the smoothing. Historically \"S\" and \"D\" kernels are the most widespread due to computational reasons, but research indicates that important gains in accuracy can be obtained using the more general \"F\" class kernels.\n\nThe most commonly used optimality criterion for selecting a bandwidth matrix is the MISE or mean integrated squared error\n\nThis in general does not possess a closed-form expression, so it is usual to use its asymptotic approximation (AMISE) as a proxy\n\nwhere\n\nThe quality of the AMISE approximation to the MISE is given by\n\nwhere \"o\" indicates the usual small o notation. Heuristically this statement implies that the AMISE is a 'good' approximation of the MISE as the sample size n → ∞.\n\nIt can be shown that any reasonable bandwidth selector H has H = \"O\"(\"n\") where the big O notation is applied elementwise. Substituting this into the MISE formula yields that the optimal MISE is \"O\"(\"n\"). Thus as \"n\" → ∞, the MISE → 0, i.e. the kernel density estimate converges in mean square and thus also in probability to the true density \"f\". These modes of convergence are confirmation of the statement in the motivation section that kernel methods lead to reasonable density estimators. An ideal optimal bandwidth selector is\n\nSince this ideal selector contains the unknown density function \"ƒ\", it cannot be used directly. The many different varieties of data-based bandwidth selectors arise from the different estimators of the AMISE. We concentrate on two classes of selectors which have been shown to be the most widely applicable in practice: smoothed cross validation and plug-in selectors.\n\nThe plug-in (PI) estimate of the AMISE is formed by replacing Ψ by its estimator formula_12\n\nwhere formula_14. Thus formula_15 is the plug-in selector. These references also contain algorithms on optimal estimation of the pilot bandwidth matrix G and establish that formula_16 converges in probability to H.\n\nSmoothed cross validation (SCV) is a subset of a larger class of cross validation techniques. The SCV estimator differs from the plug-in estimator in the second term\n\nThus formula_18 is the SCV selector.\nThese references also contain algorithms on optimal estimation of the pilot bandwidth matrix G and establish that formula_19 converges in probability to H.\n\nSilverman's rule of thumb suggests using formula_20 where formula_21 is the standard deviation of the ith variable and formula_22. Scott's rule is formula_23.\n\nIn the optimal bandwidth selection section, we introduced the MISE. Its construction relies on the expected value and the variance of the density estimator\n\nwhere * is the convolution operator between two functions, and\n\nFor these two expressions to be well-defined, we require that all elements of H tend to 0 and that \"n\" |H| tends to 0 as \"n\" tends to infinity. Assuming these two conditions, we see that the expected value tends to the true density \"f\" i.e. the kernel density estimator is asymptotically unbiased; and that the variance tends to zero. Using the standard mean squared value decomposition\n\nwe have that the MSE tends to 0, implying that the kernel density estimator is (mean square) consistent and hence converges in probability to the true density \"f\". The rate of convergence of the MSE to 0 is the necessarily the same as the MISE rate noted previously \"O\"(\"n\"), hence the covergence rate of the density estimator to \"f\" is \"O\"(n) where \"O\" denotes order in probability. This establishes pointwise convergence. The functional covergence is established similarly by considering the behaviour of the MISE, and noting that under sufficient regularity, integration does not affect the convergence rates.\n\nFor the data-based bandwidth selectors considered, the target is the AMISE bandwidth matrix. We say that a data-based selector converges to the AMISE selector at relative rate \"O\"(\"n\"), \"α\" > 0 if\n\nIt has been established that the plug-in and smoothed cross validation selectors (given a single pilot bandwidth G) both converge at a relative rate of \"O\"(\"n\") i.e., both these data-based selectors are consistent estimators.\n\nThe ks package in R implements the plug-in and smoothed cross validation selectors (amongst others). This dataset (included in the base distribution of R) contains\n272 records with two measurements each: the duration time of an eruption (minutes) and the\nwaiting time until the next eruption (minutes) of the Old Faithful Geyser in Yellowstone National Park, USA.\n\nThe code fragment computes the kernel density estimate with the plug-in bandwidth matrix formula_28 Again, the coloured contours correspond to the smallest region which contains the respective probability mass: red = 25%, orange + red = 50%, yellow + orange + red = 75%. To compute the SCV selector, codice_1 is replaced with codice_2. This is not displayed here since it is mostly similar to the plug-in estimate for this example.\n\nWe consider estimating the density of the Gaussian mixture\nfrom 500 randomly generated points. We employ the Matlab routine for\n2-dimensional data.\nThe routine is an automatic bandwidth selection method specifically designed\nfor a second order Gaussian kernel.\nThe figure shows the joint density estimate that results from using the automatically selected bandwidth.\n\nMatlab script for the example\n\nType the following commands in Matlab after\ndownloading\nand saving the function kde2d.m\nin the current directory.\n\nThe MISE is the expected integrated \"L\" distance between the density estimate and the true density function \"f\". It is the most widely used, mostly due to its tractability and most software implement MISE-based bandwidth selectors. \nThere are alternative optimality criteria, which attempt to cover cases where MISE is not an appropriate measure. The equivalent \"L\" measure, Mean Integrated Absolute Error, is\n\nIts mathematical analysis is considerably more difficult than the MISE ones. In practise, the gain appears not to be significant. The \"L\" norm is the Mean Uniform Absolute Error\n\nwhich has been investigated only briefly. Likelihood error criteria include those based on the Mean Kullback-Leibler distance\n\nand the Mean Hellinger distance\n\nThe KL can be estimated using a cross-validation method, although KL cross-validation selectors can be sub-optimal even if it remains consistent for bounded density functions. MH selectors have been briefly examined in the literature. \nAll these optimality criteria are distance based measures, and do not always correspond to more intuitive notions of closeness, so more visual criteria have been developed in response to this concern.\n\nRecent research has shown that the kernel and its bandwidth can both be optimally and objectively chosen from the input data itself without making any assumptions about the form of the distribution. The resulting kernel density estimate converges rapidly to the true probability distribution as samples are added: at a rate close to the formula_33 expected for parametric estimators. This kernel estimator works for univariate and multivariate samples alike. The optimal kernel is defined in Fourier space—as the optimal damping function formula_34 (the Fourier transform of the kernel formula_35 )-- in terms of the Fourier transform of the data formula_36, the \"empirical characteristic function\" (see Kernel density estimation):\n\nformula_37 \n\nformula_38\n\nwhere, \"N\" is the number of data points, \"d\" is the number of dimensions (variables), and formula_39 is a filter that is equal to 1 for 'accepted frequencies' and 0 otherwise. There are various ways to define this filter function, and a simple one that works for univariate or multivariate samples is called the 'lowest contiguous hypervolume filter'; formula_39 is chosen such that the only accepted frequencies are a contiguous subset of frequencies surrounding the origin for which formula_41 (see for a discussion of this and other filter functions).\n\nNote that direct calculation of the \"empirical characteristic function\" (ECF) is slow, since it essentially involves a direct Fourier transform of the data samples. However, it has been found that the ECF can be approximated accurately using a non-uniform fast Fourier transform (nuFFT) method, which increases the calculation speed by several orders of magnitude (depending on the dimensionality of the problem). The combination of this objective KDE method and the nuFFT-based ECF approximation has been referred to as \"fastKDE\" in the literature.\n\n"}
{"id": "24064208", "url": "https://en.wikipedia.org/wiki?curid=24064208", "title": "Neumann polynomial", "text": "Neumann polynomial\n\nIn mathematics, a Neumann polynomial, introduced by Carl Neumann for the special case formula_1, is a polynomial in 1/\"z\" used to expand functions in term of Bessel functions.\n\nThe first few polynomials are\n\nA general form for the polynomial is\n\nthey have the generating function \nwhere \"J\" are Bessel functions.\n\nTo expand a function \"f\" in form \nfor formula_10\ncompute\nwhere formula_12 and \"c\" is the distance of the nearest singularity of formula_13 from formula_14.\n\nAn example is the extension\nor the more general Sonine formula\nwhere formula_17 is Gegenbauer's polynomial. Then,\nthe confluent hypergeometric function\nand in particular\nthe index shift formula\nthe Taylor expansion (addition formula)\n(cf.) and the expansion of the integral of the Bessel function\nare of the same type.\n\n"}
{"id": "13053993", "url": "https://en.wikipedia.org/wiki?curid=13053993", "title": "Number Theory Foundation", "text": "Number Theory Foundation\n\nThe Number Theory Foundation (NTF) is a non-profit organization based in the United States which supports research and conferences in the field of number theory.\n\nThe NTF funds the Selfridge prize which is awarded at the ANTS conferences, and is a recurring supporter of the West Coast Number Theory conference.\nThe NTF will supply a prize of $500 for a counterexample to Selfridge's Primality Testing Conjecture.\n\n"}
{"id": "10567836", "url": "https://en.wikipedia.org/wiki?curid=10567836", "title": "Ordinal numerical competence", "text": "Ordinal numerical competence\n\nIn human developmental psychology or non-human primate experiments, ordinal numerical competence or ordinal numerical knowledge is the ability to count objects in order and to understand the greater than and less than relationships between numbers. It has been shown that children as young as two can make some ordinal numerical decisions. There are studies indicating that some non-human primates, like chimpanzees and rhesus monkeys have some ordinal numerical competence.\n\nThere is no evidence to support prenatal ordinal numerical competence. Teratogens such as stress can alter prenatal neural development, leading to diminished competence after birth. Physical effects of teratogens are common, but endocrine effects are harder to measure. These are the factors that influence neural development and by extension the development of ordinal numerical competence. Premature birth is also a risk factor for developmental problems including reduced brain activity. Brain activity is measured from outside the body with electroencephalography.\n\nThere have been a vast number of studies done on infants and their knowledge of numbers. Most research confirms that infants do in fact have a profound innate sense of number, both in abstract and finite ways. Infants as young as 49 hours can accurately match up images with a certain number of objects, with sounds that contain the same number (\"ra, ra, ra, ra\") as the number of objects in the image. Because the sounds are abstract, or visibly there, we can see that infants as young as 49 hours have some abstract numerical sense as well as concrete numerical sense shown by their recognition of the image with the corresponding number of objects. Similarly, infants around the age of 7 months can also match up images of random objects.\n\nAlthough children as young as 49 hours can match up the number of sounds with the number of objects, they can only do so at certain ratios. When 1:3 ratios were used (4 sounds and 4 objects or 12 objects), around 90% of the infants paid more attention to the corresponding image thus showing their recognition. However, when 1:2 ratios were used, only 68% of infants showed recognition of the correct corresponding image. This tells us that although infants can recognize corresponding numbers of sounds and objects, the two images of objects must be visibly different - one must have a much larger number of objects, or a much smaller number of objects.\n\nAlthough there has to be a stark difference in the choices for infants to recognize the correct matching set of numbers (1:3 vs 1:2), this seems to prove that infants have an innate numerical sense, but it may not be the same numerical sense as older children. Around the age of three and a half years children lose some of their numerical sense. Whereas children younger than three can recognize that four pebbles spread out in a line is less than six pebbles scrunched together in a line, children around the age of three and a half mysteriously lose this ability. Researchers believe that this is because children around this age begin to rely heavily on the physical properties of the world and objects within it, such that longer equals more. Although the ability to recognize that six pebbles closely lined up together is more than four pebbles spread out farther from one another goes away around that age, it comes back around four years of age when children begin to count.\n\nBoth behavioral research and brain-imaging research show distinct differences in the way \"exact\" arithmetic and \"approximate\" arithmetic are processed. Exact arithmetic is information that is precise and follows specific rules and patterns such as multiplication tables or geometric formulas, and approximate arithmetic is a general comparison between numbers such as the comparisons of greater than or less than. Research shows that exact arithmetic is language-based and processed in the left inferior frontal lobe. Approximate arithmetic is processed much differently in a different part of the brain. Approximate arithmetic is processed in the bilateral areas of the parietal lobes. This part of the brain processes visual information to understand how objects are spatially related to each other, for example, understanding that 10 of something is more than two of something. This difference in brain function can create a difference in how we experience certain types of arithmetic. Approximate arithmetic can be experienced as intuitive and exact arithmetic experienced as recalled knowledge.\n\nThe conclusions from behavioral research and brain-imaging research are supported by observations of patients with injuries to certain parts of the brain. People with left parietal injuries can lose the ability to understand quantities of things, but keep at least some ability to do exact arithmetic, such as multiplication. People with left-hemisphere brain damage can lose the ability to do exact arithmetic, but keep a sense of quantity, including the ability to compare larger and smaller numbers. This information confirms that distinct parts of the brain are used to know and use approximate and exact arithmetic.\n\nVarious researchers suggest that the processing of approximate arithmetic could be related to the numerical abilities that have been independently established in various animal species and in preverbal human infants. This may mean that approximate arithmetic is an adaptive train that humans developed through evolution. The combination of this potential evolutionary trait and language-based exact arithmetic may be the reason that humans are able to do advanced mathematics like physics.\n\nAnimals share a non-verbal system for representing number as analogue magnitudes.\nAnimals have been known to base their rationality on Weber’s Law. This historically important psychological law quantifies the perception of change in a given stimulus. The law states that the change in a stimulus that will be just noticeable is a constant ratio of the original stimulus. Weber’s Law describes discriminability between values based on perceptual continua such as line length, brightness, and weight.\n\nStudies of rhesus monkeys' foraging decisions indicate that animals spontaneously, and without training, exhibit rudimentary numerical abilities. Most animals can determine numbers in the values 1 through 9, but recent experiments have discovered that rhesus monkeys can quantify values from 1 to 30. Monkeys' numerical discrimination capacity is imposed by the ratio of the values compared, rather than absolute set size.\nThis computation process focuses around Weber’s Law and the expectation violation procedure. This suggests that rhesus monkeys have access to a spontaneous system of representation, which encodes the numerical differences between sets of one, two and three objects, and contrasts three objects from either four or five objects as well. These representations indicate the semantics of an encoded natural language. These encoded natural languages are also seen in experiments with many animals including pigeons and rats.\n\nExperiments have shown that rats are able to be trained to press one lever after hearing two bursts of white noise, then press another lever after four bursts of white noise. The interburst interval is varied between trials so the discrimination is based on number of bursts and not time duration of the sequence. Studies show that rats as well as pigeons learned to make different responses to both short and long durations of signals. During testing, rats exhibited a pattern called \"break-run-break\"; when it came to responding after a stint of little to no response, they would suddenly respond in high frequency, then return to little or no response activity. \nData suggests that rats and pigeons are able to process time and number information at the same time. The Mode Control Model shows that these animals can process number and time information by transmission pulses to accumulators controlled by switches that operate different modes.\n"}
{"id": "38912269", "url": "https://en.wikipedia.org/wiki?curid=38912269", "title": "Parent function", "text": "Parent function\n\nIn mathematics, a parent function is the simplest function of a family of functions that preserves the definition (or shape) of the entire family. For example, for the family of quadratic functions having the general form\nthe simplest function is\nThis is therefore the parent function of the family of quadratic equations.\n\nFor linear and quadratic functions, the graph of any function can be obtained from the graph of the parent function by simple translations and stretches parallel to the axes. For example, the graph of \"y\" = \"x\" − 4\"x\" + 7 can be obtained from the graph of \"y\" = \"x\" by translating +2 units along the X axis and +3 units along Y axis. This is because the equation can also be written as \"y\" − 3 = (\"x\" − 2).\n\nFor many trigonometric functions, the parent function is usually a basic sin(\"x\"), cos(\"x\"), or tan(\"x\"). For example, the graph of \"y\" = \"A\" sin(\"x\") + \"B\" cos(\"x\") can be obtained from the graph of \"y\" = sin(\"x\") by translating it through an angle α along the positive X axis (where tan(α) = ), then stretching it parallel to the Y axis using a stretch factor \"R\", where \"R\" = \"A\" + \"B\". This is because \"A\" sin(\"x\") + \"B\" cos(\"x\") can be written as \"R\" sin(\"x\"−α) (see List of trigonometric identities).\n\nThe concept of parent function is less clear for polynomials of higher power because of the extra turning points, but for the family of \"n\"-degree polynomial functions for any given \"n\", the parent function is sometimes taken as \"x\", or, to simplify further, \"x\" when \"n\" is even and \"x\" for odd \"n\". Turning points may be established by differentiation to provide more detail of the graph.\n\n\n"}
{"id": "30251309", "url": "https://en.wikipedia.org/wiki?curid=30251309", "title": "Predicative programming", "text": "Predicative programming\n\nPredicative programming is a methodology for program specification and refinement. The central idea of predicative programming is that each specification is a predicate (generally written as a boolean expression) that is true of acceptable behaviours and false of unacceptable behaviours. It follows that refinement is reversed implication universally quantified over behaviours:\nCommands in a programming language are considered to be a special case of specifications—special only because they are compilable. For example, in an environment where the program variables are formula_2, formula_3, and formula_4, the command formula_5 is considered equivalent to the predicate (represented here by a boolean expression)\nin which formula_2, formula_3, and formula_4 represent the initial values of the program variables and formula_10, formula_11, and formula_12 represent the final values of the program variables. Thus\n\n\n"}
{"id": "34450103", "url": "https://en.wikipedia.org/wiki?curid=34450103", "title": "Probability bounds analysis", "text": "Probability bounds analysis\n\nProbability bounds analysis (PBA) is a collection of methods of uncertainty propagation for making qualitative and quantitative calculations in the face of uncertainties of various kinds. It is used to project partial information about random variables and other quantities through mathematical expressions. For instance, it computes sure bounds on the distribution of a sum, product, or more complex function, given only sure bounds on the distributions of the inputs. Such bounds are called probability boxes, and constrain cumulative probability distributions (rather than densities or mass functions).\n\nThis bounding approach permits analysts to make calculations without requiring overly precise assumptions about parameter values, dependence among variables, or even distribution shape. Probability bounds analysis is essentially a combination of the methods of standard interval analysis and classical probability theory. Probability bounds analysis gives the same answer as interval analysis does when only range information is available. It also gives the same answers as Monte Carlo simulation does when information is abundant enough to precisely specify input distributions and their dependencies. Thus, it is a generalization of both interval analysis and probability theory.\n\nThe diverse methods comprising probability bounds analysis provide algorithms to evaluate mathematical expressions when there is uncertainty about the input values, their dependencies, or even the form of mathematical expression itself. The calculations yield results that are guaranteed to enclose all possible distributions of the output variable if the input p-boxes were also sure to enclose their respective distributions. In some cases, a calculated p-box will also be best-possible in the sense that\nthe bounds could be no tighter without excluding some of the possible\ndistributions.\n\nP-boxes are usually merely bounds on possible distributions. The bounds often also enclose distributions that are not themselves possible. For instance, the set of probability distributions that could result from adding random values without the independence assumption from two (precise) distributions is generally a proper subset of all the distributions enclosed by the p-box computed for the sum. That is, there are distributions within the output p-box that could not arise under any dependence between the two input distributions. The output p-box will, however, always contain all distributions that are possible, so long as the input p-boxes were sure to enclose their respective underlying distributions. This property often suffices for use in risk analysis and other fields requiring calculations under uncertainty.\n\nThe idea of bounding probability has a very long\ntradition throughout the history of probability theory. Indeed, in 1854 George Boole used the notion of interval bounds on probability in his The Laws of Thought. Also dating from the latter half of the 19th century, the inequality attributed to Chebyshev described bounds on a distribution when only the mean and\nvariance of the variable are known, and the related inequality attributed to Markov found bounds on a\npositive variable when only the mean is known.\nKyburg reviewed the history\nof interval probabilities and traced the development of the critical ideas through the 20th century, including the important notion of incomparable probabilities favored by Keynes.\nOf particular note is Fréchet's derivation in the 1930s of bounds on calculations involving total probabilities without\ndependence assumptions. Bounding probabilities has continued to the\npresent day (e.g., Walley's theory of imprecise probability.)\n\nThe methods of probability bounds analysis that could be routinely used in\nrisk assessments were developed in the 1980s. Hailperin described a computational scheme for bounding logical calculations extending the ideas of Boole. Yager described the elementary procedures by which bounds on convolutions can be computed under an assumption of independence. At about the same time, Makarov, and independently, Rüschendorf solved the problem, originally posed by Kolmogorov, of how to find the upper and lower bounds for the probability distribution of a sum of random variables whose marginal distributions, but not their joint distribution, are known. Frank et al. generalized the result of Makarov and expressed it in terms of copulas. Since that time, formulas and algorithms for sums have been generalized and extended to differences, products, quotients and other binary and unary functions under various dependence assumptions. \n\nArithmetic expressions involving operations such as additions, subtractions, multiplications, divisions, minima, maxima, powers, exponentials, logarithms, square roots, absolute values, etc., are commonly used in risk analyses and uncertainty modeling. Convolution is the operation of finding the probability distribution of a sum of independent random variables specified by probability distributions. We can extend the term to finding distributions of other mathematical functions (products, differences, quotients, and more complex functions) and other assumptions about the intervariable dependencies. There are convenient algorithms for computing these generalized convolutions under a variety of assumptions about the dependencies among the inputs.\n\nLet formula_1 denote the space of distribution functions on the real numbers formula_2 i.e., \n\nA p-box is a quintuple \n\nwhere formula_5 are real intervals, and formula_6 This quintuple denotes the set of distribution functions formula_7 such that:\n\nIf a function satisfies all the conditions above it is said to be \"inside\" the p-box. In some cases, there may be no information about the moments or distribution family other than what is encoded in the two distribution functions that constitute the edges of the p-box. Then the quintuple representing the p-box formula_9 can be denoted more compactly as [\"B\", \"B\"]. This notation harkens to that of intervals on the real line, except that the endpoints are distributions rather than points.\n\nThe notation formula_10 denotes the fact that formula_11 is a random variable governed by the distribution function \"F\", that is, \n\nLet us generalize the tilde notation for use with p-boxes. We will write \"X\" ~ \"B\" to mean that \"X\" is a random variable whose distribution function is unknown except that it is inside \"B\". Thus, \"X\" ~ \"F\" ∈ \"B\" can be contracted to X ~ B without mentioning the distribution function explicitly.\n\nIf \"X\" and \"Y\" are independent random variables with distributions \"F\" and \"G\" respectively, then \"X\" + \"Y\" = \"Z\" ~ \"H\" given by\n\nThis operation is called a convolution on \"F\" and \"G\". The analogous operation on p-boxes is straightforward for sums. Suppose\n\nIf \"X\" and \"Y\" are stochastically independent, then the distribution of \"Z\" = \"X\" + \"Y\" is inside the p-box\n\nFinding bounds on the distribution of sums \"Z\" = \"X\" + \"Y\" \"without making any assumption about the dependence\" between \"X\" and \"Y\" is actually easier than the problem assuming independence. Makarov showed that\n\nThese bounds are implied by the Fréchet–Hoeffding copula bounds. The problem can also be solved using the methods of mathematical programming.\n\nThe convolution under the intermediate assumption that \"X\" and \"Y\" have positive dependence is likewise easy to compute, as is the convolution under the extreme assumptions of perfect positive or perfect negative dependency between \"X\" and \"Y\".\n\nGeneralized convolutions for other operations such as subtraction, multiplication, division, etc., can be derived using transformations. For instance, p-box subtraction \"A\" − \"B\" can be defined as \"A\" + (−\"B\"), where the negative of a p-box \"B\" = [\"B\", \"B\"] is [\"B\"(−\"x\"), \"B\"(−\"x\")].\n\nLogical or Boolean expressions involving conjunctions (AND operations), disjunctions (OR operations), exclusive disjunctions, equivalences, conditionals, etc. arise in the analysis of fault trees and event trees common in risk assessments. If the probabilities of events are characterized by intervals, as suggested by Boole and Keynes among others, these binary operations are straightforward to evaluate. For example, if the probability of an event A is in the interval P(A) = \"a\" = [0.2, 0.25], and the probability of the event B is in P(B) = \"b\" = [0.1, 0.3], then the probability of the conjunction is surely in the interval\nso long as A and B can be assumed to be independent events. If they are not independent, we can still bound the conjunction using the classical Fréchet inequality. In this case, we can infer at least that the probability of the joint event A & B is surely within the interval\nwhere env([\"x\",\"x\"], [\"y\",\"y\"]) is [min(\"x\",\"y\"), max(\"x\",\"y\")]. Likewise, the probability of the disjunction is surely in the interval\nif A and B are independent events. If they are not independent, the Fréchet inequality bounds the disjunction\n\nIt is also possible to compute interval bounds on the conjunction or disjunction under other assumptions about the dependence between A and B. For instance, one might assume they are positively dependent, in which case the resulting interval is not as tight as the answer assuming independence but tighter than the answer given by the Fréchet inequality. Comparable calculations are used for other logical functions such as negation, exclusive disjunction, etc. When the Boolean expression to be evaluated becomes complex, it may be necessary to evaluate it using the methods of mathematical programming to get best-possible bounds on the expression. A similar problem one presents in the case of probabilistic logic (see for example Gerla 1994). If the probabilities of the events are characterized by probability distributions or p-boxes rather than intervals, then analogous calculations can be done to obtain distributional or p-box results characterizing the probability of the top event. \n\nThe probability that an uncertain number represented by a p-box \"D\" is less than zero is the interval Pr(\"D\" < 0) = [\"F\"(0), \"F̅\"(0)], where \"F̅\"(0) is the left bound of the probability box \"D\" and \"F\"(0) is its right bound, both evaluated at zero. Two uncertain numbers represented by probability boxes may then be compared for numerical magnitude with the following encodings:\nThus the probability that \"A\" is less than \"B\" is the same as the probability that their difference is less than zero, and this probability can be said to be the value of the expression \"A\" < \"B\".\n\nLike arithmetic and logical operations, these magnitude comparisons generally depend on the stochastic dependence between \"A\" and \"B\", and the subtraction in the encoding should reflect that dependence. If their dependence is unknown, the difference can be computed without making any assumption using the Fréchet operation.\n\nSome analysts use sampling-based approaches to computing probability bounds, including Monte Carlo simulation, Latin hypercube methods or importance sampling. These approaches cannot assure mathematical rigor in the result because such simulation methods are approximations, although their performance can generally be improved simply by increasing the number of replications in the simulation. Thus, unlike the analytical theorems or methods based on mathematical programming, sampling-based calculations usually cannot produce verified computations. However, sampling-based methods can be very useful in addressing a variety of problems which are computationally difficult to solve analytically or even to rigorously bound. One important example is the use of Cauchy-deviate sampling to avoid the curse of dimensionality in propagating interval uncertainty through high-dimensional problems.\n\nPBA belongs to a class of methods that use imprecise probabilities to simultaneously represent aleatoric and epistemic uncertainties. PBA is a generalization of both interval analysis and probabilistic convolution such as is commonly implemented with Monte Carlo simulation. PBA is also closely related to robust Bayes analysis, which is sometimes called Bayesian sensitivity analysis. PBA is an alternative to second-order Monte Carlo simulation.\n\n\n\n"}
{"id": "3226068", "url": "https://en.wikipedia.org/wiki?curid=3226068", "title": "Proth number", "text": "Proth number\n\nIn number theory, a Proth number is a number of the form\n\nwhere formula_2 is an odd positive integer and formula_3 is a positive integer such that formula_4. They are named after the mathematician François Proth. The first few Proth numbers are\n\nThe Cullen numbers (numbers of the form ) and Fermat numbers (numbers of the form ) are special cases of Proth numbers. Without the condition that formula_4, all odd integers greater than 1 would be Proth numbers.\n\nA Proth prime is a Proth number which is prime. The first few Proth primes are \n\nThe primality of a Proth number can be tested with Proth's theorem, which states that a Proth number formula_6 is prime if and only if there exists an integer formula_7 for which \n\nThe largest known Proth prime is formula_9, and is 9,383,761 digits long. It was found by Szabolcs Peter in the PrimeGrid distributed computing project which announced it on 6 November 2016. It is also the largest known non-Mersenne prime.\n\n"}
{"id": "28430974", "url": "https://en.wikipedia.org/wiki?curid=28430974", "title": "Reduction of summands", "text": "Reduction of summands\n\nReduction of summands is an algorithm for fast binary multiplication of non-signed binary integers. It is performed in three steps: production of summands, reduction of summands, and summation.\n\nIn binary multiplication, each row of the summands will be either zero or one of the numbers to be multiplied. Consider the following:\nThe second and fourth row of the summands are equivalent to the first term. Production of the summands requires a simple AND gate for each summand. Given enough AND gates, the time to produce the summands will be one cycle of the arithmetic logic unit.\n\nThe summands are reduced using a common 1-bit full adder that accepts two 1-bit terms and a carry-in bit. It produces a sum and a carry-out. The full adders are arranged such that the sum remains in the same column of summands, but the carry-out is shifted left. In each round of reduction, three bits in a single column are used as the two terms and carry-in for the full adder, producing a single sum bit for the column. This reduces the bits in the column by a factor of 3. However, the column to the right will shift over carry-out bits, increasing the bits in the column by a third of the number of rows of summands. At worst, the reduction will be 2/3 the number of rows per round of reduction.\n\nThe following shows how the first round of reduction is performed. Note that all \"empty\" positions of the summands are considered to be zero (a . is used here as indicator of the \"assumed zero values\"). In each row, the top three bits are the three inputs to the full adder (two terms and carry-in). The sum is placed in the top bit of the column. The carry-out is placed in the second row of the column to the left. The bottom bit is a single feed into an adder. The sum of this adder is placed in the third row of the column. Carry-out is ignored as it will always be zero, but by design it would be placed in the fourth row of the column to the left. For design, it is important to note that rows 1, 3, 5, ... (counting from the top) are filled with sums from the column itself. Rows 2, 4, 6, ... are filled with carry-out values from the column to the right.\nReduction is performed again in exactly the same way. This time, only the top three rows of summands are of interest because all other summands must be zero.\nWhen there are only two significant rows of summands, the reduction cycles end. A basic full adder normally requires three cycles of the arithmetic logic unit. Therefore, each cycle of reduction is commonly 3 cycles long.\n\nWhen there are only two rows of summands remaining, they are added using a fast adder. There are many designs of fast adders, any of which may be used to complete this algorithm.\n\nThe calculation time for the reduction of summands algorithm is: \"T\" = 1Δt + r3Δt + FA (where r is the number of reduction cycles and FA is the time for the fast adder at the end of the algorithm).\n"}
{"id": "293533", "url": "https://en.wikipedia.org/wiki?curid=293533", "title": "Sign convention", "text": "Sign convention\n\nIn physics, a sign convention is a choice of the physical significance of signs (plus or minus) for a set of quantities, in a case where the choice of sign is arbitrary. \"Arbitrary\" here means that the same physical system can be correctly described using different choices for the signs, as long as one set of definitions is used consistently. The choices made may differ between authors. Disagreement about sign conventions is a frequent source of confusion, frustration, misunderstandings, and even outright errors in scientific work. In general, a sign convention is a special case of a choice of coordinate system for the case of one dimension.\n\nSometimes, the term \"sign convention\" is used more broadly to include factors of \"i\" and 2π, rather than just choices of sign.\n\nIn relativity, the metric signature can be either (+,−,−,−) or (−,+,+,+). (Note that throughout this article we are displaying the signs of the eigenvalues of the metric in the order that presents the timelike component first, followed by the spacelike components). A similar convention is used in higher-dimensional relativistic theories; that is, (+,−,−,−...) or (−,+,+,+...). A choice of signature is associated with a variety of names:\n\nWe catalog the choices of various authors of some graduate textbooks:\n\nThe signature + − − − corresponds to the metric tensor:\n\nwhereas the signature − + + + corresponds to:\n\nThe Ricci tensor is defined as the contraction of the Riemann tensor. Some authors use the contraction formula_3, whereas others use the alternative formula_4. Due to the symmetries of the Riemann tensor, these two definitions differ by a minus sign.\n\nIn fact, the second definition of the Ricci tensor is formula_5. The sign of the Ricci tensor does not change, because the two sign conventions concern the sign of the Riemann tensor. The second definition just compensates the sign, and it works together with the second definition of the Riemann tensor (see e.g. Barrett O'Neill's Semi-riemannian geometry).\n\n\nIt is often considered good form to state explicitly which sign convention is to be used at the beginning of each book or article.\n\n"}
{"id": "24641580", "url": "https://en.wikipedia.org/wiki?curid=24641580", "title": "Speakeasy (computational environment)", "text": "Speakeasy (computational environment)\n\nSpeakeasy is a numerical computing interactive environment also featuring an interpreted programming language. It was initially developed for internal use at the Physics Division of Argonne National Laboratory by the theoretical physicist Stanley Cohen. He eventually founded Speakeasy Computing Corporation to make the program available commercially.\n\nSpeakeasy is a very long-lasting numerical package. In fact, the original version of the environment was built around a core dynamic data repository called \"Named storage\" developed in the early 1960s, while the most recent version has been released in 2006.\n\nSpeakeasy was aimed to make the computational work of the physicists at the Argonne National Laboratory easier. It was initially conceived to work on mainframes (the only kind of computers at that time), and was subsequently ported to new platforms (minicomputers, personal computers) as they became available. The porting of the same code on different platforms was made easier by using Mortran metalanguage macros to face systems dependencies and compilers deficiencies and differences.\nSpeakeasy is currently available on several platforms : PCs running Windows, macOS, Linux, departmental computers and workstations running several flavors of Linux, AIX or Solaris.\n\nSpeakeasy was also among the first interactive numerical computing environments, having been implemented in such a way on a CDC 3600 system, and later on IBM TSO machines as one was in beta-testing at the Argonne National Laboratory at the time.\n\nAlmost since the beginning (as the dynamic linking functionality was made available in the operating systems) Speakeasy features the capability of expanding its operational vocabulary using separated modules, dynamically linked to the core processor as they are needed. For that reason such modules were called \"linkules\" (LINKable-modULES). They are functions with a generalized interface, which can be written in FORTRAN or in C.\nThe independence of each of the new modules from the others and from the main processor is of great help in improving the system, especially it was in the old days.\n\nThis easy way of expanding the functionalities of the main processor was often exploited by the users to develop their own specialized packages. Besides the programs, functions and subroutines the user can write in the Speakeasy's own interpreted language, linkules add functionalities carried out with the typical performances of compiled programs.\n\nAmong the packages developed by the users, one of the most important is \"Modeleasy\", originally developed as \"FEDeasy\" in the early 1970s at the research department of the Federal Reserve Board of Governors in Washington D.C..\nModeleasy implements special objects and functions for large econometric models estimation and simulation.\nIts evolution led eventually to its distribution as an independent product.\n\nThe symbol :_ (colon+underscore) is both the Speakeasy logo and the prompt of the interactive session.\n\nThe dollar sign is used for delimiting comments; the ampersand is used to continue a statement on the following physical line, in which case the prompt becomes :& (colon+ampersand); a semicolon can separate statements written on the same physical line.\nAs its own name tells, Speakeasy was aimed to expose a syntax as friendly as possible to the user, and as close as possible to the spoken language. The best example of that is given by the set of commands for reading/writing data from/to the permanent storage. E.g. (the languages keywords are in upper case to clarify the point):\nVariables (i.e. Speakeasy objects) are given a name up to 255 character long, when LONGNAME option is ON, up to 8 characters otherwise (for backward compatibility). They are dynamically typed, depending on the value assigned to them.\nArguments of functions are usually not required to be surrounded by parenthesis or separated by commas, provided that the context remains clear and unambiguous. For example:\ncan be written :\nor even \nMany other syntax simplifications are possible; for example, to define an object named 'a' valued to a ten-elements array of zeroes, one can write any of the following statements:\nSpeakeasy is a vector-oriented language: giving a structured argument to a function of a scalar, the result is usually an object with the same structure of the argument, in which each element is the result of the function applied to the corresponding element of the argument. In the example given above, the result of function sin applied to the array (let us call it x) generated by the function grid is the array answer whose element answer(i) equals sin(x(i)) for each i from 1 to noels(x) (the number of elements of x). In other words, the statement\nis equivalent to the following fragment of program: \nThe vector-oriented statements avoid writing programs for such loops and are much faster than them.\n\nBy the very first statement of the session, the user can define the size of the \"named storage\" (or \"work area\", or \"allocator\"), which is allocated once and for all at the beginning of the session. Within this fixed-size work area, the Speakeasy processor dynamically creates and destroys the work objects as needed. A user-tunable garbage collection mechanism is provided to maximize the size of the free block in the work area, packing the defined objects in the low end or in the high end of the allocator. At any time, the user can ask about used or remaining space in the work area.\n\nWithin reasonable conformity and compatibility constraints, the Speakeasy objects can be operated on using the same algebraic syntax.\n\nFrom this point of view, and considering the dynamic and structured nature of the data held in the \"named storage\", it is possible to say that Speakeasy since the beginning implemented a very raw form of operator overloading, and a pragmatic approach to some features of what was later called \"Object Oriented Programming\", although it did not evolve further in that direction.\n\nSpeakeasy provides a bunch of predefined \"families\" of data objects: scalars, arrays (up to 15 dimensions), matrices, sets, time series.\n\nThe elemental data can be of kind real (8-bytes), complex (2x8-bytes), character-literal or name-literal ( matrices elements can be real or complex, time series values can only be real ).\n\nFor time series processing, five types of missing values are provided. They are denoted by N.A. (not available), N.C. (not computable), N.D. (not defined), along with N.B. and N.E. the meaning of which is not predetermined and is left available for the linkules developer. They are internally represented by specific (and very small) numeric values, acting as codes.\n\nAll the time series operations take care of the presence of missing values, propagating them appropriately in the results.\n\nDepending on a specific setting, missing values can be represented by the above notation, by a question mark symbol, or a blank (useful in tables). When used in input the question mark is interpreted as an N.A. missing value.\n\nIn numerical objects other than time series, the concept of \"missing values\" is meaningless, and the numerical operations on them use the actual numeric values regardless they correspond to \"missing values codes\" or not (although \"missing values codes\" can be input and shown as such).\n\nNote that, in other contexts, a question mark may have a different meaning: for example, when used as the first (and possibly only) character of a command line, it means the request to show more pieces of a long error message (which ends with a \"+\" symbol).\n\nSome support is provided for logical values, relational operators (the Fortran syntax can be used) and logical expressions.\n\nLogical values are stored actually as numeric values: with 0 meaning false and non-zero (1 on output) meaning true.\n\nSpecial objects such as \"PROGRAM\", \"SUBROUTINE\" and \"FUNCTION\" objects (collectively referred to as \"procedures\") can be defined for operations automation. Another way for running several instructions with a single command is to store them into a use-file and make the processor read them by mean of the USE command.\n\n\"USEing\" a use-file is the simplest way for performing several instruction with minimal typed input. (This operation roughly corresponds to what \"source-ing\" a file is in other scripting languages.)\n\nA use-file is an alternate input source to the standard console and can contain all the commands a user can input by the keyboard (hence no multi-line flow control construct is allowed). The processor reads and executes use-files one line at a time.\n\nUse-file execution can be concatenated but not nested, i.e. the control does not return to the caller at the completion of the called use-file.\n\nFull programming capability is achieved using \"procedures\". They are actually Speakeasy objects, which must be defined in the work area to be executed. An option is available in order to make the procedures being automatically retrieved and loaded from the external storage as they are needed.\n\nProcedures can contain any of the execution flow control constructs available in the Speakeasy programming language.\n\nA program can be run simply invoking its name or using it as the argument of the command EXECUTE. In the latter case, a further argument can identify a label from which the execution will begin.\nSpeakeasy programs differs from the other procedures for being executed at the same scoping \"level\" they are referenced to, hence they have full visibility of all the objects defined at that level, and all the objects created during their execution will be left there for subsequent uses. For that reason no argument list is needed.\n\nSubroutines and Functions are executed at a new scoping level, which is removed when they finish. The communication with the calling scoping level is carried out through the argument list (in both directions). This implements data hiding, i.e. objects created within a Subroutine or a Function are not visible to other Subroutine and Functions but through argument lists.\n\nA global level is available for storing object which must be visible from within any procedure, e.g. the procedures themselves.\n\nThe Functions differ from the Subroutines because they also return a functional value; reference to them can be part of more complex statement and are replaced by the returned functional value when evaluating the statement.\n\nIn some extent, Speakeasy Subroutines and Functions are very similar to the Fortran procedures of the same name.\n\nAn IF-THEN-ELSE construct is available for conditional execution and two forms of FOR-NEXT construct are provided for looping.\n\nA \"GO TO \"label\"\" statement is provided for jumping, while a Fortran-like computed GO TO statement can be used fort multiple branching.\n\nAn ON ERROR mechanism, with several options, provides a means for error handling.\n\nLinkules are functions usually written in Fortran ( or, unsupportedly, in C ). With the aid of Mortran or C macros and an API library, they can interface the Speakeasy workarea for retrieving, defining, manipulating any Speakeasy object.\n\nMost of the Speakeasy operational vocabulary is implemented via linkules. They can be statically linked to the core engine, or dynamically loaded as they are needed, provided they are properly compiled as shared objects (unix) or dll (windows).\n\n"}
{"id": "8199698", "url": "https://en.wikipedia.org/wiki?curid=8199698", "title": "Stationary sequence", "text": "Stationary sequence\n\nIn probability theory – specifically in the theory of stochastic processes, a stationary sequence is a random sequence whose joint probability distribution is invariant over time. If a random sequence \"X\" is stationary then the following holds:\n\nwhere \"F\" is the joint cumulative distribution function of the random variables in the subscript.\n\nIf a sequence is stationary then it is wide-sense stationary.\n\nIf a sequence is stationary then it has a constant mean (which may not be finite):\n\n\n"}
{"id": "13398615", "url": "https://en.wikipedia.org/wiki?curid=13398615", "title": "Super-prime", "text": "Super-prime\n\nSuper-prime numbers (also known as higher-order primes or prime-indexed primes) are the subsequence of prime numbers that occupy prime-numbered positions within the sequence of all prime numbers. The subsequence begins\nThat is, if \"p\"(\"i\") denotes the \"i\"th prime number, the numbers in this sequence are those of the form \"p\"(\"p\"(\"i\")). used a computer-aided proof (based on calculations involving the subset sum problem) to show that every integer greater than 96 may be represented as a sum of distinct super-prime numbers. Their proof relies on a result resembling Bertrand's postulate, stating that (after the larger gap between super-primes 5 and 11) each super-prime number is less than twice its predecessor in the sequence.\n\nsuper-primes up to \"x\".\nThis can be used to show that the set of all super-primes is small.\n\nOne can also define \"higher-order\" primeness much the same way and obtain analogous sequences of primes .\n\nA variation on this theme is the sequence of prime numbers with palindromic prime indices, beginning with\n\n\n"}
{"id": "412094", "url": "https://en.wikipedia.org/wiki?curid=412094", "title": "Symmetry of second derivatives", "text": "Symmetry of second derivatives\n\nIn mathematics, the symmetry of second derivatives (also called the equality of mixed partials) refers to the possibility under certain conditions (see below) of interchanging the order of taking partial derivatives of a function\n\nof \"n\" variables. If the partial derivative with respect to formula_2 is denoted with a subscript formula_3, then the symmetry is the assertion that the second-order partial derivatives formula_4 satisfy the identity\n\nso that they form an \"n\" × \"n\" symmetric matrix. This is sometimes known as Schwarz's theorem, Clairaut's theorem, or Young's theorem.\n\nIn the context of partial differential equations it is called the\nSchwarz integrability condition. \n\nThis matrix of second-order partial derivatives of \"f\" is called the Hessian matrix of \"f\". The entries in it off the main diagonal are the mixed derivatives; that is, successive partial derivatives with respect to different variables.\n\nIn most \"real-life\" circumstances the Hessian matrix is symmetric, although there are a great number of functions that do not have this property. Mathematical analysis reveals that symmetry requires a hypothesis on \"f\" that goes further than simply stating the existence of the second derivatives at a particular point. Schwarz' theorem gives a sufficient condition on \"f\" for this to occur.\n\nIn symbols, the symmetry says that, for example,\n\nThis equality can also be written as\n\nAlternatively, the symmetry can be written as an algebraic statement involving the differential operator \"D\" which takes the partial derivative with respect to \"x\":\n\nFrom this relation it follows that the ring of differential operators with constant coefficients, generated by the \"D\", is commutative. But one should naturally specify some domain for these operators. It is easy to check the symmetry as applied to monomials, so that one can take polynomials in the \"x\" as a domain. In fact smooth functions are possible.\n\nIn mathematical analysis, Schwarz's theorem (or Clairaut's theorem on equality of mixed partials) named after Alexis Clairaut and Hermann Schwarz, states that if\n\nhas continuous second partial derivatives at a given point in formula_9, say, formula_10 then formula_11\n\nThe partial differentiations of this function are commutative at that point. One easy way to establish this theorem (in the case where formula_13, formula_14, and formula_15, which readily entails the result in general) is by applying Green's theorem to the gradient of formula_16\n\nA weaker condition than the continuity of second partial derivatives (which is implied by the latter) which suffices to ensure symmetry is that all partial derivatives are themselves differentiable. Another strengthening of the theorem, in which \"existence\" of the permuted mixed partial is asserted, was provided by Peano:\n\n\"If formula_17 is defined on an open set formula_18 and formula_19 exist everywhere on formula_20 , and formula_21 is continuous at formula_22, then formula_23 exists at formula_24 and formula_25.\"\n\nThe result of the equality of the mixed partial derivatives under certain conditions has a long history. Nicolaus I Bernoulli implicitly assumed the result as early as 1721, but Euler was the first to provide a proof. Other proofs followed by Clairaut (1740), Lagrange (1797), Cauchy (1823) and many others in the 19th century. None of these proofs were without fault however (for example, Clairaut assumed all definite integrals could be differentiated under the integral sign). In 1867 Ernst Leonard Lindelöf published a paper criticizing in detail all the proofs he was familiar with. Finally, six years later Hermann Schwarz (1873) gave the first satisfactory proof. This was followed by successive refinements that relaxed the hypotheses in Schwarz's theorem in various ways, among others by Dini, Jordan, Peano, E. W. Hobson, W. H. Young. For a good historical account, see.\n\nThe theory of distributions (generalized functions) eliminates analytic problems with the symmetry. The derivative of an integrable function can always be defined as a distribution, and symmetry of mixed partial derivatives always holds as an equality of distributions. The use of formal integration by parts to define differentiation of distributions puts the symmetry question back onto the test functions, which are smooth and certainly satisfy this symmetry. In more detail (where \"f\" is a distribution, written as an operator on test functions, and \"φ\" is a test function),\n\nAnother approach, which defines the Fourier transform of a function, is to note that on such transforms partial derivatives become multiplication operators that commute much more obviously.\n\nThe symmetry may be broken if the function fails to have differentiable partial derivatives, which is possible if Clairaut's theorem is not satisfied (the second partial derivatives are not continuous).\nAn example of non-symmetry is the function\n\nThis function is everywhere continuous, but its derivatives at (0,0) cannot be computed algebraically. Rather, the limit of difference quotients shows that formula_27, so the graph z = f(x,y) has a horizontal tangent plane at (0,0), and the partial derivatives formula_28 exist and are everywhere continuous. However, the second partial derivatives are not continuous at (0,0), and the symmetry fails. In fact, along the \"x\"-axis the \"y\"-derivative is formula_29, and so:\nIn contrast, along the \"y\"-axis the \"x\"-derivative formula_31,\nand so formula_32.\nThat is, formula_33 at (0, 0), although the mixed partial derivatives do exist, and at every other point the symmetry does hold.\n\nThe above function, written in a cylindrical coordinate system, can be expressed as\nshowing that the function oscillates four times when traveling once around an arbitrarily small loop containing the origin. Intuitively, therefore, the local behavior of the function at formula_35 cannot be described as a quadratic form, and the Hessian matrix thus fails to be symmetric.\n\nIn general, the interchange of limiting operations need not commute. Given two variables near (0, 0) and two limiting processes on\ncorresponding to making \"h\" → 0 first, and to making \"k\" → 0 first. It can matter, looking at the first-order terms, which is applied first. This leads to the construction of pathological examples in which second derivatives are non-symmetric. This kind of example belongs to the theory of real analysis where the pointwise value of functions matters. When viewed as a distribution the second partial derivative's values can be changed at an arbitrary set of points as long as this has Lebesgue measure 0. Since in the example the Hessian is symmetric everywhere except (0,0), there is no contradiction with the fact that the Hessian, viewed as a Schwartz distribution, is symmetric.\n\nConsider the first-order differential operators \"D\" to be infinitesimal operators on Euclidean space. That is, \"D\" in a sense generates the one-parameter group of translations parallel to the \"x\"-axis. These groups commute with each other, and therefore the infinitesimal generators do also; the Lie bracket\n\nis this property's reflection. In other words, the Lie derivative of one coordinate with respect to another is zero.\n\nThe Clairaut-Schwarz theorem is the key fact needed to prove that for every formula_37(or at least twice continuously differentiable) differential form formula_38, the second exterior derivative vanishes: formula_39. This implies that every exact form (i.e., a form formula_40 such that formula_41 for some form formula_42) is closed (i.e., formula_43), since formula_44.\n"}
{"id": "15018472", "url": "https://en.wikipedia.org/wiki?curid=15018472", "title": "Szegő polynomial", "text": "Szegő polynomial\n\nIn mathematics, a Szegő polynomial is one of a family of orthogonal polynomials for the Hermitian inner product\n\nwhere dμ is a given positive measure on [−π, π]. Writing formula_2 for the polynomials, they obey a recurrence relation\n\nwhere formula_4 is a parameter, called the \"reflection coefficient\" or the \"Szegő parameter\".\n\n\n"}
{"id": "29249172", "url": "https://en.wikipedia.org/wiki?curid=29249172", "title": "Toida's conjecture", "text": "Toida's conjecture\n\nIn combinatorial mathematics, Toida's conjecture, due to Shunichi Toida in 1977, is a refinement of the disproven Ádám's conjecture in 1967. Toida's conjecture states formally:\n\nIf\n\nand\n\nthen \"formula_3\" is a CI-digraph.\n\nThe conjecture was proven in the special case where \"n\" is a prime power by Klin and Poschel in 1978, and by Golfand, Najmark, and Poschel in 1984.\n\nThe conjecture was then fully proven by Muzychuk, Klin, and Poschel in 2001 by using Schur algebra, and simultaneously by Dobson and Morris in 2002 by using the classification of finite simple groups.\n"}
{"id": "1735473", "url": "https://en.wikipedia.org/wiki?curid=1735473", "title": "Tomahawk (geometry)", "text": "Tomahawk (geometry)\n\nThe tomahawk is a tool in geometry for angle trisection, the problem of splitting an angle into three equal parts. The boundaries of its shape include a semicircle and two line segments, arranged in a way that resembles a tomahawk, a Native American axe. The same tool has also been called the shoemaker's knife, but that name is more commonly used in geometry to refer to a different shape, the arbelos (a curvilinear triangle bounded by three mutually tangent semicircles).\n\nThe basic shape of a tomahawk consists of a semicircle (the \"blade\" of the tomahawk), with a line segment the length of the radius extending along the same line as the diameter of the semicircle (the tip of which is the \"spike\" of the tomahawk), and with another line segment of arbitrary length (the \"handle\" of the tomahawk) perpendicular to the diameter. In order to make it into a physical tool, its handle and spike may be thickened, as long as the line segment along the handle continues to be part of the boundary of the shape. Unlike a related trisection using a carpenter's square, the other side of the thickened handle does not need to be made parallel to this line segment.\n\nIn some sources a full circle rather than a semicircle is used, or the tomahawk is also thickened along the diameter of its semicircle, but these modifications make no difference to the action of the tomahawk as a trisector.\n\nTo use the tomahawk to trisect an angle, it is placed with its handle line touching the apex of the angle, with the blade inside the angle, tangent to one of the two rays forming the angle, and with the spike touching the other ray of the angle. One of the two trisecting lines then lies on the handle segment, and the other passes through the center point of the semicircle. If the angle to be trisected is too sharp relative to the length of the tomahawk's handle, it may not be possible to fit the tomahawk into the angle in this way, but this difficulty may be worked around by repeatedly doubling the angle until it is large enough for the tomahawk to trisect it, and then repeatedly bisecting the trisected angle the same number of times as the original angle was doubled.\n\nIf the apex of the angle is labeled \"A\", the point of tangency of the blade is \"B\", the center of the semicircle is \"C\", the top of the handle is \"D\", and the spike is \"E\", then triangles \"ACD\" and \"ADE\" are both right triangles with a shared base and equal height, so they are congruent triangles. Because the sides \"AB\" and \"BC\" of triangle \"ABC\" are respectively a tangent and a radius of the semicircle, they are at right angles to each other and \"ABC\" is also a right triangle; it has the same hypotenuse as \"ACD\" and the same side lengths \"BC\" = \"CD\", so again it is congruent to the other two triangles, showing that the three angles formed at the apex are equal.\n\nAlthough the tomahawk may itself be constructed using a compass and straightedge, and may be used to trisect an angle, it does not contradict Pierre Wantzel's 1837 theorem that arbitrary angles cannot be trisected by compass and unmarked straightedge alone. The reason for this is that placing the constructed tomahawk into the required position is a form of neusis that is not allowed in compass and straightedge constructions.\n\nThe inventor of the tomahawk is unknown, but the earliest references to it come from 19th-century France. It dates back at least as far as 1835, when it appeared in a book by Claude Lucien Bergery, \"Géométrie appliquée à l'industrie, à l'usage des artistes et des ouvriers\" (3rd edition). Another early publication of the same trisection was made by Henri Brocard in 1877; Brocard in turn attributes its invention to an 1863 memoir by French naval officer Pierre-Joseph Glotin.\n\n"}
{"id": "5803388", "url": "https://en.wikipedia.org/wiki?curid=5803388", "title": "Tractor bundle", "text": "Tractor bundle\n\nIn conformal geometry, the tractor bundle is a particular vector bundle constructed on a conformal manifold whose fibres form an effective representation of the conformal group (see associated bundle). \n\nThe term \"tractor\" is a portmanteau of \"Tracy Thomas\" and \"twistor\", the bundle having been introduced first by T. Y. Thomas as an alternative formulation of the Cartan conformal connection, and later rediscovered within the formalism of local twistors and generalized to projective connections by Michael Eastwood \"et al.\" in \n"}
