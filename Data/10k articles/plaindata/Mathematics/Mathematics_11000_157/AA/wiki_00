{"id": "55464594", "url": "https://en.wikipedia.org/wiki?curid=55464594", "title": "ABACABA pattern", "text": "ABACABA pattern\n\nThe ABACABA pattern is a recursive fractal pattern that shows up in many places in the real world (such as in geometry, art, music, poetry, number systems, literature and higher dimensions). Patterns often show a DABACABA type subset.\n\nIn order to generate the next sequence, first take the previous pattern, add the next letter from the alphabet, and then repeat the previous pattern. The first few steps are listed here.\n\n\n"}
{"id": "32649081", "url": "https://en.wikipedia.org/wiki?curid=32649081", "title": "Askey scheme", "text": "Askey scheme\n\nIn mathematics, the Askey scheme is a way of organizing orthogonal polynomials of hypergeometric or basic hypergeometric type into a hierarchy. For the classical orthogonal polynomials discussed in , the Askey scheme was first drawn by and by , and has since been extended by and to cover basic orthogonal polynomials.\n\n give the following version of the Askey scheme:\n\n\n give the following scheme for basic hypergeometric orthogonal polynomials:\n\n"}
{"id": "140592", "url": "https://en.wikipedia.org/wiki?curid=140592", "title": "Assignment problem", "text": "Assignment problem\n\nThe assignment problem is one of the fundamental combinatorial optimization problems in the branch of optimization or operations research in mathematics. It consists of finding a maximum weight matching (or minimum weight perfect matching) in a weighted bipartite graph.\n\nIn its most general form, the problem is as follows:\n\nIf the numbers of agents and tasks are equal and the total cost of the assignment for all tasks is equal to the sum of the costs for each agent (or the sum of the costs for each task, which is the same thing in this case), then the problem is called the \"linear assignment problem\". Commonly, when speaking of the \"assignment problem\" without any additional qualification, then the \"linear assignment problem\" is meant.\n\nThe Hungarian algorithm is one of many algorithms that have been devised that solve the linear assignment problem within time bounded by a polynomial expression of the number of agents. Other algorithms include adaptations of the primal simplex algorithm, and the auction algorithm.\n\nThe assignment problem is a special case of the transportation problem, which is a special case of the minimum cost flow problem, which in turn is a special case of a linear program. While it is possible to solve any of these problems using the simplex algorithm, each specialization has more efficient algorithms designed to take advantage of its special structure.\n\nWhen a number of agents and tasks is very large, a parallel algorithm with randomization can be applied. The problem of finding minimum weight maximum matching can be converted to finding a minimum weight perfect matching. A bipartite graph can be extended to a complete bipartite graph by adding artificial edges with large weights. These weights should exceed the weights of all existing matchings to prevent appearance of artificial edges in the possible solution. As shown by , the problem of minimum weight perfect matching is converted to finding minors in the adjacency matrix of a graph. Using the isolation lemma, a minimum weight perfect matching in a graph can be found with probability at least ½. For a graph with n vertices, it requires formula_1 time.\n\nSuppose that a taxi firm has three taxis (the agents) available, and three customers (the tasks) wishing to be picked up as soon as possible. The firm prides itself on speedy pickups, so for each taxi the \"cost\" of picking up a particular customer will depend on the time taken for the taxi to reach the pickup point. The solution to the assignment problem will be whichever combination of taxis and customers results in the least total cost.\n\nHowever, the assignment problem can be made rather more flexible than it first appears. In the above example, suppose that there are four taxis available, but still only three customers. Then a fourth dummy task can be invented, perhaps called \"sitting still doing nothing\", with a cost of 0 for the taxi assigned to it. The assignment problem can then be solved in the usual way and still give the best solution to the problem.\n\nSimilar adjustments can be done in order to allow more tasks than agents, tasks to which multiple agents must be assigned (for instance, a group of more customers than will fit in one taxi), or maximizing profit rather than minimizing cost.\n\nThe formal definition of the assignment problem (or linear assignment problem) is\n\nis minimized.\n\nUsually the weight function is viewed as a square real-valued matrix \"C\", so that the cost function is written down as:\n\nThe problem is \"linear\" because the cost function to be optimized as well as all the constraints contain only linear terms.\n\nThe problem can be expressed as a standard linear program with the objective function\n\nsubject to the constraints\n\nThe variable formula_8 represents the assignment of agent formula_9 to task formula_10, taking value 1 if the assignment is done and 0 otherwise. This formulation allows also fractional variable values, but there is always an optimal solution where the variables take integer values. This is because the constraint matrix is totally unimodular. The first constraint requires that every agent is assigned to exactly one task, and the second constraint requires that every task is assigned exactly one agent.\n\n\n"}
{"id": "2872234", "url": "https://en.wikipedia.org/wiki?curid=2872234", "title": "Bird's-eye view", "text": "Bird's-eye view\n\nA bird's-eye view is an elevated view of an object from above, with a perspective as though the observer were a bird, often used in the making of blueprints, floor plans, and maps.\n\nIt can be an aerial photograph, but also a drawing. Before manned flight was common, the term \"bird's eye\" was used to distinguish views drawn from direct observation at high locations (for example a mountain or tower), from those constructed from an imagined (bird's) perspectives. Bird's eye views as a genre have existed since classical times. The last great flourishing of them was in the mid-to-late 19th century, when bird's eye view prints were popular in the United States and Europe.\n\nThe terms aerial view and aerial viewpoint are also sometimes used synonymous with bird's-eye view. The term \"aerial view\" can refer to any view from a great height, even at a wide angle, as for example when looking sideways from an airplane window or from a mountain top. Overhead view is fairly synonymous with \"bird's-eye view\" but tends to imply a less lofty vantage point than the latter term. For example, in computer and video games, an \"overhead view\" of a character or situation often places the vantage point only a few feet (a meter or two) above human height. See top-down perspective.\n\nRecent technological and networking developments have made satellite images more accessible. Microsoft Bing Maps offers direct overhead satellite photos of the entire planet but also offers a feature named Bird's eye view in some locations. The \"Bird's Eye\" photos are angled at 40 degrees rather than being straight down. Satellite imaging programs and photos have been described as offering a viewer the opportunity to \"fly over\" and observe the world from this specific angle.\n\nIn filmmaking and video production, a bird's-eye shot refers to a shot looking directly down on the subject. The perspective is very foreshortened, making the subject appear short and squat. This shot can be used to give an overall establishing shot of a scene, or to emphasise the smallness or insignificance of the subjects. These shots are normally used for battle scenes or establishing where the character is. It is shot by lifting the camera up by hands or by hanging it off something strong enough to support it. When a scene needs a large area shot, it is a crane shot.\n\nA distinction is sometimes drawn between a bird's-eye view and a bird's-flight view, or \"view-plan in isometrical projection\". Whereas a bird's-eye view shows a scene from a single viewpoint (real or imagined) in true perspective, including, for example, the foreshortening of more distant features, a bird's-flight view combines a vertical plan of ground-level features with perspective views of buildings and other standing features, all presented at roughly the same scale. The landscape appears \"as it would unfold itself to any one passing over it, as in a balloon, at a height sufficient to abolish sharpness of perspective, and yet low enough to allow of distinct view of the scene beneath\". The technique was popular among local surveyors and cartographers of the sixteenth and early seventeenth centuries.\n\n"}
{"id": "11231812", "url": "https://en.wikipedia.org/wiki?curid=11231812", "title": "Category of modules", "text": "Category of modules\n\nIn algebra, given a ring \"R\", the category of left modules over \"R\" is the category whose objects are all left modules over \"R\" and whose morphisms are all module homomorphisms between left \"R\"-modules. For example, when \"R\" is the ring of integers Z, it is the same thing as the category of abelian groups. The category of right modules is defined in a similar way.\n\nNote: Some authors use the term module category for the category of modules; this term can be ambiguous since it could also refer to a category with a monoidal-category action.\n\nThe category of left modules (or that of right modules) is an abelian category. The category has enough projectives and enough injectives. Mitchell's embedding theorem states every abelian category arises as a full subcategory of the category of modules.\n\nProjective limits and inductive limits exist in the category of (say left) modules.\n\nOver a commutative ring, together with the tensor product of modules ⊗, the category of modules is a symmetric monoidal category.\n\nThe category K-Vect (some authors use Vect) has all vector spaces over a fixed field \"K\" as objects and \"K\"-linear transformations as morphisms. Since vector spaces over \"K\" (as a field) are the same thing as modules over the ring \"K\", K-Vect is a special case of R-Mod, the category of left \"R\"-modules.\n\nMuch of linear algebra concerns the description of K-Vect. For example, the dimension theorem for vector spaces says that the isomorphism classes in K-Vect correspond exactly to the cardinal numbers, and that K-Vect is equivalent to the subcategory of K-Vect which has as its objects the free vector spaces \"K\", where \"n\" is any cardinal number.\n\nThe category of sheaves of modules over a ringed space also has enough injectives (though not always enough projectives).\n\n\n\n"}
{"id": "2610129", "url": "https://en.wikipedia.org/wiki?curid=2610129", "title": "Chainstore paradox", "text": "Chainstore paradox\n\nThe chainstore paradox is an apparent game theory paradox involving the chain store game, where a \"deterrence strategy\" appears optimal instead of the backward induction strategy of standard game theory reasoning.\n\nA monopolist (Player A) has branches in 20 towns. He faces 20 potential competitors, one in each town, who will be able to choose or . They do so in sequential order and one at a time. If a potential competitor chooses , he receives a payoff of 1, while A receives a payoff of 5. If he chooses , he will receive a payoff of either 2 or 0, depending on the response of Player A to his action. Player A, in response to a choice of , must choose one of two pricing strategies, or . If he chooses , both player A and the competitor receive a payoff of 2, and if A chooses , each player receives a payoff of 0.\n\nThese outcomes lead to two theories for the game, the induction (game theoretically correct version) and the deterrence theory (weakly dominated theory):\n\nConsider the decision to be made by the 20th and final competitor, of whether to choose or . He knows that if he chooses , Player A receives a higher payoff from choosing cooperate than aggressive, and being the last period of the game, there are no longer any future competitors whom Player A needs to intimidate from the market. Knowing this, the 20th competitor enters the market, and Player A will cooperate (receiving a payoff of 2 instead of 0).\n\nThe outcome in the final period is set in stone, so to speak. Now consider period 19, and the potential competitor's decision. He knows that A will cooperate in the next period, regardless of what happens in period 19. Thus, if player 19 enters, an aggressive strategy will be unable to deter player 20 from entering. Player 19 knows this and chooses . Player A chooses .\n\nOf course, this process of backward induction holds all the way back to the first competitor. Each potential competitor chooses , and Player A always cooperates. A receives a payoff of 40 (2×20) and each competitor receives 2.\n\nThis theory states that Player A will be able to get payoff of higher than 40. Suppose Player A finds the induction argument convincing. He will decide how many periods at the end to play such a strategy, say 3. In periods 1–17, he will decide to always be aggressive against the choice of IN. If all of the potential competitors know this, it is unlikely potential competitors 1–17 will bother the chain store, thus risking the safe payout of 1 (\"A\" will not retaliate if they choose \"\"). If a few do test the chain store early in the game, and see that they are greeted with the aggressive strategy, the rest of the competitors are likely not to test any further. Assuming all 17 are deterred, Player A receives 91 (17×5 + 2×3). Even if as many as 10 competitors enter and test Player A's will, Player A will still receive a payoff of 41 (10×0+ 7×5 + 3×2), which is better than the induction (game theoretically correct) payoff.\n\nIf Player A follows the game theory payoff matrix to achieve the optimal payoff, they will have a lower payoff than with the \"deterrence\" strategy. This creates an apparent game theory paradox: game theory states that induction strategy should be optimal, but it looks like \"deterrence strategy\" is optimal instead.\n\nThe \"deterrence strategy\" is not a Subgame perfect equilibrium: It relies on the non-credible threat of responding to with . A rational player will not carry out a non-credible threat, but the paradox is that it nevertheless seems to benefit Player A to carry out the threat.\n\nReinhard Selten's response to this apparent paradox is to argue that the idea of \"deterrence\", while irrational by the standards of Game Theory, is in fact an acceptable idea by the rationality that individuals actually employ. Selten argues that individuals can make decisions of three levels: Routine, Imagination, and Reasoning.\n\nGame theory is based on the idea that each matrix is modeled with the assumption of complete information: that \"every player knows the payoffs and strategies available to other players,\" where the word \"payoff\" is descriptive of behavior—what the player is trying to maximize. If, in the first town, the competitor enters and the monopolist is aggressive, the second competitor has observed that the monopolist is not, from the standpoint of common knowledge of payoffs and strategies, maximizing the assumed payoffs; expecting the monopolist to do so in this town seems dubious.\n\nIf competitors place even a very small probability on the possibility that the monopolist is spiteful, and places intrinsic value on being (or appearing) aggressive, and the monopolist knows this, then even if the monopolist has payoffs as described above, responding to entry in an early town with aggression will be optimal if it increases the probability that later competitors place on the monopolist's being spiteful.\n\nThe individuals use their past experience of the results of decisions to guide their response to choices in the present. \"The underlying criteria of similarity between decision situations are crude and sometimes inadequate\". (Selten)\n\nThe individual tries to visualize how the selection of different alternatives may influence the probable course of future events. This level employs the routine level within the procedural decisions. This method is similar to a computer simulation.\n\nThe individual makes a conscious effort to analyze the situation in a rational way, using both past experience and logical thinking. This mode of decision uses simplified models whose assumptions are products of imagination, and is the only method of reasoning permitted and expected by game theory.\n\nOne chooses which method (routine, imagination or reasoning) to use for the problem, and this decision itself is made on the routine level.\n\nDepending on which level is selected, the individual begins the decision procedure. The individual then arrives at a (possibly different) decision for each level available (if we have chosen imagination, we would arrive at a routine decision and possible and imagination decision). Selten argues that individuals can always reach a routine decision, but perhaps not the higher levels. Once the individuals have all their levels of decision, they can decide which answer to use...the Final Decision. The final decision is made on the routine level and governs actual behavior.\n\nDecision effort is a scarce commodity, being both time consuming and mentally taxing. Reasoning is more costly than Imagination, which, in turn is more costly than Routine. The highest level activated is not always the most accurate since the individual may be able to reach a good decision on the routine level, but makes serious computational mistakes on higher levels, especially Reasoning.\n\nSelten finally argues that strategic decisions, like those made by the monopolist in the chainstore paradox, are generally made on the level of Imagination, where deterrence is a reality, due to the complexity of Reasoning, and the great inferiority of Routine (it does not allow the individual to see herself in the other player's position). Since Imagination cannot be used to visualize more than a few stages of an extensive form game (like the Chain-store game) individuals break down games into \"the beginning\" and \"towards the end\". Here, deterrence is a reality, since it is reasonable \"in the beginning\", yet is not convincing \"towards the end\".\n\n\n\n"}
{"id": "5143496", "url": "https://en.wikipedia.org/wiki?curid=5143496", "title": "Clobber", "text": "Clobber\n\nClobber is an abstract strategy game invented in 2001 by combinatorial game theorists Michael H. Albert, J.P. Grossman and Richard Nowakowski. It has subsequently been studied by Elwyn Berlekamp and Erik Demaine among others. Since 2005, it has been one of the events in the Computer Olympiad.\n\nClobber is best played with two players and takes an average of 15 minutes to play. It is suggested for ages 8 and up. It is typically played on a rectangular white and black checkerboard. Players take turns to move one of their own pieces onto an orthogonally adjacent opposing piece, removing it from the game. The winner of the game is the player who makes the last move (i.e. whose opponent cannot move).\n\nTo start the game, each of the squares on the checkerboard is occupied by a stone. White stones are placed on the white squares and black stones on the black squares. To move, the player must pick up one of his or her own stones and \"clobber\" an opponent's stone on an adjacent square, either horizontally or vertically. Once the opponent's stone is clobbered, it must then be removed from the board and replaced by the stone that was moved. The player who, on their turn, is unable to move, loses the game.\n\nIn computational play (e.g., Computer Olympiad), clobber is generally played on a 10x10 board. There are also variations in the initial layout of the pieces. Another variant is \"Cannibal Clobber\", where a stone may not only capture stones of the opponent but also other stones of its owner. An advantage of Cannibal Clobber over Clobber is that a player may not only win, but win by a non-trivial margin. Cannibal Clobber was proposed in the summer of 2003 by Ingo Althoefer.\n\n"}
{"id": "48662", "url": "https://en.wikipedia.org/wiki?curid=48662", "title": "Computer number format", "text": "Computer number format\n\nA computer number format is the internal representation of numeric values in digital computer and calculator hardware and software. Normally, numeric values are stored as groupings of bits, named for the number of bits that compose them. The encoding between numerical values and bit patterns is chosen for convenience of the operation of the computer; the bit format used by the computer's instruction set generally requires conversion for external use such as printing and display. Different types of processors may have different internal representations of numerical values. Different conventions are used for integer and real numbers. Most calculations are carried out with number formats that fit into a processor register, but some software systems allow representation of arbitrarily large numbers using multiple words of memory.\n\nComputers represent data in sets of binary digits. The representation is composed of bits, which in turn are grouped into larger sets such as bytes.\n\nA \"bit\" is a binary digit that represents one of two states. The concept of a bit can be understood as a value of either \"1\" or \"0\", \"on\" or \"off\", \"yes\" or \"no\", \"true\" or \"false\", or encoded by a switch or toggle of some kind.\n\nWhile a single bit, on its own, is able to represent only two values, a string of bits may be used to represent larger values. For example, a string of three bits can represent up to eight distinct values as illustrated in Table 1.\n\nAs the number of bits composing a string increases, the number of possible \"0\" and \"1\" combinations increases exponentially. While a single bit allows only two value-combinations and two bits combined can make four separate values and so on. The amount of possible combinations doubles with each binary digit added as illustrated in Table 2.\n\nGroupings with a specific number of bits are used to represent varying things and have specific names.\n\nA \"byte\" is a bit string containing the number of bits needed to represent a character. On most modern computers, this is an eight bit string. Because the definition of a byte is related to the number of bits composing a character, some older computers have used a different bit length for their byte. In many computer architectures, the byte is used to address specific areas of memory. For example, even though 64-bit processors may address memory sixty-four bits at a time, they may still split that memory into eight-bit pieces. This is called byte-addressable memory. Historically, many CPUs read data in some multiple of eight bits. Because the byte size of eight bits is so common, but the definition is not standardized, the term octet is sometimes used to explicitly describe an eight bit sequence.\n\nA \"nibble\" (sometimes \"nybble\"), is a number composed of four bits. Being a half-byte, the nibble was named as a play on words. A person may need several nibbles for one bite from something; similarly, a nybble is a part of a byte. Because four bits allow for sixteen values, a nibble is sometimes known as a hexadecimal digit.\n\nOctal and hex are convenient ways to represent binary numbers, as used by computers. Computer engineers often need to write out binary quantities, but in practice writing out a binary number such as 1001001101010001 is tedious and prone to errors. Therefore, binary quantities are written in a base-8, or \"octal\", or, much more commonly, a base-16, \"hexadecimal\" or \"hex\", number format. In the decimal system, there are 10 digits, 0 through 9, which combine to form numbers. In an octal system, there are only 8 digits, 0 through 7. That is, the value of an octal \"10\" is the same as a decimal \"8\", an octal \"20\" is a decimal \"16\", and so on. In a hexadecimal system, there are 16 digits, 0 through 9 followed, by convention, with A through F. That is, a hex \"10\" is the same as a decimal \"16\" and a hex \"20\" is the same as a decimal \"32\". An example and comparison of numbers in different bases is described in the chart below.\n\nWhen typing numbers, formatting characters are used to describe the number system, for example 000_0000B or 0b000_00000 for binary and 0F8H or 0xf8 for hexadecimal numbers.\n\nEach of these number systems are positional systems, but while decimal weights are powers of 10, the octal weights are powers of 8 and the hex weights are powers of 16. To convert from hex or octal to decimal, for each digit one multiplies the value of the digit by the value of its position and then adds the results. For example:\n\nformula_1\n\nformula_2\n\nformula_3\n\nformula_4\n\nformula_5\n\nformula_6\n\nformula_7\n\nformula_8\n\nformula_9\n\nformula_10\n\nFixed-point formatting can be useful to represent fractions in binary.\n\nThe number of bits needed for the precision and range desired must be chosen to store the fractional and integer parts of a number. For instance, using a 32-bit format, 16 bits may be used for the integer and 16 for the fraction.\n\nThe eight's bit is followed by the four's bit, then the two's bit, then the one's bit. The fractional bits continue the pattern set by the integer bits. The next bit is the half's bit, then the quarter's bit, then the ⅛'s bit, and so on. For example:\n\nThis form of encoding cannot represent some values in binary. For example, the fraction formula_11, 0.2 in decimal, the closest approximations would be as follows:\nEven if more digits are used, an exact representation is impossible. The number formula_12, written in decimal as 0.333333333..., continues indefinitely. If prematurely terminated, the value would not represent formula_12 precisely.\n\nWhile both unsigned and signed integers are used in digital systems, even a 32-bit integer is not enough to handle all the range of numbers a calculator can handle, and that's not even including fractions. To approximate the greater range and precision of real numbers, we have to abandon signed integers and fixed-point numbers and go to a \"floating-point\" format.\n\nIn the decimal system, we are familiar with floating-point numbers of the form (scientific notation):\n\nor, more compactly:\n\nwhich means \"1.1030402 times 1 followed by 5 zeroes\". We have a certain numeric value (1.1030402) known as a \"significand\", multiplied by a power of 10 (E5, meaning 10 or 100,000), known as an \"exponent\". \nIf we have a negative exponent, that means the number is multiplied by a 1 that many places to the right of the decimal point. For example:\n\nThe advantage of this scheme is that by using the exponent we can get a much wider range of numbers, even if the number of digits in the significand, or the \"numeric precision\", is much smaller than the range. \nSimilar binary floating-point formats can be defined for computers. There is a number of such schemes, the most popular has been defined by Institute of Electrical and Electronics Engineers (IEEE). The IEEE 754-2008 standard specification defines a 64 bit floating-point format with:\n\n\nLet's see what this format looks like by showing how such a number would be stored in 8 bytes of memory:\n\nwhere \"S\" denotes the sign bit, \"x\" denotes an exponent bit, and \"m\" denotes a significand bit. Once the bits here have been extracted, they are converted with the computation:\n\nThis scheme provides numbers valid out to about 15 decimal digits, with the following range of numbers:\nThe specification also defines several special values that are not defined numbers, and are known as \"NaNs\", for \"Not A Number\". These are used by programs to designate invalid operations and the like. \nSome programs also use 32-bit floating-point numbers. The most common scheme uses a 23-bit significand with a sign bit, plus an 8-bit exponent in \"excess-127\" format, giving seven valid decimal digits.\nThe bits are converted to a numeric value with the computation:\n\nleading to the following range of numbers:\nSuch floating-point numbers are known as \"reals\" or \"floats\" in general, but with a number of variations:\n\nA 32-bit float value is sometimes called a \"real32\" or a \"single\", meaning \"single-precision floating-point value\".\n\nA 64-bit float is sometimes called a \"real64\" or a \"double\", meaning \"double-precision floating-point value\".\n\nThe relation between numbers and bit patterns is chosen for convenience in computer manipulation; eight bytes stored in computer memory may represent a 64-bit real, two 32-bit reals, or four signed or unsigned integers, or some other kind of data that fits into eight bytes. The only difference is how the computer interprets them. If the computer stored four unsigned integers and then read them back from memory as a 64-bit real, it almost always would be a perfectly valid real number, though it would be junk data.\n\nOnly a finite range of real numbers can be represented with a given number of bits. Arithmetic operations can overflow or underflow, producing a value too large or too small to be represented.\n\nThe representation has a limited precision. For example, only 15 decimal digits can be represented with a 64-bit real. If a very small floating-point number is added to a large one, the result is just the large one. The small number was too small to even show up in 15 or 16 digits of resolution, and the computer effectively discards it. Analyzing the effect of limited precision is a well-studied problem. Estimates of the magnitude of round-off errors and methods to limit their effect on large calculations are part of any large computation project. The precision limit is different from the range limit, as it affects the significand, not the exponent.\n\nThe significand is a binary fraction that doesn't necessarily perfectly match a decimal fraction. In many cases a sum of reciprocal powers of 2 does not matches a specific decimal fraction, and the results of computations will be slightly off. For example, the decimal fraction \"0.1\" is equivalent to an infinitely repeating binary fraction: 0.000110011 ...\n\nProgramming in assembly language requires the programmer to keep track of the representation of numbers. Where the processor does not support a required mathematical operation, the programmer must work out a suitable algorithm and instruction sequence to carry out the operation; on some microprocessors, even integer multiplication must be done in software.\n\nHigh-level programming languages such as LISP and Python offer an abstract number that may be an expanded type such as \"rational\", \"bignum\", or \"complex\". Mathematical operations are carried out by library routines provided by the implementation of the language. A given mathematical symbol in the source code, by operator overloading, will invoke different object code appropriate to the representation of the numerical type; mathematical operations on any number—whether signed, unsigned, rational, floating-point, fixed-point, integral, or complex—are written exactly the same way.\n\nSome languages, such as REXX and Java, provide decimal floating points operations, which provide rounding errors of a different form.\n\n"}
{"id": "2229899", "url": "https://en.wikipedia.org/wiki?curid=2229899", "title": "Content validity", "text": "Content validity\n\nIn psychometrics, content validity (also known as logical validity) refers to the extent to which a measure represents all facets of a given construct. For example, a depression scale may lack content validity if it only assesses the affective dimension of depression but fails to take into account the behavioral dimension. An element of subjectivity exists in relation to determining content validity, which requires a degree of agreement about what a particular personality trait such as extraversion represents. A disagreement about a personality trait will prevent the gain of a high content validity.\n\nContent validity is different from face validity, which refers not to what the test actually measures, but to what it superficially appears to measure. Face validity assesses whether the test \"looks valid\" to the examinees who take it, the administrative personnel who decide on its use, and other technically untrained observers. Content validity requires the use of recognized subject matter experts to evaluate whether test items assess defined content and more rigorous statistical tests than does the assessment of face validity. Content validity is most often addressed in academic and vocational testing, where test items need to reflect the knowledge actually required for a given topic area (e.g., history) or job skill (e.g., accounting). In clinical settings, content validity refers to the correspondence between test items and the symptom content of a syndrome.\n\nOne widely used method of measuring content validity was developed by C. H. Lawshe. It is essentially a method for gauging agreement among raters or judges regarding how essential a particular item is. Lawshe (1975) proposed that each of the subject matter expert raters (SMEs) on the judging panel respond to the following question for each item: \"Is the skill or knowledge measured by this item 'essential,' 'useful, but not essential,' or 'not necessary' to the performance of the construct?\" According to Lawshe, if more than half the panelists indicate that an item is essential, that item has at least some content validity. Greater levels of content validity exist as larger numbers of panelists agree that a particular item is essential. Using these assumptions, Lawshe developed a formula termed the content validity ratio:\nformula_1\nwhere formula_2 content validity ratio, formula_3 number of SME panelists indicating \"essential\", formula_4 total number of SME panelists. This formula yields values which range from +1 to -1; positive values indicate that at least half the SMEs rated the item as essential. The mean CVR across items may be used as an indicator of overall test content validity.\n\nLawshe (1975) provided a table of critical values for the CVR by which a test evaluator could determine, for a pool of SMEs of a given size, the size of a calculated CVR necessary to exceed chance expectation. This table had been calculated for Lawshe by his friend, Lowell Schipper. Close examination of this published table revealed an anomaly. In Schipper's table, the critical value for the CVR increases monotonically from the case of 40 SMEs (minimum value = .29) to the case of 9 SMEs (minimum value = .78) only to unexpectedly drop at the case of 8 SMEs (minimum value = .75) before hitting its ceiling value at the case of 7 SMEs (minimum value = .99). However, it is important to understand when applying the formula to 8 raters, the result from 7 Essential and 1 other rating yields a CVR of .75. If .75 was not the critical value, then you would need 8 of 8 raters of Essential that would yield a CVR of 1.00. In that case, to be consistent with the ascending order of CVRs the value for 8 raters would have to be 1.00. That would violate the same principle because you would have the \"perfect\" value required for 8 raters, but not for ratings at other numbers of raters at either higher or lower than 8 raters. Whether this departure from the table's otherwise monotonic progression was due to a calculation error on Schipper's part or an error in typing or type setting is unclear. Wilson, Pan, and Schumsky (2012), seeking to correct the error, found no explanation in Lawshe's writings nor any publications by Schipper describing how the table of critical values was computed. Wilson and colleagues determined that the Schipper values were close approximations to the normal approximation to the binomial distribution. By comparing Schipper's values to the newly calculated binomial values, they also found that Lawshe and Schipper had erroneously labeled their published table as representing a one-tailed test when in fact the values mirrored the binomial values for a two-tailed test. Wilson and colleagues published a recalculation of critical values for the content validity ratio providing critical values in unit steps at multiple alpha levels.\n\nThe table of values is the following one:\n\nN° of panelists Min. Value\n\n\n\n"}
{"id": "36017008", "url": "https://en.wikipedia.org/wiki?curid=36017008", "title": "Deletion channel", "text": "Deletion channel\n\nA deletion channel is a communications channel model used in coding theory and information theory. In this model, a transmitter sends a bit (a zero or a one), and the receiver either receives the bit (with probability formula_1) or does not receive anything without being notified that the bit was dropped (with probability formula_2). Determining the capacity of the deletion channel is an open problem.\n\nThe deletion channel should not be confused with the binary erasure channel which is much simpler to analyze.\n\nLet formula_1 be the deletion probability, formula_4. The iid binary deletion channel is defined as follows:\n\nGiven a input sequence of formula_5 bits formula_6 as input, each bit in formula_7 can be deleted with probability formula_1. The deletion positions are unknown to the sender and the receiver. The output sequence formula_9 is the sequence of the formula_6 which were not deleted, in the correct order and with no errors.\n\nThe capacity of the binary deletion channel (as an analytical expression of the deletion rate formula_1) is unknown. It has a mathematical expression. Several upper and lower bounds are known.\n\n"}
{"id": "25412108", "url": "https://en.wikipedia.org/wiki?curid=25412108", "title": "Differential invariant", "text": "Differential invariant\n\nIn mathematics, a differential invariant is an invariant for the action of a Lie group on a space that involves the derivatives of graphs of functions in the space. Differential invariants are fundamental in projective differential geometry, and the curvature is often studied from this point of view. Differential invariants were introduced in special cases by Sophus Lie in the early 1880s and studied by Georges Henri Halphen at the same time. was the first general work on differential invariants, and established the relationship between differential invariants, invariant differential equations, and invariant differential operators.\n\nDifferential invariants are contrasted with geometric invariants. Whereas differential invariants can involve a distinguished choice of independent variables (or a parameterization), geometric invariants do not. Élie Cartan's method of moving frames is a refinement that, while less general than Lie's methods of differential invariants, always yields invariants of the geometrical kind.\n\nThe simplest case is for differential invariants for one independent variable \"x\" and one dependent variable \"y\". Let \"G\" be a Lie group acting on R. Then \"G\" also acts, locally, on the space of all graphs of the form \"y\" = \"ƒ\"(\"x\"). Roughly speaking, a \"k\"-th order differential invariant is a function\ndepending on \"y\" and its first \"k\" derivatives with respect to \"x\", that is invariant under the action of the group.\n\nThe group can act on the higher-order derivatives in a nontrivial manner that requires computing the \"prolongation\" of the group action. The action of \"G\" on the first derivative, for instance, is such that the chain rule continues to hold: if\nthen\nSimilar considerations apply for the computation of higher prolongations. This method of computing the prolongation is impractical, however, and it is much simpler to work infinitesimally at the level of Lie algebras and the Lie derivative along the \"G\" action.\n\nMore generally, differential invariants can be considered for mappings from any smooth manifold \"X\" into another smooth manifold \"Y\" for a Lie group acting on the Cartesian product \"X\"×\"Y\". The graph of a mapping \"X\" → \"Y\" is a submanifold of \"X\"×\"Y\" that is everywhere transverse to the fibers over \"X\". The group \"G\" acts, locally, on the space of such graphs, and induces an action on the \"k\"-th prolongation \"Y\" consisting of graphs passing through each point modulo the relation of \"k\"-th order contact. A differential invariant is a function on \"Y\" that is invariant under the prolongation of the group action.\n\n\n\n\n"}
{"id": "39950449", "url": "https://en.wikipedia.org/wiki?curid=39950449", "title": "Ed Scheinerman", "text": "Ed Scheinerman\n\nEdward R. Scheinerman is an American mathematician, specializing in graph theory and order theory. He is a professor of applied mathematics, statistics, and computer science at Johns Hopkins University. His contributions to mathematics include Scheinerman's conjecture, now proven, stating that every planar graph may be represented as an intersection graph of line segments.\n\nScheinerman did his undergraduate studies at Brown University, graduating in 1980, and earned his Ph.D. in 1984 from Princeton University under the supervision of Douglas B. West. He joined the Johns Hopkins faculty in 1984, and since 2000 he has been an administrator there, serving as department chair, associate dean, and (since 2008) vice dean for education.\n\nHe is a two-time winner of the Mathematical Association of America's Lester R. Ford Award for expository writing, in 1991 for his paper \"Random intervals\" with Joyce Justicz and Peter Winkler, and in 2001 for his paper \"When Close is Close Enough\". In 1992 he became a fellow of the Institute of Combinatorics and its Applications, and in 2012 he became a fellow of the American Mathematical Society.\n\n\n"}
{"id": "17519721", "url": "https://en.wikipedia.org/wiki?curid=17519721", "title": "Entropic vector", "text": "Entropic vector\n\nThe entropic vector or entropic function is a concept arising in information theory. Shannon's information entropy measures and their associated identities and inequalities (both constrained and unconstrained) have received a lot of attention over the past from the time Shannon introduced his concept of Information Entropy. A lot of inequalities and identities have been found and are available in standard Information Theory texts. But recent researchers have laid focus on trying to find all possible identities and inequalities (both constrained and unconstrained) on such entropies and characterize them. Entropic vector lays down the basic framework for such a study.\n\nLet formula_1 be random variables, with formula_2\n\nA vector \"h\" in formula_3 is an entropic vector of order formula_4 if and only if there exists a tuple formula_5 with associated vector formula_6 defined by formula_7 where formula_8 y formula_9. The set of all entropic vectors of order formula_4 is denoted by formula_11\n\nAll the properties of entropic functions can be transposed to entropic vectors:\n\nformula_12 is continuous\n\nGiven a deterministic random variable formula_13, we have formula_14\n\nGiven formula_15, there exists a random variable formula_13 such that formula_17\n\nGiven formula_18 a probability distribution on formula_19, we have formula_20\n\nLet \"X\",\"Y\" be two independent random variables with discrete uniform distribution over the set formula_21. Then \n\nIt follows that\n\nformula_23\n\nThe entropic vector is thus :\n\nformula_24\n\nThe entropy satisfies the properties\n\nThe Shannon inequality is\n\nThe entropy vector that satisfies the linear combination of this region is called formula_28.\nThe region formula_11 has been studied recently, the cases for \"n\" = 1, 2, 3\n\nIt is difficult harder con the case formula_32, the number of inequalities given by monotone and submodularity properties increase when we increase \"n\", however the relationship among entropic vectors, polymatroids, are an object of study for the information theory and there are other ways to characterize those relationships mentioned\n\nThe most important results for the characterization of formula_11 is not precisely about these set, but its topological clousure i.e. the set formula_34, which says that formula_34 is a convex cone, other interesting characterization is that formula_36 (formula_28 is the set of vectors that satisfy Shannon-type inequalities) for formula_38, in other words the set of entropy vector is completely characterized by Sahnnon's Inequalities, for the case \"n\" = 4 fails this property, particularly by the Ingleton's inequality.\n\nIn 1998 Zhang and Yeung proved a new non-Shannon's inequality\nand in 2007 Matus proved that formula_43 is not\npolihedral.\n\nOne way to charactize formula_11 is by looking at some special distributions.\\\\\nDefinition: A group characterizable vector h is also denoted to beformula_45\n\nsuch that there exists a group formula_46 and subgroups formula_47 and for formula_48\n\nif formula_50 is not formula_51 and 0 otherwise. formula_52 .\n\nDefinition: formula_53 is the set of all group charactizable vectors is formula_54, and we can describe better the set formula_55\n\nTheorem: formula_56\n\nGiven a vector formula_57, is it possible to say if there exists formula_54 random variables such that their joint entropies are given by formula_59? It turns out that for formula_60 the problem has been solved. But for formula_61, it still remains unsolved. Defining the set of all such vectors formula_57 that can be constructed from a set of formula_4 random variables as formula_64, we see that a complete characterization of this space remains an unsolved mystery.\n\n"}
{"id": "34615237", "url": "https://en.wikipedia.org/wiki?curid=34615237", "title": "Equivalence (formal languages)", "text": "Equivalence (formal languages)\n\nIn formal language theory, weak equivalence of two grammars means they generate the same set of strings, i.e. that the formal language they generate is the same. In compiler theory the notion is distinguished from strong (or structural) equivalence, which additionally means that the two parse trees are reasonably similar in that the same semantic interpretation can be assigned to both.\n\nVijay-Shanker and Weir (1994) demonstrates that Linear Indexed Grammars, Combinatory Categorial Grammars, Tree-adjoining Grammars, and Head Grammars are weakly equivalent formalisms, in that they all define the same string languages.\n\nOn the other hand, if the two grammars generate the same set of derivation trees (or more generally, the same set of abstract syntactic objects), then the two languages are strongly equivalent. Chomsky (1963) introduces the notion of strong equivalence, and argues that only strong equivalence is relevant when comparing grammar formalisms. Kornai and Pullum (1990) and Miller (1994) offer a refined notion of strong equivalence that allows for isomorphic relationships between the syntactic analyses given by different formalisms. Yoshinaga, Miyao, and Tsujii (2002) offers a proof of the strong equivalency of the LTAG and HPSG formalisms.\n\nAs an example, consider the following two context-free grammars, given in Backus-Naur form:\n\nBoth grammars generate the same set of strings, viz. the set of all arithmetical expressions that can be built from the variables \"x\", \"y\", \"z\", the constants \"1\", \"2\", \"3\", the operators \"+\", \"-\", \"*\", \"/\", and parantheses \"(\" and \")\".\nHowever, a concrete syntax tree of the second grammar always reflects the usual order of operations, while a tree from the first grammar need not.\n\nFor the example string \"1+2*3\", the right part of the picture shows its unique parse tree with the second grammar; evaluating this tree in postfix order will yield the proper value, 7.\nIn contrast, the left picture part shows one of the parse trees for that string with the first grammar; evaluating it in postfix order will yield 9.\n\nSince the second grammar cannot generate a tree corresponding to the left picture part, while the first grammar can, both grammars are not strongly equivalent.\n\nIn linguistics, the weak generative capacity of a grammar is defined as the set of all strings generated by it, while a grammar's strong generative capacity refers to the set of \"structural descriptions\" generated by it.\nAs a consequence, two grammars are considered weakly equivalent if their weak generative capacities coincide; similar for strong equivalence.\nThe notion of \"generative capacity\" was introduced by Noam Chomsky in 1963.\n"}
{"id": "7316682", "url": "https://en.wikipedia.org/wiki?curid=7316682", "title": "Fitch notation", "text": "Fitch notation\n\nFitch notation, also known as Fitch diagrams (named after Frederic Fitch), is a notational system for constructing formal proofs used in sentential logics and predicate logics. Fitch-style proofs arrange the sequence of sentences that make up the proof into rows. A unique feature of Fitch notation is that the degree of indentation of each row conveys which assumptions are active for that step.\n\nEach row in a Fitch-style proof is either:\n\nIntroducing a new assumption increases the level of indentation, and begins a new vertical \"scope\" bar that continues to indent subsequent lines until the assumption is discharged. This mechanism immediately conveys which assumptions are active for any given line in the proof, without the assumptions needing to be rewriten on every line (as with sequent-style proofs).\n\nThe following example displays the main features of Fitch notation:\n0. The null assumption, \"i.e.\", we are proving a tautology\n1. Our first subproof: we assume the l.h.s. to show the r.h.s. follows\n2. A subsubproof: we are free to assume what we want. Here we aim for a reductio ad absurdum\n3. We now have a contradiction\n4. We are allowed to prefix the statement that \"caused\" the contradiction with a not\n5. Our second subproof: we assume the r.h.s. to show the l.h.s. follows\n6. We invoke the rule that allows us to remove an even number of nots from a statement prefix\n7. From 1 to 4 we have shown if P then not not P, from 5 to 6 we have shown P if not not P; hence we are allowed to introduce the biconditional\n\n\n\n"}
{"id": "11343398", "url": "https://en.wikipedia.org/wiki?curid=11343398", "title": "Forward anonymity", "text": "Forward anonymity\n\nForward anonymity, analogous to forward secrecy, in computer security and cryptography is the property which prevents an attacker who has recorded past communications from discovering the identities of the participants, even after the fact.\n\nWhen speaking of forward secrecy, system designers attempt to prevent an attacker who has recorded past communications from discovering the contents of said communications later on. One example of a system which satisfies the perfect forward secrecy property is one in which a compromise of one key by an attacker (and consequent decryption of messages encrypted with that key) does not undermine the security of previously used keys. Forward secrecy does not refer to protecting the content of the message, but rather to the protection of keys used to decrypt messages.\n\nOne example of a system which uses forward anonymity is a public key cryptography system, where the public key is well-known and used to encrypt a message, and an unknown private key is used to decrypt it. In this system, one of the keys is always said to be compromised, but messages and their participants are still unknown by anyone without the corresponding private key.\n\nOriginally introduced by Whitfield Diffie, Paul van Oorschot, and Michael James Wiener to describe a property of STS (station-to-station protocol) involving a long term secret, either a private key or a shared password.\n\nPublic Key Cryptography is a common form of a forward anonymous system. It is used to pass encrypted messages, preventing any information about the message from being discovered if the message is intercepted by an attacker. It uses two keys, a public key and a private key. The public key is published, and is used by anyone to encrypt a plaintext message. The Private key is not well known, and is used to decrypt cyphertext. Public key cryptography is known as an asymmetric decryption algorithm because of different keys being used to perform opposing functions. Public key cryptography is popular because, while it is computationally easy to create a pair of keys, it is extremely difficult to determine the private key knowing only the public key. Therefore, the public key being well known does not allow messages which are intercepted to be decrypted. This is a forward anonymous system because one compromised key (the public key) does not compromise the anonymity of the system.\n\nA variation of the public key cryptography system is a Web of trust, where each user has both a public and private key. Messages sent are encrypted using the intended recipient's public key, and only this recipient's private key will decrypt the message. They are also signed with the senders private key. This creates added security where it becomes more difficult for an attacker to pretend to be a user, as the lack of a private key signature indicates a non-trusted user.\n\nA forward anonymous system does not necessarily mean a wholly secure system. A successful cryptanalysis of a message or sequence of messages can still decode the information without the use of a private key or long term secret.\n\nForward anonymity, along with other cryptography related properties, received a burst of media attention after the leak of classified information by Edward Snowden, beginning in June, 2013, which indicated that the NSA and FBI had practices of asking companies to leave in back doors for them, allowing the companies and agencies to decrypt information stored on phones and other devices more easily, with the intention of allowing them to more easily find and arrest various criminals, while occasionally mistakenly targeting innocent civilians. They especially publicized the aid this practice provided in catching predatory pedophiles. Opponents to this practice argue that leaving in a back door to law enforcement increases the risk of attackers being able to decrypt information, as well as questioning its legality under the US Constitution, specifically being a form of illegal Search and Seizure.\n"}
{"id": "15495829", "url": "https://en.wikipedia.org/wiki?curid=15495829", "title": "Fractal in soil mechanics", "text": "Fractal in soil mechanics\n\nThe fractal approach to soil mechanics is a new line of thought. It was first raised in \"Fractal Character Of Grain-Size Distribution Of Expansion Soils\" by Yongfu Xu and Songyu Liu, published in 1999, by Fractals. There are several problems in soil mechanics which can be dealt by applying a fractal approach. One of these problems is the determination of soil-water-characteristic curve (also called (water retention curve) and/or capillary pressure curve). It is a time-consuming process considering usual laboratory experiments. Many scientists have been involved in making mathematical models of soil-water-characteristic curve (SWCC) in which constants are related to the fractal dimension of pore size distribution or particle size distribution of the soil. After the great mathematician Benoît Mandelbrot—father of fractal mathematics—showed the world fractals, Scientists of Agronomy, Agricultural engineering and Earth Scientists have developed more fractal-based models. \nIt is noteworthy that all of these models have been used to extract hydraulic properties of soils and the potential capabilities of fractal mathematics to investigate mechanical properties of soils. Therefore, it is really important to use such physically based models to promote our understanding of the mechanics of the soils. It can be of great help for researchers in the area of unsaturated soil mechanics. Mechanical parameters can also be driven from such models and of course it needs further works and researches.\n"}
{"id": "4595591", "url": "https://en.wikipedia.org/wiki?curid=4595591", "title": "Gardner–Salinas braille codes", "text": "Gardner–Salinas braille codes\n\nThe Gardner–Salinas braille codes are a method of encoding mathematical and scientific notation linearly using braille cells for tactile reading by the visually impaired. The most common form of Gardner–Salinas braille is the 8-cell variety, commonly called \"GS8\". There is also a corresponding 6-cell form called \"GS6\".\n\nThe codes were developed as a replacement for Nemeth Braille by John A. Gardner, a physicist at Oregon State University, and Norberto Salinas, an Argentinian mathematician.\n\nThe Gardner–Salinas braille codes are an example of a compact human-readable markup language. The syntax is based on the LaTeX system for scientific typesetting.\n\nThe set of lower-case letters, the period, comma, semicolon, colon, exclamation mark, apostrophe, and opening and closing double quotes are the same as in Grade-2 English Braille.\n\nApart from 0, this is the same as the Antoine notation used in French and Luxembourgish Braille.\n\nGS8 upper-case letters are indicated by the same cell as standard English braille (and GS8) lower-case letters, with dot #7 added.\n\nCompare Luxembourgish Braille.\n\nDot 8 is added to the letter forms of International Greek Braille to derive Greek letters: \n\n<nowiki>*</nowiki> Encodes the fraction-slash for the single adjacent digits/letters as numerator and denominator.\n\n<nowiki>*</nowiki> Used for any > 1 digit radicand.\n\n<nowiki>**</nowiki> Used for markup to represent inkprint text.\n\n"}
{"id": "20247295", "url": "https://en.wikipedia.org/wiki?curid=20247295", "title": "Garside element", "text": "Garside element\n\nIn mathematics, a Garside element is an element of an algebraic structure such as a monoid that has several desirable properties.\n\nFormally, if \"M\" is a monoid, then an element Δ of \"M\" is said to be a Garside element if the set of all right divisors of Δ,\nis the same set as the set of all left divisors of Δ,\nand this set generates \"M\".\n\nA Garside element is in general not unique: any power of a Garside element is again a Garside element.\n\nA Garside monoid is a monoid with the following properties:\n\nA Garside monoid satisfies the Ore condition for multiplicative sets and hence embeds in its group of fractions: such a group is a Garside group. A Garside group is biautomatic and hence has soluble word problem and conjugacy problem. Examples of such groups include braid groups and, more generally, Artin groups of finite Coxeter type.\n\nThe name was coined by Dehornoy and Paris to mark the work of Frank Arnold Garside (1915-1988), a teacher at Magdalen College School, Oxford who served as Lord Mayor of Oxford in 1984-5, on the conjugacy problem for braid groups.\n\n"}
{"id": "24139370", "url": "https://en.wikipedia.org/wiki?curid=24139370", "title": "Gentleware", "text": "Gentleware\n\nGentleware AG is a software company headquartered in Hamburg, Germany. Gentleware was founded in 2000 with software based in the open source project ArgoUML. The company is best known for Poseidon for UML, the world's most downloaded commercial UML tool, with over 1,200,000 copies distributed to over 100 countries.\n\nSince 2004 Gentleware Ukraine, has been the center of the development of the German company Gentleware AG and provides complete development, support and sales of such products as Poseidon for UML, Apollo for Eclipse, Poseidon for DSLs.\n\n"}
{"id": "26758463", "url": "https://en.wikipedia.org/wiki?curid=26758463", "title": "Gul (design)", "text": "Gul (design)\n\nA gul (also written gol, göl and gül) is a medallion-like design element typical of traditional hand-woven carpets from Central and West Asia. In Turkmen weavings they are often repeated to form the pattern in the main field.\n\nGuls are medallions, often octagonal, and often somewhat angular on a generally octagonal plan, though they can be somewhat rounded within the constraints of carpet-weaving, and some are lozenge-shaped (rhombuses). They usually have either twofold rotational symmetry or mirror reflection symmetry (often both left/right and up/down).\n\nGuls were historically described in the West as being elephant's foot motifs. Other Western guesses held that the gul was a drawing of a round Turkmen tent, with lines between tents representing irrigation canals; or that the emblem was a totemic bird. None of these descriptions have any basis in weaving tradition or culture.\n\nThe term gul, gol, göl or gül is used widely across Central and West Asia, and among carpet specialists in the West. It may derive from the Persian word gol, which means flower or rose, or from the Turkish word gül which similarly means a rose or roundel.\n\nIn Turkmen weavings, such as bags and rugs, guls are often repeated to form the basic pattern in the main field (excluding the border). \n\nThe different Turkmen tribes such as Tekke, Salor, Ersari and Yomut traditionally wove a variety of guls, some of ancient design, but gul designs were often used by more than one tribe, and by non-Turkmens.\n\nWestern authors have used comparison of the \"design vocabulary\" of tribal guls, reproduced on traditional rugs, in studying the ethnogenesis of Asian peoples.\n\nWestern artists including Hans Memling depicted oriental carpets from Turkish Anatolia with guls in several of his paintings, to the extent that these are known as Memling carpets. These guls often contain star or (hooked) dragon motifs as found on 15th century Konya carpets. The presence of the hooked motif defines a \"Memling carpet\". The artists Lorenzo Lotto and Hans Holbein who similarly depicted Anatolian carpets also have the varieties they painted named after them.\n\n\n"}
{"id": "160506", "url": "https://en.wikipedia.org/wiki?curid=160506", "title": "Hardware random number generator", "text": "Hardware random number generator\n\nIn computing, a hardware random number generator (HRNG) or true random number generator (TRNG) is a device that generates random numbers from a physical process, rather than by means of an algorithm. Such devices are often based on microscopic phenomena that generate low-level, statistically random \"noise\" signals, such as thermal noise, the photoelectric effect, involving a beam splitter, and other quantum phenomena. These stochastic processes are, in theory, completely unpredictable, and the theory's assertions of unpredictability are subject to experimental test. This is in contrast to the common paradigm of pseudo-random number generation commonly implemented in computer programs or cryptographic hardware.\n\nA hardware random number generator typically consists of a transducer to convert some aspect of the physical phenomena to an electrical signal, an amplifier and other electronic circuitry to increase the amplitude of the random fluctuations to a measurable level, and some type of analog to digital converter to convert the output into a digital number, often a simple binary digit 0 or 1. By repeatedly sampling the randomly varying signal, a series of random numbers is attained.\n\nThe main application for electronic hardware random number generators is in cryptography, where they are used to generate random cryptographic keys to transmit data securely. They are widely used in Internet encryption protocols such as Secure Sockets Layer (SSL).\n\nRandom number generators can also be built from \"random\" macroscopic processes, using devices such as coin flipping, dice, roulette wheels and lottery machines. The presence of unpredictability in these phenomena can be justified by the theory of unstable dynamical systems and chaos theory. Even though macroscopic processes are deterministic under Newtonian mechanics, the output of a well-designed device like a roulette wheel cannot be predicted in practice, because it depends on the sensitive, micro-details of the initial conditions of each use.\n\nAlthough dice have been mostly used in gambling, and as \"randomizing\" elements in games (e.g. role playing games), the Victorian scientist Francis Galton described a way to use dice to explicitly generate random numbers for scientific purposes in 1890.\n\nHardware random number generators generally produce only a limited number of random bits per second. In order to increase the available output data rate, they are often used to generate the \"seed\" for a faster cryptographically secure pseudorandom number generator, which then generates a pseudorandom output sequence at a much higher data rate.\n\nUnpredictable random numbers were first investigated in the context of gambling, and many randomizing devices such as dice, shuffling playing cards, and roulette wheels, were first developed for such use. Fairly produced random numbers are vital to electronic gambling and ways of creating them are sometimes regulated by governmental gaming commissions.\n\nRandom numbers are also used for non-gambling purposes, both where their use is mathematically important, such as sampling for opinion polls, and in situations where fairness is approximated by randomization, such as selecting jurors and military draft lotteries.\n\nThe major use for hardware random number generators is in the field of data encryption, for example to create random cryptographic keys to encrypt data. They are a more secure alternative to pseudorandom number generators (PRNGs), software programs commonly used in computers to generate \"random\" numbers. PRNGs use a deterministic algorithm to produce numerical sequences. Although these pseudorandom sequences pass statistical pattern tests for randomness, by knowing the algorithm and the conditions used to initialize it, called the \"seed\", the output can be predicted. Because the sequence of numbers produced by a PRNG is in principle predictable, data encrypted with pseudorandom numbers is potentially vulnerable to cryptanalysis. Hardware random number generators produce sequences of numbers that are assumed not to be predictable, and therefore provide the greatest security when used to encrypt data.\n\nOne early way of producing random numbers was by a variation of the same machines used to play keno or select lottery numbers. These mixed numbered ping-pong balls with blown air, perhaps combined with mechanical agitation, and used some method to withdraw balls from the mixing chamber (). This method gives reasonable results in some senses, but the random numbers generated by this means are expensive. The method is inherently slow, and is unusable for most computing applications.\n\nOn 29 April 1947, RAND Corporation began generating random digits with an \"electronic roulette wheel\", consisting of a random frequency pulse source of about 100,000 pulses per second gated once per second with a constant frequency pulse and fed into a five-bit binary counter. Douglas Aircraft built the equipment, implementing Cecil Hasting’s suggestion (RAND P-113) for a noise source (most likely the well known behavior of the 6D4 miniature gas thyratron tube, when placed in a magnetic field). Twenty of the 32 possible counter values were mapped onto the 10 decimal digits and the other 12 counter values were discarded.\n\nThe results of a long run from the RAND machine, filtered and tested, were converted into a table, which was published in 1955 in the book \"A Million Random Digits with 100,000 Normal Deviates\". The RAND table was a significant breakthrough in delivering random numbers because such a large and carefully prepared table had never before been available. It has been a useful source for simulations, modeling, and for deriving the arbitrary constants in cryptographic algorithms to demonstrate that the constants had not been selected maliciously. The block ciphers Khufu and Khafre are among the applications which use the RAND table. \"See:\" Nothing up my sleeve numbers.\n\nThere are two fundamental sources of practical quantum mechanical physical randomness: quantum mechanics at the atomic or sub-atomic level and thermal noise (some of which is quantum mechanical in origin). Quantum mechanics predicts that certain physical phenomena, such as the nuclear decay of atoms, are fundamentally random and cannot, in principle, be predicted (for a discussion of empirical verification of quantum unpredictability, see Bell test experiments). And, because we live at a temperature above absolute zero, every system has some random variation in its state; for instance, molecules of gases composing air are constantly bouncing off each other in a random way (\"see\" statistical mechanics.) This randomness is a quantum phenomenon as well (\"see\" phonon).\n\nBecause the outcome of quantum-mechanical events cannot be predicted even in principle, they are the ‘gold standard’ for random number generation. Some quantum phenomena used for random number generation include:\n\n\nThermal phenomena are easier to detect. They are somewhat vulnerable to attack by lowering the temperature of the system, though most systems will stop operating at temperatures low enough to reduce noise by a factor of two (e.g., ~150 K). Some of the thermal phenomena used include:\n\n\nIn the absence of quantum effects or thermal noise, other phenomena that tend to be random, although in ways not easily characterized by laws of physics, can be used. When several such sources are combined carefully (as in, for example, the Yarrow algorithm or Fortuna CSPRNGs), enough entropy can be collected for the creation of cryptographic keys and nonces, though generally at restricted rates. The advantage is that this approach needs, in principle, no special hardware. The disadvantage is that a sufficiently knowledgeable attacker can surreptitiously modify the software or its inputs, thus reducing the randomness of the output, perhaps substantially. The primary source of randomness typically used in such approaches is the precise timing of the interrupts caused by mechanical input/output devices, such as keyboards and disk drives, various system information counters, etc.\n\nThis last approach must be implemented carefully and may be subject to attack if it is not. For instance, the forward-security of the generator in Linux 2.6.10 kernel could be broken with 2 or 2 time complexity.\n\nAnother variable physical phenomenon that is easy to measure is clock drift.\nThere are several ways to measure and use clock drift as a source of randomness.\n\nThe Intel 82802 Firmware Hub (FWH) chip included a hardware RNG using two free running oscillators, one fast and one slow. A thermal noise source (non-commonmode noise from two diodes) is used to modulate the frequency of the slow oscillator, which then triggers a measurement of the fast oscillator. That output is then debiased using a von Neumann type decorrelation step (see below). The output rate of this device is somewhat less than 100,000 bit/s. This chip was an optional component of the 840 chipset family that supported an earlier Intel bus. It is not included in modern PCs.\n\nAll VIA C3 microprocessors have included a hardware RNG on the processor chip since 2003. Instead of using thermal noise, raw bits are generated by using four freerunning oscillators which are designed to run at different rates. The output of two are XORed to control the bias on a third oscillator, whose output clocks the output of the fourth oscillator to produce the raw bit. Minor variations in temperature, silicon characteristics, and local electrical conditions cause continuing oscillator speed variations and thus produce the entropy of the raw bits. To further ensure randomness, there are actually two such RNGs on each chip, each positioned in different environments and rotated on the silicon. The final output is a mix of these two generators. The raw output rate is tens to hundreds of megabits per second, and the whitened rate is a few megabits per second. User software can access the generated random bit stream using new non-privileged machine language instructions.\n\nA software implementation of a related idea on ordinary hardware is included in CryptoLib, a cryptographic routine library. The algorithm is called \"truerand\". Most modern computers have two crystal oscillators, one for the real-time clock and one for the primary CPU clock; truerand exploits this fact. It uses an operating system service that sets an alarm, running off the real-time clock. One subroutine sets that alarm to go off in one clock tick (usually 1/60th of a second). Another then enters a while loop waiting for the alarm to trigger. Since the alarm will not always trigger in exactly one tick, the least significant bits of a count of loop iterations, between setting the alarm and its trigger, will vary randomly, possibly enough for some uses. Truerand doesn't require additional hardware, but in a multi-tasking system great care must be taken to avoid non-randomizing interference from other processes (e.g., in the suspension of the counting loop process as the operating system scheduler starts and stops assorted processes).\n\nThe RdRand opcode will return values from an onboard hardware random number generator. It is present in Intel Ivy Bridge processors and AMD64 processors since 2015.\n\nThe bit-stream from such systems is prone to be biased, with either 1s or 0s predominating. There are two approaches to dealing with bias and other artifacts. The first is to design the RNG to minimize bias inherent in the operation of the generator. One method to correct this feeds back the generated bit stream, filtered by a low-pass filter, to adjust the bias of the generator. By the central limit theorem, the feedback loop will tend to be well-adjusted 'almost all the time'. Ultra-high speed random number generators often use this method. Even then, the numbers generated are usually somewhat biased.\n\nA second approach to coping with bias is to reduce it after generation (in software or hardware). Even if the above hardware bias reduction steps have been taken, the bit-stream should still be assumed to contain bias and correlation. There are several techniques for reducing bias and correlation, often called \"whitening\" algorithms, by analogy with the related problem of producing white noise from a correlated signal.\nThere is another way, the dynamic-statics test, which makes a statics randomness check in each random number block dynamically. This can be done usably in a short time, 1 gigabyte per second or more.\nIn this method, if one block shall be determined as a doubtful one, the block is disregarded and canceled.\nThis method is requested in the draft of ANSI(X9F1).\n\nJohn von Neumann invented a simple algorithm to fix simple bias and reduce correlation. It considers two bits at a time (non-overlapping), taking one of three actions: when two successive bits are equal, they are discarded; a sequence of 1,0 becomes a 1; and a sequence of 0,1 becomes a zero. It thus represents a falling edge with a 1, and a rising edge with a 0. This eliminates simple bias, and is easy to implement as a computer program or in digital logic. This technique works no matter how the bits have been generated. It cannot assure randomness in its output, however. What it can do (with significant numbers of discarded bits) is transform a biased random bit stream into an unbiased one.\n\nAnother technique for improving a near random bit stream is to exclusive-or the bit stream with the output of a high-quality cryptographically secure pseudorandom number generator such as Blum Blum Shub or a strong stream cipher. This can improve decorrelation and digit bias at low cost; it can be done by hardware, such as an FPGA, which is faster than doing it by software.\n\nA related method which reduces bias in a near random bit stream is to take two or more uncorrelated near random bit streams, and exclusive or them together. Let the probability of a bit stream producing a 0 be 1/2 + \"e\", where −1/2 ≤ \"e\" ≤ 1/2. Then \"e\" is the bias of the bitstream. If two uncorrelated bit streams with bias \"e\" are exclusive-or-ed together, then the bias of the result will be 2\"e\"². This may be repeated with more bit streams (see also the Piling-up lemma).\n\nSome designs apply cryptographic hash functions such as MD5, SHA-1, or RIPEMD-160 or even a CRC function to all or part of the bit stream, and then use the output as the random bit stream. This is attractive, partly because it is relatively fast compared to some other methods, but depends significantly on qualities in the hash output for which there may be little theoretical basis.\n\nMany physical phenomena can be used to generate bits that are highly biased, but each bit is independent from the others.\nA Geiger counter (with a sample time longer than the tube recovery time) or a semi-transparent mirror photon detector both generate bit streams that are mostly \"0\" (silent or transmission) with the occasional \"1\" (click or reflection).\nIf each bit is independent from the others, the Von Neumann strategy generates one random, unbiased output bit for each of the rare \"1\" bits in such a highly biased bit stream.\nWhitening techniques such as the Advanced Multi-Level Strategy (AMLS) can extract more output bits – output bits that are just as random and unbiased – from such a highly biased bit stream.\n\nOther designs use what are believed to be true random bits as the key for a high quality block cipher algorithm, taking the encrypted output as the random bit stream. Care must be taken in these cases to select an appropriate block mode, however. In some implementations, the PRNG is run for a limited number of digits, while the hardware generating device produces a new seed.\n\nSoftware engineers without true random number generators often try to develop them by measuring physical events available to the software. An example is measuring the time between user keystrokes, and then taking the least significant bit (or two or three) of the count as a random digit. A similar approach measures task-scheduling, network hits, disk-head seek times and other internal events. One Microsoft design includes a very long list of such internal values (see the CSPRNG article). Even lava lamps have been used as the physical devices to be monitored (see Lavarand).\n\nThe method is risky when it uses computer-controlled events because a clever, malicious attacker might be able to predict a cryptographic key by controlling the external events. It is also risky because the supposed user-generated event (e.g., keystrokes) can be spoofed by a sufficiently ingenious attacker, allowing control of the \"random values\" used by the cryptography.\n\nHowever, with sufficient care, a system can be designed that produces cryptographically secure random numbers from the sources of randomness available in a modern computer. The basic design is to maintain an \"entropy pool\" of random bits that are assumed to be unknown to an attacker. New randomness is added whenever available (for example, when the user hits a key) and an estimate of the number of bits in the pool that cannot be known to an attacker is kept. Some of the strategies in use include:\n\n\nIt is very easy to misconstruct hardware or software devices which attempt to generate random numbers. Also, most 'break' silently, often producing decreasingly random numbers as they degrade. A physical example might be the rapidly decreasing radioactivity of the smoke detectors mentioned earlier. Failure modes in such devices are plentiful and are complicated, slow, and hard to detect.\n\nBecause many entropy sources are often quite fragile, and fail silently, statistical tests on their output should be performed continuously. Many, but not all, such devices include some such tests into the software that reads the device.\n\nJust as with other components of a cryptography system, a software random number generator should be designed to resist certain attacks. Defending against these attacks is difficult.\n\nThe random number generator used for cryptographic purposes in version 1.1 of the Netscape browser was vulnerable, and was promptly fixed in version 2.0.\n\nThere are mathematical techniques for estimating the entropy of a sequence of symbols. None are so reliable that their estimates can be fully relied upon; there are always assumptions which may be very difficult to confirm. These are useful for determining if there is enough entropy in a seed pool, for example, but they cannot, in general, distinguish between a true random source and a pseudorandom generator.\n\nHardware random number generators should be constantly monitored for proper operation. RFC 4086, FIPS Pub 140-2 and NIST Special Publication 800-90b include tests which can be used for this. Also see the documentation for the New Zealand cryptographic software library cryptlib.\n\nSince many practical designs rely on a hardware source as an input, it will be useful to at least check that the source is still operating. Statistical tests can often detect failure of a noise source, such as a radio station transmitting on a channel thought to be empty, for example. Noise generator output should be sampled for testing before being passed through a \"whitener.\" Some whitener designs can pass statistical tests with no random input. While detecting a large deviation from perfection would be a sign that a true random noise source has become degraded, small deviations are normal and can be an indication of proper operation. Correlation of bias in the inputs to a generator design with other parameters (e.g., internal temperature, bus voltage) might be additionally useful as a further check. Unfortunately, with currently available (and foreseen) tests, passing such tests is not enough to be sure the output sequences are random. A carefully chosen design, verification that the manufactured device implements that design and continuous physical security to insure against tampering may all be needed in addition to testing for high value uses.\n\n\n\n"}
{"id": "180841", "url": "https://en.wikipedia.org/wiki?curid=180841", "title": "Hypergeometric distribution", "text": "Hypergeometric distribution\n\n\\,</math>\nformula_9\nformula_10\n\nIn statistics, the hypergeometric test uses the hypergeometric distribution to calculate the statistical significance of having drawn a specific formula_15 successes (out of formula_12 total draws) from the aforementioned population. The test is often used to identify which sub-populations are over- or under-represented in a sample. This test has a wide range of applications. For example, a marketing group could use the test to understand their customer base by testing a set of known customers for over-representation of various demographic subgroups (e.g., women, people under 30).\n\nThe following conditions characterize the hypergeometric distribution:\n\nA random variable formula_19 follows the hypergeometric distribution if its probability mass function (pmf) is given by\n\nwhere\n\n\nThe is positive when formula_26.\n\nA random variable distributed hypergeometrically with parameters formula_13, formula_14 and formula_12 is written formula_30 and has probability mass function formula_31 above.\n\nAs required, we have\n\nwhich essentially follows from Vandermonde's identity from combinatorics.\n\nAlso note that\n\nwhich follows from the symmetry of the problem, but it can also be shown by expressing the binomial coefficients in terms of factorials and rearranging the latter.\n\nThe classical application of the hypergeometric distribution is sampling without replacement. Think of an urn with two types of marbles, red ones and green ones. Define drawing a green marble as a success and drawing a red marble as a failure (analogous to the binomial distribution). If the variable \"N\" describes the number of all marbles in the urn (see contingency table below) and \"K\" describes the number of green marbles, then \"N\" − \"K\" corresponds to the number of red marbles. In this example, \"X\" is the random variable whose outcome is \"k\", the number of green marbles actually drawn in the experiment. This situation is illustrated by the following contingency table:\n\nNow, assume (for example) that there are 5 green and 45 red marbles in the urn. Standing next to the urn, you close your eyes and draw 10 marbles without replacement. What is the probability that exactly 4 of the 10 are green? \"Note that although we are looking at success/failure, the data are not accurately modeled by the binomial distribution, because the probability of success on each trial is not the same, as the size of the remaining population changes as we remove each marble.\"\n\nThis problem is summarized by the following contingency table:\nThe probability of drawing exactly \"k\" green marbles can be calculated by the formula\n\nHence, in this example calculate\n\nIntuitively we would expect it to be even more unlikely that all 5 green marbles will be among the 10 drawn.\n\nAs expected, the probability of drawing 5 green marbles is roughly 35 times less likely than that of drawing 4.\n\nElection audits typically test a sample of machine-counted precincts to see if recounts by hand or machine match the original counts. Mismatches result in either a report or a larger recount. The sampling rates are usually defined by law, not statistical design, so for a legally defined sample size \"n\", what is the probability of missing a problem which is present in \"K\" precincts, such as a hack or bug? This is the probability that \"k\" = 0. Bugs are often obscure, and a hacker can minimize detection by affecting only a few precincts, which will still affect close elections, so a plausible scenario is for \"K\" to be on the order of 5% of \"N\". Audits typically cover 1% to 10% of precincts (often 3%), so they have a high chance of missing a problem. For example if a problem is present in 5 of 100 precincts, a 3% sample has 86% probability that \"k\" = 0 so the problem would not be noticed, and only 14% probability of the problem appearing in the sample (positive \"k\"):\n\nThe sample would need 45 precincts in order to have probability under 5% that \"k\" = 0 in the sample, and thus have probability over 95% of finding the problem:\n\nIn hold'em poker players make the best hand they can combining the two cards in their hand with the 5 cards (community cards) eventually turned up on the table. The deck has 52 and there are 13 of each suit.\nFor this example assume a player has 2 clubs in the hand and there are 3 cards showing on the table, 2 of which are also clubs. The player would like to know the probability of one of the next 2 cards to be shown being a club to complete the flush.\n\nThere are 4 clubs showing so there are 9 still unseen. There are 5 cards showing (2 in the hand and 3 on the table) so there are formula_39 still unseen.\n\nThe probability that one of the next two cards turned is a club can be calculated using hypergeometric with formula_40 and formula_41. (about 31.6%)\n\nThe probability that both of the next two cards turned are clubs can be calculated using hypergeometric with formula_42 and formula_41. (about 3.3%)\n\nThe probability that neither of the next two cards turned are clubs can be calculated using hypergeometric with formula_44 and formula_41. (about 65.0%)\n\nSwapping the roles of green and red marbles:\n\nSwapping the roles of drawn and not drawn marbles:\n\nSwapping the roles of green and drawn marbles:\n\nThe hypergeometric test uses the hypergeometric distribution to measure the statistical significance of having drawn a sample consisting of a specific number of formula_15 successes (out of formula_12 total draws) from a population of size formula_13 containing formula_14 successes. In a test for over-representation of successes in the sample, the hypergeometric p-value is calculated as the probability of randomly drawing formula_15 or more successes from the population in formula_12 total draws. In a test for under-representation, the p-value is the probability of randomly drawing formula_15 or fewer successes.\n\nThe test based on the hypergeometric distribution (hypergeometric test) is identical to the corresponding one-tailed version of Fisher's exact test ). Reciprocally, the p-value of a two-sided Fisher's exact test can be calculated as the sum of two appropriate hypergeometric tests (for more information see ).\n\nThe probability of drawing any set of green and red marbles (the hypergeometric distribution) depends only on the numbers of green and red marbles, not on the order in which they appear; i.e., it is an exchangeable distribution. As a result, the probability of drawing a green marble in the formula_56 draw is\n\nThis is an ex ante probability—that is, it is based on not knowing the results of the previous draws.\n\nLet formula_58 and formula_59.\n\n\nwhere formula_79 is the standard normal distribution function\n\nThe following table describes four distributions related to the number of successes in a sequence of draws: \nLet formula_81 and formula_59. Then we can derive the following bounds:\n\nwhere\n\nis the Kullback-Leibler divergence and it is used that formula_85.\n\nIf \"n\" is larger than \"N\"/2, it can be useful to apply symmetry to \"invert\" the bounds, which give you the following:\n\nThe model of an urn with green and red marbles can be extended to the case where there are more than two colors of marbles. If there are \"K\" marbles of color \"i\" in the urn and you take \"n\" marbles at random without replacement, then the number of marbles of each color in the sample (\"k\",\"k\"...,\"k\") has the multivariate hypergeometric distribution. This has the same relationship to the multinomial distribution that the hypergeometric distribution has to the binomial distribution—the multinomial distribution is the \"with-replacement\" distribution and the multivariate hypergeometric is the \"without-replacement\" distribution.\n\nThe properties of this distribution are given in the adjacent table, where \"c\" is the number of different colors and formula_90 is the total number of marbles.\n\nSuppose there are 5 black, 10 white, and 15 red marbles in an urn. If six marbles are chosen without replacement, the probability that exactly two of each color are chosen is\n\n\n\n"}
{"id": "55573467", "url": "https://en.wikipedia.org/wiki?curid=55573467", "title": "Infinite difference method", "text": "Infinite difference method\n\nIn mathematics, infinite difference methods are numerical methods for solving differential equations by approximating them with difference equations, in which infinite differences approximate the derivatives.\n\n\n"}
{"id": "3661969", "url": "https://en.wikipedia.org/wiki?curid=3661969", "title": "Jean-Yves Béziau", "text": "Jean-Yves Béziau\n\nJean-Yves Beziau (; born January 15, 1965 in Orléans, France) is a professor and researcher of the Brazilian Research Council — CNPq — at the University of Brazil in Rio de Janeiro.\n\nBeziau works in the field of logic—in particular, paraconsistent logic, the square of opposition and universal logic. He holds a Maîtrise in Philosophy from Pantheon-Sorbonne University, a DEA in Philosophy from Pantheon-Sorbonne University, a PhD in Philosophy from the University of São Paulo, a MSc and a PhD in Logic and Foundations of Computer Science from Denis Diderot University.\n\nBéziau is the editor-in-chief of the journal \"Logica Universalis\" and of the \"South American Journal of Logic\" - an online, open-access journal - as well as of the Springer book series \"Studies in Universal Logic.\" He is also the editor of \"College Publication's\" book series \"Logic PhDs\"\n\nHe has launched four major international series of events: UNILOG (World Congress and School on Universal Logic), SQUARE (World Congress on the Square of Opposition), WOCOLOR (World Congress on Logic and Religion), LIQ (Logic in Question). Such events have included major figures of mathematics, philosophy, computer science and religion such as Saul Kripke, Michal Heller, Laurent Lafforgue, Yuri Gurevich, Pierre Cartier ,\n\n\n"}
{"id": "305842", "url": "https://en.wikipedia.org/wiki?curid=305842", "title": "Juris Hartmanis", "text": "Juris Hartmanis\n\nJuris Hartmanis (born July 5, 1928) is a prominent computer scientist and computational theorist who, with Richard E. Stearns, received the 1993 ACM Turing Award \"in recognition of their seminal paper which established the foundations for the field of computational complexity theory\".\n\nHartmanis was born in Latvia. He was a son of , a general in the Latvian Army, and sister of poet Astrid Ivask. After the Soviet Union occupied Latvia in 1940, Mārtiņš Hartmanis was arrested by Soviets and died in a prison. At the end of World War II, the wife and children of Mārtiņš Hartmanis left Latvia as refugees, fearing for their safety if the Soviet Union took over Latvia again.\n\nThey first moved to Germany, where Juris Hartmanis received the equivalent of a master's degree in physics from the University of Marburg. Then he moved to the United States, where he received master's degree in applied mathematics at the University of Kansas City (now known as the University of Missouri-Kansas City) in 1951 and a Ph.D. in mathematics from Caltech under the supervision of Robert P. Dilworth in 1955. The University of Missouri-Kansas City honored him with Honorary Doctor of Humane Letters in May 1999.\n\nAfter teaching at Cornell University and Ohio State University, Hartmanis joined the General Electric Research Laboratory in 1958. While at General Electric, he developed many principles of computational complexity theory. In 1965, he became a professor at Cornell University. At Cornell, he was one of founders and the first chairman of its computer science department (which was one of the first computer science departments in the world). Hartmanis is a Fellow of the Association for Computing Machinery and of the American Mathematical Society and a member of the National Academy of Engineering and National Academy of Sciences.\n\nHe is best known for his Turing-award-winning paper with Richard Stearns, in which he introduced time complexity classes \"TIME (f(n))\" and proved the time hierarchy theorem. Another paper by Hartmanis from 1977, with Leonard Berman, introduced the still-unsolved Berman–Hartmanis conjecture that all NP-complete languages are polynomial time isomorphic.\n\n\n"}
{"id": "33744493", "url": "https://en.wikipedia.org/wiki?curid=33744493", "title": "Kunita–Watanabe inequality", "text": "Kunita–Watanabe inequality\n\nIn stochastic calculus, the Kunita–Watanabe inequality is a generalization of the Cauchy–Schwarz inequality to integrals of stochastic processes.\n\nLet \"M\", \"N\" be continuous local martingales and \"H\", \"K\" measurable processes. Then\n\nwhere the brackets indicates the quadratic variation and quadratic covariation operators. The integrals are understood in the Lebesgue–Stieltjes sense.\n"}
{"id": "1811962", "url": "https://en.wikipedia.org/wiki?curid=1811962", "title": "Linear bounded automaton", "text": "Linear bounded automaton\n\nIn computer science, a linear bounded automaton (plural linear bounded automata, abbreviated LBA) is a restricted form of Turing machine.\n\nA linear bounded automaton is a nondeterministic Turing machine that satisfies the following three conditions:\n\nIn other words: \ninstead of having potentially infinite tape on which to compute, computation is restricted to the portion of the tape containing the input plus the two tape squares holding the endmarkers. \n\nAn alternative, less restrictive definition is as follows:\n\nThis limitation makes an LBA a somewhat more accurate model of a real-world computer than a Turing machine, whose definition assumes unlimited tape.\n\nThe strong and the weaker definition lead to the same computational abilities of the respective automaton classes, due to the \"linear speedup theorem\".\n\nLinear bounded automata are acceptors for the class of context-sensitive languages. The only restriction placed on grammars for such languages is that no production maps a string to a shorter string. Thus no derivation of a string in a context-sensitive language can contain a sentential form longer than the string itself. Since there is a one-to-one correspondence between linear-bounded automata and such grammars, no more tape than that occupied by the original string is necessary for the string to be recognized by the automaton.\n\nIn 1960, John Myhill introduced an automaton model today known as deterministic linear bounded automaton. In 1963, Peter S. Landweber proved that the languages accepted by deterministic LBAs are context-sensitive. In 1964, S.-Y. Kuroda introduced the more general model of (nondeterministic) linear bounded automata, noted that Landweber's proof also works for nondeterministic linear bounded automata, and showed that the languages accepted by them are precisely the context-sensitive languages.\n\nIn his seminal paper, Kuroda also stated two research challenges, which subsequently became famously known as the \"LBA problems\": The first LBA problem is whether the class of languages accepted by LBA is equal to the class of languages accepted by deterministic LBA. This problem can be phrased succinctly in the language of computational complexity theory as:\nThe second LBA problem is whether the class of languages accepted by LBA is closed under complement.\nAs observed already by Kuroda, a negative answer to the second LBA problem would imply a negative answer to the first problem. But the second LBA problem has an affirmative answer, which is implied by the Immerman–Szelepcsényi theorem proved 20 years after the problem was raised. As of today, the first LBA problem still remains open.\n\n"}
{"id": "234969", "url": "https://en.wikipedia.org/wiki?curid=234969", "title": "List of integrals of inverse trigonometric functions", "text": "List of integrals of inverse trigonometric functions\n\nThe following is a list of indefinite integrals (antiderivatives) of expressions involving the inverse trigonometric functions. For a complete list of integral formulas, see lists of integrals.\n\n"}
{"id": "39402871", "url": "https://en.wikipedia.org/wiki?curid=39402871", "title": "List of things named after Alexander Grothendieck", "text": "List of things named after Alexander Grothendieck\n\nThe mathematician Alexander Grothendieck (1928–2014) is the eponym of many things.\n\n"}
{"id": "39643921", "url": "https://en.wikipedia.org/wiki?curid=39643921", "title": "Markov–Krein theorem", "text": "Markov–Krein theorem\n\nIn probability theory, the Markov–Krein theorem gives the best upper and lower bounds on the expected values of certain functions of a random variable where only the first moments of the random variable are known. The result is named after Andrey Markov and Mark Krein.\n\nThe theorem can be used to bound average response times in the M/G/k queueing system.\n"}
{"id": "18894568", "url": "https://en.wikipedia.org/wiki?curid=18894568", "title": "Mathematical programming with equilibrium constraints", "text": "Mathematical programming with equilibrium constraints\n\nMathematical programming with equilibrium constraints (MPEC) is the study of \nconstrained optimization problems where the constraints include variational inequalities or complementarities. MPEC is related to the Stackelberg game. \n\nMPEC is used in the study of engineering design, economic equilibrium and multilevel games.\n\nMPEC is difficult to deal with because its feasible region is not necessarily convex or even connected.\n\n\n"}
{"id": "9758445", "url": "https://en.wikipedia.org/wiki?curid=9758445", "title": "Maximal semilattice quotient", "text": "Maximal semilattice quotient\n\nIn abstract algebra, a branch of mathematics, a maximal semilattice quotient is a commutative monoid derived from another commutative monoid by making certain elements equivalent to each other.\n\nEvery commutative monoid can be endowed with its \"algebraic\" preordering ≤ . By definition, \"x≤ y\" holds, if there exists \"z\" such that \"x+z=y\". Further, for \"x, y\" in \"M\", let formula_1 hold, if there exists a positive integer \"n\" such that \"x≤ ny\", and let formula_2 hold, if formula_1 and formula_4. The binary relation formula_5 is a monoid congruence of \"M\", and the quotient monoid formula_6 is the \"maximal semilattice quotient\" of \"M\".\n\nThis terminology can be explained by the fact that the canonical projection \"p\" from \"M\" onto formula_6 is universal among all monoid homomorphisms from \"M\" to a (∨,0)-semilattice, that is, for any (∨,0)-semilattice \"S\" and any monoid homomorphism \"f: M→ S\", there exists a unique (∨,0)-homomorphism formula_8 such that \"f=gp\".\n\nIf \"M\" is a refinement monoid, then formula_6 is a distributive semilattice.\n\nA.H. Clifford and G.B. Preston, The Algebraic Theory of Semigroups. Vol. I. Mathematical Surveys, No. 7, American Mathematical Society, Providence, R.I. 1961. xv+224 p.\n"}
{"id": "9518854", "url": "https://en.wikipedia.org/wiki?curid=9518854", "title": "Microscopic traffic flow model", "text": "Microscopic traffic flow model\n\nMicroscopic traffic flow models are a class of scientific models of vehicular traffic dynamics.\n\nIn contrast to macroscopic models, microscopic traffic flow models simulate single vehicle-driver units, so the dynamic variables of the models represent microscopic properties like the position and velocity of single vehicles.\n\nAlso known as \"time-continuous models\", all car-following models have in common that they are defined by ordinary differential equations describing the complete dynamics of the vehicles' positions formula_1 and velocities formula_2. It is assumed that the input stimuli of the drivers are restricted to their own velocity formula_2, the net distance (bumper-to-bumper distance) formula_4 to the leading vehicle formula_5 (where formula_6 denotes the vehicle length), and the velocity formula_7 of the leading vehicle. The equation of motion of each vehicle is characterized by an acceleration function that depends on those input stimuli:\n\nIn general, the driving behavior of a single driver-vehicle unit formula_9 might not merely depend on the immediate leader formula_5 but on the formula_11 vehicles in front. The equation of motion in this more generalized form reads:\n\n\nCellular automaton (CA) models use integer variables to describe the dynamical properties of the system. The road is divided into sections of a certain length formula_13 and the time is discretized to steps of formula_14. Each road section can either be occupied by a vehicle or empty and the dynamics are given by update rules of the form:\n\n(the simulation time formula_17 is measured in units of formula_14 and the vehicle positions formula_1 in units of formula_13).\n\nThe time scale is typically given by the reaction time of a human driver, formula_21. With formula_14 fixed, the length of the road sections determines the granularity of the model. At a complete standstill, the average road length occupied by one vehicle is approximately 7.5 meters. Setting formula_13 to this value leads to a model where one vehicle always occupies exactly one section of the road and a velocity of 5 corresponds to formula_24, which is then set to be the maximum velocity a driver wants to drive at. However, in such a model, the smallest possible acceleration would be formula_25 which is unrealistic. Therefore, many modern CA models use a finer spatial discretization, for example formula_26, leading to a smallest possible acceleration of formula_27.\n\nAlthough cellular automaton models lack the accuracy of the time-continuous car-following models, they still have the ability to reproduce a wide range of traffic phenomena. Due to the simplicity of the models, they are numerically very efficient and can be used to simulate large road networks in realtime or even faster.\n\n\n"}
{"id": "1579841", "url": "https://en.wikipedia.org/wiki?curid=1579841", "title": "Nixon diamond", "text": "Nixon diamond\n\nIn nonmonotonic reasoning, the Nixon diamond is a scenario in which default assumptions lead to mutually inconsistent conclusions. The scenario is:\n\n\nSince Nixon is a Quaker, one could assume that he is a pacifist; since he is Republican, however, one could also assume he is not a pacifist. The problem is how a formal logic of nonmonotonic reasoning should deal with such cases. Two approaches can be adopted:\n\n\nThe credulous approach can allow proving both something and its contrary. For this reason, the skeptical approach is often preferred. Another solution to this problem is to attach priorities to default assumptions; for example, the fact that “usually, Republicans are not pacifist” can be assumed more likely than “usually, Quakers are pacifist”, leading to the conclusion that Nixon is not pacifist.\n\nThe name \"diamond\" comes from the fact that such a scenario, when expressed in inheritance networks, is a diamond shape. This example is mentioned for the first time by Reiter and Criscuolo in a slightly different form where the person that is both a Republican and a Quaker is a John instead of Richard Nixon.\n\n\n\n"}
{"id": "387878", "url": "https://en.wikipedia.org/wiki?curid=387878", "title": "Outline of probability", "text": "Outline of probability\n\nProbability is a measure of the likeliness that an event will occur. Probability is used to quantify an attitude of mind towards some proposition of whose truth we are not certain. The proposition of interest is usually of the form \"A specific event will occur.\" The attitude of mind is of the form \"How certain are we that the event will occur?\" The certainty we adopt can be described in terms of a numerical measure and this number, between 0 and 1 (where 0 indicates impossibility and 1 indicates certainty), we call probability. Probability theory is used extensively in statistics, mathematics, science and philosophy to draw conclusions about the likelihood of potential events and the underlying mechanics of complex systems.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "10538204", "url": "https://en.wikipedia.org/wiki?curid=10538204", "title": "Parabolic Lie algebra", "text": "Parabolic Lie algebra\n\nIn algebra, a parabolic Lie algebra formula_1 is a subalgebra of a semisimple Lie algebra formula_2 satisfying one of the following two conditions:\nThese conditions are equivalent over an algebraically closed field of characteristic zero, such as the complex numbers. If the field formula_8 is not algebraically closed, then the first condition is replaced by the assumption that\nwhere formula_11 is the algebraic closure of formula_8.\n\n\n"}
{"id": "4921531", "url": "https://en.wikipedia.org/wiki?curid=4921531", "title": "Patlak plot", "text": "Patlak plot\n\nA Patlak plot (sometimes called Gjedde–Patlak plot, Patlak–Rutland plot, or Patlak analysis) is a graphical analysis technique based on the compartment model that uses linear regression to identify and analyze pharmacokinetics of tracers involving irreversible uptake, such as in the case of deoxyglucose. It is used for the evaluation of nuclear medicine imaging data after the injection of a radioopaque or radioactive tracer.\n\nThe method is model-independent because it does not depend on any specific compartmental model configuration for the tracer, and the minimal assumption is that the behavior of the tracer can be approximated by two compartments – a \"central\" (or reversible) compartment that is in rapid equilibrium with plasma, and a \"peripheral\" (or irreversible) compartment, where tracer enters without ever leaving during the time of the measurements. The amount of tracer in the region of interest is accumulating according to the equation:\n\nwhere formula_2 represents time after tracer injection, formula_3 is the amount of tracer in region of interest, formula_4 is the concentration of tracer in plasma or blood, formula_5 is the clearance determining the rate of entry into the peripheral (irreversible) compartment, and formula_6 is the distribution volume of the tracer in the central compartment. The first term of the right-hand side represents tracer in the peripheral compartment, and the second term tracer in the central compartment.\n\nBy dividing both sides by formula_4, one obtains:\n\nThe unknown constants formula_5 and formula_6 can be obtained by linear regression from a graph of formula_11 against formula_12.\n\n\n"}
{"id": "16568536", "url": "https://en.wikipedia.org/wiki?curid=16568536", "title": "Planar projection", "text": "Planar projection\n\nPlanar projections are the subset of 3D graphical projections constructed by linearly mapping points in three-dimensional space to points on a two-dimensional projection plane. The projected point on the plane is chosen such that it is collinear with the corresponding three-dimensional point and the centre of projection. The lines connecting these points are commonly referred to as projectors. \n\nThe centre of projection can be thought of as the location of the observer while the plane of projection is the surface on which the two dimensional projected image of the scene is recorded or from which it is viewed (e.g., photographic negative, photographic print, computer monitor). When the centre of projection is at a finite distance from the projection plane, a perspective projection is obtained. When the centre of projection is at infinity, all the projectors are parallel, and the corresponding subset of planar projections are referred to as parallel projections.\n\nMathematically, planar projections are linear transformations acting on a point in three-dimensional space formula_1 to give a point formula_2 on the projection plane. These transformations consist of various compositions of the five transformations: orthographic projection, rotation, shear, translation and perspective.\n\nIt is also used in maps to show the planet Earth and other planets or objects in space. This is good for maps of close-up areas.\n"}
{"id": "18449916", "url": "https://en.wikipedia.org/wiki?curid=18449916", "title": "SC (complexity)", "text": "SC (complexity)\n\nIn computational complexity theory, SC (Steve's Class, named after Stephen Cook) is the complexity class of problems solvable by a deterministic Turing machine in polynomial time (class P) and polylogarithmic space (class PolyL) (that is, O((log \"n\")) space for some constant \"k\"). It may also be called DTISP(poly, polylog), where DTISP stands for \"deterministic time and space\". Note that the definition of SC differs from P ∩ PolyL, since for the former, it is required that a single algorithm runs in both polynomial time and polylogarithmic space; while for the latter, two separate algorithms will suffice: one that runs in polynomial time, and another that runs in polylogarithmic space. (It is unknown whether SC and P ∩ PolyL are equivalent).\n\nDCFL, the strict subset of context-free languages recognized by deterministic pushdown automata, is contained in SC, as shown by Cook in 1979.\n\nIt is open if directed st-connectivity is in SC, although it is known to be in P ∩ PolyL (because of a DFS algorithm and Savitch's theorem). This question is equivalent to NL ⊆ SC.\n\nRL and BPL are classes of problems acceptable by probabilistic Turing machines in logarithmic space and polynomial time. Noam Nisan showed in 1992 the weak derandomization result that both are contained in SC. In other words, given \"polylogarithmic\" space, a deterministic machine can simulate \"logarithmic\" space probabilistic algorithms.\n"}
{"id": "32319857", "url": "https://en.wikipedia.org/wiki?curid=32319857", "title": "Satake isomorphism", "text": "Satake isomorphism\n\nIn mathematics, the Satake isomorphism, introduced by , identifies the Hecke algebra of a reductive group over a local field with a ring of invariants of the Weyl group.\nThe geometric Satake equivalence is a geometric version of the Satake isomorphism, introduced by .\n\nLet \"G\" be a Chevalley group, \"K\" be a non-Archimedean local field and \"O\" be its ring of integers. Then the Satake isomorphism identifies the Grothendieck group of complex representations of the Langlands dual formula_1 of \"G\", with the ring of \"G(O)\" invariant compactly supported functions on the affine Grassmannian. In formulas:\n\nHere \"G(O)\" acts on \"G(K)\" / \"G(O)\" by multiplication from the left.\n\n"}
{"id": "53697574", "url": "https://en.wikipedia.org/wiki?curid=53697574", "title": "Teresa W. Haynes", "text": "Teresa W. Haynes\n\nTeresa W. Haynes is an American professor of mathematics and statistics at East Tennessee State University known for her research in graph theory and particularly on dominating sets.\n\nHaynes earned three degrees from Eastern Kentucky University: a B.S in Mathematics and Education in 1975, M.A. in Mathematics and Education in 1978, and M.S in Mathematical Sciences in 1984. She completed her Ph.D in Computer Science in 1988 from the University of Central Florida. Her dissertation was \"On formula_1-formula_2-Insensitive Domination\" and was supervised by Robert C. Brigham.\n\nHaynes worked as a mathematics teacher from 1975 to 1978 and as a telephone engineer from 1978 to 1981. She became a mathematics and computer science instructor at Pikeville College in 1981, and moved to Prestonburg Community College in 1983.\nAfter completing her doctorate in 1988, she became an assistant professor at East Tennessee State, and she was promoted to full professor there in 1999.\n\nHaynes is the author of two books on dominating sets in graph theory:\n"}
{"id": "8328262", "url": "https://en.wikipedia.org/wiki?curid=8328262", "title": "Three-process view", "text": "Three-process view\n\nThe three-process view is a psychological term coined by Janet E. Davidson and Robert Sternberg.\n\nAccording to this concept, there are three kinds of insight: selective-encoding, selective-comparison, and selective-combination.\n\nSelective-encoding insight – Distinguishing what is important in a problem and what is irrelevant. (i.e. filter) \nSelective-comparison insight – Identifying information by finding a connection between acquired knowledge and experience. \nSelective-combination insight – Identifying a problem through understanding the different components and putting everything together.\n"}
{"id": "36145695", "url": "https://en.wikipedia.org/wiki?curid=36145695", "title": "Triangular network coding", "text": "Triangular network coding\n\nIn coding theory, triangular network coding (TNC) is a network coding based packet coding scheme introduced by .\nPreviously, packet coding for network coding was done using linear network coding (LNC). The drawback of LNC over large finite field is that it resulted in high encoding and decoding computational complexity. While linear encoding and decoding over GF(2) alleviates the concern of high computational complexity, coding over GF(2) comes at the tradeoff cost of degrading throughput performance.\n\nTriangular network coding therefore essentially addresses the high encoding and decoding computational complexity without degrading the throughput performance, with code rate comparable to that of linear network coding.\n\nIn TNC, coding is performed in two stages. First redundant \"0\" bits are selectively added at the head and tail of each packet such that all packets are of uniform bit length. Then the packets are XOR coded, bit-by-bit. The \"0\" bits are added in such a way that these redundant \"0\" bits added to each packet generate a triangular pattern.\n\nIn essence, the TNC decoding process, like the LNC decoding process involves Gaussian elimination. However, since the packets in TNC have been coded in such a manner that the resulting coded packets are in triangular pattern, the computational process of \"triangularization,\" with complexity of formula_1, where formula_2 is the number of packets, can be bypassed. The receiver now only needs to perform \"back-substitution,\" with complexity given as formula_3 for each bit location.\n"}
{"id": "161006", "url": "https://en.wikipedia.org/wiki?curid=161006", "title": "Unary operation", "text": "Unary operation\n\nIn mathematics, a unary operation is an operation with only one operand, i.e. a single input. This is in contrast to binary operations, which use two operands. An example is the function , where \"A\" is a set. The function \"f\" is a unary operation on \"A\".\n\nCommon notations are prefix notation (e.g. +, −, ¬), postfix notation (e.g. factorial n!), functional notation (e.g. sin \"x\" or sin(\"x\")), and superscripts (e.g. transpose \"A\"). Other notations exist as well. For example, in the case of the square root, a horizontal bar extending the square root sign over the argument can indicate the extent of the argument.\n\nAs unary operations have only one operand they are evaluated before other operations containing them. Here is an example using negation:\n\nHere, the first '−' represents the binary subtraction operation, while the second '−' represents the unary negation of the 2 (or '−2' could be taken to mean the integer −2). Therefore, the expression is equal to:\n\nTechnically there is also a unary positive but it is not needed since we assume a value to be positive:\n\nUnary positive does not change the sign of a negative operation:\n\nIn this case a unary negative is needed to change the sign:\n\nIn trigonometry the functions formula_1, formula_2, formula_3, and the other trigonometric functions are unary operations. This is because it is possible to input only one term with the operation and retrieve a result, compared with operations, such as addition, where two different terms are needed to compute a result.\n\nIn the C family of languages, the following operators are unary:\n\n\nIn the Unix/Linux shell (bash/sh), '$' is a unary operator when used for parameter expansion, replacing the name of a variable by its (sometimes modified) value. For example:\n\n\n\n"}
{"id": "54117020", "url": "https://en.wikipedia.org/wiki?curid=54117020", "title": "Unrestricted algorithm", "text": "Unrestricted algorithm\n\nAn unrestricted algorithm is an algorithm for the computation of a mathematical function that puts no restrictions on the range of the argument or on the precision that may be demanded in the result. The idea of such an algorithm was put forward by C. W. Clenshaw and F. W. J. Olver in a paper published in 1980.\n\nIn the problem of developing algorithms for computing the values of a real-valued function of a real variable, say \"g\"(\"x\"), in \"restricted\" algorithms, the error that can be tolerated in the result is specified in advance. An interval on the real line would also be specified for values where in the values of function are to be evaluated. Different algorithms may have to applied for evaluating functions outside the interval. An unrestricted algorithm envisages a situation in which a user may stipulate the value of \"x\" and also the precision required in \"g\"(\"x\"), quite arbitrarily. The algorithm should then produce an acceptable result without failure.\n"}
{"id": "35154167", "url": "https://en.wikipedia.org/wiki?curid=35154167", "title": "Waldspurger's theorem", "text": "Waldspurger's theorem\n\nIn mathematics, Waldspurger's theorem, introduced by , is a result that identifies Fourier coefficients of modular forms of half-integral weight \"k\"+1/2 with the value of a L-series at \"s\"=\"k\"/2.\n"}
{"id": "13284111", "url": "https://en.wikipedia.org/wiki?curid=13284111", "title": "Wu's method of characteristic set", "text": "Wu's method of characteristic set\n\nWenjun Wu's method is an algorithm for solving multivariate polynomial equations introduced in the late 1970s by the Chinese mathematician Wen-Tsun Wu. This method is based on the mathematical concept of characteristic set introduced in the late 1940s by J.F. Ritt. It is fully independent of the Gröbner basis method, introduced by Bruno Buchberger (1965), even if Gröbner bases may be used to compute characteristic sets.\n\nWu's method is powerful for mechanical theorem proving in elementary geometry, and provides a complete decision process for certain classes of problem.\nIt has been used in research in his laboratory (KLMM, Key Laboratory of Mathematics Mechanization in Chinese Academy of Science) and around the world. The main trends of research on Wu's method concern systems of polynomial equations of positive dimension and differential algebra where Ritt's results have been made effective. Wu's method has been applied in various scientific fields, like biology, computer vision, robot kinematics and especially automatic proofs in geometry\n\nWu's method uses polynomial division to solve problems of the form:\n\nwhere \"f\" is a polynomial equation and \"I\" is a conjunction of polynomial equations. The algorithm is complete for such problems over the complex domain.\n\nThe core idea of the algorithm is that you can divide one polynomial by another to give a remainder. Repeated division results in either the remainder vanishing (in which case the \"I\" implies \"f\" statement is true), or an irreducible remainder is left behind (in which case the statement is false).\n\nMore specifically, for an ideal \"I\" in the ring \"k\"[\"x\", ..., \"x\"] over a field \"k\", a (Ritt) characteristic set \"C\" of \"I\" is composed of a set of polynomials in \"I\", which is in triangular shape: polynomials in \"C\" have distinct main variables (see the formal definition below). Given a characteristic set \"C\" of \"I\", one can decide if a polynomial \"f\" is zero modulo \"I\". That is, the membership test is checkable for \"I\", provided a characteristic set of \"I\".\n\nA Ritt characteristic set is a finite set of polynomials in triangular form of an ideal. This triangular set satisfies\ncertain minimal condition with respect to the Ritt ordering, and it preserves many interesting geometrical properties\nof the ideal. However it may not be its system of generators.\n\nLet R be the multivariate polynomial ring \"k\"[\"x\", ..., \"x\"] over a field \"k\".\nThe variables are ordered linearly according to their subscript: \"x\" < ... < \"x\".\nFor a non-constant polynomial \"p\" in R, the greatest variable effectively presenting in \"p\", called main variable or class, plays a particular role:\n\"p\" can be naturally regarded as a univariate polynomial in its main variable \"x\" with coefficients in \"k\"[\"x\", ..., \"x\"].\nThe degree of p as a univariate polynomial in its main variable is also called its main degree.\n\nA set \"T\" of non-constant polynomials is called a triangular set if all polynomials in \"T\" have distinct main variables. This generalizes triangular systems of linear equations in a natural way.\n\nFor two non-constant polynomials \"p\" and \"q\", we say \"p\" is smaller than \"q\" with respect to Ritt ordering and written as \"p\" < \"q\", if one of the following assertions holds:\n\nIn this way, (\"k\"[\"x\", ..., \"x\"],<) forms a well partial order. However, the Ritt ordering is not a total order:\nthere exist polynomials p and q such that neither \"p\" < \"q\" nor \"p\" > \"q\". In this case, we say that \"p\" and \"q\" are not comparable.\nNote that the Ritt ordering is comparing the rank of \"p\" and \"q\". The rank, denoted by rank(\"p\"), of a non-constant polynomial \"p\" is defined to be a power of\nits main variable: mvar(\"p\") and ranks are compared by comparing first the variables and then, in case of equality of the variables, the degrees.\n\nA crucial generalization on Ritt ordering is to compare triangular sets.\nLet \"T\" = { \"t\", ..., \"t\"} and \"S\" = { \"s\", ..., \"s\"} be two triangular sets\nsuch that polynomials in \"T\" and \"S\" are sorted increasingly according to their main variables.\nWe say \"T\" is smaller than \"U\" w.r.t. Ritt ordering if one of the following assertions holds\n\nAlso, there exists incomparable triangular sets w.r.t Ritt ordering.\n\nLet I be a non-zero ideal of k[x, ..., x]. A subset T of I is a Ritt characteristic set of I if one of the following conditions holds:\n\nA polynomial ideal may possess (infinitely) many characteristic sets, since Ritt ordering is a partial order.\n\nThe Ritt–Wu process, first devised by Ritt, subsequently modified by Wu, computes not a Ritt characteristic but an extended one, called Wu characteristic set or ascending chain.\n\nA non-empty subset T of the ideal <F> generated by F is a Wu characteristic set of F if one of the following condition holds\n\nNote that Wu characteristic set is defined to the set F of polynomials, rather to the ideal <F> generated by F. Also it can be shown that a Ritt characteristic set T of <F> is a Wu characteristic set of F. Wu characteristic sets can be computed by Wu's algorithm CHRST-REM, which only requires pseudo-remainder computations and no factorizations are needed.\n\nWu's characteristic set method has exponential complexity; improvements in computing efficiency by weak chains, regular chains, saturated chain were introduced\n\nAn application is an algorithm for solving systems of algebraic equations by means of characteristic sets. More precisely, given a finite subset F of polynomials, there is an algorithm to compute characteristic sets T, ...,\nT such that:\n\nwhere W(T) is the difference of V(T) and V(h), here h is the product of initials of the polynomials in T.\n\n\n\n"}
