{"id": "12267953", "url": "https://en.wikipedia.org/wiki?curid=12267953", "title": "6-sphere coordinates", "text": "6-sphere coordinates\n\nIn mathematics, 6-sphere coordinates are the coordinate system created by inverting the Cartesian coordinates across the unit sphere. They are so named because the loci where one coordinate is constant form spheres tangent to the origin from one of six sides (depending on which coordinate is held constant and whether its value is positive or negative).\n\nThe three coordinates are\nSince inversion is its own inverse, the equations for \"x\", \"y\", and \"z\" in terms of \"u\", \"v\", and \"w\" are similar:\nThis coordinate system is formula_3-separable for the 3-variable Laplace equation.\n\n\n"}
{"id": "54426651", "url": "https://en.wikipedia.org/wiki?curid=54426651", "title": "Ackermann's formula", "text": "Ackermann's formula\n\nAckermann's formula is a control system design method for solving the pole allocation problem. One of the primary problems in control system design is the creation of controllers that will alter the dynamics of a system and alter the poles to a more suitable, and sometimes more stable, state. Such a problem can be tackled by many different methods; one such solution is the addition of a feedback loop in such a way that a gain is added to the input with which one can change the poles of the original system. If the system is controllable, an efficient method for pole placement is Ackermann's formula, which allows one to choose arbitrary poles within the system. \n\nConsider a linear time invariant system with a state-space representation\n\nwhere formula_3 is the state vector, formula_4 is the input vector, and formula_5 are matrices of compatible dimensions that represent the dynamics of our system, and, for simplicity's sake, assume formula_6. An input-output description of this system is given by the transfer function\n\nSince formula_8, the denominator of the system is given by the characteristic polynomial of formula_9. Thus, the poles of the system are the eigenvalues of formula_10.\n\nIf the system is unstable, or has a slow response or any other characteristic that does not specify the design criteria, it could be advantageous to make changes to it. The realization given by formula_11, however, represents the dynamics of the system, and sometimes cannot be altered. Thus, one approach to this problem might be to create a feedback loop with a gain formula_12 that will feed the state variable into the input.\n\nIf the system is controllable, there is always an input formula_13 such that any state formula_14 can be transferred to any other state formula_15. With that in mind, a feedback loop can be added to the system with the control input formula_16, such that the new dynamics of the system will be \n\nIn this new realization, the poles will be dependent on the characteristic polynomial formula_19 of formula_20, that is \n\nComputing the characteristic polynomial and choosing a suitable feedback matrix can be a challenging task, especially in larger systems. One way to make computations easier is through Ackermann's formula. For simplicity's sake, consider a single input vector with no reference parameter formula_22, such as \n\nwhere formula_25 is a feedback vector of compatible dimensions. Given that the system is still controllable, Ackermann's method states that the design process can be simplified by the following equation: \n\nin which formula_27 is the desired characteristic polynomial evaluated at matrix formula_10. \n\nAssume that the system is controllable. Defining formula_29 gives\n\nCalculating the powers of formula_31 results in \n\nReplacing the previous equations into formula_36 yields \n\nNoting that formula_39.\n\nRewriting the above equation as a matrix product and omitting the terms that formula_25 does not appear isolated gives\n\nFrom the Cayley–Hamilton theorem, formula_43, thus\n\nformula_44\n\nNote that formula_45 is the controllability matrix of the system. Since the system is controllable, formula_46 is invertible. Thus, \n\nTo find formula_48, both sides can be multiplied by the vector formula_49. This yields \n\nThus,\n\nConsider\n\nformula_52\n\nWe know from the characteristic polynomial of formula_10 that the system is unstable since formula_54, the matrix formula_10 will only have positive eigenvalues. Thus, to stabilize the system we shall put a feedback gain formula_56\n\nFrom Ackermann's formula, we can find a matrix formula_57 that will change the system so that its characteristic equation will be equal to a desired polynomial. Suppose we want formula_58.\n\nThus, formula_59 and computing the controllability matrix yields\n\nAlso, we have that formula_62\n\nFinally, from Ackermann's formula\n"}
{"id": "30625626", "url": "https://en.wikipedia.org/wiki?curid=30625626", "title": "Acta Mathematica Hungarica", "text": "Acta Mathematica Hungarica\n\nThis journal is indexed by the following services:\n\n"}
{"id": "49266213", "url": "https://en.wikipedia.org/wiki?curid=49266213", "title": "Andrew Guinand", "text": "Andrew Guinand\n\nAndrew Paul Guinand (known as Andrew Guinand), was an Australian mathematician and a professor at the University of New England.\n\nGuinand attended St Peter's College, Adelaide from 1924 to 1929. In 1930, he entered St Mark's College of the University of Adelaide to study mathematics. After graduating in 1933, Guinand attended the University of Oxford on a Rhodes Scholarship, where Edward Charles Titchmarsh supervised his doctoral research. From 1937 to 1938, he studied at Göttingen, after which he studied at Princeton University, until in 1940 he joined the Royal Canadian Air Force and served as a navigator.\n\nGuinand worked as an assistant at the University of Cambridge, and then as a lecturer at the Royal Military College of Science, where he was eventually promoted to Associate professor of Mathematics. In 1955, he was named Head of Department at the University of New England at Armidale. After two years at Armidale, he moved to the University of Alberta in Edmonton, Alberta, and in 1960 moved again to the University of Saskatchewan. In 1964, he moved once more, becoming the first chairman of the mathematics department at Trent University which had been founded the previous year.\n\nGuinand's research included work in number theory (particularly prime numbers and the Riemann hypothesis), as well as generalizations of the Fourier transform, in addition to publications on assorted topics including air navigation and the computation of pi.\n\nIn 1959, he published a paper on the Poisson summation formula for which he presented a simpler solution. Guinand's work on this problem was largely forgotten and remained in obscurity until it was re-discovered by Yves Meyer in 2015.\n\n"}
{"id": "23986673", "url": "https://en.wikipedia.org/wiki?curid=23986673", "title": "Bonse's inequality", "text": "Bonse's inequality\n\nIn number theory, Bonse's inequality, named after H. Bonse, relates the size of a primorial to the smallest prime that does not appear in its prime factorization. It states that if \"p\", ..., \"p\", \"p\" are the smallest \"n\" + 1 prime numbers and \"n\" ≥ 4, then\n\n"}
{"id": "5615284", "url": "https://en.wikipedia.org/wiki?curid=5615284", "title": "Bretschneider's formula", "text": "Bretschneider's formula\n\nIn geometry, Bretschneider's formula is the following expression for the area of a general quadrilateral:\nHere, , , , are the sides of the quadrilateral, is the semiperimeter, and and are two opposite angles.\n\nBretschneider's formula works on any quadrilateral, whether it is cyclic or not.\n\nThe German mathematician Carl Anton Bretschneider discovered the formula in 1842. The formula was also derived in the same year by the German mathematician Karl Georg Christian von Staudt.\n\nDenote the area of the quadrilateral by . Then we have\n\nTherefore\n\nThe law of cosines implies that\nbecause both sides equal the square of the length of the diagonal . This can be rewritten as\n\nAdding this to the above formula for yields\n\nNote that: formula_9 (a trigonometric identity true for all formula_10)\n\nFollowing the same steps as in Brahmagupta's formula, this can be written as\n\nIntroducing the semiperimeter\nthe above becomes\n\nand Bretschneider's formula follows after taking the square root of both sides:\n\nBretschneider's formula generalizes Brahmagupta's formula for the area of a cyclic quadrilateral, which in turn generalizes Heron's formula for the area of a triangle.\n\nThe trigonometric adjustment in Bretschneider's formula for non-cyclicality of the quadrilateral can be rewritten non-trigonometrically in terms of the sides and the diagonals and to give\n\n\n"}
{"id": "51399", "url": "https://en.wikipedia.org/wiki?curid=51399", "title": "Buckingham π theorem", "text": "Buckingham π theorem\n\nIn engineering, applied mathematics, and physics, the Buckingham theorem is a key theorem in dimensional analysis. It is a formalization of Rayleigh's method of dimensional analysis. Loosely, the theorem states that if there is a physically meaningful equation involving a certain number \"n\" of physical variables, then the original equation can be rewritten in terms of a set of \"p\" = \"n\" − \"k\" dimensionless parameters , , ..., constructed from the original variables. (Here \"k\" is the number of physical dimensions involved; it is obtained as the rank of a particular matrix.)\n\nThe theorem provides a method for computing sets of dimensionless parameters from the given variables, or nondimensionalization, even if the form of the equation is still unknown.\n\nAlthough named for Edgar Buckingham, the theorem was first proved by French mathematician Joseph Bertrand in 1878. Bertrand considered only special cases of problems from electrodynamics and heat conduction, but his article contains, in distinct terms, all the basic ideas of the modern proof of the theorem and clearly indicates the theorem's utility for modelling physical phenomena. The technique of using the theorem (“the method of dimensions”) became widely known due to the works of Rayleigh (the first application of the theorem \"in the general case\" to the dependence of pressure drop in a pipe upon governing parameters probably dates back to 1892, a heuristic proof with the use of series expansions, to 1894).\n\nFormal generalization of the theorem for the case of arbitrarily many quantities was given first by A. Vaschy in 1892, then in 1911—apparently independently—by both A. Federman and D. Riabouchinsky, and again in 1914 by Buckingham. It was Buckingham's article that introduced the use of the symbol \"\" for the dimensionless variables (or parameters), and this is the source of the theorem's name.\n\nMore formally, the number of dimensionless terms that can be formed, \"p\", is equal to the nullity of the dimensional matrix, and \"k\" is the rank. For experimental purposes, different systems that share the same description in terms of these dimensionless numbers are equivalent.\n\nIn mathematical terms, if we have a physically meaningful equation such as\n\nwhere the \"q\" are the \"n\" physical variables, and they are expressed in terms of \"k\" independent physical units, then the above equation can be restated as\n\nwhere the are dimensionless parameters constructed from the \"q\" by \"p\" = \"n\" − \"k\" dimensionless equations — the so-called \"Pi groups\" — of the form\n\nwhere the exponents \"a\" are rational numbers (they can always be taken to be integers by redefining as being raised to a power that clears all denominators).\n\nThe Buckingham theorem provides a method for computing sets of dimensionless parameters from given variables, even if the form of the equation remains unknown. However, the choice of dimensionless parameters is not unique; Buckingham's theorem only provides a way of generating sets of dimensionless parameters and does not indicate the most \"physically meaningful\".\n\nTwo systems for which these parameters coincide are called \"similar\" (as with similar triangles, they differ only in scale); they are equivalent for the purposes of the equation, and the experimentalist who wants to determine the form of the equation can choose the most convenient one. Most importantly, Buckingham's theorem describes the relation between the number of variables and fundamental dimensions.\n\nIt will be assumed that the space of fundamental and derived physical units forms a vector space over the rational numbers, with the fundamental units as basis vectors, and with multiplication of physical units as the \"vector addition\" operation, and raising to powers as the \"scalar multiplication\" operation:\nrepresent a dimensional variable as the set of exponents needed for the fundamental units (with a power of zero if the particular fundamental unit is not present). For instance, the standard gravity \"g\" has units of formula_4 (distance over time squared), so it is represented as the vector formula_5 with respect to the basis of fundamental units (distance, time).\n\nMaking the physical units match across sets of physical equations can then be regarded as imposing linear constraints in the physical-units vector space.\n\nGiven a system of \"n\" dimensional variables (with physical dimensions) in \"k\" fundamental (basis) dimensions, write the \"dimensional matrix\" \"M\", whose rows are the fundamental dimensions and whose columns are the dimensions of the variables: the (\"i\", \"j\")th entry is the power of the \"i\"th fundamental dimension in the \"j\"th variable. The matrix can be interpreted as taking in a combination of the dimensions of the variable quantities and giving out the dimensions of this product in fundamental dimensions. So\nis the units of\n\nA dimensionless variable is a quantity with fundamental dimensions raised to the zeroth power (the zero vector of the vector space over the fundamental dimensions), which is equivalent to the kernel of this matrix.\n\nBy the rank–nullity theorem, a system of \"n\" vectors (matrix columns) in \"k\" linearly independent dimensions (the rank of the matrix is the number of fundamental dimensions) leaves a nullity, p, satisfying (\"p\" = \"n\" − \"k\"), where the nullity is the number of extraneous dimensions which may be chosen to be dimensionless.\n\nThe dimensionless variables can always be taken to be integer combinations of the dimensional variables (by clearing denominators). There is mathematically no natural choice of dimensionless variables; some choices of dimensionless variables are more physically meaningful, and these are what are ideally used.\n\nThis example is elementary but serves to demonstrate the procedure.\n\nSuppose a car is driving at 100 km/h; how long does it take to go 200 km?\n\nThis question considers three dimensioned variables: distance \"d\", time \"t\", and velocity \"v\", and we are seeking some law of the form . These variables admit a basis of two dimensions: time dimension \"T\" and distance dimension \"D\". Thus there is 3 − 2 = 1 dimensionless quantity.\n\nThe dimensional matrix is\n\nin which the rows correspond to the basis dimensions \"D\" and \"T\", and the columns to the considered dimensions \"D\", \"T\", and \"V\", where the latter stands for the velocity dimension. The elements of the matrix correspond to the powers to which the respective dimensions are to be raised. For instance, the third column (1, −1), states that , represented by the column vector formula_9, is expressible in terms of the basis dimensions as formula_10, since formula_11.\n\nFor a dimensionless constant formula_12, we are looking for vectors formula_13 such that the matrix-vector product \"M\"a equals the zero vector [0,0]. In linear algebra, the set of vectors with this property is known as the kernel (or nullspace) of (the linear map represented by) the dimensional matrix. In this particular case its kernel is one-dimensional. The dimensional matrix as written above is in reduced row echelon form, so one can read off a non-zero kernel vector to within a multiplicative constant:\n\nIf the dimensional matrix were not already reduced, one could perform Gauss–Jordan elimination on the dimensional matrix to more easily determine the kernel. It follows that the dimensionless constant, replacing the dimensions by the corresponding dimensioned variables, may be written:\n\nSince the kernel is only defined to within a multiplicative constant, the above dimensionless constant raised to any arbitrary power yields another (equivalent) dimensionless constant.\n\nDimensional analysis has thus provided a general equation relating the three physical variables:\n\nor, letting formula_17 denote a zero of function formula_18,\n\nwhich can be written as\n\nThe actual relationship between the three variables is simply formula_21. In other words, in this case formula_18 has one physically relevant root, and it is unity. The fact that only a single value of \"C\" will do and that it is equal to 1 is not revealed by the technique of dimensional analysis.\n\nWe wish to determine the period \"T\" of small oscillations in a simple pendulum. It will be assumed that it is a function of the length \"L\", the mass \"M\", and the acceleration due to gravity on the surface of the Earth \"g\", which has dimensions of length divided by time squared. The model is of the form\n\nThere are 3 fundamental physical dimensions in this equation: time formula_24, mass formula_25, and length formula_26, and 4 dimensional variables, \"T\", \"M\", \"L\", and \"g\". Thus we need only 4 − 3 = 1 dimensionless parameter, denoted π, and the model can be re-expressed as\n\nwhere π is given by\n\nfor some values of \"a\", ..., \"a\".\n\nThe dimensions of the dimensional quantities are:\n\nThe dimensional matrix is:\n\nWe are looking for a kernel vector \"a\" = [\"a\", \"a\", \"a\", \"a\"] such that the matrix product of \"M\" on \"a\" yields the zero vector [0,0,0]. The dimensional matrix as written above is in reduced row echelon form, so one can read off a kernel vector within a multiplicative constant:\n\nWere it not already reduced, one could perform Gauss–Jordan elimination on the dimensional matrix to more easily determine the kernel. It follows that the dimensionless constant may be written:\n\nIn fundamental terms:\n\nwhich is dimensionless. Since the kernel is only defined to within a multiplicative constant, if the above dimensionless constant is raised to any arbitrary power, it will yield another equivalent dimensionless constant.\n\nThis example is easy because three of the dimensional quantities are fundamental units, so the last (\"g\") is a combination of the previous. Note that if \"a\" were non-zero, there would be no way to cancel the \"M\" value; therefore \"a\" \"must\" be zero. Dimensional analysis has allowed us to conclude that the period of the pendulum is not a function of its mass. (In the 3D space of powers of mass, time, and distance, we can say that the vector for mass is linearly independent from the vectors for the three other variables. Up to a scaling factor, formula_37 is the only nontrivial way to construct a vector of a dimensionless parameter.)\n\nThe model can now be expressed as:\n\nAssuming the zeroes of \"f\" are discrete, we can say \"gT\"/\"L\" = \"C\", where \"C\" is the \"n\"th zero of the function \"f\". If there is only one zero, then \"gT\"/\"L\" = \"C\". It requires more physical insight or an experiment to show that there is indeed only one zero and that the constant is in fact given by \"C\" = 4π.\n\nFor large oscillations of a pendulum, the analysis is complicated by an additional dimensionless parameter, the maximum swing angle. The above analysis is a good approximation as the angle approaches zero.\n\nA simple example of dimensional analysis can be found for the case of the mechanics of a thin, solid and parallel-sided rotating disc. There are five variables involved which reduce to two non-dimensional groups. The relationship between these can be determined by numerical experiment using, for example, the finite element method.\n\n\n\n\n"}
{"id": "1197057", "url": "https://en.wikipedia.org/wiki?curid=1197057", "title": "CellML", "text": "CellML\n\nCellML is an XML based markup language for describing mathematical models. Although it could theoretically describe any mathematical model, it was originally created with the Physiome Project in mind, and hence used primarily to describe models relevant to the field of biology. This is reflected in its name CellML, although this is simply a name, not an abbreviation. CellML is growing in popularity as a portable description format for computational models, and groups throughout the world are using CellML for modelling or developing software tools based on CellML. CellML is similar to Systems Biology Markup Language SBML but provides greater scope for model modularity and reuse, and is not specific to descriptions of biochemistry.\n\nThe CellML language grew from a need to share models of cardiac cell dynamics among researchers at a number of sites across the world. The original working group formed in 1998 consisted of David Bullivant, Warren Hedley, and Poul Nielsen; all three were at that time members of the Department of Engineering Science at the University of Auckland. The language was an application of the XML specification developed by the World Wide Web Consortium – the decision to use XML was based on late 1998 recommendations from Warren Hedley and André (David) Nickerson. Existing XML-based languages were leveraged to describe the mathematics (content MathML), metadata (RDF), and links between resources (XLink). The CellML working group first became aware of the SBML effort in late 2000, when Warren Hedley attended the 2nd workshop on Software Platforms for Systems Biology in Tokyo.\n\nThe working group collaborated with a number of researchers at Physiome Sciences Inc. (particularly Melanie Nelson, Scott Lett, Mark Grehlinger, Prasad Ramakrishna, Jeremy Rice, Adam Muzikant, and Kam-Chuen Jim) to draft the initial CellML 1.0 specification, which was published on the 11th of August 2001. This first draft was followed by specifications for CellML Metadata and an update to CellML to accommodate structured nesting of models with the addition of the <import> element. Physiome Sciences Inc. also produced the first CellML capable software. The National Resource for Cell Analysis and Modeling (NRCAM) at the University of Connecticut Health Center also produced early CellML capable software called Virtual Cell.\n\nIn 2002 the CellML 1.1 specification was written, in which imports were added. Imports provide the ability to incorporate external components into a model, enabling modular modelling. This specification was frozen in early 2006. Work has continued on metadata and other specifications.\n\nIn July 2009 the CellML website was completely revamped, and an initial version of the new CellML repository software (PMR2) was released.\n\nA CellML model consists of a number of components, each described in their own component element. A component can be an entirely conceptual entity created for modelling convenience, or it can have some real physical interpretation (for example, it could represent the cell membrane).\n\nEach component contains a number of variables, which must be declared by placing a variable element inside the component. For example, a component representing a cell membrane may have a variable called V representing the potential difference (voltage) across the cell membrane.\n\nMathematical relationships between variables are expressed within components, using MathML. MathML is used to make declarative expressions (as opposed to procedural statements as in a computer programming language). However, most CellML processing software will only accept a limited of range of mathematics (for example, some processing software requires equations with a single variable on one side of an equality). The choice of MathML makes CellML particularly suited for describing models containing differential equations. There is no mechanism for the expression of stochastic models or any other form of randomness.\n\nComponents can be connected in other components using a connection element, which describes the name of two components to be connected, and the variables in the first component which are mapped to variables in the second component. Such connections are a statement that the variable in one component is equivalent to another variable in another component.\n\nCellML models also allow relationships between components to be expressed. The CellML specification defines two types of relationship, encapsulation and containment, however more can be defined by the user. The containment relationship is used to express that one component is physically within another. The encapsulation relationship is special because it is the only relationship that affects the interpretation of the rest of the model. The effect of encapsulation is that components encapsulated beneath other components are private and cannot be accessed except by the component directly above in the encapsulation tree. The modeller is free to use encapsulation as a conceptual tool, and it does not necessarily have any physical interpretation.\n\nCellML is defined by core specifications as well as additional specifications for metadata, used to annotate models and specify simulations.\n\nCellML 1.0 was the first final specification, and is used to describe many of the models in the CellML Model Repository.\n\nCellML 1.0 has some biochemistry specific elements for describing the role of variables in a reaction model.\n\nCellML 1.1 introduced the ability to import components and units. In order to fully support this feature, variables in CellML 1.1 accept variable names as initial values.\n\nCellML has several metadata specifications, used to annotate models or provide information for running and/or visualizing simulations of models.\n\nCellML.org aims to provide a focal point for the CellML community. Members can submit, review, and update models and receive feedback and help from the community. A CellML discussion mailing list can be found at CellML-discussion mailing list. The scope of this mailing list includes everything related to the development and use of CellML.\n\nA repository of several hundred biological models encoded into CellML can be found on the CellML community website at CellML Model Repository. These models are actively undergoing a curation process aiming to provide annotations with biological ontologies such as Gene Ontology and to validate the models against standards of unit balance and biophysical constrains such as conservation of mass, charge, energy etc.\n\n\n"}
{"id": "452085", "url": "https://en.wikipedia.org/wiki?curid=452085", "title": "Communication Theory of Secrecy Systems", "text": "Communication Theory of Secrecy Systems\n\n\"Communication Theory of Secrecy Systems\" is a paper published in 1949 by Claude Shannon discussing cryptography from the viewpoint of information theory. It is one of the foundational treatments (arguably \"the\" foundational treatment) of modern cryptography. It is also a proof that all theoretically unbreakable ciphers must have the same requirements as the one-time pad.\n\nShannon published an earlier version of this research in the classified report \"A Mathematical Theory of Cryptography,\" Memorandum MM 45-110-02, Sept. 1, 1945, Bell Laboratories. This classified report also precedes the publication of his \"A Mathematical Theory of Communication\", which appeared in 1948.\n\n\n\n"}
{"id": "391421", "url": "https://en.wikipedia.org/wiki?curid=391421", "title": "Comparametric equation", "text": "Comparametric equation\n\nA comparametric equation is an equation that describes a parametric relationship between a function and a dilated version of the same function, where the equation does not involve the parameter. For example, \"ƒ\"(2\"t\") = 4\"ƒ\"(\"t\") is a comparametric equation, when we define \"g\"(\"t\") = \"ƒ\"(2\"t\"), so that we have \"g\" = 4\"ƒ\" no longer contains the parameter, \"t\". The comparametric equation \"g\" = 4\"ƒ\" has a family of solutions, one of which is \"ƒ\" = \"t\".\nTo see that \"ƒ\" = \"t\" is a solution, we merely substitute back in: \"g\" = \"ƒ\"(2\"t\") = (2\"t\") = 4\"t\" = 4\"ƒ\", so that \"g\" = 4\"ƒ\".\n\nComparametric equations arise naturally in signal processing when we have multiple measurements of the same phenomenon, in which each of the measurements was acquired using a different sensitivity. For example, two or more differently exposed pictures of the same subject matter give rise to a comparametric relationship, the solution of which is the response function of the camera, image sensor, or imaging system.\n\nComparametric equations have been used in many areas of research, and have many practical applications to the real world. They are used in radar, microphone arrays, and have been used in processing crime scene video in homicide trials in which the only evidence against the accused was video recordings of the murder.\n\nAn existing solution is Comparametric Camera Respnose Function (CCRF) for real-time comparametric analysis. It has applications in the analysis of multiple images. \n\n"}
{"id": "38635109", "url": "https://en.wikipedia.org/wiki?curid=38635109", "title": "Complexification (Lie group)", "text": "Complexification (Lie group)\n\nIn mathematics, the complexification or universal complexification of a real Lie group is given by a continuous homomorphism of the group into a complex Lie group with the universal property that every continuous homomorphism of the original group into another complex Lie group extends compatibly to a complex analytic homomorphism between the complex Lie groups. The complexification, which always exists, is unique up to isomorphism. Its Lie algebra is a quotient of the complexification of the Lie algebra of the original group. They are isomorphic if the original group has a quotient by a discrete normal subgroup which is linear.\n\nFor compact Lie groups, the complexification, sometimes called the Chevalley complexification after Claude Chevalley, can be defined as the group of complex characters of the Hopf algebra of representative functions, i.e. the matrix coefficients of finite-dimensional representations of the group. In any finite-dimensional faithful unitary representation of the compact group it can be realized concretely as a closed subgroup of the complex general linear group. It consists of operators with polar decomposition , where is a unitary operator in the compact group and is a skew-adjoint operator in its Lie algebra. In this case the complexification is a complex algebraic group and its Lie algebra is the complexification of the Lie algebra of the compact Lie group.\n\nIf is a Lie group, a universal complexification is given by a complex Lie group and a continuous homomorphism with the universal property that, if is an arbitrary continuous homomorphism into a complex Lie group , then there is a unique complex analytic homomorphism such that . \n\nUniversal complexifications always exist and are unique up to a unique complex analytic isomorphism (preserving inclusion of the original group).\n\nIf is connected with Lie algebra , then its universal covering group is simply connected. Let be the simply connected complex Lie group with Lie algebra . Let be the natural homomorphism and the natural covering map. Then given a homomorphism , there is a unique complex analytic homomorphism such that . Let be the intersection of the kernels of the homomorphisms as varies over all possibilities. Then is a closed normal complex Lie subgroup of and the quotient group is a universal complexification. In particular if is simply connected, its universal complexification is just .\n\nFor non-connected Lie groups with identity component and component group , the extension\n\ninduces an extension\n\nand the complex Lie group is a complexification of .\n\nThe universal property implies that the universal complexification is unique up to complex analytic isomorphism.\n\nIf the original group is linear, so too is the universal complexification and the homomorphism between the two is an inclusion. give an example of a connected real Lie group for which the homomorphism is not injective even at the Lie algebra level: they take the product of by the universal covering group of and quotient out by the discrete cyclic subgroup generated by an irrational rotation in the first factor and a generator of the center in the second.\n\nIf is a compact Lie group, the *-algebra of matrix coefficients of finite-dimensional unitary representations is a uniformly dense *-subalgebra of , the *-algebra of complex-valued continuous functions on . It is naturally a Hopf algebra with comultiplication given by\n\nThe characters of are the *-homomorphisms of into . They can be identified with the point evaluations for in and the comultiplication allows the group structure on to be recovered. The homomorphisms of into also form a group. It is a complex Lie group and can be identified with the complexification of . The *-algebra is generated by the matrix coefficients of any faithful representation of . It follows that defines a faithful complex analytic representation of .\n\nThe original approach of to the complexification of a compact Lie group can be concisely stated within the language of classical invariant theory, described in . Let be a closed subgroup of the unitary group where is a finite-dimensional complex inner product space. Its Lie algebra consists of all skew-adjoint operators such that lies in for all real . Set with the trivial action of on the second summand. The group acts on , with an element acting as . The commutant (or centralizer algebra) is denoted by . It is generated as a *-algebra by its unitary operators and its commutant is the *-algebra spanned by the operators . The complexification of consists of all operators in such that commutes with and acts trivially on the second summand in . By definition it is a closed subgroup of . The defining relations (as a commutant) show that is an algebraic subgroup. Its intersection with coincides with , since it is \"a priori\" a larger compact group for which the irreducible representations stay irreducible and inequivalent when restricted to . Since is generated by unitaries, an invertible operator lies in if the unitary operator and positive operator in its polar decomposition both lie in . Thus lies in and the operator can be written uniquely as with a self-adjoint operator. By the functional calculus for polynomial functions it follows that lies in the commutant of if with in . In particular taking purely imaginary, must have the form with in the Lie algebra of . Since every finite-dimensional representation of occurs as a direct summand of , it is left invariant by and thus every finite-dimensional representation of extends uniquely to . The extension is compatible with the polar decomposition. Finally the polar decomposition implies that is a maximal compact subgroup of , since a strictly larger compact subgroup would contain all integer powers of a positive operator , a closed infinite discrete subgroup.\n\nThe decomposition derived from the polar decomposition\n\nwhere is the Lie algebra of , is called the Cartan decomposition of . The exponential factor is invariant under conjugation by but is not a subgroup. The complexification is invariant under taking adjoints, since consists of unitary operators and of positive operators.\n\nThe Gauss decomposition is a generalization of the LU decomposition for the general linear group and a specialization of the Bruhat decomposition. For it states that with respect to a given orthonormal basis an element of can be factorized in the form\n\nwith lower unitriangular, upper unitriangular and diagonal if and only if all the principal minors of are non-vanishing. In this case and are uniquely determined.\n\nIn fact Gaussian elimination shows there is a unique such that is upper triangular.\n\nThe upper and lower unitriangular matrices, and , are closed unipotent subgroups of GL(\"V\"). Their Lie algebras consist of upper and lower strictly triangular matrices. The exponential mapping is a polynomial mapping from the Lie algebra to the corresponding subgroup by nilpotence. The inverse is given by the logarithm mapping which by unipotence is also a polynomial mapping. In particular there is a correspondence between closed connected subgroups of and subalgebras of their Lie algebras. The exponential map is onto in each case, since the polynomial function lies in a given Lie subalgebra if and do and are sufficiently small.\n\nThe Gauss decomposition can be extended to complexifications of other closed connected subgroups of by using the root decomposition to write the complexified Lie algebra as\n\nwhere is the Lie algebra of a maximal torus of and are the direct sum of the corresponding positive and negative root spaces. In the weight space decomposition of as eigenspaces of acts as diagonally, acts as lowering operators and as raising operators. are nilpotent Lie algebras acting as nilpotent operators; they are each other's adjoints on . In particular acts by conjugation of , so that is a semidirect product of a nilpotent Lie algebra by an abelian Lie algebra.\n\nBy Engel's theorem, if is a semidirect product, with abelian and nilpotent, acting on a finite-dimensional vector space with operators in diagonalizable and operators in nilpotent, there is a vector that is an eigenvector for and is annihilated by . In fact it is enough to show there is a vector annihilated by , which follows by induction on , since the derived algebra annihilates a non-zero subspace of vectors on which and act with the same hypotheses.\n\nApplying this argument repeatedly to shows that there is an orthonormal basis of consisting of eigenvectors of with acting as upper triangular matrices with zeros on the diagonal.\n\nIf and are the complex Lie groups corresponding to and , then the Gauss decomposition states that the subset\n\nis a direct product and consists of the elements in for which the principal minors are non-vanishing. It is open and dense. Moreover, if denotes the maximal torus in ,\n\nThese results are an immediate consequence of the corresponding results for .\n\nIf denotes the Weyl group of and denotes the Borel subgroup , the Gauss decomposition is also a consequence of the more precise Bruhat decomposition\n\ndecomposing into a disjoint union of double cosets of . The complex dimension of a double coset is determined by the length of as an element of . The dimension is maximized at the Coxeter element and gives the unique open dense double coset. Its inverse conjugates into the Borel subgroup of lower triangular matrices in </sub>.\n\nThe Bruhat decomposition is easy to prove for . Let be the Borel subgroup of upper triangular matrices and the subgroup of diagonal matrices. So . For in , take in so that maximizes the number of zeros appearing at the beginning of its rows. Because a multiple of one row can be added to another, each row has a different number of zeros in it. Multiplying by a matrix in , it follows that lies in . For uniqueness, if , then the entries of vanish below the diagonal. So the product lies in , proving uniqueness.\n\nThe group has a natural filtration by normal subgroups with zeros in the first superdiagonals and the successive quotients are Abelian. Defining and to be the intersections with , it follows by decreasing induction on that . Indeed, and are specified in by the vanishing of complementary entries on the th superdiagonal according to whether preserves the order or not.\n\nThe Bruhat decomposition for the other classical simple groups can be deduced from the above decomposition using the fact that they are fixed point subgroups of folding automorphisms of . For , let be the matrix with 's on the antidiagonal and 's elsewhere and set\n\nThen is the fixed point subgroup of the involution . It leaves the subgroups and invariant. If the basis elements are indexed by , then the Weyl group of consists of satisfying\n, i.e. commuting with . Analogues of and are defined by intersection with , i.e. as fixed points of . The uniqueness of the decomposition implies the Bruhat decomposition for .\n\nThe same argument works for . It can be realised as the fixed points of in where .\n\nThe Iwasawa decomposition\n\ngives a decomposition for for which, unlike the Cartan decomposition, the direct factor is a closed subgroup, but it is no longer invariant under conjugation by . It is the semidirect product of the nilpotent subgroup by the Abelian subgroup .\n\nFor and its complexification , this decomposition can be derived as a restatement of the Gram–Schmidt orthonormalization process.\n\nIn fact let be an orthonormal basis of and let be an element in . Applying the Gram–Schmidt process to , there is a unique orthonormal basis and positive constants such that\n\nSince the decomposition is direct for , it is enough to check that . From the properties of the Iwasawa decomposition for , the map is a diffeomorphism onto its image in , which is closed. On the other hand, the dimension of the image is the same as the dimension of , so it is also open. So because is connected.\n\nThe Iwasawa decomposition can be used to describe complex structures on the s in complex projective space of highest weight vectors of finite-dimensional irreducible representations of . In particular the identification between and can be used to formulate the Borel–Weil theorem. It states that each irreducible representation\nof can be obtained by holomorphic induction from a character of , or equivalently that it is realized in the space of sections of a holomorphic line bundle on .\n\nThe closed connected subgroups of containing are described by Borel–de Siebenthal theory. They are exactly the centralizers of tori . Since every torus is generated topologically by a single element , these are the same as centralizers of elements in . By a result of Hopf is always connected: indeed any element is along with contained in some maximal torus, necessarily contained in .\n\nGiven an irreducible finite-dimensional representation with highest weight vector of weight , the stabilizer of in is a closed subgroup . Since is an eigenvector of , contains . The complexification also acts on and the stabilizer is a closed complex subgroup containing . Since is annihilated by every raising operator corresponding to a positive root , contains the Borel subgroup . The vector is also a highest weight vector for the copy of corresponding to , so it is annihilated by the lowering operator generating if . The Lie algebra of is the direct sum of and root space vectors annihilating , so that\n\nThe Lie algebra of is given by . By the Iwasawa decomposition . Since fixes , the -orbit of in the complex projective space of coincides with the orbit and\n\nIn particular\n\nUsing the identification of the Lie algebra of with its dual, equals the centralizer of in , and hence is connected. The group is also connected. In fact the space is simply connected,\nsince it can be written as the quotient of the (compact) universal covering group of the compact semisimple group by a connected subgroup, where is the center of . If is the identity component of , has as a covering space, so that . The homogeneous space has a complex structure, because is a complex subgroup. The orbit in complex projective space is closed in the Zariski topology by Chow's theorem, so is a smooth projective variety. The Borel–Weil theorem and its generalizations are discussed in this context in , , and .\n\nThe parabolic subgroup can also be written as a union of double cosets of \n\nwhere is the stabilizer of in the Weyl group . It is generated by the reflections corresponding to the simple roots orthogonal to .\n\nThere are other closed subgroups of the complexification of a compact connected Lie group \"G\" which have the same the complexified Lie algebra. These are the other real forms of \"G\".\n\nIf \"G\" is a simply connected compact Lie group and σ is an automorphism of period 2, then the fixed point subgroup \"K\" = \"G\" is \"automatically connected\". (In fact this is true for any automorphism of \"G\", as shown for inner automorphisms by Steinberg and in general by Borel.) \n\nThis can be seen most directly when the involution σ corresponds to a Hermitian symmetric space. In that case σ is inner and implemented by an element in a one-parameter subgroup exp \"tT\" contained in the center of \"G\". The innerness of σ implies that \"K\" contains a maximal torus of \"G\", so has maximal rank. On the other hand, the centralizer of the subgroup generated by the torus \"S\" of elements exp \"tT\" is connected, since if \"x\" is any element in \"K\" there is a maximal torus containing \"x\" and \"S\", which lies in the centralizer. On the other hand, it contains \"K\" since \"S\" is central in \"K\" and is contained in \"K\" since \"z\" lies in \"S\". So \"K\" is the centralizer of \"S\" and hence connected. In particular \"K\" contains the center of \"G\".\n\nFor a general involution σ, the connectedness of \"G\" can be seen as follows.\n\nThe starting point is the Abelian version of the result: if \"T\" is a maximal torus of a simply connected group \"G\" and σ is an involution leaving invariant \"T\" and a choice of positive roots (or equivalently a Weyl chamber), then the fixed point subgroup \"T\" is connected. In fact the kernel of the exponential map from formula_18 onto \"T\" is a lattice Λ with a Z-basis indexed by simple roots, which σ permutes. Splitting up according to orbits, \"T\" can be written as a product of terms T on which σ acts trivially or terms T where σ interchanges the factors. The fixed point subgroup just corresponds to taking the diagonal subgroups in the second case, so is connected.\n\nNow let \"x\" be any element fixed by σ, let \"S\" be a maximal torus in C(\"x\") and let \"T\" be the identity component of C(\"x\", \"S\"). Then \"T\" is a maximal torus in \"G\" containing \"x\" and \"S\". It is invariant under σ and the identity component of \"T\" is \"S\". In fact since \"x\" and \"S\" commute, they are contained in a maximal torus which, because it is connected, must lie in \"T\". By construction \"T\" is invariant under σ. The identity component of \"T\" contains \"S\", lies in C(\"x\") and centralizes \"S\", so it equals \"S\". But \"S\" is central in \"T\", to \"T\" must be Abelian and hence a maximal torus. For σ acts as multiplication by −1 on the Lie algebra formula_19, so it and therefore also formula_18 are Abelian.\n\nThe proof is completed by showing that σ preserves a Weyl chamber associated with \"T\". For then \"T\" is connected so must equal \"S\". Hence \"x\" lies in \"S\". Since \"x\" was arbitrary, \"G\" must therefore be connected.\n\nTo produce a Weyl chamber invariant under σ, note that there is no root space formula_21 on which both \"x\" and \"S\" acted trivially, for this would contradict the fact that C(\"x\", \"S\") has the same Lie algebra as \"T\". Hence there must be an element \"s\" in \"S\" such that \"t\" = \"xs\" acts non-trivially on each root space. In this case \"t\" is a \"regular element\" of \"T\"—the identity component of its centralizer in \"G\" equals \"T\". There is a unique Weyl alcove \"A\" in formula_18 such that \"t\" lies in exp \"A\" and 0 lies in the closure of \"A\". Since \"t\" is fixed by σ, the alcove is left invariant by σ and hence so also is the Weyl chamber \"C\" containing it.\n\nLet \"G\" be a simply connected compact Lie group with complexification \"G\". The map \"c\"(\"g\") = (\"g\"*) defines an automorphism of \"G\" as a real Lie group with \"G\" as fixed point subgroup. It is conjugate-linear on formula_23 and satisfies \"c\" = id. Such automorphisms of either \"G\" or formula_23 are called conjugations.\nSince \"G\" is also simply connected any conjugation \"c\" on formula_23 corresponds to a unique automorphism \"c\" of \"G\".\n\nThe classification of conjugations \"c\" reduces to that of involutions σ of \"G\" because\ngiven a \"c\" there is an automorphism φ of the complex group \"G\" such that\n\ncommutes with \"c\". The conjugation \"c\" then leaves \"G\" invariant and restricts to an involutive automorphism σ. By simple connectivity the same is true at the level of Lie algebras. At the Lie algebra level \"c\" can be recovered from σ by the formula\n\nfor \"X\", \"Y\" in formula_28.\n\nTo prove the existence of φ let ψ = \"c\"\"c\" an automorphism of the complex group \"G\". On the Lie algebra level it defines a self-adjoint operator for the complex inner product\n\nwhere \"B\" is the Killing form on formula_23. Thus ψ is a positive operator and an automorphism along with all its real powers. In particular take\n\nIt satisfies\n\nFor the complexification \"G\", the Cartan decomposition is described above. Derived from the polar decomposition in the complex general linear group, it gives a diffeomorphism\n\nOn \"G\" there is a conjugation operator \"c\" corresponding to \"G\" as well as an involution σ commuting with \"c\". Let \"c\" = \"c\" σ and let \"G\" be the fixed point subgroup of \"c\". It is closed in the matrix group \"G\" and therefore a Lie group. The involution σ acts on both \"G\" and \"G\". For the Lie algebra of \"G\" there is a decomposition\n\ninto the +1 and −1 eigenspaces of σ. The fixed point subgroup \"K\" of σ in \"G\" is connected since \"G\" is simply connected. Its Lie algebra is the +1 eigenspace formula_35. The Lie algebra of \"G\" is given by\n\nand the fixed point subgroup of σ is again \"K\", so that \"G\" ∩ \"G\" = \"K\". In \"G\", there is a Cartan decomposition\n\nwhich is again a diffeomorphism onto the direct and corresponds to the polar decomposition of matrices.\nIt is the restriction of the decomposition on \"G\". The product gives a diffeomorphism onto a closed subset of \"G\". To check that it is surjective, for \"g\" in \"G\" write \"g\" = \"u\" ⋅ \"p\" with \"u\" in \"G\" and \"p\" in \"P\". Since \"c\" \"g\" = \"g\", uniqueness implies that σ\"u\" = \"u\" and σ\"p\" = \"p\". Hence \"u\" lies in \"K\" and \"p\" in \"P\".\n\nThe Cartan decomposition in \"G\" shows that \"G\" is connected, simply connected and noncompact, because of the direct factor \"P\". Thus \"G\" is a noncompact real semisimple Lie group.\n\nMoreover, given a maximal Abelian subalgebra formula_38 in formula_39, \"A\" = exp formula_38 is a toral subgroup such that σ(\"a\") = \"a\" on \"A\"; and any two such formula_38's are conjugate by an element of \"K\".\nThe properties of \"A\" can be shown directly. \"A\" is closed because the closure of \"A\" is a toral subgroup satisfying σ(\"a\") = \"a\", so its Lie algebra lies in formula_42 and hence equals formula_38 by maximality. \"A\" can be generated topologically by a single element exp \"X\", so formula_38 is the centralizer of \"X\" in formula_42. In the \"K\"-orbit of any element of formula_42 there is an element \"Y\" such that (X,Ad \"k\" Y) is minimized at \"k\" = 1. Setting \"k\" = exp \"tT\" with \"T\" in formula_35, it follows that (\"X\",[\"T\",\"Y\"]) = 0 and hence [\"X\",\"Y\"] = 0, so that \"Y\" must lie in formula_38. Thus formula_42 is the union of the conjugates of formula_38. In particular some conjugate of \"X\" lies in any other choice of formula_38, which centralizes that conjugate; so by maximality the only possibilities are conjugates of formula_38.\n\nA similar statements hold for the action of \"K\" on formula_53 in formula_54. Morevoer, from the Cartan decomposition for \"G\", if \"A\" = exp formula_55, then\n\n\n"}
{"id": "52549872", "url": "https://en.wikipedia.org/wiki?curid=52549872", "title": "Deterministic rendezvous problem", "text": "Deterministic rendezvous problem\n\nThe deterministic rendezvous problem is a variant of the rendezvous problem where the players, or \"robots\", must find each other by following a deterministic sequence of instructions. Although each robot follows the same instruction sequence, a unique label assigned to each robot is used for symmetry breaking. Typically, the robots act synchronously, though non-synchronous versions of the deterministic rendezvous problem exist.\n\nIn the synchronous version of the deterministic rendezvous problem, both robots are initially placed at arbitrary nodes in a finite, connected, undirected graph. The size and structure of the graph is unknown to the robots.\n\nThe information known by a robot is as follows:\n\nTo solve the deterministic rendezvous problem, both robots must be given a sequence of deterministic instructions which allow the robots to use their known information to find each other. The robots are considered to have found each other if they are both occupying the same node in the graph during the same time step.\n\nA number of algorithms exist to solve the deterministic rendezvous problem. Some of the solutions are listed below:\n\nIn 2006, Dessmark et al. presented a solution which solves the deterministic rendezvous problem in time proportional to formula_1, where:\n\nThis solution is not ideal, since \"τ\" is an input parameter of the deterministic rendezvous problem and can therefore be arbitrarily large.\n\nIn 2008, Kowalski and Malinowski presented a solution which solves the problem in formula_2 time. This is a significant improvement, since this time complexity is not dependent on \"τ\". This solution has one major drawback, though. It makes use of \"backtracking\", where the robots must keep track of each edge that they have traversed. This is a drawback because it places assumptions on the structure of the graph (namely, how it is labeled), and the robots' sensory and memory capabilities.\n\nThe solution presented by Ta-Shma and Zwick in 2014 solves the problem in formula_3 time. In addition to the reduced time complexity (which doesn't rely on \"τ\"), this solution also doesn't use backtracking. Instead, it uses the notion of universal traversal sequences to help solve the problem.\n\nA universal traversal sequence is a sequence of instructions comprising a graph traversal for any regular graph with a set number of vertices and for any starting vertex. Since the robots may not be in a regular graph, a universal traversal sequence for \"n\" nodes and degree \"d\" is used, where \"d\" is the maximum degree of the graph. Any instructions in the chosen universal traversal sequence which specify that the robot travels to a neighbor of the current node which doesn't exist (i.e., the current node has degree < \"d\") are ignored. By doing this, all nodes in the graph with degree less than \"d\" are treated as having enough self-loops to bring their total degree up to \"d\", effectively allowing the graph to be treated as regular for the purpose of universal traversals.\n\nThe basic idea of Ta-Shma and Zwick's solution is that if one of the robots completes a complete traversal of the graph while the other robot remains idle, or \"rests\", then the two robots are guaranteed to meet. Since the size of the graph is unknown, the robots run universal traversal sequences for increasing values of \"n\", while periodically resting. Whether a robot rests before or after a traversal depends upon the value of its label.\n\nFor example, one of the robots could run the sequence:\nformula_4\nwhile the other robot runs the sequence:\nformula_5\nwhere U is a universal traversal sequence for a graph with \"i\" nodes, u is the number of steps in that universal traversal sequence, and 0 represents \"k\" steps where the robot rests. The universal traversal sequence for the size of the actual graph will be run by one robot while the other rests for the number of steps in that traversal. However, this only works if the two robots are activated at the same time.\n\nTo cover the case where the robots are activated at different times, the sequence to run will include rest periods of length u after each step (in the traversal and the rest period). For example, one of the robots would run the sequence:\nformula_6\nwhere σ is the \"i\" step of U and π is the \"i\" step of U.\n\nIn order to formally present the sequence that each robot will run, some additional notation is needed. Let:\n\nAssuming that one robot's label is 0 and that the other robot's label is 1, the sequence that each robot would run is:\nformula_10\n\nThe sub-sequence formula_11 is known as a \"block\" and can be rewritten as formula_12.\n\nIf σ = U and m = u, the block can be further simplified to:\nformula_13\nformula_14\nBoth of the above lines are known as \"chunks\". One chunk consists of a universal traversal sequence with interleaved rest periods, while the other chunk is a rest period of length 2\"m\".\n\nIf the robot's label is 0, then each block it runs is equal to:\nformula_15\nIf the label is 1, then each block it runs is equal to:\nformula_16\n\nThe sequence of instructions listed above will allow two robots with labels 0 and 1 to meet after O(\"n\") time steps.\n\nTa-Shma and Zwick show how to extend this solution to allow the robots to meet after only O(\"n\") time steps and how to deal with arbitrary labels (which increases the time until the robots meet to O(\"n\"+\"l\") time steps).\n"}
{"id": "27073288", "url": "https://en.wikipedia.org/wiki?curid=27073288", "title": "Dini continuity", "text": "Dini continuity\n\nIn mathematical analysis, Dini continuity is a refinement of continuity. Every Dini continuous function is continuous. Every Lipschitz continuous function is Dini continuous.\n\nLet formula_1 be a compact subset of a metric space (such as formula_2), and let formula_3 be a function from formula_1 into itself. The modulus of continuity of formula_5 is\n\nThe function formula_5 is called Dini-continuous if\n\nAn equivalent condition is that, for any formula_9,\n\nwhere formula_11 is the diameter of formula_1.\n\n"}
{"id": "39456471", "url": "https://en.wikipedia.org/wiki?curid=39456471", "title": "Driver scheduling problem", "text": "Driver scheduling problem\n\nThe driver scheduling problem (DSP) is type of problem in operations research and theoretical computer science.\n\nThe DSP consists of selecting a set of duties (assignments) for the drivers or pilots of vehicles (e.g., buses, trains, boats, or planes) involved in the transportation of passengers or goods.\n\nThis very complex problem involves several constraints related to labour and company rules and also different evaluation criteria and objectives. Being able to solve this problem efficiently can have a great impact on costs and quality of service for public transportation companies. There is a large number of different rules that a feasible duty might be required to satisfy, such as\nOperations research has provided optimization models and algorithms that lead to efficient solutions for this problem. Among the most common models proposed to solve the DSP are the Set Covering and Set Partitioning Models (SPP/SCP). In the SPP model, each work piece (task) is covered by only one duty. In the SCP model, it is possible to have more than one duty covering a given work piece.\nIn both models, the set of work pieces that needs to be covered is laid out in rows, and the set of previously defined feasible duties available for covering specific work pieces is arranged in columns. The DSP resolution, based on either of these models, is the selection of the set of feasible duties that guarantees that there is one (SPP) or more (SCP) duties covering each work piece while minimizing the total cost of the final schedule.\n"}
{"id": "256363", "url": "https://en.wikipedia.org/wiki?curid=256363", "title": "Experimental mathematics", "text": "Experimental mathematics\n\nExperimental mathematics is an approach to mathematics in which computation is used to investigate mathematical objects and identify properties and patterns. It has been defined as \"that branch of mathematics that concerns itself ultimately with the codification and transmission of insights within the mathematical community through the use of experimental (in either the Galilean, Baconian, Aristotelian or Kantian sense) exploration of conjectures and more informal beliefs and a careful analysis of the data acquired in this pursuit.\"\n\nAs expressed by Paul Halmos: \"Mathematics is not a deductive science—that's a cliché. When you try to prove a theorem, you don't just list the hypotheses, and then start to reason. What you do is trial and error, experimentation, guesswork. You want to find out what the facts are, and what you do is in that respect similar to what a laboratory technician does.\"\n\nMathematicians have always practised experimental mathematics. Existing records of early mathematics, such as Babylonian mathematics, typically consist of lists of numerical examples illustrating algebraic identities. However, modern mathematics, beginning in the 17th century, developed a tradition of publishing results in a final, formal and abstract presentation. The numerical examples that may have led a mathematician to originally formulate a general theorem were not published, and were generally forgotten.\n\nExperimental mathematics as a separate area of study re-emerged in the twentieth century, when the invention of the electronic computer vastly increased the range of feasible calculations, with a speed and precision far greater than anything available to previous generations of mathematicians. A significant milestone and achievement of experimental mathematics was the discovery in 1995 of the Bailey–Borwein–Plouffe formula for the binary digits of π. This formula was discovered not by formal reasoning, but instead\nby numerical searches on a computer; only afterwards was a rigorous proof found.\n\nThe objectives of experimental mathematics are \"to generate understanding and insight; to generate and confirm or confront conjectures; and generally to make mathematics more tangible, lively and fun for both the professional researcher and the novice\".\n\nThe uses of experimental mathematics have been defined as follows:\n\n\nExperimental mathematics makes use of numerical methods to calculate approximate values for integrals and infinite series. Arbitrary precision arithmetic is often used to establish these values to a high degree of precision – typically 100 significant figures or more. Integer relation algorithms are then used to search for relations between these values and mathematical constants. Working with high precision values reduces the possibility of mistaking a mathematical coincidence for a true relation. A formal proof of a conjectured relation will then be sought – it is often easier to find a formal proof once the form of a conjectured relation is known.\n\nIf a counterexample is being sought or a large-scale proof by exhaustion is being attempted, distributed computing techniques may be used to divide the calculations between multiple computers.\n\nFrequent use is made of general mathematical software such as Mathematica, although domain-specific software is also written for attacks on problems that require high efficiency. Experimental mathematics software usually includes error detection and correction mechanisms, integrity checks and redundant calculations designed to minimise the possibility of results being invalidated by a hardware or software error.\n\nApplications and examples of experimental mathematics include:\n\n\n\nSome plausible relations hold to a high degree of accuracy, but are still not true. One example is:\n\nThe two sides of this expression actually differ after the 42nd decimal place.\n\nAnother example is that the maximum height (maximum absolute value of coefficients) of all the factors of \"x\" − 1 appears to be the same as the height of the \"n\"th cyclotomic polynomial. This was shown by computer to be true for \"n\" < 10000 and was expected to be true for all \"n\". However, a larger computer search showed that this equality fails to hold for \"n\" = 14235, when the height of the \"n\"th cyclotomic polynomial is 2, but maximum height of the factors is 3.\n\nThe following mathematicians and computer scientists have made significant contributions to the field of experimental mathematics:\n\n\n"}
{"id": "2132356", "url": "https://en.wikipedia.org/wiki?curid=2132356", "title": "Fixed-point index", "text": "Fixed-point index\n\nIn mathematics, the fixed-point index is a concept in topological fixed-point theory, and in particular Nielsen theory. The fixed-point index can be thought of as a multiplicity measurement for fixed points.\n\nThe index can be easily defined in the setting of complex analysis: Let \"f\"(\"z\") be a holomorphic mapping on the complex plane, and let \"z\" be a fixed point of \"f\". Then the function \"f\"(\"z\") − \"z\" is holomorphic, and has an isolated zero at \"z\". We define the fixed point index of \"f\" at \"z\", denoted \"i\"(\"f\", \"z\"), to be the multiplicity of the zero of the function \"f\"(\"z\") − \"z\" at the point \"z\".\n\nIn real Euclidean space, the fixed-point index is defined as follows: If \"x\" is an isolated fixed point of \"f\", then let \"g\" be the function defined by\n\nThen \"g\" has an isolated singularity at \"x\", and maps the boundary of some deleted neighborhood of \"x\" to the unit sphere. We define \"i\"(\"f\", \"x\") to be the Brouwer degree of the mapping induced by \"g\" on some suitably chosen small sphere around \"x\".\n\nThe importance of the fixed-point index is largely due to its role in the Lefschetz–Hopf theorem, which states:\n\nwhere Fix(\"f\") is the set of fixed points of \"f\", and \"Λ\" is the Lefschetz number of \"f\".\n\nSince the quantity on the left-hand side of the above is clearly zero when \"f\" has no fixed points, the Lefschetz–Hopf theorem trivially implies the Lefschetz fixed point theorem.\n\n"}
{"id": "1103773", "url": "https://en.wikipedia.org/wiki?curid=1103773", "title": "Fractional Fourier transform", "text": "Fractional Fourier transform\n\nIn mathematics, in the area of harmonic analysis, the fractional Fourier transform (FRFT) is a family of linear transformations generalizing the Fourier transform. It can be thought of as the Fourier transform to the \"n\"-th power, where \"n\" need not be an integer — thus, it can transform a function to any \"intermediate\" domain between time and frequency. Its applications range from filter design and signal analysis to phase retrieval and pattern recognition.\n\nThe FRFT can be used to define fractional convolution, correlation, and other operations, and can also be further generalized into the linear canonical transformation (LCT). An early definition of the FRFT was introduced by Condon, by solving for the Green's function for phase-space rotations, and also by Namias, generalizing work of Wiener on Hermite polynomials.\n\nHowever, it was not widely recognized in signal processing until it was independently reintroduced around 1993 by several groups. Since then, there has been a surge of interest in extending Shannon's sampling theorem for signals which are band-limited in the Fractional Fourier domain.\n\nA completely different meaning for \"fractional Fourier transform\" was introduced by Bailey and Swartztrauber as essentially another name for a z-transform, and in particular for the case that corresponds to a discrete Fourier transform shifted by a fractional amount in frequency space (multiplying the input by a linear chirp) and evaluating at a fractional set of frequency points (e.g. considering only a small portion of the spectrum). (Such transforms can be evaluated efficiently by Bluestein's FFT algorithm.) This terminology has fallen out of use in most of the technical literature, however, in preference to the FRFT. The remainder of this article describes the FRFT.\n\nThe continuous Fourier transform formula_1 of a function is a unitary operator of \"L\" that maps the function ƒ to its frequential version ƒ̂ (all expressions are taken in the \"L\" sense, rather than pointwise):\n\nand ƒ is determined by ƒ̂ via the inverse transform formula_3\n\nLet us study its \"n\"-th iterated formula_5 defined by \nformula_6 and formula_7 when \"n\" is a non-negative integer, and formula_8. Their sequence is finite since formula_1 is a 4-periodic automorphism: for every function ƒ, formula_10.\n\nMore precisely, let us introduce the parity operator formula_11 that inverts formula_12, formula_13. Then the following properties hold:\n\nThe FrFT provides a family of linear transforms that further extends this definition to handle non-integer powers of the FT.\n\nNote: some authors write the transform in terms of the \"order \" instead of the \"angle \", in which case the is usually times . Although these two forms are equivalent, one must be careful about which definition the author uses.\n\nFor any real , the -angle fractional Fourier transform of a function ƒ is denoted by formula_16 and defined by\nFormally, this formula is only valid when the input function is in a sufficiently nice space (such as L1 or Schwartz space), and is defined via a density argument, in a way similar to that of the ordinary Fourier transform (see article), in the general case.\n\nIf is an integer multiple of π, then the cotangent and cosecant functions above diverge. However, this can be handled by taking the limit, and leads to a Dirac delta function in the integrand. More directly, since formula_17 \nmust be simply or for an even or odd multiple of respectively.\n\nFor , this becomes precisely the definition of the continuous Fourier transform, and for it is the definition of the inverse continuous Fourier transform.\n\nThe FrFT argument is neither a spatial one nor a frequency . We will see why it can be interpreted as linear combination of both coordinates . When we want to distinguish the -angular fractional domain, we will let formula_18 denote the argument of formula_19.\n\nRemark: with the angular frequency ω convention instead of the frequency one, the FrFT formula is the Mehler kernel,\n\nThe -th order fractional Fourier transform operator, formula_19, has the properties:\n\n\n\n\n\n\n\n\n\n\nThe FrFT is an integral transform\nwhere the α-angle kernel is\n\nHere again the special cases are consistent with the limit behavior when approaches a multiple of .\n\nThe FrFT has the same properties as its kernels :\n\nThere also exist related fractional generalizations of similar transforms such as the discrete Fourier transform.\nThe discrete fractional Fourier transform is defined by Zeev Zalevsky in\n. A quantum algorithm to implement a version of the discrete fractional Fourier transform \nin subpolynomial time is described by Somma.\n\nFractional wavelet transform (FRWT): A generalization of the classical wavelet transform (WT) in the fractional Fourier transform (FRFT) domains. The FRWT is proposed in order to rectify the limitations of the WT and the FRFT. This transform not only inherits the advantages of multiresolution analysis of the WT, but also has the capability of signal representations in the fractional domain which is similar to the FRFT. Compared with the existing FRWT, the FRWT (defined by Shi, Zhang, and Liu 2012) can offer signal representations in the time-fractional-frequency plane.\n\nSee also the chirplet transform for a related generalization of the Fourier transform.\n\nThe Fourier transform is essentially bosonic; it works because it is consistent with the superposition principle and related interference patterns. There is also a fermionic Fourier transform. These have been generalized into a supersymmetric FRFT, and a supersymmetric Radon transform. There is also a fractional Radon transform, a symplectic FRFT, and a symplectic wavelet transform. Because quantum circuits are based on unitary operations, they are useful for computing integral transforms as the latter are unitary operators on a function space. A quantum circuit has been designed which implements the FRFT.\n\nThe usual interpretation of the Fourier transform is as a transformation of a time domain signal into a frequency domain signal. On the other hand, the interpretation of the inverse Fourier transform is as a transformation of a frequency domain signal into a time domain signal. Apparently, fractional Fourier transforms can transform a signal (either in the time domain or frequency domain) into the domain between time and frequency: it is a rotation in the time-frequency domain. This perspective is generalized by the linear canonical transformation, which generalizes the fractional Fourier transform and allows linear transforms of the time-frequency domain other than rotation.\n\nTake the below figure as an example. If the signal in the time domain is rectangular (as below), it will become a sinc function in the frequency domain. But if we apply the fractional Fourier transform to the rectangular signal, the transformation output will be in the domain between time and frequency.\n\nActually, fractional Fourier transform is a rotation operation on the time frequency distribution. From the definition above, for \"α\" = 0, there will be no change after applying fractional Fourier transform, and for \"α\" = \"π\"/2, fractional Fourier transform becomes a Fourier transform, which rotates the time frequency distribution with \"π\"/2. For other value of \"α\", fractional Fourier transform rotates the time frequency distribution according to α. The following figure shows the results of the fractional Fourier transform with different values of \"α\".\n\nFractional Fourier transform can be used in time frequency analysis and DSP. It is useful to filter noise, but with the condition that it does not overlap with the desired signal in the time–frequency domain. Consider the following example. We cannot apply a filter directly to eliminate the noise, but with the help of the fractional Fourier transform, we can rotate the signal (including the desired signal and noise) first. We then apply a specific filter, which will allow only the desired signal to pass. Thus the noise will be removed completely. Then we use the fractional Fourier transform again to rotate the signal back and we can get the desired signal.\n\nFractional Fourier transforms are also used to design optical systems and optimize holographic storage efficiency.\nThus, using just truncation in the time domain, or equivalently low-pass filters in the frequency domain, one can cut out any convex set in time–frequency space; just using time domain or frequency domain methods without fractional Fourier transforms only allow cutting out rectangles parallel to the axes.\n\nOther time-frequency transforms:\n\n\n"}
{"id": "730378", "url": "https://en.wikipedia.org/wiki?curid=730378", "title": "Goddard–Thorn theorem", "text": "Goddard–Thorn theorem\n\nIn mathematics, and in particular, in the mathematical background of string theory, the Goddard–Thorn theorem (also called the no-ghost theorem) is a theorem describing properties of a functor that quantizes bosonic strings. It is named after Peter Goddard and Charles Thorn.\n\nThe name \"no-ghost theorem\" stems from the fact that in the original statement of the theorem, the natural inner product induced on the output vector space is positive definite. Thus, there were no so-called ghosts (Pauli–Villars ghosts), or vectors of negative norm. The name \"no-ghost theorem\" is also a word play on the no-go theorem of quantum mechanics.\n\nThere are two naturally isomorphic functors that are typically used to quantize bosonic strings. In both cases, one starts with positive-energy representations of the Virasoro algebra of central charge 26, equipped with Virasoro-invariant bilinear forms, and ends up with vector spaces equipped with bilinear forms. Here, \"Virasoro-invariant\" means \"L\" is adjoint to \"L\" for all integers \"n\".\n\nThe first functor historically is \"old canonical quantization\", and it is given by taking the quotient of the weight 1 primary subspace by the radical of the bilinear form. Here, \"primary subspace\" is the set of vectors annihilated by \"L\" for all strictly positive \"n\", and \"weight 1\" means \"L\" acts by identity. A second, naturally isomorphic functor, is given by degree 1 BRST cohomology. Older treatments of BRST cohomology often have a shift in the degree due to a change in choice of BRST charge, so one may see degree −1/2 cohomology in papers and texts from before 1995. A proof that the functors are naturally isomorphic can be found in Section 4.4 of Polchinski's \"String Theory\" text.\n\nThe Goddard–Thorn theorem amounts to the assertion that this quantization functor more or less cancels the addition of two free bosons, as conjectured by Lovelace in 1971. Lovelace's precise claim was that at critical dimension 26, Virasoro-type Ward identities cancel two full sets of oscillators. Mathematically, this is the following claim:\n\nLet \"V\" be a unitarizable Virasoro representation of central charge 24 with Virasoro-invariant bilinear form, and let π be the irreducible module of the R Heisenberg Lie algebra attached to a nonzero vector λ in R. Then the image of \"V\" ⊗ π under quantization is canonically isomorphic to the subspace of V on which \"L\" acts by 1-(λ,λ).\n\nThe no-ghost property follows immediately, since the positive-definite Hermitian structure of \"V\" is transferred to the image under quantization.\n\nThe bosonic string quantization functors described here can be applied to any conformal vertex algebra of central charge 26, and the output naturally has a Lie algebra structure. The Goddard–Thorn theorem can then be applied to concretely describe the Lie algebra in terms of the input vertex algebra.\n\nPerhaps the most spectacular case of this application is Borcherds's proof of the Monstrous Moonshine conjecture, where the unitarizable Virasoro representation is the Monster vertex algebra (also called \"Moonshine module\") constructed by Frenkel, Lepowsky, and Meurman. By taking a tensor product with the vertex algebra attached to a rank 2 hyperbolic lattice, and applying quantization, one obtains the monster Lie algebra, which is a generalized Kac–Moody algebra graded by the lattice. By using the Goddard–Thorn theorem, Borcherds showed that the homogeneous pieces of the Lie algebra are naturally isomorphic to graded pieces of the Moonshine module, as representations of the monster simple group.\n\nEarlier applications include Frenkel's determination of upper bounds on the root multiplicities of the Kac-Moody Lie algebra whose Dynkin diagram is the Leech lattice, and Borcherds's construction of a generalized Kac-Moody Lie algebra that contains Frenkel's Lie algebra and saturates Frenkel's 1/∆ bound.\n\n"}
{"id": "32573740", "url": "https://en.wikipedia.org/wiki?curid=32573740", "title": "Hautus lemma", "text": "Hautus lemma\n\nIn control theory and in particular when studying the properties of a linear time-invariant system in state space form, the Hautus lemma, named after Malo Hautus, can prove to be a powerful tool. This result appeared first in and. Today it can be found in most textbooks on control theory.\n\nThere exist multiple forms of the lemma.\nThe Hautus lemma for controllability says that given a square matrix formula_1 and a formula_2 the following are equivalent:\n\nThe Hautus lemma for stabilizability says that given a square matrix formula_1 and a formula_2 the following are equivalent:\nThe Hautus lemma for observability says that given a square matrix formula_1 and a formula_17 the following are equivalent:\n\nThe Hautus lemma for detectability says that given a square matrix formula_1 and a formula_17 the following are equivalent:\n\n"}
{"id": "31111767", "url": "https://en.wikipedia.org/wiki?curid=31111767", "title": "Herbert Grötzsch", "text": "Herbert Grötzsch\n\nCamillo Herbert Grötzsch (21 May 1902 – 15 May 1993) was a German mathematician. He was born in Döbeln and died in Halle. Grötzsch worked in graph theory. He was the discoverer and eponym of the Grötzsch graph, a triangle-free graph that requires four colors in any graph coloring, and Grötzsch's theorem, the result that every triangle-free planar graph requires at most three colors. He also introduced the concept of a quasiconformal mapping. He was one of Paul Koebe's students.\n\n\n\n"}
{"id": "8161285", "url": "https://en.wikipedia.org/wiki?curid=8161285", "title": "High frequency content measure", "text": "High frequency content measure\n\nIn signal processing, the high frequency content measure is a simple measure, taken across a signal spectrum (usually a STFT spectrum), that can be used to characterize the amount of high-frequency content in the signal. The magnitudes of the spectral bins are added together, but multiplying each magnitude by the bin \"position\" (proportional to the frequency). Thus if \"X\"(\"k\") is a discrete spectrum with \"N\" unique points, its high frequency content measure is:\n\nIn contrast to perceptual measures, this is not based on any evidence about its relevance to human hearing. Despite that, it can be useful for some applications, such as onset detection.\n\nThe measure has close similarities to the spectral centroid measure, being essentially the same calculation but without normalization according to overall magnitude.\n\n"}
{"id": "19515158", "url": "https://en.wikipedia.org/wiki?curid=19515158", "title": "Higher-dimensional algebra", "text": "Higher-dimensional algebra\n\nIn mathematics, especially (higher) category theory, higher-dimensional algebra is the study of categorified structures. It has applications in nonabelian algebraic topology, and generalizes abstract algebra.\n\nA first step towards defining higher dimensional algebras is the concept of 2-category of higher category theory, followed by the more 'geometric' concept of double category.\n\nA higher level concept is thus defined as a category of categories, or super-category, which generalises to higher dimensions the notion of category – regarded as any structure which is an interpretation of Lawvere's axioms of the \"elementary theory of abstract categories\" (ETAC). Ll.\n\n, Thus, a supercategory and also a super-category, can be regarded as natural extensions of the concepts of meta-category, multicategory, and multi-graph, \"k\"-partite graph, or colored graph (see a color figure, and also its definition in graph theory).\n\nSupercategories were first introduced in 1970, and were subsequently developed for applications in theoretical physics (especially quantum field theory and topological quantum field theory) and mathematical biology or mathematical biophysics.\n\nOther pathways in HDA involve: bicategories, homomorphisms of bicategories, variable categories (\"aka\", indexed, or parametrized categories), topoi, effective descent, and enriched and internal categories.\n\nIn higher-dimensional algebra (HDA), a double groupoid is a generalisation of a one-dimensional groupoid to two dimensions, and the latter groupoid can be considered as a special case of a category with all invertible arrows, or morphisms.\n\nDouble groupoids are often used to capture information about geometrical objects such as higher-dimensional manifolds (or \"n\"-dimensional manifolds). In general, an \"n\"-dimensional manifold is a space that locally looks like an \"n\"-dimensional Euclidean space, but whose global structure may be non-Euclidean.\n\nDouble groupoids were first introduced by Ronald Brown in 1976, in ref. and were further developed towards applications in nonabelian algebraic topology. A related, 'dual' concept is that of a double algebroid, and the more general concept of R-algebroid.\n\nMany of the higher dimensional algebraic structures are noncommutative and, therefore, their study is a very significant part of nonabelian category theory, and also of Nonabelian Algebraic Topology (NAAT) which generalises to higher dimensions ideas coming from the fundamental group. Such algebraic structures in dimensions greater than 1 develop the nonabelian character of the fundamental group, and they are in a precise sense \"‘more nonabelian than the groups' \". These noncommutative, or more specifically, nonabelian structures reflect more accurately the geometrical complications of higher dimensions than the known homology and homotopy groups commonly encountered in classical algebraic topology.\nAn important part of nonabelian algebraic topology is concerned with the properties and applications of homotopy groupoids and filtered spaces. Noncommutative double groupoids and double algebroids are only the first examples of such higher dimensional structures that are nonabelian. The new methods of Nonabelian Algebraic Topology (NAAT) \"``can be applied to determine homotopy invariants of spaces, and homotopy classification of maps, in cases which include some classical results, and allow results not available by classical methods\"\". Cubical omega-groupoids, higher homotopy groupoids, crossed modules, crossed complexes and Galois groupoids are key concepts in developing applications related to homotopy of filtered spaces, higher dimensional space structures, the construction of the fundamental groupoid of a topos E in the general theory of topoi, and also in their physical applications in nonabelian quantum theories, and recent developments in quantum gravity, as well as categorical and topological dynamics. Further examples of such applications include the generalisations of noncommutative geometry formalizations of the noncommutative standard models \"via\" fundamental double groupoids and spacetime structures even more general than topoi or the lower-dimensional noncommutative spacetimes encountered in several topological quantum field theories and noncommutative geometry theories of quantum gravity.\n\nA fundamental result in NAAT is the generalised, higher homotopy van Kampen theorem proven by R. Brown which states that ``\"the homotopy type of a topological space can be computed by a suitable colimit or homotopy colimit over homotopy types of its pieces'. A related example is that of van Kampen theorems for categories of covering morphisms in lextensive categories. Other reports of generalisations of the van Kampen theorem include statements for 2-categories and a topos of topoi .\nImportant results in HDA are also the extensions of the Galois theory in categories and variable categories, or indexed/`parametrized' categories. The Joyal–Tierney representation theorem for topoi is also a generalisation of the Galois theory.\nThus, indexing by bicategories in the sense of Benabou one also includes here the Joyal–Tierney theory.\n\nIn quantum field theory, there exist quantum categories. and quantum double groupoids./ One can consider quantum double groupoids to be fundamental groupoids defined via a 2-functor, which allows one to think about the physically interesting case of quantum fundamental groupoids (QFGs) in terms of the bicategory Span(Groupoids), and then constructing 2-Hilbert spaces and 2-linear maps for manifolds and cobordisms. At the next step, one obtains cobordisms with corners via natural transformations of such 2-functors. A claim was then made that, with the gauge group SU(2), \"the extended TQFT, or ETQFT, gives a theory equivalent to the Ponzano–Regge model of quantum gravity\"; similarly, the Turaev–Viro model would be then obtained with representations of SU(2). Therefore, one can describe the state space of a gauge theory – or many kinds of quantum field theories (QFTs) and local quantum physics, in terms of the transformation groupoids given by symmetries, as for example in the case of a gauge theory, by the gauge transformations acting on states that are, in this case, connections. In the case of symmetries related to quantum groups, one would obtain structures that are representation categories of quantum groupoids, instead of the 2-vector spaces that are representation categories of groupoids.\n\n\n"}
{"id": "4603066", "url": "https://en.wikipedia.org/wiki?curid=4603066", "title": "Horseshoe lemma", "text": "Horseshoe lemma\n\nIn homological algebra, the horseshoe lemma, also called the simultaneous resolution theorem, is a statement relating resolutions of two objects formula_1 and formula_2 to resolutions of\nextensions of formula_1 by formula_2. It says that if an object formula_5 is an extension of formula_1 by formula_2, then a resolution of formula_5 can be built up inductively with the \"n\"th item in the resolution equal to the coproduct of the \"n\"th items in the resolutions of formula_1 and formula_2. The name of the lemma comes from the shape of the diagram illustrating the lemma's hypothesis.\n\nLet formula_11 be an abelian category with enough projectives. If\n\nis a diagram in formula_11 such that the column is exact and the\nrows are projective resolutions of formula_1 and formula_2 respectively, then\nit can be completed to a commutative diagram\n\nwhere all columns are exact, the middle row is a projective resolution\nof formula_5, and formula_16 for all \"n\". If formula_11 is an\nabelian category with enough injectives, the dual statement also holds.\n\nThe lemma can be proved inductively. At each stage of the induction, the properties of projective objects are used to define maps in a projective resolution of formula_5. Then the snake lemma is invoked to show that the simultaneous resolution constructed so far has exact rows.\n\n\n"}
{"id": "4593774", "url": "https://en.wikipedia.org/wiki?curid=4593774", "title": "K-edge-connected graph", "text": "K-edge-connected graph\n\nIn graph theory, a connected graph is \"k\"-edge-connected if it remains connected whenever fewer than \"k\" edges are removed.\n\nThe edge-connectivity of a graph is the largest \"k\" for which the graph is \"k\"-edge-connected.\n\nEdge connectivity and the enumeration of \"k\"-edge-connected graphs was studied by Camille Jordan in 1869.\n\nLet formula_1 be an arbitrary graph.\nIf subgraph formula_2 is connected for all formula_3 where formula_4, then \"G\" is \"k\"-edge-connected.\nThe edge connectivity of formula_5 is the maximum value \"k\" such that \"G\" is \"k\"-edge-connected. The smallest set \"X\" whose removal disconnects \"G\" is a minimum cut in \"G\".\n\nThe edge connectivity version of Menger's theorem provides an alternative and equivalent characterization, in terms of edge-disjoint paths in the graph. If and only if every two vertices of \"G\" form the endpoints of \"k\" paths, no two of which share an edge with each other, then \"G\" is \"k\"-edge-connected. In one direction this is easy: if a system of paths like this exists, then every set \"X\" of fewer than \"k\" edges is disjoint from at least one of the paths, and the pair of vertices remains connected to each other even after \"X\" is deleted. In the other direction, the existence of a system of paths for each pair of vertices in a graph that cannot be disconnected by the removal of few edges can be proven using the max-flow min-cut theorem from the theory of network flows.\n\nMinimum vertex degree gives a trivial upper bound on edge-connectivity. That is, if a graph formula_1 is \"k\"-edge-connected then it is necessary that \"k\" ≤ δ(\"G\"), where δ(\"G\") is the minimum degree of any vertex \"v\" ∈ \"V\". Obviously, deleting all edges incident to a vertex, \"v\", would then disconnect \"v\" from the graph.\n\nEdge connectivity is the dual concept to girth, the length of the shortest cycle in a graph, in the sense that the girth of a planar graph is the edge connectivity of its dual graph, and vice versa. These concepts are unified in matroid theory by the girth of a matroid, the size of the smallest dependent set in the matroid. For a graphic matroid, the matroid girth equals the girth of the underlying graph, while for a co-graphic matroid it equals the edge connectivity.\n\nThe 2-edge-connected graphs can also be characterized by the absence of bridges, by the existence of an ear decomposition, or by Robbins' theorem according to which these are exactly the graphs that have a strong orientation.\n\nThere is a polynomial-time algorithm to determine the largest \"k\" for which a graph \"G\" is \"k\"-edge-connected. A simple algorithm would, for every pair \"(u,v)\", determine the maximum flow from \"u\" to \"v\" with the capacity of all edges in \"G\" set to 1 for both directions. A graph is \"k\"-edge-connected if and only if the maximum flow from \"u\" to \"v\" is at least \"k\" for any pair \"(u,v)\", so \"k\" is the least \"u-v\"-flow among all \"(u,v)\".\n\nIf \"n\" is the number of vertices in the graph, this simple algorithm would perform formula_7 iterations of the Maximum flow problem, which can be solved in formula_8 time. Hence the complexity of the simple algorithm described above is formula_9 in total.\n\nAn improved algorithm will solve the maximum flow problem for every pair \"(u,v)\" where \"u\" is arbitrarily fixed while \"v\" varies\nover all vertices. This reduces the complexity to formula_10 and is sound since, if a cut of capacity less than \"k\" exists,\nit is bound to separate \"u\" from some other vertex. It can be further improved by an algorithm of Gabow that runs in worst case formula_8 time. \n\nThe Karger–Stein variant of Karger's algorithm provides a faster randomized algorithm for determining the connectivity, with expected runtime formula_12.\n\nA related problem: finding the minimum \"k\"-edge-connected spanning subgraph of \"G\" (that is: select as few as possible edges in \"G\" that your selection is \"k\"-edge-connected) is NP-hard for formula_13.\n\n"}
{"id": "22812263", "url": "https://en.wikipedia.org/wiki?curid=22812263", "title": "L. W. Beineke", "text": "L. W. Beineke\n\nLowell Wayne Beineke (born 1939) is a professor of graph theory at Indiana University – Purdue University Fort Wayne. Beineke is known for his elegant characterization of line graphs (derived graph) in terms of the nine Forbidden graph characterization.\n\nBeineke has taught mathematics at Indiana University – Purdue University Fort Wayne since 1965. He received a B.S. from Purdue University in 1961 and a M.S. from the University of Michigan in 1962 and a Ph.D., in 1965, his Ph.D. advisor was Frank Harary. Beineke holds the Jack W. Schrey chair of mathematical sciences. Beineke was recipient of the Amoco Foundation the Outstanding Teaching Award in 1978, and again in 1992.\n\n\n\n"}
{"id": "20547369", "url": "https://en.wikipedia.org/wiki?curid=20547369", "title": "List of probabilistic proofs of non-probabilistic theorems", "text": "List of probabilistic proofs of non-probabilistic theorems\n\nProbability theory routinely uses results from other fields of mathematics (mostly, analysis). The opposite cases, collected below, are relatively rare; however, probability theory is used systematically in combinatorics via the probabilistic method. They are particularly used for non-constructive proofs.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "35198299", "url": "https://en.wikipedia.org/wiki?curid=35198299", "title": "Manin–Drinfeld theorem", "text": "Manin–Drinfeld theorem\n\nIn mathematics, the Manin–Drinfeld theorem, proved by and , states that the difference of two cusps of a modular curve has finite order in the Jacobian variety.\n\n"}
{"id": "28762298", "url": "https://en.wikipedia.org/wiki?curid=28762298", "title": "Mengenlehreuhr", "text": "Mengenlehreuhr\n\nThe Mengenlehreuhr (German for \"Set Theory Clock\") or Berlin-Uhr (\"Berlin Clock\") is the first public clock in the world that tells the time by means of illuminated, coloured fields, for which it entered the \"Guinness Book of Records\" upon its installation on 17 June 1975. Commissioned by the Senate of Berlin and designed by Dieter Binninger, the original full-sized Mengenlehreuhr was originally located at the Kurfürstendamm on the corner with Uhlandstraße. After the Senate decommissioned it in 1995, the clock was relocated to a site in Budapester Straße in front of Europa-Center, where it stands today.\n\nThe Mengenlehreuhr consists of 24 lights which are divided into one circular blinking yellow light on top to denote the seconds, two top rows denoting the hours and two bottom rows denoting the minutes.\n\nThe clock is read from the top row to the bottom. The top row of four red fields denote five full hours each, alongside the second row, also of four red fields, which denote one full hour each, displaying the hour value in 24-hour format. The third row consists of eleven yellow-and-red fields, which denote five full minutes each (the red ones also denoting 15, 30 and 45 minutes past), and the bottom row has another four yellow fields, which mark one full minute each. The round yellow light on top blinks to denote even- (when lit) or odd-numbered (when unlit) seconds.\n\nGiven the photo of the clock at the top of the article as an example, two fields are lit in the first row (five hours multiplied by two, i.e. ten hours), but no fields are lit in the second row; therefore the hour value is 10. Six fields are lit in the third row (five minutes multiplied by six, i.e. thirty minutes), while the bottom row has one field on (plus one minute). Hence, the lights of the clock altogether tell the time as 10:31.\n\nThis clock may be the key to the unsolved section of \"Kryptos\", a sculpture at the CIA headquarters. After revealing that part of the deciphered text of the sculpture, in positions 64-69, reads \"BERLIN\", the sculptor, Jim Sanborn, gave \"The New York Times\" another clue in November 2014, that letters 70–74 in part 4 of the sculpture's code, which read \"MZFPK\", will become \"CLOCK\" when decoded, a direct reference to the Berlin Clock. Sanborn further stated that in order to solve section 4, \"You'd better delve into that particular clock\".\nHowever, Sanborn also said that, \"There are several really interesting clocks in Berlin.\"\n\n"}
{"id": "4058621", "url": "https://en.wikipedia.org/wiki?curid=4058621", "title": "Mixture (probability)", "text": "Mixture (probability)\n\nIn probability theory and statistics, a mixture is a probabilistic combination of two or more probability distributions. The concept arises mostly in two contexts:\n\n"}
{"id": "37703993", "url": "https://en.wikipedia.org/wiki?curid=37703993", "title": "Nick Wormald", "text": "Nick Wormald\n\nNicholas Charles Wormald is an Australian mathematician and professor of mathematics at Monash University.\n\nHe specializes in probabilistic combinatorics, graph theory, graph algorithms, Steiner trees, web graphs, mine optimization, and other areas in combinatorics.\n\nIn 1979, he earned a Ph.D. in mathematics from the University of Newcastle with a dissertation entitled \"Some problems in the enumeration of labelled graphs\"\n\nIn 2006, he won the Euler Medal from the Institute of Combinatorics and its Applications. \n\nHe has held the Canada Research Chair in Combinatorics and Optimization at the University of Waterloo.\n\nIn 2012, he was recognized with an Australian Laureate Fellowship for his achievements.\n\nIn 2017, he was elected as a Fellow of the Australian Academy of Science.\n\n"}
{"id": "2788896", "url": "https://en.wikipedia.org/wiki?curid=2788896", "title": "Ontic", "text": "Ontic\n\nIn philosophical ontology, ontic (from the Greek , genitive : \"of that which is\") is physical, real, or factual existence.<br>\n\n\"Ontic\" describes what is there, as opposed to the nature or properties of that being. To illustrate:\n\n\nHarald Atmanspacher writes extensively about the philosophy of science, especially as it relates to Chaos theory, determinism, causation, and stochasticity. He explains that \"\"ontic\" states describe all properties of a physical system exhaustively. ('Exhaustive' in this context means that an \"ontic\" state is 'precisely the way it is,' without any reference to epistemic knowledge or ignorance.)\"\n\nIn an earlier paper, Atmanspacher portrays the difference between an epistemic perspective of a system, and an ontic perspective:\n\nThe British philosopher Roy Bhaskar, who is closely associated with the philosophical movement of critical realism writes:\n\nRuth Groff offers this expansion of Bhaskar's note above:\n\n\n"}
{"id": "102221", "url": "https://en.wikipedia.org/wiki?curid=102221", "title": "Orthogonality", "text": "Orthogonality\n\nIn mathematics, orthogonality is the generalization of the notion of perpendicularity to the linear algebra of bilinear forms. Two elements \"u\" and \"v\" of a vector space with bilinear form \"B\" are orthogonal when . Depending on the bilinear form, the vector space may contain nonzero self-orthogonal vectors. In the case of function spaces, families of orthogonal functions are used to form a basis.\n\nBy extension, orthogonality is also used to refer to the separation of specific features of a system. The term also has specialized meanings in other fields including art and chemistry.\n\nThe word comes from the Greek ' (\"orthos\"), meaning \"upright\", and ' (\"gonia\"), meaning \"angle\".\nThe ancient Greek ὀρθογώνιον \"orthogōnion\" (< ὀρθός \"orthos\" 'upright' + γωνία \"gōnia\" 'angle') and classical Latin \"orthogonium\" originally denoted a rectangle. Later, they came to mean a right triangle. In the 12th century, the post-classical Latin word \"orthogonalis\" came to mean a right angle or something related to a right angle.\n\n\nA set of vectors in an inner product space is called pairwise orthogonal if each pairing of them is orthogonal. Such a set is called an orthogonal set.\n\nIn certain cases, the word \"normal\" is used to mean \"orthogonal\", particularly in the geometric sense as in the normal to a surface. For example, the \"y\"-axis is normal to the curve at the origin. However, \"normal\" may also refer to the magnitude of a vector. In particular, a set is called orthonormal (orthogonal plus normal) if it is an orthogonal set of unit vectors. As a result, use of the term \"normal\" to mean \"orthogonal\" is often avoided. The word \"normal\" also has a different meaning in probability and statistics.\n\nA vector space with a bilinear form generalizes the case of an inner product. When the bilinear form applied to two vectors results in zero, then they are orthogonal. The case of a pseudo-Euclidean plane uses the term hyperbolic orthogonality. In the diagram, axes x′ and t′ are hyperbolic-orthogonal for any given \"ϕ\".\n\nIn Euclidean space, two vectors are orthogonal if and only if their dot product is zero, i.e. they make an angle of 90° (π/2 radians), or one of the vectors is zero. Hence orthogonality of vectors is an extension of the concept of perpendicular vectors to spaces of any dimension.\n\nThe orthogonal complement of a subspace is the space of all vectors that are orthogonal to every vector in the subspace. In a three-dimensional Euclidean vector space, the orthogonal complement of a line through the origin is the plane through the origin perpendicular to it, and vice versa.\n\nNote that the geometric concept two planes being perpendicular does not correspond to the orthogonal complement, since in three dimensions a pair of vectors, one from each of a pair of perpendicular planes, might meet at any angle.\n\nIn four-dimensional Euclidean space, the orthogonal complement of a line is a hyperplane and vice versa, and that of a plane is a plane.\n\nBy using integral calculus, it is common to use the following to define the inner product of two functions \"f\" and \"g\" with respect to a nonnegative weight function \"w\" over an interval :\n\nIn simple cases, .\n\nWe say that functions \"f\" and \"g\" are orthogonal if their inner product (equivalently, the value of this integral) is zero:\n\nOrthogonality of two functions with respect to one inner product does not imply orthogonality with respect to another inner product.\n\nWe write the norm with respect to this inner product as\n\nThe members of a set of functions are \"orthogonal\" with respect to \"w\" on the interval if\nThe members of such a set of functions are \"orthonormal\" with respect to \"w\" on the interval if\nwhere\nis the Kronecker delta.\nIn other words, every pair of them (excluding pairing of a function with itself) is orthogonal, and the norm of each is 1. See in particular the orthogonal polynomials.\n\n\n\n\n\nIn art, the perspective (imaginary) lines pointing to the vanishing point are referred to as \"orthogonal lines\".\n\nThe term \"orthogonal line\" often has a quite different meaning in the literature of modern art criticism. Many works by painters such as Piet Mondrian and Burgoyne Diller are noted for their exclusive use of \"orthogonal lines\" — not, however, with reference to perspective, but rather referring to lines that are straight and exclusively horizontal or vertical, forming right angles where they intersect. For example, an essay at the Web site of the Thyssen-Bornemisza Museum states that \"Mondrian ... dedicated his entire oeuvre to the investigation of the balance between orthogonal lines and primary colours.\" \n\nOrthogonality in programming language design is the ability to use various language features in arbitrary combinations with consistent results. This usage was introduced by Van Wijngaarden in the design of Algol 68:\n\nThe number of independent primitive concepts has been minimized in order that the language be easy to describe, to learn, and to implement. On the other hand, these concepts have been applied “orthogonally” in order to maximize the expressive power of the language while trying to avoid deleterious superfluities.\nOrthogonality is a system design property which guarantees that modifying the technical effect produced by a component of a system neither creates nor propagates side effects to other components of the system. Typically this is achieved through the separation of concerns and encapsulation, and it is essential for feasible and compact designs of complex systems. The emergent behavior of a system consisting of components should be controlled strictly by formal definitions of its logic and not by side effects resulting from poor integration, i.e., non-orthogonal design of modules and interfaces. Orthogonality reduces testing and development time because it is easier to verify designs that neither cause side effects nor depend on them.\n\nAn instruction set is said to be orthogonal if it lacks redundancy (i.e., there is only a single instruction that can be used to accomplish a given task) and is designed such that instructions can use any register in any addressing mode. This terminology results from considering an instruction as a vector whose components are the instruction fields. One field identifies the registers to be operated upon and another specifies the addressing mode. An orthogonal instruction set uniquely encodes all combinations of registers and addressing modes.\n\nIn communications, multiple-access schemes are orthogonal when an ideal receiver can completely reject arbitrarily strong unwanted signals from the desired signal using different basis functions. One such scheme is TDMA, where the orthogonal basis functions are nonoverlapping rectangular pulses (\"time slots\").\n\nAnother scheme is orthogonal frequency-division multiplexing (OFDM), which refers to the use, by a single transmitter, of a set of frequency multiplexed signals with the exact minimum frequency spacing needed to make them orthogonal so that they do not interfere with each other. Well known examples include (a, g, and n) versions of 802.11 Wi-Fi; WiMAX; ITU-T G.hn, DVB-T, the terrestrial digital TV broadcast system used in most of the world outside North America; and DMT (Discrete Multi Tone), the standard form of ADSL.\n\nIn OFDM, the subcarrier frequencies are chosen so that the subcarriers are orthogonal to each other, meaning that crosstalk between the subchannels is eliminated and intercarrier guard bands are not required. This greatly simplifies the design of both the transmitter and the receiver. In conventional FDM, a separate filter for each subchannel is required.\n\nWhen performing statistical analysis, independent variables that affect a particular dependent variable are said to be orthogonal if they are uncorrelated, since the covariance forms an inner product. In this case the same results are obtained for the effect of any of the independent variables upon the dependent variable, regardless of whether one models the effects of the variables individually with simple regression or simultaneously with multiple regression. If correlation is present, the factors are not orthogonal and different results are obtained by the two methods. This usage arises from the fact that if centered by subtracting the expected value (the mean), uncorrelated variables are orthogonal in the geometric sense discussed above, both as observed data (i.e., vectors) and as random variables (i.e., density functions).\nOne econometric formalism that is alternative to the maximum likelihood framework, the Generalized Method of Moments, relies on orthogonality conditions. In particular, the Ordinary Least Squares estimator may be easily derived from an orthogonality condition between the explanatory variables and model residuals.\n\nIn taxonomy, an orthogonal classification is one in which no item is a member of more than one group, that is, the classifications are mutually exclusive.\n\nIn combinatorics, two \"n\"×\"n\" Latin squares are said to be orthogonal if their superimposition yields all possible \"n\" combinations of entries.\n\nIn synthetic organic chemistry orthogonal protection is a strategy allowing the deprotection of functional groups independently of each other. In chemistry and biochemistry, an orthogonal interaction occurs when there are two pairs of substances and each substance can interact with their respective partner, but does not interact with either substance of the other pair. For example, DNA has two orthogonal pairs: cytosine and guanine form a base-pair, and adenine and thymine form another base-pair, but other base-pair combinations are strongly disfavored. As a chemical example, tetrazine reacts with transcyclooctene and azide reacts with cyclooctyne without any cross-reaction, so these are mutually orthogonal reactions, and so, can be performed simultaneously and selectively. Bioorthogonal chemistry refers to chemical reactions occurring inside living systems without reacting with naturally present cellular components. In supramolecular chemistry the notion of orthogonality refers to the possibility of two or more supramolecular, often non-covalent, interactions being compatible; reversibly forming without interference from the other.\n\nIn analytical chemistry, analyses are \"orthogonal\" if they make a measurement or identification in completely different ways, thus increasing the reliability of the measurement. This is often required as a part of a new drug application.\n\nIn the field of system reliability orthogonal redundancy is that form of redundancy where the form of backup device or method is completely different from the prone to error device or method. The failure mode of an orthogonally redundant back-up device or method does not intersect with and is completely different from the failure mode of the device or method in need of redundancy to safeguard the total system against catastrophic failure.\n\nIn neuroscience, a sensory map in the brain which has overlapping stimulus coding (e.g. location and quality) is called an orthogonal map.\n\nIn board games such as chess which feature a grid of squares, 'orthogonal' is used to mean \"in the same row/'rank' or column/'file'\". This is the counterpart to squares which are \"diagonally adjacent\". In the ancient Chinese board game Go a player can capture the stones of an opponent by occupying all orthogonally-adjacent points.\n\nStereo vinyl records encode both the left and right stereo channels in a single groove. The V-shaped groove in the vinyl has walls that are 90 degrees to each other, with variations in each wall separately encoding one of the two analogue channels that make up the stereo signal. The cartridge senses the motion of the stylus following the groove in two orthogonal directions: 45 degrees from vertical to either side. A pure horizontal motion corresponds to a mono signal, equivalent to a stereo signal in which both channels carry identical (in-phase) signals.\n\n\n"}
{"id": "22092656", "url": "https://en.wikipedia.org/wiki?curid=22092656", "title": "Packet erasure channel", "text": "Packet erasure channel\n\nThe packet erasure channel is a communication channel model where sequential packets are either received or lost (at a known location). This channel model is closely related to the binary erasure channel.\n\nAn erasure code can be used for forward error correction on such a channel.\n\n\n"}
{"id": "2680620", "url": "https://en.wikipedia.org/wiki?curid=2680620", "title": "Parameterized post-Newtonian formalism", "text": "Parameterized post-Newtonian formalism\n\nPost-Newtonian formalism is a calculational tool that expresses Einstein's (nonlinear) equations of gravity in terms of the lowest-order deviations from Newton's law of universal gravitation. This allows approximations to Einstein's equations to be made in the case of weak fields. Higher-order terms can be added to increase accuracy, but for strong fields, sometimes it is preferable to solve the complete equations numerically. Some of these post-Newtonian approximations are expansions in a small parameter, which is the ratio of the velocity of the matter forming the gravitational field to the speed of light, which in this case is better called the speed of gravity. In the limit, when the fundamental speed of gravity becomes infinite, the post-Newtonian expansion reduces to Newton's law of gravity.\n\nThe parameterized post-Newtonian formalism or PPN formalism, is a version of this formulation that explicitly details the parameters in which a general theory of gravity can differ from Newtonian gravity. It is used as a tool to compare Newtonian and Einsteinian gravity in the limit in which the gravitational field is weak and generated by objects moving slowly compared to the speed of light. In general, PPN formalism can be applied to all metric theories of gravitation in which all bodies satisfy the Einstein equivalence principle (EEP). The speed of light remains constant in PPN formalism and it assumes that the metric tensor is always symmetric.\n\nThe earliest parameterizations of the post-Newtonian approximation were performed by Sir Arthur Stanley Eddington in 1922. However, they dealt solely with the vacuum gravitational field outside an isolated spherical body. Dr. Ken Nordtvedt (1968, 1969) expanded this to include seven parameters. Clifford Martin Will (1971) introduced a stressed, continuous matter description of celestial bodies.\n\nThe versions described here are based on Wei-Tou Ni (1972), Will and Nordtvedt (1972), Charles W. Misner et al. (1973) (see \"Gravitation\" (book)), and Will (1981, 1993) and have ten parameters.\n\nTen post-Newtonian parameters completely characterize the weak-field behavior of the theory. The formalism has been a valuable tool in tests of general relativity. In the notation of Will (1971), Ni (1972) and Misner et al. (1973) they have the following values:\n\nformula_1 is the 4 by 4 symmetric metric tensor with indexes formula_2 and formula_3 going from 0 to 3. Below, an index of 0 will indicate the time direction and indices formula_4 and formula_5 (going from 1 to 3) will indicate spatial directions.\n\nIn Einstein's theory, the values of these parameters are chosen (1) to fit Newton's Law of gravity in the limit of velocities and mass approaching zero, (2) to ensure conservation of energy, mass, momentum, and angular momentum, and (3) to make the equations independent of the reference frame. In this notation, general relativity has PPN parameters\nformula_6 and formula_7\n\nIn the more recent notation of Will & Nordtvedt (1972) and Will (1981, 1993, 2006) a different set of ten PPN parameters is used.\n\nThe meaning of these is that formula_19, formula_20 and formula_21 measure the extent of preferred frame effects. formula_22, formula_23, formula_24, formula_25 and formula_21 measure the failure of conservation of energy, momentum and angular momentum.\n\nIn this notation, general relativity has PPN parameters\n\nThe mathematical relationship between the metric, metric potentials and PPN parameters for this notation is:\nwhere repeated indexes are summed. formula_32 is on the order of potentials such as formula_33, the square magnitude of the coordinate velocities of matter, etc. formula_34 is the velocity vector of the PPN coordinate system relative to the mean rest-frame of the universe. formula_35 is the square magnitude of that velocity. formula_36 if and only if formula_37, formula_38 otherwise.\n\nThere are ten metric potentials, formula_33, formula_40, formula_41, formula_42, formula_43, formula_44, formula_45, formula_46, formula_47 and formula_48, one for each PPN parameter to ensure a unique solution. 10 linear equations in 10 unknowns are solved by inverting a 10 by 10 matrix. These metric potentials have forms such as:\nwhich is simply another way of writing the Newtonian gravitational potential,\nwhere formula_59 is the density of rest mass, formula_60 is the internal energy per unit rest mass, formula_61 is the pressure as measured in a local freely falling frame momentarily comoving with the matter, and formula_62 is the coordinate velocity of the matter.\n\nStress-energy tensor for a perfect fluid takes form\n\nExamples of the process of applying PPN formalism to alternative theories of gravity can be found in Will (1981, 1993). It is a nine step process:\n\n\nA table comparing PPN parameters for 23 theories of gravity can be found in Alternatives to general relativity#PPN parameters for a range of theories.\n\nMost metric theories of gravity can be lumped into categories. Scalar theories of gravitation include conformally flat theories and stratified theories with time-orthogonal space slices.\n\nIn conformally flat theories such as Nordström's theory of gravitation the metric is given by formula_98 and for this metric formula_99, which violently disagrees with observations. In stratified theories such as Yilmaz theory of gravitation the metric is given by formula_100 and for this metric formula_101, which also disagrees violently with observations.\n\nAnother class of theories is the quasilinear theories such as Whitehead's theory of gravitation. For these formula_102. The relative magnitudes of the harmonics of the Earth's tides depend on formula_17 and formula_20, and measurements show that quasilinear theories disagree with observations of Earth's tides.\n\nAnother class of metric theories is the bimetric theory. For all of these formula_20 is non-zero. From the precession of the solar spin we know that formula_106, and that effectively rules out bimetric theories.\n\nAnother class of metric theories is the scalar tensor theories, such as Brans–Dicke theory. For all of these, formula_107. The limit of formula_108 means that formula_109 would have to be very large, so these theories are looking less and less likely as experimental accuracy improves.\n\nThe final main class of metric theories is the vector-tensor theories. For all of these the gravitational \"constant\" varies with time and formula_20 is non-zero. Lunar laser ranging experiments tightly constrain the variation of the gravitational \"constant\" with time and formula_106, so these theories are also looking unlikely.\n\nThere are some metric theories of gravity that do not fit into the above categories, but they have similar problems.\n\nBounds on the PPN parameters Will (2006)\n† Will, C.M., \"Is momentum conserved? A test in the binary system PSR 1913 + 16\", \"Astrophysical Journal Letters\" , vol. 393, no. 2, July 10, 1992, p. L59-L61.\n\n‡ Based on formula_112 from Will (1976, 2006). It is theoretically possible for an alternative model of gravity to bypass this bound, in which case the bound is formula_113 from Ni (1972).\n\n\n"}
{"id": "275991", "url": "https://en.wikipedia.org/wiki?curid=275991", "title": "Range (mathematics)", "text": "Range (mathematics)\n\nIn mathematics, and more specifically in naive set theory, the range of a function refers to either the \"codomain\" or the \"image\" of the function, depending upon usage. Modern usage almost always uses \"range\" to mean \"image\".\n\nThe codomain of a function is some arbitrary super-set of image. In real analysis, it is the real numbers. In complex analysis, it is the complex numbers.\n\nThe image of a function is the set of all outputs of the function. The image is always a subset of the codomain.\n\nAs the term \"range\" can have different meanings, it is considered a good practice to define it the first time it is used in a textbook or article.\n\nOlder books, when they use the word \"range\", tend to use it to mean what is now called the codomain. More modern books, if they use the word \"range\" at all, generally use it to mean what is now called the image. To avoid any confusion, a number of modern books don't use the word \"range\" at all.\n\nAs an example of the two different usages, consider the function formula_1 as it is used in real analysis, that is, as a function that inputs a real number and outputs its square. In this case, its codomain is the set of real numbers formula_2, but its image is the set of non-negative real numbers formula_3, since formula_4 is never negative if formula_5 is real. For this function, if we use \"range\" to mean \"codomain\", it refers to formula_2. When we use \"range\" to mean \"image\", it refers to formula_3.\n\nAs an example where the range equals the codomain, consider the function formula_8, which inputs a real number and outputs its double. For this function, the codomain and the image are the same (the function is a surjection), so the word range is unambiguous; it is the set of all real numbers.\n\nWhen \"range\" is used to mean \"codomain\", the image of a function \"f\" is already implicitly defined. It is (by definition of image) the (maybe trivial) subset of the \"range\" which equals {\"y\" | there exists an \"x\" in the domain of \"f\" such that \"y\" = \"f\"(\"x\")}.\n\nWhen \"range\" is used to mean \"image\", the range of a function \"f\" is by definition {\"y\" | there exists an \"x\" in the domain of \"f\" such that \"y\" = \"f\"(\"x\")}. In this case, the codomain of \"f\" must not be specified, because any codomain which contains this image as a (maybe trivial) subset will work.\n\nIn both cases, image \"f\" ⊆ range \"f\" ⊆ codomain \"f\", with at least one of the containments being equality.\n\n\n"}
{"id": "34805600", "url": "https://en.wikipedia.org/wiki?curid=34805600", "title": "Rod Downey", "text": "Rod Downey\n\nRodney Graham Downey (born 20 September 1957) is an Australian and New Zealand mathematician and computer scientist, a professor in the School of Mathematics and Statistics at Victoria University of Wellington in New Zealand. He is known for his work in mathematical logic and computational complexity theory, and in particular for founding the field of parameterised complexity together with Michael Fellows.\n\nDowney earned a bachelor's degree at the University of Queensland in 1978, and then went on to graduate school at Monash University, earning a doctorate in 1982 under the supervision of John Crossley. After holding teaching and visiting positions at the Chisholm Institute of Technology, Western Illinois University, the National University of Singapore, and the University of Illinois at Urbana-Champaign, he came to New Zealand in 1986 as a lecturer at Victoria University. He was promoted to reader in 1991, and was given a personal chair at Victoria in 1995.\n\nDowney was president of the New Zealand Mathematical Society from 2001 to 2003.\n\nDowney is the co-author of three books:\nHe is also the author or co-author of over 200 research papers, including a highly cited sequence of four papers with Michael Fellows and Karl Abrahamson setting the foundation for the study of parameterised complexity.\n\nIn 1990, Downey won the Hamilton Research Award from the Royal Society of New Zealand\nIn 1992 Downey won the Research Award of the New Zealand Mathematical Society \"for penetrating and prolific investigations that have made him a leading expert in many aspects of recursion theory, effective algebra and complexity\" , New Zealand Mathematical Society, retrieved 19 February 2012. In 1994, he won the New Zealand Association of Scientists Research Award, and became a fellow of the Royal Society of New Zealand in 1996. In 2006, he became the first New Zealand based mathematician to give an \nInvited Lecture at the International Congress of Mathematicians. He has also given \ninvited lectures at the International Congress of Logic, Methodology and \nPhilosophy of Science and the ACM Conference on Computational Complexity.\nHe was elected as an ACM Fellow in 2007 \"for contributions to computability and complexity theory\", becoming the second ACM Fellow in New Zealand,\nand in the same year was elected as a fellow of the New Zealand Mathematical Society.\nIn 2010 he won the Shoenfield Prize (for articles) of the Association for Symbolic Logic for his work with Denis Hirschfeldt, Andre Nies, and Sebastiaan Terwijn on randomness.\nIn 2011 the Royal Society of New Zealand gave him their Hector Medal \"for his outstanding, internationally acclaimed work in recursion theory, computational complexity, and other aspects of mathematical logic and combinatorics.\" In 2012 he became a fellow of the American Mathematical Society. In 2013, he became a Fellow of the Australian Mathematical Society. In 2014, he was awarded the Nerode Prize from the European Association for Theoretical Computer Science, jointly with Hans Bodlaender, Michael Fellows, Danny Hermelin, Lance Fortnow and Rahul Santhanam for their work on kernelization lower bounds. In October 2016, Downey received a distinguished Humboldt Research Award for his academic contributions. With Denis Hirschfeldt, Downey won another Shoenfield Prize \nfrom the Association for Symbolic Logic, this time the 2016 book prize for Algorithmic Randomness and Complexity. In 2018, Downey delivered the Goedel Lecture of the Association for Symbolic Logic in the European Summer Meeting at Udine, Italy. In 2018, Downey was awarded the Rutherford Medal, the highest honour awarded by the Royal Society of New Zealand, \"for his pre-eminent revolutionary research into computability, including development of the theory of parameterised complexity and the algorithmic study of randomness.\"\n"}
{"id": "240694", "url": "https://en.wikipedia.org/wiki?curid=240694", "title": "Ronald Graham", "text": "Ronald Graham\n\nRonald Lewis \"Ron\" Graham (born October 31, 1935) is an American mathematician credited by the American Mathematical Society as being \"one of the principal architects of the rapid development worldwide of discrete mathematics in recent years\". He has done important work in scheduling theory, computational geometry, Ramsey theory, and quasi-randomness.\n\nHe is currently the Chief Scientist at the California Institute for Telecommunications and Information Technology (also known as Cal-(IT)) and the Irwin and Joan Jacobs Professor in Computer Science and Engineering at the University of California, San Diego (UCSD).\n\nGraham was born in Taft, California. In 1962, he received his Ph.D. in mathematics from the University of California, Berkeley and began working at Bell Labs and later AT&T Labs. He was director of information sciences in AT&T Labs, but retired from AT&T in 1999 after 37 years.\n\nHis 1977 paper considered a problem in Ramsey theory, and gave a large number as an upper bound for its solution. This number has since become well known as the largest number ever used in a mathematical proof (was listed as such in the \"Guinness Book of Records\"), and is now known as Graham's number, although it has since then been surpassed by even larger numbers such as TREE(3).\n\nGraham popularized the concept of the Erdős number, named after the highly prolific Hungarian mathematician Paul Erdős (1913–1996). A scientist's Erdős number is the minimum number of coauthored publications away from a publication with Erdős. Graham's Erdős number is 1. He co-authored almost 30 papers with Erdős, and was also a good friend. Erdős often stayed with Graham, and allowed him to look after his mathematical papers and even his income. Graham and Erdős visited the young mathematician Jon Folkman when he was hospitalized with brain cancer.\n\nBetween 1993 and 1994 Graham served as the president of the American Mathematical Society. Graham was also featured in \"Ripley's Believe It or Not\" for being not only \"one of the world's foremost mathematicians\", but also \"a highly skilled trampolinist and juggler\", and past president of the International Jugglers' Association.\n\nHe has published about 320 papers and five books, including \"Concrete Mathematics\" with Donald Knuth and Oren Patashnik.\n\nHe is married to Fan Chung Graham (known professionally as Fan Chung), who is the Akamai Professor in Internet Mathematics at the University of California, San Diego.\n\nIn 2003, Graham won the American Mathematical Society's annual Steele Prize for Lifetime Achievement. The prize was awarded on January 16 that year, at the Joint Mathematics Meetings in Baltimore, Maryland. In 1999 he was inducted as a Fellow of the Association for Computing Machinery. Graham has won many other prizes over the years; he was one of the laureates of the prestigious Pólya Prize the first year it was ever awarded, and among the first to win the Euler Medal. The Mathematical Association of America has also awarded him both the Lester R. Ford prize which was \"...established in 1964 to recognize authors of articles of expository excellence published in \"The American Mathematical Monthly\"...\", and the Carl Allendoerfer prize which was established in 1976 for the same reasons, however for a different magazine, the \"Mathematics Magazine\". In 2012 he became a fellow of the American Mathematical Society.\n\n\n\n"}
{"id": "1312946", "url": "https://en.wikipedia.org/wiki?curid=1312946", "title": "Serial decimal", "text": "Serial decimal\n\nIn computers, a serial decimal numeric representation is one in which ten bits are reserved for each digit, with a different bit turned on depending on which of the ten possible digits is intended. ENIAC and CALDIC used this representation.\n\n"}
{"id": "276599", "url": "https://en.wikipedia.org/wiki?curid=276599", "title": "Subhash Kak", "text": "Subhash Kak\n\nSubhash Kak (born 26 March 1947 in Srinagar) is an Indian American computer scientist. He is Regents Professor and a previous Head of Computer Science Department at Oklahoma State University–Stillwater who has made contributions to cryptography, artificial neural networks, and quantum information.\n\nKak is also notable for his Indological publications on the history of science, the philosophy of science, ancient astronomy, and the history of mathematics.\nOn 28 August 2018, he was appointed member of Indian Prime Minister’s Science, Technology and Innovation Advisory Council (PM-STIAC). \n\nSubhash Kak was born to Ram Nath Kak and Sarojini Kak in Srinagar. He completed his BE from Regional Engineering College, Srinagar (Presently National Institute of Technology, Srinagar) and Ph.D. from Indian Institute of Technology Delhi in 1970, where he was immediately offered a faculty position. During 1975-1976, he was a visiting faculty at Imperial College, London, and a guest researcher at Bell Laboratories, Murray Hill. In 1977, he was a visiting researcher at Tata Institute of Fundamental Research, Bombay. In 1979, he joined Louisiana State University, Baton Rouge, where he was the Donald C. and Elaine T. Delaune Distinguished Professor of Electrical and Computer Engineering. In 2007, he joined the Computer Science department at Oklahoma State University–Stillwater.\n\nHe is the author of an autobiography, \"The Circle of Memory\", and several books of poems. He has also authored scholarly papers on art, architecture and music, and he was the anchor of a documentary on Hindustani classical music.\n\nHis brother is the computer scientist Avinash Kak.\n\nHis research is in the fields of cryptography, random sequences, artificial intelligence, quantum mechanics, and information theory. He proposed a test of algorithmic randomness and a type of instantaneously trained neural networks (INNs) (which he and his students have called \"CC4 network\" and others have called \"Kak neural networks\"). He was the first to formulate the discrete and the number theoretic Hilbert transforms. He claims to be amongst the first to apply information metrics to quantum systems.\n\nHe has proposed a hierarchy of languages for communication in biological systems which, in order of increasing complexity, are associative, reorganizational, and quantum. He was featured as one of the pioneers of quantum learning in the journal Neuroquantology edited by Cheryl Fricasso and Stanley Krippner, and also featured as one of the interviewees in the area of mathematics and information in the long-standing PBS series Closer to Truth.\n\nKak proposed a fast matrix multiplication algorithm for cross-wired meshes. He proposed the use of repeating decimals and other random sequences for error correction coding and cryptography. In cryptography, he has advanced new methods of secret sharing that are of importance in distributed systems such as wireless and sensor networks.\n\nKak has argued that there are limits to the intelligence machines can have and it cannot equal biological intelligence. asserts that:\n\nThe Kak neural network, also called the CC4 network is an instantaneously trained neural network that creates a new \"hidden neuron\" for each training sample, achieving immediate training for binary data. The training algorithm for binary data creates links to the new hidden node that simply reflects the binary values in the training vector. Hence, no computation is involved. For effective generalization, the network requires unary coding of the input data, and it can be generalized to non-binary inputs as well.\n\nKak's three-stage protocol is a protocol for quantum cryptography suggested by Kak. This method consists of random rotations of the polarization by both parties. In principle, this method can be used for continuous, unbreakable encryption of data if single photons are used. The basic polarization rotation scheme has been implemented. The three-stage protocol has been proposed as a solution to get around the requirement of expensive single-photon sources and receivers in other quantum cryptography protocols.\n\nKak's writings concerning the astronomy of the Vedic period in his book \"The Astronomical Code of the Rigveda\" support the \"Indigenous Aryans\" theory, questioning mainstream views on the Indo-Aryan migration theory and the nature of early Indian science. While Kak's interpretation has been included in recent overviews of astronomy in the Vedic period in India and the West, his chronology and astronomical calculations have been discredited by several Indologists, such as Michael Witzel, and Western historians, such as Kim Pfloker. Alan Sokal labeled Kak \"one of the leading intellectual luminaries of the Hindu-nationalist diaspora.\"\n\n\"The Astronomical Code of the Rigveda\" claims regularities in the organization of the Rigveda, connecting the structure to certain numbers in the astronomy-based ritual of the five-layered \"vedi\" (Vedic fire altar). Kak arranges the number of hymns in each book of the Rigveda as follows, and compares the arrangement to the \"vedi\":\n<poem>RV 10:191 RV 9:114\nRV 7 :104 RV 8: 92\nRV 5 : 87 RV 6: 75\nRV 3 : 62 RV 4: 58\nRV 2 : 43 RV 1:191</poem>\nHe then computes various sums and subtractions within the diagram, finding numbers related to the distance between the Earth and the Sun, and the sidereal periods of various planets. \n\nKak's archaeoastronomical claims have the effect of significantly extending the Vedic period, postulating the arrival of Indo-Aryan speakers to the 7th millennium BC. This claim is in contradiction with mainstream Indology and historical linguistics and science historians\n\nWhile Klaus Klostermaier has stated that \"Subhash Kak, with his 'decoding of the Rigveda' has opened up an entirely new approach to the study of Vedic cosmology from an empirical astronomical/mathematical viewpoint,\" other scholars like Meera Nanda have said that Kak's \"method is breathtakingly ad hoc and reads like numerology 101.\" Kak's method depends on the structure of the Rigveda as redacted by the shakhas in the late Brahmana period, well within the Indian Iron Age, when it was organized into mandalas (\"books\"). According to Witzel, this leaves Kak's approach attempt to date the text flawed, because this process of redaction took place long after the composition of the individual hymns during the \"samhita prose\" period. \n\nKak prepared the section on archaeoastronomical sites in India for the thematic study on \"Heritage Sites of Astronomy and Archaeoastronomy in the context of the UNESCO World Heritage Convention\" prepared for UNESCO by the International Council on Monuments and Sites (ICOMOS) and the International Astronomical Union (IAU).\n\nKak co-authored \"In Search of the Cradle of Civilization\" (1995) participating in the controversy in the politics of India around the \"indigenous Aryans\" theory. The chronology espoused in this book is based on the archaeoastronomical readings obtained by correlating textual references and archaeological remains.\n\nKak's book \"The Asvamedha: The Rite and Its Logic\" (2002) provides an interpretation of the Vedic \"aśvamedhá\" (horse sacrifice) rite. He argues that the details of this rite are connected to the Agnicayana ritual.\n\nIn the books \"The Nature of Physical Reality\" and \"Mind and Self\" and other publications, Kak argues that there are limits to the extent the world is computable. His \"philosophy of recursionism\" is expounded in his books \"The Gods Within\", \"The Architecture of Knowledge\", and \"The Prajna Sutra\".\n\nKak claimed to be the first to have used the term \"quantum neural computing\", taking a Quantum mind position.\nHe sees the brain as a machine that reduces the infinite possibilities of a \"quantum-like universal consciousness\", which is a consequence of the \"recursive nature of reality\".\n\nIn \"The Architecture of Knowledge\", Kak talks about quantum mechanics, neuroscience, computers, and consciousness. The book is one of the twenty planned monographs in the multi-volume series on the Project of History of Indian Science, Philosophy and Culture under the general editorship of Professor D. P. Chattopadhyaya. The book provides philosophical connections to contemporary science that reach back not only to the Greek but also to the Indian tradition. \n\nThe book seeks to find a consistent framework for knowledge in logic, purpose, and awareness, and sees science as representation and transformation of machines, of reality, and of life. Reality is seen in different layers, and\n\nwith the dual aspects of purposive and reflexive behaviour in each layer, we see parallels in the structures in quantum theory, neuroscience, and computers. The overarching unity is provided by human consciousness. As conscious subjects, we examine the universe through the agency of our minds. In our strivings to describe the outer world using formal knowledge, shadows of the architecture of the inner world are also unveiled.\n\nMore recently, he has spoken of two kinds of consciousness that he calls big-C and little-C, where big-C represents phenomenal consciousness associated with awareness, whereas little-C are those aspects of consciousness that relate to cognitive tasks. He has argued that machines will be able to emulate little-C quite effectively.\n\n\n\n\n\n\n\n\n"}
{"id": "28357829", "url": "https://en.wikipedia.org/wiki?curid=28357829", "title": "Time evolution of integrals", "text": "Time evolution of integrals\n\nIn many applications, one needs to calculate the rate of change of a volume or surface integral whose domain of integration, as well as the integrand, are functions of a particular parameter. In physical applications, that parameter is frequently time \"t\".\n\nThe rate of change of one-dimensional integrals with sufficiently smooth integrands, is governed by this extension of the fundamental theorem of calculus:\n\nThe calculus of moving surfaces provides analogous formulas for volume integrals over Euclidean domains, and surface integrals over differential geometry of surfaces, curved surfaces, including integrals over curved surfaces with moving contour boundaries.\n\nLet \"t\" be a time-like parameter and consider a time-dependent domain Ω with a smooth surface boundary \"S\". Let \"F\" be a time-dependent invariant field defined in the interior of Ω. Then the rate of change of the integral formula_2\n\nis governed by the following law:\n\nwhere \"C\" is the velocity of the interface. The velocity of the interface \"C\" is the fundamental concept in the calculus of moving surfaces. In the above equation, \"C\" must be expressed with respect to the exterior normal. This law can be considered as the generalization of the fundamental theorem of calculus.\n\nA related law governs the rate of change of the surface integral\n\nThe law reads\n\nwhere the formula_6-derivative is the fundamental operator in the calculus of moving surfaces, originally proposed by Jacques Hadamard. formula_7 is the trace of the mean curvature tensor. In this law, \"C\" need not be expression with respect to the exterior normal, as long as the choice of the normal is consistent for \"C\" and formula_8. The first term in the above equation captures the rate of change in \"F\" while the second corrects for expanding or shrinking area. The fact that mean curvature represents the rate of change in area follows from applying the above equation to formula_9 since formula_10 is area:\n\nThe above equation shows that mean curvature formula_8 can be appropriately called the \"shape gradient\" of area. An evolution governed by\n\nis the popular mean curvature flow and represents steepest descent with respect to area. Note that for a sphere of radius \"R\", \nformula_14, and for a circle of radius \"R\", \nformula_15\nwith respect to the exterior normal.\n\nSuppose that \"S\" is a moving surface with a moving contour γ. Suppose that the velocity of the contour γ with respect to \"S\" is \"c\". Then the rate of change of the time dependent integral:\n\nis\n\nThe last term captures the change in area due to annexation, as the figure on the right illustrates.\n"}
{"id": "17633579", "url": "https://en.wikipedia.org/wiki?curid=17633579", "title": "Topological combinatorics", "text": "Topological combinatorics\n\nThe discipline of combinatorial topology used combinatorial concepts in topology and in the early 20th century this turned into the field of algebraic topology.\n\nIn 1978 the situation was reversed — methods from algebraic topology were used to solve a problem in combinatorics – when László Lovász proved the Kneser conjecture, thus beginning the new study of topological combinatorics. Lovász's proof used the Borsuk–Ulam theorem and this theorem retains a prominent role in this new field. This theorem has many equivalent versions and analogs and has been used in the study of fair division problems.\n\nIn another application of homological methods to graph theory Lovász proved both the undirected and directed versions of a conjecture of András Frank: Given a k-connected graph \"G\", \"k\" points formula_1, and \"k\" positive integers formula_2 that sum up to formula_3, there exists a partition formula_4 of formula_5 such that formula_6, formula_7, and formula_8 spans a connected subgraph.\n\nIn 1987 the necklace splitting problem was solved by Noga Alon using the Borsuk–Ulam theorem. It has also been used to study complexity problems in linear decision tree algorithms and the Aanderaa–Karp–Rosenberg conjecture. Other areas include topology of partially ordered sets and bruhat orders.\n\nAdditionally, methods from differential topology now have a combinatorial analog in discrete Morse theory.\n\n\n"}
{"id": "637138", "url": "https://en.wikipedia.org/wiki?curid=637138", "title": "Transfer principle", "text": "Transfer principle\n\nIn model theory, a transfer principle states that all statements of some language that are true for some structure are true for another structure. One of the first examples was the Lefschetz principle, which states that any sentence in the first-order language of fields that is true for the complex numbers is also true for any algebraically closed field of characteristic 0.\n\nAn incipient form of a transfer principle was described by Leibniz under the name of \"the Law of Continuity\". Here infinitesimals are expected to have the \"same\" properties as appreciable numbers. Similar tendencies are found in Cauchy, who used infinitesimals to define both the continuity of functions (in Cours d'Analyse) and a form of the Dirac delta function.\n\nIn 1955, Jerzy Łoś proved the transfer principle for any hyperreal number system. Its most common use is in Abraham Robinson's non-standard analysis of the hyperreal numbers, where the transfer principle states that any sentence expressible in a certain formal language that is true of real numbers is also true of hyperreal numbers.\n\nThe transfer principle concerns the logical relation between the properties of the real numbers R, and the properties of a larger field denoted *R called the hyperreal numbers. The field *R includes, in particular, infinitesimal (\"infinitely small\") numbers, providing a rigorous mathematical realisation of a project initiated by Leibniz.\n\nThe idea is to express analysis over R in a suitable language of mathematical logic, and then point out that this language applies equally well to *R. This turns out to be possible because at the set-theoretic level, the propositions in such a language are interpreted to apply only to internal sets rather than to all sets. As Robinson put it, \"the sentences of [the theory] are interpreted in *R in Henkin's sense.\"\n\nThe theorem to the effect that each proposition valid over R, is also valid over *R, is called the transfer principle.\n\nThere are several different versions of the transfer principle, depending on what model of non-standard mathematics is being used. \nIn terms of model theory, the transfer principle states that a map from a standard model to a non-standard model is an elementary embedding (an embedding preserving the truth values of all statements in a language), or sometimes a \"bounded\" elementary embedding (similar, but only for statements with bounded quantifiers).\n\nThe transfer principle appears to lead to contradictions if it is not handled correctly.\nFor example, since the hyperreal numbers form a non-Archimedean ordered field and the reals form an Archimedean ordered field, the property of being Archimedean (\"every positive real is larger than 1/\"n\" for some positive integer \"n\"\") seems at first sight not to satisfy the transfer principle. The statement \"every positive hyperreal is larger than 1/\"n\" for some positive integer \"n\"\" is false; however\nthe correct interpretation is \"every positive hyperreal is larger than 1/\"n\" for some positive hyperinteger \"n\"\". In other words, the hyperreals appear to be Archimedean to an internal observer living in the non-standard universe, but appear\nto be non-Archimedean to an external observer outside the universe.\n\nA freshman-level accessible formulation of the transfer principle is Keisler's book \"\".\n\nEvery real formula_1 satisfies the inequality\nwhere formula_3 is the integer part function. By a typical application of the transfer principle, every hyperreal formula_1 satisfies the inequality\nwhere formula_6 is the natural extension of the integer part function. If formula_1 is infinite, then the hyperinteger formula_8 is infinite, as well.\n\nHistorically, the concept of number has been repeatedly generalized. The addition of 0 to the natural numbers formula_9 was a major intellectual accomplishment in its time. The addition of negative integers to form formula_10 already constituted a departure from the realm of immediate experience to the realm of mathematical models. The further extension, the rational numbers formula_11, is more familiar to a layperson than their completion formula_12, partly because the reals do not correspond to any physical reality (in the sense of measurement and computation) different from that represented by formula_11. Thus, the notion of an irrational number is meaningless to even the most powerful floating-point computer. The necessity for such an extension stems not from physical observation but rather from the internal requirements of mathematical coherence. The infinitesimals entered mathematical discourse at a time when such a notion was required by mathematical developments at the time, namely the emergence of what became known as the infinitesimal calculus. As already mentioned above, the mathematical justification for this latest extension was delayed by three centuries. Keisler wrote:\n\nThe self-consistent development of the hyperreals turned out to be possible if every true first-order logic statement that uses basic arithmetic (the natural numbers, plus, times, comparison) and quantifies only over the real numbers was assumed to be true in a reinterpreted form if we presume that it quantifies over hyperreal numbers. For example, we can state that for every real number there is another number greater than it:\n\nThe same will then also hold for hyperreals:\n\nAnother example is the statement that if you add 1 to a number you get a bigger number:\n\nwhich will also hold for hyperreals:\n\nThe correct general statement that formulates these equivalences is called the transfer principle. Note that, in many formulas in analysis, quantification is over higher-order objects such as functions and sets, which makes the transfer principle somewhat more subtle than the above examples suggest.\n\nThe transfer principle however doesn't mean that R and *R have identical behavior. For instance, in *R there exists an element \"ω\" such that\n\nbut there is no such number in R. This is possible because the nonexistence of this number cannot be expressed as a first order statement of the above type. A hyperreal number like \"ω\" is called infinitely large; the reciprocals of the infinitely large numbers are the infinitesimals.\n\nThe hyperreals *R form an ordered field containing the reals R as a subfield. Unlike the reals, the hyperreals do not form a standard metric space, but by virtue of their order they carry an order topology.\n\nThe hyperreals can be developed either axiomatically or by more constructively oriented methods. The essence of the axiomatic approach is to assert (1) the existence of at least one infinitesimal number, and (2) the validity of the transfer principle. In the following subsection we give a detailed outline of a more constructive approach. This method allows one to construct the hyperreals if given a set-theoretic object called an ultrafilter, but the ultrafilter itself cannot be explicitly constructed. Vladimir Kanovei and Shelah give a construction of a definable, countably saturated elementary extension of the structure consisting of the reals and all finitary relations on it.\n\nIn its most general form, transfer is a bounded elementary embedding between structures.\n\nThe ordered field R of nonstandard real numbers properly includes the real field R. Like all ordered fields that properly include R, this field is non-Archimedean. It means that some members \"x\" ≠ 0 of R are infinitesimal, i.e.,\n\nThe only infinitesimal in \"R\" is 0. Some other members of R, the reciprocals \"y\" of the nonzero infinitesimals, are infinite, i.e.,\n\nThe underlying set of the field R is the image of R under a mapping \"A\"  \"A\" from subsets \"A\" of R to subsets of R. In every case\n\nwith equality if and only if \"A\" is finite. Sets of the form \"A\" for some formula_22 are called standard subsets of R. The standard sets belong to a much larger class of subsets of R called internal sets. Similarly each function\n\nextends to a function\n\nthese are called standard functions, and belong to the much larger class of internal functions. Sets and functions that are not internal are external.\n\nThe importance of these concepts stems from their role in the following proposition and is illustrated by the examples that follow it.\n\nThe transfer principle:\n\n\n\nThe appropriate setting for the hyperreal transfer principle is the world of \"internal\" entities. Thus, the well-ordering property of the natural numbers by transfer yields the fact that every internal subset of formula_9 has a least element. In this section internal sets are discussed in more detail.\n\n"}
{"id": "82739", "url": "https://en.wikipedia.org/wiki?curid=82739", "title": "Tychonoff's theorem", "text": "Tychonoff's theorem\n\nIn mathematics, Tychonoff's theorem states that the product of any collection of compact topological spaces is compact with respect to the product topology. The theorem is named after Andrey Nikolayevich Tikhonov (whose surname sometimes is transcribed \"Tychonoff\"), who proved it first in 1930 for powers of the closed unit interval and in 1935 stated the full theorem along with the remark that its proof was the same as for the special case. The earliest known published proof is contained in a 1937 paper of Eduard Čech.\n\nSeveral texts identify Tychonoff's theorem as the single most important result in general topology [e.g. Willard, p. 120]; others allow it to share this honor with Urysohn's lemma.\n\nThe theorem depends crucially upon the precise definitions of compactness and of the product topology; in fact, Tychonoff's 1935 paper defines the product topology for the first time. Conversely, part of its importance is to give confidence that these particular definitions are the most useful (i.e. most well-behaved) ones.\n\nIndeed, the Heine–Borel definition of compactness—that every covering of a space by open sets admits a finite subcovering—is relatively recent. More popular in the 19th and early 20th centuries was the Bolzano–Weierstrass criterion that every sequence admits a convergent subsequence, now called sequential compactness. These conditions are equivalent for metrizable spaces, but neither one implies the other in the class of all topological spaces.\n\nIt is almost trivial to prove that the product of two sequentially compact spaces is sequentially compact—one passes to a subsequence for the first component and then a subsubsequence for the second component. An only slightly more elaborate \"diagonalization\" argument establishes the sequential compactness of a countable product of sequentially compact spaces. However, the product of continuum many copies of the closed unit interval (with its usual topology) fails to be sequentially compact with respect to the product topology, even though it is compact by Tychonoff's theorem (e.g., see ).\n\nThis is a critical failure: if \"X\" is a completely regular Hausdorff space, there is a natural embedding from \"X\" into [0,1], where \"C\"(\"X\",[0,1]) is the set of continuous maps from \"X\" to [0,1]. The compactness of [0,1] thus shows that every completely regular Hausdorff space embeds in a compact Hausdorff space (or, can be \"compactified\".) This construction is the Stone–Čech compactification. Conversely, all subspaces of compact Hausdorff spaces are completely regular Hausdorff, so this characterizes the completely regular Hausdorff spaces as those that can be compactified. Such spaces are now called Tychonoff spaces.\n\nTychonoff's theorem has been used to prove many other mathematical theorems. These include theorems about compactness of certain spaces such as the Banach–Alaoglu theorem on the weak-* compactness of the unit ball of the dual space of a normed vector space, and the Arzelà–Ascoli theorem characterizing the sequences of functions in which every subsequence has a uniformly convergent subsequence. They also include statements less obviously related to compactness, such as the De Bruijn–Erdős theorem stating that every minimal \"k\"-chromatic graph is finite, and the Curtis–Hedlund–Lyndon theorem providing a topological characterization of cellular automata.\n\nAs a rule of thumb, any sort of construction that takes as input a fairly general object (often of an algebraic, or topological-algebraic nature) and outputs a compact space is likely to use Tychonoff: e.g., the Gelfand space of maximal ideals of a commutative C* algebra, the Stone space of maximal ideals of a Boolean algebra, and the Berkovich spectrum of a commutative Banach ring.\n\n1) Tychonoff's 1930 proof used the concept of a complete accumulation point.\n\n2) The theorem is a quick corollary of the Alexander subbase theorem.\n\nMore modern proofs have been motivated by the following considerations: the approach to compactness via convergence of subsequences leads to a simple and transparent proof in the case of countable index sets. However, the approach to convergence in a topological space using sequences is sufficient when the space satisfies the first axiom of countability (as metrizable spaces do), but generally not otherwise. However, the product of uncountably many metrizable spaces, each with at least two points, fails to be first countable. So it is natural to hope that a suitable notion of convergence in arbitrary spaces will lead to a compactness criterion generalizing sequential compactness in metrizable spaces that will be as easily applied to deduce the compactness of products. This has turned out to be the case.\n\n3) The theory of convergence via filters, due to Henri Cartan and developed by Bourbaki in 1937, leads to the following criterion: assuming the ultrafilter lemma, a space is compact if and only if each ultrafilter on the space converges. With this in hand, the proof becomes easy: the (filter generated by the) image of an ultrafilter on the product space under any projection map is an ultrafilter on the factor space, which therefore converges, to at least one \"x\". One then shows that the original ultrafilter converges to \"x\" = (\"x\"). In his textbook, Munkres gives a reworking of the Cartan–Bourbaki proof that does not explicitly use any filter-theoretic language or preliminaries.\n\n4) Similarly, the Moore–Smith theory of convergence via nets, as supplemented by Kelley's notion of a universal net, leads to the criterion that a space is compact if and only if each universal net on the space converges. This criterion leads to a proof (Kelley, 1950) of Tychonoff's theorem, which is, word for word, identical to the Cartan/Bourbaki proof using filters, save for the repeated substitution of \"universal net\" for \"ultrafilter base\".\n\n5) A proof using nets but not universal nets was given in 1992 by Paul Chernoff.\n\nAll of the above proofs use the axiom of choice (AC) in some way. For instance, the third proof uses that every filter is contained in an ultrafilter (i.e., a maximal filter), and this is seen by invoking Zorn's lemma. Zorn's lemma is also used to prove Kelley's theorem, that every net has a universal subnet. In fact these uses of AC are essential: in 1950 Kelley proved that Tychonoff's theorem implies the axiom of choice in ZF. Note that one formulation of AC is that the Cartesian product of a family of nonempty sets is nonempty; but since the empty set is most certainly compact, the proof cannot proceed along such straightforward lines. Thus Tychonoff's theorem joins several other basic theorems (e.g. that every nonzero vector space has a basis) in being \"equivalent\" to AC.\n\nOn the other hand, the statement that every filter is contained in an ultrafilter does not imply AC. Indeed, it is not hard to see that it is equivalent to the Boolean prime ideal theorem (BPI), a well-known intermediate point between the axioms of Zermelo-Fraenkel set theory (ZF) and the ZF theory augmented by the axiom of choice (ZFC). A first glance at the second proof of Tychnoff may suggest that the proof uses no more than (BPI), in contradiction to the above. However, the spaces in which every convergent filter has a unique limit are precisely the Hausdorff spaces. In general we must select, for each element of the index set, an element of the nonempty set of limits of the projected ultrafilter base, and of course this uses AC. However, it also shows that the compactness of the product of compact Hausdorff spaces can be proved using (BPI), and in fact the converse also holds. Studying the \"strength\" of Tychonoff's theorem for various restricted classes of spaces is an active area in set-theoretic topology.\n\nThe analogue of Tychonoff's theorem in pointless topology does not require any form of the axiom of choice.\n\nTo prove that Tychonoff's theorem in its general version implies the axiom of choice, we establish that every infinite cartesian product of non-empty sets is nonempty. The trickiest part of the proof is introducing the right topology. The right topology, as it turns out, is the cofinite topology with a small twist. It turns out that every set given this topology automatically becomes a compact space. Once we have this fact, Tychonoff's theorem can be applied; we then use the finite intersection property (FIP) definition of compactness. The proof itself (due to J. L. Kelley) follows:\n\nLet {\"A\"} be an indexed family of nonempty sets, for \"i\" ranging in \"I\" (where \"I\" is an arbitrary indexing set). We wish to show that the cartesian product of these sets is nonempty. Now, for each \"i\", take \"X\" to be \"A\" with the index \"i\" itself tacked on (renaming the indices using the disjoint union if necessary, we may assume that \"i\" is not a member of \"A\", so simply take \"X\" = \"A\" ∪ {\"i\"}).\n\nNow define the cartesian product\n\nalong with the natural projection maps \"π\" which take a member of \"X\" to its \"i\"th term.\n\nWe give each \"X\" the topology whose open sets are the cofinite subsets of \"X\", plus the empty set (the cofinite topology) \"and\" the singleton {\"i\"}.\nThis makes \"X\" compact, and by Tychonoff's theorem, \"X\" is also compact (in the product topology). The projection maps are continuous; all the \"A\"'s are closed, being complements of the singleton open set {\"i\"} in \"X\". So the inverse images π(\"A\") are closed subsets of \"X\". We note that\n\nand prove that these inverse images are nonempty and have the FIP. Let \"i\", ..., \"i\" be a finite collection of indices in \"I\". Then the \"finite\" product \"A\" × ... × \"A\"\nis non-empty (only finitely many choices here, so AC is not needed); it merely consists of \"N\"-tuples. Let \"a\" = (\"a\", ..., \"a\") be such an \"N\"-tuple. We extend \"a\" to the whole index set: take \"a\" to the function \"f\" defined by \"f\"(\"j\") = \"a\" if \"j\" = \"i\", and \"f\"(\"j\") = \"j\" otherwise. \"This step is where the addition of the extra point to each space is crucial\", for it allows us to define \"f\" for everything outside of the \"N\"-tuple in a precise way without choices (we can already choose, by construction, \"j\" from \"X\" ). π\"\"(\"f\") = \"a\" is obviously an element of each \"A\" so that \"f\" is in each inverse image; thus we have\n\nBy the FIP definition of compactness, the entire intersection over \"I\" must be nonempty, and the proof is complete.\n\n\n"}
{"id": "14703193", "url": "https://en.wikipedia.org/wiki?curid=14703193", "title": "Type inhabitation", "text": "Type inhabitation\n\nIn type theory, a branch of mathematical logic, in a given typed calculus, the type inhabitation problem for this calculus is the following problem: given a type formula_1 and a typing environment formula_2, does there exist a formula_3-term M such that formula_4? With an empty type environment, such an M is said to be an inhabitant of formula_1.\n\nIn the case of simply typed lambda calculus, a type has an inhabitant if and only if its corresponding proposition is a tautology of minimal implicative logic. Similarly, a System F type has an inhabitant if and only if its corresponding proposition is a tautology of second-order logic.\n\nFor most typed calculi, the type inhabitation problem is very hard. Richard Statman proved that for simply typed lambda calculus the type inhabitation problem is PSPACE-complete. For other calculi, like System F, the problem is even undecidable.\n\n"}
{"id": "1017002", "url": "https://en.wikipedia.org/wiki?curid=1017002", "title": "Upper topology", "text": "Upper topology\n\nIn mathematics, the upper topology on a partially ordered set \"X\" is the coarsest topology in which the closure of a singleton formula_1 is the order section formula_2 for each formula_3. If formula_4 is a partial order, the upper topology is the least order consistent topology in which all open sets are up-sets. However, not all up-sets must necessarily be open sets. The lower topology induced by the preorder is defined similarly in terms of the down-sets. The preoder inducing the upper topology is its specialization preorder, but the specialization preorder of the lower topology is opposite to the inducing preorder.\n\nThe real upper topology is most naturally defined on the upper-extended real line formula_5 by the system formula_6 of open sets. Similarly, the real lower topology formula_7 is naturally defined on the lower real line formula_8. A real function on a topological space is upper semi-continuous if and only if it is lower-continuous, i.e. is continuous with respect to the lower topology on the lower-extended line formula_9. Similarly, a function into the upper real line is lower semi-continuous if and only if it is upper-continuous, i.e. is continuous with respect to the upper topology on formula_10. \n\n"}
{"id": "538119", "url": "https://en.wikipedia.org/wiki?curid=538119", "title": "Vanishing point", "text": "Vanishing point\n\nA vanishing point is a point on the image plane of a perspective drawing where the two-dimensional perspective projections (or drawings) of mutually parallel lines in three-dimensional space appear to converge. When the set of parallel lines is perpendicular to a picture plane, the construction is known as one-point perspective, and their vanishing point corresponds to the oculus, or \"eye point\", from which the image should be viewed for correct perspective geometry. Traditional linear drawings use objects with one to three sets of parallels, defining one to three vanishing points.\n\nThe vanishing point may also be referred to as the \"direction point\", as lines having the same directional vector, say \"D\", will have the same vanishing point. Mathematically, let be a point lying on the image plane, where is the focal length (of the camera associated with the image), and let be the unit vector associated with , where . If we consider a straight line in space with the unit vector and its vanishing point , the unit vector associated with is equal to , assuming both are assumed to point towards the image plane.\n\nWhen the image plane is parallel to two world-coordinate axes, lines parallel to the axis which is cut by this image plane will have images that meet at a single vanishing point. Lines parallel to the other two axes will not form vanishing points as they are parallel to the image plane. This is one-point perspective. Similarly, when the image plane intersects two world-coordinate axes, lines parallel to those planes will meet form two vanishing points in the picture plane. This is called two-point perspective. In three-point perspective the image plane intersects the , , and axes and therefore lines parallel to these axes intersect, resulting in three different vanishing points.\n\nThe vanishing point theorem is the principal theorem in the science of perspective. It says that the image in a picture plane of a line in space, not parallel to the picture, is determined by its intersection with and its vanishing point. Some authors have used the phrase, \"the image of a line includes its vanishing point\". Guidobaldo del Monte gave several verifications, and Humphry Ditton called the result the \"main and Great Proposition\". Brook Taylor wrote the first book in English on perspective in 1714, which introduced the term \"vanishing point\" and was the first to fully explain the geometry of multipoint perspective, and historian Kirsti Andersen compiled these observations. She notes, in terms of projective geometry, the vanishing point is the image of the point at infinity associated with , as the sightline from through the vanishing point is parallel to .\n\nAs a vanishing point originates in a line, so a vanishing line originates in a plane that is not parallel to the picture . Given the eye point , and the plane parallel to and lying on , then the vanishing line of is . For example, when is the ground plane and is the horizon plane, then the vanishing line of is the horizon line . Anderson notes, \"Only one particular vanishing line occurs, often referred to as the \"horizon\".\n\nTo put it simply, the vanishing line of some plane, say , is obtained by the intersection of the image plane with another plane, say , parallel to the plane of interest (), passing through the camera center. For different sets of lines parallel to this plane , their respective vanishing points will lie on this vanishing line. The horizon line is a theoretical line that represents the eye level of the observer. If the object is below the horizon line, its vanishing lines angle up to the horizon line. If the object is above, they slope down. All vanishing lines end at the horizon line.\n\n1. Projections of two sets of parallel lines lying in some plane appear to converge, i.e. the vanishing point associated with that pair, on a horizon line, or vanishing line formed by the intersection of the image plane with the plane parallel to and passing through the pinhole. \nProof: Consider the ground plane , as which is, for the sake of simplicity, orthogonal to the image plane. Also, consider a line that lies in the plane , which is defined by the equation .\nUsing perspective pinhole projections, a point on projected on the image plane will have coordinates defined as,\nThis is the parametric representation of the image of the line with as the parameter. When it stops at the point on the axis of the image plane. This is the vanishing point corresponding to all parallel lines with slope in the plane . All vanishing points associated with different lines with different slopes belonging to plane will lie on the axis, which in this case is the horizon line.\n\n2. Let , , and be three mutually orthogonal straight lines in space and , , be the three corresponding vanishing points respectively. If we know the coordinates of one of these points, say , and the direction of a straight line on the image plane, which passes through a second point, say , we can compute the coordinates of both and \n\n3. Let , , and be three mutually orthogonal straight lines in space and , , be the three corresponding vanishing points respectively. The orthocenter of the triangle with vertices in the three vanishing points is the intersection of the optical axis and the image plane.\n\nA curvilinear perspective is a drawing with either 4 or 5 vanishing points. In 5-point perspective the vanishing points are mapped into a circle with 4 vanishing points at the cardinal headings N, W, S, E and one at the circle's origin.\n\nA reverse perspective is a drawing with vanishing points that are placed outside the painting with the illusion that they are \"in front of\" the painting.\n\nSeveral methods for vanishing point detection make use of the line segments detected in images. Other techniques involve considering the intensity gradients of the image pixels directly.\n\nThere are significantly large numbers of vanishing points present in an image. Therefore, the aim is to detect the vanishing points that correspond to the principal directions of a scene. This is generally achieved in two steps. The first step, called the accumulation step, as the name suggests, clusters the line segments with the assumption that a cluster will share a common vanishing point. The next step finds the principal clusters present in the scene and therefore it is called the search step.\n\nIn the accumulation step, the image is mapped onto a bounded space called the accumulator space. The accumulator space is partitioned into units called cells. Barnard assumed this space to be a Gaussian sphere centered on the optical center of the camera as an accumulator space. A line segment on the image corresponds to a great circle on this sphere, and the vanishing point in the image is mapped to a point. The Gaussian sphere has accumulator cells that increase when a great circle passes through them, i.e. in the image a line segment intersects the vanishing point. Several modifications have been made since, but one of the most efficient techniques was using the Hough Transform, mapping the parameters of the line segment to the bounded space. Cascaded Hough Transforms have been applied for multiple vanishing points.\n\nThe process of mapping from the image to the bounded spaces causes the loss of the actual distances between line segments and points.\n\nIn the search step, the accumulator cell with the maximum number of line segments passing through it is found. This is followed by removal of those line segments, and the search step is repeated until this count goes below a certain threshold. As more computing power is now available, points corresponding to two or three mutually orthogonal directions can be found.\n\n\n\n"}
{"id": "20797548", "url": "https://en.wikipedia.org/wiki?curid=20797548", "title": "Vertex cycle cover", "text": "Vertex cycle cover\n\nIn mathematics, a vertex cycle cover (commonly called simply cycle cover) of a graph \"G\" is a set of cycles which are subgraphs of \"G\" and contain all vertices of \"G\". \n\nIf the cycles of the cover have no vertices in common, the cover is called vertex-disjoint or sometimes simply disjoint cycle cover. In this case the set of the cycles constitutes a spanning subgraph of \"G\". A disjoint cycle cover of an undirected graph (if it exists) can be found in polynomial time by transforming the problem into a problem of finding a perfect matching in a larger graph.\n\nIf the cycles of the cover have no edges in common, the cover is called edge-disjoint or simply disjoint cycle cover.\n\nSimilar definitions exist for digraphs, in terms of directed cycles. Finding a vertex-disjoint cycle cover of a directed graph can also be performed in polynomial time by a similar reduction to perfect matching. However, adding the condition that each cycle should have length at least 3 makes the problem NP-hard.\n\nThe permanent of a (0,1)-matrix is equal to the number of vertex-disjoint cycle covers of a directed graph with this adjacency matrix. This fact is used in a simplified proof showing that computing the permanent is #P-complete.\n\nThe problems of finding a vertex disjoint and edge disjoint cycle covers with minimal number of cycles are NP-complete. The problems are not in complexity class APX. The variants for digraphs are not in APX either.\n"}
{"id": "34191584", "url": "https://en.wikipedia.org/wiki?curid=34191584", "title": "Walter Hayman", "text": "Walter Hayman\n\nWalter Kurt Hayman FRS (born in Cologne, 6 January 1926) is a British mathematician known for contributions to complex analysis. He is Emeritus Professor at Imperial College London.\n\nHayman was born in Cologne, Germany, the grandson of mathematician Kurt Hensel. He immigrated to Britain in 1938, studying at Gordonstoun School, and later at St John's College, Cambridge, with Mary Cartwright and John Edensor Littlewood. He taught at King's College, Newcastle, and the University of Exeter.\n\nIn 1947, he married Margaret Riley Crann: together, they founded the British Mathematical Olympiad.\n\nHayman was elected to the Royal Society in 1956 and of the Finnish Academy of Science and Letters in 1978: he was elected \"Foreign member\" of the Accademia dei Lincei on 16 December 1985. In 1992 he received an honorary doctorate from the Faculty of Mathematics and Science at Uppsala University, Sweden In 1995 He was awarded the De Morgan Medal by the London Mathematical Society. In 2008, an issue of the Journal \"Computational Methods and Function Theory\" was dedicated to him on the occasion of his 80th birthday.\n\n\n\n\n"}
{"id": "2848762", "url": "https://en.wikipedia.org/wiki?curid=2848762", "title": "ZetaGrid", "text": "ZetaGrid\n\nZetaGrid was at one time the largest distributed computing project designed to explore roots of the Riemann zeta function, checking over one billion roots a day.\n\nRoots of the zeta function are of particular interest in mathematics, since the presence of even a single one that is out of line with the rest would disprove the Riemann hypothesis, with far-reaching consequences for all of mathematics. So far, every single one of them has failed to provide a counterexample to the Riemann hypothesis.\n\nThe project ended in November 2005 due to instability of hosting provider. Over 10 first zeroes were checked. After the results have been analyzed, the project administrator has stated that they will be posted on the American Mathematical Society website.\n\n"}
